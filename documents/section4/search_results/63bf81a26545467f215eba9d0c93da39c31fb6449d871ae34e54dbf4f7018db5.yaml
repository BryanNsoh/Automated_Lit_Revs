- DOI: https://doi.org/10.1016/j.infsof.2017.06.001
  analysis: '>'
  authors:
  - Sergi Nadal
  - Victor Herrero
  - Óscar Romero
  - Alberto Abelló
  - Xavier Franch
  - Stijn Vansummeren
  - Danilo Valerio
  citation_count: 56
  full_citation: '>'
  full_text: '>

    Typesetting math: 100% Skip to main content Skip to article Journals & Books Search
    Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download
    full issue Outline Abstract Keywords 1. Introduction 2. Big Data definition and
    dimensions 3. Related work 4. Bolster: a semantic extension for the λ-architecture
    5. Exemplar use case 6. Bolster instantiation 7. Industrial experiences 8. Conclusions
    Acknowledgements References Show full outline Cited by (59) Figures (8) Show 2
    more figures Tables (13) Table 1 Table 2 Table 3 Table Table Table Show all tables
    Information and Software Technology Volume 90, October 2017, Pages 75-92 A software
    reference architecture for semantic-aware Big Data systems Author links open overlay
    panel Sergi Nadal a, Victor Herrero a, Oscar Romero a, Alberto Abelló a, Xavier
    Franch a, Stijn Vansummeren b, Danilo Valerio c Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.infsof.2017.06.001 Get rights and content Abstract
    Context: Big Data systems are a class of software systems that ingest, store,
    process and serve massive amounts of heterogeneous data, from multiple sources.
    Despite their undisputed impact in current society, their engineering is still
    in its infancy and companies find it difficult to adopt them due to their inherent
    complexity. Existing attempts to provide architectural guidelines for their engineering
    fail to take into account important Big Data characteristics, such as the management,
    evolution and quality of the data. Objective: In this paper, we follow software
    engineering principles to refine the λ-architecture, a reference model for Big
    Data systems, and use it as seed to create Bolster, a software reference architecture
    (SRA) for semantic-aware Big Data systems. Method: By including a new layer into
    the λ-architecture, the Semantic Layer, Bolster is capable of handling the most
    representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability
    and Veracity). Results: We present the successful implementation of Bolster in
    three industrial projects, involving five organizations. The validation results
    show high level of agreement among practitioners from all organizations with respect
    to standard quality factors. Conclusion: As an SRA, Bolster allows organizations
    to design concrete architectures tailored to their specific needs. A distinguishing
    feature is that it provides semantic-awareness in Big Data Systems. These are
    Big Data system implementations that have components to simplify data definition
    and exploitation. In particular, they leverage metadata (i.e., data describing
    data) to enable (partial) automation of data exploitation and to aid the user
    in their decision making processes. This simplification supports the differentiation
    of responsibilities into cohesive roles enhancing data governance. Previous article
    in issue Next article in issue Keywords Big DataSoftware reference architectureSemantic-awareData
    managementData analysis 1. Introduction Major Big Data players, such as Google
    or Amazon, have developed large Big Data systems that align their business goals
    with complex data management and analysis. These companies exemplify an emerging
    paradigm shift towards data-driven organizations, where data are turned into valuable
    knowledge that becomes a key asset for their business. In spite of the inherent
    complexity of these systems, software engineering methods are still not widely
    adopted in their construction [21]. Instead, they are currently developed as ad-hoc,
    complex architectural solutions that blend together several software components
    (usually coming from open-source projects) according to the system requirements.
    An example is the Hadoop ecosystem. In Hadoop, lots of specialized Apache projects
    co-exist and it is up to Big Data system architects to select and orchestrate
    some of them to produce the desired result. This scenario, typical from immature
    technologies, raises high-entry barriers for non-expert players who struggle to
    deploy their own solutions overwhelmed by the amount of available and overlapping
    components. Furthermore, the complexity of the solutions currently produced requires
    an extremely high degree of specialization. The system end-user needs to be what
    is nowadays called a “data scientist”, a data analysis expert proficient in managing
    data stored in distributed systems to accommodate them to his/her analysis tasks.
    Thus, s/he needs to master two profiles that are clearly differentiated in traditional
    Business Intelligence (BI) settings: the data steward and the data analyst, the
    former responsible of data management and the latter of data analysis. Such combined
    profile is rare and subsequently entails an increment of costs and knowledge lock-in.
    Since the current practice of ad-hoc design when implementing Big Data systems
    is hence undesirable, improved software engineering approaches specialized for
    Big Data systems are required. In order to contribute towards this goal, we explore
    the notion of Software Reference Architecture (SRA) and present Bolster, an SRA
    for Big Data systems. SRAs are generic architectures for a class of software systems
    [3]. They are used as a foundation to derive software architectures adapted to
    the requirements of a particular organizational context. Therefore, they open
    the door to effective and efficient production of complex systems. Furthermore,
    in an emergent class of systems (such as Big Data systems), they make it possible
    to synthesize in a systematic way a consolidated solution from available knowledge.
    As a matter of fact, the detailed design of such a complex architecture has already
    been designated as a major Big Data software engineering research challenge [12],
    [39]. Well-known examples of SRAs include the successful AUTOSAR SRA [40] for
    the automotive industry, the Internet of Things Architecture (IoT-A) [66], an
    SRA for web browsers [24] and the NIST Cloud Computing Reference Architecture
    [38]. As an SRA, Bolster paves the road to the prescriptive development of software
    architectures that lie at the heart of every new Big Data system. Using Bolster,
    the work of the software architect is not to produce a new architecture from a
    set of independent components that need to be assembled. Instead, the software
    architect knows beforehand what type of components are needed and how they are
    interconnected. Therefore, his/her main responsibility is the selection of technologies
    for those components given the concrete requirements and the goals of the organization.
    Bolster is a step towards the homogenization and definition of a Big Data Management
    System (BDMS), as done in the past for Database Management Systems (DBMS) [17]
    and Distributed Database Management Systems (DDBMS) [49]. A distinguishing feature
    of Bolster is that it provides an SRA for semantic-aware Big Data Systems. These
    are Big Data system implementations that have components to simplify data definition
    and data exploitation. In particular, such type of systems leverage on metadata
    (i.e., data describing data) to enable (partial) automation of data exploitation
    and to aid the user in their decision making processes. This definition supports
    the differentiation of responsibilities into cohesive roles, the data steward
    and the data analyst, enhancing data governance. Contributions The main contributions
    of this paper are as follows: • Taking as building blocks the five “V’s” that
    define Big Data systems (see Section 2), we define the set of functional requirements
    sought in each to realize a semantic-aware Big Data architecture. Such requirements
    will further drive the design of Bolster. • Aiming to study the related work on
    Big Data architectures, we perform a lightweight Systematic Literature Review.
    Its main outcome consists on the division of 21 works into two great families
    of Big Data architectures. • We present Bolster, an SRA for semantic-aware Big
    Data systems. Combining principles from the two identified families, it succeeds
    on satisfying all the posed Big Data requirements. Bolster relies on the systematic
    use of semantic annotations to govern its data lifecycle, overcoming the shortcomings
    present in the studied architectures. • We propose a framework to simplify the
    instantiation of Bolster to different Big Data ecosystems. For the sake of this
    paper, we precisely focus on the components of the Apache Hadoop and Amazon Web
    Services (AWS) ecosystems. • We detail the deployment of Bolster in three different
    industrial scenarios, showcasing how it adapts to their specific requirements.
    Furthermore, we provide the results of its validation after interviewing practitioners
    in such organizations. Outline The paper is structured as follows. Section 2 introduces
    the Big Data dimensions and requirements sought. Section 3 presents the Systematic
    Literature Review. 4 Bolster: a semantic extension for the, 5 Exemplar use case
    and 6 detail the elements that compose Bolster, an exemplar case study implementing
    it and the proposed instantiation method respectively. Further, Sections 7 report
    the industrial deployments and validation. Finally, Section 8 wraps up the main
    conclusions derived from this work. 2. Big Data definition and dimensions Big
    Data is a natural evolution of BI, and inherits its ultimate goal of transforming
    raw data into valuable knowledge. Nevertheless, traditional BI architectures,
    whose de-facto architectural standard is the Data Warehouse (DW), cannot be reused
    in Big Data settings. Indeed, the so-popular characterization of Big Data in terms
    of the three “V’s (Volume, Velocity and Variety)” [31], refers to the inability
    of DW architectures, which typically rely on relational databases, to deal and
    adapt to such large, rapidly arriving and heterogeneous amounts of data. To overcome
    such limitations, Big Data architectures rely on NOSQL (Not Only SQL), co-relational
    database systems where the core data structure is not the relation [43], as their
    building blocks. Such systems propose new solutions to address the three V’s by
    (i) distributing data and processing in a cluster (typically of commodity machines)
    and (ii) by introducing alternative data models. Most NOSQL systems distribute
    data (i.e., fragment and replicate it) in order to parallelize its processing
    while exploiting the data locality principle, ideally yielding a close-to-linear
    scale-up and speed-up [49]. As enunciated by the CAP theorem [9], distributed
    NOSQL systems must relax the well-known ACID (Atomicity, Consistency, Isolation,
    Durability) set of properties and the traditional concept of transaction to cope
    with large-scale distributed processing. As result, data consistency may be compromised
    but it enables the creation of fault-tolerant systems able to parallelize complex
    and time-consuming data processing tasks. Orthogonally, NOSQL systems also focus
    on new data models to reduce the impedance mismatch [23]. Graph, key-value or
    document-based modeling provide the needed flexibility to accommodate dynamic
    data evolution and overcome the traditional staticity of relational DWs. Such
    flexibility is many times acknowledged by referring to such systems as schemaless
    databases. These two premises entailed a complete rethought of the internal structures
    as well as the means to couple data analytics on top of such systems. Consequently,
    it also gave rise to the Small and Big Analytics concepts [57], which refer to
    performing traditional OLAP/Query&Reporting to gain quick insight into the data
    sets by means of descriptive analytics (i.e., Small Analytics) and Data Mining/Machine
    Learning to enable predictive analytics (i.e., Big Analytics) on Big Data systems,
    respectively. In the last years, researchers and practitioners have widely extended
    the three “V’s” definition of Big Data as new challenges appear. Among all existing
    definitions of Big Data, we claim that the real nature of Big Data can be covered
    by five of those “V’s”, namely: (a) Volume, (b) Velocity, (c) Variety, (d) Variability
    and (e) Veracity. Note that, in contrast to other works, we do not consider Value.
    Considering that any decision support system (DSS) is the result of a tightly
    coupled collaboration between business and IT [18], Value falls into the business
    side while the aforementioned dimensions focus on the IT side. In the rest of
    this paper we refer to the above-mentioned “V’s” also as Big Data dimensions.
    In this section, we provide insights on each dimension as well as a list of linked
    requirements that we consider a Big Data architecture should fulfill. Such requirements
    were obtained in two ways: firstly inspired by reviewing related literature on
    Big Data requirements [1], [10], [14], [16], [54]; secondly they were validated
    and refined by informally discussing with the stakeholders from several industrial
    Big Data projects (see Section 7) and obtaining their feedback. Finally, a summary
    of devised requirements for each Big Data dimension is depicted in Table 1. Note
    that such list does not aim to provide an exhaustive set of requirements for Big
    Data architectures, but a high-level baseline on the main requirements any Big
    Data architecture should achieve to support each dimension. Table 1. Requirements
    for a Big Data Architecture (BDA). Requirement Empty Cell 1. Volume R1.1 The BDA
    shall provide scalable storage of massive data sets. R1.2 The BDA shall be capable
    of supporting descriptive analytics. R1.3 The BDA shall be capable of supporting
    predictive and prescriptive analytics. 2. Velocity R2.1 The BDA shall be capable
    of ingesting multiple, continuous, rapid, time varying data streams. R2.2 The
    BDA shall be capable of processing data in a (near) real-time manner. 3. Variety
    R3.1 The BDA shall support ingestion of raw data (structured, semi-structured
    and unstructured). R3.2 The BDA shall support storage of raw data (structured,
    semi-structured and unstructured). R3.3 The BDA shall provide mechanisms to handle
    machine-readable schemas for all present data. 4. Variability R4.1 The BDA shall
    provide adaptation mechanisms to schema evolution. R4.2 The BDA shall provide
    adaptation mechanisms to data evolution. R4.3 The BDA shall provide mechanisms
    for automatic inclusion of new data sources. 5. Veracity R5.1 The BDA shall provide
    mechanisms for data provenance. R5.2 The BDA shall provide mechanisms to measure
    data quality. R5.3 The BDA shall provide mechanisms for tracing data liveliness.
    R5.4 The BDA shall provide mechanisms for managing data cleaning. 2.1. Volume
    Big Data has a tight connection with Volume, which refers to the large amount
    of digital information produced and stored in these systems, nowadays shifting
    from terabytes to petabytes (R1.1). The most widespread solution for Volume is
    data distribution and parallel processing, typically using cloud-based technologies.
    Descriptive analysis [55] (R1.2), such as reporting and OLAP, has shown to naturally
    adapt to distributed data management solutions. However, predictive and prescriptive
    analysis (R1.3) show higher-entry barriers to fit into such distributed solutions
    [60]. Classically, data analysts would dump a fragment of the DW in order to run
    statistical methods in specialized software, (e.g., R or SAS) [48]. However, this
    is clearly unfeasible in the presence of Volume, and thus typical predictive and
    prescriptive analysis methods must be rethought to run within the distributed
    infrastructure, exploiting the data locality principle [49]. 2.2. Velocity Velocity
    refers to the pace at which data are generated, ingested (i.e., dealt with the
    arrival of), and processed, usually in the range of milliseconds to seconds. This
    gave rise to the concept of data stream [5] and creates two main challenges. First,
    data stream ingestion, which relies on a sliding window buffering model to smooth
    arrival irregularities (R2.1). Second, data stream processing, which relies on
    linear or sublinear algorithms to provide near real-time analysis (R2.2). 2.3.
    Variety Variety deals with the heterogeneity of data formats, paying special attention
    to semi-structured and unstructured external data (e.g., text from social networks,
    JSON/XML-formatted scrapped data, Internet of Things sensors, etc.) (R3.1). Aligned
    with it, the novel concept of Data Lake has emerged [59], a massive repository
    of data in its original format. Unlike DW that follows a schema on-write approach,
    Data Lake proposes to store data as they are produced without any preprocessing
    until it is clear how they are going to be analyzed (R3.2), following the load-first
    model-later principle. The rationale behind a Data Lake is to store raw data and
    let the data analyst decide how to cook them. However, the extreme flexibility
    provided by the Data Lake is also its biggest flaw. The lack of schema prevents
    the system from knowing what is exactly stored and this burden is left on the
    data analyst shoulders (R3.3). Since loading is not that much of a challenge compared
    to the data transformations (data curation) to be done before exploiting the data,
    the Data Lake approach has received lots of criticism and the uncontrolled dump
    of data in the Data Lake is referred to as Data Swamp [58]. 2.4. Variability Variability
    is concerned with the evolving nature of ingested data, and how the system copes
    with such changes for data integration and exchange. In the relational model,
    mechanisms to handle evolution of intension (R4.1) (i.e., schema-based), and extension
    (R4.2) (i.e., instance-based) are provided. However, achieving so in Big Data
    systems entails an additional challenge due to the schemaless nature of NOSQL
    databases. Moreover, during the lifecycle of a Big Data-based application, data
    sources may also vary (e.g., including a new social network or because of an outage
    in a sensor grid). Therefore, mechanisms to handle data source evolution should
    also be present in a Big Data architecture (R4.3). 2.5. Veracity Veracity has
    a tight connection with data quality, achieved by means of data governance protocols.
    Data governance concerns the set of processes and decisions to be made in order
    to provide an effective management of the data assets [34]. This is usually achieved
    by means of best practices. These can either be defined at the organization level,
    depicting the business domain knowledge, or at a generic level by data governance
    initiatives (e.g., Six Sigma [26]). However, such large and heterogeneous amount
    of data present in Big Data systems begs for the adoption of an automated data
    governance protocol, which we believe should include, but might not be limited
    to, the following elements: • Data provenance (R5.1), related to how any piece
    of data can be tracked to the sources to reproduce its computation for lineage
    analysis. This requires storing metadata for all performed transformations into
    a common data model for further study or exchange (e.g., the Open Provenance Model
    [44]). • Measurement of data quality (R5.2), providing metrics such as accuracy,
    completeness, soundness and timeliness, among others [6]. Tagging all data with
    such adornments prevents analysts from using low quality data that might lead
    to poor analysis outcomes (e.g., missing values for some data). • Data liveliness
    (R5.3), leveraging on conversational metadata [59] which records when data are
    used and what is the outcome users experience from it. Contextual analysis techniques
    [4] can leverage such metadata in order to aid the user in future analytical tasks
    (e.g., query recommendation [20]). • Data cleaning (R5.4), comprising a set of
    techniques to enhance data quality like standardization, deduplication, error
    localization or schema matching. Usually such activities are part of the preprocessing
    phase, however they can be introduced along the complete lifecycle. The degree
    of automation obtained here will vary depending on the required user interaction,
    for instance any entity resolution or profiling activity will infer better if
    user aided. Including the aforementioned automated data governance elements into
    an architecture is a challenge, as they should not be intrusive. First, they should
    be transparent to developers and run as under the hood processes. Second, they
    should not overburden the overall system performance (e.g., Interlandi et al.
    [28] show how automatic data provenance support entails a 30% overhead on performance).
    2.6. Summary The discussion above shows that current BI architectures (i.e., relying
    on RDMS), cannot be reused in Big Data scenarios. Such modern DSS must adopt NOSQL
    tools to overcome the issues posed by Volume, Velocity and Variety. However, as
    discussed for Variability and Veracity, NOSQL does not satisfy key requirements
    that should be present in a mature DSS. Thus, Bolster is designed to completely
    satisfy the aforementioned set of requirements, summarized in Table 1. 3. Related
    work In this section, we follow the principles and guidelines of Systematic Literature
    Reviews (SLR) as established in [35]. The purpose of this review is to systematically
    analyze the current landscape of Big Data architectures, with the goal to identify
    how they meet the devised requirements, and thus aid in the design of an SRA.
    Nonetheless, in this paper we do not aim to perform an exhaustive review, but
    to depict, in a systematic manner, an overview on the landscape of Big Data architectures.
    To this end, we perform a lightweight SLR, where we focus on high quality works
    and evaluate them with respect to the previously devised requirements. 3.1. Selection
    of papers The search was ranged from 2010 to 2016, as the first works on Big Data
    architectures appeared by then. The search engine selected was Scopus,1 as it
    indexes all journals with a JCR impact factor, as well as the most relevant conferences
    based on the CORE index.2 We have searched papers with title, abstract or keywords
    matching the terms “big data” AND “architecture”. The list was further refined
    by selecting papers only in the “Computer Science” and “Engineering” subject areas
    and only documents in English. Finally, only conference papers, articles, book
    chapters and books were selected. By applying the search protocol we obtained
    1681 papers covering the search criteria. After a filter by title, 116 papers
    were kept. We further applied a filter by abstract in order to specifically remove
    works describing middlewares as part of a Big Data architecture (e.g., distributed
    storage or data stream management systems). This phase resulted in 44 selected
    papers. Finally, after reading them, sixteen papers were considered relevant to
    be included in this section. Furthermore, five non-indexed works considered grey
    literature were additionally added to the list, as considered relevant to depict
    the state of the practice in industry. The process was performed by our research
    team, and in case of contradictions a meeting was organized in order to reach
    consensus. Details of the search and filtering process are available at [46].
    3.2. Analysis In the following subsections, we analyze to which extent the selected
    Big Data architectures fulfill the requirements devised in Section 2. Each architecture
    is evaluated by checking whether it satisfies a given requirement (✓) or it does
    not (✗). Results are summarized in Table 2, where we make the distinction between
    custom architectures and SRAs. For the sake of readability, references to studied
    papers have been substituted for their position in Table 2. Table 2. Fulfillment
    of each requirement in the related work. Custom Architectures Volume Velocity
    Variety Variability Veracity Empty Cell R1.1 R1.2 R1.3 R2.1 R2.2 R3.1 R3.2 R3.3
    R4.1 R4.2 R4.3 R5.1 R5.2 R5.3 R5.4 A1 CQELS [52] ✗ ✓ ✗ ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗ ✗ ✗ ✗
    A2 AllJoyn Lambda [64] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A3 CloudMan [53] ✓ ✓ ✓ ✗
    ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A4 AsterixDB [2] ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ A5 M3Data
    [29] ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ A6 [61] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A7 λ-arch.
    [42] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A8 Solid [41] ✗ ✓ ✗ ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗
    A9 Liquid [13] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ A10 RADStack [68] ✓ ✓ ✗ ✓ ✓ ✓ ✗ ✓
    ✗ ✗ ✗ ✗ ✗ ✗ ✓ A11 [37] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A12 HaoLap [56] ✓ ✓ ✗ ✗ ✗
    ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A13 [65] ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✓ ✓ ✗ ✓ A14 SHMR [25] ✓ ✓ ✗
    ✗ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ A15 Tengu [62] ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✓ ✗ ✗ ✗ ✗ A16 [67]
    ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ A17 [11] ✓ ✓ ✓ ✗ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ A18 D-Ocean
    [71] ✓ ✓ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ Software Reference Architectures Volume Velocity
    Variety Variability Veracity Empty Cell R1.1 R1.2 R1.3 R2.1 R2.2 R3.1 R3.2 R3.3
    R4.1 R4.2 R4.3 R5.1 R5.2 R5.3 R5.4 A19 NIST [22] ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✓ ✗ ✓ ✓ ✓
    A20 [50] ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ A21 [19] ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗
    Bolster ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 3.2.1. Requirements on Volume Most architectures
    are capable of dealing with storage of massive data sets (R1.1). However, we claim
    those relying on Semantic Web principles (i.e. storing RDF data), [A1,A8] cannot
    deal with such requirement as they are inherently limited by the storage capabilities
    of triplestores. Great effort is put on improving such capabilities [69], however
    no mature scalable solution is available in the W3C recommendations.3 There is
    an exception to the previous discussion, as SHMR [A14] stores semantic data on
    HBase. However, this impacts its analytical capabilities with respect to those
    offered by triplestores. Oppositely, Liquid [A9] is the only case where no data
    are stored, offering only real-time support and thus not addressing the Volume
    dimension of Big Data. Regarding analytical capabilities, most architectures satisfy
    the descriptive level (R1.2) via SQL-like [A4,A10,A11,A18] or SPARQL [A1,A8] languages.
    Furthermore, those offering MapReduce or similar interfaces [A2,A3,A6,A13,A14,A15,A20]
    meet the predictive and prescriptive level (R1.3). HaoLap [A12] and SHMR [A14]
    are the only works where MapReduce is narrowed to descriptive queries. 3.2.2.
    Requirements on Velocity Several architectures are capable of ingesting data streams
    (R2.1), either by dividing the architecture in specialized Batch and Real-time
    Layers [A2,A6,A7,A10,A11,A15,A20], by providing specific channels like data feeds
    [A4] or by solely considering streams as input type [A1,A8,A9]. Regarding processing
    of such data streams (R2.2), all architectures dealing with its ingestion can
    additionally perform processing, with the exception of AsterixDB [A4] and M3Data
    [A5], where data streams are stored prior to querying them. 3.2.3. Requirements
    on Variety Variety is handled in diverse ways in the studied architectures. Concerning
    ingestion of raw data (R3.1), few proposals cannot deal with such requirement,
    either because they are narrowed to ingest specific data formats [A8,A16], or
    because specific wrappers need to be defined on the sources [A1,A19]. Concerning
    storage of raw data (R3.2), many architectures define views to merge and homogenize
    different formats into a common one (including those that do it at ingestion time)
    [A4,A5,A10,A12,A14,A15,A17]. On the other hand, the λ-architecture and some of
    the akin architectures [A2,A6,A7,A11] and [A20] are the only ones natively storing
    raw data. In schema management (R3.3), all those architectures that favored ingesting
    and storing raw data cannot deal with such requirement, as no additional mechanism
    is present to handle it. Oppositely, the ones defining unified views are able
    to manage them, likewise relational database schemas. There is an exception to
    the previous discussion, D-Ocean [A18], which defines a data model for unstructured
    data, hence favouring all requirements. 3.2.4. Requirements on Variability Requirements
    on Variability are poorly covered among the reviewed works. Schema evolution is
    only handled by CQELS [A1], AsterixDB [A4] and D-Ocean [A18]. CQELS uses specific
    wrapper configuration files which via a user interface map new elements to ontology
    concepts. On the other hand, AsterixDB parses schemas at runtime. Finally, D-Ocean’s
    unstructured data model embraces the addition of new features. Furthermore, only
    AsterixDB considers data evolution (R4.2) using adaptive query processing techniques.
    With respect to automatic inclusion of data sources (R4.3), CQELS has a service
    allowing wrappers to be plugged at runtime. Moreover, other architectures provide
    such feature as AsterixDB with the definition of external tables at runtime, [A19]
    providing a discovery channel or Tengu [A15] by means of an Enterprise Service
    Bus. 3.2.5. Requirements on Veracity Few of the studied architectures satisfy
    requirements on Veracity. All works covering data provenance (R5.1) log the operations
    applied on derived data in order to be reproduced later. On the other hand, measurement
    of data quality (R5.2) is only found in [A19] and [A13], the former by storing
    such metadata as part of its Big Data lifecycle and the latter by tracking data
    quality rules that validate the stored data. Regarding data liveliness (R5.3),
    [A16] tracks it in order to boost reusage of results computed by other users.
    Alternatively, [A19] as part of its Preservation Management activity applies aging
    strategies, however it is limited to its data retention policy. Finally, with
    respect to data cleaning (R5.4) we see two different architectures. In [A5,A13,A17,A19]
    cleansing processes are triggered as part of the data integration phase (i.e.
    before being stored). Differently, [A10,A20] execute such processes on unprocessed
    raw data before serving them to the user. 3.3. Discussion Besides new technological
    proposals, we devise two main families of works in the Big Data architectures
    landscape. On the one hand, those presented as an evolution of the λ-architecture
    [A7] after refining it [A2,A6,A10,A11,A15]; and, on the other hand, those positioned
    on the Semantic Web principles [A1,A8]. Some architectures aim to be of general-purpose,
    while others are tailored to specific domains, such as: multimedia data [A14],
    cloud manufacturing [A3], scientific testing [A15], Internet of Things [A2] or
    healthcare [A13]. It can be concluded from Table 2 that requirements related to
    Volume, Velocity and Variety are more fulfilled with respect to those related
    to Variability and Veracity. This is due to the fact, to some extent, that Volume,
    Velocity and partly Variety (i.e., R3.1, R3.2) are core functionalities in NOSQL
    systems, and thus all architectures adopting them benefit from that. Furthermore,
    such dimensions have a clear impact on the performance of the system. Most of
    the architectures based on the λ-architecture naturally fulfil them for such reason.
    On the other hand, partly Variety (i.e., R3.3), Variability and Veracity are dimensions
    that need to be addressed by respectively considering evolution and data governance
    as first-class citizens. However, this fact has an impact on the architecture
    as a whole, and not on individual components, hence causing such low fulfillment
    across the studied works. 4. Bolster: a semantic extension for the λ-architecture
    In this section, we present Bolster, an SRA solution for Big Data systems that
    deals with the 5 “Vs”. Briefly, Bolster adopts the best out of the two families
    of Big Data architectures (i.e., λ-architecture and those relying on Semantic
    Web principles). Building on top of the λ-architecture, it ensures the fulfillment
    of requirements related to Volume and Velocity. However, in contrast to other
    approaches, it is capable of completely handling Variety, Variability and Veracity
    leveraging on Semantic Web technologies to represent machine-readable metadata,
    oppositely to the studied Semantic Web-based architectures representing data.
    We first present the methodology used to design the SRA. Next, we present the
    conceptual view of the SRA and describe its components. 4.1. The design of Bolster
    Bolster has been designed following the framework for the design of empirically-grounded
    reference architectures [15], which consists of a six-step process described as
    follows: Step 1: decision on type of SRA. The first step consists on deciding
    the type of SRA to be designed, which is driven by its purpose. Using the characterization
    from [3], we conclude that Bolster should be of type 5 (a preliminary, facilitation
    architecture designed to be implemented in multiple organizations). This entails
    that the purpose of its design is to facilitate the design of Big Data systems,
    in multiple organizations and performed by a research-oriented team. Step 2: selection
    of design strategy. There are two strategies to design SRAs, from scratch or from
    existing architectures. We will design Bolster based on the two families of Big
    Data architectures identified in Section 3. Step 3: empirical acquisition of data.
    In this case, we leverage on the Big Data dimensions (the five “V’s”) discussed
    in Section 2 and the requirements defined for each of them. Such requirements,
    together with the design strategy, will drive the design of Bolster. Step 4: construction
    of SRA. The rationale and construction of Bolster is depicted in Section 4.2,
    where a conceptual view is presented. A functional description of its components
    is later presented in Section 4.3, and a functional example in Section 5. Step
    5: enabling SRA with variability. The goal of enabling an SRA with variability
    is to facilitate its instantiation towards different use cases. To this end, we
    provide the annotated SRA using a conceptual view as well as the description of
    components, which can be selectively instantiated. Later, in Section 6, we present
    methods for its instantiation. Step 6: evaluation of the SRA. The last step of
    the design of an SRA is its evaluation. Here, and leveraging on the industrial
    projects where Bolster has been adopted, in Section 7.2, we present the results
    of its validation. 4.2. Adding semantics to the λ-architecture The λ-architecture
    is the most widespread framework for scalable and fault-tolerant processing of
    Big Data. Its goal is to enable efficient real-time data management and analysis
    by being divided into three layers (Fig. 1). • The Batch Layer stores a copy of
    the master data set in raw format as data are ingested. This layer also pre-computes
    Batch Views that are provided to the Serving Layer. • The Speed Layer ingests
    and processes real-time data in form of streams. Results are then stored, indexed
    and published in Real-time Views. • The Serving Layer, similarly as the Speed
    Layer, also stores, indexes and publishes data resulting from the Batch Layer
    processing in Batch Views. Download : Download high-res image (164KB) Download
    : Download full-size image Fig. 1. λ-architecture. The λ-architecture succeeds
    at Volume requirements, as tons of heterogeneous raw data can be stored in the
    master data set, while fast querying through the Serving Layer. Velocity is also
    guaranteed thanks to the Speed Layer, since real-time views complement query results
    with real-time data. For these reasons, the λ-architecture was chosen as departing
    point for Bolster. Nevertheless, we identify two main drawbacks. First, as pointed
    out previously, it completely overlooks Variety, Variability and Veracity. Second,
    it suffers from a vague definition, hindering its instantiation. For example,
    the Batch Layer is a complex subsystem that needs to deal with data ingestion,
    storage and processing. However, as the λ-architecture does not define any further
    component of this layer, its instantiation still remains challenging. Bolster
    (Fig. 2) addresses the two drawbacks identified in the λ-architecture: • Variety,
    Variability and Veracity are considered first-class citizens. With this purpose,
    Bolster includes the Semantic Layer where the Metadata Repository stores machine-readable
    semantic annotations, in an analogous purpose as of the relational DBMS catalog.
    • Inspired by the functional architecture of relational DBMSs, we refine the λ-architecture
    to facilitate its instantiation. These changes boil down to a precise definition
    of the components and their interconnections. We therefore introduce possible
    instantiations for each component by means of off-the-shell software or service.
    Download : Download high-res image (384KB) Download : Download full-size image
    Fig. 2. Bolster SRA conceptual view. (For interpretation of the references to
    color in the text, the reader is referred to the web version of this article.)
    Finally, note that this SRA aims to broadly cover different Big Data use cases,
    however it can be tailored by enabling or disabling components according to each
    particular context. In the following subsections we describe each layer present
    in Bolster as well as their interconnections. In bold, we highlight the necessary
    functionalities they need to implement to cope with the respective requirements.
    4.3. Bolster components In this subsection, we present, for each layer composing
    Bolster, the list of its components and functional description. 4.3.1. Semantic
    Layer The Semantic Layer (depicted blue in Fig. 2) contains the Metadata Management
    System (MDM), the cornerstone for a semantic-aware Big Data system. It is responsible
    of providing the other components with the necessary information to describe and
    model raw data, as well as keeping the footprint about data usage. With this purpose,
    the MDM contains all the metadata artifacts, represented by means of RDF ontologies
    leveraging the benefits provided by Semantic Web technologies, needed to deal
    with data governance and assist data exploitation. We list below the main artifacts
    and refer the interested reader to [8], [63] for further details: 1. Data analysts
    should work using their day-by-day vocabulary. With this purpose, the Domain Vocabulary
    contains the business concepts (e.g., customer, order, lineitem) and their relationships
    (R5.1). 2. In order to free data analysts from data management tasks and decouple
    this role from the data steward, each vocabulary term must be mapped to the system
    views. Thus, the MDM must be aware of the View Schemata (R3.3) and the mappings
    between the vocabulary and such schemata. 3. Data analysts tend to repeat the
    same data preparation steps prior to conducting their analysis. To enable reusability
    and a collaborative exploitation of the data, on the one hand, the MDM must store
    Pre-processing Domain Knowledge about data preparation rules (e.g., data cleaning,
    discretization, etc.) related to a certain domain (R5.4), and on the other hand
    descriptive statistics to assess data evolution (R4.2). 4. To deal with automatic
    inclusion of new data sources (R4.3), each ingested element must be annotated
    with its schema information (R4.1). To this end, the Data Source Register tracks
    all input data sources together with the required information to parse them, the
    physical schema, and each schema element has to be linked to the attributes it
    populates, the logical schema (R3.3). Furthermore, for data provenance (R5.1),
    the Data Transformations Log has to keep track of the performed transformation
    steps to produce the views, the last processing step within the Big Data system.
    Populating these artifacts is a challenge. Some of them can be automatically populated
    and some others must be manually annotated. Nonetheless, all of these artifacts
    are essential to enable a centralized master metadata management and hence, fulfil
    the requirements related to Variety, Variability and Veracity. Analogously to
    database systems, data stewards are responsible of populating and maintaining
    such artifacts. That is why we claim for the need that the MDM provides a user
    friendly interface to aid such processes. Finally, note that most of the present
    architectural components must be able to interact with the MDM, hence it is essential
    that it provides language-agnostic interfaces. Moreover, such interfaces cannot
    pose performance bottlenecks, as doing so would highly impact in the overall performance
    of the system. 4.3.2. Batch Layer This layer (depicted yellow in Fig. 2) is in
    charge of storing and processing massive volumes of data. In short, we first encounter
    Batch Ingestion, responsible for periodically ingesting data from the batch sources,
    then the Data Lake, capable of managing large amounts of data. The last step is
    the Batch Processing component, which prepares, transforms and runs iterative
    algorithms over the data stored in the Data Lake to shape them accordingly to
    the analytical needs of the use-case at hand. Batch Ingestion Batch sources are
    commonly big static raw data sets that require periodic synchronizations (R3.1).
    Examples of batch sources can be relational databases, structured files, etc.
    For this reason, we advocate for a multiple component instantiation, as required
    by the number of sources and type. These components need to know which data have
    already been moved to the Data Lake by means of Incremental Bulks Scheduling and
    Orchestration. The MDM then comes into play as it traces this information. Interaction
    between the ingestion components and the MDM occurs in a two-phase manner. First,
    they learn which data are already stored in the Data Lake, to identify the according
    incremental bulk can be identified. Second, the MDM is enriched with specific
    information regarding the recently brought data (R5.3). Since Big Data systems
    are multi-source by nature, the ingestion components must be built to guarantee
    its adaptability in the presence of new sources (R4.3). Data Lake This component
    is composed of a Massive Storage system (R1.1). Distributed file systems are naturally
    good candidates as they were born to hold large volumes of data in their source
    format (R3.2). One of their main drawbacks is that its read capabilities are only
    sequential and no complex querying is therefore feasible. Paradoxically, this
    turns out to be beneficial for the Batch Processing, as it exploits the power
    of cloud computing. Different file formats pursuing high performance capabilities
    are available, focusing on different types of workload [45]. They are commonly
    classified as horizontal, vertical and hybrid, in an analogous fashion as row-oriented
    and column-oriented databases, respectively. Batch Processing This component models
    and transforms the Data Lake’s files into Batch Views ready for the analytical
    use-cases. It is responsible to schedule and execute Batch Iterative Algorithms,
    such as sorting, searching, indexing (R1.2) or more complex algorithms such as
    PageRank, Bayesian classification or genetic algorithms (R1.3). The processing
    components, must be designed to maximize reusability by creating building blocks
    (from the domain-knowledge metadata artifacts) that can be reused in several views.
    Consequently, in order to track Batch Data Provenance, all performed transformations
    must be communicated to the MDM (R5.1). Batch Processing is mostly represented
    by the MapReduce programming model. Its drawbacks appear twofold. On one hand,
    when processing huge amounts of batch data, several jobs may usually need to be
    chained so that more complex processing can be executed as a single one. On the
    other hand, intermediate results from Map to Reduce phases are physically stored
    in hard disk, completely detracting the Velocity (in terms of response time).
    Massive efforts are currently put on designing new solutions to overcome the issues
    posed by MapReduce. For instance, by natively including other more atomic relational
    algebra operations, connected by means of a directed acyclic graph; or by keeping
    intermediate results in main memory. 4.3.3. Speed Layer The Speed Layer (depicted
    green in Fig. 2) deals primarily with Velocity. Its input are continuous, unbounded
    streams of data with high timeliness and therefore require novel techniques to
    accommodate such arrival rate. Once ingested, data streams can be dispatched either
    to the Data Lake, in order to run historical queries or iterative algorithms,
    or to the Stream Processing engine, in charge of performing one-pass algorithms
    for real-time analysis. Stream Ingestion The Stream Ingestion component acts as
    a message queue for raw data streams that are pushed from the data sources (R3.1).
    Multiple sources can continuously push data streams (e.g., sensor or social network
    data), therefore such component must be able to cope with high throughput rates
    and scale according to the number of sources (R2.1). One of the key responsibilities
    is to enable the ingestion of all incoming data (i.e., adopt a No Event Loss policy).
    To this end, it relies on a distributed memory or disk-based storage buffer (i.e.
    event queue), where streams are temporarily stored. This component does not require
    any knowledge about the data or schema of incoming data streams, however, for
    each event, it must know its source and type, for further matching with the MDM.
    To assure fault-tolerance and durability of results in such a distributed environment,
    techniques such as write-ahead logging or the two-phase commit protocol are used,
    nevertheless that has a clear impact on the availability of data to next components.
    Dispatcher The responsibilities of the Dispatcher are twofold. On the one hand,
    to ensure data quality, via MDM communication, it must register and validate that
    all ingested events follow the specified schema and rules for the event on hand
    (i.e., Schema Typechecking (R4.1, R5.2)). Error handling mechanisms must be triggered
    when an event is detected as invalid, and various mitigation plans can be applied.
    The simplest alternative is event rejection, however most conservative approaches
    like routing invalid events to the Data Lake for future reprocess can contribute
    to data integrity. On the other hand, the second responsibility of the Dispatcher
    is to perform Event Routing, either to be processed in a real-time manner (i.e.,
    to the Stream Processing component), or in a batch manner (i.e., to the Data Lake)
    for delayed process. In contrast to the λ-architecture, which duplicates all input
    streams to the Batch Layer, here only those that will be used by the processing
    components will be dispatched if required. Moreover, before dispatching such events,
    different routing strategies can influence the decision on where data is shipped,
    for instance by means of evaluating QoS cost models or analyzing the system workload,
    as done in [37]. Other approaches like sampling or load shedding can be used here,
    to ensure that either real-time processing or Data Lake ingestion are correctly
    performed. Stream Processing The Stream Processing component is responsible of
    performing One-Pass Algorithms over the stream of events. The presence of a summary
    is required as most of these algorithms leverage on in-memory stateful data structures
    (e.g., the Loosy Counting algorithm to compute heavy hitters, or HyperLogLog to
    compute distinct values). Such data structures can be leveraged to maintain aggregates
    over a sliding window for a certain period of time. Different processing strategies
    can be adopted, being the most popular tuple-at-a-time and micro-batch processing,
    the former providing low latency while the latter providing high throughput (R2.2).
    Similarly as the Batch Processing, this component must communicate to the MDM
    all transformations applied to populate Real-time Views in order to guarantee
    Stream Data Provenance (R5.1). 4.3.4. Serving Layer The Serving Layer (depicted
    red in Fig. 2) holds transformed data ready to be delivered to end-users (i.e.
    it acts as a set of database engines). Precisely, it is composed by Batch and
    Real-time Views repositories. Different alternatives exist when selecting each
    view engine, however as they impose a data model (e.g., relational or key-value),
    it is key to perform a goal-driven selection according to end-user analytical
    requirements [27]. It is worth noting that views can also be considered new sources,
    in case it is required to perform transformations among multiple data models,
    resembling a feedback loop. Further, the repository of Query Engines is the entry
    point for data analysts to achieve their analytical task, querying the views and
    the Semantic Layer. Batch Views As in the λ-architecture, we seek Scalable and
    Fault-Tolerant Databases capable to provide Random Reads, achieved by indexing,
    and the execution of Aggregations and UDFs (user defined functions) over large
    stable data sets (R1.1). The λ-architecture advocates for recomputing Batch Views
    every time a new version is available, however we claim incremental approaches
    should be adopted to avoid unnecessary writes and reduce processing latency. A
    common example of Batch View is a DW, commonly implemented in relational or columnar
    engines. However databases implementing other data models such as graph, key-value
    or documents also can serve the purpose of Batch Views. Each view must provide
    a high-level query language, serving as interface with the Query Engine (e.g.,
    SQL), or a specific wrapper on top of it providing such functionalities. Real-time
    Views As opposite to Batch Views, Real-time Views need to provide Low Latency
    Querying over dynamic and continuously changing data sets (R2.1). In order to
    achieve so, in-memory databases are currently the most suitable option, as they
    dismiss the high cost it entails to retrieve data from disk. Additionally, Real-Time
    views should support low cost of updating in order to maintain Sketches and Sliding
    Windows. Finally, similarly to Batch Views, Real-time Views must provide mechanisms
    to be queried, considering as well Continuous Query Languages. Query Engines Query
    Engines, play a crucial role to enable efficiently querying the views in a friendly
    manner for the analytical task on hand. Data analysts query the system using the
    vocabulary terms and apply domain-knowledge rules on them (R1.2, R1.3). Thanks
    to the MDM artifacts, the system must internally perform the translation from
    Business Requirements to Database Queries over Batch and Real-time Views (R3.3),
    hence making data management tasks transparent to the end-user. Furthermore, the
    Query Engine must provide to the user the ability for Metadata Query and Exploration
    on what is stored in the MDM (R5.1, R5.2, R5.3). 4.3.5. Summary Table 3 summarizes
    for each component the fulfilled requirements discussed in Section 2. Table 3.
    Bolster components and requirements fulfilled. Component Volume Velocity Variety
    Variability Veracity Empty Cell R1.1 R1.2 R1.3 R2.1 R2.2 R3.1 R3.2 R3.3 R4.1 R4.2
    R4.3 R5.1 R5.2 R5.3 R5.4 Metadata Management System ✓ ✓ ✓ ✓ ✓ ✓ Batch Ingestion
    ✓ ✓ ✓ Data Lake ✓ ✓ Batch Processing ✓ ✓ ✓ Stream Ingestion ✓ ✓ Dispatcher ✓ ✓
    Stream Processing ✓ ✓ Batch Views ✓ Real-time Views ✓ Query Engines ✓ ✓ ✓ ✓ ✓
    ✓ 5. Exemplar use case The goal of this section is to provide an exemplar use
    case to illustrate how Bolster would accommodate a Big Data management and analytics
    scenario. Precisely, we consider the online social network benchmark described
    in [70]. Such benchmark aims to provide insights on the stream of data provided
    by Twitter’s Streaming API, and is characterized by workloads in media, text,
    graph, activity and user analytics. 5.1. Semantic representation Fig. 3 depicts
    a high level excerpt of the content stored in the MDM. In dark and light blue,
    the domain knowledge and business vocabulary respectively which has been provided
    by the Domain Expert. In addition, the data steward has, possibly in a semi-automatic
    manner [47], registered a new source (Twitter Stream API4) and provided mappings
    for all JSON fields to the logical attributes (in red). For the sake of brevity,
    only the relevant subgraph of the ontology is shown. Importantly, to meet the
    Linked Open Data principles, this ontology should be further linked to other ontologies
    (e.g., the Open Provenance Model [44]). Download : Download high-res image (497KB)
    Download : Download full-size image Fig. 3. Excerpt of the content in the Metadata
    Repository. (For interpretation of the references to color in the text, the reader
    is referred to the web version of this article.) 5.2. Data ingestion As raw JSON
    events are pushed to the Stream Ingestion component, they are temporary stored
    in the Event Queue. Once replicated, to guarantee durability and fault tolerance,
    they are made available to the Dispatcher, which is aware on how to retrieve and
    parse them by querying the MDM. Twitter’s documentation5 warns developers that
    events with missing counts rarely happen. To guarantee data quality such aspect
    must be checked. If an invalid event is detected, it should be discarded. After
    this validation, the event at hand must be registered in the MDM to guarantee
    lineage analysis. Furthermore the Dispatcher sends the raw JSON event to the Stream
    Processing and Data Lake components. At this point, there is a last ingestion
    step missing before processing data. The first workload presented in the benchmark
    concerns media analytics, however as depicted in Fig. 3, the API only provides
    the URL of the image. Hence, it is necessary to schedule a batch process periodically
    fetching such remote images and loading them into the Data Lake. 5.3. Data processing
    and analysis Once all data are available to be processed in both Speed and Batch
    Layers, we can start executing the required workloads. Many of such workloads
    concern predictive analysis (e.g., topic modeling, sentiment analysis, location
    prediction or collaborative filtering). Hence, the proposed approach is to periodically
    refresh statistical models in an offline manner (i.e., in the Batch Layer), in
    order to assess predictions in an online manner (i.e., in the Speed Layer). We
    distinguish between those algorithms generating metadata (e.g., Latent Dirichlet
    Allocation (LDA)) and those generating data (e.g., PageRank). The former will
    store its results in the MDM using a comprehensive vocabulary (e.g., OntoDM [51]);
    and the latter will store them into Batch Views. Once events have been dispatched,
    the required statistical model has to be retrieved from the MDM to assess predictions
    and store outcomes into Real-time Views. Finally, as described in [70], the prototype
    application provides insights based on tweets related to companies in the S&P
    100 index. Leveraging on the MDM, the Query Engine is capable of generating queries
    to Batch and Real-time Views. 6. Bolster instantiation In this section we list
    a set of candidate tools, with special focus on the Apache Hadoop and Amazon Web
    Services ecosystems, to instantiate each component in Bolster. In the case when
    few tools from such ecosystems were available, we propose commercial tools which
    were considered in the industrial projects where Bolster was instantiated. Further,
    we present a method to instantiate the reference architecture. We propose a systematic
    scoring process driven by quality characteristics, yielding, for each component,
    the most suitable tool. 6.1. Available tools 6.1.1. Semantic Layer Metadata Management
    System Two different off-the-shelf open source products can instantiate this layer,
    namely Apache Stanbol6 and Apache Atlas.7 Nevertheless, the features of the former
    fall short for the proposed requirements of the MDM. Not surprisingly, this is
    due to the novel nature of Bolster’s Semantic Layer. Apache Atlas satisfies the
    required functionalities more naturally and it might appear as a better choice,
    however it is currently under heavy development as an Apache Incubator project.
    Commercial tools such as Cloudera Navigator8 or Palantir9 are also candidate tools.
    Metadata Storage We advocate for the adoption of Semantic Web storage technologies
    (i.e. triplestores), to store all the metadata artifacts. Even though such tools
    allow storing and reasoning over large and complex ontologies, that is not the
    pursued purpose here, as our aim is to allow a simple and flexible representation
    of machine-readable schemas. That is why triplestores serve better the purpose
    of such storage. Virtuoso10 is at the moment the most mature triplestore platform,
    however other options are available such as 4store11 or GraphDB.12 Nonetheless,
    given the graph nature of triples, any graph database can as well serve the purpose
    of metadata storage (e.g., AllegroGraph13 or Neo4j14). 6.1.2. Batch Layer Batch
    Ingestion This components highly depends on the format of the data sources, hence
    it is complex to derive a universal driver due to technological heterogeneity.
    Instantiating this component usually means developing ad-hoc scripting solutions
    adapting to the data sources as well as enabling communication with the MDM. Massive
    data transfer protocols such as FTP or Hadoop’s copyFromLocal15 will complement
    such scripts. However, some drivers for specific protocols exist such as Apache
    Sqoop,16 the most widespread solution to load data from/to relational sources
    through JDBC drivers. Data Lake Hadoop Distributed File System and Amazon S317
    perfectly fit in this category, as they are essentially file systems storing plain
    files. Regarding data file formats, some current popular options are Apache Avro,18
    Yahoo Zebra19 or Apache Parquet20 for horizontal, vertical and hybrid fragmentation
    respectively. Batch Processing Apache MapReduce 21 and Amazon Elastic MapReduce22
    are nowadays the most popular solutions. Alternatively, Apache Spark23 and Apache
    Flink24 are gaining great popularity as next generation replacement for the MapReduce
    model. However, to the best of our knowledge, only Quarry [32] is capable to interact
    with the MDM and, based on the information there stored, automatically produce
    batch processes based on user-defined information requirements. 6.1.3. Speed Layer
    Stream Ingestion All tools in the family of “message queues” are candidates to
    serve as component for Stream Ingestion. Originated with the purpose of serving
    as middleware to support enterprise messaging across heterogeneous systems, they
    have been enhanced with scalability mechanisms to handle high ingestion rates
    preserving durability of data. Some examples of such systems are Apache ActiveMQ25
    or RabbitMQ.26 However, some other tools were born following similar principles
    but aiming Big Data systems since its inception, being Apache Kafka27 and AWS
    Kinesis Firehose28 the most popular options. Dispatcher Here we look for tools
    that allow developers to define data pipelines routing data streams to multiple
    and heterogeneous destinations. It should also allow the developer to programmatically
    communicate with the MDM for quality checks. Apache Flume29 and Amazon Kinesis
    Streams30 are nowadays the most prevalent solutions. Stream Processing In contrast
    to Batch Processing, it is unfeasible to adopt classical MapReduce solutions considering
    the performance impact they yield. Thus, in-memory distributed stream processing
    solutions like Apache Spark Streaming,31 Apache Flink Streaming32 and Amazon Kinesis
    Analytics33 are the most common alternatives. 6.1.4. Serving Layer Batch Views
    A vast range of solutions are available to hold specialized views. We distinguish
    among three families of databases: (distributed) relational, NOSQL and NewSQL.
    The former is mostly represented by major vendors who evolved their traditional
    centralized databases into distributed ones seeking to improve its storage and
    performance capabilities. Some common solutions are Oracle,34 Postgres-XL35 or
    MySQL Cluster.36 Secondly, in the NOSQL category we might drill-down to the specific
    data model implemented: Apache HBase37 or Apache Cassandra38 for column-family
    key-value; Amazon DynamoDB39 or Voldemort40 for key-value; Amazon Redshift41 or
    Apache Kudu42 for column oriented; Neo4j43 or OrientDB44 for graph; and MongoDB45
    or RethinkDB46 for document. Finally, NewSQL are high-availability main memory
    databases which usually are deployed in specialized hardware, where we encounter
    SAP Hana,47 NuoDB48 or VoltDB.49 Real-time Views In-memory databases are currently
    the most popular options, for instance Redis,50 Elastic,51 Amazon ElastiCache.52
    Alternatively, PipelineDB53 offers mechanism to query a data stream via continuous
    query languages. Query Engine There is a vast variety of tools available for query
    engines. OLAP engines such as Apache Kylin54 provide multidimensional analysis
    capabilities, on the other hand solutions like Kibana55 or Tableau56 enable the
    user to easily define complex charts over the data views. 6.2. Component selection
    Selecting components to instantiate Bolster is a typical (C)OTS (commercial off-the-shelf)
    selection problem [36]. Considering a big part of the landscape of available Big
    Data tools is open source or well-documented, we follow a quality model approach
    for their selection, as done in [7]. To this end, we adopt the ISO/IEC 25000 SQuaRE
    standard (Software Product Quality Requirements and Evaluation) [30] as reference
    quality model. Such model is divided into characteristics and subcharacteristics,
    where the latter allows the definition of metrics (see ISO 25020). In the context
    of (C)OTS, the two former map to the hierarchical criteria set, while the latter
    to evaluation attributes. Nevertheless, the aim of this paper is not to provide
    exhaustive guidelines on its usage whatsoever, but to supply a blueprint to be
    tailored to each organization. Fig. 4 depicts the subset of characteristics considered
    relevant for such selection. Note that not all subcharacteristics are applicable,
    given that we are assessing the selection of off-the-shelf software for each component.
    Download : Download high-res image (144KB) Download : Download full-size image
    Fig. 4. Selected characteristics and subcharacteristics from SQuaRE. 6.2.1. Evaluation
    attributes Previously, we discussed that ISO 25020 proposes candidate metrics
    for each present subcharacteristic. However, we believe that they do not cover
    the singularities required for selecting open source Big Data tools. Thus, in
    the following subsections we present a candidate set of evaluation attributes
    which were used in the use case applications described in Section 7. Each has
    associated a set of ordered values from worst to better and its semantics. Functionality
    After analyzing the artifacts derived from the requirement elicitation process,
    a set of target functional areas should be devised. For instance, in an agile
    methodology, it is possible to derive such areas by clustering user stories. Some
    examples of functional areas related to Big Data are: Data and Process Mining,
    Metadata Management, Reporting, BI 2.0 or Real-time Analysis. Suitability specifically
    looks at such functional areas, while with the other evaluation attributes we
    evaluate information exchange and security concerns. Suitability Number of functional
    areas targeted in the project which benefit from its adoption. Interoperability
    1, no input/output connectors with other considered tools 2, input/output connectors
    available with some other considered tools 3, input/output connectors available
    with many other considered tools Compliance 1, might rise security or privacy
    issues 2, does not raise security or privacy issues Reliability It deals with
    trustworthiness and robustness factors. Maturity is directly linked to the stability
    of the software at hand. To that end, we evaluate it by means of the Semantic
    Versioning Specification.57 The other two factors, Fault Tolerance and Recoverability,
    are key Big Data requirements to ensure the overall integrity of the system. We
    acknowledge it is impossible to develop a fault tolerant system, thus our goal
    here is to evaluate how the system reacts in the presence of faults. Maturity
    1, major version zero (0.y.z) 2, public release (1.0.0) 3, major version (x.y.z)
    Fault Tolerance 1, the system will crash if there is a fault 2, the system can
    continue working if there is a fault but data might be lost 3, the system can
    continue working and guarantees no data loss Recoverability 1, requires manual
    attention after a fault 2, automatic recovery after fault Usability In this subcharacteristic,
    we look at productive factors regarding the development and maintenance of the
    system. In Understandability, we evaluate the complexity of the system’s building
    blocks (e.g., parallel data processing engines require knowledge of functional
    programming). On the other hand, Learnability measures the learning effort for
    the team to start developing the required functionalities. Finally, in Operability,
    we are concerned with the maintenance effort and technical complexity of the system.
    Understandability 1, high complexity 2, medium complexity 3, low complexity Learnability
    1, the operating team has no knowledge of the tool 2, the operating team has small
    knowledge of the tool and the learning curve is known to be long 3, the operating
    team has small knowledge of the tool and the learning curve is known to be short
    4, the operating team has high knowledge of the tool Operability 1, operation
    control must be done using command-line 2, offers a GUI for operation control
    Efficiency Here we evaluate efficiency aspects. Time Behaviour measures the performance
    at processing capabilities, measured by the way the evaluated tool shares intermediate
    results, which has a direct impact on the response time. On the other hand, Resource
    Utilisation measures the hardware needs for the system at hand, as it might affect
    other coexisting software. Time Behaviour 1, shares intermediate results over
    the network 2, shares intermediate results on disk 3, shares intermediate results
    in memory Resource Utilisation 1, high amount of resources required (on both master
    and slaves) 2, high amount of resources required (either on master or slaves)
    3, low amount of resources required Maintainability It concerns continuous control
    of software evolution. If a tool provides fully detailed and transparent documentation,
    it will allow developers to build robust and fault-tolerant software on top of
    them (Analyzability). Furthermore, if such developments can be tested automatically
    (by means of unit tests) the overall quality of the system will be increased (Testability).
    Analyzability 1, online up to date documentation 2, online up to date documentation
    with examples 3, online up to date documentation with examples and books available
    Testability 1, doesn’t provide means for testing 2, provides means for unit testing
    3, provides means for integration testing Portability Finally, here we evaluate
    the adjustment of the tool to different environments. In Adaptability, we analyse
    the programming languages offered by the tool. Instability and Co-existence evaluate
    the effort required to install such tool and coexistence constraints respectively.
    Adaptability 1, available in one programming language 2, available in many programming
    languages 3, available in different programming languages and offering API access
    Instability 1, requires manual build 2, self-installing package 3, shipped as
    part of a platform distribution Co-existence 1, cannot coexist with other selected
    tools 2, can coexist with all selected tools 6.3. Tool evaluation The purpose
    of the evaluation process is, for each of the candidate tools to instantiate Bolster,
    to derive a ranking of the most suitable one according to the evaluation attributes
    previously described. The proposed method is based on the weighted sum model (WSM),
    which allows weighting criteria (wi) in order to prioritize the different subcharacteristics.
    Weights should be assigned according to the needs of the organization. Table 4
    depicts an example selection for the Batch Processing component for the use case
    described in Section 7.1.2. For each studied tool, the Atomic and Weighted columns
    indicate its unweighted (fi) and weighted score (wifi), respectively using a range
    from one to five. For each characteristic, the weighted average of each component
    is shown in light grey (i.e., the average of each weighted subcharacteristic ).
    Finally, in black, the final score per tool is depicted. From the exemplar case
    of Table 4, we can conclude that, for the posed weights and evaluated scores,
    Apache Spark should be the selected tool, in from of Apache MapReduce and Apache
    Flink respectively. Table 4. Example tool selection for Batch Processing. 7. Industrial
    experiences In this section we depict three industrial projects, involving five
    organizations, where Bolster has been successfully adopted. For each project,
    we describe the use case context and the specific Bolster instantiation in graphical
    form. Finally we present the results of a preliminary validation that measure
    the perception of Bolster from the relevant industrial stakeholders. 7.1. Use
    cases and instantiation 7.1.1. BDAL: Big Data analytics lab This project takes
    place in a multinational company in Barcelona.58 It runs a data-driven business
    model and decision making relies on predictive models. Three main design issues
    were identified: (a) each department used its own processes to create data matrices,
    which were then processed to build predictive models. For reusability, data sets
    were preprocessed in ad-hoc repositories (e.g., Excel sheets), generating a data
    governance problem; (b) data analysts systematically performed data management
    tasks, such as parsing continuous variable discretization or handling missing
    values, with a negative impact on their efficiency; (c) data matrices computation
    resulted in an extremely time consuming process due to their large volumes. Thus,
    their update rate was usually in the range of weeks to months. The main goal was
    to develop a software solution to reduce the exposure of data analysts to data
    management and governance tasks, as well as boost performance in data processing.
    Bolster instantiation Bolster’s Semantic Layer allowed the organization to overcome
    the data governance problem, consider additional data sources, and provide automation
    of data management processes. Additionally, there was a boost of performance in
    data processing thanks to the distributed computing and parallelism in the storage
    and processing of the Batch and Serving Layers. The nature of the data sources
    and analytical requirements did not justify the components in the Speed Layer,
    thus Bolster’s instantiation was narrowed to Batch, Semantic and Serving Layers.
    Fig. 5 depicts the tools that compose Bolster’s instantiation instantiation for
    this use case. Download : Download high-res image (341KB) Download : Download
    full-size image Fig. 5. Bolster instantiation for the BDAL use case . 7.1.2. H2020
    SUPERSEDE project The SUPERSEDE59 project proposes a feedback-driven approach
    for software life-cycle management. It considers user feedback and runtime data
    as an integral part of the design, development, and maintenance of software services
    and applications. The ultimate goal is to improve the quality perceived by software
    end-users as well as support developers and engineers to make the right software
    adaptation and evolution decisions. Three use cases proposed by industrial partners,
    namely: Siemens AG Oesterreich (Austria), Atos (Spain) and SEnerCon GmbH (Germany),
    are representative of different data-intensive application domains in the areas
    of energy consumption management in home automation and entertainment event webcasting.
    SUPERSEDE’s Big Data architecture is the heart of the analysis stage that takes
    place in the context of a monitor-analyze-plan-execute (MAPE) process [33]. Precisely,
    some of its responsibilities are (i) collecting and analyzing user feedback from
    a variety of sources, (ii) supporting decision making for software evolution and
    adaptation based on the collected data, and (iii) enacting the decision and assessing
    its impact. This set of requirements yielded the following challenges: (a) ingest
    multiple fast arriving data streams from monitored data and process them in real-time,
    for instance with sliding window operations; (b) store and integrate user feedback
    information from multiple and different sources; (c) use all aforementioned data
    in order to analyze multi-modal user feedback, identify profiles, usage patterns
    and identify relevant indicators for usefulness of software services. All implemented
    in a performance oriented manner in order to minimize overhead. Bolster instantiation
    Bolster allowed the definition of a data governance protocol encompassing the
    three use cases in a single instantiation of the architecture, while preserving
    data isolation. The Speed Layer enabled the ingestion of continuous data streams
    from a variety of sources, which were also dispatched to the Data Lake. The different
    analytical components in the Serving Layer allowed data analysts to perform an
    integrated analysis. Fig. 6 depicts the tools that compose Bolster’s instantiation
    for this use case. Download : Download high-res image (366KB) Download : Download
    full-size image Fig. 6. Bolster instantiation for the SUPERSEDE use case . 7.1.3.
    WISCC: World Information System for Chagas Control The WISCC project funded by
    the World Health Organization (WHO) is part of the Programme on Control of the
    Chagas disease. The goal of this project is to control and eliminate the Chagas
    disease, one of the 17 diseases in the 2010 first Report on Neglected Tropical
    Diseases. To this end, the aim is to build an information system serving as an
    integrated repository of all information, from different countries and organizations,
    related to the Chagas disease. Such holistic view should aid scientists to derive
    valuable insights and forecasts, leading to Chagas’ eradication. The role of the
    Big Data architecture is to ingest and integrate data from a variety of data sources
    and formats. Currently, the big chunk of data is ingested from DHIS2,60 an information
    system where national ministries enter data related to inspections, diagnoses,
    etc. Additionally, NGOs make available similar information according to their
    actions. The information dealt with is continuously changing by nature at all
    levels: data, schema and sources. Thus, the challenge falls in the flexibility
    of the system to accommodate such information and the one to come. Additionally,
    flexible mechanisms to query such data should be defined, as future information
    requirements will be totally different from today’s. Bolster instantiation Instantiating
    Bolster favored a centralized management, in the Semantic Layer, of the different
    data sources along with the provided schemata, a feature that facilitated the
    data integration and Data Lake management tasks. Similarly to the BDAL use case,
    the ingestion and analysis of data was performed with batch processes, hence dismissing
    the need to instantiate the Speed Layer. Fig. 7 depicts the tools that compose
    Bolster’s instantiation for this use case. Download : Download high-res image
    (354KB) Download : Download full-size image Fig. 7. Bolster instantiation for
    the WISCC use case . 7.1.4. Summary In this subsection, we discuss and summarize
    the previously presented instantiations. We have shown how, as an SRA, Bolster
    can flexibly accomodate different use cases with different requirements by selectively
    instantiating its components. Due to space reasons, we cannot show the tool selection
    tables per component, instead we present the main driving forces for such selection
    using the dimensions devised in Section 2. Table 5 depicts the key dimensions
    that steered the instantiation of Bolster in each use case. Table 5. Characterization
    of use cases and Big Data dimensions. Use Case Volume Velocity Variety Variability
    Veracity BDAL ✓ ✓ ✓ ✓ SUPERSEDE ✓ ✓ ✓ ✓ WISCC ✓ ✓ ✓ Most of the components have
    been successfully instantiated with off-the-shelf tools. However, in some cases
    it was necessary to develop customized solutions to satisfy specific project requirements.
    This was especially the case for the MDM, for which off-the-shelf tools were unsuitable
    in two out of three projects. It is also interesting to see that, due to the lack
    of connectors between components, it has been necessary to use glue code techniques
    (e.g., in WISCC dump files to a UNIX file system and batch loading in R). As final
    remark, note that the deployment of Bolster in all described use cases occurred
    in the context of research projects, which usually entail a low risk. However,
    in data-driven organizations such information processing architecture is the business’s
    backbone, and adopting Bolster can generate risk as few components from the legacy
    architecture will likely be reused. This is due to the novelty in the landscape
    of Big Data management and analysis tools, which lead to a paradigm shift on how
    data are stored and processed. 7.2. Validation The overall objective of the validation
    is to “assess to which extent Bolster leads to a perceived quality improvement
    in the software or service targeted in each use case”. Hence, the validation of
    the SRA involves a quality evaluation where we investigated how Big Data practitioners
    perceive Bolster’s quality improvements. To this end, as before, we rely on SQuaRE’s
    quality model, however now focusing on the quality-in-use model. The model is
    hierarchically composed by a set of characteristics and sub-characteristics. Each
    (sub-)characteristic is quantified by a Quality Measure (QM), which is the output
    of a measurement function applied to a number of Quality Measure Elements (QME).
    7.2.1. Selection of participants For each of the five aforementioned organizations,
    in the three use cases, a set of practitioners was selected as participants to
    report their perception about the quality improvements achieved with Bolster using
    the data collection method detailed in Section 7.2.2. Care was taken in selecting
    participants with different backgrounds (e.g., a broad range of skills, different
    seniority levels) and representative of the actual target population of the SRA.
    This is summarized in Table 6, which depicts the characteristics of the respondents
    in each organization. Recall that the SUPERSEDE project involves three industrial
    partners, hence we refer by SUP-1, SUP-2 and SUP-3 to, respectively, Siemens,
    Atos and SEnerCon. Table 6. List of participants per organization. ID Org. Function
    Seniority Specialties #1 BDAL Data analyst Senior Statistics #2 BDAL SW architect
    Junior Non-relational databases, Java #3 SUP-1 Research scientist Senior Statistics,
    machine learning #4 SUP-1 Key expert Senior Software engineering #5 SUP-1 SW developer
    Junior Java, security #6 SUP-1 Research scientist Senior Stream processing, semantic
    web #7 SUP-2 Dev. team head Senior CDN, relational databases #8 SUP-2 Project
    manager Senior Software engineering #9 SUP-3 SW developer Junior Web technologies,
    statistics #10 SUP-3 SW developer Junior Java, databases #11 SUP-3 SW architect
    Senior Web technologies, project leader #12 WISCC SW architect Senior Statistics,
    software engineering #13 WISCC Research scientist Senior Non-relational databases,
    semantic web #14 WISCC SW developer Junior Java, web technologies 7.2.2. Definition
    of the data collection methods The quality characteristics were evaluated by means
    of questionnaires. In other words, for each characteristic (e.g., trust), the
    measurement method was the question whether a participant disagrees or agrees
    with a descriptive statement. The choice of the participant (i.e., the extent
    of agreement in a specific rating scale) was the QME. For each characteristic,
    a variable numbers of QMEs were collected (i.e., one per participant). The final
    QM was represented by the mean opinion score (MOS), computed by the measurement
    function where N is the total number of participants. We used a 7-values rating
    scale, ranging from 1 strongly disagree to 7 strongly agree. Table 7 depicts the
    set of questions in the questionnaire along with the quality subcharacteristic
    they map to. Table 7. Validation questions along with the subcharacteristics they
    map to. Subcharacteristic Question Usefulness • The presented Big Data architecture
    would be useful in my UC Satisfaction • Overall I feel satisfied with the presented
    architecture Trust • I would trust the Big Data architecture to handle my UC data
    Perceived Relative Benefit • Using the proposed Big Data architecture would be
    an improvement with respect to my current way of handling and analyzing UC data
    Functional Completeness • In general, the proposed Big Data architecture covers
    the needs of the UC (subdivided into user stories) Functional Appropriateness
    • The proposed Big Data architecture facilitates the storing and management of
    the UC data • The proposed Big Data architecture facilitates the analysis of historical
    UC data • The proposed Big Data architecture facilitates the real-time analysis
    of UC data stream • The proposed Big Data architecture facilitates the exploitation
    of the semantic annotation of UC data • The proposed Big Data architecture facilitates
    the visualization of UC data statistics Functional Correctness • The extracted
    metrics obtained from the Big Data architecture (test metrics) match the results
    rationally expected Willingness to Adopt • I would like to adopt the Big Data
    architecture in my UC 7.2.3. Execution of the validation The heterogeneity of
    organizations and respondents called for a strict planning and coordination for
    the validation activities. A thorough time-plan was elaborated, so as to keep
    the progress of the evaluation among use cases. The actual collection of data
    spanned over a total duration of three weeks. Within these weeks, each use case
    evaluated the SRA in a 3-phase manner: 1. (1 week): A description of Bolster in
    form of an excerpt of Section 4 of this paper was provided to the respondents,
    as well as access to the proposed solution tailored to each organization. 2. (1
    h): For each organization, a workshop involving a presentation on the SRA and
    a Q&A session was carried out. 3. (1 day): The questionnaire was provided to each
    respondent to be answered within a day after the workshop. Once the collection
    of data was completed, we digitized the preferences expressed by the participants
    in each questionnaire. We created summary spreadsheets merging the results for
    its analysis. 7.2.4. Analysis of validation results Fig. 8 depicts, by means of
    boxplots, the aggregated MOS for all respondents (we acknowledge the impossibility
    to average ordinal scales, however we consider them as their results fall within
    the same range). The top and bottom boxes respectively denote the first and third
    quartile, the solid line the median and the whiskers maximum and minimum values.
    The dashed line denotes the average, and the diamond shape the standard deviation.
    Note that Functional Appropriateness is aggregated into the average of the 5 questions
    that compose it, and functional completeness is aggregated into the average of
    multiple user-stories (a variable number depending on the use case). Download
    : Download high-res image (126KB) Download : Download full-size image Fig. 8.
    Validation per quality factor. We can see that, when taking the aggregated number,
    none of the characteristics scored below the mean of the rating scale (1–7) indicating
    that Bolster was on average well-perceived by the use cases. Satisfaction sub-characteristics
    (i.e., Satisfaction, Trust, and Usefulness) present no anomaly, with usefulness
    standing out as the highest rated one. As far as regards Functional Appropriateness,
    Bolster was perceived to be overall effective, with some hesitation with regard
    to the functionality offered for the semantic exploitation of the data. All other
    scores are considerably satisfactory. The SRA is marked as functionally complete,
    and correct, and expected to bring benefits in comparison to current techniques
    used in the use cases. Ultimately this leads to a large intention to use. Discussion
    We can conclude that generally user’s perception is positive, being most answers
    in the range from Neutral to Strongly Agree. The preliminary assessment shows
    that the potential of the Bolster SRA is recognized also in the industry domain
    and its application is perceived to be beneficial in improving the quality-in-use
    of software products. It is worth noting, however, that some respondents showed
    reluctancy regarding the Semantic Layer in Bolster. We believe this aligns with
    the fact that Semantic Web technologies have not yet been widely adopted in industry.
    Thus, lack of known successful industrial use cases may raise caution among potential
    adopters. 8. Conclusions Despite their current popularity, Big Data systems engineering
    is still in its inception. As any other disruptive software-related technology,
    the consolidation of emerging results is not easy and requires the effective application
    of solid software engineering concepts. In this paper, we have focused on an architecture-centric
    perspective and have defined an SRA, Bolster, to harmonize the different components
    that lie in the core of such kind of systems. The approach uses the semantic-aware
    strategy as main principle to define the different components and their relationships.
    The benefits of Bolster are twofold. On the one hand, as any SRA, it facilitates
    the technological work of Big Data adopters by providing a unified framework which
    can be tailored to a specific context instead of a set of independent components
    that are glued together in an ad-hoc manner. On the other hand, as a semantic-aware
    solution, it supports non-expert Big Data adopters in the definition and exploitation
    of the data stored in the system by facilitating the decoupling of the data steward
    and analyst profiles. However, we anticipate that in the long run, with the maturity
    of such technologies, the role of software architect will be replaced in favor
    of the database administrator. In this initial deployment, Bolster includes components
    for data management and analysis as a first step towards the systematic development
    of the core elements of Big Data systems. Thus, Bolster currently maps to the
    role played by a relational DBMS in traditional BI systems. As future work, we
    foresee the need to design a generic tool providing full-fledged functionalities
    for Metadata Management System. Acknowledgements We thank Gerhard Engelbrecht
    for his assistance in setting up the validation process, and Silverio Martínez
    for his comments and insights that helped to improve this paper. This work was
    partly supported by the H2020 SUPERSEDE project, funded by the EU Information
    and Communication Technologies Programme under grant agreement no. 644018, and
    the GENESIS project, funded by the Spanish Ministerio de Ciencia e Innovación
    under project TIN2016-79269-R. References [1] D. Agrawal, S. Das, A. El Abbadi
    Big Data and cloud computing: current state and future opportunities EDBT 2011
    (2011) Google Scholar [2] S. Alsubaiee, Y. Altowim, H. Altwaijry, A. Behm, V.R.
    Borkar, Y. Bu, M.J. Carey, I. Cetindil, M. Cheelangi, K. Faraaz, E. Gabrielova,
    R. Grover, Z. Heilbron, Y. Kim, C. Li, G. Li, J.M. Ok, N. Onose, P. Pirzadeh,
    V.J. Tsotras, R. Vernica, J. Wen, T. Westmann AsterixDB: a scalable, open source
    BDMS PVLDB, 7 (14) (2014), pp. 1905-1916 CrossRefView in ScopusGoogle Scholar
    [3] S. Angelov, P.W.P.J. Grefen, D. Greefhorst A framework for analysis and design
    of software reference architectures Inf. Softw. Technol., 54 (4) (2012), pp. 417-431
    View PDFView articleView in ScopusGoogle Scholar [4] M. Aufaure What’s up in business
    intelligence? A contextual and knowledge-based perspective ER 2013 (2013) Google
    Scholar [5] B. Babcock, S. Babu, M. Datar, R. Motwani, J. Widom Models and issues
    in data stream systems PODS 2002 (2002) Google Scholar [6] C. Batini, A. Rula,
    M. Scannapieco, G. Viscusi From data quality to Big Data quality J. Database Manage.,
    26 (1) (2015), pp. 60-82 View in ScopusGoogle Scholar [7] B. Behkamal, M. Kahani,
    M.K. Akbari Customizing ISO 9126 quality model for evaluation of B2B applications
    Inf. Softw. Technol., 51 (3) (2009), pp. 599-609 View PDFView articleView in ScopusGoogle
    Scholar [8] B. Bilalli, A. Abelló, T. Aluja-Banet, R. Wrembel Towards intelligent
    data analysis: the metadata challenge IoTBD 2016 (2016), pp. 331-338 CrossRefView
    in ScopusGoogle Scholar [9] E.A. Brewer Towards robust distributed systems (abstract)
    PODC 2000 (2000) Google Scholar [10] C.L.P. Chen, C. Zhang Data-intensive applications,
    challenges, techniques and technologies: a survey on Big Data Inf. Sci., 275 (2014),
    pp. 314-347 CrossRefGoogle Scholar [11] J.O. e Sá, C. Martins, P. Simões Big Data
    in cloud: a data architecture WorldCIST 2015 (2015) Google Scholar [12] D. Esteban
    Interoperability and Standards in the European Data Economy - Report on EC Workshop
    European Commission (2016) Google Scholar [13] R.C. Fernandez, P. Pietzuch, J.
    Kreps, N. Narkhede, J. Rao, J. Koshy, D. Lin, C. Riccomini, G. Wang Liquid: unifying
    nearline and offline Big Data integration CIDR 2015 (2015) Google Scholar [14]
    G. Fox, W. Chang NIST Big Data Interoperability Framework: Volume 3, Use Case
    and General Requirements NIST Special Publication (2015) Google Scholar 1500-3
    [15] M. Galster, P. Avgeriou Empirically-grounded reference architectures: a proposal
    QoSA+ISARCS 2011 (2011), pp. 153-158 CrossRefGoogle Scholar [16] A. Gani, A. Siddiqa,
    S. Shamshirband, F. Hanum A survey on indexing techniques for Big Data: taxonomy
    and performance evaluation Knowl. Inf. Syst., 46 (2) (2016), pp. 241-284 CrossRefView
    in ScopusGoogle Scholar [17] H. Garcia-Molina, J.D. Ullman, J. Widom Database
    Systems - The Complete Book (2. ed.) Pearson Education (2009) Google Scholar [18]
    S. García, O. Romerós, R. Ravents DSS from an RE perspective: a systematic mapping
    J. Syst. Softw., 117 (2016), pp. 488-507 View PDFView articleView in ScopusGoogle
    Scholar [19] B. Geerdink A reference architecture for Big Data solutions - introducing
    a model to perform predictive analytics using Big Data technology IJBDI 2015,
    2 (4) (2015), pp. 236-249 CrossRefGoogle Scholar [20] A. Giacometti, P. Marcel,
    E. Negre A framework for recommending OLAP queries DOLAP 2008 (2008) Google Scholar
    [21] I. Gorton, J. Klein Distribution, data, deployment: software architecture
    convergence in Big Data systems IEEE Softw., 32 (3) (2015), pp. 78-85 View in
    ScopusGoogle Scholar [22] N.W. Grady, M. Underwood, A. Roy, W.L. Chang Big Data:
    challenges, practices and technologies: NIST Big Data public working group workshop
    at IEEE Big Data 2014 IEEE Big Data 2014 (2014) Google Scholar [23] J. Gray, D.T.
    Liu, M.A. Nieto-Santisteban, A.S. Szalay, D.J. DeWitt, G. Heber Scientific data
    management in the coming decade SIGMOD Rec., 34 (4) (2005), pp. 34-41 CrossRefView
    in ScopusGoogle Scholar [24] A. Grosskurth, M.W. Godfrey A reference architecture
    for web browsers ICSM 2005 (2005), pp. 661-664 CrossRefView in ScopusGoogle Scholar
    [25] K. Guo, W. Pan, M. Lu, X. Zhou, J. Ma An effective and economical architecture
    for semantic-based heterogeneous multimedia Big Data retrieval J. Syst. Softw.,
    102 (2015), pp. 207-216 View PDFView articleView in ScopusGoogle Scholar [26]
    M.J. Harry, R.R. Schroeder Six Sigma: The Breakthrough Management Strategy Revolutionizing
    the World’s Top Corporations Broadway Business (2005) Google Scholar [27] V. Herrero,
    A. Abelló, O. Romero NOSQL design for analytical workloads: variability matters
    ER 2016 (2016), pp. 50-64 CrossRefView in ScopusGoogle Scholar [28] M. Interlandi,
    K. Shah, S.D. Tetali, M. Gulzar, S. Yoo, M. Kim, T.D. Millstein, T. Condie Titian:
    data provenance support in spark PVLDB, 9 (3) (2015), pp. 216-227 CrossRefGoogle
    Scholar [29] B. Ionescu, D. Ionescu, C. Gadea, B. Solomon, M. Trifan An architecture
    and methods for Big Data analysis SOFA 2014 (2014) Google Scholar [30] ISO, IEC25010:
    2011 systems and software engineering–systems and software quality requirements
    and evaluation (SQuaRE)–system and software quality models (2011). Google Scholar
    [31] H.V. Jagadish, J. Gehrke, A. Labrinidis, Y. Papakonstantinou, J.M. Patel,
    R. Ramakrishnan, C. Shahabi Big data and its technical challenges Commun. ACM,
    57 (7) (2014), pp. 86-94 CrossRefView in ScopusGoogle Scholar [32] P. Jovanovic,
    O. Romero, A. Simitsis, A. Abelló, H. Candón, S. Nadal Quarry: digging up the
    gems of your data treasury EDBT 2015 (2015) Google Scholar [33] J. Kephart, D.
    Chess, C. Boutilier, R. Das, J.O. Kephart, W.E. Walsh, An architectural blueprint
    for autonomic computing(2007). Google Scholar [34] V. Khatri, C.V. Brown Designing
    data governance Commun. ACM, 53 (1) (2010), pp. 148-152 CrossRefView in ScopusGoogle
    Scholar [35] B. Kitchenham, S. Charters, Guidelines for performing systematic
    literature reviews in software engineering, 2007. Google Scholar [36] J. Kontio
    A case study in applying a systematic method for COTS selection ICSE 1996 (1996),
    pp. 201-209 Google Scholar [37] J. Kroß, A. Brunnert, C. Prehofer, T.A. Runkler,
    H. Krcmar Stream processing on demand for Lambda architectures EPEW 2015 (2015)
    Google Scholar [38] F. Liu, J. Tong, J. Mao, R. Bohn, J. Messina, L. Badger, D.
    Leaf NIST cloud computing reference architecture: recommendations of the National
    Institute of Standards and Technology (2012) Google Scholar [39] N.H. Madhavji,
    A.V. Miranskyy, K. Kontogiannis Big picture of Big Data software engineering:
    with example research challenges BIGDSE 2015 (2015) Google Scholar [40] S. Martínez-Fernández,
    C.P. Ayala, X. Franch, E.Y. Nakagawa A survey on the benefits and drawbacks of
    AUTOSAR WASA 2015 (2015) Google Scholar [41] M.A. Martínez-Prieto, C.E. Cuesta,
    M. Arias, J.D. Fernández The solid architecture for real-time management of big
    semantic data Future Gener. Comput. Syst., 47 (2015), pp. 62-79 View PDFView articleView
    in ScopusGoogle Scholar [42] N. Marz, J. Warren Big Data: Principles and Best
    Practices of Scalable Realtime Data Systems Manning Publications Co. (2015) Google
    Scholar [43] E. Meijer, G.M. Bierman A co-relational model of data for large shared
    data banks Commun. ACM, 54 (4) (2011), pp. 49-58 CrossRefView in ScopusGoogle
    Scholar [44] L. Moreau, B. Clifford, J. Freire, J. Futrelle, Y. Gil, P.T. Groth,
    N. Kwasnikowska, S. Miles, P. Missier, J. Myers, B. Plale, Y. Simmhan, E.G. Stephan,
    J.V. den Bussche The open provenance model core specification (v1.1) Future Gener.
    Comput. Syst., 27 (6) (2011), pp. 743-756 View PDFView articleView in ScopusGoogle
    Scholar [45] R.F. Munir, O. Romero, A. Abelló, B. Bilalli, M. Thiele, W. Lehner
    ResilientStore: a heuristic-based data format selector for intermediate results
    MEDI 2016 (2016), pp. 42-56 CrossRefView in ScopusGoogle Scholar [46] S. Nadal,
    V. Herrero, O. Romero, A. Abelló, X. Franch, S. Vansummeren, Details on Bolster
    - state of the art, 2016. Google Scholar [47] S. Nadal, O. Romero, A. Abelló,
    P. Vassiliadis, S. Vansummeren An integration-oriented ontology to govern evolution
    in Big Data ecosystems DOLAP 2017 (2017) Google Scholar [48] C. Ordonez Statistical
    model computation with UDFs IEEE Trans. Knowl. Data Eng., 22 (12) (2010), pp.
    1752-1765 View in ScopusGoogle Scholar [49] M.T. Özsu, P. Valduriez Principles
    of Distributed Database Systems, Third Edition Springer (2011) Google Scholar
    [50] P. Pääkkönen, D. Pakkala Reference architecture and classification of technologies,
    products and services for Big Data systems Big Data Res., 2 (4) (2015), pp. 166-186
    View PDFView articleView in ScopusGoogle Scholar [51] P. Panov, S. Dzeroski, L.N.
    Soldatova OntoDM: an ontology of data mining ICDM 2008 (2008) Google Scholar [52]
    D.L. Phuoc, H.Q. Nguyen-Mau, J.X. Parreira, M. Hauswirth A middleware framework
    for scalable management of linked streams J. Web Sem., 16 (2012), pp. 42-51 Google
    Scholar [53] S. Qanbari, S.M. Zadeh, S. Vedaei, S. Dustdar CloudMan: a platform
    for portable cloud manufacturing services IEEE Big Data 2014 (2014) Google Scholar
    [54] P. Russom, Big Data analytics, TDWI Best Practices Report, Fourth Quarter(2011)
    6. Google Scholar [55] R. Sharda, D.A. Asamoah, N. Ponna Business analytics: research
    and teaching perspectives ITI 2013 (2013) Google Scholar [56] J. Song, C. Guo,
    Z. Wang, Y. Zhang, G. Yu, J. Pierson HaoLap: a Hadoop based OLAP system for Big
    Data J. Syst. Softw., 102 (2015), pp. 167-181 View PDFView articleView in ScopusGoogle
    Scholar [57] M. Stonebraker What Does Big Data Mean BLOG@ACM (2012) Google Scholar
    [58] M. Stonebraker Why the ’Data Lake’ is Really a ’Data Swamp’ BLOG@ACM (2014)
    Google Scholar [59] I. Terrizzano, P.M. Schwarz, M. Roth, J.E. Colino Data wrangling:
    the challenging journey from the wild to the lake CIDR 2015 (2015) Google Scholar
    [60] C.-W. Tsai, C.-F. Lai, H.-C. Chao, A.V. Vasilakos Big Data analytics: a survey
    J. Big Data, 2 (1) (2015), pp. 1-32 Google Scholar [61] B. Twardowski, D. Ryzko
    Multi-agent architecture for real-time Big Data processing IEEE/WIC/ACM 2014 (2014)
    Google Scholar [62] T. Vanhove, G. van Seghbroeck, T. Wauters, F.D. Turck, B.
    Vermeulen, P. Demeester Tengu: an experimentation platform for Big Data applications
    ICDCS 2015 (2015) Google Scholar [63] J. Varga, O. Romero, T.B. Pedersen, C. Thomsen
    Towards next generation BI systems: the analytical metadata challenge DaWaK 2014
    (2014) Google Scholar [64] M. Villari, A. Celesti, M. Fazio, A. Puliafito AllJoyn
    Lambda: an architecture for the management of smart environments in IoT SMARTCOMP
    2014 (2014) Google Scholar [65] Y. Wang, L. Kung, C. Ting, T.A. Byrd Beyond a
    technical perspective: understanding Big Data capabilities in health care HICSS
    2015 (2015) Google Scholar [66] M. Weyrich, C. Ebert Reference architectures for
    the internet of things IEEE Softw., 33 (1) (2016), pp. 112-116 View in ScopusGoogle
    Scholar [67] Z. Xie, Y. Chen, J. Speer, T. Walters, P.A. Tarazaga, M. Kasarda
    Towards use and reuse driven Big Data management ACM/IEEE-CE 2015 (2015) Google
    Scholar [68] F. Yang, G. Merlino, X. Léauté, The RADStack: open source Lambda
    architecture for interactive analytics, 2015. Google Scholar [69] K. Zeng, J.
    Yang, H. Wang, B. Shao, Z. Wang A distributed graph engine for web scale RDF data
    PVLDB, 6 (4) (2013), pp. 265-276 CrossRefView in ScopusGoogle Scholar [70] R.
    Zhang, I. Manotas, M. Li, D. Hildebrand Towards a Big Data benchmarking and demonstration
    suite for the online social network era with realistic workloads and live data
    BPOE 2015 (2015) Google Scholar [71] Y. Zhuang, Y. Wang, J. Shao, L. Chen, W.
    Lu, J. Sun, B. Wei, J. Wu D-Ocean: an unstructured data management system for
    data ocean environment Front. Comput. Sci., 10 (2) (2016), pp. 353-369 Google
    Scholar Cited by (59) A Software Reference Architecture for Journalistic Knowledge
    Platforms 2023, Knowledge-Based Systems Show abstract Three decades of software
    reference architectures: A systematic mapping study 2021, Journal of Systems and
    Software Show abstract MEdit4CEP-SP: A model-driven solution to improve decision-making
    through user-friendly management and real-time processing of heterogeneous data
    streams 2021, Knowledge-Based Systems Citation Excerpt : Moreover, the sources
    which produce these data are heterogeneous, which means that these huge amounts
    of data are represented in a wide range of data structures. These two problems
    lead us to the term Big Data, which is grounded in the three V’s [1]: the first,
    Volume, refers to the amount of data generated; the second, Velocity, is related
    to the speed at which the data are generated; and the third, Variety, encompasses
    the heterogeneity of these data. Being able to work with these large and heterogeneous
    datasets, performing specific domain analytics and reacting in real time to situations
    of interest gives a company a notable competitive advantage. Show abstract Application
    of microservices patterns to big data systems 2023, Journal of Big Data Performance
    Analysis of Lambda Architecture-Based Big-Data Systems on Air/Ground Surveillance
    Application with ADS-B Data 2023, Sensors CoVaMaT: Functionality for Variety Reuse
    Through a Supporting Tool 2023, Communications in Computer and Information Science
    View all citing articles on Scopus 1 http://www.scopus.com. 2 http://www.core.edu.au/conference-portal.
    3 https://www.w3.org/2001/sw/wiki/Category:Triple_Store. 4 https://dev.twitter.com/streaming/overview.
    5 https://dev.twitter.com/streaming/overview/processing. 6 https://stanbol.apache.org.
    7 http://atlas.incubator.apache.org. 8 https://www.cloudera.com/products/cloudera-navigator.html.
    9 https://www.palantir.com. 10 http://virtuoso.openlinksw.com. 11 http://4store.org.
    12 http://graphdb.ontotext.com/graphdb. 13 http://allegrograph.com. 14 http://neo4j.com.
    15 https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-common/FileSystemShell.html#copyFromLocal.
    16 http://sqoop.apache.org. 17 https://aws.amazon.com/s3. 18 https://avro.apache.org.
    19 http://pig.apache.org/docs/r0.9.1/zebra_overview.html. 20 https://parquet.apache.org.
    21 https://hadoop.apache.org. 22 https://aws.amazon.com/elasticmapreduce. 23 http://spark.apache.org.
    24 https://flink.apache.org. 25 http://activemq.apache.org. 26 https://www.rabbitmq.com.
    27 http://kafka.apache.org. 28 https://aws.amazon.com/kinesis/firehose. 29 https://flume.apache.org.
    30 https://aws.amazon.com/kinesis/streams. 31 http://spark.apache.org/streaming.
    32 https://flink.apache.org. 33 https://aws.amazon.com/kinesis/analytics. 34 https://www.oracle.com/database.
    35 http://www.postgres-xl.org. 36 https://www.mysql.com/products/cluster. 37 https://hbase.apache.org.
    38 http://cassandra.apache.org. 39 https://aws.amazon.com/dynamodb. 40 http://www.project-voldemort.com/voldemort.
    41 https://aws.amazon.com/redshift. 42 http://getkudu.io. 43 http://neo4j.com.
    44 http://orientdb.com/orientdb. 45 https://www.mongodb.org. 46 https://www.rethinkdb.com.
    47 https://hana.sap.com. 48 http://www.nuodb.com. 49 https://voltdb.com. 50 http://redis.io.
    51 https://www.elastic.co. 52 https://aws.amazon.com/elasticache. 53 https://www.pipelinedb.com.
    54 http://kylin.apache.org. 55 https://www.elastic.co/products/kibana. 56 http://www.tableau.com.
    57 http://semver.org. 58 No details about the company can be revealed due to non-disclosure
    agreements. 59 https://www.supersede.eu/. 60 https://www.dhis2.org. View Abstract
    © 2017 Elsevier B.V. All rights reserved. Recommended articles Vibration effects
    on convection in a rotating fluid saturated porous layer distant from the axis
    of rotation International Journal of Heat and Mass Transfer, Volume 141, 2019,
    pp. 112-115 S. Govender View PDF A US multicenter study of hepatitis C treatment
    of liver transplant recipients with protease-inhibitor triple therapy Journal
    of Hepatology, Volume 61, Issue 3, 2014, pp. 508-514 James R. Burton Jr., …, Norah
    A. Terrault View PDF A graph-based method for interactive mapping revision in
    DL-Lite Expert Systems with Applications, Volume 211, 2023, Article 118598 Weizhuo
    Li, …, Guilin Qi View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 56 Captures Readers: 204 Social Media Shares, Likes & Comments: 3 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply. We use cookies that
    are necessary to make our site work. We may also use additional cookies to analyze,
    improve, and personalize our content and your digital experience. For more information,
    see ourCookie Policy Cookie Settings Accept all cookies'
  inline_citation: '>'
  journal: Information and software technology
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: A software reference architecture for semantic-aware Big Data systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2022.3162863
  analysis: '>'
  authors:
  - Engin Zeydan
  - Josep Mangues‐Bafalluy
  citation_count: 3
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 10 Recent
    Advances in Data Engineering for Networking Publisher: IEEE Cite This PDF Engin
    Zeydan; Josep Mangues-Bafalluy All Authors 4 Cites in Papers 3042 Full Text Views
    Open Access Comment(s) Under a Creative Commons License Abstract Document Sections
    I. Introduction II. Data Connection Frameworks III. Data Ingestion Frameworks
    IV. Data Analysis and Processing Frameworks V. Data Storage Frameworks Show Full
    Outline Authors Figures References Citations Keywords Metrics Footnotes Abstract:
    This tutorial paper examines recent advances in data engineering, focusing on
    aspects of network management and orchestration. We provide a comprehensive analysis
    of standardization efforts as well as platform development activities related
    to data engineering driven network design. We then focus on the integration aspects
    of the data engineering ecosystem and telecommunication networks. The results
    of our tutorial investigation show that despite various efforts towards standardization
    and network management and orchestration platforms, there is still a significant
    gap in applying recent developments in the evolving data engineering world to
    the telecommunication domain. New advanced functionalities in data engineering
    as well as clear separations between the building blocks of data engineering pipelines
    within the proposed standardized architectures have been overlooked or not explored
    in detail by the standardization or platform development bodies in the telecommunication
    domain. Therefore, at the end of the paper, we discuss these gaps and research
    challenges in the context of future development processes for data engineering-driven
    network design and applications of data engineering concepts in telecommunication
    networks. We also propose several recommendations for early adoption of these
    technologies and frameworks in telecommunication infrastructures and platforms.
    A high level illustration of the individual modules of a general data engineering
    platform to support the entire lifecycle of AI/ML. Published in: IEEE Access (
    Volume: 10) Page(s): 34449 - 34496 Date of Publication: 28 March 2022 Electronic
    ISSN: 2169-3536 DOI: 10.1109/ACCESS.2022.3162863 Publisher: IEEE Funding Agency:
    CCBY - IEEE is not the copyright holder of this material. Please follow the instructions
    via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles
    and stipulations in the API documentation. SECTION I. Introduction Over the past
    few decades, telecommunication operators and service providers have experienced
    exponential growth in connectivity. At the same time, there has been an increased
    demand for massive connectivity, huge amounts of data, and in some cases ultra-low
    latency communications. Because of this, network complexity has increased placing
    a tremendous burden on telecommunication providers to manage and orchestrate the
    network. To address the highly complex issues that such larger and highly integrated
    networks pose in the design, analysis, deployment, and management phases, recent
    advances in data science and engineering technologies in both academia and industry,
    have encouraged the adoption of various Artificial Intelligence (AI)/Machine Learning
    (ML) platforms and frameworks at different layers of the telecommunication network
    infrastructure. On the other hand, the advanced techniques used by large companies,
    such as Google, Facebook, NetFlix, Apple, and Amazon, to demonstrate how to leverage
    data, have led to major breakthroughs in the business landscape in terms of improving
    products, services, and customer experiences. As a result of the increasing computing
    power of computers and advances in AI/ML algorithms from IT and the cloud giants,
    new data processing capabilities are being introduced that have disrupted entire
    industries, including telecommunications. Therefore, improving network intelligence
    has been the focus of interest in the telecommunication world in recent years
    with many diverse and compelling use cases [1]–[7]. (Note that the detailed use
    cases and their classifications within each Standards Developing Organizations
    (SDOs) and alliance bodies are given in Section XI.) Advanced algorithms (e.g.,
    neural network-based Deep Learning (DL) algorithms) and computational patterns
    used in AI/ML platforms can help discover valuable information hidden in vast
    amounts of numerical data (images, videos, datasets, etc.). This will enable better
    decision making in the development of telecommunication infrastructure products,
    services and applications, while opening up innovative business opportunities.
    The recently evolving AI/ML ecosystem, its open source community [8], partnerships
    between the public private industry and academia [9], and the results from the
    labs of large cloud and IT giants like Facebook AI Research (FAIR),1 Google AI2
    and Microsoft AI3 etc. will support and enhance data-driven intelligence by gaining
    real-time (from streamed data) and long-term (from stored data) insights to understand
    what is going on in their infrastructure and develop competitive advantages. This
    will help develop better and more personalized services for telecommunication
    providers. However, a major challenge for all telecommunication providers in the
    world is to leverage recent advances and find current technologies to develop
    data science and engineering platforms. This is because of the various challenges
    in managing infrastructure and utilizing vast computing resources. Telecommunication
    companies serve millions of users who depend on their services for their daily
    needs. In order to maintain these critical services without interruption, telecommunication
    providers must be prepared to obtain the most relevant and accurate information
    so that they can make informed decisions and take the necessary actions. For this
    reason, the design, implementation and maintenance of systems that can process
    incoming telecommunication-related raw data sources and produce high-quality,
    reliable information to support data analytics and AI/ML systems is critical and
    falls within the scope of data engineering. A. Related Works There are numerous
    comprehensive review and position papers on the recent developments and deployment
    of AI/ML software, libraries and frameworks and their applications for data centers,
    network traffic, Software Defined Networking (SDN)/Network Function Virtualization
    (NFV)-enabled networks or Industry 4.0 [8], [10]–[25], [25]–[40]. In [10], the
    authors bridge the gap between DL and mobile and wireless networking research
    by presenting recent surveys and showing a typical pipeline of an application-level
    mobile data processing system. The paper in [11] provides an overview of DL for
    wireless communication networks. The authors in [12] explore the applications
    of DL algorithms in wireless networks for different network layers, including
    the physical layer, the data link layer and the routing layer. The paper in [13]
    gives an overview on unsupervised ML approaches that are applied in the networking
    domain. In [14], a framework for data-driven networks for proactive optimisation
    and related technologies in online data analytics for 5G systems are explored.
    The paper in [15] provides an overview of evolving ML algorithms applied to self-organizing
    cellular networks. The authors in [16] focus on the possible solutions on how
    ML can help support the targeted 5G network requirements. The papers in [17],
    [18] provide an overview of ML algorithms and their applications in SDNs. The
    authors in [19] focus on the use of AI and ML for the design and operation of
    Beyond 5G networks. The paper in [20] gives an overview of ML in wireless communications
    and presents some unresolved issues. The paper in [21] gives an introduction to
    the use of data science and describes steps towards knowledge discovery in the
    context of wireless networks. In [22], various software architecture practices
    that exist in the context of ML-based software systems are described. The authors
    in [23] provide an overview of related research on AI-based green communication
    and how it can be used to accelerate applications in 6G. The position paper in
    [24] presents an agenda for addressing the challenges associated with analyzing
    network data through AI/ML so that it can be naturally adopted in the network
    domain. In [25]. a comprehensive overview of ML solutions for 5G cellular networks
    is given. However, the focus of these contributions is on data science aspects
    rather than on data engineering frameworks and building End-to-End (E2E) data
    engineering pipelines. There is also a lack of detailed analysis of the applications
    of these technologies in larger areas/domains of E2E network management and orchestration.
    From a network management perspective, the paper in [26] addresses ML solutions
    that can be used as a tool for implementing network management, automation and
    self-organization from a 5G perspective. Self-healing solutions for emerging and
    future mobile networks are explored in [27]. Networking issues in Big Data are
    addressed in [28]. The authors in [29] discuss the role of AI techniques in the
    emerging concept of Zero-touch network and Service Management (ZSM). Although
    these papers focus on network and service management issues, the emphasis is on
    applying data science concepts to network management rather than exploring data
    engineering aspects. The survey paper in [30] brings together both Big Data Analytics
    (BDA) and Network Traffic Monitoring and Analysis (NTMA) research, and focuses
    specifically on approaches and technologies that can manage the big NTMA data.
    However, the focus of this research is only on aspects of network traffic monitoring
    and analysis, concentrates on a limited number of available Big Data tools, and
    lacks analysis of network management and orchestration aspects. The survey paper
    in [39] focuses on Data Center Networking and divides it into infrastructure and
    operations. However, the focus of this paper is on the operational and infrastructure
    aspects of data center networking rather than merging it with data engineering
    concepts and developments. The authors in [31] conducted a comprehensive survey
    of wired, wireless, and hybrid data centers to assess whether they can meet the
    requirements of a real-time analytic Internet of Things (IoT) network. However,
    the analysis focuses only on real-time analytics and does not examine the network
    management and orchestration aspects. The paper in [32] provides an overview of
    the role of BDA in designing a variety of data communication networks. The authors
    in [33] aim to design an industrial data platform that provides higher real-time
    performance and compression ratio for industrial data acquisition and processing.
    However, these works lack details on building an E2E data engineering pipeline
    and do not unify these data engineering components with network management and
    orchestration. The authors in [34] provide an overview of Big Data tools, but
    the focus is on manufacturing applications rather than telecommunication networks.
    The review paper in [8] provides a comprehensive overview of the latest AI software
    developments with comparisons and trends in the development and usage process.
    In [35], a detailed survey of distributed learning frameworks such as federated
    learning, federated distillation, distributed inference, and multi-agent reinforcement
    learning over real-world wireless communication networks is presented along with
    the rationale for their deployment over wireless networks. However, these papers
    mainly focus on data science developments rather than data engineering aspects
    by presenting only the latest developments in DL, distributed learning or machine
    learning frameworks and libraries. The paper in [36] provides an overview of popular
    Big Data frameworks for processing big data and compares them using batch and
    iterative workloads. However, the authors do not address how they relate to recent
    developments in network management and the orchestration aspects of telecommunications.
    In [37], the authors present an example BDA pipeline and architecture called LambdaTel
    for telecommunication enterprises. Similar to our six-stage categorization of
    the data engineering pipeline, the life cycle of BDA is divided into four sequential
    stages namely data acquisition, data preprocessing, data storage and data analytics
    in [38] for wireless networks. On the other hand, these pipelines do not focus
    on the application of these sequential phases to network management and orchestration
    problems in industry, academia, and standardization bodies. Moreover, they only
    cover a few components compared to the data engineering pipeline proposed here
    (e.g., details on the data management & orchestration component are missing).
    In contrast to the survey and review papers, this paper provides a tutorial view
    of the recent date engineering solutions to drive state-of-the-art developments
    in telecommunications, including papers in this research area that fills in the
    gaps of the previous survey and tutorial papers on BDA and network management
    & orchestration. Despite the previous work on the potential applications of Big
    Data infrastructures in telecommunication networks, there is still a lack of a
    general E2E data engineering architecture for telecommunication and networking
    domains. In particular, it is unclear to practitioners, researchers and developers
    in this emerging field how to systematically collect, ingest, analyse, process,
    visualize, and manage telecommunication data for network management and orchestration.
    Therefore, in this tutorial article, we explain how telecommunication networks
    can be used for data-driven analysis and autonomous optimization in future telecommunication
    networks using data engineering. In this paper, we mainly focus on current data
    engineering concepts and technologies related to network management and orchestration,
    which are essential pillars for creating data-driven intelligent support for telecommunication
    operators. Finally, we point out gap analyses, major challenges, and research
    issues that need to be addressed in the future. Data engineering concepts have
    emerged to develop scalable and resilient data warehouse systems or tools that
    can support complex analytics across AI/ML infrastructures, platforms, or products
    desired by data scientists, ML engineers, or product teams. It is designed as
    a superset of Business Intelligence (BI), data warehousing, DataOps, data management,
    data architecture, orchestration, and software engineering. Depending on complex
    business and technical requirements at large scale, data engineers are expected
    to continuously evolve data pipelines and processing models. An integrated environment
    for data acquisition, storage, analysis, monitoring, visualization, and a scalable
    computing environment in which data-driven applications and analytics tools can
    be deployed is of interest to the data engineering field. Data analytics enables
    organizations to gain key insights into the user experience using data pipelines.
    These pipelines are a key component to continuously leverage the evolving data-driven
    ecosystem in the ecosystem. Historically, data engineering has its roots in the
    trending topic of Big Data, when the Hadoop project code was first released in
    2006 [41]. Hadoop became popular by providing distributed Big Data storage and
    processing capabilities. The main systems consisted of four main modules: Hadoop
    core, Hadoop Distributed File System (HDFS). Map-Reduce [42], and Yet Another
    Resource Negotiator (YARN) [43]. After the introduction of Hadoop, many Apache
    projects emerged that built their core functionalities on top of Hadoop and from
    which the Hadoop ecosystem evolved. At the same time, Hadoop is still relevant
    today and continues to be used as the core foundation for many data engineering
    projects. In parallel with this movement, many organizations today rely on similar
    design principles and advanced algorithms on distributed systems to process data
    storage, messaging, management, and compute functions on multiple servers in parallel.
    The movement and processing of data can be achieved by creating streaming pipelines
    or data pipelines. In data pipelining, multiple data processing modules are chained
    together and the output of each module is used as input to the next module. Data
    engineering toolboxes enable organizations to process huge amounts of data reliably
    and quickly while gaining access to better, cheaper, and more accessible data
    analytics software and services. On the other hand, each newly introduced technology
    or component within a data engineering pipeline brings its own configuration,
    protocols, metrics, and tools, adding complexity to the overall platform. Today’s
    challenges and requirements for a data engineering solution include processing
    millions of tasks per second, latency in the order of sub-milliseconds, stateful
    computation (via functions that store data across processing items or events),
    and ensuring fault tolerance (e.g., by checkpointing state to recover state and
    positions in the stream). Most toolboxes are designed for scalability and are
    deployed in distributed environments where aspects of data distribution, replication,
    and coordination become important differentiators. At the same time, much of the
    software is becoming commoditized through open source software and packages, while
    the supply of data engineering solutions from cloud providers such as Microsoft,
    Amazon, and Google is increasing. As the data toolbox matures and the data engineering
    ecosystem blossoms, innovative solutions such as Online Analytical Processing
    (OLAP), scalable machine learning analytics will become more tangible to larger
    communities and enterprises. B. Overview and Tutorial Objectives Fig. 1 shows
    an overview of the different modules of a general data engineering platform. It
    mainly consists of six modules: Data Connect, Data Ingest, Data Analysis, Data
    Storage, Data Monitoring & Visualization, and Data Management & Orchestration.
    Note that the connections between each module are only loosely represented and
    there may be multiple interfaces between these modules depending on the use case.
    Therefore, there may be multiple pipelines based on Service Level Agreements (SLAs)
    or non-functional requirements. One pipeline may be suitable for real-time notifications,
    while another may be more suitable for more relaxed requirements. Furthermore,
    in some scenarios, such as IoT networks, raw data (e.g., temperature, humidity)
    collected via the Data Connect module can be integrated with the Data Visualization
    component either via mobile applications or web user interfaces for direct visualization
    in a dashboard. In other scenarios, further data analysis and processing using
    recent advances in AI/ML algorithms may be required (e.g., in the case of high
    quality prediction, statistical analysis or root cause analysis [44]), which takes
    place between the Data Connection and Data Visualization modules. FIGURE 1. A
    high level illustration of the individual modules of a general data engineering
    platform to support the entire lifecycle of AI/ML. Show All In other scenarios
    that require high reliability of data (e.g., in Ultra Reliable Low Latency Communications
    (URLLC) services of 5G networks), the Data Ingestion and Data Analysis modules
    (for ultra-low latency real-time event processing) need to be embedded in the
    data engineering pipeline. More details on the modules and the corresponding distributed
    computing frameworks/landscapes available today to run each of these modules are
    provided later in the paper. In addition to the open source frameworks, we have
    also listed vendor-specific tools/frameworks for each module whose main advantage
    over in-house setup is the ease of set-up and maintenance support for the data
    engineering applications. On the other hand, cost can be considered as one of
    the main disadvantages when these applications are deployed on a large scale with
    vendor-specific tools. The main contributions of this paper can be summarized
    as follows: Our goal is to provide a comprehensive and thorough overview of the
    recent advances and major technologies used in the context of data engineering.
    The paper categorizes the various modules of a general data engineering platform
    into Data Connect, Data Ingest, Data Analysis, Data Storage, Data Monitoring &
    Visualization and Data Management & Orchestration. This allows for easy understanding
    and comparison of studies within each area of the data engineering landscape.
    Our goal is to link the capabilities of the data engineering ecosystem with a
    possible link to future telecommunications systems. Unlike previous work on data
    engineering, this paper also explores the necessary link that needs to be established
    between recent advances in data engineering and traditional telecommunication
    ecosystems in the context of network management and orchestration. The paper provides
    a comprehensive discussion of challenges, gaps and future directions in the convergence
    of data engineering and network management and orchestration, and also identifies
    research directions that arise. To drive research in data engineering solutions
    for future network management and orchestration solutions, we also discuss possible
    solution methods for each of the above challenges. The remainder of the paper
    is arranged as follows. Section II introduces Data Connection frameworks and the
    possible data sources. Section III presents frameworks for Data Ingestion. Section
    IV discusses the latest frameworks for Data Processing and Analysis. Section V
    presents the latest frameworks for Data Storage. Section VI presents frameworks
    for Data Monitoring and Visualization. Section VII presents frameworks for Data
    Management and Orchestration. Section VIII discusses the relationship of data
    engineering projects with data science frameworks and AI/ML platforms used in
    the industry. Section IX gives an overview of network lifecycle management and
    orchestration. Section X provides an overview of standardization efforts in network
    management and orchestration and how they can be related to data engineering frameworks.
    Section XI gives an overview of data engineering use cases in telecommunication
    networks. Section XII provides the gap analysis, challenges and future directions.
    Finally, Section XIII presents the conclusions of the paper. SECTION II. Data
    Connection Frameworks The data connection serves as a trigger to connect to a
    data source, which can be either in a local file, on the web, on a mobile/IoT
    device, in a metastore table, or in a data store. Data sources can be in various
    forms, such as static data sources from files, databases (MySQL, MongoDB, etc.),
    network/server/storage or clickstream (web stream) logs, data taken from online
    resources, streaming on-demand data sources from third-party Application Programming
    Interfaces (APIs) or frameworks. This stage can be configured to transfer data
    into a Data Ingestion module. The connection frameworks are especially important
    for telecommunication service providers to enrich the already existing data with
    third party sources. A. Available Tools Representational state transfer (REST)-APIs
    are mainly used to import data from many third-party APIs into the Big Data processing
    cluster. It benefits from the ideas of stateless servers and structured access
    to resources. An API gateway is a software component and acts an entry point into
    a system. It is responsible for allowing multiple APIs, backend systems or microservices
    to be accessed reliably and securely by end users. Kafka Connect, which is part
    of Apache Kafka [45], is used to enable data flows between Kafka and various types
    of systems such as message queues, Hadoop, Spark, Flink, TensorFlow, databases,
    object stores or flat files. It supports pluggable connectors (which are essentially
    jar files that can be downloaded from Confluent Hub for example). Apache Flume
    [46] is a service for efficiently collecting, aggregating and moving large amounts
    of data in a distributed and reliable manner. GraphQL4 is a query language for
    APIs and designed as an alternative to REST-API that allows a variety of different
    frameworks to connect from the client side during client-server communication.
    The advantage of GraphQL is that it prevents over- and under-fetching of data
    compared to REST-APIs. Falcor5 is another open source library used to retrieve
    data that may reside in a client’s memory or over the network on the server. Apache
    NiFi6 is mainly used to automate data movement between different systems. It provides
    a web-based user interface for creating, monitoring or transforming (e.g., converting
    Comma Separated Values (CSV) files into individual JavaScript Object Notation
    (JSON) records) data streams. 1) Vendor Specific Tools Amazon’s API Gateway (for
    connecting to devices), Azure Event Hub and Data Gateway (a proxy that provides
    on-premise access to data) and Google Cloud Platform (GCP)’s Cloud Dataflow, other
    connector platforms such as FiveTrain,7 Stitch,8 Matillion9 are some example tools
    and frameworks for data connectivity. B. Data Sources to Connect In telecommunication
    operators, there are various sources of information from which data can be retrieved
    for further analysis by data engineering frameworks. Generally data is available
    in three different systems: Information Technology (IT), network and application
    systems. IT systems data is collected by various IT systems such as Customer Relationship
    Management (CRM) systems, billing systems or customer care services. Some examples
    are basic customer, account, and user profile and characteristics information,
    billing data, business consumption data, social media data and marketing/sales
    department promotions/campaigns, etc. Network systems data (from both wired and
    wireless networks) includes network equipment data, Call Detail Records (CDRs),
    eXtended Data Records (XDRs), Machine-to-Machine (M2M) data, traffic data (both
    signalling and payload data), Operations Support Systems (OSS) data (network events
    (outages, alarms), network performance data, etc.), voice, SMS, network service
    data, user equipment (UE) mobility and location updates, quality-of-service (QoS)
    parameter data collected from access, transport and core networks (e.g. from Access
    and Mobility Management Function (AMF), NG-RAN, Authentication, Authorization
    and Accounting (AAA), Home Subscriber Station (HSS) servers, etc.). Some of the
    typical data sources in mobile networks are also described in [47]. Application/service
    data is data from products and services (e.g., online mobile payments, online
    music and e-wallet applications or vehicle tracking, power grid information and
    health services, other value-added services, etc.) provided by telecommunication
    operators that contain user data (e.g., user access modes, addresses, timestamps,
    business preferences, consumption habits, customer care agent’s data). Note that
    the underlying structures of this data are complex (either unstructured (text,
    images, videos), structured or semi-structured), so targeted data engineering
    pipelines for different data types are required depending on the use case. SECTION
    III. Data Ingestion Frameworks Along with the advent of 5G, IoT and mobile sensor
    devices, powerful messaging platforms are needed that can ingest and cache all
    traffic for later processing. To reliably publish and subscribe to events, highly
    available, fault-tolerant ingest pipelines are required that can serve as the
    backbone of the streaming data infrastructure. To realize this, data ingest frameworks
    enable replication and partitioning of data across nodes in the cluster. The data
    ingestion module acts as an intermediary or multi-tenant data hub that connects
    incoming data from different data sources to diverse sinks to ensure that data
    is not lost during this movement. The source systems can be any vendor application,
    database or event. Data Ingestion is generally used to move data between external
    systems and Big Data clusters or data lake (e.g. based on Hadoop) for batch or
    stream processing (e.g., for filtering and mapping operations). A stream is an
    unbounded and continuously updated data set. In general, it consists of sequences
    of key-value pairs that are ordered, replayable, and fault-tolerant. Streaming
    data can be injected into clusters in real-time or near real-time. When loaded
    the data can be used for later processing (e.g., with Apache Spark) or storage
    (e.g., with HDFS). When communicating during data ingestion, two types of message
    delivery patterns can occur [48]. One is queuing and the other is streaming. A.
    Queuing In queuing, the order is not important because all events from the message
    queue are transferred from the device/user to a system. Some examples are payments
    and transaction processing, where they are mostly used for mission critical systems.
    Message queues provide asynchronous communication protocols where the sender and
    receiver do not need to interact with the message queues at the same time and
    are common features of data ingestion frameworks. Platforms and protocols such
    as RabbitMQ [49], JMS, AMQP and others enable asynchronous data integration between
    multiple systems by acting as a central hub. They are best suited for message
    queuing applications, such as real-time transaction services with zero tolerance
    to data loss. Therefore, consistency and durability are the most important features
    of these systems. On the other hand, these systems may suffer from scalability
    issues during peak loads (e.g., RabbitMQ is not designed as a distributed system).
    This is also true for web service/API based architectures due to their synchronous
    communication [50], [51]. Apache Kafka [45], on the other hand, is a popular and
    widely used framework for real-time processing of streaming data and is best suited
    for managing data pipelines to move large amounts of data between different systems.
    As a common event bus that decouples producers and consumers, it can handle hundreds
    of thousands of events per second, is scalable and widely used in the industry
    (used by many companies such as Twitter, Netflix, etc.). Therefore, integration
    with other technologies and frameworks in creating data pipelines has also become
    easy. For example, data sharing is possible with many interfaces including file-based
    systems, real-time messaging, REST web service, Structured Query Language (SQL)
    or Not Only SQL (NoSQL) databases, data lakes or data warehouses. In addition
    to simple data sharing, messaging and integration, it can also be used for data
    storage and processing. However, Kafka is not suitable for storing and processing
    large files such as images and videos as a whole. To ensure the reliability of
    streaming requests, Table 1 provides the three different consistency guarantees
    available for data ingestion or stream processing. Note that Message Queuing Telemetry
    Transport (MQTT), a lightweight publish-subscribe messaging transport protocol,
    provides a real-time and reliable messaging service and has also defined similar
    QoS levels. QoS level 0, level 1, and level 2 are at most once, at least once,
    and exactly once respectively. QoS level 0 acts as a best-delivery mechanism,
    QoS level 1 waits for the receiver’s PUBACK packet and retransmits it if it is
    not received, and QoS level 2 sends a sequence of four messages to guarantee exactly
    once reception. TABLE 1 Comparisons of Three Different Consistency Guarantees
    Based on Reliability Requirements of the Streaming Request B. Streaming Stream
    processing engines/platforms (such as Spark Streaming, Apache Flink) enable strictly
    ordered and exclusive message passing while allowing computational logic to be
    applied to message streams [52]. In a streaming-based communication pattern, the
    ordering of events is important so that behaviour can be analyzed based on the
    sequence of ordered events. Streaming is most suitable for stateful applications
    and OLAP-oriented use cases (e.g. BI, dashboards, ML, etc.). Some examples are
    user behaviour analysis, anomaly detection and web traffic log analysis. In streaming,
    the loss of data can be tolerated in some parts as long as the correct ordering
    of events is maintained. This is because stream processing systems provide fault
    tolerance and retries by rewinding the stream and replaying each event from the
    point of failure or occurrence of an error [53]. Apache Storm [54] is a distributed
    stream processing framework designed for both batch and distributed streaming
    data processing. Libraries of processing frameworks such as Spark Streaming (an
    extension of Spark’s API [55] for stream processing), Apache Flink (using DataStream
    API for bounded (finite size)/unbounded (infinite in size) streams) also have
    data ingestion capabilities. Spark Streaming uses a micro-batch architecture for
    continuous data processing that can ingest data from Apache Flume, TCP websockets,
    or Kafka producers. Spark relies on exactly-once processing to ensure correctness.
    Apache Pulsar10 has recently emerged as a competing technology to Kafka. Pulsar
    has similar features to Kafka and acts as a distributed pub-sub messaging system
    with some differences in architecture, performance, and features. Pulsar can also
    integrate with full-fledge stream processing frameworks like Spark and Flink.
    On the other hand, Pulsar offers more flexibility as it uses a layered design
    compared to the monolithic design of Kafka. For example, Kafka uses Zookeeper
    (with plans to remove it in future releases with a new controller metadata quorum)
    and the Kafka broker itself (a two-tier architecture that tightly couples storage
    and serving), while Pulsar requires three distributed systems: Zookeeper, Apache
    Bookkeeper (three-tier architecture), and also RocksDB for certain storage tasks.
    The computations are performed on a broker in one tier and the stateful storage
    is managed in another tier (Bookkeeper). Therefore, Pulsar aims to separate serving
    and storage in different tiers and provide both queuing (messaging) and streaming
    capabilities in a single system. To achieve this, Pulsar offers four types of
    subscriptions, depending on the application’s ordering and consumption scalability
    requirements: Exclusive subscription: Only individual consumers may subscribe.
    Failover subscription: Multiple consumers may subscribe to a single topic. Shared
    subscription: Messages are delivered to multiple consumers in a round-robin fashion.
    This allows the number of consumers to scale beyond the number of partitions.
    Key_Shared subscription: Multiple consumers can join the same subscription and
    message delivery will be shared among consumers that have the same key. It allows
    higher scaling as well as order guarantees at the key level. For messaging/streaming
    applications, exclusive and failover subscription modes are most suitable for
    scenarios where partition level order guarantees are needed, while queuing applications
    can use shared and key shared subscriptions [56]. C. Use Cases and Requirements
    During data ingestion, data enrichment, filtering, aggregation, transformation,
    etc. can also be performed for better use in sinks. Stream-based architectures
    have been shown to provide a better architectural foundation for many use cases
    in the industry including the use case for fraud detection [57]. In the telecommunications
    domain, the authors in [37] have provided several use cases for the application
    of streaming. Within the traditional telecommunication landscape, there are several
    components within the OSSs, Business Support Systems (BSSs), and OSS-BSS integration
    modules. The OSS/BSS landscape covers various functionalities across mediation,
    billing, CRM, e-business, data warehouse, service assurance, provisioning, etc.
    [58]. Therefore, the data ingestion frameworks described in this paper (such as
    Apache Kafka) can be used in various ways in these OSS, BSS and OSS-BSS integration
    development projects, e.g. in critical business applications such as payment or
    fraud detection applications. Application requirements may vary depending on real-time
    (less than 10 ms), near-real-time (less than a few minutes), or high-throughput
    (processing data on the order of PB/day in a single cluster). At the same time,
    data ingestion can be performed with different components: Batch processing jobs
    managed by data orchestration engines can be used for complex processing and deep
    analytics. On the other hand, streaming jobs (e.g., Spark Streaming, which consumes
    data from Apache Kafka streams) can be used for fast feedback and anomaly detection,
    sync-async, publish-subscribe, change data capture, or REST services that expect
    data for the data cluster. Various changes can be made in the data ingestion frameworks
    to meet these different application requirements. For example, for applications
    that require low-latency, the write-once-read-multiple times (WORM) model can
    be used, where a dataset generated or copied from the source can be used multiple
    times later for different analyses [41]. When ingesting data streams, it should
    be possible to query the data as soon as it enters the system to enable immediate
    actions and insights. For this reason, various preprocessing analytics such as
    cleansing, profiling, aggregation or enrichment of the dataset can improve query
    response time. In the WORM model, querying can be faster if there is an additional
    cost in data entry, for example if each map-reduce job is entered before the analytic
    queries. Some of the fast query systems like Apache Druid [59], HiveQL [60] are
    based on the principle of additional cost incurred in data ingestion. For this
    reason, before executing queries, each data segment must be ingested using some
    map-reduce ingestion jobs. This is also related to the mutation rate requirements
    of data. For data coming from transaction applications, extensive writes must
    be performed via Online Transaction Processing (OLTP) (which incurs additional
    costs) and for data coming from OLAP systems, extensive reads can be performed
    but only small writes. In the telecommunication industry, there are many use cases
    where open source or proprietary/vendor-specific frameworks for data ingestion
    are already in use, e.g. in monitoring, middleware, event hub platforms, and business
    applications (billing, supply chain management, BSS, B2B products, etc.). Communication
    of these various entities in a large enterprise via message brokers has certain
    advantages over REST-based API. In fact, there can be data ingestion issues when
    scaling applications over REST-APIs. As the application grows and more services
    are added, complex relationships between services arise, requiring API connections
    to be remapped. This also delays the workflow development and implementation process.
    Moreover, due to the synchronous communication structure of REST-based API’s,
    when there is an influx of data, i.e., sudden burst requests, some of the services
    may operate slowly or even be unavailable, affecting the reliability of whole
    application [50]. For this reason, message queuing (especially for streaming applications)
    can be more robust compared to REST APIs and scale with the requirements of each
    application [51]. In the case of microservices based architecture, the ability
    to use pub/sub systems may be an appropriate way to inform microservices of the
    potential events (request) that may require a reply from the corresponding receiver
    (response). D. Other Available Tools In addition to the Apache Kafka and Apache
    Pulsar streaming/messaging tools described above, there are other data ingestion
    frameworks that can work at scale [61]–[65]. Logstash and Beats are the core components
    of Elastic Stack that are used for ingesting data from any data source and transferring
    it to Elasticsearch [61]. Apache Heron is a real-time stream processing engine
    that has proven itself at Twitter for big data [62]. Apache Gobblin11 is a distributed
    data integration framework that simplifies data integration for both streaming
    and batch data ecosystems. It is used to extract, transform, and load large amounts
    of data from various sources, such as databases, REST APIs, onto Hadoop, and also
    simplifies data ingestion, organizational replication, and lifecycle management.
    Apache RocketMQ12 is a distributed messaging and streaming platform. Redis PubSub
    [63] is an implementation of the messaging system provided by Redis. Apache Sqoop
    [64] is used to import/export data from/to MySQL databases to/from HDFS in specific
    file formats. Apache Camel [65] is an open source integration framework designed
    as message-oriented middleware that provides interfaces for the Enterprise Integration
    Patterns. A good comparison of some of the most recent message queuing systems
    (Kafka, RabbitMQ, RocketMQ, ActiveMQ, and Pulsar) can be found in [66]. A curating
    list of existing streaming frameworks and applications can also be found in [67].
    1) Vendor Specific Tools Amazon’s Kinesis Streams,13 (for buffering purposes and
    works similarly to Kafka), Facebook’s Puma, Swift, and Stylus stream processing
    systems [48], Google’s Cloud Pub/Sub14 (data ingestion and messaging for event-driven
    systems as well as streaming analytics), and Azure’s Event Hubs,15 IoT Hub,16
    Stream Analytics17 are the respective data ingestion services offered by the IT
    and cloud giants. Splunk18 collects and analyzes machine-generated data on a large
    scale. As a messaging system, Oracle Enterprise Messaging Service and IBM Websphere
    MQ are other examples of event buses for processing asynchronous data streams.
    SECTION IV. Data Analysis and Processing Frameworks After data is ingested, it
    must be analyzed to gain insight so that resources, services, or assets can be
    managed more efficiently. The data processing and analysis phase ensures that
    this is achieved through various tasks such as data transformation, enrichment,
    filtering/deduplication, mapping, cleansing, updating state, joining, grouping,
    defining windows, aggregation, staging, integrity checking and combining streaming
    and batch data. Data analysis and processing frameworks generally fall between
    data ingestion and data storage frameworks. All data processing tools first consume
    data, then process it, and finally produce results. Batch processing involves
    processing large amounts of data, usually stored in data lakes/warehouse. In this
    case, the data does not change much or at all. Moreover, SLAs are loose in terms
    of processing time for a given dataset. In batch processing, the results are obtained
    slowly but accurately. In stream processing on the other hand, the data is constantly
    changing (e.g., Twitter or Facebook feeds or weather sensor data in real-time)
    and is not stored in large data warehouses. At the same time, SLAs are more rigid
    and data processing should be done in real-time to serve other processes, i.e.,
    no shifts in data processing are allowed as in batch processing. In streaming,
    the results can be fast depending on the requirements, but sometimes they need
    to be approximate because of the trade-off between low latency analysis and efficient
    use of computational resources over a distributed infrastructure. For this reason,
    the paradigm of approximate computing is used in the literature for streaming
    analytics, which allows trading output accuracy for computational power by analyzing
    only a subset of the input [68]. A. Data Analytic Categorization Data analysis
    can be categorized into four dimensions [71]. Descriptive analytics is used to
    understand what has happened in the past, including the recent past [72]. In this
    analytics, the data is examined statistically. Example: An alarm monitoring system
    has received an unusually high number of alarms at a telecommunication provider’s
    network operation center. Using descriptive analytics can help understand what
    is going on in the infrastructure by using real data and corresponding statistics
    (alarm timestamp, location, severity, etc.). Diagnostic analytics is used to understand
    why something happened in the past [73]. It is a step further to descriptive analysis.
    For example, if many alarms have occurred in a telecommunication system, diagnostic
    analysis helps to understand the root cause of the alarms, such as power failure
    or connection error. Predictive analytics is used to predict what will happen
    or is most likely to happen in the future [74]. In a ML system, the model is trained
    and later used for inference in production systems. For example, an increase in
    alarms in network monitoring systems can be predicted to take preventive measures
    much earlier before an outage occurs, e.g., in transport links. Prescriptive analytics
    is used to recommend actions to influence outcomes [74]. It is another step further
    in predictive analytics. For example, if we know that alarms will increase due
    to a transport network failure, network operators can enable redundant transport
    links to route user traffic to a different routing path. B. Model Deployment The
    frameworks developed in this module also complete the full cycle of the ML process
    through tasks such as feature extraction, distributed Graphics Processing Unit
    (GPU)-based model training, model deployment, model serving, and A/B testing.
    In practice, the models can be deployed in three different ways [75]: Offline
    deployment: The model is deployed in an offline container and predictions are
    made ad hoc. Online deployment: The model is deployed in an online model serving
    cluster where clients can send their prediction or batch requests over the network.
    Library deployment: This case is similar to the online deployment in a serving
    container, except that the model is now embedded as a library and as another service.
    Table 2 summarizes the various choices for ML deployment. TABLE 2 ML Deployment
    Options C. Stream Processing Engines One of the earliest versions of data analysis
    and processing frameworks was based on the Hadoop’s Map-Reduce paradigm. As the
    ecosystem continued to evolve, new and more advanced versions of Map-Reduce emerged
    based solely on batch processing, extending it to streaming applications as well.
    In streaming applications, data is continuously generated from multiple sources
    and frequently updated. Moreover, in most cases, the data sources send their information
    simultaneously. Therefore, it is important to analyze streaming data sources and
    derive, track, and interpret important streaming data quality metrics. In streaming
    applications, each data must be accurately processed to preserve its sequential
    order in time as well as its relationship with other data sources. A complex job
    like streaming with multiple input data streams and stages usually has many internal
    states and performs more remote calls to other REST endpoints and databases, e.g.,
    for joining operations. Streaming can occur in various forms, e.g., socket-based
    streaming (connecting to a socket to get data), file-based streaming (connecting
    to a file stream to get data), or data ingestion tool-based streaming (e.g., connecting
    to Kafka or Kinesis to get queued messages reliably (i.e., with acknowledgements)).
    Note that analyzing and processing data in real-time data enables organizations
    to make decisions proactively, rather than reactively. Some of the applications
    of streaming data include scenarios such as sending real-time notifications to
    users via email or push notifications when events change in their user account,
    sending sensor data in vehicles, industrial equipment or machinery for predictive
    maintenance purposes, tracking of geolocation of user phones or vehicles for transportation
    and supply-chain purposes, monitoring patients’ vital bodily functions for health
    purposes, or collecting streaming data and create personalized products and services
    for users of an online retail company. For these various reasons, streaming data
    analytics is becoming increasingly important for most industries. Recently, many
    frameworks have also emerged that can are suitable for batch (offline data) and
    streaming (real-time events) analytics (Apache Spark [55], Apache Flink [76],
    Apache Beam,27 etc.) while operating under a unified data processing layer to
    perform common computations and reduce data infrastructure complexity. These frameworks
    are known for their extremely scalable data processing capabilities that can analyze
    petabytes of data while extending to hundreds of instances. They can also provide
    different levels of abstraction. On the other hand, each of these frameworks has
    its own strengths and weaknesses. Apache Spark [55] is a distributed computing
    platform that can run standard Extract-Transform-Load (ETL) processes on Big Data
    in batch and streaming mode using a set of APIs. It is capable of processing jobs
    10 times faster than its Map-Reduce counterpart. The spark core API is able to
    load data from different platforms (e.g. Kafka, Flink, Kinesis, HDFS, Amazon S3,
    Azure Blob, etc.) and write it back to other platforms (e.g. Hadoop, Elasticsearch,
    Cassandra, etc.) after processing the data. The Spark ecosystem is also extensive
    and includes a number of libraries that can run on top of the Spark core and can
    easily implement and support various workloads and Spark programs such as BigDL
    [77] (which supports inference, transfer learning, and distributed training),
    TensorFlowOnSpark28 (which supports inference, ML pipelines, and distributed training),
    Deeplearning4j29 (which supports inference, transfer learning and distributed
    training), and DL Pipelines30 (which supports large-scale inference/scoring, transfer
    learning, multimodal training, ML pipeline, and Spark SQL) for DL or Ray for Reinforcement
    Learning [78], etc. It can also work on training a model online using various
    techniques, including supervised, unsupervised learning (to ensure that the model
    is stable) and for online predictions or running model inference at run time.
    The core abstraction in Spark is essentially a dataframe or dataset (in the latest
    versions) that can be transformed to perform a number of operations. Spark Streaming
    can support flat files, TCP/IP data, Apache Kafka, Apache Flume [46], Amazon Kinesis
    or Twitter, Facebook, and other social media platforms as data sources. Similar
    to Spark, Apache Flink [76] can be used for multiple operations and processes
    such as filtering, mapping, or running multiple stream joins on real-time or streaming
    data. Flink’s core APIs like DataStream API supports both bounded and unbounded
    streams, while DataSet API supports bounded data sets for batch use cases. DataStream
    APIs allows for large and more sophisticated operations compared to DataSet APIs.
    Flink is designed to process streaming data quickly and accurately with its stateful
    stream processing capabilities. D. Windowing Operation and Partitioning There
    are two main window types that can be applied to the data stream (see Table 3).
    For streaming applications, time spans are critical. The window concepts (tumbling,
    sliding and session window) that are mentioned above are closely related to the
    time characteristics, which are another important concept for data engineering
    pipeline and frameworks. In any data processing, the following key concepts of
    timing are important: Event time: represents the time at which an event is generated
    at the source of the system. Ingestion time: represents the time at which the
    event enters dataflow. Processing time: represents the time when an event was
    processed/observed in the system. TABLE 3 Various Options to Select Different
    Types of Window Operations Depending on Use Case In an ideal scenario, the interval
    between event time and processing time should be zero (real-time applications).
    However, this may not be the case for various reasons (e.g. network congestion,
    software logic, etc.). Note that depending on the use case, time characteristics
    may be important (e.g. fraud detection, most billing applications). Unlike Apache
    Spark Streaming which has time-based window criteria, Flink provides more powerful
    window semantics (both time- and data-driven window options) and allows to work
    with data-based or custom window criteria. Apache Samza [79], developed by Linkedin,
    is a stream processing framework for real-time analytics and is well integrated
    with Kafka. Apache Beam is used to build an execution pipeline that can implement
    both batch and stream processing under a unified programming model. For example,
    in the case of model serving, multiple runners such as Spark or Flink can be executed
    using Beam’s distributed processing backends. So, in this respect, it is more
    of a software development kit than a stream processing engine. Frameworks such
    as Apache Livy31 enable a REST service for executing Spark jobs via third-party
    applications. In order to process large amounts of data, the data must first be
    partitioned so that it can be processed in parallel. For this reason, partitioning
    is a key concept. It can be done either in memory or on storage disk to improve
    performance and reduce cost. Most data processing frameworks perform partitioning
    in memory (e.g. Spark or Flink). There are several ways to manage a partition
    in Spark or Flink. Some of them are listed in the Table 4. Spark recommends 2–3
    tasks per CPU in a given cluster to keep the level of parallelism high. For this
    reason, the recommended number of partitions in a given system is simply the number
    of CPUs ×[2or3] . TABLE 4 Various Ways to Manage Partitions in Spark or Flink
    TABLE 5 Characteristics of Open-Source Frameworks in Data Engineering for Networking
    and Corresponding Related Works E. Query-Based Analysis Using SQL as a common
    interface for data has many advantages. Query-based engines allow to build applications
    that interact programmatically with metastore tables. Many different systems provide
    an SQL interface for streaming and batch data on different platforms. Originally
    developed approaches such as Apache Hive [60], Apache Pig [80], Trino32 and Apache
    Impala [81] are used to query large distributed batch data with SQL-like syntax
    from Big Data storage systems such as HDFSs. On the other hand, newer data processing
    frameworks like Apache Spark SQL library (supports partial SQL functionalities),
    Apache Drill [82] (supports writing SQL queries similar to MySQL, Microsoft SQL
    Server, or Oracle), Apache Flink [76], Presto [83], Samza SQL [79], Apache Kylin
    [84], Apache Pinot [85] are some examples that are being used beyond these originally
    developed approaches to query data across many dimensions and metrics, various
    data sources including Hadoop clusters, NoSQL databases (Hbase, MongoDB, Cassandra,
    etc.), file systems/formats (CSV, Parquet, Avro, JSON, binary files, etc.), and
    cloud storage platforms. The graph query languages Cypher [86] and GQL33 (which
    are more declarative and easy to read compared to SQL) are used for graph database
    queries. Kafka Streams or ksqlDB [45] is a Java-based library designed to provide
    streaming processing applications based on Kafka in a fault-tolerant and distributed
    manner. It can provide full-fledged stream processing capabilities that include
    consumption of Kafka topics, provision of state information, basic transformations,
    filtering, sliding windows, calls to ML jobs, exactly-once processing, etc. Similarly,
    ksqlDB allows processing of records/events using a SQL-like language. On the other
    hand, Kafka Streams is not well-suited for processing large amounts of data. Apache
    Kudu [87] is designed for fast analysis of high speed data (that is rapidly changing
    data), and can perform queries over billions of rows and terabytes of data per
    second. In addition, there are several ways to access query-based Big Data frameworks,
    such as through a custom shell, a REST interface, a web interface, or through
    transport protocols such as Java Database Connectivity (JDBC)/Open Database Connectivity
    (ODBC) drivers, database protocols (MySQL, Postgres, Hive, etc.), and Remote Procedure
    Call (RPC) with Thrift, Protobuf, JSON, XML and so on. Spark’s Thrift server,
    for example, enables this functionality by turning SQL queries into Spark jobs.
    Apache Arrow34 is a language independent big data layout, enables in-memory analytics
    for fast processing and movement of data. It essentially enables sharing and serialization
    of high performance data and serves as a communication interface. Apache Arrow
    provides bindings between many components, e.g. reading a file in a given format
    (e.g., in Parquet data format) and converting it to another format (e.g., Spark
    dataframe/dataset) for further processing without conversion issues. Some other
    frameworks, such as the Ray architecture [78], are based on Apache Arrow. 1) Vendor
    Specific Tools Google provides Big Data tools like DataFlow,35 DataProc, BigQuery
    (query engine for static datasets), Cloud AutoML, etc. as a service to its customers.
    Amazon offers SageMaker for ML, AWS Kinesis36 for real-time streaming, Amazon
    Athena (based on Presto), and AWS Redshift Spectrum for interactive query services
    and EMR. Microsoft offers Azure Event Hubs.37 Databricks offers Databricks Unified
    Analytics Platform as a managed service. Dremio offers a cloud data lake engine
    for Big Data queries. SECTION V. Data Storage Frameworks Data storage is part
    of the data pipeline. Storage systems can be divided into three different systems:
    A. Relational Databases Relational databases consist of constructs (e.g., tables
    and rows) and constraints (e.g., primary keys and referential integrity constraints)
    and provide both OLTP and OLAP. For structured data, traditional relational/transactional
    databases/systems such as MySQL, PostgreSQL are used for persistence performance.
    In these systems, data related to users (e.g. credentials data from frontend),
    production and business systems can be stored. In these systems reading data in
    the database is cheaper than writing data to the database. On the other hand,
    (horizontal) scaling of these datasets is a big problem, where with increasing
    table size or many concurrent queries, important operators like grouping or joining
    become slower (with bad time complexity). B. Data Lakes Data Lakes have essentially
    evolved to complement data warehouses, which in the 2010s were unable to support
    semi-structured and unstructured data. Data lakes can store more types and amounts
    of raw data than relational databases, which have several scaling issues. They
    are based on unlimited, cheap storage. Unstructured data such as text, documents,
    images, videos, etc., semi-structured data such as web server logs, streaming
    data from IoT, etc., and data with high variety, volume, and velocity are stored
    in Data Lakes (e.g. in distributed storage systems (Hadoop clusters HDFS, Ceph
    [88]), NoSQL databases (such as MongoDB, Apache Cassandra (inspired by Amazon
    DynamoDB [89]), CouchDB [90], Hbase, CosmosDB), NewSQL databases (such as MariaDB,
    MemSQL, VoltDB, InfluxDB, NuoDB)). Depending on the data models, NoSQL databases
    can also be divided into the following categories: Graph databases store data
    as nodes and the connections between the data are called links or relationships.
    Graph data is used to create whole connections of data that is representations
    of data elements and their mutual relationships. In networks, for example, network
    services can be represented as graphs to better focus on the connections between
    network components. Graphs are versatile and can model some systems better than
    representations in tabular format. Graph databases (e.g., Neo4j [91], Apache Giraph38)
    are used by analytics engines to derive more insights, values and patterns from
    networked behaviour. They are particularly useful for mesh connections in a data
    lake. Apache Spark’s API GraphX is also used for graphs and graph-parallel computations.
    (Some use cases are social networks, knowledge graphs, etc.), Key-value stores
    store data as a key-value pair containing an attribute name (or “key”) and a value
    (e.g., for user profiling use cases). Some examples are Memcached as an in-memory
    key-value store used for read performance [92] and Redis server39 is an in-memory
    key-value store that can work as a cache, message broker, or database, Column-oriented
    databases store data as a set of columns. (for analytical use cases). Some examples
    are Cassandra [93] or Hbase [94], Document-oriented databases store data in formats
    such as JSON or XML documents. Some examples are MongoDB,40 Elasticsearch [61],
    Apache CouchDB. Data Lakes are designed to be much cheaper, easy to write, and
    store large amounts of data. However, it is difficult to access/read data from
    Data Lakes because the data may not be stored according to the analysis requirements
    and lacks consistency/isolation features. They also have complex system architectures
    due to multiple storage systems with different semantics. On the other hand, recent
    advances in ML libraries and data science ecosystem projects (e.g., TensorFlow)
    are starting to be integrated into data lakes after data preparation. C. Data
    Warehouses Data Warehouses have been widely used since the 1980s to prepare data
    for business analysis and decision making. They are specifically designed for
    SQL analysis and BI that require a well-defined schema, indexes, etc., for storage
    with strong management features (e.g., Atomicity, Consistency, Isolation, Durability
    (ACID) transaction support). Data Warehouses are basically Massively Parallel
    Processing (MPP) databases that can handle large amounts of data (usually structured).
    In a ETL workflow, data is taken from an operational data system or source such
    as a data lake, transformed, and placed into a data warehouse so that a materialized
    view of the data can be created for reports and BI. For analytics purposes, data
    warehouses can be used to run queries (usually written in SQL) over repositories
    of current and historical data to gain insights. Data warehouses are useful for
    long-used data that does not change or as repositories for more refined forms
    of data (enriched, aggregated, etc.). Some examples of data warehouse tools are
    Presto and Apache Hive, as well as Google BigQuery, Amazon Redshift and Snowflake
    for cloud native data warehouses and IBM, Oracle, Teradata for on-premise data
    warehouses. D. Available Tools The following are some examples of open source
    databases. Apache Pinot [85] can be used as a real-time distributed OLAP datastore
    and also provides fast OLAP queries on large datasets with low latency. Apache
    Hudi41 is a storage abstraction framework that helps enterprises build and manage
    petabyte-scale data lakes. Hudi enables features such as upserts and incremental
    pulls to absorb data changes and apply them at scale to Hudi Data Lakes. ClickHouse42
    is an open source OLAP column-oriented database, similar to Druid and Pinot, designed
    to aggregate as much information (on the order of several petabytes) as quickly
    as possible. TimescaleDB43 is an open-source relational database for time series
    data, intended for query-oriented workloads and based on PostgreSQL. Delta Lake44
    (from Databricks) is an open-source storage layer. It mainly provides ACID transactions
    for Data Lakes, Apache Spark and Big Data workloads/engines for interactive, batch,
    and streaming queries. Databricks has recently developed a new Lakehouse database
    paradigm that combines the benefits of Data Warehouses and Data Lakes into a single
    technology [95]. Iceberg45 (from Netflix), recently released by Netflix as open
    source, is a new table format for storing large, slow-moving tabular and analytical
    datasets. Alluxio46 provides a distributed storage system and distributed data
    orchestration across hybrid clouds. For object metadata management, Hive metastore
    takes care of mapping from SQL tables to files and directories in the storage
    component. The Hive metastore service (a binary API based on the Thrift protocol)
    is used to update metadata stored in Relational Database Management System (RDMS)
    such as MySQL, MariaDB or PostgreSQL. 1) Vendor Specific Tools Amazon offers S3
    for low-cost storage in Data Lakes, AWS Redshift for data warehousing and DynamoDB
    for database purposes, Amazon Neptune (as a graph database), AWS Redshift and
    Amazon Aurora (as a relational database), and AWS Glue for metadata management
    (similar to the Hive metastore service). GCP also offers several storage options,
    which are as follows: Cloud Storage (a service for storing objects, i.e., immutable
    data), Cloud Spanner (NewSQL database with unlimited scale, strong consistency
    and up to 99.999% availability), BigTable (a scalable NoSQL key-value database
    service for large analytic and operational workloads), BigQuery (serverless multi-cloud
    data warehouse), Cloud SQL (fully managed database service and can manage relational
    databases such as MySQL, PostgreSQL), and Cloud Datastore (NoSQL document database).
    Microsoft offers Azure Blob Storage, Data Lake and Cosmos DB. Other proprietary
    database solutions include MPP databases (Teradata, Vertica), SAP HANA, InfiniteGraph
    or Tableau. For metastores and their management, platforms such as DataHub, Alation
    and Collibra can be used. There are also two categories of solutions for Hadoop:
    Hadoop on-premise solutions are Cloudera Manager, Mapr, IBM InfoSphere and Hortonworks.
    On the other hand, due to the challenges of managing Hadoop systems on-premises,
    most organizations have also invested in Data Lakes in the cloud. Hadoop in-cloud
    solutions. There are several reasons for organizations to move to the cloud, including
    flexibility, efficiency (performance, durability and cost), security, consistency,
    easy access to AI platforms, reduced organizational and engineering overhead,
    etc. Some examples of Hadoop in-cloud solutions are Microsoft Azure HDInsight,
    Amazon EMR, Google Dataproc, SAP Cloud Platform, Qubole. E. Practical Aspects
    Depending on the technical requirements, there are different ways to select databases.
    In the enterprise database layer, there can be a combination of NoSQL, in-memory,
    or relational databases may be present to take advantage of each strength depending
    on the use case. For example, if the data is unstructured, Data Lakes may be chosen;
    if the data is structured and the workload is transactional, SQL databases (in
    the case of single-node systems), newSQL databases (in the case of horizontal
    scalability), or NoSQL databases can be selected. On the other hand, if the data
    is structured but the workload is data analysis solutions such as MongoDB, i.e.
    databases offering NoSQL services (in the case latency requirements in milliseconds)
    or one of the data warehouse solutions described above (in the case of latency
    requirements in seconds) can be chosen, depending on the latency requirements.
    There are also various data source formats used commonly by some Big Data processing
    systems and platforms to store data or exchange data. These include columnar data
    formats such as Parquet and ORC as well as other various data formats such as
    CSV, TXT, JSON, JDBS, Avro, binary files, etc. Apache Parquet is an open file
    format for columnar data that can be used in HDFS to store data along with its
    schema information and enable various I/O optimizations (e.g., compression), fast
    columnar analysis and aggregation. It is the default data source for many Big
    Data processing frameworks and platforms, including Apache Spark for analytics
    workloads. The open file format of Avro data storage is also used in Apache Spark
    and Apache Kafka when (de)serializing messages. The Avro file format is row-oriented
    (as opposed to Parquet). It is a framework for serializing data and can provide
    direct mapping to JSON, which improves the speed and efficiency of data processing.
    Data applications often access and use hot/warm storage databases (e.g., Redis)
    for real-time access, while cold event data is stored in lower-cost storage devices
    (e.g., cloud storage, HDFS). Depending on the storage characteristics, there are
    also different types of durability. In the persistent type, the data is not lost
    even if the cluster fails completely. In replicated type, the data is not lost
    even if a limited node in the cluster fails and in transient type, the data is
    lost in case of failures. In most data processing frameworks, a sharded and persistent
    database is required that can provide a control store database responsible for
    storing metadata such as task specifications, task dependencies, or critical system
    information to recover from failures (i.e., for fault tolerance purposes). When
    a node in the cluster fails and critical information is in danger of being lost,
    this datastore is used to regenerate the data by re-executing the tasks required
    for lineage-based fault tolerance. These specifications are typically stored in
    a control store database (e.g., ZooKeeper in the Hadoop ecosystem [96] or the
    global control store in Ray [78], a cluster of Redis databases). Finally, note
    that data storage can also be costly as data accumulated over years. Therefore,
    it is desirable to store more processed smart data than more large raw data, unless
    required by regulation. SECTION VI. Data Monitoring and Visualization Frameworks
    Monitoring and visualization of data plays an important role in the world of telecommunications.
    With proper monitoring and visualization, it is easy to uncover insights and patterns,
    understand relationships between observations, or describe trends or seasonality
    in telecommunication data. Data visualization is traditionally used for regular
    performance reporting to fully explain and present the results of data analysis
    or the data itself. It can also serve as an interface for users to run or compile
    analytics on data processing and analysis frameworks (e.g., queries against loaded
    data) and visualize the results. Many business decisions are made on dashboard-based
    pipelines through daily monitoring and interpretation of the data itself. Notification
    services and alerts, monitoring dashboards using tools like Grafana47 or Kibana
    [61], business intelligence dashboards (drill downs, top K results), ad hoc query
    clients/notebooks like Jupyter, heatmaps and user feedback can all be done during
    the data presentation or visualization stage. Some of the open source tools and
    frameworks available for data visualization are as follows. For exploring, visualizing
    and discovering data through dashboard visualization, Kibana from ELK stack [61]
    provides a free and open user interface. Grafana is used as an analytics and monitoring
    tool that allows to monitor infrastructure, applications, and metrics by connecting
    to databases. Kiali provides [97] dashboards, service mesh observability. Gephi48
    provides an open graph visualization platform. Dash49 is a production Python framework
    for building web applications and data visualization apps using Flask, Plotly.js,
    and React.js. Apache Superset50 is an open-source visualization tool developed
    by Airbnb that is used to visualize analytics results (with chart types such as
    word counts, heat maps, boxplots, etc.). Apache Superset can also be used for
    queries with Apache Druid. Apache Druid [59] serves as an analytics database,
    but can also be used as a dashboard for quick results on complicated analytics
    tasks. As a data application framework, Streamlit51 can be used to easily create
    ML and data science web applications. Metabase52 is another open source tool for
    business intelligence purposes. To monitor data pipelines, entire infrastructure,
    or cloud native applications, Prometheus [98], Datadog,53 Sentry54 provide monitoring
    services for servers, databases, tools, and cloud applications. One of the most
    popular cloud native monitoring tools is Prometheus. Prometheus can identify the
    applications to be monitored (either 3rd party or on-premise) through its service
    discovery feature, which can be scrapped through Exporters. The scraped data can
    be stored in local storage to be queried with PromQL or visualized with Grafana.
    Prometheus also allows sending alerts to various notification systems such as
    email, chat systems, etc. based on the configured alert rules. 1) VENDOR SPECIFIC
    TOOLS For reports and dashboard outputs, Tableau, Looker, and Mode; for embedded
    analytics outputs, Sisense; for advanced analytics outputs Thoughtspot, Outlier
    Analytics, Anodot, Sisu; for building ML and data science web application framework,
    Plotly Dash; for data visualization, Google’s Cloud Datalab,55 TIBCO Spotfire,
    MicroStrategy, Zepl, SAP’s Lumira, Microsoft’s Power BI; for running high-performance
    queries on petabytes of structured data to create powerful reports and dashboards
    Amazon Redshift and Vertica; and for monitoring applications and infrastructure
    in the cloud, Amazon’s CloudWatch, Google’s StackDriver, and Microsoft’s Azure
    Monitor can be used. SECTION VII. Data Orchestration and Management Frameworks
    The use and popularity of microservices and cloud/container orchestration frameworks
    is increasing due to various benefits such as service discovery or easy horizontal
    scaling. There is no sign that the ecosystem of data engineering and infrastructure
    is coalescing into a unified, manageable form. Over time, new distributed databases,
    frameworks, platforms, and libraries will be introduced into the ecosystem. Because
    of this, data management frameworks can bring everything together with suitable
    APIs as these systems evolve into more complex structures day by day. For example,
    stream processing jobs are more akin to microservices and thus require support
    for managing services and applications including cluster management, debugging
    and continuous monitoring. Note that approaches such as centralized management
    approaches (e.g., scheduling) can create a bottleneck in the system that can only
    provide a finite amount of throughput (i.e., in terms of processing capabilities
    of tasks per second). Therefore, the importance of automated and distributed resource
    management, scheduling, and orchestration is steadily increasing in the modern
    data ecosystem. The main goal is to start or manage computational resources, services
    or containers with a single or a set of API calls to reduce operational costs
    and complexity. In data applications, all computations consume and produce data
    which must be orchestrated in a data-aware manner. Several open-source distributed
    frameworks are available for distributed orchestration, seamless distributed execution,
    workflow definition and execution, resource management, or resource-aware scheduling
    of clusters, data centers, or cloud environments. These frameworks are mainly
    used with applications (e.g., Kafka, Hadoop, Elasticsearch, Spark, etc.) and provide
    APIs for resource management, orchestration, elasticity, high availability (or
    zero downtime), fault tolerance, and scheduling. Traditional CRON jobs and other
    similar solutions such as configuration files are loosely coupled and do not allow
    users to formally define dependencies. Recently developed workflow engines, on
    the other hand, allow dependency graphs, proper execution order, consolidated
    logs, expression of tasks using operators, and use data pipelines/workflows as
    a code paradigm. A. Scaling The number of data sources that an organization can
    ingest and process is growing much faster than the number of resources for data
    engineering. In addition, the variance of data traffic in production systems can
    be unexpectedly high at certain times. For these reasons, the system should be
    scalable without compromising throughput and latency. Scalability is the ability
    of the system to provide a moderate performance as the load increases (e.g., high
    volume, high throughput or high velocity data). Scalability can refer to different
    dimensions: Processing, Serving or Storage. A scalable architecture means that
    the system should continue to function smoothly even if 1 user or 1 million users
    access the application. For systems, there are two ways to scale a database: Vertical
    scaling (or Scaling up): is done by improving the Central Processing Unit (CPU),
    Random Access Memory (RAM), and storage capacity of the existing machine in hardware
    or by optimizing algorithms and application code in software. This eventually
    reaches its limits when the amount of data on a single machine grows. Horizontal
    scaling (or Scaling out): is done by adding additional machines to the database
    cluster. Note that requests in this case not only consume CPU, but also require
    network resources. In this case, it is important to shard the data so that a single
    query can be processed on a single machine. Due to difficulties in horizontal
    scaling with traditional SQL databases, NoSQL and NewSQL databases with horizontal
    scaling options have emerged. However, they lack strong consistency guarantees
    and relational models (see Section V for details). The complexity of the scaling
    process depends on the type of service that infrastructure provides. There are
    two types of services: In stateless services, scaling is usually done based on
    user traffic and on-demand, e.g., web server applications. Deploying and scaling
    stateless services is relatively straightforward. In stateful services, consistency
    of user data across data centers is critical for scaling, e.g., for scaling databases.
    Deploying and scaling stateful services is more difficult and complex because
    copies of the database need to be managed in different data centers. A possible
    solution to reduce complexity would be to partition the database and enable different
    replication factors in different locations so that the overall replication factor
    can be reduced. This would also help reduce traffic between data centers. B. Parallelism
    To increase processing capacity, most data engineering pipeline frameworks exploit
    the potential of parallelism. Parallelism can be either over data or task selected
    according to the application. In data parallelism, input data is partitioned to
    scale processing (e.g., in low-cost HDFS). In task parallelism, multiple tasks
    (CPUs) are executed over the same data. In event processing, there are various
    forms of shuffling and data exchange capabilities within parallel computing frameworks.
    These mainly fall into the following categories: Forward or one-to-one: In this
    case, the data is processed with the same node and there is no data exchange between
    the nodes of the cluster. Broadcast: In this mode, data is broadcast to all nodes
    because it must be used by the tasks running on each node. HashKey: In this case,
    the data is collected/grouped by key to elegantly distribute the data among the
    work tasks of the nodes in a cluster. Rebalance or random: In this case, random
    repartitioning is used when not much is known about the data. The goal is to balance
    the distributions between the nodes to increase the data processing capabilities
    of the nodes. C. Microservices and Deployment Microservices and microservices-based
    architecture are some of the recent trends that enable flexibility and scalability.
    Data orchestration and management teams are adapting them to run and manage distributed
    application components. Service mesh architecture enables microservices to communicate
    with each other. Typical types of communication between microservices can be divided
    into three dimensions: Event-driven based on platforms such as the message/event
    bus, Orchestration based on REST callouts using some frameworks such as Apache
    Camel, Orchestration based on some workflow engines like Apache AirFlow.56 In
    microservices architecture, different deployment patterns are possible [99]: All-at-once
    deployment: Making changes on top of existing configurations, terminating the
    old version and releasing the new version. So, the deployment is immediate. This
    pattern is better suited for workloads with low concurrency. Blue/Green deployment:
    Allows traffic to be moved to a new live environment (green) while the old production
    environment (blue) is still kept warm. Later, the transition to the new version
    can take place. This pattern is better suited for workloads with medium concurrency.
    Canary/Linear Deployment: Deploying a small number of requests for a new software
    version to analyze the impact on a small number or subset of users. Then, full
    rollout can be done. This pattern is better suited for workloads with high concurrency.
    Kubernetes also has similar rollout deployment strategies such as Recreate, Ramped,
    Blue/Green, Canary, A/B testing and Shadow.57 D. CI/CD and IaC Continuous Integration
    (CI)/Continuous Development (CD) are terms that have been common in the industry
    for a decade. Platform engineers and DevOps teams are continuously developing
    infrastructure tools to improve engineer productivity. At the same time, data
    applications should also be flexible enough to be deployed by CI/CD platforms
    for testing and development purposes. For data engineering applications, integration
    can be done in three modes: Data integration: This is related to data movement
    patterns. It can take place either at the node level or at the cloud level, and
    ETL is an example of this. Data integration is challenging as it has to deal with
    heterogeneous data sources with different sampling rates and data generation models.
    Application integration: uses APIs such as REST, SOAP, etc. between applications
    and their internal subscriptions. In application integration, the amount of data
    exchange is not as large as data integration. Event integration: combines the
    benefits of application and data integration, where a significant amount of data
    is also exchanged when events are triggered between applications. Infrastructure
    as a code (IaC) enables scalable infrastructure deployment via standardized interfaces
    such as YAML or JSON files. IaC can be implemented via containers (e.g. via Docker,
    LXC [100]), container orchestration (e.g. via Kubernetes [101], Docker Swarm [69],
    Apache Mesos [102]) and infrastructure provisioning (e.g., via Terraform). Two
    different models for IaC or automation are: Declarative model requires the user
    to define a desired state to be provisioned by Kubernetes. The relationship between
    the infrastructure and the application is declarative. Applications make declarative
    requests to the infrastructure (e.g., via YAML, a data serialization language
    and easily understood by humans and machines) with implementation details abstracted
    by the underlying framework (e.g., the Kubernetes cluster). One of the main advantages
    of this model is that once the plan is created, it is the responsibility of the
    framework to develop and execute the work plan in a way that is optimized for
    the complexity of the infrastructure. This ensures the transition from the infrastructure
    as code paradigm to the infrastructure as data paradigm. Any new application can
    be easily enriched using the declarative tag of the declarative YAML management
    file. Imperative model requires users to specify commands or plans in a specific
    order to achieve and maintain a desired state, such as the service provided by
    Apache AirFlow. Although this model provides flexibility, imperative models have
    scaling issues as the number of components increases and complexity grows exponentially.
    E. Available Tools For highly reliable distributed coordination of multiple machines
    in a cluster, distributed key-value stores such as Apache Zookeeper [96], etcd,58
    Consul59 provide reliable data stores for distributed system access. They can
    perform functions such as leader elections, fault tolerance for machine failures,
    network configuration automation, and service discovery. To ensure strong consistency
    and replication, consensus algorithms and protocols such as Raft consensus algorithms
    and Paxos protocols are used to determine the order in which data is stored and
    when it becomes visible to users. Data management tools such as Deequ60 are particularly
    useful for dealing with corrupt or bad data in large datasets. MLflow [103], is
    an open source ML lifecycle platform, is used to manage the E2E ML lifecycle so
    that model-based experiments and quality metrics can be managed, tracked, and
    reproduced. Kubeflow [104] provides a data automation framework for Kubernetes
    clusters and enables connectivity with a variety of databases and services. This
    enables a highly modular design. Weights & Biases61 is another related developer
    tool for ML. These ML lifecycle management frameworks act as CI/CD tools in the
    ML domain. Some of the most popular systems for defining workflows and programmatically
    scheduling jobs/tasks are Apache Mesos [102] and Apache YARN [43] (for running
    the cluster and monitoring executed jobs, e.g. Cloudera/Hortonworks run Spark
    jobs using YARN), Apache AirFlow (provides workflow-level abstraction for building
    data pipelines using Directed Acyclic Graphs (DAGs)), Dagster62 (for data orchestration),
    Prefect63 for automating data flows and creating, executing and monitoring millions
    of data workflows and pipelines, Spotify’s Luigi,64 LinkedIn’s open source scheduler
    Azkaban,65 and Apache Oozie [105] (for distributed coordination and scheduling
    of workflows in Hadoop clusters), Apache TEZ [106] (for building complex directed
    acyclic graphs of tasks to process data), Apache Ambari67 (provides a management
    interface), Kubernetes [101] (provides high flexibility as the dominant container
    orchestration framework), OpenShift68 (provides a web console to run tasks directly),
    Yunikorn69 (resource scheduler for containerized systems), and Docker Swarm. Apache
    Calcite [107] is a dynamic data management framework and is used to as an intermediary
    between applications and data stores and data processing engines. Amundsen69 (from
    Lyft), Metacat70 (from Netflix), DataHub71 (from Linkedin) provide both metadata
    management and data discovery options to improve the productivity of data professionals
    interacting with data. Some of these tools (such as Kubernetes) are also important
    tools for managing and deploying containers (e.g., container lifecycle and resources,
    or facilitating application development through container orchestration) in the
    context of network operations and management in 5G networks. Some common IaC and
    automation tools that can help eliminate the risk of human error, increase the
    speed of code development speed (by reliably building, testing, and deploying
    software), and reduce costs are Ansible,72 Chef73 and Puppet74 for configuration
    management, Terraform75 (infrastructure deployment orchestration tool) and Jenkins,76
    GitHub Actions77 as part of the CI/CD chain. These tools have significantly improved
    the way applications and workflows are deployed and managed within the infrastructure
    through configuration (installing packages, configuring servers, and deploying
    applications to the infrastructure) and orchestration of the infrastructure. Great
    expectations provides data quality, documentation, profiling and testing.78 Finally,
    the literature also provides an excellent curated list of data pipeline frameworks,
    libraries79 and workflow engines.80 1) Vendor-Specific Tools Amazon offers AWS
    Elastic Beanstalk, Amazon ECS (as a container orchestration framework), and AWS
    CloudFormation for infrastructure provisioning and IaC tool, CodeDeploy for deployment,
    CodePipeline for unit and integration testing, CI/CD pipeline, Microsoft provides
    Azure Resource Manager (ARM) and Pipelines for the deployment and management service
    and CI/CD Pipeline over Azure, TestPlans for unit and integration testing, Google
    provides Google Composer as a managed Apache Airflow service on GCP and Google
    Kubernetes Engine (GKE),81 Deployment Manager for infrastructure automation, Cloud
    Build for deployment, unit and integration testing, CI/CD pipeline, Puppet Enterprise
    also offers Puppet as configuration management, HashiCorp offers Terraform for
    deploying and managing any cloud, infrastructure or service. Pulumi provides a
    modern Infrastructure as Code for building, deploying, and managing infrastructures
    in any cloud. For visibility and observability of data pipelines, Unravel, Fiddler
    and Acceldata can be used. For data modeling and analytical engineering workflow,
    dbt (Data Build Tool) and LookML can also be used to transform data into data
    warehouses more efficiently. Considering all the above descriptions, starting
    from data connection to data orchestration, Table VCharacteristics of open-source
    frameworks in data engineering for networking and corresponding related workstable.5
    shows the summary of characteristics of open source frameworks in the data engineering
    pipeline and the corresponding related works. In addition, some of the most important
    tools and their connection to the components of the data engineering pipeline
    framework are described in Fig. 2. FIGURE 2. Some key data engineering tools and
    their connection to the components of the data engineering framework. Show All
    SECTION VIII. Relation With Data Science Frameworks With the advent of DL and
    new computational workloads, scaling and distributed computing are becoming increasingly
    important in AI/ML. Data Engineering also helps data science developers build
    distributed computing frameworks so that they can focus on developing their own
    AI/ML algorithms instead of dealing with the intricacies of distributed computing.
    On the other hand, AI/ML based platforms build on Data Science libraries play
    an important role. For example, in a processing pipeline, data scientists work
    on defining and preparing the model, and data engineers implement the aspects
    that serve the model. For this reason, the ability to interact with AI/ML models
    (export, import, etc.) from data science tools is important. In the ecosystem,
    there are several open source platforms for AI/ML. Model training and online prediction,
    inference layers can be done through these special projects developed for a number
    of different important workloads. A. Data Science Frameworks Some of these related
    Data Science projects and tools can be summarized as follows: For building Machine
    Learning applications, Spark MLlib (Apache Spark’s scalable machine learning library)
    [110], statistical modeling libraries (scikit-learn (machine learning in Python)
    [111] and statsmodel84), Apache MADlib (Big Data Machine Learning in SQL) [112],
    Aerosolve85 (a machine learning package designed for humans), Mahout86 for developing
    scalable, performant ML applications and Knime87 as an open-source platform for
    data analytics. To build and train models with DL, libraries and frameworks such
    as TensorFlow (an E2E open source machine learning platform) [113], Deeplearning4j
    (DL for Java), Torch (an open-source machine learning library based on C) [114],
    PyTorch (an open-source machine learning framework that accelerates the path from
    research prototyping to production deployment) [109], Chainer [115], a Python-based
    DL framework that aims for flexibility and intuitiveness over neural networks,
    Sonnet88 a library built on TensorFlow 2.0 to provide simple, composable abstractions
    for ML research, BigDL [77], a distributed DL framework for Apache Spark, Apache
    Singa [116], which focuses on distributed training of DL and machine learning
    models, Apache MXNet [117], an open-source DL framework suitable for flexible
    prototyping in research and production, Deeplearning.scala,89 for building complex
    neural networks, Sparkflow,90 an implementation of TensorFlow on Spark, Theano
    [118], Caffe [119], Keras [120], PaddlePaddle,91 ONNX92 to support DL model creation
    and deployment, Microsoft’s Cognitive Toolkit (CNTK)93 for distributed DL and
    more recently Ludwig,94 JAX95 and Trax96 are used. For developing Reinforcement
    Learning applications, Ray provides (a fast and simple framework) RLlib for building
    and running distributed, parallel, scalable reinforcement learning-based applications
    [121], Stable Baselines97 to produce a set of improved implementations of reinforcement
    learning algorithms based on OpenAI Baselines, Garage98 to develop and evaluate
    reinforcement learning algorithms, Coach,99 a Python-based reinforcement learning
    framework that includes implementations of many state-of-the-art algorithms, Tensorforce,100
    a TensorFlow library for applied reinforcement learning, ChainerRL, a deep reinforcement
    learning library that includes several state-of-the-art deep reinforcement algorithms
    in Python with Chainer [115] (a flexible DL framework), OpenAI [122]’s Gym, Retro
    and Neural MMO frameworks (environments for training agents with reinforcement
    learning, e.g. DotA as an application), Unity’s ML agents, Microsoft’s Project
    Malmo and DeepMind’s Lab and Control Suit (e.g., 3D learning environment, RLax
    library,101 AlphaGo as an application) are also developing their own new distributed
    reinforcement learning algorithms to be implemented or building their own infrastructure/tools
    for their applications to achieve the required flexibility and performance. For
    Distributed Training frameworks such as Horovod [123] along with TensorFlow, Keras,
    PyTorch, and Apache MXNet, Distributed TensorFlow, for Model Serving (takes the
    trained model and sends predictions or recommendations to specific applications)
    tools/frameworks such as Clipper [124], which is used for low-latency prediction
    serving systems for ML when integrated with client systems, TensorFlow Serving,
    TorchServe,102 Ray Serve, or Seldon103 can be used depending on use case that
    requires low-latency model deployment for large-scale prediction services. For
    Hyperparameter Search (either via manual search, grid search, Bayesian optimization,
    evolutionary optimization or random search), Advisor104 (which is an open source
    implementation of Google’s Vizier) is used for the hyperparameter tuning system
    for black-box optimization, Hyperopt [125] and Tune105 are used for distributed
    hyperparameter optimization and scalable hyperparameter tuning, respectively.
    For NLP, tools such as spaCy [126], Hugging Face [127], AllenNLP [128] and more
    recently GPT-3 [129] can be used. 1) Vendor-Specific Tools Proprietary software
    such as SAS, Datatron, ModelOp, etc. can also be integrated with various products
    and services for ML and model serving purposes. Some companies like Cloudera,
    Databricks, Dataiku, Domino Data Lab, etc. have also been offered data science
    workbenches/platforms as ML service. B. Machine Learning Platforms in Industrial
    Environments Several startups and cloud companies offer E2E ML tools and platforms,
    including IT and cloud giants Google, Amazon and Microsoft. The following is an
    overview of ML platforms used in industrial environments: Uber uses Michelangelo
    [75] as its internal ML-as-a-service platform. It consists of open-source components
    such as HDFS, Kafka, Spark, Samza, Cassandra with libraries such as MLLib, XGBoost
    and Tensorflow. Airbnb uses Bighead, a combination of Zipline data management
    tool, the containerized Jupyter notebook service Redspot, and the Bighead library
    for data pipeline abstractions, transformations, and data track lineage. Netflix
    uses Metaflow,106 a Python-based framework built on AWS, to handle model training
    and data management by running DAGs on an AWS Serverless Orchestration platform.
    Lyft has open-sourced its cloud native platform called Flyte,107 that can invoke
    machine learning and operations together, termed as ML operations (MLOps). Amazon
    offers SageMaker as a complete solution for ML with the latest libraries such
    as TensorFlow, Keras, PyTorch, and MXNet and model deployment options in the cloud
    or at the Edge. Microsoft uses Azure Machine Learning Studio which supports a
    full range of frameworks including TensorFlow, Keras, PyTorch, MXNet scikit-learn,
    and XGBoost. IBM uses Watson ML to support various frameworks on both CPUs and
    GPUs in collaboration with its own products. Databricks uses MLflow [103], an
    open source platform for managing the lifecycle of ML which includes four components:
    MLflow Tracking (for recording and querying experiments), Projects (for packaging
    code and running it on any platform), Models (for deploying ML models across environments)
    and Model Registry (for discovering, storing, annotating, and managing models
    through a central repository). Intel Analytics Zoo,108 which provides an E2E analytics
    and AI platform (high-level pipeline APIs, integrated DL models, etc.), is available
    as open source. Apple provides Overton [130] to build, monitor and improve ML
    systems in production environments. Facebook uses FBLearner [131] as a ML platform
    to automate tasks such as training on clusters and developing custom ML code.
    Google offers the Cloud AI Platform109 to develop AI applications and run them
    both on GCP and on-premises. SECTION IX. Network Service Management and Orchestration
    Overview Thanks to advances in network services, new applications such as the
    tactile internet, holographic-type communications and teledriving are expected
    to emerge in the next decade. Many of these E2E services also require high levels
    of precision which has significant implications for the management of these networks
    and services. Managing and maintaining distributed computing functions in a network
    environment with elevated levels of service requirements requires hundreds of
    operations at any given time. This makes the human-centric and standard network
    service management and operation solutions already used inadequate and ineffective.
    Therefore, automation of network management and service deployment is critical
    and there is a need for unified network Lifecycle Management (LCM) and orchestration
    across multiple administrative and technological domains. Network service orchestration
    enables network operators to connect and configure systems and multiple network
    elements through a optimized workflow. This enables the delivery of optimal services
    to users and contributes to automation by coordinating interactions and service
    flows across multi-domain, multi-layer, multi-vendor networks. Approaches that
    rely on intelligent automation of network operations (e.g., via the emerging field
    of Zero Touch network and service management (ZSM) [132]) can make a big difference
    when multiple network functions need to be owned, maintained, and operated at
    scale throughout the network. A. Different Aspects of Network Service LCM Network
    service LCM addresses the necessary operations to create, deliver, manage and
    orchestrate network services to meet the diverse needs of end users and enterprises
    over network infrastructures. The goal is to guarantee autonomic network service
    assurance and dynamic service delivery. Some of these operations include network
    functions deployment, provisioning, onboarding, updating on the fly, storing,
    ensuring zero downtime, demand-based scaling in an intelligent and cost-efficient
    manner, supplying suitable infrastructure resource orchestration, anomaly detection
    at run time. These operations are achieved via software-based/virtualized network
    functions or cloud native microservices deployed across fog/edge/cloud infrastructure.
    The ETSI-defined cross-domain E2E network service LCM is divided into three main
    processes (see [133] and references therein): Service on-boarding procedure is
    used to add new service model to the E2E service management catalogue. Some examples
    of service management include E2E service orchestration (to control the service
    model and maintain the service catalogue), domain orchestration (to send alerts
    when changes are made to the catalogue and to request missing entries in the catalogue),
    ZSM integration fabric (to manage subscriptions, data generation and consumption)
    and ZSM data services (to store data and provide data persistence). Service fulfilment
    procedure is used to manage E2E service instances from instantiation to termination.
    The following processes are provided for the provisioning of E2E service instances.
    (i) Service instantiation (creates an E2E service instance), (ii) Service activation
    (activates an E2E service instance), (iii) Service configuration (modifies the
    configuration of an E2E service instance), (iv) Service deactivation (deactivates
    an E2E service instance), (v) Service decommissioning (removes an E2E service
    instance and releases all its resources), (vi) Update E2E inventory (keeps up-to-date
    information about resources and domain service instances). Service assurance procedure
    is used to ensure that E2E service level requirements are met. The following processes
    are provided to deliver E2E service assurance. (i) Service assurance set-up (assures
    an E2E service), (ii) Service quality management (manages service quality), (iii)
    Service problem management (investigates cross domain service problems), (iv)
    Service assurance tear-down (defines procedures to tear down the collection of
    information). B. Management Platforms In this section, we describe two of the
    most popular management and orchestration platforms. 1) ONAP Open Network Automation
    Platform (ONAP) project leverages SDN and NFV technologies to improve network
    service deployments and provisioning.110 It is designed to provide a unified framework
    for monitoring solutions to observe and verify E2E SLAs and Key Performance Indicators
    (KPIs). ONAP provides a scalable and distributed approach to managing multi-site
    and multi-Virtualized Infrastructure Manager (VIM) resources. This is achieved
    through the components of the so-called Data Collection, Analytics and Events
    (DCAE) module, which can be geographically distributed across multiples sites
    and hierarchically inter-connected. Data monitoring at different levels is usually
    done with open-source software such as Prometheus, which can distribute its functionalities
    across geographically separated sites. Synchronization messages and local processing
    results are exchanged between different sites using a submodule called Data Movement
    as a Platform (DMaaP). This submodule supports both file-based and message-based
    data exchange via the publish & subscribe paradigm. 2) Open Source MANO Open Source
    MANO (OSM) is a collaborative open source project to develop an NFV Management
    and Orchestration (MANO) stack that is conforms to the European Telecommunications
    Standards Institute (ETSI) NFV Information Models and APIs.111 The focus is on
    the Network Service Orchestrator (NSO) part of ETSI MANO NFV Orchestrator (NFVO).
    Network slice support, LCM of Network Slice Instances (NSIs), monitoring capabilities
    including Virtual Network Function (VNF) metrics collection are some features
    of recent OSM releases. For deployment and management, OSM can operate in two
    modes, namely Full E2E Management (Integrated Modelling) and Standalone Management(Vanilla
    NFV/3GPP) [134]. SECTION X. Network Management and Orchestration in Standardization
    In recent years, several standards organizations, independent alliances, and forums
    have been involved in developing standards for developing platforms that work
    with AI and ML. In addition, zero-touch network management and orchestration frameworks,
    which are based on significantly simplifying the tasks performed by human to manage
    and orchestrate network slices, are currently being extensively researched by
    standardisation bodies. Standards organizations such as Open Radio Access Network
    (O-RAN), ETSI, The 3rd Generation Partnership Project (3GPP) are working on embedding
    intelligence into emerging next generation architectures to efficiently meet the
    diverse needs of communication network users. In this subsection, we provide an
    overview of some of the work that has been done in these SDOs to build a AI/ML
    platform. A good overview of existing standardization efforts related to AI for
    5G systems as well as some of the identified gaps in standardization, are also
    summarized in [135]. A. O-RAN’S AI/ML Architecture O-RAN alliance aims to define
    a next-generation radio access network (RAN) infrastructure based on software-defined
    technology and general-purpose hardware, driven by both intelligence and openness
    at every layer of the RAN architecture [136]. O-RAN currently offers an attractive
    solution for creating next-generation multivendor networks that embrace the concepts
    of programmable, open, collaborative, and intelligent communications. Therefore,
    AI and ML are the main pillars for the realization of O-RAN. O-RAN high-level
    architecture can be divided into two layers, namely Service, Management and Orchestration
    (SMO) and radio access site as shown in Fig. 3 [137]. In the radio access entities,
    there are RAN intelligent controllers (RICs) (near real-time (near-RT), non real-time
    (non-RT)), (the vertically divided control (CP) and user (UP) planes of the central
    units (CU)) as well as open interfaces that interconnect the O-RAN nodes. Both
    near-RT and non-RT controllers are introduced to extend the existing network functions
    with more embedded intelligence within the O-RAN architecture. Near-RT RIC is
    interfaced with a centralized unit control plane (CU-CP) for transmission of signals
    and configuration messages and centralized unit user plane (CU-UP) for data transmission
    and can be used for control loops on the order of ms time scale. Non-RT RIC is
    interfaced with near-RT RIC via interface A1 (for policy management and coordination)
    and to the CU-CP, the Distributed Unit (DU) and the Radio Unit (RU) via interface
    O1 and can be used for control loops on the time-scale in the order of greater
    than 500 ms [138]. In addition, applications (xApps/rApps) are introduced to be
    hosted either on the near-RT RIC or on the non-RT RIC depending on how sensitive
    the applications are to control processes. FIGURE 3. A high-level illustration
    of the O-RAN architecture with RAN intelligent controllers (near real-time and
    non real-time), control and distributed units, and mapping with data engineering
    pipeline components. Show All In the architecture, different interfaces (O1, A1
    and E2) are used depending on the results of the AI/ML algorithms, the actions
    and the actors. For example, the O1 interface is used to configure Control and
    Data Units and near-RT RIC for fault and performance management. A1 interface
    enables non-RT RIC to provide RAN optimization functionalities to near-RT RIC
    functions. These functionalities include policy management, ML model management,
    or data enrichment. The E2 interface is used for communication between near-RT
    RIC and centralized/distributed units of RAN in the O-RAN architecture. AI/ML
    algorithms can run on top of near-RT RIC (e.g., xApps for energy, resource or
    beamformer optimization, traffic steering) or the non-RT RICs (e.g., rApps for
    RAN automation applications such as network deployment, optimization, frequency
    band selector). These algorithms can also be reconfigured based on data availability,
    control timescales, network load, and overall mobile operator requirements. Data
    pipeline generation via O-RAN architecture: To enable automated and intelligent
    network functions, O-RAN architecture has been standardized to include three types
    of control loops (categorized by the time sensitivity of the required decision-making
    process) and AI/ML dedicated nodes. The first control loop is responsible for
    scheduling at the Transmission Time Interval (TTI) level and operates on a time
    scale of TTI ms or above. The second control loop operates in the near-RT TIC
    and operates within the range between 10–500 ms and above. The third control loop
    operates in the non-RT RIC and makes decisions at a time greater than 500 ms (e.g.,
    for policy and orchestration purposes). These control loops can also operate in
    parallel. Offline/online training and inference can be supported via O-RAN components
    such as SMO, non-RT and near-RT RICs. In line with recent developments in data
    engineering, a mapping can be made between the components of O-RAN and the existing
    data engineering ecosystem. The O1 interface is used for data collectors and preprocessing
    entities within the service and management orchestrator (e.g., within the Open
    Network Automation Platform (ONAP) [139]). The O1 interface can be used to transmit
    RAN metrics associated with the performance of the RAN nodes to the SMO. In addition,
    non-RT RIC placed in the SMO can enable RAN optimizations using the collected
    RAN metrics and contextual (external) data. The SMO can later control the RAN
    and apply configuration changes. Fine-grained data collection is performed via
    the E2 interface, to enable near-RT control and optimization of RAN elements.
    Data collectors and preprocessing entities may use the previously defined data
    connection frameworks described in Section II. The Virtual Event Streaming (VES)
    collector in O-RAN112 is used as a telemetry collection interface and supports
    data gathering from O-RAN. Subsequently, the collected data can be shared with
    non-RT RIC using the data ingestion frameworks described in Section III. AI/ML
    models trained in AI/ML platforms can later be queried by non-RT RIC via batch
    processing or Big Data query engines. Regular AI/ML workflow including model training,
    inference and updates, batch data processing, big data query generation processes
    and policy-based guidance of applications/features can be performed using the
    data analysis and processing frameworks defined in Section IV. Note that in the
    O-RAN architecture AI/ML model training can be hosted in the non-RT RIC, while
    ML model inference can be located either in the non-RT RIC or in the near-RT RIC
    when supervised/unsupervised ML/DL approaches are used. If reinforcement algorithms
    are used, ML training and inference host can be colocated either in the non-RT
    RIC or in the near-RT RIC. The subjects of the action are near-RT RIC, CU, DU
    and RU units. The goal of this analysis can be predictions, alarms or suggested
    actions in the event of unknown network conditions (e.g., SLA violation prediction,
    predictive maintenance of a RAN instance, energy optimization for coverage maximization.)
    When near-RT is subject to an action, the ML inference results or policies/intents
    can be transferred via the A1 interface to near-RT RIC, where near-RT RIC can
    apply streaming, real-time, or interactive analytics to the dataset. Appropriate
    configurations can then be applied to control or data units via the E2 interface
    which establishes communication between the lower RAN modules (RU, CU and DU)
    and the near-RT RIC and is used for time-sensitive control of the RAN components.
    For performance monitoring, performance data from the ML models deployed either
    in near-RT or non-RR RICs can be linked to the data visualization and monitoring
    frameworks. By monitoring these relevant performance metrics, decisions can be
    made such as whether or not a model re-training is required. These update decisions
    can be triggered by either a rule-based policy (e.g., threshold-based) or by using
    trend analysis approaches. B. ETSI’S AI/ML Architecture ETSI has several specification
    groups working on embedding intelligence into network services and management
    infrastructures. ETSI’s Industry Specification Group (ISG) ZSM aims to develop
    a new horizontal and vertical E2E architectural framework designed for closed-loop
    100% automation and optimized for data-driven AI/ML algorithms [132]. An extensive
    list of requirements for zero-touch management that contains more than 170 topics
    on autonomic management is already defined by the ETSI ZSM group. [140]. In ETSI’s
    ISG NFV, the AI/ML platform will eventually be considered as part of the MANO
    stack. The ETSI Technical Committee (TC) Core Network and Interoperability Testing
    (INT) is investigating the Generic Autonomic Network Architecture (GANA) for the
    purposes of autonomic networking [141]. Similarly, the ETSI Experiential Networked
    Intelligence (ENI) group is working on the application of AI in telecommunication
    networks to help operators manage infrastructure and provide more resilient services
    offered to end users and has also recent published standards [142]. Experiential
    learning is learning through experience. The ETSI ENI architecture uses AI techniques
    and policies driven by context awareness and metadata so that the services provided
    can be adapted to environmental conditions, user needs and business goals. The
    main goal is to develop a control loop based on the “observe-orient-decide-act”
    model. Fig. 4 shows an overview of the ENI architecture and reference points and
    their corresponding mappings with the presented data engineering components. API
    broker in the middle is an optional functional block and acts as a gateway (i.e.
    translator) and maps the data connection framework of the data engineering pipeline.
    Within the ENI system, there are several function blocks that mainly represent
    the management and application components that are connected to the semantic bus.
    These functional blocks can be implemented as part of the data analysis and processing
    and data storage framework of the data engineering pipeline. For example, situation
    awareness blocks are used to detect events and behaviors in the ENI system and
    its environment. In the case of high traffic and large amounts of information,
    the corresponding streaming application used in the data analysis and processing
    framework is an important differentiating factor. The policy management functional
    block allows users to create and edit policies so that consistent and scalable
    decisions can be made about the system behaviour. The model-driven engineering
    block uses a set of domain models that abstract all concepts related to managing
    objects in the ENI system. Therefore, both the policy management functional block
    and the model-driven engineering block can be mapped to the data storage framework
    in the data engineering pipeline. A comprehensive overview of ETSI ENI on using
    AI techniques for network management and orchestration can also be found in the
    references [143], [144]. FIGURE 4. Overview of the ETSI ENI reference points and
    the architecture and mapping to the components of the data engineering pipeline.
    Show All C. ITU Machine Learning Pipeline International Telecommunication Union
    (ITU)’s FG-ML5G group is working on ML pipeline [145]. Fig. 5 shows an example
    realization of the high level architecture in an IMT-2020 network and the corresponding
    mapping. In this pipeline of Fig. 5, there are several nodes for creating ML pipelines:
    source (represented by SRC) is a node that generates data to be used as input
    to the ML function. collector (represented by C) is a node responsible for collecting
    data from SRC. pre-processor (represented by PP) is a node used for pre-processing
    the data model (represented by M) is a ML model used for prediction (note that
    training of the model is performed in a sandbox (not shown in Fig. 5)). policy
    (represented by P) represents the control mechanism for improving the operation
    distributor (represented by D) is responsible for distributing the ML results
    to the corresponding sinks sink (represented by C) is the target node of the ML
    output where the action is performed (for inference purposes). FIGURE 5. High-level
    architecture in an IMT-2020 network and mapping to data engineering pipeline components.
    Show All Note that in Fig. 5, some subsets of nodes (e.g., PP, M, P, D) are inside
    the ML pipeline and are not shown. In Fig. 5, latency-sensitive applications use
    “ML pipeline 1”, while latency-tolerant applications use “ML pipeline 2”. Inputs
    from UE are processed by the ML pipeline represented by arrows 1->2->4-> “ML pipeline
    2”, to make predictions for the Core Network (CN) (e.g., MPP-based ML applications).
    In the ML pipeline arrows represented by 5->4-> “ML pipeline 2”->6, the inputs
    of CN (as well as some combinations of UE inputs) are combined to make some predictions
    in the CN so that these actions can be performed by management functions (e.g.,
    actions such as Self Organizing Network (SON)-level decisions at CN or closed-loop
    decisions about resource allocations in the network). In the ML pipeline arrows
    represented by 1->3-> “ML pipeline 1”->7, the inputs of UE are used for latency
    sensitive applications in the access network, where model hosting and serving
    are also performed. When mapped to the corresponding data engineering pipeline
    frameworks, the connection of SRC to collector (represented by C) is via the data
    connection module, where collector can be selected from data ingestion framework
    and ML pipeline 1 & 2 falls into the category of data analysis and processing
    frameworks. D. 3GPP Network Data Analytics Function The architecture framework
    for 5G management and orchestration is specified in 3GPP. TS 29.520 in R-16 is
    the standardization effort of 3GPP for 5G network automation using ML and data
    analytics. Within the latest approved 5G specification in 3GPP (Release 15), 3GPP
    identifies two main building blocks responsible for data analytics [146]: NWDAF
    (Network Data Analytics Function) collects data from core network functions and
    provides network data analytics services to other Network Functions (NFs) of the
    5G Core which are subscribed as NWDAF consumers [147]. The NWDAF offers two services
    (called Nnwdaf services). The first is called Nnwdaf_EventsSubscription service,
    which allows NF service consumers such as PCF, OAM to subscribe or unsubscribe
    to various analytics events provided by the NWDAF. The second is called Nnwdaf_AnalyticsInfo
    service, which is used by NF consumers to request and receive specific analytics
    from the NWDAF. Hence, through a service-oriented interface N nwdaf , other NFs
    can access analytics information. In 3GPP Rel. 15, Policy Control Function (PCF)
    and Network Slice Selection Function (NSSF) are envisioned as possible consumers
    of network data analysis. For instance, PCF may use this data to adjust QoS parameters,
    or NSSF can use the slice-level load data for slice selection. Some relevant use
    cases defined in 3GPP TR 23.791 are related to service experience prediction,
    load analysis, UE behaviour and pattern prediction, etc. MDAF (Management Data
    Analytics Function) is responsible for providing management data analytics services.
    The analytics results generated with the Operation, Administration and Management
    (OAM) data can be used by other management functions such as C-SON (Centralized
    SON) to recommend appropriate actions to network operators. Finally, Fig. 6 shows
    the functional framework for the management and network data analytics services
    in 3GPP 5G systems and the corresponding mapping to the defined components of
    the data engineering pipeline. Data connection and data ingestion modules are
    located within Service Based Management and Control Interfaces. Note that both
    the NWDAF and the MDAF provide data analytics. Data analysis and processing frameworks
    can be deployed in NWDAF and MDAF, while data ingestion frameworks are deployed
    as part of the service-based management and control interfaces. Using data visualization
    and monitoring tools (e.g., Grafana, Kibana), graphical dashboards can be used
    to provide charts and notifications on the current operational status of each
    monitored source as well as analytical results. FIGURE 6. Functional framework
    of the 3GPP 5G system to support management and network data analytics services
    and mapping to the components of the data engineering pipeline. Show All E. Other
    Activities and Summary There are also other industry alliances in GSMA,113 BDVA,114
    and TM-Forum115 that are also working on specifications of AI in larger domains
    including telecommunications and their corresponding gap analysis. In summary,
    recent advances in the standardization bodies have brought their own ideas and
    proposals for shaping the possible integration options of the AI/ML platform with
    the network infrastructure. On the other hand, recent technological advances in
    data engineering are progressing rapidly and the novelties and new functionalities
    of each framework in data engineering have not yet been sufficiently explored
    in the telecommunications standardization bodies. Similarly, a clear separation
    of data engineering pipelines within the proposed architectures has been neglected.
    SECTION XI. Data Engineering Use Cases in Network Management and Orchestration
    There are several use cases in the telecommunications industry where the data
    engineering frameworks described above can be applied. Some of the relevant use
    cases are also discussed within the standardization bodies as well as the alliance
    organization in ETSI [1], 3GPP [2], ITU [3], GSM Association (GSMA) [4]. Some
    descriptions of them are as follows. ETSI ENI document [1] has classified use
    cases in four different dimensions: (i) infrastructure management (energy optimization
    using AI, handling planned peak events), (ii) network operations (intelligent
    fronthaul management and orchestration, radio coverage and capacity optimization),
    (iii) service orchestration and management (closed-loop (autonomic) fault-management,
    autonomic performance management, context-aware service experience operation,
    intelligent network slicing management) and (iv) assurance (network fault identification
    and prevention, assurance of service requirements) ITU document [3] has compiled
    more than 30 use-cases and their requirements. The use cases are divided into
    five categories: (i) Network slice,service, (ii) User plane, (iii) Applications,
    (iv) Signaling management, (v) Security. The requirements are divided into three
    categories: (i) Data collection, (ii) Data storage and processing, (iii) ML applications.
    GSMA report [4] has detailed typical seven different use cases for intelligent
    autonomous networks in China: (i) AI for network planning and deployment, (ii)
    AI for network maintenance and monitoring, (iii) AI for network optimization and
    configuration, (iv) AI for service quality measurement and improvement, (v) AI
    for network energy saving and efficiency improvement, (vi) AI for network security
    protection, (vii) AI for operational services. Data engineering solutions can
    help provide closed-loop automation, self-organizing, self-healing, self-decision
    making and self-optimizing network solutions for a variety of problems in network
    management and orchestration, network planning and design, network construction,
    network optimization, and network operations. The scope of data engineering solutions
    can be diverse in wireless, fixed networks, core networks and data centers. For
    example, considering that about 2000 parameters need to be optimized in 5G networks
    [148], network automation using the recent advances in data engineering and data
    science becomes crucial factor to bypass the human-based optimization process.
    In [11], [149], several novel use cases for (wireless) network design using DL
    and AI capabilities are presented. Below are some examples where data engineering
    frameworks can enable or influence their functionalities: Using data connection
    frameworks, providing API gateways for network providers, Using data ingestion
    frameworks, real-time monitoring applications for hardware (routers, switches,
    other network devices), software and security (threat discovery and mitigation,
    DDoS, etc.), data distribution (multimedia distribution (IPTV, content delivery
    service, etc.), text messaging service, chatbots (for quick access to inquiries
    and information (e.g., known faults, etc.))), OSS/BSS-related functionalities
    (providing real-time information (inventory/assets) to supply retail stores as
    part of supply chain management, billing services, network fault ticket management,
    network alarm management), Using data processing and analytics frameworks, leveraging
    AI/ML algorithms for CDR processing for churn analysis, real-time alarm correlation
    to identify and predict network faults, root cause analysis for network faults,
    preventive maintenance, anomaly detection, Customer 360 (tracking and analyzing
    user behaviour/needs and their interactions with channels), inventory/asset tracking
    as part of B2B products, recommender systems, network analytics, contact tracing,
    fraud detection, traffic flow prediction, intelligent network and service slices
    management and configuration, intelligent initial access and handover at RAN,
    context-aware service experience optimization, intelligent carrier management
    SD-WAN, SLA path adaptation for network delays, intelligent software rollouts,
    energy optimizations with AI, policy-driven IP-managed networks, intelligent fronthaul
    management and orchestration, service requirement assurance, federated learning
    for privacy awareness, transmission optimization, opportunistic data transmission
    in vehicular networks, predictive power management, automated scaling of VNFs,
    automated deployment of network service slices, automated site design based on
    coverage and capacity. Using data monitoring and visualization frameworks, visualization
    of transport network equipment to enable data-driven infrastructure decisions,
    visualization of service mesh topology to monitor traffic flow and metrics display.
    Finally, note that each of the frameworks or their interconnected versions in
    the Table VCharacteristics of open-source frameworks in data engineering for networking
    and corresponding related workstable.5 can be deployed at different levels of
    the network depending on the requirements of use cases. These levels can be divided
    into three levels: In node level operation, data is collected, processed or analyzed
    at individual nodes (e.g., at UEs or device level) and no network connection is
    established. This is useful for data security and privacy, reducing latency and
    complexity (since the data is processed at the device level). However, since the
    processing is done at the node level, performance limitations in the analysis
    results are expected. In network level operation, data is collected, processed
    and analyzed within a single domain of the network (e.g., at RAN or in the core
    network). This increases data diversity because the data catalogue contains data
    from multiple nodes in that domain. (e.g., from AMF, User Plane Function (UPF),
    etc. in the 5G core domain) leading to better performance optimizations. On the
    other hand, data security/privacy and delays are some of the drawbacks of network
    level processing. On the other hand in global level operation, data collection,
    processing and analysis are done with complete knowledge of the data sources in
    the network in different domains (e.g., E2E network service management). One main
    benefit of this approach is high performance. On the other hand, there are some
    issues related to global data integration and deployment cost in this way of operation.
    SECTION XII. Gap Analysis, Challenges and Future Directions Most telecommunication
    companies today require more comprehensive solutions that address both the complexity
    of their infrastructure and the intense needs of their users. Since data engineering
    technologies are younger compared to traditional telecommunication services and
    products, a few telecommunication infrastructure providers are aware of the capabilities
    of data engineering solutions. However, there is a growing number of use cases
    and a growing community and interest in early and rapid adoption of data engineering
    tools and technologies in the telecommunications world. Some early case studies
    have highlighted some of the existing gaps and challenges in the adoption of Big
    Data analytics in the telecommunication industry [150]–[155]. As telecommunication
    providers try maintain their status quo, they are at risk of being left behind
    with their products and services in a rapidly changing ecosystem. In this section,
    we explore the potential gaps, challenges, and future directions in the adoption
    of data engineering approaches in the telecommunication industry. A. Gap Analysis
    Our survey results show that there are several gaps between developments in the
    world of data engineering and telecommunications. These can be summarized as follows:
    (i) Data engineering framework deployment risks: It is critical for telecommunication
    infrastructure and service providers to understand the advantages and disadvantages
    of the technologies currently in use and the emerging cutting-edge technologies
    in the data engineering world. There are stringent requirements for operational
    efficiency, availability, reliability, robustness, and stability of telecommunication
    networks when systems are deployed in production environments. For this reason,
    the risks associated with the potential deployment of these emerging technologies
    within a mature and traditional telecommunications infrastructure must be fully
    assessed. For example, in the case of deploying ML models, the inherent randomness
    of ML systems may make it difficult to achieve reproducible results or workflows
    across different experiments [156], which may affect the reliability of the overall
    deployment process of the data engineering pipeline. There are several industry
    players such as Nokia, HPE, Juniper, etc. that already offer network solution
    products (mostly in the wireless area) that leverage BDA as listed in Table 3
    of [32]. However, most of the newer products are not very mature and not fully
    tested in production environments. Therefore, most of the new AI/ML-based technologies,
    e.g., advances in DL based neural network architectures, have not yet been tested
    in real-world applications for telecommunication applications. (This is due to
    their low Technology Readiness Levelss (TRLs)), which makes it difficult to assess
    their potential adoption. One way to monitor failures or potentially problematic
    scenarios in data engineering/science projects and their use in production is
    to monitor their adoption in other industries so that they can be intelligently
    adopted in the telecommunication domain. For example, some of the challenges described
    in [156] in developing pipelines for data management, model learning, verification
    and deployment in various domains such as computer vision, human-in-the-loop neuropathology,
    etc. may also be useful for telecommunication operators. TRL assessments of some
    of the most important and representative AI technologies, as listed in [157],
    can help telecommunication operators better understand the state of the art of
    a particular technology when planning to adopt it in telecommunication infrastructure.
    The integration and scaling issues raised in [158] when deploying AI/ML in a cross-organizational
    context (e.g., between a hospital and a service provider) may be useful for telecommunication
    operators when integrating AI/ML platforms into their vertical industries [159],
    [160]. For this reason, it will be valuable for telecommunication providers to
    have first-hand knowledge of the shortcomings of these platforms, recognize the
    weaknesses of these systems, adopt the good parts of the most needed frameworks,
    and learn from past experiences. This will allow them to adapt to the changing
    landscape of data technology with less operational and technical complexity. (ii)
    Operational costs: Introducing new features and capabilities related to emerging
    use cases within the telecommunication infrastructure is attractive but at the
    same time can be costly due to operational expenditures. For example, IT and cloud
    giant Google has shown that deploying ML-enabled systems real world incurs huge
    ongoing maintenance costs due to a variety of tasks such as configurations, data
    collection, feature extraction, data verification, analytics tools, machine resource
    management, process management tools, serving infrastructure and monitoring in
    addition to creation of ML code [161]. Moreover, operations units and network
    engineers working on the day-to-day operation of telecommunication networks should
    have additional skills such as data modeling, software engineering and system
    design, ML libraries, etc. On the other hand, data engineers, data scientists,
    and ML engineers working in the telecommunication world should acquire domain
    expertise in the functioning of the legacy systems so that accurate modeling of
    these systems using the data landscape is possible. (iii) Support for services:
    Traditional telecommunication infrastructures and vendor-based solutions are mature
    technologies with advanced enterprise support in case of service outages or activation
    of new features. On the other hand, many enterprises also rely on “microservices”
    because they are highly flexible and can be easily developed to meet dynamic business
    needs. Microservices based design requires constant communication between the
    various components of these services to keep them in sync with each other. On
    the other hand, the data engineering ecosystem is still young and rapidly evolving.
    Hence, some open source technologies developed within the data engineering ecosystem
    may have a larger community and advanced ecosystem compared to vendor-based data
    analytics solutions. For example, Big Data technology providers such as Cloudera/Hortonworks
    or MapR offer support services and subscription service models for their open
    source toolboxes. However, deploying these open source data engineering technologies
    as a telecommunication service may require extensive support from internal teams
    such as OSS or CRM departments of telecommunication providers. The authors in
    [162] have shown that providing reliable data science services (e.g., when simply
    combining open data from different open APIs) can pose several software engineering
    challenges to enterprises. (iv) Performance guarantees: Mobile and fixed networks
    have different SLA guarantees, as mobile networks consist of RAN, transport and
    core networks. For example, RAN is prone to interference and complex propagation
    environments that can lead to unpredictable outcomes. Therefore, in different
    network scenarios and conditions, algorithms within different layers in the protocol
    state (e.g., in the RAN domain, L3 algorithms (load balancing, mobility and session
    management, etc.) and L1/L2 algorithms (power control, link adaptation, scheduling,
    etc.)) aim to improve telecommunication-specific KPIs collected from various data
    sources (flows, logs, streams, databases, etc.) and bring network conditions to
    a steady state. At the same time, in the world of data engineering, other KPIs
    such as scalability, latency (which measures how close the system is to delivering
    streaming and messaging in real time, for example P95 latency of less than 5 ms
    means that 95% of all data processing requests should complete in less than 5
    ms), input rate (how much data flows from a system like Kafka or Pulsar in one
    second), processing rate (which indicates how fast data analysis can be performed),
    etc. For example, if the input rate is greater than the processing rate, the system
    lags behind, so scaling within the cluster must be done to handle the greater
    data load. For this reason, data engineering and telecommunication-specific KPIs
    are interrelated. However, there is currently no standard way to combine network-specific
    KPIs with data engineering KPIs. The specification of the common KPIs and the
    resulting necessary SLAs remains an open research area, depending on the different
    data-related use cases in such an integrated production system. (v) Customized
    functionalities: Some of the custom demands from the telecommunication world would
    be difficult to implement in the data engineering ecosystem, as the convergence
    of these requirements may be different. Therefore, some use case may require more
    effort and investment. Some examples that require complex functionalities in telecommunication
    world are response time less than 1 ms and reliability of above 99.99999% in connected
    autonomous solutions (e.g., drone delivery systems, drone swarms, etc.), guaranteed
    microsecond delay jitter in industrial automation and robotics solutions [163],
    [164], data rates up to 100 Gbps for highly mobile hotspots [165]. (vi) E2E ML
    lifecycle management: Due to exponential use of AI/ML technologies in software
    and hardware systems, the development and deployment of ML systems currently tends
    to be rushed, isolated from real-world environments, and without the context of
    larger systems or broader products into which they are to be integrated for deployment
    [156]. For this reason, current ML project lifecycle processes and guidelines
    do not follow clearly defined processes and testing standards that facilitate
    the development of high quality and reliable results. This is also true for development
    in the telecommunications sector, which relies on AI/ML technologies. In a typical
    telecommunications system, typical concerns such as data, experimentation or model
    management, deployment, reproducibility, and testing & monitoring should be considered
    depending on the ML platform. In a ML project lifecycle management as described
    in [166], all business requirements and goals of the project must be defined first
    before the project starts. After the business requirements are co-decided and
    the project objectives are defined, data collection and preparation phase follows.
    Then come the feature engineering and model training stages (in the case of DL,
    these are grouped under the term model training) and model evaluation are performed
    during the AI/ML training process. After the best model is selected, the model
    deployment, model serving, model monitoring, and model serving stages need to
    be executed sequentially. Depending on which execution stage, different feedback
    must also be provided to the previous stages. For example, in training phase,
    model evaluation stage provides feedback to the model training, data collection
    & processing, and even to goal definition stages. Similarly, model maintenance
    can provide feedback to the model training and data collection & processing stages.
    A lean Machine Learning Technology Readiness Levels (MLTRL) framework for developing
    and deploying robust, reliable, and responsible ML systems, as proposed in [156],
    can also be used in a telecommunications engineering project. (vii) Data collection
    and preprocessing: The data collection/gathering must be continuous to keep resulting
    ML model up-to-date and compatible with practical infrastructure and systems [167].
    Most ML models must function in dynamic data environments in production. For this
    reason, “concept drifts” (i.e. the degradation of model performance due to less
    similar data in production on which the model was trained) are likely and can
    affect the accuracy and reliability of the model over time. Therefore, building
    robust ML models in a changing mobile environment is different and requires active
    (continuous) learning. For this reason, continuous training has been proposed
    as part of the MLOps practice to re-train the production model [167], [168] frequently
    as new data becomes available or model performance degrades. Uniform and homogeneous
    data collection from all components of the network (NFV, IoT, 5G, etc.) and the
    data discovery process remain a significant gap between practice and ongoing efforts
    in both standardization and framework development. Not all vendor-provided functionality
    can be standards-compliant or has clear interfaces for acquiring data for analytics
    services. At the same time, obtaining real data can be time consuming and complex,
    especially when it comes to obtaining the accurate data set from a variety of
    sources (data streams, logs, databases, etc.) and extracting useful information
    from it. Data may be dirty, not easily accessible (e.g., proprietary) or not available
    at certain times within production systems. In addition, the frequency of sampling
    the system and temporally stationary conditions of distribution over the sampled
    data are also critical to data collection and must be investigated depending on
    the application. In parallel with data collection, data stored in Data Lakes needs
    to be cleaned and categorized as it may be incomplete, not correctly normalized
    or labelled, and noisy. These data should also be prepared for further analysis
    in the data engineering pipeline applications (e.g., in data processing and analysis
    frameworks for AI/ML algorithms). Some of the preprocessing activities are handling
    missing data (imputation of missing values for numeric and categorical data),
    data scaling, e.g. min-max scaler, standard scaler, max abs scaler, robust scaler,
    power transformer, quantile transformer, normalizer, etc.,116 outlier data, transforming
    data types, dimensionality reduction, identifying numerical and categorical features,
    encoding categorical features, feature engineering/selection, sampling tasks,
    to name a few are other missing dimensions in the current data engineering architectures
    for network management and orchestration. These aspects are critical as missing
    values or incorrectly populated datasets can lead to inconsistent analysis results.
    As a general rule, less than 1% missing data is trivial to handle, 1–5% missing
    data can be manageable, 5–15% missing data requires sophisticated methods to handle,
    and more than 15% can seriously affect any kind of interpretation [169]. Some
    of the most popular solutions are either removing the data (which leads to information
    loss and biased assessments) or using some advanced imputation techniques and
    maximum likelihood methods [170]. Multiple imputation using chained equations
    (MICE) [171] and factor analysis of mixed data (FAMD) [172] are some of the commonly
    used imputation techniques for hybrid missing data sets (e.g., in both categorical
    and continuous data). Recent developments and solutions using DL such as DataWig
    (which trains a neural network based classifier to predict the missing values)
    can also be used for scenarios with many missing observations [173]. As the authors
    note in [174], while there are many approaches that deal with missing values,
    they are mostly designed for matrices only. However, in many real-world applications,
    the data is not only available in numeric format, but may also be in textual form
    or as an image. Another main problem with the above traditional approaches is
    that rare values that are common in heavy tailed real-world datasets cannot be
    accurately identified with the trained models [175]. In particular, mobile data
    collected from network devices is often subject to redundancy, loss, mislabeling
    or class imbalance, and thus needs to be preprocessed before it can be used directly
    for training. Compared to traditional ML, DL methods that process missing values
    in batch mode, telecommunication systems are more dynamic and more missing data
    is received in less time per second due to the nature of networks and wireless
    communication infrastructure. Therefore, special care must be taken when processing
    dynamic and fast telecommunication traffic. (viii) Model training with real data:
    The goal of model training is to optimize and rapidly converge model parameters
    to optimal and consistent values given a set of training data. However, developing
    a model to meet specific product requirements may require combinatorial search
    for parameters, variables, etc., which can become difficult as the model becomes
    more complex. Training models for production systems can be costly due to a lack
    of either data or properly labeled data (especially for supervised trained models
    that require data augmentation, e.g., labeling large amounts of data, experts
    experience). In such scenarios, simulated data can be used to train the models,
    but the gap between practice and theory, i.e. the lack of adequate knowledge transfer
    from simulation to real conditions, can be a major problem. Training requires
    observing a wide range of scenarios and in the case of network management and
    operations, generating different network configurations and scenarios that can
    potentially disrupt network operations for training purposes. For example, in
    reinforcement learning applications, the agent must interact with the environment
    as it tries different actions and receives feedback to improve the outcome based
    on its actions. During this process, the agent makes several mistakes while learning
    which requires a large number of steps to converge to an optimal or near-optimal
    solution. For this reason, the training of the agent is not performed in the real
    infrastructure, since errors can have serious consequences for the networks (failures,
    false alarms, downtime, etc.). Instead, a simulator can be built that mimics the
    real network environment, and the agent is trained offline in this simulator environment.
    However, the simulator must meet high fidelity requirements because the real network
    may be different from the network used for training. Moreover, the agent cannot
    operate appropriately if the discrepancy between the real and simulated environments
    is large [176]. In the case of ever-changing mobile network environments, models
    of ML should have the ability to learn continuously (active learning discussed
    in Section XI. B bullet no. (xviii)) or perform transfer learning [177]. Transfer
    learning can accelerate the training process when the conditions in the mobile
    network change significantly. Basically, transfer learning aims to reduce the
    amount of training data required to learn a task, by reusing the feature extraction
    layers learned on other datasets [177]. In other words, it enables the rapid transfer
    of knowledge from pre-trained models to other types of datasets. Transfer learning
    can be used to improve the performance of models that learn with limited data.
    For example, if software viruses are spreading rapidly in the network, the anomaly
    detection model or antivirus software detection model built into the network equipment
    should be able to respond to these attacks in a timely manner with the limited
    information available. In transfer learning, a model learnt in a particular environment
    (e.g. cellular Base Stations (BSs) operating at low-bands, < 1 GHz) can be transferred
    and adapted to another network node operating in a different environment (e.g.
    cellular BSs operating at mid-bands (1–6 GHz) or high-bands (>20 GHz)). This is
    technically referred to as frequency-based transfer learning in [178]. When the
    system dynamics in the environment change, e.g., due to a device malfunction,
    a different, previously unknown terrain, different frequencies, etc., the result
    is a completely different data set than the one previously used for training.
    Therefore, the previously trained model must be trained again in this new scenario/at
    this new frequency, which is inefficient because all these datasets must be acquired
    again. Transfer learning approaches applied in wireless communication aim to tune
    the existing models with a small amount of data in the changed environment [179].
    For example, frequency-based transfer learning transfers the models trained on
    different frequencies to the target frequency, while scene-based transfer learning
    transfers the models trained on different scenes to target scenes that use the
    same frequency [178]. The authors in [178] also showed that both frequency-based
    and scene-based transfer learning models can predict path loss with small errors
    by using limited data of the new environment and learning the regularities between
    path loss and scenario information in detail. (ix) Model deployments in real-world
    environments: Comprehensive monitoring of system behavior and taking automatic
    actions (without direct human intervention) are crucial for higher system reliability
    in the long run. On the other hand, deploying models in production is still not
    an easy task. There are pre-deployment, deployment, and non-technical challenges
    during the deployment and during the operation of ML models in practice [180].
    According to the authors in [181], the main challenges are mainly related to model
    integration (operational support, code and model reuse, software engineering anti-patterns
    and mixed team dynamics), model monitoring (feedback loops, outlier detection,
    custom design tools) and model updating (concept drift and continuous delivery).
    Note that in a production environment dozens or hundreds of models may be running
    simultaneously. Therefore, the developed ML models should be monitored, alerted
    and automatically recovered in case of failures to achieve a certain service level
    goal. Depending on the area where improvements are needed in terms of intelligence,
    use case requirements and static/dynamic characteristics of the environment, the
    update frequency of the ML model may also vary. For example, choosing an optimal
    threshold in auto-encoder-based neural networks (e.g., in anomaly detection applications)
    is important to achieve a good trade-off in certain metrics such as precision
    and recall [182]. Thus, if a new training/validation dataset is created frequently,
    the optimal threshold and the corresponding model must also be updated frequently
    to cope with changes in the state of outside world. In the case of dynamic network
    environments (e.g., RAN algorithms using L1 to L3 transmission parameters, modulation
    and coding schemes, resource allocations, etc.), the model and corresponding hyperparameters
    should also be updated frequently (fast time scales on the order of seconds/milliseconds),
    as the ML models can degrade or exhibit biases and user behaviour may change over
    time [183]. In network optimization, the model update frequency can be on the
    order of hours/days/weekly (e.g. hyper-parameters for SONs algorithms). However,
    for network design, this update frequency can be on the order of weeks/months
    (e.g., when deploying new cell in a given geographic region) (see Figure 1 of
    [183] for more information on the main areas of performance improvement). For
    this reason, depending on the scenario considered, an appropriate feedback loop
    for the deployment of the model is also required to achieve good and timely results.
    As frequency of ML model usage in a service provider increases, many models need
    to be supported either sequentially or concurrently by a model server. Several
    deployment options are available, such as A/B testing (one set of data is used
    by one model and the remaining is used by another model) [184], ensembling (combine
    multiple models to get a stronger model) [185] or cascading [186] (make predictions
    based on a model (e.g., detect anomalies within the infrastructure or find the
    root case). Depending on the requirements and the complexity of the deployment
    pattern, different options for the selection of algorithms, architectures, tools,
    etc. need to be defined. For example, unsupervised learning algorithms may offer
    lower latency and cost savings at the expense of performance degradation in data
    analysis and processing compared to supervised learning algorithms. As an alternative
    solution, the developed ML models for production systems can also be embedded
    in the operating system kernel and provided as a system service. (x) New architecture
    for event driven applications: Traditional telecommunication systems and their
    legacy applications are based on an application architecture that are using APIs
    [187]. A common gap in the traditional telecommunication network architectures
    is that they were not originally designed for event-driven applications, although
    recent efforts on Service Based Architecture (SBA) in the 5G core network have
    shown some tendencies towards their deployment [188]. On the other hand, data-driven
    architectures for telecommunication systems are based on using huge amounts of
    data and transferring them to a AI/ML platform for large scale analytics [14].
    However, this may disregard some of the already existing application capabilities,
    such as enterprise integration capability or agility. For this reason, a balance
    is needed between a data-driven architecture (which specializes in transferring
    large amounts of data between applications) and an application architecture (which
    ensure that the functionality of one application is executed in response to a
    request from another application). The general approach in the industry is to
    move to stateful, event-driven and event-time-aware processing using the concepts
    of events, streams, producers, and consumers. Many industries have already started
    to move from a monolithic architecture to a microservice architecture for scalability
    and maintainability reasons [189]. Event-Driven Architecture (EDA) has already
    proven itself in the cloud and IT communities and will become the software architecture
    paradigm of choice in the telecommunication domain in the coming years. Together
    with the introduction of the concept of SBA in the 5G core [190], it is expected
    that it will soon be used in the architecture of mobile networks. An event is
    a change of state or an update in the system. EDA uses a sequence of events to
    trigger and control communication between microservices (i.e., decoupled services).
    It is particularly suitable for applications based on microservices interconnected
    by fast asynchronous events [191]. In an event-driven system, there are collections
    of independent services between which there is no direct coupling. The data schema
    is the only dependency between them. This increases the resilience (since failures
    in a service do not escalate) and extensibility (easy addition of new independent
    services to the existing systems, e.g., notification service) of the systems.
    Therefore, an EDA can successfully provide streaming, Pub/Sub, and Push patterns,
    while web services with REST/HTTP, API gateways, cronjobs, RabbitMQ, Kafka or
    data at rest with a Data Lake cannot. As a result, enterprises are starting to
    adapt to EDAs or event sourcing. There are several ways in which events captured
    in real-time can be useful for data analysis (e.g., by correlating events with
    other introduced features, recent incidents, etc.). For example, in most streaming
    frameworks (e.g., Spark Streaming), window operations are performed as each message
    is received by the streaming processing framework (i.e., in processing time).
    However, the exact way to customize window operations is to support more advanced
    event time windows and perform computations based on event time, i.e., when the
    event was created [53]. Another good example of the application of EDA in telecommunication
    systems would be Network Management System (NMS), where critical events can be
    quickly responded in order to mitigate the problems in the network. The general
    workflow associated with EDA for this example could be as follows: (i) NMS detects
    an anomaly and publishes an AnomalyDetected event (ii) The Root Cause Service
    subscribes to the event, processes it and computes the location and root cause
    of the anomaly (iii) The Root Cause Service then publishes the RootCause event
    (iv) The Region Support Service subscribes to this event and sends a notification
    to personnel in that region explaining the root cause of the problem. (xi) Ecosystem
    integration: Telecommunications network technologies are becoming more complex
    with each passing decade than previous generations. In 5G networks, for example,
    URLLC, Enhanced Mobile Broadband (eMBB) and massive Machine Type Communications
    (mMTC) type communications require specialized technologies (e.g, Massive MIMO
    [192], coordination algorithms (Carrier Aggregation (CA), Coordinated Multi-Point
    (CoMP) transmission/reception [193], Single Frequency Networks [194], Multi-Connectivity),
    new spectrum (high frequency bands (>20 Ghz) such as from millimeter and terahertz
    (THz) wavebands to visible light), Device-to-Device [195], dynamic network slicing
    [196], network virtualization [197], Edge Computing [198], integrated satellite-terrestrial
    communications [199], [200], intent-based networking [201], etc.) that need to
    be embedded in telecommunication networks. As services become more complex (e.g.,
    with dozens of microservices interacting with each other), management and orchestration
    operations also become more complex and costly. To tackle this complexity in the
    management and control plane of the telecommunications infrastructure, there are
    several automation tools that can manage the network service management lifecycle
    (e.g., Open Source MANO (OSM),117 Open Network Automation Platform (ONAP),118
    Cloudify,119 etc.). However, they also require complex MANO procedures. For this
    reason, specialized skills (e.g., network virtualization, cloud services, etc.)
    are required to fully exploit their application potential and seize the opportunity
    to develop new and innovative value-added services. At the same time, these new
    technologies also bring their own specific challenges and obstacles when it comes
    to integration with data engineering frameworks. Therefore, the tools and libraries
    selected from the data engineering ecosystem should be well integrated with the
    broader telecommunication infrastructure systems based on the use cases and requirements.
    The support of the data engineering ecosystem or community for high quality tools
    and adoption of the latest technologies into the telecommunication infrastructure
    are also crucial in this process. (xii) Licensing: In parallel with ecosystem
    integration, the licensing gaps for hybrid deployment types need to be further
    explored. Many of the open source tools for data engineering are licensed under
    Apache 2.0, which does not imply vendor-specific licensing. On the other hand,
    legacy telecommunication infrastructures are based on various vendor-specific
    equipment. Avoiding vendor dependency helps enterprises to develop their own customized
    services and explore new opportunities and business goals. The gap between the
    interplay of open source and vendor-locked systems deployments is an ongoing issue
    and needs to be further explored. (xiii) Synchronization aspects: In a traditional
    telecommunication system, one task may orchestrate multiple calls to internal
    or external services. Telecommunications systems require strict synchronization
    between multiple components of the data engineering platform and the telecommunications
    infrastructure. If synchronous orchestration of services fails, the entire service
    flow fails. Some of these synchronization requirements also arise during data
    collection, model and hyperparameter updates when multiple actions need to be
    performed by the AI/ML platform between the interconnected network domains (e.g.,
    joint actions performed on both the core network and the transport networks).
    For example, after the data connection and ingestion phases are completed, the
    extracted and transformed data must be continuously synchronized with the original
    data sources. However, since data sources can be heterogeneous and change dynamically
    over time, a data source may be out of sync and out of date at the time of integration.
    This can lead to discrepancies in data schema and definition and cause problems
    in synchronizing these heterogeneous data sources. The development of such solutions
    for telecommunication networks in the field of data engineering is still an open
    research area. (xiv) Lack of rigorous methodology in networking: Throughout its
    evolution, networking has evolved both scientifically and through trial and error
    and configuration based deployments in real systems. As a result, there are complex
    interaction patterns among the components of telecommunication networks. For example,
    in most cases, the network is designed to be distributed and each node (router,
    switch, gateway, etc.) overlooks only a portion of its environment. This also
    makes it difficult to apply conventional approaches/algorithms from computer vision
    or Natural Language Processing (NLP) (which also use standardized datasets such
    as the MNIST (Modified National Institute of Standards and Technology) database
    for handwritten digits or the ImageNet database, etc.) for direct comparisons
    of learning or inference algorithms to the complex networking systems. Therefore,
    a more rigorous and scientific approach is required when designing AI/ML systems
    in the area of complex and large-scale telecommunication systems. (xv) Hybrid
    approach to data operations: Note that all of the above analytics frameworks including
    data collection, data analysis, data monitoring, data visualization, etc. can
    be performed either at the device, network or global level as described in Section
    XI. However, depending on the use case, a hybrid approach may also be required,
    comprising a flexible and distributed analytics architecture where some necessary
    data processing is performed at the device level and/or some partial processing
    is performed at network the level. Distributing some of the functionalities of
    these frameworks across these levels can help improve network performance by reducing
    bandwidth overhead or network latency (which can be helpful for real-time applications).
    In [202], the authors have shown the benefits of such a hybrid approach to reduce
    the cost of data communication while ensuring that the accuracy of decision making
    for IoT networks does not significantly decrease. A flexible placement strategy
    of different data analytics modules that can be dynamically selected, combined
    or switched to achieve the best I/O performance is also explored in [203]. The
    benefits of data orchestration of use case-based analytics for 5G scenarios are
    proposed in [204]. In the case of such a hybrid approach, a different data analysis
    setup can be created for different use cases (e.g., mMTC, eMBB or URLLC in 5G
    network slices). In a network slicing setup where data flow over the industry
    outside the industrial site (e.g., a factory) is not desired, the edge computing
    paradigm can be enabled. In this edge computing setup, for example, AI-based image
    processing for quality inspections can be performed at the edge instead of in
    cloud servers to reduce traffic and eliminate critical I/O performance bottlenecks.
    On the other hand, other network slices can continue to run their analytics modules
    on central servers. In the IoT data processing scenario, computationally intensive
    data training and inference generation can be performed at a global level (e.g.,
    in the cloud). At the same time, data generated locally at the device level (e.g.,
    from sensors) can be transformed and aggregated locally to save transmission energy
    and increase data protection while maintaining global accuracy as much as possible.
    Another example of functionality distribution is via federated learning. In federated
    learning, with limited interaction between nodes in the network, local construction
    of AI/ML models can be instantiated within a single component/node of the network.
    At a later stage, these small models at the individual distributed nodes can be
    sent back to a network level coordinator to build a global model and view of the
    network domain. Finally, the global models can be sent back to the local devices/nodes
    to improve performance [205]. (xvi) Practical aspects versus system complexity:
    Another problem that is usually overlooked in the design of data systems is the
    increase of system complexity in practical systems, e.g. algorithms that are data
    hungry (increased amount of data) and require high-computational (excessive use
    of CPU, RAM or storage capacity in servers). Indeed, deep neural network architectures
    require complex structures and in many cases provide powerful results (e.g., high
    classification accuracy) that represent a trade-off between accuracy and computational
    cost (e.g., computational cost for inference, time for hyperparameter optimization).
    However, despite their high model performance metrics demonstrated in particular
    in the fields of NLP and computer vision, they also require a significant amount
    of computational resources and power (e.g. Convolutional Neural Networks (CNN)
    which rely on operators such as convolution, rectified linear unit (ReLU), pooling
    and classification) and larger systems such as multicore CPUs and GPUs for fast
    and accurate performance computation [206]. For this reason, in some use cases,
    the deployment of deep neural networks, especially on embedded and mobile devices
    (e.g., training a complex image classification model using local data on resource-constrained
    (in terms of energy and capacity) mobile devices) may be either expensive or not
    possible. Therefore, very deep neural networks may not be suitable for these scenarios,
    as they would compromise some performance metrics (e.g., accuracy). Instead, lightweight
    architectures that are less suitable for complex tasks should be chosen. This
    trade-off should be considered especially in resource-constrained smart environments
    [207]. As a solution, some advanced techniques and toolboxes can be used to deploy
    these complex DL models in mobile network applications (e.g., while compensating
    for small performance degradations) [10]. On the other hand, in some cases, e.g.,
    when exploring tabular datasets, tree ensemble algorithms such as XGBoost can
    outperform deep neural network models in terms of accuracy, inference efficiency,
    and optimization time, as shown in [208], which also needs to be considered before
    increasing model complexity. At the same time, note that adding new and more sophisticated
    data components can also slow down the entire process of data engineering pipeline
    in practical systems. In addition, new systems or components may poorly represent
    uncertainty, and may lack transparency and trust. Therefore, it is important to
    weigh the technical pros and cons of the benefits of purely research-based solutions
    when designing the entire data engineering pipeline in practical real-world systems.
    (xvii) Cloud vs. on-premise infrastructure: When designing a data engineering
    pipeline, the different deployment options (e.g., cloud (public), on-premise (private),
    or hybrid cloud) and the corresponding trade-offs should be thoroughly analyzed.
    First of all, there are several advantages to using cloud services. For example,
    cloud services offer high availability, easy scalability, resilience, cost reductions,
    and easy accessibility when a product reaches a higher level. On the other hand,
    building an on-premise infrastructure can ensure that privacy, security and regulatory
    compliance for mission-critical services. From a cost perspective, iteratively
    processing data in the data engineering pipeline (e.g., ML-based data analytics
    and processing frameworks) and running applications 24/7 in the cloud can be expensive
    compared to on-premise solutions. For this reason, in some scenarios, enterprises
    may be interested in taking advantage of both private and public clouds. Hybrid
    options can leverage the different features and characteristics of multiple platforms
    as well as traditional on-premise resources. For example, if the data load in
    one of the frameworks in the data engineering pipeline explodes, additional public
    resources in the cloud can be helpful until the data load levels drop back below
    a certain threshold. Hybrid options can also be beneficial for high availability
    and disaster recovery scenarios [209]. Day-to-day production systems can be maintained
    on-promise while a backup or recovery environment can be moved to the cloud to
    provide agility in a disaster recovery scenario. xviii) Computing resources for
    training in wireless networks: Wireless networks also have their own challenges,
    such as uncertainties in the environment (e.g., dynamic channel, security, congestion,
    interference, connectivity, network expansion, etc.), limited resources (e.g.,
    transmit power, spectrum) or hardware constraints (e.g., computational power)
    that make training models difficult [35]. Mobile data is dynamic, distributed
    over a large geographic area, exhibits changing patterns over time, and has inherent
    characteristics associated with human mobility, location topology, local culture
    (e.g. events, festivals), etc. For example, the spatio-temporal behaviour of residents
    may differ significantly depending on the time of day or week [210]. Some of the
    devices (e.g., mobile devices) also have limited hardware capacities and cannot
    train complex ML/DL models with large datasets. In complex and large architectures
    and environments such as 5G, powerful hardware and software are required to support
    both training and inference (as data volume and quality become increasingly important)
    if intelligence is to be built on top of the network infrastructure, as described
    in survey paper [10] and the articles referenced therein. Therefore, computational
    and time resources for training processes need to be considered when learning
    with large datasets especially in wireless applications where patterns change
    over time [210], [211]. When model training is performed with large distributed
    datasets on central servers, additional communication and storage costs are incurred
    and the solution does not scale. An elegant solution is to perform model execution
    on distributed nodes while ensuring good performance on local data and reducing
    the load on central servers (e.g., federated learning on wireless networks [212]).
    To stabilize the training process and accelerate convergence, the optimization
    process can also be updated as conditions change [213]. Finally Table 6 provides
    a summary of the gap analysis described above. TABLE 6 Summary of the Gap Analysis
    B. Challenges In order to reap the benefits of integrating data engineering ecosystem
    solutions at different layers of the telecommunication network infrastructure
    for both telecommunication providers and users, there are also some challenges
    that need to be overcome. Some of the challenges in putting together a data pipeline
    architecture are related to the following issues: (i) Inter-working between different
    programming languages, tools, computation runtimes: Developers and data engineers
    use a variety of tools and programming languages (Python, Java, Scala, R, Julia,
    SAS, etc.). Using multiple languages often increases the cost of effective testing
    and leads to difficulties in transferring responsibility to others. For example,
    some message queuing systems such as RabbitMQ [49] or Kafka [45] can be implemented
    in Java, some other data modules such as Apache Spark are written and work best
    in Scala programming language (there is also support for Java and Python), most
    of ML algorithms are better supported by the Python programming language and libraries,
    and user web applications can be written in the C# programming language. For this
    reason, the field of data tools and systems is inherently heterogeneous, diverse
    and fragmented, as multiple workflows are involved in the process of creating
    data engineering pipelines. Therefore, supporting multiple languages and decoupling
    the components of the data engineering pipeline can be critical to reducing the
    overall complexity of the system and accommodating heterogeneity. Furthermore,
    it is desirable that entire layouts of existing frameworks in a data engineering
    pipeline are language-independent and provide software abstractions. In summary,
    integration with other systems is an ever-growing area and requires overarching
    tools when data connections between different frameworks are required. (ii) Choosing
    the right toolset: Big Data can be categorized under “7 Vs”: volume, velocity,
    variety, variability, veracity, visualization and value [214]. So, depending on
    rhe use case and the different industry requirements, either one or more of these
    Vs may be important. For URLLC applications (e.g., telemedicine and autonomous
    driving) velocity and veracity, for eMBB applications (e.g., remote metering),
    volume, or for mMTC applications, veracity of data may be important parameters
    to optimize when selecting the appropriate data engineering tools from a variety
    of design solutions. At the same time, to manage the complex workflows and the
    needs of different stakeholders demanding various network services, a comprehensive
    list of tools, platforms and frameworks should be used based on the different
    characteristics of the data sources and the requirements of the data processing.
    For example, in data ingestion and transformation, Apache Storm [54] can be used
    for high volume real-time data, Apache Nifi can be used for medium volume real-time
    data, and Sqoop can be used for batch data with low latency requirements. In addition,
    extensive comparisons of some of the latest message queueing systems (e.g., Kafka,
    RabbitMQ, RocketMQ, ActiveMQ, and Pulsar) have shown that Kafka can be used for
    higher throughput, RabbitMQ is more suitable for lower latency, while RocketMQ
    can provide both low latency and high quality of service for applications and
    services [66]. Some tools, such as Apache Druid, only allow querying a single
    data set, so joining with multiple other data sources is not possible. Since in
    such scenarios it is not an optimal combine all data sources into a single data
    source, e.g., due to the nature of the different services producing data, other
    custom tools such as Presto can be used for these purposes. As another example,
    when developing a streaming application, there is an inherent trade-off between
    data quality and data speed. To provide a fault-tolerant and scalable system with
    an exactly-once-guarantees, various platforms such as Spark’s Structured Streaming
    and Delta Lake can be used. For out-of-order data processing, Flink’s data stream
    processing is an ideal candidate. Some OLAP solutions designed for Big Data such
    as ClickHouse itself, are only designed for fast queries over large data set and
    do not support real-time record-by-record ingestion. Only after integration with
    a streaming platform such as Kafka is real-time data streaming possible, allowing
    ClickHouse to act as a message consumer. Therefore, depending on the required
    reliability of the request (either streaming or batch) and possible trade-offs
    in performance, data engineers need to choose different tools. Given all these
    different options, it can be difficult to find a suitable set of tools for building
    a data pipeline. The choice depends on numerous factors, such as the analysis
    results of the pros and cons of the tools or the understanding of their suitability
    for the use cases under consideration. Ideally, the selected tools should not
    be tied to a specific vendor, should be supported by a large community, should
    have clear documentation, should be easy to integrate with the rest of the platform,
    and should be independent of various software, including cloud services and third-party
    vendors. (iii) Support for containerization: There is a growing need for support
    for containerization to build flexible, service-oriented and cloud-native applications
    [215]. The general trend is to build services using infrastructures such as Kubernetes
    clusters (to enable production-grade container orchestration). A production system
    would run multiple machines, each with hundreds of containers that can be restarted,
    rescheduled or terminated at any time. As an example of a scale-out architecture,
    one container-based microservice can be exposed with REST-APIs over Hypertext
    Transfer Protocol (HTTP), another container can be accessed using Protobuf and
    gRPC, or another with real-time streaming requirements can expose its microservice
    via websocket APIs. Therefore, using frameworks such as Kubernetes to deploy containers/microservices
    provides flexibility in deployment, ease of automation, movement, and scaling.
    On the other hand, although modern open source projects such as Pulsar, Spark
    or Flink provide native support for Kubernetes, there are still many components
    in the Hadoop ecosystem that have not moved away from YARN or do not provide standard
    support (e.g., Kafka). For example, automatically resizing jobs in a container
    (scaling up/down, scaling out/in) for stream processing jobs depending on lags
    or other performance parameters is also currently a challenging problem. (iv)
    Lack of a unified framework for data processing and analysis: In a general data
    engineering pipeline, online and offline data processing are handled in separate
    pipelines, each using different computing engines such as Kafka, Spark Streaming,
    Flink, Hive, Map-Reduce, etc. However, this can add maintenance overhead for enterprise
    development teams. Inside telecommunication operator, there are a variety of data
    analytics nodes and tools deployed in various sub-units to perform customer experience
    management, service quality of service management, revenue assurance, or user/marketing
    analytics. On the other hand, it is a difficult task to integrate all these separate
    analytics nodes with traditional systems (e.g., with data visualization/notification
    applications for reporting, with network management and orchestration tools for
    service automation) in a single framework. In data engineering, some frameworks
    such as Spark or Flink can provide a single, unified data engineering pipeline
    solution for both online real-time and offline data. However, to generalize application
    development, some other computational patterns such as distributed training, model
    serving, streaming, distributed data processing, distributed reinforcement learning,
    etc. need to be implemented as libraries in addition to these frameworks. Although
    there are frameworks that provide a unique set of abstractions and a unified APIs
    for both batch and stream processing jobs, consolidating an advanced data engineering
    pipeline cannot be achieved with a single unified framework to perform general
    distributed computation, online multi-stream processing, window operations, stateful
    analysis or DL simultaneously. For example, Kafka’s data ingestion benefits may
    outperform Spark Streaming data ingestion framework, while additional data processing
    such as multi-stream joins or generating additional features for online and offline
    data can be more effortlessly performed only with Spark and not with Kafka. Similarly,
    adding support for some libraries (e.g., a current DL framework) may be excluded
    from the mainstream development process due to lack of resources, suitable use
    cases or interest in the community (e.g., because industry needs are not yet mature
    enough) and because it is very time-consuming to append a new framework to the
    overall AI/ML stack. (v) Use of multiple AI/ML frameworks: In many organizations,
    it is common to use multiple systems and frameworks for different workloads. For
    example, in data storage, a data lake, many data warehouses, custom specialty
    databases for graphs, streaming, time-series databases, etc. are common practice.
    At the same time, some of the emerging areas like DL are advancing very quickly
    and depending on the task at hand, different DL frameworks can be more effective.
    While it is easy to experiment with a new framework, it is very expensive to add
    production support for each new DL library. In cross-domain applications where
    many of these different computational frameworks or patterns need to be combined,
    serious challenges can arise. For example, in cases where reinforcement learning
    or some online learning applications require processing data streams, training
    and deploying models which may exceed the limits of specific purpose integrated
    systems. This, of course, increases complexity. In practice, one way to overcome
    this problem is to find a way to connect the different frameworks together to
    create applications that are independent of any particular framework. Another
    way is to build a new system from scratch that can supports the functionalities
    of these frameworks with simple APIs for new algorithms, creating general purpose
    systems. However, these two ways have their own pros and cons. For example, when
    merging different systems, it is not efficient to move data between frameworks,
    which can lead to additional overhead and inflexibility (e.g., inferences drawn
    by the system cannot be updated frequently due to model update difficulties).
    In addition, the learning curve of all these different frameworks can be steep.
    On the other hand, designing and developing a new system from scratch and moving
    to a new general-purpose system can require a great deal of engineering effort
    for new application development processes. Despite these challenges, many organizations
    of today are moving to develop their own internal data platforms consisting of
    a variety of open-source tools and frameworks, rather than relying on closed proprietary
    systems. (vi) Data security and privacy: Legal compliance, encryption, key management
    and data governance & integrity are the main pillars of data security and privacy.
    If not managed properly, a large data set distributed across an enterprise can
    cause major headaches for data owners in terms of security, authentication, authorization
    and information integrity. With strict regulatory requirements (e.g., the GDPR
    (General Data Protection Regulation) in Europe) preventing data from being moved
    to the cloud, many organizations are looking for and investing in tools that can
    allow only authorized individuals to manage sensitive data on-premises. At the
    same time, data sources cannot always be trusted, which can lead to gaps in the
    system. For this reason, ensuring data security is also crucial in model training
    and validation. The accuracy and integrity of the data set must be ensured by
    avoiding data collection from faulty or compromised network nodes/users to protect
    against unfavorable data sets. All data breaches must be detected as soon as possible.
    For this reason, data stream processing is ideal for developing security applications
    that allow to respond immediately. For example, in a typical enterprise, bots,
    scraper detection, or access monitoring are importance requirements that can be
    met with available stream processing platforms that provide state management and
    checkpointing capabilities. Real-time data should also comply with privacy regulations,
    similar to data stored in data warehouses, data lakes, or traditional data stores.
    Some companies such as Confluent are already offering new connectors, such as
    the Privitar Kafka connector which improves the value of streaming data assets
    without compromising user privacy.120 Data stream processors such as Splunk DSP
    can also mask sensitive data. (vii) Event streaming support: Traditional event
    streaming in OSS uses various protocols such as Simple Network Management Protocol
    (SNMP) for routers and service gateways, gRPC and protobuf (Google binary protocol
    buffer) for telemetry, or syslog events for soft switches for monitoring purposes.
    However, these various vendor-specific protocols also bring some challenges. Some
    of them are: Complexity in real-time analysis, multiple data semantics & naming
    across different device types and data sources. In BSS systems, there are also
    various challenges related to different systems for broadband, mobile and fixed
    services, technology stacks (fiber, copper, 4G/5G, etc.), and other Value Added
    Services (VASs). Several BSS system components need to be extended to include
    services such as recommendations, augmented reality, payment integration, etc.
    and integration with legacy middleware components of CRM systems (ETL, Enterprise
    Service Bus (ESB)) also needs to be done. Together with data ingestion frameworks,
    these disparate data streams can be normalized to a common schema facilitating
    real-time analysis and display of the global network infrastructure. Moreover,
    data ingestion frameworks can be used to achieve asynchronous communication between
    components and interoperability between different service providers of OSS and
    BSS systems. On the other hand, streaming support for model serving purposes is
    an important feature when selecting data processing frameworks. Although most
    model serving applications are based on REST, it is not desirable to use REST
    inside streaming applications and make many calls outside the execution environment.
    For this reason, new libraries (such as Flink Tensorflow [216]) are gradually
    emerging that can support streaming model serving. (viii) Architecture decisions:
    Various data architecture decisions for streaming and batch processing involve
    trade-offs, and organizations are willing to choose the one that offers more flexible
    scaling, lower operational overhead with high availability and reliable performance.
    Some important considerations when choosing a data architecture are scalability,
    operability (which is more difficult with stream processing jobs due to potential
    lags), bridging both offline and online scenarios (especially useful for applications
    using active learning) and ease of data access and movement capabilities (due
    to the inherent semantics differences in the data). Different data architectures
    are available depending on the use case and SLAs. Table 7 summarizes the descriptions
    and drawbacks of these different available architectures. TABLE 7 Comparisons
    of Architectural Choices (ix) Operational complexity: Selecting data engineering
    systems that can work in a single system reduces operational complexity. However,
    there will not be a single platform, system or compute runtime that can handle
    all the entire underlying heterogeneous data engineering infrastructure. This
    is because there are different data types (big data or small data, graph or log
    data, etc.) and access patterns (streaming or parallel) in the data landscape
    or ecosystem. Also, the introduction of new technologies requires new, well-trained
    people who can handle the sheer growth of the technology stack. However, managing
    and deploying data tools is getting easier by the day. The latest data engineering
    tools greatly abstract and simplify workflows, allowing data engineers to focus
    on selecting the simplest and most cost-effective solutions that deliver the greatest
    value to the business. As a result, these incremental developments in the data
    tooling landscape are expected to significantly reduce the operational complexity
    of deploying future data architectures. (x) Batch computing challenges: Most frameworks
    that rely on batch data ETL using SQL or SQL-like functionality can be difficult
    to integrate when complex logic is required compared to simple low-level programming.
    Batch computing queries become problematic when resources are limited because
    aggregations are not additive when new elements are added to the computed results.
    As a result, batch computing can also be inflexible and difficult to manage, which
    can lead to errors. When using third party solutions to improve efficiency, utilization,
    and performance, some dependencies and versions (e.g., jobs that depend on Spark
    or Hive versions) can be difficult to integrate due to heterogeneity in workloads
    (e.g., analytic, transactional), infrastructure (e.g., cloud, on-premises), deployments
    (e.g., Kubernetes, bare-metal nodes, custom Platform as a Service (PaaS)) and
    data pipeline environment, increasing operational overhead. To overcome these
    challenges, frameworks such as Kubernetes with its containerized approach can
    be used. (xi) System stability: System stability depends on both data consistency
    and the longevity of the systems used. If the same data engineering pipeline cluster
    is used by multiple use cases, the entire cluster may become unstable during sudden
    traffic spikes. For example, a throughput-intensive application may impact or
    slow down the data availability of another application or service if the pipelines
    are not adequately planned. Solving data consistency problems caused by multiple
    systems is a difficult engineering challenge. Data inconsistencies can lead to
    data loss or duplication of data. Bringing these events to a consistent state
    requires additional effort from the data and operations engineering teams. As
    a platform for data orchestration and management, the multi-tenancy support in
    Kubernetes aims to enable such isolation and fair resource sharing between multiple
    use cases so that their workloads can be reliably shared in a single cluster.
    However, this approach should also be extended to the E2E components of the entire
    data engineering pipeline. (xii) Data sharing: Many telecommunication service
    providers struggle to understand and identify what data should be shared, while
    ensuring regulatory compliance and defining/understanding the standardized interfaces
    for data sharing. Implementing a secure and distributed approach to data sharing
    between different network nodes is critical. In future networks, for example,
    many telecommunication service providers will have to share part of their infrastructure
    with each other. This will also force them to share critical infrastructure-related
    information for better network management and orchestration [218]. Traditional
    APIs used for data sharing can perform poorly (a slow-working API can be a bottleneck
    for the service) or converting legacy services to an API-based service can be
    costly. Monolithic databases where every user retrieves the data can lead to a
    single point of failure and scalability issues. Big Data transfers can also have
    consistency issues due to the longer duration of data transfers when data is shared.
    To address these data sharing issues in a scalable way and improve the quality
    of data sharing with third-party vendors or internal departments of an organization,
    investments can be made in building standardized interfaces for accessing relevant
    data and event-based applications and architectures for complex events. Another
    possible solution for data sharing is to integrate the latest developments in
    blockchain-based systems into telecommunication networks [219] so that intelligence
    and data can be shared between the owners of the individual network domains in
    a secure and reliable manner. (xiii) Coexistence with non-AI/ML capable systems:
    In a traditional telecommunications infrastructure, not all deployed equipment
    will be intelligent. In some cases, the AI/ML-enabled systems will need to interact
    with non-AI/ML-enabled systems. For example, in some situations, some of the UEs/edge
    devices may be used for model training while others act as normal mobile/edge
    devices. In this scenario, the AI/ML platform should be able to distinguish AIML-enabled
    nodes. This can help AI/ML nodes to participate in distributed model training
    or model service and ensure unexpected interventions (e.g., interference, traffic,
    congestion, etc.) by non-AI/ML nodes. Moreover, human-motivated actions or misleading
    behaviours that can be performed on nodes not operated by AI/ML may negatively
    affect the learning process of AI/ML-enabled nodes. On the other hand, in some
    cases, especially during the autonomous learning process (e.g., during the exploration
    phase of reinforcement learning algorithms), the actions performed by AI/ML-enabled
    devices are unreliable. To avoid unexpected behaviour of telecommunication systems
    in this case, non AI/ML-enabled nodes can be used until the learning process is
    successfully completed. (xiv) Small dataset: One of the components for building
    a data engineering pipeline is the data storage ecosystem, in which HDFS plays
    a key role. However, HDFS also brings practical limitations in storing a large
    amount of small data. For this reason, the size of files that need to be stored
    in databases, or the amount of data that can be sent to a web service that feeds
    the data into the database must be adjusted accordingly via configuration parameters.
    As a result, applications that rely on HDFS (e.g., Spark jobs) slow down because
    the applications spend most of its time on I/O operations instead of focusing
    on data processing or analysis aspects. A possible solution to this problem in
    Hadoop would be to store data in SequenceFile format where each small file is
    stored in a larger single file. (xv) Testing distributed systems: Testing a distributed
    system with multiple components operating in spatially separated locations is
    usually more difficult and complicated. In a typical data engineering workflow
    used to develop a system, determining where the system fails requires additional
    investigative work as the components involved in the data pipeline grow larger.
    For a typical system, several proven types of traditional software testing must
    be performed before the system is put into production. Similar procedures can
    be used when testing data engineering pipelines. These procedures are: (i) unit
    testing (to test a small part or subset of the functionality of a data engineering
    component), (ii) regression testing (to reproduce the previously found bugs and
    fix), (iii) integration testing (to test the system, when individual data engineering
    components are integrated with each other), (iv) E2E testing (to test full functionality
    of a data engineering system in a staging environment) and (v) stress testing
    (to test the scalability limits of the data engineering system on a large scale,
    e.g., number of users supported, data traffic support, number of commands executed,
    etc.). (xvi) Lack of standardization: There are many data-driven telecommunication
    use cases in the standardization community, but no implementation details for
    real-world telecommunication network environments (e.g., in 5G and beyond). This
    can lead to significant challenges in building robust data engineering pipelines
    when extending enterprise building blocks. Although some strategies are presented
    in [220], they are limited to procedures of BDA techniques in IT with limited
    applications in the network domain. However, extensive standardization efforts
    are needed to generalize these concepts for used by a large community. (xvii)
    Backlogged data pipelines: A common challenge for all pipelines is the delays
    that occur in the ingestion pipeline. Note that in telecommunication networks,
    especially in 5G and beyond mobile networks, SLAs are very strict when time-critical
    AI/ML-based decision making processes need to be made. Thus, if any component
    of the data pipeline fails, it can lead to serious SLA misses. In networks, for
    example, the packet transmission times are on the order of milliseconds and the
    inference time of the developed models should be an order of magnitude shorter,
    otherwise there will be overhead as traffic increases. (xviii) Limited data availability:
    Unavailability of sufficient data for AI/ML training and model building purposes
    is a major challenge in almost all industrial use cases [221]. Every time new
    data emerges, it also brings new knowledge and hence needs to be integrated into
    the training process. This is also true for the telecommunication industry. At
    the same time, recent advances such as semi-supervised learning, federated learning
    or active learning can help to create larger training datasets or to introduce
    new knowledge into the training process. For a good summary of existing ML approaches
    that work with limited data see [222]. Among these algorithms, semi-supervised
    learning aims to train ML models that use both labelled and unlabeled data [223].
    These methods use a large amount of unlabeled data and proportional lack of labelled
    data to achieve an optimal result. For example, the labelling of unlabelled examples
    can be done by a semi-supervised algorithm based on their proximity to known labelled
    examples. The main advantage of this approach is that it generates additional
    labelled data that can be used to train the ML model. Therefore, semi-supervised
    learning is particularly beneficial for scenarios where more training data is
    needed. The concept of federated learning aims to distribute the copies of the
    ML algorithm to the distributed sites/devices where the data is kept, perform
    the training iterations locally, and finally send the computational results (e.g.,
    updated neural network weights) to the central repository to update the main algorithm
    [212]. The main advantage is that the data remains with the owner and the algorithms
    can still be trained on the distributed data. Active learning aims to reduce the
    amount of data required for human labelling [224]. In this learning method, a
    query-based strategy is used to select the most informative examples that a human
    operator can label. Once new examples are labelled, the ML model is updated based
    on the newly labelled examples and this process is repeated to train the model
    and improve performance. In telecommunications, unlabeled instances can be selected
    for active labeling. For example, when it is difficult to obtain enough labelled
    network fault data to find the root cause of faults in cellular networks, an active
    learning strategy can be used [225], [226]. In semi-supervised learning cases,
    auto-encoder based approaches can be used to find the root cause of faults in
    cellular networks [182]. Finally, federated learning in wireless communication
    allows each UE to build local federated learning models based on their local measurements
    and send them to BSs to build a global federated learning model [227]. Finally
    Table 8 provides a summary of the challenges described above. TABLE 8 Summary
    of the Challenges C. Future Directions and Road Ahead Today, we have enormously
    large datasets, increased computing power (GPUs, cloud, etc.), extensive open
    source software tools and increased industry investment as well as a large community
    developing new data science/engineering applications and services. Similarly,
    data applications are attracting large scale number of users and the data engineering
    ecosystem has the potential to support a larger number of users. The involvement
    of telecommunication industry in data value chain would provide strategic business
    value to telecommunication infrastructure and service providers. For this reason,
    telecommunication providers are looking forward to interacting with and benefiting
    from the data engineering ecosystem more frequently as it is open source, royalty-free
    and community-driven. At the same time, there is still much to be done to develop,
    deploy, enable operation, debug/test and extend the data applications within the
    telecommunication infrastructure. It is critical to identify the applications,
    services, and products that will benefit most from transformations of data engineering
    in the networks and IT organizations of telecommunications providers. When designing
    a data engineering pipeline in an enterprise based on use case requirements, both
    telecommunication and data engineering experts should be consulted, as their perspective
    on each use case is different and shared ideas can be of great benefit. Data infrastructure
    is already undergoing a significant architectural change [228]. Traditional data
    warehouses are moving from on-premise to cloud-based data warehouses to increase
    scalability, wide expansion, flexibility and ease of use (e.g. e-commerce data
    migration case to Google BiqQuery121), and next-generation Data Lakes are beginning
    to include more ACID-like features and interactive SQL query capabilities (e.g.
    Presto [83]). More flexible and consistent Extract-Load-Transform (ELT) pipelines
    are taking the place of traditional ETL processes, (e.g., dbt122). In the area
    of data management and orchestration, several hundred data pipelines are orchestrated
    using dataflow automation tools (e.g., with AirFlow, Dagster123). Tools like Superset
    help provide self-service insights (reports, dashboards, etc.) and are also accessible
    to non-technical users. Our survey results shows that telecommunication providers
    can move beyond the traditional boundaries of telecommunication networks (e.g.,
    RAN or OSS/BSS operations, etc.) to reap the benefits of deploying data engineering
    frameworks on an evolving data infrastructure. They can leverage the power of
    data engineering systems deployed in a distributed, scalable and optimized architecture
    for their own business needs. This would also result in lower Operating Expenditures
    (OPEX) and Capital Expenditures (CAPEX), simplify network deployment and management,
    and ensure high customer satisfaction in addition to improved value-added services.
    For optimized network management and orchestration, the coexistence of the data
    engineering frameworks described above with traditional systems at different layers
    of the network infrastructure is critical. The integration of the tools and frameworks
    of the data engineering ecosystem should serve as a complement to the traditional
    systems. For example, if the deployed data processing and analysis framework is
    not able to adequately handle the dynamic changes in the network environment,
    existing non-AI/ML-based solutions (e.g., predefined hand-crafted, and rule-based
    approaches that do not consider model-based approaches and take reactive actions
    based on human experience) can meet the new requirements. This coexistence is
    also important for security reasons. Enabling such hybrid approaches can help
    ensure rapid response in production environments. Typically, individual service
    technologies in the telecommunications world are implemented by multiple vendors
    and devices running on their network are usually locked-in and expensive. Compared
    to telecommunication infrastructure, the data engineering infrastructure is young,
    innovative, and growing rapidly. Data engineering technologies and platforms are
    evolving and improving at a rapid pace. In addition, most of the innovative and
    disruptive technologies being developed in the data engineering community are
    being released as open source. For this reason, telecommunication systems must
    be prepared to adopt and deal with these new data engineering technologies rather
    than remain in legacy systems that cannot take advantage of the data. For example,
    by starting with a simple, small E2E data engineering pipeline in a production
    environment, rather than working on a more complex data pipeline, can help to
    avoid numerous mistakes, detect errors early, and solve integration issues with
    traditional telecommunication infrastructure. In addition, AI/ML solutions do
    not have to found for every task and every problem. In many cases, simple solutions
    such as rule-based systems instead of ML systems can also help to find an intermediate
    solution. These results can be used later and iterated step by step to collect
    more data needed for more complex data engineering solutions. Moreover, the functionality
    of the data engineering pipeline can be progressively extended during this process
    through a series of iterations. For new and untested frameworks, it makes sense
    for organizations to use public cloud resources first (e.g., AWS, GCP or Azure)
    and then move to on-premises resources once a stable definition of workload pipeline
    is in place. AI/ML algorithms are predicted to be integrated into telecommunication
    networks in the next decade. There are more and more number of use cases for real-time
    data, and the systems that process this data should be mature for the requirements
    of telecommunication systems. Network management and orchestration based on data
    engineering can be used to track evolving traffic patterns, user behaviour, etc.,
    and take these trends into account in the planning and operational phases. It
    is important to choose a modular system that can cover multiple use cases. As
    a starting point, there is no need to reinvent the wheel, as there is a good chance
    that an existing tool/framework can support initial efforts to integrate AI/ML
    systems into the telecommunication specific applications (both in IT and network).
    Familiarity with the data engineering ecosystem and tools/frameworks, diversity
    of expertise across technologies, and integration skills in bringing disparate
    pieces together into new telecommunication applications will be very useful for
    network-focused product and service development teams. Moving forward, a few useful
    questions to consider are: How easily can a new framework or approach be integrated
    and tested on large scale in the telecommunication infrastructure? How accurately
    can the impact of the new changes/ updates in the data engineering pipeline be
    measured in telecommunication infrastructure to avoid system complexity, poor
    resource utilization or degradation of the KPIs for a particular service? Does
    the improvement of a framework in the pipeline affect or degrade other components
    in the data engineering pipeline and telecommunication infrastructure while keeping
    maintenance tasks at a low level? How does the addition of each framework in the
    data engineering pipeline in an integrated environment impact an organization’s
    policy standards (e.g., data storage, compliance and regulations, security, network,
    operations, management, data traffic flow, servers, workloads, legacy applications,
    or reporting)? How quickly can the network engineers of the telecommunication
    world and the data engineers of the data engineering world would be brought together
    to accelerate the process? SECTION XIII. Conclusion The data engineering ecosystem
    will inevitably play an important role in next generation network management and
    orchestration systems. In this tutorial paper, we highlight the recent advances
    in data engineering based networks to meet the needs of network management and
    orchestration. We first provide a comprehensive analysis of existing frameworks
    and platforms, and then focus on recent standardization activities. Finally, we
    discuss the gaps, challenges, and future directions in building a data engineering-oritented
    networking system for telecommunication networks. Our tutorial analysis shows
    that data engineering frameworks can be used for a variety of purposes, ranging
    from data ingestion to data visualization, enabling telecommunication network
    operators to leverage the data generated by their users, environment, or network
    equipment. Authors Figures References Citations Keywords Metrics Footnotes More
    Like This Managing Big Data Stream Pipelines Using Graphical Service Mesh Tools
    2021 IEEE Cloud Summit (Cloud Summit) Published: 2021 Some key problems of data
    management in army data engineering based on big data 2017 IEEE 2nd International
    Conference on Big Data Analysis (ICBDA) Published: 2017 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/9668973/09743922.pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Recent Advances in Data Engineering for Networking
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.54368/qijirse.2.1.0007
  analysis: '>'
  authors:
  - A. Karunamurthy
  - Mayank Yuvaraj
  - J. Shahithya
  - V. Thenmozhi
  citation_count: 0
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 'Quing: International Journal of Innovative Research in Science and Engineering'
  limitations: '>'
  pdf_link: null
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: 'Cloud Database: Empowering Scalable and Flexible Data Management'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1101/2020.09.13.274779
  analysis: '>'
  authors:
  - Ben Blamey
  - Salman Toor
  - Martin Dahlö
  - Håkan Wieslander
  - P J Harrison
  - Ida‐Maria Sintorn
  - Alan Sabirsh
  - Carolina Wählby
  - Ola Spjuth
  - Andreas Hellander
  citation_count: 0
  full_citation: '>'
  full_text: ">\nJournal of XYZ, 2017, 1–11\ndoi: xx.xxxx/xxxx\nManuscript in Preparation\n\
    Paper\nP A P E R\nRapid development of cloud-native intelligent data\npipelines\
    \ for scientifc data streams using the\nHASTE Toolkit\nBen Blamey1,*, Salman Toor1,\
    \ Martin Dahlö2,3, Håkan Wieslander1, Philip J\nHarrison2,3, Ida-Maria Sintorn1,3,4,\
    \ Alan Sabirsh5, Carolina Wählby1,3, Ola\nSpjuth2,3,† and Andreas Hellander1,†\n\
    1Department of Information Technology, Uppsala University, Sweden and 2Department\
    \ of Pharmaceutical\nBiosciences and Science for Life Laboratory, Uppsala University,\
    \ Sweden and 3Science for Life Laboratory,\nUppsala University and 4Vironova AB,\
    \ Stockholm, Sweden and 5Advanced Drug Delivery, Pharmaceutical\nSciences, R&D,\
    \ AstraZeneca, Gothenburg, Sweden\n*Correspondence: ben.blamey@it.uu.se\n†Co-senior\
    \ authors\nAbstract\nThis paper introduces the HASTE Toolkit, a cloud-native software\
    \ toolkit capable of partitioning data streams in order to\nprioritize usage of\
    \ limited resources. This in turn enables more efcient data-intensive experiments.\
    \ We propose a model\nthat introduces automated, autonomous decision making in\
    \ data pipelines, such that a stream of data can be partitioned\ninto a tiered\
    \ or ordered data hierarchy. Importantly, the partitioning is online and based\
    \ on data content rather than a priori\nmetadata. At the core of the model are\
    \ interestingness functions and policies. Interestingness functions assign a quantitative\n\
    measure of interestingness to a single data object in the stream, an interestingness\
    \ score. Based on this score, a policy\nguides decisions on how to prioritize\
    \ computational resource usage for a given object. The HASTE Toolkit is a collection\
    \ of\ntools to adapt data stream processing to this pipeline model. The result\
    \ is smart data pipelines capable of efective or even\noptimal use of e.g. storage,\
    \ compute and network bandwidth, to support experiments involving rapid processing\
    \ of\nscientifc data characterized by large individual data object sizes. We demonstrate\
    \ the proposed model and our toolkit\nthrough two microscopy imaging case studies,\
    \ each with their own interestingness functions, policies, and data hierarchies.\n\
    The frst deals with a high content screening experiment, where images are analyzed\
    \ in an on-premise container cloud\nwith the goal of prioritizing the images for\
    \ storage and subsequent computation. The second considers edge processing of\n\
    images for upload into the public cloud for a real-time control loop for a transmission\
    \ electron microscope.\nKey words: Stream Processing, Interestingness Functions,\
    \ HASTE, Tiered Storage, Image Analysis\nIntroduction\nLarge datasets are both\
    \ computationally and fnancially expen-\nsive to process, transport and store.\
    \ Such datasets are ubiq-\nuitous throughout the life sciences, including imaging,\
    \ where\ndiferent types of microscopy are used to e.g.\nobserve and\nquantify\
    \ efects of drugs on cell morphology. Modern imag-\ning techniques can generate\
    \ image streams at rates of up to\n1TB/hour [1]. Clearly, the processing, storing\
    \ and communi-\ncation of these images can be slow, resource-intensive and ex-\n\
    pensive, efectively becoming a bottleneck to scale experiments\nin support of\
    \ data-driven life science. Another prominent ex-\nCompiled on: September 13,\
    \ 2020.\nDraft manuscript prepared by the author.\n1\npreprint (which was not\
    \ certified by peer review) is the author/funder. All rights reserved. No reuse\
    \ allowed without permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\n2\n|\n\
    Journal of XYZ, 2017, Vol. 00, No. 0\nKey Points\n• We propose a pipeline model\
    \ for building intelligent pipelines for streams, accounting for actual information\
    \ content in\ndata rather than a priori metadata, and present the HASTE Toolkit,\
    \ a cloud-native software toolkit for supporting rapid\ndevelopment according\
    \ to the proposed model.\n• We demonstrate how the HASTE Toolkit enables intelligent\
    \ resource optimization in two image analysis case studies based\non a) high-content\
    \ imaging and b) transmission electron microscopy.\n• We highlight the challenges\
    \ of storage, processing and transfer in streamed high volume, high velocity scientifc\
    \ data for\nboth cloud and cloud-edge use cases.\nample is human genome sequencing,\
    \ where the global storage\nrequirements is predicted to be between 2 and 40 exabytes\
    \ (1\nexabyte = 1018 bytes) by 2025, and with modern techniques\ngenerating data\
    \ at the order of ~60 GB/h [2].\nSimilarly, in\nlarge-scale modeling, a single\
    \ computational experiment in a\nsystems biology context can generate terabytes\
    \ of data [3].\nThis work is motivated by some of the most critical as-\npects\
    \ of scalable scientifc discovery for spatial and temporal\nimage data. There\
    \ are two primary concerns: (1) not all data\nis equally valuable. With datasets\
    \ outgrowing resources, data\nstorage should be prioritized for data that is most\
    \ relevant (or\ninteresting) for the study at hand and poor quality, or unin-\n\
    teresting, data (e.g. out-of-focus images) should be discarded\nor archived; (2)\
    \ when resources are limited, or if decisions are\nrequired in real-time, we have\
    \ to be smart about how the data\nis (pre)processed and which subsets of the data\
    \ are stored for\nmore detailed (and potentially computer intensive) analysis\
    \ –\nprioritizing more interesting subsets of the data.\nThe general challenges\
    \ of management and availability of\nlarge datasets are often popularized and\
    \ summarized trough\nthe so-called Vs of big data. Initially, the focus was on\
    \ the three\nVs: velocity, volume and variety, but this list has since grown\n\
    with the increasing number of new use-cases to also include Vs\nsuch as veracity,\
    \ variability, virtualization and value [4]. Dur-\ning the last decade, a number\
    \ of frameworks have been de-\nsigned to address these challenges, ofering reliable,\
    \ efcient\nand secure large-scale data management solutions. However,\naccording\
    \ to a white paper published by IDC [5], only 30% of\nthe generated data is in\
    \ the form that it can be efciently ana-\nlyzed. This highlights the current gap\
    \ between large-scale data\nmanagement and efcient data analysis. To close this\
    \ gap, it\nis essential to design and develop intelligent data management\nframeworks\
    \ that can help organize the available datasets for\nefcient analyses. In this\
    \ work, we address this challenge by\nproposing a model that helps a data pipeline\
    \ developer make\nonline decisions about individual data objects’1 priority based\n\
    in actual information content, or interestingness, rather than\ntraditional metadata.\n\
    A range of existing work in life science applications has dis-\ncussed the challenges\
    \ of transporting, storing and analyzing\ndata, often advocating a streamed approach.\
    \ In [6] the authors\nexplicitly discuss the constraints of cloud upload bandwidth,\n\
    and its efect on overall throughput for mass-spectrometry\nbased metabolomics.\n\
    In their application, uploading large\ndatasets from the instrument to the cloud\
    \ represents a bottle-\nneck and they advocate a stream-based approach with online\n\
    analysis where data is processed when it arrives, rather than\nwaiting for upload\
    \ of the complete dataset. Hillman et al. [7]\ndeveloped a stream based pipeline\
    \ with Apache Flink and Kafka\nfor processing of proteomics data from liquid chromatography-\n\
    1 We use the generic term data object but note that analogous terms in\nvarious\
    \ contexts include: documents, messages and blobs\nmass spectrometry (LC/MS) and\
    \ note the advantages of a real-\ntime approach to analysis: “a scientist could\
    \ see what is hap-\npening in real-time and possibly stop a problematic experi-\n\
    ment to save time”. Zhang et al. [8] developed a client/server\napplication for\
    \ interactive visualization of MS spectra, adopt-\ning a stream-based approach\
    \ to achieve better user interactiv-\nity. In genomics, [9] presented the htsget\
    \ protocol to enable\nclients to download genomic data in a more fne-grained fash-\n\
    ion, and allow for processing chunks as they come from the\nsequencer. In [10],\
    \ the authors note that a single electron mi-\ncroscope can produce 1 TB of images\
    \ per day, requiring a min-\nimum of 1000 CPU hours for analysis. Adapting their\
    \ Scipion\nsoftware [11] (intended for Cryo EM image analysis) for use in\nthe\
    \ cloud, they discuss the challenges of data transfer to/from\nthe cloud, comparing\
    \ transfer rates for diferent providers. [12]\nproposes excluding outliers in\
    \ streaming data, using an ‘Outlier\nDetection and Removal’ (ODR) algorithm which\
    \ they evaluate\non fve bioinformatics datasets.\nRather than handling one particular\
    \ type of data or dealing\nwith a specifc data pipeline, the aim of the present\
    \ work is\nto distill efective architectural patterns into a pipeline model\n\
    to allow for repeatable implementations of smart systems ca-\npable of online\
    \ resource prioritization in scenarios involving\nlarge-scale data production,\
    \ such as from a scientifc instru-\nment. Computers in the lab connected directly\
    \ to such an in-\nstrument, used together with cloud resources, are an exam-\n\
    ple of edge computing [13]. Under that paradigm, computa-\ntional resources outside\
    \ the cloud (such as mobile devices, and\nmore conventional compute nodes) are\
    \ used in conjunction\nwith cloud computing resources to deliver benefts to an\
    \ appli-\ncation such as reduced cost, better performance, or improved\nuser experience.\
    \ General computer science challenges include\nsecurity, deployment, software\
    \ complexity, and resource man-\nagement/workload allocation. In our context,\
    \ the streams of\nlarge data objects generated by scientifc instruments create\n\
    particular challenges within the edge computing paradigm as\nthe data often needs\
    \ to be uploaded to the cloud for process-\ning, storage, or wider distribution.\
    \ Whilst limited compute re-\nsources at the edge are often insufcient for low-latency\
    \ pro-\ncessing of these datasets, intelligent workload allocation can\nimprove\
    \ throughput (as discussed in Case Study 2).\nIn this paper we propose a pipeline\
    \ model for partitioning\nand prioritizing stream datasets into data hierarchies\
    \ (DHs) ac-\ncording to an interestingness function (IF) and accompanying pol-\n\
    icy, applied to objects in the stream, for more efective use of\nhardware (in\
    \ edge and cloud contexts).\nWe present this as\na general approach to mitigating\
    \ resource management chal-\nlenges, with a focus on image data. Our model allows\
    \ for au-\ntonomous decision making, while providing a clear model for\ndomain\
    \ experts to manage the resources in distributed systems\n– by encoding domain\
    \ expertise via the IF. To that end, this pa-\npreprint (which was not certified\
    \ by peer review) is the author/funder. All rights reserved. No reuse allowed\
    \ without permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\nBlamey\
    \ et al.\n|\n3\nper introduces the HASTE2 Toolkit, intended for developing in-\n\
    telligent stream processing pipelines based on this model. Two\ncase studies presented\
    \ in this paper document how microscopy\npipelines can be adapted to the HASTE\
    \ pipeline model.\nWhilst the core ideas of intelligent data pipelines in HASTE\n\
    is generally applicable to many scenarios involving scien-\ntifc datasets, we\
    \ here focus on case-studies involving image\nstreams, in particular from microscopy.\n\
    Background: Stream Processing and Workfow\nManagement\nA fundamental component\
    \ to the pipelines presented in this\npaper is a stream processing engine.\nSystems\
    \ for stream\nprocessing are generally concerned with high frequency mes-\nsage\
    \ infux, and those objects can be small in size, such as\na few KB. Examples of\
    \ such data objects include sensor read-\nings from IoT devices (such as MQTT\
    \ messages), those gener-\nated from telecoms, web and cloud applications, e-commerce\n\
    and fnancial applications, or the aggregation and analysis\nlog entries.\nWell-known\
    \ examples of mature, enterprise-\ngrade frameworks for cloud-based stream processing\
    \ in these\ncontexts include Apache Flink, Apache Spark Streaming, and\nApache\
    \ Log Flume. Resilience and fault tolerance are key fea-\ntures of these frameworks\
    \ (often achieved with various forms\nof redundancy and replication). These frameworks\
    \ are com-\nmonly used in conjunction with various queuing applications,\ne.g.,\
    \ Apache Kafka, and vendor-specifc products such as AWS\nKinesis – these also\
    \ include basic processing functionality.\nWhilst the maturity, support, documentation,\
    \ features and\nperformance (order of MHz message processing throughput)\nboasted\
    \ by these frameworks is attractive for scientifc com-\nputing application, streamed\
    \ scientifc data (and its process-\ning) tends to have diferent characteristics:\
    \ data objects used in\nscientifc computing applications (such as microscopy images,\n\
    and matrices from other scientifc computing domains) can be\nlarger in size, which\
    \ can create performance issues when inte-\ngrated with these enterprise frameworks\
    \ described above [14].\nFor example, data object sizes in imaging applications\
    \ could be\na few MB.\nTo address this gap, we have previously developed and re-\n\
    ported on a stream processing framework focusing on scien-\ntifc computing applications,\
    \ HarmonicIO [15].\nHarmonicIO\nsacrifces some of these features, and is intended\
    \ for lower-\nfrequency applications (towards kHz, not MHz), and was able\nto\
    \ achieve better streaming performance under some condi-\ntions in one study for\
    \ larger message sizes [14]. The HASTE\nToolkit has been developed with images\
    \ as the primary use-\ncase and for this reason HarmonicIO is the default supported\n\
    streaming framework in the toolkit. However, we stress here\nthat in principle\
    \ any streaming framework can be used.\nFurthermore,\nunder\nthe\nemerging\nedge\n\
    computing\nparadigm,\nthere are some stream processing frameworks\navailable,\
    \ often focusing on traditional IoT use-cases. Being\nin their infancy, efective\
    \ automated scheduling and operator\nplacement in hybrid edge/cloud deployment\
    \ scenarios remains\nan open research challenge for this context. Within this\
    \ area,\nthere is signifcant research efort concerning real-time video\nanalysis,\
    \ where images collected at the edge (from cameras)\nare streamed to the cloud\
    \ for analysis – some degree of lossy\ncompression is typically used in such applications.\n\
    By contrast, workfow frameworks are broad class of soft-\nware frameworks intended\
    \ to facilitate the development of\n2 HASTE: Hierarchical Analysis of Spatial\
    \ and Temporal Data http://haste.\nresearch.it.uu.se/\ndata-processing pipelines.\
    \ There are a large number of such\nframeworks (more than 100 are listed in [16]).\
    \ In such frame-\nworks, one generally defnes processing operations (often as\n\
    the invocation of external processes), which are triggered by\nevents such as\
    \ the creation of a new fle on disk, or a commit be-\ning pushed to a Git repository.\
    \ Such frameworks generally han-\ndle large numbers of fles, of arbitrary size,\
    \ and often include\nsome degree of fault tolerance. But in contrast to stream\
    \ pro-\ncessing frameworks, they may lack functionality specifc for\nstreams,\
    \ such as window operations, more complex schedul-\ning and placing of operators,\
    \ and are generally intended for\nhigher latency and/or lower ingress rates (than\
    \ the 100kHz+\nrange of the stream processing frameworks described above),\nand\
    \ are often fle-system centric, with objects being written\nback to disk between\
    \ each processing step.\nThe HASTE toolkit attempts to fll a gap between these\
    \ two\nclasses of software (stream processing frameworks, and workfow\nmanagement\
    \ systems): applications where latency and high data\nobject throughput are important\
    \ (and use of a flesystem as a\nqueuing platform are perhaps unsuitable for that\
    \ reason), but\nnot as high as some enterprise stream processing applications;\n\
    whilst being fexible enough to accommodate a broad range of\nintegration approaches,\
    \ processing steps with external tools,\nand the large message sizes characteristic\
    \ of scientifc comput-\ning applications.\nThe priority-driven approach of the\
    \ HASTE pipeline model\nreconciles the resource requirements of life science pipelines\n\
    (characterised by streams relatively of large message, with ex-\npensive per-message\
    \ processing steps), with the requirements\nfor low-latency and high throughput,\
    \ allowing for real time hu-\nman supervision, inspection, interactive analysis\
    \ – as well as\nreal-time control of laboratory equipment.\nHASTE Pipeline Model\n\
    The key ideas of the HASTE pipeline model are the use of inter-\nestingness functions\
    \ and a policy to autonomously induce data\nhierarchies. These structures are\
    \ then used to manage and op-\ntimize diferent objectives such as communication,\
    \ processing\nand storage of the datasets. The HASTE Toolkit enables rapid\nconstructions\
    \ of smart pipelines following this model. Central\nto the approach is that decisions\
    \ are made based on actual data\ncontent rather than on a priori metadata associated\
    \ with the\ndata objects. The following subsections introduces the compo-\nnents\
    \ of the pipeline model.\nOverview\nFigure 1 illustrates the proposed HASTE model\
    \ and logical\narchitecture.\nOne or more streaming data sources generate\nstreams\
    \ of data objects. The stream then undergoes feature\nextraction (relevant to\
    \ the context) – this data extraction can\nbe performed in parallel, as an idempotent\
    \ function of a sin-\ngle object. The intention is that computationally cheap\
    \ initial\nfeature extraction can be used to prioritize subsequent, more\nexpensive,\
    \ downstream processing.\nAn Interestingness Function (IF) computes an interesting-\n\
    ness score for each object from these extracted features. This\ncomputation can\
    \ be a simple procedure, e.g. to nominate one\nof the extracted features as the\
    \ interestingness score associ-\nated with the data object. In more complex cases\
    \ it can also be\na machine learning model trained either before the experiment\n\
    or online during the experiment that generates the stream. Fi-\nnally, a policy\
    \ is applied which determines where to store the\nobject within a Data Hierarchy\
    \ (DH), or send it for further\ndownstream processing, based on the interestingness\
    \ scores.\npreprint (which was not certified by peer review) is the author/funder.\
    \ All rights reserved. No reuse allowed without permission. \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September\
    \ 14, 2020. The copyright holder for this\n4\n|\nJournal of XYZ, 2017, Vol. 00,\
    \ No. 0\nFigure 1. Logical Architecture for the HASTE pipeline model. A stream\
    \ of data objects is generated by one or more streaming sources (such as a microscope).\
    \ These\nobjects undergo online, automated feature extraction, and an IF is applied\
    \ with the extracted features as input. This associates an interestingness score\
    \ with each\nobject in the stream. A user-defned policy is then used to organize\
    \ the data objects into a data hierarchy to be used for optimizing subsequent\
    \ communication,\nstorage and downstream processing.\nInterestingess Functions\n\
    The IF is a user provided function, to be applied to the extracted\nfeatures from\
    \ the data objects in the stream. The purpose of\nthe IF is to associate an interestingness\
    \ score with each object.\nExamples of IFs in image analysis contexts could be\
    \ features\nrelating to image quality, detected phenomena in images, etc.\nThe\
    \ computed IF score is used for determining priority for\nsubsequent processing,\
    \ communication, and/or storage of that\nobject. In this sense, IFs have some\
    \ similarities to the con-\ncept of document (data object) ‘hotness’ in tiered\
    \ storage con-\ntexts, where a more recently accessed ’hot’ document would\nbe\
    \ stored in a high-performance tier. Whilst much of that line\nof work uses only\
    \ fle-system information, other work takes\nsome consideration of the application\
    \ itself, for example [17]\nmodel access as a Zipf distribution, for a review\
    \ see [3].\nOur present work generalizes the concept of ‘hotness’, in a\nnumber\
    \ of ways: (1) our IFs always take consideration of se-\nmantics at the level\
    \ of the scientifc application – in the case\nof microscopy imaging this could\
    \ be image focus, or quality\nfeatures – perhaps combined with business logic\
    \ for particular\ncolor channels, etc. – rather than fle system semantics (such\n\
    as fle access history). This approach allows an immediate, on-\nline decision\
    \ about the object’s interestingness – rather then\ninferring it from subsequent\
    \ access patterns. (2) Tiered stor-\nage is just one potential application of\
    \ HASTE: we use IFs to\nprioritize data objects for storage, compute, and communica-\n\
    tion (3) with HASTE, the intention is that users’ confgure IFs\nthemselves, together\
    \ with the associated policy. Currently, the\noutput of the IF is scalar valued.\
    \ This is intended to assure\nsmooth integration in cases where the IF is a machine\
    \ learnt\nmodel, outputting a probability, rank, or some other statistical\nmeasure.\n\
    Further,\nwe propose general software abstractions for\nthese ideas, and demonstrate\
    \ the potential benefts of online\ninterestingness-based prioritization in two\
    \ case studies: both\nin terms of the optimization of various resources (compute,\n\
    communication, storage), but also from an experimental and\nscientifc viewpoint\
    \ – selecting the best data (or outliers) for\ninspection and further analysis.\n\
    Policies for inducing Data Hierarchies\nIn applications utilizing tiered storage,\
    \ more interesting data\nobjects would be saved in higher performance, more expen-\n\
    sive tiers – readily accessible for downstream processing (while\nless interesting\
    \ objects could be cheaply archived) – explored\nin Case Study 1. Whereas, in\
    \ an edge computing contexts, we\nmay want to prioritize data objects for computation\
    \ at the cloud\nedge, to make more efective use of that resource – explored\n\
    in Case Study 2. In both cases we refer to these structures as\ndata hierarchies\
    \ (DHs). In a HASTE pipeline DHs are ‘induced’\nwithin the source data by the\
    \ IF and a policy. The policy takes\nthe interestingness score as input and applies\
    \ a set of rules to\ndetermine how an object is placed within the DH, for exam-\n\
    ple, its allocation within a tiered storage system; or where it\nshould be stored\
    \ or processed downstream.\nListing 1 shows\nhow a user can defne a policy, a\
    \ simple dictionary mapping\nintervals of interestingness scores to the tiers\
    \ (which are con-\nfgured separately). In this paper we demonstrate two forms\
    \ of\npolicy: the interval model mentioned above (where the tier is\ndetermined\
    \ directly from the interestingness score, Case Study\n1) and a priority-based\
    \ policy, where data objects are queued\n(according to their interestingness)\
    \ for upload and processing\n(as in Case Study 2).\nA beneft of the HASTE pipeline\
    \ model is the clear role sepa-\nration – all the domain-specifc knowledge is\
    \ efectively encap-\nsulated within the IF whilst the choice of how to form DHs\
    \ and\noptimize storage tier allocation is encapsulated entirely within\nthe policy.\
    \ This allows the scientifc question of what consti-\ntutes an interesting data\
    \ object, and the computing infrastruc-\npreprint (which was not certified by\
    \ peer review) is the author/funder. All rights reserved. No reuse allowed without\
    \ permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\nBlamey\
    \ et al.\n|\n5\nture, or indeed, budgetary, concerns of how to make best use of\n\
    computing resources (including storage), to be separated and\nworked on by team\
    \ members with diferent expertise. Impor-\ntantly, this de-coupling allows the\
    \ possibility for IFs to be re-\nused among scientists, and between contexts where\
    \ the data\nmay be similar, but the dataset size, and available computing\ninfrastructure,\
    \ may be diferent.\nThe HASTE Toolkit\nThe HASTE Toolkit implements the core functionality\
    \ needed\nfor rapidly constructing smart pipelines based on the proposed\nmodel.\n\
    HASTE Storage Client\nThe HASTE Storage Client (HSC) serves as the main entry-\n\
    point for the user.\nIt is confgured with the IF, the pol-\nicy, and the confguration\
    \ associated with the tiers, and pro-\ncesses each data object arriving in the\
    \ stream. It can be in-\nstalled as a standalone Python module (see: https://github.\n\
    com/HASTE-project/HasteStorageClient, version 0.13 was used\nfor this study.).\
    \ It allows a DH to be realized within HASTE\nas tiered storage.\nIt is a library\
    \ with the core prioritiza-\ntion functionality:\nit invokes the IF on incoming\
    \ data ob-\njects,\nand applies the policy to form the data hierarchy.\nThe extracted\
    \ features are used to compute interestingness\nscores,\nalong with other metadata\
    \ and logging info,\nare\nsaved in a database by the HSC. It is intended to be\
    \ adopted\nwithin the Python-based stream processing framework of\nchoice, an\
    \ example can be found at:\nhttps://github.com/\nHASTE-project/HasteStorageClient/blob/master/example.py.\
    \ An\nexisting pipeline can be adapted to use HASTE according to the\nfollowing\
    \ steps:\n• Install\nthe\nHSC\nfrom\nPyPI\npip install\nhaste-storage-client,\
    \ or from source.\n• Confgure\none\nor\nmore\nstorage\ntiers\n(on\na\nHASTE-\n\
    compatible storage platform)3.\n• Defne an IF for the context – it can use spatial,\
    \ temporal\nor other metadata associated with the data object.\n• Run feature\
    \ extraction on the object prior to invoking the\nHSC.\n• Deploy a MongoDB instance.\n\
    The scripts https://github.\ncom/HASTE-project/k8s-deployments/ can be adapted\
    \ for this\npurpose.\nOther key components of the HASTE Toolkit\nThis section\
    \ lists other various components in the HASTE\ntoolkit, and describes how they\
    \ relate to the key ideas of IFs,\ndata hierarchies (DHs) and policies.\nThe HASTE\
    \ Agent: A command-line application (developed\nfor the microscopy use case in\
    \ Case Study 2), which uploads\nnew documents on disk to the cloud, whilst performing\
    \ intel-\nligently prioritized pre-processing of objects waiting to be up-\nloaded,\
    \ so as to minimize the overall upload duration.\n(see:\nhttps://github.com/HASTE-project/haste-agent\
    \ (v0.1 was used\nfor this study)). The functionality of this tool is discussed\
    \ in\ndetail in Case Study 2.\nThe HASTE Gateway: Cloud gateway service, which\
    \ receives\ndata objects in the cloud, and forwards them for further pro-\n3 At\
    \ the time of writing, supported platforms are: OpenStack Swift, Pachy-\nderm\
    \ [18], and POSIX-compatible flesystems.\ncessing. Deployed as a Docker container.\
    \ (see: https://github.\ncom/HASTE-project/haste-gateway, v0.1 was used in this\
    \ study.).\nThe HASTE Report Generator:\nAn auxiliary command\nline\ntool\nfor\n\
    exporting\ndata\nfrom\nthe\nExtracted\nFea-\ntures Database.\n(see:\nhttps://github.com/HASTE-project/\n\
    haste-report-generator).\nThe Extracted Features Database. MongoDB is used by\
    \ the\nHASTE Storage Client to hold a variety of the metadata: ex-\ntracted features,\
    \ interestingness scores, and tier/DH allocation.\nTiered Storage. Tiered storage\
    \ is one way that a data hierar-\nchy (DH) can be realized. The HSC allows existing\
    \ storage to be\norganized into a tiered storage system, where tiers using vari-\n\
    ous drivers built into the HSC can be confgured. In Case Study\n2 the tiers are\
    \ flesystem directories, into which image fles are\nbinned according to the user-defned\
    \ policy. The idea is that in\nother deployments, less expensive disks/cloud storage\
    \ could be\nused for less interesting data. Note that the policy can also send\n\
    data deemed unusable (e.g. quality below a certain threshold)\ndirectly to trash.\
    \ Tiered storage drivers are managed by the\nHASTE storage client.\nOur\nGitHub\n\
    project\npage\n(https://github.com/\nHASTE-project)\nshowcases\nother\ncomponents\n\
    relating\nto\nvarious example pipelines developed within the HASTE project,\n\
    including IFs developed for specifc use cases as well as scripts\nfor automated\
    \ deployment.\nExperiments and Results\nIn this section we illustrate the utility\
    \ of the toolkit in two\nreal-world case studies chosen to demonstrate how the\
    \ HASTE\npipeline model can be realized in practice to optimize resource\nusage\
    \ in two very diferent infrastructure and deployment sce-\nnarios. Table 1 summarizes\
    \ the objectives of the case studies.\nCase Study 1 concerns data management for\
    \ a high-content\nscreening experiment in a scientifc laboratory at Uppsala Uni-\n\
    versity, Sweden. A small on-premises compute cluster running\nKubernetes [19]\
    \ provides the necessary infrastructure to handle\nthe immediate data fow from\
    \ the experiment, but both stor-\nage capacity and manual downstream analysis\
    \ is a concern. We\nuse the HASTE toolkit to build a pipeline that captures the\
    \ input\ndata as an image stream and bins images into tiers according to\nimage\
    \ quality. The overall goal is to organize the images into\ntiers for subsequent\
    \ processing, to both ensure that the best\nimages are allocated to high performance\
    \ storage for high per-\nformance analysis, and to help the scientist prioritize\
    \ manual\nwork to appropriate subsets of data.\nCase Study 2 concerns processing\
    \ an image stream from\na transmission electron microscope (TEM). During streaming,\n\
    there is an opportunity to pre-process images using a desktop\nPC co-located with\
    \ the microscope, before being uploaded for\nstream processing in the cloud. This\
    \ is an example of an edge\ncomputing [13]) scenario, where very limited but low-latency\n\
    local infrastructure is leveraged together with a large cloud in-\nfrastructure.\
    \ The ultimate goal is real-time control of the mi-\ncroscope (see Figure 6),\
    \ and consequently end-to-end latency\nis a key concern. This latency is constrained\
    \ by image upload\ntime. Here we develop a pipeline using the HASTE tools with\
    \ an\nIF that predicts the efectiveness of pre-processing individual\nimages at\
    \ the edge prior to cloud upload.\nCase Study 1 - Smart data management for high-\n\
    content imaging experiments\nThis case study focuses on adoption of the HASTE\
    \ toolkit in a\nhigh-content microscopy setting – the input is a stream of im-\n\
    ages arriving from an automated microscope. This deployment\npreprint (which was\
    \ not certified by peer review) is the author/funder. All rights reserved. No\
    \ reuse allowed without permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\n6\n|\n\
    Journal of XYZ, 2017, Vol. 00, No. 0\nTable 1. Overview of the two case studies\
    \ used in this paper.\nCase Study 1 - Cell Profling\nCase Study 2 - Real Time\
    \ Processing with a TEM\nApplication\nHigh-Content Imaging\nReal Time Control\
    \ of Microscopy\nPrioritization of...\nStorage\nCommunication & Compute\nGoal\n\
    Tiered Storage\nReduce end-to-end latency for cloud upload\nDeployment Setting\n\
    On-premises Cloud, Kubernetes\nCloud Edge & Public Cloud (SNIC)\nInterestingness\
    \ Function (IF)\nCellProfler Pipeline – Image Quality\nEstimation of Size Reduction\
    \ (Sampling, Splines)\nPolicy\nFixed Interestingness Thresholds\nDynamic Interestingness\
    \ Rank\nFigure 2. Architecture for Case Study 1. In this case study, the DH is\
    \ realized as\nstorage tiers. Images streamed from the microscope are saved to\
    \ disk (Network\nAttached Storage). This disk is polled by the ‘client’, which\
    \ pushes a message\nabout the new fle to RabbitMQ. Workers pop these messages\
    \ from the queue,\nanalyze the image, and move it to the storage tiers confgured\
    \ in the data\nhierarchy, using the HASTE Storage Client, confgured with an appropriate\
    \ IF\nand Policy. Icons indicate the components running as Kubernetes pods.\n\
    uses an on-premises compute cluster running Kubernetes with\na local NAS. While\
    \ we want online analysis, we consider this a\n‘high latency’ application – images\
    \ can remain unprocessed for\nsome seconds or minutes until compute resources\
    \ are available.\nThis is a contrast to Case Study 2, where low-latency process-\n\
    ing is a goal.\nImage quality is an issue in microscopy: images that have\ndebris,\
    \ are out of focus, or unusable for some other reason re-\nlating to the experimental\
    \ setup. Such images can disrupt sub-\nsequent automated analysis and are distracting\
    \ for human in-\nspection. Furthermore, their storage, computation and trans-\n\
    portation have avoidable performance and fnancial costs.\nFor this case study,\
    \ the HASTE toolkit is used to prioritize\nstorage. The developed IF is a CellProfler\
    \ pipeline performing\nout of focus prediction using the imagequality plugin [20]).\
    \ The\nPolicy is a fxed threshold used to bin images into a DH accord-\ning to\
    \ image quality. See Table 1 for an overview of the case\nstudies.\nFigure 2 illustrates\
    \ the key components of the architecture:\n• Client – monitors the source directory\
    \ for new image\nfles, adding the name of each fle to the queue.\n(see:\nhttps://github.com/HASTE-project/cellprofiler-pipeline/\n\
    tree/master/client, v3 was used for this study).\n• Queue – a RabbitMQ queue to\
    \ store flenames (and associ-\nated metadata). Version 3.7.15 was used for this\
    \ study.\n• Worker – waits for a flename message (on the queue), runs\na CellProfler\
    \ pipeline on it, computes an interestingness\nscore from the CellProfler features\
    \ (according to a user-\ndefned function). (see: https://github.com/HASTE-project/\n\
    cellprofiler-pipeline/tree/master/worker, v3 was used for\nthis study)\nThe deployment\
    \ scripts for Kubernetes & Helm used to\ndeploy these services for this study\
    \ are available at: https:\n//github.com/HASTE-project/k8s-deployments, v1.1 was\
    \ used.\nThe image; together with its interestingness score and\nmetadata are\
    \ passed to the HASTE Storage Client – which allo-\ncates the images to Tiered\
    \ Storage/DH, and saves metadata in\nthe the Extracted Feature Database. Each\
    \ image is processed\nindependently, which simplifes scaling.\nThe HASTE toolkit\
    \ simplifes the development, deployment\nand confguration of this pipeline – in\
    \ particular, the interac-\ntion between the flesystems used in the input image\
    \ stream\nand archive of the processed images. When using our image\nprocessing\
    \ pipeline, user efort is focused on (a) defning a\nsuitable IF and (b) defning\
    \ a policy which determines how\nthe output of that function relates to DH allocation\
    \ (storage\ntiers). Both of these are declared within the Kubernetes de-\nployment\
    \ script. When developing the pipeline itself, one is\nable to provide the interestingness\
    \ score (the output of the IF),\nand the policy as arguments to the HASTE tools,\
    \ and delegate\nresponsibility to applying the policy (with respect to the stor-\n\
    age tiers), recording all associated metadata to the Extracted\nFeature Database.\n\
    The client, queue and workers are all deployed in Docker\ncontainers. Auto-scaling\
    \ is confgured for the workers: they\nare scaled up when processing images, and\
    \ scaled back down\nagain when idle. A message containing the image flenames\n\
    (and other metadata) is queued, but the fle content is read\nfrom the NAS for\
    \ processing and tiering.\nThe code for the worker is an extension of Distributed-\n\
    CellProfler (released as part of CellProfler v3.0) [21]4, which\nit to run within\
    \ AWS5. The key beneft of our containerized\nsystem is that because it runs in\
    \ Docker, and is not dependent\non AWS services, it can be used for local deployments\
    \ in labo-\nratory settings, so that images do not need to be uploaded to\nthe\
    \ public cloud for processing. Alternatively, our system can\nbe used with any\
    \ cloud computing provider able to host Docker\ncontainers. We use the open-source\
    \ message broker RabbitMQ\nin place of Amazon SQS (simple queue service). Our\
    \ Kubernetes\ndeployment scripts handle the necessary confguration, and a\nbeneft\
    \ of RabbitMQ is that it has a built in management web\nGUI. A helper script is\
    \ provided to confgure the credentials for\nthe management GUI.\nEvaluation\n\
    For validation of this case study we simulated analysis and\ntiering using a high\
    \ content screening dataset previously col-\nlected in the lab, consisting of\
    \ 2699 images of cortical neuronal\ncells, imaged with an ImageXpress XLS, the\
    \ dataset is available\nat [22]. In doing so, we demonstrate that our system is\
    \ able to\nhandle a large number of images. To simulate the microscope,\nthe images\
    \ were copied into the source directory, triggering\nmessages from the client,\
    \ which were read by workers to ana-\nlyze the images (with CellProfler) to extract\
    \ the relevant fea-\ntures from the results, apply the IF, and allocate them to\
    \ the\ntiers according to the policy. Running in our laboratory Ku-\nbernetes\
    \ environment, 17 workers were able to process images\n4 Version 3.1.8 was used\
    \ for this study.\n5 https://github.com/CellProfiler/Distributed-CellProfiler\n\
    preprint (which was not certified by peer review) is the author/funder. All rights\
    \ reserved. No reuse allowed without permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\nBlamey\
    \ et al.\n|\n7\nFigure 3. Histograms of the PLLS feature scores (top), and when\
    \ converted to\nan Interestingness Score (bottom), by application of the Logistic\
    \ Function (the\nIF for Case Study 1, middle). The vertical lines on the bottom\
    \ plot indicate tier\nboundaries confgured in the policy. c.f. example images\
    \ in Figure 4\nTable 2. Image allocation for Case Study 1.\nTier\nImage Count\n\
    Data (MB)\nTier A\n726\n6 789\nTier B\n731\n6 836\nTier C\n606\n5 667\nTier D\n\
    636\n5 947\nTotal\n2699\n25 239\nsimultaneously.\nWe use the PLLS (Power Log Log\
    \ Slope) feature as the basis\nof our interestingness score, as it has been shown\
    \ to be a robust\nmeasure of image focus [23]. In this case study, we use the\n\
    logistic function f as an IF, applying it to the PLLS feature x, to\ncompute the\
    \ interestingness score. The logistic function has\noutput in the range (0,1):\n\
    f(x) =\n1\n1 + e–k(x–x0)\nThe PLLS values will depend on a number of factors (such\n\
    as magnifcation, number of cells, stainings, exposure times,\netc.). The parameters\
    \ of this IF can be chosen to ft the modal-\nity, based on a sample of pre-images\
    \ for calibration. In this\ncase, we chose (k = 4.5, x0 = –1.4). The policy is\
    \ defned to\nmap the interestingness score in the intervals (i/4, (i + 1)/4) for\n\
    i ∈ (0, 1, 2, 3) to the respective storage tiers. Figure 3 shows\nhistograms of\
    \ the PLLS feature and Interestingness Score.\nFor this evaluation, these tiers\
    \ were simply directories on\ndisk. Any storage system compatible with the HASTE\
    \ Storage\nClient could be used, the key idea is that diferent storage plat-\n\
    forms (with diferent performance and cost) can be used for the\ndiferent tiers.\
    \ In this case, we simply use the tiers as a con-\nvenient way to partition the\
    \ dataset for further analysis and\ninspection. Figure 4 shows examples of the\
    \ images according\nFigure 4. Example images from the high content screening dataset\
    \ (Case Study\n1), according to automatically assigned tier. Tier A is the most\
    \ in-focus, with\nthe highest PLLS feature values and interestingness scores.\n\
    interestingness_function(features):\nplls = features[’PLLS’]\nint_score = 1/(1\
    \ + exp(-(4.5) * (plls - (-1.4))))\nreturn int_score\nSee: https://github.com/HASTE-project/cellprofiler-pipeline/blob/\n\
    master/worker/haste/pipeline/worker/LogisticInterestingnessModel.py\nstorage_policy:\n\
    [ [0.,\n0.25, tierD],\n[0.25, 0.50, tierC],\n[0.50, 0.75, tierB],\n[0.75, 1.00,\
    \ tierA] ]\nSee: https://github.com/HASTE-project/k8s-deployments/\nblob/master/pipeline_worker.yaml\n\
    Listing 1: Pseudocode for Image Tier Placement (Case Study 1). The IF is the\n\
    logistic function, applied to the previously extracted PLLS feature. The policy\n\
    shows thresholds for the diferent tiers.\nto tiers, and Table 2 shows the results.\n\
    Case Study 2 - Prioritizing analysis of TEM images at\nthe Cloud Edge\nThis case\
    \ study is concerned with the prioritized processing\nof a stream of images from\
    \ a microscope (according to an IF),\napplied to a hybrid edge/cloud stream processing\
    \ deployment\ncontext. In this example, we show how the HASTE tools can fa-\n\
    cilitate a better use of constrained upload bandwidth and edge\ncompute resources.\
    \ The image stream comes from MiniTEMTM\n- a 25keV transmission electron microscope\
    \ [24] (Vironova,\nSweden), connected to a desktop PC from which the micro-\n\
    scope is operated and the image stream received, via propri-\netary driver software.\
    \ The stream processing application pre-\nprocesses the TEM images locally (i.e.\
    \ at the cloud edge), to\nreduce their image size, with the efect of reducing\
    \ their up-\nload time to the cloud, and hence the end-to-end processing\nlatency.\n\
    preprint (which was not certified by peer review) is the author/funder. All rights\
    \ reserved. No reuse allowed without permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\n8\n|\n\
    Journal of XYZ, 2017, Vol. 00, No. 0\nFigure 5. Architecture for Case Study 2,\
    \ showing internal functionality of the\nHaste Desktop Agent at the cloud edge.\
    \ Images streamed from the microscope\nare queued at the edge for uploading after\
    \ (potential) pre-processing. The DH\nis realized as a priority queue. Images\
    \ are prioritized in this queue depending\non the IF which estimates the extent\
    \ of their size reduction under this pre-\nprocessing operator: those with a greater\
    \ estimated reduction are prioritized\nfor processing (vice-versa for upload).\
    \ This estimate is calculated by interpo-\nlating the reduction achieved in nearby\
    \ images (see Figure 7). This estimated\nspline is the IF for this case study.\n\
    The purpose of the pipeline is to automate a typical work-\nfow for TEM analysis,\
    \ which proceeds as follows: a sample is\nloaded into the microscope (in this\
    \ case a tissue sample), the\noperator performs an ‘initial sweep’ over the sample\
    \ at low\nmagnifcation, to locate target (i.e. interesting) regions of the\nsample.\
    \ In the conventional workfow, the search for ‘target’\nareas of the sample is\
    \ done by human inspection. The opera-\ntor then images identifed target areas\
    \ of the sample at higher\nmagnifcation for subsequent visual/digital analysis.\n\
    Automating this process entails the detection of target re-\ngions of the sample\
    \ using an automated image processing\npipeline, based on a set of images from\
    \ the initial sweep. Such a\npipeline would output machine-readable instructions\
    \ to direct\nthe microscope to perform the high magnifcation imaging, re-\nducing\
    \ the need for human supervision of sample imaging. The\nimage processing pipeline\
    \ used to detect target regions can be\ncostly and slow and could hence preferably\
    \ be performed in\nthe cloud. Performing image processing in the cloud has sev-\n\
    eral advantages: it allows short-term rental of computing re-\nsources without\
    \ incurring the costs associated with up-front\nhardware investment and on-premises\
    \ management of hard-\nware. Machines with GPUs for deep learning, as well as\
    \ secure,\nbacked-up storage of images in the cloud, are available accord-\ning\
    \ to a pay-per-use model. With our overall aim of supporting\na real-time control\
    \ loop, and given the expense of the equip-\nment, sample throughput is important.\
    \ Despite images being\ncompressed as PNGs, upload bandwidth is a bottleneck.\
    \ Note\nthat PNG compression is lossless, so as not to interfere with\nsubsequent\
    \ image analysis. Consequently, we wish to upload\nall the images from the ‘initial\
    \ sweep’ into the cloud as quickly\nas possible, and this is what is targeted\
    \ here.\nA pre-processing operator, would reduce the compressed\nimage size to\
    \ an extent depending on the image content. How-\never, this operator itself has\
    \ a computational cost but because\nof the temporary backlog of images waiting\
    \ to be uploaded,\nthere is an opportunity to pre-process some of the waiting\n\
    images to reduce their size (see Figure 5). The available up-\nload bandwidth\
    \ with respect to the computational cost of the\npre-processing operator, means\
    \ that (in our experiment) there\nis insufcient time to pre-process all images\
    \ prior to upload.\nIn fact, to pre-process all of them would actually increase\n\
    end-to-end latency, due to the computational cost of the pre-\nprocessing operation\
    \ and limited fle size reduction for some\nimages (content dependent). The solution\
    \ is to prioritize im-\nages for upload and pre-processing respectively, whilst\
    \ both\nFigure 6. Architecture of the intended application: full control loop\
    \ for the\nMiniTEM, with automatic imaging of target areas identifed in initial\
    \ scan.\nControl of microscope acquisition is future work. The internals of the\
    \ HASTE\nDesktop Agent (where the HASTE model is applied) are shown in Figure\
    \ 5.\nprocesses, as well as the enqueuing of new images from the\nmicroscope,\
    \ are occurring concurrently.\nFeature Extraction, the Interestingness Function,\
    \ and Policy\nSamples for TEM analysis are typically supported by a metal\ngrid,\
    \ which then obscures (blocks) regions of the sample in\n(in this case) a honeycomb\
    \ pattern. The blocked regions ap-\npear black in the images. As the sample holder\
    \ moves under\nthe camera, the extent to which the sample is obscured is a\npiecewise\
    \ smooth (but irregular) function of document index,\ndependent on the particular\
    \ magnifcation level, and speed and\ndirection of the sample holder movement.\
    \ Images can be pre-\nprocessed to remove noise from blocked regions of the image,\n\
    reducing the size of the image under PNG compression. The ex-\ntent of fle size\
    \ reduction (under our pre-processing operator)\nis related to the extent to which\
    \ the grid obscures the image.\nConsequently, the predicted extent of fle size\
    \ reduction\ncan be modelled with linear spline interpolation, based on the\n\
    actual fle size reduction of images sampled from the queue,\ndescribed in more\
    \ detail in [25]. The fle size reduction cor-\nresponds to feature extraction\
    \ in the HASTE pipeline model,\nand the spline estimate – the estimate of message\
    \ size reduc-\ntion – can be encapsulated as an IF, see Figure 1. The HASTE\n\
    tools, specifcally the HASTE Agent allow that IF to be used\nas a scheduling heuristic\
    \ to prioritize upload and local (pre-\n)processing respectively (i.e. corresponding\
    \ to the policy in-\nducing the DH in HASTE).\nAvailable compute resource at the\
    \ cloud edge are prioritized\non those images expected to yield the greatest reduction\
    \ in fle\nsize (normalized by the compute cost, i.e. CPU time, incurred\nin doing\
    \ so). Conversely, upload bandwidth is prioritized on\n(a) images that have been\
    \ processed in this way, followed by\n(b) those images for which the extent of\
    \ fle size reduction is\nexpected to be the least – under the aim of minimizing\
    \ the\noverall upload time.\nAn important distinction between the this setting\
    \ and that\nin Case Study 1 is that the IF and DH are dynamic in this case\nstudy.\n\
    The HASTE Agent manages the 3 processes occurring simul-\ntaneously: new images\
    \ are arriving from the microscope, im-\nages are being pre-processed, and images\
    \ are being uploaded.\nEvaluation\nWhen evaluated on a set of kidney tissue sample\
    \ images [26]\nour edge-based processing approach yielded up to a 25% reduc-\n\
    tion in end-to-end stream processing latency (for this partic-\nular choice of\
    \ processing operator and dataset) compared to a\nbaseline approach without any\
    \ prioritization, when compared\nto performing no stream processing at all [25].\
    \ This is a signif-\npreprint (which was not certified by peer review) is the\
    \ author/funder. All rights reserved. No reuse allowed without permission. \n\
    bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version\
    \ posted September 14, 2020. The copyright holder for this\nBlamey et al.\n|\n\
    9\nFigure 7. Image size reduction (normalized by CPU cost) over index, showing\n\
    which images are processed at the edge. Those marked ‘processed’ were pro-\ncessed\
    \ at the cloud edge prior to upload (and vice-versa) – selected either to\nsearch\
    \ for new areas of high/low reduction, or to exploit known areas (using the\n\
    IF). The line shows the fnal revision of the splines estimation of the message\n\
    size reduction (the IF). Note how this deviates from the true value (measured\n\
    independently for illustration purposes on the same hardware), in regions of\n\
    low reduction. Note the oscillating pattern which is an artifact movement over\n\
    the grid in the miniTEM. Adapted from [25].\ncant gain obtained with relative\
    \ ease due to the HASTE Toolkit.\nTo verify the pre-processing operator, it was\
    \ applied to all\nimages after the live test was performed. Figure 7, shows how\n\
    the image size reduction (y-axis - normalized with computa-\ntional cost) can\
    \ be modelled as a smooth function of the docu-\nment index (x-axis). The colors\
    \ and symbols show which im-\nages were processed prior to upload based on either\
    \ searching\n(black crosses); or on the basis of the IF; those selected for\n\
    pre-processing (blue dots), and those which were not (orange\ncrosses). As can\
    \ be seen and expected there is one peak (the\ncentral one) where more images\
    \ should optimally have been\nscheduled for pre-processing prior to upload. That\
    \ they were\nnot is a combination of the heuristics in the sampling strategy,\n\
    and the uploading speed. That is, they were simply uploaded\nbefore the IF (the\
    \ spline estimate) was good enough to schedule\nthem for pre-processing. The blue\
    \ line in Figure 7 corresponds\nto the fnal spline.\nDiscussion\nThis paper has\
    \ discussed an approach to the design and de-\nvelopment of smart systems for\
    \ processing large data streams.\nThe key idea of a HASTE pipeline is based on\
    \ prioritization\nwith an interestingness function, and the application of a pol-\n\
    icy.\nWe demonstrated in two distinct case studies that this\nsimple model can\
    \ yield signifcant performance gains for data-\nintensive experiments. We argue\
    \ that IFs (and the prioritiza-\ntion and binning that they achieve) should be\
    \ considered more\na ‘frst class citizen’ in the next generation of workfow man-\n\
    agement systems, and that the prioritization of data using IFs\nand policies are\
    \ useful concepts for designing and developing\nsuch systems.\nThe ability to\
    \ express informative IFs are critical to the ef-\nfciency of a HASTE pipeline.\
    \ IFs are chosen by the domain\nexpert to quantify aspects of the data to determine\
    \ online pri-\noritization. In this work we provide two examples of increas-\n\
    ing complexity. In Case Study 1, the IF is a static, idempotent\nfunction of a\
    \ single image – which can be checked against a\nstatic threshold, to determine\
    \ a priority ‘bin’ or tier to store the\nimage. In Case Study 2, the prioritization\
    \ of the queue of im-\nages waiting to be uploaded is revised online, as the underlying\n\
    model is revised. The strength of our proposed model is that,\nhaving defned an\
    \ IF, by making small changes to the policy,\nthe user is able to reconfgure the\
    \ pipeline for diferent deploy-\nment scenarios and datasets, with diferent resulting\
    \ resource\nallocation. The HASTE toolkit is an initial implementation of\nthis\
    \ vision. An avenue for future work will explore the creation\nof IFs through\
    \ training in real-time, using active learning and\npotentially also reinforcement\
    \ learning.\nThe policy-driven approach of resource prioritization pro-\nposed\
    \ under the HASTE pipeline paradigm can be generalized\nto optimize utilization\
    \ of diferent forms of constrained com-\nputational resources. In some contexts\
    \ (such as Case Study 1)\nwe are concerned with processing data streams for long-term\n\
    storage, so storage requirements (and associated costs) are the\nkey concern.\
    \ In other contexts, with a focus on real time con-\ntrol, automation and robotics,\
    \ the priority can be more about\nachieving complex analysis with low-latency.\
    \ In Case Study 2\nthis is manifest as a need to achieve edge to cloud upload\
    \ in\nthe shortest possible time.\nThe diferent policies for the two case studies\
    \ refect this: in\nCase Study 1, the user defnes a policy to ‘bin’ images according\n\
    their interestingness score (i.e. image quality), these thresh-\nolds are pre-defned\
    \ by the user. That is to say, the user decides\nexplicit interestingness thresholds,\
    \ and this determines the re-\nsources (in this case, storage) which are allocated,\
    \ and the fnal\ncost. In similar deployment scenarios where cloud storage is\n\
    used (especially blob storage) costs would depend on the num-\nber of images within\
    \ each interestingness bound. Whereas in\nCase Study 2, by modelling the predicted\
    \ extent of message\nsize reduction as an IF within the HASTE tools, we can defne\
    \ a\npolicy to prioritize image processing and upload with the goal\nof minimizing\
    \ the total upload time for the next step in the\npipeline.\nThese policies induce\
    \ two forms of DH: In Case Study 2, the\nDH is manifest as a priority queue, updated\
    \ in real time as new\nimages arrive, are pre-processed, and eventually removed\
    \ –\nwhereas the available resources (CPU, network) are fxed. By\ncontrast, the\
    \ data hierarchy in Case Study 1 is static, defned\nby fxed thresholds on interestingness\
    \ score – in this case, it is\nthe resources (in this case, storage, and consequent\
    \ processing)\nwhich are variable, determined by how many images end up in\neach\
    \ tier of the hierarchy.\nFinally we note that the IF and policy could also be\
    \ used to\nprioritize data based on some measure of confdence. In many\nscientifc\
    \ analyses there exists a signifcant amount of uncer-\ntainty in several steps\
    \ of the modeling process. For example in\na classifcation setting the class labels\
    \ predicted can be highly\nuncertain. If in the top tier of the hierarchy we would\
    \ place only\nthose data points for which we are confdent in the predicted\nlabel,\
    \ downstream analysis would see a reduction in noise and\nan increased separability\
    \ of the (biological) efects under study,\nas discussed in [27].\nConclusion\n\
    In this paper we have proposed a new model for creating in-\ntelligent data pipelines,\
    \ and presented a software implementa-\ntion, the HASTE Toolkit. We have shown\
    \ how these tools can\nbe leveraged in imaging experiments to organize datasets\
    \ into\nDHs. We have shown benefts in terms of cost reduction and\nperformance\
    \ improvement, in terms of compute resources of\nvarious kinds). In our case studies,\
    \ we have studied some typ-\nical deployment scenarios, and shown how prioritization\
    \ can\nbe achieved in these scenarios. Conceptualizing data analysis\npipelines\
    \ around IFs allows better use of various computing\nresources, and provides a\
    \ conceptual structure for us to think\nabout the involvement of humans in such\
    \ pipelines (and their\nmonitoring), as well as a means of managing scientifc\
    \ experi-\nmentation – either with instruments or through simulation.\nThe proposed\
    \ HASTE pipeline model is intended as a means\nof bringing structure to large\
    \ scientifc datasets – a means of\npreprint (which was not certified by peer review)\
    \ is the author/funder. All rights reserved. No reuse allowed without permission.\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version\
    \ posted September 14, 2020. The copyright holder for this\n10\n|\nJournal of\
    \ XYZ, 2017, Vol. 00, No. 0\ncurating a Data Lake [28], whilst avoiding creating\
    \ a data swamp\n[29, 30]. It is efectively a design pattern creating an data hier-\n\
    archy from “runtime knowledge\" about the dataset – extracted\nin real time. The\
    \ HASTE Toolkit is intended to help scientists\nachieve this.\nThe key contribution\
    \ made by the HASTE Toolkit is the de-\nsign of an API which allows the user to\
    \ express how they would\nlike their data to be prioritized, whilst hiding from\
    \ them the\ncomplexity of implementing this behaviour for diferent con-\nstrained\
    \ resources in diferent deployment contexts. Our hope\nis that the toolkit will\
    \ allow intelligent prioritization to be\n‘bolted on’ to new and existing systems\
    \ – and is consequently\nintended to be usable with a range of technologies in\
    \ diferent\ndeployment scenarios.\nIn the general context of big data, the HASTE\
    \ Toolkit should\nbe seen an efort to address challenges related to data streams\n\
    and efcient placement and management of data. It provides\nthe technical foundation\
    \ for automatically organizing incom-\ning datasets in a way that makes them self-explainable\
    \ and\neasy to use based on the features of data objects rather than tra-\nditional\
    \ metadata. It also enables efcient data management\nand storage based on data\
    \ hierarchies using dynamic policies.\nThis lays the foundation for domain experts\
    \ to efciently select\nthe best-suited data from a massive dataset for downstream\n\
    analysis.\nDeclarations\nList of abbreviations\n• DH - Data Hierarchy. Conceptual\
    \ structures in datasets, re-\nalized as, e.g. tiered storage systems.\n• HASTE\
    \ - Hierarchical Analysis of Spatial (TE)mporal data.\n• HSC - Haste Storage Client.\
    \ A core HASTE component for\nmanaging data hierarchies.\n• IF - Interestingness\
    \ function.\nApplied to a document in\nHASTE to compute an interestingness score.\n\
    • PLLS - Power Log Log Slope.\nEthical Approval\nNot applicable.\nConsent for\
    \ Publication\nNot applicable.\nCompeting Interests\nThe authors declare that\
    \ they have no competing interests.\nFunding\nThe HASTE Project (Hierarchical\
    \ Analysis of Spatial and Tempo-\nral Image Data, http://haste.research.it.uu.se/)\
    \ is funded by the\nSwedish Foundation for Strategic Research (SSF) under award\n\
    no. BD15-0008, and the eSSENCE strategic collaboration for\neScience.\nAcknowledgements\n\
    Thanks to Anders Larsson and Oliver Stein for help with soft-\nware deployment\
    \ and testing for Case Study 1. Thanks to Polina\nGeorgiev for providing the images\
    \ used in the evaluation of\nCase Study 1. Resources from The Swedish National\
    \ Infrastruc-\nture for Computing (SNIC) [31] were used for Case Study 2.\nReferences\n\
    1. Ouyang W, Zimmer C. The Imaging Tsunami: Computa-\ntional Opportunities and\
    \ Challenges.\nCurrent Opinion in\nSystems Biology 2017 Aug;4:105–113.\n2. Stephens\
    \ ZD, Lee SY, Faghri F, Campbell RH, Zhai C, Efron\nMJ, et al.\nBig Data: Astronomical\
    \ or Genomical?\nPLOS\nBiology 2015 Jul;13(7):e1002195.\n3. Blamey B, Wrede F,\
    \ Karlsson J, Hellander A, Toor S. Adapt-\ning the Secretary Hiring Problem for\
    \ Optimal Hot-Cold\nTier Placement Under Top-K Workloads.\nIn: 2019 19th\nIEEE/ACM\
    \ International Symposium on Cluster, Cloud and\nGrid Computing (CCGRID) Larnaca,\
    \ Cyprus; 2019. p. 576–\n583.\n4. Sivarajah U, Kamal MM, Irani Z, Weerakkody V.\n\
    Critical\nAnalysis of Big Data Challenges and Analytical Methods.\nJournal of\
    \ Business Research 2017;70:263–286.\n5. Reinsel D, Gantz J, Rydning J, Data Age\
    \ 2025: The Digitiza-\ntion of the World from Edge to Core (Seagate White Paper);\n\
    2018.\n6. Rinehart D, Johnson CH, Nguyen T, Ivanisevic J, Benton HP,\nLloyd J,\
    \ et al.\nMetabolomic Data Streaming for Biology-\nDependent Data Acquisition.\n\
    Nature Biotechnology 2014\nJun;32(6):524–527.\n7. Hillman C, Petrie K, Cobley\
    \ A, Whitehorn M.\nReal-Time\nProcessing of Proteomics Data: The Internet of Things\
    \ and\nthe Connected Laboratory. In: 2016 IEEE International Con-\nference on\
    \ Big Data (Big Data); 2016. p. 2392–2399.\n8. Zhang Y, Bhamber R, Riba-Garcia\
    \ I, Liao H, Unwin RD,\nDowsey AW. Streaming Visualisation of Quantitative Mass\n\
    Spectrometry Data Based on a Novel Raw Signal Decompo-\nsition Method. PROTEOMICS\
    \ 2015;15(8):1419–1427.\n9. Kelleher J, Lin M, Albach CH, Birney E, Davies R,\
    \ Gourtovaia\nM, et al. Htsget: A Protocol for Securely Streaming Genomic\nData.\
    \ Bioinformatics 2019 Jan;35(1):119–121.\n10. Cuenca-Alba J, del Cano L, Gómez\
    \ Blanco J, de la Rosa\nTrevín JM, Conesa Mingo P, Marabini R, et al. ScipionCloud:\n\
    An Integrative and Interactive Gateway for Large Scale\nCryo Electron Microscopy\
    \ Image Processing on Commercial\nand Academic Clouds. Journal of Structural Biology\
    \ 2017\nOct;200(1):20–27.\n11. de la Rosa-Trevín JM, Quintana A, del Cano L, Zaldívar\
    \ A,\nFoche I, Gutiérrez J, et al. Scipion: A Software Framework\ntoward Integration,\
    \ Reproducibility and Validation in 3D\nElectron Microscopy.\nJournal of Structural\
    \ Biology 2016\nJul;195(1):93–99.\n12. Wang D, Fong S, Wong RK, Mohammed S, Fiaidhi\
    \ J,\nWong KKL. Robust High-Dimensional Bioinformatics Data\nStreams Mining by\
    \ ODR-ioVFDT. Scientifc Reports 2017\nFeb;7(1):43167.\n13. Shi W, Dustdar S. The\
    \ Promise of Edge Computing. Com-\nputer 2016 May;49(5):78–81.\n14. B Blamey,\
    \ A Hellander, S Toor. Apache Spark Streaming,\nKafka and HarmonicIO: A Performance\
    \ Benchmark and Ar-\nchitecture Comparison for Enterprise and Scientifc Com-\n\
    puting. In: Bench’19 Denver, Colorado, USA; 2019. .\n15. Torruangwatthana P, Wieslander\
    \ H, Blamey B, Hellander A,\nToor S. HarmonicIO: Scalable Data Stream Processing\
    \ for\nScientifc Datasets. In: 2018 IEEE 11th International Con-\nference on Cloud\
    \ Computing (CLOUD) San Francisco, CA,\nUSA; 2018. p. 879–882.\n16. Awesome Pipeline;.\
    \ https://github.com/pditommaso/awesome-\npipeline.\n17. Chan SG, Tobagi FA. Modeling\
    \ and Dimensioning Hierar-\npreprint (which was not certified by peer review)\
    \ is the author/funder. All rights reserved. No reuse allowed without permission.\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version\
    \ posted September 14, 2020. The copyright holder for this\nBlamey et al.\n|\n\
    11\nchical Storage Systems for Low-Delay Video Services. IEEE\nTransactions on\
    \ Computers 2003 Jul;52(7):907–919.\n18. Novella JA, Emami Khoonsari P, Herman\
    \ S, Whitenack\nD, Capuccini M, Burman J, et al.\nContainer-Based\nBioinformatics\
    \ with Pachyderm.\nBioinformatics 2019\nMar;35(5):839–846.\n19. Kubernetes,\n\
    Kubernetes\nDocumentation;.\nhttps://kubernetes.io/docs/home/.\n20. Bray MA, Carpenter\
    \ AE.\nQuality Control for High-\nThroughput Imaging Experiments Using Machine\
    \ Learning\nin Cellprofler.\nIn: Johnston PA, Trask OJ, editors. High\nContent\
    \ Screening: A Powerful Approach to Systems Cell\nBiology and Phenotypic Drug\
    \ Discovery Methods in Molec-\nular Biology, New York, NY: Springer New York;\
    \ 2018.p.\n89–112.\n21. McQuin C, Goodman A, Chernyshev V, Kamentsky L, Cimini\n\
    BA, Karhohs KW, et al. CellProfler 3.0: Next-Generation\nImage Processing for\
    \ Biology. PLoS Biology 2018 Jul;16(7).\n22. Polina\nGeorgiev,\nBen\nBlamey,\n\
    Ola\nSpjuth,\nSnat10\nKnockout\nMice\nCortical\nNeuronal\nCells\n(ImageXpress\n\
    XLS\nExample\nImages);\n2020.\nhttp://doi.org/10.17044/scilifelab.12811997.v1.\n\
    23. Bray MA, Fraser AN, Hasaka TP, Carpenter AE. Workfow\nand Metrics for Image\
    \ Quality Control in Large-Scale High-\nContent Screens. Journal of Biomolecular\
    \ Screening 2012\nFeb;17(2):266–274.\n24. Vironova AB, MiniTEM: Automated Transmission\
    \ Electron\nMicroscopy Analysis;.\nhttps://www.vironova.com/our-\nofering/minitem/.\n\
    25. Blamey B, Sintorn IM, Hellander A, Toor S.\nResource-\nand Message Size-Aware\
    \ Scheduling of Stream Process-\ning at the Edge with Application to Realtime\
    \ Microscopy.\narXiv:191209088 [cs] 2019 Dec;.\n26. Ben\nBlamey,\nIda-Maria\n\
    Sintorn,\nHASTE\nminiTEM\nExample\nImages\n(Dataset);\n2020.\nhttps://doi.org/10.17044/scilifelab.12771614.v1.\n\
    27. Wieslander H, Harrison PJ, Skogberg G, Jackson S, Friden\nM, Karlsson J, et\
    \ al. Deep Learning and Conformal Predic-\ntion for Hierarchical Analysis of Large-Scale\
    \ Whole-Slide\nTissue Images. IEEE Journal of Biomedical and Health In-\nformatics\
    \ 2020;p. 1–1.\n28. Pentaho,\nHadoop,\nand\nData\nLakes;.\nhttps://jamesdixon.wordpress.com/2010/10/14/pentaho-\n\
    hadoop-and-data-lakes/.\n29. Brackenbury W, Liu R, Mondal M, Elmore AJ, Ur B,\
    \ Chard\nK, et al.\nDraining the Data Swamp: A Similarity-Based\nApproach.\nIn:\
    \ Proceedings of the Workshop on Human-\nIn-the-Loop Data Analytics HILDA’18,\
    \ Houston, TX, USA:\nAssociation for Computing Machinery; 2018. p. 1–7.\n30. Hai\
    \ R, Geisler S, Quix C. Constance: An Intelligent Data\nLake System.\nIn: Proceedings\
    \ of the 2016 International\nConference on Management of Data SIGMOD ’16, San\
    \ Fran-\ncisco, California, USA: Association for Computing Machin-\nery; 2016.\
    \ p. 2097–2100.\n31. Toor S, Lindberg M, Falman I, Vallin A, Mohill O, Freyhult\n\
    P, et al. SNIC Science Cloud (SSC): A National-Scale Cloud\nInfrastructure for\
    \ Swedish Academia.\nIn: E-Science (e-\nScience), 2017 IEEE 13th International\
    \ Conference On IEEE;\n2017. p. 219–227.\npreprint (which was not certified by\
    \ peer review) is the author/funder. All rights reserved. No reuse allowed without\
    \ permission. \nbioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779;\
    \ this version posted September 14, 2020. The copyright holder for this\n"
  inline_citation: '>'
  journal: bioRxiv (Cold Spring Harbor Laboratory)
  limitations: '>'
  pdf_link: https://www.biorxiv.org/content/biorxiv/early/2020/09/14/2020.09.13.274779.full.pdf
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Rapid development of cloud-native intelligent data pipelines for scientific
    data streams using the HASTE Toolkit
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
