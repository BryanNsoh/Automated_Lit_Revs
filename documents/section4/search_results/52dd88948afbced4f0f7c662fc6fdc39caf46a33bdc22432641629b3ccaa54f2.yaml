- DOI: https://doi.org/10.1049/iet-its.2020.0054
  analysis: '>'
  authors:
  - Zhihao Zheng
  - Ximan Ling
  - Pu Wang
  - Jianguo Xiao
  - Fan Zhang
  citation_count: 17
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register IET HUB HOME JOURNALS
    IET PRIZE PROGRAMME SUBJECTS Visit IET IET Intelligent Transport Systems Research
    Article Free Access Hybrid model for predicting anomalous large passenger flow
    in urban metros Zhihao Zheng,  Ximan Ling,  Pu Wang,  Jianhe Xiao,  Fan Zhang
    First published: 22 February 2021 https://doi.org/10.1049/iet-its.2020.0054Citations:
    17 SECTIONS PDF TOOLS SHARE Abstract Machine learning models have been widely
    adopted for passenger flow prediction in urban metros; however, the authors find
    machine learning models may underperform under anomalous large passenger flow
    conditions. In this study, they develop a prediction framework that combines the
    advantage of complex network models in capturing the collective behaviour of passengers
    and the advantage of online learning algorithms in characterising rapid changes
    in real-time data. The proposed method considerably improves the accuracy of passenger
    flow prediction under anomalous conditions. This study can also serve as an exploration
    of interdisciplinary methods for transportation research. 1 Introduction It is
    widely accepted that developing efficient and reliable urban metros is beneficial
    to urban development and social welfare [1]. As the backbone of urban public transportation
    [2], urban metros are characterised by large capacity, high-speed, and high reliability
    [3]. In recent years, passenger volumes have increased in many metro systems around
    the world. For example, the annual ridership of the Shenzhen metro increased by
    an average of 23% per year from 2016 to 2018 [4]. Even for the mature New York
    City subway, its ridership gained an average of 1.5% growth from 2013 to 2015
    [5]. The growth of ridership in an urban metro is regarded as a good sign for
    the mitigation of traffic congestion [6]. However, large passenger flows especially
    those unexpected will pose great pressure to the operation of the urban metro
    [7, 8]. Anomalous large passenger flows may also increase the possibility of train
    malfunctions [9] and threaten the safety of passengers. Hence, predicting anomalous
    large passenger flow is of particular importance for the management and operation
    of urban metros. Time series analysis and machine learning (ML) have been employed
    in the passenger flow prediction. ML models are demonstrated to have superior
    overall performance. However, in this study, we find that ML models may underperform
    under anomalous large passenger flow conditions. There are two possible reasons.
    Firstly, the prediction errors generated under anomalous conditions have limited
    contribution to the overall error because of their rare occurrence. Consequently,
    the patterns of anomalous large passenger flow are easy to be ‘forgotten’ by ML
    models [10]. Secondly, some anomalous large passenger flow patterns are not recorded
    in the historical training data. Hence, ML models may not be able to accurately
    predict a pattern that never occurs before. In this study, we develop a hybrid
    model for predicting the anomalous large passenger flow exiting a metro station.
    The hybrid model comprises an offline model under ordinary conditions and an online
    model under anomalous conditions. A complex network method for anomaly detection
    is proposed to switch between the two models and the online learning algorithm
    is applied to train the online model. Specifically, a complex network index is
    defined to identify the periods when online learning is applied to the pre-trained
    offline model for real-time updating. The complex network method has the advantage
    of capturing the collective behaviour of passengers travelling to a metro station
    [11]. The online learning algorithm has the advantage of characterising the anomalous
    trend of passenger flow. Our results show that the proposed hybrid model considerably
    improves the prediction accuracy of ML models under anomalous conditions. In what
    follows, related works are first reviewed in Section 2. In Section 3, we propose
    the framework for predicting the anomalous large passenger flow in urban metros.
    The analysis and discussion of the prediction results are presented in Sections
    4 and 5. Finally, Section 6 concludes the results and the contribution of this
    study. 2 Related work 2.1 Ordinary passenger flow prediction The problem of passenger
    flow prediction is one kind of time-series problem. Starting from the autoregressive
    moving-average [12], several time-series models have been developed. The autoregressive
    integrated moving-average (ARIMA) was proposed to transform non-stationary time
    series into stationary time series. The seasonal ARIMA (SARIMA) model was designed
    to capture the seasonality of a time series. Time-series models have been widely
    used in transit passenger flow predictions. Ma et al. [13] predicted the passenger
    flow in the bus transit of Jinan, China by combining three time series models
    (a weekly pattern model [autoregression (AR)], a daily pattern model (SARIMA),
    and an hourly pattern model (ARIMA)) using the interacting multiple models algorithm.
    Zhang et al. [14] combined the ARIMA model and the linear regression (LR) model
    to predict the passenger flow in the bus transit of Shenzhen, China with the smartcard
    data of bus passengers and the bus global positioning system data. Kalman filtering,
    which is widely used in automatic control theory, was recently introduced to address
    the time-series problems. Jiao et al. [15] proposed three modified Kalman filtering
    models based on (i) an error correction coefficient, (ii) deviation between real-time
    data and historical data, and (iii) Bayesian combination and non-parametric regression.
    The three models were used to predict the passenger flow in the Beijing metro.
    Diao et al. [16] predicted the passenger flow in the Chongqing metro using time
    series decomposition and a hybrid Gaussian process model. ML models have been
    increasingly used in passenger flow predictions. Sun et al. [17] decomposed the
    passenger flow series of the Shanghai metro into high-frequency series and low-frequency
    series and used two support vector regression (SVR) models to predict the high-frequency
    series and low-frequency series, respectively. Ling et al. [18] trained an SVR
    model for each time window to improve the accuracy of passenger flow prediction
    in the Shenzhen metro. Tang et al. [19] studied the impacts of various features
    (temporal, spatial, and weather) on metro passenger flow prediction using LR and
    SVR. Wei and Chen [20] used empirical mode decomposition to decompose the passenger
    flow series of the Taipei metro before feeding them into a back-propagation neural
    network. Li et al. [21] used data of inbound volume, outbound volume, and train
    timetable to build a radial basis function (RBF) neural network and predicted
    the passenger flow in the Beijing metro. Deep learning models have recently achieved
    great success in many areas from speech recognition [22] to natural language processing
    [23] and computer vision [24]. Several deep learning architectures have also been
    designed for passenger flow predictions. Bai et al. [25] employed the deep belief
    networks (DBNs) to learn the passenger flow patterns in the bus transit of Guangzhou,
    and the outputs of each DBN were fused to obtain the final prediction of passenger
    flow. Liu and Chen [26] designed a deep neural network and included several temporal
    features (such as day of the week, time of day, and holidays) to predict the passenger
    flow in the bus rapid transit of Xiamen, China. Two years later, Liu et al. [3]
    proposed a deep learning architecture based on the long short-term memory (LSTM)
    model. The proposed architecture considered the spatial–temporal characteristics
    of metro passenger flow, external environment features (e.g. weather), and the
    operation information. Hao et al. [27] employed a sequence-to-sequence deep learning
    model with an attention mechanism to improve the prediction accuracy of passenger
    flow in longer periods (next 30–60 min). The advantage of these deep learning
    architectures relies on their ability to capture the complex non-linear relationship
    in traffic data [3]. 2.2 Anomalous large passenger flow prediction Relatively
    little attention is paid to the prediction of anomalous large passenger flow during
    large commercial and recreational events. Pereira et al. [28] employed information
    about the time to the next event and event categories to develop an artificial
    neural network (ANN) for predicting arrivals at metro stations. Noursalehi et
    al. [29] developed an ensemble method combining a univariate state-space model
    and a dynamic factor model (DFM) for predicting station arrivals in the London
    Underground. The univariate state-space model was used to capture arrival patterns
    using automatic fare collection transactions and the DFM was used for characterising
    event patterns using event information. Similarly, Rodrigues et al. [30] combined
    the smartcard data and event information to predict transit arrivals during planned
    events (e.g. music shows). Various event features, such as the time to the start
    of the event, event topics, and venues, were used to generate the Bayesian additive
    model. Ni et al. [31] discovered a positive correlation between passenger flow
    and social media posts. A LR of this correlation was combined with the SARIMA
    model to predict the passenger flow when an event occurs. Besides incorporating
    external event information, some classical models are also reconstructed to improve
    predictions. Li et al. [32] used multiscale RBF networks to predict metro arrivals
    during special events (concerts, sporting events, parades etc.). Different kernel
    widths were used in the RBFs; however, the model does not show robust performance
    during large-scale events. Chen et al. [33] combined four generalised autoregressive
    conditional heteroscedasticity (GARCH) models with the ARIMA model to improve
    the accuracy of anomalous passenger flow fluctuation prediction during special
    events (e.g. concerts, athletic competitions). The GARCH components are used to
    model volatilities in the time series. 2.3 Other traffic prediction problems under
    anomalous conditions Previous works on other traffic prediction problems (traffic
    flow, travel time etc.) provided some insights for the time series prediction
    during anomalous conditions. These methods can be characterised as: (a) using
    different models and model parameters for different regimes; (b) incorporating
    new data into the model once available. For example, Wu et al. [34] developed
    a non-parametric online boosting non-parametric regression model dedicated to
    predicting traffic flow under incident conditions in a highway network and a base
    model without online boosting was used under ordinary traffic conditions. The
    transition between the models was made based on the prediction errors. Allström
    et al. [35] suggested using non-parametric models for traffic prediction under
    recurrent traffic conditions and using parametric models under non-recurring traffic
    conditions. Salamanis et al. [36] clustered traffic flow and incident data into
    different categories and built different models using the data in different clusters.
    The authors reported improved results by using different models per class. Haworth
    et al. [37] proposed a local online kernel ridge regression approach for predicting
    travel times. The method is featured with allowing parameters to vary by the time
    of the day and incorporating new traffic data when they are available [37]. Chen
    et al. [38] presented an ensemble learning algorithm for short-term traffic flow
    prediction. The authors selected several gradient boosting regression trees models
    with different structures and parameters and used the prediction results of these
    models as the candidates in the Lasso ensemble framework to produce the final
    prediction. Peled et al. [39] used an ordinary prediction model for ordinary traffic
    flow and another model under incident conditions by simulating real-time traffic
    incidents. Castro-Neto et al. [40] showed that an online-SVR model outperforms
    Gaussian maximum likelihood, Holt exponential smoothing, and ANN under anomalous
    traffic conditions because the model can learn patterns from the most recently
    collected data, which can capture the irregular real-time trend of traffic. Nevertheless,
    this feature also leads to the poor performance of online models under ordinary
    conditions. Similarly, Jeong et al. [41] proposed a weighted online-SVR for traffic
    flow prediction. Here, we summarise the current development of passenger flow
    prediction. Firstly, the methods for predicting anomalous large flow are less
    studied. Most previous models focus on improving the overall prediction accuracy
    on a large data set while neglecting the model performance during non-recurrent
    large flow events. Secondly, a few works employ external data such as the event
    calendar data and social media data to predict the anomalous large flow. However,
    the large flow may also occur when no event information is released on the Internet,
    and on the other hand, some events may not cause large flows. The reliability
    of the prediction may be a problematic issue for the models based on event calendar
    data and social media data. In this study, we develop an anomaly detection method
    to switch between an offline model and an online model. The proposed hybrid model
    combines the advantage of complex network models in anticipating collective human
    behaviours, the advantage of the offline models in modelling complex relationships
    in passenger flow data, and the advantage of online learning algorithms in capturing
    the real-time trend of anomalous passenger flow. 3 Methodology In this section,
    we develop a hybrid model for predicting the anomalous large passenger flow in
    urban metros. The problem is formulated in Section 3.1. In Section 3.2, we introduce
    the anomaly detection method based on a complex network index . In Section 3.3,
    we explain how offline and online models are switched between ordinary and anomalous
    conditions. 3.1 Problem formulation The smartcard data have been widely used in
    transit studies. The transaction time, card ID, line no., and station ID are recorded
    when a passenger enters or exits a station. The number of passengers entering
    a station and exiting a station can be estimated for each station i during time
    window t. In this study, the span of a time window is 15 min, which is widely
    adopted in previous studies [18, 29, 32, 42]. Hence, there are 64 15 minute time
    windows during the metro service hours (7:00 a.m.–11:00 p.m.). Next, the origin–destination
    (OD) matrices of passengers are estimated. The number of passengers departing
    from an origin station i and arriving at a destination station j during time window
    t is denoted as . The OD matrices are calculated based on the time window when
    a passenger tap-out of a station, therefore, are known values for all OD pairs
    at time window t. In the following part of the paper, the three parameters , ,
    and are denoted using the ‘flow’ terminology instead of the ‘number of passengers’
    terminology: : the inflow of station i during time window t; : the outflow of
    station i during time window t; : the OD flow from station i to station j during
    time window t. In this study, we focus on the outflow of stations, which is more
    challenging than predicting the inflow because a large inflow usually follows
    a large outflow [43]. The problem of anomalous large outflow prediction is defined
    as developing models to predict the anomalously large at time window t. 3.2 Detecting
    the collective travel behaviour of passengers 3.2.1 Anomalous mobility network
    (AMN) Anomalous large outflow usually occurs during commercial, entertainment,
    or unexpected social events [28]. The AMN was successfully used in anticipating
    large crowd gatherings in urban areas [44]. Here, we employ the AMN to identify
    the time windows when anomalous large flows collectively occur between a large
    number of OD pairs, which consequently generates anomalous large outflows. An
    AMN is generated by links satisfying (1) where and represent the mean and standard
    deviation of the OD flow between metro stations i and j during time period tp,
    and tp is the weekday or weekend time of day to which the specific time window
    t belongs. As shown in (1), we have to determine for discriminating the mobility
    fluxes in AMNs between ordinary and anomalous passenger flow conditions. As suggested
    in [44], we use the Jensen–Shannon divergence (JSD) to measure the similarity
    between two probability distributions of mobility fluxes in AMNs. We generate
    AMNs under different values of and measure the similarity between the two distributions
    using the JSD (2) where the Kullback–Leibler (KL) divergence (3) is to measure
    the non-symmetric distance between two probability distributions. In the context
    of this study, is the probability that the normalised OD flow (4) of a link in
    an AMN during the anomalous condition is equal to x, whereas is the probability
    that of a link in an AMN during the same period in the ordinary condition is equal
    to x. As shown in Fig. 1a, a larger JSD value indicates the AMNs generated under
    anomalous conditions can be better discriminated from the ones generated under
    ordinary conditions. We can see that is a feasible value for the given example
    time periods and the given example days of 17 October (ordinary day) and 31 October
    (event day) since the JSD values remain roughly as 1 for all given time periods
    (Fig. 1b). To justify this conclusion, we also compare the AMNs in the event day
    to AMNs in all other non-event days (i.e. all ordinary Fridays in the training
    set) under different values of (Fig. 1c). We find that JSD values are densely
    distributed at 1 when is applied. Therefore, we use as the threshold to generate
    AMNs. Fig. 1 Open in figure viewer PowerPoint The determination of for discriminating
    the mobility fluxes in AMNs between ordinary and anomalous passenger flow conditions
    (a) Values of JSD between the AMN for 19:00–19:15 of 31 October 2014 and the AMN
    for 19:00–19:15 of 17 October 2014, (b) Values of JSD for all time periods of
    17 October and 31 October under different values of , (c) Distribution of JSD
    values obtained by comparing AMNs on 31 October to AMNs in other non-event days
    (i.e. all ordinary Fridays in the training set) under different values of In an
    AMN, each node represents a metro station, and each directed edge represents the
    fact that the anomalous large flow is detected from the origin station i to the
    destination station j during time window . The number of directed edges pointing
    to a node is the in-degree of the node. The index is used to label the level of
    large flow conditions. 3.2.2 Identifying anomalous large flow conditions A critical
    threshold is determined for each station to identify its anomalous large flow
    condition. We use the Kernel density estimation (KDE) [45, 46] to estimate the
    probability density function of for each station and find a that discriminates
    ordinary and anomalous conditions. Based on [47], clusters of representing ordinary
    and anomalous conditions will be separated by a low-density region. Since is very
    small under ordinary conditions regardless of different time of the day, a feasible
    will be the right boundary of the first cluster (as well as the largest cluster)
    we find starting from the left side of the probability density curve. In other
    words, is the smallest corresponding to a local minimum of the probability density
    function. If no local minima are found, the maximum obtained in the training set
    is used as since the KDE suggests there is only one cluster found. The process
    of searching for can be formulated as follows. Firstly, KDE is used to estimate
    the density distribution of data samples. For a specific kernel K, the density
    at a point y is estimated using a set of data samples (5) where N is the number
    of data samples and h is the smoothing parameter. The Gaussian kernel is used
    in this study (6) Secondly, we plot the estimated density of obtained from a metro
    station. Here, the logarithmic density is used since it makes local minima easier
    to observe. Take the Window of the World station as an example (Fig. 2a), the
    value of corresponding to the first local minimum (the red circle) of the logarithmic
    density curve is selected as . Fig. 2 Open in figure viewer PowerPoint The determination
    of for identifying anomalous in-degree (a) Example of using KDE for determining
    . The value of corresponding to the first local minimum (the red circle) of the
    logarithmic density curve is selected as , (b) Distribution of under different
    smoothing parameter h We use the default smoothing parameter in the Scikit-learn
    library [48]. As shown in Fig. 2b, a larger h will increase for some stations
    because a larger bandwidth makes the density curve smoother. While a smaller h
    will be very sensitive to density changes. However, we can observe that the values
    of the centre around 13–15 for all tested h, indicating that the values of under
    ordinary conditions are densely located and are distant from anomalous so that
    the changes in the smoothing parameter h does not affect the determination of
    greatly. 3.3 Generating the hybrid model 3.3.1 Hybrid learning framework In the
    proposed framework, is used to identify the anomalous large outflows. When is
    above , the offline model becomes an online model that is continuously updated
    using the recent outflow observations; otherwise, the offline model is used for
    prediction. Note that online updates will not be preserved after the model is
    switched back to offline. In this study, we test the hybrid learning framework
    using three different models: a LR model, a fully-connected neural network (FCNN)
    model, and a LSTM model. The description of these models and the parameter tuning
    process can be found in the Appendix. 3.3.2 Offline model The offline model is
    used to predict outflows under ordinary conditions. An offline model is trained
    for each station using the historical data of the station. The training set is
    denoted as follows: (7) where is the collection of training samples of a station
    i, represents the observation in each sample and represents the target the model
    learns to predict, contains a series of previous outflow records , and represents
    the outflow we try to predict. In this study, we use four previous records at
    each prediction step. During the training process, the optimiser will update model
    parameters after a batch of samples is passed into the model. Thirty per cent
    of the training set are set aside as the validation set for early stopping. The
    training will terminate when the validation score is not improving for five consecutive
    epochs. 3.3.3 Online model The offline model becomes an online model by continuously
    updating its parameters using real-time observations when . The online model is
    then used to predict outflows. Once the in-degree of the station returns to ordinary
    values , the model stops updating and the parameters of the model will be restored
    to the point before online learning. A prerequisite of online learning is that
    the offline model should be trained using an iterative optimisation method since
    we have to adjust model parameters dynamically. Take the stochastic gradient descent
    algorithm as an example, it performs the following update using a single training
    example: (8) where is the parameter to be estimated, is the learning rate, and
    is the loss associated with the ith sample in online learning. In this study,
    could be the squared error of the prediction and the actual value : . Unlike offline
    learning, no real-time validation is available to terminate the online learning
    process for preventing the model over-fit to the single training sample. Motivated
    by restoring the predictive power of offline models exhibited during offline learning,
    we use the mean relative error (of the same time window) in the validation set
    used in offline learning as a baseline to terminate online learning. In other
    words, we terminate each online update when the model achieves a comparable performance
    shown in ordinary conditions. Take the outflows of a station as an example. When
    , the online training sample is used to update the parameters of the currently
    used model, where and . and y are used as the observation and the target to perform
    several epochs of gradient descents and update model parameters. The training
    terminates when , where loss is the relative error after each parameter adjustment,
    and is the mean relative error of the predictions in time period tp in the validation
    set used in offline learning. Next, the updated model is used to predict the outflow
    using the recent four observations . Once is collected, the model is updated again
    using the newly collected data sample. The procedure above is repeated until ,
    and the offline model with the original parameters will be used again. The procedure
    is formulated as Algorithm 1 (see Fig. 3). Fig. 3 Open in figure viewer PowerPoint
    Algorithm 1: online learning algorithm 4 Results We use the smartcard data collected
    during the last three months of 2014 in the Shenzhen metro as a case study. The
    outflows during October and November are used as the training set. The outflows
    during the period with seven confirmed crowd gathering events in December are
    used as the testing set (Table 1) [44]. Table 1. Seven crowd gathering events
    in Shenzhen during December 2014 Date Venue Event Associated station 31 December
    Window of the World New Year Fireworks Show Window of the World 24 December Sea
    World Christmas Eve Activity Sea World 31 December Sea World New Year''s Eve Activity
    Sea World 31 December OCT Harbor New Year''s Eve Activity Qiaocheng East 31 December
    COCO Park New Year''s Eve Activity Shopping Park 31 December Laojie New Year''s
    Eve Activity Laojie 31 December Grand Theater New Year''s Eve Activity Grand Theater
    The outflows of each station are scaled using its mean and standard deviation
    for faster convergence in model training. The predictions are re-scaled to the
    original magnitude and compared with actual values for evaluation: (9) where and
    are the mean and standard deviation of outflows of station i in the training set.
    The root mean square error (RMSE) and the mean absolute per cent error (MAPE)
    are used to evaluate the prediction accuracy (10) (11) where is the predicted
    outflow and is the actual outflow. We test the performance of the proposed hybrid
    learning approach using an LR model, an FCNN model, and an LSTM model. We also
    present the results of using only the offline models and the online models that
    continuously update parameters for the entire testing set. Several other popular
    offline models are also presented. Model descriptions can be found in the Appendix.
    The model training and testing process were implemented on a Linux server with
    40 processors (Intel(R) Xeon(R) CPU E5-2640 v4@2.40 GHz) and 64 GB memory. We
    first show the anomaly detection results using (Fig. 4). Time windows during which
    of a station is larger than are identified. Outflows during these time windows
    are identified as anomalous large outflows (Fig. 5). The online models are implemented
    during these anomalous outflow periods. Fig. 4 Open in figure viewer PowerPoint
    Anomalous (red circles) in December identified by the obtained from the training
    set (October and November). The dash line represents the critical threshold Fig.
    5 Open in figure viewer PowerPoint Identified anomalous outflows of the seven
    case studies. Ordinary outflows and anomalous outflows are denoted with blue and
    red lines, respectively The performance of hybrid models and benchmark models
    are presented in Table 2. We first compare the performance among the offline models.
    It is expected that complex ML models (SVR, FCNN, and LSTM) easily outperform
    simple linear models (LR and ARIMA) and the naïve baseline [shadowing prediction
    (SP)] for their capability in modelling the non-linearity of outflow patterns.
    However, we surprisingly find that the situation is completely reversed during
    period when simple models obtain much better results, indicating real-time observations
    are the better predictors than the historical mappings of outflow patterns under
    anomalous conditions. This observation motivates and justifies the implementation
    of online learning for ML models under anomalous conditions. Table 2. Performance
    comparison of different models for the six tested stations Learning mode Model
    Testing data set Testing data set RMSE MAPE, % RMSE MAPE, % offline learning SP
    186.80 24.14 156.68 16.88 LR 167.72 23.17 159.68 17.49 ARIMA 166.25 23.20 162.85
    17.90 SVR 160.69 21.06 245.03 26.51 FCNN 148.86 21.05 227.22 26.44 LSTM 144.48
    19.20 222.48 24.01 online learning for the entire testing set LR 169.74 23.03
    163.63 16.19 FCNN 176.00 40.73 259.47 31.98 LSTM 143.70 28.52 211.73 23.91 online
    learning when LR 166.24 22.88 154.13 (↓3.5%) 16.61 (↓5%) FCNN 131.06 19.20 185.81
    (↓18.2%) 20.01 (↓24%) LSTM 121.77 17.23 168.55 (↓24.2%) 17.08 (↓28.9%) ↓Indicates
    the percentage change compared with the corresponding offline models.Bold values
    indicate the hybrid LSTM model achieves the lowest RMSE and MAPE on the testing
    data set. As shown in Table 2, all three models (LR, FCNN, and LSTM) are improved
    by implementing online learning during period. For example, the RMSE and MAPE
    of the FCNN are reduced by 18.2 and 24%, and the RMSE and MAPE of the LSTM are
    reduced by 24.2 and 28.9% during period. Since the LR model directly benefits
    from making a prediction linearly dependent on the last four observations, there
    are fewer opportunities for the LR model to receive improvements as large as the
    FCNN and the LSTM. In general, the hybrid LSTM has the lowest RMSE and MAPE across
    the entire testing set. We further evaluate the model performance when online
    learning is applied to the entire testing set to test the efficacy of using to
    switch between models. Unlike the proposed hybrid learning framework in which
    models are only updated when , we update models whenever new data are available
    during the testing process. We find that prediction errors are increased if we
    update the models for both ordinary and anomalous conditions. Since under ordinary
    conditions, online learning only learns the mappings of to , while the mappings
    of to are already learned by the offline model using historical data. While under
    anomalous conditions, real-time patterns are important because historical mappings
    of past observations to future values could not give us much knowledge about what
    is currently happening. Therefore, it is unnecessary to implement online learning
    under ordinary conditions. This result is also in accordance with previous findings
    [40]. The prediction results of the LSTM models of two representative events are
    shown in Figs. 6 and 7. For the anomalous outflow observed at the Qiaocheng East
    station (Fig. 6a), the offline LSTM cannot well capture the rapid changes in outflows.
    However, by switching the offline model to the online model properly, the prediction
    accuracy is considerably improved by the proposed hybrid learning framework. For
    the anomalous outflow observed at the Window of the World station (Fig. 7a), although
    the anomalous outflow is more than two times as large as the historical outflow,
    the temporal patterns are similar. Therefore, the offline LSTM can achieve comparable
    performance with the hybrid LSTM. However, the performance can still be improved
    through online learning during the anomaly period. This result also inspires us
    to pay more attention to the anomalous temporal patterns of passenger flow that
    fall out of the generality range of data-driven models. We also present the number
    of epochs we need to update the LSTM model during each time window. We can observe
    that it takes only a few epochs to update the models for both stations, and the
    performance of models are greatly improved (Figs. 6b and 7b). Fig. 6 Open in figure
    viewer PowerPoint Qiaocheng East station on 31 December 2014 (a) Offline LSTM
    versus hybrid LSTM, (b) Number Of epochs needed for each update during online
    learning Fig. 7 Open in figure viewer PowerPoint Window of the World station on
    31 December 2014 (a) Offline LSTM versus hybrid LSTM, (b) Number Of epochs needed
    for each update during online learning In summary, the experiment results suggest
    that the hybrid learning framework improves the performance of ML models in dealing
    with anomalous metro passenger flow bursts. 5 Discussion 5.1 Efficacy of the hybrid
    learning framework Our experiment results suggest using different models for the
    ordinary regime and the anomalous regime is the critical design in our prediction
    framework. Online updating under anomalous traffic conditions can facilitate models
    to dynamically adapt to new patterns in the data. This feature is particularly
    important for ML models. Since anomalous large outflows tend to be non-recurring
    and be very different from historical values, it is imperative to do online updates
    if the anomalous outflow patterns are not learned before. Even though similar
    patterns have been observed in the training set, they are probably not learned
    by models because typical optimisation algorithms train a model by minimising
    the average error to which the data representing anomalous conditions contribute
    very little [10, 49]. Utilising real-time observations to update models can also
    be justified when simple models (SP, LR, as well as ARIMA) outperform many ML
    models under anomalous conditions. Since the predictions of these models are made
    either directly or linearly dependent on the recent observations. In the meantime,
    online updating under ordinary conditions is not necessary since ordinary outflow
    patterns are well learned by the offline models using a large amount of historical
    data. 5.2 Efficacy of the complex network method Our experimental results show
    that the index is a good indicator for anomalous large outflows and it provides
    a more convenient and sensitive way for anomaly detection (i) If the anomaly metric
    is based on the outflow directly, we have to determine an anomaly threshold for
    each time window individually since the scales of outflows in different time windows
    are different. In comparison, will always be close to zero regardless of the time
    as long as no anomaly occurs. Therefore, we can have a universal threshold for
    each subway station, which simplifies the anomaly detection and allows more data
    points to be involved in the clustering process for stable results. Fig. 8 Open
    in figure viewer PowerPoint In-degree of an AMN is more sensitive to the occurrence
    of crowding events than outflow . Red symbols represent the in-degree anomalous
    index , whereas blue symbols represent the outflow anomalous index for two crowding
    events (ii) The in-degree is a more sensitive indicator of crowding events than
    the outflow . Here, we define an in-degree anomalous index , where the parameter
    i represents the metro station where an anomalous large passenger flow occurred,
    the parameter t represents the time window, and the parameter tp represents the
    time period to which t belongs. Similarly, we define an outflow anomalous index
    . As shown in Fig. 8 for crowding events, is much more sensitive than to the occurrence
    of crowding events (especially at the beginning of the events). The reason for
    this greater sensitivity could be that the AMN approach is based on the collective
    signals of mobility fluxes (which are typical during crowding events), whereas
    the outflow is a single cumulative signal. This feature can also help the anomaly
    detection algorithm better discriminate between ordinary conditions and anomalous
    conditions. 6 Conclusions In this study, we propose a hybrid model to predict
    anomalous large outflow in an urban metro. The prediction accuracy is enhanced
    by combining the anomalous mobility network method with the online learning algorithm.
    The complex network index has the advantage of capturing anomalous and collective
    passenger flow patterns over the whole urban metro network, and is used to determine
    when to implement online learning. Compared with using multiple pre-trained models
    under different conditions, the biggest advantage of using online learning is
    that it can adapt to unexpected outflow patterns that never occurred in the past.
    For future research directions, we can also investigate the conditions when the
    anomalous small passenger flow happens. Although the sudden decrease in travel
    demand is less detrimental to metro systems than the sudden increase in travel
    demand, the information is useful for metro operators to adjust train frequency
    dynamically for the sake of resource optimising and energy saving. 7 Acknowledgments
    PW is supported by the National Natural Science Foundation of China (no. 71871224).
    9 Appendix: Model description and parameter tuning 9.1 Shadowing prediction The
    SP model is a naïve baseline that predicts the outflow at time using the last
    known outflow at time t. 9.2 Linear regression A simple linear model with the
    parameters fit by the stochastic gradient descent algorithm is used. 9.3 Auto-regressive
    integrated moving average ARIMA is a classical approach for time series analysis.
    An ARIMA model can be denoted as , where p is the order of the AR term, d is the
    differencing order, and q is the order of the moving averge term. Firstly, we
    use the Dickey–Fuller test [50] to test the stationarity of the outflow time series
    at each station for determining the d. If the time series is stationary, then
    d is set to 0; otherwise, we should perform the differencing of the time series
    until it reaches stationary. Since all outflow time series passed the stationarity
    test in this study, is set for all tested stations. Secondly, the autocorrelation
    function (ACF) and partial ACF (PACF) plots were used to determine q and p, respectively.
    Take the outflows at Window of the World station as an example (Fig. 9), q is
    set to 0 because the ACF tails off to zero, while p is set to 3 since the PACF
    cuts off after the third lag. The values of for six tested stations are listed
    in Table 3. The ARIMA models are fit by an exact maximum likelihood via Kalman
    filter using the Python statsmodels module [51]. Table 3. Parameters of the ARIMA
    models Station p d q Window of the World 3 0 0 Sea World 4 0 0 OCT Harbor 4 0
    0 COCO Park 3 0 0 Laojie 3 0 0 Grand Theater 4 0 0 Fig. 9 Open in figure viewer
    PowerPoint ACF and PACF plots of the outflow time series at Window of the World
    station 9.4 Support vector regression SVR is a regression process that tries to
    find a hyperplane that minimises the deviation of the data points to it [52].
    In this study, the SVR models were implemented using the Scikit-learn library,
    and hyperparameters were tuned through pilot experiments. There are two free parameters
    in the SVR model: C and epsilon. C is the regularisation parameter. A larger C
    leads to a more complex model that achieves higher training accuracy, while a
    lower C produces a simpler model with lower accuracy but more generality. Epsilon
    defines the tolerance distance within which no penalty is given to prediction
    errors. The default values for C and epsilon are 1 and 0.1 in the Scikit-learn
    library. We conduct a grid search to find the optimal combination of C and epsilon
    for each station in a range that and . 9.5 Fully-connected neural network An FCNN
    is composed of a series of fully-connected layers. Each neuron receives input
    from all the neurons in the previous layer and outputs information to the neurons
    in the next layer. The three-layer structure is widely used and suggested [53],
    which contains one input layer, one hidden layer, and one output layer. The number
    of neurons in the hidden layer was tuned through pilot experiments. We take the
    Window of the World station as an experiment target and run the model training
    process three times (Table 4). We find that the model is not benefiting from the
    increase of neurons, probably because even fewer neurons are enough for the regression
    task in this study. Therefore, we put 32 neurons in the hidden layer since it
    offers relatively higher accuracy and lower standard deviation. The rectified
    linear unit activation function is used in the hidden layer for capturing the
    non-linearity of outflow patterns, while a linear activation function is used
    in the output layer since the outflow prediction is a regression problem. The
    batch size for offline learning is 32, a widely used default value in the TensorFlow
    library [54], and the optimiser used to train the model is the Adam algorithm
    [55]. Table 4. Parameter tuning for FCNN Number of neurons Average loss of three
    experiments 16 0.0610 (±0.00035) 32 0.0607 (±0.00001) 64 0.0613 (±0.00025) 128
    0.0618 (±0.00006) 9.6 Long short-term memory LSTM is a type of recurrent neural
    network (RNN) that is used to solve the long-term dependency problem of an RNN.
    Also, the feature in processing sequential data makes LSTM a good option for time
    series prediction. Take an LSTM network with one LSTM unit in the hidden layer
    as an example, the LSTM unit will be unrolled into several time steps according
    to the size of the input sequence (four in this study). At each time step t, the
    LSTM unit takes the hidden state from the previous time step and the input from
    the current time step to produce the current state . The final state is passed
    to the output layer to produce the final prediction based on the learned weight
    matrix. In Fig. 10, we illustrate how information flow is passed through an LSTM
    unit: At a particular time step t, the forget gate takes and as input and outputs
    a number between 0 and 1 to determine how much information can be kept from the
    previous cell state (12) where is the output of the previous time step and is
    the input of the current time step. Next, the input gate is used to generate information
    for updating the cell state. The previous hidden state and current input will
    be transformed between 0 and 1 by a sigmoid function to decide how much information
    will be kept. 1 means the valve stays the same. A tanh function is used to scale
    and between −1 and 1 to regulate the network (13) (14) Next, the previous state
    is updated by forgetting some information and adding some new information (15)
    where denotes the Hadamard product, decides what information to forget and decides
    what information to add. Finally, the output gate determines the output based
    on the current cell state (16) (17) where is the output of the current time step,
    , , and are weight matrices of the forget gate, the input gate, and the output
    gate, and , , , and are the biases. and tanh are the widely used activation functions
    and , which are given by (18) (19) Since we use a common fully-connected layer
    as the final output layer, the final prediction can be calculated as (20) where
    denotes the weight matrix associated with the connection between the final output
    layer and the previous layer, X is the output from the previous layer after processed
    by the activation function, b is the bias, is the activation function for the
    final output (linear activation function is used in this study). Fig. 10 Open
    in figure viewer PowerPoint Illustration of the LSTM unit The parameter tuning
    process is similar to FCNN, but we tested more structures of the LSTM network
    (Table 5). Similar results such as FCNN are found for LSTM, hence we adopted a
    single hidden layer with 32 LSTM units for the LSTM network as well. We also used
    the same optimiser and batch size for training FCNN to train the LSTM network.
    Table 5. Parameter tuning for LSTM Number of neurons in each hidden layer Averge
    loss of three experiments 16 0.0501 (±0.00170) 32 0.0511 (±0.00064) 64 0.0523
    (±0.00080) 128 0.0518 (±0.00026) 32–16 0.0536 (±0.00232) 32–32 0.0513 (±0.00162)
    32–32–16 0.0511 (±0.00232) 8 References Citing Literature Volume14, Issue14 December
    2020 Pages 1987-1996 Citation Statements beta Supporting 0 Mentioning 12 Contrasting
    0 Explore this article''s citation statements on scite.ai powered by   Figures
    References Related Information Recommended Urban rail transit passenger flow forecast
    based on LSTM with enhanced long‐term features Dan Yang,  Kairun Chen,  Mengning
    Yang,  Xiaochao Zhao IET Intelligent Transport Systems Investigating long‐term
    vehicle speed prediction based on BP‐LSTM algorithms Li Yufang,  Chen Mingnuo,  Zhao
    Wanzhong IET Intelligent Transport Systems Vehicle type detection and passenger
    satisfaction analysis using smartphone sensors and digital surveys Arto Perttula,  Nhan
    Nguyen,  Jussi Collin,  Jani-Pekka Jokinen IET Intelligent Transport Systems Hybrid
    short‐term prediction of traffic volume at ferry terminal based on data fusion
    Weibin Zhang,  Jinjun Tang,  Henrickson Kristian,  Yajie Zou,  Yinhai Wang IET
    Intelligent Transport Systems Multistation coordinated and dynamic passenger inflow
    control for a metro line Xingrong Wang,  Jianjun Wu,  Xin Yang,  Xin Guo,  Haodong
    Yin,  Huijun Sun IET Intelligent Transport Systems Download PDF ABOUT THE IET
    IET PRIVACY STATEMENT CONTACT IET Copyright (2024) The Institution of Engineering
    and Technology. The Institution of Engineering and Technology is registered as
    a Charity in England & Wales (no 211014) and Scotland (no SC038698) Additional
    links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About Cookies Manage
    Cookies Accessibility Wiley Research DE&I Statement and Publishing Policies HELP
    & SUPPORT Contact Us Training and Support DMCA & Reporting Piracy OPPORTUNITIES
    Subscription Agents Advertisers & Corporate Partners CONNECT WITH WILEY The Wiley
    Network Wiley Press Room Copyright © 1999-2024 John Wiley & Sons, Inc or related
    companies. All rights reserved, including rights for text and data mining and
    training of artificial technologies or similar technologies.'
  inline_citation: '>'
  journal: IET intelligent transport systems (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Hybrid model for predicting anomalous large passenger flow in urban metros
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.36227/techrxiv.14896773
  analysis: '>'
  authors:
  - Christopher Nixon
  - Mohamed Sedky
  - Mohamed Hassan
  citation_count: 1
  full_citation: '>'
  full_text: '>

    LOG IN SIGN UP TechRxiv 9,151,197 views 4,235,640 downloads About TechRxiv TechRxiv
    (pronounced "tech archive") is an open, moderated preprint server for unpublished
    research in the areas of engineering, computer science, and related technology.
    https://www.techrxiv.org/ Public Documents 9174 Members by author by title by
    keyword Filter All Sort by Most Recent BIOENGINEERING 869 COMMUNICATION, NETWORKING
    AND BROADCAST TECHNOLOGIES 2283 COMPONENTS, CIRCUITS, DEVICES AND SYSTEMS 1068
    COMPUTING AND PROCESSING 3371 ENGINEERED MATERIALS, DIELECTRICS AND PLASMAS 248
    ENGINEERING PROFESSION 543 FIELDS, WAVES AND ELECTROMAGNETICS 853 GENERAL TOPICS
    FOR ENGINEERS 647 GEOSCIENCE 268 NUCLEAR ENGINEERING 70 PHOTONICS AND ELECTROOPTICS
    345 POWER, ENERGY AND INDUSTRY APPLICATIONS 1200 ROBOTICS AND CONTROL SYSTEMS
    879 TRANSPORTATION 387 AEROSPACE 265 SIGNAL PROCESSING AND ANALYSIS 1948 Terahertz
    Communications and Sensing for 6G and Beyond: A Comprehensive Review Wei Jiang
    and 14 more April 04, 2024 Next-generation cellular technologies, commonly referred
    to as the sixth generation (6G), are envisioned to support a higher system capacity,
    better performance, and network sensing capabilities. The terahertz (THz) band
    is one potential enabler to this end due to the large unused frequency bands and
    the high spatial resolution enabled by the short signal wavelength and large bandwidth.
    Different from earlier surveys, this paper presents a comprehensive treatment
    and technology survey on THz communications and sensing in terms of advantages,
    Rapid Feasibility Assessment of Energy Unit Integration in Distribution Networks
    Sicheng Gong and 2 more April 03, 2024 In contemporary heavy-load distribution
    networks, preceding feasibility assessment is imperative before incorporating
    additional energy units. However, the feasibility examination for massive combined
    operational scenarios of relevant units is computationally intensive with repetitive
    power flow calculations. To this end, this paper proposes a rapid assessment framework,
    the kernel of which is to learn from formerly examined scenarios, thus forming
    expansive feasible/infeasible regions to geometrically rule in/out subsequent
    scenarios. Without running the power flow computation in most scenarios, we accelerate
    the assessment process. Moreover, enlightened by heuristic hypersurface search,
    such prechecking efficiency can be further boosted. In a risk-averse manner, this
    framework can be conceptualized using the exact grid model. Especially, evidenced
    by testing on a 10.5kV distribution grid, the framework shows a significant assessment
    efficiency improvement and strict accuracy guarantee, where we observe at least
    76.13% assessment time reduction and zero accuracy loss in all testing cases.
    We anticipate this work to be a starting point for more sophisticated geometry-accelerating
    feasibility assessment methods. Improving Molecular De Novo Drug Design with Transformers
    Dhaval Soni and 7 more April 03, 2024 Drug design is undergoing a transformation
    as we challenge conventional methods by integrating state-of-the-art artificial
    intelligence with the intricate domain of molecular biology. At the heart of our
    endeavor lies a significant challenge: the scarcity of datasets containing active
    compounds for emerging target proteins. To confront this obstacle, we''re pioneering
    an innovative approach. We''re merging the advanced Generative Pre-trained Transformer
    (GPT) architecture with the nuanced capabilities of Long Short-Term Memory (LSTM)
    networks, with the aim of generating Simplified Molecular Input Line Entry System
    (SMILES) strings to unveil novel therapeutic pathways. Additionally, we''re employing
    a Bidirectional Encoder Representations from Transformers (BERT) pretraining strategy
    to enrich our model with comprehensive molecular data, including amino acid sequences
    and molecular SMILES datasets. Through meticulous fine-tuning on a meticulously
    curated protein-ligand complex dataset, we''re achieving precise conditional generation
    via autoregressive supervised learning. Our research introduces a groundbreaking
    method to assess molecular affinity, validated against established proteins, showcasing
    superior binding affinities compared to certain FDA-approved drugs in docking
    experiments. By pushing the boundaries of generative algorithms and establishing
    a robust framework for evaluating molecular affinity, we''re driving forward the
    field of de novo drug design, offering promising therapeutic avenues and enabling
    deeper exploration of the chemical landscape. Formalising a Gateway-based Blockchain
    Interoperability Solution with Event-B Guzmán Llambías and 2 more April 03, 2024
    A document by Guzman Llambias . Click on the document to view its contents. Magnetic
    Behavior of NO Fe-Si Sheets under Tensile and Compressive Stress Carlo Appino
    and 6 more April 03, 2024 The stress dependence of the magnetic properties of
    non-oriented Fe-Si steel sheets has been investigated by measurement and analysis
    of hysteresis loop, magnetization curve, and energy losses taken at different
    peak polarization values Jp (0.5 T – 1.5 T) between DC and f = 400 Hz. The salient
    feature of the material response to the stress lies in the monotonic deterioration
    of the soft magnetic properties, across the whole (Jp - f) domain, on passing
    from the maximum tensile stress (σ = +30 MPa) to the maximum compression (σ =
    -30 MPa). This is understood in terms of stress-induced redistribution of the
    domains between easy axes, making magnetic hardening by compression directly related
    to unfavorably directed domains and 90° domain-wallmediated magnetization transitions.
    The loss decomposition is carried out across the whole investigated frequency
    range, taking into account the skin effect at the highest frequencies. Quasi-static
    and dynamic losses follow a same trend with σ, both monotonically increasing on
    passing from the tensile to the compressive stress limits, according to the theoretically
    expected relationship existing between the hysteresis and the excess loss components.
    The latter is shown to identify the correlation regions where the magnetization
    is reversed of size comparable with the average grain size and loosely following
    the dependence of the loss figure on the applied stress. A bio-inspired hardware
    implementation of an analog spike-based hippocampus memory mo... Daniel Casanueva-Morato
    and 4 more April 03, 2024 The need for processing at the edge the increasing amount
    of data that is being produced by multitudes of sensors has led to the demand
    for mode power efficient computational systems, by exploring alternative computing
    paradigms and technologies. Neuromorphic engineering is a promising approach that
    can address this need by developing electronic systems that faithfully emulate
    the computational properties of animal brains. In particular, the hippocampus
    stands out as one of the most relevant brain region for implementing auto associative
    memories capable of learning large amounts of information quickly and recalling
    it efficiently. In this work, we present a computational spike-based memory model
    inspired by the hippocampus that takes advantage of the features of analog electronic
    circuits: energy efficiency, compactness, and real-time operation. This model
    can learn memories, recall them from a partial fragment and forget. It has been
    implemented as a Spiking Neural Networks directly on a mixed-signal neuromorphic
    chip. We describe the details of the hardware implementation and demonstrate its
    operation via a series of benchmark experiments, showing how this research prototype
    paves the way for the development of future robust and low-power mixed-signal
    neuromorphic processing systems. Exploratory Study of oneM2M-based Interoperability
    Architectures for IoT: A Smart Cit... VJS Pranavasri and 6 more April 03, 2024
    The advent of the Internet of Things (IoT) has ushered in transformative possibilities
    for smart cities, with the potential to revolutionize urban living through enhanced
    connectivity and data-driven decision-making. However, the effective realization
    of IoT in smart cities hinges upon the seamless interoperability of diverse devices
    and systems. To address this critical need, the oneM2M standards initiative has
    emerged as a foundational framework for IoT interoperability. In this research
    paper, we perform an exploratory analysis of three prominent open-source oneM2M
    based interoperability systems-Mobius, OM2M, and ACME. We leverage an existing
    large-scale system provided by our Smart City Living Lab deployed at IIIT Hyderabad,
    sprawling a 66-acre campus featuring over 370 nodes across eight verticals. We
    investigate the architectural characteristics of each solution, considering their
    strengths and limitations in facilitating IoT interoperability. Through this analysis,
    our paper aims to provide valuable insights for stakeholders seeking to implement
    IoT interoperability solutions in the context of smart cities. By evaluating the
    strengths and limitations of Mobius, OM2M, and ACME, we seek to offer guidance
    for selecting the most suitable solution. Our analysis reveals that the optimal
    framework choice depends on specific quality constraints: Mobius excels in performance,
    while ACME offers advantages in ease of setup for smaller-scale implementations.
    Comparing Concepts of Service Blocking Queues in Hardware-in-the-Loop Systems
    Tobias Konheiser and 3 more April 03, 2024 ZF is developing an autonomous driving
    system, which requires extensive testing of the developed devices and software
    on hardware-in-the-loop (HIL) systems. Therefore, a robust and high-performing
    HIL system is essential. The purpose of a HIL system is to replay recorded data
    to the device-undertest. Recordings are loaded, processed and streamed to the
    deviceunder-test with real-time requirements. This streaming chain includes processing
    nodes and queues. This requires careful management of queue configurations. An
    overflow in the queue will result in packet loss, while an underflow may violate
    the real-time constraint. This study aims to develop and evaluate concepts for
    service blocking queues. These concepts block or pause the incoming service to
    a queue when necessary to avoid queue overflows and associated data loss. However,
    an out-of-the-box solution is not available and different approaches affect the
    behaviour and performance of the system. Therefore, the developed concepts are
    evaluated against each other and against the existing system based on selected
    performance parameters in specific scenarios. The scenarios cover a wide range
    of situations, reflecting standard input data with varying numbers of parallel
    streams and bottleneck scenarios forcing queue overflows or blockages. The developed
    service blocking queue concepts eliminate data loss in all scenarios, but introduce
    overhead, resulting in reduced system performance. However, the service blocking
    queue concept using a modified token-bucket approach proved to be the best solution,
    as the elimination of data loss justifies the additional overhead. This concept
    is proposed for implementation and deployment on the HIL system. Generative AI-Based
    Text Generation Methods Using Pre-Trained GPT 2 Model Rohit Pandey and 7 more
    April 03, 2024 A text generation model is a machine learning model that uses neural
    networks, especially transformers architecture to generate contextually relevant
    text based on linguistic patterns learned from extensive corpora. The models are
    trained on a huge amount of textual data so that they can model and learn complex
    concepts of any language like its grammar, vocabulary, phrases, and styles. FlowDep
    - An efficient and optical-flow-based algorithm of obstacle detection for aut...
    Chen-Fu Yeh and 7 more April 03, 2024 Obstacle detection is crucial for the safety
    and efficiency of autonomous vehicles. For mini-vehicles such as palm-sized drones,
    it is a challenge to implement traditional methods like Lidar due to high costs
    and physical constraints. Vision-based deep learning approaches, while accurate,
    are too resource-intensive for the mini-vehicles. To address this issue, we introduce
    Flowdep, a novel optical-flow-based algorithm inspired by the low-resolution but
    efficient motion-detection mechanisms in insects. Flowdep combines optic flow
    and IMU (or positioning information) to estimate the depth of every image pixel.
    We also generate a variant of Flowdep using the artificial neural network (Flowdep-ANN).
    Our tests show that Flowdep and Flowdep-ANN are 5.8 to 114.7 times faster than
    the DNN networks we tested, while the accuracies of Flowdep and Flowdep-ANN are
    on par with these networks. We further tested Flowdep and Flowdep-ANN on a small
    autonomous vehicle with Raspberry Pi4 as the computing platform, and both models
    successfully performed real-time object detection. The present work demonstrates
    the potential of using optical flow as an efficient approach to estimate depth
    and detect obstacles in resource-constrained mini-vehicles. Misinformative Data
    Visualizations in the Sports Media Domain Drew Scott April 03, 2024 Sports are
    data-driven: individual performances are measured using statistics and teams leverage
    data analytics to outperform competition. Sports media-which is created by media
    outlets, teams, and individuals-engage its consumers by creating narratives about
    the sport, teams, and players. Due to the importance of data in the sports world,
    data visualizations are a pillar in the sports media landscape. These data visualizations,
    while appearing to accurately convey data to its consumers, can be misinformative;
    media creators often have incentives to present specific narratives which don''t
    always fit the data. This work contributes to an existing misinformative data
    visualization taxonomy. In doing so, it makes it easier to understand the techniques
    and design choices used to create misinformative visualizations in all domains,
    not only in sports media. A Survey of RFID Authentication Protocols Drew Scott
    April 03, 2024 "Radio frequency identification" (RFID) systems are ubiquitous
    in today''s world. In an RFID system, it is a desirable to attain mutual authentication
    between a reader and a tag before commencing application-level communications.
    This is because tags should not share secret information with unknown parties
    and readers need to defend against tag impersonation. Authentication protocols
    designed for communication between computers, however, are not appropriate for
    RFID systems because tags are extremely resource constrained (low energy, small
    memory, etc.). Thus, there have been many attempts to design secure and practical
    authentication protocols for RFID systems over the years since RFID systems became
    prevalent. This survey summarizes and compares these protocols. The Effect of
    Multipath in Distributed Arrays with Time Reversal Hassna Ouassal and 2 more April
    03, 2024 This article examines the effect of multipath channels on the performance
    of distributed arrays that employ time reversal. A model of the signal received
    from a distributed array is formulated, and a statistical analysis of the variation
    in signal power in the presence of phase noise and multipath is given. We present
    the impact these nonidealities have on received signal power, and we analyze the
    received power for three specific cases: continuous waveform, impulse waveform,
    and modulated rectangular pulse waveform in the presence of standard channel models.
    It is shown that for larger arrays in multipath channels, the change in power
    between coherent and incoherent states converges to the line-of-sight channel.
    It is further shown that in a line-of-sight channel time-reversal completely cancels
    unknown channel delays resulting in coherent signals from all nodes in a distributed
    array, while in a multipath channel only the main diagonal round-trip paths are
    coherent. Nevertheless, this additional benefit improves signal coherence in complex
    channels and can aide in distributed array synchronization using two-way time
    transfer. Disproof of Hodge Conjecture by Graph Theory Jihyeon Yoon April 02,
    2024 Hodge conjecture is turned out to be false in extension of graph theory based
    on its algebraic attribute. Hash3D: Training-free Acceleration for 3D Generation
    Xingyi Yang and 1 more April 02, 2024 The evolution of 3D generative modeling
    has been notably propelled by the adoption of 2D diffusion models. Despite this
    progress, the cumbersome optimization process per se presents a critical hurdle
    to efficiency. In this paper, we introduce Hash3D, a universal acceleration for
    3D generation without model training. Central to Hash3D is the insight that feature-map
    redundancy is prevalent in images rendered from camera positions and diffusion
    time-steps in close proximity. By effectively hashing and reusing these feature
    maps across neighboring timesteps and camera angles, Hash3D substantially prevents
    redundant calculations, thus accelerating the diffusion model''s inference in
    3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly,
    this feature-sharing mechanism not only speed up the generation but also enhances
    the smoothness and view consistency of the synthesized 3D objects. Our experiments
    covering 5 textto-3D and 3 image-to-3D models, demonstrate Hash3D''s versatility
    to speed up optimization, enhancing efficiency by 1.3 ∼ 4×. Additionally, Hash3D''s
    integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing
    text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly
    30 seconds. The code is provided in https://github.com/Adamdad/hash3D. Area and
    Power Efficient Implementation of Configurable Ring Oscillator PUF Enas Abulibdeh
    and 4 more April 02, 2024 Physically Unclonable Function (PUF) is an emerging
    hardware security primitive that provides a promising solution for lightweight
    security. PUFs can be used to generate a secret key that depends on the random
    manufacturing process variation of the device for lightweight authentication and
    device identification. This work proposes an optimized version of the Configurable
    Ring Oscillator (CRO) PUF that aims to reduce power consumption and area overhead.
    The proposed design eliminates the duplication of ROs, reduces the switching activity,
    and introduces the inter-stage delay as an additional source of randomness. The
    proposed PUF has been implemented in 22nm FDSOI technology using the Synopsys
    tools. A comprehensive security analysis has been acquired utilizing Challenge-Response
    Pairs collected from 8 chips. Results show an average of 49.42%, 38.25%, 9.95%,
    and 45.5% for uniformity, diffuseness, reliability, and uniqueness, respectively.
    Compared with the state-of-the-art, the proposed design achieves an area and power
    reduction of 75% and 65.1%, respectively. With the proposed PUF delivering 10
    32 CRPs, it is classified as a strong PUF. Additionally, the proposed design passes
    NIST tests and achieves an average prediction accuracy of 67.1% of machine learning
    modeling. A Hero Or A Killer? Overview Of Opportunities, Challenges, And Implications
    Of Text-T... Mijat Kustudic and 1 more April 02, 2024 SORA is a text-to-video
    model that can create videos based on simple user prompts. The model promises
    to revolutionize the way content is created. When SORA is released to the general
    public, it may transform a wide array of industries but also pose significant
    challenges and risks. This research aims to provide a comprehensive understanding
    of SORA''s opportunities, challenges, and implications. It explores its potential
    applications in film-making, education, gaming, advertising, accessibility, healthcare,
    and social media content creation. Additionally, it delves into its potential
    challenges and risks, including misinformation, privacy concerns, bias, regulatory
    complexities, and dependence on technology. This research provides important recommendations
    to promote responsible deployment of the AI model. Advancements and Challenges
    in Robot Grasping and Manipulation for Aspiring Researche... Claudio Zito April
    02, 2024 Robot grasping and manipulation represent pivotal aspects of robotics
    research with profound implications for the future of autonomous systems. This
    report delves into the intricacies of designing robotic hands, the hurdles in
    creating robust manipulation actions, and the advancements in the field that poised
    to catalyze a new era of autonomy. Drawing inspiration from science fiction''s
    portrayal of robotics, we bridge the conceptual gap between fiction and ongoing
    real-world technical research, aiming to provide a comprehensive overview for
    students interested in robotics. ← Previous 1 2 3 4 5 6 7 8 9 … 509 510 Next →
    TechRxiv | Powered by Authorea.com Home About Submission Guidelines FAQs Terms
    of Use Privacy Policy Contact Us'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: https://www.techrxiv.org/articles/preprint/SALAD_An_Exploration_of_Split_Active_Learning_based_Unsupervised_Network_Data_Stream_Anomaly_Detection_using_Autoencoders/14896773/1/files/28843851.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'SALAD: An Exploration of Split Active Learning based Unsupervised Network
    Data Stream Anomaly Detection using Autoencoders'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.36227/techrxiv.14896773.v1
  analysis: '>'
  authors:
  - Christopher Nixon
  - Mohamed Sedky
  - Mohamed Hassan
  citation_count: 1
  full_citation: '>'
  full_text: '>

    Posted on 17 Jul 2020 — CC-BY 4.0 — https://doi.org/10.36227/techrxiv.14896773.v1
    — e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed.
    They should not b...

    SALAD: An Exploration of Split Active Learning based

    Unsupervised Network Data Stream Anomaly Detection using

    Autoencoders

    Christopher Nixon 1, Mohamed Sedky 2, and Mohamed Hassan 2

    1Staﬀordshire University

    2Aﬃliation not available

    October 30, 2023

    Abstract

    Machine learning based intrusion detection systems monitor network data streams
    for cyber attacks. Challenges in this space

    include detection of unknown attacks, adaptation to changes in the data stream
    such as changes in underlying behaviour, the

    human cost of labeling data to retrain the machine learning model and the processing
    and memory constraints of a real-time

    data stream. Failure to manage the aforementioned factors could result in missed
    attacks, degraded detection performance,

    unnecessary expense or delayed detection times. This research evaluated autoencoders,
    a type of feed-forward neural network, as

    online anomaly detectors for network data streams. The autoencoder method was
    combined with an active learning strategy to

    further reduce labeling cost and speed up training and adaptation times, resulting
    in a proposed Split Active Learning Anomaly

    Detector (SALAD) method.

    The proposed method was evaluated with the NSL-KDD, KDD Cup 1999, and UNSW-NB15

    data sets, using the scikit-multiﬂow framework.

    Results demonstrated that a novel Adaptive Anomaly Threshold method,

    combined with a split active learning strategy oﬀered superior anomaly detection
    performance with a labeling budget of just

    20%, signiﬁcantly reducing the required human expertise to annotate the network
    data. Processing times of the autoencoder

    anomaly detector method were demonstrated to be signiﬁcantly lower than traditional
    online learning methods, allowing for

    greatly improved responsiveness to attacks occurring in real time. Future research
    areas are applying unsupervised threshold

    methods, multi-label classiﬁcation, sample annotation, and hybrid intrusion detection.

    1

    1

    SALAD: An Exploration of Split Active Learning

    based Unsupervised Network Data Stream

    Anomaly Detection using Autoencoders

    Christopher Nixon, Mohamed Sedky, and Mohamed Hassan

    Abstract—Machine learning based intrusion detection systems monitor network data
    streams for cyber attacks. Challenges in this

    space include detection of unknown attacks, adaptation to changes in the data
    stream such as changes in underlying behaviour, the

    human cost of labeling data to retrain the machine learning model and the processing
    and memory constraints of a real-time data

    stream. Failure to manage the aforementioned factors could result in missed attacks,
    degraded detection performance, unnecessary

    expense or delayed detection times. This research evaluated autoencoders, a type
    of feed-forward neural network, as online anomaly

    detectors for network data streams. The autoencoder method was combined with an
    active learning strategy to further reduce labeling

    cost and speed up training and adaptation times, resulting in a proposed Split
    Active Learning Anomaly Detector (SALAD) method. The

    proposed method was evaluated with the NSL-KDD, KDD Cup 1999, and UNSW-NB15 data
    sets, using the scikit-multiﬂow framework.

    Results demonstrated that a novel Adaptive Anomaly Threshold method, combined
    with a split active learning strategy offered superior

    anomaly detection performance with a labeling budget of just 20%, signiﬁcantly
    reducing the required human expertise to annotate the

    network data. Processing times of the autoencoder anomaly detector method were
    demonstrated to be signiﬁcantly lower than

    traditional online learning methods, allowing for greatly improved responsiveness
    to attacks occurring in real time. Future research

    areas are applying unsupervised threshold methods, multi-label classiﬁcation,
    sample annotation, and hybrid intrusion detection.

    Index Terms—Active Learning, Online Learning, Autoencoders, Anomaly Detection,
    Intrusion Detection System.

    !

    1

    INTRODUCTION

    Intrusion Detection Systems (IDS) monitor a computer net-

    work for cyber attacks. Traditional intrusion detection tech-

    niques rely on human subject matter experts to carefully

    produce signatures that can accurately detect a cyber attack

    at the network layer. For over a decade research has focused

    on improving IDS with machine learning (ML) methods in

    order to reduce the overall demand for human effort [1].

    The majority of this research has centred around misuse

    detection whereby the ML based IDS is trained using a data

    set in which all cyber attacks are labeled, the drawback of

    this being that only the labeled attacks will be known to the

    model, missing unknown or new attacks, and that labeling

    of the initial data set is a time consuming and complex task

    prone to human error. An alternative to misuse detection is

    to use an anomaly detector whereby only the ‘normal’ net-

    work data is learned and any signiﬁcant deviations treated

    as an anomaly meaning that new attacks will be detected,

    a challenge with this approach is the potential for false

    positives.

    IDS capture network packet data directly from the net-

    work, requiring efﬁcient real-time processing of each new

    packet as part of a continuous data stream. This network

    data stream is non-stationary and can change over time,

    a characteristic known as concept drift, which requires the

    ML model to adapt in order that detection performance is

    not degraded [2]. Adaptation requires detecting a change

    in the posterior probability of a class label, necessitating

    the ground truth to be known. Active learning (AL) is an

    attempt to lower the labeling cost, and speed up the adap-

    tion times, of change detection by employing uncertainty or

    random strategies according to a labeling budget [3]

    An hypothesis that this research aims to test is that

    anomaly detectors monitoring non-stationary network data

    streams will experience increased false positives over time,

    which can be corrected by applying adaptation techniques

    to update the anomaly detector. This will be expanded

    by a further hypothesis that active learning strategies can

    provide good adaptation with minimal labeling cost, and

    reduced learning times, for anomaly detection.

    Unsupervised learning allows for a model to be trained

    without all the class labels being known, typically achieved

    by learning a representation of the underlying data struc-

    ture. Common unsupervised techniques, such as cluster-

    ing, are impeded by high degrees of time complexity and

    memory usage [4]. Models based on neural networking

    are gaining increased attention in the IDS ﬁeld and a type

    of feed-forward neural network, the autoencoder, is able

    to learn the representation of data without class labels by

    encoding a latent representation of the data, which can be

    utilised for anomaly detection by calculating the error of

    the decoded output from the original, and comparing to a

    predetermined anomaly threshold [5]. This research aims to

    test the hypothesis that autoencoders provide an effective

    online anomaly detector for network data streams when

    combined with active learning methods.

    The remainder of this paper is organised as follows:

    Section 2, introduces related work; Section 3, describes the

    proposed Split Active Learning Anomaly Detector (SALAD)

    2

    method; Section 4, presents the evaluation results; Section

    5, discusses how SALAD provides a low cost anomaly

    detector for network data streams; and Section 6, presents

    conclusions.

    2

    RELATED WORK

    2.1

    Neural Networking Anomaly Detection

    Intrusion detection systems can be either anomaly based or

    misuse based, where the former learns the normal behaviour

    and detects deviations, allowing for detection of previously

    unseen, unknown attacks, and the latter learns known attack

    signatures resulting in high levels of detection accuracy

    [6]. A challenge with network data streams is that they

    generate large volumes of data that become increasingly

    expensive for a human expert to analyse and correctly

    label. Anomaly detectors are beneﬁcial because they only

    need to learn the representation of a single ‘normal’ class

    from which anomalies can be distinguished meaning that

    new, previously unseen, attacks can be detected without

    requiring new data labels and re-training of the model [6].

    Unsupervised machine learning methods are well suited

    to the anomaly detection task as they can learn the repre-

    sentation of the underlying data to determine normal and

    anomaly classes [6], as well as learning useful features that

    better separate the classes. Buczak and Guven [1] have

    provided a comprehensive survey of IDS machine learn-

    ing techniques, including anomaly detection, in most cases

    misuse and anomaly detection are combined into a hybrid

    system. This review brieﬂy introduces recent studies within

    the unsupervised anomaly detection space, adopting neural

    networking methods familiar to the visual processing area,

    for comparison to the proposed approach.

    Alrawashdeh and Purdy [7] evaluated Restricted Boltz-

    mann Machines (RBM) arranged into a deep belief net-

    work combined with a logistic regression classiﬁer trained

    using back propagation. Although the study claims to be

    ‘anomaly’ based the model is actually trained to identify

    known classes so would be more ‘misuse’ based in its

    approach. The accuracy of their model, with the 10% KDD

    Cup 1999 data set, is 97.91% [7]. The authors further build on

    there work by replacing the RBM activation function with a

    novel ‘Adaptive Linear Function’ (ALF) for intrusion detec-

    tion with the aim of improving accuracy and convergence

    time [8]. Evaluated with KDD Cup 1999 and NSL-KDD data

    sets, the accuracy was 98.59% and 96.2% respectively [8].

    Roshan et al. [9] proposed a novel intrusion detection

    approach using a Clustering Extreme Learning Machine

    (CLUS-ELM) method. This method allows for both unsuper-

    vised and supervised updates to the model, using a decision

    maker element to perform informed change detection based

    on the cluster output, in this design unsupervised refers

    to guessing the correct cluster for a given data sample as

    opposed to being told the label by a ‘human expert’. The

    mean square error calculation used by the decision maker

    will still require the ground truth to be known. Results were

    evaluated using the NSL-KDD data set, with a detection

    rate for known attacks of 84% and 81% for unsupervised

    and supervised modes, 77% and 84% for unknown attacks,

    where the false positive rate was less than 3% [9]. The

    author remarks that the better unsupervised detection rates

    for known attacks compared to the supervised ones are

    unexpected and could be due to inaccuracies in the NSL-

    KDD data set [9].

    Chen, Cao and Mai [10] proposed an ofﬂine anomaly

    detection method whereby Convolutional Neural Networks

    (CNN) are used to extract features which are then con-

    densed into a spherical hyperplane by a deep Support Vec-

    tor Data Description (deep-SVDD) technique. The method

    is trained on normal samples only so that such normal

    samples concentrate around the center of the sphere and

    attack samples concentrate on the outside as outliers allow-

    ing them to be detected as a one-class anomaly detector.

    Their method was evaluated with the KDD Cup 1999 data

    set, achieving an accuracy of 96% when all attack types are

    present.

    Hassan et al. [11] proposed a combined CNN for feature

    reduction and Weight Dropped, Long Short Term Mem-

    ory (WDLSTM) network for representation of dependencies

    among features, using the connection drop out regularisa-

    tion method. The proposed supervised learning network

    was evaluated with the UNSW-NB15 data set, returning an

    F1-Score of 0.88 for abnormal samples and overall accuracy

    of 97.17% via ofﬂine holdout training.

    The reviewed studies all demonstrate different network

    topologies for cyber intrusion detection, all of which have

    elements of supervised learning and traditional ofﬂine batch

    training. They do not address the problem of a truly unsu-

    pervised anomaly detector for online data streams as will be

    explored in this paper.

    2.2

    Autoencoder Anomaly Detection

    An autoencoder is a type of feed-forward neural network

    that uses an encoding function to produce a latent code

    representation of the input data, and a decoding function to

    reconstruct the input from the code representation [12]. The

    mean square error between the reconstructed output and

    original input can be calculated using equation 1, where f

    is the encoding function and g is the decoding function [12],

    which can then be compared to an anomaly threshold to

    label a sample as either normal or anomalous.

    ˆX = g(f(X))

    RE = 1

    n

    n

    X

    j=1

    (Xj − ˆXj)2

    (1)

    In our previous work [12], we reviewed autoencoder

    based anomaly intrusion detection methods, whereby single

    layer denoising models [13], Long Short Term Memory

    (LSTM), Recurrent Neural Network [14], [15], ensembled

    stacked autoencoders [16], [17], and sparsely connected

    networks [18], [15] were demonstrated across a range of

    IDS data sets. Vaiyapuri and Binbusayyis [19] evaluated a

    number of autoencoder network architectures for anomaly

    detection, ﬁnding the use of a contractive penalty to regulate

    the network provided the best performance when evaluated

    ofﬂine using the NSL-KDD and UNSW-NB15 data sets.

    A number of methods were proposed in the literature

    to determine the anomaly threshold, an important param-

    eter in deciding whether to label a sample as a positive

    3

    detection. The threshold can be set to the average RE value

    observed during training [19]. Na¨ıve Anomaly Threshold

    (NAT) sets the threshold at the maximum observed RE

    during training [16]. Stochastic Anomaly Threshold (SAT)

    [13] sets the threshold based on the best observed accuracy

    when stepping through threshold values between the mean

    and 3 * standard deviation of the normal sample distribu-

    tion. Nicolau and McDermott [13] proposed an anomaly

    threshold method using Kernel Density Estimation.

    Aiming to ﬁnd an optimal network conﬁguration, we

    evaluated in [12], an undercomplete autoencoder, regulated

    with connection dropout, with a prequential online test

    using the KDD Cup 1999 and UNSW-NB15 data sets. Ap-

    plying a single layer autoencoder with dropout probability

    of 0.1, using the Stochastic Anomaly Threshold method,

    provided an accuracy of 98% and F1-score of 0.812, using

    the KDD Cup 1999 data set, with a signiﬁcantly improved

    running time compared to traditional Na¨ıve Bayes (NB) and

    Hoeffding Adaptive Tree (HAT) online methods. Evaluation

    on the UNSW-NB15 data set using a 3-layer network and

    dropout probability of 0.2 returned an accuracy of 79.1% and

    F1-score of 0.703. The results showed that the SAT threshold

    performed better than the NAT, and that more complex data

    sets beneﬁt from experimenting with the number of layers

    and regularisation of the network.

    2.3

    Concept Drift Detection with Active Learning

    Non-stationary network data streams may experience real

    concept drift [2], whereby the posterior probability of classes

    will change over time due to changes in network behaviors,

    the cause of which could be either benign or adversarial in

    nature. The posterior probability is deﬁned as p(y|X) which

    represents the probability of class y given an observation

    X [2]. Autoencoders determine outliers using the RE-score,

    based on the hypothesis that adversarial behaviour deviates

    from the learned ‘normal’ representation resulting in scores

    above the anomaly threshold. Real concept drift presents a

    challenge that the aforementioned hypothesis will weaken

    overtime, with changing benign data also scoring above

    threshold, raising the false positive rate. Increasing the

    anomaly threshold does not present an optimal solution

    as although the false positive rate may lower, the false

    negative rate could increase and so is not recommended.

    The hypothesis of this research was that a change in under-

    lying ‘benign’ network behaviour will result in a raised false

    positive rate and that learning the representation of the new

    behaviour will remedy this effect. Note that the change in

    benign activity could be from an unplanned change such as

    a network fault, in which case the usefulness of the anomaly

    detector is extended to a fault detector, however for the

    purposes of this research this will not be considered further.

    Change detection is a set of methods that proactively

    monitor the data stream for concept drift [2]. Traditional

    methods such as adaptive windowing and statistical process

    control (SPC) [2], rely on fully supervised labels and are

    therefore not well suited to applications where data label-

    ing is expensive, such as network data streams. Moreover

    unsupervised techniques that rely solely on monitoring a

    change compared to a reference distribution will not always

    detect real concept drift [20]. Sethi and Kantardzic [21]

    proposed a semi-supervised Margin Density Drift Detector

    (MD3) to reduce labeling costs through an active learning

    approach. First, using an unsupervised method, samples

    that fall below an uncertainty threshold are added to the

    margin. Density of the margin is compared to a training

    reference distribution to detect drift before conﬁrming by

    testing accuracy with data labels, sensitivity can be adjusted

    through a varying factor of the reference distribution’s stan-

    dard deviation. A fading factor is utilised to give greater

    importance to more recent samples within a moving average

    of margin density [21]. MD3 can work with ensembles,

    calculating if a sample should be included within the margin

    by comparing the distance between the mean predicted class

    probabilities to the margin threshold (θ), given by equation

    2. A possible beneﬁt of this approach would be that the

    change in density of uncertain samples that are borderline

    outliers could indicate a concept drift that requires further

    analysis, prompting further action such as re-training. As

    the anomaly detector only requires labeled normal data to

    re-train, this would be a cheaper approach to other methods

    that require fully labeled data. A possible drawback is that

    the frequency of drifts could demand increased human

    expertise. Evaluation with the NSL-KDD data set reported

    an accuracy of 89.4 and 89.9 % using the SVM and random

    subspace ensemble methods, respectively where the ﬁrst

    15% of the data stream is used as a training set. The total

    labeling cost was 7.9%.

    (p(ˆyc1|X) − p(ˆyc2|X)) ≤ θ

    (2)

    Shan et al. [22] also proposed an AL change detection

    strategy based on margin uncertainty, ‘OALEnsemble’, how-

    ever in this approach the ensemble members are trained on

    different windows of the data set, with a stable classiﬁer

    and a series of short window ‘dynamic’ classiﬁers that are

    continually replaced as new blocks of the data stream are

    processed, to balance the detection of both sudden and grad-

    ual concept drifts. Similar to [21], labeling is restricted to

    samples within the uncertainty margin, with the addition of

    a random labeling algorithm to randomly include samples

    outside of the margin where drift may also be occurring [22].

    The stable classiﬁer is incrementally trained with all new

    data, whilst dynamic classiﬁers are only trained on the most

    recent block and given a weight, providing importance to

    more recent data [22]. The incremental update of the stable

    classiﬁer is restricted to models that feature local replacement

    such as very fast decision trees (VFDT) [2], and so would

    not be appropriate for autoencoder methods. The labeling

    rate is constrained by pro-actively adjusting the sensitivtiy

    threshold in order to manage the cost of the algorithm

    during periods of high uncertainty. Random sampling is

    desirable as it enables the classiﬁer to be trained from the

    whole distribution, reducing bias [3]. The idea of gradu-

    ally retraining the autoencoders with new ‘normal’ data

    in response to concept drift, whilst retaining the previous

    models for a period of time, moderating their importance

    with a weight scheme, could allow for the detection of

    both gradual and sudden changes in benign behaviour,

    however the problem of global replacement must be carefully

    considered as training on small data sets could degrade the

    autoencoders ability to represent normal data.

    4

    Dang [23] evaluated AL for IDS, using a novel strategy

    with the Na¨ıve Bayes classiﬁer, selecting instances with the

    greatest distance from the population distribution of proba-

    bilities under the hypothesis that a bigger change of P(A|B)

    reﬂects a rare event that should be learned. The method

    was evaluated with the CICIDS 2012 data set, achieving an

    AUC-score of 90% compared to 85% with the uncertainty

    strategy with 10% of labeled data, and performance decreas-

    ing beyond this. The author argues that this indicates that

    good quality data is more important over larger volumes of

    data [23]. It may also be true that the method reduces class

    imbalance by proactively sampling examples with weaker

    performance that could reﬂect minority classes.

    Zhang et al. [24] evaluted an Open-CNN method trained

    by AL labeling the ‘unknown’ detected attacks. Accuracy

    with the CTU data set was near equivalent to 100% label cost

    at just 1% of labeled attacks using an uncertainty strategy,

    demonstrating that only a low label cost is necessary to train

    the ML model.

    ˇZliobait˙e et al. [3] discussed three requirements for AL

    strategies: 1) balancing the labeling budget over time, 2)

    detect changes anywhere within the problem space and 3)

    preserve the distribution for unbiased change detection. A

    number of strategies were evaluated against these require-

    ments, including ﬁxed uncertainty as demonstrated by [21],

    and uncertainty with randomisation, whereby the sensitivity

    threshold is randomly selected from a standard distribution

    to occasionally include samples outside of the uncertainty

    margin. Fixed uncertainty is only able to satisfy requirement

    one, and randomised uncertainty satisﬁes requirement one

    and two, but neither can preserve the probability density of

    labeled data compared to the original distribution, which

    can bias the model [3]. A further split strategy is intro-

    duced which satisﬁes all three requirements by splitting

    the the data stream into two, using uncertainty and ran-

    dom strategy exclusively on either stream. Both streams

    are used for training, but only the randomised stream is

    used for change detection [3]. Shan et al. [22] presents a

    split strategy, although in this approach adaptation is blind,

    based on incrementally updating the ensemble members

    with both uncertainty and random labels, offering no pro-

    active change detection, this could reduce overall adaptation

    speeds [2].

    An objective of this research was to satisfy all three

    AL requirements outlined by ˇZliobait˙e et al. [3]. MD3 [21]

    will be biased towards uncertain samples and will miss

    change occurring outside of the margin which will affect

    overall detection performance. The work of Shan et al. [22]

    could be further improved by introducing pro-active change

    detection method to the randomly labeled data as suggested

    by ˇZliobait˙e et al. [3] in order to increase adaptation time. In

    this research random, uncertainty, variable uncertainty, split

    and blind strategies are compared. The proposed hypothesis

    is that only the split strategy with informed change detec-

    tion approach will be able to satisfy all three requirements

    and that the change detection approach will offer faster

    adaptation times to a blind approach. The informed ap-

    proach can use a well known change detector such as Drift

    Detection Method (DDM) [25] to monitor the classiﬁcation

    error of the anomaly detector.

    3

    METHODS

    The aim of this research was to explore that autoenoders

    can provide a low cost online anomaly detection solution

    when combined with AL methods. In our previous work

    [12] we evaluated dropout probability, NAT with decay and

    SAT anomaly thresholds, and single vs stacked network

    structure, to ﬁnd optimal autoencoder parameters. Build-

    ing on this work, in this paper, we further introduced a

    novel Adaptive Anomaly Threshold (AAT) method and also

    evaluated an AL based Active Stream Framework (ASF)

    [3] with which we compared blind, random, uncertainty,

    variable uncertainty and split AL strategies. The uncertainty

    strategy was adapted for use with autoencoders using a

    novel distance from RE method. All methods were evalu-

    ated using a prequential, interleaved test-then-train method

    [2], whereby the model is ﬁrst tested on a previously unseen

    sample before training in a chunk wise fashion [12], after an

    initial period of pre-training. Results were compared against

    traditional Na¨ıve Bayes (NB) and Hoeffding Adaptive Tree

    (HAT) online learning methods using the KDD Cup 19991

    10% [26] and UNSW-NB152 [27] data sets.

    The Keras3 neural networking [28], version 2.3.1, and

    Scikit-Multiﬂow4 stream learning [29], version 0.4.1, frame-

    works for Python were used for this evaluation. Evaluations

    were ran on a Windows 10 64bit PC with Intel i7 1.8GHz

    processor and 8GB RAM.

    Observed metrics during evaluation included: accuracy,

    F1-score, kappa and total running time. For prequential

    evaluation the scikit-multiﬂow default of updating evalu-

    ation metrics every 200 samples was used.

    3.1

    Adaptive Anomaly Threshold

    From evaluating the make up of the data stream and per-

    formance achieved with both the NAT and SAT threshold

    methods [12] a proposed hypothesis was that chunks of

    the data stream that contained only normal samples beneﬁt

    from a na¨ıve approach whereby the maximum RE is used,

    therefore all samples will fall below this value, giving an

    accuracy of 100%. For anomaly samples the second hypoth-

    esis was that between the maximum value and the mean

    observed RE a threshold can be found that best splits normal

    and anomaly samples, similar to the stochastic approach. A

    third hypothesis was that the mean RE will change overtime

    due to concept drift, and so will become less sensitive to

    more recent samples when taken over a long stream.

    To address the above three hypothesis an ‘Adaptive

    Anomaly Threshold’ (AAT) method was proposed that

    combines the NAT, SAT and Fading Factor [30] methods.

    The proposed method is given in algorithm 1. Normal

    samples were used to update the fading average RE-score

    over the stream, using a fading factor α [30] in order to give

    more importance to more recent sample values, satisfying

    hypothesis 3 above. The maximum RE of normal samples

    over the data stream is also recorded and used to ﬁnd

    the ﬁrst value of the anomaly threshold φ. If the initial

    1. http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html

    2. https://www.unsw.adfa.edu.au/unsw-canberra-

    cyber/cybersecurity/ADFA-NB15-data sets/

    3. https://keras.io/

    4. https://scikit-multiﬂow.github.io/

    5

    maximum value of φ achieves an accuracy of 1.0 or 100%,

    then this fulﬁlled the ﬁrst hypothesis that all samples are

    normal and no further action was required. Otherwise

    hypothesis 2 is assumed and a stochastic approach was

    then used to step through potential threshold values until

    the highest accuracy is found.

    Algorithm 1: Adaptive Anomaly Threshold

    Input : autoencoder m, X, y, threshold φ, step size

    v ← [> 0], fading factor α

    Output: φ

    /* Initialise fading sum, fading

    increment, and max RE variables

    */

    1 S0 ← 0; N0 ← 0; REmax ← 0;

    /* Find the fading mean RE of normal

    samples

    */

    2 Xy←0 ⊆ X;

    3 REi ← predictRE(m, Xy←0);

    4 Si ← REi + α ∗ Si−1;

    5 Ni ← 1 + α ∗ Ni−1;

    6 REµα ← Si

    Ni ;

    /* Find the maximum normal sample RE of

    the data stream

    */

    7 if REi > REMAX then

    8

    REMAX ← REi;

    9 end

    /* Set threshold to the maximum

    observed RE

    */

    10 φ ← REMAX;

    /* Calculate accuracy, only attempt to

    find a lower threshold if accuracy

    is not 100%

    */

    11 ˆy ← predict(m,φ,X);

    12 accw ← calcAccuracy(ˆy,y);

    13 if accw < 1.0 then

    /* Step through to the fading mean

    of the stream RE to find

    threshold that yeilds the highest

    accuracy

    */

    14

    φw ← φ;

    15

    while φ > REµα do

    16

    φ ← φ − v;

    17

    ˆy ← predict(m,φ,X);

    18

    acc ← calcAccuracy(ˆy,y);

    19

    if acc > accw then

    20

    φw ← φ;

    21

    accw ← acc;

    22

    end

    23

    end

    24

    φ ← φw;

    25 end

    The proposed autoencoder anomaly detector is depicted

    in ﬁgure 1. The sample X is inputted to the autoencoder net-

    work from which a Reconstruction Error (RE) is produced

    based on the loss value between the approximate output

    and the original input. The RE is compared to an anomaly

    threshold value with samples scoring above threshold being

    labeled as an ‘anomaly’ and those below being ‘normal’ or

    benign. If a label Y is provided then the anomaly thresh-

    old is updated using a novel adaptive anomaly threshold

    method, which also maintains a memory of the population

    mean RE throughout the data stream by using a fading fac-

    tor [2] memory mechanism to prioritise more recent samples

    for faster adaptation. The adaptive anomaly threshold is

    demonstrated to be superior to ﬁxed and other threshold

    determination methods from the literature. Note that the

    use of labels to ﬁnd the anomaly threshold results in a semi-

    supervised method.

    Fig. 1: Autoencoder Anomaly Detector

    3.2

    Active Stream Framework

    The proposed autoencoder anomaly detector is a semi-

    supervised method requiring class labels to be known.

    Class annotation is also important to detect changes in the

    data stream that require learning to occur in order for the

    model to adapt. Given the inﬁnite nature of a data stream,

    labeling all samples is infeasibly expensive, therefore AL

    methods were explored to minimise the labeling cost for

    both updating the model and threshold, whilst identifying

    and adapting to changes in the data stream.

    ˇZliobait˙e et al. [3], proposed an active stream framework,

    which combines change detection with a labeling strategy

    and a ﬁxed budget B. Algorithm 2 gives the active stream

    framework evaluated in this research. The active learning

    strategy is an important part of the framework as it deter-

    mines whether or not the current data sample Xi, yi should

    be labeled. Blind, random, uncertainty, variable uncertainty

    and split strategies were evaluated in this research [3], [21],

    [22]. The framework maintains a running estimate of label

    usage ˆui over a fading window, calculated by equation 3,

    where w is the size of the fading window and labeli is

    the labeling decision either 0 or 1 at time i. The spending

    6

    estimate ˆb is then calculated from ˆui over w, given in

    equation 4 [3]. During this evaluation, w was set to 1000.

    The labeled samples are then used to train the model and

    perform change detection. If a warning signal is received

    then a new autoencoder (AEL) is trained with the most

    recent examples, and when a change is signaled, the current

    model is replaced with AEL, completing adaptation to the

    new concept. For this evaluation the Drift Detection Method

    (DDM) [25] change detector was used.

    ˆui = ˆui−1 ∗ (w − 1)

    w

    + labeli

    (3)

    ˆb = ˆui

    w

    (4)

    Algorithm 2: Active Stream Framework

    Input : Autoencoder AE, Labeling budget B,

    budget window w, strategy(parameters),

    Change Detector D

    Output: AE

    /* Initialise active stream framework

    */

    1 ˆb ← 0; ˆu0 ← 0;

    /* Check if current spending estimate

    is below budget and strategy decides

    to label

    */

    2 if ˆb < B AND strategy(parameters) = 1 then

    3

    Update label estimate ˆui (equation 3) where

    labeli = 1;

    /* Incrementally fit the autoencoder

    and predict current label

    */

    4

    AE ← partialFit(AE, Xi, yi);

    5

    ˆyi ← predict(AE, Xi);

    /* Update the change detector

    statistics

    */

    6

    updateChangeDetector(D, yi ̸= ˆyi);

    7

    if AEL then

    /* If an alternative AE exists

    then update this with the

    labeled samples

    */

    8

    AEL ←partialFit(AELXi, yi);

    /* If change is detected then

    replace the current AE with the

    alternate

    */

    9

    if changeSignalled(D) then

    10

    Replace AE with AEL;

    11

    end

    12

    else if warningSignalled(D) then

    /* If warning is signalled then

    create new alternate AE and

    train

    */

    13

    Create new AEL;

    14

    AEL ←partialFit(AEL, Xi, yi);

    15 else

    16

    Update label estimate ˆui (equation 3) where

    labeli = 0;

    17 end

    18 Update spending estimate ˆb (equation 4);

    3.3

    Active Learning Strategies

    The following section outlines the active learning strategies

    evaluated in this research. ˇZliobait˙e et al. [3] outlined three

    objectives of active learning strategies, which will need to

    be met by any proposed strategies:

    1)

    balance the labeling budget B over inﬁnite time;

    2)

    detect changes anywhere in the instance space;

    3)

    preserve the distribution of incoming data for de-

    tecting changes.

    A random active learning strategy randomly selects a

    sample to label based on Bernoulli probability with a given

    budget B. The random strategy satisﬁes all three objectives

    of [3].

    The uncertainty strategy labels a sample based on the level

    of uncertainty from the classiﬁer compared to a threshold,

    and attempts to label the samples where there is the least

    conﬁdence [3]. A common approach is to use the classiﬁer’s

    predicted probability for class c compared to the threshold

    θ: P(yc|X) ≤ θ [3], [21], [22].

    Autoencoders do not provide a direct class probability,

    instead they provide a reconstruction error from which a

    normal or anomaly classiﬁcation decision can be made. This

    research proposed a novel method whereby the RE squared

    difference from the anomaly threshold φ is used as a mea-

    sure of uncertainty, equation 5, assuming the hypothesis

    that the lower the difference compared to the average of the

    population, then the greater the uncertainty for the sample.

    The difference is squared to make all values positive.

    di = (φ − REXi)2

    (5)

    In order to accommodate changes in the data stream and

    avoid a scenario where the strategy stops learning due to

    high variance, a fading factor α was used to produce a

    fading average of differences davg, calculated using equation

    6. This allowed for the more recent samples to have a greater

    bearing on the strategy outcome.

    Si = di + α ∗ Si−1

    Ni = 1 + α ∗ Ni−1

    davg = Si

    Ni

    (6)

    Using davg the fading standard deviation dstd of the

    stream was calculated using equation 7.

    Vi = (di − davg)2 + α ∗ Vi−1

    dstd =

    r

    Vi

    Ni

    (7)

    Finally, the strategy returned a labeling decision of 1

    where di < davg − dstdθ, equation 8, requiring a sample

    to be below the average by so many θ standard deviations,

    where θ was the conﬁdence threshold. θ = 2 should capture

    samples where the difference is the lowest 5% of all samples.

    labeling =

    (

    1,

    di < davg − dstdθ

    0,

    otherwise

    (8)

    7

    Fig. 2: Split Active Learning Anomaly Detector

    The uncertainty strategy algorithm is given in 3,

    whereby the autoencoder AE model is used to predict the

    RE for sample Xi, and the fading average and standard

    deviation of the difference from the anomaly threshold φ

    over the stream used to provide a label output of 0 or 1

    based on equation 8. On its own, an uncertainty strategy

    cannot satisfy all three active learning objectives as: the

    number of labeled samples will depend on the amount of

    uncertainty within the data stream and could vary above

    the intended budget, this is instead limited by line 2 of

    algorithm 2; only samples within the uncertainty margin

    are labeled, changes occurring outside of the margin will

    be missed; and change detection will be based on the

    distribution of uncertain samples that are trained on [3].

    The strategy should reﬂect regions where real concept drift

    is occurring as higher uncertainty could reﬂect a change,

    resulting in faster adaptation times [21], [22].

    Variable uncertainty is based on the uncertainty strategy,

    but instead of using a ﬁxed conﬁdence θ, this is instead

    varied depending on the amount of labeling that is being

    requested from the strategy, so that more labels will increase

    the conﬁdence and fewer will decrease to attenuate the

    labeling and better manage budget [3]. This approach also

    has the beneﬁt that it is not limited to a ﬁxed labeling ceiling

    Algorithm 3: Uncertainty Strategy

    Input : Conﬁdence θ, Fading Factor α, X,

    autoencoder AE, Threshold φ

    Output: label

    1 S0 ← 0; N0 ← 0; V0 ← 0; label ← 0;

    2 REi ← predictRE(AE, Xi);

    3 Calculate difference di of REi from φ, using equation

    5;

    4 Calculate the fading average difference davg, using

    equation 6;

    5 Calculate the fading standard deviation of

    differences dstd using equation 7;

    6 if di < davg − dstdθ then

    7

    label ← 1;

    8 end

    and can better utilise higher budgets to accurately identify

    concept drift [22]. Similar to the uncertainty strategy this

    also does not satisfy all three requirements [3].

    The split strategy, given in algorithm 4, combines the

    random and variable uncertainty strategies to beneﬁt from

    their respective strengths of accessing the entire stream

    distribution for change detection, and adapting to potential

    8

    change in higher regions of uncertainty. Due to the incor-

    poration of the random strategy, this also meets all three

    requirements of [3].

    Algorithm 4: Split Strategy

    Input : Label Budget B, Conﬁdence θ, Fading

    Factor α, X, autoencoder AE, Threshold φ,

    Step s

    Output: label

    1 label ← 0;

    2 if randomStrategy(B) = True then

    3

    label ← 1;

    4 else if varUncertaintyStrategy(θ,α,Xi, AE,

    φ,s) = True then

    5

    label ← 1;

    The proposed Split Active Learning Anomaly Detector

    (SALAD) method is depicted in ﬁgure 2. This method re-

    duces the labeling cost of the data stream to a ﬁxed budget

    by adopting an active learning strategy to determine which

    labels should be updated, satisfying the requirements of

    ˇZliobait˙e et al. [3]. Labeled samples are used to train the

    anomaly detector and the predictions input to a change

    detector which monitors for real concept drift occurring in

    the data stream [2]. Where real concept drift occurs, the

    current anomaly detector is replaced with a new one that

    has been trained on samples since a warning signal was

    produced. The result of this method is faster training of the

    anomaly detector and the ability to quickly adapt to changes

    occurring in the data stream.

    4

    RESULTS

    4.1

    Adaptive Anomaly Threshold

    The accuracy and F1-score of the Adaptive Anomaly

    Threshold method was compared to the Stochastic Anomaly

    Threshold with memory (SAT FF), HAT and NB algorithms.

    SAT FF is a novel modiﬁed version of the SAT algorithm

    to update the threshold based on a fading average [30] of

    previous thresholds to allow for memory when processing

    over a data stream. The parameter values for the autoen-

    coder methods are given in table 1, where p represents the

    dropout probability; l is the number of hidden layers, h the

    ratio of hidden units to visible units; opt is the optimiser

    used to train the network with α learning rate; β is the

    threshold sensitivity; α is the fading factor; and v is the

    step size. NB and HAT algorithms used the scikit-multiﬂow

    default parameters [29].

    Method

    Parameters

    Prequential Evaluation

    batch size = 100, pretrain size = 10000

    Autoencoder

    l = 1, p = 0.1, h = 0.6, opt = adagrad

    (α = 0.01)

    SAT FF

    β = 1.1, v = 0.001, α = 0.4

    Adaptive

    Anomaly

    Threshold

    β = 1.18, v = 0.001, α = 0.4

    TABLE 1: Evaluation Parameters

    The accuracy and F1 scores with the KDD Cup 1999

    data set are plotted in ﬁgure 3. SAT FF and AAT are

    (a) Accuracy

    (b) F1-score

    Fig. 3: KDD Cup 1999 AAT, SAT FF, NB and HAT accuracy

    and F1-score

    close to HAT in terms of mean performance, with better

    kappa and F1 metrics when taken as an average across all

    batches, as shown in table 2. SAT FF and AAT were also

    signiﬁcantly faster with a total running time (RT) of 14.04s

    and 19.18s, compared to 510.93s and 794.76s with NB and

    SAT, respectively. Note that running time will vary based

    on the underlying system performance and frameworks

    used, however the time of SAT FF is an order of magnitude

    better compared to both NB and HAT algorithms. Overall

    AAT returned the best mean accuracy and kappa results, an

    important metric for data stream learning.

    Algorithm

    Accuracy %

    Kappa

    F1-score

    RT

    µ±SD

    µ±SD

    µ±SD

    (s)

    AE AAT

    98.78±7.88

    0.954±0.202 0.802±0.395 19.18

    AE SAT FF

    98.16±8.65

    0.854±0.360 0.812±0.387 14.04

    NB

    93.34±20.22

    0.721±0.445 0.810±0.380 510.93

    HAT

    98.57±0.60

    0.820±0.379 0.811±0.383 794.76

    TABLE 2: KDD Cup 1999 AAT, SAT FF, NB and HAT Results

    As demonstrated in our previous work [31], the UNSW-

    NB15 data set proved to be more challenging for on-

    9

    (a) Accuracy

    (b) F1-score

    Fig. 4: UNSW-NB15 AAT, SAT, SAT FF, NB and HAT accu-

    racy and F1-score

    line learning, requiring the number of network layers and

    dropout probability to be adjusted to better provide separa-

    tion between normal and anomaly class distributions, with

    l = 3 and p = 0.2 being selected. The accuracy and F1-

    score results of the AAT method compared to SAT, SAT FF,

    NB and HAT are plotted in ﬁgure 4. Table 3 gives average

    accuracy of the SAT and SAT FF algorithms as 70.39% and

    62.96%, respectively, which is considerably lower than that

    of NB and HAT. AAT returned the highest overall accuracy

    of the anomaly threshold methods, at 86.31% with 3 layers

    and dropout probability of 0.2, although kappa was lower,

    demonstrating reduced conﬁdence in the anomaly decision

    for all methods. The results show that AAT is able to provide

    near equivalent performance to NB and HAT methods with

    a signiﬁcantly lower running time.

    4.2

    Active Stream Framework

    4.2.1

    Labeling Budget

    The effects of the labeling budget was evaluated with the

    random strategy as this is the only strategy to maintain

    Algorithm

    Accuracy %

    Kappa

    F1-score

    RT

    µ±SD

    µ±SD

    µ±SD

    (s)

    AE AAT

    86.31±16.32

    0.298±0.411 0.767±0.335 18.55

    AE SAT

    70.39±32.71

    0.364±0.443 0.613±0.390 12.14

    AE SAT FF

    62.96±38.95

    0.420±0.458 0.528±0.418 11.01

    NB

    83.69±28.99

    0.399±0.480 0.832±0.343 350.39

    HAT

    92.85±11.19

    0.436±0.479 0.813±0.340 610.94

    TABLE 3: UNSW-NB15 AAT, SAT, SAT FF, NB and HAT

    Results

    the sample distribution of the stream so as to not add any

    bias to the results. Budget B was evaluated at values of 0.2

    (20%), 0.5 (50%) and 1.0 (100%). The results are given in table

    5 and mean accuracy plotted against the blind adaption

    AAT approach for comparison in ﬁgure 5. The greater the

    labeling budget, typically the higher the accuracy, kappa

    and F1 scores, the exception being UNSW-NB15 where

    B = 0.5 has a slightly higher accuracy and kappa. The

    difference in accuracy between 20% and 100% labels is 0.76%

    (KDD’99) and 2.69% (UNSW-NB15), demonstrating a small

    loss in performance for an 80% saving in labeling cost and

    approximate running time reduction of 54-62%; this reﬂects

    the results of ˇZliobait˙e et al. [3], where a small loss of

    accuracy was observed between a B of 100% and 10% when

    tested with a number of non-cyber data sets.

    Comparing to the blind adaptation of previous experi-

    ments, whereby no active learning is used, a labeling budget

    of 0.5 achieved a higher accuracy and F1 for half the labeling

    cost on both data sets. ASF RAND 1.0 is equivalent to the

    blind approach with full labels, but with the addition of

    change detection, with average accuracy and F1 improved

    across both data sets, although lower towards the end of the

    UNSW-NB15 stream as shown in ﬁgure 5b. Note the lower

    running time of the blind approach due to use of a chunk

    size of 100 vs 10 which inﬂuences the number of gradient

    updates and hence training time of the network.

    Strategy

    B

    Accuracy %

    Kappa

    F1-score

    RT

    µ±SD

    µ±SD

    µ±SD

    (s)

    KDD Cup 1999

    Random

    0.2

    98.32±8.50

    0.932±0.217 0.811±0.381 55.9

    Random

    0.5

    98.94±7.27

    0.956±0.182 0.821±0.376 85.8

    Random

    1.0

    99.08±7.02

    0.962±0.176 0.825±0.374 145.4

    Blind

    1.0

    98.78±7.88

    0.954±0.202 0.802±0.395 19.18

    UNSW-NB15

    Random

    0.2

    87.07±19.48

    0.598±0.376 0.752±0.350 55.5

    Random

    0.5

    90.85±12.16

    0.619±0.265 0.791±0.338 84.0

    Random

    1.0

    89.76±12.74

    0.549±0.431 0.793±0.334 121.2

    Blind

    1.0

    86.31±16.32

    0.298±0.411 0.767±0.335 18.55

    TABLE 4: Random Strategy Budget Size: KDD’99 and

    UNSW-NB15 Comparison

    4.2.2

    Active Learning Strategies

    The results of each active learning strategy with a budget

    of 0.2 (20%) are given in Table 5, with accuracy and F1-

    score for both data sets plotted in ﬁgure 6. Each strategy was

    executed 5 times with the average and standard deviation

    presented. The worst performing strategy was the ﬁxed

    uncertainty strategy, reﬂecting the results of ˇZliobait˙e et al.

    [3], which was expected as the algorithm is biased only

    towards uncertain samples and cannot vary the amount of

    samples labeled, meaning that change occurring outside of

    10

    (a) KDD’99 Accuracy

    (b) UNSW-NB15 Accuracy

    Fig. 5: Labeling Budget Accuracy Comparison for Random

    Strategy

    the ﬁxed margin will be missed. It is also possible that the

    RE=value of normal samples outside of the margin may

    increase as the AE is trained more on uncertain samples,

    leading to higher false positives and lower F1-score.

    The split strategy, returned the best results across both

    data sets, combining random and variable uncertainty

    strategies. Note that the total running time is between that

    of the random and variable uncertainty strategies, indicting

    time complexity savings where uncertain samples were ﬁrst

    selected by the random strategy. The Kappa of the split

    strategy was observed as 0.717 (table 5) for the UNSW-NB15

    data set, this is much higher than the performance of the

    blind AAT, NB, HAT and other AL strategies, indicating a

    higher level of conﬁdence in the anomaly decisions.

    5

    DISCUSSION

    This research evaluated online anomaly detection in the

    form of a prequential evaluation method whereby the model

    is ﬁrst tested on the next sample or chunk in the stream

    before training. The anomaly threshold is a key parameter

    for anomaly detection and ﬁnding an optimal threshold

    Strategy

    Accuracy %

    Kappa

    F1-score

    RT

    µ±SD

    µ±SD

    µ±SD

    (s)

    KDD Cup 1999

    Random

    98.32±8.50

    0.932±0.217 0.811±0.381 55.9

    Uncertainty

    93.32±23.40

    0.892±0.303 0.762±0.422 81.6

    Var Uncert

    98.61±8.65

    0.951±0.194 0.817±0.379 74.1

    Split

    98.85±7.55

    0.947±0.199 0.819±0.378 69.6

    UNSW-NB15

    Random

    87.07±19.48

    0.598±0.376 0.752±0.350 55.5

    Uncertainty

    83.95±16.40

    0.348±0.304 0.762±0.334 53.8

    Var Uncert

    87.51±16.26

    0.452±0.368 0.768±0.339 64.6

    Split

    90.88±14.96

    0.717±0.363 0.791±0.343 63.3

    TABLE 5: Active Learning Strategy Comparison

    for a data stream is non-trivial. A number of methods for

    ﬁnding the threshold were compared including ﬁxed, na¨ıve,

    stochastic and adaptive techniques. The adaptive anomaly

    threshold (AAT) was introduced as a novel hybrid of the

    na¨ıve and stochastic methods in order to better adapt to

    chunks of normal or anomaly samples based on initial ob-

    served accuracy. Overall AAT outperformed other methods

    and is a recommended contribution of this research to be

    explored further.

    The results observed with the KDD’99 data set and AAT

    threshold method provide strong evidence that the hypoth-

    esis of effective anomaly detection for network data streams

    can be supported by the autoencoder method with both

    strong detection and run time performance compared to

    traditional methods. UNSW-NB15 results could be strength-

    ened by further design choices.

    The AAT method makes use of blind adaptation,

    whereby the model is trained on all labeled samples. This

    has the drawback of high cost due to full labels and slow

    adaptation times to change occurring in the data stream.

    The research further explored change detection and active

    learning strategies, as outlined by ˇZliobait˙e et al. [3], to

    further improve performance for a lower overall cost.

    An ASF framework was implemented along with the

    random, uncertainty, variable uncertainty and split active

    learning strategies. With the uncertainty strategy, a new

    method for AE was proposed, whereby the average RE

    difference from the threshold is used as a baseline to detect

    samples with high uncertainty, deﬁned as being in the

    proportion of the population with the smallest difference,

    tuned by a conﬁdence parameter.

    The use of ASF demonstrated that better accuracy, kappa

    and F1 scores can be achieved, compared to blind adapta-

    tion, with just 20% of the labeling cost, enabled by active

    learning of the most important samples to accelerate the

    learning process [3]. The results align to those presented by

    ˇZliobait˙e et al. [3], with a split strategy being recommended

    as this fulﬁlls all three active learning requirements to

    maintain a ﬁxed budget, access to all samples within the

    stream and preserve the distribution of incoming data for

    detecting changes. Unlike ˇZliobait˙e et al. [3], this research

    recommends inclusion of the uncertain samples with the

    change detection to improve per class performance.

    6

    CONCLUSION

    The aim of this research was to explore semi-supervised

    online autoencoder methods for the task of anomaly in-

    11

    (a) KDD’99 Accuracy

    (b) KDD’99 F1-score

    (c) UNSW-NB15 Accuracy

    (d) UNSW-NB15 F1-score

    Fig. 6: ASF Strategy Comparison, B = 20%

    trusion detection on non-stationary network data streams,

    adapting to concept drift over time, with minimal label-

    ing cost, by adopting an active learning change detection

    strategy. A unique contribution of this research was to

    compare a selection of anomaly threshold methods, propos-

    ing memory adaptations for data streams and a hybrid

    Adaptive Anomaly Threshold method which demonstrated

    superior performance. One of the more striking ﬁndings of

    the research is that the processing time of the autoencoder

    anomaly detector method is signiﬁcantly lower when com-

    pared to traditional online learning techniques, making it

    well adjusted for high speed online network data streams,

    demonstrating an ability to detect an equivalent number of

    cyber attacks to traditional online learning methods, in a

    signiﬁcantly reduced time frame. An area of future research

    would be to explore alternative threshold methods, such

    as clustering, which may allow for better identiﬁcation of

    classes that overlap with normal samples and multi-label

    classiﬁcation.

    A further contribution of this research was to evaluate

    the autoencoder method with an Active Stream Framework,

    allowing the labeling cost of the data stream to be sig-

    niﬁcantly reduced to a budget of 20%. A novel variable

    uncertainty strategy was proposed for autoencoders where

    the posterior probability is not available, instead tracking

    the distribution of sample RE distances from the anomaly

    threshold to determine uncertainty. An area of future re-

    search should be how to efﬁciently annotate samples, pos-

    sibly by unsupervised clustering methods such as those

    demonstrated by [32].

    Overall this research has demonstrated that the pro-

    posed Split Active Learning Anomaly Detector (SALAD)

    method can demonstrate high levels of performance with

    network data streams, which signiﬁcantly reduced the label-

    ing cost. The results are not perfect however, and it would

    be recommended to combine in a hybrid intrusion detection

    model whereby misuse detection is used before or after the

    anomaly detector to further identify classes, reduce false

    positives and better identify minority classes. Multi-label

    classiﬁcation would be a further research area to expand

    on this work and provide additional context to detections.

    REFERENCES

    [1]

    A. L. Buczak and E. Guven, “A survey of data mining and machine

    learning methods for cyber security intrusion detection,” IEEE

    12

    Communications Surveys & Tutorials, vol. 18, no. 2, pp. 1153–1176,

    2016.

    [2]

    J. Gama, I. ˇZliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia,

    “A survey on concept drift adaptation,” ACM computing surveys

    (CSUR), vol. 46, no. 4, p. 44, 2014.

    [3]

    I. ˇZliobait˙e, A. Bifet, B. Pfahringer, and G. Holmes, “Active

    learning with drifting streaming data,” IEEE transactions on neural

    networks and learning systems, vol. 25, no. 1, pp. 27–39, 2013.

    [4]

    S. Mansalis, E. Ntoutsi, N. Pelekis, and Y. Theodoridis, “An eval-

    uation of data stream clustering algorithms,” Statistical Analysis

    and Data Mining: The ASA Data Science Journal, vol. 11, no. 4, pp.

    167–187, 2018.

    [5]

    I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.

    MIT

    Press, 2016, http://www.deeplearningbook.org.

    [6]

    S. Dua and X. Du, Data mining and machine learning in cybersecurity.

    Auerbach Publications, 2016.

    [7]

    K. Alrawashdeh and C. Purdy, “Toward an online anomaly in-

    trusion detection system based on deep learning,” in 2016 15th

    IEEE International Conference on Machine Learning and Applications

    (ICMLA).

    IEEE, 2016, pp. 195–200.

    [8]

    ——, “Fast activation function approach for deep learning based

    online anomaly intrusion detection,” in 2018 IEEE 4th International

    Conference on Big Data Security on Cloud (BigDataSecurity), IEEE

    International Conference on High Performance and Smart Comput-

    ing,(HPSC) and IEEE International Conference on Intelligent Data and

    Security (IDS).

    IEEE, 2018, pp. 5–13.

    [9]

    S. Roshan, Y. Miche, A. Akusok, and A. Lendasse, “Adaptive and

    online network intrusion detection system using clustering and

    extreme learning machines,” Journal of the Franklin Institute, vol.

    355, no. 4, pp. 1752–1779, 2018.

    [10] X. Chen, C. Cao, and J. Mai, “Network anomaly detection based

    on deep support vector data description,” in 2020 5th IEEE Inter-

    national Conference on Big Data Analytics (ICBDA).

    IEEE, 2020, pp.

    251–255.

    [11] M. M. Hassan, A. Gumaei, A. Alsanad, M. Alrubaian, and

    G. Fortino, “A hybrid deep learning model for efﬁcient intrusion

    detection in big data environment,” Information Sciences, vol. 513,

    pp. 386–396, 2020.

    [12] C. Nixon, M. Sedky, and M. Hassan, “Autoencoders: A low cost

    anomaly detection method for computer network data streams,”

    in Proceedings of the 2020 4th International Conference on Cloud and

    Big Data Computing, ser. ICCBDC ’20.

    New York, NY, USA:

    Association for Computing Machinery, 2020, p. 58–62. [Online].

    Available: https://doi.org/10.1145/3416921.3416937

    [13] M. Nicolau and J. McDermott, “A hybrid autoencoder and density

    estimation model for anomaly detection,” in International Confer-

    ence on Parallel Problem Solving from Nature.

    Springer, 2016, pp.

    717–726.

    [14] A. H. Mirza and S. Cosan, “Computer network intrusion detection

    using sequential lstm neural networks autoencoders,” in 2018 26th

    Signal Processing and Communications Applications Conference (SIU).

    IEEE, 2018, pp. 1–4.

    [15] T. Kieu, B. Yang, C. Guo, and C. S. Jensen, “Outlier detection

    for time series with recurrent autoencoder ensembles,” in 28th

    international joint conference on artiﬁcial intelligence, 2019.

    [16] Y. Mirsky, T. Doitshman, Y. Elovici, and A. Shabtai, “Kitsune: an

    ensemble of autoencoders for online network intrusion detection,”

    arXiv preprint arXiv:1802.09089, 2018.

    [17] X. Li, W. Chen, Q. Zhang, and L. Wu, “Building auto-encoder

    intrusion detection system based on random forest feature selec-

    tion,” Computers & Security, p. 101851, 2020.

    [18] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga, “Outlier detection

    with autoencoder ensembles,” in Proceedings of the 2017 SIAM

    International Conference on Data Mining.

    SIAM, 2017, pp. 90–98.

    [19] T. Vaiyapuri and A. Binbusayyis, “Application of deep autoen-

    coder as an one-class classiﬁer for unsupervised network intru-

    sion detection: a comparative evaluation,” PeerJ Computer Science,

    vol. 6, pp. 1–26, 2020.

    [20] B.

    Krawczyk,

    L.

    L.

    Minku,

    J.

    Gama,

    J.

    Stefanowski,

    and

    M. Wo´zniak, “Ensemble learning for data stream analysis: A

    survey,” Information Fusion, vol. 37, pp. 132–156, 2017.

    [21] T. S. Sethi and M. Kantardzic, “On the reliable detection of

    concept drift from streaming unlabeled data,” Expert Systems with

    Applications, vol. 82, pp. 77–99, 2017.

    [22] J. Shan, H. Zhang, W. Liu, and Q. Liu, “Online active learning

    ensemble framework for drifted data streams,” IEEE transactions

    on neural networks and learning systems, vol. 30, no. 2, pp. 486–498,

    2018.

    [23] Q.-V. Dang, “Active learning for intrusion detection systems,” in

    IEEE Research, Innovation and Vision for the Future, 2020.

    [24] Z. Zhang, Y. Zhang, J. Niu, and D. Guo, “Unknown network

    attack detection based on open-set recognition and active learning

    in drone network,” Transactions on Emerging Telecommunications

    Technologies, vol. n/a, no. n/a, p. e4212. [Online]. Available:

    https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4212

    [25] J. Gama, P. Medas, G. Castillo, and P. Rodrigues, “Learning with

    drift detection,” in Brazilian symposium on artiﬁcial intelligence.

    Springer, 2004, pp. 286–295.

    [26] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, “A detailed

    analysis of the kdd cup 99 data set,” in 2009 IEEE Symposium

    on Computational Intelligence for Security and Defense Applications.

    IEEE, 2009, pp. 1–6.

    [27] N. Moustafa and J. Slay, “Unsw-nb15: a comprehensive data

    set for network intrusion detection systems (unsw-nb15 network

    data set),” in 2015 military communications and information systems

    conference (MilCIS).

    IEEE, 2015, pp. 1–6.

    [28] F. Chollet et al., “Keras,” https://keras.io, 2015.

    [29] J. Montiel, J. Read, A. Bifet, and T. Abdessalem, “Scikit-multiﬂow:

    a multi-output streaming framework,” The Journal of Machine

    Learning Research, vol. 19, no. 1, pp. 2915–2914, 2018.

    [30] J. Gama, R. Sebasti˜ao, and P. P. Rodrigues, “On evaluating stream

    learning algorithms,” Machine Learning, vol. 90, no. 3, pp. 317–346,

    2013.

    [31] C. Nixon, M. Sedky, and M. Hassan, “Practical application of

    machine learning based online intrusion detection to internet of

    things networks,” in 2019 IEEE Global Conference on Internet of

    Things (GCIoT).

    IEEE, 2019, pp. 1–5.

    [32] Z. Cataltepe, U. Ekmekci, T. Cataltepe, and I. Kelebek, “Online fea-

    ture selected semi-supervised decision trees for network intrusion

    detection,” in NOMS 2016-2016 IEEE/IFIP Network Operations and

    Management Symposium.

    IEEE, 2016, pp. 1085–1088.

    Christopher Nixon received his BEng(Hons)

    in Computer Science in 2008 and Masters by

    Research in Computing Science with distinction

    in 2020 from Staffordshire University. He has

    11 years of experience in the ﬁnancial services

    and telecoms industries, leading a team of cyber

    security experts in problem areas such as end

    user, cloud and network security. His research

    interests include machine learning and its appli-

    cation to cyber security.

    Dr Mohamed Sedky is an Associate Professor

    in Artiﬁcial Intelligence and Machine Learning

    at Staffordshire University. After he received his

    BEng(Hons) in Electro-Physics and Communi-

    cations in Alexandria University, Egypt, in 1996,

    Mohamed founded SKM communication sys-

    tems. He ﬁnished his MSc degree in Communi-

    cations and Electronics from the AAST in 2002.

    In 2009, he gained a PhD from Staffordshire

    University. He has led the Artiﬁcial Intelligence

    (AI) & Internet of Things (IoT) research group

    in Staffordshire University to conduct research in the development of

    artiﬁcial intelligent techniques and simulation tools for different Internet

    of things applications, robotic, and more recently applying his work in

    forensic science towards the detection and classiﬁcation of microplas-

    tics.

    Mohamed Hassan is leading the cyber security

    award at Staffordshire University. He holds a

    Master of Science degree in computer science

    with distinction and has extensive industry and

    academic experience in computing and cyber

    security. His teaching and research focus in the

    ﬁeld of cyber security, machine/deep learning

    and digital forensics. He is also interested in

    other topics in cyber security, such as malware

    analysis and reverse engineering, security in

    embedded systems and IoT security.

    '
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.14896773.v1
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: 'SALAD: An Exploration of Split Active Learning based Unsupervised Network
    Data Stream Anomaly Detection using Autoencoders'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.compchemeng.2019.03.034
  analysis: '>'
  authors:
  - Chao Ning
  - Fengqi You
  citation_count: 213
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Background on optimization
    under uncertainty 3. Existing methods for data-driven optimization under uncertainty
    4. Future research directions and opportunities 5. Conclusions Appendix. Supplementary
    materials Research Data References Show full outline Cited by (251) Figures (3)
    Extras (1) Supplement Computers & Chemical Engineering Volume 125, 9 June 2019,
    Pages 434-448 Optimization under uncertainty in the era of big data and deep learning:
    When machine learning meets mathematical programming Author links open overlay
    panel Chao Ning, Fengqi You Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.compchemeng.2019.03.034
    Get rights and content Highlights • An introduction to optimization under uncertainty
    is presented. • Recent advances in data-driven optimization under uncertainty
    are reviewed. • Future perspective on a “closed-loop” data-driven optimization
    framework is given. • Potential application of deep learning to data-driven optimization
    is discussed. Abstract This paper reviews recent advances in the field of optimization
    under uncertainty via a modern data lens, highlights key research challenges and
    promise of data-driven optimization that organically integrates machine learning
    and mathematical programming for decision-making under uncertainty, and identifies
    potential research opportunities. A brief review of classical mathematical programming
    techniques for hedging against uncertainty is first presented, along with their
    wide spectrum of applications in Process Systems Engineering. A comprehensive
    review and classification of the relevant publications on data-driven distributionally
    robust optimization, data-driven chance constrained program, data-driven robust
    optimization, and data-driven scenario-based optimization is then presented. This
    paper also identifies fertile avenues for future research that focuses on a closed-loop
    data-driven optimization framework, which allows the feedback from mathematical
    programming to machine learning, as well as scenario-based optimization leveraging
    the power of deep learning techniques. Perspectives on online learning-based data-driven
    multistage optimization with a learning-while-optimizing scheme are presented.
    Previous article in issue Next article in issue Keywords Data-driven optimizationDecision
    making under uncertaintyBig dataMachine learningDeep learning 1. Introduction
    Optimization applications abound in many areas of science and engineering (Biegler
    and Grossmann, 2004; Grossmann and Biegler, 2004; Sakizlis et al., 2004). In real
    practice, some parameters involved in optimization problems are subject to uncertainty
    due to a variety of reasons, including estimation errors and unexpected disturbance
    (Sahinidis, 2004). Such uncertain parameters can be product demands in process
    planning (Liu and Sahinidis, 1996), kinetic constants in reaction-separation-recycling
    system design (Acevedo and Pistikopoulos, 1998), and task durations in batch process
    scheduling (Li and Ierapetritou, 2008), among others. The issue of uncertainty
    could unfortunately render the solution of a deterministic optimization problem
    (i.e. the one disregarding uncertainty) suboptimal or even infeasible (Ben-Tal
    and Nemirovski, 2002). The infeasibility, i.e. the violation of constraints in
    optimization problems, has a disastrous consequence on the solution quality. Motivated
    by the practical concern, optimization under uncertainty has attracted tremendous
    attention from both academia and industry (Sahinidis, 2004; Pistikopoulos, 1995).
    In the era of big data and deep learning, intelligent use of data has a great
    potential to benefit many areas. Although there is no rigorous definition of big
    data (John Walker, 2014), people typically characterize big data with five Vs,
    namely, volume, velocity, variety, veracity and value (Yin and Kaynak, 2015).
    Torrents of data are routinely collected and archived in process industries, and
    these data are becoming an increasingly important asset in process control, operations
    and design (Qin, 2014; Yin et al., 2015; Venkatasubramanian, 2019). Nowadays,
    a wide array of emerging machine learning tools can be leveraged to analyze data
    and extract accurate, relevant, and useful information to facilitate knowledge
    discovery and decision-making. Deep learning, one of the most rapidly growing
    machine learning subfields, demonstrates remarkable power in deciphering multiple
    layers of representations from raw data without any domain expertise in designing
    feature extractors (Goodfellow et al., 2016). More recently, dramatic progress
    of mathematical programming (Grossmann, 2012), coupled with recent advances in
    machine learning (Jordan and Mitchell, 2015), especially in deep learning over
    the past decade (LeCun et al., 2015), sparks a flurry of interest in data-driven
    optimization (Bertsimas et al., 2018; Bertsimas and Thiele, 2006; Calfa et al.,
    2015; Calfa et al., 2015; Campbell and How, 2015; Jiang and Guan, 2015; Ning and
    You, 2017a; Levi et al., 2015). In the data-driven optimization paradigm, uncertainty
    model is formulated based on data, thus allowing uncertainty data “speak” for
    themselves in the optimization algorithm. In this way, rich information underlying
    uncertainty data can be harnessed in an automatic manner for smart and data-driven
    decision making. In this review paper, we summarize and classify the existing
    contributions of data-driven optimization under uncertainty, highlight the current
    research trends, point out the research challenges, and introduce promising methodologies
    that can be used to tackle these challenges. We briefly review conventional mathematical
    programming techniques for hedging against uncertainty, alongside their wide spectrum
    of applications in Process Systems Engineering (PSE). We then summarize the existing
    research papers on data-driven optimization under uncertainty and classify them
    into four categories according to their unique approach for uncertainty modeling
    and distinct optimization structures. Based on the literature survey, we identify
    three promising future research directions on optimization under uncertainty in
    the era of big data and deep learning and highlight respective research challenges
    and potential methodologies. The rest of this paper is organized as follows. In
    Section 2, background on mathematical programming techniques for decision making
    under uncertainty is given. Section 3 presents a comprehensive literature review,
    where relevant research papers are summarized and classified into four categories.
    Section 4 discusses promising future research directions to further advance the
    area of data-driven optimization. Conclusions are provided in Section 5. 2. Background
    on optimization under uncertainty In recent years, mathematical programming techniques
    for decision making under uncertainty have gained tremendous popularity among
    the PSE community, as witnessed by various successful applications in process
    synthesis and design (Pistikopoulos, 1995; Rooney and Biegler, 2003), production
    scheduling and planning (Li and Ierapetritou, 2008; Verderame et al., 2010), and
    process control (Mesbah, 2016; Krieger and Pistikopoulos, 2014; Chiu and Christofides,
    2000). In this section, we present some background knowledge of methodologies
    for optimization under uncertainty, along with computational algorithms and applications
    in PSE. Specifically, we briefly review three leading modeling paradigms for optimization
    under uncertainty, namely stochastic programming, chance-constrained programming,
    and robust optimization. 2.1. Stochastic programming Stochastic programming is
    a powerful modeling paradigm for decision making under uncertainty that aims to
    optimize the expected objective value across all the uncertainty realizations
    (Birge and Louveaux, 2011). The key idea of the stochastic programming approach
    is to model the randomness in uncertain parameters with probability distributions
    (Birge, 1997; Kall and Wallace, 1994). In general, the stochastic programming
    approach can effectively accommodate decision making processes with various time
    stages. In single-stage stochastic programs, there are no recourse variables and
    all the decisions must be made before knowing uncertainty realizations. By contrast,
    stochastic programming with recourse can take corrective actions after uncertainty
    is revealed. Among the stochastic programming approach with recourse, the most
    widely used one is the two-stage stochastic program, in which decisions are partitioned
    into “here-and-now” decisions and “wait-and-see” decisions. The general mathematical
    formulation of a two-stage stochastic programming problem is given as follows
    (Birge and Louveaux, 2011). (1) The recourse function Q(x, ω) is defined by, (2)
    where x represents first-stage decisions made “here-and-now” before the uncertainty
    ω is realized, while the second-stage decisions y are postponed in a “wait-and-see”
    manner after observing the uncertainty realization. The objective of the two-stage
    stochastic programming model includes two parts: the first-stage objective cTx
    and the expectation of the second-stage objective b(ω)Ty(ω). The constraints associated
    with the first-stage decisions are Ax ≥ d, x ∈ X, and the constraints of the second-stage
    decisions are and y(ω) ∈ Y. Sets X and Y can include nonnegativity, continuity
    or integrality restrictions. The resulting two-stage stochastic programming problem
    is computationally expensive to solve because of the growth of computational time
    with the number of scenarios. To this end, decomposition based algorithms have
    been developed in the existing literature, including Benders decomposition or
    the L-shaped method (Vanslyke and Wets, 1969; Laporte and Louveaux, 1993), and
    Lagrangean decomposition (Oliveira et al., 2013). The location of binary decision
    variables is critical for the design of computational algorithms. For stochastic
    programs with integer recourse, the expected recourse function is no longer convex,
    and even discontinuous, thus hindering the employment of conventional L-shaped
    method. As a result, research efforts have made on computational algorithms for
    efficient solution of two-stage stochastic mixed-integer programs (Küçükyavuz
    and Sen, 2017), such as Lagrangian relaxation (Caroe and Schultz, 1999), branch-and-bound
    scheme (Ahmed et al., 2004), and an improved L-shaped method (Li and Grossmann,
    2018; You and Grossmann, 2013). Stochastic programming has demonstrated various
    applications in PSE, such as design and operations of batch processes (Ierapetritou
    and Pistikopoulos, 1995; Bonfill et al., 2004; Bonfill et al., 2005; Chu and You,
    2013), process flowsheet optimization (Steimel and Engell, 2015), energy systems
    (Liu et al., 2010, Peng et al., 2018; Yue and You, 2016; Gao and You, 2017), and
    supply chain management (Zeballos et al., 2016; Gao and You, 2015; You et al.,
    2009; Gebreslassie et al., 2012; Tong et al., 2014; You et al., 2011; Gupta and
    Maranas, 2003). Due to its wide applicability, immense research efforts have been
    made on the variants of stochastic programming approach. For instance, the two-stage
    formulation in (1) can be readily extended to a multi-stage stochastic programming
    setup by utilizing scenario trees. Other extensions include stochastic nonlinear
    programming (Li et al., 2011), and stochastic programs with endogenous uncertainties
    (Gupta and Grossmann, 2014; Goel and Grossmann, 2007). 2.2. Chance constrained
    optimization As another powerful paradigm for optimization under uncertainty,
    chance constrained programming aims to optimize an objective while ensuring constraints
    to be satisfied with a specified probability in uncertain environment (Prékopa,
    1995) {Uryasev, 2000}. As in the stochastic programming approach, probability
    distribution is the key uncertainty model to capture the randomness of uncertain
    parameters in chance constrained optimization. The chance constrained program
    was first introduced in the seminal work of (Charnes and Cooper, 1959), and attracted
    considerable attention ever since. Such chance constraints or probabilistic constraints
    are flexible enough to quantify the trade-off between objective performance and
    system reliability (Li et al., 2008). The generic formulation of a chance constrained
    optimization problem is presented as follows, (3) where x represents the vector
    of decision variables, X denotes the deterministic feasible region, f is the objective
    function to be minimized, ξ is a random vector following a known probability distribution
    with the support set Ξ, represents a constraint mapping, 0 is a vector of all
    zeros, and parameter ε is a pre-specified risk level. The chance constraint guarantees
    that decision x satisfies constraints with a probability of at least 1 − ε. Note
    that when the number of constraints m = 1, the above optimization model is an
    individual chance constrained program; for m > 1, it is called joint chance constrained
    program (Miller and Wagner, 1965). A salient merit of chance constrained programs
    is that it allows decision makers choose their own risk levels for the improvement
    in objectives. To model sequential decision-making processes, two-stage chance
    constrained optimization with recourse was recently studied and had various applications
    (Liu et al., 2016; Quddus et al., 2018). Despite of its promising modeling power,
    the resulting chance constrained program is generally computationally intractable
    for the following two main reasons. First, calculating the probability of constraint
    satisfaction for a given x involves a multivariate integral, which is believed
    to be computationally prohibitive. Second, the feasible region is not convex even
    if set X is convex and G(x, ξ) is convex in x for any realizations of uncertain
    vector ξ (Prékopa, 1995). In light of these computational challenges, a large
    body of related literature is devoted into the development of solution algorithms
    for chance constrained optimization problems, such as sample average approximation
    (Luedtke and Ahmed, 2008), sequential approximation (Hong et al., 2011), and convex
    conservative approximation schemes (Nemirovski and Shapiro, 2006). Note that chance
    constrained programs admit convex reformulation for some very special cases. For
    example, individual chance constrained programs are endowed with tractable convex
    reformulations for normal distributions (Birge and Louveaux, 2011). Chance constraints
    with right-hand-side uncertainty are convex if uncertain parameters are independent
    and follow log-concave distributions (Prékopa, 1995). In the PSE community, chance
    constraints are usually employed for customer demand satisfaction, product quality
    specification, and service level of process systems {Maranas, 1997; Yue and You,
    2013; Gupta et al., 2000; You and Grossmann, 2011; Chu et al., 2015; Zipkin, 2000}.
    Due to its practical relevance, chance constrained optimization has been applied
    in numerous applications, including model predictive control (Shen et al., 2018;
    Cannon et al., 2009), process design and operations (Li et al., 2008; Chu et al.,
    2015; Gong et al., 2017), refinery blend planning (Yang et al., 2017), biopharmaceutical
    manufacturing (Liu et al., 2016), and supply chain optimization (Mitra et al.,
    2008; You and Grossmann, 2011; You and Grossmann, 2008; Ye and You, 2016). 2.3.
    Robust optimization As a promising alternative paradigm, robust optimization does
    not require accurate knowledge on probability distributions of uncertain parameters
    (Ben-Tal and Nemirovski, 2000; Bertsimas and Sim, 2004; Ben-Tal et al., 2009).
    Instead, it models uncertain parameters using an uncertainty set, which includes
    possible uncertainty realizations. It is worth noting that uncertainty set is
    a paramount ingredient in robust optimization framework (Ben-Tal et al., 2009).
    Given a specific uncertainty set, the idea of robust optimization is to hedge
    against the worst case within the uncertainty set. The worst-case uncertainty
    realization is defined based on different contexts: it could be the realization
    giving rise to the largest constraint violation, the realization leading to the
    lowest asset return (Gregory et al., 2011) or the one resulting in the highest
    regret (Assavapokee et al., 2008). The conventional box uncertainty set is not
    a good choice since it includes the unlikely-to-happen scenario where uncertain
    parameters simultaneously increase to their highest values. The conventional box
    uncertainty set is defined as follows (Soyster, 1973). (4) where Ubox is a box
    uncertainty set, u is a vector of uncertain parameters, ui is the i-th component
    of uncertainty vector u. uiL and uiU represent the lower bound and the upper bound
    of uncertain parameter ui, respectively. Box uncertainty set simply defines the
    range of each uncertain parameter in vector u. One cannot easily control the size
    of this uncertainty set to meet his or her risk-averse attitude. To this end,
    researchers propose the following budgeted uncertainty set (Bertsimas and Sim,
    2004). (5) where Ubudget denotes a budgeted uncertainty set, u and ui have the
    same definitions as in (4), is the nominal value of ui, Δui is the largest possible
    deviation of uncertain parameter ui, zi denotes the extent and direction of parameter
    deviation, and Γ is an uncertainty budget. Traditional robust optimization approaches,
    also known as static robust optimization (Bertsimas et al., 2011), make all the
    decisions at once. This modeling framework cannot well represent sequential decision-making
    problems (Lorca et al., 2016; Atamtürk and Zhang, 2007; Bertsimas et al., 2013).
    Adaptive robust optimization (ARO) was proposed to offer a new paradigm for optimization
    under uncertainty by incorporating recourse decisions (Ben-Tal et al., 2004).
    Due to the flexibility of adjusting recourse decisions after observing uncertainty
    realizations, ARO typically generates less conservative solutions than static
    robust optimization (Ben-Tal et al., 2009). The general form of a two-stage adaptive
    robust mixed-integer programming model is given as follows: (6) where x is the
    first-stage decision made before uncertainty u is realized, while the second-stage
    decision y is postponed in a “wait-and-see” manner. x includes both continuous
    and integer variables, while y only includes continuous variables. c and b are
    the vectors of the cost coefficients. U is an uncertainty set that characterizes
    the region of uncertainty realizations. ARO approaches could be applied to address
    uncertainty in a variety of applications, including process design (Gong and You,
    2018; Gong and You, 2017; Gong et al., 2016), process scheduling (Shi and You,
    2016), supply chain optimization (Gong et al., 2016; Tong et al., 2014), among
    others. Besides the two-stage ARO framework, the multistage ARO method has attracted
    immense attention due to its unique feature in reflecting sequential realizations
    of uncertainties over time (Delage and Iancu, 2015). In multistage ARO, decisions
    are made sequentially, and uncertainties are revealed gradually over stages. Note
    that the additional value delivered by ARO over static robust optimization is
    its adjustability of recourse decisions based on uncertainty realizations (Ben-Tal
    et al., 2004). Accordingly, the multistage ARO method has demonstrated applications
    in process scheduling and planning (Lappas and Gounaris, 2016; Ning and You, 2017b).
    Despite popularity of the above three leading paradigms for optimization under
    uncertainty, these approaches have their own limitations and specific application
    scopes. To this end, research efforts have been made on “hybrid” methods that
    leverage the synergy of different optimization approaches to inherit their corresponding
    strengths and complement respective weaknesses (McLean and Li, 2013; Liu et al.,
    2016; Baringo and Baringo, 2018; Zhao and Guan, 2013; Keyvanshokooh et al., 2016;
    Parpas et al., 2009). For instance, stochastic programming was integrated with
    robust optimization for supply chain design and operation under multi-scale uncertainties
    (Yue and You, 2016). Robust chance constrained optimization along with global
    solution algorithms were developed and applied to process design under price and
    demand uncertainties (Parpas et al., 2009). 3. Existing methods for data-driven
    optimization under uncertainty In this section, we review the recent advances
    in optimization under uncertainty in the era of big data and deep learning. Recent
    years have witnessed a rapidly growing number of publications on data-driven optimization
    under uncertainty, an active area integrating machine learning and mathematical
    programming. These publications cover various topics and can be roughly classified
    into four categories, namely data-driven stochastic program, data-driven chance
    constrained program, data-driven robust optimization, and data-driven scenario-based
    optimization. Unlike the conventional mathematical programming techniques, these
    data-driven approaches do not presume the uncertainty model is perfectly given
    a priori, rather they all focus on the practical setting where only uncertainty
    data are available. 3.1. Data-driven stochastic program and distributionally robust
    optimization The literature review of data-driven stochastic program, also known
    as distributionally robust optimization (DRO), is presented in detail in this
    subsection. The motivation of this emerging paradigm on data-driven optimization
    under uncertainty is first presented, followed by its model formulation. In this
    modeling paradigm, the uncertainty is modeled via a family of probability distributions
    that well capture uncertainty data on hand. This set of probability distributions
    is referred to as ambiguity set. We then present and analyze various types of
    ambiguity sets alongside their corresponding strengths and weaknesses. Finally,
    the extension of DRO to the multistage decision-making setting is also discussed,
    as well as their recent applications in PSE. In the stochastic programming approach,
    it is assumed that the probability distribution of uncertain parameters is perfectly
    known. However, such precise information of the uncertainty distribution is rarely
    available in practice. Instead, what the decision maker has is a set of historical
    and/or real-time uncertainty data and possibly some prior structure knowledge
    of the probability. Moreover, the assumed probability in conventional stochastic
    programming might deviate from the true distribution. Therefore, relying on a
    single probability distribution could result in sub-optimal solutions, or even
    lead to the deterioration in out-of-sample performance (Smith and Winkler, 2006).
    Motivated by these weaknesses of stochastic programming, DRO emerges as a new
    data-driven optimization paradigm which hedges against the worst-case distribution
    in an ambiguity set. Rather than assuming a single uncertainty distribution, the
    DRO approach constructs an uncertainty set of probability distributions from uncertainty
    data through statistical inference and big data analytics. In this way, DRO is
    capable of hedging against the distribution errors, and accounts for the input
    of uncertainty data. The general model formulation of data-driven stochastic programming
    is presented as follows (Delage and Ye, 2010). (7) where x is the vector of decision
    variables, X is the feasible set, l is the objective function, and ξ represents
    a random vector whose probability distribution is only known to reside in an ambiguity
    set . The DRO approach aims for optimal decisions under the worst-case distribution,
    and as a result offers performance guarantee over the family of distributions.
    The DRO or data-driven stochastic optimization framework enjoys two salient merits
    compared with the conventional stochastic programming approach. First, it allows
    the decision maker to incorporate partial distribution information learned from
    uncertainty data into the optimization. As a result, the data-driven stochastic
    programming approach greatly mitigates the issue of optimizer''s curse and improves
    the out-of-sample performance. Second, data-driven stochastic programming inherits
    the computational tractability from robust optimization and some resulting problems
    can be solved exactly in polynomial time without resorting to the approximation
    scheme via sampling or discretization. For example, optimization problem (7) for
    a convex program with continuous variables and a moment-based ambiguity set is
    proved to be solvable in polynomial time (Delage and Ye, 2010). The choice of
    ambiguity sets plays a critical role in the performance of DRO. When choosing
    ambiguity set, the decision maker needs to consider the following three factors,
    namely tractability, statistical meaning, and performance (Hanasusanto et al.,
    2015). First, the data-driven stochastic programming problem with the ambiguity
    set should be computationally tractable, meaning the resulting optimization could
    be formulated as linear, conic quadratic or semidefinite programs. Second, the
    derived ambiguity set should have clear statistical meaning. Therefore, various
    ways of constructing ambiguity sets based on uncertainty data were extensively
    studied (Delage and Ye, 2010; Esfahani and Kuhn, 2018; Shang and You, 2018). Third,
    the devised ambiguity set should be tight to increase the performance of resulting
    decisions. One commonly used approach to constructing ambiguity set is moment-based
    approaches, in which first and second order information is extracted from uncertainty
    data using statistical inference (Calafiore and El Ghaoui, 2006). The ambiguity
    set that specifies the support, first and second moment information is shown as
    follows, (8) where ξ represents the uncertainty vector, Ξ is the support, represents
    the probability distribution of ξ, denotes the set of all probability measures,
    denotes the expectation with respect to distribution . Parameters μ and Σ represent
    the mean vector and covariance matrix estimated from uncertainty data, respectively.
    The ambiguity set in (8) fails to account for the fact that the mean and covariance
    matrix are also subject to uncertainty. To this end, an ambiguity set was proposed
    based on the distribution''s support information as well as the confidence regions
    for the mean and second-moment matrix in the work of (Delage and Ye, 2010). The
    resulting DRO problem could be solved efficiently in polynomial time. (9) where
    ξ represents the uncertainty vector, Ξ is the support, represents the probability
    distribution of ξ. The equality constraint enforces that all uncertainty realizations
    reside in the support set Ξ. Parameters ψ1 and ψ2 are used to define the sizes
    of confidence regions for the first and second moment information, respectively.
    The moment-based ambiguity sets typically enjoy the advantage of computational
    tractability. For example, DRO with the ambiguity set based on principal component
    analysis and first-order deviation functions was developed (Shang and You, 2018).
    Additionally, the computational effectiveness of this data-driven DRO method was
    demonstrated via process network planning and batch production scheduling. Recently,
    a data-driven DRO model was developed for the optimal design and operations of
    shale gas supply chains to hedge against uncertainties associated with shale well
    estimated ultimate recovery and product demand (Gao et al., 2019). However, the
    moment-based ambiguity set is not guaranteed to converge to the true probability
    distribution as the number of uncertainty data goes to infinity. Consequently,
    this type of ambiguity set suffers from the conservatism with moderate uncertainty
    data. To address the above issue with moment-based methods, ambiguity sets based
    on statistical distance between probability distributions were developed, as shown
    below, (10) where is the probability distribution of uncertain parameters, represents
    the reference distribution such as the empirical distribution, d denotes some
    statistical distance between two distributions, and θ stands for the confidence
    level. Ambiguity set in (10) can be further classified based on the adopted distance
    metric, such as Kullback–Leibler divergence (Hu and Hong, 2013) and Wasserstein
    distance (Esfahani and Kuhn, 2018). For example, a DRO model was proposed for
    lot-sizing problem, in which the chi-square goodness-of-fit test and robust optimization
    were combined. The ambiguity set of demand was constructed from uncertainty data
    by using a hypothesis test in statistics, called the chi-square goodness-of-fit
    test (Klabjan et al., 2013). This set is well defined by linear constraints and
    second order cone constraints. It is worth noting that the input of their model
    is histograms, which make it possible to use a finite dimensional probability
    vector to characterize the distribution. The adopted statistic belonged to the
    phi-divergences, which motivated researchers to construct distribution uncertainty
    set by using the phi-divergences (Bayraksan and Love, 2015). To account for the
    sequential decision-making process, researchers recently developed the adaptive
    DRO method by incorporating recourse decision variables (Hanasusanto and Kuhn,
    2018; Bertsimas et al., 2019). A general two-stage data-driven stochastic programming
    model is presented in the following form: (11) where x presents the vector of
    first-stage decision variables that need to be determined before observing uncertainty
    realizations, y denotes the vector of second-stage decision variables that can
    be adjustable based on the realized uncertain parameters ξ, sets X and Y can include
    nonnegativity, continuity or integrality restrictions, and Q represents the recourse
    function. The objective of the above data-driven stochastic program is to minimize
    the worst-case expected cost with respect to all possible uncertainty distributions
    within the ambiguity set . Based on the literature, multistage data-driven DRO
    is becoming a rapidly evolving research direction. Data-driven stochastic programming
    has several salient merits over the conventional stochastic programming approach.
    However, there are few papers on its PSE applications in the existing literature
    (Shang and You, 2018; Gao et al., 2019). As the trend of big data has fueled the
    increasing popularity of data-driven stochastic programming in many areas, DRO
    emerges as a new data-driven optimization paradigm which hedges against the worst-case
    distribution in an ambiguity set, and has various applications in power systems,
    such as unit commitment problems (Xiong et al., 2017; Chen et al., 2018; Duan
    et al., 2018; Zhao and Guan, 2016), and optimal power flow (Wang et al., 2018;
    Guo et al., 2018). 3.2. Data-driven chance constrained program In contrast to
    the data-driven stochastic programming approach reviewed in Section 3.1., data-driven
    chance constrained programming is another paradigm focusing on chance constraint
    satisfaction under the worst-case probability instead of optimizing the worst-case
    expected objective. Although both data-driven chance constrained program and DRO
    adopt ambiguity sets in the uncertainty models, they have distinct model structures.
    Specifically, data-driven chance constrained program features constraints subject
    to uncertainty in probability distributions, while DRO typically only involves
    the worst-case expectation of an objective function with respect to a family of
    probability distributions. As introduced in Section 2, the chance constrained
    programming approach assumes the complete distribution information is perfectly
    known. However, the decision maker only has access to a finite number of uncertainty
    realizations or uncertainty data. On the one hand, such complete knowledge of
    distribution is usually estimated from limited number of uncertainty data or obtained
    from expert knowledge. On the other hand, even if the probability distribution
    is available, the chance constrained program is computationally cumbersome. In
    practice, one can only have partial information on the probability distribution
    of uncertainty. Therefore, data-driven chance constrained optimization emerges
    as another paradigm for hedging against uncertainty in the era of big data. The
    general form of data-driven chance constrained program is given by, (12) where
    x represents the vector of decision variables, X denotes the deterministic feasible
    region, f is the objective function, ξ is a random vector following a probability
    distribution that belongs to an ambiguity set . represents a constraint mapping,
    0 is a vector of all zeros, and parameter ε is a pre-specified risk level. The
    data-driven chance constraints enforce classical chance constraints to be satisfied
    for every probability distribution within the ambiguity set. The computational
    tractability of the resulting data-driven chance constrained program can vary
    depending on both the ambiguity sets and the structure of the optimization problem.
    In the following, we summarize the relevant papers according to the adopted uncertainty
    set of distributions and optimization structures. Distributionally robust individual
    linear chance constraints under the ambiguity set comprised of all distributions
    sharing the same known mean and covariance were reformulated as convex second-order
    cone constraints (Calafiore and El Ghaoui, 2006). The deterministic convex conditions
    to enforce distributionally robust chance constraints were provided under distribution
    families of (a) independent random variables with box-type support and (b) radially
    symmetric non-increasing distributions over the orthotope support. The worst-case
    conditional value-at-risk (CVaR) approximation for distributionally robust joint
    chance constraints was studied assuming first and second moment (Zymler et al.,
    2013), and the resulting conservative approximation can be cast as semidefinite
    program. In addition to moment information, a specific structural information
    of distributions called unimodality was incorporated into the ambiguity set, and
    the corresponding ambiguous risk constraints were reformulated as a set of second
    second-order cone constraints (Li et al., 2017). Instead of assuming unimodality
    of distributions, data-driven robust individual chance constrained programs along
    with convex approximations were recently developed using a mixture distribution-based
    ambiguity set with fixed component distribution and uncertain mixture weights
    (Chen et al., 2018). In real world applications, exact moment information can
    be challenging to obtain, and can only be estimated through confidence intervals
    from uncertainty realizations (Delage and Ye, 2010). To accommodate this moment
    uncertainty, attempts were made in the context of distributionally robust chance
    constraints, including constructing convex moment ambiguity set (El Ghaoui et
    al., 2003), employing Chebyshev ambiguity set with bounds on second-order moment
    (Cheng et al., 2014), characterizing a family of distributions with upper bounds
    on both mean and covariance (Zhang et al., 2018). Ambiguous joint chance constraints
    were studied where the ambiguity set was characterized by the mean, convex support,
    and an upper bound on the dispersion (Hanasusanto et al., 2017), and the resulting
    constraints were conic representable for right-hand-side uncertainty. In addition
    to generalized moment bounds (Wiesemann et al., 2014), structural properties of
    distributions, such as symmetry, unimodality, multimodality and independence,
    were further integrated into distributionally robust chance constrained programs
    leveraging a Choquet representation (Hanasusanto et al., 2015). Nonlinear extensions
    of distributionally robust chance constraints were made under the ambiguity sets
    defined by mean and variance (Yang and Xu, 2016), convex moment constraints (Xie
    and Ahmed, 2018), mean absolute deviation (Postek et al., 2018), and a mixture
    of distributions (Lasserre and Weisser, 2018). Although moment-based ambiguity
    sets achieve certain success, they do not converge to the true probability distribution
    as the number of available uncertainty data increases. Consequently, the resulting
    data-driven chance-constrained programs tend to generate conservative solutions.
    To this end, data-driven chance-constrained programs with distance-based ambiguity
    set were proposed to alleviate the undesirable consequence of moment-based data-driven
    chance-constrained programs. The ambiguity set defined by the Prohorov metric
    was introduced into the distributionally robust chance constraints, and the resulting
    optimization problem was approximated by using robust sampled problem (Erdogan
    and Iyengar, 2006). Distributionally robust chance constraints with the ambiguity
    set containing all distributions close to a reference distribution in terms of
    Kullback–Leibler divergence were cast as classical chance constraints with an
    adjusted risk level (Hu and Hong, 2013). Data-driven chance constrained programs
    with ϕ-divergence based ambiguity set were proposed (Jiang and Guan, 2016), and
    further extensions were made using the kernel smoothing method (Calfa et al.,
    2015). Recently, data-driven chance constraints over Wasserstein balls were exactly
    reformulated as mixed-integer conic constraints (Chen et al., 2018; Ji and Lejeune,
    2018). Leveraging the strong duality result (Gao and Kleywegt, 2016), distributionally
    robust chance constrained programs with Wasserstein ambiguity set were studied
    for linear constraints with both right and left hand uncertainty (Xie, 2018),
    as well as for general nonlinear constraints (Hota et al., 2018). Data-driven
    chance constrained programs have successful applications in a number of areas,
    such as power system (Xie and Ahmed, 2018), stochastic control (Van Parys et al.,
    2016), and vehicle routing problem (Ghosal and Wiesemann, 2018). 3.3. Data-driven
    robust optimization As a paramount ingredient in robust optimization, uncertainty
    sets endogenously determine robust optimal solutions and therefore should be devised
    with special care. However, uncertainty sets in the conventional robust optimization
    methodology are typically set a priori using a fixed shape and/or model without
    providing sufficient flexibility to capture the structure and complexity of uncertainty
    data. For example, the geometric shapes of uncertainty sets in (4) and (5) do
    not change with the intrinsic structure and complexity of uncertainty data. Furthermore,
    these uncertainty sets are specified by finite number of parameters, thereby having
    limited modeling flexibility. Motivated by this knowledge gap, data-driven robust
    optimization emerges as a powerful paradigm for addressing uncertainty in decision
    making. A data-driven ARO framework that leverages the power of Dirichlet process
    mixture model was proposed (Ning and You, 2017a). The data-driven approach for
    defining uncertainty set was developed based on Bayesian machine learning. This
    machine learning model was then integrated with the ARO method through a four-level
    optimization framework. This developed framework effectively accounted for the
    correlation, asymmetry and multimode of uncertainty data, so it generated less
    conservative solutions. Its salient feature is that multiple basic uncertainty
    sets are used to provide a high-fidelity description of uncertainties. Although
    the data-driven ARO has a number of attractive features, it does not account for
    an important evaluation metric, known as regret, in decision-making (Bell, 1982).
    Motivated by the knowledge gap, a data-driven bi-criterion ARO framework was developed
    that effectively accounted for the conventional robustness as well as minimax
    regret (Ning and You, 2018a). In some applications, uncertainty data in large
    datasets are usually collected under multiple conditions. A data-driven stochastic
    robust optimization framework was proposed for optimization under uncertainty
    leveraging labeled multi-class uncertainty data (Ning and You, 2018b). Machine
    learning methods including Dirichlet process mixture model and maximum likelihood
    estimation were employed for uncertainty modeling, which is illustrated in Fig.
    1. This framework was further proposed based on the data-driven uncertainty model
    through a bi-level optimization structure. The outer optimization problem followed
    the two-stage stochastic programming approach, while ARO was nested as the inner
    problem for maintaining computational tractability. Download : Download high-res
    image (289KB) Download : Download full-size image Fig. 1. The data-driven uncertainty
    model based on the Dirichlet process mixture model (Ning and You, 2018b). To mitigate
    computational burden, research effort has been made on convex polyhedral data-driven
    uncertainty set based on machine learning techniques, such as principal component
    analysis and support vector clustering. A data-driven robust optimization framework
    that leveraged the power of principal component analysis and kernel smoothing
    for decision-making under uncertainty was studied (Ning and You, 2018c). In this
    approach, correlations between uncertain parameters were effectively captured,
    and latent uncertainty sources were identified by principal component analysis.
    To account for asymmetric distributions, forward and backward deviation vectors
    were utilized in the uncertainty set, which was further integrated with robust
    optimization models. A data-driven static robust optimization framework based
    on support vector clustering that aims to find the hypersphere with minimal volume
    to enclose uncertainty data was proposed (Shang et al., 2017). The adopted piecewise
    linear kernel incorporates the covariance information, thus effectively capturing
    the correlation among uncertainties. These two data-driven robust optimization
    approaches utilized polyhedral uncertainty learned from data, and thus enjoying
    computational efficiency. Various types of data-driven uncertainty sets were developed
    for static robust optimization based on statistical hypothesis tests (Bertsimas
    et al., 2018), copula (Zhang et al., 2018), and probability density contours (Zhang
    et al., 2018). To address multistage decision making under uncertainty, a data-driven
    approach for optimization under uncertainty based on multistage ARO and nonparametric
    kernel density M-estimation was developed (Ning and You, 2017b). The salient feature
    of the framework was its incorporation of distributional information to address
    the issue of over-conservatism. Robust kernel density estimation was employed
    to extract probability distributions from data. This data-driven multistage ARO
    framework exploited robust statistics to be immunized to data outliers. An exact
    robust counterpart was developed for solving the resulting data-driven ARO problem.
    In recent years, data-driven robust optimization has been applied to a variety
    of areas, such as power systems (Ning and You, 2019), energy systems (Zhao et
    al., 2019), planning and scheduling (Ning and You, 2017b; Zhang et al., 2018),
    process control (Ning and You, 2018c; Shang and You, 2019), and transportation
    systems (Miao et al., 2019; Zhao and You, 2019). 3.4. Scenario optimization approach
    for chance constrained programs A salient feature of scenario-based optimization
    is that it does not require the explicit knowledge of probability distribution
    as in the stochastic programming approach. Additionally, scenario-based optimization
    uses uncertainty scenarios to seek an optimal solution having a high probabilistic
    guarantee of constraint satisfaction instead of utilizing scenarios or samples
    to approximate the expectation term as in stochastic programming. Although the
    scenario-based optimization can be regarded as a special type of robust optimization
    that has a discrete uncertainty set consisting of uncertainty data, it can provide
    probabilistic guarantee for those unobserved uncertainty data in the testing data
    set. Note that the scenario-based optimization approach provides a viable and
    data-driven route to achieving approximate solutions of chance-constrained programs.
    The scenario-based optimization approach is a general data-driven optimization
    under uncertainty framework in which uncertainty data or random samples are utilized
    in a more direct manner compared with other data-driven optimization methods.
    This data-driven optimization framework was first introduced in (Calafiore and
    Campi, 2005), and has gained great popularity within the systems and control community
    (Campi et al., 2009). As in data-driven chance constrained programs, the knowledge
    of true underlying uncertainty distribution is not required in scenario optimization
    but a finite number of uncertainty realizations. Specifically, the scenario approach
    enforces the constraint satisfaction with N independent identically distributed
    uncertainty data u(1), … , u(N). The resulting scenario optimization problem is
    given by, (13) where x is the vector of decision variables, X represents a deterministic
    convex and closed set unaffected by uncertainty, c is the vector of cost coefficients,
    and f denotes the constraint function affected by uncertainty u. Note that function
    f is typically assumed to be convex in x, and can have arbitrarily nonlinear dependence
    on u, as opposed to data-driven nonlinear chance constrained program assuming
    the constraint function must be quasi-convex in u (Yang and Xu, 2016). Additionally,
    scenario-based optimization can be considered as a special case of data-driven
    robust optimization when the uncertainty set is constructed as a union of u(1),
    … , u(N). In the scenario optimization literature, is referred to as the multi-sample
    or scenario that is drawn from the product probability space. Due to the random
    nature of the multi-sample, the optimal solution of the scenario optimization
    problem (13), denoted as x*(ω), is also random. One key merit of the scenario
    approach is that the scenario optimization problem admits the same problem type
    as its deterministic counterpart, so that it can be solved efficiently by convex
    optimization algorithms when f(x, u) is convex in x (Boyd and Vandenberghe, 2004).
    Moreover, the optimal solution x*(ω) is guaranteed to satisfy the constraints
    with other unseen uncertainty realizations with a high probability (Campi and
    Garatti, 2008). For the sake of clarity, we revisit the following definition and
    theorem (Campi and Garatti, 2008). Definition (Violation probability) The violation
    probability of a given decision x is defined as follows: (14) where V(x) denotes
    the probability of violation for a given x, and Ξ represents the support of uncertainty
    u. We say a decision x is ε-feasible if V(x) ≤ ε. Theorem. Assuming x*(ω) is the
    unique optimal solution of the scenario optimization problem. It holds that (15)
    where n is the number of decision variables, N denotes the number of uncertainty
    data, and is a product probability governing the sample generation. The above
    theorem implies that the optimal solution x*(ω) satisfies the corresponding chance
    constraint with a certain confidence level. The proof of this theorem depends
    on the fundamental fact that the number of support constraints, the removal of
    which changes the optimal solution, is upper bounded by the number of decision
    variables (Calafiore and Campi, 2005). Note that (15) holds with equality for
    the fully-supported convex optimization problem (Campi and Garatti, 2008), meaning
    that the probability bound is tight. Additionally, the result holds true irrespective
    of probability distribution information or even its support set. By exploiting
    the structured dependence on uncertainty, the sample size required by the scenario
    optimization problem was reduced through a tighter bound on Helly''s dimension
    (Zhang et al., 2015). Rather than focusing on the constraint violation probability,
    considerable research efforts have been made on the degree of violation (Kanamori
    and Takeda, 2012), expected probability of constraint violation (Calafiore, 2009),
    and the performance bounds for objective values (Esfahani et al., 2015). To make
    a trade-off between feasibility and performance, the case was studied where some
    of the sampled constraints were allowed to be violated for improving the performance
    of the objective (Calafiore, 2010). Subsequent work along this direction includes
    a sampling-and-discarding method (Campi and Garatti, 2011). A wait-and-judge scenario
    optimization framework was proposed in which the level of robustness was assessed
    a posteriori after the optimal solution was obtained (Campi and Garatti, 2018).
    Recently, the extension of scenario-based optimization to the multistage decision
    making setting was made (Kariotoglou et al., 2016; Vayanos et al., 2012). While
    the scenario optimization problems with continuous decision variables are extensively
    studied (Campi et al., 2009), the mixed-integer scenario optimization was less
    developed. An attempt to extend the scenario theory to random convex programs
    with mixed-integer decision variables was made (Calafiore et al., 2012), and the
    Helly dimension in the mixed-integer scenario program was proved to depend geometrically
    on the number of integer variables. This result suggests that the required sample
    size can be prohibitively large for scenario programs with many discrete variables.
    Along this research direction, two sampling algorithms within the framework of
    S-optimization were recently developed for solving mixed-integer convex scenario
    programs (De Loera et al., 2018). In some real-world applications, the required
    sample size can be very large, resulting in great computational burden for scenario
    optimization problems with huge number of sampling constraints. One way to circumvent
    this difficulty is to devise sequential solution algorithms. Along this direction,
    sequential randomized algorithms were developed for convex scenario optimization
    problems (Chamanbaz et al., 2016), and fell into the framework of Sequential Probabilistic
    Validation (SPV) (Alamo et al., 2015). The motivation behind these sequential
    algorithms is that validating a given solution with a large number of samples
    is less computational expensive than solving the corresponding scenario optimization
    problem. Recently, a repetitive scenario design approach was proposed by iterating
    between reduced-size scenario optimization problems and the probabilistic feasibility
    check (Calafiore, 2017). The trade-off between the sample size and the expected
    number of repetitions was also revealed in the repetitive scenario design (Calafiore,
    2017). Note that the classical scenario-based approach is an extreme situation
    in the trade-off curve, where one seeks to find the solution at one step. Another
    effective way to reduce the computation cost of large-scale scenario optimization
    is to employ distributed algorithms (Margellos et al., 2018; Carlone et al., 2014).
    Particularly, the sampled constraints were distributed among multiple processors
    of a network, and the large-scale scenario optimization problems can be efficiently
    solved via constraint consensus schemes (Carlone et al., 2014). Along this direction,
    a distributed computing framework was developed for the scenario convex program
    with multiple processors connected by a graph (You et al., 2018). The major advantage
    of this approach is that the computational cost for each processor becomes lower
    and the original scenario optimization problem can be solved collaboratively.
    Other contribution to reduce computational cost is made based on a non-iterative
    two-step procedure, i.e. the optimization step and detuning step (Care et al.,
    2014). As a consequence, the total sample complexity was greatly decreased. Traditionally,
    the field of scenario optimization has focused on convex optimization problems,
    in which the number of support constraints is upper bounded by the number of decision
    variables. However, such upper bounds are no longer available in nonconvex scenario
    optimization problems, giving rise to research challenges of extending the scenario
    theory to the nonconvex setting. To date, few works have considered nonconvex
    uncertain program using the scenario approach. One contribution is that of (Campi
    et al., 2018), in which assessing the generalization of the optimal solution in
    a wait-and-judge manner through the concept of support sub-sample was proposed.
    The proposed approach can be employed to general nonconvex setups, including mixed-integer
    scenario optimization problems. Another attempt to address nonconvex scenario
    optimization made use of the statistical learning theory for bounding the violation
    probability, and devised a randomized solution algorithm (Alamo et al., 2009).
    The statistical learning theory-based method provided the probabilistic guarantee
    for all feasible solutions, as opposed to the convex scenario approach where such
    guarantee is valid only for the optimal solution. This unique feature regarding
    probabilistic guarantees for all feasible solutions granted by the statistical
    learning based method is of practical relevance (Calafiore et al., 2011), since
    it is computationally challenging to solve nonconvex optimization problems to
    global optimality. A class of non-convex scenario optimization problem, which
    has non-convex objective functions and convex constraints, was recently studied
    (Grammatico et al., 2016). Since the Helly''s dimension for the optimal solution
    of such non-convex scenario program can be unbounded, the direct application of
    scenario approaches based on Helly''s theorem is impossible. To overcome the research
    challenge, the feasible region was restricted to the convex hull of few optimizers,
    thus enabling the application of sample complexity results (Campi and Garatti,
    2008). 4. Future research directions and opportunities Several promising research
    directions in data-driven optimization under uncertainty are highlighted in this
    section. We specifically focus on some ideas on closed-loop data-driven optimization,
    integration of deep learning and scenario-based optimization, and learning-while-optimizing
    frameworks. 4.1. A “closed-loop” data-driven optimization framework with feedback
    from mathematical programming to machine learning The framework of data-driven
    optimization under uncertainty could be considered as a “hybrid” system that integrates
    the data-driven system based on machine learning to extract useful and relevant
    information from data, and the model-based system based on the mathematical programming
    to derive the optimal decisions from the information. Existing data-driven optimization
    approaches adopt a sequential and open-loop scheme, which could be further improved
    by introducing feedback steps from the model-based system to data-driven system.
    A “closed-loop” data-driven optimization paradigm that explores the information
    feedback to fully couple upper-stream machine learning and downstream mathematical
    programming could be a more effective and rigorous approach. 4.1.1. The issues
    of conventional “open-loop” data-driven optimization methods It is widely recognized
    that data-driven optimization is a promising way to hedging against uncertainty
    in the era of big data and deep learning. Such promise hinges heavily on the organic
    integration and effective interaction between machine learning and mathematical
    programming. In existing data-driven optimization frameworks, the tasks performed
    by the data-driven system and the model-based system are treated separately in
    a sequential fashion. More specifically, data serve as input to a data-driven
    system. After that, useful, accurate and relevant uncertainty information is extracted
    through the data-driven system and further passed along to the model-based system
    based on mathematical programming for rigorous and systematic optimization under
    uncertainty, using paradigms such as robust optimization and stochastic programming.
    However, due to the sequential connection between these two systems, the machine
    learning model is trained without interacting with the “downstream” mathematical
    programming. Accordingly, from a control theoretical perspective, such “hybrid”
    systems in the existing data-driven optimization literature are essentially open
    loop. In contrast to open-loop systems, closed-loop systems using the feedback
    control strategy deliver amazingly superior system performance (e.g. stability,
    robustness to disturbances, and safety) in virtually every area of science and
    engineering, such as biological systems, social networks, and mechanical systems
    (A˚ström and Kumar, 2014). Therefore, there should be a “feedback” channel for
    information flow returning from the model-based system to the data-driven system,
    in addition to the information flow that is fed into the mathematical programming
    problem from the machine learning results. The design of such feedback loops from
    mathematical programming to machine learning deserves further attention in future
    research. Although there are closed-loop machine learning methods in the case
    of reinforcement learning (Shin and Lee, 2019), to the best of our knowledge,
    there are few works on developing a closed-loop strategy for data-driven mathematical
    programming under uncertainty. Different from mathematical programming, reinforcement
    learning is a kind of machine learning that aims to find an action policy to increase
    an agent''s performance in terms of reward by interacting with an environment.
    4.1.2. A “closed-loop” data-driven optimization framework Due to its critical
    role in the training of machine learning models, loss functions could provide
    a foundation for considering feedback steps in future research efforts. Instead
    of using a mathematical-programming-agnostic loss function (e.g. logistic or squared-error
    loss), a loss function that incorporates the objective function of mathematical
    programming could be used to train the machine learning model. Specifically, a
    weighted sum of the conventional loss function and the objective function in the
    mathematical programming problem should be useful in handling issues experienced
    with current “open-loop” data-driven frameworks. An iterative scheme between machine
    learning and mathematical programming offers an alternative promising path to
    close the loop of the data-driven system and the model-based system. Fig. 2 presents
    the potential schematic of the closed-loop data-driven mathematical programming
    system. From the figure, we can see that the feedback from the model-based system
    serves as input to the data-driven system. In this way, the “hybrid” system becomes
    a closed-loop one in which information can be transmitted in both directions.
    Such feedback strategy should be beneficial to the “hybrid” system and could provide
    an effective way to organically integrate machine learning and mathematical programming.
    Download : Download high-res image (248KB) Download : Download full-size image
    Fig. 2. The schematic of “closed-loop” data-driven mathematical programming framework.
    Research challenges emerge from the feedback step in the “hybrid” system. In typical
    PSE applications, the problem size of mathematical programs tends to be large.
    Such large-scale mathematical programming problems in conjunction with big data
    could pose a computational challenge for the training of machine learning. Additionally,
    how to design an effective feedback strategy to “close the loop” poses another
    key challenge to be addressed. 4.1.3. Incorporating “prior” knowledge in the data-driven
    optimization framework In addition to uncertainty data, some available domain-specific
    knowledge or “prior” knowledge could serve as another informative input to the
    data-driven system. Relying solely on the data to develop the uncertainty model
    could unfavorably influence the downstream mathematical programming. The prior
    knowledge depicts what the decision maker knows about the uncertainty, and it
    can come in different forms. For example, the prior knowledge could be the structural
    information of probability distributions, upper and lower bounds of uncertain
    parameters or certain correlation relationship among uncertainties. Incorporating
    such “prior” knowledge in the data-driven optimization framework could be substantially
    useful and provides more reliable results in the face of messy data. 4.2. Leveraging
    deep learning techniques for hedging against uncertainty in data-driven optimization
    Recently, deep learning has shown great promise due to its amazing power in hierarchical
    representation of data (Goodfellow et al., 2016). The deep learning techniques
    are now shaping and revolutionizing many areas of science and engineering (LeCun
    et al., 2015). In recent years, deep learning has a wide array of applications
    in the PSE domain, such as process monitoring (Zhu et al., 2019; Zhang and Zhao,
    2017), refinery scheduling (Gao et al., 2014), and soft sensor (Shang et al.,
    2014). For extensive surveys on deep learning in the PSE area, we refer the reader
    to the review papers on this subject (Venkatasubramanian, 2019; Lee et al., 2018).
    In real applications, uncertainty data exhibit very complex and highly nonlinear
    characteristic. Therefore, it should be promising to explore the potential opportunities
    of leveraging deep neural networks with various architectures to uncover useful
    patterns of uncertainty data for mathematical programming. In this section, a
    variety of deep learning techniques are first summarized along with their unique
    features from a practical point of view, and future research directions on how
    to leverage the power of deep learning in optimization under uncertainty are further
    suggested. Research opportunities of integrating data-driven scenario-based optimization
    with deep generative models are then presented. 4.2.1. Various types of deep learning
    techniques and their potentials In this subsection, we present three types of
    deep learning techniques, including deep belief networks, convolutional neural
    networks, and recurrent neural networks, and explore their potential applications
    in data-driven optimization under uncertainty. • Deep belief networks Among deep
    learning techniques, deep belief networks (DBNs) are becoming increasingly popular
    primarily because its unique feature in capturing a hierarchy of latent features
    (Mohamed et al., 2012). DBNs essentially belong to probabilistic graphical models
    and are structured by stacking a series of restricted Boltzmann machines (RBMs).
    This specific network structure is designed based on the fact that a single RBM
    with only one hidden layer fall shorts of capturing the intrinsic complexities
    in high-dimensional data. As the building blocks for DBNs, RBMs are characterized
    as two layers of neurons, namely hidden layer and visible layer. Note that the
    hidden layer can be regarded as the abstract representation of the visible layer.
    There are undirected connections between these two layers, while there exist no
    intra-connections within each layer. The training process of DBNs typically involves
    the pre-training and fine-tuning procedures in a layer-wise scheme. Armed with
    multiple layers of hidden variables, DBNs enjoy unique power in extracting a hierarchy
    of latent features automatically, which is desirable in many practical applications.
    As a result, DBNs have been applied in a wide spectrum of areas, including fault
    diagnosis (Zhang and Zhao, 2017), soft sensor (Shang et al., 2014), and drug discovery
    (Gawehn et al., 2016). DBNs can decipher complicated nonlinear correlation among
    uncertain parameters. Recently, deep Gaussian process model was proposed as a
    special type of DBN based Gaussian process mappings. Due to its unique advantage
    in nonlinear regression, deep Gaussian process model should be used to characterize
    the relationship between uncertain parameters, such as product price and demand.
    • Convolutional neural networks Convolutional neural networks (CNNs) are one specialized
    version of deep neural networks (Krizhevsky et al., 2017), and they have become
    increasingly popular in areas such as image classification, speech recognition,
    and robotics. Inspired by the visual neuroscience, CNNs are designed to fully
    exploit the three main ideas, namely sparse connectivity, weight sharing, and
    equivariant representations (Goodfellow et al., 2016). This kind of neural network
    is suited for processing data in the form of multiple arrays, particularly two-dimensional
    image data. The architecture of a CNN typically consists of convolution layers,
    nonlinear layers, and pooling layers. In convolution layers, feature maps are
    extracted by performing convolutions between local patch of data and filters.
    The filters share the same weights when moving across the dataset, leading to
    reduced number of parameters in networks. The obtained results are further passed
    through a nonlinear activation function, such as rectified linear unit (ReLU).
    After that, pooling layers, such as max pooling and average pooling, are applied
    to aggregate semantically similar features. Such different types of layers are
    alternatively connected to extract hierarchical features with various abstractions.
    For the purpose of classification, a fully connected layer is stacked after extracting
    the high-level features. Although CNNs are mainly used for image classification,
    they have been used to learn spatial features of traffic flow data at nearby locations
    which exhibit strong spatial correlations (Wu et al., 2018). Given its unique
    power in spatial data modeling, CNNs hold the potential to model uncertainty data
    with large spatial correlations, such as demand data in different adjacent market
    locations. In addition, the CNNs can be trained for the labeled multi-class uncertainty
    data to perform the task of classification. Therefore, the output of the CNN potentially
    acts as the probability weights used in the data-driven stochastic robust optimization
    framework. • Recurrent neural networks Besides the aforementioned models for spatial
    data, recurrent neural networks (RNNs) are widely recognized as the state-of-the-art
    deep learning technique for processing time series data, especially those from
    language and speech (Graves et al., 2013). RNNs can be considered as feedforward
    neural networks if they are unfolded in time scale. The architecture of neural
    networks in a RNN possesses a unique structure of directed cycles among hidden
    units. In addition, the inputs of the hidden unit come from both the hidden unit
    of previous time and the input unit at current time. Accordingly, these hidden
    units in the architecture of RNNs constitute the state vectors and store the historical
    information of past input data. With this special architecture, RNNs are well-suited
    for feature learning for sequential data and demonstrate successful applications
    in various areas, including natural speech recognition (Graves et al., 2013),
    and load forecasting (Vermaak and Botha, 1998). However, one drawback of RNNs
    is its weakness in storing long-term memory due to gradient vanishing and exploding
    problems. To address this issue, research efforts have been made on variants of
    RNNs, such as long short-term memory (LSTM) and gated recurrent unit (GRU) (Hochreiter
    and Schmidhuber, 1997). By explicitly incorporating input, output and forget gates,
    LSTM enhances the capability of memorizing the long-term dependency among sequential
    data. In sequential mathematical programming under uncertainty, massive time series
    of uncertain parameters are collected. Uncertainty data realized at different
    time stages often exhibit temporal dynamics. To this end, deep learning techniques,
    such as deep RNNs and LSTM, could be leveraged to decipher the temporal dynamics
    and trajectories of uncertainty over time stages. Therefore, exploring the integration
    between deep learning and multistage optimization under uncertainty is another
    promising research direction. 4.2.2. Deep generative models for scenario-based
    optimization Despite the various successful applications of scenario-based optimization,
    this type of data-driven optimization framework has its own limitations. In general,
    scenario-based optimization enjoys computational efficiency by constraint sampling
    and provides the feasibility guarantee regardless of probability types. These
    advantages of scenario-based optimization rely heavily on the key assumption that
    sufficient amount of uncertainty data is available. However, in most practical
    cases, this assumption does not hold, and on the contrary the amount of uncertainty
    data sampled from the underlying true distribution is quite limited. Moreover,
    acquiring uncertainty data could be extremely expensive or time consuming in some
    specific cases, which greatly hinders the applicability of the scenario-based
    approach (Gupta and Rusmevichientong, 2017). Existing studies of the scenario-based
    optimization neglect the aforementioned practical situation (Campi et al., 2009;
    Zhang et al., 2015; Calafiore, 2017). The practical challenge of handling insufficient
    amount of data requires further research attention, and data-driven scenario-based
    optimization frameworks addressing this issue need to be developed. This knowledge
    gap could be potentially filled by leveraging the power of deep generative models
    for the data-driven scenario-based optimization, whose schematic is shown in Fig.
    3. Instead of assuming unlimited uncertainty scenarios sampled from the true distribution,
    deep generative models could be leveraged to learn the intrinsic useful patterns
    from the available uncertainty data and to generate synthetic data. These synthetic
    uncertainty data generated by the deep learning techniques mimic the real uncertainty
    data, and should be potentially useful in the scenario-based optimization model.
    Deep generative models can be utilized to generate synthetic uncertainty data
    with the aim for better decision with insufficient uncertainty data. To be more
    precise, in deep generative models, the true data distribution is learned either
    explicitly or implicitly, and then the learned distribution is used to generate
    new data points referred to as synthetic data. One of the most commonly used deep
    generative models is variational autoencoders (VAEs) (Goodfellow et al., 2016).
    VAEs generate new data samples through the architecture of synthesizing an encoder
    network and a decoder network in an unsupervised fashion. The function of encoders
    is to reduce the dimension of input data and extracts the latent features, while
    the decoder network aims to reconstruct data given the latent variables. In this
    way, the VAE model learns the complicated target distribution by maximizing the
    lower bound of the data log-likelihood. The advantage of this technique is that
    its quality is easily evaluated via log-likelihood or importance sampling. However,
    researchers have found out that VAEs typically tend to generate blurry images,
    meaning a noticeable difference between the true distribution and the learned
    one (Goodfellow et al., 2016). Recently, an emerging deep generative model named
    generative adversarial networks (GANs) was proposed and has become increasingly
    popular in various areas, such as image processing (Ledig et al., 2017), renewable
    scenario generation (Chen et al., 2018), and molecular designs (Sanchez-Lengeling
    and Aspuru-Guzik, 2018). Different from VAEs, GANs implicitly learn the data distribution
    through a zero-sum game between two competing neural networks, namely generator
    network and discriminator network (Goodfellow et al., 2014). Given the noise input,
    the generator network competes against the discriminator network by generating
    plausible synthetic data. On the contrary, the discriminator network attempts
    to distinguish the real uncertainty data from the synthetic data. These two networks
    compete against each other. Accordingly, the data distribution resulted from the
    generator network will be the true distribution once the Nash equilibrium is achieved.
    The required sample size for random convex programs scales linearly with the number
    of decision variables (Alamo et al., 2015), implying that the “small data” regime
    should be frequently encountered for large-scale optimization problems. Consequently,
    data-driven scenario-based optimization tends to suffer severely from the issue
    of insufficient uncertainty data. Leveraging the power of deep generative models
    could be a promising way to addressing this challenge. The required sample size
    to guarantee the constraint satisfaction could become large for optimization problems
    with a huge number of decision variables and a small value of risk level (Alamo
    et al., 2015). Therefore, the available amount of uncertainty data might not be
    enough for the purpose of probabilistic guarantee. However, the number of uncertainty
    data can still be sufficient for training generative models. Download : Download
    high-res image (264KB) Download : Download full-size image Fig. 3. The schematic
    of the scenario-based optimization framework based on deep learning. Although
    deep learning could be a silver bullet in many areas, a lot of research challenges
    still persist in organically integrating the state-of-the-art deep learning techniques
    with optimization under uncertainty. The discussion in Section 4.2. is aimed to
    serve as a good starting point to promote the employment of deep learning in the
    field of data-driven optimization under uncertainty. 4.3. Online learning-based
    data-driven optimization: a learning-while-optimizing paradigm for addressing
    uncertainty In conventional data-driven optimization frameworks, a batch of uncertainty
    data serves as input to the data-driven system, in which learning typically takes
    place only once and is termed as batch machine learning. Most, if not all, of
    the papers on data-driven optimization under uncertainty are restricted to such
    learning of data (Ning and You, 2017a; Esfahani and Kuhn, 2018; Jiang and Guan,
    2016; Shang et al., 2017), so they fail to account for real-time data. For example,
    in data-driven robust optimization methods, uncertainty sets are learned from
    a batch of uncertainty data. Once these data-driven uncertainty sets are obtained,
    they remain fixed for the model-based system based on mathematical programming
    and are not updated or refined. Additionally, probability distributions of uncertainties
    and their support sets could be time variant and evolves gradually, rendering
    the data-driven system “outdated”. Such obsolete data-driven system inevitably
    deteriorates the resulting solution quality of the mathematical programming problem.
    In many practical settings, uncertainty data are collected sequentially in an
    online fashion (Shalev-Shwartz, 2012). Although previous works have explored the
    online learning of uncertainty sets (Shang and You, 2019), they typically re-train
    the data-driven system from scratch using the existing and new addition of data,
    thus making these approaches suitable only for systems with slow dynamics. Therefore,
    few studies to date investigate the real-time data analytics for systems with
    fast dynamics such as those encountered in chemical processes, establishing research
    opportunities for the PSE community. An online-learning-based data-driven optimization
    paradigm, in which learning takes place iteratively to account for real-time data,
    could be a promising research direction. More specifically, a learning-while-optimizing
    scheme could be explored by taking advantage of deep reinforcement learning. On
    the one hand, the uncertainty model should be time varying to accommodate real-time
    uncertainty data. On the other hand, decisions are made sequentially under uncertainty.
    After decisions are made, uncertainties are realized and then collected in the
    database. There are research challenges associated with such online-learning-based
    frameworks. Updating the data-driven system in an online fashion is paramount
    in implementing the learning-while-optimizing scheme and poses a key research
    challenge. Additionally, developing efficient algorithms to solve the resulting
    online-learning-based mathematical programming problems creates the computational
    challenge. There exist some theoretical research challenges as well. One theoretical
    challenge is to investigate the convergence of solutions when the probability
    distribution shifts to a new one. Another challenge is to provide theoretical
    bounds for computational complexity and required memory for the online-learning-based
    data-driven optimization. 5. Conclusions Although conventional stochastic programming,
    robust optimization, and chance constrained optimization are the most recognized
    modeling paradigms for hedging against uncertainty, it is foreseeable that in
    the near future data-driven mathematical programming frameworks would experience
    a rapid growth fueled by big data and deep learning. We reviewed recent progress
    of data-driven mathematical programming under uncertainty in terms of systematic
    uncertainty modeling, organic integration of machine learning and mathematical
    programming, and efficient computational algorithms for solving the resulting
    mathematical programming problems. The advantages and disadvantages of different
    data-driven uncertainty models were also analyzed in detail. Future research could
    be directed toward devising feedback steps to close the loop of the data-driven
    system and the model-based system, leveraging the power of deep generative models
    for the data-driven scenario-based optimization, and developing data-driven mathematical
    programming frameworks with online learning for real-time data. Appendix. Supplementary
    materials Download : Download XML file (272B) Research data for this article Data
    not available / No data was used for the research described in the article Further
    information on research data References A˚ström and Kumar, 2014 K.J. A˚ström,
    P.R. Kumar Control: a perspective Automatica, 50 (2014), pp. 3-43 View PDFView
    articleCrossRefGoogle Scholar Acevedo and Pistikopoulos, 1998 J. Acevedo, E.N.
    Pistikopoulos Stochastic optimization based algorithms for process synthesis under
    uncertainty Comput. Chem. Eng., 22 (1998), pp. 647-671 View PDFView articleView
    in ScopusGoogle Scholar Ahmed et al., 2004 S. Ahmed, M. Tawarmalani, N.V. Sahinidis
    A finite branch-and-bound algorithm for two-stage stochastic integer programs
    Math. Program., 100 (2004), pp. 355-377 CrossRefView in ScopusGoogle Scholar Alamo
    et al., 2009 T. Alamo, R. Tempo, E.F. Camacho Randomized strategies for probabilistic
    solutions of uncertain feasibility and optimization problems IEEE Trans. Autom.
    Control, 54 (2009), pp. 2545-2559 View in ScopusGoogle Scholar Alamo et al., 2015
    T. Alamo, R. Tempo, A. Luque, D.R. Ramirez Randomized methods for design of uncertain
    systems: sample complexity and sequential algorithms Automatica, 52 (2015), pp.
    160-172 View PDFView articleView in ScopusGoogle Scholar Assavapokee et al., 2008
    T. Assavapokee, M.J. Realff, J.C. Ammons Min-max regret robust optimization approach
    on interval data uncertainty J. Optim. Theory Appl., 137 (2008), pp. 297-316 CrossRefView
    in ScopusGoogle Scholar Atamtürk and Zhang, 2007 A. Atamtürk, M. Zhang Two-stage
    robust network flow and design under demand uncertainty Oper. Res., 55 (2007),
    pp. 662-673 CrossRefView in ScopusGoogle Scholar Baringo and Baringo, 2018 L.
    Baringo, A. Baringo A stochastic adaptive robust optimization approach for the
    generation and transmission expansion planning IEEE Trans. Power Syst., 33 (2018),
    pp. 792-802 View in ScopusGoogle Scholar Bayraksan and Love, 2015 G. Bayraksan,
    D.K. Love Data-Driven Stochastic Programming Using Phi-Divergences The Operations
    Research Revolution (2015), pp. 1-19 CrossRefView in ScopusGoogle Scholar Bell,
    1982 D.E. Bell Regret in decision making under uncertainty Oper. Res., 30 (1982),
    pp. 961-981 CrossRefView in ScopusGoogle Scholar Ben-Tal and Nemirovski, 2000
    A. Ben-Tal, A. Nemirovski Robust solutions of linear programming problems contaminated
    with uncertain data Math. Program., 88 (2000), p. 411 View in ScopusGoogle Scholar
    Ben-Tal and Nemirovski, 2002 A. Ben-Tal, A. Nemirovski Robust optimization—methodology
    and applications Math. Program., 92 (2002), pp. 453-480 CrossRefView in ScopusGoogle
    Scholar Ben-Tal et al., 2004 A. Ben-Tal, A. Goryashko, E. Guslitzer, A. Nemirovski
    Adjustable robust solutions of uncertain linear programs Math. Program., 99 (2004),
    pp. 351-376 CrossRefView in ScopusGoogle Scholar Ben-Tal et al., 2009 A. Ben-Tal,
    L.E. Ghaoui, A. Nemirovski Robust Optimization Princeton University Press (2009)
    Google Scholar Ben-Tal et al., 2009 A. Ben-Tal, L. El Ghaoui, A. Nemirovski Robust
    Optimization University Press, Princeton (2009) Google Scholar Bertsimas and Sim,
    2004 D. Bertsimas, M. Sim The price of robustness Oper. Res., 52 (2004), p. 35
    View in ScopusGoogle Scholar Bertsimas and Thiele, 2006 D. Bertsimas, A. Thiele
    Robust and data-driven optimization: modern decision-making under uncertainty
    INFORMS Tutor. Oper. Res. (2006), pp. 95-122 CrossRefGoogle Scholar Bertsimas
    et al., 2011 D. Bertsimas, D.B. Brown, C. Caramanis Theory and applications of
    robust optimization SIAM Rev., 53 (2011), pp. 464-501 CrossRefView in ScopusGoogle
    Scholar Bertsimas et al., 2013 D. Bertsimas, E. Litvinov, X.A. Sun, J. Zhao, T.
    Zheng Adaptive Robust Optimization for the Security Constrained Unit Commitment
    Problem IEEE Trans. Power Syst., 28 (2013), pp. 52-63 View in ScopusGoogle Scholar
    Bertsimas et al., 2018 D. Bertsimas, V. Gupta, N. Kallus Data-driven robust optimization
    Math. Program., 167 (2018), pp. 235-292 CrossRefView in ScopusGoogle Scholar Bertsimas
    et al., 2019 D. Bertsimas, M. Sim, M. Zhang Adaptive distributionally robust optimization
    Manag. Sci., 0 (2019) null Google Scholar Biegler and Grossmann, 2004 L.T. Biegler,
    I.E. Grossmann Retrospective on optimization Comput. Chem. Eng., 28 (2004), pp.
    1169-1192 View PDFView articleView in ScopusGoogle Scholar Birge and Louveaux,
    2011 J.R. Birge, F. Louveaux Introduction to Stochastic Programming Springer Science
    & Business Media (2011) Google Scholar Birge, 1997 J.R. Birge State-of-the-Art-Survey—stochastic
    programming: computation and applications INFORMS J. Comput., 9 (1997), pp. 111-133
    CrossRefView in ScopusGoogle Scholar Bonfill et al., 2004 A. Bonfill, M. Bagajewicz,
    A. Espuña, L. Puigjaner Risk management in the scheduling of batch plants under
    uncertain market demand Ind. Eng. Chem. Res., 43 (2004), pp. 741-750 View in ScopusGoogle
    Scholar Bonfill et al., 2005 A. Bonfill, A. Espuña, L. Puigjaner Addressing robustness
    in scheduling batch processes with uncertain operation times Ind. Eng. Chem. Res.,
    44 (2005), pp. 1524-1534 CrossRefView in ScopusGoogle Scholar Boyd and Vandenberghe,
    2004 S. Boyd, L. Vandenberghe Convex Optimization Cambridge University Press (2004)
    Google Scholar Calafiore and Campi, 2005 G. Calafiore, M.C. Campi Uncertain convex
    programs: randomized solutions and confidence levels Math. Program., 102 (2005),
    pp. 25-46 CrossRefView in ScopusGoogle Scholar Calafiore and El Ghaoui, 2006 G.C.
    Calafiore, L. El Ghaoui On distributionally robust chance-constrained linear programs
    J. Optim. Theory Appl., 130 (2006), pp. 1-22 CrossRefView in ScopusGoogle Scholar
    Calafiore et al., 2011 G. Calafiore, F. Dabbene, R. Tempo Research on probabilistic
    methods for control system design Automatica, 47 (2011), pp. 1279-1293 View PDFView
    articleView in ScopusGoogle Scholar Calafiore et al., 2012 G. Calafiore, D. Lyons,
    L. Fagiano On mixed-integer random convex programs 2012 IEEE 51st IEEE Conference
    on Decision and Control (CDC) (2012), pp. 3508-3513 CrossRefView in ScopusGoogle
    Scholar Calafiore, 2009 G. Calafiore On the expected probability of constraint
    violation in sampled convex programs J. Optim. Theory Appl., 143 (2009), pp. 405-412
    CrossRefView in ScopusGoogle Scholar Calafiore, 2010 G.C. Calafiore Random convex
    programs SIAM J. Optim., 20 (2010), pp. 3427-3464 CrossRefView in ScopusGoogle
    Scholar Calafiore, 2017 G. Calafiore Repetitive scenario design IEEE Trans. Autom.
    Control, 62 (2017), pp. 1125-1137 View in ScopusGoogle Scholar Calfa et al., 2015
    B.A. Calfa, A. Agarwal, S.J. Bury, J.M. Wassick, I.E. Grossmann Data-driven simulation
    and optimization approaches to incorporate production variability in sales and
    operations planning Ind. Eng. Chem. Res., 54 (2015), pp. 7261-7272 CrossRefView
    in ScopusGoogle Scholar Calfa et al., 2015 B.A. Calfa, I.E. Grossmann, A. Agarwal,
    S.J. Bury, J.M. Wassick Data-driven individual and joint chance-constrained optimization
    via kernel smoothing Comput. Chem. Eng., 78 (2015), pp. 51-69 View PDFView articleView
    in ScopusGoogle Scholar Campbell and How, 2015 T. Campbell, J.P. How Bayesian
    nonparametric set construction for robust optimization American Control Conference
    (ACC), 2015 (2015), pp. 4216-4221 CrossRefView in ScopusGoogle Scholar Campi and
    Garatti, 2008 M.C. Campi, S. Garatti The exact feasibility of randomized solutions
    of uncertain convex programs SIAM J. Optim., 19 (2008), pp. 1211-1230 CrossRefView
    in ScopusGoogle Scholar Campi and Garatti, 2011 M.C. Campi, S. Garatti A sampling-and-discarding
    approach to chance-constrained optimization: feasibility and optimality J. Optim.
    Theory Appl., 148 (2011), pp. 257-280 CrossRefView in ScopusGoogle Scholar Campi
    and Garatti, 2018 M.C. Campi, S. Garatti Wait-and-judge scenario optimization
    Math. Program., 167 (2018), pp. 155-189 CrossRefView in ScopusGoogle Scholar Campi
    et al., 2009 M.C. Campi, S. Garatti, M. Prandini The scenario approach for systems
    and control design Ann. Rev. Control, 33 (2009), pp. 149-157 View PDFView articleView
    in ScopusGoogle Scholar Campi et al., 2018 M.C. Campi, S. Garatti, F.A. Ramponi
    A general scenario theory for nonconvex optimization and decision making IEEE
    Trans. Autom. Control, 63 (2018), pp. 4067-4078 CrossRefView in ScopusGoogle Scholar
    Cannon et al., 2009 M. Cannon, B. Kouvaritakis, X.J. Wu Probabilistic constrained
    MPC for multiplicative and additive stochastic uncertainty IEEE Trans. Autom.
    Control, 54 (2009), pp. 1626-1632 View in ScopusGoogle Scholar Care et al., 2014
    A. Care, S. Garatti, M.C. Campi FAST-Fast algorithm for the scenario technique
    Oper. Res., 62 (2014), pp. 662-671 CrossRefView in ScopusGoogle Scholar Carlone
    et al., 2014 L. Carlone, V. Srivastava, F. Bullo, G.C. Calafiore Distributed random
    convex programming via constraints consensus SIAM J. Control Optim., 52 (2014),
    pp. 629-662 CrossRefView in ScopusGoogle Scholar Caroe and Schultz, 1999 C.C.
    Caroe, R. Schultz Dual decomposition in stochastic integer programming Oper. Res.
    Lett., 24 (1999), pp. 37-45 View PDFView articleView in ScopusGoogle Scholar Chamanbaz
    et al., 2016 M. Chamanbaz, F. Dabbene, R. Tempo, V. Venkataramanan, Q.G. Wang
    Sequential randomized algorithms for convex optimization in the presence of uncertainty
    IEEE Trans. Autom. Control, 61 (2016), pp. 2565-2571 View in ScopusGoogle Scholar
    Charnes and Cooper, 1959 A. Charnes, W.W. Cooper Chance-constrained programming
    Manag. Sci., 6 (1959), pp. 73-79 CrossRefGoogle Scholar Chen et al., 2018 Y.W.
    Chen, Q.L. Guo, H.B. Sun, Z.S. Li, W.C. Wu, Z.H. Li A Distributionally robust
    optimization model for unit commitment based on Kullback-Leibler divergence IEEE
    Trans. Power Syst., 33 (2018), pp. 5147-5160 CrossRefView in ScopusGoogle Scholar
    Chen et al., 2018 Z. Chen, S. Peng, J. Liu Data-driven robust chance constrained
    problems: a mixture model approach J. Optim. Theory Appl., 179 (2018), pp. 1065-1085
    CrossRefView in ScopusGoogle Scholar Chen et al., 2018 Y.Z. Chen, Y.S. Wang, D.
    Kirschen, B.S. Zhang Model-free renewable scenario generation using generative
    adversarial networks IEEE Trans. Power Syst., 33 (2018), pp. 3265-3275 CrossRefView
    in ScopusGoogle Scholar Chen et al., 2018 Z. Chen, D. Kuhn, W. Wiesemann Data-Driven
    Chance Constrained Programs Over Wasserstein Balls (2018) arXiv:1809.00210 Google
    Scholar Cheng et al., 2014 J. Cheng, E. Delage, A. Lisser Distributionally robust
    stochastic knapsack problem SIAM J. Optim., 24 (2014), pp. 1485-1506 CrossRefView
    in ScopusGoogle Scholar Chiu and Christofides, 2000 T.Y. Chiu, P.D. Christofides
    Robust control of particulate processes using uncertain population balances AIChE
    J., 46 (2000), pp. 266-280 View in ScopusGoogle Scholar Chu and You, 2013 Y. Chu,
    F. You Integration of scheduling and dynamic optimization of batch processes under
    uncertainty: two-stage stochastic programming approach and enhanced generalized
    benders decomposition algorithm Ind. Eng. Chem. Res., 52 (2013), pp. 16851-16869
    CrossRefView in ScopusGoogle Scholar Chu et al., 2015 Y. Chu, F. You, J.M. Wassick,
    A. Agarwal Simulation-based optimization framework for multi-echelon inventory
    systems under uncertainty Comput. Chem. Eng., 73 (2015), pp. 1-16 View PDFView
    articleView in ScopusGoogle Scholar Chu et al., 2015 Y. Chu, F. You, J.M. Wassick,
    A. Agarwal Integrated planning and scheduling under production uncertainties:
    bi-level model formulation and hybrid solution method Comput. Chem. Eng., 72 (2015),
    pp. 255-272 View PDFView articleView in ScopusGoogle Scholar De Loera et al.,
    2018 J.A. De Loera, R.N. La Haye, D. Oliveros, E. Roldan-Pensado Chance-constrained
    convex mixed-integer optimization and beyond: two sampling algorithms within s-optimization
    J. Convex Anal., 25 (2018), pp. 201-218 View in ScopusGoogle Scholar Delage and
    Iancu, 2015 E. Delage, D.A. Iancu Robust Multistage Decision Making DM Aleman,
    AC Thiele (Eds.), INFORMS Tutorials in Operations Research, Catonsville, MD (2015),
    pp. 20-46 CrossRefGoogle Scholar Delage and Ye, 2010 E. Delage, Y.Y. Ye Distributionally
    Robust optimization under moment uncertainty with application to data-driven problems
    Oper. Res., 58 (2010), pp. 595-612 CrossRefView in ScopusGoogle Scholar Duan et
    al., 2018 C. Duan, L. Jiang, W.L. Fang, J. Liu Data-driven affinely adjustable
    distributionally robust unit commitment IEEE Trans. Power Syst., 33 (2018), pp.
    1385-1398 CrossRefView in ScopusGoogle Scholar El Ghaoui et al., 2003 L. El Ghaoui,
    M. Oks, F. Oustry Worst-case value-at-risk and robust portfolio optimization:
    a conic programming approach Oper. Res., 51 (2003), pp. 543-556 View in ScopusGoogle
    Scholar Erdogan and Iyengar, 2006 E. Erdogan, G. Iyengar Ambiguous chance constrained
    problems and robust optimization Math. Program., 107 (2006), pp. 37-61 CrossRefView
    in ScopusGoogle Scholar Esfahani and Kuhn, 2018 P.M. Esfahani, D. Kuhn Data-driven
    distributionally robust optimization using the Wasserstein metric: performance
    guarantees and tractable reformulations Math. Program., 171 (2018), pp. 115-166
    Google Scholar Esfahani et al., 2015 P.M. Esfahani, T. Sutter, J. Lygeros Performance
    bounds for the scenario approach and an extension to a class of non-convex programs
    IEEE Trans. Autom. Control, 60 (2015), pp. 46-58 Google Scholar Gao and Kleywegt,
    2016 R. Gao, A.J. Kleywegt Distributionally Robust Stochastic Optimization With
    Wasserstein Distance (2016) arXiv:1604.02199 Google Scholar Gao and You, 2015
    J. Gao, F. You Deciphering and handling uncertainty in shale gas supply chain
    design and optimization: novel modeling framework and computationally efficient
    solution algorithm AIChE J., 61 (2015), pp. 3739-3755 CrossRefView in ScopusGoogle
    Scholar Gao and You, 2017 J. Gao, F. You Modeling framework and computational
    algorithm for hedging against uncertainty in sustainable supply chain design using
    functional-unit-based life cycle optimization Comput. Chem. Eng., 107 (2017),
    pp. 221-236 View PDFView articleView in ScopusGoogle Scholar Gao et al., 2014
    X.Y. Gao, C. Shang, Y.H. Jiang, D.X. Huang, T. Chen Refinery scheduling with varying
    crude: a deep belief network classification and multimodel approach AIChE J.,
    60 (2014), pp. 2525-2532 CrossRefView in ScopusGoogle Scholar Gao et al., 2019
    J. Gao, C. Ning, F. You Data-driven distributionally robust optimization of shale
    gas supply chains under uncertainty AIChE J., 65 (2019), pp. 947-963 CrossRefView
    in ScopusGoogle Scholar Gawehn et al., 2016 E. Gawehn, J.A. Hiss, G. Schneider
    Deep learning in drug discovery Mol. Inf., 35 (2016), pp. 3-14 CrossRefView in
    ScopusGoogle Scholar Gebreslassie et al., 2012 B.H. Gebreslassie, Y. Yao, F. You
    Design under uncertainty of hydrocarbon biorefinery supply chains: multiobjective
    stochastic programming models, decomposition algorithm, and a comparison between
    CVaR and downside risk AIChE J., 58 (2012), pp. 2155-2179 CrossRefView in ScopusGoogle
    Scholar Ghosal and Wiesemann, 2018 S. Ghosal, W. Wiesemann The Distributionally
    Robust Chance Constrained Vehicle Routing Problem (2018) Available Optimization
    Online Goel and Grossmann, 2007 V. Goel, I.E. Grossmann A class of stochastic
    programs with decision dependent uncertainty Math. Program., 108 (2007), pp. 355-394
    Google Scholar Gong and You, 2017 J. Gong, F. You Optimal processing network design
    under uncertainty for producing fuels and value-added bioproducts from microalgae:
    two-stage adaptive robust mixed integer fractional programming model and computationally
    efficient solution algorithm AIChE J., 63 (2017), pp. 582-600 CrossRefView in
    ScopusGoogle Scholar Gong and You, 2018 J. Gong, F. You Resilient design and operations
    of process systems: nonlinear adaptive robust optimization model and algorithm
    for resilience analysis and enhancement Comput. Chem. Eng., 116 (2018), pp. 231-252
    View PDFView articleView in ScopusGoogle Scholar Gong et al., 2016 J. Gong, D.J.
    Garcia, F. You Unraveling optimal biomass processing routes from bioconversion
    product and process networks under uncertainty: an adaptive robust optimization
    approach ACS Sustain. Chem. Eng., 4 (2016), pp. 3160-3173 CrossRefView in ScopusGoogle
    Scholar Gong et al., 2017 J. Gong, M. Yang, F. You A systematic simulation-based
    process intensification method for shale gas processing and NGLs recovery process
    systems under uncertain feedstock compositions Comput. Chem. Eng., 105 (2017),
    pp. 259-275 View PDFView articleView in ScopusGoogle Scholar Goodfellow et al.,
    2014 I.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    et al. Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, K.Q. Weinberger (Eds.),
    Generative Adversarial Nets, 27, Advances in Neural Information Processing Systems
    27 (2014) Goodfellow et al., 2016 I. Goodfellow, Y. Bengio, A. Courville, Y. Bengio
    Deep Learning, 1, MIT Press Cambridge (2016) Grammatico et al., 2016 S. Grammatico,
    X.J. Zhang, K. Margellos, P. Goulart, J. Lygeros A scenario approach for non-convex
    control design IEEE Trans. Autom. Control, 61 (2016), pp. 334-345 View in ScopusGoogle
    Scholar Graves et al., 2013 A. Graves, A.R. Mohamed, G. Hinton, IEEE Speech recognition
    with deep recurrent neural networks 2013 IEEE International Conference on Acoustics,
    Speech and Signal Processing (2013), pp. 6645-6649 View in ScopusGoogle Scholar
    Gregory et al., 2011 C. Gregory, K. Darby-Dowman, G. Mitra Robust optimization
    and portfolio selection: the cost of robustness Eur. J. Oper. Res., 212 (2011),
    pp. 417-428 View PDFView articleView in ScopusGoogle Scholar Grossmann and Biegler,
    2004 I.E. Grossmann, L.T. Biegler Part II. Future perspective on optimization
    Comput. Chem. Eng., 28 (2004), pp. 1193-1218 View PDFView articleView in ScopusGoogle
    Scholar Grossmann, 2012 I.E. Grossmann Advances in mathematical programming models
    for enterprise-wide optimization Comput. Chem. Eng., 47 (2012), pp. 2-18 View
    PDFView articleView in ScopusGoogle Scholar Guo et al., 2018 Y. Guo, K. Baker,
    E. Dall''Anese, Z. Hu, T. Summers Stochastic optimal power flow based on data-driven
    distributionally robust optimization 2018 Annual American Control Conference (ACC)
    (2018), pp. 3840-3846 CrossRefView in ScopusGoogle Scholar Gupta and Grossmann,
    2014 V. Gupta, I.E. Grossmann A new decomposition algorithm for multistage stochastic
    programs with endogenous uncertainties Comput. Chem. Eng., 62 (2014), pp. 62-79
    View PDFView articleCrossRefView in ScopusGoogle Scholar Gupta and Maranas, 2003
    A. Gupta, C.D. Maranas Managing demand uncertainty in supply chain planning Comput.
    Chem. Eng., 27 (2003), pp. 1219-1227 View PDFView articleView in ScopusGoogle
    Scholar Gupta and Rusmevichientong, 2017 V. Gupta, P. Rusmevichientong Small-Data,
    Large-Scale Linear Optimization (2017) Google Scholar Gupta et al., 2000 A. Gupta,
    C.D. Maranas, C.M. McDonald Mid-term supply chain planning under demand uncertainty:
    customer demand satisfaction and inventory management Comput. Chem. Eng., 24 (2000),
    pp. 2613-2621 View PDFView articleView in ScopusGoogle Scholar Hanasusanto and
    Kuhn, 2018 G.A. Hanasusanto, D. Kuhn Conic Programming reformulations of two-stage
    distributionally robust linear programs over Wasserstein balls Oper. Res., 66
    (2018), pp. 849-869 CrossRefView in ScopusGoogle Scholar Hanasusanto et al., 2015
    G.A. Hanasusanto, V. Roitch, D. Kuhn, W. Wiesemann A distributionally robust perspective
    on uncertainty quantification and chance constrained programming Math. Program.,
    151 (2015), pp. 35-62 CrossRefView in ScopusGoogle Scholar Hanasusanto et al.,
    2017 G.A. Hanasusanto, V. Roitch, D. Kuhn, W. Wiesemann Ambiguous joint chance
    constraints under mean and dispersion information Oper. Res., 65 (2017), pp. 751-767
    CrossRefView in ScopusGoogle Scholar Hochreiter and Schmidhuber, 1997 S. Hochreiter,
    J. Schmidhuber Long short-term memory Neural Comput., 9 (1997), pp. 1735-1780
    CrossRefView in ScopusGoogle Scholar Hong et al., 2011 L.J. Hong, Y. Yang, L.W.
    Zhang Sequential convex approximations to joint chance constrained programs: a
    Monte Carlo approach Oper. Res., 59 (2011), pp. 617-630 CrossRefView in ScopusGoogle
    Scholar Hota et al., 2018 A.R. Hota, A. Cherukuri, J. Lygeros Data-Driven Chance
    Constrained Optimization Under Wasserstein Ambiguity Sets (2018) arXiv:1805.06729
    Google Scholar Hu and Hong, 2013 Z. Hu, L.J. Hong Kullback-Leibler Divergence
    Constrained Distributionally Robust Optimization (2013) Available at Optimization
    Online Google Scholar Ierapetritou and Pistikopoulos, 1995 M.G. Ierapetritou,
    E.N. Pistikopoulos Design of multiproduct batch plants with uncertain demands
    Comput. Chem. Eng., 19 (1995), pp. S627-S632 Google Scholar Ji and Lejeune, 2018
    Ji, R., Lejeune, M., 2018. Data-driven distributionally robust chance-constrained
    programming with Wasserstein metric. Google Scholar Jiang and Guan, 2015 R. Jiang,
    Y. Guan Data-driven chance constrained stochastic program Math. Program., 158
    (2015), pp. 291-327 Google Scholar Jiang and Guan, 2016 R.W. Jiang, Y.P. Guan
    Data-driven chance constrained stochastic program Math. Program., 158 (2016),
    pp. 291-327 CrossRefView in ScopusGoogle Scholar John Walker, 2014 S. John Walker
    Big Data: A Revolution That will Transform how we live, work, and think Taylor
    & Francis (2014) Google Scholar Jordan and Mitchell, 2015 M.I. Jordan, T.M. Mitchell
    Machine learning: trends, perspectives, and prospects Science, 349 (2015), pp.
    255-260 CrossRefView in ScopusGoogle Scholar Küçükyavuz and Sen, 2017 S. Küçükyavuz,
    S. Sen An Introduction to Two-Stage Stochastic Mixed-Integer Programming Leading
    Developments From INFORMS Communities (2017), pp. 1-27 INFORMS CrossRefGoogle
    Scholar Kall and Wallace, 1994 P. Kall, S.W. Wallace Stochastic Programming John
    Wiley and Sons Ltd (1994) Google Scholar Kanamori and Takeda, 2012 T. Kanamori,
    A. Takeda Worst-case violation of sampled convex programs for optimization with
    uncertainty J. Optim. Theory Appl., 152 (2012), pp. 171-197 CrossRefView in ScopusGoogle
    Scholar Kariotoglou et al., 2016 N. Kariotoglou, K. Margellos, J. Lygeros On the
    computational complexity and generalization properties of multi-stage and stage-wise
    coupled scenario programs Syst. Control Lett., 94 (2016), pp. 63-69 View PDFView
    articleView in ScopusGoogle Scholar Keyvanshokooh et al., 2016 E. Keyvanshokooh,
    S.M. Ryan, E. Kabir Hybrid robust and stochastic optimization for closed-loop
    supply chain network design using accelerated Benders decomposition Eur. J. Oper.
    Res., 249 (2016), pp. 76-92 View PDFView articleView in ScopusGoogle Scholar Klabjan
    et al., 2013 D. Klabjan, D. Simchi-Levi, M. Song Robust stochastic lot-sizing
    by means of histograms Prod. Oper. Manag., 22 (2013), pp. 691-710 CrossRefView
    in ScopusGoogle Scholar Krieger and Pistikopoulos, 2014 A. Krieger, E.N. Pistikopoulos
    Model predictive control of anesthesia under uncertainty Comput. Chem. Eng., 71
    (2014), pp. 699-707 View PDFView articleView in ScopusGoogle Scholar Krizhevsky
    et al., 2017 A. Krizhevsky, I. Sutskever, G.E. Hinton ImageNet Classification
    with deep convolutional neural networks Commun. ACM, 60 (2017), pp. 84-90 CrossRefView
    in ScopusGoogle Scholar Laporte and Louveaux, 1993 G. Laporte, F.V. Louveaux The
    integer l-shaped method for stochastic integer programs with complete recourse
    Oper. Res. Lett., 13 (1993), pp. 133-142 View PDFView articleView in ScopusGoogle
    Scholar Lappas and Gounaris, 2016 N.H. Lappas, C.E. Gounaris Multi-stage adjustable
    robust optimization for process scheduling under uncertainty AIChE J., 62 (2016),
    pp. 1646-1667 CrossRefView in ScopusGoogle Scholar Lasserre and Weisser, 2018
    J. Lasserre, T. Weisser Distributionally Robust Polynomial Chance-Constraints
    Under Mixture Ambiguity Sets (2018) Google Scholar LeCun et al., 2015 Y. LeCun,
    Y. Bengio, G. Hinton Deep learning Nature, 521 (2015), p. 436 CrossRefView in
    ScopusGoogle Scholar Ledig et al., 2017 C. Ledig, L. Theis, F. Huszár, J. Caballero,
    A. Cunningham, A. Acosta, et al. Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network CVPR (2017), p. 4 Google Scholar Lee et
    al., 2018 J.H. Lee, J. Shin, M.J. Realff Machine learning: overview of the recent
    progresses and implications for the process systems engineering field Comput.
    Chem. Eng., 114 (2018), pp. 111-121 View PDFView articleGoogle Scholar Levi et
    al., 2015 R. Levi, G. Perakis, J. Uichanco The data-driven newsvendor problem:
    new bounds and insights Oper. Res., 63 (2015), pp. 1294-1306 CrossRefView in ScopusGoogle
    Scholar Li and Grossmann, 2018 C. Li, I.E. Grossmann An improved L-shaped method
    for two-stage convex 0–1 mixed integer nonlinear stochastic programs Comput. Chem.
    Eng., 112 (2018), pp. 165-179 View PDFView articleView in ScopusGoogle Scholar
    Li and Ierapetritou, 2008 Z. Li, M.G. Ierapetritou Process scheduling under uncertainty:
    review and challenges Comput. Chem. Eng., 32 (2008), pp. 715-727 View PDFView
    articleView in ScopusGoogle Scholar Li et al., 2008 P. Li, H. Arellano-Garcia,
    G. Wozny Chance constrained programming approach to process optimization under
    uncertainty Comput. Chem. Eng., 32 (2008), pp. 25-45 View PDFView articleView
    in ScopusGoogle Scholar Li et al., 2008 P. Li, H. Arellano-Garcia, G. Wozny Chance
    constrained programming approach to process optimization under uncertainty Comput.
    Chem. Eng., 32 (2008), pp. 25-45 View PDFView articleView in ScopusGoogle Scholar
    Li et al., 2011 X. Li, A. Tomasgard, P.I. Barton Nonconvex generalized benders
    decomposition for stochastic separable mixed-integer nonlinear programs J. Optim.
    Theory Appl., 151 (2011), pp. 425-454 CrossRefView in ScopusGoogle Scholar Li
    et al., 2017 B. Li, R. Jiang, J.L. Mathieu Ambiguous risk constraints with moment
    and unimodality information Math. Program., 173 (2017), pp. 151-192 View in ScopusGoogle
    Scholar Liu and Sahinidis, 1996 M.L. Liu, N.V. Sahinidis Optimization in process
    planning under uncertainty Ind. Eng. Chem. Res., 35 (1996), pp. 4154-4165 View
    in ScopusGoogle Scholar Liu et al., 2010 P. Liu, E.N. Pistikopoulos, Z. Li Decomposition
    based stochastic programming approach for polygeneration energy systems design
    under uncertainty Ind. Eng. Chem. Res., 49 (2010), pp. 3295-3305 CrossRefView
    in ScopusGoogle Scholar Liu et al., 2016 X. Liu, S. Kucukyavuz, J. Luedtke Decomposition
    algorithms for two-stage chance-constrained programs Math. Program., 157 (2016),
    pp. 219-243 CrossRefView in ScopusGoogle Scholar Liu et al., 2016 S.S. Liu, S.S.
    Farid, L.G. Papageorgiou Integrated optimization of upstream and downstream processing
    in biopharmaceutical manufacturing under uncertainty: a chance constrained programming
    approach Ind. Eng. Chem. Res., 55 (2016), pp. 4599-4612 CrossRefView in ScopusGoogle
    Scholar Liu et al., 2016 C. Liu, C. Lee, H. Chen, S. Mehrotra Stochastic robust
    mathematical programming model for power system optimization IEEE Trans. Power
    Syst., 31 (2016), pp. 821-822 View in ScopusGoogle Scholar Lorca et al., 2016
    Á. Lorca, X.A. Sun, E. Litvinov, T. Zheng Multistage adaptive robust optimization
    for the unit commitment problem Oper. Res., 61 (2016), pp. 32-51 CrossRefView
    in ScopusGoogle Scholar Luedtke and Ahmed, 2008 J. Luedtke, S. Ahmed A sample
    approximation approach for optimization with probabilistic constraints SIAM J.
    Optim., 19 (2008), pp. 674-699 CrossRefView in ScopusGoogle Scholar Maranas, 1997
    C.D. Maranas Optimization accounting for property prediction uncertainty in polymer
    design Comput. Chem. Eng., 21 (1997), pp. S1019-S1024 View PDFView articleView
    in ScopusGoogle Scholar Margellos et al., 2018 K. Margellos, A. Falsone, S. Garatti,
    M. Prandini Distributed constrained optimization and consensus in uncertain networks
    via proximal minimization IEEE Trans. Autom. Control, 63 (2018), pp. 1372-1387
    CrossRefView in ScopusGoogle Scholar McLean and Li, 2013 K. McLean, X. Li Robust
    scenario formulations for strategic supply chain optimization under uncertainty
    Ind. Eng. Chem. Res., 52 (2013), pp. 5721-5734 CrossRefView in ScopusGoogle Scholar
    Mesbah, 2016 A. Mesbah Stochastic model predictive control an overview and perspectives
    for future research IEEE Control Syst. Mag., 36 (2016), pp. 30-44 CrossRefView
    in ScopusGoogle Scholar Miao et al., 2019 F. Miao, S. Han, S. Lin, Q. Wang, J.A.
    Stankovic, A. Hendawi, et al. Data-driven robust taxi dispatch under demand uncertainties
    IEEE Trans. Control Syst. Technol., 27 (2019), pp. 175-191 CrossRefView in ScopusGoogle
    Scholar Miller and Wagner, 1965 B.L. Miller, H.M. Wagner Chance constrained programming
    with joint constraints Oper. Res., 13 (1965), p. 930 CrossRefGoogle Scholar Mitra
    et al., 2008 K. Mitra, R.D. Gudi, S.C. Patwardhan, G. Sardar Midterm supply chain
    planning under uncertainty: a multiobjective chance constrained programming framework
    Ind. Eng. Chem. Res., 47 (2008), pp. 5501-5511 CrossRefView in ScopusGoogle Scholar
    Mohamed et al., 2012 A.R. Mohamed, G.E. Dahl, G. Hinton Acoustic modeling using
    deep belief networks IEEE Trans. Audio Speech Lang. Process., 20 (2012), pp. 14-22
    View in ScopusGoogle Scholar Nemirovski and Shapiro, 2006 A. Nemirovski, A. Shapiro
    Convex approximations of chance constrained programs SIAM J. Optim., 17 (2006),
    pp. 969-996 Google Scholar Ning and You, 2017a C. Ning, F. You Data-driven adaptive
    nested robust optimization: general modeling framework and efficient computational
    algorithm for decision making under uncertainty AIChE J., 63 (2017), pp. 3790-3817
    CrossRefView in ScopusGoogle Scholar Ning and You, 2017b C. Ning, F. You A data-driven
    multistage adaptive robust optimization framework for planning and scheduling
    under uncertainty AIChE J., 63 (2017), pp. 4343-4369 CrossRefView in ScopusGoogle
    Scholar Ning and You, 2018a C. Ning, F. You Adaptive robust optimization with
    minimax regret criterion: multiobjective optimization framework and computational
    algorithm for planning and scheduling under uncertainty Comput. Chem. Eng., 108
    (2018), pp. 425-447 View PDFView articleView in ScopusGoogle Scholar Ning and
    You, 2018b C. Ning, F. You Data-driven stochastic robust optimization: general
    computational framework and algorithm leveraging machine learning for optimization
    under uncertainty in the big data era Comput. Chem. Eng., 111 (2018), pp. 115-133
    View PDFView articleView in ScopusGoogle Scholar Ning and You, 2018 C. Ning, F.
    You Data-driven decision making under uncertainty integrating robust optimization
    with principal component analysis and kernel smoothing methods Comput. Chem. Eng.,
    112 (2018), pp. 190-210 View PDFView articleView in ScopusGoogle Scholar Ning
    and You, 2019 C. Ning, F. You Data-driven adaptive robust unit commitment under
    wind power uncertainty: a Bayesian nonparametric approach IEEE Trans. Power Syst.
    (2019), 10.1109/TPWRS.2019.2891057 Google Scholar Oliveira et al., 2013 F. Oliveira,
    V. Gupta, S. Hamacher, I.E. Grossmann A Lagrangean decomposition approach for
    oil supply chain investment planning under uncertainty with risk considerations
    Comput. Chem. Eng., 50 (2013), pp. 184-195 View PDFView articleView in ScopusGoogle
    Scholar Parpas et al., 2009 P. Parpas, B. Rustem, E. Pistikopoulos Global optimization
    of robust chance constrained problems J. Global Optim., 43 (2009), pp. 231-247
    CrossRefView in ScopusGoogle Scholar Peng et al., 2018 X. Peng, T.W. Root, C.T.
    Maravelias Optimization-based process synthesis under seasonal and daily variability:
    application to concentrating solar power AIChE J. (2018), 10.1002/aic.16458 Google
    Scholar Pistikopoulos, 1995 E.N. Pistikopoulos Uncertainty in process design and
    operations Comput. Chem. Eng., 19 (1995), pp. 553-563 View PDFView articleView
    in ScopusGoogle Scholar Postek et al., 2018 K. Postek, A. Ben-Tal, D. den Hertog,
    B. Melenberg Robust optimization with ambiguous stochastic constraints under mean
    and dispersion information Oper. Res., 66 (2018), pp. 814-833 CrossRefView in
    ScopusGoogle Scholar Prékopa, 1995 A. Prékopa Stochastic Programming, Volume 324
    of Mathematics and Its Applications Kluwer Academic Publishers Group, Dordrecht
    (1995) Google Scholar Qin, 2014 S.J. Qin Process data analytics in the era of
    big data AIChE J., 60 (2014), pp. 3092-3100 CrossRefView in ScopusGoogle Scholar
    Quddus et al., 2018 M.A. Quddus, S. Chowdhury, M. Marufuzzaman, F. Yu, L.K. Bian
    A two-stage chance-constrained stochastic programming model for a bio-fuel supply
    chain network Int. J. Prod. Econ., 195 (2018), pp. 27-44 View PDFView articleView
    in ScopusGoogle Scholar Rooney and Biegler, 2003 W.C. Rooney, L.T. Biegler Optimal
    process design with model parameter uncertainty and process variability AIChE
    J., 49 (2003), pp. 438-449 View in ScopusGoogle Scholar Sahinidis, 2004 N.V. Sahinidis
    Optimization under uncertainty: state-of-the-art and opportunities Comput. Chem.
    Eng., 28 (2004), pp. 971-983 View PDFView articleView in ScopusGoogle Scholar
    Sakizlis et al., 2004 V. Sakizlis, J.D. Perkins, E.N. Pistikopoulos Recent advances
    in optimization-based simultaneous process and control design Comput. Chem. Eng.,
    28 (2004), pp. 2069-2086 View PDFView articleView in ScopusGoogle Scholar Sanchez-Lengeling
    and Aspuru-Guzik, 2018 B. Sanchez-Lengeling, A. Aspuru-Guzik Inverse molecular
    design using machine learning: generative models for matter engineering Science,
    361 (2018), pp. 360-365 CrossRefView in ScopusGoogle Scholar Shalev-Shwartz, 2012
    S. Shalev-Shwartz Online learning and online convex optimization Found. Trends
    Mach. Learn., 4 (2012), pp. 107-194 Google Scholar Shang and You, 2018 C. Shang,
    F. You Distributionally robust optimization for planning and scheduling under
    uncertainty Comput. Chem. Eng., 110 (2018), pp. 53-68 View PDFView articleView
    in ScopusGoogle Scholar Shang and You, 2019 C. Shang, F. You A data-driven robust
    optimization approach to stochastic model predictive control J. Process Control,
    75 (2019), pp. 24-39 View PDFView articleView in ScopusGoogle Scholar Shang et
    al., 2014 C. Shang, F. Yang, D.X. Huang, W.X. Lyu Data-driven soft sensor development
    based on deep learning technique J. Process Control, 24 (2014), pp. 223-233 View
    PDFView articleView in ScopusGoogle Scholar Shang et al., 2017 C. Shang, X. Huang,
    F. You Data-driven robust optimization based on kernel learning Comput. Chem.
    Eng., 106 (2017), pp. 464-479 View PDFView articleView in ScopusGoogle Scholar
    Shen et al., 2018 W. Shen, Z. Li, B. Huang, N.M. Jan Chance-constrained model
    predictive control for SAGD process using robust optimization approximation Ind.
    Eng. Chem. Res. (2018), 10.1021/acs.iecr.8b03207 Google Scholar Shi and You, 2016
    H. Shi, F. You A computational framework and solution algorithms for two-stage
    adaptive robust scheduling of batch manufacturing processes under uncertainty
    AIChE J., 62 (2016), pp. 687-703 CrossRefView in ScopusGoogle Scholar Shin and
    Lee, 2019 J. Shin, J.H. Lee Multi-timescale, multi-period decision-making model
    development by combining reinforcement learning and mathematical programming Comput.
    Chem. Eng., 121 (2019), pp. 556-573 View PDFView articleView in ScopusGoogle Scholar
    Smith and Winkler, 2006 J.E. Smith, R.L. Winkler The optimizer''s curse: skepticism
    and postdecision surprise in decision analysis Manag. Sci., 52 (2006), pp. 311-322
    CrossRefView in ScopusGoogle Scholar Soyster, 1973 A.L. Soyster Technical note—convex
    programming with set-inclusive constraints and applications to inexact linear
    programming Oper. Res., 21 (1973), pp. 1154-1157 CrossRefGoogle Scholar Steimel
    and Engell, 2015 J. Steimel, S. Engell Conceptual design and optimization of chemical
    processes under uncertainty by two-stage programming Comput. Chem. Eng., 81 (2015),
    pp. 200-217 View PDFView articleView in ScopusGoogle Scholar Tong et al., 2014
    K. Tong, J. Gong, D. Yue, F. You Stochastic programming approach to optimal design
    and operations of integrated hydrocarbon biofuel and petroleum supply chains ACS
    Sustain. Chem. Eng., 2 (2014), pp. 49-61 CrossRefView in ScopusGoogle Scholar
    Tong et al., 2014 K. Tong, F. You, G. Rong Robust design and operations of hydrocarbon
    biofuel supply chain integrating with existing petroleum refineries considering
    unit cost objective Comput. Chem. Eng., 68 (2014), pp. 128-139 View PDFView articleView
    in ScopusGoogle Scholar Uryasev, 2000 S. Uryasev Conditional value-at-risk: Optimization
    algorithms and applications Proceedings of the IEEE/IAFE/INFORMS 2000 Conference
    on Computational Intelligence for Financial Engineering (CIFEr)(Cat. No. 00TH8520),
    IEEE (2000), pp. 49-57 View in ScopusGoogle Scholar Van Parys et al., 2016 B.P.G.
    Van Parys, D. Kuhn, P.J. Goulart, M. Morari Distributionally robust control of
    constrained stochastic systems IEEE Trans. Autom. Control, 61 (2016), pp. 430-442
    View in ScopusGoogle Scholar Vanslyke and Wets, 1969 R.M. Vanslyke, R. Wets L-shaped
    linear programs with applications to optimal control and stochastic programming
    SIAM J. Appl. Math., 17 (1969), p. 638 CrossRefGoogle Scholar Vayanos et al.,
    2012 P. Vayanos, D. Kuhn, B. Rustem A constraint sampling approach for multi-stage
    robust optimization Automatica, 48 (2012), pp. 459-471 View PDFView articleView
    in ScopusGoogle Scholar Venkatasubramanian, 2019 V. Venkatasubramanian The promise
    of artificial intelligence in chemical engineering: is it here, finally? AIChE
    J., 65 (2019), pp. 466-478 CrossRefView in ScopusGoogle Scholar Verderame et al.,
    2010 P.M. Verderame, J.A. Elia, J. Li, C.A. Floudas Planning and scheduling under
    uncertainty: a review across multiple sectors Ind. Eng. Chem. Res., 49 (2010),
    pp. 3993-4017 CrossRefView in ScopusGoogle Scholar Vermaak and Botha, 1998 J.
    Vermaak, E.C. Botha Recurrent neural networks for short-term load forecasting
    IEEE Trans. Power Syst., 13 (1998), pp. 126-132 Google Scholar Wang et al., 2018
    C. Wang, R. Gao, F. Qiu, J. Wang, L. Xin Risk-based distributionally robust optimal
    power flow with dynamic line rating IEEE Trans. Power Syst., 33 (2018), pp. 6074-6086
    CrossRefView in ScopusGoogle Scholar Wiesemann et al., 2014 W. Wiesemann, D. Kuhn,
    M. Sim Distributionally robust convex optimization Oper. Res., 62 (2014), pp.
    1358-1376 CrossRefView in ScopusGoogle Scholar Wu et al., 2018 Y. Wu, H. Tan,
    L. Qin, B. Ran, Z. Jiang A hybrid deep learning based traffic flow prediction
    method and its understanding Transp. Res. Part C, 90 (2018), pp. 166-180 View
    PDFView articleGoogle Scholar Xie and Ahmed, 2018 W.J. Xie, S. Ahmed On deterministic
    reformulations of distributionally robust joint chance constrained optimization
    problems SIAM J. Optim., 28 (2018), pp. 1151-1182 CrossRefView in ScopusGoogle
    Scholar Xie and Ahmed, 2018 W. Xie, S. Ahmed Distributionally robust chance constrained
    optimal power flow with renewables: a conic reformulation IEEE Trans. Power Syst.,
    33 (2018), pp. 1860-1867 CrossRefView in ScopusGoogle Scholar Xie, 2018 W. Xie
    On Distributionally Robust Chance Constrained Program with Wasserstein Distance
    (2018) arXiv:1806.07418 Google Scholar Xiong et al., 2017 P. Xiong, P. Jirutitijaroen,
    C. Singh A distributionally robust optimization model for unit commitment considering
    uncertain wind power generation IEEE Trans. Power Syst., 32 (2017), pp. 39-49
    View in ScopusGoogle Scholar Yang and Xu, 2016 W.Z. Yang, H. Xu Distributionally
    robust chance constraints for non-linear uncertainties Math. Program., 155 (2016),
    pp. 231-265 CrossRefGoogle Scholar Yang et al., 2017 Y. Yang, P. Vayanos, P.I.
    Barton Chance-constrained optimization for refinery blend planning under uncertainty
    Ind. Eng. Chem. Res., 56 (2017), pp. 12139-12150 CrossRefView in ScopusGoogle
    Scholar Ye and You, 2016 W. Ye, F. You A computationally efficient simulation-based
    optimization method with region-wise surrogate modeling for stochastic inventory
    management of supply chains with general network structures Comput. Chem. Eng.,
    87 (2016), pp. 164-179 View PDFView articleView in ScopusGoogle Scholar Yin and
    Kaynak, 2015 S. Yin, O. Kaynak Big data for modern industry: challenges and trends
    Proc. IEEE, 103 (2015), pp. 143-146 View in ScopusGoogle Scholar Yin et al., 2015
    S. Yin, X. Li, H. Gao, O. Kaynak Data-based techniques focused on modern industry:
    an overview IEEE Trans. Ind. Electron., 62 (2015), pp. 657-667 View in ScopusGoogle
    Scholar You and Grossmann, 2008 F. You, I.E. Grossmann Mixed-integer nonlinear
    programming models and algorithms for large-scale supply chain design with stochastic
    inventory management Ind. Eng. Chem. Res., 47 (2008), pp. 7802-7817 CrossRefView
    in ScopusGoogle Scholar You and Grossmann, 2011 F. You, I.E. Grossmann Stochastic
    inventory management for tactical process planning under uncertainties: MINLP
    models and Algorithms AIChE J., 57 (2011), pp. 1250-1277 CrossRefView in ScopusGoogle
    Scholar You and Grossmann, 2011 F. You, I.E. Grossmann Balancing responsiveness
    and economics in process supply chain design with multi-echelon stochastic inventory
    AIChE J., 57 (2011), pp. 178-192 CrossRefView in ScopusGoogle Scholar You and
    Grossmann, 2013 F. You, I.E. Grossmann Multicut Benders decomposition algorithm
    for process supply chain planning under uncertainty Ann. Oper. Res., 210 (2013),
    pp. 191-211 CrossRefView in ScopusGoogle Scholar You et al., 2009 F. You, J.M.
    Wassick, I.E. Grossmann Risk management for a global supply chain planning under
    uncertainty: models and algorithms AIChE J., 55 (2009), pp. 931-946 CrossRefView
    in ScopusGoogle Scholar You et al., 2011 F. You, J.M. Pinto, I.E. Grossmann, L.
    Megan Optimal distribution-inventory planning of industrial gases. II. MINLP models
    and algorithms for stochastic cases Ind. Eng. Chem. Res., 50 (2011), pp. 2928-2945
    CrossRefView in ScopusGoogle Scholar You et al., 2018 K. You, R. Tempo, P. Xie
    Distributed algorithms for robust convex optimization via the scenario approach
    IEEE Trans. Autom. Control, 64 (2018) 1-1 Google Scholar Yue and You, 2013 D.
    Yue, F. You Planning and scheduling of flexible process networks under uncertainty
    with stochastic inventory: MINLP models and algorithm AIChE J., 59 (2013), pp.
    1511-1532 CrossRefView in ScopusGoogle Scholar Yue and You, 2016 D. Yue, F. You
    Optimal supply chain design and operations under multi-scale uncertainties: nested
    stochastic robust optimization modeling framework and solution algorithm AIChE
    J., 62 (2016), pp. 3041-3055 CrossRefView in ScopusGoogle Scholar Zeballos et
    al., 2016 L.J. Zeballos, C.A. Méndez, A.P. Barbosa-Povoa Design and planning of
    closed-loop supply chains: a risk-averse multistage stochastic approach Ind. Eng.
    Chem. Res., 55 (2016), pp. 6236-6249 CrossRefView in ScopusGoogle Scholar Zhang
    and Zhao, 2017 Z.P. Zhang, J.S. Zhao A deep belief network based fault diagnosis
    model for complex chemical processes Comput. Chem. Eng., 107 (2017), pp. 395-407
    View PDFView articleView in ScopusGoogle Scholar Zhang et al., 2015 X.J. Zhang,
    S. Grammatico, G. Schildbach, P. Goulart, J. Lygeros On the sample size of random
    convex programs with structured dependence on the uncertainty Automatica, 60 (2015),
    pp. 182-188 View PDFView articleGoogle Scholar Zhang et al., 2018 Y. Zhang, R.
    Jiang, S. Shen Ambiguous chance-constrained binary programs under mean-covariance
    information SIAM J. Optim., 28 (2018), pp. 2922-2944 CrossRefView in ScopusGoogle
    Scholar Zhang et al., 2018 Y. Zhang, X.Z. Jin, Y.P. Feng, G. Rong Data-driven
    robust optimization under correlated uncertainty: a case study of production scheduling
    in ethylene plant (Reprinted from computers and Chemical Engineering, vol 109,
    pg 48-67, 2017) Comput. Chem. Eng., 116 (2018), pp. 17-36 View PDFView articleCrossRefGoogle
    Scholar Zhang et al., 2018 Y. Zhang, Y.P. Feng, G. Rong Data-driven rolling-horizon
    robust optimization for petrochemical scheduling using probability density contours
    Comput. Chem. Eng., 115 (2018), pp. 342-360 View PDFView articleView in ScopusGoogle
    Scholar Zhao and Guan, 2013 C.Y. Zhao, Y.P. Guan Unified stochastic and robust
    unit commitment IEEE Trans. Power Syst., 28 (2013), pp. 3353-3361 View in ScopusGoogle
    Scholar Zhao and Guan, 2016 C.Y. Zhao, Y.P. Guan Data-driven stochastic unit commitment
    for integrating wind generation IEEE Trans. Power Syst., 31 (2016), pp. 2587-2596
    View in ScopusGoogle Scholar Zhao and You, 2019 S. Zhao, F. You Resilient supply
    chain design and operations with decision-dependent uncertainty using a data-driven
    robust optimization approach AIChE J., 65 (2019), pp. 1006-1021 Google Scholar
    Zhao et al., 2019 L. Zhao, C. Ning, F. You Operational optimization of industrial
    steam systems under uncertainty using data-driven adaptive robust optimization
    AIChE J. (2019), 10.1002/aic.16500 Google Scholar Zhu et al., 2019 W. Zhu, Y.
    Ma, M.G. Benton, J.A. Romagnoli, Y. Zhan Deep learning for pyrolysis reactor monitoring:
    from thermal imaging toward smart monitoring system AIChE J., 65 (2019), pp. 582-591
    CrossRefView in ScopusGoogle Scholar Zipkin, 2000 P.H. Zipkin Foundations of inventory
    management McGraw-Hill (2000) Google Scholar Zymler et al., 2013 S. Zymler, D.
    Kuhn, B. Rustem Distributionally robust joint chance constraints with second-order
    moment information Math. Program., 137 (2013), pp. 167-198 CrossRefView in ScopusGoogle
    Scholar Cited by (251) A data-driven approach for optimal operational and financial
    commodity hedging 2024, European Journal of Operational Research Show abstract
    Data-driven optimization for seismic-resilient power network planning 2024, Computers
    and Operations Research Show abstract Data-driven robust optimization for a multi-trip
    truck-drone routing problem 2024, Expert Systems with Applications Show abstract
    Enhancing robustness: Multi-stage adaptive robust scheduling of oxygen systems
    in steel enterprises under demand uncertainty 2024, Applied Energy Show abstract
    Uncertainties in model predictive control for decentralized autonomous demand
    side management of electric vehicles 2024, Journal of Energy Storage Show abstract
    Data-driven robust optimization based on position-regulated support vector clustering
    2024, Journal of Computational Science Show abstract View all citing articles
    on Scopus View Abstract © 2019 Elsevier Ltd. All rights reserved. Part of special
    issue 28th European Symposium of Computer Aided Process Engineering Edited by
    Stefan Radl, Jiří Jaromír Klemeš, Petar Varbanov, Thomas Wallek View special issue
    Recommended articles Incorporating agricultural waste-to-energy pathways into
    biomass product and process network through data-driven nonlinear adaptive robust
    optimization Energy, Volume 180, 2019, pp. 556-571 Jack Nicoletti, …, Fengqi You
    View PDF Recent advances in mathematical programming techniques for the optimization
    of process systems under uncertainty Computers & Chemical Engineering, Volume
    91, 2016, pp. 3-14 Ignacio E. Grossmann, …, Qi Zhang View PDF Optimizing Return
    on Investment in Biomass Conversion Networks under Uncertainty Using Data-Driven
    Adaptive Robust Optimization Computer Aided Chemical Engineering, Volume 46, 2019,
    pp. 67-72 Jack Nicoletti, …, Fengqi You View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 215 Captures Readers: 476 Mentions References:
    3 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: Computers & Chemical Engineering
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Optimization under uncertainty in the era of big data and deep learning:
    When machine learning meets mathematical programming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/mvt.2019.2953857
  analysis: '>'
  authors:
  - Yuanwei Liu
  - Suzhi Bi
  - Zhiyuan Shi
  - Lajos Hanzo
  citation_count: 66
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Vehicular Technology Mag...
    >Volume: 15 Issue: 1 When Machine Learning Meets Big Data: A Wireless Communication
    Perspective Publisher: IEEE Cite This PDF Yuanwei Liu; Suzhi Bi; Zhiyuan Shi;
    Lajos Hanzo All Authors 58 Cites in Papers 2323 Full Text Views Abstract Document
    Sections Current Challenges Big Data in Wireless Networks Machine Learning in
    Wireless Networks A Unified, Big Data-Aided AI Framework Case Study: Social Network-Aware
    Wireless Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: We have witnessed an exponential growth in commercial data services,
    which has led to the so-called big data era. Machine learning, one of the most
    promising artificial intelligence (AI) tools for analyzing this deluge of data,
    has been called upon in many industry and academic research areas. In this article,
    we briefly review big data analysis and machine learning, along with their potential
    applications in next-generation (NG) wireless networks. Next, we invoke big data
    analysis to predict the requirements of mobile users and exploit such analysis
    to improve the performance of "social network-aware wireless." In particular,
    a unified, big data-aided machinelearning framework is proposed that consists
    of feature extraction, data modeling, and prediction/online refinement. The main
    benefits of this proposed framework are that, by relying on big data that reflects
    both the spectral and other challenging requirements of users, we can refine the
    motivation, problem formulations, and methodology of powerful machine-learning
    algorithms in the context of wireless networks. Published in: IEEE Vehicular Technology
    Magazine ( Volume: 15, Issue: 1, March 2020) Page(s): 63 - 72 Date of Publication:
    24 December 2019 ISSN Information: DOI: 10.1109/MVT.2019.2953857 Publisher: IEEE
    Funding Agency: We have witnessed an exponential growth in commercial data services,
    which has led to the so-called big data era. Machine learning, one of the most
    promising artificial intelligence (AI) tools for analyzing this deluge of data,
    has been called upon in many industry and academic research areas. In this article,
    we briefly review big data analysis and machine learning, along with their potential
    applications in next-generation (NG) wireless networks. Next, we invoke big data
    analysis to predict the requirements of mobile users and exploit such analysis
    to improve the performance of “social network-aware wireless.” In particular,
    a unified, big data-aided machine-learning framework is proposed that consists
    of feature extraction, data modeling, and prediction/online refinement. The main
    benefits of this proposed framework are that, by relying on big data that reflects
    both the spectral and other challenging requirements of users, we can refine the
    motivation, problem formulations, and methodology of powerful machine-learning
    algorithms in the context of wireless networks. To characterize the efficiency
    of the proposed framework, a pair of intelligent, practical applications are provided
    as case studies to predict 1) the positioning of drone-mounted areal base stations
    (BSs) according to the specific teletraffic requirements by gleaning valuable
    data from social networks and 2) the content-caching requirements of BSs according
    to users’ preferences by mining data from social networks. Finally, open research
    opportunities are identified for motivating future investigations. Current Challenges
    The current family of NG techniques seeks to provide high-quality communication
    services that rely on high throughput, massive connectivity, and low delay. For
    cellular systems, a 1–10-Gb/s downlink transmission rate and a delay under 1 ms
    are expected. Meanwhile, 5G standards also allow for conventional high-speed cellular
    communication to coexist with machine-to-machine and Internet of Things (IoT)
    services, with an emphasize on wide-coverage, advanced, dense connectivity (associated
    with up to one million sensor connections within a square kilometer area) [1].
    For instance, the IoT networks conceived for industrial control and health monitoring
    generate vast amounts of sensing data. In addition, autonomous connected vehicles
    of the near future will support millions of high-velocity devices, which will
    significantly increase the data traffic of the emerging NG network. The term artificial
    intelligence was first coined by John McCarthy in 1956 [2]. As a branch of computer
    science, AI’s promise is to enhance the intelligence of computers by imitating
    the actions of human beings, i.e., understanding natural language and planning/perceiving
    sounds and objects in problem solving and learning, as shown in Figure 1. There
    are numerous techniques used for formulating AI solutions. The early seminal approaches
    tend to explicitly program a decision system by leveraging domain-specific knowledge,
    leading to the concept of expert systems. Various carefully defined rules have
    to be structured and shaped to create such domain-specific expert programs. In
    contrast to expert systems, which contain millions of lines of code along with
    decision trees and complex rules, machine learning is of potentially lower complexity;
    hence, it has made tremendous progress over the last several decades [3]. The
    stylized relationship between machine learning and AI is illustrated in Figure
    1. The core motivation behind machine learning is that of allowing autonomous
    learning/training based on its access to huge amounts of data rather than writing
    hard-coded routines of specific instructions. The resultant algorithm then offers
    a variety of intelligent actions, ranging from learning based on past experience,
    reasoning for comprehending complex ideas, and generalizing to new situations.
    Figure 1 The relationships between AI and machine learning. In a rule-based system,
    static knowledge is explicitly represented as a set of rules that can be thought
    of as a collection of facts, constraints, or regulators. An expert system constitutes
    one of the classic examples of a rule-based system and makes use of various domain-specific
    expert programs. The implementation and integration of heuristic knowledge also
    provide a basis for transparency and flexibility. By contrast, machine learning
    tends to automatically learn computational models based on its access to huge
    amounts of data without relying on hard-coded routines of specific instructions.
    Show All In this light, a natural question arises: How can big data and machine
    learning help to enhance the performance of future 5G and beyond wireless networks?
    At the time of this writing, it is not so widely understood how best to harness
    machine-learning techniques for solving the typical optimization problems in wireless
    networks scenarios that depend on big data analysis. Contributing to the solution
    of this problem motivates us to develop this treatise, where big data resources
    are utilized by analytical machine-learning tools in support of intelligent applications
    in wireless networks. The main contributions of this article can be summarized
    as follows: The beneficial exploitation of big data resources in wireless networks
    is discussed. Three classification approaches suitable for machine learning are
    proposed according to different criteria. The pros and cons are presented and
    consider compelling application scenarios. A unified framework for invoking machine-learning
    techniques in social network-aware wireless is proposed and augmented by considering
    a pair of intelligent application scenarios. Big Data in Wireless Networks In
    this section, we classify the family of existing data sources into three broad
    categories, i.e., general wireless data, social network-aware data (social data),
    and cloud data, as shown in Figure 2. The corresponding application scenarios
    are discussed in the following sections. Figure 2 A flowchart of the classifications
    of wireless big data: its source, the information hidden in it, and its applications.
    M2M: machine-to-machine; P2P: point-to-point. Show All Wireless Data The big data
    generated by wireless users contain useful information about their activity patterns
    versus time, frequency, and space. For instance, from the data traffic/demand
    variation over time, we can infer the interference power at different frequencies,
    the congestion-level distribution at different locations, and so on. By making
    use of these spectral patterns, we can efficiently manage wireless resources to
    improve the system’s spectral efficiency and enhance user quality of service (QoS).
    As displayed in Figure 2, one of these intelligent applications is load balancing,
    which relies on proactive wireless resource allocation. In this context, the operator
    can adjust the transmit power, frequency, or direction (e.g., through sectorized
    antennas) of different BS transmitters that rely on estimating the mobile users’
    distributions. Furthermore, the operator can dispatch mobile BSs in advance when
    a surge of regional data traffic is anticipated. Another important application
    is constituted by wireless security surveillance. Given the spectral activity
    patterns inferred, we can detect anomalies in the radio environment (i.e., the
    perspective of rogue BSs) based on atypical, measured, real-time spectrum usage.
    The key challenge of the aforementioned applications is to derive such a “radio
    map” from the vast amount of noisy wireless big data so that we can accurately
    characterize the spectral usage patterns in different dimensions and scales [4].
    Social Data The main cause of the soaring data volume on the Internet is online
    social networks. The penetration of the mobile Internet into our daily lives makes
    convenient multimedia communications ubiquitously accessible for everyone. The
    amount of social data has reached an astonishing magnitude and is set to exhibit
    an increasing trend in the next few years. In 2017, on average, more than 500
    million Twitter messages were generated per day, over 80% of them initiated from
    mobile terminals (data from online). On the one hand, social network data feature
    strong ties to public events occurring in the physical world. For instance, an
    important football game or political event may inspire heated online discussions
    that last for days; meanwhile, the frequent sharing of high-score online evaluations
    about a movie premiere or a newly opened restaurant may attract a large number
    of customers in the real world. On the other hand, mobile social network data
    contain rich information about the contexts/preferences of individuals or social
    groups. As an example, we can infer from tweets that mobile users visiting a famous
    tourist site are unhappy about its wireless services. To address this shortcoming,
    we can improve the typical tourist experience by temporarily allocating more bandwidth
    to the nearby BSs. As a result, a social network-aware wireless concept can be
    understood. Here, the major technical difficulty is to surmise the true “meaning”
    of the users’ messages and take the appropriate actions. Given the vast amount
    of social network data and the diverse nature of information conveyed, e.g., text
    or multimedia, this may be achievable through advanced machine learning. Cloud
    Data Another major cause of the data tsunami in wireless networks is the transmission
    of multimedia content stored on cloud servers. It is predicted that, by 2019,
    more than 80% of the world’s Internet traffic will be videos, i.e., YouTube short
    clips, Netflix long clips, and Facebook live streaming (data from online). Meanwhile,
    online audio streaming services also contribute a large portion of the remaining
    20% data traffic. A unique feature of cloud-based big data is that the users’
    preferences concerning specific content are often similar and correlated. For
    instance, 3% of YouTube videos account for more than 90% of its total views (data
    from online). In other words, most of the content transferred over the Internet
    is based on its popularity. We can, therefore, reduce the teletraffic of the system
    by exploiting users’ preference for specific cloud content. For instance, we can
    precache the most popular videos at edge servers so that no real-time backhaul
    data downloading is needed for frequent requests. Moreover, by caching the video
    content at multiple nearby BSs, we can form a virtual antenna array and support
    seamless handovers for mobile terminals. Additionally, each individual user preference
    for specific multimedia content, if available, can be used for predicting the
    user’s future demand. The network operator can then perform prefeeding or recommendation
    actions. Naturally, the main technical challenge is to accurately predict users’
    preference distribution. Another challenge is to correctly label the lengthy number
    of videos for future reference/searching. In this case, using manual labeling
    is infeasible for high-population networks; hence, AI techniques are needed for
    designing content-oriented, intelligent wireless transmissions. Machine Learning
    in Wireless Networks In this section, we detail how machine-learning techniques
    are classified and how they can be applied to wireless communications. The pros
    and cons of each machine-learning family are discussed and displayed in Figure
    3. Figure 3 The classifications of machine learning, each classification’s corresponding
    pros and cons, and some application scenarios for each. The intent of this figure
    is to provide a systematic review of practical applications for modern machine-learning
    methods in wireless communication. By categorizing the various approaches into
    three principal groups according to 1) how much human supervision they require,
    2) whether they are able to perform incremental learning, and 3) how they generalize
    large-scale training/testing scenarios, we explicitly demonstrate the pros and
    cons of each scheme and highlight the most appropriate applications. NOMA: nonorthogonal
    multiple access; UAV: unmanned aerial vehicle; MEC: mobile edge computing; mm-wave:
    millimeter-wave; SARSA: state–action–reward–state–action; AC-3 algorithm: arc-consistency
    algorithm 3; DDPG: deep deterministic policy gradient; PPO: proximal policy optimization.
    Show All Human Supervision Requirement According to whether or not the algorithms
    require human supervision, machine-learning models may be grouped into the following
    three general categories [5, 6]: Supervised learning: Long considered a major
    branch of machine learning, supervised learning has been extensively studied and
    developed. Large quantities of human-labeled data should always be readily available
    for learning a functional mapping between the training samples observed and the
    desired output. The advantage of supervised learning is that both the convergence
    speed and the action quality are high, although they typically require a large
    amount of data to be labeled manually, thus making the data processing more complex.
    Attractive scenarios for applying supervised learning are wireless resource allocation
    and encoder and decoder design, where the objective function definition of the
    application is clear and collecting sufficient training data is relatively easy
    and less costly. Unsupervised learning: This model relies on vast amounts of unlabeled
    data for inferring the underlying information structure without depending on external
    resources and human supervision. The advantage of unsupervised learning is that
    no prior knowledge is required; however, this comes at the cost of potentially
    reducing its accuracy. Another disadvantage is that the automatically discovered
    data are not always representative of real-world conditions. Given its unique
    features, unsupervised learning is suitable for solving the problems of user association,
    user grouping for hybrid multiple access, attack detection of malicious users,
    and so on. Reinforcement learning: Initially designed to discover optimal action
    spaces through adaption and interactions in uncertain time-varying environments,
    this model provides another way of learning from unlabeled data as long as either
    positive or negative feedback can be gleaned during the learning process by trial
    and error. Reinforcement learning does not require direct supervision. In fact,
    the interactive learning paradigm is capable of learning to act so that it may
    prepare itself for achieving an ever-improving performance. The disadvantage of
    reinforcement learning is, however, that it relies on huge amounts of resources.
    Another drawback is that the resultant high-performance solutions often lack plausible
    physical interpretations. Nonetheless, successful application scenarios for reinforcement
    learning have been found in unmanned aerial vehicle (UAV) communications, autonomous
    driving, mobile edge computing (MEC), and wireless caching placement. Learning
    Capability Based on its learning capability, we can classify the family of machine-learning
    models into the following two subsets [7]: Batch learning-based algorithms typically
    train a model after a sufficiently large amount of training data have been collected
    prior to the learning task. This offline learning procedure has the advantage
    of allowing for more involved machine learning based on all the available data,
    where the model tends to be updated infrequently in general. The main advantage
    of batch learning is that the convergence speed is high; however, batch learning
    may not be suitable for real-time learning of rapidly fluctuating processes subject
    to stringent delay requirements. As a result, its beneficial application scenarios
    are wireless caching or wireless offloading. Online learning enables a model to
    learn from a stream of data instances arriving sequentially, which is achieved
    by continuously changing and adapting its structure and parameters. The “on-the-fly”
    learning scheme holds the promise of being both memory efficient and highly scalable
    in solving large-scale learning problems. A particularly remarkable advantage
    of online learning is that it is eminently suitable for real-time processing.
    Nevertheless, its convergence speed is typically slow; therefore, it can be used
    for cognitive radio networks or UAV movement scheduling. Generalization Requirement
    On the basis of how the methods generalize their findings from the training data
    to hitherto unseen examples, machine-learning models can be divided into the following
    two parts [8]: Instance-based learning algorithms [9] tend to use the whole set
    of instances found in the training data stream for constructing inference structures
    and predicting unseen instances. Although they are quite capable of competent
    generalization, this is achieved at the cost of extended search time and high
    memory requirements. An especially remarkable advantage of instance-based learning
    is that it does not require any prior-model assumptions. Therefore, instance-based
    learning is suitable for complex wireless scenarios, such as power spectrum density
    estimation or user demand prediction. However, large data sets are required for
    high-quality instance-based learning. Model-based learning aims to find the optimal
    parameters of the algorithms designed [10] so as to optimize the objective function
    and maximize the generalization capability in the face of previously unseen testing
    data. Such models usually suffer from limited accuracy but offer higher computational
    efficiency. Hence, the key advantage of model-based learning is that its implementation
    cost is low. Given these attributes, model-based learning can be readily used
    for the movement trajectory prediction of mobile users, channel modeling estimation
    in millimeter-wave communications, and so on. A Unified, Big Data-Aided AI Framework
    In this section, we propose the new, unified machine-learning framework presented
    in Figure 4. According to social, cloud, or wireless data, we implement advanced
    machine-learning approaches that analyze the data used to extract useful information.
    More particularly, the proposed framework includes the following three stages,
    as illustrated in Figure 4: Feature extraction: During this stage, we extract
    features from data sets that include social, cloud, or wireless data, as depicted
    in Figure 2. Let us consider as an example social data, which include social media,
    social commerce, or mobile social networks. We can derive social context and user
    preferences with the help of social data by invoking natural language processing
    or other techniques. The detailed procedure includes a pair of steps, the first
    of which is syntax processing. Here, its input is the tremendous amount of social
    data, which are subjected to natural language processing techniques, e.g., language
    identification, tokenization, lemmatization, stemming, etc. The output should
    be some useful geo- and time-tagged keywords according to the specific application,
    e.g., crime prevention. The second step is semantic analysis, in which the input
    is determined by the keywords subtracted during the last stage through the use
    of sophisticated techniques, e.g., lexical semantics, clustering content, sentiment
    analysis, and summarization. The outputs are expected to be context-aware user
    preferences. A suitable example is to apply Twitter data to predict topics of
    high popularity for content caching in wireless networks. Data modeling: The feature
    extraction stage mentioned previously can be regarded as the predata processing
    stage. This data modeling stage constitutes the core process of establishing a
    machine-learning model. More particularly, the feature we extracted during the
    last stage, i.e., user mobility and the content popularity of users, represents
    the data set we seek for modeling the problem. Typically, we first formulate an
    objective function [e.g., maximizing the quality of experience (QoE), reducing
    delay, or improving energy efficiency] for enhancing the performance of the wireless
    networks considered. Then, we utilize the modeling strategies presented in the
    data modeling stage of Figure 4 to mathematically represent the formulated problems.
    Prediction/online refinement: After establishing the machine-learning model, we
    proceed to either the prediction or the online refinement stage. For the prediction
    stage, it is implied that the established model can be used only for predicting
    user behavior/network performance without updating the model invoked. During the
    online refinement stage, the established model can be refined based on changes
    to the real-time data input. The intuitive difference between prediction and online
    refinement is that the system provides a flexible mechanism for inference testing,
    relying on either an offline or periodically refined model. The selection of these
    two variants ultimately depends on the particular context of a specific application.
    For the use case when the data distribution does not show large variations over
    the pertinent time scales, predictions that depend on an offline trained model
    are always the best choice for ease of computation. Conversely, online refinement
    must be undertaken to improve local errors when the data are intrinsically time
    varying; however, the latter is more computationally demanding. Note that a significant
    advantage of this framework is that the machine-learning model depends not only
    on historical data but also on real-time data. For example, we could periodically
    use the past two weeks of data to predict the next day’s trends. By exploiting
    these time-sequence characteristics, the prediction capability of this model becomes
    quite accurate and timely. The techniques displayed in the prediction/online refinement
    stage of Figure 4 can all be beneficially applied. Figure 4 A unified big data-aided
    machine-learning framework. Show All Case Study: Social Network-Aware Wireless
    Having introduced the three key stages of the proposed framework, the next important
    step is to identify suitable application scenarios. In this section, we consider
    a pair of intelligent applications as a way of demonstrating how best to apply
    our big data-aided machine-learning framework for fostering intelligent wireless
    networks. In both case studies, we develop the position information collected
    using Twitter’s application programming interface to infer the periodic behaviors
    of users in social networks. We propose efficient algorithms based on neural networks
    for predicting the user’s mobility or content popularity, both of which belong
    to the feature extraction stage. Then, reinforcement learning is used to solve
    the problems formulated in the data modeling stage, either for cooperative caching
    or UAV deployment, for example. Dynamic UAV Deployment and Movement Design Thanks
    to the rapid capability upgrade of drones, BSs on the fly have emerged as an efficient
    solution for improving radio coverage conditions in the face of rapidly fluctuating
    traffic demands by appropriately adjusting the position of drones. Because of
    the availability of drone-mounted BSs, how best to position them for satisfying
    dynamically evolving data requests, and so maximize their benefits to operators,
    becomes one of the most challenging and critical problems. In such a scenario,
    the strategies for deploying drone-mounted BSs can be categorized into the two
    following types: Predeploy UAVs before the requests arrive: Based on the proposed
    framework, historical social data can be used for predicting forthcoming data
    requests in a specific area. If the outputs of data mining indicate that data
    requests may be expected to exceed the capability of fixed BSs covering that area,
    drone-mounted BSs can be deployed to the area in advance. The goal of this strategy
    is to provide an improved user experience by enhancing the network’s capability
    before congestions happen. Move UAVs based on real-time requests: Naturally, historical
    social data cannot exactly predict user requests, as they fluctuate dynamically.
    Therefore, a real-time UAV action is desired for improving the network’s capability
    to enhance user experience. Real-time social data related to the area of interest,
    i.e., tweets complaining about poor network quality, can be further analyzed,
    and more drone-mounted BSs can be dispatched to the hotspot area. In particular,
    by analyzing the critical tweets, we first decide the satisfaction level of users
    about the current transmit rate based on the QoE model, which identifies potential
    teletraffic congestion events. Next, the required number of UAVs as well as their
    movements can be determined for maximizing the users’ sum mean opinion score (MOS)
    by adopting machine-learning approaches. In this case study, multiple UAVs are
    used as flying BSs serving ground users by considering both of the aforementioned
    types of UAV deployments. Each UAV is capable of roaming in a free 3D space. Because
    ground users are intended to move flexibly, UAVs can fly dynamically according
    to the users’ real-time positional information. Figure 5(a) shows the four steps
    of UAV deployment and movement design that rely on the proposed framework. Step
    1 is considered the feature extraction stage. Here, the movement of users is the
    key feature we would like to extract. Multiagent Q-learning is the core algorithm
    used during the data modeling stage, which is employed in steps 2 and 4 for designing
    the specific deployment and movement of UAVs. Figure 5(b) illustrates how a UAV
    moves from an initial position to its final destination through the use of reinforcement
    learning for formulating its trajectory. In our scenarios, multiple UAVs move
    simultaneously according to the predicted movement of ground users; this can be
    termed the prediction stage. By doing so, the predeployment of UAVs can be achieved
    for enhancing the energy efficiency of networks. Figure 5 A case study of the
    deployment and movement design for multiple UAV networks [14]. (a) The procedure
    for UAV deployment and movement and (b) the trajectory for UAV deployment and
    movement. API: application programming interface. Show All Figure 6 compares the
    throughput between the moving UAV solution (both a predeployed and real-time movement
    solution) and the static one. It can be observed that the instantaneous transmit
    rate decreases as time elapses. This is because users are roaming during each
    time slot and, when user density is reduced, the instantaneous sum of the transmit
    rate is affected. It can also be observed that the redeploying of UAVs based on
    the movement of users is an efficient method of mitigating the downward trend
    compared to the static scenario. Figure 6 A comparison of throughput between a
    moving-UAV and static-UAV solution. Show All Wireless Caching Placement and Resource
    Allocation Due to the rapid development of mass-storage techniques, storage becomes
    an increasingly low cost resource, while the opposite trend prevails for spectral
    resources. The caching process has two parts: caching placement and cached content
    delivery. In contrast to most existing research contributions, which have assumed
    that the caching placement process follows a specified distribution, in this case
    study, the content caching at each BS is dynamically varied according to user
    requirements. More particularly, with the aid of social data within a given area
    and the corresponding location information of BSs, the caching placement can be
    improved considerably. Typically, the main objective of caching placement is to
    identify the most popular K files to be stored in the BS during a particular period
    to reduce the potential latency when users download these files. To efficiently
    allocate wireless content, e.g., video clips to BSs, some prior information on
    user mobility and content popularity is required. However, the complicated relationship
    between historical and future information makes conventional approaches less applicable.
    AI algorithms are capable of predicting network demands in the near future, which
    helps with efficiently caching the most popular content when the communication
    resources (e.g., bandwidth, storage capacity, and computing speed, etc.) are finite.
    Considering some recent, vile terrorist attacks as an example, typically, there
    are immediate reactions on social media, i.e., Facebook and Twitter. In the ensuing
    period, the discussions, related posts, and messages that use social data increase.
    By invoking natural language processing to analyze the posts, keywords can be
    extracted. The keywords may then be classified or mapped to different hot topics.
    The hot topics determine what content should be cached in particular BSs. Based
    on time-geotagged hot topics, machine learning may be deployed for training this
    learning model. The final goal is that, according to the historical keywords in
    a certain past period of, e.g., 24 h, the content to be cached in the next period
    of 2 h, e.g., can be dynamically decided. Figure 7(a) depicts the detailed procedure
    of wireless caching. More specifically, we can still map the wireless caching
    placement to our proposed framework. In the feature extraction stage, both user
    mobility and content popularity must be predicted to support proactive content
    placement. During the data modeling stage, neutral networks and reinforcement
    learning may be invoked to mathematically model mobility/content popularity and
    cooperatively cache placement, respectively. During the prediction stage, the
    outputs from neural networks [13] can be used for predicting the cooperative caching
    allocation. Figure 7(b) shows the total MOS of users in the network versus time
    for different algorithms. During the initial time period, the optimal content
    is placed according to the users’ positions. Then, because we consider a user
    movement scenario, the positions of users change with the time period; therefore,
    the content placement is not optimal. As a result, users’ total MOS decreases.
    We can observe that the Q-learning algorithm, which is a close relative of the
    reinforcement learning algorithm, is capable of achieving a near-optimal performance,
    while outperforming global K-means-based caching. Figure 7 A case study of content
    placement in wireless cooperative caching [12]. (a) The flowchart for wireless
    caching and (b) the simulation results for wireless caching. Show All Future Challenges
    and Concluding Remarks In this article, the design challenges of invoking machine-learning
    techniques for enhancing wireless networks with the aid of big data solutions
    were investigated. First, we identified the key features of big data, which spawn
    new research opportunities for wireless networks. Then, we discussed the classification
    of machine-learning techniques and their corresponding application scenarios in
    wireless networks. Furthermore we proposed a unified, big data-aided machine-learning
    framework. We followed this by introducing a pair of case studies in wireless
    networks, i.e., UAV deployment/trajectory design and wireless caching placement.
    Still, numerous open research opportunities must be pursued in the context of
    machine-learning-aided wireless communications: Adaptive, nonorthogonal, massive
    multiple access: Nonorthogonal multiple access (NOMA) is a multifaceted technique
    used for enhancing wireless networks. Note that each form has advantages and disadvantages.
    Accordingly, different NOMA techniques are suitable for various scenarios, [15,
    Table 9]. Motivated by this, a software-defined NOMA architecture is proposed
    for supporting diverse user scenarios; however, teletraffic demands can vary seasonally
    or due to public events. Therefore, machine learning can be used to predict data
    traffic. The multiple access settings can be categorized into two types: pre-
    and real-time settings. Batch learning can be used for presettings, while real-time
    settings are typically based on users’ social media feedback regarding, e.g.,
    the ability to dynamically change the multiple access settings on demand, which
    may invoke online learning, as mentioned in the “Machine Learning in Wireless
    Networks” section. Autonomous driving in vehicle-to-everything networks: Autonomous
    driving has the potential to benefit society in numerous ways, such as reducing
    traffic congestion and mitigating the environmental footprint of modern-day traffic.
    The combination of autonomous driving and vehicle-to-infrastructure (V2I) communications
    enables automated vehicles to receive up-to-date information about neighboring
    vehicles’ dynamics and other traffic information, thereby enhancing both safety
    and traffic efficiency. The deep-reinforcement-learning models mentioned in the
    “Machine Learning in Wireless Networks” section, which are trained by interacting
    with its environment, may be utilized to optimize the behaviors of vehicles by
    exploring the environment in an iterative manner and learning from mistakes. With
    the aid of cloud or wireless data, V2I network requirements can be updated in
    real time. In this case, the autonomous vehicle becomes capable of safely reaching
    its destination, while avoiding traffic jams with the aid of up-to-date traffic
    information. Intelligent computation offloading: MEC is a promising technique
    used to meet the ever-increasing computational demands of mobile applications
    by providing computing capabilities at the edge of wireless networks, while migrating
    the computationally intensive tasks to the MEC server. The core concept of MEC
    is to provide abundant computing capabilities at the edges of networks and so
    mitigate both the backhaul and fronthaul load and reduce mobile users’ energy
    consumption. Task offloading decisions and computational resource allocation constitute
    a pair of challenges in MEC. Obtaining an optimal offloading policy in such a
    dynamic MEC system in real time is challenging; however, machine learning is capable
    of intelligent inferences from historic information. By using the proposed framework,
    a real-time dynamic MEC system that operates with the assistance of social data
    can be designed utilizing machine-learning algorithms to attain significant performance-versus-complexity
    benefits. Authors Figures References Citations Keywords Metrics More Like This
    A Novel Hybrid Machine Learning Algorithm for Limited and Big Data Modeling With
    Application in Industry 4.0 IEEE Access Published: 2020 Big Data Analytics, Machine
    Learning, and Artificial Intelligence in Next-Generation Wireless Networks IEEE
    Access Published: 2018 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE vehicular technology magazine
  limitations: '>'
  pdf_link: null
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: 'When Machine Learning Meets Big Data: A Wireless Communication Perspective'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2021.3060863
  analysis: '>'
  authors:
  - M. Mazhar Rathore
  - Syed Attique Shah
  - Dhirendra Shukla
  - Elmahdi Bentafat
  - Spiridon Bakiras
  citation_count: 156
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences Loading
    [MathJax]/extensions/MathMenu.js IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More
    Sites Subscribe Donate Cart Create Account Personal Sign In Personal Sign In *
    Required *Email Address *Password Forgot Password? Sign In Don''t have a Personal
    Account? Create an IEEE Account now. Create Account Learn more about personalization
    features. IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/9312710/09359733.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'The Role of AI, Machine Learning, and Big Data in Digital Twinning: A Systematic
    Literature Review, Challenges, and Opportunities'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3184558.3186242
  analysis: '>'
  authors:
  - Muhammad Imran
  - Carlos Castillo
  - Fernando Dı́az
  - Sarah Vieweg
  citation_count: 51
  full_citation: '>'
  full_text: ">\nProcessing Social Media Messages in Mass Emergency:\nSurvey Summary\n\
    Muhammad Imran\nQatar Computing Research Institute (HBKU)\nDoha, Qatar\nmimran@hbku.edu.qa\n\
    Carlos Castillo\nUniversitat Pompeu Fabra\nBarcelona, Spain\nchato@acm.org\nFernando\
    \ Diaz\nSpotify\nNew York, NY, USA\ndiazf@acm.org\nSarah Vieweg∗\nQatar Computing\
    \ Research Institute (HBKU)\nDoha, Qatar\nsarahvieweg@gmail.com\nABSTRACT\nMillions\
    \ of people use social media to share information during dis-\nasters and mass\
    \ emergencies. Information available on social media,\nparticularly in the early\
    \ hours of an event when few other sources\nare available, can be extremely valuable\
    \ for emergency responders\nand decision makers, helping them gain situational\
    \ awareness and\nplan relief efforts. Processing social media content to obtain\
    \ such\ninformation involves solving multiple challenges, including parsing\n\
    brief and informal messages, handling information overload, and\nprioritizing\
    \ different types of information. These challenges can be\nmapped to information\
    \ processing operations such as filtering, clas-\nsifying, ranking, aggregating,\
    \ extracting, and summarizing. This\nwork highlights these challenges and presents\
    \ state of the art com-\nputational techniques to deal with social media messages,\
    \ focusing\non their application to crisis scenarios.\nKEYWORDS\nsocial media;\
    \ disaster response; emergency management\nACM Reference Format:\nMuhammad Imran,\
    \ Carlos Castillo, Fernando Diaz, and Sarah Vieweg. 2018.\nProcessing Social Media\
    \ Messages in Mass Emergency: Survey Summary.\nIn WWW ’18 Companion: The 2018\
    \ Web Conference Companion, April 23–27,\n2018, Lyon, France. ACM, New York, NY,\
    \ USA, 5 pages. https://doi.org/10.\n1145/3184558.3186242\n1\nINTRODUCTION\nSudden-onset\
    \ emergencies such as natural or human-induced disas-\nters bring uncertainties\
    \ and an increasing need for time-critical in-\nformation for formal response\
    \ organizations, affected communities\nand other concerned populations [13, 46].\
    \ In particular, immedi-\nately after sudden-onset events, when information is\
    \ most needed,\ninformation may be scarce.\n∗The author now works at Facebook.\n\
    This paper is published under the Creative Commons Attribution 4.0 International\n\
    (CC BY 4.0) license. Authors reserve their rights to disseminate the work on their\n\
    personal and corporate Web sites with the appropriate attribution.\nWWW ’18 Companion,\
    \ April 23–27, 2018, Lyon, France\n© 2018 IW3C2 (International World Wide Web\
    \ Conference Committee), published\nunder Creative Commons CC BY 4.0 License.\n\
    ACM ISBN 978-1-4503-5640-4/18/04..\nhttps://doi.org/10.1145/3184558.3186242\n\
    The growing adaption of Information and Communication Tech-\nnologies (ICT) and\
    \ social networking platforms such as Twitter and\nFacebook has created numerous\
    \ opportunities to disseminate and\nconsume time-critical information in the form\
    \ of images, videos and\ntextual messages during natural disasters and emergencies\
    \ [47, 49].\nSocial media postings are useful for a number of crisis response\n\
    and management tasks such as: gaining insights into the situation\nas it unfolds,\
    \ identifying urgent needs of the affected communi-\nties, and assessing the severity\
    \ of damage [8]. Hence, many formal\ndisaster response or emergency management\
    \ organizations are in-\nterested in finding ways to quickly and easily locate\
    \ and organize\nthe information that is most useful to them [48].\nHowever, the\
    \ time-critical analysis of high-velocity, high-volume\nsocial media streams involves\
    \ solving multiple challenges, includ-\ning real-time parsing of brief and informal\
    \ messages, handling\ninformation overload, determining information credibility,\
    \ and pri-\noritizing useful information. These challenges can be mapped to\n\
    classical information processing tasks such as filtering, classifying,\nextracting,\
    \ aggregating, ranking, visualizing, and summarizing in-\nformation. For example,\
    \ automatic classification techniques help\nreduce information overload by filtering\
    \ out irrelevant messages,\nand summarization techniques help gain situational\
    \ awareness by\nextracting important summaries from the relevant messages.\nIn\
    \ addition to the textual content of social media, multimedia\ncontent (images\
    \ and videos) can be valuable for disaster response\nand management [36]. For\
    \ instance, images posted can be used to\nevaluate the severity of damage to critical\
    \ infrastructure such as\nbuildings, bridges, and roads [33]. Furthermore, in\
    \ areas without\nInternet access, satellites and Unmanned Aerial Vehicles (UAV)\n\
    can collect imagery data to increase situational awareness [34].\nHowever, in\
    \ contrast to the main advances in text processing (e.g.,\nnatural language processing)\
    \ techniques, less has been done for\nmultimedia content processing.\nThis paper\
    \ summarizes an extended survey [18] covering state\nof the art methods for processing\
    \ social media to support various\ndisaster response and management operations.\
    \ We further include\nnew research questions and challenges for social media information\n\
    processing in crisis response.\nTrack: Journal Papers\n \nWWW 2018, April 23-27,\
    \ 2018, Lyon, France\n507\n2\nINFORMATION NEEDS, DATA\nCHARACTERISTICS, AND ACQUISITION\n\
    2.1\nInformation needs\nThe information needs of formal emergency response organizations\n\
    varies with their roles and duties, as well as with characteristics\nof their\
    \ specific context and its evolution over time [15, 16]. For\ninstance, humanitarian\
    \ and governmental emergency management\norganizations seek high-level information\
    \ about a disaster, such as\nthe scale of the event, the number of people living\
    \ in the affected\nareas, and the overall economic impact. In contrast, organizations\n\
    such as local police forces and firefighters benefit from information\nconcerning\
    \ individual emergency cases, such as urgent medical\nemergency reports, and the\
    \ location of severely injured or trapped\npeople. Roughly, these two types can\
    \ be characterized as seeking\nto understand “the big picture” versus finding\
    \ “actionable insights.”\n2.2\nData characteristics and acquisition\nThe factors\
    \ that can trigger an increase in social media commu-\nnications can be endogenous\
    \ or exogenous. The former includes\nspontaneous popularity increases of an information\
    \ “meme” due to\ninformation cascades or contagion. The latter includes large-scale\n\
    events such as disasters, emergencies, and mass convergence events.\nData quality,\
    \ in terms of readability, grammar, and sentence struc-\nture, vary significantly\
    \ across social media platforms. For instance,\nmessages in Twitter are often\
    \ brief, informal, unstructured, and\ncontain grammar and spelling mistakes. Preprocessing\
    \ this data is\nan essential step before it can be used for any computation.\n\
    For automatic data collection, most large social media platforms\nprovide Application\
    \ Programming Interfaces (APIs). Generally,\nthese APIs can be categorized into\
    \ two types: search APIs and\nstreaming APIs. Search APIs provide access to archived\
    \ messages\nup to a certain limit. Streaming APIs allow subscribers to continu-\n\
    ously consume data from a real-time data feed (again data limits\nmaybe imposed).\
    \ Different data collection strategies can be used\nto collect data, depending\
    \ on the information need expressiveness\nallowed on these APIs. For example,\
    \ a geographical rectangle (or\nbounding box) can be defined to acquire geo-tagged\
    \ messages, a\nboolean query composed of keywords can be defined to acquire\n\
    messages matching such query, or a specific set of users can be\nspecified to\
    \ collect everything those users post.\n2.3\nEvent detection\nSocial media data\
    \ processing systems often start with the auto-\nmatic detection of an event.\
    \ Crisis situations can be expected (i.e.,\npredicted or forewarned) or unexpected.\
    \ Expected crises include\nextreme weather events such as storms and tornadoes,\
    \ where in-\nformation is usually broadcast publicly before the event happens.\n\
    Unexpected events include earthquakes and technological disasters\nsuch as industrial\
    \ accidents, which can be anticipated only to a\nvery coarse degree. The Topic\
    \ Detection and Tracking (TDT) re-\nsearch community has developed techniques\
    \ based on data from\nnews media and other sources [3] including story segmentation,\n\
    topic detection, new event detection [5, 40, 41], link detection, and\ntopic tracking\
    \ [11, 45]. A popular approach to detect new events\nin a data stream is to look\
    \ for sudden increases in the frequency\n(“bursts”) of sets of keywords. However,\
    \ this approach has some\nshortcomings. For instance, many popular hashtags (such\
    \ as #fol-\nlowfriday) do not represent real-world events. Other approaches\n\
    include using wavelet-based clustering of frequency signals, topic\nclustering\
    \ with meta-data analysis, and domain-specific approaches\nusing predetermined\
    \ rules [12, 50].\n2.4\nRetrospective versus live data processing\nOnce an event\
    \ is detected, its continuous tracking to obtain real-\ntime insights is important\
    \ for crisis responders. To be effective\nduring emergencies, the data should\
    \ be processed in real-time, and\nnot retrospectively. Live data analysis (online\
    \ processing) is usually\nperformed over a current stream of data relevant for\
    \ an event, often\nprovided in real-time or with a short delay. The trade-off\
    \ between\nretrospective and real-time data analysis is a matter of accuracy\n\
    versus latency. However, in time-critical situations, the urgency\nwith which\
    \ the output of an analysis is required remains high, thus\nonline algorithms\
    \ using stream processing architectures can play a\nvital role [19, 28].\n3\n\
    CLASSIFICATION, CLUSTERING, AND\nSUMMARIZATION\nSocial media communications during\
    \ disasters are now so abun-\ndant that it is necessary to sift through hundreds\
    \ of thousands,\neven millions of data points to find useful information. In recent\n\
    years, several text and multimedia processing methods have been\ndeveloped to\
    \ efficiently process this overwhelming amount of in-\nformation.\n3.1\nInformation\
    \ classification\nA key task is to separate relevant and useful information from\n\
    irrelevant and/or noisy content. One well-known approach to this\ntask is supervised\
    \ classification, which requires labeled examples\n(usually thousands of them).\
    \ The selection of the set of categories\nto be used is mainly driven by two main\
    \ factors: the data that is\npresent in social media during crises and the information\
    \ needs of\nusers [17].\nA classification task broadly includes different typologies\
    \ of\nmessages by information provided (e.g., affected or injured people,\ninfrastructure\
    \ damage, urgent needs of affected population) [19],\nor by information source\
    \ (e.g., eyewitness accounts, official govern-\nment sources, TV, radio) [35],\
    \ or by information credibility factors\n(e.g., fake news, rumors, disinformation,\
    \ misinformation) [9], or by\ntemporal aspects (e.g., using different temporal\
    \ phases of an event\nincluding pre, during, and post) [11], by geographical locations\
    \ (e.g.,\nbased on particular geographical area or places near to the disaster\n\
    zone), or by factual, subjective, or emotional content.\n3.2\nInformation clustering\n\
    Clustering, which is an unsupervised machine-learning technique,\ncovers a family\
    \ of methods that seek to identify and explain im-\nportant hidden patterns in\
    \ unlabeled data. To process social media\ndata during crises, clustering can\
    \ help gather semantically simi-\nlar messages that need to be processed/examined\
    \ by humans, for\ninstance by displaying multiple equivalent messages as a single\n\
    item instead of multiple ones [6, 42]. Clustering approaches can be\nTrack: Journal\
    \ Papers\n \nWWW 2018, April 23-27, 2018, Lyon, France\n508\nused to find anomalies\
    \ in crisis data streams and to discover human\nannotation errors to improve the\
    \ quality of supervised classification\nprocess [17, 20].\n3.3\nInformation summarization\n\
    Categorizing messages reduces the amount of information that has\nto be processed\
    \ by humans, but the remaining set of messages, even\nin single categories, can\
    \ still be overwhelming for crisis responders.\nInformation summarization techniques\
    \ generate text-based sum-\nmaries of an event as it unfolds. Text summarization\
    \ systems are\nusually designed to surface core topics discussed in a set of relevant\n\
    documents by locating key sentences. Other systems can produce\na summary by abstracting\
    \ or generating new sentences [4].\nIn the context of disaster data summarization,\
    \ temporal infor-\nmation summarization or update summarization methods can infer\n\
    the importance of different sentences using disaster-specific fea-\ntures, including\
    \ geo-location and language models representing the\nway in which people write\
    \ in social media about a disaster [24].\nText summarization systems can generate\
    \ evolving summaries of\na disaster over time [23]. Other approaches adapt a two-step\
    \ ap-\nproach in which first critical sub-events are identified and then\nsummarized\
    \ [43]\n3.4\nInformation veracity\nWhile social media can be a valuable resource\
    \ for crisis response ef-\nforts, one of the barriers that hinders its use as\
    \ a resource is the lack\nof trust on its contents [14]. Determining information\
    \ credibility is\ndifficult, especially during an ongoing emergency situation,\
    \ as it in-\nvolves dealing with misinformation (unintentional), disinformation\n\
    (intentional), and rumors (unverifiable). Automatic techniques have\nbeen proposed\
    \ to determine information credibility on social media,\nmost such methods are\
    \ based on superficial characteristics of the\nmessages and of the history of\
    \ the users who post them [9, 39].\n3.5\nSemantic enrichment\nSemantic enrichment\
    \ can be used to add layers of semantics (e.g.,\nmeta-data) to information so\
    \ that algorithms can understand and\ninterpret its meaning. Named entity recognition\
    \ is a well-known\nsemantic enrichment technique useful to identify named entities\n\
    (person, places, and organization) from a give piece of information\n(e.g., a\
    \ tweet). The identified named entities can then be linked to\nconcepts (in many\
    \ cases real-world concepts). After named-entity\nlinking, messages that have\
    \ been semantically enriched can be used\nto provide faceted search, a popular\
    \ approach to interactively search\nthrough complex information spaces. Such semantic\
    \ techniques\nhave been used in social media processing systems e.g., Twitris\
    \ [38].\nAutomatic processing systems interact with each other for a\nvariety\
    \ of reasons, allowing users to engage in information “trian-\ngulation.” However,\
    \ allowing systems to communicate information\nin a unified way is a challenging\
    \ endeavor. An effective way to\nenable such heterogeneous communications is through\
    \ machine-\nunderstandable ontologies. In crisis informatics domain ontologies\n\
    such as HXL 1, MOAC 2 define and categorize different concepts to\nfacilitate\
    \ a common understanding.\n1http://hxl.humanitarianresponse.info/\n2http://observedchange.com/moac/ns/\n\
    3.6\nMultimedia processing\nDespite the wealth of research studies on the processing\
    \ of social\nmedia textual content, comparatively less has been done on multi-\n\
    media (images and videos) content. Images shared on social media\ncan significantly\
    \ help in various disaster response tasks [36]. Addi-\ntionally, messages containing\
    \ images/photos have been found to\nbe more helpful for other tasks such as estimating\
    \ the epicenter of\nan earthquake [25].\nOther approaches combine features from\
    \ textual and imagery\ncontent to, for example, classify visually relevant and\
    \ irrelevant\ntweets [10, 51] or to generate event timelines using image-text\n\
    summarization [51]. Visual summaries of trending topics can be\ngenerated using\
    \ a multimodal-LDA (MMLDA) method [7], and im-\nage classification can be used\
    \ to support a number of crisis response\ntasks [2] such as critical infrastructure\
    \ damage assessment [33] and\nirrelevant image filtering [30].\nSatellite imagery\
    \ and aerial imagery captured via unmanned\naerial vehicles (UAVs) is increasingly\
    \ considered important and\nuseful for disaster response. Aerial imagery can be\
    \ captured and\nprocessed faster and at a lower cost, in comparison to satellite\
    \ im-\nagery data, and can potentially help gain situational awareness\nduring\
    \ the early hours of a disaster event. Hybrid systems com-\nbining human computation\
    \ and machine learning techniques have\nbeen proposed for UAV imagery processing\
    \ [34].\n4\nSYSTEMS FOR SOCIAL MEDIA\nMONITORING AND PROCESSING\nMany systems\
    \ for processing social media during disasters have\nbeen developed at various\
    \ levels of maturity, ranging from proof-\nof-concept to production-level systems.\
    \ For instance, Twitris [38]\nprovides support for automatic classification of\
    \ tweets and performs\nsemantic enrichment. Emergency Situation Awareness (ESA)\
    \ [37]\nperforms event detection, text classification, online clustring, and\n\
    geotagging. SensePlace2 [26] filters and extracts geographical, tem-\nporal, and\
    \ thematic information from tweets. Artificial Intelligence\nfor Disaster Response\
    \ (AIDR) [19] uses supervised machine learning\ntechniques to categories tweets\
    \ into humanitarian categories in\nreal-time.\nCommon elements in these systems\
    \ include lists and timelines\nto show recent important messages and groups containing\
    \ seman-\ntically similar messages, maps for geotagged messages, graphs and\n\
    charts to show visual summaries (e.g., proportion of messages), and\ntime series\
    \ graphs to represent volume of words, topics, or concepts\nover time.\n5\nCURRENT\
    \ RESEARCH CHALLENGES AND\nFUTURE DIRECTIONS\nIn this section, we suggest some\
    \ research directions based on the\nsurvey.\n5.1\nDomain adaptation and transfer\
    \ learning\nObtaining human-labeled data to train automatic classifiers takes\n\
    time; using data from past disasters may yield good results, but\nusing data from\
    \ the current disaster usually yields better results.\nCollecting recent training\
    \ data rapidly is a challenge, and failing\nTrack: Journal Papers\n \nWWW 2018,\
    \ April 23-27, 2018, Lyon, France\n509\nto do so may introduce unwanted inaccuracies\
    \ or delays in the\nprocessing of important information. While some recent studies\n\
    have demonstrated the utility of labeled data collected from past\n(potentially\
    \ similar) disasters [22, 29, 32], robust machine learning\ntechniques tested\
    \ in different types of disasters (floods, earthquakes,\nhurricanes etc.) have\
    \ yet to be developed. Methods such as domain\nadaptation and transfer learning\
    \ can be employed to utilize data\nfrom past events, possibly in combination with\
    \ unlabeled from the\nnew event.\n5.2\nOnline and active learning\nIn contrast\
    \ to the batch learning approach in which machine learn-\ning models are trained\
    \ using a batch of training examples, an on-\nline learning technique updates\
    \ models upon receiving each new\ntraining instance (e.g., by receiving new labeled\
    \ data from digital\nvolunteers that are active immediately following a disaster\
    \ [27]).\nOnline learning helps models dynamically adapt to new changes\nand patterns\
    \ in the data. However, two core issues in online learning\ntechniques that use\
    \ crowsourcing are (i) which tasks (e.g., tweets)\nto select for crowdsourcing?\
    \ and (ii) when is the suitable time to\ncreate each crowdsourcing tasks?\nDue\
    \ to the abundance of duplicate messages, particularly on\nTwitter, asking for\
    \ labels for messages that closely resemble each\nothers can potentially waste\
    \ crowdsourcing work. Moreover, pres-\nence of duplicate messages in the training\
    \ and testing sets may\nlead to misleading measurements of accuracy. Possible\
    \ solutions\ninclude, for example, having an aggressive de-duplication strategy\n\
    and employing an active learning technique to help select messages\nthat are most\
    \ likely to increase a system’s accuracy. Additionally,\ntechniques to uncover\
    \ new concepts and to track concept evolution\nin the data stream can detect underlying\
    \ data changes and trigger\nmodel re-training processes.\n5.3\nInformation credibility\n\
    Rumor detection, dealing with misinformation and disinformation\ncases, and determining\
    \ information credibility can address a core\nquestion of many humanitarian organizations:\
    \ how much should\nthey trust information found on social media. Despite several\
    \ recent\nattempts, information credibility issues remain largely unresolved.\n\
    Determining the credibility of information on social media is a\nchallenging task.\
    \ Techniques that try to tackle this issue should\nconsider different factors\
    \ such as what is the evidence? (e.g., search\nfor more eyewitnesses), is there\
    \ any bias associated with the pro-\nvided information?, and consider a variety\
    \ of different sources.\n5.4\nApplications of deep learning\nTraditional text\
    \ classification techniques rely on manually engi-\nneered features like cue words\
    \ and TF-IDF vectors, and do not\nperform well when data features change due to\
    \ high variability in\nsocial media data. Deep Neural Networks (DNNs) use distributed\n\
    condensed representation of words and learn higher level abstract\nfeatures automatically.\
    \ Contrary to sparse discrete representations,\ndistributed representations generalize\
    \ well and thus can play an im-\nportant role learning high variability in crisis\
    \ data. Convolutional\nNeural Network (CNN) are useful for the classification\
    \ of tweets into\nbinary (information vs. not-information) and multi-class (affected\n\
    individuals, infrastructure damage, donations) categories [31]. A\nproposed variation\
    \ of CNN with multilayer perceptron (MLP-CNN)\nhas also shown to perform well\
    \ in multi-class classification [29].\nAlthough deep neural network techniques\
    \ are promising for social\nmedia data classification, they require large amounts\
    \ of training\ndata. Recent attempts to make crisis-related labeled data available\n\
    are hopeful [21, 35], but much more remains to be done.\n5.5\nFrom situational\
    \ awareness to actionable\ninsights\nGaining situational awareness is a core task\
    \ of many humanitar-\nian organizations to gain insights from the disaster area.\
    \ However,\nin some cases it is actionable knowledge in the form of pieces of\n\
    information that are sought. These may include implicit and ex-\nplicit requests\
    \ related to emergency needs that should be fulfilled\nor serviced as soon as\
    \ possible. Despite extensive research that\nmainly focuses on understanding and\
    \ extracting updates contribut-\ning situational awareness from social media platforms,\
    \ limited work\nhas been done that focuses on understanding how actionable each\n\
    piece of information is in a given context and for a given user.\nComputational\
    \ techniques that can automatically identify action-\nable messages from a live\
    \ data stream during emergency events,\nand that are able to assess their urgency\
    \ and categorize them to\nmatch different information needs of humanitarian organizations,\n\
    are essential for crisis responders to launch rapid relief efforts.\n5.6\nHumanitarian\
    \ crises and health\nNatural disasters, long-term wars, and conflicts create severe\
    \ health\nissues to the people who live under these circumstances. According\n\
    to the World Health Organization (WHO), over 130 million people\naround the world\
    \ are suffering with critical health issues due to hu-\nmanitarian crises.3 Although\
    \ this work does not extensively discuss\nthe role of social media and computational\
    \ methods to deal with\nhumanitarian health issues, there exist several studies\
    \ examining\nsocial media uses and the advantages of different computational\n\
    techniques in this space. Al-garadi et al. presents a detailed survey\non the\
    \ use of social media to track pandemics that covers a number\nof machine learning\
    \ techniques to process social media data. In\na recent work [44], Twitter data\
    \ has been used during different\nepidemics to find information that is potentially\
    \ useful for various\nhealth organizations.\n6\nCONCLUSIONS AND TRENDS\nSocial\
    \ media platforms add new functionalities and new users ev-\nery day; at the same\
    \ time, some communications are moving to\nplatforms that are oriented to one-on-one\
    \ conversations or small\ngroups instead of broadcasting. Cameras to capture 360-degree\
    \ and\nVirtual Reality (VR) video, as well as UAVs with cameras, are becom-\n\
    ing more affordable and easy to use, expanding the user base that\nproduces these\
    \ contents. In the light of these trends, we could envi-\nsion that different\
    \ types of information and more complex forms\nof information will emerge, creating\
    \ new challenges. We expect\nactive research and new developments in this space\
    \ using machine\nintelligence, human intelligence, or a combination of both.\n\
    3http://www.who.int/hac/donorinfo/highlights/highlights37/en/\nTrack: Journal\
    \ Papers\n \nWWW 2018, April 23-27, 2018, Lyon, France\n510\nREFERENCES\n[1] Mohammed\
    \ Ali Al-garadi, Muhammad Sadiq Khan, Kasturi Dewi Varathan, Ghu-\nlam Mujtaba,\
    \ and Abdelkodose M Al-Kabsi. 2016. Using online social networks\nto track a pandemic:\
    \ A systematic review. Journal of biomedical informatics 62\n(2016), 1–11.\n[2]\
    \ Firoj Alam, Muhammad Imran, and Ferda Ofli. 2017. Image4Act: Online Social\n\
    Media Image Processing for Disaster Response.. In Proc. of ASONAM. 1–4.\n[3] James\
    \ Allan. 2012. Topic detection and tracking: event-based information organi-\n\
    zation. Vol. 12. Springer Science & Business Media.\n[4] Javed Aslam, Fernando\
    \ Diaz, Matthew Ekstrand-Abueg, Richard McCreadie,\nVirgil Pavlu, and Tetsuya\
    \ Sakai. 2015. TREC 2014 temporal summarization track\noverview. Technical Report.\
    \ NIST.\n[5] Marco Avvenuti, Stefano Cresci, Andrea Marchetti, Carlo Meletti,\
    \ and Maurizio\nTesconi. 2014. EARS (earthquake alert and report system): a real\
    \ time decision\nsupport system for earthquake crisis management. In Proc. of\
    \ KDD. ACM, 1749–\n1758.\n[6] Michele Berlingerio, Francesco Calabrese, Giusy\
    \ Di Lorenzo, Xiaowen Dong,\nYiannis Gkoufas, and Dimitrios Mavroeidis. 2013.\
    \ SaferCity: a system for detect-\ning and analyzing incidents from social media.\
    \ In Proc. of ICDM Workshops. IEEE,\n1077–1080.\n[7] Jingwen Bian, Yang Yang,\
    \ and Tat-Seng Chua. 2013. Multimedia summarization\nfor trending topics in microblogs.\
    \ In Proc. of CIKM. ACM, 1807–1812.\n[8] Carlos Castillo. 2016. Big crisis data:\
    \ social media in disasters and time-critical\nsituations. Cambridge University\
    \ Press.\n[9] Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information\
    \ credi-\nbility on twitter. In Proc. of WWW. ACM, 675–684.\n[10] Tao Chen, Dongyuan\
    \ Lu, Min-Yen Kan, and Peng Cui. 2013. Understanding and\nclassifying image tweets.\
    \ In Proc. of ACM MM. ACM, 781–784.\n[11] Soudip Roy Chowdhury, Muhammad Imran,\
    \ Muhammad Rizwan Asghar, Sihem\nAmer-Yahia, and Carlos Castillo. 2013. Tweet4act:\
    \ Using incident-specific profiles\nfor classifying crisis-related messages..\
    \ In Proc. of ISCRAM.\n[12] Courtney D Corley, Chase Dowling, Stuart J Rose, and\
    \ Taylor McKenzie. 2013.\nSocial sensor analytics: Measuring phenomenology at\
    \ scale. In Proc. of ISI. IEEE,\n61–66.\n[13] John Harrald and Theresa Jefferson.\
    \ 2007. Shared situational awareness in\nemergency management mitigation and response.\
    \ In Proc. of HICSS. IEEE, 23–23.\n[14] Amanda L Hughes and Leysia Palen. 2012.\
    \ The evolving role of the public\ninformation officer: An examination of social\
    \ media in emergency management.\nJ. of Homeland Security and Emergency Management\
    \ 9, 1 (2012).\n[15] Amanda L Hughes, Steve Peterson, and Leysia Palen. 2014.\
    \ Social media in\nemergency management. Issues in Disaster Science and Management:\
    \ A Critical\nDialogue Between Scientists and Emergency Managers (2014).\n[16]\
    \ Amanda L Hughes, Lise AA St Denis, Leysia Palen, and Kenneth M Anderson.\n2014.\
    \ Online public communications by police & fire services during the 2012\nHurricane\
    \ Sandy. In Proc. of SIGCHI. ACM, 1505–1514.\n[17] Muhammad Imran and Carlos Castillo.\
    \ 2015. Towards a data-driven approach\nto identify crisis-related topics in social\
    \ media streams. In Proc. of WWW. ACM,\n1205–1210.\n[18] Muhammad Imran, Carlos\
    \ Castillo, Fernando Diaz, and Sarah Vieweg. 2015.\nProcessing social media messages\
    \ in mass emergency: A survey. ACM Computing\nSurveys (CSUR) 47, 4 (2015), 67.\n\
    [19] Muhammad Imran, Carlos Castillo, Ji Lucas, Patrick Meier, and Sarah Vieweg.\n\
    2014. AIDR: Artificial intelligence for disaster response. In Proc. of WWW. ACM,\n\
    159–162.\n[20] Muhammad Imran, Sanjay Chawla, and Carlos Castillo. 2016. A robust\
    \ framework\nfor classifying evolving document streams in an expert-machine-crowd\
    \ setting.\nIn Proc. of ICDM. IEEE, 961–966.\n[21] Muhammad Imran, Prasenjit Mitra,\
    \ and Carlos Castillo. 2016. Twitter as a Life-\nline: Human-annotated Twitter\
    \ Corpora for NLP of Crisis-related Messages. In\nProceedings of the Tenth International\
    \ Conference on Language Resources and Eval-\nuation (LREC 2016) (23-28). European\
    \ Language Resources Association (ELRA),\nParis, France.\n[22] Muhammad Imran,\
    \ Prasenjit Mitra, and Jaideep Srivastava. 2016. Enabling Rapid\nClassification\
    \ of Social Media Communications During Crises. Int. J. of Inf. Sys.\nfor Crisis\
    \ Response and Management 8, 3 (2016), 1–17.\n[23] Chris Kedzie, Kathleen McKeown,\
    \ and Fernando Diaz. 2014. Summarizing disas-\nters over time. In Proc. of KDD\
    \ Workshops.\n[24] Chris Kedzie, Kathleen McKeown, and Fernando Diaz. 2015. Predicting\
    \ Salient\nUpdates for Disaster Summarization.. In Proc. of ACL.\n[25] Yuan Liang,\
    \ James Caverlee, and John Mander. 2013. Text vs. images: on the\nviability of\
    \ social media to assess earthquake damage. In Proc. of WWW. ACM,\n1003–1006.\n\
    [26] Alan M MacEachren, Anuj Jaiswal, Anthony C Robinson, Scott Pezanowski,\n\
    Alexander Savelyev, Prasenjit Mitra, Xiao Zhang, and Justine Blanford. 2011.\n\
    Senseplace2: Geotwitter analytics support for situational awareness. In Proc.\
    \ of\nVAST. IEEE, 181–190.\n[27] Patrick Meier. 2015. Digital humanitarians: how\
    \ big data is changing the face of\nhumanitarian response. CRC Press.\n[28] Leonardo\
    \ Neumeyer, Bruce Robbins, Anish Nair, and Anand Kesari. 2010. S4:\nDistributed\
    \ stream computing platform. In Proc. of ICDM Workshops. IEEE, 170–\n177.\n[29]\
    \ Dat Tien Nguyen, Kamla Al-Mannai, Shafiq R Joty, Hassan Sajjad, Muhammad\nImran,\
    \ and Prasenjit Mitra. 2017. Robust Classification of Crisis-Related Data\non\
    \ Social Networks Using Convolutional Neural Networks.. In Proc. of ICWSM.\n632–635.\n\
    [30] Dat Tien Nguyen, Firoj Alam, Ferda Ofli, and Muhammad Imran. 2017. Automatic\n\
    Image Filtering on Social Networks Using Deep Learning and Perceptual Hashing\n\
    During Crises. In Proc. of ISCRAM.\n[31] Dat Tien Nguyen, Shafiq Joty, Muhammad\
    \ Imran, Hassan Sajjad, and Prasenjit\nMitra. 2016. Applications of Online Deep\
    \ Learning for Crisis Response Using\nSocial Media Information. arXiv preprint\
    \ arXiv:1610.01030 (2016).\n[32] Dat Tien Nguyen, Kamela Ali Al Mannai, Shafiq\
    \ Joty, Hassan Sajjad, Muham-\nmad Imran, and Prasenjit Mitra. 2016. Rapid Classification\
    \ of Crisis-Related\nData on Social Networks using Convolutional Neural Networks.\
    \ arXiv preprint\narXiv:1608.03902 (2016).\n[33] Dat Tien Nguyen, Ferda Ofli,\
    \ Muhammad Imran, and Prasenjit Mitra. 2017. Dam-\nage Assessment from Social\
    \ Media Imagery Data During Disasters. In Proc. of\nASONAM. 1–8.\n[34] Ferda Ofli,\
    \ Patrick Meier, Muhammad Imran, Carlos Castillo, Devis Tuia, Nicolas\nRey, Julien\
    \ Briant, Pauline Millet, Friedrich Reinhard, Matthew Parkan, et al. 2016.\nCombining\
    \ human computing and machine learning to make sense of big (aerial)\ndata for\
    \ disaster response. Big data 4, 1 (2016), 47–59.\n[35] Alexandra Olteanu, Carlos\
    \ Castillo, Fernando Diaz, and Sarah Vieweg. 2014.\nCrisisLex: A Lexicon for Collecting\
    \ and Filtering Microblogged Communications\nin Crises.. In Proc. of ICWSM.\n\
    [36] Robin Peters and Porto de Albuqerque Joao. 2015.\nInvestigating images as\n\
    indicators for relevant social media messages in disaster management. In Proc.\
    \ of\nISCRAM.\n[37] Robert Power, Bella Robinson, John Colton, and Mark Cameron.\
    \ 2014. Emergency\nsituation awareness: Twitter case studies. In Proc. of ISCRAM\
    \ MED. Springer,\n218–231.\n[38] Hemant Purohit and Amit P Sheth. 2013. Twitris\
    \ v3: From Citizen Sensing to\nAnalysis, Coordination and Action.. In Proc. of\
    \ ICWSM.\n[39] Vahed Qazvinian, Emily Rosengren, Dragomir R Radev, and Qiaozhu\
    \ Mei. 2011.\nRumor has it: Identifying misinformation in microblogs. In Proc.\
    \ of EMNLP.\nAssociation for Computational Linguistics, 1589–1599.\n[40] Alan\
    \ Ritter, Oren Etzioni, Sam Clark, et al. 2012. Open domain event extraction\n\
    from twitter. In Proc. of KDD. ACM, 1104–1112.\n[41] Bella Robinson, Robert Power,\
    \ and Mark Cameron. 2013. A sensitive twitter\nearthquake detector. In Proc. of\
    \ WWW. ACM, 999–1002.\n[42] Jakob Rogstadius, Maja Vukovic, CA Teixeira, Vassilis\
    \ Kostakos, Evangelos Kara-\npanos, and Jim Alain Laredo. 2013. CrisisTracker:\
    \ Crowdsourced social media\ncuration for disaster awareness. IBM J. of Research\
    \ and Development 57, 5 (2013),\n4–1.\n[43] Koustav Rudra, Siddhartha Banerjee,\
    \ Niloy Ganguly, Pawan Goyal, Muhammad\nImran, and Prasenjit Mitra. 2016. Summarizing\
    \ Situational and Topical Informa-\ntion During Crises. arXiv preprint arXiv:1610.01561\
    \ (2016).\n[44] Koustav Rudra, Ashish Sharma, Niloy Ganguly, and Muhammad Imran.\
    \ 2017.\nClassifying Information from Microblogs during Epidemics. In Proceedings\
    \ of the\n2017 International Conference on Digital Health. ACM, 104–108.\n[45]\
    \ Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes\n\
    Twitter users: real-time event detection by social sensors. In Proc. of WWW. ACM,\n\
    851–860.\n[46] Diane H Sonnenwald and Linda G Pierce. 2000. Information behavior\
    \ in dynamic\ngroup work contexts: interwoven situational awareness, dense social\
    \ networks\nand contested collaboration in command and control. Inf. Proc. & Mgmt.\
    \ 36, 3\n(2000), 461–479.\n[47] Kate Starbird, Leysia Palen, Amanda L Hughes,\
    \ and Sarah Vieweg. 2010. Chatter\non the red: what hazards threat reveals about\
    \ the social life of microblogged\ninformation. In Proc. of CSCW. ACM, 241–250.\n\
    [48] Sarah Vieweg, Carlos Castillo, and Muhammad Imran. 2014. Integrating social\n\
    media communications into the rapid assessment of sudden onset disasters. In\n\
    Proc. of SOCINFO. Springer, 444–461.\n[49] Sarah Vieweg, Amanda L Hughes, Kate\
    \ Starbird, and Leysia Palen. 2010. Mi-\ncroblogging during two natural hazards\
    \ events: what twitter may contribute to\nsituational awareness. In Proc. of SIGCHI.\
    \ ACM, 1079–1088.\n[50] Jianshu Weng and Bu-Sung Lee. 2011. Event detection in\
    \ twitter. Proc. of ICWSM\n11 (2011), 401–408.\n[51] Shize Xu, Liang Kong, and\
    \ Yan Zhang. 2012. A picture paints a thousand words:\na method of generating\
    \ image-text timelines. In Proc. of CIKM. ACM, 2511–2514.\nTrack: Journal Papers\n\
    \ \nWWW 2018, April 23-27, 2018, Lyon, France\n511\n"
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: http://dl.acm.org/ft_gateway.cfm?id=3186242&type=pdf
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Processing Social Media Messages in Mass Emergency
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.trpro.2023.11.391
  analysis: '>'
  authors:
  - Minh-Huong Le-Nguyen
  - Fabien Turgis
  - Pierre-Emmanuel Fayemi
  - Albert Bifet
  citation_count: 0
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords References Transportation Research Procedia Volume 72,
    2023, Pages 171-178 Real-time learning for real-time data: online machine learning
    for predictive maintenance of railway systems Author links open overlay panel
    Minh-Huong Le-Nguyen a b, Fabien Turgis b, Pierre-Emmanuel Fayemi b, Albert Bifet
    a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.trpro.2023.11.391
    Get rights and content Under a Creative Commons license open access Abstract In
    the railway, an unexpected fault reduces service availability and may cause fatalities.
    Therefore, maintenance must be timely. Nowadays, the omnipresence of onboard sensors
    generates data enormously, thus enabling data-driven predictive maintenance. For
    this, machine learning has come into prominence. Traditionally, a machine learning
    model is trained on a batch of data. However, this approach lags behind on fast-paced
    data streams. For real-time learning on real-time data, we propose a pipeline
    that employs online machine learning to address predictive maintenance of sensorized
    railway systems. We showcase the implementation and experimental results of two
    modules automating data preprocessing to demonstrate the potentials of online
    learning. We also discuss the intuition of another module using online clustering
    to monitor the evolution of system health. Previous article in issue Next article
    in issue Keywords Big dataTransport services and sustainable citiesOnline machine
    learningPredictive maintenanceRailway View PDF References Bengio et al., 2013
    Y. Bengio, A. Courville, P. Vincent Representation Learning: A Review and New
    Perspectives IEEE Transactions on Pattern Analysis and Machine Intelligence, 35
    (2013), pp. 1798-1828, 10.1109/TPAMI.2013.50 View in ScopusGoogle Scholar Canizo
    et al., 2017 M. Canizo, E. Onieva, A. Conde, S. Charramendieta, S. Trujillo Real-time
    predictive maintenance for wind turbines using Big Data frameworks 2017 IEEE International
    Conference on Prognostics and Health Management (ICPHM) (2017), pp. 70-77, 10.1109/ICPHM.2017.7998308
    View in ScopusGoogle Scholar Cao et al., 2006 F. Cao, M. Ester, W. Qian, A. Zhou
    Density-Based Clustering over an Evolving Data Stream with Noise Proceedings of
    the Sixth SIAM International Conference on Data Mining, April 20-22, 2006, , Bethesda,
    MD, USA (2006), 10.1137/1.9781611972764.29 Google Scholar Cohen, 1960 J. Cohen
    A Coefficient of Agreement for Nominal Scales Educational and Psychological Measurement,
    20 (1960), pp. 37-46, 10.1177/001316446002000104 Google Scholar Diez et al., 2016
    A. Diez, N.L.D. Khoa, M. Makki Alamdari, Y. Wang, F. Chen, P. Runcie A clustering
    approach for structural health monitoring on bridges J Civil Struct Health Monit,
    6 (2016), pp. 429-445, 10.1007/s13349-016-0160-0 View in ScopusGoogle Scholar
    Feng et al., 2019 X. Feng, C. Weng, X. He, X. Han, L. Lu, D. Ren, M. Ouyang Online
    State-of-Health Estimation for Li-Ion Battery Using Partial Charging Segment Based
    on Support Vector Machine IEEE Transactions on Vehicular Technology, 68 (2019),
    pp. 8583-8592, 10.1109/TVT.2019.2927120 View in ScopusGoogle Scholar Hochreiter
    and Schmidhuber, 1997 S. Hochreiter, J. Schmidhuber Long Short-Term Memory Neural
    Comput, 9 (1997), pp. 1735-1780, 10.1162/neco.1997.9.8.1735 View in ScopusGoogle
    Scholar Jardine et al., 2006 A.K.S. Jardine, D. Lin, D. Banjevic A review on machinery
    diagnostics and prognostics implementing condition-based maintenance Mechanical
    Systems and Signal Processing, 20 (2006), pp. 1483-1510, 10.1016/j.ymssp.2005.09.012
    View PDFView articleView in ScopusGoogle Scholar Järvelin and Kekäläinen, 2002
    K. Järvelin, J. Kekäläinen Cumulated gain-based evaluation of IR techniques ACM
    Trans. Inf. Syst., 20 (2002), pp. 422-446, 10.1145/582415.582418 View in ScopusGoogle
    Scholar Le Nguyen et al., 2021 M.H. Le Nguyen, F. Turgis, P.-E. Fayemi, A. Bifet
    A Complete Streaming Pipeline for Real-time Monitoring and Predictive Maintenance
    Proceedings of the 31st European Safety and Reliability Conference. Presented
    at the 31st European Safety and Reliability Conference (2021), p. 2119, 10.3850/978-981-18-2016-8_400-cd
    Google Scholar MIMOSA OSA-CBM 2001 MIMOSA OSA-CBM, 2001. Google Scholar Ribeiro
    et al., 2016 R.P. Ribeiro, P. Pereira, J. Gama Sequential anomalies: a study in
    the Railway Industry Mach Learn, 105 (2016), pp. 127-153, 10.1007/s10994-016-5584-6
    View in ScopusGoogle Scholar Sahal et al., 2020 R. Sahal, J.G. Breslin, M.I. Ali
    Big data and stream processing platforms for Industry 4.0 requirements mapping
    for a predictive maintenance use case Journal of Manufacturing Systems, 54 (2020),
    pp. 138-151, 10.1016/j.jmsy.2019.11.004 View PDFView articleView in ScopusGoogle
    Scholar Su and Huang, 2018 C.-J. Su, S.-F. Huang Real-time big data analytics
    for hard disk drive predictive maintenance Computers & Electrical Engineering,
    71 (2018), pp. 93-101, 10.1016/j.compeleceng.2018.07.025 View PDFView articleView
    in ScopusGoogle Scholar Tian et al., 2019 H. Tian, N.L.D. Khoa, A. Anaissi, Y.
    Wang, F. Chen Concept Drift Adaption for Online Anomaly Detection in Structural
    Health Monitoring Proceedings of the 28th ACM International Conference on Information
    and Knowledge Management, CIKM ’19, Association for Computing Machinery, New York,
    NY, USA (2019), pp. 2813-2821, 10.1145/3357384.3357816 View in ScopusGoogle Scholar
    Zhao et al., 2019 Zhao, Y., Nasrullah, Z., Li, Z., 2019. PyOD: A Python Toolbox
    for Scalable Outlier Detection. arXiv:1901.01588 [cs, stat]. Google Scholar Zubaroğlu
    and Atalay, 2021 A. Zubaroğlu, V. Atalay Data stream clustering: a review Artif
    Intell Rev, 54 (2021), pp. 1201-1236, 10.1007/s10462-020-09874-x View in ScopusGoogle
    Scholar Cited by (0) © 2023 The Author(s). Published by Elsevier B.V. Part of
    special issue TRA Lisbon 2022 Conference Proceedings Transport Research Arena
    (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal Edited by Luís de
    Picado Santos, Jorge Pinho de Sousa, Elisabete Arsenio Download full issue Other
    articles from this issue Public-private partnership in high-speed railway infrastructures:
    elements for improvement 2023 Mario González-Medrano, José-María Rotellar-García
    View PDF ADSCENE scenarios data base: Focus on accident data support for validation
    of Automated Driving functions 2023 L. Guyonvarch, …, S. Geronimi View PDF Wide
    bandgap semiconductors enabling highly efficient electrified vehicles 2023 Christoph
    Abart, …, Adrian Lis View PDF View more articles Recommended articles Article
    Metrics Captures Readers: 4 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply.'
  inline_citation: '>'
  journal: Transportation Research Procedia
  limitations: '>'
  pdf_link: null
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: 'Real-time learning for real-time data: online machine learning for predictive
    maintenance of railway systems'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s00521-021-06219-9
  analysis: '>'
  authors:
  - Liang Tan
  - Keping Yu
  - Ali Kashif Bashir
  - Xiaofan Cheng
  - Fangpeng Ming
  - Liang Zhao
  - Xiaokang Zhou
  citation_count: 90
  full_citation: '>'
  full_text: ">\nSPECIAL ISSUE ON IOT-BASED HEALTH MONITORING SYSTEM\nToward real-time\
    \ and efficient cardiovascular monitoring for COVID-19\npatients by 5G-enabled\
    \ wearable medical devices: a deep learning\napproach\nLiang Tan1,2 • Keping Yu3\n\
    • Ali Kashif Bashir4,5 • Xiaofan Cheng1 • Fangpeng Ming1 • Liang Zhao6 •\nXiaokang\
    \ Zhou7\nReceived: 12 October 2020 / Accepted: 8 June 2021 / Published online:\
    \ 4 July 2021\n\x02 The Author(s), under exclusive licence to Springer-Verlag\
    \ London Ltd., part of Springer Nature 2021\nAbstract\nPatients with deaths from\
    \ COVID-19 often have co-morbid cardiovascular disease. Real-time cardiovascular\
    \ disease\nmonitoring based on wearable medical devices may effectively reduce\
    \ COVID-19 mortality rates. However, due to\ntechnical limitations, there are\
    \ three main issues. First, the traditional wireless communication technology\
    \ for wearable\nmedical devices is difﬁcult to satisfy the real-time requirements\
    \ fully. Second, current monitoring platforms lack efﬁcient\nstreaming data processing\
    \ mechanisms to cope with the large amount of cardiovascular data generated in\
    \ real time. Third,\nthe diagnosis of the monitoring platform is usually manual,\
    \ which is challenging to ensure that enough doctors online to\nprovide a timely,\
    \ efﬁcient, and accurate diagnosis. To address these issues, this paper proposes\
    \ a 5G-enabled real-time\ncardiovascular monitoring system for COVID-19 patients\
    \ using deep learning. Firstly, we employ 5G to send and receive\ndata from wearable\
    \ medical devices. Secondly, Flink streaming data processing framework is applied\
    \ to access electro-\ncardiogram data. Finally, we use convolutional neural networks\
    \ and long short-term memory networks model to obtain\nautomatically predict the\
    \ COVID-19 patient’s cardiovascular health. Theoretical analysis and experimental\
    \ results show\nthat our proposal can well solve the above issues and improve\
    \ the prediction accuracy of cardiovascular disease to 99.29%.\nKeywords 5G \x02\
    \ Cardiovascular monitoring \x02 Deep learning \x02 Flink \x02 CNN \x02 LSTM\n\
    1 Introduction\nCOVID-19 is the ﬁrst global coronavirus pandemic that\nhumanity\
    \ has ever faced, and awareness of COVID-19 is\nstill growing. In addition to\
    \ advanced medical technology,\nbig data plays a crucial role in the prevention\
    \ and control of\nCOVID-19 [1, 2]. In the real-time environment constituted\n\
    by the Internet of Things (IoT), not only sensors and\nmobile devices have generated\
    \ a large amount of data, but\nalso software applications, web, and other resources\
    \ have\ngenerated massive data [3, 4]. Especially in medical and\nhealthcare,\
    \ due to the emergence of a large number of\nwearable medical devices, the data\
    \ generated by these\ndevices need to be collected remotely in real time. How-\n\
    ever, it brought many new challenges. One of the most\ncritical challenges is\
    \ to extract streaming data in real time\nand process and analyze different types\
    \ of data [5].\nThe death of COVID-19 patients is often accompanied\nby underlying\
    \ cardiovascular and other diseases. Cardio-\nvascular\ndiseases\ncharacterized\n\
    by\nsuddenness\nhave\nbecome one of the main diseases threatening human health\n\
    [6]. A large number of medical practices show that when\nthe heart suddenly stops,\
    \ the best rescue time is within 4\nminutes. At the same time, we found that if\
    \ we can detect\nsubtle signs in advance and take effective measures, 70%\nof\
    \ cardiovascular disease patients, including heart attacks,\ncan avoid death.\
    \ Currently, electrocardiogram (ECG) is the\nmost straightforward and efﬁcient\
    \ clinical examination\nmethod for all kinds of cardiovascular and cerebrovascular\n\
    diseases. Therefore, real-time and effective monitoring,\nanalysis, and diagnosis\
    \ of the COVID-19 patient’s ECG\nsignal are particularly important [7]. Moreover,\
    \ the com-\nmunication between doctors and COVID-19 patients is not\naccurate,\
    \ convenient, and timely, such as telephone and\ntext communication. Although\
    \ doctors and hospitals can\nobserve\nthe\nhealth\nof\nCOVID-19\npatients\nthrough\n\
    Extended author information available on the last page of the article\n123\nNeural\
    \ Computing and Applications (2023) 35:13921–13934\nhttps://doi.org/10.1007/s00521-021-06219-9\n\
    (0123456789().,-volV)(0123456789().\n,- volV)\ninspections, this traditional hospital-based\
    \ diagnosis and\ntreatment lack immediacy and continuity, and it is difﬁcult\n\
    to capture the COVID-19 patients’ signs. Therefore, it is of\npractical signiﬁcance\
    \ to provide real-time ECG diagnosis\nand predict services for those in need [8].\n\
    With wearable medical devices (such as heart rate\nmonitoring cuffs, blood pressure,\
    \ blood glucose meters,\netc.), it can continuously track COVID-19 patients’ health\n\
    and provide personalized healthcare solutions [9]. How-\never, the data continuously\
    \ generated by wearable medical\ndevices need not only real-time processing but\
    \ also diag-\nnosis and prediction. Existing wireless communication\ntechnologies\
    \ have problems such as high latency and low\nspeed. Moreover, relying on manual\
    \ recognition of elec-\ntrocardiogram diagnosis methods can no longer meet the\n\
    current medical needs. Simultaneously, traditional machine\nlearning-based methods\
    \ need to manually extract electro-\ncardiogram features, which cannot realize\
    \ the automation\nof the diagnosis process and the accuracy of the results.\n\
    Therefore, it is a signiﬁcant challenge to process a large\namount of data generated\
    \ by sensors and diagnose them in\nreal time in a critical situation [10].\nGiven\
    \ the aforementioned difﬁculties with traditional\ncardiovascular diseases diagnoses\
    \ and wearable device\ndata processing and analysis, the automatic classiﬁcation\n\
    and diagnosis of assisted ECG signals for COVID-19\npatients based on deep learning\
    \ is an effective solution to\nthe above problems. In addition, with the emergence\
    \ of 5G\n[11, 12], a solution with high throughput and low latency\nhas been provided\
    \ for processing a large amount of mon-\nitoring data [13]. Moreover, the development\
    \ of big data\ntechnology also provides many open source platforms for\nreal-time\
    \ processing of streaming data, such as Spark,\nDruid, and Flink. With the continuous\
    \ advancement of\nartiﬁcial intelligence technology, the application of artiﬁ-\n\
    cial intelligence technology to medical diagnosis is a\ngeneral trend [14]. In\
    \ recent years, in order to meet the\nneeds of high-speed and high-precision ECG\
    \ analysis, deep\nneural networks have been widely used in automatic ECG\ndiagnosis\
    \ [15].\nThis article proposes a real-time cardiovascular moni-\ntoring system\
    \ for COVID-19 patients based on 5G and\ndeep learning with the assist of wearable\
    \ medical devices\nthat can transmit human ECG signal data. The Second 2\nintroduces\
    \ related work. The overall architecture of the\nreal-time monitoring system is\
    \ proposed in Sect. 3. Sec-\ntion 4 is the ECG signal classiﬁcation algorithm\
    \ based on\ndeep learning. In Sect. 5, we put forward the experimental\nmethod\
    \ and evaluation. Finally, Sect. 6 summary the\nconclusion.\n2 Related works\n\
    In recent years, big data analysis related to healthcare has\nbecome an important\
    \ issue in many research ﬁelds, such as\nmachine learning, deep learning, and\
    \ data mining using\nmedical and health data and information available in hos-\n\
    pitals. The progress of the data collection process comes\nfrom the tremendous\
    \ development of technology in the\nmedical and health ﬁeld, in which data records\
    \ are col-\nlected through three main stages of digital data ﬂow gen-\nerated\
    \ from patient clinical records, health research records\nand organization operations\
    \ [16]. Analyzing these data for\ncomputer-aided diagnosis and then developing\
    \ real-time\nsystems has become the development trend of today’s\nsmart medical\
    \ care.\nSun and Reddy et al. [17] discussed an overview of\nhealth care data\
    \ sources. This research analyzes that health\ncare data plays a very important\
    \ role in many systems such\nas disease prediction, prevention methods, medical\
    \ guid-\nance, and emergency medical decision-making to improve\nhealth, reduce\
    \ costs and increase efﬁciency. And in the\nexisting research, a variety of Spark\
    \ machine learning\nmodels have been used in medical databases. For example,\n\
    in [18], a real-time health prediction system using spark\nmachine learning streaming\
    \ big data is introduced. The\nsystem is tested on tweets of users with health\
    \ attributes.\nThe system receives the tweets, extracts features, and uses\ndecision\
    \ tree algorithms to predict The health of the user\nand ﬁnally, sends the information\
    \ directly to the user to\ntake appropriate action. In addition, Alottaibi et\
    \ al. [19]\nproposed a Sehaa-Kingdom of Saudi Arabia (KSA) Central\nArab Healthcare\
    \ Twitter data big data analysis tool. The\nsystem uses two different machine\
    \ learning algorithms,\nincluding naive Bayes and logistic regression algorithms,\n\
    and applies multiple feature extraction methods to detect\nvarious diseases in\
    \ KSA. In [20], a system based on\nApache Spark that can predict heart disease\
    \ in real time\nuses memory computing to apply machine learning to\nstreaming\
    \ data. The system is divided into two stages. The\nﬁrst stage is to apply classiﬁcation\
    \ algorithms to data for\nheart disease prediction through Spark MLlib and Spark\n\
    streaming, and the second stage is to use Apache Cassandra\nto store massively\
    \ generated data and visualizations.\nIn addition, many systems that use wearable\
    \ devices to\ncollect data to predict cardiovascular disease have been\nproposed\
    \ in recent years [21, 22]. Al-Makhadmeh and\nTolba et al. [23] proposed a heart\
    \ disease detection system\nbased on wearable medical equipment. The system trans-\n\
    mits the collected patient heart data to the medical system\nand then uses feature\
    \ extraction technology and deep\nlearning model Value feature extraction and\
    \ correct clas-\nsiﬁcation. Lin et al. [24] proposed a system based on\n13922\n\
    Neural Computing and Applications (2023) 35:13921–13934\n123\nSupport Vector Machine\
    \ (SVM) classiﬁer to predict\npatients suffering from left ventricular hypertrophy.\
    \ The\nsystem collects young people’s age, height, weight and\nelectrocardiogram\
    \ data for rapid diagnosis. Khade et al.\n[25] proposed a cardiovascular disease\
    \ prediction system\nbased on SVM and Convolutional Neural Networks\n(CNN). The\
    \ system sends ECG signals to the SVM clas-\nsiﬁer and Boosted Decision Tree to\
    \ classify cardiovascular\ndiseases and then uses CNN to predict severe Degree,\
    \ and\nthe accuracy rate is 88.3%. Zhao et al. [26] proposed a\nsystem that uses\
    \ CNN to detect the original ECG signal in a\nwearable device. This system avoids\
    \ the traditional manual\nfeature extraction process and implements the system\n\
    based on the cloud. In addition, Kumar et al. [27] proposed\na three-layer framework\
    \ system with an ML model to\nreceive data from wearable devices and perform analysis\n\
    and processing. The ﬁrst layer collects ECG data from\nwearable devices. The second\
    \ layer stores healthcare data\nin the cloud, and the third layer uses logistic\
    \ regression\nalgorithms to predict cardiovascular diseases.\nMoreover, many researchers\
    \ have used deep learning in\nthe ﬁeld of myocardial infarction detection and\
    \ coronavirus\ndetection. M Hammad et al. [28] proposed an automatic\ndetection\
    \ method for myocardial infarction based on the\nend-to-end model of a deep convolutional\
    \ neural network\nand used the focal loss function optimization model for\ndata\
    \ imbalance, and ﬁnally reached 98.84% on the PTB\ndata set. [29] provides a promising\
    \ solution by proposing a\nCOVID-19 detection system based on deep learning. The\n\
    simulation results reveal that the proposed deep learning\nmodalities can be considered\
    \ and adopted for quick\nCOVID-19 screening; [30] propose a modiﬁed version of\n\
    Fuzzy C-Means for segmenting 3D medical volumes,\nwhich has been rarely implemented\
    \ for 3D medical image\nsegmentation; in [31], two new quantum information hid-\n\
    ing methods are proposed for telemedicine image sharing.\nThe results show that\
    \ the proposed methods have excellent\nvisual quality, high embedding capability\
    \ and security.\nMost researches rely on speciﬁc medical care data\nsources and\
    \ apply them on ofﬂine systems. However,\nmedical data sources are diverse and\
    \ new data are con-\nstantly being produced. Real-time healthcare analysis\ninvolves\
    \ real-time streaming data processing, machine\nlearning algorithms, and real-time\
    \ analysis, while tradi-\ntional data transmission has defects such as high latency\n\
    and low throughput. In addition, machine learning algo-\nrithms rely on manual\
    \ extraction for feature extraction.\nThere are limitations in its effective feature\
    \ extraction and\nfeature weighting methods, which limits the classiﬁcation\n\
    ability to a certain extent. Therefore, we use 5G technology\n[32, 33] and deep\
    \ learning [34, 35] to propose a real-time\nmedical monitoring system for cardiovascular\
    \ diseases,\nwhich is used to process real-time data streams transmitted\nfrom\
    \ wearable devices to predict the health of patients in\nreal time and send timely\
    \ information to patients.\n3 The online prediction system\nThe real-time online\
    \ health monitoring system mainly uses\nopen source frameworks such as Kafka,\
    \ Flink, and Ten-\nsorﬂow to realize the transmission of personal data and the\n\
    construction of monitoring models. The system can obtain\nthe COVID-19 patients’\
    \ core information through wearable\ndevices, namely ECG signals, analyze the\
    \ data with deep\nlearning algorithms in Cloud, and predict the COVID-19\npatients’\
    \ health risk. The overall structure diagram is\nshown in Fig. 1, which consists\
    \ of four modules. The ﬁrst\nmodule is responsible for data acquisition and transmis-\n\
    sion. The second module is responsible for streaming\nstorage and analysis, and\
    \ the third module is responsible\nfor the training module of deep learning. The\
    \ fourth stage\nis the module responsible for health diagnose and predict\nfor\
    \ Covid-19 patients.\n3.1 Details of each module\nAcquisition and transmission\
    \ mainly use acquisition\nequipment, 5G network infrastructure and other facilities.\n\
    Details are as follows:\n(1)\nAcquisition equipment It is composed of various\n\
    sensor modules, controllers, processors and power\nsupplies embedded in wearable\
    \ devices. The main\nfunction is to complete the collection, processing and\n\
    transmission of ECG and Global Positioning System\n(GPS) positioning data. The\
    \ selection of these\nmodules needs to meet the needs of usability and\nwearability,\
    \ so as to make the wearer feel comfort-\nable and move as much as possible. The\
    \ photoelec-\ntric\nsensor,\nECG\nsensor\nand\nGPS\nsensor\nare\nintegrated into\
    \ the microcontroller. The sensor\nconverts the collected electrical signal into\
    \ an analog\nelectrical signal, which is processed by the controller\nand becomes\
    \ a digital signal, which is transmitted to\nthe handheld device of the individual\
    \ user through\nthe wireless sensor module. The positioning infor-\nmation and\
    \ ECG information obtained by the\nhandheld device can be uploaded to the cloud\
    \ server\nthrough the 5G network.\n(2)\n5G infrastructure 5g network is a digital\
    \ cellular\nnetwork, including supplier service area covered by a\nseries of small\
    \ geographic area called cellular. The\nanalog signal with the ECG information\
    \ is digitized\non a handheld device, and an analog-to-digital\nconverter turns\
    \ the digitized information into a\nNeural Computing and Applications (2023) 35:13921–13934\n\
    13923\n123\nbitstream for transmission. In the handheld device’s\nnetwork, 5g\
    \ network local antenna array and auto-\nmatic sending and receiving devices with\
    \ low-power\nconsumption and handheld devices to communicate\nby radio waves [36].\
    \ The frequency of the commu-\nnication channel is assigned by the public pool\n\
    selection frequency. 5G infrastructure includes 5G\naccess network and 5G core\
    \ network. Its main\npurpose is to provide high-bandwidth, low-latency\ncommunications.\
    \ Compared with the traditional\nnetwork, 5G network has higher access rate and\n\
    lower delay. It can meet the access requirements of\nultra-high trafﬁc density,\
    \ ultra-high connection num-\nber density and ultra-high mobility. It also improves\n\
    the spectrum efﬁciency of the network, and reduces\nthe operation and maintenance\
    \ costs while improving\nthe network energy efﬁciency. it is very suitable for\n\
    the real-time system environment of this article.\n(3)\nAccess to 5G infrastructure\
    \ Due to the low power\nconsumption, the wearable device adopts a Bluetooth\n\
    connected to a handheld terminal with 5G connection\nfunction (such as a 5G mobile\
    \ phone), and the data\ncollected by the wearable device are uploaded to the\n\
    cloud after connecting to the 5G network through the\nhandheld terminal.\nStreaming\
    \ storage and reading stageThe framework is\nmainly divided into two stages, the\
    \ speciﬁc introduction is\nas described in next section. In the ﬁrst stage, the\
    \ message\nqueue collects information from different COVID-19\npatients, including\
    \ the input of source data and the con-\nsumption of source data. The second stage\
    \ is the stream\nprocessing pipeline, in which the Flink stream receives the\n\
    monitoring data stream related to the ECG attribute, and\nthen adopts a batch\
    \ and stream integrated (streaming for a\nsingle COVID-19 patient, and a batch\
    \ for the entire\nCOVID-19 patients). Perform feature engineering and\nfeature\
    \ selection operations on the data stream, and wait for\ndata storage and model\
    \ training.\nThe training framework of the ECG risk modelIt is\nmainly responsible\
    \ for receiving preprocessed input data,\nonline learning and notiﬁcation of whether\
    \ to update our\ndeployed model through the message middleware accord-\ning to\
    \ the newly obtained data stream. The initial model of\nthe model is obtained\
    \ through training by inputting labeled\ndata. Consider the complexity of the\
    \ model and the con-\ntinuity of ECG data. We use the CNN?LSTM module as\nthe\
    \ bottom layer of the model, and the detailed training\nmethod is in Section IV.\
    \ The trained model waits for the\nmonitoring of the monitoring module to achieve\
    \ the pur-\npose of optimization.\nECG monitoring and risk inference moduleIn\
    \ order to\nimprove the optimization of the model, we have added a\nregistered\
    \ monitoring middleware to the monitoring mod-\nule. The middleware mainly has\
    \ two functions:\n(1)\nWhether the value of the data in the monitoring\nsystem\
    \ has expired. The time from data entering the\nsystem to the model making a risk\
    \ judgment is much\nshorter than the processing time after an ECG\naccident occurs,\
    \ thereby ensuring the real-time\nnature of the system.\n(2)\nAnother is to monitor\
    \ whether the model changes.\nWhen the model changes, the judgment module is\n\
    notiﬁed to verify whether the model is optimized. So\nas to achieve the purpose\
    \ of real-time perfecting the\nmodel. Finally, when the model is perfected this\n\
    time, the output stream is sent back to Kafka again,\nand the monitoring module\
    \ will determine whether\nthere is an abnormality in the COVID-19 patients’\n\
    data stream. For special abnormal information, the\nalarm function of the system\
    \ will be triggered. For\nFig. 1 Architecture design of Cardiovascular Monitoring\
    \ System for COVID-19 Patients\n13924\nNeural Computing and Applications (2023)\
    \ 35:13921–13934\n123\nnormal prediction results, the inference module will\n\
    push the result predictions to the data center for\nstorage. In this way, through\
    \ the personal client,\nCOVID-19 patients can obtain personal monitoring\ninformation\
    \ and personalized health reports in real\ntime.\n3.2 Details of streaming data\n\
    The process of collecting data sets by physical equipment\nis often real time\
    \ and continuous. The collected data\nmainly include COVID-19 patient personal\
    \ information,\nECG information, positioning information, network delay\nmonitoring\
    \ information, etc. The device processes the data\nset as time passes, that is\
    \ to say, it is transmitted to the\nserver receiving end through the handheld\
    \ terminal in the\nform of streaming data. The whole service adopts the\nClient/Server\
    \ method for individual users, and the receiver\naccepts data through the Transmission\
    \ Control Protocol\n(TCP). However, directly receiving and processing this\nkind\
    \ of high-throughput data will cause huge pressure on\nnetwork resources, so it\
    \ is necessary to buffer the data\nwhen receiving the data. The message queue\
    \ is used as a\nbuffering solution to effectively solve the problem of data\n\
    inconsistency. As a high-throughput distributed messaging\nsystem, Kafka can support\
    \ real-time data processing and\nprovide real-time data to the next processor,\
    \ so Kafka is\nselected as a data transmission tool.\nSecondly, a large number\
    \ of iterative calculations will\nbe generated in the data processing calculation\
    \ process, and\nthe processor is required to be able to efﬁciently support\niterative\
    \ data processing in real time. Choose Flink with\nhigher-level APIs and better\
    \ benchmark results [37]. Flink\nprovides a wealth of APIs. At the same time,\
    \ the integra-\ntion with Druid and Redis is already quite high. We will\nchoose\
    \ Redis to store the ﬁnal calculation results. Below\nwe will elaborate on the\
    \ process of streaming data from the\nperspective of Kafka and Flink frameworks.\
    \ The details of\nstreaming data are shown in Fig. 2.\n3.2.1 Kafka data stream\
    \ processing\nInput of source data The physical device uses the hand-\nheld device\
    \ to ﬁrst send to the streaming storage and\nreading framework via TCP, and then\
    \ TCP binds the IP and\nport number to start the data receiving thread. After\n\
    receiving the data, the system will store the source data in\nthe datasource.\
    \ In order to effectively analyze and process\nthe original data, the system needs\
    \ to extract, transform,\nand load (ETL) a large amount of original data to the\
    \ target\nstorage data warehouse. In the face of a large number of\nCOVID-19 patient\
    \ data, a buffer queue is set up from the\nsource data to the data warehouse,\
    \ namely kafka.\n(1)\nAt this stage, Kafka will create topics through Linux\n\
    commands. Topics are created according to the data\ntypes (UserInfo, ECGInfo,\
    \ GPSInfo). Second, con-\nﬁgure the properties of the producer. Finally, various\n\
    objects are initialized to complete the conﬁguration\nof Kafka. The conﬁgured\
    \ objects include serialized\nobjects, partitioners, acks objects, etc.\n(2)\n\
    When sending data to the Kafka cluster, in order to\nachieve Kafka ﬂow card control,\
    \ the system needs to\ncreate a buffer area for the coming data. After setting\n\
    the buffer data size, when the buffer area is full, it\nwill be sent to Kafka\
    \ uniformly.\n(3)\nIn data parsing, ﬁrst call the dataParser method,\nobtain the\
    \ data type according to the data type\nposition in the data frame deﬁnition,\
    \ and record the\ndata type. At the same time, if the whole frame of\ndata is\
    \ all 0 during the analysis, it means that the\nframe data is invalid data, and\
    \ the frame data is\nskipped. After analyzing the data type, enter the\nanalysis\
    \ function of different data items according to\nthe data type, analyze the whole\
    \ data, and store it in\nthe form of an object.\n(4)\nWhen each test item creates\
    \ the ProducerRecord\nobject, the source data needs to be reorganized to\nform\
    \ a processing form in Flink. There are two\nobjects involved in sending messages,\
    \ KafkaPro-\nducer and ProducerRecord. ProducerRecord speciﬁes\nthe topic information\
    \ that needs to be sent, the\nmessage content value, and can also specify partition\n\
    information and key values. After the partition is\nselected, the producer can\
    \ determine which topic and\nwhich partition to send the message to. Inside the\n\
    Producer, a separate thread will send the record to\nthe corresponding broker.\n\
    (5)\nAfter the Kafka server successfully receives the\nmessage sent by the Producer,\
    \ it will respond with a\nresponse. If the message is written successfully, it\n\
    will return a RecordMetaData object. If it fails, the\nProducer will resend the\
    \ message. If it fails after a\nfew times, it will return an error message.\n\
    Consumption of source data After Kafka processes the\ndata, consumers need to\
    \ subscribe to the message and read\nthe data in Kafka. In this article, Kafka\
    \ consumers are the\ntarget storage data warehouse and the Flink clusters. In\n\
    Flink, Kafka’s Partition needs to correspond to Flink’s\nparallel task instance.\
    \ Flink can also guarantee that even\nafter a failure, the allocation of partitions\
    \ to Flink instances\ncan be maintained, so partition determinism is maintained,\n\
    which ensures that data processing is exactly at once. This\neffectively guarantees\
    \ the integrity of the system data.\nNeural Computing and Applications (2023)\
    \ 35:13921–13934\n13925\n123\n3.2.2 Flink data stream processing\nAfter conﬁrming\
    \ that the Kafka consumer is Flink, it is\nnecessary to create a Flink-Kafka consumer\
    \ object. The\nrealization of a data processing process based on Flink\nmainly\
    \ includes the following ﬁve steps. First, you need to\nobtain the execution environment\
    \ for data processing, and\nthen load the initial data. After the data are loaded\
    \ into\nFlink, specify the data conversion method, that is, to realize\nthe speciﬁc\
    \ data processing process, that is, the speciﬁc\ndeep learning-based classiﬁcation\
    \ method in Sect. 4. In\naddition, the model update is also based on the data\
    \ col-\nlection situation within a period of time to update the\nmodel, the notice\
    \ service will let the new model act on the\nsystem’s prediction. After the processing\
    \ is completed,\nspecify the storage location of the data calculation result.\n\
    Since Flink is lazy loaded, after the above steps are deﬁned,\nthe execution of\
    \ the program needs to be triggered at the\nend before the Fink cluster will start\
    \ processing data. For\npatients detected as suspected of COVID-19 patient, the\n\
    system will also alert management personnel to deal with\nsuch situations.\n4\
    \ Cardiovascular disease classification\nalgorithm based on Deep Learning\n4.1\
    \ Data preprocessing\nThe ECG signal is a weak electrical signal, which is easily\n\
    interfered by electrical signals from various other sources,\nincluding baseline\
    \ drift, EMG interference and power fre-\nquency interference. In order to obtain\
    \ a truly useful ECG\nsignal, we need to denoise these high-frequency or low-\n\
    frequency noises. In addition, a complete ECG is com-\nposed of a long continuous\
    \ time series. If it is directly input\ninto the neural network classiﬁcation,\
    \ it will greatly\nincrease the complexity of the network calculation, and it\
    \ is\nnot conducive to extracting good feature information.\nTherefore, in the\
    \ study of ECG signal classiﬁcation, the\nentire ECG signal is usually divided\
    \ into several small\nsegments in units of heart beats according to speciﬁc rules.\n\
    The denoising of ECG signals and heartbeat segmentation\nare collectively referred\
    \ to as preprocessing.\n4.1.1 ECG signal denoising\nThe ECG signal contains a\
    \ variety of different types of\nhigh-frequency or low-frequency noise. The purpose\
    \ of\ndenoising the ECG signal is to suppress the noise in the\nsignal, and to\
    \ enhance the part that can contribute to feature\nextraction. In order to obtain\
    \ useful signals, this paper uses\ndiscrete wavelet transform (DWT) to process\
    \ ECG signals.\nBecause the tightness of the ECG signal processing has a\nFig.\
    \ 2 ECG data ﬂow process\n13926\nNeural Computing and Applications (2023) 35:13921–13934\n\
    123\ngreater impact on the signal, Daubechies wavelet is more\nsuitable. In this\
    \ paper, DB8 wavelet basis is selected to\ndecompose the ECG signal into 8 layers,\
    \ and the coefﬁ-\ncients of each layer are obtained. The frequency difference\n\
    of the three noises of ECG signal is relatively large, so the\nsoft threshold\
    \ processing method is selected, that is, dif-\nferent thresholds are used for\
    \ quantization processing at\ndifferent transform scales. Finally, the ECG signal\
    \ is\nreconstructed according to the low-frequency coefﬁcients\nof the 8th layer\
    \ obtained by DB8 wavelet decomposition\nand the high-frequency coefﬁcients of\
    \ each layer, and the\ndenoised ECG signal is obtained. The evaluation method\
    \ of\nthe denoising effect of the ECG signal is reﬂected by the\nsignal-to-noise\
    \ ratio (SNR) and the mean square error\n(MSE). The larger the SNR value, the\
    \ less noise the\ndenoised ECG signal contains and the better the denoising\n\
    effect; the smaller the MSE, the smaller the degree of\ndistortion of the ECG\
    \ signal. The calculation method is\nshown in formula (1) and (2) where s(i) represents\
    \ the\noriginal ECG signal containing noise, x(i) represents the\ndenoised ECG\
    \ signal, and N represents the length of the\ncollected ECG signal.\nSNR ¼ 10\
    \ \x02 lg\nXN\nn¼1\nsðiÞ2\n½xðiÞ \x03 sðiÞ\x042\nð1Þ\nMSE ¼\nPN\nn¼1 ½xðiÞ \x03\
    \ sðiÞ\x042\nN\nð2Þ\n4.1.2 Heartbeat segmentation\nAfter the ECG signal is denoised,\
    \ the R peak in the\nwaveform needs to be located and segmented. For the R\npeak\
    \ detection task, combined with the real-time require-\nments of ECG signal detection,\
    \ the current mainstream R\npeak positioning method P-T algorithm is used [38].\n\
    According to the characteristics of ECG signals, the algo-\nrithm uses the characteristics\
    \ of large slope of QRS com-\nplexes, and searches for the peak value of R waves\
    \ in ECG\nsignals through differentiation and adaptive threshold\nmethods to achieve\
    \ positioning effect. After the detection\nof the R peak position is completed,\
    \ the heart beat seg-\nmentation of the entire ECG signal is performed, and the\
    \ R\npeak is taken as the reference position, and the forward and\nbackward intercepts\
    \ are performed, respectively. The\nintercepted length is at least a complete\
    \ heartbeat, and a\ncomplete heartbeat is about 0.6s \x05 0.8s, so the sampling\n\
    point must be greater than 360*0.8 = 288. Too many\nsampling points will cause\
    \ correlation interference between\ndifferent types of waveforms. This paper selects\
    \ 300\nsampling points, divides all the ECG signals in MIT-BIH,\nand normalizes\
    \ each collected heartbeat, limiting the\namplitude of each heartbeat to \x03\
    1; 1\n½\n\x04.\n4.1.3 Data enhancement\nECG classiﬁcation is an imbalance problem.\
    \ Abnormal\nheartbeats are much smaller than normal heartbeats.\nBecause deep\
    \ learning has a strong ability to express and\nexplain, it is difﬁcult for the\
    \ model to learn a small number\nof sample features during the training process,\
    \ which\nmakes the model invalid in practical applications. There-\nfore, less\
    \ data must be enhanced to solve the imbalance\nproblem. This paper adopts the\
    \ Synthetic Minority Over-\nsample Technique (SMOTE) oversampling algorithm pro-\n\
    posed by Chawla in 2002 [39]. The idea of this algorithm is\nto synthesize new\
    \ minority samples through a certain\nstrategy. The synthesis strategy is for\
    \ each minority sample\na, A sample b is randomly selected from its nearest\n\
    neighbor, and then a point on the line between a and b is\nrandomly selected as\
    \ the newly synthesized minority\nsample. Therefore, this paper uses the algorithm\
    \ to syn-\nthesize new sample data from the similarities between the\nexisting\
    \ minority heartbeat samples. The SMOTE algo-\nrithm is shown in formula (3).\n\
    Xnew ¼ Xi þ randð0; 1Þ \x06 ðXi;j \x03 XjÞ\nð3Þ\nwhere rand(0, 1) represents the\
    \ random number generated\nbetween 0 and 1, Xnew represents the newly generated\n\
    heartbeat samples of S, V, F and Q, Xi represents the ith\nheartbeat sample in\
    \ the minority class, Xi;j represents the\nheartbeat sample b in the ith neighborhood\
    \ of the ith\nheartbeat sample a in the minority class.\n4.2 Convolutional neural\
    \ networks\nCNN is a kind of multilayer neural network used for image\nclassiﬁcation,\
    \ segmentation or detection that has developed\nrapidly in recent years [40, 41].\
    \ It inherits the unique fea-\nture extraction capabilities of deep learning.\
    \ Because of its\nadvantages such as local connection, weight sharing, and\ndown-sampling,\
    \ it effectively reduces the number of\nparameters in the neural network structure\
    \ with a large\namount of data, reduces the complexity of the operation,\nand\
    \ reduces the memory of the operation. CNN is con-\nstructed by different combinations\
    \ of input layer, hidden\nlayer and output layer. The hidden layer usually includes\
    \ a\nconvolutional computing layer, a pooling computing layer,\nand a fully connected\
    \ layer. The network structure of 1D-\nCNN is shown in Fig. 3.\nThe convolution\
    \ operation layer is to perform convo-\nlution operation on the input sequence\
    \ or picture, and the\npurpose is to extract the characteristics of the input\
    \ signal.\nConvolution operation includes convolution operation and\nactivation\
    \ function. The convolution operation is to mul-\ntiply a set of weights with\
    \ the input, expressed in matrix\nform as formula (4), where X represents the\
    \ matrix\nNeural Computing and Applications (2023) 35:13921–13934\n13927\n123\n\
    representation of the input signal and W is the convolution\nkernel. The size\
    \ and number of the weight matrix can be\ncustomized by experience; \x06 means\
    \ convolution. After the\nconvolution operation, the result needs to be determined\n\
    and transformed by the activation function, and the output\nthat reaches the threshold\
    \ is mapped to another space\nthrough nonlinear changes, and then the features\
    \ can be\nnonlinearly classiﬁed.\nSðtÞ ¼ ðX \x06 WÞðtÞ\nð4Þ\nThe pooling operation\
    \ layer is to down-sample the features\nextracted by the convolutional layer.\
    \ This process can\nretain important pair of feature information without\nchanging\
    \ the number of feature maps. In this way, the\nmodel reduces spatial information\
    \ to obtain better com-\nputing\nperformance\nand\nreduces\nthe\nrisk\nof\nmodel\n\
    overﬁtting.\nThe ﬁnal classiﬁer in the entire CNN network is\nimplemented in the\
    \ fully connected layer, and the output\nafter convolution and pooling is ﬂattened\
    \ into a single\nvalue vector, and the probability calculation is performed\n\
    through the softmax function to obtain the ﬁnal category\noutput. The softmax\
    \ function converts the input into a\nprobability value ranging from 0 to 1, and\
    \ the sum of all\nprobability values is 1.\nThis paper uses a network composed\
    \ of multiple con-\nvolutional layers and pooling layers to extract features of\n\
    the ECG signal. Feature extraction technology can replace\nmanual labor, avoiding\
    \ the inaccuracy of feature selection\ndue to human reasons and saving a lot of\
    \ time spent on\nfeatures.\n4.3 Long short-term memory networks\nLong short-term\
    \ memory network (LSTM) is a variant of\nrecurrent neural network. The traditional\
    \ RNN hidden layer\nis used as a memory unit. As the model progresses over\ntime,\
    \ the effective information of the input data is gradu-\nally weakened. The LSTM\
    \ redesigned the memory module\nto retain the backpropagation error between the\
    \ time step\nand the level, so that the network model continues to\nmaintain the\
    \ learning state in multiple time steps and thus\nhas the ability to capture the\
    \ causality of long-distance\ninformation. Therefore, LSTM is suitable for ECG\
    \ signals\nwith timing characteristics.\nThe core of LSTM is composed of input\
    \ gate, output\ngate and forget gate. These three control gates can enable\nLSTM\
    \ neurons to read, write, reset and update long-dis-\ntance historical information.\
    \ The structure diagram is\nshown in Fig. 4. The overall calculation formula is\
    \ as\nfollows.\nLSTM realizes the selective loss of information in\nneurons through\
    \ the forget gate in the structure, and the\ncalculation formula is as (5).\n\
    ft ¼ rðWxf xt þ Whf ht\x031 þ bf Þ\nð5Þ\nwhere Wf represents the weight matrix\
    \ of the forgetting\ngate, ht\x031 is the previous output in the network, xt repre-\n\
    sents the current input, bf represents the bias term of the\nforgetting gate,\
    \ and rð\x02Þ represents the sigmoid function.\nThe output of the input gate consists\
    \ of two parts. The\ncalculation formulas are as (6) and (7). The output of the\n\
    input gate is composed of two parts, it represents the cur-\nrent output, which\
    \ is realized by the sigmoid function, and\n~\nCt represents the current state,\
    \ which is realized by the tanh\nfunction.\nit ¼ rðWxixt þ Whiht\x031 þ biÞ\n\
    ð6Þ\n~\nCt ¼ tanh ðWxcxt þ Whcht\x031 þ bcÞ\nð7Þ\nAmong them, it represents the\
    \ current output, realized by\nthe sigmoid function,\n~\nCtt represents the current\
    \ state,\nrealized by the tanh function, Wð\x02Þ represents the weight\nmatrix\
    \ of this part, and b\x02 represents the bias term of this\npart.\nThe output\
    \ gate calculation formulas are as (8), (9) and\n(10).\nFig. 3 1D-CNN Network\
    \ Structure\nFig. 4 LSTM Network Structure\n13928\nNeural Computing and Applications\
    \ (2023) 35:13921–13934\n123\nCt ¼ ft \x02 Ct\x031 þ it \x02 ~\nCt\nð8Þ\not ¼\
    \ rðWxoxt þ Whoht\x031 þ boÞ\nð9Þ\nht ¼ ot \x02 tanh ðctÞ\nð10Þ\nCt is the output\
    \ state of the cell, which is composed of the\nproduct of the output ft of the\
    \ forgetting gate and the state\nCt\x031 at the previous moment and the sum of\
    \ the product of\nthe two outputs in the input gate. Wo represents the weight\n\
    matrix of the output gate, and bo represents the offset term\nof the output gate,\
    \ ht represents the current ﬁnal output.\nIn LSTM, the current state information\
    \ ct and the pre-\nvious state information ct\x031 have a linear relationship.\n\
    When the forget gate is open, that is, when the output of the\nsigmoid unit is\
    \ close to 1, the gradient will not disappear,\nand the new state The information\
    \ is the weighted average\nof the previous state information and the accumulated\n\
    information at the current moment, so regardless of the\nsequence length, as long\
    \ as the forget gate is open, the\nnetwork can remember the past state information,\
    \ that is,\nLSTM can capture long-term dependencies.\n4.4 ECG signal classification\
    \ based\non CNN1LSTM\nBoth CNN and RNN can classify image and text input, so\n\
    there is an opportunity to combine the two network models\nto improve classiﬁcation\
    \ efﬁciency. If the input adds time\ncharacteristics that CNN itself cannot handle,\
    \ the combi-\nnation of the two is more advantageous. Since the ECG\nsignal is\
    \ a physiological signal collected in accordance\nwith time, it contains rich\
    \ time domain features. The\npositional relationship between the waveforms of\
    \ various\nstages in a heartbeat beat is close. The input of the neuron\nin LSTM\
    \ is not only affected by the input at the current\nmoment, but also related to\
    \ the output at the previous\nmoment, that is, there is an association between\
    \ nodes at\ndifferent moments in the time series, which can save\ncontextual information,\
    \ and is dependent on long-distance\ntime. Time series are particularly effective.\
    \ Therefore, this\npaper uses the local perceptual ﬁeld characteristics of CNN\n\
    and the memory function of LSTM to construct a classi-\nﬁcation model combining\
    \ CNN?LSTM. The speciﬁc\nmodel structure is shown in Fig. 5. Among them, the size\n\
    of the convolution kernel is increased from 21 by 2, and the\nsliding step size\
    \ is 2 to extract the morphological charac-\nteristics of the ECG signal. The\
    \ number of convolution\nkernels is 4, 16, 32, and 64. The convolutional layer\
    \ per-\nforms feature reorganization to form a feature map. The\npooling part\
    \ selects a maximum pooling operation with a\nsize of 3 and a step size of 2 to\
    \ compress the feature vector\nsize. The signal features extracted by the convolutional\n\
    network are sent to the 128-unit LSTM network for time\nanalysis. Finally, various\
    \ predicted probabilities are output\nthrough the fully connected layer and the\
    \ softmax function.\nModel training process\n–\nCNN extract features Input the\
    \ preprocessed data into\nthe CNN network, and extract intermediate features\n\
    after convolution and pooling. The calculation formula\nis as (11).\nXn\nj ¼ Reluð\n\
    X\nj2Mj\nðXn\x031\nj\n\x06 Wn\nj Þ þ bn\nj Þ\nð11Þ\nAmong them, Xn\nj represents\
    \ the jth feature of the ECG\nsignal after the nth layer of convolution, W represents\n\
    the convolution kernel, and b represents the bias term.\n–\nLSTM extraction features\
    \ take the middle feature Xn\nj\nof the ECG signal after feature extraction as\
    \ the input of\nthe LSTM layer, and use the formula introduced in the\nprevious\
    \ section to calculate the output.\n–\nSoftmax classiﬁcation The signal features\
    \ extracted by\nCNN and LSTM are sent to the fully connected layer,\nwhere 5 classiﬁcation\
    \ labels are encoded with one-hot,\nand then the softmax function is used to generate\
    \ the\nprobability pk of each heartbeat type. The calculation\nformula is as (12).\n\
    pk ¼ softmaxðxÞ\nexpðhT\nk xÞ\nP\nk expðhT\nk xÞ\nð12Þ\nwhere x is the input sample\
    \ data, k is the heartbeat type,\nand h is the model parameter. In this paper,\
    \ x represents\neach heartbeat, k = 1,2,3,4,5, corresponding to cate-\ngories\
    \ N, S, V, F and Q, respectively.\n–\nBackpropagation and weight update: After\
    \ the cat-\negory is judged, the ECG classiﬁcation loss is calcu-\nlated, and\
    \ the loss is back-propagated according to the\nchain rule to calculate the gradient\
    \ of each weight and\nuse gradient descent to update the weights. The\ncalculation\
    \ formula is as (13).\nhj ¼ hj \x03 a 5JðhÞ\n5hj\n;\nj ¼ 1; 2; . . .; k\nð13Þ\n\
    –\nIterative training Repeat the above steps until the\nnetwork converges or reaches\
    \ the maximum number of\ntraining cycles. If the effect of the model training\
    \ result\nis improved, save the model.\nModel testing process\nLoad the optimal\
    \ model. Load the optimal parameters of\nthe model training stage, input the ECG\
    \ signal data of the\ntest set into the CNN?LSTM network for calculation, and\n\
    output the ﬁnal results and evaluation indicators.\nNeural Computing and Applications\
    \ (2023) 35:13921–13934\n13929\n123\n4.5 Discussion and analysis\nIn the patient’s\
    \ ECG signal, usually abnormal ECG signal\ndata are far less than normal ECG data.\
    \ This unbalanced\ndata distribution often leads to models that are more\ninclined\
    \ to learn with multiple data categories, and the\nlearning of a few categories\
    \ of data is insufﬁcient, espe-\ncially in the process of small-batch gradient\
    \ descent opti-\nmization. If there are only a few abnormal ECG signal data\n\
    in the ECG data set, it makes the direction of gradient\ndescent heavily dependent\
    \ on normal ECG data, resulting\nin a low recognition rate of abnormal ECG signals.\
    \ Usu-\nally, the solution to this problem is to perform multi-\nsampling of categories\
    \ with a small amount of data or\nunder-sampling categories with a large amount\
    \ of data. In\nrecent years, many people have used the popular generative\nconfrontation\
    \ network in deep learning to enhance data to\nachieve data balance.\n5 Experimental\
    \ results and discussion\n5.1 Dataset introduction\nThis paper uses the most widely\
    \ used ECG signal database in\nthe ﬁeld of ECG signal classiﬁcation research,\
    \ namely the\nMIT-BIH arrhythmia database. The MIT-BIH ECG database\ncontains\
    \ 48 half-hour records of the 24-hour dual-channel\nECG records of 47 subjects.\
    \ Among them, 23 records of the\n100 series are randomly selected from more than\
    \ 4000 Holter\ncollectors, and the other 25 records of the 200 series are\nunusual\
    \ but clinically important arrhythmia signals. The\nECG signal data are stored\
    \ in a binary format with a sampling\nfrequency of 360 Hz, and the atr ﬁle in\
    \ each record indicates\nthe type of heartbeat. This paper is classiﬁed into 5\
    \ categories\naccording to AAMI standards, namely N (normal heart beat),\nS (supraventricular\
    \ odor), V (ventricular odor), F (fusion\nheart beat), Q (undeﬁned heart beat).\
    \ The number of various\nheartbeats is shown in Table 1.\n5.2 Experimental settings\
    \ and evaluation\nindicators\nThe hardware conﬁguration of the experimental platform\
    \ is\nIntel i7-6700 CPU, the graphics card is GTX1080Ti, the\nmemory is 32G, the\
    \ operating system is Window10 system,\nand the model is implemented based on\
    \ the Python pro-\ngramming language and Tensorﬂow framework. When\ntraining the\
    \ network model parameters, the initial value of\nthe learning rate is 0.001,\
    \ and then the ReduceLROnPla-\nteau function makes the learning rate adaptive\
    \ to the\nmodel; the Dropout parameter is 0.2, Batch_size is 128,\nand Epoch is\
    \ 1000. For S, V, F, and Q, ﬁrst select 20% as\nthe test set, and use the remaining\
    \ 80% to generate new\ndata. In order to maintain the credibility of the experiment,\n\
    this paper adopts a tenfold cross-validation method.\nIn order to evaluate the\
    \ performance of the model in this\npaper, the following two indicators are used:\n\
    (1) In medical diagnosis, normal heartbeats and abnor-\nmal heartbeats are negative\
    \ and positive respectively. If the\ntrue type is negative and classiﬁed as negative,\
    \ it is\nrecorded as true negative (TN); the true type is positive and\nclassiﬁed\
    \ as negative and recorded as false negative (FN);\nthe true type is classiﬁed\
    \ as negative. Negatives are clas-\nsiﬁed as positives and counted as false positives;\
    \ those that\nare true to positives are classiﬁed as negatives and counted\nas\
    \ true positives (TP). In the ECG signal judgment model,\nAccuracy (Acc), Speciﬁcity\
    \ (Spe) and Sensitivity (Sen) are\ngenerally used as indicators for judgment.\
    \ Among them,\naccuracy represents the probability that the model is\naccurately\
    \ classiﬁed for a given test set, and speciﬁcity\nrepresents the probability that\
    \ a negative example is\naccurately predicted by the model, and sensitivity repre-\n\
    sents the probability that a positive example is correctly\nFig. 5 CNN-LSTM Network\
    \ Structure\n13930\nNeural Computing and Applications (2023) 35:13921–13934\n\
    123\nclassiﬁed by the model. The three evaluation index for-\nmulas are as (14),\
    \ (15) and (16).\nSen ¼\nTP\nTP þ TN \a 100%\nð14Þ\nSpe ¼\nTN\nTN þ FP \a 100%\n\
    ð15Þ\nAcc ¼\nTN þ TP\nTP þ FP þ FN þ TN \a 100%\nð16Þ\n(2) ROC curve: This indicator\
    \ is based on the three models\nof SVM, CNN, LSTM and the scheme of this article.\
    \ After\ntraining 4 models, they are used to predict the test set and\nobtain\
    \ the ROC curve. The area under the ROC curve can\nbe used to judge the performance\
    \ of the model.\n5.3 Result analysis\nIn (1), the probability of Acc, Spe and\
    \ Sen in different\nmethods of cardiovascular disease are shown in Fig. 6. As\n\
    can be seen from the this ﬁgure, through the predictions of\ndifferent models\
    \ on the test set, it is found that the clas-\nsiﬁcation effect of traditional\
    \ machine learning SVM is less\nthan that of deep learning methods, and our proposed\n\
    method is higher than other deep learning methods in Acc,\nSpe, and Sen. They\
    \ reached 99.29%, 99.53%, and 97.77%,\nrespectively.\nIn (2), four trained models\
    \ are used to predict the test\nset, and the ROC curve is shown in Fig. 7. It\
    \ can clearly\nsee that the ROC curves of these four models have a large\ndeviation\
    \ from the 45-degree diagonal. Among them, the\narea under the SVM model is small,\
    \ and the trained SVM\nmodel has a general classiﬁcation effect on the ECG\nsignals\
    \ of coronary patients. This shows that traditional\nmachine learning has a large\
    \ amount of data and has a poor\nTable 1 Number of various\nheartbeats\nHeartbeat\
    \ type\nDescription\nNumber\nN (Normal heart beat)\nNormal beat\n83513\nLeft bundle\
    \ branch block beat\nRight bundle branch block beat\nAtrial escape beats\nNodal(junctional)\
    \ escape\nS (Supraventricular odor)\nAtrial premature\n2184\nAberrant atrial premature\n\
    Nodal(junctional) premature\nSupra-Ventricular premature\nV (Ventricular odor)\n\
    Premature Ventricular\n6975\ncontraction Ventricular escape\nF (Fusion heart beat)\n\
    Fusion of Ventricular and Normal\n801\nQ (Undeﬁned heart beat)\nPaced\n3593\n\
    Fusion of Paced and Normal\nUnclassiﬁable\nTotal\n97066\nFig. 6 Acc, Spe and Sen\
    \ for each model\nFig. 7 ROC curve comparison chart\nNeural Computing and Applications\
    \ (2023) 35:13921–13934\n13931\n123\nfeature extraction effect on data with more\
    \ features, which\nmakes the generalization of the model poor; the area under\n\
    CNN and LSTM is not much different, and the ECG signal\nclassiﬁcation effect is\
    \ good. It also shows that the deep\nlearning model can effectively extract the\
    \ unilateral fea-\ntures of the ECG data to a certain extent, making the model\n\
    generalization better; it can be seen that the area of our\nproposed\nmethod\n\
    is\nthe\nlargest,\nshowing\nthat\nthe\nCNN?LSTM models can not only propose the\
    \ character-\nistics of the ECG signal itself, but also accurately propose\nthe\
    \ time-domain features in it. Combining them for clas-\nsiﬁcation can effectively\
    \ improve the classiﬁcation. The\neffect makes the model strong in generalization.\n\
    6 Conclusion\nAiming at the accuracy and timeliness of heart rate\ndetection for\
    \ COVID-19 patients, this paper proposes a\nreal-time cardiovascular monitoring\
    \ system based on 5G\nand deep learning to ensure low latency and high\nthroughput\
    \ of ECG signal data transmission in wearable\ndevices. In order to achieve real-time\
    \ ECG signal data\nmonitoring, analysis and diagnosis, it adopts the new\ngeneration\
    \ of wireless communication technology 5G, plus\nthe real-time data processing\
    \ platform Flink framework,\nand ﬁnally uses the classiﬁcation of the proposed\
    \ deep\nlearning ECG signal model to realize real-time ECG signal\ndata monitoring,\
    \ analysis and diagnosis. The correct rate of\nmodel prediction can reach 99.29%,\
    \ which proves that the\nreal-time medical monitoring system is necessary and\
    \ can\neffectively predict the actual situation of the COVID-19\npatient’s cardiovascular\
    \ system. It has high practical value\nand can realize early warning of emergencies\
    \ in time.\nOur next plan is to further optimize the deep learning\nmodel, try\
    \ to increase the one-dimensional time series\nsignal to the two-dimensional space,\
    \ use the Generative\nAdversarial Networks to enhance the data of the insufﬁ-\n\
    cient number of heartbeat types, solve the problem of ECG\nsignal imbalance, and\
    \ consider using migration learn to\nincrease model accuracy and reduce training\
    \ time.\nAcknowledgements This work was supported in part by the National\nNatural\
    \ Science Foundation of China under Grant No. 61373162, in\npart by the Sichuan\
    \ Provincial Science and Technology Department\nProject under Grant No. 2019YFG0183,\
    \ in part by the Sichuan\nProvincial Key Laboratory Project under Grant No. KJ201402,\
    \ and in\npart by the Japan Society for the Promotion of Science (JSPS) Grants-\n\
    in-Aid\nfor\nScientiﬁc\nResearch\n(KAKENHI)\nunder\nGrant\nJP18K18044 and JP21K17736.\n\
    Declarations\nConflicts of interest The authors declare that there is no conflict\
    \ of\ninterest regarding the publication of this article.\nReferences\n1. Zhou\
    \ X, Hu Y, Liang W, Ma J, Jin Q (2020) Variational lstm\nenhanced anomaly detection\
    \ for industrial big data. IEEE Trans\nInd Inform 17(5):3469-3477. https://doi.org/10.1109/TII.2020.\n\
    3022432\n2. Yu K, Tan L, Shang X, Huang J, Srivastava G, Chatterjee P\n(2021)\
    \ Efﬁcient and privacy-preserving medical research support\nplatform against covid-19:\
    \ a blockchain-based approach. IEEE\nConsumer Electron Mag 10(2):111–120\n3. Yu\
    \ K, Tan L, Aloqaily M, Yang H, Jararweh Y (2021) Block-\nchain-enhanced data\
    \ sharing with traceable and direct revocation\nin iiot. IEEE Trans Ind Inf. https://doi.org/10.1109/TII.2021.\n\
    3049141\n4. Yu K, Guo Z, Shen Y, Wang W, Lin JC-W, Sato T (2021) Secure\nartiﬁcial\
    \ intelligence of things for implicit group recommenda-\ntions. IEEE Internet\
    \ Things J. https://doi.org/10.1109/JIOT.2021.\n3079574\n5. Manogaran G, Lopez\
    \ D (2018) Health data analytics using\nscalable logistic regression with stochastic\
    \ gradient descent. Int J\nAdv Intell Paradigms 10(1–2):118–132\n6. Zhou X, Liang\
    \ W, Wang KI, Wang H, Yang LT, Jin Q (2020)\nDeep-learning-enhanced human activity\
    \ recognition for internet\nof healthcare things. IEEE Int Things J 7(7):6429–6438\n\
    7. Wang J, Huang X, Tang S, Shi GM, Ma X, Guo J (2019) Blood\ntriglyceride monitoring\
    \ with smartphone as electrochemical\nanalyzer for cardiovascular disease prevention.\
    \ IEEE J Biomed\nHealth Inf 23(1):66–71\n8. Li MH, Yadollahi A, Taati B (2017)\
    \ Noncontact vision-based\ncardiopulmonary monitoring in different sleeping positions.\
    \ IEEE\nJ Biomed Health Inf 21(5):1367–1375\n9. Saadatnejad S, Oveisi M, Hashemi\
    \ M (2020) Lstm-based ecg\nclassiﬁcation for continuous monitoring on personal\
    \ wearable\ndevices. IEEE J Biomed Health Inf 24(2):515–523\n10. Hu H, Wen Y,\
    \ Chua T-S, Li X (2014) Toward scalable systems\nfor big data analytics: a technology\
    \ tutorial. IEEE Access\n2:652–687\n11. Zhang X, Yang L, Ding Z, Song J, Zhai\
    \ Y, Zhang D (2021)\nSparse vector coding-based multi-carrier noma for in-home\n\
    health networks. IEEE J Select Areas Commun 39(2):325–337.\nhttps://doi.org/10.1109/JSAC.2020.3020679\n\
    12. Feng C, Yu K, Bashir AK, Al-Otaibi YD, Lu Y, Chen S, Zhang D\n(2021) Efﬁcient\
    \ and secure data sharing for 5g ﬂying drones: a\nblockchain-enabled approach.\
    \ IEEE Netw 35(1):130–137\n13. Li S, DaXu L, Zhao S (2015) The internet of things:\
    \ a survey. Inf\nSyst Front 17(2):243–259\n14. Yu K, Tan L, Lin L, Chen X, Yi\
    \ Z, Sato T (2021) Deep learning\nempowered breast cancer auxiliary diagnosis\
    \ for 5gb remote\ne-health. IEEE Wireless Commun. https://doi.org/10.1109/MWC.\n\
    001.2000374\n15. Sannino G, DePietro G (2018) A deep learning approach for ecg-\n\
    based heartbeat classiﬁcation for arrhythmia detection. Future\nGenerat Comput\
    \ Syst 86:446–455\n16. Ed-daoudy A, Maalmi K (2019) A new internet of things archi-\n\
    tecture for real-time prediction of various diseases using machine\nlearning on\
    \ big data environment. J Big Data 6(1): 104\n17. Sun J, Reddy C.K. (2013) ‘‘Big\
    \ data analytics for healthcare,’’ In:\nProceedings of the 19th ACM SIGKDD international\
    \ conference\non Knowledge discovery and data mining, pp. 1525–1525\n18. Nair\
    \ LR, Shetty SD, Shetty SD (2018) Applying spark based\nmachine learning model\
    \ on streaming big data for health status\nprediction. Comput Electr Eng 65:393–399\n\
    19. Alotaibi S, Mehmood R, Katib I, Rana O, Albeshri A (2020)\nSehaa: a big data\
    \ analytics tool for healthcare symptoms and\n13932\nNeural Computing and Applications\
    \ (2023) 35:13921–13934\n123\ndiseases detection using twitter, apache spark,\
    \ and machine\nlearning. Appl Sci 10(4):1398\n20. Ed-Daoudy A, Maalmi K (2019)\
    \ ‘‘Real-time machine learning for\nearly detection of heart disease using big\
    \ data approach. In: 2019\nInternational Conference on Wireless Technologies,\
    \ Embedded\nand Intelligent Systems (WITS). IEEE, pp. 1–5\n21. Manogaran G, Varatharajan\
    \ R, Priyan M (2018) Hybrid recom-\nmendation system for heart disease diagnosis\
    \ based on multiple\nkernel learning with adaptive neuro-fuzzy inference system.\n\
    Multimed Tools Appl 77(4):4379–4399\n22. Melin P, Miramontes I, Prado-Arechiga\
    \ G (2018) A hybrid model\nbased on modular neural networks and fuzzy systems\
    \ for classi-\nﬁcation of blood pressure and hypertension risk diagnosis. Expert\n\
    Syst Appl 107:146–164\n23. Al-Makhadmeh Z, Tolba A (2019) Utilizing iot wearable\
    \ medical\ndevice for heart disease prediction using higher order boltzmann\n\
    model: a classiﬁcation approach. Measurement 147:106815\n24. Yin H, Jha NK (2017)\
    \ A health decision support system for\ndisease diagnosis based on wearable medical\
    \ sensors and\nmachine learning ensembles. IEEE Trans Multi-Scale Comput\nSyst\
    \ 3(4):228–241\n25. Jabeen F, Maqsood M, Ghazanfar MA, Aadil F, Khan S, Khan\n\
    MF, Mehmood I (2019) An iot based efﬁcient hybrid recom-\nmender system for cardiovascular\
    \ disease. Peer-to-Peer Netw\nAppl 12(5):1263–1276\n26. Kumar PM, Lokesh S, Varatharajan\
    \ R, Babu GC, Parthasarathy P\n(2018) Cloud and iot based disease prediction and\
    \ diagnosis\nsystem for healthcare using fuzzy neural classiﬁer. Future Gen-\n\
    erat Comput Syst 86:527–534\n27. Kumar PM, Gandhi UD (2018) A novel three-tier\
    \ internet of\nthings architecture with machine learning algorithm for early\n\
    detection of heart diseases. Comput Electr Eng 65:222–235\n28. Hammad M, Alkinani\
    \ MH, Gupta BB et al (2021) Myocardial\ninfarction detection based on deep neural\
    \ network on imbalanced\ndata. Multimed Syst. https://doi.org/10.1007/s00530-020-00728-\n\
    8\n29. Sedik A, Hammad M, Abd El-Samie FE et al (2021) Efﬁcient\ndeep learning\
    \ approach for augmented detection of Coronavirus\ndisease. Neural Comput Appl.\
    \ https://doi.org/10.1007/s00521-\n020-05410-8\n30. AlZu’bi S, Shehab M, Al-Ayyoub\
    \ M, Jararweh Y, Gupta B\n(2020) Parallel implementation for 3d medical volume\
    \ fuzzy\nsegmentation. Patt Recognit Lett 130:312–318\n31. Abd El-Latif A.A, Abd-El-Atty\
    \ B, Hossain M.S, Rahman M.A,\nAlamri A, Gupta B.B. (2018) Efﬁcient quantum information\n\
    hiding for remote medical image sharing. IEEE Access 6:\n21 075–21 083\n32. Zhen\
    \ L, Bashir AK, Yu K, Al-Otaibi YD, Foh CH, Xiao P (2021)\nEnergy-efﬁcient random\
    \ access for leo satellite-assisted 6g\ninternet of remote things. IEEE Int Things\
    \ J 8(7):5114–5128\n33. Tan L, Xiao H, Yu K, Aloqaily M, Jararweh Y (2021) A\n\
    blockchain-empowered crowdsourcing system for 5g-enabled\nsmart cities. Comput\
    \ Standards Interfaces 76:103517\n34. Zhang J, Yu K, Wen Z, Qi X, Paul AK (2021)\
    \ 3d reconstruction\nfor motion blurred images using deep learning-based intelligent\n\
    systems. Comput Mater Continua 66(2):2087–2104\n35. Yu K, Lin L, Alazab M, Tan\
    \ L, Gu B (2020) Deep learning-based\ntrafﬁc safety solution for a mixture of\
    \ autonomous and manual\nvehicles in a 5g-enabled intelligent transportation system.\
    \ IEEE\nTrans Intell Transp Syst. https://doi.org/10.1109/TITS.2020.\n3042504\n\
    36. Gong Y, Zhang L, Liu RP, Yu K, Srivastava G (2021) Nonlinear\nmimo for industrial\
    \ internet of things in cyber-physical systems.\nIEEE Trans Ind Inf 17(8):5533–5541.\
    \ https://doi.org/10.1109/TII.\n2020.3024631\n37. Greco L, Ritrovato P, Xhafa\
    \ F (2019) An edge-stream computing\ninfrastructure for real-time analysis of\
    \ wearable sensors data.\nFuture Generat Comput Syst 93:515–528\n38. Pan J, Tompkins\
    \ WJ (1985) A real-time qrs detection algorithm.\nIEEE Trans biomed Eng 3:230–236\n\
    39. Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002)\nSmote: synthetic minority\
    \ over-sampling technique. J Artif Intell\nRes 16:321–357\n40. Guo Z, Yu K, Jolfaei\
    \ A, Bashir AK, Almagrabi AO, Kumar N\n(2021) A fuzzy detection system for rumors\
    \ through explainable\nadaptive learning. IEEE Trans Fuzzy Syst. https://doi.org/10.\n\
    1109/TFUZZ.2021.3052109\n41. Guo Z, Shen Y, Bashir AK, Imran M, Kumar N, Zhang\
    \ D, Yu K\n(2021) Robust spammer detection using collaborative neural\nnetwork\
    \ in internet of thing applications. IEEE Int Things J\n8(12):9549–9558. https://doi.org/10.1109/JIOT.2020.3003802\n\
    Publisher’s Note Springer Nature remains neutral with regard to\njurisdictional\
    \ claims in published maps and institutional afﬁliations.\nAuthors and Afﬁliations\n\
    Liang Tan1,2 • Keping Yu3\n• Ali Kashif Bashir4,5 • Xiaofan Cheng1 • Fangpeng\
    \ Ming1 • Liang Zhao6 •\nXiaokang Zhou7\n& Keping Yu\nkeping.yu@aoni.waseda.jp\n\
    Liang Tan\njkxy_tl@sicnu.edu.cn\nAli Kashif Bashir\nDr.alikashif.b@ieee.org\n\
    Xiaofan Cheng\n978029808@qq.com\nFangpeng Ming\n11451287@qq.com\nLiang Zhao\n\
    lzhao@sau.edu.cn\nXiaokang Zhou\nzhou@biwako.shiga-u.ac.jp\n1\nCollege of Computer\
    \ Science, Sichuan Normal University,\nChengdu 610101, China\n2\nChina and Institute\
    \ of Computing Technology, Chinese\nAcademy of Sciences, Beijing 100190, China\n\
    3\nGlobal Information and Telecommunication Institute,\nWaseda University, Tokyo,\
    \ Japan\nNeural Computing and Applications (2023) 35:13921–13934\n13933\n123\n\
    4\nDepartment of Computing and Mathematics, Manchester\nMetropolitan University,\
    \ Manchester, UK\n5\nSchool of Information and Communication Engineering,\nUniversity\
    \ of Electronics Science and Technology of China\n(UESTC), Chengdu, China\n6\n\
    School of Computer Science, Shenyang Aerospace\nUniversity, Shenyang 110136, China\n\
    7\nFaculty of Data Science, Shiga University, Hikone, and\nRIKEN Center for Advanced\
    \ Intelligence Project, Tokyo,\nJapan\n13934\nNeural Computing and Applications\
    \ (2023) 35:13921–13934\n123\n"
  inline_citation: '>'
  journal: Neural computing & applications (Print)
  limitations: '>'
  pdf_link: https://link.springer.com/content/pdf/10.1007/s00521-021-06219-9.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Toward real-time and efficient cardiovascular monitoring for COVID-19 patients
    by 5G-enabled wearable medical devices: a deep learning approach'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.watres.2022.118973
  analysis: '>'
  authors:
  - Guangtao Fu
  - Yiwen Jin
  - Siao Sun
  - Zhiguo Yuan
  - David Butler
  citation_count: 67
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Advances
    in deep learning 3. Deep learning applications 4. Future research challenges 5.
    Conclusions Declaration of competing interest Acknowledgements Appendix. Supplementary
    materials Data availability References Show full outline Cited by (82) Figures
    (5) Tables (1) Table 1 Extras (1) Document Water Research Volume 223, 1 September
    2022, 118973 The role of deep learning in urban water management: A critical review
    Author links open overlay panel Guangtao Fu a, Yiwen Jin a, Siao Sun b, Zhiguo
    Yuan c, David Butler a Show more Share Cite https://doi.org/10.1016/j.watres.2022.118973
    Get rights and content Under a Creative Commons license open access Highlights
    • Key application areas of deep learning are identified in urban water and wastewater
    management. • Popular application areas include anomaly detection, system state
    forecasting, asset monitoring and assessment. • Industrial application of deep
    learning is still at an early stage with few implementations reported. • Research
    challenges include data privacy, algorithm development, explainability, multi-agent
    system and digital twin. • Deep learning should drive urban water systems towards
    high intelligence and autonomy. Abstract Deep learning techniques and algorithms
    are emerging as a disruptive technology with the potential to transform global
    economies, environments and societies. They have been applied to planning and
    management problems of urban water systems in general, however, there is lack
    of a systematic review of the current state of deep learning applications and
    an examination of potential directions where deep learning can contribute to solving
    urban water challenges. Here we provide such a review, covering water demand forecasting,
    leakage and contamination detection, sewer defect assessment, wastewater system
    state prediction, asset monitoring and urban flooding. We find that the application
    of deep learning techniques is still at an early stage as most studies used benchmark
    networks, synthetic data, laboratory or pilot systems to test the performance
    of deep learning methods with no practical adoption reported. Leakage detection
    is perhaps at the forefront of receiving practical implementation into day-to-day
    operation and management of urban water systems, compared with other problems
    reviewed. Five research challenges, i.e., data privacy, algorithmic development,
    explainability and trustworthiness, multi-agent systems and digital twins, are
    identified as key areas to advance the application and implementation of deep
    learning in urban water management. Future research and application of deep learning
    systems are expected to drive urban water systems towards high intelligence and
    autonomy. We hope this review will inspire research and development that can harness
    the power of deep learning to help achieve sustainable water management and digitalise
    the water sector across the world. Graphical abstract Download : Download high-res
    image (134KB) Download : Download full-size image Previous article in issue Next
    article in issue Keywords Artificial intelligenceData analyticsDeep learningDigital
    twinWater management 1. Introduction Computer simulations have been playing an
    increasingly significant role in water management since they were pioneered for
    the planning and design of water resources systems in the Harvard Water Programme
    in 1955 (Reuss, 2003). Physically-based models have been developed over many years
    to represent the Urban Water System (UWS) at varying levels of complexity and
    widely used to support its planning, operation and management (e.g., IWA, 2019).
    However, the advance of physically-based models has now essentially stalled due
    to the challenges in 1) the complexity of UWSs and their interactions with other
    systems such as ecosystems and climate systems, particularly in capturing human
    perceptions, behaviours and cascaded impacts, 2) the difficulty in determining
    modelling assumptions, various processes and model structures and calibrating
    a large number of model parameters, which can lead to the equifinality problem,
    3) data scarcity and uncertainty for high resolution modelling, 4) intensive computing
    power required by real-time simulation and optimisation and 5) human resources
    and skills required by model development and maintenance, which makes it difficult
    to transfer from one UWS to another. On the other hand, machine learning, which
    is a subset of Artificial Intelligence (AI) allows systems to learn directly from
    data, examples and experience without pre-defined rules, and is being recognised
    as a potentially disruptive technology to transform global economies, environments
    and societies (The Royal Society, 2017). This is happening against the backdrop
    of transformations required to address pressing challenges, including climate
    change, biodiversity loss, and the COVID-19 pandemic (Butler et al., 2016). Machine
    learning will undoubtedly play a key role in the transformation of the scientific
    discipline and practice in the water sector (IWA, 2019) and help tackle water
    challenges such as resource efficiency, water supply, water pollution, flooding
    and drought, contributing to achieving the water-related United Nations’ sustainable
    development goals (Mehmood et al., 2020; Vinuesa et al., 2020). Deep learning,
    a subset of machine learning, is regarded as one of driving forces for recent
    breakthroughs in AI. Deep learning typically uses large, multi-layer artificial
    neural networks (ANNs) to process large raw data sets, thus also termed as deep
    networks. Conventional machine learning technologies such as multi-layer perceptron
    (MLP) neural networks are limited in their ability to process raw data and need
    domain expertise for data processing before learning. Deep learning helps to solve
    this problem and enables automatic feature extraction using multiple levels of
    representations from raw data to more abstract levels (Lecun et al., 2015). Popular
    deep learning algorithms include Convolutional Neural Networks (CNNs), Long Short-Term
    Memory (LSTM), Autoencoders, Graph Neural Networks (GNNs) and Deep Reinforcement
    Learning (DRL). These deep learning algorithms have had great success in many
    areas such as image recognition and have already been applied across a range of
    industry sectors such as healthcare and finance. In the water sector, the power
    of deep learning is increasingly recognised with research publications, case studies
    and applications growing at a rapid speed (Shen, 2018; Makropoulos and Savic,
    2019). This is therefore a good point in time to examine the current application
    of deep learning techniques in urban water management and provide a perspective
    of deep learning research in advancing water engineering and boosting practical
    application of Dl techniques to real-world water problems. This paper aims to
    provide a critical review of the role of deep learning in the planning and management
    of UWSs. We will examine the progress of deep learning research and application
    in key urban water challenges and discuss potential directions where advances
    in deep learning research are needed to boost the development of intelligent UWSs
    and the digitalisation process in the water sector. The remaining paper is organised
    as below: Section 2 introduces the advances in deep learning in comparison to
    conventional machine learning; Section 3 reviews deep learning application to
    urban water management, including demand prediction, leakage detection, contamination
    detection, sewer defect and blockage assessment, wastewater system prediction,
    urban flooding, asset monitoring and system control; Section 4 presents future
    research challenges; and conclusions are drawn in Section 5. 2. Advances in deep
    learning Historically AI has been through several ‘highs’ and ‘lows’, but recently
    deep learning is driving the development and application of AI across various
    industries. Compared to conventional machine learning, deep learning has advanced
    in many aspects as discussed below. First, it enables the automatic extraction
    of features from raw data through multiple levels of representation learning starting
    from raw data to higher, more abstract levels (Lecun et al., 2015). This eliminates
    the requirement of feature engineering and domain knowledge to extract features
    from raw data before they are fed into machine learning algorithms. Further, this
    improves the learning capacity through amplification of important patterns and
    suppression of irrelevant variations in the input data, together with the exponential
    advantage in representing complex non-linear functions from stacking a large number
    of hidden layers in deep networks (Lecun et al., 2015; Shen, 2018). Second, the
    wide adoption of the rectified linear unit activation function, which is simply
    a half-wave rectifier f(x) = max(x, 0), and its variations brings the following
    advantages (Goodfellow et al., 2016): 1) fast training of deep networks due to
    computation savings from its derivatives (which is 1 for a positive input, otherwise
    0) and error terms; 2) solving the vanishing gradient problem due to its higher
    gradients and linearity; 3) allowing the activation of hidden layers to output
    true zero values for negative inputs, which leads to sparse representation, a
    desirable property in representation learning, while the sigmoid activation function
    can only learn to approximate a zero output, e.g. a value very close to zero but
    not a true zero value. Third, the development of the stochastic gradient descent
    method has made the training of deep networks very efficient, especially for large
    datasets as it randomly selects a small subset (called a mini-batch) from the
    training dataset each time, and this process is repeated until the training is
    converged. The stochastic gradient descent method has been improved with many
    extensions, such as Adam (Kingma and Ba, 2015) which has gained popularity for
    deep learning applications. Further, the network training has been made more computationally
    efficient and effective with a range of techniques including improved architectures,
    unsupervised pre-training, weights sharing, model compression and distillation,
    and regularization methods (e.g., dropout) (Lecun et al., 2015; Shen, 2018). Finally,
    the success of deep learning is also built on the advances in hardware accelerators
    such as graphics processing units (GPU) and the availability of large datasets.
    Data parallelization is a commonly used GPU strategy for accelerating deep learning
    training, well aligned with mini-batch training. In the strategy, a copy of the
    network is stored and trained on its own mini-batch of data in each GPU, and its
    computed gradients and losses are then transferred to the shared processor (e.g.,
    CPU) for aggregation before being rebroadcast to GPUs for parameter updates. This
    strategy substantially accelerates the training of deep networks for large datasets
    and improves the learning capacity of deep networks. Architectures of several
    popular deep learning algorithms including autoencoders, LSTMs, CNNs, DRL and
    GNNs and their key features are shown in Fig. 1, together with the conventional
    multi-layer perceptron ANN. More information about these architectures is provided
    in Supplementary Material, together with a brief introduction of machine learning
    and MLP neural networks. Other networks such as Generative Adversarial Networks
    (GAN) and transformers (Vaswani et al., 2017) are discussed in the literature
    (Xu and Liang, 2021; Goodfellow et al., 2016) . Download : Download high-res image
    (575KB) Download : Download full-size image Fig. 1. Multi-layer perceptron neural
    network and popular deep learning algorithms. (a) a fully-connected three-layer
    neural network with non-linear activations such as sigmoid, (b) autoencoders typically
    composed of two symmetrical parts: encoder and decoder, which are trained to reconstruct
    the inputs by passing through a bottleneck layer and thus a unsupervised learning
    method, (c) LSTM cells typically consisting of a forget gate, input gate and output
    gate are the key building block for LSTM networks, which can learn long-term dependencies
    in time series, (d) CNNs use convolution and pooling to extract higher-order features
    from input images, (e) DRL combines reinforcement learning and deep learning to
    train an agent for maximum returns through interaction with the environment, (f)
    GNNs are designed for graph-structured data to represent diverse relationships
    via message passing between graph nodes. 3. Deep learning applications Deep learning
    has been used in a wide range of application areas in urban water management,
    predominantly covering anomaly detection, system prediction, asset assessment,
    system operation and planning and maintenance (Fig. 2). Anomaly detection is a
    type of diagnostic analytics, which aims to identify various failure events (e.g.
    leakages, contamination events, blockages and cyber-attacks). System prediction
    and asset assessment provide an understanding of the current and future states
    of the UWS being hydraulic or asset conditions. System operation, planning and
    maintenance are optimisation problems aiming to identify the best solutions given
    specific constraints. Deep learning can play a role in all the key issues in the
    life cycle of the UWS, with a diverse range of architectures for different problems.
    At present, however, there are few well-tested deep learning algorithms or products
    readily available for solving UWS problems. A number of challenges related to
    data and algorithmic development hinder the development and implementation of
    deep learning approaches in the water sector (Fig. 2). Download : Download high-res
    image (602KB) Download : Download full-size image Fig. 2. Key application areas
    in urban water management and relevant architectures of deep learning. The latest
    applications are reviewed and categorised in detail in the following sections:
    water supply and distribution systems (Section 3.1), urban wastewater systems
    (Section 3.2), urban flooding (Section 3.3) and cyber security and asset monitoring
    (Section 3.4). Reinforcement learning, as an emerging approach for prescriptive
    analytics, has just started to receive applications in system control and operation,
    hence it is reviewed in Section 3.5. Gaps in industrial application are discussed
    in Section 3.6. Table 1 provides a summary of the latest developments of the key
    problems reviewed, covering the aspects of popular algorithms, data requirements,
    case studies and advantages. Most problems are related to classification, anomaly
    detection and regression tasks using images or time series data; CNNs, LSTMs and
    their hybrids are amongst the most popular models for solving these problems.
    Table 1. Application of deep learning in urban water systems. System Problem Popular
    algorithm Data requirement Case study Open access dataset Advantage Water supply
    and distribution systems Short-term demand forecasting LSTM, hybrid CNN-LSTM Historical
    demand data, weather and demographic data Pilot study in Cairo (Nasser et al.,
    2020) – High temporal resolution (15 mins), handling peak demands Leakage detection
    and localisation 1-D and 2-D CNNs pressure, flow, acoustic and vibration signals
    Real-world network case studies, synthetic data from hydraulic models (e.g., Javadiha
    et al., 2019; Zhou et al., 2019b)., laboratory test beds (Shukla and Piratla,
    2020; Cody et al., 2020) The BattLeDIM 2020 dataset (Liu et al., 2019) High accuracy,
    automatic detection and alarm generation Contamination detection LSTM, hybrid
    CNN-LSTM, GAN, GNN Water quality (e.g., chlorine, pH, conductivity, turbidity)
    and flow rate Real-world sensor datasets (Li et al., 2019a)s The GECCO challenge
    dataset (Muharemi et al., 2019) and The CANARY data set (U.S. EPA, 2012; Z. L.
    Li et al., 2022) Considering correlations of multivariate time series Water pipeline
    inspection Autoencoders CCTV videos Pipeline field survey (Jiao et al., 2021)
    – High efficiency in processing videos Cyber attack detection, soft sensing LSTM,
    autoencoders and GNN System state variables Test-bed systems (e.g., Deng and Hooi,
    2021), synthetic data, benchmark networks (e.g., Taormina and Galelli, 2018) The
    BATADAL dataset (Taormina et al., 2018) Accurate prediction Real-time control
    such as pump scheduling DRL Demand data, hydraulic models Benchmark networks -
    Anytown and d-town (Hajgató et al., 2020) – Automation and robust control in an
    uncertain environment Urban wastewater systems Sewer defect detection and assessment
    CNNs (e.g., R-CNNs and YOLO) CCTV videos Sewer surveillance data (e.g., Hassan
    et al., 2019; Kumar et al., 2020b; Wang et al., 2021b) The Sewer-ML dataset with
    1.3 million images (Haurum and Moeslund, 2021) Automatic assessment and decision
    support Prediction of water quality, flow and depth including CSOs LSTM Rainfall
    data, observed flow and water depth data Pilot WWTP (Dairi et al., 2019), real-world
    sewer systems such as Drammen, Copenhagen (Zhang et al., 2018a) – Improved predictive
    accuracy and generalisation Flood forecasting CNNs, U-NET Urban catchment data,
    rainfall data, flow Synthetic data generated using flood models (Guo et al., 2021b;
    Li et al., 2021) Flood risk maps such as those from the UK Environment Agency.
    The data and code in the study (Guo et al., 2021b) are publicly available. Fast
    and accurate prediction, up-scaling Data processing of urban catchments, weather
    and flood data CNNs, autoencoders Aerial photography, LiDAR data, satellite data
    and radar weather data Real-world urban catchments and cities (Yang and Cervone,
    2019; Iqbal et al., 2021) Global and local data sets at varying resolutions More
    accurate, higher-resolution data, automatic data processing Soft sensing, surrogate
    modelling LSTM, hybrid CNN-LSTM and GNN System state variables Test-bed systems,
    synthetic data, benchmark networks (e.g., Cheng et al., 2020) – Fast and accurate
    system monitoring and prediction Flood control, CSO control, wastewater plant
    operation DRL Rainfall, hydraulic models Real world networks, Benchmark Simulation
    Model, laboratory plant (e.g., Bowes et al., 2021; Hernández-del-Olmo et al.,
    2016 & 2018) The case study data and code from the study (Bowes et al., 2021)
    are publicly available. Automation and robust control in an uncertain environment
    3.1. Water supply and distribution systems 3.1.1. Demand forecasting Demand forecasting
    is a typical time series forecasting problem, which is normally regarded as a
    supervised learning problem, thus LSTM models have been predominantly used to
    learn the time dependence in historical data. LSTM models are able to predict
    hourly or sub-hourly demands by capturing the features from the previous time-step
    demands without considering other weather and demographic factors. A Gated Recurrent
    Unit (GRU) based RNN was able to achieve more accurate and reliable water demand
    predictions with 15 min and 24 h forecast horizons than traditional machine learning
    models (Guo et al., 2018). The performance of LSTM models in high temporal resolution
    (i.e., 15 mins and 1 h) demand predictions was further confirmed by Mu et al.
    (2020) through comparisons with AutoRegressive Integrated Moving Average (ARIMA),
    Support Vector Machine (SVM) and random forest models, in particularly for handling
    demand spikes. LSTMs are able to predict the daily water demand profile at a 1
    h time step in an online learning setting with a few days learning from scratch,
    so they can be used to generate demand predictions for optimising the next day
    pump operation, which is particularly useful for small water suppliers with limited
    historical data (Kühnert et al., 2021). With the increasing adoption of smart
    water meters, LSTMs can be used to provide reliable water demand predictions based
    on water consumption data and thus offer opportunities for water utilities to
    optimise resources and operations. Nasser et al. (2020) showed that the LSTM outperformed
    SVMs and random forest models using aggregated 10-minute smart metre data of 2–20
    households from a pilot study in Cairo, however, it fails to predict the peak
    demands. Recent studies showed that hybrid deep learning models achieve high performance
    in daily water demand predictions when weather and demographic factors are considered.
    Du et al. (2021) developed a hybrid LSTM model for daily water demand forecasting,
    which combines two LSTMs with discrete wavelet transformation and principal component
    analysis to pre-process the raw data: one LSTM uses de-noised demand sequence
    to predict the baseline trend, and the other LSTM uses the demand residuals to
    predict the artificial noise, while the principal components of weather and holiday
    factors are also used as input to both networks. Hu et al. (2019) applied the
    CNN to extract the features of the previous five-day water consumption data and
    the daily maximum temperature, before they are passed to the bi-directional LSTM
    model for daily water demand prediction, and the hybrid CNN-LSTM model achieved
    higher predictive accuracy than LSTM, Bi-LSTM and CNN. Data preprocessing techniques
    such as time series signal decomposition can facilitate feature extraction and
    thus improve the predictive accuracy of GRU-based models (Hu et al., 2021). In
    summary, previous research shows the LSTM and its hybrid algorithms are the most
    popular deep learning approach for water demand forecasting and they outperform
    traditional machine learning methods due to the capacity in capturing temporal
    dependence. However, applications are limited to short-term demand forecasting
    with the forecast time step of less than one day. 3.1.2. Leakage detection and
    localisation Deep learning approaches for leakage detection and localisation can
    be roughly grouped into two categories: classification and prediction-based approaches.
    The classification approach trains deep learning models on labelled data (i.e.,
    pressure, flow, acoustic and vibration signals) to identify normal and abnormal
    events. A major disadvantage of this approach is the effort in collecting and
    labelling a large amount of data, though hydraulic models can be used to generate
    synthetic data for training (e.g., Javadiha et al., 2019; Zhou et al., 2019b).
    The prediction-based approach generally uses deep learning models to predict the
    system states (pressure or flow) and then classify the residuals between predicted
    and measured state values using a threshold. In the literature, CNNs are the predominant
    deep learning approach to leakage detection and localisation and in most studies
    they are trained using labelled data to classify normal and abnormal events. There
    are a few prediction-based deep learning approaches in the literature, and one
    example used a LSTM to predict water demand, which achieved a high detection accuracy,
    high true positive rate and low false positive rate (Wang et al., 2020). The following
    review will focus on the CNNs-based classification approach, organised by data
    sources, i.e., pressure/flow data, acoustic/vibration and their input format.
    CNNs have been trained on pressure data for multi-point leakage detection. Six
    CNN models were trained by Fang et al. (2019) using pressure data, which were
    collected from a Water Distribution System (WDS) in a laboratory platform and
    manually labelled for leakage events. The best CNN model achieved an accuracy
    of 97.33% and 92.11% when 21 and 8 pressure sensors were used, respectively. It
    was observed that accuracy decreases for multi-leak events, from 96.43% for single-point
    leakage to 91.56% for three-point leakage. The high accuracies achieved are partially
    due to the high number of sensors used (between 8 and 21 sensors for a network
    of 400 m), which is impractical for real-world networks. The idea of using hydraulic
    models to generate pressure data with a large number of leak events for training
    was proposed by Zhou et al. (2019b). Hydraulic models have been used for synthetic
    data generation before, however, data were generated under normal conditions with
    no or limited number of leakage scenarios. Zhou et al. (2019b) developed a fully-linear
    DenseNet to effectively extract leakage features in the WDS so that leaks could
    be localised to the pipe level. Once trained off-line using a large number of
    leakage signals (e.g., 200 leak events per pipe), the model was used online to
    locate leaks using real-time pressure data from in situ or mobile loggers. Results
    from two WDSs show that the model is able to accurately identify the pipe where
    leaks occur. Similarly, synthetic leakage data were used for CNN training (Javadiha
    et al., 2019) and autoencoder training (Fan et al., 2021). However, they produced
    pressure residual maps from the differences between pressure measurements provided
    by sensors and pressure estimates obtained from a hydraulic network model, and
    then converted the maps to 2D images to train CNNs for leakage localisation. The
    pressure residual maps characterise the impact of all possible leak localizations
    but might be affected by uncertainties in hydraulic modelling. Indeed, the impacts
    of hydraulic model uncertainties, such as random demands and leak sizes, were
    considered when synthetic data are used for leakage localisation (Javadiha et
    al., 2019; Zhou et al., 2019b) and leakage detection (Fan et al., 2021). Acoustic
    and vibration signals, which are generated by leaks from pressure changes by cracks
    when elastic waves are propagated through the pipe, can be learnt by CNNs (Kang
    et al., 2018; Nam et al., 2021) or autoencoders (Cody et al., 2020) to classify
    normal or abnormal events. Kang et al. (2018) trained and tested a one-dimensional
    CNN on a dataset consisting of 1580 normal and 660 abnormal 10-second signals
    pairs, collected from six accelerometer sensors in a looped WDS in Seoul. The
    dataset was segmented into 1-second signals and labelled for learning. The detection
    accuracy was improved by data preprocessing through a general denoising method
    and a bandpass filter for extraction of leak frequency bands or by integrating
    a SVM into the CNN to provide a diverse feature classification. In the study of
    Shukla and Piratla (2020), a CNN model adapted from a pre-trained AlexNet network
    was used to detect leaks on polyvinyl chloride (PVC) pipelines using scalogram
    images, which are the wavelet transformation of raw acceleration signals without
    any preprocessing such as noise reduction or application of filters, from an experimental
    pipeline test bed, and it can predict leakage sizes and locations, in addition
    to leakage detection. An autoencoder was applied by Cody et al. (2020) based on
    spectrograms of acoustic data, which uses a 2D CNN for preprocessing of the spectrograms,
    followed by a variational autoencoder layer to reach the latent layer. The autoencoder
    was tested using data collected from a laboratory test bed that was connected
    to a municipal water system via a service line, thus ensuring realistic baseline
    variation, and it achieved an accuracy of 97.2% for detecting a 0.25 L/s leak.
    In addition to leakage detection, the autoencoder model has been used to detect
    the anomalies of the internal surface of water pipelines using CCTV video (Jiao
    et al., 2021). The input data format for CNNs has been studied in the literature
    as it is closely related to feature extraction and thus affects the detection
    accuracy. As CNNs are well-suited for processing 2D images, previous research
    attempted to convert one-dimension pressure signals to 2D images for leakage detection
    and localisation (Javadiha et al., 2019; M. Zhou et al., 2019a), or convert acoustic
    signals to 2D images (Cody et al., 2020). However, this process may lead to the
    loss of some useful information and increase in computational costs (Zhou et al.,
    2021). Thus, one-dimensional CNNs (1D CNNs) are now often employed to directly
    process original 1D time series signals for leakage detection and localisation.
    For example, 1D CNNs were used extract features directly from vibration signals
    (Kang et al., 2018) and pressure data (Fang et al., 2019; Zhou et al., 2021).
    However, the raw signals (i.e., acoustic data) could be transformed through different
    techniques such as Fast Fourier Transform (FFT), wavelet transforms, and time-domain
    features, before being input to a 1D CNN for training. Rahimi et al. (2020) showed
    that converting the acoustic signal to a 1D image through FFT can effectively
    help detect leakage in plastic and composite water tanks and thus significantly
    improve the performance of CNNs. In this direction, a new advance is the development
    of a novel time-frequency CNN by Guo et al. (2021a), which can capture the leakage
    spectrograms through three different resolutions, i.e., high-frequency, high-time,
    and transitional time-frequency resolutions. Attempts to apply CNNs to other aspects
    of leakage detection have been made such as transfer learning to fine tune CNNs
    building on the knowledge from a pre-trained CNN model such as AlexNet (Shukla
    and Piratla, 2020; Zhou et al., 2021), and leakage zone identification using spatial
    clustering and CNNs (Hu et al., 2021b). Further efforts have been made to use
    satellite images for leakage detection by training a CNN model. However, such
    approaches do not support real-time continuous monitoring and are more suitable
    for large leaks, and tend to result in high rates of false alarms owing to the
    resolution of the satellite images (Shukla and Piratla, 2020). In summary, CNNs
    are the only widely used deep learning method for leakage detection using either
    flow/pressure data or acoustic/vibration data, and the focus has been on how to
    best capture anomalous signals by improving training data size or data format
    transformation. However other deep learning algorithms such as LSTM, GAN and GNN
    should be explored to capture spatial and temporal relationships from multi-source
    and multi-site data. 3.1.3. Contamination and water quality One challenge in anomaly
    detection in high-volume data is the unbalanced data problem due to the typical
    low frequency of anomalous events and highly variable and dynamic sensor data.
    This was tackled in the water quality anomaly detection competition series organised
    at the Genetic and Evolutionary Computation Conference (GECCO), where the real-world
    drinking water quality dataset used is extremely imbalanced with the ratio of
    abnormal events being 1.452% only. When tested on this dataset, a balanced LSTM
    model, which includes a fixed rate of 10% positive samples in each batch of training
    data, was shown to achieve a higher F1 score (a combined measure of the model''s
    precision and recall) of 0.7819 than the standard LSTM and other machine learning
    methods (Qian et al., 2020). Using the same data set, Muharemi et al. (2019) also
    showed that the LSTM and a ‘deep’ network (with three hidden layers of six neurons
    each) have a high performance with an F score of 0.9 outperforming traditional
    machine learning methods except SVM, when all methods were trained using time
    series cross validation. However, all the methods generalized badly to a new data
    set. Chen et al. (2018a) used a 1D CNN, consisting of two convolutional, two max-pooling
    layers and two fully connected layers (each with 128 neurons) to extract the features
    in raw water quality data before they are fed into a bi-directional LSTM, and
    they claimed the CNN-LSTM approach was suitable for water quality detection problems
    but no performance results were provided. In general, deep learning methods outperform
    traditional machine learning methods in terms of feature learning accuracy and
    fewer false positive rates, though a fair comparison between different studies
    is difficult due to different datasets, models and parameters employed (Dogo et
    al., 2019). The broad range of possible anomaly types poses another challenge
    in water quality anomaly detection. Water quality data from in-situ sensors are
    likely to encounter the following common anomalies with decreasing priority: large
    sudden spike, low variability (persistent values), constant off-set, sudden shift,
    data oscillation, drift and small sudden spike (Leigh et al., 2019). Rodriguez-Perez
    et al. (2020) applied LSTMs to classify different types of river water quality
    anomalies considering two parameters (i.e., turbidity and conductivity) separately,
    and they found that sudden spikes and small sudden spikes are more likely to be
    detected when the LSTM model is trained using ‘normal’ water quality data, whereas
    long-term data drift is more likely to be detected when anomalous data are included
    in the training dataset. They concluded that the LSTM model considerably minimized
    false detection rates in comparison with the regression-based ARIMA approach,
    though its performance varied for different water quality parameters and monitoring
    sites. Water quality anomaly detection has made use of multivariate time series
    data from water quality sensors. This is distinct from leakage detection which
    normally relies on flow and pressure data or acoustic signals. The GECCO challenge
    dataset includes chlorine oxide, pH value, redox potential, electric conductivity,
    turbidity, temperature and flow rate, so all these parameter data are input into
    an machine learning model to identify anomaly events directly (Muharemi et al.,
    2019). However, this approach needs labelled data for training. In another direction
    of research, machine learning models can be used to predict a water quality parameter
    value using other parameter data so the residuals between estimated and observed
    data can be used for outlier classification for each parameter, which can then
    be fused for anomaly event detection (Arad et al., 2013; Li et al., 2022b). To
    fully exploit complex multivariate correlations, GAN-based approaches have been
    developed as they can consider the entire variable set concurrently in order to
    capture the latent spatio-temporal interactions amongst the variables. It has
    been shown that GAN models are well-suited for complex anomaly detection problems
    and have superior performance over existing unsupervised methods when the generator
    and discriminator are both represented by a LSTM network (Li et al., 2019a).  GNN-based
    approaches can effectively learn the relationships between multiple sensors and
    allow users to deduce the root cause of a detected anomaly. Experiments on two
    real-world sensor datasets show that a novel attention-based GNN approach detects
    anomalies with higher accuracies (including precision, recall and F1) than baseline
    approaches including deep autoencoders, LSTM, and GAN models (Deng and Hooi, 2021).
    More importantly, GNN (in particular attention-based networks) can provide a certain
    level of interpretability for the detected anomalies as attention weights indicate
    the importance of the neighbouring nodes (sensors) in modelling the node''s behaviour.
    In summary, the availability of open quality data boosted the application of various
    deep learning methods (LSTM, GAN, and GNN) to contamination detection, however,
    their performance needs to be further tested on measured data which represent
    the multivariate complexity in the real world. 3.2. Urban wastewater systems 3.2.1.
    Sewer defect and blockage The internal surface condition of sewers is traditionally
    assessed manually using Closed-circuit television (CCTV) videos by professional
    inspectors. This process is labour-intensive and time-consuming. In recent years,
    CNNs have been widely used for automated sewer defect detection, more specifically
    for the following tasks: 1) image classification: classifying CCTV images according
    to contained defects; 2) object detection: identifying the types of defects and
    their locations; 3) semantic segmentation: labelling the pixels belonged to a
    defect. Image Classification. Kumar et al. (2018) trained a series of binary-classification
    CNNs, each for a single type of defect only. To reduce the time required for training,
    a single CNN was set up to classify the frames into multiple defect classes by
    Meijer et al. (2019). Apart from sewer defects, Gutierrez-Mondragon et al. (2020)
    trained a CNN to identify the obstruction level of sewer pipes. Some popular CNNs
    in the computer vision domain have been tested on sewer defect detection. For
    instance, Hassan et al. (2019) fine-tuned AlexNet to extract feature maps from
    sewer frames. To tackle the imbalance between the datasets of defective and normal
    pipes, a hierarchical classification structure was used: a high-level classification
    that classifies normal and defective pipes, followed by a low-level classification
    that classifies defective pipes into specific types of defects (Xie et al., 2019).
    Similarly, ResNet18 was adopted by Li et al. (2019b) in the hierarchical structure,
    which contains residual learning operations to enhance learning process (He et
    al., 2016). Chen et al. (2018b) used a lightweight network called SqueezeNet for
    high-level classification, and InceptionV3 for low-level classification due to
    its relatively high recognition ability. Various techniques have been considered
    in CNNs to improve efficiency and accuracy. For example, Chen et al. (2019) improved
    a binary classification CNN for sewer defects by introducing a cost-sensitive
    activation layer and Cost-Mean Loss. Kumar et al. (2020a) leveraged a CNN interpretation
    technique called class activation mapping to visualize the learnt weights and
    then guide the adjustments of CNNs. Moreover, the text shown in the sewer inspection
    frames normally includes the pipe property and the driving distance from the starting
    point, which reveals the location of the frames. Therefore, CNNs were used to
    extract the distance and thus determine the location of defective frames (Moradi
    et al., 2020). Object Detection. Prior research aimed to detect not only the types
    of defects, but also the locations of defects in the frames. Further, multiple
    types of defects may be contained in the same image, which is a difficult issue
    for classification models. Object detection models can be applied to solve these
    issues. Current CNN-based approaches can be mainly divided into two groups: 1)
    region-based or two-stage detection, which means that regions of interests need
    to be first extracted by a separate network, 2) one-stage detection, in which
    no region is required. A two-stage detection method called faster R-CNN were used
    by Cheng & Wang (2018) and Zhang et al. (2018d). Specifically, Cheng & Wang (2018)
    used the Zeiler-Fergus network as the CNN part for feature extraction, while Zhang
    et al. (2018d) used VGG-16 to extract features. In contrast, Yin et al. (2020)
    used a one-stage network called YOLOv3 for real-time automated sewer defect detection.
    Kumar et al. (2020b) carried out a comparison of three methods, i.e., YOLOv3,
    faster R-CNN and single-shot detector, and concluded that YOLOv3 is suitable for
    onsite detection due to its faster speed, while faster R-CNN is more suitable
    for offsite review due to its superior detection accuracy. Moreover, in Wang et
    al. (2021a), defect tracking was proposed based on the detection results of a
    faster R-CNN to facilitate the counting of the number of defects across consecutive
    video frames. Semantic Segmentation. Semantic segmentation models can annotate
    each pixel of detected objects in the images. Kunzel et al. (2018) applied a two-data-stream
    CNN named Full-Resolution Residual Network (FRRN) to unrolled and stitched CCTV
    frames for automatic detection and classification of defects and structural elements
    in sewer pipes. Pan et al. (2020) segmented sewer defects by adding feature reuse
    and attention mechanism blocks in CNN-based U-Net. Wang & Cheng (2020) integrated
    a deep CNN with dilated convolution called DilaSeg with a recurrent neural network
    (RNN) formulated from dense conditional random field (CRF), where DilaSeg works
    for feature map extraction and CRF-formulated RNN is responsible for resolving
    the local ambiguities. Furthermore, sewer condition assessment was proposed by
    Wang et al. (2021b) to evaluate the severity of operation and maintenance defects,
    based on semantic segmentation results. In summary, the superpower of CNN-based
    algorithms in image processing has been leveraged for sewer defect detection,
    with YOLO and faster R-CNN showing a clear advantage over other deep learning
    and traditional machine learning methods, however, only a few types of defects
    were considered in most studies so future work should investigate more defect
    types aiming to provide a condition assessment for sewers. 3.2.2. System state
    prediction Recent research showed that GRU and LSTM models have a better performance
    than traditional ANNs in predicting CSO water depth. Using monitored water depth
    and rainfall data in a real-world case study, the deep learning models improve
    the generalisation for multi-site CSO prediction by leveraging spatio correlations
    across multiple sites in addition to making use of temporal trends at individual
    sites (Zhang et al., 2018a). Sewer flow and water depth are commonly predicted
    based on rainfall data and observed flow and water depth data at previous time
    steps. In the case of predicting CSO water levels, Palmitessa et al. (2021) investigated
    the predictive accuracy of LSTM networks in scenarios of limited or missing antecedent
    observations, and they found that LSTM networks were capable of compensating for
    the missing observed data with the other input data (e.g., time of the day and
    rainfall intensity). Because infiltration process is not negligible, use of groundwater
    level data as an additional input can improve the performance of LSTMs in predicting
    the flow at various sites of a sewer system (Sufi Karimi et al., 2019). Application
    of predicted system behaviours to operation practices have been demonstrated in
    the literature. The high predictive capability of LSTMs in sewer flow modelling
    was used for improving in-sewer storage control in order to reduce overflow at
    the WWTP (Zhang et al., 2018b), and for the operation of the WWTP (Zhang et al.,
    2018c). Dairi et al. (2019) developed a hybrid RNN-RBM method to predict multivariate
    water quality influent time series, which was then used for anomaly detection.
    This approach can help monitor and detect abnormal influent conditions that can
    affect the operation of WWTPs, thus improving operational resilience. Amongst
    six GRU and LSTM variants, the LSTM model shows an overall high accuracy in predicting
    the influent flow, influent temperature, influent biochemical oxygen demand (BOD),
    effluent chloride, effluent BOD, and power consumption in a WWTP (Cheng et al.,
    2020). The COD mass flow in the WWTP was predicted based on 1-minute measured
    data for temperature, pH, NH3-N, sewage inflow and influent COD, with a hybrid
    CNN-LSTM model, which can support the development of feedforward control systems
    for aeration and chemical dosing (Wang et al., 2019). Similar to demand prediction,
    RNN and LSTM models have been predominantly used for prediction of the key state
    variables of urban wastewater systems including water quality, flow and water
    level at various components and CSOs in both the sewer system and the WWTP. However,
    more work is needed to identify the complex relationships between variables in
    both wastewater system and demand prediction. 3.3. Urban flooding 3.3.1. Data
    processing for hydrodynamic flood modelling The predictive accuracy of hydrodynamic
    flood models significantly relies on high resolution data (e.g., catchment and
    weather data), however, the availability of such data is a main challenge in many
    cities. Deep learning can play a key role in processing big data of aerial photography,
    LiDAR data, satellite data and radar weather data to generate high resolution,
    multispectral data for improving urban flood modelling (Pollard et al., 2018).
    Deep learning has gained wide application in remote sensing, due to its power
    in information extraction from raw images. The most commonly used models are CNNs,
    RNNs, autoencoders and their hybrids. Applications include image segmentation,
    land use classification, terrain attribute extraction, object identification (e.g.
    building, bridges), and multi-source image fusion, however, a detailed review
    of these areas is out of the scope of this review and more information can be
    found in Shen (2018) and other reviews in the field of remote sensing and hydrology.
    Processing raw images with deep learning provides high-resolution urban catchment
    data, particularly useful for areas with low-resolution images. The availability
    of semantic information from CNN-based classification enables large-scale 3D city
    reconstruction (Zhu et al., 2017), which could be potentially used for flood damage
    assessment, flood emergency planning, and real-time flood decision analytics.
    Notably, remote sensing imagery has been used for disaster assessment during a
    flood event using deep learning (Yang and Cervone, 2019; Iqbal et al., 2021).
    Deep learning can be applied to provide high resolution weather and flood data
    where no such data are available. CNNs have been used to improve the accuracy
    of rainfall nowcasting at high spatial resolution (Barrington et al., 2019), estimate
    flood extent using images from unmanned aerial vehicles (Hashemi-Beni and Gebrehiwot,
    2021) and monitor water depth using CCTV videos (De Vitry et al., 2019) and crowdsourced
    photos (Alizadeh and Behzadan, 2021). Accurate representation of rainfall and
    flood depth at high resolution provides high quality data to calibrate urban flood
    models. 3.3.2. Urban flood forecasting In the last several years, deep learning
    has been extensively studied for river flow and flood forecasting and fluvial
    flood inundation in hydrology (Kabir et al., 2020; Xu and Liang, 2021; Xu et al.,
    2020), however, it has not received much attention in urban pluvial flood predictions
    mainly due to the challenge in learning large datasets of high resolution urban
    catchment features (Li et al., 2021) and lack of measured flood and water infrastructure
    data. Flood data in urban areas are generally unavailable compared to the availability
    of long river flow records such as the large data set of 30 years from several
    hundred basins (Nearing et al., 2021). A few studies have showed that CNNs are
    capable for urban flood prediction. One example is a hybrid model developed by
    (Guo et al., 2021b) to predict the maximum flood depth for rainfall events. This
    hybrid model uses a convolutional autoencoder to process the urban catchment data
    and a feedforward fully connected neural network, which is attached to the latent
    layer of the autoencoder, to process the hyetograph data. Five terrain surface
    feature maps including elevation, slope, aspect, curvature and masque, are divided
    into patches to train the CNNs. The model provided accurate predictions for areas
    of different characteristics (e.g., flat, steep, around buildings, upstream and
    downstream) when tested on three case studies. The autoencoder model was later
    improved by adding skip connections from encoding blocks to decoding blocks, applying
    average pooling in the encoding part and converting rainfall time series into
    9 event characteristics (Löwe et al., 2021). CNNs were also tested for the assessment
    of urban surface water flood risks using catchment data and outperformed traditional
    machine learning methods such as Naïve Bayes (Li et al., 2021). In summary, deep
    learning has found more applications in data processing than in flood prediction,
    with most algorithms based on CNNs. However, it is worth investigating other architectures
    such as hybrid CNN-LSTM algorithms to improve flood predictive accuracy in the
    future. 3.4. Cyber security and asset monitoring Water and wastewater infrastructure
    is considered as one of the main targets for cyberattacks amongst 16 critical
    infrastructure systems by the US Department of Homeland Security, and it is not
    uncommon to see cybersecurity incidents reported. For example, 25 cyber security
    incidents were reported in 2015 alone in the US, making the water and wastewater
    sector the third most targeted sector after manufacturing and energy sectors (Hassanzadeh
    et al., 2020). Thus the security of urban water infrastructure has drawn increasing
    attention in the practical and research communities. Deep learning methods, including
    LSTM, autoencoders and GNN models, have enabled significant improvements in cyber-attack
    detection in high-dimensional datasets. A LSTM model was developed to detect cyber-attacks
    in a water treatment system, and was trained using data under normal conditions
    and evaluated using 36 different attack scenarios (Inoue et al., 2017). Autoencoders
    were also tested with 14 attack scenarios considering various components such
    as pumps, tanks and controllers in a benchmark water distribution system (i.e.,
    C-Town), and results showed autoencoders substantially outperform traditional
    machine learning methods, i.e., XGBoost and LightGBM (Taormina and Galelli, 2018).
    Erba et al. (2020) evaluated different evasion attacks which modify anomalous
    data to evade deep autoencoder-based detectors. Using two datasets from water
    treatment test-bed systems with various attack scenarios, a novel attention-based
    GNN approach was shown to outperform a set of baseline models including LSTM,
    autoencoders and hybrid models (Deng and Hooi, 2021). Tsiami and Makropoulos (2021)
    showed that a convolutional GNN was able to leverage the inherent interdependencies
    of the SCADA data for cyber-attack detection in water distribution systems and
    interpret model predictions based on feature interdependencies. A deep generative
    model with variational inference was developed for cyber-attacks based on autonomously
    learnt normal system behaviours from raw observations such as pump pressure and
    tank water level (Chandy et al., 2019). Research and innovation in UWS development
    should be prioritised to mitigate the substantial risks and vulnerabilities that
    are created from water digitalisation and the uptake of AI technologies in the
    water sector. It is particularly urgent to develop cybersecurity best-practice
    guidelines for UWSs, as part of critical national infrastructure. As explained
    above, deep learning-based methods, including LSTM, autoencoders and GNN models,
    could be used to increase cybersecurity and enable significant improvements in
    cyber-attack detection in high-dimensional datasets. For example, adversarial
    machine learning has been used to fool detection algorithms (Erba et al., 2020],
    and insight gained could help develop new deep learning detection methods. These
    methods, however, need to be tested with more cyber-attack scenarios from the
    real-world UWSs and consider network traffic data (Taormina et al., 2018). In
    addition to anomaly detection, deep learning can be developed as a soft sensor
    or a surrogate model for water asset monitoring. It is common that measurements
    in water systems are not available due to either no sensors or faults in the cyber
    system. Soft sensing has been considered as a solution to replace missing measurements
    (primary measurements) with predicted values based on the other measurements (secondary
    measurements) available. 2D CNN and LSTM models were trained using one-minute
    operation data collected from 100 sensors for one year in a water treatment works
    and their results were combined using multiple linear regression to achieve a
    significantly higher predictive accuracy (measured by root mean square errors)
    than individual CNN and LSTM models (Cao et al., 2018). This ensemble model can
    be used as a soft sensor to predict flow and water level, in case of any missing
    data from the 100 sensors. Similarly LSTM models can also provide accurate predictions
    for key variables in wastewater treatment plants (WWTP) (Cheng et al., 2020).
    The use of deep learning models as a surrogate of hydraulic models was also demonstrated
    using a deep belief network, which is a variant of multi-layer perceptron networks
    with stacked restricted Boltzmann machines (Wu and Rahman, 2017). A GNN based
    on K-localized spectral filtering was used to re-construct the pressures at all
    nodes from a limited number of nodal pressure measurements, and this approach
    was shown achieving a relative error below 5% on average with an observation ratio
    of 5% when tested with three benchmark networks (Hajgató et al., 2021). This shows
    the promise of using GNNs as a soft sensor or a surrogate model for pressure monitoring
    across the entire network. In addition to system states, GNNs were used to fill
    missing pipe attribute data (i.e., diameters and materials) in wastewater networks
    (Belghaddar et al., 2021). A CNN model was used to monitor the changes of the
    Fat-Oil-Grease layer and various hydraulic processes in the pump sump in a wastewater
    system (Moreno-Rodenas et al., 2021), and could potentially be used to predict
    pump sump failure. In summary, cyber security and asset monitoring have received
    significantly increasing research efforts with a diverse range of algorithms including
    LSTM, autoencoders and GNN models tested. However, the research questions in these
    areas are similar to those in leakage, contamination, and blockage detection and
    localisation and thus data and experience could be shared. 3.5. System control
    and operation DRL has emerged as a new technology for real-time control and operation
    and has received applications in many fields including water resources and hydropower
    reservoir operation (Xu et al., 2021). However, DRL has not received much attention
    in UWSs. Applications are mainly reported in urban drainage systems and WWTPs
    which are introduced below. Only one application was found in water distribution
    systems, and it tested the DQN-based DRL for pump control using two benchmark
    water distribution systems, i.e., Anytown and d-town (Hajgató et al., 2020). The
    pump speed was determined considering the system states - nodal pressures and
    pump speed ratios, and maximizing a reward which was formulated considering the
    three objectives including the number of nodes exceeding the required pressure
    ranges, pump efficiency and feed ratio of the pumps. They showed that the DQN
    can achieve comparable results to some commonly used optimisation algorithms.
    3.5.1. Flood control of urban drainage systems DRL has been used for flood risk
    management through developing control strategies of retention (or detention) ponds.
    Mullapudi et al. (2020) developed a DQN based DRL algorithm for real-time, non-predictive
    control of a distributed stormwater system with multiple detention ponds, which
    has an objective of achieving desired water levels and flows in the system using
    the water level and outflows at each control site as the state variables. The
    algorithm was effective for control of individual detention ponds but it was proven
    challenging for system-level control with multiple ponds due to temporal dynamics,
    system interactions and high dimensionality. Research has shown that flood control
    of urban drainage systems can be improved through policy-based deep learning with
    incorporation of rainfall forecasts into decision making. Bowes et al. (2021)
    applied a DDPG actor-critic algorithm to flood control of urban drainage systems
    which operates the valves at the bottom of retention ponds. Different from DQN-based
    approaches, this algorithm can control valves over a continuous action space.
    It was tested on a hypothetical urban catchment which has two sub-catchments,
    two retention ponds controlled by valves and a tidally influenced water body.
    The outputs from SWMM simulations were used to train the algorithm. The system
    state is represented by the current flood depth and volume at the ponds and downstream
    nodes, the current valve positions, the sum of the 24 h rainfall forecast for
    each subcatchment, and the mean value of the 24 h tide forecast. The actions that
    the agent can take at any step are to close or open any valve to any degree. The
    reward is formulated based on how well the agent prevents flooding and maintains
    certain target pond water depths. RL is shown to outperform model predictive control
    (MPC) and rule-based control strategies, by effectively learning to proactively
    manage pond levels using current and forecast conditions. Further, the DDPG algorithm
    is shown to be robust when considering uncertainties in input data (i.e., rainfall
    forecasts) and system states (i.e. water level) (Saliba et al., 2020). Previous
    research has shown the promise of using DRL for automated control of urban drainage
    systems to reduce flooding. However, a number of challenges arose from learning
    real-time control rules for complex systems, for example, the formulation of reward
    functions to guide system behaviours, control of multiple distributed storage
    tanks, handling of multiple objectives, use of future forecasts and choice of
    different DRL approaches (Blumensaat et al., 2019; Bowes et al., 2021; Mullapudi
    et al., 2020). 3.5.2. Wastewater treatment plants RL has been applied to reduce
    operational costs in WWTPs. Using a WWTP Benchmark Simulation Model 1 (Gernaey
    et al., 2014), Hernández-del-Olmo et al. (2016) showed that RL can better control
    the DO set points of a proportional-integral (PI) controller considering the system
    state variables (i.e., ammonium and DO concentrations) and save operational costs
    when compared to manual operation and ammonium-based PI controllers. Q-learning
    based RL was also tested for the control of the advanced oxidation process of
    phenols using Fenton''s reagent in a laboratory plant (Syafiie et al., 2011) and
    for optimising the hydraulic retention time of anaerobic and aerobic reactors
    (Pang et al., 2019). In a model-free RL approach, it is important to introduce
    a shadowing period for RL to learn from human operations before the RL agent is
    deployed to control the WWTP. Hernández-del-Olmo et al. (2018) showed that an
    initial shadowing period of 30 days improves dramatically the agent''s learning
    speed and reduces significantly the operational cost, though a longer shadowing
    period can make the learning more effective In addition to the traditional RL,
    a few applications of DRL have been made in WWTPs. A policy-based DRL was developed
    for energy consumption reduction through controlling pumps between primary and
    secondary treatment, and it used probabilistic inflow forecasts to minimize energy
    consumption and reduce the number of alarms for tank level exceeding the limits
    (Filipe et al., 2019). Further, a multi-agent DDPG approach was applied to control
    dissolved oxygen and chemical dosage in a WWTP under continuous action and state
    spaces (Chen et al., 2021). In this approach, various reward functions were tested
    in order to develop sustainable control strategies. In summary, various policy-
    and value-based DRL approaches have been applied to system control of UWSs, though
    the number of applications is limited and most in urban wastewater systems. Compared
    to the other deep learning algorithms, however, there was lack of comparisons
    of DRL applications with non-deep learning methods such as evolutionary optimisation.
    3.6. Real-world application The application of deep learning methods to UWSs is
    still at an early stage as most reported studies have used benchmark networks,
    synthetic data, and laboratory-based or pilot systems. Real-world systems and
    monitoring data have been used in the literature, however, they were mainly for
    model development and demonstration of deep learning potential in comparison to
    traditional machine learning models. The studies reviewed generally aimed to demonstrate
    improved performances of deep learning in comparison to traditional machine learning
    models or to develop an improved deep learning architecture through comparing
    different architecture designs. To the best of our knowledge, there is no reporting
    of deployed applications into day-to-day operation and management of real-world
    UWSs with measurable benefits or lessons learned from an innovative use of deep
    learning technology. learning. That is, no deep learning methods have reached
    the slope of enlightenment stage on the Gartner hype cycle curve (Fig. 3). Download
    : Download high-res image (180KB) Download : Download full-size image Fig. 3.
    Deep learning application to urban water management problems on the Gartner curve.
    The water applications reviewed have received varying levels of attention and
    have advanced to different stages of maturity (Fig. 3). Leakage detection is the
    most popular problem for deep learning research in urban water management, which
    is mainly due to data availability and the drive to reduce leakage and resources
    (i.e., water and energy) consumption world-wide. Recall that various sensors have
    been deployed in the water industry to collect water demand, pressure, acoustic
    and vibration data, all of which have been applied to leakage detection. Leakage
    detection is perhaps in the stage of trough of disillusionment on the Gartner
    hype cycle curve where deep learning fails to deliver the high expectation but
    interest in developing new methods and tools continues as demonstrated by the
    Battle of the Leakage Detection and Isolation Methods (Liu et al., 2019). It is
    likely to move to the next stage – slope of enlightenment – where it will receive
    wider practical implementation. A strategy towards wider industrial implementation
    may involve demonstrating the reliability of deep learning technologies through
    pilot studies, benchmarking with domain knowledge and other existing methods,
    collecting more field data for performance improvement and developing the next
    generation of deep learning models. This process may be iterative and the slope
    of enlightenment can be long, thus it is key to establish close collaborations
    between deep learning researchers and water companies before mainstream adoption
    starts to take off, reaching the Plateau of Productivity. Sewer defect detection
    is largely at the same stage on the Gartner curve as leakage detection. This problem
    was expected to capitalize on the power of deep learning in image processing (Lecun
    et al., 2015), however, challenges have arisen due to lack of large data sets
    and the effort required for labelling, the complexity of various defects and the
    difficulty in providing direct support for pipe maintenance investments. Building
    on more open access images recently made available (Haurum and Moeslund, 2021)
    (Table 1), research should focus on how deep learning methods are developed to
    streamline sewer condition assessments for directly supporting investment decisions.
    On the contrary, research on contamination detection, though having received much
    attention, is based on simplified case studies and data sets, and it needs to
    make a breakthrough in tackling the complexity of real pollution events before
    reaching the peak of inflated expectations. Application of deep learning to asset
    monitoring has received high expectations as part of the recent development of
    predictive maintenance. Predictive maintenance seeks to maximize the value of
    assets throughout their lifecycle building on predicted system states, which are
    normally learnt from large amounts of data. Compared with other systems such as
    manufacturing systems where the benefit of predictive maintenance has been demonstrated,
    UWSs are more complex with a large number of components and state variables affected
    by highly uncertain environments, but generally with less data for deep learning
    training. The deep learning algorithms developed in the literature are piecemeal,
    unscalable and lack generalisation for real-world UWSs. Amongst system state prediction
    problems, short-term demand forecasting has received the most attention and achieved
    high predictive accuracy, but medium- and long-term forecasting suffers from similar
    data availability and uncertainty- challenges in real-world problems. Flood prediction
    using AI technology at the river basin and catchment scales was rising rapidly
    in recent years, though its role in hydrology is still debated (Nearing et al.,
    2021). This was partially driven by efforts from big technology companies such
    as Google (Nevo et al., 2019). However, the application of deep learning to urban
    flooding, where only a few studies were reported (e.g., Guo et al., 2021b; Li
    et al., 2021), needs more research to tackle flood flashiness and high resolution
    urban features. Examples of system operation have showed the benefits of DRL technology
    as discussed in Section 3.6, which, however, have not been fully appreciated by
    the water research communities. DRL, as the only deep learning algorithm that
    can provide solutions to operation optimisation problems, has been mainly tested
    in the areas of flood control in the sewer system and wastewater treatment operation.
    Further, it has received many more applications in urban wastewater systems than
    water distributions systems as reviewed earlier. This is not surprising as optimal
    operation problems traditionally draw more attention in urban wastewater systems
    than water distribution systems. In addition to optimal operation problems, DRL
    can also solve optimal long-term planning, maintenance and management problems
    (Fig. 2), such as intervention pathway development (Sadr et al., 2020), where
    solutions at a time step depend on the solutions chosen at previous time steps.
    However, it has received no applications in this area. The potential impacts and
    challenges of DRL are discussed in the two future research areas – multi-agent
    systems and digital twins – in Sections 4.4 and 4.5. Overall, it has been demonstrated
    that deep learning is able to achieve higher efficiency and accuracy than traditional
    machine learning models when applied to classification, anomaly detection and
    regression tasks in UWSs (Table 1). However, significant research gaps remain
    in the development of deep learning methods to gain more understanding of the
    processes, systematically improve reliability, resilience and sustainability of
    UWSs, and ultimately building autonomous UWSs (Butler et al., 2016). Research
    advances in the areas identified in Section 4 will help bridge the gaps and propel
    the application of deep learning to industrial implementation. 4. Future research
    challenges We believe that, in time, deep learning will fundamentally transform
    how UWSs will be planned, managed and operated in response to environmental and
    social challenges as has already been achieved in some sectors such as the finance
    and retail sectors (The Royal Society, 2017). At the same time, deep learning
    has started to substantially transform some scientific disciplines, such as high-energy
    physics, astronomy and computational biology (Shen, 2018), and is already transforming
    water research as reviewed in Section 3. Here we discuss five potential research
    challenges that need to be addressed to advance water engineering and science
    and boost deep learning-powered application to solve real-world water problems
    in the face of environmental change. Fig.4 illustrates the key research areas.
    In addition to research challenges, the industrial application of AI in the water
    sector is affected by many other challenges such as data silo, public policy,
    water regulation, culture, work force, institutional management and wider AI ecosystem
    (Garrido-Baserba et al., 2020; IWA, 2019), which are not discussed in this review.
    Download : Download high-res image (499KB) Download : Download full-size image
    Fig. 4. Five key research challenges in use of deep learning (more generally AI)
    for urban water system management to tackle social-environmental change. 4.1.
    Data privacy The availability of big data was one of the driving factors that
    lead to the breakthroughs in deep learning, at the same time, this technology
    is bringing challenges and risks that are related to data such as availability,
    quality, accessibility, security, privacy and cyber-attack. For example, the high-resolution
    water consumption data from smart meters can reveal personal privacy and behaviours,
    so how can water companies be willing to open their data for deep learning development
    while they should recognise privacy invasion and associated risks arising from
    data sharing in the legal framework for data protection, such as the new EU General
    Data Protection Regulation? Privacy Enhancing Technologies (PETs) should be developed
    and deployed to support effective data sharing and collaborative learning over
    distributed data. PETs provide a solution to share data and train AI algorithms
    without the need of pooling data or sharing raw data. This might be particularly
    suitable for deep learning-based approaches, which could be potentially trained
    using data across distributed data centres or mobile devices (sensors) in different
    water utilities. PETs have been extensively studied in the computer science and
    machine learning research communities, in particular with a huge surge in fundamental
    research. However, their application to the water sector at scale requires further
    research. Amongst many PETs, federated learning has received growing interests
    as it helps to protect data generated on a device (or a water utility) by training
    a deep learning model locally and sharing model updates such as gradient data
    instead of raw data. Federated learning has been deployed by many big technology
    companies, so they can play a critical role in supporting privacy-sensitive applications
    in the water sector where the training data are distributed at the edge (Li et
    al., 2020), . Use of synthetic data is an emerging approach to significantly accelerate
    the development of deep learning models. This approach can make use of the high-fidelity
    models that have been invested in the water sector in the last decades to generate
    large data sets for deep learning training. amongst other advantages, it can effectively
    reduce data privacy risks and increase the size of training data sets. As reviewed
    in Section 3, many studies relied on synthetic data generated by physically-based
    UWS models for deep learning training. GAN can produce new (both structured and
    unstructured) data sets that resemble the training data set with similar data
    structures and spatiotemporal dependencies, thus represents an important research
    area for wide adoption of deep learning technologies in data-limited situations.
    4.2. Algorithmic development and learning system design Designing an appropriate
    algorithm to a given problem is a key challenge in the development of deep learning
    models and this will encourage their application to real-world water problems.
    Some choices (e.g. supervised or unsupervised, regression or classification) are
    trivial and straightforward when the real-world water problem is clearly formulated
    and the training data (e.g., time series or image data) are identified. However,
    in many cases it is a difficult task to choose a proper method given a myriad
    of deep learning algorithms available. For example, CNNs, LSTMs, GANs and hybrid
    algorithms have all been used for contamination detection (Section 3.1.3). Even
    when the algorithm is chosen, the next challenge results from determining its
    architecture to achieve the best performance. For example, amongst many other
    factors, the input data format (1D or 2D), the number of convolution layers, and
    the size of the filters need to be determined for a CNN before it can be trained
    with the data. The network architecture is normally designed and tested manually,
    which is a time-consuming and error-prone process. However, this problem, referred
    to as a network architecture search (NAS) problem, can be solved using optimisation
    algorithms. NAS is a subfield of automated machine learning, which aims to automate
    the task of applying machine learning to real-world problems, covering the entire
    process from the raw data to model deployment. Previous research has shown that
    NAS approaches outperform manually designed network architectures on many tasks
    such as computer vision, however, applications in water management are limited
    with a few studies such as design of CNNs for algal classification in river catchments
    (Park et al., 2019). Further, the capacity of NAS in designing a hybrid network
    (e.g., CNN+LSTM) needs to be investigated for complex water problems. End-to-end
    learning is enabled by deep learning to train a single learning model for complex
    problems, which normally consist of a sequence of tasks that are solved by learning
    models separately. End-to-end learning allows a single model to produce the required
    outputs directly from inputs, without deep knowledge of the specific, intermediate
    tasks. One example is sewer condition assessment, which normally involves the
    following tasks: 1) image pre-processing to remove noise and improve the image
    quality, 2) image segmentation to detect pipe components and pipe defects, 3)
    feature extraction using feature descriptors such as histograms of orientated
    gradients, Scale Invariant Feature Transform or GIST, 4) defect classification
    according to different types of defect, and 5) condition assessment to determine
    defect severity and grading. Each of the tasks can be conducted using a range
    of machine learning models. With end-to-end learning, however, a single possibly
    complex model can be trained for sewer condition assessment using raw images.
    This learning approach thrives on a very large training data set to achieve a
    certain accuracy, which is a key factor limiting its application to solving real-world
    problems. Development of small data-based deep learning approaches is essential
    to advance the application of deep learning in the water sector. Though data are
    being collected at an unprecedented speed in the sector, big data are in many
    cases still not available for urban water infrastructure systems (Makropoulos
    and Savic, 2019), especially compared with other sectors such as finance and retail.
    The superior performance of deep learning approaches normally relies on large
    amounts of training data, however, new approaches should be developed for deep
    learning models with small data. First, more efforts are needed to investigate
    how to iteratively improve the quality of existing data in order to improve the
    performance of a fixed AI model. This is a data-centric AI approach promoted by
    Ng (2022), which plays a key role in the development of AI products in industry.
    Second, transfer learning is an approach to exploit deep learning models that
    are trained using external data from another task, beyond the available data on
    the current task, but it has received less attention in urban water applications
    and should be further studied to leverage the benefits of the recent AI advances.
    Third, incorporating domain knowledge into deep learning models is another approach
    to reduce the need for large datasets in order to boost accuracy. Domain knowledge
    based deep learning has been applied to fault diagnosis of pipelines (Feng et
    al., 2021) and medical image analysis (Xie et al., 2021). In UWS design problems,
    domain knowledge (e.g., physical laws) has been demonstrated to significantly
    improve the optimisation efficiency (Liu et al., 2020), however, research is required
    for real-world deep learning applications, though the potential of incorporating
    domain knowledge into deep learning for image analysis tasks has been demonstrated
    using different approaches, such as fusing hand-crafted features into deep learning
    at the input-, feature- and decision-levels and attention mechanism designed to
    represent radiologists’ knowledge (Xie et al., 2021). 4.3. Explainability AI explainability
    has drawn increasing attention as AI technologies are increasingly used in our
    society. Many machine learning (in particular deep learning) models are used as
    ‘black boxes’ which can provide accurate predictions once trained, but it is difficult
    to explain how these predictions are generated or what features are important
    in making such a prediction. Explainability is a key principle or embedded in
    other principles such as transparency, fairness and accountability in many AI
    development frameworks adopted by companies and governments for building trustworthy
    AI systems (The Royal Society, 2019), thus it is discussed below. It is widely
    recognised that there is a trade-off between performance and explainability in
    AI models. For example, linear regression has high explainability but suffers
    from low performance, and deep learning suffers from low explainability but has
    high performance. The trade-off should be explicitly explored by AI system developers
    and made clear to users, aiming at achieving a good balance between performance
    and explainability. Compared to conventional machine learning algorithms, developing
    explainable deep learning has unique difficulties arising from (Samek et al.,
    2021): 1) multi-scale and distributed nature of network representations, 2) instability
    from the high depth of networks, 3) searching a reference data point on which
    to base the explanation, 4) evaluation of explainability which allows for comparisons
    of AI methods and solutions. A diversity of approaches have been developed to
    interpret deep learning at the global and local levels in the literature but need
    to be tested on UWS applications. Local explainability aims to understand what
    input features contribute positively or negatively to each decision (or sample),
    while global explainability focuses on making the entire process of model reasoning
    transparent and understandable, contributing to model validation and knowledge
    discovery. Often, developers and users need different levels of explainability,
    for example, users need local approaches to understand how a specific decision
    is made from the data while developers might need global approaches to understand
    how an AI system works (The Royal Society, 2019). The most influential inputs
    or features in determining an output can be identified through perturbation, masking
    or removing different parts of inputs to analyse how the output changes. These
    approaches provide a local level of explainability effects. Interpretable models
    (e.g., linear regression) can be developed to approximate a deep learning model
    locally and globally for explanations. A popular method is called local interpretable
    model-agnostic explanations (LIME), for text and image classification problems.
    Another method SHAP (SHapley Additive exPlanations) was applied to identify the
    most important environmental factors and their interactions for beach closure
    (L. L. Li et al., 2022). The need for explainable AI varies in different domains.
    Many UWS applications (Table 1) in the water sector are rather different from
    those in some sectors such as healthcare and justice, where the decision of AI
    systems has a big impact on people. For example, the outcome of deep learning-based
    leakage detection systems has no direct consequences on customers from false positive
    predictions, and they can still be deployed without giving rise to concerns about
    explainability, in particular when their accuracy has been well validated. In
    many UWS planning and management applications, deep learning systems are not used
    for automated decision-making, instead their predictions are fed into a complex
    human decision-making process. In the example of leakage detection, the detection
    results can be verified by experienced operators or collecting real-time data
    using mobile loggers and then used to inform network maintenance investment decision-making.
    From a wider perspective, however, increasing the explainability of machine learning
    systems is desirable for developing better AI models and applications in the water
    sector due to 1) explainability can help us extrapolate an machine learning system''s
    behaviour to situations in which it has not been explicitly tested (The Royal
    Society, 2017), and identify situations in which it may fail and 2) explainability
    should be developed in a wider context of AI principles. However, stakeholder
    engagement should be used to define what form of explainability is useful for
    UWS applications. 4.4. Multi-agent systems A multi-agent system consists of multiple
    intelligent agents which interact in a shared environment to achieve common or
    conflicting goals. An agent can be a software component, robot, or person. Each
    agent typically has its own observations of the environment, actions and goals,
    but critically it interacts with other agents through its actions or changes to
    the shared environment. The agents can be cooperative, competitive, or mixed.
    Multi-agent systems can be used to tackle dynamic interactions between different
    components of the UWS system and the environment, which become important with
    increasing system complexity and uncertainty. Agent-based models have been widely
    applied to river basin and hydrological systems, with a goal to understand the
    collective behaviours of multiple agents and develop land and water management
    options (Yang et al., 2009). However, there are few applications of multi-agent
    systems in the UWS, focusing on designing agents and solving a technological problem.
    Potential applications are discussed below. Co-operative multi-agent systems are
    needed for autonomous decision-making of optimal planning and operation problems.
    In particular, multi-agent systems can be combined with the latest DRL technology
    to develop an autonomous decision-making framework that can generate optimal actions
    in response to a dynamic environment. One example is the development of a multi-agent
    DRL approach to simultaneously optimise DO and chemical dosage in a WWTP (Chen
    et al., 2021). Similarly this framework could be applied to control of storage
    tanks and SuDS measures (e.g., retention/detention ponds) in the sewer system,
    control of water tanks and pumps in the water distribution system, or co-operation
    of multiple drones in flood emergency operations. These system components, represented
    by multi-agents, can autonomously act to achieve the objectives defined. In the
    example of pump operations, an agent can be designed to control pumps in one part
    of the water network to meet peak demands by keeping high pressure and another
    agent can be designed to control pumps in an adjacent part of the water network
    to maintain low pressure for leakage management. The two agents need to co-operate
    with each other to meet conflicting operation objectives in the water network
    through effective handling of dynamic environments. In doing so, the application
    of multi-agent systems could build a more decentralized, highly efficient UWS
    system. In addition, agents representing various stakeholders such as landowners,
    urban planners and water users can negotiate their interest in the UWS planning
    and management. Human-machine interactions can be a key research area in the field
    of multi-agent systems and they could be useful in real-time operation and disaster
    recovery of a complex UWS. For example, in the case of emergency operation in
    the aftermath of flooding, the behaviour of human agents should be incorporated
    in assessing the effectiveness of rescue and recovery operations by other agents
    such as unmanned aerial vehicles. Strategic planning and optimal design problems
    are extremely challenging with a large scale of problem domain, a large number
    of stakeholders, a wide range of deep uncertainties and interactions with the
    environment, thus human-machine interactions can be key to tackle these problems
    as in multi-objective optimisation problems (Tang et al., 2020). 4.5. Digital
    twins and autonomous systems The concept of digital twins has generated great
    interest and momentum in the water sector. Though sharing many similar characteristics,
    a digital twin is different from a traditional model which often operates in isolation
    from the physical world. Amongst many definitions of digital twin (e.g., Makropoulos
    and Savic, 2019; Therrien et al., 2020), IBM defines it as a virtual representation
    of a physical system across its lifecycle, using real-time data to enable understanding,
    learning and reasoning. Though there is no consensus on the form of a digital
    twin, it should have the following key features: 1) data-driven coupling of mathematical
    models (i.e., physically-based, machine learning or hybrid) with the physical
    UWS that they represent, 2) integrated with real-time data streams from sensor
    networks so as to represent the true state of the current physical system, 3)
    able to analyse ‘what if’ scenarios and provide predictions of the future state,
    4) closing the loop from the digital twin to the physical system through design,
    maintenance and operation strategies derived from the digital twin, and 5) continuously
    updated with data from the physical world and used in (near) real time simulations
    for improved system performance and services. The development of digital twins
    will be transformational in how we interact with, manage and control the physical
    system. To date, the machine learning component is largely missing in the development
    of digital twins in the water sector. To the best of our knowledge, all the reported
    examples of digital twins were based on physically-based models (Bartos and Kerkez,
    2021; Garrido-Baserba et al., 2020; Pesantez et al., 2022; Valverde-Pérez et al.,
    2021), though a few used machine learning to enhance the performance of hydraulic
    models such as estimation of the operating speed of pumps (Bonilla et al., 2022)
    or the influent to the WWTP (Valverde-Pérez et al., 2021). The deep learning applications
    reviewed in Section 3 can potentially be a key part of digital twins, enabling
    the UWS become an autonomous system through automated operation. Machine learning
    is a powerful technology to not only help improve our understanding of physical
    systems, but also provide solutions to improve system performance. The development
    of explainable AI can further provide an insight into system processes. Physics-guided
    machine learning is a new research direction that leverages the knowledge on the
    system (e.g., physical constraints or process‐based theories) to develop more
    accurate, generalizable machine learning models, and it has found many applications
    in hydrology (Nearing et al., 2021) and water quality (Varadharajan et al., 2022)
    and is potentially useful to develop digital twins of UWSs. In the development
    of digital twins, we envision much of the future advance regarding deep learning
    application will come from the development of systems that combine CNNs and LSTM
    networks to understand and predict system behaviours and then use DRL to decide
    where to search for optimal interventions. Once realised, this will significantly
    empower the digital twin towards achieving high intelligence and autonomy. However,
    this advance is going to be hard won, requiring a great deal of concerted and
    sustained fundamental research and development in all five challenges to fully
    materialize the promise of digital twins. 5. Conclusions Deep learning has been
    widely recognised as a potentially disruptive technology in the age of Industry
    4.0 and has already transformed many sectors. A critical review on the role of
    deep learning in urban water management has found the following key points: 1)
    Deep learning has showed great potential in urban water management relating to
    five key application areas, including anomaly detection, system prediction, asset
    assessment, system operation and planning and maintenance. However, no attempts
    have been made to solve strategic planning, optimal maintenance and intervention
    development problems. 2) The application of deep learning methods in the water
    sector is still at an early stage as most studies used benchmark networks, synthetic
    data, laboratory-based or pilot systems to test the performance of deep learning
    methods. It lacks reporting of deployed applications with measurable benefits
    or lessons learned from an innovative use of deep learning technology. 3) Deep
    learning application to different problems has advanced to different stages of
    maturity on the Gartner hype cycle curve but none has reached the stage of industrial
    application. Sewer defect detection and leakage detection are believed to be more
    advanced than other applications. Significant research efforts are required in
    the development of deep learning methods to fill knowledge gaps in understanding
    water processes and improve system performance before they can gain wider adoption
    in the water sector. 4) Further research on the five challenges, i.e., data privacy
    and cybersecurity, algorithmic development, explainability and trustworthiness,
    multi-agent systems and digital twin, is recommended for shaping urban water management
    fuelled by deep learning technology. The great promise of deep learning lies in
    its empowerment of digital twins towards high intelligence and autonomy of UWSs,
    which we expect will be materialised through the development of deep learning
    systems that combine CNNs and LSTM networks to understand and predict system behaviours
    and then use deep reinforcement learning to decide where to search for optimal
    interventions We hope this review will spark thoughts and actions on future research
    and applications that harness the power of deep learning to help the digitalisation
    of urban water systems and inspire more researchers to join in the water intelligence
    community to revolutionize water research and practice. Declaration of competing
    interest The authors declare that they have no known competing financial interests
    or personal relationships that could have appeared to influence the work reported
    in this paper. Acknowledgements This work was supported by the Royal Society under
    the Industry Fellowship Scheme (Ref: IF160108), the UK Engineering and Physical
    Sciences Research Council under the Alan Turing Institute (Ref: EP/N510129/1)
    and the National Natural Science Foundation of China (Grant No. 42071272). Appendix.
    Supplementary materials Download : Download Word document (40KB) Data availability
    No data was used for the research described in the article. References Alizadeh
    Kharazi and Behzadan, 2021 B. Alizadeh Kharazi, A.H. Behzadan Flood depth mapping
    in street photos with image processing and deep neural networks Comput. Environ.
    Urban Syst. (2021), 10.1016/j.compenvurbsys.2021.101628 Google Scholar Arad et
    al., 2013 J. Arad, M. Housh, L. Perelman, A. Ostfeld A dynamic thresholds scheme
    for contaminant event detection in water distribution systems Water Res., 47 (2013),
    pp. 1899-1908, 10.1016/j.watres.2013.01.017 View PDFView articleView in ScopusGoogle
    Scholar Barrington et al., 2019 L. Barrington, C. Bromberg, J. Hickey, J. Burge
    Machine learning for precipitation nowcasting from radar images 33rd Conf. Neural
    Inf. Process. Syst. (NeurIPS 2019), Vancouver, Canada (2019) Google Scholar Bartos
    and Kerkez, 2021 M. Bartos, B. Kerkez Pipedream: an interactive digital twin model
    for natural and urban drainage systems Environ. Model. Softw., 144 (2021), Article
    105120, 10.1016/j.envsoft.2021.105120 View PDFView articleView in ScopusGoogle
    Scholar Belghaddar et al., 2021 Y. Belghaddar, N. Chahinian, A. Seriai, A. Begdouri,
    R. Abdou, C. Delenne Graph convolutional networks: application to database completion
    of wastewater networks Water (Basel), 13 (2021), p. 1681 CrossRefView in ScopusGoogle
    Scholar Blumensaat et al., 2019 F. Blumensaat, J.P. Leitão, C. Ort, J. Rieckermann,
    A. Scheidegger, P.A. Vanrolleghem, K. Villez How urban Storm- And wastewater management
    prepares for emerging opportunities and threats: digital transformation, ubiquitous
    sensing, new data sources, and beyond - a horizon scan Environ. Sci. Technol.,
    53 (2019), pp. 8488-8498, 10.1021/acs.est.8b06481 View in ScopusGoogle Scholar
    Bonilla et al., 2022 C.A. Bonilla, A. Zanfei, B. Brentan, I. Montalvo, J. Izquierdo
    A digital twin of a water distribution system by using graph convolutional networks
    for pump speed-based state estimation Water (Switzerland), 14 (2022), 10.3390/w14040514
    Google Scholar Bowes et al., 2021 B.D. Bowes, A. Tavakoli, C. Wang, A. Heydarian,
    M. Behl, P.A. Beling, J.L. Goodall Flood mitigation in coastal urban catchments
    using real-time stormwater infrastructure control and reinforcement learning J.
    Hydroinformatics, 23 (2021), pp. 529-547, 10.2166/HYDRO.2020.080 View in ScopusGoogle
    Scholar Butler et al., 2016 D. Butler, S. Ward, C. Sweetapple, M. Astaraie-Imani,
    K. Diao, R. Farmani, G. Fu Reliable, resilient and sustainable water management:
    the Safe & SuRe approach Global Challenges, 1 (1) (2016), pp. 63-77, 10.1002/gch2.1010
    Google Scholar Cao et al., 2018 K. Cao, H. Kim, C. Hwang, H. Jung CNN-LSTM coupled
    model for prediction of waterworks operation data J. Inf. Process. Syst., 14 (2018),
    pp. 1508-1520, 10.3745/JIPS.02.0104 View in ScopusGoogle Scholar Chandy et al.,
    2019 S.E. Chandy, A. Rasekh, Z.A. Barker, M.E. Shafiee Cyberattack detection using
    deep generative models with variational inference J. Water Resour. Plann. Manage.,
    145 (2) (2019), Article 04018093 View in ScopusGoogle Scholar Chen et al., 2021
    K. Chen, H. Wang, B. Valverde-Pérez, S. Zhai, L. Vezzaro, A. Wang Optimal control
    towards sustainable wastewater treatment plants based on multi-agent reinforcement
    learning Chemosphere, 279 (2021), Article 130498, 10.1016/j.chemosphere.2021.130498
    View PDFView articleView in ScopusGoogle Scholar Chen et al., 2019 Y. Chen, S.
    Zhong, K. Chen, S. Chen, S. Zheng Automated detection of sewer pipe defects based
    on cost-sensitive convolutional neural network Proceedings of the 2019 2nd International
    Conference on Signal Processing and Machine Learning (2019), 10.1145/3372806.3372816
    Google Scholar Chen et al., 2018a X. Chen, F. Feng, J. Wu, W. Liu Anomaly detection
    for drinking water quality via deep bilSTM ensemble. GECCO 2018 Companion - Proc
    2018 Genet. Evol. Comput. Conf. Companion (2018), pp. 3-4, 10.1145/3205651.3208203
    Google Scholar Chen et al., 2018b K. Chen, H. Hu, C. Chen, L. Chen, C. He An intelligent
    sewer defect detection method based on convolutional neural network 2018 IEEE
    International Conference on Information and Automation, China (2018) Google Scholar
    Cheng et al., 2020 T. Cheng, F. Harrou, F. Kadri, Y. Sun, T. Leiknes Forecasting
    of wastewater treatment plant key features using deep learning-based models: a
    case study IEEE Access, 8 (2020), pp. 184475-184485, 10.1109/access.2020.3030820
    View in ScopusGoogle Scholar Cheng and Wang, 2018 J.C.P. Cheng, M Wang Automated
    detection of sewer pipe defects in closed-circuit television images using deep
    learning techniques Autom. Constr., 95 (2018), pp. 155-171 View PDFView articleView
    in ScopusGoogle Scholar Cody et al., 2020 R.A. Cody, B.A. Tolson, J. Orchard Detecting
    leaks in water distribution pipes using a deep Autoencoder and Hydroacoustic spectrograms
    J. Comput. Civ. Eng., 34 (2020), Article 04020001, 10.1061/(asce)cp.1943-5487.0000881
    View in ScopusGoogle Scholar Dairi et al., 2019 A. Dairi, T. Cheng, F. Harrou,
    Y. Sun, T. Leiknes Deep learning approach for sustainable WWTP operation : a case
    study on data-driven in fl uent conditions monitoring Sustain. Cities Soc., 50
    (2019), Article 101670, 10.1016/j.scs.2019.101670 View PDFView articleView in
    ScopusGoogle Scholar De Vitry et al., 2019 M.M. De Vitry, S. Kramer, J.D. Wegner,
    J.P. Leitão Scalable flood level trend monitoring with surveillance cameras using
    a deep convolutional neural network Hydrol. Earth Syst. Sci, 23 (2019), pp. 4621-4634
    Google Scholar Deng and Hooi, 2021 A. Deng, B. Hooi Graph Neural Network-Based
    Anomaly Detection in Multivariate Time Series Proceedings of the AAAI Conference
    on Artificial Intelligence, 35 (2021), pp. 4027-4035 CrossRefView in ScopusGoogle
    Scholar Dogo et al., 2019 E.M. Dogo, N.I. Nwulu, B. Twala, C. Aigbavboa, E.M.
    Dogo, N.I. Nwulu, B. Twala, C. Aigbavboa, E.M. Dogo A survey of machine learning
    methods applied to anomaly detection on drinking-water quality data drinking-water
    quality data Urban Water J, 16 (2019), pp. 235-248, 10.1080/1573062X.2019.1637002
    View in ScopusGoogle Scholar Du et al., 2021 B. Du, Q. Zhou, J. Guo, S. Guo, L.
    Wang Deep learning with long short-term memory neural networks combining wavelet
    transform and principal component analysis for daily urban water demand forecasting
    Expert Syst. Appl., 171 (2021), Article 114571, 10.1016/j.eswa.2021.114571 View
    PDFView articleView in ScopusGoogle Scholar Erba et al., 2020 A. Erba, R. Taormina,
    S. Galelli, M. Pogliani, M. Carminati, S. Zanero, N.O. Tippenhauer Constrained
    concealment attacks against reconstruction-based anomaly detectors in industrial
    control systems Annual Computer Security Applications Conference (2020) 2020 Google
    Scholar Fan et al., 2021 X. Fan, X. Zhang, X.B. Yu Machine learning model and
    strategy for fast and accurate detection of leaks in water supply network J. Infrastruct.
    Preserv. Resil., 2 (2021), 10.1186/s43065-021-00021-6 Google Scholar Fang et al.,
    2019 Q.S. Fang, J.X. Zhang, C.L. Xie, Y.L. Yang Detection of multiple leakage
    points in water distribution networks based on convolutional neural networks Water
    Sci. Technol. Water Supply, 19 (2019), pp. 2231-2239, 10.2166/ws.2019.105 View
    in ScopusGoogle Scholar Feng et al., 2021 J. Feng, Y. Yao, S. Lu, Y. Liu Domain
    knowledge-based deep-broad learning framework for fault diagnosis IEEE Trans.
    Ind. Electron., 68 (2021), pp. 3454-3464, 10.1109/TIE.2020.2982085 View in ScopusGoogle
    Scholar Filipe et al., 2019 J. Filipe, R.J. Bessa, M. Reis, R. Alves, P. Póvoa
    Data-driven predictive energy optimization in a wastewater pumping station Appl.
    Energy, 252 (2019), Article 113423, 10.1016/j.apenergy.2019.113423 View PDFView
    articleView in ScopusGoogle Scholar Garrido-Baserba et al., 2020 M. Garrido-Baserba,
    L. Corominas, U. Cortés, D. Rosso, M. Poch The fourth-revolution in the water
    sector encounters the digital revolution Environ. Sci. Technol., 54 (2020), pp.
    4698-4705, 10.1021/acs.est.9b04251 View in ScopusGoogle Scholar Gernaey et al.,
    2014 K.V. Gernaey, U. Jeppsson, P.A. Vanrolleghem, J.B. Copp Benchmarking of Control
    Strategies for Wastewater Treatment Plants IWA Publishing, London, UK (2014) IWA
    Scientific and Technical Report No. 23ISBN 9781843391463 Google Scholar Goodfellow
    et al., 2016 I. Goodfellow, Y. Bengio, A. Courville Deep Learning MIT Press (2016)
    Google Scholar Gutierrez-Mondragon et al., 2020 Gutierrez-Mondragon, M.A., Garcia-Gasulla,
    D., Alvarez-Napagao, S., Brossa-Ordoñez, J., and Gimenez-Esteban, R., 2020. Obstruction
    level detection of sewer videos using convolutional neural networks. arXiv preprint.
    arXiv:2002.01284. Google Scholar Guo et al., 2018 G. Guo, S. Liu, Y. Wu, J. Li,
    R. Zhou, X. Zhu Short-term water demand forecast based on deep learning method
    J. Water Resour. Plan. Manag., 144 (2018), pp. 1-11, 10.1061/(ASCE)WR.1943-5452.0000992
    Google Scholar Guo et al., 2021a G. Guo, X. Yu, S. Liu, Z. Ma, Y. Wu, X. Xu, X.
    Wang, K. Smith, X. Wu Leakage detection in water distribution systems based on
    time–frequency convolutional neural network J. Water Resour. Plan. Manag., 147
    (2021), Article 04020101, 10.1061/(asce)wr.1943-5452.0001317 View in ScopusGoogle
    Scholar Guo et al., 2021b Z. Guo, J.P. Leitão, N.E. Simões, V. Moosavi Data-driven
    flood emulation: speeding up urban flood predictions by deep convolutional neural
    networks J. Flood Risk Manag., 14 (2021), pp. 1-14, 10.1111/jfr3.12684 Google
    Scholar Hajgató et al., 2021 Hajgató, G., Gyires-Tóth, B., Paál, G., 2021. Reconstructing
    nodal pressures in water distribution systems with graph neural networks. arXiv
    preprint. arXiv:2104.13619. Google Scholar Hajgató et al., 2020 G. Hajgató, G.
    Paál, B. Gyires-Tóth Deep reinforcement learning for real-time optimization of
    pumps in water distribution systems J. Water Resour. Plan. Manag., 146 (2020),
    Article 04020079, 10.1061/(asce)wr.1943-5452.0001287 View in ScopusGoogle Scholar
    U.S. EPA 2012 U.S. EPA CANARY User''s Manual Version 4.3.2 U.S. Environmental
    Protection Agency, Washington, DC (2012) EPA/600/R-08/040B Google Scholar Hashemi-Beni
    and Gebrehiwot, 2021 L. Hashemi-Beni, A.A. Gebrehiwot Flood extent mapping: an
    integrated method using deep learning and region growing using UAV optical data
    IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 14 (2021), pp. 2127-2135, 10.1109/JSTARS.2021.3051873
    View in ScopusGoogle Scholar Hassan et al., 2019 S.I. Hassan, L.M. Dang, I. Mehmood,
    S. Im, C. Choi, J. Kang, Y.S. Park, H. Moon Underground sewer pipe condition assessment
    based on convolutional neural networks Autom. Constr., 106 (2019), Article 102849
    View PDFView articleView in ScopusGoogle Scholar Hassanzadeh et al., 2020 A. Hassanzadeh,
    A. Rasekh, S. Galelli, M. Aghashahi, R. Taormina, A. Ostfeld, M.K. Banks A review
    of cybersecurity incidents in the water sector J. Environ. Eng., 146 (2020), Article
    03120003, 10.1061/(asce)ee.1943-7870.0001686 View in ScopusGoogle Scholar Haurum
    and Moeslund, 2021 J.B. Haurum, T.B. Moeslund Sewer-ML: a multi-label sewer defect
    classification dataset and benchmark Proc. IEEE Comput. Soc. Conf. Comput. Vis.
    Pattern Recognit. (2021), pp. 13451-13462, 10.1109/CVPR46437.2021.01325 View in
    ScopusGoogle Scholar He et al., 2016 K. He, X. Zhang, S. Ren, J. Sun Deep residual
    learning for image recognition Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (2016) Google Scholar Hernández-del-Olmo et al.,
    2018 F. Hernández-del-Olmo, E. Gaudioso, R. Dormido, N. Duro Tackling the start-up
    of a reinforcement learning agent for the control of wastewater treatment plants
    Knowl.-Based Syst, 144 (2018), pp. 9-15, 10.1016/j.knosys.2017.12.019 View PDFView
    articleView in ScopusGoogle Scholar Hernández-del-Olmo et al., 2016 F. Hernández-del-Olmo,
    E. Gaudioso, R. Dormido, N. Duro Energy and environmental efficiency for the N-ammonia
    removal process in wastewater treatment plants by means of reinforcement learning
    Energies, 9 (2016), p. 755, 10.3390/en9090755 View in ScopusGoogle Scholar Hu
    et al., 2019 P. Hu, J. Tong, J. Wang, Y. Yang, Oliveira Turci, L. De A hybrid
    model based on CNN and Bi-LSTM for urban water demand prediction 2019 IEEE Congr.
    Evol. Comput. CEC 2019 - Proc (2019), pp. 1088-1094, 10.1109/CEC.2019.8790060
    View in ScopusGoogle Scholar Hu et al., 2021 X. Hu, Y. Han, B. Yu, Z. Geng, J.
    Fan Novel leakage detection and water loss management of urban water supply network
    using multiscale neural networks J. Clean. Prod., 278 (2021), Article 123611,
    10.1016/j.jclepro.2020.123611 View PDFView articleView in ScopusGoogle Scholar
    Inoue et al., 2017 J. Inoue, Y. Yamagata, Y. Chen, C.M. Poskitt, J. Sun Anomaly
    Detection for a Water Treatment System Using Unsupervised Machine Learning IEEE
    International Conference on Data Mining Workshops, ICDMW, IEEE, New Orleans, LA,
    USA (2017), pp. 1058-1065, 10.1109/ICDMW.2017.149 View in ScopusGoogle Scholar
    Iqbal et al., 2021 U. Iqbal, P. Perez, W. Li, J. Barthelemy How computer vision
    can facilitate flood management: a systematic review Int. J. Disaster Risk Reduct.,
    53 (2021), Article 102030, 10.1016/j.ijdrr.2020.102030 View PDFView articleView
    in ScopusGoogle Scholar IWA 2019 IWA, 2019. Digital Water: industry leaders chart
    the transformation journey. https://iwa-network.org/wp-content/uploads/2019/06/IWA_2019_Digital_Water_Report.pdf
    (accessed 1-4-2021). Google Scholar Javadiha et al., 2019 M. Javadiha, J. Blesa,
    A. Soldevila, V. Puig Leak localization in water distribution networks using deep
    learning 2019 6th Int. Conf. Control. Decis. Inf. Technol. CoDIT (2019), pp. 1426-1431,
    10.1109/CoDIT.2019.8820627 2019 View in ScopusGoogle Scholar Jiao et al., 2021
    Y. Jiao, R. Rayhana, J. Bin, Z. Liu, A. Wu, X. Kong A steerable pyramid autoencoder
    based framework for anomaly frame detection of water pipeline CCTV inspection
    Measurement, 174 (2021), Article 109020, 10.1016/j.measurement.2021.109020 View
    PDFView articleView in ScopusGoogle Scholar Kabir et al., 2020 S. Kabir, S. Patidar,
    X. Xia, Q. Liang, J. Neal, G. Pender A deep convolutional neural network model
    for rapid prediction of fluvial flood inundation J. Hydrol., 590 (2020), Article
    125481, 10.1016/j.jhydrol.2020.125481 View PDFView articleView in ScopusGoogle
    Scholar Kang et al., 2018 J. Kang, Y. Park, J. Lee, S. Wang, D. Eom Novel leakage
    detection by ensemble CNN-SVM and graph-based localization in water distribution
    systems IEEE Trans. Ind. Electron., 65 (2018), pp. 4279-4289 CrossRefView in ScopusGoogle
    Scholar Kingma and Ba, 2015 D.P. Kingma, J.L. Ba Adam: a method for stochastic
    optimization 3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc (2015),
    pp. 1-15 Google Scholar Kühnert et al., 2021 C. Kühnert, N.M. Gonuguntla, H. Krieg,
    D. Nowak, J.A. Thomas Application of LSTM networks for water demand prediction
    in optimal pump control Water (Switzerland), 13 (2021), pp. 1-19, 10.3390/w13050644
    View in ScopusGoogle Scholar Kumar et al., 2018 S.S. Kumar, D.M. Abraham, M.R.
    Jahanshahi, T. Iseley, J. Starr Automated defect classification in sewer closed
    circuit television inspections using deep convolutional neural networks Autom.
    Constr., 91 (2018), pp. 273-283 View PDFView articleCrossRefView in ScopusGoogle
    Scholar Kumar et al., 2020a S.S. Kumar, D. Abraham, M. Rosenthal Construction
    research Congress 2020: computer applications leveraging visualization techniques
    to develop improved deep neural network architecture for sewer defect identification
    Construction Research Congress 2020: Infrastructure Systems and Sustainability,
    Reston, VA, American Society of Civil Engineers (2020) Google Scholar Kumar et
    al., 2020b S.S. Kumar, M. Wang, D.M. Abraham, M.R. Jahanshahi, T. Iseley, J.C.P.
    Cheng Deep learning–based automated detection of sewer Defects in CCTV videos
    J. Comput. Civil Eng., 34 (1) (2020), Article 04019047 View in ScopusGoogle Scholar
    Kunzel et al., 2018 J. Kunzel, T. Werner, P. Eisert, J. Waschnewski, R. Möller,
    R. Hilpert Automatic analysis of sewer pipes based on unrolled monocular fisheye
    images 2018 IEEE Winter Conference on Applications of Computer Vision (WACV) (2018),
    10.1109/WACV.2018.00223 Google Scholar Lecun et al., 2015 Y. Lecun, Y. Bengio,
    G. Hinton Deep learning Nature, 521 (2015), pp. 436-444, 10.1038/nature14539 View
    in ScopusGoogle Scholar Leigh et al., 2019 C. Leigh, O. Alsibai, R.J. Hyndman,
    S. Kandanaarachchi, O.C. King, J.M. Mcgree, C. Neelamraju, J. Strauss, P. Dilini,
    R.D.R. Turner, K. Mengersen, E.E. Peterson A framework for automated anomaly detection
    in high frequency water-quality data from in situ sensors Sci. Total Environ.,
    664 (2019), pp. 885-898, 10.1016/j.scitotenv.2019.02.085 View PDFView articleView
    in ScopusGoogle Scholar Li et al., 2022a L. Li, J. Qiao, G. Yu, L. Wang, H.Y.
    Li, C. Liao, Z. Zhu Interpretable tree-based ensemble model for predicting beach
    water quality Water Res., 211 (2022), Article 118078, 10.1016/j.watres.2022.118078
    View PDFView articleView in ScopusGoogle Scholar Li et al., 2019a D. Li, D. Chen,
    B. Jin, L. Shi, J. Goh, S.K. Ng MAD-GAN: multivariate anomaly detection for time
    series data with generative adversarial networks International Conference on Artificial
    Neural Networks, Springer (2019), pp. 703-716 CrossRefView in ScopusGoogle Scholar
    Li et al., 2019b D. Li, A. Cong, S. Guo Sewer damage detection from imbalanced
    CCTV inspection data using deep convolutional neural networks with hierarchical
    classification Autom. Constr., 101 (2019), pp. 199-208 View PDFView articleGoogle
    Scholar Li et al., 2020 T. Li, A.K. Sahu, A. Talwalkar, V. Smith Federated Learning:
    challenges, Methods, and Future Directions IEEE Signal Process. Mag., 37 (2020),
    pp. 50-60, 10.1109/MSP.2020.2975749 Google Scholar Li et al., 2021 Z. Li, H. Liu,
    C. Luo, G. Fu Assessing surface water flood risks in urban areas using machine
    learning Water (Switzerland), 13 (2021), pp. 1-14, 10.3390/w13243520 Google Scholar
    Li et al., 2022b Z. Li, Chi Zhang, H. Liu, Chao Zhang, M. Zhao, Q. Gong, G. Fu
    Developing stacking ensemble models for multivariate contamination detection in
    water distribution systems Sci. Total Environ., 828 (2022), Article 154284, 10.1016/j.scitotenv.2022.154284
    View PDFView articleView in ScopusGoogle Scholar Liu et al., 2020 H. Liu, C.A.
    Shoemaker, Y. Jiang, G. Fu, C. Zhang Preconditioning water distribution network
    optimization with head loss-based design method J. Water Resour. Plan. Manag.,
    146 (2020), 10.1061/(ASCE)WR.1943-5452.0001299 Google Scholar Liu et al., 2019
    Liu, R., Zhang, Z., Zhang, D., 2019. Leakage detection and isolation in water
    distribution network based on data mining and genetic optimized hydraulic simulation.
    pp. 1–5. doi:10.5281/zenodo.3911523. Google Scholar Löwe et al., 2021 R. Löwe,
    J. Böhm, D.G. Jensen, J. Leandro, S.H. Rasmussen U-FLOOD – Topographic deep learning
    for predicting urban pluvial flood water depth J. Hydrol., 603 (2021), 10.1016/j.jhydrol.2021.126898
    Google Scholar Makropoulos and Savic, 2019 C. Makropoulos, D.A. Savic Urban Hydroinformatics:
    past, present and future Water (Switzerland), 11 (2019), p. 1959 CrossRefView
    in ScopusGoogle Scholar Mehmood et al., 2020 H. Mehmood, S.K. Mukkavilli, I. Weber,
    A. Koshio, C. Meechaiya, T. Piman, K. Mubea, C. Tortajada, K. Mahadeo, D. Liao
    Strategic Foresight to Applications of Artificial Intelligence to Achieve Water-Related
    Sustainable Development Goals United Nations University Institute for Water, Environment
    and Health, Hamilton, Canada (2020) UNU-INWEH Report Series Google Scholar Meijer
    et al., 2019 D. Meijer, L. Scholten, F. Clemens, A. Knobbe A defect classification
    methodology for sewer image sets with convolutional neural networks Autom. Constr.,
    104 (2019), pp. 281-298 View PDFView articleView in ScopusGoogle Scholar Moradi
    et al., 2020 S. Moradi, T. Zayed, F. Nasiri, F. Golkhoo Automated anomaly detection
    and localization in sewer inspection videos using proportional data modeling and
    deep learning–based text recognition J. Infrastruct. Syst., 26 (3) (2020), Article
    04020018 View in ScopusGoogle Scholar Moreno-Rodenas et al., 2021 A.M. Moreno-Rodenas,
    A. Duinmeijer, F. Clemens Deep-learning based monitoring of FOG layer dynamics
    in wastewater pumping stations Water Res., 202 (2021), Article 117482 View PDFView
    articleView in ScopusGoogle Scholar Mu et al., 2020 L. Mu, F. Zheng, R. Tao, Q.
    Zhang, Z. Kapelan Hourly and daily urban water demand predictions using a long
    short-term memory based model J. Water Resour. Plan. Manag., 146 (2020), Article
    05020017, 10.1061/(asce)wr.1943-5452.0001276 View in ScopusGoogle Scholar Muharemi
    et al., 2019 F. Muharemi, D. Logofătu, F. Leon Machine learning approaches for
    anomaly detection of water quality on a real-world data set J. Inf. Telecommun.
    (2019), 10.1080/24751839.2019.1565653 Google Scholar Mullapudi et al., 2020 A.
    Mullapudi, M.J. Lewis, C.L. Gruden, B. Kerkez Deep reinforcement learning for
    the real time control of stormwater systems Adv. Water Resour., 140 (2020), 10.1016/j.advwatres.2020.103600
    Google Scholar Nam et al., 2021 Y.W. Nam, Y. Arai, T. Kunizane, A. Koizumi Water
    leak detection based on convolutional neural network (CNN) using actual leak sounds
    and the hold-out method Water Supply (2021), pp. 1-9, 10.2166/ws.2021.109 Google
    Scholar Nasser et al., 2020 A.A. Nasser, M.Z. Rashad, S.E. Hussein A two-layer
    water demand prediction system in urban areas based on micro-services and LSTM
    neural networks IEEE Access, 8 (2020), pp. 147647-147661, 10.1109/ACCESS.2020.3015655
    View in ScopusGoogle Scholar Nearing et al., 2021 G.S. Nearing, F. Kratzert, A.K.
    Sampson, C.S. Pelissier, D. Klotz, J.M. Frame, C. Prieto, H.V. Gupta What role
    does hydrological science play in the age of machine learning? Water Resour. Res.,
    57 (2021), 10.1029/2020WR028091 Google Scholar Nevo et al., 2019 Nevo, S., Anisimov,
    V., Elidan, G., El-Yaniv, R., Giencke, P., Gigi, Y., Hassidim, A., Moshe, Z.,
    Schlesinger, M., Shalev, G., Tirumali, A., Wiesel, A., Zlydenko, O., Matias, Y.,
    2019. ML for flood forecasting at Scale 2–5. Google Scholar Ng, 2022 Ng A., 2022.
    MLOps: From Model-centric to Data-centric AI. https://www.deeplearning.ai/wp-content/uploads/2021/06/MLOps-From-Model-centric-to-Data-centric-AI.pdf
    (accessed 7-3-2022). Google Scholar Palmitessa et al., 2021 R. Palmitessa, P.S.
    Mikkelsen, M. Borup, A.W.K. Law Soft sensing of water depth in combined sewers
    using LSTM neural networks with missing observations J. Hydro-Environ. Res., 38
    (2021), pp. 106-116, 10.1016/j.jher.2021.01.006 View PDFView articleView in ScopusGoogle
    Scholar Pan et al., 2020 G. Pan, Y. Zheng, S. Guo, Y. Lv Automatic sewer pipe
    defect semantic segmentation based on improved U-Net Autom. Constr., 119 (2020),
    Article 103383 View PDFView articleView in ScopusGoogle Scholar Pang et al., 2019
    J. Pang, S. Yang, L. He, Y. Chen, N. Ren Intelligent control/operational strategies
    in WWTPs through an integrated Q-learning algorithm with ASM2d-guided reward Water
    (Switzerland), 11 (2019), 10.3390/w11050927 Google Scholar Park et al., 2019 J.
    Park, H. Lee, C.Y. Park, S. Hasan, T. Heo, W.H. Lee Algal morphological identification
    in watersheds for drinking water supply using neural architecture search for convolutional
    neural network Water (Switzerland), 11 (2019), p. 1338, 10.3390/w11071338 View
    in ScopusGoogle Scholar Pesantez et al., 2022 J.E. Pesantez, F. Alghamdi, S. Sabu,
    G. Mahinthakumar, E.Z. Berglund Using a digital twin to explore water infrastructure
    impacts during the COVID-19 pandemic Sustain. Cities Soc., 77 (2022), Article
    103520, 10.1016/j.scs.2021.103520 View PDFView articleView in ScopusGoogle Scholar
    Pollard et al., 2018 J.A. Pollard, T. Spencer, S. Jude Big Data Approaches for
    coastal flood risk assessment and emergency response Wiley Interdiscip. Rev. Clim.
    Chang., 9 (2018), Article e543, 10.1002/wcc.543 View in ScopusGoogle Scholar Qian
    et al., 2020 K. Qian, J. Jiang, Y. Ding, S. Yang Deep learning based anomaly detection
    in water distribution systems 2020 IEEE Int. Conf. Networking, Sens. Control.
    ICNSC 2020 (2020), 10.1109/ICNSC48988.2020.9238099 Google Scholar Rahimi et al.,
    2020 M. Rahimi, A. Alghassi, M. Ahsan, J. Haider Deep learning model for industrial
    leakage detection using acoustic emission signal Informatics (2020), 10.3390/informatics7040049
    Google Scholar Reuss, 2003 M. Reuss Is it time to resurrect the Harvard water
    program? J. Water Resour. Plan. Manag., 129 (2003), pp. 357-360, 10.1061/(asce)0733-9496(2003)129:5(357)
    View in ScopusGoogle Scholar Rodriguez-Perez et al., 2020 J. Rodriguez-Perez,
    C. Leigh, B. Liquet, C. Kermorvant, E. Peterson, D. Sous, K. Mengersen Detecting
    technical anomalies in high-frequency water-quality data using artificial neural
    networks Environ. Sci. Technol., 54 (2020), pp. 13719-13730, 10.1021/acs.est.0c04069
    View in ScopusGoogle Scholar Sadr et al., 2020 S.M.K. Sadr, A. Casal-Campos, G.
    Fu, R. Farmani, S. Ward, D. Butler Strategic planning of the integrated urban
    wastewater system using adaptation pathways Water Res., 182 (2020), Article 116013,
    10.1016/j.watres.2020.116013 View PDFView articleView in ScopusGoogle Scholar
    Saliba et al., 2020 S.M. Saliba, B.D. Bowes, S. Adams, P.A. Beling, J.L. Goodall
    Deep reinforcement learning with uncertain data for real-time stormwater system
    control and flood mitigation Water (Switzerland), 12 (2020), pp. 1-19, 10.3390/w12113222
    View in ScopusGoogle Scholar Samek et al., 2021 W. Samek, G. Montavon, S. Lapuschkin,
    C.J. Anders, K.R. Müller Explaining deep neural networks and beyond: a review
    of methods and applications Proc. IEEE, 109 (2021), pp. 247-278, 10.1109/JPROC.2021.3060483
    View in ScopusGoogle Scholar Shen, 2018 C. Shen A transdisciplinary review of
    deep learning research and its relevance for water resources scientists Water
    Resour. Res., 54 (2018), pp. 8558-8593, 10.1029/2018WR022643 View in ScopusGoogle
    Scholar Shukla and Piratla, 2020 H. Shukla, K. Piratla Leakage detection in water
    pipelines using supervised classification of acceleration signals Autom. Constr.,
    117 (2020), Article 103256, 10.1016/j.autcon.2020.103256 View PDFView articleView
    in ScopusGoogle Scholar Sufi Karimi et al., 2019 H. Sufi Karimi, B. Natarajan,
    C.L. Ramsey, J. Henson, J.L. Tedder, E. Kemper Comparison of learning-based wastewater
    flow prediction methodologies for smart sewer management J. Hydrol., 577 (2019),
    10.1016/j.jhydrol.2019.123977 Google Scholar Syafiie et al., 2011 S. Syafiie,
    F. Tadeo, E. Martinez, T. Alvarez Model-free control based on reinforcement learning
    for a wastewater treatment problem Appl. Soft Comput. J., 11 (2011), pp. 73-82,
    10.1016/j.asoc.2009.10.018 View PDFView articleView in ScopusGoogle Scholar Tang
    et al., 2020 R. Tang, K. Li, W. Ding, Y. Wang, H. Zhou, G. Fu Reference point
    based multi-objective optimization of reservoir operation: a comparison of three
    algorithms Water Resour. Manag., 34 (2020), pp. 1005-1020, 10.1007/s11269-020-02485-9
    View in ScopusGoogle Scholar Taormina and Galelli, 2018 R. Taormina, S. Galelli
    Deep-learning approach to the detection and localization of cyber-physical attacks
    on water distribution systems J. Water Resour. Plan. Manag., 144 (2018), Article
    04018065, 10.1061/(asce)wr.1943-5452.0000983 View in ScopusGoogle Scholar Taormina
    et al., 2018 R. Taormina, S. Galelli, N.O. Tippenhauer, E. Salomons, A. Ostfeld,
    D.G. Eliades, M. Aghashahi, R. Sundararajan, M. Pourahmadi, M.K. Banks, B.M. Brentan,
    E. Campbell, G. Lima, D. Manzi, D. Ayala-Cabrera, M. Herrera, I. Montalvo, J.
    Izquierdo, E. Luvizotto, S.E. Chandy, A. Rasekh, Z.A. Barker, B. Campbell, M.E.
    Shafiee, M. Giacomoni, N. Gatsis, A. Taha, A.A. Abokifa, K. Haddad, C.S. Lo, P.
    Biswas, M.F.K. Pasha, B. Kc, S.L. Somasundaram, M. Housh, Z. Ohar Battle of the
    attack detection algorithms: disclosing cyber attacks on water distribution networks
    J. Water Resour. Plan. Manag., 144 (2018), Article 04018048, 10.1061/(asce)wr.1943-5452.0000969
    View in ScopusGoogle Scholar Tsiami and Makropoulos, 2021 L. Tsiami, C. Makropoulos
    Cyber—physical attack detection in water distribution systems with temporal graph
    convolutional neural networks Water (Basel), 13 (9) (2021), p. 1247 CrossRefView
    in ScopusGoogle Scholar The Royal Society 2019 The Royal Society, 2019. Explainable
    AI: the basics. Google Scholar The Royal Society 2017 The Royal Society Machine
    Learning: the Power and Promise of Computers That Learn By Example Report by the
    Royal Society (2017), 10.1126/scitranslmed.3002564 Google Scholar Therrien et
    al., 2020 J.D. Therrien, N. Nicolaï, P.A. Vanrolleghem A critical review of the
    data pipeline: how wastewater system operation flows from data to intelligence
    Water Sci. Technol., 82 (2020), pp. 2613-2634, 10.2166/wst.2020.393 View in ScopusGoogle
    Scholar Valverde-Pérez et al., 2021 B. Valverde-Pérez, B. Johnson, C. Wärff, D.
    Lumley, E. Torfs, I. Nopens, L. Townley Digital Water: operational digital twins
    in the urban water sector: case studies Int. Water Assoc. (2021) Google Scholar
    Varadharajan et al., 2022 C. Varadharajan, A.P. Appling, B. Arora, D.S. Christianson,
    V.C. Hendrix, V. Kumar, A.R. Lima, J. Müller, S. Oliver, M. Ombadi, T. Perciano,
    J.M. Sadler, H. Weierbach, J.D. Willard, Z. Xu, J. Zwart Can machine learning
    accelerate process understanding and decision-relevant predictions of river water
    quality? Hydrol. Process., 36 (2022), pp. 1-22, 10.1002/hyp.14565 Google Scholar
    Vaswani et al., 2017 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A.N. Gomez, Ł. Kaiser, I. Polosukhin Attention is all you need Adv. Neural Inf.
    Process. Syst. 2017-Decem (2017), pp. 5999-6009 View in ScopusGoogle Scholar Vinuesa
    et al., 2020 R. Vinuesa, H. Azizpour, I. Leite, M. Balaam, V. Dignum, S. Domisch,
    A. Felländer, S.D. Langhans, M. Tegmark, F. Fuso Nerini The role of artificial
    intelligence in achieving the Sustainable Development Goals Nat. Commun., 11 (2020),
    pp. 1-10, 10.1038/s41467-019-14108-y Google Scholar Wang et al., 2020 X. Wang,
    G. Guo, S. Liu, Y. Wu, X. Xu, K. Smith Burst detection in district metering areas
    using deep learning method J. Water Resour. Plan. Manag., 146 (2020), Article
    04020031, 10.1061/(asce)wr.1943-5452.0001223 View in ScopusGoogle Scholar Wang
    et al., 2019 Z. Wang, Y. Man, Y. Hu, J. Li, M. Hong, P. Cui A deep learning based
    dynamic COD prediction model for urban sewage Environ. Sci. Water Res. Technol.,
    5 (2019), pp. 2210-2218, 10.1039/c9ew00505f View in ScopusGoogle Scholar Wang
    et al., 2021a M. Wang, S.S. Kumar, J.C.P. Cheng Automated sewer pipe defect tracking
    in CCTV videos based on defect detection and metric learning Autom. Constr., 121
    (2021), Article 103438 View PDFView articleView in ScopusGoogle Scholar Wang et
    al., 2021b M. Wang, H. Luo, J.C.P. Cheng Towards an automated condition assessment
    framework of underground sewer pipes based on closed-circuit television (CCTV)
    images Tunnell. Underground Space Technol., 110 (2021), Article 103840 View PDFView
    articleView in ScopusGoogle Scholar Wang and Cheng, 2020 M. Wang, J.C.P. Cheng
    A unified convolutional neural network integrated with conditional random field
    for pipe defect segmentation Comput. Aided Civ. Infrastruct. Eng., 35 (2) (2020),
    pp. 162-177 CrossRefView in ScopusGoogle Scholar Wu and Rahman, 2017 Z.Y. Wu,
    A. Rahman Optimized deep learning framework for water distribution data- driven
    modeling Procedia Eng, 186 (2017), pp. 261-268, 10.1016/j.proeng.2017.03.240 View
    PDFView articleView in ScopusGoogle Scholar Xie et al., 2019 Q. Xie, D. Li, J.
    Xu, Z. Yu, J. Wang Automatic detection and classification of sewer defects via
    hierarchical deep learning IEEE Trans. Autom. Sci. Eng., 16 (4) (2019), pp. 1836-1847
    CrossRefView in ScopusGoogle Scholar Xie et al., 2021 X. Xie, J. Niu, X. Liu,
    Z. Chen, S. Tang, S. Yu A survey on incorporating domain knowledge into deep learning
    for medical image analysis Med. Image Anal., 69 (2021), pp. 1-27, 10.1016/j.media.2021.101985
    Google Scholar Xu and Liang, 2021 T. Xu, F. Liang Machine learning for hydrologic
    sciences: an introductory overview Wiley Interdiscip. Rev. Water, 8 (2021), pp.
    1-29, 10.1002/wat2.1533 Google Scholar Xu et al., 2020 W. Xu, Y. Jiang, X. Zhang,
    Y. Li, R. Zhang, G. Fu Using long short-term memory networks for river flow prediction
    Hydrol. Res., 51 (2020), pp. 1358-1376, 10.2166/nh.2020.026 View in ScopusGoogle
    Scholar Xu et al., 2021 W. Xu, F. Meng, W. Guo, X. Li, G. Fu Deep reinforcement
    learning for optimal hydropower reservoir operation J. Water Resour. Plann. Manage.,
    147 (8) (2021), Article 04021045, 10.1061/(ASCE)WR.1943-5452.0001409 View in ScopusGoogle
    Scholar Yang et al., 2009 Y.C.E. Yang, X. Cai, D.M. Stipanović A decentralized
    optimization algorithm for multiagent system-based watershed management Water
    Resour. Res., 45 (2009), pp. 1-18, 10.1029/2008WR007634 Google Scholar Yang and
    Cervone, 2019 L. Yang, G. Cervone Analysis of remote sensing imagery for disaster
    assessment using deep learning: a case study of flooding event Soft Comput, 23
    (2019), pp. 13393-13408 CrossRefView in ScopusGoogle Scholar Yin et al., 2020
    X. Yin, Y. Chen, A. Bouferguene, H. Zaman, M. Al-Hussein, L. Kurach A deep learning-based
    framework for an automated defect detection system for sewer pipes Autom. Constr.,
    109 (2020), Article 102967 View PDFView articleView in ScopusGoogle Scholar Zhang
    et al., 2018a Zhang, D., Lindholm, G., Ratnaweera, H., 2018a. DeepCSO: forecasting
    of combined sewer overflow at a citywide level using multi-task deep learning.
    arXiv preprint. arXiv:1811.06368. Google Scholar Zhang et al., 2018b D. Zhang,
    N. Martinez, G. Lindholm, H. Ratnaweera Manage sewer in-line storage control using
    hydraulic model and recurrent neural network Water Resour. Manag., 32 (2018),
    pp. 2079-2098, 10.1007/s11269-018-1919-3 View in ScopusGoogle Scholar Zhang et
    al., 2018c D. Zhang, E. Skullestad, G. Lindholm, H. Ratnaweera Hydraulic modeling
    and deep learning based flow forecasting for optimizing inter catchment wastewater
    transfer J. Hydrol., 567 (2018), pp. 792-802, 10.1016/j.jhydrol.2017.11.029 View
    PDFView articleView in ScopusGoogle Scholar Zhang et al., 2018d Z. Zhang, B. Li,
    X. Lv, K. Liu Research on pipeline defect detection based on optimized faster
    R-CNN algorithm DEStech Trans. Comput. Sci. Eng. (2018), pp. 469-474, 10.12783/dtcse/ammms2018/27322
    View in ScopusGoogle Scholar Zhou et al., 2019a M. Zhou, Z. Pan, Y. Liu, Q. Zhang,
    Y. Cai, H. Pan Leak Detection and Location based on ISLMD and CNN in a pipeline
    IEEE Access, 7 (2019), pp. 30457-30464, 10.1109/ACCESS.2019.2902711 View in ScopusGoogle
    Scholar Zhou et al., 2021 M. Zhou, Y. Yang, Y. Xu, Y. Hu, Y. Cai, J. Lin, H. Pan
    A pipeline leak detection and localization approach based on ensemble TL1DCNN
    IEEE Access, 9 (2021), pp. 47565-47578, 10.1109/ACCESS.2021.3068292 View in ScopusGoogle
    Scholar Zhou et al., 2019b X. Zhou, Z. Tang, W. Xu, F. Meng, X. Chu, K. Xin, G.
    Fu Deep learning identifies accurate burst locations in water distribution networks
    Water Res., 166 (2019), Article 115058, 10.1016/j.watres.2019.115058 View PDFView
    articleView in ScopusGoogle Scholar Zhu et al., 2017 X.X. Zhu, D. Tuia, L. Mou,
    G.S. Xia, L. Zhang, F. Xu, F. Fraundorfer Deep learning in remote sensing: a Comprehensive
    Review and List of Resources IEEE Geosci. Remote Sens. Mag., 5 (2017), pp. 8-36,
    10.1109/MGRS.2017.2762307 View in ScopusGoogle Scholar Cited by (82) Medium-term
    water consumption forecasting based on deep neural networks 2024, Expert Systems
    with Applications Show abstract A system dynamics approach to management of water
    resources in Qatar 2024, Sustainable Production and Consumption Show abstract
    A review of graph and complex network theory in water distribution networks: Mathematical
    foundation, application and prospects 2024, Water Research Show abstract Deep
    reinforcement learning challenges and opportunities for urban water systems 2024,
    Water Research Show abstract Management of biofilm by an innovative layer-structured
    membrane for membrane biofilm reactor (MBfR) to efficient methane oxidation coupled
    to denitrification (AME-D) 2024, Water Research Show abstract Real-time water
    quality prediction in water distribution networks using graph neural networks
    with sparse monitoring data 2024, Water Research Show abstract View all citing
    articles on Scopus © 2022 The Author(s). Published by Elsevier Ltd. Recommended
    articles Soft sensing of water depth in combined sewers using LSTM neural networks
    with missing observations Journal of Hydro-environment Research, Volume 38, 2021,
    pp. 106-116 Rocco Palmitessa, …, Adrian W.K. Law Real-time water quality prediction
    in water distribution networks using graph neural networks with sparse monitoring
    data Water Research, Volume 250, 2024, Article 121018 Zilin Li, …, Guangtao Fu
    View PDF The latest innovative avenues for the utilization of artificial Intelligence
    and big data analytics in water resource management Results in Engineering, Volume
    20, 2023, Article 101566 Hesam Kamyab, …, Yongtae Ahn View PDF Show 3 more articles
    Article Metrics Citations Citation Indexes: 65 Captures Readers: 251 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: Water research (Oxford)
  limitations: '>'
  pdf_link: null
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'The role of deep learning in urban water management: A critical review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.55083/irjeas.2023.v11i04012
  analysis: '>'
  authors:
  - Shubhodip Sasmal
  citation_count: 0
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: International research journal of engineering & applied sciences
  limitations: '>'
  pdf_link: null
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Real-time Data Processing with Machine Learning Algorithms
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.future.2018.06.042
  analysis: '>'
  authors:
  - Xiang Fei
  - Nazaraf Shah
  - Nandor Verba
  - Kuo-Ming Chao
  - Víctor Sánchez-Anguix
  - Jerzy Lewandowski
  - Anne James
  - Zahid Usman
  citation_count: 90
  full_citation: '>'
  full_text: '>

    Loading [MathJax]/jax/output/SVG/fonts/TeX/Main/Italic/Main.js Skip to main content
    Skip to article Journals & Books Search Register Sign in Brought to you by: University
    of Nebraska-Lincoln View PDF Download full issue Future Generation Computer Systems
    Volume 90, January 2019, Pages 435-450 CPS data streams analytics based on machine
    learning for Cloud and Fog Computing: A survey Author links open overlay panel
    Xiang Fei a, Nazaraf Shah a, Nandor Verba a, Kuo-Ming Chao a, Victor Sanchez-Anguix
    a d, Jacek Lewandowski a e, Anne James b, Zahid Usman c Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.future.2018.06.042 Get rights and content
    Highlights • Cloud and Fog computing has emerged as a promising paradigm for Internet
    of things (IoT) and Cyber-Physical Systems (CPS). • One characteristic of CPS
    is the reciprocal feedback loops between physical processes and cyber elements
    (computation, software and networking), which implies that machine learning based
    data stream analytics is one of the core components of CPS as it extracts the
    insights and the knowledge from the data streams generated by various sensors
    and other monitoring components embedded in the physical systems, supports informed
    decision making, enables feedback from the physical processes to the cyber counterparts,
    and eventually facilitates the integration of cyber and physical systems. • There
    have existed many successful CPS data streams analytics based on machine learning
    techniques and thus it is necessary to have a survey on these machine learning
    techniques; and further, to explore how they should be deployed and integrated
    to the Cloud and Fog architecture for better fulfilment of the requirements, such
    as mission criticality and time criticality, of cyber physical systems. • To the
    best of our knowledge, this paper is the first to systematically study the machine
    learning techniques for CPS data stream analytics from various perspectives, especially
    from the time complexity’s point of view, which leads to the discussion and guidance
    of how the CPS machine learning methods should be deployed in the Cloud and Fog
    architecture. Abstract Cloud and Fog computing has emerged as a promising paradigm
    for the Internet of things (IoT) and cyber–physical systems (CPS). One characteristic
    of CPS is the reciprocal feedback loops between physical processes and cyber elements
    (computation, software and networking), which implies that data stream analytics
    is one of the core components of CPS. The reasons for this are: (i) it extracts
    the insights and the knowledge from the data streams generated by various sensors
    and other monitoring components embedded in the physical systems; (ii) it supports
    informed decision making; (iii) it enables feedback from the physical processes
    to the cyber counterparts; (iv) it eventually facilitates the integration of cyber
    and physical systems. There have been many successful applications of data streams
    analytics, powered by machine learning techniques, to CPS systems. Thus, it is
    necessary to have a survey on the particularities of the application of machine
    learning techniques to the CPS domain. In particular, we explore how machine learning
    methods should be deployed and integrated in Cloud and Fog architectures for better
    fulfilment of the requirements of mission criticality and time criticality arising
    in CPS domains. To the best of our knowledge, this paper is the first to systematically
    study machine learning techniques for CPS data stream analytics from various perspectives,
    especially from a perspective that leads to the discussion and guidance of how
    the CPS machine learning methods should be deployed in a Cloud and Fog architecture.
    Previous article in issue Next article in issue Keywords Cyber–physical systems
    (CPS)Machine learningCloud computingFog computingEdge computingAnalytics 1. Introduction
    and motivation In this section we discuss definitions of cyber–physical systems
    (CPS) and highlight application areas. 1.1. Cyber–physical systems: Definitions
    and characteristics Recent advances in computing, communication and sensing technologies
    have given rise to CPS, one of the most prominent ICT technologies that pervade
    various sectors of the physical world and also an integral part of everyday life
    [[1], [2], [3], [4]]. The term cyber–physical systems (CPS) was coined in the
    US in 2006 [5], with the realization of the increasing importance of the interactions
    between interconnected computing systems [6]. There have been various definitions
    of CPS, each of them throwing some light at some of the relevant factors. • The
    National Science Foundation [7] defines CPS as “Cyber–physical systems (CPS) are
    engineered systems that are built from, and depend upon, the seamless integration
    of computational algorithms and physical components. Advances in CPS will enable
    capability, adaptability, scalability, resiliency, safety, security, and usability
    that will far exceed the simple embedded systems of today. CPS technology will
    transform the way people interact with engineered systems — just as the Internet
    has transformed the way people interact with information. New smart CPS will drive
    innovation and competition in sectors such as agriculture, energy, transportation,
    building design and automation, healthcare, and manufacturing”. • Lee [1] defines
    CPS as “A cyber–physical system (CPS) is an orchestration of computers and physical
    systems. Embedded computers monitor and control physical processes, usually with
    feedback loops, where physical processes affect computations and vice versa”.
    • The National Institute of Standards and Technology [4] defines the subject of
    CPS as “Systems that integrate the cyber world with the physical world are often
    referred to as cyber–physical systems (CPS). The computational and physical components
    of such systems are tightly interconnected and coordinated to work effectively
    together, sometimes with humans in the loop” Despite their differences in length,
    detail and the semantics of some terms, there are some common characteristics
    that can be extracted from these definitions. More specifically, we argue that
    CPS have the following inherent characteristics: • Integration of cyber elements
    (computation, software and networking), engineered elements (physical processes)[[1],
    [7], [8], [9], [10], [11]], and human factors [4]. • Reciprocal feedback loops
    between physical processes and computations, (simulation and decision making),
    sensing and actuation elements, and monitoring and control elements [[4], [1],
    [8], [9], [12]]. • Networked physical components and tightly coupled, interconnected
    processes that require cooperation and coordination [[2], [4], [13]]. In addition
    to this, the National Institute of Standards and Technology also highlights the
    fact that CPS require the integration and cooperation of two technologies for
    successful deployment [4]. Firstly learning and predictive capabilities are necessary
    to provide the integration of physical and digital models and, more importantly,
    to provide the ability for the digital world to autonomously change its logic
    based on the state of the physical world (e.g., diagnostics and prognostics).
    Secondly, it is stated that CPS require open architectures and standards that
    provide for modularity and composability of systems, thus allowing complex and
    dynamic applications. Particularly, CPS interconnect virtual and physical worlds.
    The physical system typically has a virtual twin which can be used for monitoring
    and control. The desired predictive capabilities in CPS require these systems
    to potentially collect and analyse data from the physical and digital world. In
    the end, the predictive capability informs decision makers to take appropriate
    actions or control to change the course of physical world. Finally it should be
    highlighted that current applications of CPS include automotive systems, manufacturing,
    medical devices, military systems, assisted living, traffic control and safety,
    process control, power generation and distribution, energy conservation, HVAC
    (heating, ventilation and air conditioning), aircraft, instrumentation, water
    management systems, trains, physical security (access control and monitoring),
    asset management and distributed robotics (telepresence, telemedicine) [1]. 1.2.
    Data stream analytics in CPS Mining data streams, acquired from various sensors
    and other monitoring components embedded in the physical systems, plays an essential
    role in CPS. It extracts insights and knowledge, provides learning and predictive
    capabilities for decision support and autonomous behaviour, enables the feedback
    from the physical processes to the cyber counterparts, and eventually facilitates
    the integration of cyber and physical systems [14]. Silva et al. [15] provides
    a formal definition of a data stream as: A data stream S is a massive sequence
    of data objects X 1 , X 2 …, X N , i.e.,  S = X i i = 1 N , which is potentially
    unbounded ( N → ∞ ). Each data object is described by an n-dimensional attribute
    vector X i = x j i j = 1 n belonging to an attribute space Ω that can be continuous,
    categorical, or mixed. Data streams feature massive, potentially unbounded sequences
    of data objects that are continuously generated at rapid rates [15], which leads
    to the fundamental shift in the data analytics (information source) from traditional
    a priori information alone based or off-line batch approaches, to stream analytics.
    The key challenge in stream analytics is the extraction of valuable knowledge
    in real time from a massive, continuous and dynamic data stream in only a single
    scan [16]. The reader should additionally consider that the insights extracted
    from physical devices, such as sensors, feature perishable insights, i.e., they
    have to be provided quickly, as otherwise they lose value to feed the logic of
    the CPS software. In CPS, data streams are most beneficial at the time they are
    produced, as any change reported by the data (e.g. a sensor anomaly, a fault in
    the physical process being sensed, or a change of system state) should be detected
    as soon as possible and acted upon. Furthermore, as opposed to stream analytics
    for purely software systems, the insights being revealed by data streams in CPS
    are often tied to a safety-critical action that must be performed to ensure the
    health of the CPS itself [14]. Analysis of these ever-growing data streams becomes
    a challenging task with traditional analytical tools. Innovative and effective
    analytic techniques and technologies are required to operate, continuously and
    in real-time, on the data streams and other sources data [17]. Machine learning
    is a discipline that aims to enable computers to, without being explicitly programmed,
    automate data-driven model building and hidden insights discovery, i.e., to automate
    behaviour or the logic for the resolution of a particular problem, via iterative
    learning from example data or past experience [[18], [19], [20]]. In the past,
    there have existed many successful applications of machine learning, including
    systems that analyse past sales data to predict customer behaviour, optimize robot
    behaviour so that a task can be completed using minimum resources, and extract
    knowledge from bioinformatics data [20]. In this particular survey, we will focus
    on stream analytics methods that are based on machine learning algorithms. 1.3.
    Cloud and fog computing The interconnection of sensor and actuator systems with
    decision making and analytics have traditionally been performed by either local
    static controllers or uploaded to the Cloud for analysis. Supported by the paradigms
    of Internet of Things (IoT), Cloud computing experts propose the virtualization
    of devices to provide their data-based capabilities and their connection as a
    service for users within a Sensing and Actuation as a Service (SAaaS) [21] or
    as Things as a Service (TaaS) [22]. Another role that Cloud computing has played
    in supporting CPS is focused on the analysis of the data received from devices.
    The Cloud can provide a vast number of processing and storage resources which
    can be used to analyse large amounts of data [23] or streams [24]. These Cloud
    capabilities are concentrated in centralized and remote data centres, which has
    several drawbacks. The security aspect of storing, analysing and managing data
    in the Cloud is an increasing concern [25], while the remote nature of the Cloud
    also has reliability and latency issues [26]. The paradigm of Fog computing as
    proposed by [27] extends the Cloud to the edge of the network to better utilize
    resources available on gateways and connected devices. This extension allows data
    to be stored and processed locally to increase reliability and security, while
    decreasing the latencies between devices and the processing elements [28]. Fog
    computing systems are typically characterized by a large number of heterogeneous
    nodes, increased mobility and a strong presence of streaming and real-time applications
    [27]. The hosts or gateways used in Fog systems vary from PC based computing nodes
    [29], mobile devices [30] and resource constrained System on Chip (SoC) devices
    [31], routers, switches, set top boxes, proxy servers and base stations [32].
    These hosts all have varying storage, processing and networking capabilities.
    While computing nodes have the most resources and are most reliable, they usually
    communicate with devices using Ethernet or Wi-Fi based networks. The mobile devices
    and SoC based devices have fewer resources but provide a wider range of wireless
    communication possibilities for polyglot gateways [33], that can be used to connect
    to a wider range of heterogeneous devices using low-power Machine to Machine (M2M)
    communication protocols. These distinguishing properties of the Fog are essential
    for providing elastic resources and services to end users at the edge of networks
    [28]. Fog computing is rapidly finding its way into CPS and IoT. Adopting IoT
    paradigms into CPS can provide several types of services, such as weather monitoring,
    smart grid, sensor and actuator networks in manufacturing environments, smart
    building control and intelligent transport. These services produce a large amount
    of data that need to be processed for the extraction of knowledge and system control
    [34]. The platforms deployed in Fog computing vary based on hosts and application
    domains, but they can be categorized in a similar way as in Cloud computing. Infrastructure
    based platforms allow users to deploy Virtual Machines (VM’s) [35] or lightweight
    virtualization images [36]. Platform based solutions as in [37] provide a platform
    for users for application-style system deployments. The third type of platform
    provides networking and analytics capabilities that the user can configure and
    use without the need to program and deploy their own applications. From the hosts’
    perspective there are a number of differences between the Cloud and the Fog. The
    main difference is the resources. While the Cloud is considered to have virtually
    unlimited storage and processing capabilities, in the Fog such resources are much
    more restricted so their optimal management is crucial. Inter-host communication
    in the Cloud is fast due to high speed networks. In the Fog, due to wireless communication
    and varying network types, delays can occur and can vary largely in size between
    hosts. When we consider device to host communication, the Cloud adds significant
    networking delays when accessing remote devices. From a platform perspective we
    can see that Cloud solutions offer full control of resources using VM’s style
    solutions or other Platform as a Service (PaaS) options, while Fog solutions tend
    to share more interdependent and constrained resources between users. Cloud computing
    has a well-established business model as compared to the relatively new concept
    of Fog computing. However, this fact has been recognized by researchers and efforts
    can be seen in literature resolving billing, accounting, monitoring and pricing
    for a Fog business model [38]. CPS requires large computational capabilities to
    process, analyse, and simulate the collected data from sensors to make decisions
    and to instruct controllers, in a limited timeframe. The volume and velocity of
    sensor and visualization data in CPS requires a large amount of storage and software
    applications to process them. The division of the labour of latency tolerant and
    deep analytics tasks between Fog and Cloud depends upon processing power of the
    edge nodes and application domain. The edge nodes with limited computational power
    may only focus on performance of latency sensitive tasks. On the other hand, machine
    learning algorithms that require intensive computing resources should be executed
    in the Cloud. The Cloud service model with elastic and flexible architecture presents
    an appropriate solution to support the emerging CPS. However, the study on how
    data and applications should be distributed between edge devices and the cloud
    has derived little attention from the academic and industry research communities.
    This obviously includes the decision on where machine learning methods for stream
    analytics should be executed: the edge or the cloud. The existing machine learning
    methods with different processing properties have their own strengths and weaknesses,
    so several methods or their variants have been proposed to address diverse requirements
    from different applications. Some methods, for example, may cope better than others
    with incomplete datasets or large datasets, while some may require more computational
    power than others. Given the emerging and promising Cloud and Fog computing architecture
    and the foreseeable integration of CPS, more specifically the machine learning
    based data analytics in CPS, it is necessary to investigate what machine learning
    techniques have been employed in the context of CPS. Further we should consider
    how they should be adapted and deployed in the Cloud–Fog-Edge architecture for
    better fulfilment of the requirements of the application, such as mission criticality
    and time criticality. This research aims to identify and analyse the properties
    of current well-known machine learning methods employed in the context of CPS
    and the characteristics of stream data in CPS to provide a comprehensive analysis
    on their relation. This will help determine how to map data and machine learning
    methods to the Cloud and Edge computing to meet CPS requirements. More specifically,
    we will focus on the analysis of the machine learning models employed in stream
    analytics from the perspective of time complexity. This measure will provide important
    indications to the appropriateness of Edge computing to host tasks, as it has
    limited computational powers, RAM and storage whereas the Cloud has more flexibilities,
    capacities and capabilities to deal with resource-intensive tasks on demand. The
    required qualities for the outputs and the types of results (e.g. precision and
    accuracy rates) have significant influence on the resources and response time
    of the selected methods, so the correlation between them should be investigated.
    To the best of our knowledge, this paper is the first to systematically study
    the machine learning based data stream analysis in CPS and its deployment in the
    emerging Cloud–Fog-Edge architecture. The remainder of the paper is organized
    as follows. We present related work in Section 2. In Section 3, the machine learning
    methods are reviewed from the perspective of the functions they provide for typical
    CPS applications. Then, the time complexities of general machine learning techniques
    are discussed in Section 4, along with how machine learning methods should be
    deployed for effective and efficient integration within Cloud and Fog computing
    architecture. We conclude the paper with some future research directions. 2. Related
    work Traditional CPS may have limited computation and storage capabilities due
    to the tiny size of the devices embedded into the systems. Chaâri et al. [2] investigated
    the integration of CPS into Cloud computing, and presented an overview of research
    efforts in three areas: (1) remote brain, (2) big data manipulation, and (3) virtualization.
    More specifically, real-time processing, enabled by offloading computation and
    big data processing to the Cloud, was explored. Nevertheless, Chaâri et al. [2]
    did not include an exhaustive analysis of the emerging Fog and Edge computing
    technologies, and how these technologies should co-operate with CPS. The authors
    in [16] and [15] presented a survey on data stream analytics from the perspective
    of clustering algorithms. Apart of summarizing the unique characteristics of data
    stream processing by comparison with traditional data processing, in [16], data
    stream clustering algorithms were categorized into five methods (i.e., hierarchical
    methods, partitioning methods, grid-based methods, density-based methods, and
    model-based methods). Similarly, [15] analysed 13 most relevant clustering algorithms
    employed in the context of data stream analytics. In addition to the categories
    listed in [15], the authors in [16] identified three commonly-studied window models
    in data streams, i.e., sliding windows, damped windows, and landmark windows.
    Differently to [15] and [16], we do not solely focus on clustering algorithms,
    but we also extend analytics to other types of machine learning algorithms. In
    [20], the authors studied machine learning techniques employed in transportation
    systems, and identified various conventional machine learning methods such as
    regression (linear regression, polynomial regression and multivariate regression),
    decision tree, artificial neural networks (ANNs), support vector machines (SVMs)
    and clustering. Despite the useful insights provided by the work, the analysis
    is exclusively carried out in the light of a very particular type of CPS application;
    and further, no advanced machine learning methods, e.g. deep learning methods,
    was introduced. The survey provided in [39] recognized the changes that were needed
    to move from a conventional technology-driven transport system into a more powerful
    multi-functional data-driven intelligent transportation system (D2ITS), i.e. a
    system that employed machine learning and other intelligent methods to optimize
    its performance to provide a more privacy-aware and people-centric intelligent
    system. The paper identified both the data sources that drove intelligent transport
    systems (ITS), (e.g. GPS, laser radar, seismic sensor, ultrasonic sensor, meteorological
    sensor, etc.), and the learning mechanisms for real-time traffic control and transportation
    system analysis. Examples of the learning mechanisms were online learning (e.g.,
    state-space neural network, real-time Kalman filter, combination of online nearest
    neighbour and fuzzy inference, hidden Markov model), adaptive dynamic programming
    (ADP), reinforcement learning (RL) and ITS-oriented Learning. The article offers
    a thorough and sound view on transport systems, but the insights are not extrapolated
    to other CPS domains and applications. The authors in [40] presented an analysis
    on a number of existing data mining and predictive machine learning methods for
    big data analytics with the goal of optimizing the dynamic electrical market and
    consumers’ expectations in the smart grid. Similarly, authors in [41] review the
    benefits and gaps of the combination of artificial neural networks, genetic algorithms,
    support vector machines and fuzzy logic for the forecasting of power grid. Another
    similar review is carried out in [42] to analyse the big data methods used to
    manage the smart grid. The authors identified different predictive tasks that
    can be carried out in the smart grid domain such as power generation management,
    power forecasting, load forecasting, operation and control fault diagnosis, and
    so forth. The authors mapped to the corresponding statistical or machine learning
    methods with the required data inputs or sources. 3. Machine learning methods
    in CPS applications In this section we consider some typical CPS applications.
    3.1. Typical CPS applications Smart grid Smart grid is a complex system ranging
    from micro grid to national or international networks involving different levels
    of facility, management and technology. A smart grid is considered as a cyber–physical
    system as it monitors and manages the power generation, loading, and consumptions
    through a number of sensors. These sensors gather the stream data that is fed
    to analytic methods and control systems to balance and distribute power generation
    and consumption [43]. Due to complexity and dynamics of power market, and the
    volatile nature of renewable energy, it is important to have good forecasting
    and prediction on market trend and energy production to correctly estimate the
    amount of power to generate. In addition to this purpose, applications of analytics
    to the smart grid also include fault detection at infrastructure, device, system
    and application levels [10]. Machine learning is a promising tool to analyse the
    data stream and providing results to inform decisions and actions. Intelligent
    Transportation Systems (ITS) An intelligent transportation system (ITS) is an
    advanced application which aims to provide innovative services relating to transport
    and traffic management, and enable users to be better informed and make safer,
    more coordinated, and smarter use of transport networks. ITS brings significant
    improvement in transportation system performance, including reduced congestion
    and increased safety and traveller convenience [[44], [45], [46]]. ITS meets the
    core characteristics of CPS. Enabled by Information and Communication Technologies
    (ICT), elements within the transportation system, such as vehicles, roads, traffic
    lights and message signs, are becoming intelligent by embedding microchips and
    sensors in them. In return, this allows communications with other agents of the
    transportation network, and the application of advanced data analysis and recognition
    techniques — to the data acquired from embedded sensors. As a result, intelligent
    transportation systems empower actors in the transportation system to make better-informed
    decisions, e.g. whether it is choosing which route to take; when to travel; whether
    to mode-shift (take mass transit instead of driving); how to optimize traffic
    signals; where to build new roadways; or how to hold providers of transportation
    services accountable for results [[39], [46]]. Smart manufacturing/industrial
    4.0 Manufacturing applications, such as object detection, force and torque sensor
    based assembly operations, require accuracy of object detection, pose estimation
    and assembly to within few micrometres. Moreover, this accuracy has to pass the
    test of time and repeatability (i.e., the results should be precise). Manufacturing
    in general and automotive manufacturing in particular, requires operation involving
    handling, inspection or assembly to be completed in few seconds. For example,
    BMWs mini plant in Oxford has a car coming of production line every 68 s [47].
    Applications, such as welding, require real time data processing, analysis and
    results. For example, to track the position of joining plates on real time basis
    and adjust the movement of weld guns on real time basis for precise and accurate
    welding at high speed [48]. 3.2. Machine learning in a nutshell Machine learning
    is the discipline that aims to make computers and software learn how to program
    itself and improve with experience/data, with the goal of solving particular problems
    [49]. Typically, a machine learning algorithm is a specific recipe that tells
    a computer/software how to improve itself from experience. A model is the result
    of training a machine learning algorithm with a set of data or experiences of
    a given problem, and it can be employed to solve future related problems. Machine
    learning algorithms fall into one of the following categories: supervised learning,
    unsupervised learning, and reinforcement learning. Next, we briefly discuss each
    of these categories and describe some of the most relevant techniques for each
    category: • In supervised learning, the aim is learning a mapping from an input
    to an expected output that is provided by a supervisor or oracle (i.e., labelled
    data) [18]. Depending on the type of output, we say that we either have a classification
    or a regression problem. In the first case, we aim to produce a discrete and finite
    number of possible outputs, while in the second case the range of possible outputs
    are infinite and numeric [18]. • In unsupervised learning, there is no such supervisor
    and only the input data is present. The aim of these algorithms is finding regularities
    in the input [[18], [20]]. • Finally, reinforcement learning applies to the cases
    where the learner is a decision-making agent that takes actions in an environment
    and receives reward (or penalty) for its actions in trying to solve a problem.
    Thus, the learning process is guided by a series of feedback/reward cycles [20].
    Here, the learning algorithm is not based on given examples of optimal outputs,
    in contrast to supervised learning, but instead it must discover them by a process
    of trial and error [50] Next, we describe some of the most usual machine learning
    algorithms employed in the context of CPS data stream analytics. Decision trees
    and random forests A decision tree is a supervised machine learning algorithm
    that is organized in a tree-like hierarchical structure composed by decision nodes
    and leaves. Leaves represent expected outputs, and decision nodes branch the path
    to one of the expected outputs according to the value of a specific input attribute.
    Decision tree algorithms exist in the form of classification and regression algorithms
    [18]. One of the main advantages of decision trees is that the model is human
    readable and understandable. A Random Forest is an ensemble of random trees constructed
    by means of bagging. By this process, a training dataset of N samples is divided
    into k different datasets of N ′ samples uniformly sampled with replacement from
    the original dataset, and consisting of a random selection of the input attributes.
    Then, each dataset is employed to train a different decision tree, guided by the
    heuristic that the combination of the resulting models should be more robust to
    overfitting. Each tree provides an output that can be aggregated by a wide variety
    of rules [[51], [52]]. Artificial Neural Networks (ANNs) and variants ANNs are
    machine learning algorithms that resemble the architecture of the nervous system,
    organized as interconnected networks of neurons organized in layers. These versatile
    algorithms are typically employed for supervised, unsupervised, and reinforcement
    learning. The inputs of the network (input layer) are transformed by weighted
    (non) linear combinations that generate values that can be further transformed
    in other layers of the network until they reach the output layer. Due to their
    ability to represent potentially complex relationship between the inputs and the
    expected output, ANNs, such as the multilayer perceptron (MLP), have gained popularity
    in machine learning and data analytics realm. The multilayer perceptron is a nonparametric
    estimator that can be used for both classification and regression. Convolutional
    Neural Networks (CNNs) exploit translational invariance within their structures
    by extracting features through receptive fields and learning by weight sharing.
    CNNs usually include two parts. The first part is a feature extractor, which learns
    features from raw data automatically and is composed of multiple similar stages
    and layers. The second part is a trainable fully-connected MLP or other classifiers
    such as SVM, which performs classification based on the learned features from
    the previous part [[53], [54]]. Recurrent Neural Networks (RNNs) are a family
    of neural networks that has gained popularity in the last few years [55], and
    they are of special relevance to stream analytics due to this characteristic.
    In addition to this, the surge of data and computing power present in the last
    decade have given rise to deep neural networks [56] that stack multiple non-linear
    layers of neurons to represent more complex relationships between inputs and outputs
    or more efficient representations of the inputs. For various closely related definitions
    of deep learning, please refer to [56]. Support Vector Machines (SVMs) Support
    vector machines (SVMs) are supervised learning methods that classify data patterns
    by identifying a boundary or hyperplane with maximum margin between data points
    of each class/category [[20], [51]]. The support vector machine is fundamentally
    a two-class classifier, although multiclass classifiers can be built up by combining
    multiple two-class SVMs. Despite the fact that they were initially devised for
    classification tasks, SVMs have been further extended to regression problems [57].
    Bayesian networks and variants Bayesian networks are probabilistic graphical models
    based on directed acyclic graphs where the nodes are random variables and the
    direct arcs indicate the direct influences, specified by the conditional probability,
    between two random variables [[18], [58]]. Some popular machine learning algorithms
    such as Naïve Bayes, a popular supervised classifier, and Hidden Markov models
    (HMMs) can be considered as special cases of Bayesian networks. The second specializes
    at processing sequences of outputs by learning implicit states that generate outputs
    [[18], [50]]. This paradigm has been used for both supervised and unsupervised
    tasks. Evolutionary computation Evolutionary Computing is the collective name
    for a range of problem-solving techniques based on the principles of biological
    evolution, such as natural selection and genetic inheritance. The fundamental
    metaphor of evolutionary computing relates this powerful natural evolution to
    a particular style of problem solving — that is a pseudo trial-and-error guided
    by the value of a given fitness function that measures the goodness of the evolved
    individual/solution [59]. Evolutionary computing techniques mostly involve metaheuristic
    optimization algorithms, such as genetic algorithms and swarm intelligence. Genetic
    algorithms have been employed in supervised [60], unsupervised [61], and reinforcement
    learning problems [62]. Clustering Clustering is an unsupervised family of algorithms
    that involve processing data and partitioning the samples into subsets known as
    clusters. The aim of this process is to classify similar objects into the same
    cluster while keeping dissimilar objects in different clusters [16]. The separation
    criteria may include (among others) maximization of similarities inside clusters,
    minimization of similarities between different clusters, and minimization of the
    distance between cluster elements and cluster centres. One of the most popular
    clustering algorithms is called k-means clustering where k denotes the number
    of clusters. Self-organizing map (SOM) SOM is an automatic data-analysis method
    widely applied to clustering problems. SOM represents a distribution of input
    data items using a finite set of models. These models are automatically associated
    with the nodes of a regular grid in an orderly fashion such that more similar
    models become automatically associated with nodes that are adjacent in the grid,
    whereas less similar models are situated farther away from each other in the grid.
    This organization, a kind of similarity diagram of the models, makes it possible
    to obtain an insight into the topographic relationships of data, especially of
    high-dimensional data items [63]. Q -learning Q -learning is a kind of reinforcement
    learning technique that is a simple way for agents to learn how to act optimally
    in controlled Markovian domains. It amounts to an incremental method for dynamic
    programming which imposes limited computational demands. It works by successively
    improving its evaluations of the quality of particular actions at particular states
    [64]. 3.3. Machine learning methods in CPS Table 1 shows an overview of machine
    learning methods where they have been used in the loose context of CPS. They have
    been used to carry out tasks in three different application domains: smart grid,
    transport and manufacturing. ANN is one of the most popular methods having been
    used in the various application domains, as it is capable of performing long term
    forecasting by regressing the time series stream to predict trends. For example,
    in smart grid and manufacturing, ANN can efficiently predict energy consumption
    of consumers which is beneficial for demand side management. Only few researchers
    use ANN for real-time or short-term predication [[84], [68]], as it requires considerable
    time to process and tune the parameters before it can be deployed. Most applications
    require large amount of input data and training time to produce meaningful models
    with certain degree of accuracy and confidence [[65], [41], [70]]. ANN can work
    alone and produce acceptable results, but it often works with other learning methods
    such as SVM, GA, Bayesian etc. to improve training efficiency or modelling accuracy
    [41]. In the table, the term ANN was broadly used, but it has a lot of variants
    with various activation functions and structures forming a hybrid model to meet
    the purposes such as forecasting, classification, clustering, and regression for
    various applications. [41] has carried out detailed analysis of these variations
    and hybrid approaches. Here, we classify applications into this category using
    ANN as the main body for their solutions. Table 1. Overview of machine learning
    methods in the context of CPS. Machine learning method Domain Functional category
    Task Reference ANN Smart grid Forecasting/Prediction/Regression Electrical power
    prediction, load forecasting [[65], [66], [67], [68], [69], [41]] Transport Pattern
    recognition/ Clustering Behaviour/Event Recognition [51] Forecasting/Prediction/Regression
    Traffic flow features [70] Road-side CO and NO2 concentrations estimation [71]
    Travel time prediction [[72], [73], [74]] Classification Obstacle detection and
    recognition [75] Image processing [76] Manufacturing Forecasting/Prediction/Regression/
    optimization Energy Consumption/ Process parameters optimization [[77], [78]]
    Random forest Smart grid Forecasting/Prediction/Regression Demand side load forecasting/Price
    forecasting [[65], [79]] Anomaly/Fault detection Power record faults [80] Transport
    Pattern recognition/Clustering Behaviour/Event recognition [51] Manufacturing
    Anomaly/Fault detection Tooling wear/Errors detection [[81], [82], [83]] SVM Smart
    Grid Forecasting/Prediction/Regression Price prediction [[84], [85]] Electrical
    power prediction, [[86], [67], [69], [87]] Anomaly/Fault detection Non-technical
    loss detection [[69], [88], [89]] Blackout warning [[86], [90]] Power line attacks
    [90] Transport Classification Unintentional vehicle lane departure prediction
    [91] Obstacles classification [[92], [75]] Pattern recognition/Clustering Behaviour/Event
    recognition [[51], [93]] Anomaly/Fault detection Mechanism failure [94] Forecasting/Prediction/Regression
    Travel time prediction [[95], [74]] Manufacturing Forecasting/Prediction/Regression
    Machine maintenance [96] Design/Configuration Feature design; Production processing
    [[97], [98]] Anomaly/Fault detection Quality control [[99], [100]] Smart home
    Pattern recognition/Clustering Activity recognition [[101], [102]] Decision tree
    Smart grid Anomaly/Fault detection Fault detection predict an energy demand [[103],
    [104]] Forecasting/Prediction/Regression [104] Transport Forecasting/Prediction/Regression
    To predict the traffic congestion level and pollution level; bus travel time [[105],
    [106]] [106] Anomaly/Fault detection Cyber attacks/detect stereotypical motor
    movements [107] Manufacturing Classification/Diagnosis Quality control/Fault diagnosis
    [[108], [109]] Bayesian network Transport Classification Event and behaviour detect
    [51] Smart grid Anomaly/Fault detection Non-technical losses and fault detection
    [103] Manufacturing Anomaly/Fault detection Fault detection in the production
    line [110] Forecasting/Prediction/Regression Tool wear prediction/Energy consumption
    prediction [[111], [112]] Self-organizing map Transport Clustering Obstacle detection
    and recognition [75] Evolutionary computing Smart grid Optimization/Forecasting/Prediction
    Short term load forecasting [113] Swarm computing Smart grid Optimization economic
    load dispatch/feature selection [[114], [115]] Manufacturing Anomaly/Fault detection/Process
    optimization Fault detection, classification and location for long transmission
    lines/Process optimization Automatic fault diagnosis of bearings [[116], [117],
    [118]] HMM Smart grid Optimization Optimal decisions on smart home usage [119]
    Manufacturing Anomaly/Fault detection Automatic fault diagnosis of bearings [[117],
    [120]] Reinforcement learning/ Q -learning-based ADP algorithm Smart grid Optimization
    Aided optimal customer decisions for an interactive smart grid [119] Transport
    Optimization the road latent cost [121] Deep learning/Autoencoder model/ convolutional
    neural network (CNN)/Recurrent Neural Networks (RNNs) Smart grid Forecasting/Prediction/classification/
    Regression Building energy consumption [122] Transport Traffic flow prediction;
    processing roads images/commanding steering; detecting train door anomaly and
    predicting breakdowns Anomaly-based detection of malicious activity [[123], [124],
    [125], [126], [94]] Other To classify various human activities; To detect congestive
    heart failure [54] Other SVM has been widely adopted to address the issues in
    product feature design, fault detection, forecasting, clustering and pattern recognition
    across the application domains such as manufacturing, smart grid, transportation
    as well as smart home due to its maturity and transparency. The method can take
    different sizes of input data to carry out classification and regression, so it
    has been used in applications that require a short response time such as described
    in [[85], [86]]. It also used in conjunction with other machine learning methods
    such as ANN, and Bayesian by exploiting its characteristics to provide complimentary
    functions to address complex problems [[68], [96], [97]]. The authors in [97]
    used a trained SVM classifier from the classified design examples such as features
    and components, which are obtained from a hierarchical clustering, to recommend
    different additive manufacturing design features. In the case study, it only shows
    21 design features from hundreds that were used to train and to build the model.
    The faults in products or tools in manufacturing can lead to a big loss of time
    and a serious consequence if they are not detected and resolved earlier. Authors
    in [81]and [82] reported the use of the Random Forest to analyse the big data
    for tooling condition monitoring in milling production and silicon in semiconductor
    manufacturing. It also has been used in predicting the short term electricity
    price from the historical data [79] and detecting false electricity records from
    the sensors [80]. [51] reported the use of Random Forest to model a driver profile
    effectively. These applications required a reasonably large amount of historic
    data for the training to achieve accurate and time was not considered to be a
    crucial factor. Decision Tree is a well-known method for classification, so it
    is predicable that the researchers have used it to detect the faults in power
    system and motor movement and for quality management in production. It also has
    been used to predict energy demand, bus travelling time, and to determine the
    correlation between traffic congestion and air pollution. The accuracy of fault
    detection, quality prediction, classification and rare events forecasting are
    associated with probabilities, as all the input factors cannot be certain due
    to the dynamic environments and complex human behaviour and interactions. Bayesian
    Network is a well-studied method to model complex probability networks as it has
    been used in different applications to explain the possible occurrences of outputs
    with input variables. It does not require a large amount of input data to form
    the network, if the probabilities of variables are known. The network can be large
    and complex, but its processing time is linear [[51], [103], [110]] showed the
    consistent characteristics in these applications. Table 1 also shows where the
    machine learning methods have been used across four application domains and the
    tasks that have been carried out to gain the benefits of analysing and interpreting
    large volumes of data streams generated. The most common area for researchers
    and industry practitioners adopting the methods is to increase accuracy of predication
    and forecasting in their CPS applications. The authors in [[41], [65], [66], [67],
    [68], [69], [79], [104]] reported adoption of machine learning to predict electrical
    power consumption, demand, supply and load in order to improve demand response
    management in Smart Grid. Machine learning is a well employed tool to predict
    traffic flow, air population emitted by cars, traffic congestion and travel time
    by transport [[70], [71], [72], [73], [74], [105], [106], [94], [123], [124],
    [125], [126]]. Machine learning also has been extensively applied in manufacturing
    by predicting energy consumption in production line, machine maintenance, and
    tool wearing [[77], [78], [102], [111], [112]]. Diagnosis and fault detection
    is another function for which machine learning has been widely used in the manufacturing
    domain. Fault detection applications include root cause of power faults in production,
    tooling wearing and mechanic faults, cause of the fault in components or products,
    and quality control [[51], [99], [100], [110], [117], [120]]. Smart Grid also
    has several machine learning applications for anomaly and fault detection such
    as non-technical loss detection, blackout warning, power line and cyber attacks,
    faults in demand management and power line faults [[69], [88], [89], [86], [90],
    [80], [103], [104]]. The utilization of machine learning for mechanical fault
    diagnosis and prevention of cyber attacks in transport systems can be explored
    further, as only two [[94], [107]] reported the benefits of machine learning in
    this area. Machine learning is also a popular solution to configure plant/production,
    optimize electrical load/dispatch, reduce road latent cost, and forecast short
    term in electricity usage [[75], [97], [98], [113], [119], [121]]. Machine learning
    has been exploited in other applications such as clustering road obstacles, classifying
    driving behaviours and traffic incidents and improving production quality [[51],
    [75], [91], [108], [109]]. From Table 1, it can be seen that functions of MLs
    have brought various benefits to different application areas and they have generated
    different levels of impacts in various areas, but the potential of machine learning
    is not fully realized yet, as the field is still evolving and the prevailing complexity
    may currently hinder take-up. 4. Temporal complexity analysis Machine learning
    algorithms are able to learn from selected samples to derive rules, trends, patterns
    or properties of a true population. The concept or hypothesis space, however,
    can be large and complex such that it cannot be learned or modelled in polynomial
    time. In these cases, learning to achieve highly accurate results by exhaustively
    exploring parameter values may not be possible practically but approximation may
    be achieved. As it is natural, the goal of all machine learning applications is
    to minimize the differences between the target concept and the output produced
    by the trained models. The representation, quality and quantity of the selected
    samples, which are input parameters to the learning algorithms, are important
    attributes to increase the possibility of the successful learning. The probability
    of reaching successful learning by increasing accuracy of approximating to the
    target concept also depends on the complexity of learning and time. Learning is
    a trade-off between time and accuracy. In principle, the higher accuracy, the
    more time is required for training. Information and computation are two main dimensions
    to measure the complexity of learning algorithms. The sample complexity is concerned
    with the number, distribution and sufficiency of training samples. The computational
    complexity of a solution method is the size of computational resources required
    to derive the concepts from the training data. This can be further classified
    into time and space complexity. Space complexity denotes the memory required for
    the computational model being selected to solve the problem. The time complexity
    is measured by the number of computational executions required to reach or approximate
    to the target concept. In this paper, we intend to show asymptotic time complexity
    rather than the actual runtime of the algorithms which will be various depending
    on its operating computational environment including hardware and software. Table
    2 shows a list of machine learning methods used by the applications illustrated
    in Table 1 and their corresponding time complexities, represented by big O, and
    the factors contributing to the complexities. Since there are many different variants
    to each machine learning method, it is not feasible to list them exhaustively,
    but some examples to illustrate measurement of complexity are given. For example,
    varieties of Bayesian Network models derived from various approximate and exact
    inference algorithms to infer unobserved variables, can lead to different computational
    complexities. Several hybrid learning methods have been proposed to resolve or
    improve the insufficiency of one individual method. This complicates the measurement
    of the execution due to the interdependency, as one method may reduce the complexity
    for the other in the model, but the overall complexity calculation still needs
    to consider all the methods involved. More algorithms and their time complexity
    can be found in [127]. For example, [119] used Q -learning algorithms to model
    the interaction with users in smart homes There were a maximum of 20 steps to
    interact with users before appropriate recommendation could be made. Its asymptotic
    time complexity is up to 203 and the authors have concluded that Q -learning algorithm
    outperformed greedy or random decision strategies [119] in their simulated cases.
    Fig. 1 shows the complexity level in big O when the number of steps decreases
    in the simulation. The authors did not report the actual runtime, so the asymptotic
    complexity cannot be correlated with the experimental one. Table 2. Time complexity
    of some of the most common machine learning algorithms. Machine learning method
    Asymptotic time complexity Factors Decision tree learning [128] O ( M ⋅ N 2 )
    M : size of the training samples N : number of attributes Hidden Markov model
    forward–backward pass [52] O ( N 2 ⋅ M ) N : number of states M : number of observations
    Multilayer perceptrons [127] O ( n ⋅ M ⋅ P ⋅ N ⋅ e ) n : input variables M : number
    hidden neurons P : number outputs N : number of observations e : Number of epochs
    Deep learning (convolutional neural networks) [129] O ( D ⋅ N ⋅ L ⋅ S 2 ⋅ M 2
    ⋅ e ) L : number of input variables N :number of filters (width) S : spatial size
    (length) of the filter M : size of the output. D : number of convolutional layers
    (depth) e : number of epochs Support vector machine [130] O ( N 3 ) or O ( N 2
    ) N : vectors C : upper bound of samples N 2 when C is small; N 3 when C is big
    Genetic algorithms [127] O ( P ⋅ log P ⋅ I ⋅ C ) C : number of genes/chromosome
    P : population size I : Number of iterations Random forest [[52], [131]] ( K ⋅
    N ⋅ log N ) N : number of samples K : input variables Self-organizing map [132]
    O ( N ⋅ C ) N : input vector size C : cycle size Reinforcement learning [133]
    O ( N 3 ) N : number of steps to reach the goal Particle swarm optimization (PSO)
    [134] O ( P + G e n ⋅ P ⋅ D ) P : number of particles D : number of dimensions
    G e n : number of generations Bayesian network (exact learning models of bounded
    tree-width) [135] O ( 3 N ⋅ N ( w + 1 ) ) N : size of nodes W : width of tree.
    [66] used three machine learning methods, SVM, LS-SVM and BPNN, for energy usage
    forecasting over 283 households with 500 point data (hours) for each. The total
    number of data points for training in the experiments was 141,500 ( 283 ∗ 500
    ) . In their empirical study, the computational times of these methods are 335.39,
    26.22, and 29.28 s respectively over a laptop to produce reasonably accurate results.
    The authors recommend running these approaches in Cloud and distributed computing
    to improve performance. SVM has better accuracy in reducing errors, but it took
    more time than others due to the overhead of using GA to find key parameters for
    SVM. The BPNN has more errors than the other two and it requires more runtime
    than LS-SVM. The authors, however, did not include key parameter values such as
    generations and input points for GA and BPNN, so cannot derive their time complexity
    in relation to actual runtime. The time complexity of LS-SVM is O (141,5002).
    Fig. 2 shows the time complexity of LM-SVM by applying the data from [66] with
    simulation output and the actual runtimes in seconds. Download : Download high-res
    image (86KB) Download : Download full-size image Fig. 1. Complexity level and
    number of steps in Q -learning. It shows actual runtimes against the complexity
    level and the correlation between them without carrying out the actual experiments,
    the researchers can estimate its actual runtime by giving the number of samples
    when the underlying machine or environment has the same characteristics. The authors
    in [136] report the applications of the Particle Swarm Optimization (PSO) method
    to balance different loads by considering price to dispatch them. Their test case
    one includes 6 factors (dimensions), 6 generators (particles) and 100 generations
    to evolve, and its time complexity in theory is 3606 ( 6 + 6 ∗ 100 ∗ 6 ) before
    it has a satisfactory convergent result. In their test case two, it increases
    to 7 factors, 40 generators and 400 generations, so 40 + 40 ∗ 400 ∗ 7 (so the
    asymptotic time complexity is 112,040). In another test case it has 5 factors,
    20 generators, and 400 generations (asymptotic time complexity is 40,020) and
    its actual computational runtime is 0.29282 s that is around 10 and 200 times
    slower than the other approaches [136] in the simulation. Fig. 3 shows the relationship
    between complexity and actual runtime by extending the figures given in the paper.
    The line is the time complexity in log with respect to actual runtime. The researchers
    can refer this to approximate the actual runtime of an application with the same
    computational resources by giving key parameter values of the learning method.
    The approximation is not rigid, as we assume that the space complexity is changing
    linearly. Download : Download high-res image (526KB) Download : Download full-size
    image Fig. 2. Time complexity of LM-SVM. For deep NN learning methods such as
    CNN, the weights in the convolutional layers are trained and updated in a similar
    way as traditional ANNs/MLPs except that the number of filters and layers are
    orders of magnitude higher than those in traditional ANNs and MLPs. The authors
    in [129] report their experimental results on computational time complexity of
    a CNN model by varying different key parameters such as depth, filter size and
    number, width and pooling layer of the network to find their trade-offs between
    two parameters to investigate the overall performance in terms of time complexity
    and output accuracy. We share the same view with the authors [129] that introducing
    computational time and memory constraints can give better understanding the value
    of machine learning methods in realistic business applications. Download : Download
    high-res image (195KB) Download : Download full-size image Fig. 3. Relationship
    between complexity and actual runtime of particle swarm optimization method. The
    training of these deep NN models needs massive resources (e.g. to accommodate
    the training data) and time. Therefore they should be carried out on the Cloud.
    However, the operation time of these models is only proportional to the number
    of neurons no matter how large the training data is, the online analysis tasks
    can be deployed on the Edge/Fog. As it has been observed in this analysis, few
    research outputs report the empirical time complexity of their approaches. Therefore,
    the estimation on the empirical time complexity of a training algorithm still
    has room for more extensive study. This information may be vital for decision
    making on-the-fly if a learning task can be deployed in the Edge devices. 5. On-line
    learning methods If we take a look at Table 2 we will observe that the asymptotic
    complexity of the classic learning algorithms reported in the literature review
    normally takes into consideration many terms (e.g., number of samples, iterations,
    structure parameters, etc.). In theory, this could result in high order polynomial
    behaviour, which would deter the deployment of the learning phase in Edge devices.
    This is because firstly, over time more and more streaming data will be accumulated
    and it is impractical and often infeasible to accommodate large volumes of streaming
    data in the machine’s main memory. Secondly, it is also infeasible to regularly
    reconstruct new models from the scratch with accumulated streaming data in real-time.
    Furthermore CPS data streams feature perishable insights, i.e., information that
    must be acted upon fast, as insights obtained from streaming data, such as from
    sensors, quickly lose their value if they were to be processed in ‘batch mode’
    [16]. As a result, a new paradigm of learning, i.e. incremental and on-line learning
    algorithms should be adopted. Losing et al. [137] gives the definition of incremental
    learning for supervised learning as below (we change the notations/symbols for
    consistency reasons). An incremental learning algorithm generates, on a given
    stream of training data S 1 , S 2 … , S N , a sequence of models H 1 , H 2 … ,
    H N , where S i is labelled training data S i = ( X i , Y i ) ∈ R n × { 1 , …
    , C } and H i : R n { 1 , … , C } is a model function solely depending on H i
    − 1 and the recent p examples S i , … , S i − p , with p being strictly limited.
    Losing et al. [137] further specify on-line learning algorithms as incremental
    learning algorithms which are additionally bounded in model complexity and run-time,
    capable of endless/lifelong learning on a device with restricted resources. Incremental
    and online learning algorithms aim for minimal processing time and space; and
    thus fit in CPS data processing environments. Losing et al. [137] evaluate eight
    popular incremental methods representing different algorithm classes such as Bayesian,
    linear, and instance-based models as well as tree-ensembles and neural networks.
    Experiments are carried out to evaluate these algorithms with respect to accuracy,
    convergence speed as well as model complexity, aiming at facilitating the choice
    of the best method for a given application. However, it primarily covers supervised
    incremental learning algorithms with stationary datasets, although robustness
    of the methods to different types of real concept drift are also investigated.
    Gama et al. [138] considers dynamically changing and non-stationary environments
    where the data distribution can change over time yielding the phenomenon of concept
    drift, which applies to most of the real world CPS applications. Adaptive learning
    algorithms, defined as advanced incremental learning algorithms that are able
    to update predictive models online during their operation to react to concept
    drifts, are explored. A taxonomy for adaptive algorithms, presented in four modules
    as memory, change detection, learning, and loss estimation, is proposed; and the
    methods within each module are also listed. Gama et al. [138] focuses on online
    supervised learning. Ade et al. [139] includes some unsupervised incremental learning
    approaches that learn from unlabelled data samples to adjust pre-learned concepts
    to environmental changes. Most of the incremental clustering algorithms for pattern
    discovery rely on similarity measures between the data points. One approach is
    called Concept Follower (CF) that includes CF1 and CF2 [140]. CF1 and CF2 learn
    from unlabelled data samples to adjust pre-learned concepts to environmental changes.
    Initially, a supervised learner is used to learn and label a set of concepts.
    When a new sample is collected, CF1 calculates the distance of the sample to all
    concepts and the concept with the minimal distance to the sample is identified.
    If the distance is smaller than the predefined threshold, CF1 considers the concept
    a match and then slightly shifts, by a learning rate parameter, towards the classified
    sample to adjust to the concept drift; otherwise CF1 detects the abrupt change
    and repeats the initial supervised learning stage. Compared to CF1, CF2 supports
    problems areas with unbalanced sample ratio between concepts. This is done by
    CF2 adjusting all concepts in the proximity of the sample instead of, as does
    CF1, adjusting only the concept closest to the sample. Next, we discuss on some
    of the most relevant online approaches to the machine learning algorithms identified
    in this article. Artificial neural networks Classically, ANN are trained using
    a training set and optimization methods such as gradient descent and backpropagation
    to minimize a cost function correlated to the error derived from the current state
    of the network. The online version can adapt to the arrival of new data by pre-training
    the network with the available training set, and then adapting the pre-trained
    network by using stochastic gradient descent over the new series of available
    data. This type of setting would benefit from a combination of both Cloud technologies
    (i.e., for pre-training the network), and Edge computing (i.e. for adapting the
    network). While the use of stochastic gradient descent allows adopting a batch
    algorithm like backpropagation in a non-batch setting, there are specialized learning
    algorithms, called on-line sequential learning methods, for training neural networks
    in an on-line setting in which data becomes available with time [[141], [142],
    [143], [144]]. They can be efficiently deployed in an Edge device as they do not
    need to store past training samples. The online sequential learning methods tend
    to be ad-hoc for networks with specific activation functions, or with specific
    architectures (e.g., single hidden layer). Therefore, the complexity of problems
    represented by these networks may not be as vast as the one represented by classic
    neural networks or deep learning approaches. Decision trees Generating a classic
    decision tree requires that all of the training samples are considered [145].
    This is hardly applicable in a stream analytics context, as training samples arrive
    constantly. Therefore, different learning mechanisms are required to properly
    train decision trees in a stream analytics context, in which the trees can evolve
    from a stream of data. Some approaches with a default tree structure provide a
    series of greedy steps to adapt to the new training samples. These include ID5R
    algorithm [146], an adaptation of the popular ID3 learning algorithm for stream
    data, and ITI [147]. These greedy changes were in some cases suboptimal and ended
    up in inappropriate adaptations to change. The other approach to learning decision
    trees from streams is to maintain a set of statistics at nodes and only split
    a node when sufficient and statistically significant information is available
    to make the split. Hoeffding inequality [[148], [149], [150]] is the backbone
    to these approaches, which provide bounds for the number of observations that
    are necessary to obtain an estimated mean that does not differ from the mean of
    the underlying random variable. Some researchers have recently argued that the
    assumptions underlying the Hoeffding inequality are not appropriate when constructing
    on-line trees. Some methods split the nodes of the decision tree based on other
    modelling paradigms such as McMiarmid’s bound [151], or Gaussian processes [152].
    Random forests The general idea behind on-line random forests consists of providing
    both a method to carry out on-line bagging, and a method to carry out on-line
    learning of random trees. Abdulsalam et al. [153] take an approach that carries
    out on-line bagging by dividing the incoming samples of data into blocks with
    a certain size. Then, blocks of data randomly selected are employed for either
    training or testing a tree in the model. The training block is redirected to a
    chosen tree, and an on-line learning algorithm for trees is employed to update
    the current tree. Later on, the learning model is enhanced to adapt to the random
    arrival of labelled examples in the stream, with blocks of different sizes and
    frequency [154]. Another alternative to the on-line bagging process described
    above is employed by Saffari et al. [155]. In this case, each new sample is presented
    a number of times controlled by a Poisson distribution, to each random tree in
    the model. Then, the random trees gradually grow by creating random tests and
    thresholds at decision nodes and choosing the best one after a number of statistics
    have been gathered that guarantee that the test is the best from the ones randomly
    created at the decision node. Other approaches opt for avoiding on-line bagging
    at the forest level, and the sub-sampling is carried out at the tree level [156].
    When a new sample arrives to the random forest, this sample is presented to all
    of the trees. Then, the individual tree decides if the sample will be used to
    influence the structure of the tree, or used to estimate class membership probabilities
    in the leaf to which they are assigned. Support vector machines Classification
    in support vector machines is based on the idea of finding the maximum margin
    hyperplane that separates elements from different categories. By definition, one
    should have access to the entire training dataset in order to build such maximum
    margin hyperplanes. Otherwise, there would be no guarantee that estimated hyperplanes
    are optimal. This assumption limits the applicability of classic support vector
    learning algorithms to an online setting, and it forces scholars to devise new
    methods that are adapted to the online setting. The incremental approach to support
    vector learning typically requires deciding if a new sample should become a support
    vector that modifies the current hyperplane. The algorithm also needs to determine
    if previously calculated support vectors are still as relevant after the observation
    of the new sample, and remove those that are no longer relevant. Otherwise, online
    approaches to support vector learning incur in the risk of growing linearly with
    the infinite number of samples [157]. To tackle this problem, there have been
    a number of proposals that aim to build a support vector model with adequate predictive
    performance while also minimizing the number of support vectors in the resulting
    model [[157], [158], [159], [160]]. 6. Discussion So far machine learning methods
    of various categories have been employed for data stream analysis purposes. Little
    literature has studied the integration of these methods to the Cloud and Fog computing
    architecture. The very nature of CPS requires a computing paradigm that offers
    latency sensitive monitoring, intelligent control and data analytics for intelligent
    decision making. In contrast to the Cloud, the Fog performs latency-sensitive
    applications at the edge of network, however latency tolerant tasks are efficiently
    performed in the Cloud for deep analytics [161]. Cloud computing provides on demand
    and scalable storage and processing services that can scale up to requirements
    of IoT based CPS. However, for healthcare applications, manufacturing control
    applications, connected vehicle applications, emergency response, and other latency
    sensitive applications, the delay caused by transferring data to the Cloud and
    back to the application becomes unacceptable [[162], [163], [164]]. The latency
    sensitive applications rely on the Fog for their time critical functionality.
    The adoption of Fog computing not only greatly improves the response time of time
    sensitive applications but also brings some new challenges such as business model,
    security, privacy and scalability. It is perceived that in time critical services
    Fog computing is cost-effective compared to Cloud computing due to its lesser
    latency and in some cases due to spare capacity of locally available resources.
    The view is endorsed by study carried out in [163], which shows that with high
    number of latency sensitive applications Fog computing outperforms Cloud computing
    in terms of power consumption service latency and cost. Since data stream analytics
    processes the data in one scan, due to perishable insights, some algorithms, are
    infeasible for streaming data as they require multiple scans of data [165]. In
    addition, for memory-based methods such as the Parzen probability density model
    and nearest-neighbour methods, the entire training set needs to be stored in order
    to make predictions for future data points. Also, a metric is required to be defined
    to measure the similarity of any two vectors in input space. These requirements
    are both memory consuming and generally slow at making predictions for test data
    points. Therefore they should not be employed for data stream analysis, even though
    the Fog computing is introduced. ANN (MLP), DT and SVM are the most commonly used
    machine learning methods in surveyed CPS. In terms of accuracy, it is observed
    that the performance of these machine learning methods is task dependent. For
    example, [75] pointed out that the best classifier differs according to the weather
    conditions. The classifier based on MLP behaves better than SVM (and SOM) for
    sunny and foggy conditions, whereas for rainy conditions, the SVM-based model
    is the most appropriate. [166] concluded that in automatic Stereotypical Motor
    Movements (SMM) recognition, SVM appears to outperform DT on overall accuracy
    by ∼ 6 percentage points (although at times DT did outperform SVM), regardless
    of feature set used. In terms of the operation (classification or regression)
    time, [107] discovered the noticeably lower detection latency provided by DT while
    [76] ascertained that SVM was not fast enough for real-time classification (classification
    time being around 2.2 s) compared to ANN with seven hidden nodes (classification
    time being around 100 ms). For those machine learning methods that need massive
    training data and take iterations to converge, such as ANN, HMM and reinforcement
    learning methods, it is recommended to deploy the training tasks onto the Cloud
    while deploying the on-line analysis tasks on the Edge/Fog. For deep NN learning
    methods such as CNN, the weights in the convolutional layers are trained and updated
    in the similar way to traditional MLPs except that the number of weights and layers
    are orders of magnitude higher than MLPs. As the training of these deep NN models
    needs massive resources (e.g. to accommodate the training data) and time, they
    should be carried out on the Cloud. However, the operation latency of these models
    is only proportional to the number of neurons no matter how large the training
    data is, the online analysis tasks can be deployed on the Edge/Fog. When machine
    learning methods are deployed on the Edge, trade-offs are needed among accuracy,
    operation time, and the parameters of these methods such as sliding window sizes,
    number of iterations and prediction/forecast time lags [[51], [71]]. Application
    dependent data pre-processing proved effective in improving the performance of
    the data analysis. For example, in [76], before employing an ANN classifier, a
    simple gradient detector and an intensity-bump detector with loose (low) threshold
    values are applied to quickly filter out non-lane markings. As the remaining samples
    are much smaller in number, the classification time was significantly reduced.
    Due to space limitations, this paper does not investigate the data pre-processing
    techniques for machine learning methods in CPS. The distributed and parallel environment
    provided by Cloud and Fog computing may facilitate the execution of machine learning
    methods (such as Random Forest) to further reduce the classification time as the
    sets of sub-tasks (such as the decision trees involved in Random Forest) can be
    run in parallel. The data stream properties also could affect the choice of the
    methods. For example, fuzzy logic is more capable of dealing with fuzzy information
    without requiring large volumes of samples. Existing deep learning methods will
    require substantial numbers of samples in the training process. In addition, ANN
    is likely to be more appropriate to deal with multiple variant datasets than reinforcement
    learning methods. 7. Conclusion and future research directions Data stream analytics
    is one of the core components in CPS and machine learning methods have proved
    to be effective techniques of data analytics. The rise of Cloud and Fog computing
    paradigm calls for the study of how the machine learning based CPS data stream
    analytics should be integrated to such a paradigm in order to better meet CPS
    requirements of mission criticality and time criticality. This paper investigated
    and summarized the existing machine learning methods for CPS data stream analytics
    from various perspectives, especially from the time complexity point of view.
    The investigation led to the discussion and guidance of how the CPS machine learning
    methods should be integrated to the Cloud and Fog architecture. In the future,
    more effective and efficient machine learning methods should be developed for
    analysing the ever growing data streams in CPS. of Distributed and parallel environments
    provided by the Cloud and Fog computing [167] may be utilized. Hierarchical and
    composable machine learning methods that are well suited to partitioned execution
    across the Cloud and the Edge are needed, as are transfer and continual learning
    techniques to deal with the non-stationarity of data streams. In the meanwhile,
    studies should be carried out on the development of Cloud and Edge systems that
    facilitate the CPS data stream analytics by accommodating the discrepancy and
    the heterogeneity between the capabilities of Edge devices and data centre servers
    and among the Edge devices themselves, providing uniformed APIs [168] and services
    [[169], [170]]. Acknowledgements This work is partially supported by EU H2020
    programme (Project NOESIS under grant no 769980) and Innovate UK project(Project
    SABOT under grand no 103539). References [1] Lee E.A. The past present and future
    of cyber-physical systems: A focus on models Sensors, 15 (2015), pp. 4837-4869,
    10.3390/s150304837 View in ScopusGoogle Scholar [2] Chaâri R., Ellouze F., Koubâa
    A., Qureshi B., Pereira N., Youssef H., Tovar E. Cyber-physical systems clouds:
    A survey Comput. Netw., 108 (2016), pp. 260-278, 10.1016/j.comnet.2016.08.017
    View PDFView articleView in ScopusGoogle Scholar [3] A. Rayes, S. Salam, Internet
    of Things (IoT) overview, internet things from hype to real, 2017, pp. 1–34. http://dx.doi.org/10.1016/J.FUTURE.2013.01.010.
    Google Scholar [4] NIST, Strategic vision and business drivers for 21st century
    cyber-physical systems, 2013. https://www.nist.gov/sites/default/files/documents/el/Exec-Roundtable-SumReport-Final-1-30-13.pdf.
    Google Scholar [5] E.a. Lee, Cyber-physical systems — are computing foundations
    adequate? October 1, 2006, pp. 1–9. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.8011&rep=rep1&type=pdf.
    Google Scholar [6] Wang L., Törngren M., Onori M. Current status and advancement
    of cyber-physical systems in manufacturing J. Manuf. Syst., 37 (2015), pp. 517-527,
    10.1016/j.jmsy.2015.04.008 View PDFView articleView in ScopusGoogle Scholar [7]
    NSF, Cyber-physical systems (CPS), 2017. https://www.nsf.gov/pubs/2017/nsf17529/nsf17529.htm.
    Google Scholar [8] J. Shi, J. Wan, H. Yan, H. Suo, A survey of cyber-physical
    systems, in: 2011 Int. Conf. Wirel. Commun. Signal Process., 2011, pp. 1–6. http://dx.doi.org/10.1109/WCSP.2011.6096958.
    Google Scholar [9] Guan X., Member S., Yang B., Chen C., Dai W., Wang Y. A comprehensive
    overview of cyber-physical systems: from perspective of feedback system IEEE/CAA
    J. Autom. Sin., 3 (2016), pp. 1-14, 10.1109/JAS.2016.7373757 View in ScopusGoogle
    Scholar [10] S.K. Khaitan, J.D. McCalley, Cyber physical system approach for design
    of power grids: A survey, in: 2013 IEEE Power Energy Soc. Gen. Meet., 2013, pp.
    1–5. http://dx.doi.org/10.1109/PESMG.2013.6672537. Google Scholar [11] Lee J.,
    Bagheri B., Kao H.A. A cyber-physical systems architecture for industry 4.0-based
    manufacturing systems Manuf. Lett., 3 (2015), pp. 18-23, 10.1016/j.mfglet.2014.12.001
    View PDFView articleView in ScopusGoogle Scholar [12] Asadollah S.A., Inam R.,
    Hansson H. A Survey on Testing for Cyber Physical System Lect. Notes Comput. Sci.,
    vol. 9447 (2015), pp. 194-207, 10.1007/978-3-319-25945-1_12 Including Subser.
    Lect. Notes Artif. Intell. Lect. Notes Bioinformatics View in ScopusGoogle Scholar
    [13] A. Humayed, J. Lin, F. Li, B. Luo, Cyber-physical systems security — a survey,
    4662, 2017. http://dx.doi.org/10.1109/JIOT.2017.2703172. Google Scholar [14] I.
    Akkaya, Data-driven cyber-physical systems via real-time stream analytics and
    machine learning, 2016, p. 136. https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-159.pdf.
    Google Scholar [15] a Silva J., Faria E.R., Barros R.C., Hruschka E.R., De Carvalho
    A.C.P.L.F., Gama J. Data stream clustering: A survey ACM Comput. Surv., 46 (2013),
    pp. 13:1-13:31, 10.1145/2522968.2522981 Google Scholar [16] Mousavi M., Bakar
    A.A., Vakilian M. Data stream clustering algorithms: A review Int. J. Adv. Soft
    Comput. Appl., 7 (2015), pp. 1-15 View in ScopusGoogle Scholar [17] M.D. Jayanthi,
    A framework for real-time streaming analytics using machine learning approach,
    2016, pp. 85–92. Google Scholar [18] E. Alpaydın, Introduction to Machine Learning,
    second ed., 2010. Google Scholar [19] SAS, Machine kearning: What it is & why
    it matters, (n.d.). https://www.sas.com/it_it/insights/analytics/machine-learning.html.
    Google Scholar [20] P. Bhavsar, I. Safro, N. Bouaynaya, R. Polikar, D. Dera, Mchine
    learning in transportation data analysis, in: Data Anal. Intell. Transp. Syst.,
    2017, pp. 283–307. Google Scholar [21] Distefano S., Merlino G., Puliafito A.
    A utility paradigm for IoT: The sensing cloud Pervasive Mob. Comput., 20 (2015),
    pp. 127-144, 10.1016/j.pmcj.2014.09.006 View PDFView articleView in ScopusGoogle
    Scholar [22] Christophe B., Boussard M., Lu M., Pastor A., Toubiana V. The web
    of things vision: Things as a service and interaction patterns Bell Labs Tech.
    J., 16 (2011), pp. 55-61, 10.1002/bltj.20485 View in ScopusGoogle Scholar [23]
    Zhang Y., Qiu M., Tsai C.-W., Hassan M.M., Alamri A. Health-CPS: Healthcare cyber-physical
    system assisted by cloud and big data IEEE Syst. J., 11 (2017), pp. 88-95, 10.1109/JSYST.2015.2460747
    Google Scholar [24] Hossain M.S., Hassan M.M., Al Qurishi M., Alghamdi A. Resource
    allocation for service composition in cloud-based video surveillance platform
    2012 IEEE Int. Conf. Multimed. Expo Work, IEEE (2012), pp. 408-412, 10.1109/ICMEW.2012.77
    View in ScopusGoogle Scholar [25] Botta A., de Donato W., Persico V., Pescapé
    A. Integration of cloud computing and internet of things: A survey Future Gener.
    Comput. Syst., 56 (2016), pp. 684-700, 10.1016/j.future.2015.09.021 View PDFView
    articleView in ScopusGoogle Scholar [26] Stojmenovic I. Fog computing: A cloud
    to the ground support for smart things and machine-to-machine networks 2014 Australas.
    Telecommun. Networks Appl. Conf., IEEE (2014), pp. 117-122, 10.1109/ATNAC.2014.7020884
    View in ScopusGoogle Scholar [27] Bonomi F., Milito R., Zhu J., Addepalli S. Fog
    computing and its role in the internet of things Proc. First Ed. MCC Work. Mob.
    Cloud Comput, MCC ’12, ACM Press, New York, New York, USA (2012), p. 13, 10.1145/2342509.2342513
    View in ScopusGoogle Scholar [28] Dastjerdi A.V., Buyya R. Fog computing: Helping
    the internet of things realize its potential Computer, 49 (2016), pp. 112-116,
    10.1109/MC.2016.245 View in ScopusGoogle Scholar [29] Aazam M., Huh E.-N. Fog
    computing and smart gateway based communication for cloud of things 2014 Int.
    Conf. Futur. Internet Things Cloud, IEEE (2014), pp. 464-470, 10.1109/FiCloud.2014.83
    View in ScopusGoogle Scholar [30] Hong K., Lillethun D., Ramachandran U., Ottenwälder
    B., Koldehofe B. Mobile fog: a programming model for large-scale applications
    on the internet of things Proc. Second ACM SIGCOMM Work. Mob. Cloud Comput, MCC
    ’13, ACM Press, New York, New York, USA (2013), p. 15, 10.1145/2491266.2491270
    View in ScopusGoogle Scholar [31] Jalali F., Hinton K., Ayre R., Alpcan T., Tucker
    R.S. Fog computing may help to save energy in cloud computing IEEE J. Sel. Areas
    Commun., 34 (2016), pp. 1728-1739, 10.1109/JSAC.2016.2545559 View in ScopusGoogle
    Scholar [32] R. Mahmud, R. Kotagiri, R. Buyya, Fog computing: A taxonomy, survey
    and future directions, 2018, pp. 103–130. http://dx.doi.org/10.1007/978-981-10-5861-5_5.
    Google Scholar [33] Datta S.K., Bonnet C., Nikaein N. An IoT gateway centric architecture
    to provide novel M2M services 2014 IEEE World Forum Internet Things, IEEE (2014),
    pp. 514-519, 10.1109/WF-IoT.2014.6803221 View in ScopusGoogle Scholar [34] Ochoa
    S.F., Fortino G., Di Fatta G. Cyber-physical systems internet of things and big
    data Future Gener. Comput. Syst., 75 (2017), pp. 82-84, 10.1016/j.future.2017.05.040
    View PDFView articleView in ScopusGoogle Scholar [35] Bittencourt L.F., Lopes
    M.M., Petri I., Rana O.F. Towards virtual machine migration in fog computing 2015
    10th Int. Conf. P2P, Parallel, Grid, Cloud Internet Comput., IEEE (2015), pp.
    1-8, 10.1109/3PGCIC.2015.85 View in ScopusGoogle Scholar [36] Bellavista P., Zanni
    A. Feasibility of fog computing deployment based on Docker containerization over
    Raspberry Pi Proc. 18th Int. Conf. Distrib. Comput. Netw, ICDCN ’17, ACM Press,
    New York, New York, USA (2017), pp. 1-10, 10.1145/3007748.3007777 Google Scholar
    [37] Al-Fuqaha A., Khreishah A., Guizani M., Rayes A., Mohammadi M. Toward better
    horizontal integration among IoT services IEEE Commun. Mag., 53 (2015), pp. 72-79,
    10.1109/MCOM.2015.7263375 View in ScopusGoogle Scholar [38] Yi S., Li C., Li Q.
    A survey of fog computing: Concepts, applications and issues Proc. 2015 Work.
    Mob. Big Data, Mobidata ’15, ACM Press, New York, New York, USA (2015), pp. 37-42,
    10.1145/2757384.2757397 View in ScopusGoogle Scholar [39] Zhang J., c Wang F.-Y.,
    Wang K., Lin W.-H., Xu X., Chen C. Data-driven intelligent transportation systems:
    A survey IEEE Trans. Intell. Transp. Syst., 12 (2011), pp. 1624-1639, 10.1109/TITS.2011.2158001
    View in ScopusGoogle Scholar [40] Diamantoulakis P.D., Kapinas V.M., Karagiannidis
    G.K. Big data analytics for dynamic energy management in smart grids Big Data
    Res., 2 (2015), pp. 94-101, 10.1016/j.bdr.2015.03.003 View PDFView articleView
    in ScopusGoogle Scholar [41] Raza M.Q., Khosravi A. A review on artificial intelligence
    based load demand forecasting techniques for smart grid and buildings Renew. Sustain.
    Energy Rev., 50 (2015), pp. 1352-1372, 10.1016/j.rser.2015.04.065 View PDFView
    articleView in ScopusGoogle Scholar [42] Zhou K., Fu C., Yang S. Big data driven
    smart energy management: From big data to big insights Renew. Sustain. Energy
    Rev., 56 (2016), pp. 215-225, 10.1016/j.rser.2015.11.050 View PDFView articleView
    in ScopusGoogle Scholar [43] Yu X., Xue Y. Smart grids: A cyber–physical systems
    perspective Proc. IEEE, 104 (2016), pp. 1058-1070, 10.1109/JPROC.2015.2503119
    View in ScopusGoogle Scholar [44] EU, Framework for the deployment of intelligent
    transport systems in the field of road transport and for interfaces with other
    modes of transport, 2010. http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2010:207:0001:0013:EN:PDF.
    Google Scholar [45] Dimitrakopoulos G., Demestichas P. Intelligent transportation
    systems: Systems based on cognitive networking principles and management functionality
    IEEE Veh. Technol. Mag. (2009), pp. 77-84 Google Scholar [46] S. Ezell, Intelligent
    transportation systems, 2010. https://www.itif.org/files/2010-1-27-ITS_Leadership.pdf.
    Google Scholar [47] MINI plant oxford, assembly, (n.d.). http://miniplantoxford.co.uk/production/assembly.aspx.
    Google Scholar [48] Kuss A., Dietz T., Spenrath F., Verl A. Automated planning
    of robotic MAG welding based on adaptive gap model Procedia CIRP, 62 (2017), pp.
    612-617, 10.1016/j.procir.2016.07.008 View PDFView articleView in ScopusGoogle
    Scholar [49] Mitchell T.M. The Discipline of Machine Learning Carnegie Mellon
    University, School of Computer Science, Machine Learning Department (2006) Google
    Scholar [50] Bishop C.M. Pattern Recognition and Machine Learning Springer, New
    York, New York, USA (2006) Google Scholar [51] Ferreira J., Carvalho E., Ferreira
    B.V., de Souza C., Suhara Y., Pentland A., Pessin G. Driver behavior profiling:
    An investigation with different smartphone sensors and machine learning PLoS One,
    12 (2017), p. e0174959, 10.1371/journal.pone.0174959 Google Scholar [52] Breinman
    L. Random forests Mach. Learn., 45 (2001), pp. 5-32, 10.1186/1478-7954-9-29 Google
    Scholar [53] Z. Wang, T. Oates, Encoding time series as images for visual inspection
    and classification using tiled convolutional neural networks, work, in: Twenty-Ninth
    AAAI Conf. Artif. Intell., 2015, pp. 40–46. http://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10179.
    Google Scholar [54] Y. Zheng, Q. Liu, E. Chen, Y. Ge, J.L. Zhao, Time series classification
    using multi-channels deep convolutional neural networks, in: Web-Age Inf. Manag.
    SE - 33, 8485, 2014, pp. 298–310. http://dx.doi.org/10.1007/978-3-319-08010-9_33.
    Google Scholar [55] Goodfellow I., Bengio Y., Courville A. Deep Learning MIT Press
    (2016) Google Scholar [56] Deng L., Yu D. Deep learning: Methods and applications
    Found. Trends Signal Process., 7 (2014), pp. 197-387, 10.1561/2000000039 Google
    Scholar [57] H.,V.  Drucker, C.J. Burges, L. Kaufman, A.J. Smola, Vapnik, Support
    vector regression machines, in: Adv. Neural Inf. Process. Syst. 1997, pp. 155–161.
    Google Scholar [58] Santafe G. Advances on Supervised and Unsupervised Learning
    of Bayesian Network Models University of the Basque Country (2007) http://www.sc.ehu.es/ccwbayes/members/guzman/pdfs/guzmanTesis.pdf
    Google Scholar [59] Eiben A.E., Smith J.E. Introduction to Evolutionary Computing
    Springer (2003) Google Scholar [60] Ishibuchi H., Nakashima T., Murata T. A fuzzy
    classifier system that generates fuzzy if-then rules for pattern classification
    problems Proc. 1995 IEEE Int. Conf. Evol. Comput., IEEE (1995), pp. 759-764, 10.1109/ICEC.1995.487481
    View in ScopusGoogle Scholar [61] Maulik U., Bandyopadhyay S. Genetic algorithm-based
    clustering technique Pattern Recognit., 33 (2000), pp. 1455-1465, 10.1016/S0031-3203(99)00137-5
    View PDFView articleView in ScopusGoogle Scholar [62] S. Whiteson, Evolutionary
    computation for reinforcement learning, 2012, pp. 325–355. http://dx.doi.org/10.1007/978-3-642-27645-3_10.
    Google Scholar [63] Kohonen T. Essentials of the self-organizing map Neural Netw.,
    37 (2013), pp. 52-65, 10.1016/j.neunet.2012.09.018 View PDFView articleView in
    ScopusGoogle Scholar [64] Watkins C.J.C.H., Dayan P. Q -learning Mach. Learn.,
    8 (1992), pp. 279-292, 10.1007/BF00992698 Google Scholar [65] Sideratos G., Ikonomopoulos
    A., Hatziargyriou N. A committee of machine learning techniques for load forecasting
    in a smart grid environment Int. J. Energy Power., 4 (2015), p. 98, 10.14355/ijep.2015.04.016
    Google Scholar [66] Xiao L., Shao W., Wang C., Zhang K., Lu H. Research and application
    of a hybrid model based on multi-objective optimization for electrical load forecasting
    Appl. Energy, 180 (2016), pp. 213-233, 10.1016/j.apenergy.2016.07.113 View PDFView
    articleView in ScopusGoogle Scholar [67] Yu W., An D., Griffith D., Yang Q., Xu
    G. Towards statistical modeling and machine learning based energy usage forecasting
    in smart grid ACM SIGAPP Appl. Comput. Rev., 15 (2015), pp. 6-16, 10.1145/2753060.2753061
    Google Scholar [68] Guan C., Luh P.B., Michel L.D., Wang Y., Friedland P.B. Very
    short-term load forecasting: Wavelet neural networks with data pre-filtering IEEE
    Trans. Power Syst., 28 (2013), pp. 30-41, 10.1109/TPWRS.2012.2197639 View in ScopusGoogle
    Scholar [69] Kusiak A., Zheng Haiyang, Song Zhe Short-term prediction of wind
    farm power: A data mining approach IEEE Trans. Energy Convers., 24 (2009), pp.
    125-136, 10.1109/TEC.2008.2006552 View in ScopusGoogle Scholar [70] Chan K.Y.,
    Dillon T.S., Singh J., Chang E. Neural-network-based models for short-term traffic
    flow forecasting using a hybrid exponential smoothing and Levenberg–Marquardt
    algorithm IEEE Trans. Intell. Transp. Syst., 13 (2012), pp. 644-654, 10.1109/TITS.2011.2174051
    View in ScopusGoogle Scholar [71] Zito P., Chen Haibo, Bell M.C. Predicting real-time
    roadside CO and CO2 concentrations using neural networks IEEE Trans. Intell. Transp.
    Syst., 9 (2008), pp. 514-522, 10.1109/TITS.2008.928259 View in ScopusGoogle Scholar
    [72] Khosravi A., Mazloumi E., Nahavandi S., Creighton D., Van Lint J.W.C. Prediction
    intervals to account for uncertainties in travel time prediction IEEE Trans. Intell.
    Transp. Syst., 12 (2011), pp. 537-547, 10.1109/TITS.2011.2106209 View in ScopusGoogle
    Scholar [73] van Lint J.W.C. Online learning solutions for freeway travel time
    prediction IEEE Trans. Intell. Transp. Syst., 9 (2008), pp. 38-47, 10.1109/TITS.2008.915649
    View in ScopusGoogle Scholar [74] Yin T., Zhong G., Zhang J., He S., Ran B. A
    prediction model of bus arrival time at stops with multi-routes Transp. Res. Procedia,
    25 (2017), pp. 4623-4636, 10.1016/j.trpro.2017.05.381 View PDFView articleView
    in ScopusGoogle Scholar [75] Castaño F., Beruvides G., Haber R., Artuñedo A. Obstacle
    recognition based on machine learning for on-chip LiDAR sensors in a cyber-physical
    system Sensors, 17 (2017), p. 2109, 10.3390/s17092109 View in ScopusGoogle Scholar
    [76] Kim Z. Robust lane detection and tracking in challenging scenarios IEEE Trans.
    Intell. Transp. Syst., 9 (2008), pp. 16-26, 10.1109/TITS.2007.908582 View in ScopusGoogle
    Scholar [77] Shin S.-J., Woo J., Rachuri S. Predictive analytics model for power
    consumption in manufacturing Procedia CIRP, 15 (2014), pp. 153-158, 10.1016/j.procir.2014.06.036
    View PDFView articleView in ScopusGoogle Scholar [78] Shen C., Wang L., Li Q.
    Optimization of injection molding process parameters using combination of artificial
    neural network and genetic algorithm method J. Mater. Process. Technol., 183 (2007),
    pp. 412-418, 10.1016/j.jmatprotec.2006.10.036 View PDFView articleView in ScopusGoogle
    Scholar [79] Lahouar A., Ben Hadj Slama J. Random forests model for one day ahead
    load forecasting IREC2015 Sixth Int. Renew. Energy Congr., IEEE (2015), pp. 1-6,
    10.1109/IREC.2015.7110975 Google Scholar [80] Zhou Jian, Ge Zhaoqiang, Gao Shang,
    Xu Yanli Fault record detection with random forests in data center of large power
    grid 2016 IEEE PES Asia-Pacific Power Energy Eng. Conf, IEEE (2016), pp. 1641-1645,
    10.1109/APPEEC.2016.7779771 View in ScopusGoogle Scholar [81] Wu D., Jennings
    C., Terpenny J., Kumara S. Cloud-based machine learning for predictive analytics:
    Tool wear prediction in milling 2016 IEEE Int. Conf. Big Data (Big Data), IEEE
    (2016), pp. 2062-2069, 10.1109/BigData.2016.7840831 View in ScopusGoogle Scholar
    [82] Gkorou D., Hoogenboom T., Ypma A., Tsirogiannis G., Giollo M., Sonntag D.,
    Vinken G., van Haren R., van Wijk R.J., Nije J. Towards big data visualization
    for monitoring and diagnostics of high volume semiconductor manufacturing Proc.
    Comput. Front. Conf. ZZZ, CF’17, ACM Press, New York, New York, USA (2017), pp.
    338-342, 10.1145/3075564.3078883 View in ScopusGoogle Scholar [83] Auret L., Aldrich
    C. Unsupervised process fault detection with random forests Ind. Eng. Chem. Res.,
    49 (2010), pp. 9184-9194, 10.1021/ie901975c View in ScopusGoogle Scholar [84]
    Oldewurtel F., Ulbig A., Parisio A., Andersson G., Morari M. Reducing peak electricity
    demand in building climate control using real-time pricing and model predictive
    control 49th IEEE Conf. Decis. Control, IEEE (2010), pp. 1927-1932, 10.1109/CDC.2010.5717458
    View in ScopusGoogle Scholar [85] Chao H. Efficient pricing and investment in
    electricity markets with intermittent resources Energy Policy, 39 (2011), pp.
    3945-3953, 10.1016/j.enpol.2011.01.010 View PDFView articleView in ScopusGoogle
    Scholar [86] Gupta S., Kambli R., Wagh S., Kazi F. Support-vector-machine-based
    proactive cascade prediction in smart grid using probabilistic framework IEEE
    Trans. Ind. Electron., 62 (2015), pp. 2478-2486, 10.1109/TIE.2014.2361493 View
    in ScopusGoogle Scholar [87] Zhang W.Y., Hong W.-C., Dong Y., Tsai G., Sung J.-T.,
    Fan G. Application of SVR with chaotic GASA algorithm in cyclic electric load
    forecasting Energy, 45 (2012), pp. 850-858, 10.1016/j.energy.2012.07.006 View
    PDFView articleView in ScopusGoogle Scholar [88] Nagi J., Mohammad A.M., Yap K.S.,
    Tiong S.K., Ahmed S.K. Non-technical loss analysis for detection of electricity
    theft using support vector machines 2008 IEEE 2nd Int. Power Energy Conf., IEEE
    (2008), pp. 907-912, 10.1109/PECON.2008.4762604 View in ScopusGoogle Scholar [89]
    Nizar A.H., Dong Z.Y., Jalaluddin M., Raffles M.J. Load profiling method in detecting
    non-technical loss activities in a power utility 2006 IEEE Int. Power Energy Conf.,
    IEEE (2006), pp. 82-87, 10.1109/PECON.2006.346624 View in ScopusGoogle Scholar
    [90] Esmalifalak M., Nguyen Nam Tuan, Zheng Rong, Han Zhu Detecting stealthy false
    data injection using machine learning in smart grid 2013 IEEE Glob. Commun. Conf.,
    IEEE (2013), pp. 808-813, 10.1109/GLOCOM.2013.6831172 View in ScopusGoogle Scholar
    [91] Albousefi A.A., Ying H., Filev D., Syed F., Prakah-Asante K.O., Tseng F.,
    Yang H.-H. A two-stage-training support vector machine approach to predicting
    unintentional vehicle lane departure J. Intell. Transp. Syst., 21 (2017), pp.
    41-51, 10.1080/15472450.2016.1196141 View articleView in ScopusGoogle Scholar
    [92] A. Ponz, C.H. Rodríguez-Garavito, F. García, P. Lenz, C. Stiller, J.M. Armingol,
    Laser scanner and camera fusion for automatic obstacle classification in ADAS
    application, 2015, pp. 237–249. http://dx.doi.org/10.1007/978-3-319-27753-0_13.
    Google Scholar [93] Liu T., Yang Y., Huang G.-B., Yeo Y.K., Lin Z. Driver distraction
    detection using semi-supervised machine learning IEEE Trans. Intell. Transp. Syst.,
    17 (2016), pp. 1108-1120, 10.1109/TITS.2015.2496157 View in ScopusGoogle Scholar
    [94] Ribeiro R.P., Pereira P., Gama J. Sequential anomalies: a study in the Railway
    industry Mach. Learn., 105 (2016), pp. 127-153, 10.1007/s10994-016-5584-6 View
    in ScopusGoogle Scholar [95] Moridpour S., Anwar T., Sadat M.T., Mazloumi E. A
    genetic algorithm-based support vector machine for bus travel time prediction
    2015 Int. Conf. Transp. Inf. Saf., IEEE (2015), pp. 264-270, 10.1109/ICTIS.2015.7232119
    View in ScopusGoogle Scholar [96] Susto G.A., Schirru A., Pampuri S., McLoone
    S., Beghi A. Machine learning for predictive maintenance: A multiple classifier
    approach IEEE Trans. Ind. Inform., 11 (2015), pp. 812-820, 10.1109/TII.2014.2349359
    View in ScopusGoogle Scholar [97] Yao X., Moon S.K., Bi G. A hybrid machine learning
    approach for additive manufacturing design feature recommendation Rapid Prototyp.
    J., 23 (2017), pp. 983-997, 10.1108/RPJ-03-2016-0041 View in ScopusGoogle Scholar
    [98] Madureira A., Santos J.M., Gomes S., Cunha B., Pereira J.P., Pereira I. Manufacturing
    rush orders rescheduling: a supervised learning approach 2014 Sixth World Congr.
    Nat. Biol. Inspired Comput., NaBIC 2014, IEEE (2014), pp. 299-304, 10.1109/NaBIC.2014.6921895
    View in ScopusGoogle Scholar [99] Wuest T., Irgens C., Thoben K.-D. An approach
    to monitoring quality in manufacturing using supervised machine learning on product
    state data J. Intell. Manuf., 25 (2014), pp. 1167-1180, 10.1007/s10845-013-0761-y
    View in ScopusGoogle Scholar [100] Ma H., Wang Y., Wang K. Automatic detection
    of false positive RFID readings using machine learning algorithms Expert Syst.
    Appl., 91 (2018), pp. 442-451, 10.1016/j.eswa.2017.09.021 View PDFView articleView
    in ScopusGoogle Scholar [101] Cook D.J., Crandall A.S., Thomas B.L., Krishnan
    N.C. CASAS: A smart home in a box Computer, 46 (2013), pp. 62-69, 10.1109/MC.2012.328
    View in ScopusGoogle Scholar [102] Krishnan N.C., Cook D.J. Activity recognition
    on streaming sensor data Pervasive Mob. Comput., 10 (2014), pp. 138-154, 10.1016/j.pmcj.2012.07.003
    View PDFView articleView in ScopusGoogle Scholar [103] Monedero I., Biscarri F.,
    León C., Guerrero J.I., Biscarri J., Millán R. Detection of frauds and other non-technical
    losses in a power utility using Pearson coefficient, Bayesian networks and decision
    trees Int. J. Electr. Power Energy Syst., 34 (2012), pp. 90-98, 10.1016/j.ijepes.2011.09.009
    View PDFView articleView in ScopusGoogle Scholar [104] Simmhan Y., Aman S., Kumbhare
    A., Liu R., Stevens S., Zhou Q., Prasanna V. Cloud-based software platform for
    big data analytics in smart grids Comput. Sci. Eng., 15 (2013), pp. 38-47, 10.1109/MCSE.2013.39
    View in ScopusGoogle Scholar [105] Osaba E., Onieva E., Moreno A., Lopez-Garcia
    P., Perallos A., Bringas P.G. Decentralised intelligent transport system with
    distributed intelligence based on classification techniques IET Intell. Transp.
    Syst., 10 (2016), pp. 674-682, 10.1049/iet-its.2016.0047 View in ScopusGoogle
    Scholar [106] Garcia F.C.C., Retamar A.E. Towards building a bus travel time prediction
    model for Metro Manila 2016 IEEE Reg. 10 Conf., IEEE (2016), pp. 3805-3808, 10.1109/TENCON.2016.7848775
    View in ScopusGoogle Scholar [107] Vuong T.P. Cyber-physical Intrusion Detection
    for Robotic Vehicles University of Greenwich (2017) http://gala.gre.ac.uk/17445/7/TuanVuong2017.pdf
    Google Scholar [108] Lieber D., Stolpe M., Konrad B., Deuse J., Morik K. Quality
    prediction in interlinked manufacturing processes based on supervised & unsupervised
    machine learning Procedia CIRP, 7 (2013), pp. 193-198, 10.1016/j.procir.2013.05.033
    View PDFView articleView in ScopusGoogle Scholar [109] Sugumaran V., Muralidharan
    V., Ramachandran K.I. Feature selection using decision tree and classification
    through proximal support vector machine for fault diagnostics of roller bearing
    Mech. Syst. Signal Process., 21 (2007), pp. 930-942, 10.1016/j.ymssp.2006.05.004
    View PDFView articleView in ScopusGoogle Scholar [110] Liu Y., Jin S. Application
    of Bayesian networks for diagnostics in the assembly process by considering small
    measurement data sets Int. J. Adv. Manuf. Technol., 65 (2013), pp. 1229-1237,
    10.1007/s00170-012-4252-7 View in ScopusGoogle Scholar [111] Hao L., Bian L.,
    Gebraeel N., Shi J. Residual life prediction of multistage manufacturing processes
    with interaction between tool wear and product quality degradation IEEE Trans.
    Autom. Sci. Eng., 14 (2017), pp. 1211-1224, 10.1109/TASE.2015.2513208 View in
    ScopusGoogle Scholar [112] Nannapaneni S., Mahadevan S., Rachuri S. Performance
    evaluation of a manufacturing process under uncertainty using Bayesian networks
    J. Clean. Prod., 113 (2016), pp. 947-959, 10.1016/j.jclepro.2015.12.003 View PDFView
    articleView in ScopusGoogle Scholar [113] Coelho V.N., Coelho I.M., Coelho B.N.,
    Reis A.J.R., Enayatifar R., Souza M.J.F., Guimarães F.G. A self-adaptive evolutionary
    fuzzy model for load forecasting problems on smart grid environment Appl. Energy,
    169 (2016), pp. 567-584, 10.1016/j.apenergy.2016.02.045 View PDFView articleView
    in ScopusGoogle Scholar [114] Chakraborty S., Senjyu T., Yona A., Saber A.Y.,
    Funabashi T. Solving economic load dispatch problem with valve-point effects using
    a hybrid quantum mechanics inspired particle swarm optimisation IET Gener. Transm.
    Distrib., 5 (2011), p. 1042, 10.1049/iet-gtd.2011.0038 View in ScopusGoogle Scholar
    [115] Ramos C.C.O., Souza A.N., Chiachia G., Falcão A.X., Papa J.P. A novel algorithm
    for feature selection using harmony search and its application for non-technical
    losses detection Comput. Electr. Eng., 37 (2011), pp. 886-894, 10.1016/j.compeleceng.2011.09.013
    View PDFView articleView in ScopusGoogle Scholar [116] Ray P., Mishra D.P. Support
    vector machine based fault classification and location of a long transmission
    line Eng. Sci. Technol., 19 (2016), pp. 1368-1380, 10.1016/j.jestch.2016.04.001
    View PDFView articleView in ScopusGoogle Scholar [117] Yuwono M., Qin Y., Zhou
    J., Guo Y., Celler B.G., Su S.W. Automatic bearing fault diagnosis using particle
    swarm clustering and hidden Markov model Eng. Appl. Artif. Intell., 47 (2016),
    pp. 88-100, 10.1016/j.engappai.2015.03.007 View PDFView articleView in ScopusGoogle
    Scholar [118] Navalertporn T., Afzulpurkar N.V. Optimization of tile manufacturing
    process using particle swarm optimization Swarm Evol. Comput., 1 (2011), pp. 97-109,
    10.1016/j.swevo.2011.05.003 View PDFView articleView in ScopusGoogle Scholar [119]
    Li D., Jayaweera S.K. Machine-learning aided optimal customer decisions for an
    interactive smart grid IEEE Syst. J., 9 (2015), pp. 1529-1540, 10.1109/JSYST.2014.2334637
    View in ScopusGoogle Scholar [120] Tai A.H., Ching W.-K., Chan L.Y. Detection
    of machine failure: Hidden Markov model approach Comput. Ind. Eng., 57 (2009),
    pp. 608-619, 10.1016/j.cie.2008.09.028 View PDFView articleView in ScopusGoogle
    Scholar [121] Zheng J., Ni L.M. Modeling heterogeneous routing decisions in trajectories
    for driving experience learning Proc. 2014 ACM Int. Jt. Conf. Pervasive Ubiquitous
    Comput., UbiComp ’14 Adjun., ACM Press, New York, New York, USA (2014), pp. 951-961,
    10.1145/2632048.2632089 View in ScopusGoogle Scholar [122] Mocanu E., Nguyen P.H.,
    Gibescu M., Kling W.L. Deep learning for estimating building energy consumption
    Sustain. Energy Grids Netw., 6 (2016), pp. 91-99, 10.1016/j.segan.2016.02.005
    View PDFView articleView in ScopusGoogle Scholar [123] Lv Y., Duan Y., Kang W.,
    Li Z., Wang F.-Y. Traffic flow prediction with big data: A deep learning approach
    IEEE Trans. Intell. Transp. Syst. (2014), pp. 1-9, 10.1109/TITS.2014.2345663 View
    PDFView articleView in ScopusGoogle Scholar [124] M. Bojarski, D. Del Testa, D.
    Dworakowski, B. Firner, B. Flepp, P. Goyal, L.D. Jackel, M. Monfort, U. Muller,
    J. Zhang, X. Zhang, J. Zhao, K. Zieba, End to end learning for self-driving cars,
    2016. http://arxiv.org/abs/1604.07316. Google Scholar [125] Chen Z., Huang X.
    End-to-end learning for lane keeping of self-driving cars 2017 IEEE Intell. Veh.
    Symp., IEEE (2017), pp. 1856-1860, 10.1109/IVS.2017.7995975 View in ScopusGoogle
    Scholar [126] A. Taylor, Anomaly-based detection of malicious activity in in-vehicle
    networks, University of Ottawa, 2017. https://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwi1-cnir6XYAhVNY1AKHYY-DPEQFgg2MAE&url=https%3A%2F%2Fruor.uottawa.ca%2Fbitstream%2F10393%2F36120%2F3%2FTaylor_Adrian_2017_thesis.pdf&usg=AOvVaw1mk_GeMwTMT0Yn6kUxiMK.
    Google Scholar [127] P. Nicolas, Time complexity: Graph and machine learning algorithms,
    2015. http://www.scalaformachinelearning.com/2015/11/time-complexity-in-machine-learning.html.
    Google Scholar [128] J. Su, H. Zhang, A fast decision tree learning algorithm,
    in: 21st Natl. Conf. Artif. Intell., Vol. 1. 5, 2006, pp. 500–505. Google Scholar
    [129] K. He, J. Sun, Convolutional neural networks at constrained time cost, in:
    IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 5353–5360. http://dx.doi.org/10.1109/CVPR.2015.7299173.
    Google Scholar [130] L. Bottou, C. Lin, Support vector machine solvers, 2006.
    Google Scholar [131] G. Louppen, Understanding random forest from theory to practice,
    University of Liège, 2014. https://arxiv.org/pdf/1407.7502.pdf. Google Scholar
    [132] Roussinov D.G., Chen H. A scalable self-organizing map algorithm for textual
    classification: A neural network approach to thesaurus generation Commun. Cogn.
    Artif. Intell. Spring., 15 (1998), pp. 81-112 Google Scholar [133] S. Koenig,
    R.G. Simmons, Complexity analysis of real-time reinforcement learning, in: Proc.
    AAAI Conf. Artif. Intell., 1993, pp. 99–105. Google Scholar [134] Simon D. Biogeography-based
    optimization IEEE Trans. Evol. Comput., 12 (2008), pp. 702-713, 10.1109/TEVC.2008.919004
    View in ScopusGoogle Scholar [135] J.H. Korhonen, P. Parviainen, Exact learning
    of bounded tree-width Bayesian networks, in: Proc. 16th Int. Conf. AI Stat., 2013,
    pp. 370–378. Google Scholar [136] Bhattacharya A., Chattopadhyay P.K. Biogeography-based
    optimization for different economic load dispatch problems IEEE Trans. Power Syst.,
    25 (2010), pp. 1064-1077, 10.1109/TPWRS.2009.2034525 View in ScopusGoogle Scholar
    [137] Losing V., Hammer B., Wersing H. Incremental on-line learning: A review
    and comparison of state of the art algorithms Neurocomputing, 275 (2018), pp.
    1261-1274, 10.1016/j.neucom.2017.06.084 View PDFView articleView in ScopusGoogle
    Scholar [138] Gama J., Žliobaite I., Bifet A., Pechenizkiy M., Bouchachia A. A
    survey on concept drift adaptation ACM Comput. Surv., 46 (2014), pp. 1-37, 10.1145/2523813
    Google Scholar [139] A. R.R., D. P.R. Methods for incremental learning: A survey
    Int. J. Data Min. Knowl. Manag. Process., 3 (2013), pp. 119-125, 10.5121/ijdkp.2013.3408
    Google Scholar [140] Hadas D., Yovel G., Intrator N. Using unsupervised incremental
    learning to cope with gradual concept drift Conn. Sci., 23 (2011), pp. 65-83,
    10.1080/09540091.2011.575929 View in ScopusGoogle Scholar [141] Liang Nan-Ying,
    Huang Guang-Bin, Saratchandran P., Sundararajan N. A fast and accurate online
    sequential learning algorithm for feedforward networks IEEE Trans. Neural Netw.,
    17 (2006), pp. 1411-1423, 10.1109/TNN.2006.880583 View in ScopusGoogle Scholar
    [142] Suresh S., Dong K., Kim H.J. A sequential learning algorithm for self-adaptive
    resource allocation network classifier Neurocomputing, 73 (2010), pp. 3012-3019,
    10.1016/j.neucom.2010.07.003 View PDFView articleView in ScopusGoogle Scholar
    [143] Huynh H.T., Won Y. Regularized online sequential learning algorithm for
    single-hidden layer feedforward neural networks Pattern Recognit. Lett., 32 (2011),
    pp. 1930-1935, 10.1016/j.patrec.2011.07.016 View PDFView articleView in ScopusGoogle
    Scholar [144] Guo L., Hao J., Liu M. An incremental extreme learning machine for
    online sequential learning problems Neurocomputing, 128 (2014), pp. 50-58, 10.1016/j.neucom.2013.03.055
    View PDFView articleGoogle Scholar [145] Quinlan J.R. Induction of decision trees
    Mach. Learn., 1 (1986), pp. 81-106, 10.1007/BF00116251 View in ScopusGoogle Scholar
    [146] Utgoff P.E. Incremental induction of decision trees Mach. Learn., 4 (1989),
    pp. 161-186, 10.1023/A:1022699900025 View in ScopusGoogle Scholar [147] Kalles
    D., Morris T. Efficient incremental induction of decision trees Mach. Learn.,
    24 (1996), pp. 231-242, 10.1007/BF00058613 Google Scholar [148] Domingos P., Hulten
    G. Mining high-speed data streams Proc. Sixth ACM SIGKDD Int. Conf. Knowl. Discov.
    Data Min., KDD ’00, ACM Press, New York, New York, USA (2000), pp. 71-80, 10.1145/347090.347107
    View in ScopusGoogle Scholar [149] Gama J., Rocha R., Medas P. Accurate decision
    trees for mining high-speed data streams Proc. Ninth ACM SIGKDD Int. Conf. Knowl.
    Discov. Data Min., KDD ’03, ACM Press, New York, New York, USA (2003), 10.1145/956750.956813
    Google Scholar [150] Pfahringer B., Holmes G., Kirkby R. New options for hoeffding
    trees Proc. 20th Aust. Jt. Conf. Adv. Artif. Intell., Springer-Verlag, Berlin,
    Heidelberg (2007), pp. 90-99 http://dl.acm.org/citation.cfm?id=1781238.1781251
    CrossRefView in ScopusGoogle Scholar [151] Rutkowski L., Pietruczuk L., Duda P.,
    Jaworski M. Decision trees for mining data streams based on the McDiarmid’s bound
    IEEE Trans. Knowl. Data Eng., 25 (2013), pp. 1272-1279, 10.1109/TKDE.2012.66 View
    in ScopusGoogle Scholar [152] Rutkowski L., Jaworski M., Pietruczuk L., Duda P.
    The CART decision tree for mining data streams Inf. Sci. (N,Y)., 266 (2014), pp.
    1-15, 10.1016/j.ins.2013.12.060 View PDFView articleView in ScopusGoogle Scholar
    [153] Abdulsalam H., Skillicorn D.B., Martin P. Streaming random forests 11th
    Int. Database Eng. Appl. Symp., IDEAS 2007, IEEE (2007), pp. 225-232, 10.1109/IDEAS.2007.4318108
    View in ScopusGoogle Scholar [154] Abdulsalam H., Skillicorn D.B., Martin P. Classification
    using streaming random forests IEEE Trans. Knowl. Data Eng., 23 (2011), pp. 22-36,
    10.1109/TKDE.2010.36 View in ScopusGoogle Scholar [155] Saffari A., Leistner C.,
    Santner J., Godec M., Bischof H. On-line random forests 2009 IEEE 12th Int. Conf.
    Comput. Vis. Work, ICCV Work, IEEE (2009), pp. 1393-1400, 10.1109/ICCVW.2009.5457447
    View in ScopusGoogle Scholar [156] M. Denil, D. Matheson, N. De Freitas, Consistency
    of online random forests, in: Int. Conf. Mach. Learn., 2013, pp. 1256–1264. Google
    Scholar [157] Kivinen J., Smola A.J., Williamson R.C. Online learning with kernels
    IEEE Trans. Signal Process., 52 (2004), pp. 2165-2176, 10.1109/TSP.2004.830991
    View in ScopusGoogle Scholar [158] Laskov P., Gehl C., Kruger S., Muller K.-R.
    Incremental support vector learning: Analysis, implementation and applications
    J. Mach. Learn. Res., 7 (2006), pp. 1909-1936 View in ScopusGoogle Scholar [159]
    Karasuyama M., Takeuchi I. Multiple incremental decremental learning of support
    vector machines IEEE Trans. Neural Netw., 21 (2010), pp. 1048-1059, 10.1109/TNN.2010.2048039
    View in ScopusGoogle Scholar [160] Gu B., Sheng V.S., Tay K.Y., Romano W., Li
    S. Incremental support vector learning for ordinal regression IEEE Trans. Neural
    Netw. Learn. Syst., 26 (2015), pp. 1403-1416, 10.1109/TNNLS.2014.2342533 View
    in ScopusGoogle Scholar [161] Tang B., Chen Z., Hefferman G., Pei S., Wei T.,
    He H., Yang Q. Incorporating intelligence in fog computing for big data analysis
    in smart cities IEEE Trans. Ind. Inform., 13 (2017), pp. 2140-2150, 10.1109/TII.2017.2679740
    View in ScopusGoogle Scholar [162] Garcia Lopez P., Montresor A., Epema D., Datta
    A., Higashino T., Iamnitchi A., Barcellos M., Felber P., Riviere E. Edge-centric
    computing: Vision and challenges ACM SIGCOMM Comput. Commun. Rev., 45 (2015),
    pp. 37-42, 10.1145/2831347.2831354 Google Scholar [163] Gu L., Zeng D., Guo S.,
    Barnawi A., Xiang Y. Cost efficient resource management in fog computing supported
    medical cyber-physical system IEEE Trans. Emerg. Top. Comput., 5 (2017), pp. 108-119,
    10.1109/TETC.2015.2508382 View in ScopusGoogle Scholar [164] Xu B., Xu L., Cai
    H., Jiang L., Luo Y., Gu Y. The design of an m -health monitoring system based
    on a cloud computing platform Enterp. Inf. Syst., 11 (2017), pp. 17-36, 10.1080/17517575.2015.1053416
    View in ScopusGoogle Scholar [165] Tu Q., Lu J.F., Yuan B., Tang J.B., Yang J.Y.
    Density-based hierarchical clustering for streaming data Pattern Recognit. Lett.,
    33 (2012), pp. 641-645, 10.1016/j.patrec.2011.11.022 View PDFView articleView
    in ScopusGoogle Scholar [166] Goodwin M.S., Haghighi M., Tang Q., Akcakaya M.,
    Erdogmus D., Intille S. Moving towards a real-time system for automatically recognizing
    stereotypical motor movements in individuals on the autism spectrum using wireless
    accelerometry Proc. 2014 ACM Int. Jt. Conf. Pervasive Ubiquitous Comput., UbiComp
    ’14 Adjun., ACM Press, New York, New York, USA (2014), pp. 861-872, 10.1145/2632048.2632096
    View in ScopusGoogle Scholar [167] Verba N., Chao K.-M., James A., Goldsmith D.,
    Fei X., Stan S.-D. Platform as a service gateway for the fog of things Adv. Eng.
    Inform., 33 (2017), pp. 243-257, 10.1016/j.aei.2016.11.003 View PDFView articleView
    in ScopusGoogle Scholar [168] I. Stoica, D. Song, R.A. Popa, D. Patterson, M.W.
    Mahoney, R. Katz, A.D. Joseph, M. Jordan, J.M. Hellerstein, J. Gonzalez, K. Goldberg,
    A. Ghodsi, D. Culler, P. Abbeel, A Berkeley view of systems challenges for AI,
    2017. http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-159.html. Google
    Scholar [169] Baryannis G., Kritikos K., Plexousakis D. A specification-based
    QoS-aware design framework for service-based applications Serv. Oriented Comput.
    Appl., 11 (2017), pp. 301-314, 10.1007/s11761-017-0210-4 View in ScopusGoogle
    Scholar [170] Wang W., Lee K., Murray D. A global generic architecture for the
    future internet of things Serv. Oriented Comput. Appl., 11 (2017), pp. 329-344,
    10.1007/s11761-017-0213-1 View in ScopusGoogle Scholar Cited by (0) Xiang Fei
    received a BSc and a PhD from Southeast University China in 1992 and 1999 respectively.
    After graduation, Xiang Fei worked on a number of projects including European
    IST Programs and EPSRC. H joined Cogent at Coventry University as a Research Fellow
    in October 2008 and he is currently working as a senior lecturer for the School
    of Computing, Electronics and Maths at Coventry University. Nazaraf Shah is a
    Senior Lecturer at Coventry, UK. He received the PhD in Multi-Agent Systems from
    Coventry University, Coventry, UK, in 2006. His research interests include intelligent
    agents, service oriented computing and cloud computing. He has more than 50 publications
    in various international conference proceedings and journals. Nandor Verba received
    the B. E. degree in Systems Theory with specialization in Automation from the
    Technical University of Cluj-Napoca, Romania in 2015. He is currently pursuing
    a Ph.D. in Computing at Coventry University, UK. His research interests include
    Wireless Sensor Networks, Internet of Things, Cloud and Fog computing. Kuo-Ming
    Chao is a Professor of Computing at Coventry University, UK. His research interests
    include the areas of Service-Oriented Computing, IoT, Cloud Computing, CPS and
    Big Data etc., as well as their applications. He has over 180 refereed publications.
    He is a co-editor-in-chief of Service-Oriented Computing and Applications –a Springer
    Journal. Victor Sanchez-Anguix received a BSc. in Computer Science at Universitat
    Politècnica de València. Later, he obtained a MSc. in Artificial Intelligence
    and Pattern Recognition by Universitat Politècnica de València. In February 2013
    Victor Sanchez-Anguix defended his PhD. thesis in Computer Science at Universitat
    Politècnica de València. After finishing my PhD. he started an industry position
    as a data scientist at Atrapalo.com. Currently, Victor Sanchez-Anguix is working
    as a lecturer at Coventry University. Jacek Lewandowski is a Lecturer in Information
    Systems at Coventry University, UK. He received the Ph.D. degree in computer science
    from Coventry University, Coventry, U.K., in 2014. His research focuses on WSN,
    IoT, cloud and fog computing, artificial intelligence as well as information and
    decision systems development for industrial and medical applications. Anne James
    is Professor at the College of Science and Technology, School of Science & Technology,
    Nottingham Trent University. Before that she was Professor of Data Systems Architecture
    in the Distributed Systems and Modelling Applied Research Group at Coventry University.
    Anne has been active in teaching in higher education and in participating in research
    projects as part of national and international teams. Her main duties currently
    involve leading a Computing and Technology department, teaching and supervising
    research students. The research interests of Professor James are in the general
    area of creating distributed systems to meet new and unusual data and information
    challenges. She currently has projects in spatial data infrastructure, bioinformatics,
    Cloud forensics, and intrusion detection. Professor James has successfully supervised
    around 30 research degrees and has published around 200 papers in peer reviewed
    journals or conferences. Zahid Usman is a chartered engineer with extensive experience
    in both industrial and research environments. His research experience expands
    to over six years within the areas of manufacturing information, machine vision,
    robotics and automation. He is an active researcher in robotic metrology and assembly,
    machine vision and manufacturing informatics. He has been involved in several
    research projects with leading aerospace and automotive industries. He also has
    experience of working for leading manufacturing organizations such as Rolls Royce
    and Massey Fergusons Tractors. As a teacher, Dr Usman is focused on training engineering
    students to be industrially ready by conducting industry based teaching. Zahid
    Usman is currently working for Rolls-Royce plc in the United Kingdom. View Abstract
    © 2018 Elsevier B.V. All rights reserved. Part of special issue Special Issue
    on Edge of the Cloud Edited by Anne James, Giancarlo Fortino, Joe-Air Jiang, Kuo-Ming
    Chao View special issue Recommended articles Association between leukoaraiosis
    and cerebral blood flow territory alteration in asymptomatic internal carotid
    artery stenosis Clinical Radiology, Volume 73, Issue 5, 2018, pp. 502.e9-502.e14
    Y.-F. Chen, …, S.-F. Jiang View PDF A novel framework for wind speed prediction
    based on recurrent neural networks and support vector machine Energy Conversion
    and Management, Volume 178, 2018, pp. 137-145 Chuanjin Yu, …, Guanghao Zhai View
    PDF Assessment of lean manufacturing effect on business performance using Bayesian
    Belief Networks Expert Systems with Applications, Volume 42, Issue 19, 2015, pp.
    6539-6551 Gülçin Büyüközkan, …, İbrahim S. Karakadılar View PDF Show 3 more articles
    Article Metrics Citations Citation Indexes: 92 Captures Readers: 316 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie Settings
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Future generation computer systems
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'CPS data streams analytics based on machine learning for Cloud and Fog Computing:
    A survey'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s10489-022-03344-3
  analysis: '>'
  authors:
  - Marta Fernandes
  - Juan M. Corchado
  - Goreti Marreiros
  citation_count: 52
  full_citation: '>'
  full_text: ">\nhttps://doi.org/10.1007/s10489-022-03344-3\nMachine learning techniques\
    \ applied to mechanical fault diagnosis\nand fault prognosis in the context of\
    \ real industrial manufacturing\nuse-cases: a systematic literature review\nMarta\
    \ Fernandes1\n· Juan Manuel Corchado2 · Goreti Marreiros1\nAccepted: 5 February\
    \ 2022\n© The Author(s), under exclusive licence to Springer Science+Business\
    \ Media, LLC, part of Springer Nature 2022\nAbstract\nWhen put into practice in\
    \ the real world, predictive maintenance presents a set of challenges for fault\
    \ detection and prognosis\nthat are often overlooked in studies validated with\
    \ data from controlled experiments, or numeric simulations. For this reason,\n\
    this study aims to review the recent advancements in mechanical fault diagnosis\
    \ and fault prognosis in the manufacturing\nindustry using machine learning methods.\
    \ For this systematic review, we searched Web of Science, ACM Digital Library,\n\
    Science Direct, Wiley Online Library, and IEEE Xplore between January 2015 and\
    \ October 2021. Full-length studies\nthat employed machine learning algorithms\
    \ to perform mechanical fault detection or fault prognosis in manufacturing\n\
    equipment and presented empirical results obtained from industrial case-studies\
    \ were included, except for studies not written\nin English or published in sources\
    \ other than peer-reviewed journals with JCR Impact Factor, conference proceedings\
    \ and\nbook chapters/sections. Of 4549 records, 44 primary studies were selected.\
    \ In 37 of those studies, fault diagnosis and\nprognosis were performed using\
    \ artificial neural networks (n=12), decision tree methods (n=11), hybrid models\
    \ (n=8),\nor latent variable models (n=6), with one of the studies employing two\
    \ different types of techniques independently. The\nremaining studies employed\
    \ a variety of machine learning techniques, ranging from rule-based models to\
    \ partition-based\nalgorithms, and only two studies approached the problem using\
    \ online learning methods. The main advantages of these\nalgorithms include high\
    \ performance, the ability to uncover complex nonlinear relationships and computational\
    \ efficiency,\nwhile the most important limitation is the reduction in model performance\
    \ in the presence of concept drift. This review\nshows that, although the number\
    \ of studies performed in the manufacturing industry has been increasing in recent\
    \ years,\nadditional research is necessary to address the challenges presented\
    \ by real-world scenarios.\nKeywords Machine learning · Fault detection · Fault\
    \ prognosis · Predictive maintenance · Manufacturing industry ·\nIndustrial case-study\n\
    \x02 Marta Fernandes\nmmdaf@isep.ipp.pt\nJuan Manuel Corchado\ncorchado@usal.es\n\
    Goreti Marreiros\nmgt@isep.ipp.pt\n1\nGECAD - Research Group on Intelligent Engineering\
    \ and\nComputing for Advanced Innovation and Development,\nPolytechnic of Porto\
    \ (ISEP/IPP), Porto, Portugal\n2\nBISITE Research Centre, University of Salamanca\
    \ (USAL),\nSalamanca, Spain\n1 Introduction\nMachine maintenance, with its impact\
    \ on machine down-\ntime and production costs, is directly related to a manufac-\n\
    turing companies’ ability to be competitive in terms of cost,\nquality, and performance\
    \ [1, 2]. The purpose of maintenance\ngoes beyond repairing an equipment after\
    \ it malfunctions.\nIts main objective is to maintain the functionality of machin-\n\
    ery and minimize breakdowns.\nAs the name suggests, predictive maintenance consists\
    \ in\nthe early detection of problems. Under a predictive main-\ntenance program,\
    \ maintenance is performed by monitoring\nthe actual condition of machinery and\
    \ repairing or replacing\n/ Published online: 4 March 2022\nApplied Intelligence\
    \ (2022) 52:14246–14280\n1 3\ncomponents after a certain level of deterioration\
    \ has been\ndetected, instead of performing repairs after a fault has\noccurred\
    \ [3]. This approach has several advantages over\nreactive and preventive maintenance\
    \ strategies [4, 5],\nnamely:\n–\nPrevention of catastrophic failures.\n–\nExtension\
    \ of an equipment’s useful life.\n–\nOptimization of preventive maintenance tasks.\n\
    –\nImproved management of the maintenance inventory.\n–\nOptimization of equipment\
    \ availability.\n–\nImproved productivity.\nBy preventing serious failures, reducing\
    \ unexpected\nfaults, and maximizing the mean time between failures\n(MTBF), predictive\
    \ maintenance helps reduce workplace\naccidents and their severity, reduces the\
    \ number of repairs\nand the mean time to repair (MTTR) and extends the useful\n\
    life of equipment, all of which results in increased earnings,\nless maintenance\
    \ and production costs and more sustainable\nmanufacturing [4, 6]. According to\
    \ Sullivan et al. [5],\nthe successful implementation of a predictive maintenance\n\
    program can lead to an average reduction of maintenance\ncosts between 25% and\
    \ 30% and a return on investment\n(ROI) of 1000%.\nPredictive maintenance is a\
    \ form of condition-based\nmaintenance [4], which relies on the prediction and\n\
    detection of incipient faults in the equipment based on\nparameter measurements\
    \ that reflect a machine’s real\ncondition [7–9]. In condition-based maintenance,\
    \ decision-\nmaking is supported by diagnostics and prognostics\ntechniques [7].\n\
    Diagnostics, which involves performing fault detection\nand identification (FDI),\
    \ is generally performed using\nhardware redundancy methods or analytical redundancy\n\
    methods. Hardware redundancy consists in measuring the\nsame parameters using\
    \ more than one sensor and then\ncomparing the duplicate signals by means of various\n\
    techniques, such as signal processing methods [10].\nAnalytical redundancy methods\
    \ are based on mathematical\nmodels of the system and can be divided in quantitative,\n\
    or model-based, methods and qualitative, or data-driven,\nmethods [10, 11]. Both\
    \ methods compare predicted or\nestimated parameters to real, measured values,\
    \ but while\nmodel-based methods estimate the parameters of interest\nbased on\
    \ a mathematical model of the system under normal\noperating conditions, data-driven\
    \ methods employ historical\ndata and artificial intelligence algorithms to predict\
    \ such\nparameters or detect anomalous values.\nWhile diagnostics deals with the\
    \ detection, isolation and\nidentification of faults, prognostics aims to predict\
    \ faults in\nthe monitored system before they occur [7]. Specifically,\nprognostics\
    \ techniques are used to estimate how soon -\ni.e., estimation of the remaining\
    \ useful life (RUL) - and\nhow likely a fault is to occur, but most of the literature\n\
    on machine prognostics focuses on the former type of\nprediction [7]. RUL estimation\
    \ methods, which can also be\ndata-driven, aim to predict how long a machine will\
    \ function\nbefore a fault occurs or if the machine is going to fail in a\ngiven\
    \ time interval [7].\nSince they don’t require additional hardware, analytical\n\
    redundancy methodologies are less expensive to implement\nthan hardware redundancy\
    \ methods [10, 11]. Given the\nemergence of Internet of Things (IoT) technologies\
    \ in\nindustrial settings it is now possible to obtain a real-\ntime digital representation\
    \ of the production processes and\ncurrent status of the equipment [12], which\
    \ has led to\nan exponential growth of the volume of industrial data\n[13]. Data-driven\
    \ methods, in particular machine learning\nand data mining techniques, are well\
    \ suited to extract\nknowledge from this wealth of data and have successfully\n\
    been used in the context of predictive maintenance [9,\n14]. Moreover, although\
    \ model-based methods can produce\ngood results if the model of the system is\
    \ precise, building\nan accurate mathematical model of a system is an arduous\n\
    task that makes model-based methods a less viable option\nfor complex systems\
    \ [7, 10]. Recent review papers [9,\n15] focusing on the use of machine learning\
    \ techniques\nfor predictive maintenance have identified that commonly\nused data-driven\
    \ methods include artificial neural networks\n[16–20], support vector machines\
    \ [21–23], decision trees\n(including ensemble methods) [24, 25] , k-means [26,\
    \ 27]\nand logistic regression [28, 29], among others.\nPredicting and detecting\
    \ faults in industrial equipment\nare difficult tasks that require the choice\
    \ of adequate\ntechniques to obtain accurate results. The present study\nperforms\
    \ a systematic literature review of the machine\nlearning methods used for the\
    \ detection of mechanical faults\nand the prognosis of faults in manufacturing\
    \ equipment in\nreal-world scenarios. It is meant to serve as a foundation for\n\
    the implementation of predictive maintenance systems and\nhelp identify future\
    \ research opportunities. The literature on\nmechanical fault detection and fault\
    \ prognosis is vast, but to\nthe best of the authors’ knowledge no systematic\
    \ literature\nreview on this specific topic of study exists.\nThe review focuses\
    \ on the detection of mechanical\nfaults because these types of faults are a leading\
    \ cause\nof breakdowns in manufacturing equipment [30, 31]. As\nmentioned above,\
    \ fault prognosis aims to predict the time\nleft before a machine breaks down\
    \ and/or the probability\nof failure, without seeking to identify the type of\
    \ fault\n(diagnostics techniques can be used for this purpose) [7].\nTherefore,\
    \ primary studies focusing on both mechanical\nfault detection and fault prognosis\
    \ were considered in this\nreview.\nAnother important aspect of this review is\
    \ that only\nreal-world industrial cases are considered. When put into\n1 3\n\
    Machine learning techniques applied to mechanical fault diagnosis...\n14247\n\
    practice in the real world, predictive maintenance presents\na set of challenges\
    \ for fault diagnosis and prognosis\nthat are often overlooked in studies validated\
    \ with data\nobtained from controlled experiments, testbeds, or numeric\nsimulations.\
    \ Manufacturing systems are characterized by\ncomplex, non-stationary processes\
    \ where noise and other\ndisturbances are a reality [8, 32, 33]. This conditions\
    \ the\nchoice and applicability of machine learning methods, as\ndo other aspects\
    \ of practical order such as the absence\nof historical fault data that occurs\
    \ frequently in industrial\nsettings and restricts the learning task to unsupervised\
    \ and\nsemi-supervised methods. For these reasons, this study\naims to present\
    \ an overview of the current landscape of\nfault diagnosis and prognosis in real-world\
    \ scenarios using\nmachine learning techniques.\nThe study here presented was\
    \ guided by five research\nquestions aimed at characterizing the relevant research\
    \ in\nterms of publication sources and scientific fields, as well\nas examining\
    \ the state-of-the-art machine learning methods\nfor mechanical fault detection\
    \ and fault prognosis in\nmanufacturing equipment, their strengths and weaknesses,\n\
    and their application in the context of data stream learning.\nA search for eligible\
    \ publications was conducted in\nfive academic databases, which, after applying\
    \ a set of\ncriteria, culminated in the selection of forty-four primary\nstudies.\n\
    The rest of this document is organized as follows:\nSection 2 presents the review\
    \ protocol developed for this\nstudy, including the definition of the research\
    \ questions,\nsearch strategy, study selection criteria and the data\nextraction\
    \ strategy. The results obtained from conducting\nthe review and answering the\
    \ research questions are\ndescribed in Section 3 and discussed in Section 4. Finally,\n\
    Section 5 presents the concluding remarks and provides\ndirections for future\
    \ work.\n2 Methods\nThis study follows the PRISMA statement [34], which\nestablishes\
    \ a checklist and a flow diagram for reporting\nsystematic reviews. However, the\
    \ PRISMA statement is\noriented towards the healthcare field, whereas the present\n\
    review covers themes related to engineering and computer\nscience. Healthcare\
    \ research differs significantly from\nresearch performed in engineering and computer\
    \ science\nand, as such, the PRISMA statement does not apply in\nits entirety.\
    \ For this reason, this study is also guided by\nthe procedure presented in [35],\
    \ which adapts different\nmedical guidelines for performing systematic reviews\
    \ to the\nparticularities of software engineering, but is applicable to\nother\
    \ scientific fields as well. The three main phases of\nthis procedure, namely\
    \ planning, conducting, and reporting\nthe review, as well as related activities\
    \ are presented in\nTable 1.\nThe need for this review was identified while researching\n\
    the literature of interest for the first author’s PhD thesis\nabout machine learning\
    \ methods for fault detection and\nprediction. As far as the authors are aware,\
    \ no systematic\nliterature review of the machine learning methods used\nfor mechanical\
    \ fault detection and fault prognosis in\nmanufacturing equipment in real-world\
    \ scenarios currently\nexists.\nBefore undertaking the necessary research work,\
    \ a\nreview protocol was developed to establish suitable research\nquestions and\
    \ define the search strategy, study selection\ncriteria and the data extraction\
    \ process. The protocol is\ndescribed in more detail in the following subsections.\n\
    2.1 Research questions\nThe first step in developing the protocol consisted in\n\
    formulating meaningful research questions to guide a state-\nof-the-art review\
    \ of the topic of study (Table 2).\nThe first research question is intended to\
    \ help understand\nwhere papers that describe the use of machine learning\nfor\
    \ mechanical fault detection and fault prognosis in\nmanufacturing equipment have\
    \ recently been published.\nThe purpose is to identify not only the publication\
    \ venues\nwhere the studies have been published and whether they\ntend to cluster\
    \ around specific venues or not, but also\ndetermine the types of venues in which\
    \ they were published.\nThe latter is of particular interest considering this\
    \ review\nfocuses on industrial case-studies. Because fault detection\nand prognosis\
    \ are studied in a wide range of scientific\nfields, the second research question\
    \ aims to identify the\nfields that most commonly use machine learning methods\n\
    for that purpose and if multidisciplinary approaches are\npresent.\nThe purpose\
    \ of research question three is to survey\nthe machine learning algorithms and\
    \ methods employed\nin the recent literature about mechanical fault detection\n\
    Table 1 Systematic review process\nPhase\nActivities\nPlanning the review\nIdentification\
    \ of the need for the review\nDevelopment of the review protocol\nConducting the\
    \ review\nIdentification of research\nSelection of primary studies\nStudy quality\
    \ assessment\nData extraction and monitoring\nData synthesis\nReporting the review\n\
    1 3\nM. Fernandes et al.\n14248\nTable 2 Research questions\nID\nResearch Question\n\
    RQ1\nIn which publication venues are studies about the use of\nmachine learning\
    \ for mechanical fault detection and fault\nprognosis in manufacturing equipment\
    \ published?\nRQ2\nIn which scientific fields has the use of machine learning\n\
    for mechanical fault detection and fault prognosis in\nmanufacturing equipment\
    \ been researched?\nRQ3\nWhat machine learning algorithms and methods are currently\n\
    employed for mechanical fault detection and fault prognosis\nin manufacturing\
    \ equipment?\nRQ4\nWhat limitations and advantages do those algorithms and\nmethods\
    \ present?\nRQ5\nWhich of those algorithms and methods are used for data\nstream\
    \ learning?\nand fault prognosis in manufacturing equipment. To answer\nthis question,\
    \ aspects such as which machine learning\nalgorithms are most frequently used,\
    \ what types of learning\ntasks are addressed, or whether hybrid and ensemble\n\
    methods are used should be considered. It is also important\nto learn why these\
    \ algorithms are being used and what their\nweaknesses are, a matter addressed\
    \ by research question\nfour.\nMuch of the data used to predict and detect faults\n\
    in manufacturing equipment is acquired by sensors that\nmonitor the machines and\
    \ produce high-speed data streams.\nClassical machine learning methods are not\
    \ adequate to\nlearn from these data streams, a task that presents unique\nchallenges\
    \ [36]. For that reason, research question five\nfocuses on machine learning methods\
    \ meant for data\nstream learning. The main aim is to determine how\nwidespread\
    \ the use of these methods is for mechanical fault\ndetection and fault prognosis\
    \ in manufacturing equipment,\nbut also to understand how such techniques are\
    \ being\nused.\n2.2 Search strategy\nTo identify recently published research about\
    \ machine\nlearning methods for mechanical fault detection and\nfault prognosis\
    \ in manufacturing equipment in real world\nscenarios, the following search strategy\
    \ was devised.\n2.2.1 Information sources\nThe five academic databases listed\
    \ in Table 3 were chosen\nafter considering search systems that were appropriate\
    \ for\nsystematic reviews [37] and whose subject was compatible\nwith the topic\
    \ of study. Although IEEE Xplore is not\nideally suited for systematic reviews,\
    \ it is an important\nresearch database in the fields of engineering, electron-\n\
    ics, and computer science and can be used to supple-\nment the results obtained\
    \ from the other four databases\n[37].\n2.2.2 Search string\nThe search string\
    \ used to find publications with the potential\nof being included in this systematic\
    \ review was built by\ncombining several search terms using the Boolean operators\n\
    OR and AND (Table 4).\nTo identify studies that use machine learning, the terms\n\
    “mining”, “learning” and “knowledge discovery” were\nincluded. The decision to\
    \ use other terms besides “machine\nlearning” stems from the fact that there is\
    \ considerable\noverlap between machine learning and data mining, and the\nterms\
    \ are often used interchangeably. Moreover, although\n“mining” and “learning”\
    \ are meant to represent “data\nmining” and “machine learning”, respectively,\
    \ the choice of\nusing broader terms was made with the intention of finding\n\
    research that employs other, related terms, such as “pattern\nmining” or “data\
    \ stream learning”. “knowledge discovery”\nwas included as well because it often\
    \ makes use of machine\nlearning techniques and can be relevant in the context\
    \ of\nfault detection and prognosis.\nTo find research pertaining to mechanical\
    \ fault detection\nand fault prognosis, the inclusion of the terms “fault\ndetection”,\
    \ “fault prediction” and “fault prognosis” was an\nobvious choice. However, it\
    \ also made sense to include the\nterm “predictive maintenance” since studies\
    \ about this topic\noften propose fault detection or prognosis methods.\nThe search\
    \ string is purposefully broad, not containing\nany terms that allude to mechanical\
    \ faults, manufacturing\nequipment or industrial case-studies. If the string included\n\
    those terms, the search results would be too narrow and\nmany studies that do\
    \ not explicitly use those terms would be\nleft out.\nSince the chosen academic\
    \ databases have slightly\ndifferent rules for building search strings, after\
    \ devising the\ngeneral search string presented in Table 4, specific strings\n\
    were created for each of them. The following example\nillustrates the search string\
    \ specified for the Web of Science\n(the TS tag field indicates the search terms\
    \ should be looked\nup in the title, abstract and keywords):\nTS\n=\n((“mining”\n\
    OR\n“learning”\nOR\n“knowledge\ndiscovery”) AND (“fault detection” OR “fault prediction”\n\
    OR “fault prognosis” OR “predictive maintenance”))\n2.3 Study selection criteria\n\
    A set of inclusion and exclusion criteria was defined to\nselect the relevant\
    \ studies from the search results. As can\nbe seen in Table 5, the studies that\
    \ should be included\nin the systematic literature review are those whose subject\n\
    1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n14249\n\
    Table 3 Research databases\nSearch system\nDatabases\nURL\nWeb of Science\nCore\
    \ Collection – SCI-EXPANDED, SSCI, CPCI-S, CPCI-SSH\nhttps://www.webofknowledge.com/\n\
    ACM Digital Library\nThe ACM Guide to Computing Literature\nhttps://dl.acm.org/\n\
    Science Direct\nhttps://www.sciencedirect.com/\nWiley Online Library\nhttps://onlinelibrary.wiley.com/\n\
    IEEE Xplore\nhttps://ieeexplore.ieee.org/Xplore/home.jsp\nmatter is the use of\
    \ machine learning techniques for the\ndetection of mechanical faults or prediction\
    \ of faults in\nmanufacturing equipment. Only studies that meet one or\nmore of\
    \ the inclusion criteria are of interest for the purpose\nof this review.\nThe\
    \ exclusion criteria presented in Table 6 are meant\nto filter out research that\
    \ does not satisfy other important\ncharacteristics. Duplicate publications are\
    \ to be eliminated,\nas are publications that are not written in English or\n\
    studies that were published in venues other than conference\nproceedings, book\
    \ chapters/sections or journals with impact\nfactor as defined in Clarivate’s\
    \ Journal Citation Reports\n(JCR). Additionally, only full-length articles published\n\
    since 2015 that present empirical results obtained from\nindustrial case-studies\
    \ are to be considered.\n2.4 Data extraction strategy\nThe data collected from\
    \ the selected studies is meant to\nanswer the systematic review’s research questions.\
    \ For\nthat purpose, a data form template was created to extract\ninformation\
    \ from each of the selected studies in a consistent\nmanner (Table 7). To determine\
    \ the ‘scientific fields’ of\npublications, an examination of the scientific categories\n\
    of the publication venues will be carried out. In the\ncase of conferences, the\
    \ necessary information will be\nobtained from the official websites, whereas\
    \ for journals the\ncategories defined by Clarivate’s Journal Citation Reports\n\
    (JCR) will be taken into consideration. Whenever a given\npublication is indexed\
    \ in more than one JCR category, the\ncategory with the highest ranking will be\
    \ chosen. If two\ncategories or more have the same ranking, the authors of this\n\
    review will decide which category is more appropriate. The\nTable 4 Search string\n\
    Field of study\nSearch terms\nMachine Learning\n(“mining” OR “learning” OR “knowl-\n\
    edge discovery”)\nAND\nFault Prediction/Detection\n(“fault detection” OR “fault\
    \ prediction”\nOR “fault prognosis” OR “predictive\nmaintenance”)\n‘country of\
    \ research’ will be defined based on the country\nof affiliation of the first\
    \ author.\n3 Results\nAs can be seen in Fig. 1, the execution of the previously\n\
    presented protocol resulted in the selection of 44 primary\nstudies. In the identification\
    \ phase, the search queries\nperformed in the Web of Science, Science Direct,\
    \ ACM\nDigital Library, Wiley Online Library, and IEEE Xplore\ndatabases yielded\
    \ a total of 4549 records. After removing\nduplicate entries, a total of 3377\
    \ studies remained. These\nrecords were screened based on publication details,\
    \ such as\npublication venue (EC3) and language (EC5), as well as on\nthe information\
    \ provided by the title and the abstract. Of\nthe 3377 studies evaluated, 2821\
    \ did not meet the selection\ncriteria. Additionally, seven publications had to\
    \ be discarded\nbecause the full text was not available. The remaining\n549 publications\
    \ underwent a more detailed full text\nassessment to determine if they met the\
    \ inclusion criteria\nand provided empirical results obtained from industrial\n\
    case-studies (EC6). 505 studies had to be excluded, while\nthe 44 studies that\
    \ met the described criteria were selected\nfor inclusion in the systematic review.\n\
    3.1 Distribution of publications by year and country\nAs can be seen in Fig. 2,\
    \ there is a clear trend of increase\nin publications from 2016 to 2019. The majority\
    \ (88.6%)\nof selected studies have been published since 2018, with\na noticeable\
    \ surge in the number of publications that\nyear. However, the number of annual\
    \ publications has\nTable 5 Inclusion criteria\nID\nCriteria\nIC1\nThe publication\
    \ focuses on the use of machine learning\nalgorithms and methods for mechanical\
    \ fault detection in\nmanufacturing equipment.\nIC2\nThe publication focuses on\
    \ the use of machine learning algo-\nrithms and methods for prognosis of faults\
    \ in manufacturing\nequipment.\n1 3\nM. Fernandes et al.\n14250\nTable 6 Exclusion\
    \ criteria\nID\nCriteria\nEC1\nDuplicate publication.\nEC2\nPublication is not\
    \ a full-length article.\nEC3\nStudy not published in a peer-reviewed journal\
    \ with JCR\nImpact Factor, in conference proceedings, or in a book\nchapter/section.\n\
    EC4\nStudy published before 2015.\nEC5\nPublication not written in English.\n\
    EC6\nStudy without empirical results obtained from an industrial\ncase-study.\n\
    been decreasing since 2020. Considering the search for\npublications to include\
    \ in this review was undertaken in\nOctober 2021, it’s unclear if this trend will\
    \ continue until\nthe end of 2021. Moreover, since the COVID-19 pandemic\naffected\
    \ the scientific community significantly, delaying\nresearch work and publications\
    \ [38], it is reasonable to\nexpect that many studies that were planned for 2020\
    \ and\n2021 will only be published in later years. It is also worth\nnoting that\
    \ no study from 2015 was selected for inclusion in\nthe systematic review.\nThese\
    \ publications come from 21 different countries\n(Table 8), but the distribution\
    \ of the number of publications\nper country is positively skewed, i.e., most\
    \ nations only\npublished one or two studies. Only three countries published\n\
    Table 7 Data extraction form template\nData extraction form\nTitle:\nAuthors:\n\
    Country of research:\nPublication venue:\nPublication details:\nType of publication\
    \ venue:\nScientific field:\nResearch questions answered:\nContent:\n– Which machine\
    \ learning algorithms are used?\n– How are they applied to the diagnosis of mechanical\
    \ faults or\nprognosis of faults in manufacturing equipment?\n– What types of\
    \ learning tasks are addressed (e.g., supervised,\nunsupervised, reinforcement\
    \ learning)?\n– Are hybrid learning methods used?\n– What types of learning techniques\
    \ are employed (e.g., ensemble\nlearning, transfer learning, online learning,\
    \ etc.)?\n– What are the benefits/limitations of the machine learning techniques\n\
    used in the study?\n– Which data stream learning techniques are used?\nmore than\
    \ two studies, but together they were responsible\nfor publishing 43.2% of the\
    \ studies included in this review\n(Fig. 3). China, Germany, and Greece were the\
    \ countries\nthat published the most studies, with Germany and Greece\ncontributing\
    \ with six studies each and China with seven\nstudies.\n3.2 RQ1: In which publication\
    \ venues are studies\nabout the use of machine learning for mechanical\nfault\
    \ detection and fault prognosis\nin manufacturing equipment published?\nThe 44\
    \ selected studies were published in 36 distinct\nvenues, of which 17 are journals\
    \ and 19 are conferences,\nwith only five venues publishing more than one study\n\
    about the topic of interest (Table 9). The top publication\nsources include IEEE\
    \ Access with five studies and the 2019\n31st International Conference on Advanced\
    \ Information\nSystems Engineering (CAiSE), CIRP Annals, Sensors and\nThe International\
    \ Journal of Advanced Manufacturing\nTechnology with two studies each. 16 of the\
    \ 36 publication\nvenues are affiliated with the Institute of Electrical\nand\
    \ Electronics Engineers (IEEE), representing 17.6%\nof journals, 68.4% of conferences\
    \ and 20% of the top\npublication venues.\nMore than 52% of these distinct venues\
    \ are conferences,\nbut only 45.5% of the selected studies were published in\n\
    conference proceedings versus 54.5% that were published\nin journals (Fig. 4).\
    \ This reveals that the average number\nof papers published in journals is greater\
    \ than the average\nnumber of papers published in conference proceedings.\nFurthermore,\
    \ four of the five publication venues where more\nthan one study was published\
    \ are scientific journals and\ntogether these four venues published 25% of all\
    \ the studies\nincluded in this review, which seems to imply there is a\npreference\
    \ for publishing in scientific journals.\n3.3 RQ2: In which scientiﬁc ﬁelds has\
    \ the use\nof machine learning for mechanical fault detection\nand fault prognosis\
    \ in manufacturing equipment\nbeen researched?\nThe results show the recent research\
    \ on machine learning\nfor fault detection and prognosis in the manufacturing\n\
    industry has been explored mostly by the computer science\ncommunity. As shown\
    \ in Table 10, computer science\napproaches account for 47.7% of the selected\
    \ studies.\nLagging considerably behind, but still worth considering,\nare engineering\
    \ and multidisciplinary studies, with 25% and\n13.6% respectively.\nMultidisciplinary\
    \ approaches involve several disciplines,\nsuch as telecommunications and cybernetics,\
    \ but contribu-\ntions from the fields of computer science, engineering, and\n\
    1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n14251\n\
    Fig. 1 PRISMA flow diagram\nof study selection\nFig. 2 Distribution of selected\n\
    publications per year\n1 3\nM. Fernandes et al.\n14252\nTable 8 Provenance of\
    \ the publications included in the systematic review\nCountry or area\nNumber\
    \ of publications\nCountry or area\nNumber of publications\nCountry or area\n\
    Number of publications\nChina\n7\nTurkey\n2\nPortugal\n1\nGermany\n6\nUK\n2\n\
    Slovenia\n1\nGreece\n6\nUSA\n2\nSouth Africa\n1\nAustralia\n2\nBelgium\n1\nSpain\n\
    1\nItaly\n2\nIndia\n1\nSweden\n1\nSouth Korea\n2\nNetherlands\n1\nSwitzerland\n\
    1\nSingapore\n2\nNorway\n1\nTaiwan\n1\nautomation and control systems are strongly\
    \ prevalent even\nin this broader category.\n3.4 RQ3: What machine learning algorithms\n\
    and methods are currently employed for mechanical\nfault detection and fault prognosis\n\
    in manufacturing equipment?\nThe selected primary studies employ a variety of\
    \ machine\nlearning algorithms and methods to perform mechanical\nfault detection\
    \ and fault prognosis, including combina-\ntions of different algorithms. Most\
    \ studies also perform\ncomparative analyses between different machine learn-\n\
    ing algorithms to demonstrate the value of the proposed\nmethod or to select the\
    \ most adequate algorithm. In the\nlatter case, only the selected (or best performing)\
    \ algo-\nrithms will be described in this review. These algorithms\ninclude: AdaBoost;\
    \ agglomerative clustering (AC); autoen-\ncoder; autoregressive integrated moving\
    \ average (ARIMA);\nback-propagation neural network (BPNN); classification\nand\
    \ regression trees (CART); classification based on associ-\nations – classifier\
    \ building algorithm (CBA-CB) ; convolu-\ntional neural network (CNN); deep neural\
    \ network (DNN);\nFig. 3 Share of publications by country\ndensity-based spatial\
    \ clustering of applications with noise\n(DBSCAN); discrete Bayes filter (DBF);\
    \ eXtended classi-\nfier system (XCS); frequent pattern growth (FP-Growth);\n\
    Gaussian mixture models (GMM); gradient boosting deci-\nsion trees (GBDT); hidden\
    \ Markov model (HMM); hier-\narchical clustering (HC); isolation forest (IF);\
    \ k-means;\nK-multi-dimensional time-series clustering (K-MDTSC); k-\nnearest\
    \ neighbors (K-NN); k-singular value decomposition\n(K-SVD); local outlier factor\
    \ (LOF); logistic regression\n(LR); long short-term memory (LSTM); LSTM autoen-\n\
    coder; long short-term memory - generative adversar-\nial network (LSTM-GAN);\
    \ mean shift clustering (MSC);\nmicro-cluster continuous outlier detection (MCOD);\
    \ mul-\ntilayer perceptron (MLP); na¨ıve Bayes (NB); neighbour-\nhood component\
    \ analysis (NCA); partial least squares\nregression (PLSR); principal component\
    \ analysis (PCA);\nquadratic discriminant analysis (QDA); quantitative asso-\n\
    ciation rule mining algorithm (QARMA); random forest\n(RF); random survival forest\
    \ (RSF); recurrent neural net-\nwork (RNN); simple linear regression; spectral\
    \ clustering\n(SC); stacked sparse autoencoders (SSAE); support vector\nmachines\
    \ (SVM).\nTo facilitate the analysis of the data, the algorithms were\ngrouped\
    \ into different categories, as shown in Table 11.\nFigure 5 illustrates that\
    \ most studies included in this\nreview (84.1%) use machine learning algorithms\
    \ belonging\nto four categories, namely artificial neural networks with\n12 publications,\
    \ decision trees with 11 publications, hybrid\nmodels with eight publications\
    \ and latent variable models\nwith six publications. One of these studies uses\
    \ both an\nartificial neural network and a hybrid model to address\ndifferent\
    \ problems. The remaining eight studies apply\nalgorithms from a variety of categories.\
    \ It is also worth\nnoting that 13 studies make use of ensemble learning\ntechniques.\n\
    The selected studies handle different types of learning\ntasks depending on the\
    \ problems under consideration and\nthe data that is available. As can be seen\
    \ in Fig. 6, 53.3% of\npublications employ supervised learning techniques, 28.9%\n\
    use unsupervised learning techniques, 15.6% make use of\nboth supervised and unsupervised\
    \ techniques and 2.2%\n1 3\nMachine learning techniques applied to mechanical\
    \ fault diagnosis...\n14253\nTable 9 Studies per publication venue\nPublication\
    \ venue\nStudies\nVenue type\n(International conference) Machine Learning for\
    \ Cyber Physical Systems (ML4CPS 2018)\n[39]\nconference\n2016 IEEE International\
    \ Conference on Systems, Man, and Cybernetics (SMC)\n[40]\nconference\n2016 International\
    \ Symposium on Leveraging Applications of Formal Methods, Verification and Validation\
    \ (ISoLA)\n[41]\nconference\n2017 13th IEEE Conference on Automation Science and\
    \ Engineering (CASE)\n[42]\nconference\n2018 14th IEEE/ASME International Conference\
    \ on Mechatronic and Embedded Systems and Applications (MESA)\n[43]\nconference\n\
    2018 IEEE 20th Conference on Business Informatics (CBI)\n[44]\nconference\n2018\
    \ IEEE International Conference on Big Data (Big Data)\n[45]\nconference\n2018\
    \ IEEE International Symposium on INnovations in Intelligent SysTems and Applications\
    \ (INISTA)\n[46]\nconference\n2018 International Conference on Information and\
    \ Computer Technologies (ICICT)\n[47]\nconference\n2019 16th International Conference\
    \ on Distributed Computing and Artificial Intelligence (DCAI)\n[48]\nconference\n\
    2019 16th International Symposium on Wireless Communication Systems (ISWCS)\n\
    [49]\nconference\n2019 Chinese Automation Congress (CAC)\n[50]\nconference\n2019\
    \ Genetic and Evolutionary Computation Conference (GECCO)\n[51]\nconference\n\
    2019 IEEE 17th International Conference on Industrial Informatics (INDIN)\n[52]\n\
    conference\n2019 IEEE 5th World Forum on Internet of Things (WF-IoT)\n[53]\nconference\n\
    2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA)\n\
    [54]\nconference\n2020 26th ACM SIGKDD International Conference on Knowledge Discovery\
    \ & Data Mining\n[55]\nconference\n2020 International Conference on Development\
    \ and Application Systems (DAS)\n[56]\nconference\nAdvances in Manufacturing\n\
    [57]\njournal\nApplied Sciences\n[58]\njournal\nBusiness & Information Systems\
    \ Engineering\n[59]\njournal\nComplexity\n[60]\njournal\nComputers & Industrial\
    \ Engineering\n[61]\njournal\nElectronics\n[62]\njournal\nEngineering Applications\
    \ of Artificial Intelligence\n[63]\njournal\nExpert Systems with Applications\n\
    [64]\njournal\nIEEE Transactions on Industrial Electronics\n[31]\njournal\nIEEE\
    \ Transactions on Industrial Informatics\n[65]\njournal\nJournal of Manufacturing\
    \ Systems\n[66]\njournal\nSimulation Modelling Practice and Theory\n[67]\njournal\n\
    Studies in Informatics and Control\n[68]\njournal\n2019 31st International Conference\
    \ on Advanced Information Systems Engineering (CAiSE)\n[69, 70]\nconference\n\
    CIRP Annals\n[71, 72]\njournal\nSensors\n[73, 74]\njournal\nThe International\
    \ Journal of Advanced Manufacturing Technology\n[75, 76]\njournal\nIEEE Access\n\
    [77–81]\njournal\n1 3\nM. Fernandes et al.\n14254\nFig. 4 Proportion of publications\
    \ in conferences and journals\ncombine semi-supervised, unsupervised, and supervised\n\
    techniques. The use of unsupervised techniques is motivated\nmostly by an absence\
    \ of labeled data [47, 48, 50, 54, 59–\n62, 65, 70], although in some studies\
    \ they are employed\nto detect outliers [74], reduce dimensionality [31, 75] or\n\
    extract features [81]. In studies [39, 42, 45, 58, 66, 69,\n71], labeled data\
    \ was available, but was used to validate the\nunsupervised learning models.\n\
    3.5 RQ4: What limitations and advantages do those\nalgorithms and methods present?\n\
    Of the 44 selected studies, 33 described the motivations\nfor choosing a particular\
    \ machine learning algorithm or\ncombination of algorithms. Some of these motivations\
    \ relied\non the inherent strengths of the algorithms employed, while\nothers\
    \ considered the specific advantages an algorithm\ncould have for fault detection\
    \ and prediction, or for its\nimplementation in industrial environments. In addition,\
    \ the\nbenefits provided by the proposed approach were also\nreported in several\
    \ studies. On the contrary, only eleven\nstudies presented the limitations of\
    \ either the algorithms\nemployed or the proposed approaches.\nIn the following\
    \ subsections, Tables 12, 13, 14, 15 and 16\nsummarize the advantages and limitations\
    \ of these machine\nlearning algorithms and methods. After each table, they are\n\
    described in more detail.\n3.5.1 Decision trees\nModels in the decision tree category\
    \ have several char-\nacteristics that make them suitable for implementation in\n\
    industrial contexts. In [41], the authors decided to use a clas-\nsification tree\
    \ due to its interpretability. CART models are\nwhite box classifiers whose outputs\
    \ can be represented by a\nseries of if statements. This allows factory engineers\
    \ to ana-\nlyze the model and understand the reasoning that led to a\ngiven decision.\
    \ However, this advantage is lost when using\nensembles of trees, since they combine\
    \ several base models\nto obtain a more robust output.\nNonetheless, ensemble\
    \ tree models are highly valued for\ntheir efficiency and effectiveness. Random\
    \ forest models\nwere used for these reasons in [44] and [53]. Specifically,\n\
    in [44] a random forest was used to develop a proof\nof concept that would allow\
    \ the authors to demonstrate\nthat relevant results could be obtained from real\
    \ world\ndata in a short period of time. More complex algorithms\nwould not have\
    \ been appropriate in a situation where\nhigh predictive power and low implementation\
    \ effort were\nnecessary. Additionally, the ability of random forest models\n\
    to reduce variance and increase generalizability was also\ntaken into consideration,\
    \ since the amount of training\nsamples was relatively small, but the feature\
    \ space was\nlarge. The predictive tool proposed in [76] also makes use\nof an\
    \ ensemble method, specifically GBDT, due to the\nalgorithm’s low computational\
    \ complexity and predictive\npower when handling large-scale datasets. The algorithm’s\n\
    ability to assess the importance of features was also\nessential to determine\
    \ which time lag should be used as\ninput to DPCA. However, the proposed approach\
    \ has the\ndisadvantage of lacking interpretability, not only because\nGBDT uses\
    \ an ensemble of decision trees, but also because\nthe data used to predict failures\
    \ in the milling machine\nTable 10 Proportion of studies per scientific field\n\
    Scientific field\nStudies\nNumber of publications\nComputer Science\n[39, 41,\
    \ 44–48, 51, 55, 59, 61, 63, 64, 67, 69, 70, 77–81]\n47.7% (n = 21)\nEngineering\n\
    [31, 54, 57, 58, 62, 65, 66, 71, 72, 75, 76]\n25% (n = 11)\nMultidisciplinary\n\
    [40, 43, 52, 53, 56, 60]\n13.6% (n = 6)\nAutomation & Control Systems\n[42, 50]\n\
    4.5% (n = 2)\nInstruments & Instrumentation\n[73, 74]\n4.5% (n = 2)\nOperations\
    \ Research & Management Science\n[68]\n2.3% (n = 1)\nWireless Communications,\
    \ Net-\nworking and Signal Processing\n[49]\n2.3% (n = 1)\n1 3\nMachine learning\
    \ techniques applied to mechanical fault diagnosis...\n14255\nTable 11 Machine\
    \ learning algorithms and methods employed in the selected primary studies\nCategory\n\
    Method\nPublications\nLearning task\nARIMA models\nARIMA\n[48, 61]\nunsupervised\n\
    Partition-based algorithms\nMCOD\n[69]\nunsupervised\nDecision Trees\nAdaboost\n\
    [72]\nsupervised\nCART\n[41, 46, 68]\nsupervised\nIF\n[67]\nsupervised\nRF\n[43,\
    \ 44, 46, 52, 53]\nsupervised\nRSF\n[71]\nunsupervised\nGBDT\n[76]\nsupervised\n\
    Dynamic Bayes Networks\nDBF\n[63]\nsupervised\nHMM\n[42]\nunsupervised\nHybrid\
    \ models\nARIMA + LSTM\n[50]\nunsupervised\nDBSCAN + RF\n[74]\nunsupervised +\
    \ supervised\nDBSCAN + SVM\n[70]\nunsupervised + supervised\n[HC / time series\
    \ clustering] + RNN\n[59]\nunsupervised + supervised\nOne-class SVM + K-Means\
    \ + RF\n[45]\nsemi-supervised + unsupervised\n+ supervised\nAutoencoder + Simple\
    \ Linear\nRegression\n[58]\nunsupervised + supervised\nGMM + FP-Growth + CBA-CB\n\
    [75]\nunsupervised + supervised\nCNN + NCA + Medium Gaus-\nsian SVM / CNN + NCA\
    \ +\nensemble subspace K-NN\n[80]\nsupervised\nInstance-based algorithms\nK-NN\n\
    [39]\nsupervised\nLatent Variable Models\nPCA\n[65]\nunsupervised\nGMM\n[47]\n\
    unsupervised\nK-Means\n[54]\nunsupervised\nPLSR\n[64]\nsupervised\nK-SVD\n[60]\n\
    unsupervised\nK-MDTSC\n[62]\nunsupervised\nArtificial Neural Networks\nANN\n[57]\n\
    supervised\nBPNN\n[40]\nsupervised\nCNN\n[78]\nsupervised\nDNN\n[77]\nsupervised\n\
    LSTM\n[70]\nsupervised\nMLP\n[56]\nsupervised\nSSAE + BPNN\n[31]\nunsupervised\
    \ + supervised\nSSAE + Softmax Classifier\n[81]\nunsupervised + supervised\nLSTM\
    \ Autoencoder\n[73]\nsupervised\nLSTM - GAN\n[79]\nsupervised\nRNN\n[55]\nsupervised\n\
    Conditional Variational Autoencoder\n[66]\nunsupervised\nRule-based models\nR4RE\
    \ (“Rules 4 Rare Events” based on QARMA)\n[49]\nsupervised\nXCSR\n[51]\nsupervised\n\
    consists in the principal components obtained from the\napplication of DPCA, which\
    \ do not represent any physical\nproperties or measurements of the system. The\
    \ study\npresented in [67] used an ensemble method as well due to its\nefficiency\
    \ in terms of computation time and memory when\nhandling large amounts of data.\n\
    In [71], the proposed model (manufacturing system-\nwide balanced random forest\
    \ [MBRSF]) incorporated a\n1 3\nM. Fernandes et al.\n14256\nFig. 5 Number of publications\n\
    per category of machine\nlearning algorithms\nrandom survival forest because of\
    \ its ability to handle\nbias and variance issues. The model captured complex\n\
    fault patterns and diverse fault propagation pathways\nand made breakdown predictions\
    \ for a time horizon\nnot yet found in the manufacturing systems literature.\n\
    Another advantage pointed out by the authors, was\nthe theoretical guarantee provided\
    \ for the prognostic\nperformance due to the integration of the RSF model with\n\
    data balancing techniques. Research undertaken by the\nauthors demonstrated the\
    \ MBRSF could attain a prognostic\nperformance, with respect to an integrated\
    \ Brier score, 90%\nbetter than other methods.\n3.5.2 Artiﬁcial neural networks\n\
    Like ensemble methods, artificial neural networks suffer\nfrom a lack of interpretability,\
    \ making them unsuitable for\nuse in situations where it is necessary to know\
    \ what factors\ncontributed to a machine failure. They do, however, possess\n\
    several advantages including good fault tolerance, the\nability to learn complex\
    \ nonlinear relationships and strong\ngeneralization abilities, which motivated\
    \ their application\nin [57]. Likewise, in [77], a deep neural network was used\n\
    due to its ability to map the complex relationship between\nsignals and the health\
    \ status of industrial equipment. The\nuse of a deep learning model was also considered\
    \ because\nsuch models are capable of uncovering patterns in raw\ntime series\
    \ data, which eliminated the need to use signal\nprocessing techniques.\nThe method\
    \ described in [78] explored the ability of\nconvolutional neural networks to\
    \ recognize and classify\nimages by transforming time series data into images\
    \ and\nusing them as inputs to a CNN model. This approach\nhas been shown to be\
    \ suitable for maintaining temporal\ninformation and learning time-invariant features,\
    \ thus\nresulting in improved classification performance. The\nproposed framework\
    \ also included the option of using a\nparametric rectified linear unit (PReLU)\
    \ function as an\nactivation function to further improve performance when\ndealing\
    \ with large datasets.\nIn [40], to overcome the BPNN’s limitations the authors\n\
    used a genetic algorithm to optimize the network’s initial\nweights, thresholds\
    \ and number of hidden layer neurons.\nWith this technique, they were able to\
    \ obtain faster\nconvergence, more accurate fault predictions and less\ncomputational\
    \ complexity.\nFig. 6 Types of learning tasks\nconsidered in the selected\nstudies\n\
    1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n14257\n\
    Table 12 Advantages and limitations of the decision tree algorithms employed for\
    \ mechanical fault detection and fault prognosis\nPublication\nAdvantages\nLimitations\n\
    [41]\nAlgorithm: interpretability.\nNot identified\n[44]\nAlgorithm: high predictive\
    \ per-\nformance; low implementation\neffort;\nreduces\nvariance\nand\nincreases\
    \ generalizability.\nNot identified\n[76]\nAlgorithm: high predictive per-\nformance;\n\
    low\ncomputational\ncomplexity; provides information\nabout feature importance.\n\
    Proposed approach: lacks interpretability.\n[53]\nAlgorithm: efficiency; effectiveness.\n\
    Not identified\n[67]\nAlgorithm: computational efficiency\nNot identified\n[71]\n\
    Algorithm: handles bias and variance issues\nProposed approach: theoretical\n\
    guarantee for prognostic perfor-\nmance; captures complex fault\npatterns and\
    \ fault propagation\npathways.\nNot identified\nSimilarly, the authors of the\
    \ study proposed in [73]\nchose to detect faults and predict the RUL using LSTM-\n\
    autoencoders because the combination of LSTMs and\nautoencoders has shown potential\
    \ for accurate time series\nforecasting. According to the authors, LSTM-autoencoders\n\
    have produced better forecasts than multilayer perceptrons,\ndeep belief networks\
    \ or LSTMs, due to their ability to\nidentify the temporal patterns present in\
    \ time series data\nand their superior feature extraction capability. However,\n\
    the hyperparameters of the network impacts its performance\nsignificantly and\
    \ choosing them can be a difficult task.\nThe proposed approach, whereby one LSTM-autoencoder\n\
    for each health state is trained, can be adjusted to handle\ndifferent health\
    \ states (labels) and be applied to different\nmachines. However, the complexity\
    \ of the architecture\ncan increase rapidly, and the system might not be able\n\
    to identify neighbouring health states. Additionally, this\napproach requires\
    \ labelled data, which is not easily available\nin industrial settings.\nDeep\
    \ learning models are able to learn features from raw\ndata as long as the training\
    \ and test data share the same\ndistribution and feature space. However, under\
    \ time-varying\nconditions, such as those encountered in real industrial\nsettings,\
    \ this condition often does not hold. To handle this\nissue, the method proposed\
    \ in [31] goes beyond simple\npattern recognition and classification of existing\
    \ faults by\nusing deep learning to identify the dynamic properties of\nthe machine\
    \ tool. This enabled the early detection of fault\nfeatures and the diagnosis\
    \ of the machine’s health status\nunder time-varying operation.\nThe framework\
    \ presented in [66] also addresses the issue\nof time-varying operations. The\
    \ proposed method relies\non two conditional variational autoencoder (CVAE) models\n\
    to estimate the health index of the machining centre and\npredict its future condition\
    \ for a given operating regime.\nThe authors of this study chose the CVAE due\
    \ to its ability\nto remove noise from sensor data and extract meaningful\nfeatures\
    \ from the data automatically. Additionally, CVAEs\nare capable of learning complex\
    \ conditional probability\ndistributions regardless of the dimensionality of the\
    \ feature\nspace and can, therefore, be used to generate conditional\ndata. This\
    \ characteristic can be very helpful when handling\nindustrial data since it facilitates\
    \ the simulation of different\nproduction sequences regarding the current health\
    \ state of\na machine. Owing to these characteristics, the authors were\nable\
    \ to develop a method that can estimate a machine’s\nhealth under time-varying\
    \ operations in a scenario where\nvery little labelled data was available, as\
    \ is often the case in\nreal industrial settings.\nIn [81], the problem of different\
    \ data distributions in\nthe training and test data was also addressed, as was\
    \ the\nissue of insufficient or low-quality training data, a common\nproblem in\
    \ the manufacturing industry. To solve these\nissues, the authors of the study\
    \ combined deep transfer\nlearning with digital-twin technology. The digital entity\
    \ was\nused to simulate the entire product life cycle and generate\nvast amounts\
    \ of data under different working conditions,\nwhile deep transfer learning was\
    \ used to extract knowledge\nfrom the digital domain and apply it in the physical\
    \ domain\nwhere the model was fine-tuned. With this approach, the\nauthors of\
    \ the study were able to explore shared knowledge\nand create a model that remained\
    \ viable when put into\nproduction.\nChanges in the data distribution were also\
    \ the focus of\nthe method proposed in [55]. SERMON is a model capable\nof modelling\
    \ the temporal dependency present in streaming\n1 3\nM. Fernandes et al.\n14258\n\
    Table 13 Advantages and limitations of the artificial neural networks employed\
    \ for mechanical fault detection and fault prognosis\nPublication\nAdvantages\n\
    Limitations\n[31]\nProposed\napproach:\nearly\ndetection\nof\nfault\nfeatures;\
    \ diagnosis of machine’s health status\nunder time-varying operation.\nAlgorithm:\
    \ performance decreases when training\nand test data don’t share the same distribution.\n\
    [40]\nProposed approach: faster convergence; improved\nprediction performance;\
    \ less computational com-\nplexity.\nAlgorithm: slow convergence speed; low preci-\n\
    sion; falls easily into local minimum; number of\nhidden layers difficult to determine.\n\
    [57]\nAlgorithm: fault tolerance; learns complex nonlin-\near relationships; strong\
    \ generalization abilities.\nAlgorithm: lack of interpretability.\n[81]\nProposed\
    \ approach: knowledge sharing; genera-\ntion of vast amounts of data through simulation\
    \ of\nthe entire product life cycle.\nAlgorithm: performance decreases when training\n\
    and test data don’t share the same distribution.;\npoor performance in case of\
    \ insufficient or low-\nquality training data.\n[78]\nProposed Approach: improved\
    \ classification per-\nformance; maintains temporal information and\nlearns time-invariant\
    \ features.\nNot identified\n[77]\nAlgorithm: learns complex nonlinear relation-\n\
    ships; uncovers patterns in raw time series data.\nNot identified\n[55]\nProposed\n\
    approach:\nonline\nlearning\ncapabil-\nity; autonomous structural evolution; capable\
    \ of\nadapting to drifts in the input data; capable of\nlearning under finitely/infinitely\
    \ delayed label sce-\nnarios.\nNot identified\n[73]\nAlgorithm: capable of processing\
    \ time series\ndata; superior feature extraction capability; better\nforecasting\
    \ ability.\nAlgorithm: choice of suitable hyperparameters\nis complex and affects\
    \ the performance of the\nnetwork;\nProposed approach: can be adjusted to different\n\
    types of machines and labels.\nProposed approach: using multiple ANNs can be\n\
    a difficult task; might not be able to identify\nneighbour states; requires labelled\
    \ data.\n[79]\nProposed approach: capable of generating large\nvolumes of fault\
    \ data; avoids mode collapse;\nimproved accuracy and efficiency.\nNot identified\n\
    [66]\nAlgorithm: denoising effect; automatic extraction\nof meaningful features;\
    \ capable of learning com-\nplex probability distributions; well-understood\n\
    and stable; can be used for conditional data gener-\nation.\nNot identified\n\
    Proposed\napproach:\nestimation\nof\nmachine’s\nhealth status under time-varying\
    \ operations; capa-\nble of handling sparse industrial data; product-\nspecific\
    \ health index can be used for scheduling\nmaintenance and production.\ndata and\
    \ adapt to the changing characteristics of the data as\nit arrives in real-time.\
    \ It does so thanks to the self-evolving\narchitectures of its two RNNs. The SERN\
    \ component can\ndynamically change the number of hidden units, while the\nMERN\
    \ component can dynamically change the number of\nhidden layers as well. Moreover,\
    \ to handle scenarios where\nlabels might be delayed or inexistent, SERMON includes\n\
    a mapping unit that employs an autoencoder to suggests\npossible data labels.\n\
    An LSTM-GAN was used in [79] as part of a PdM\nmethodology that can not only monitor\
    \ the health state\nof machines and predict faults before they occur, but also\n\
    provides the factory’s maintenance staff with maintenance\nplans that are appropriate\
    \ to deal with the issues detected\nby the state prediction and fault prediction\
    \ models. The\nGAN was used to generate a large volume of synthetic\nfault data\
    \ to improve the accuracy of the model. However,\nGANs may suffer from mode collapse.\
    \ Due to the\ninclusion of memory units, gate structures and attention\nmechanisms,\
    \ LSTM networks can alleviate the mode\ncollapse issue. Moreover, LSTMs are capable\
    \ of extracting\npatterns from long sequences of input data, making them\nideal\
    \ to detect abnormalities in condition monitoring\ndata.\n1 3\nMachine learning\
    \ techniques applied to mechanical fault diagnosis...\n14259\nTable 14 Advantages\
    \ and limitations of the hybrid models employed for mechanical fault detection\
    \ and fault prognosis\nPublication\nAdvantages\nLimitations\n[45]\nProposed approach:\
    \ detects and classifies different\nmechanical faults in unlabeled data.\nNot\
    \ identified\n[50]\nLSTM: captures nonlinear relationships in sequen-\ntial data.\n\
    ARIMA: models linear associations present in\ntime series data.\nNot identified\n\
    Proposed approach: optimization of the perfor-\nmance of the proposed fault prognosis\
    \ model.\n[59]\nWPGMC clustering: interpretability.\nRNN: captures complex, nonlinear\
    \ relationships\nin time series data.\nNot identified\nProposed approach: uncovers\
    \ patterns of wear and\ntear in unlabeled data.\n[74]\nRandom Forest: improved\
    \ performance; robust-\nness when handling numerical data and real-world\nproblems.\n\
    Not identified\nProposed approach: improved accuracy.\n[58]\nAutoencoder: can\
    \ learn the relationship between\nthe input data variables.\nProposed approach:\
    \ anomaly threshold is defined\narbitrarily; could use more a more sophisticated\n\
    model to improve prediction accuracy.\nProposed approach: can learn from unlabeled\
    \ data;\napplicable to different domains.\n[75]\nGMM: capable of reducing the\
    \ number of clusters.\nNot identified\nFP-Growth: handles large databases efficiently;\n\
    can handle itemsets with low support threshold.\nProposed approach: interpretability;\
    \ can handle\ndifferent types of sensor data; simple to set-up.\n3.5.3 Hybrid\
    \ models\nHybrid machine learning models are created with the\nintention of solving\
    \ tasks that a single algorithm, or\ntype of algorithms, is not suited to handle.\
    \ The analysis\nperformed in [45] demonstrated that supervised learning\nalgorithms\
    \ are more appropriate for classification tasks but\nrequire labeled data and\
    \ misclassify unknown faults. Semi-\nsupervised algorithms can overcome these\
    \ limitations, but\nare not capable of distinguishing between fault types, which\n\
    is where unsupervised clustering algorithms can be useful.\nThe combination of\
    \ these different types of algorithms led\nto the development of a predictive\
    \ maintenance system\ncapable of detecting and classifying different mechanical\n\
    faults from unlabeled data.\nIn [50], Cheng et al. decided to combine the strengths\
    \ of\nARIMA models and LSTM neural networks to optimize the\nperformance of the\
    \ proposed fault prognosis model. Since\nLSTM networks are artificial neural networks\
    \ capable of\nhandling long-term dependencies, they are ideal to capture\nnonlinear\
    \ relationships in sequential data. Conversely,\nARIMA models, which were developed\
    \ for time series\nanalysis, are well suited to model the linear associations\n\
    present in time series data.\nThe study presented\nin\n[59] employed clustering\n\
    techniques and a recurrent neural network to overcome\nthe problem of missing\
    \ labels. The weighted pair-group\nmethod using centroid (WPGMC) was chosen for\
    \ its\nability to create homogeneous groups that could be more\neasily interpreted,\
    \ while the RNN was selected because\nit possesses internal memory and is, therefore,\
    \ able to\ncapture complex, non-linear relationships in time series\ndata. This\
    \ ability is particularly important to uncover\nthe patterns of wear and tear\
    \ that occur in industrial\nequipment.\nIn [74] a random forest was used to perform\
    \ fault\ndetection due to its robustness when handling numerical\ndata and real-world\
    \ problems. Nonetheless, to improve the\nmodel’s performance, DBSCAN was first\
    \ used to detect\noutliers that might represent noise in the sensor data. This\n\
    method improved the random forest’s accuracy by 1.462%\nand further experiments\
    \ demonstrated that using DBSCAN\nto detect and remove outliers improved the accuracy\
    \ of other\nmodels as well.\n1 3\nM. Fernandes et al.\n14260\nTable 15 Advantages\
    \ and limitations of the latent variable models employed for mechanical fault\
    \ detection and fault prognosis\nPublication\nAdvantages\nLimitations\n[47]\n\
    Algorithm: suitable to handle data generated from\nmultimodal distributions.\n\
    Algorithm: assumes the data was generated from\na mixture of finite Gaussian distributions.\n\
    [65]\nProposed approach: real-time analytics; scalable\ndistributed implementation.\n\
    Not identified\n[54]\nAlgorithm: cluster centers can adapt to new data;\ncapable\
    \ of classifying data despite noise and\noutliers.\nNot identified\nProposed approach:\
    \ capable of gaining a deep\nunderstanding\nabout\nthe\nequipment’s\nperfor-\n\
    mance.\n[64]\nAlgorithm: interpretability; suitable for small\nsample size and\
    \ high-dimensional data.\nNot identified\nProposed approach: Produces stable and\
    \ consistent results.\n[60]\nAlgorithm: high computational efficiency; greater\n\
    adaptability; robust to noise; uncovers patterns in\nraw time series data.\nNot\
    \ identified\nProposed approach: superior periodic impulse extraction.\n[62]\n\
    Algorithm: can handle raw multi-dimensional\ntime series; solves the problem of\
    \ empty cluster\ncreation.\nAlgorithm: the time-series must be synchronous\nThe\
    \ approach proposed in [58] constructs a health index\nby taking advantage of\
    \ an autoencoder’s ability to learn the\nrelationship between the input data variables.\
    \ Simple linear\nregression was subsequently used to predict future values\nof\
    \ the health index and calculate the RUL. The proposed\nmethodology is capable\
    \ of learning from unlabeled data,\nand it was demonstrated that it can be applied\
    \ in different\ndomains. However, it is precisely because run-to-failure\ndata\
    \ wasn’t available that the anomaly threshold had to be\ndefined somewhat arbitrarily.\
    \ Additionally, the prediction\naccuracy could be improved by using algorithms\
    \ more\nsophisticated than simple linear regression.\nThe methodology proposed\
    \ in [75] combines several\nlearning models to perform fault prediction in a press\n\
    module. An important concern when developing the fault\nprediction method was\
    \ its interpretability, which is why\nthe authors opted for an association rules\
    \ approach.\nAdditionally, the proposed approach requires few tuning of\nparameters\
    \ and is generic enough to be applied to other\ntypes of sensor data.\n3.5.4 Latent\
    \ variable models\nIn the study presented in [47], the authors identified\nmultimodal\
    \ distributions when plotting the data. As a non-\nparametric method of density\
    \ estimation, a GMM represents\nan appropriate choice for this kind of problem.\
    \ However,\nthis type of model has the disadvantage of assuming the data\nis generated\
    \ from a mixture of finite Gaussian distributions\nof unknown parameters.\nIn\
    \ [65], Yu et al. developed a fault detection system\nusing a distributed version\
    \ of PCA. The selection of PCA\ntook into consideration its real-time analytics\
    \ ability when\nintegrated with cloud computing, as well as the scalability\n\
    of the distributed implementation. PCA was also a natural\nchoice since labeled\
    \ data was unavailable.\nIn [54], the authors opted for a cognitive analytics-\n\
    based approach in order to gain a deeper understanding of\nhow an industrial robot\
    \ arm performed. Unlike what the\nauthors identified as traditional data analytics\
    \ frameworks,\nthe proposed framework merges the information from the\ndifferent\
    \ data sources and analyses the correlation between\nthe data features to understand\
    \ how the robot arm operates\nunder normal circumstances. K-means was used due\
    \ to its\nability to accurately cluster the data even in the presence\nof noise,\
    \ but also because the cluster centres can be\ndynamically adapted when new data\
    \ arrives.\nPSLR was used in [64] because it is theoretically\nadequate to handle\
    \ high-dimensional data and small sample\nsizes. PSLR was also chosen due to its\
    \ explanatory power.\nUsing correlation plots it’s possible to determine the\n\
    contribution of each variable to the prediction result. In\naddition, PSLR produces\
    \ results that are stable, consistent,\nand can be easily maintained.\nThe authors\
    \ of [60] took advantage of K-SVD’s robust-\nness to noise and its ability to\
    \ capture the characteristic\n1 3\nMachine learning techniques applied to mechanical\
    \ fault diagnosis...\n14261\nTable 16 Advantages and limitations of other algorithms\
    \ and methods employed for mechanical fault detection and fault prognosis\nCategory\n\
    Publication\nAdvantages\nLimitations\nDynamic Bayes Networks\n[42]\nAlgorithm:\
    \ suitable to model time series data; can\ndetect long-term degradation; can handle\
    \ dynamic\nfeatures in an unsupervised way.\nAlgorithm:\nperformance\ndecreases\n\
    when\ntraining and test data don’t share the same\ndistribution.\nProposed approach:\
    \ handles data with asyn-\nchronous sampling rates; doesn’t require compre-\n\
    hensive domain knowledge.\n[63]\nAlgorithm: capable of integrating data from\n\
    heterogeneous sources; capable of incorporating\ninformation about uncertainty;\
    \ able to rapidly\nadapt to changes; short execution time; low\nmemory requirements;\
    \ high performance.\nNot identified\nPartition-based algorithms\n[69]\nAlgorithm:\
    \ appropriate for online learning; low-\nmemory and processing requirements; compre-\n\
    hensible results.\nAlgorithm: Sensitive to the input parameters.\nRule-based models\n\
    [49]\nAlgorithm: allows for quantification of the conse-\nquent item in closed\
    \ intervals; incorporates online\npruning of the generated rules within the search\n\
    process.\nNot identified\nProposed approach: improvement of the RUL\nestimates;\
    \ reduced error rates in the test set.\n[51]\nAlgorithm: appropriate for online\
    \ learning; detects\ndependencies between variables.\nNot identified\nProposed\
    \ approach: provides valuable information\nto identify the origins of failures.\n\
    components hidden in raw signals to denoise the original\nvibration signal. However,\
    \ prior to doing so the K-SVD was\nimproved (IKSVD) to make it significantly more\
    \ efficient\nand adaptable. IKSVD in combination with fast spectral\ncorrelation\
    \ (FSC) demonstrated to be superior to traditional\napproaches when it comes to\
    \ extract periodic impulses from\nvibration data.\nThe algorithm proposed in [62]\
    \ is based on k-means\nbut employs a generalized notion of the Euclidean distance\n\
    to handle multi-dimensional time-series and also addresses\nthe issues k-means\
    \ has with empty clusters. Moreover, the\nalgorithm is capable of handling raw\
    \ time series without\nneeding any transformations such as the Fourier transform,\n\
    or the wavelet transform. Because of the generalized\ndistance defined for the\
    \ algorithm, it is necessary for the\ntime series data to be synchronous but this\
    \ can be achieved\nwith adequate data pre-processing.\n3.5.5 Other approaches\n\
    The authors of [42] used a HMM since these models\nassume that a system’s current\
    \ hidden state is influenced\nby its previous hidden state. This means a HMM is\
    \ an\nappropriate model for time series data and can be used to\ndetect long-term\
    \ degradation. The HMM is also capable\nof handling dynamic features in an unsupervised\
    \ way. In\naddition, the proposed approach, which combines a HMM\nwith sliding\
    \ windows and a genetic algorithm, can handle\ndata with asynchronous sampling\
    \ rates and doesn’t require\ncomprehensive domain knowledge. Nonetheless, since\
    \ the\nparameters of the HMM depend on the feature values of\nthe production cycles,\
    \ when the contamination probability\ndistribution of a production cycle differs\
    \ substantially from\nthe probability distributions of the production cycles used\n\
    to train the HMM the model’s performance decreases. This\ncan be remedied by taking\
    \ into consideration the advice of\nthe maintenance experts when choosing production\
    \ cycles\nto train the HMM.\nThe R4RE algorithm proposed in [49] is an improvement\n\
    of the QARMA framework. As such, like QARMA, R4RE\nis fully distributed and guarantees\
    \ that all the resulting rules\nare meaningful and meet the interestingness criteria\
    \ defined\nby the user. The R4RE algorithm surpasses QARMA by\novercoming two\
    \ of its important limitations, namely it\nallows for quantification of the consequent\
    \ item in closed\nintervals and incorporates online pruning of the generated\n\
    rules within the search process. In the study presented in\n[49], these developments\
    \ resulted in the improvement of the\nRUL estimates and reduced the error rates\
    \ obtained in the\ntest set.\n1 3\nM. Fernandes et al.\n14262\nThe rule-based\
    \ method (XCS) presented in [51] has a\n“covering” mechanism that enables the\
    \ recalibration of the\nrule set for unseen data without needing to re-train and\n\
    re-test the whole model, making it appropriate for online\nlearning. Since the\
    \ proposed model is also suitable to detect\ndependencies between variables and\
    \ to recognize different\nfailure patterns, the rules generated by the XCS can\
    \ provide\nvaluable information to identify the origins of failures.\nIn [63],\
    \ the choice of a DBF to predict the degradation of\nmachinery took into account\
    \ the difficulties of implement-\ning a predictive maintenance system. DBFs are\
    \ well suited\nto predict faults in industrial settings due to their ability\n\
    to integrate data from heterogeneous sources, to incorpo-\nrate information about\
    \ uncertainty and to rapidly adapt\nto changes. Moreover, these models have a\
    \ short execu-\ntion time, low memory requirements and high performance,\nwhich\
    \ are key properties of industrial systems.\nIn [69], MCOD was chosen because\
    \ of its low-memory\nand processing requirements, which make it ideal for\nprocessing\
    \ streaming data, as well as for producing results\nthat are easy to understand.\
    \ This algorithm has the downside\nof being sensitive to the input parameters,\
    \ which can\ninfluence the number of outlier reports.\n3.6 RQ5: Which of those\
    \ algorithms and methods\nare used for data stream learning?\nOnly the studies\
    \ presented in [55] and [69] proposed\nmethods for detecting faults directly from\
    \ real-time data and\napplied stream learning techniques.\nThe model proposed\
    \ in [55], named SERMON, consists\nof two RNNs that work in a cooperative manner\
    \ to\nobtain better results in terms of modelling the temporal\ndependency present\
    \ in streaming data, and the ability to\nself-evolve allows them to adapt to the\
    \ changes (drifts) that\ncharacterize non-stationary data. This model is described\
    \ in\nmore detail in Section 4.3.\nThe algorithm used in [69], MCOD, is a state-of-the-\n\
    art clustering algorithm developed for outlier detection in\ndata streams that\
    \ is applied using a sliding window over\nthe most recent data [82]. As such,\
    \ some parameters that\naffect the functionality of the algorithm and how it is\
    \ used\nwith streaming data must be defined. Parameters R and\nk define, respectively,\
    \ the radius of the neighborhood and\nthe minimum number of neighbors that must\
    \ exist inside\nthat radius for a point to be considered an inlier. The\nwindow\
    \ size W constrains the amount of data that will be\nprocessed at each step, either\
    \ as a time interval or as the\nnumber of datapoints, and the slide size S determines\
    \ the\nspeed/length of movement of the window. MCOD performs\ndistance-based outlier\
    \ detection, that is, a given object is\nconsidered an outlier if it has less\
    \ than k neighbors inside\nradius R.\nAsides from these studies, it is mentioned\
    \ in studies [49,\n51] and [39] that the proposed approaches are suitable or\n\
    can potentially be used for online learning, but in none of\nthem is that actually\
    \ performed and described.\n4 Discussion\nThe results presented in Section 3 will\
    \ be discussed in\nmore detail in this section with the purpose of identifying\n\
    interesting trends and ideas. This section also aims to\nprovide an overview of\
    \ the challenges faced when using\nmachine learning methods to detect mechanical\
    \ faults and\npredict faults in real manufacturing scenarios and consider\nhow\
    \ future research efforts might address them.\n4.1 RQ1: In which publication venues\
    \ are studies\nabout the use of machine learning for mechanical\nfault detection\
    \ and fault prognosis\nin manufacturing equipment published?\nAs presented in\
    \ Section 3.2, studies about the topic of\ninterest have been published in a variety\
    \ of conferences and\njournals, ranging from journals about operations research\n\
    & management science to multidisciplinary conferences.\nWhile the journals considered\
    \ in this review are all peer-\nreviewed journals with JCR Impact Factor (exclusion\n\
    criterion 3), 58.8% of which are ranked Q1 [31, 59–61,\n63–67, 73, 74], the same\
    \ quality verification could not\nbe performed for conferences since there is\
    \ no ranking\nsystem that evaluates the quality of conferences across\ndifferent\
    \ scientific fields. However, it was observed that\nof the eleven computer science\
    \ studies published in\nconferences, six were published in conferences ranked\
    \ by\nthe Computing Research and Education Association of\nAustralasia (CORE).\
    \ Two in conferences ranked C [41, 46],\nthree in conferences ranked A [51, 69,\
    \ 70] and one in a\nconference ranked A* [55].\nAs can be observed in Fig. 7,\
    \ the number of publications\nin conferences decreased in 2017 in comparison with\
    \ the\nprevious year but increased considerably between 2017 and\n2019. However,\
    \ the number of publications in conferences\ndecreased again in 2020 and there\
    \ were no conference\npublications in 2021 (until October). On the other hand,\n\
    while no study was published in journals before 2017, the\nnumber of journal publications\
    \ has been increasing steadily\nsince 2018 and the number of publications in 2021\
    \ has\nalready equalled the number of studies published in 2020\n(as of October\
    \ 2021).\nFigure 8 shows the distribution of publications in\nconferences and\
    \ journals for the most prolific countries.\nWhile China published considerably\
    \ more in journals\nthan in conference proceedings, the opposite was true\n1 3\n\
    Machine learning techniques applied to mechanical fault diagnosis...\n14263\n\
    Fig. 7 Publications in\nconferences and journals across\nthe years\nfor Germany\
    \ and Greece. However, since most countries\ncontemplated in this review only\
    \ published one or two\nstudies it is difficult to discern which type of venue\
    \ is\nfavored.\n4.2 RQ2: In which scientiﬁc ﬁelds has the use\nof machine learning\
    \ for mechanical fault detection\nand fault prognosis in manufacturing equipment\n\
    been researched?\nIt was shown in Section 3.3 that 21 of the 44 selected\nstudies\
    \ consist in contributions from the computer science\ncommunity. This can be attributed\
    \ to this review’s focus\non machine learning, a discipline that arose from the\n\
    intersection of computer science and statistics and is seen\nas a major branch\
    \ of artificial intelligence [83, 84]. In\naddition, the presence of eleven engineering\
    \ studies and six\nmultidisciplinary studies is in line with the nature of fault\n\
    detection and prognosis, which involves knowledge from\ndifferent areas of engineering\
    \ and computer science.\nA more in-depth analysis reveals about as many\ncomputer\
    \ science studies were published in conferences\nas in journals (Table 17). This\
    \ can be attributed to the\nfact that, while most scientific fields prefer to\
    \ publish in\nhigh quality journals, the computer science community\ntypically\
    \ favors publishing in prestigious conferences [85].\nFig. 8 Publications in\n\
    conferences and journals for the\ntop 3 countries\n1 3\nM. Fernandes et al.\n\
    14264\nTable 17 Publication per scientific field and venue type\nScientific Field\n\
    Conference\nJournal\nComputer Science\n11\n10\nMultidisciplinary\n5\n1\nEngineering\n\
    1\n10\nAutomation & Control Systems\n2\n0\nInstruments & Instrumentation\n0\n\
    2\nOperations Research &\nManagement Science\n0\n1\nWireless Communications,\n\
    Networking and Signal\nProcessing\n1\n0\nThis aspect is further supported by the\
    \ fact that about 44%\nof the conferences where computer science studies were\n\
    published have a CORE ranking of A or A*.\nMost multidisciplinary studies and\
    \ all the studies from\nthe fields of automation & control systems and wireless\n\
    communications, networking, and signal processing were\npublished in conferences.\
    \ On the contrary, the majority of\nengineering studies were published in journals,\
    \ as were all\nof the studies from the remaining scientific fields.\nFigure 9\
    \ shows the percentages of studies published per\nyear by the top three scientific\
    \ fields. Computer science\nstudies were published almost every year between 2016\
    \ and\n2021, except for 2017 when all the studies originated from\nthe field of\
    \ engineering. Although the largest proportion\nof publications between 2018 and\
    \ 2020 came from the\ncomputer science community, the number of publications\n\
    from this field has been decreasing since 2019. On the\ncontrary, the number of\
    \ engineering publications has been\nincreasing since 2019 and, until October\
    \ 2021, there were\nmore publications from the field of engineering in 2021 than\n\
    from computer science. The number of multidisciplinary\nstudies published since\
    \ 2018 has been very similar, with\nno notable increase or decrease in the number\
    \ of yearly\npublications.\nAs seen in Section 3.2, several studies were published\
    \ in\nvenues affiliated with the IEEE. This might be explained by\nthe fact that\
    \ 86.4% of the 44 selected publications consist in\ncomputer science, engineering\
    \ or multidisciplinary studies,\nwhich are some of the areas of focus of that\
    \ organization\n[86].\n4.3 RQ3: What machine learning algorithms\nand methods\
    \ are currently employed for mechanical\nfault detection and fault prognosis\n\
    in manufacturing equipment?\nIn this subsection, the different algorithms and\
    \ techniques\npresented in Section 3 are examined in more detail.\nThe descriptions\
    \ of the studies are organized according\nto the categories identified in Section\
    \ 3.4: decision tree\nmodels, artificial neural networks, hybrid models, latent\n\
    variable models and other approaches. Additionally, within\neach category, the\
    \ studies are organized according to\nsubcategories (where pertinent) and year\
    \ of publication.\n4.3.1 Decision Trees\nMachine learning algorithms and methods\
    \ belonging to\nthe category of decision trees were some of the most\ncommonly\
    \ used for the tasks of detecting mechanical\nfaults and predicting faults in\
    \ manufacturing equipment in\nindustrial environments. Ensemble methods, in particular,\n\
    were widely used, as exemplified by the application of\nrandom forest models in\
    \ 11.4% of the studies under\nconsideration. In 2018, Amihai et al. [44] derived\
    \ key asset\nhealth condition indices from raw vibration data and used a\nrandom\
    \ forest model to forecast these metrics up to seven\ndays ahead. A comparison\
    \ of RMSE values for different\nlook-ahead times demonstrated the random forest\
    \ always\nperformed better than a persistence model. In that same\nyear, the authors\
    \ of [46] tested different machine learning\nalgorithms to predict equipment faults\
    \ using process data\nfrom anode manufacturing machines. The best results were\n\
    Fig. 9 Proportion of studies\npublished per year by the top 3\nscientific fields\n\
    1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n14265\n\
    obtained with a random forest model (accuracy = 99.2%;\nmax depth = 5-10) and\
    \ with a decision tree model (accuracy\n= 99.2%; max depth = 5), showing it was\
    \ possible to predict\nfaults 5 to 10 minutes before their occurrence. Paolanti\
    \ et al.\n[43] implemented in 2018 a predictive maintenance system\nto predict\
    \ the health status of the spindle’s rotor of a CNC\nwoodworking machine. To achieve\
    \ this, the authors trained\na random forest model on drive and vibration data\
    \ collected\nfrom the machine to classify its condition into one of four\nclasses,\
    \ having obtained an average accuracy of 92%. In\na study published in 2019,Binding2019,\
    \ operational data\nand downtime data from a large central imprint printing\n\
    press were used to predict failure events with a prediction\nhorizon of 30 minutes.\
    \ After analyzing the data, the authors\nfocused on the prediction of mechanical\
    \ failures in print\nunits, such as leakages and deterioration of components.\n\
    To achieve this, different classification models were trained\nand evaluated,\
    \ namely logistic regression, random forest\nand extreme gradient boosted trees\
    \ (XGBoost). Considering\nthe F1-score for different decision thresholds, the\
    \ random\nforest and XGBoost models yielded the best results, but\nthe authors\
    \ of the study chose to use the random forest\nmodel in the implementation of\
    \ the predictive maintenance\nsystem. The predictive algorithms were also used\
    \ to help\nidentify print unit failures in the downtime data, in a manner\nsimilar\
    \ to iterative semi-supervised labelling schemes. Also\nin 2019, the study described\
    \ in,Aremu2019 used Kullback-\nLeibler divergence to construct a health indicator\
    \ (HI) of\nmulti-sensor systems to represent a system’s deviation from\nits normal\
    \ state. The usefulness of the HI for prognosis\npurposes was evaluated by comparing\
    \ the RUL predictions\nfor a semiconductor manufacturing equipment using the\n\
    original data and the HI data. The results obtained using\nrandom forest regression\
    \ and Gaussian process regression\ndemonstrated the constructed HI always provided\
    \ more\naccurate predictions, with the random forest model outdoing\nthe Gaussian\
    \ process in terms of RMSE (20.34 vs 24.7) and\nMAE (26.02 vs 28.63).\nOther examples\
    \ of decision tree ensembles include the\nwork presented in [72] in 2017, where\
    \ the authors described\na procedure for fault prediction that leveraged cyclic\n\
    manufacturing process data from similar work systems\nto improve the accuracy\
    \ of the fault detection model.\nSince faulty cycles were rare, machine-to-machine\
    \ (M2M)\ncommunication was used to acquire data from five injection\nmoulding\
    \ machines, thus increasing the amount of available\nfault data. To assess the\
    \ effect of using data from several\nmachines of the same type on model performance,\
    \ three\nAdaBoost models, with decision stumps as estimators, were\nfitted to\
    \ the data and evaluated using a machine-to-machine\nmethodology (M2M): 1) a model\
    \ trained and tested with\ndata from all the work systems, 2) a model trained\
    \ and\ntested with data from a single work system and 3) a\nmodel trained with\
    \ data from all the work systems except\none whose data was used exclusively to\
    \ test the model.\nThrough a series of experiments, the authors demonstrated\n\
    that the best performing model was the one trained and\ntested on data from all\
    \ the machines (F1-score = 0.082),\nwhile the performance of the model tested\
    \ with data that\nwas not used for training was considerably worse (F1-score\n\
    = 0.03). It should be noted that the performance of the\nproposed method depends\
    \ on the degree of imbalance of\nthe data. The F1-score results were low because\
    \ the ratio\nof faulty to normal cycles (1:1484) was very low, but the\nmodel\
    \ trained with data from all the machines performed\nconsiderably better than\
    \ random guessing. Additionally,\nthese results show that sharing data from similar\
    \ work\nsystems can improve the fault detection accuracy if data\nfrom the system\
    \ of interest is also used to train the model.\nIn 2020, the authors of [76] proposed\
    \ a predictive tool for a\ncyber physical production system that uses GBDT to\
    \ predict\nequipment failures in a CNC milling machine. Data was\ncollected from\
    \ the machine’s central control system, as well\nas from external sensors used\
    \ to monitor parameters such as\nvibration severity and amplitude. Rolling summary\
    \ statistics\nof these variables, within 10, 30 and 60 second windows,\nwere also\
    \ added to the historical data. Additionally,\ninformation about the machine’s\
    \ operation mode, operator\ndoor mode (open or close) and program block number\
    \ was\nused to infer when failure events occurred and label the\ndata accordingly.\
    \ GBDT was used initially to determine\nthe relative importance of features and\
    \ select the most\nappropriate time lag (60 sec) to use as input to dynamic\n\
    principal component analysis (DPCA). DPCA was used to\neliminate the autocorrelations\
    \ present in the data and extract\nthe principal components from the normalised\
    \ 60-second\nlag feature space. This data was then fed to the GDBT\nto learn a\
    \ binary classification model that predicted the\nprobability of a production\
    \ stop. Using the AUC score, the\nauthors showed the predictive tool has an accuracy\
    \ of 73%\non unseen data.\nTree ensembles were also used for survival analysis\
    \ [71]\nand anomaly detection [67]. In 2019, the study presented\nin [71] explored\
    \ the rich data provided by the plant\nfloor automation and information system\
    \ (PFS) of a real-\nworld automotive manufacturing line to learn complex\nand\
    \ dynamic machine breakdown patterns. The authors\nproposed a manufacturing system-wide\
    \ balanced random\nforest (MBRSF) model, whereby a random survival forest\nwas\
    \ used to estimate a hazard function from balanced\nsystem-wide data with the\
    \ purpose of quantifying the\nlikelihood of breakdown events over time. Experiments\n\
    performed on 20 machines demonstrated the performance\nof the MBRSF was about\
    \ 90% better, in terms of the\nintergrated Briers score, than the performance\
    \ of other\nsurvival models. In a study published in 2020, Kolokas\n1 3\nM. Fernandes\
    \ et al.\n14266\net al. [67] presented a methodology for fault prognosis\nthat\
    \ used an anomaly detection technique to predict faults\nfrom process data, but\
    \ approached the problem as a case of\nbinary classification. An isolation forest\
    \ was used to detect\nanomalies in real industrial data, related to aluminum and\n\
    plastic production, and correlate them with upcoming faults\naccording to a predefined\
    \ forecasting horizon. The model’s\nperformance was assessed using the Matthews\
    \ correlation\ncoefficient (MCC) to measure the correlation between the\nanomalies\
    \ detected by the IF and the data’s target labels,\nhaving obtained results up\
    \ to MCC = 0.73.\nCART models are also present, being the models of\nchoice in\
    \ 6.8% of the selected studies. In 2016, Linard\nand Bueno [41] described a new\
    \ method for dynamic\nmaintenance scheduling of large-scale printers. Labelled\n\
    data obtained from printing test pages was used to train\na decision tree that\
    \ was deployed in real-time to predict\nwhether failures would occur or not in\
    \ the nozzles of the\nprinters. The output of the decision tree was then used\
    \ to\nupdate an automatic maintenance schedule defined by a\ntimed automaton.\
    \ The authors compared the performance\nof different classifiers but decided to\
    \ use a decision tree\nnot only because it provided the best results (precision\
    \ =\n0.788; recall = 0.631), but also due to its nature as a white\nbox model\
    \ since interpretability is particularly important in\nindustrial contexts. In\
    \ 2019, decision tree models were also\nused in [68] to estimate the failures\
    \ of cold forging machines\nin an industrial company of the automotive industry.\
    \ The\ndecision tree model provided better results than the other\nevaluated algorithms,\
    \ successfully predicting failures that\noccurred unexpectedly in the factory\
    \ between 2014 and\n2017 with an accuracy of 77%.\n4.3.2 Artiﬁcial neural networks\n\
    Publications\nwhere\nartificial\nneural\nnetworks\nwere\nemployed for mechanical\
    \ fault detection and fault prog-\nnosis account for more than a quarter of the\
    \ studies under\nconsideration (27.3%).\nIn 2016, Qing et al. [40] proposed a\
    \ BPNN optimized by\na multilevel genetic algorithm (MGA-BPNN) to predict the\n\
    RUL of segment bearings in continuous casting equipment.\nThe proposed model aimed\
    \ to enhance the nonlinear\nlearning and generalization abilities of the BPNN\
    \ and thus\nobtain an improved forecasting model. Experimental results\nshowed\
    \ the MGA-BPNN model was better at predicting the\nRUL than either a BPNN or a\
    \ BPNN optimized by a genetic\nalgorithm and could be used as an effective means\
    \ of fault\nprognosis.\nIn 2017, the authors of [57] describe a system\nframework\
    \ for predictive maintenance based on industry 4.0\nconcepts. The system performed\
    \ fault prognosis using an\nartificial neural network to uncover the hidden patterns\
    \ of\ndegradation that led to a backlash error in a CNC machine\ncenter. After\
    \ the model was trained using historical data,\nthe artificial neural network\
    \ was deployed in real-time to\nmake predictions based on condition monitoring\
    \ data. These\npredictions were used by a decision support system to\nformulate\
    \ a maintenance strategy.\nLuo et al. [31] proposed, in 2018, a method for\nearly\
    \ fault detection in CNC machine tools under time-\nvarying conditions that relied\
    \ on deep learning to identify\nimpulse responses from vibration data. The deep\
    \ learning\nmodel consisted of a layer of stacked sparse autoencoders\n(SSAE),\
    \ meant to reduce the dimensionality of the input\ndata, and a back-propagation\
    \ neural network (BPNN)\nlayer that classified the vibration signals into impulse\
    \ and\nnon-impulse responses (accuracy = 97.3%). The impulse\nresponses selected\
    \ by the deep learning model were used to\nidentify the dynamic properties of\
    \ the machine tool, which\nwere then used to develop a health index that reflected\
    \ the\nequipment’s gradual deterioration process.\nIn the study presented in [70]\
    \ in 2019, an LSTM\nnetwork was built to predict faults in industrial ovens\n\
    from sensor data and log events. The network was trained\nusing consecutive time\
    \ series and was used to predict\nthe five subsequent future events, i.e., it\
    \ predicted events\n25 minutes into the future. Considering the data used to\n\
    train the network was strongly imbalanced, its performance\nwas assessed using\
    \ the\nMatthews correlation coefficient\n(0.691), recall (0.790) and F1-score\
    \ (0.803) as evaluation\nmetrics. The results showed the values of the evaluation\n\
    metrics decreased the further into the future a prediction\nwas, but the network’s\
    \ performance was acceptable for all\npredictions.\nAlso in 2019, the fault diagnosis\
    \ method proposed in\n[81] took advantage of digital twin technology to transfer\n\
    fault information from the virtual entity to its physical\ncounterpart. The digital\
    \ twin consisted in a high-fidelity\ndynamic virtual model of a car body-side\
    \ production line\nthat simulated the entire product life cycle. This simulation\n\
    data was used to build a diagnosis model that combined a\nSSAE layer to perform\
    \ feature extraction from unsupervised\ndata and a softmax classifier that used\
    \ the extracted features\nas inputs and assigned probabilities to the class labels.\n\
    Subsequently, deep transfer learning was used to relay\nthe knowledge gained in\
    \ the virtual space to a new fault\ndiagnosis model built in the physical space.\
    \ Monitoring data\nfrom the physical entity was used to improve the model,\nand\
    \ an adaptation layer between the feature extraction and\nclassification layers\
    \ minimized the distance between the\ndata distributions from the virtual space\
    \ and the physical\nspace. The virtual and physical entities of this digital twin-\n\
    assisted fault diagnosis method cooperated with each other\nto provide accurate\
    \ fault predictions (average accuracy =\n97.96%) and adapt to new working conditions.\n\
    1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n14267\n\
    The study described in [78] introduced in 2020 a\npredictive maintenance framework\
    \ to detect and classify\nthe severity of mechanical faults in conveyor AC motors.\n\
    Principal component analysis was used to reduce the\ndimensionality of time series\
    \ data collected from the\nconveyor system to two channels, after which the data\n\
    was encoded into images using the Gramian angular\nfield method. The resulting\
    \ images were used to train a\nconvolutional neural network which outputted the\
    \ “fault\nseverity in the system”, i.e., based on the input images\nthe CNN classified\
    \ the motor’s state as “no fault”, “minor\nfault” or “critical fault”. To improve\
    \ the model’s accuracy\nwhen using more extensive networks, the authors added\n\
    the option to use a PReLU activation function instead of\nthe more common rectified\
    \ linear unit function (ReLU).\nThe proposed approach was compared with an SVM\
    \ and\na CNN that used ReLU as its activation function. As\nshown in the experimental\
    \ results, for small datasets the\nCNN’s performance was very similar using either\
    \ PReLU or\nReLU, with an accuracy of 100% in both cases. The SVM\nperformed considerably\
    \ worse, having obtained an accuracy\nof 55.2%.\nIn 2020, deep learning was also\
    \ used to perform fault\nprognosis from times series data in [77]. The authors\n\
    proposed a TensorFlow-enabled deep neural network to\nperform multiclass classification\
    \ of the condition of a small\ntrolley’s cylinder in an automobile production\
    \ line. The\nperformance of the proposed approach was compared to two\nother methods,\
    \ namely PCA and HMM. The TensorFlow-\nenabled DNN performed better in all of\
    \ the experiments,\nwith an average accuracy of 80% versus 63% for the HMM\nand\
    \ 50% for PCA. After training the DNN model offline\nusing historical data, it\
    \ was deployed in real-time to track\nthe degradation of the equipment.\nIn another\
    \ study presented in 2020 [56], a multilayer\nperceptron was used for fault prognosis\
    \ of industrial\npackaging robots. Due to the facility’s lack of IoT\ntechnology,\
    \ the data of interest, which consisted in failure\nnotifications and associated\
    \ information, was obtained from\nthe enterprise resource planning (ERP) system.\
    \ The MLP\nwas composed of eight input nodes, 20 hidden layer nodes\nand four\
    \ output nodes that indicated where and when a\nfuture failure would occur (accuracy\
    \ = 91%). The authors\nof the study also performed a component-based reliability\n\
    analysis whose results validated the MLP’s predictions\n(reliability = 75%).\n\
    Still in 2020, the method proposed by Das et al. combines\ntwo self-evolving recurrent\
    \ neural networks to detect\nmachine faults autonomously and in an online fashion\
    \ [55].\nThe model, named SERMON, consists of two components:\nSERN, a Skip-connected\
    \ Evolving Recurrent Network, and\nMERN, a Multilayer Evolving Recurrent Network.\
    \ The two\nnetworks work in a cooperative manner to obtain better\nresults in\
    \ terms of modelling the temporal dependency\npresent in streaming data, and the\
    \ ability to self-evolve\nallows them to adapt to the changes (drifts) that characterize\n\
    non-stationary data. SERMON also includes a mapping\nunit (MU) that suggests possible\
    \ data labels in the event\nof a delay in the arrival of the true label. SERMON\
    \ was\nvalidated using data from a real-world industrial case study,\ni.e., to\
    \ predict the condition of a 3D printing nozzle as either\n“healthy” or “clogged”\
    \ based on nozzle shape features\nsuch as symmetry shape feature and slope feature,\
    \ among\nothers. The performance of SERMON was compared with\nseven other models\
    \ in terms of classification rate, parameter\ncount, hidden unit count and execution\
    \ time and considered\nscenarios of no-delay, finite delay, and infinite delay\
    \ in\nreceiving labels. The classification rate and hidden unit\ncount of SERMON\
    \ was better than all the other models\nin all scenarios (average accuracy in\
    \ a no-delay scenario:\n72.08%; average accuracy in a finite/infinite delay scenario:\n\
    69.39%) and while one model (SkipE-RNN) obtained better\nresults in terms of parameter\
    \ count and execution time,\nits accuracy rate was always more than 10% lower\
    \ than\nSERMON’s.\nIn 2021, Bampoula et al. [73] proposed an approach\nfor fault\
    \ detection and prediction based on LSTM-\nautoencoders. A prototype was tested\
    \ in a steel production\nfactory using three months of historical data obtained\n\
    from a rolling mill machine and focused on the analysis\nof the surface temperatures\
    \ and hydraulic forces of the\nmachine’s two cylinders. The condition monitoring\
    \ data\nobtained from the machine was segmented into time-series\nsequences according\
    \ to three possible equipment health\nstates, namely “good”, “bad” and “intermediate”\
    \ operating\nconditions. Subsequently, each LSTM-autoencoder was\ntrained using\
    \ a dataset consisting only of time-series\nsequences corresponding to a given\
    \ state. Afterwards, new\ndata was fed to each LSTM-autoencoder and classified\n\
    according to the highest accuracy obtained. That is, if the\nLSTM-autoencoder\
    \ that obtained the highest accuracy was\nthe one trained only with healthy data,\
    \ the new data was\nclassified as “good”. Finally, the authors considered the\n\
    fatigue rate of the machine was constant and estimated\nthe remaining useful life\
    \ based on the classification\naccuracy. Performance results obtained with the\
    \ prototype\ndemonstrated that unnecessary preventive maintenance\nactions could\
    \ be reduced, therefore decreasing the cost of\nmaintenance operations.\nIn 2021,\
    \ a PdM methodology was proposed in [79]\nthat uses an LSTM-GAN to monitor the\
    \ health state\nof machines, as well as predict when and in which\nmachine a fault\
    \ will occur. The proposed PdM methodology\nalso includes a maintenance decision\
    \ model that suggests\nmaintenance operations according to the output of the\n\
    prediction model. The methodology was tested in a\n1 3\nM. Fernandes et al.\n\
    14268\nmanufacturing factory located in China, where sensing\ndevices monitored\
    \ eight different machines (two automated\nguided vehicles, two robots, two milling\
    \ machines and\ntwo turning machines) for over two years. Initially, the\nhealth\
    \ state of the manufacturing system was predicted as\nbeing in one of four states:\
    \ “good”, “watching”, “warning”\nand “fault”. If a machine was in “good” condition,\
    \ no\nmaintenance was required. If it was in either “watching”\nor “warning” states,\
    \ a minor maintenance strategy would\nbe implemented. In case of a “fault” state,\
    \ the fault\ntype and time of occurrence would be predicted by the\nfault prediction\
    \ model and a major maintenance strategy\nwould be implemented. Both the state\
    \ prediction and\nthe fault prediction abilities of the LSTM-GAN were\ncompared\
    \ with the results obtained using three other types\nof neural networks. The comparison\
    \ analysis revealed the\nLSTM-GAN outperformed the other networks in both state\n\
    prediction (average accuracy = 98.87%) and fault prediction\n(average accuracy\
    \ = 98.92%).\nIn an additional study published in 2021 [66], the health\nmodel\
    \ of a predictive maintenance system that takes into\naccount time-varying operational\
    \ conditions and allows for\nthe subsequent scheduling of maintenance and production\n\
    was introduced. The system uses condition monitoring\nsensor data, production\
    \ data and future production orders\nto create a production schedule that incorporates\
    \ the\nnecessary maintenance actions. The proposed framework\nwas validated in\
    \ a real industrial use case with data\nfrom a multifunctional machining centre\
    \ used to produce\nautomotive components. The machine’s condition was\nassessed\
    \ using two CVAE models: 1) HA-CVAE that takes\nas input condition monitoring\
    \ data and the corresponding\noperating regime information and derives a set of\
    \ health\nindexes that model the underlying trend of degradation\nunder time-varying\
    \ operational conditions, and 2) DS-\nCVAE, a data simulator used to generate\
    \ realistic sensor\ndata based on the conditional probability distribution\nlearned\
    \ from the training data for a specific operating\nregime and health state. The\
    \ estimation of the health index\nwas evaluated using metrics described in the\
    \ prognostics\nand health management (PHM) literature to assess the\ntrajectory\
    \ of the health index over time (e.g., monotonicity,\nconsistency). Validation\
    \ experiments demonstrated that the\nproposed method was not only capable of estimating\
    \ the\nmachine’s health under different operating conditions and\nin a scenario\
    \ where labelled data was scarce but was also\nable to predict the machine’s future\
    \ health and degradation\ncondition.\n4.3.3 Hybrid models\nHybrid models integrate\
    \ different machine learning models\nand techniques to solve problems that a single\
    \ model is not\ncapable of handling, or to obtain better performance. 18.2%\n\
    of the studies selected in this review made use of hybrid\nmodels, in some cases\
    \ to address the absence of labeled\ndata.\nIn 2018, Syafrudin et al. [74] proposed\
    \ and described\nthe implementation of a real-time monitoring system that\ncombined\
    \ IoT-based sensors, big data technology and a\nhybrid prediction model to predict\
    \ faults in manufacturing\nequipment. The system was tested for 8 months in\n\
    an automotive manufacturing company in Korea. In the\nproposed prediction model,\
    \ the DBSCAN algorithm was\nfirst used to detect outliers in the sensor data that\
    \ might\nrepresent noise introduced by problems in the sensing\ndevices or by\
    \ network connection issues. The detected\noutliers were removed from the dataset\
    \ before it was used\nto train a random forest model which was then deployed in\n\
    the monitoring system to perform fault prediction in real-\ntime. When compared\
    \ with other classification models, such\nas naive Bayes (accuracy = 93.57%),\
    \ logistic regression\n(accuracy = 97.95%) and multilayer perceptron (accuracy\n\
    = 96.78%), the DBSCAN + random forest hybrid model\nachieved better results (accuracy\
    \ = 100%). Furthermore,\nthe use of DBSCAN to remove noisy data improved\nthe\
    \ performance of the other classifiers as well, but the\nproposed model remained\
    \ the best performing one.\nIn the same year, Strauß et al. [45] proposed a predictive\n\
    maintenance approach that combined semi-supervised,\nunsupervised and supervised\
    \ learning techniques to detect\nand classify mechanical faults in a heavy lift\
    \ EMS at the\nBMW Group. Although fault data was available, the authors\ntook\
    \ into consideration the fact that this information is\noften scarce. As such,\
    \ the problem of fault detection was\ninitially approached using a semi-supervised\
    \ method to\nperform anomaly detection. Three different models were\nbuilt using\
    \ normal data exclusively but were evaluated\nusing a dataset containing both\
    \ normal and fault data to\nassess their ability to detect data points that diverged\
    \ from\n‘normality’. Since there were several types of faults and the\nsemi-supervised\
    \ models were unable to distinguish between\nthem, unsupervised models were used\
    \ to cluster the fault\ndata. By using semi-supervised and unsupervised models\n\
    together, the authors were able to create a dataset that\ncontained normal data,\
    \ as well as instances of three different\ntypes of failures. This data was then\
    \ used to train and\nevaluate eight supervised models, four of which had an F1-\n\
    score of more than 90%. Model deployment in the predictive\nmaintenance system\
    \ took into consideration not only each\nmodel’s performance but also computational\
    \ requirements\n- the final selected models were one-class SVM, K-means\nand random\
    \ forest.\nIn 2019, the study presented in [50] proposed a fault\nprognosis model\
    \ which combined an ARIMA model with a\nLSTM network to predict faults in a ball\
    \ bearing automatic\n1 3\nMachine learning techniques applied to mechanical fault\
    \ diagnosis...\n14269\nproduction line. The ARIMA model was used to forecast the\n\
    linear component of the time series (sensor data) collected\nfrom the production\
    \ line, whereas the LSTM forecasted the\nnonlinear component obtained from the\
    \ ARIMA model’s\nprediction. The final predicted value resulted from the\nsummation\
    \ of the linear component with the prediction\nerror of the nonlinear component\
    \ of the ARIMA prediction.\nExperimental verification demonstrated that the proposed\n\
    hybrid model performed better than either model by itself\n(MAE = 0.00425; RMSE\
    \ = 0.03584).\nAs described in the artificial neural networks subsection,\nin\
    \ 2019 Rousopoulou et al. [70] presented a solution for\npredictive maintenance\
    \ of the industrial ovens used by a\nmedical devices manufacturer. However, while\
    \ the authors\nselected an LSTM network to predict faults from condition\nmonitoring\
    \ sensor data and log events, they decided to\ncombine an outlier detection method\
    \ with a classifier\nto detect faults in unlabeled acoustic data. Three outlier\n\
    detection algorithms were tested and compared, namely\nDBSCAN, LOF and mean absolute\
    \ deviation (MAD), with\nDBSCAN yielding the best results, i.e., it detected the\
    \ most\noutliers. The detected outliers were marked as faulty, but\nsince this\
    \ class accounted for only 14% of the data, the\nsynthetic minority oversampling\
    \ technique (SMOTE) was\nused to increase the number of data points belonging\
    \ to the\nminority class and balance the dataset. This data was then\nused to\
    \ train an SVM capable of detecting new faults in live\naudio measurements with\
    \ an accuracy of 85% and an F1-\nscore of 0.86. A fault notification was issued\
    \ by the system\nif the model detected five consecutive faults.\nIn another study\
    \ published in [59], the development\nof a prognostic maintenance model in a context\
    \ where\nno labeled data existed was described. The study was\ncarried out at\
    \ a German automotive manufacturer to address\nthe situation where maintenance\
    \ of a milling tool was\nperformed subjectively by machine operators based on\n\
    visual inspections. Although no labels were available, the\nproposed method was\
    \ developed with the intention of\nuncovering latent information hidden in historic\
    \ data that\nincluded maintenance and production records, control data\nand sensor\
    \ data. After performing a thorough analysis of\nthe data, taking domain knowledge\
    \ into consideration, the\nauthors approached the problem from two orthogonally\n\
    related dimensions: 1) time dimension, that is, the time\nwhen a tool was replaced\
    \ and 2) condition dimension,\nreferring to information about damaged and undamaged\n\
    tools that was inferred from the available data. Based\non these dimensions, a\
    \ 4-field matrix was defined to\ndifferentiate between correct and incorrect tool\
    \ replacement\ndecisions. Clustering techniques were applied along both\ndimensions\
    \ to assign the data observations to the 4-\nfield matrix. The time dimension\
    \ was grouped using\nthe agglomerative hierarchical clustering algorithm named\n\
    weighted pair-group method using centroid (WPGMC),\nwith cluster one representing\
    \ tool replacements that were\nperformed at a late moment in the tool’s lifetime\
    \ and cluster\ntwo representing tool replacements that were performed\nearly.\
    \ When considering the condition dimension, time\nseries’ sequences were clustered\
    \ into two groups using\nthe MAD to measure the intensity of the sequences’\n\
    oscillations. Sequences with a lower MAD value, i.e.,\nweaker oscillations, were\
    \ assigned to cluster one, while\nthe remaining sequences were assigned to cluster\
    \ two. The\nresults obtained from clustering the data were orthogonally\nrelated\
    \ and, based on that, the data observations were\nassigned to each of the quadrants\
    \ of the 4-field matrix.\nSince time series’ sequences of “type 1” in the 4-field\n\
    matrix represented the replacement of an undamaged tool\nlate in its lifetime,\
    \ thus reflecting correct decisions made\nby the machine operators, this data\
    \ was used to train and\ntest a RNN to predict the tools’ RUL. The RNN model\n\
    was then used to predict the RUL of “type 3” observations\n(undamaged tools replaced\
    \ too early), showing that using\nthe prognostic model would have resulted in\
    \ an extension\nof the tools’ lifetimes for about one-third of these tool\nreplacements.\n\
    In 2020, Tran et al. [80] described a method to detect\ndrill faults from sound\
    \ data recorded from a drill machine\nat Valmet AB in Sweden. To detect abnormalities\
    \ in the\ndrilling machine, sound data was collected when the drill\nwas broken\
    \ and when it was operating normally. The\ndrill sounds were converted to images,\
    \ specifically mel\nspectrogram images and scalogram images, and features\nwere\
    \ extracted from these images using a pre-trained CNN\narchitecture – VGG19 architecture\
    \ trained on the ImageNet\ndataset. NCA was then used to select the most representative\n\
    features and reduce the dimensionality of the data.\nAfterwards, the features\
    \ obtained from the mel spectrogram\nimages were used to train several classifiers\
    \ based on KNN\nand SVM, whose performance was compared to select\nthe best model.\
    \ The best overall accuracy was obtained\nby the Medium Gaussian SVM and the Quadratic\
    \ SVM,\nbut since the purpose of this study was the detection of\nbroken drills,\
    \ the selected model was the Medium Gaussian\nSVM, which attained an accuracy\
    \ of 90.12% and a recall\nof 0.88 when classifying the broken sounds. A similar\n\
    procedure was followed using the scalogram images, but\nin this instance the best\
    \ performing classifier was the\nensemble subspace KNN with an accuracy of 80.25%.\n\
    These two approaches were compared with additional\ntechniques and the results\
    \ demonstrated that the proposed\nmethods performed considerably better at classifying\
    \ the\ndrill sound signals.\nIn 2021, the authors of [58] proposed a framework\n\
    based on autoencoders and simple linear regression to\nconstruct a health index\
    \ that was subsequently used to\n1 3\nM. Fernandes et al.\n14270\npredict the\
    \ RUL of industrial equipment. The proposed\nmethodology is applicable to situations\
    \ where no fault\nhistory data is available. To construct the health index, an\n\
    autoencoder is used to learn the normal structure of the\ndata. The health index\
    \ consists in the difference between\nthe input data and the reconstructed data\
    \ which is calculated\nacross all variables using the mean absolute error (MAE).\n\
    Subsequently, the authors use simple linear regression to\npredict the trend of\
    \ the health index as new data is fed into\nthe system. If the trend of the health\
    \ index increases and the\nslope parameter of the regression becomes large, it\
    \ means\nthe monitored equipment is displaying abnormal behaviour.\nThe RUL is\
    \ defined as the difference between the health\nindex at the current time and\
    \ the time at which a failure is\npredicted to occur. This methodology was applied\
    \ in two\nindustrial use-cases: a pump equipment and a robot arm. In\nthe first\
    \ case, data was collected from three different pumps\nand a small amount of failure\
    \ data was obtained for purposes\nof model validation. Experimental results demonstrated\
    \ the\nvalue of the health index rose before the occurrence of\na failure, which\
    \ was accurately predicted. For the robot\narm use case, vibration sensors were\
    \ attached to the edge\nof the arms of five different robots to collect data at\
    \ an\naverage rate of 1500 samples per day for periods of time\nbetween three\
    \ and ten months. Once again, the proposed\nmethod was capable of correctly predicting\
    \ the occurrence\nof faults. To verify the reliability of their proposal, the\n\
    authors run additional experiments to predict the RUL with\nan isolation forest.\
    \ A comparison of results based on the\nMAE and the root mean square error (RMSE)\
    \ demonstrated\nthe proposed method was better at predicting the RUL in all\n\
    the experiments undertaken using data from both the pumps\nand the robot arms.\n\
    Also published in 2021, the study presented in [75]\ndescribes a methodology for\
    \ fault detection and prediction\nin cold forming processes of a Phillips factory\
    \ in the\nNetherlands using GMM, the FP-Growth algorithm and\nCBA-CB. In this\
    \ study, information about the normal\noperating conditions of a press module\
    \ was collected for\nover a year. The data included: material batch, maintenance\n\
    logs and data from acoustic emission sensors. Using the\nmatrix profile - a data\
    \ structure for time series analysis -, two\nmeta-time series were obtained from\
    \ the acoustic emission\ndata, which were used for anomaly detection, and for\
    \ fault\nprediction using rule mining. For anomaly detection, the\nauthors opted\
    \ for a statistical approach since no labelled\ndata was available that could\
    \ guide the definition of the\nanomaly threshold. The matrix profile was also\
    \ used for\nfault prediction by first mining salient subsequences to find\ncommon\
    \ patterns. Subsequently, PCA was applied to the\nsalient subsequences and the\
    \ resulting principal components\nwere clustered using GMM. The acoustic emission\
    \ data\nwas then segmented into non-overlapping time windows\nand each pattern\
    \ within a window was labelled according\nto previously discovered clusters. Finally,\
    \ the acoustic\nemission data was integrated with the maintenance logs\nand FP-Growth\
    \ was used to mine association rules from\nthis data, which were then used to\
    \ build a classifier\nusing a modified version of the CBA-CB algorithm. The\n\
    results obtained with the fault prediction module were\ncompared with the performance\
    \ of a majority classifier,\nwhich obtained a high micro F1-score but was unable\
    \ to\npredict any events. The proposed method, on the other\nhand, was able to\
    \ predict faults related to three of the\nfour maintenance events of interest\
    \ with a micro F1-score\nof 0.632. It should be noted these events are extremely\n\
    infrequent, occurring less than 0.05% of the time.\n4.3.4 Latent variable models\n\
    Publications that employ latent variable models account for\n13.6% of the selected\
    \ studies.\nIn 2018, Amruthnath et al. [47] described a methodology\nfor mechanical\
    \ fault detection of a furnace fan using\nunsupervised learning. The authors began\
    \ by computing\na 99.9% confidence interval using Hotelling’s T-squared\nstatistic,\
    \ after which the data was clustered using a Gaussian\nmixture model fitted by\
    \ expectation-maximization. The\nclusters obtained using the GMM model were then\n\
    identified using the T-squared statistic and the maintenance\ncrew’s expert knowledge.\
    \ Using only unlabeled vibration\ndata, the proposed methodology was able to discover\n\
    a healthy state, a faulty state, and a reset state (fan\nsubstitution).\nThe study\
    \ presented in [65] in 2020 proposed a big\ndata architecture for predictive maintenance.\
    \ The fault\ndetection system used sensor data, such as temperature and\nvibration,\
    \ to predict failures in manufacturing equipment.\nSince the data was unlabeled,\
    \ anomalies in the equipment\nwere detected by a distributed version of PCA that\
    \ was\nimplemented using MapReduce. The output of the PCA\nmodel was combined\
    \ with a deterministic mechanism that\nmonitored the number of anomalies detected\
    \ in a 5-minute\ntime window to warn the factory engineers about an\nimpending\
    \ failure if the number of anomalies exceeded a\ncertain threshold. The proposed\
    \ architecture was tested from\n2013 to 2018 in a real production environment.\n\
    In the same year, a framework was proposed in [54]\nthat consists in a cognitive\
    \ analytics-based approach for\nmachine condition monitoring and anomaly detection.\
    \ The\nauthors used unsupervised learning and sensor fusion to\ncontinuously monitor\
    \ the health of an industrial robot and\ndetect anomalies in its operation, as\
    \ well as predict the\ntime when maintenance activities might be needed. The\n\
    experimental study was performed using only data that\nreflected the robot’s operation\
    \ under normal conditions,\n1 3\nMachine learning techniques applied to mechanical\
    \ fault diagnosis...\n14271\ni.e., no information about faulty states was available.\
    \ The\ndata was obtained from three independent sources, namely\nthe robot controller,\
    \ an energy meter and accelerometer\nsensors. The proposed cognitive analytics\
    \ framework\ninvolved performing several pre-processing tasks to the data\nobtained\
    \ from each independent source so that it could\nbe synchronized before it was\
    \ fused and clustered using\nthe k-means algorithm. Considering three positions\
    \ for the\nrobot arm (top, middle and bottom), the data was grouped\ninto three\
    \ clusters. Since this data represents the robot’s\noperation under healthy conditions,\
    \ the framework’s AI\nengine model, which monitored data arriving in real-time,\n\
    calculated the distance from the new data points to the\ncentres of the clusters.\
    \ If that distance exceeded a predefined\nthreshold, the data point was flagged\
    \ as an anomaly. The AI\nengine was also responsible for monitoring the deviation\
    \ of\nthe clusters’ centres over time to determine when the robot\nmight need\
    \ maintenance actions.\nAlso in 2020, a method that combines FSC with an\nimproved\
    \ version of K-SVD was proposed in [60] to detect\nearly weak faults in rotating\
    \ machinery (or faults affected by\nstrong background noise). IKSVD is an improved\
    \ version\nof K-SVD that uses self-adaptive matching pursuit instead\nof optimal\
    \ matching pursuit for sparse coding, which\nmakes IKSVD considerably more adaptable\
    \ and efficient.\nThe proposed method was used to detect faults in a coal\nmill\
    \ of a cement plant, which displayed a large vibration\nphenomenon related to\
    \ the bearing pedestal. Traditional\napproaches using envelope demodulation spectrum\
    \ and FSC\ncould not extract the fault features of the rolling bearing,\nbut the\
    \ proposed method, which uses the IKSVD method to\nenhance the impulse feature\
    \ components of the signal and\nFSC to extract the fault features from the denoised\
    \ signal,\nwas able to detect a fault in the small gear of the coal mill.\nDisassemblage\
    \ of the mesh gears confirmed the presence of\nwear damage on the small gear tooth.\n\
    The study presented in [64] in 2021 describes a predictive\nmaintenance approach\
    \ that consists in predicting the current\nwear of a rotating metal bush at Tata\
    \ Steel in the UK.\nMotivated by the lack of practical studies in industrial\n\
    contexts, the authors of the study put together a predictive\nmaintenance system\
    \ and demonstrated how a data-driven\nmodel could be used for bearing wear prediction\
    \ in\nsituations where the data is scarce, high-dimensional and\nof poor-quality.\
    \ To predict the condition of the bush, data\nwas collected from the set-up sheet,\
    \ the data warehouse\nand two sensor-based data sources that log process-related\n\
    parameters. Three different models were applied to the data\nto assess their performance\
    \ in predicting the condition of the\nbush, namely PLSR, an ANN and a random forest.\
    \ RMSE\nand R2 were used as performance metrics to compare the\nresults obtained\
    \ by each model. In experiments undertaken\nwith different training sample sizes,\
    \ the PLSR had the\nlargest R2 and the smallest RMSE on average and, as such,\n\
    was implemented in the predictive maintenance system\ndeployed at Tata Steel to\
    \ monitor the metal bush’s condition.\nIt should be noted that the purpose of\
    \ this study was\nthe prediction of the real-time condition of the bush and\n\
    not its future condition. That is, the aim of the authors\nwas developing a PdM\
    \ system that predicted the bush’s\ncondition at a given moment and not the state\
    \ of the bush\nafter a period of time. This approach made sense because\nthe condition\
    \ of the component could only be determined\nat the end of each maintenance cycle,\
    \ and, therefore, the\nfactory personnel had no means of knowing if the bush\n\
    really needed to be replaced at that time or if they could\npostpone (or anticipate)\
    \ the replacement. By predicting the\ncurrent condition of the bush, the proposed\
    \ PdM approach\ncould facilitate the transition from preventive maintenance\n\
    to predictive maintenance and reduce maintenance costs at\nthe factory.\nIn the\
    \ same year, k-multi-dimensional time-series clus-\ntering (K-MDTSC), a modified\
    \ version of k-means that\nis capable of handling multivariate time-series, was\
    \ pro-\nposed to predict the wear of welding electrodes used in\nthe body-in-white\
    \ welding stage of a car manufacturing\nplant [62]. K-MDTSC is based on k-means\
    \ but employs\na generalized notion of the Euclidean distance to han-\ndle multi-dimensional\
    \ time-series and addresses the issues\nk-means has with empty clusters. Moreover,\
    \ K-MDTSC\nworks directly with raw synchronous time-series, without\nrequiring\
    \ any transformation of the time-series data. To\nvalidate the proposed algorithm,\
    \ voltage and current data\nwas collected from the welding process performed by\
    \ two\nrobots at the body-in-white shop of the plant. After the\ndata was pre-processed,\
    \ K-MDTSC was used to cluster the\nmulti-dimensional time-series and discover\
    \ different weld-\ning profiles. The clusters were then characterized according\n\
    to the wear and tear of the welding electrodes. After analyz-\ning the results,\
    \ the domain experts realized the wear of the\nelectrodes wasn’t having a negative\
    \ impact on the welding\nprocess and that preventive maintenance operations were\n\
    being performed before they were actually necessary. These\npreventive maintenance\
    \ operations could, therefore, be post-\nponed, resulting in a reduction in maintenance\
    \ time and\ncosts.\n4.3.5 Other approaches\n18.2% of the selected studies employed\
    \ machine learning\nalgorithms belonging to other categories, such as instance-\n\
    based algorithms, rule-based models, or dynamic Bayes\nnetworks, to name a few.\n\
    The study presented in [42] in 2017 describes a predictive\nmaintenance approach\
    \ that combines sliding windows with\na genetic algorithm and a hidden Markov\
    \ model to estimate\n1 3\nM. Fernandes et al.\n14272\nand predict a hard masking\
    \ deposition tool’s long-term\ndegradation. Since the available data was sampled\
    \ at\ndifferent rates, summary statistics were calculated over\nsliding windows\
    \ to synchronize the data features. To\nhandle the features’ dynamic nature, a\
    \ HMM was used\nto cluster the time series data and estimate the tool’s\ndegradation\
    \ by considering past and present states of\nthe tool’s condition. The genetic\
    \ algorithm was used in\nconjunction with the HMM to select the most suitable\n\
    subset of features. Considering the tool’s degradation was\nestimated and predicted\
    \ in an unsupervised manner, the\nproposed method was evaluated using historical\
    \ data and\ntaking into consideration information provided by the\nmaintenance\
    \ experts of the semiconductor manufacturing\ncompany.\nIn 2019, Naskos et al.\
    \ [69] proposed a method that\nwas capable of detecting oil leakages in real time\
    \ in the\nlarge tanks of a BENTELER Automotive factory. To detect\noutliers in\
    \ real-time, the authors applied the micro-cluster\ncontinuous outlier detection\
    \ (MCOD) algorithm to streams\nof sensor data. Additionally, domain knowledge\
    \ of the\nproduction cycle was used to determine the operational\nstatus of the\
    \ machinery and enhance the algorithm’s\nperformance. When compared with variants\
    \ of the proposed\nmethod, including the application of MCOD to raw data (no\n\
    prior domain knowledge), the combination of MCOD with\ndomain knowledge obtained\
    \ the best results.\nIn another study published in 2019, Graß et al. [39]\ndescribed\
    \ an anomaly detection method to detect faults in\nthe fans of a reflow oven.\
    \ Asides from the absence of labeled\ndata, which conditioned the type of learning\
    \ methods that\ncould be applied to the problem, the authors also had to\nconsider\
    \ that different items were processed in the same\nproduction line. A reconfiguration\
    \ of machine parameters\noccurs whenever the production of a new item begins,\n\
    leading to different patterns of sensor measurements in the\ntime series data\
    \ that should not be interpreted as anomalies.\nTo deal with this problem the\
    \ authors began by clustering\nthe data according to the different machine configurations.\n\
    After this, for each cluster, the data was segmented, and\nsuitable features were\
    \ extracted for each segment. Finally,\nK-NN was used to define an anomaly threshold\
    \ based\non the mean distance between a given segment and its k\nnearest neighbors.\
    \ The proposed approach was tested using\nseven years of historical data, successfully\
    \ demonstrating its\nability to detect fan malfunctions.\nContinuing in 2019,\
    \ the authors of [49] used a\nquantitative association rule mining method to predict\
    \ the\nRUL of industrial equipment. The proposed algorithm,\nnamed “Rules 4 Rare\
    \ Events” (R4RE), improves an\nalgorithm previously proposed by the same authors,\
    \ by\nallowing quantifications of the consequent item in closed\nintervals and\
    \ integrating online pruning of the generated\nrules. After being applied to sensor\
    \ data collected from a\nreal factory between October and December of 2018, the\n\
    R4RE algorithm produced about 4500 rules that estimated\nthe RUL (RUL-time) of\
    \ the machines that were being\nmonitored. Additionally, the authors used an expanded\n\
    dataset to predict the RUL in terms of produced parts (RUL-\nparts), i.e., the\
    \ number of units a machine can produce\nbefore a failure occurs. Measuring the\
    \ RUL in terms of\nparts was considered by the authors as being a more robust\n\
    measure, since it does not consider the periods when a\nmachine was idle or turned\
    \ off. When compared with other\nmachine learning models, the R4RE model achieved\
    \ the\nbest results in the prediction of the RUL-time (RMSE =\n34.2 ; MAE = 28.7;\
    \ MAPE% = 20.1) and was among the\ntop contenders in the prediction of the RUL-parts\
    \ (RMSE =\n668.7 ; MAE = 120.8; MAPE% = 3.76).\nStill in 2019, Chen et al. [51]\
    \ also used a rule-based\nmethod to predict the RUL of machinery. Specifically,\
    \ the\nauthors used a modified version of the eXtended Classifier\nSystem (XCS)\
    \ to predict the RUL of a digital radio\nfrequency matching box (RF-MB), a machine\
    \ employed in\nthe semiconductor manufacturing process. XCS is a rule-\nbased\
    \ machine learning method that can recalibrate its rule\nset though interaction\
    \ with the environment. Whenever the\nrule set does not satisfy the current environmental\
    \ condition,\na special mechanism generates a new rule that matches\nit. However,\
    \ XCS can only process binary input data and,\nas such, the authors applied a\
    \ modified version of XCS\n(XCSR), which is capable of processing continuous-valued\n\
    inputs. Moreover, since XCSR is a classifier, the estimation\nof the RF-MB’s RUL\
    \ was framed as a classification\nproblem. Fisher discriminant analysis (FDA)\
    \ was applied\nto the data to reduce the large number of variables, before\nusing\
    \ XCRS to predict the RUL with an accuracy of 97.3%.\nIn 2020, Ruiz-Sarmiento\
    \ et al. [63] proposed a predictive\nmodel to estimate and predict the degradation\
    \ of machinery\nused in the stainless-steel industry, specifically the drums\n\
    of the heating coilers of Steckel mills. The model\nconsisted in a discrete Bayes\
    \ filter that incorporated\nexpert knowledge, configuration parameters, and real\
    \ time\nsensor data. The expert knowledge was obtained from the\nfactory specialists\
    \ that helped identify suitable variables and\ninteractions, as well as define\
    \ the parameters that affected\nthe machines’ degradation. The DBF model was able\
    \ to\nestimate the machinery’s health status, but it was also used\nto simulate\
    \ np manufacturing processes and predict the\nmachinery’s degradation after execution\
    \ of those processes.\nThe performance of the predictive model was evaluated and\n\
    compared with other models using real data from a factory\nin Spain. The proposed\
    \ model obtained the best results in all\ninstances (average RMSE = 0.59).\nThe\
    \ fault detection method of a predictive maintenance\nsystem developed for a mechanical\
    \ metallurgy company is\n1 3\nMachine learning techniques applied to mechanical\
    \ fault diagnosis...\n14273\ndescribed in another study published in 2020 [48].\
    \ Since\nlabeled data was unavailable, the authors used a prediction-\nbased anomaly\
    \ detection technique to discover unusual\noccurrences in sensor data obtained\
    \ from monitoring dif-\nferent CNC machines. An autoregressive integrated moving\n\
    average (ARIMA) model was fit to each independent data\nfeature and a 95% prediction\
    \ interval was calculated for\nthe model’s forecast. Data points outside the bounds\
    \ of the\nprediction interval were flagged as anomalies, but since\nisolated anomalies\
    \ may not represent an impending fault\na 30-minute time window was used to calculate\
    \ the mov-\ning average of anomaly occurrences. The predictive system\nissued\
    \ a fault alarm if the average of anomalies exceeded\na user-defined value (default\
    \ = 0.85). Additionally, since\nan imminent fault might affect more than one variable,\
    \ the\nauthors proposed a fault detection mechanism that corre-\nlated the anomalies\
    \ detected for each variable and issued an\nalarm according to a threshold that\
    \ considered the number\nof variables with correlated anomalies.\nIn 2021, Mohan\
    \ et al. published a study describing a\nmethod that also uses an ARIMA model\
    \ to help industries\ntransition from industry 3.0 to industry 4.0 without having\
    \ to\nundergo considerable structural changes [61]. The authors\nproposed using\
    \ the ARIMA model to forecast the oil\ncontamination level of a high-pressure\
    \ sand moulding line\nin a foundry and subsequently calculate the RUL of the\n\
    equipment. In this study, sensor data was collected from\nthe hydraulic unit of\
    \ the moulding line every three minutes\nand the data was used to forecast the\
    \ oil’s contamination\nlevel every three hours. The hydraulic unit’s RUL was\n\
    computed based on the model’s 95% confidence level and\non a threshold level for\
    \ the oil contamination. Additionally,\nwhenever the moving average of the oil\
    \ contamination was\ngreater that the threshold value, the window size used for\n\
    calculating the moving average changed accordingly so\nthat a warning message\
    \ could be issued sooner. During\nthe study period (September 2018 to December\
    \ 2019),\nthe breakdown time of the high-pressure moulding line\nwas reduced by\
    \ 84% and the number of breakdowns was\nreduced by 88%. Furthermore, the MTBF\
    \ increased from\n604 to 5349 minutes and the MTTR reduced from 83 to\n46 minutes.\
    \ Considering the downtime caused by the oil\ncontamination specifically, the\
    \ proposed approach managed\nto achieve zero downtime.\n4.4 RQ4: What limitations\
    \ and advantages do those\nalgorithms and methods present?\nAs described in Section\
    \ 3.5, a large variety of advantages\nwas identified in the studies under consideration.\
    \ In\naddition, some studies identified not only the advantages\nof the machine\
    \ learning algorithms used but also of\nthe methods developed to address a given\
    \ problem.\nNotwithstanding, some benefits were mentioned more\nfrequently than\
    \ others due to their importance in the context\nof fault detection and prognosis\
    \ in industrial environments.\nHigh performance [40, 44, 49, 50, 59, 63, 71, 73,\
    \ 74,\n76, 78, 79], as well as the ability to uncover complex non-\nlinear relationships\
    \ in the data [50, 57, 59, 66, 71, 77],\nwere two of the reasons most frequently\
    \ given for choosing\nan algorithm. The stochastic behavior of a manufacturing\n\
    system and the intricate relationships between its compo-\nnents mean these systems\
    \ are marked by unpredictability\n[87]. Additionally, in real-world scenarios\
    \ different prod-\nucts are often manufactured in the same production line,\n\
    requiring changes in machine configurations, components,\nand production materials\
    \ [32]. These non-stationary condi-\ntions further complicate the task of detecting\
    \ and predicting\nfaults. It is, therefore, crucial that machine learning algo-\n\
    rithms can discover the nonlinear and dynamic patterns\nthat characterize these\
    \ events. High performance is equally\ndesirable and is directly related to an\
    \ algorithm’s ability\nto model the system. However, when detecting or predict-\n\
    ing failures in the real-world the definition of performance\nmust consider the\
    \ tradeoff between false positives and false\nnegatives. Depending on the business\
    \ requirements, fail-\nure to predict a fault might have serious consequences,\
    \ in\nwhich case false positives are preferable to false negatives.\nHowever,\
    \ there are less critical situations where a false pos-\nitive, which can imply\
    \ unnecessary stoppages and use of\nresources, will be far more costly. This tradeoff\
    \ must be\ncarefully defined with the help of maintenance experts.\nComputational\
    \ efficiency is also seen as an important\nadvantage for machine learning algorithms\
    \ [40, 53, 60, 63,\n67, 69, 76, 79]. While advanced algorithms can produce\nvery\
    \ good results, they can also be quite demanding in terms\nof computational resources.\
    \ Artificial neural networks, for\nexample, require a lot of CPU and GPU processing\
    \ power.\nThey also require a lot of memory, as do ensemble tree\nmethods, and\
    \ the amount of data being processed impacts\nthe usage of computational resources\
    \ as well. Furthermore,\ndemanding computations can greatly increase energy\n\
    consumption [88]. For these reasons, among others, the\ndevelopment of a predictive\
    \ maintenance system requires\nsignificant investment [89]. As such, choosing\
    \ machine\nlearning algorithms that are computationally efficient can\nbe a more\
    \ cost-effective option, while also being more\nenvironmentally friendly, a concern\
    \ whose importance and\nurgency cannot be understated.\nOther advantages identified\
    \ in more than one study\ninclude the algorithms’ suitability for processing time\n\
    series data, interpretability, ability to uncover fault patterns\nfrom unlabeled\
    \ data, suitability for online learning, and\nusefulness for root cause analysis.\
    \ Sensor data obtained\nfrom the monitorization of manufacturing equipment\nconsists\
    \ in time series data. Since time series are a\n1 3\nM. Fernandes et al.\n14274\n\
    sequence of data points ordered by time that may have\nan internal structure,\
    \ the methods used to analyze time\nseries should ideally be capable of taking\
    \ this structure into\naccount. Many machine learning algorithms, however, are\n\
    not suited for this task, which means a much greater feature\nengineering effort\
    \ is necessary before they can be applied\nto time series data. For this reason,\
    \ algorithms capable\nof handling raw time series data can be advantageous. A\n\
    model’s interpretability is also seen as beneficial [41, 54,\n59, 64, 75], not\
    \ only because it can be useful to debug or\nfine-tune a model, among other technical\
    \ aspects, but also\nbecause understanding how a model came up with a result\n\
    increases the user’s trust in the model. It also enables better\ninformed decision-making,\
    \ and, in industrial contexts, it can\nassist in the identification of a fault’s\
    \ root cause. In fact, the\nlatter was identified as an important characteristic\
    \ in other\nstudies as well [41, 51, 64].\nAnother quality that is often necessary\
    \ is the ability\nto learn from unlabeled data, i.e., unsupervised learning\n\
    [39, 42, 45, 47, 48, 50, 54, 58–62, 65, 66, 69–71]. For\na variety of reasons,\
    \ historical fault data can be hard to\nobtain. Faults might not occur very frequently,\
    \ or they might\nnot be logged correctly. It is also possible that records of\n\
    these events exist but not for the same time periods as the\navailable condition\
    \ monitoring data. For whatever reason,\nthe lack of historical fault data is\
    \ a problem that arises\nfrequently (e.g., labeled data was absent in 10 of the\
    \ 44\nselected studies - 22.7%). Even when labels are present,\ndata representing\
    \ the condition of machines during normal\noperation is usually much more abundant\
    \ than fault events,\nwhich causes the representative classes to be imbalanced.\n\
    When the class imbalance is extreme, applying techniques,\nsuch as resampling,\
    \ to solve the problem might not produce\nviable results. In situations such as\
    \ these, machine learning\nalgorithms suitable for unsupervised learning can be\
    \ used\nto extract knowledge from the data and assist in the\ndetection of faults.\
    \ However, validating these models is not\na straightforward task if no historical\
    \ data exists that can be\nused to evaluate their performance. In these circumstances,\n\
    the development of unsupervised approaches should be\nguided by domain knowledge\
    \ and tested in real production\nenvironments, as demonstrated in publications\
    \ [47, 59, 60,\n62, 65, 75].\nAs reported in Section 3.5, only eleven studies\
    \ identified\nthe limitations of the machine learning algorithms used [31,\n40,\
    \ 42, 47, 57, 58, 62, 69, 73, 76, 81]. While most of them\nare details specific\
    \ to the chosen algorithms, such as the\nlack of interpretability of artificial\
    \ neural networks, or the\nslow convergence speed of BPNNs, the decrease in model\n\
    performance when the training and test data do not share the\nsame distribution\
    \ is a limitation with important implications\nfor fault detection and prediction.\
    \ This issue is related to\nthe concept of online learning and will be discussed\
    \ in more\ndetail in the next subsection.\n4.5 RQ5: Which of those algorithms\
    \ and methods\nare used for data stream learning?\nIn many real-world scenarios,\
    \ like the manufacturing\nindustry, the data generating processes are non-stationary,\n\
    causing the distribution of the data to change over time\nin what is called concept\
    \ drift [32, 90]. This means the\nhistorical data used to train a model and the\
    \ data used\nto make predictions when the model is deployed come\nfrom different\
    \ probability distributions, which affects the\nmodel’s performance. This issue\
    \ is particularly apparent in\nthe context of predictive maintenance since monitoring\
    \ data\nis acquired at a high frequency and arrives continuously\nin the form\
    \ of data streams. To prevent learning models\nfrom becoming obsolete over time\
    \ they have to be updated\nregularly with new input data [90, 91]. However, traditional\n\
    machine learning models are trained using batches of data\nand are not adequate\
    \ to process continuous flows of data,\ni.e., data streams. To handle this type\
    \ of data, it is necessary\nto use machine learning algorithms capable of learning\n\
    incrementally or through small batches of recent data. This\ntype of learning,\
    \ whereby algorithms process high-speed\ndata while adapting to concept drifts,\
    \ is known as online\nlearning, or data stream learning [90, 92].\nIn the scope\
    \ of this review, only two studies [55, 69]\nused a data stream learning algorithm,\
    \ while three others\n[39, 49, 51] used algorithms suitable for online learning,\n\
    but did not apply them in that context. Considering what\nhas been said about\
    \ the non-stationarity of manufacturing\nenvironments and how it brings about\
    \ concept drift, it is\nreasonable to assume the performance of the approaches\n\
    proposed in the other selected studies would degrade over\ntime. One notable exception\
    \ is the study presented in [81],\nwhere digital twin technology and transfer\
    \ learning were\nused to address concept drift induced by changing working\nconditions.\
    \ Other studies have used transfer learning to deal\nwith different probability\
    \ distributions in the training and\ntesting data [32], but transfer learning\
    \ alone isn’t sufficient\nfor continuous adaptation.\nMore studies focused on\
    \ data stream learning techniques\nare necessary, particularly studies performed\
    \ in real indus-\ntrial environments, not only to validate the applicability of\n\
    theoretical methods, but also to address several aspects of\npractical order,\
    \ like pre-processing streaming data, detecting\nconcept drift in semi-supervised\
    \ and unsupervised settings,\nand handling legacy system, among others [92, 93].\
    \ As this\nsystematic review has revealed, research of online learning\ntechniques\
    \ applied to fault detection and prognosis in the\nmanufacturing industry is still\
    \ in its infancy.\n1 3\nMachine learning techniques applied to mechanical fault\
    \ diagnosis...\n14275\n5 Conclusion\nThis study presents a systematic literature\
    \ review of\nthe machine learning methods used for mechanical fault\ndetection\
    \ and fault prognosis in manufacturing equipment in\nreal-world scenarios. The\
    \ review was conducted according\nto the PRISMA guidelines and the guidelines\
    \ for software\nengineering systematic reviews described in [35]. Following\n\
    the steps defined in the review protocol, an initial set of 4549\nrecords published\
    \ between January 2015 and October 2021\nwere identified (3377 without duplicates).\
    \ After assessing\nthem based on selection criteria, 44 primary studies were\n\
    selected for inclusion in the systematic review. These\nstudies were then examined\
    \ in more detail based on five\nresearch questions aimed at characterizing the\
    \ publication\nsources and scientific fields, the machine learning methods\nused,\
    \ their advantages and limitations, and their application\nin the context of data\
    \ stream learning.\nAbout 84% of the selected studies employed machine\nlearning\
    \ techniques belonging to one of four categories:\ndecision trees, artificial\
    \ neural networks, hybrid models\nand latent variable models. However, although\
    \ every study\nperformed detection of mechanical faults or prognosis\nof faults\
    \ in real manufacturing scenarios, each study is\ndistinct in terms of the manufacturing\
    \ context where the\nstudy was undertaken, the machinery for which faults were\n\
    detected or predicted, and the characteristics of the data that\nwas available.\
    \ These differences are to be expected from\nindustrial case-studies but made\
    \ it difficult to compare the\ndifferent techniques.\nWhile the number of publications\
    \ is considerably larger\nin the second half of the period considered for the\
    \ review\nthan in the first half, only 44 studies were selected for inclu-\nsion\
    \ in this review from a preliminary group of 3377 (with-\nout duplicates). The\
    \ literature on mechanical fault detection\nand fault prognosis in manufacturing\
    \ equipment is extensive\nbut, despite the economic, safety and environmental\
    \ benefits\npredictive maintenance can provide, the number of stud-\nies performed\
    \ in real-world manufacturing scenarios is still\nreduced. Studies developed under\
    \ experimental conditions\ntend to disregard the numerous challenges presented\
    \ by\nmanufacturing environments, which raises questions about\ntheir applicability.\
    \ Research interest in this topic of study\nseems to be increasing, but there\
    \ are still several issues that\nneed to be addressed.\nAn important problem that\
    \ needs to be considered when\nperforming fault detection and prognosis in the\
    \ manufac-\nturing industry is the inherent complexity of manufacturing\nsystems\
    \ and the time-varying properties of production\nprocesses. More research is needed\
    \ to develop machine\nlearning algorithms and methods that can handle noisy,\n\
    non-stationary data and capture the nonlinear patterns\nof interaction between\
    \ machinery components. A line of\nresearch that can be pursued to deal with the\
    \ issue of non-\nstationarity is online learning, also known as data stream\n\
    learning. Online learning techniques that learn incremen-\ntally, or from small\
    \ batches of recent data, are ideal to\nprocess high-speed streams of sensor data,\
    \ while contin-\nuously adapting to the changes in the data’s probability\ndistribution\
    \ caused by non-stationary environments (i.e.,\nconcept drift). Learning models\
    \ that do not account for\nconcept drift will eventually become outdated. As this\n\
    review has shown, there is still a deficit of studies devoted\nto online learning\
    \ methods, particularly where it relates to\nthe detection of mechanical faults\
    \ or prediction of faults in\nthe manufacturing industry. As such, this line of\
    \ research\nprovides promising opportunities for future research.\nAnother concern\
    \ common in real-world scenarios is the\nabsence of labeled data, which restricts\
    \ the learning task\nto unsupervised and semi-supervised methods. Due to this\n\
    issue, almost half of the studies selected in this review\nemployed unsupervised\
    \ learning techniques, but more work\nis necessary not only to demonstrate the\
    \ effectiveness of\nthese models, but also to develop new methods capable\nof\
    \ learning complex nonlinear relationships in the absence\nof labels, while adapting\
    \ to concept drift. To successfully\nperform fault detection and prognosis in\
    \ manufacturing\nenvironments, it is important to consider these factors\ncollectively.\n\
    Predictive maintenance provides economic, safety and\nenvironmental benefits,\
    \ but the development of a predictive\nmaintenance system can be laborious and\
    \ requires a\nsignificant upfront investment. To justify such an investment\n\
    in terms of time and money, and derive benefits from it, it\nis essential that\
    \ the models developed perform as accurately\nas possible, but it is also important\
    \ to consider other\naspects, such as computational efficiency or interpretability,\n\
    in accordance with the business’s needs.\nAcknowledgements The present work has\
    \ been developed under\nproject PIANiSM (EUREKA - ITEA3: 17008; ANI|P2020 40125)\n\
    and has received Portuguese National Funds through FCT (Por-\ntuguese Foundation\
    \ for Science and Technology) under project\nUIDB/00760/2020 and Ph.D. Scholarship\
    \ SFRH/BD/136253/2018.\nDeclarations\nConﬂict of Interests The authors declare\
    \ that they have no conflict of\ninterest.\nReferences\n1. Aboelmaged\nMG\n(2014)\n\
    Predicting\ne-readiness\nat\nfirm-\nlevel:\nAn\nanalysis\nof\ntechnological,\n\
    organizational\nand\nenvironmental\n(TOE)\neffects\non\ne-maintenance\nreadiness\n\
    in\nmanufacturing\nfirms.\nInt\nJ\nInf\nManag\n34(5):639–651.\nhttps://doi.org/10.1016/j.ijinfomgt.2014.05.002\n\
    1 3\nM. Fernandes et al.\n14276\n2. Holmberg K, Adgar A, Arnaiz A, Jantunen E,\
    \ Mascolo J (2010).\nIn: Holmberg K, Adgar A, Arnaiz A, Jantunen E, Mascolo J,\n\
    Mekid S (eds) E-maintenance. Springer, London\n3. Muller A, Crespo Marquez A,\
    \ Iung B (2008) On the concept of\ne-maintenance: Review and current research.\
    \ Reliab Eng Syst Saf\n93(8):1165–1187. https://doi.org/10.1016/j.ress.2007.08.006\n\
    4. Mobley RK (2001) Predictive Maintenance. In: Plant Engineer’s\nHandbook. Elsevier,\
    \ pp 867–888\n5. Sullivan G, Pugh R, Melendez AP, Hunt WD (2010) Operations\n\
    & maintenance best practices-a guide to achieving operational\nefficiency (release\
    \ 3). Technical Report, Pacific Northwest\nNational Lab.(PNNL), Richland\n6. Rødseth\
    \ H, Schjølberg P (2016) Data-driven predictive mainte-\nnance for green manufacturing.\
    \ In: Proceedings of the 6th inter-\nnational workshop of advanced manufacturing\
    \ and automation.\nAdvances in Economics, Business and Management Research.\n\
    Atlantis Press, pp 36–41\n7. Jardine AKS, Lin D, Banjevic D (2006) A review on\
    \ machin-\nery diagnostics and prognostics implementing condition-based\nmaintenance.\n\
    Mech\nSyst\nSignal\nProcess\n20(7):1483–1510.\nhttps://doi.org/10.1016/J.YMSSP.2005.09.012,\n\
    https://www.\nsciencedirect.com/science/article/pii/S0888327005001512\n8. Kan\n\
    MS,\nTan\nACC,\nMathew\nJ\n(2015)\nA\nreview\non\nprognostic\ntechniques\nfor\n\
    non-stationary\nand\nnon-linear\nrotating\nsystems.\nMech\nSyst\nSignal\nProcess\n\
    62:1–20.\nhttps://doi.org/10.1016/j.ymssp.2015.02.016,\nhttps://www.\nsciencedirect.com/science/article/pii/S0888327015000898\n\
    9. Zhang\nW,\nYang\nD,\nWang\nH\n(2019)\nData-Driven\nMethods\nfor\nPredictive\n\
    Maintenance\nof\nIndustrial\nEquipment:\nA\nSurvey.\nIEEE\nSyst\nJ\n13(3):2213–2227.\n\
    https://doi.org/10.1109/JSYST.2019.2905565\n10. Hwang\nI,\nKim\nS,\nKim\nY,\n\
    Seah\nCE\n(2010)\nA\nsurvey\nof\nfault\ndetection,\nisolation,\nand\nreconfiguration\n\
    meth-\nods.\nIEEE\nTrans\nControl\nSyst\nTechnol\n18(3):636–653.\nhttps://doi.org/10.1109/TCST.2009.2026285,\n\
    https://ieeexplore.\nieee.org/abstract/document/5282515/\n11. Gertler JJ (2017)\
    \ Fault detection and diagnosis in engineer-\ning systems. CRC Press. https://www.taylorfrancis.com/books/\n\
    9781351448796\n12. Boyes H, Hallaq B, Cunningham J, Watson T (2018) The indus-\n\
    trial internet of things (IIoT): An analysis framework. Comput Ind\n101:1–12.\
    \ https://doi.org/10.1016/j.compind.2018.04.015\n13. O’Donovan P, Leahy K, Bruton\
    \ K, O’Sullivan DTJ (2015) Big\ndata in manufacturing: a systematic mapping study.\
    \ J Big Data\n2(1). https://doi.org/10.1186/s40537-015-0028-x\n14. Qin SJ (2009)\
    \ Data-driven Fault Detection and Diagnosis for\nComplex Industrial Processes.\
    \ In: IFAC Proceedings Volumes,\nvol 42. Elsevier, pp 1115–1125\n15. Carvalho\
    \ TP, Soares FAAMN, Vita R, Francisco RP, Basto JP,\nAlcal´a SGS (2019) A systematic\
    \ literature review of machine\nlearning methods applied to predictive maintenance.\
    \ Comput Ind\nEngi 137:106024. https://doi.org/10.1016/j.cie.2019.106024\n16.\
    \ Hu H, Tang B, Gong X, Wei W, Wang H (2017) Intelligent\nfault diagnosis of the\
    \ high-speed train with big data based on\ndeep neural networks. IEEE Trans Ind\
    \ Inf 13(4):2106–2116.\nhttps://doi.org/10.1109/TII.2017.2683528\n17. Mathew V,\
    \ Toby T, Singh V, Rao BM, Kumar MG (2017)\nPrediction of remaining useful lifetime\
    \ (rul) of turbofan engine\nusing machine learning. In: 2017 IEEE International\
    \ Conference\non Circuits and Systems (ICCS). IEEE, pp 306–311\n18. Shao H, Jiang\
    \ H, Wang F, Zhao H (2017) An enhance-\nment\ndeep\nfeature\nfusion\nmethod\n\
    for\nrotating\nmachinery\nfault\ndiagnosis.\nKnowl-Based\nSyst\n119:200–220.\n\
    https://doi.org/10.1016/j.knosys.2016.12.012\n19. Wang L, Zhang Z, Long H, Xu\
    \ J, Liu R (2017) Wind\nturbine\ngearbox\nfailure\nidentification\nwith\ndeep\n\
    neu-\nral\nnetworks.\nIEEE\nTrans\nInd\nInf\n13(3):1360–1368.\nhttps://doi.org/10.1109/TII.2016.2607179\n\
    20. You D, Gao X, Katayama S (2015) Wpd-pca-based laser\nwelding process monitoring\
    \ and defects diagnosis by using\nfnn\nand\nsvm.\nIEEE\nTrans\nInd\nElectron\n\
    62(1):628–636.\nhttps://doi.org/10.1109/TIE.2014.2319216\n21. Baptista M, Sankararaman\
    \ S, de Medeiros IP, Nascimento Jr\nC, Prendinger H, Henriques EMP (2018) Forecasting\
    \ fault\nevents\nfor\npredictive\nmaintenance\nusing\ndata-driven\ntech-\nniques\
    \ and arma modeling. Comput Ind Eng 115:41–53.\nhttps://doi.org/10.1016/j.cie.2017.10.033\n\
    22. Li\nC,\nSanchez\nR-V,\nZurita\nG,\nCerrada\nM,\nCabrera\nD,\nV´asquez RE (2015)\
    \ Multimodal deep support vector clas-\nsification with homologous features and\
    \ its application to\ngearbox\nfault\ndiagnosis.\nNeurocomputing\n168:119–127.\n\
    https://doi.org/10.1016/j.neucom.2015.06.008\n23. Susto GA, Schirru A, Pampuri\
    \ S, McLoone S, Beghi A\n(2015) Machine learning for predictive maintenance: A\
    \ mul-\ntiple classifier approach. IEEE Trans Ind Inf 11(3):812–820.\nhttps://doi.org/10.1109/TII.2014.2349359\n\
    24. Canizo M, Onieva E, Conde A, Charramendieta S, Trujillo S\n(2017) Real-time\
    \ predictive maintenance for wind turbines using\nbig data frameworks. In: 2017\
    \ ieee international conference on\nprognostics and health management (icphm).\
    \ IEEE, pp 70–77\n25. Krishnakumari A, Elayaperumal A, Saravanan M, Arvindan C\n\
    (2017) Fault diagnostics of spur gear using decision tree and\nfuzzy classifier.\
    \ Int J Adv Manuf Technol 89(9-12):3487–3494.\nhttps://doi.org/10.1007/s00170-016-9307-8\n\
    26. Amruthnath N, Gupta T (2018) A research study on unsupervised\nmachine learning\
    \ algorithms for early fault detection in predictive\nmaintenance. In: 2018 5th\
    \ International Conference on Industrial\nEngineering and Applications (ICIEA).\
    \ IEEE, pp 355–361\n27. Uhlmann E, Pontes RP, Geisert C, Hohwieler E (2018) Cluster\n\
    identification of sensor data for predictive maintenance in a\nselective laser\
    \ melting machine tool. Procedia Manuf 24:60–65.\nhttps://doi.org/10.1016/j.promfg.2018.06.009\n\
    28. Ahmad W, Khan SA, Islam MMM, Kim J-M (2019) A reliable\ntechnique for remaining\
    \ useful life estimation of rolling element\nbearings using dynamic regression\
    \ models. Reliab Eng Syst Safety\n184:67–76. https://doi.org/10.1016/j.ress.2018.02.003\n\
    29. Yu J (2018) Tool condition prognostics using logistic regression\nwith penalization\
    \ and manifold regularization. Appl Soft Comput\n64:454–467. https://doi.org/10.1016/j.asoc.2017.12.042\n\
    30. Haq R (2020) Enterprise artificial intelligence transformation :\na playbook\
    \ for the next generation of business and technology\nleaders. Wiley\n31. Luo\n\
    B,\nWang\nH,\nLiu\nH,\nLi\nB,\nPeng\nF\n(2018)\nEarly\nfault detection of machine\
    \ tools based on deep learning and\ndynamic identification. IEEE Trans Ind Electron\
    \ 66(1):509–518.\nhttps://doi.org/10.1109/TIE.2018.2807414\n32. Bang SH, Ak R,\
    \ Narayanan A, Lee YT, Cho H (2019) A survey\non knowledge transfer for manufacturing\
    \ data analytics, vol 104.\nElsevier B.V.\n33. Quintana G, Ciurana J (2011) Chatter\
    \ in machining pro-\ncesses: A review. Int J Mach Tools Manuf 51(5):363–376.\n\
    https://doi.org/10.1016/j.ijmachtools.2011.01.001\n34. Liberati A, Altman DG,\
    \ Tetzlaff J, Mulrow C, Gøtzsche PC,\nIoannidis JPA, Clarke M, Devereaux PJ, Kleijnen\
    \ J, Moher\nD (2009) The PRISMA statement for reporting systematic\nreviews and\
    \ meta-analyses of studies that evaluate health care\ninterventions: explanation\
    \ and elaboration. In: Journal of Clinical\nEpidemiology, vol 62. Pergamon, pp\
    \ e1–e34\n1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n\
    14277\n35. Kitchenham\nB\n(2004)\nProcedures\nfor\nperforming\nsystem-\natic reviews.\
    \ Keele, UK, Keele Univ 33(2004):1–26. https://\nwww.researchgate.net/publication/228756057\
    \ Procedures for\nPerforming Systematic Reviews\n36. Gama J (2012) A survey on\
    \ learning from data streams: Current\nand future trends, vol 1. https://link.springer.com/content/pdf/10.\n\
    1007/s13748-011-0002-6.pdf\n37. Gusenbauer M, Haddaway NR (2020) Which academic\
    \ search\nsystems are suitable for systematic reviews or meta-analyses?\nEvaluating\
    \ retrieval qualities of Google Scholar, PubMed,\nand 26 other resources. Res\
    \ Synthes Methods 11(2):181–217.\nhttps://doi.org/10.1002/jrsm.1378\n38. Subramanya\n\
    SH,\nLama\nB,\nAcharya\nKP\n(2020)\nImpact\nof\nCOVID-19\npandemic\non\nthe\n\
    scientific\ncommunity.\nhttps://doi.org/10.5339/QMJ.2020.21\n39. Grass A, Beecks\
    \ C, Soto JAC (2019) Unsupervised Anomaly\nDetection in Production Lines. In:\
    \ Beyerer JKO (ed) Machine\nLearning For Cyber Physical Systems, ML4CPS 2018.\
    \ Technolo-\ngien fur die intelligente Automation, vol 9, pp 18–25\n40. Liu Q,\
    \ Zhang F, Liu M, Shen W (2016) A fault prediction method\nbased on modified Genetic\
    \ Algorithm using BP neural network\nalgorithm. In: 2016 IEEE International Conference\
    \ on Systems,\nMan, and Cybernetics (SMC), pp 4614–4619\n41. Linard A, Bueno MLP\
    \ (2016) Towards Adaptive Scheduling\nof Maintenance for Cyber-Physical Systems.\
    \ In: Margaria, T\nSB (ed) Leveraging Applications of Formal Methods, Verification\n\
    and Validation: Foundational Techniques, Pt I, Lecture Notes in\nComputer Science,\
    \ vol 9952, pp 134–150\n42. Kinghorst J, Geramifard O, Luo M, Chan HL, Yong K,\
    \ Folmer\nJ, Zou M, Vogel-Heuser B (2017) Hidden Markov model-\nbased predictive\
    \ maintenance in semiconductor manufacturing: A\ngenetic algorithm approach. In:\
    \ IEEE International Conference on\nAutomation Science and Engineering. IEEE,\
    \ pp 1260–1267\n43. Paolanti M, Romeo L, Felicetti A, Mancini A, Frontoni E,\n\
    Loncarski J (2018) Machine Learning approach for Predictive\nMaintenance in Industry\
    \ 4.0. In: 2018 14th IEEE/ASME\nInternational Conference on Mechatronic and Embedded\
    \ Systems\nand Applications (MESA), pp 1–6\n44. Amihai I, Gitzel R, Kotriwala\
    \ AM, Pareschi D, Subbiah S, Sosale\nG, Strecker C (2018) An Industrial Case Study\
    \ Using Vibration\nData and Machine Learning to Predict Asset Health. In: Proper\n\
    HA (ed) 2018 20TH IEEE INTERNATIONAL CONFERENCE\nON BUSINESS INFORMATICS (IEEE\
    \ CBI 2018), VOL 1.\nConference on Business Informatics. IEEE; IEEE Comp Soc,\n\
    pp 178–185\n45. Strauß P, Schmitz M, W¨ostmann R, Deuse J (2018) Enabling\nof\
    \ Predictive Maintenance in the Brownfield through Low-Cost\nSensors, an IIoTArchitecture\
    \ and Machine Learning. In: Abe N,\nLiu H, Pu C, Hu X, Ahmed N, Qiao M, Song Y,\
    \ Kossmann D,\nLiu B, Lee K, Tang J, He J, Saltz J (eds) Proceedings - 2018\n\
    IEEE International Conference on Big Data, Big Data 2018. IEEE;\nIEEE Comp Soc;\
    \ Expedia Grp; Baidu; Squirrel AI Learning;\nAnkura; Springer, IEEE International\
    \ Conference on Big Data,\npp 1474–1483. https://doi.org/10.1109/BigData.2018.8622076\n\
    46. Kolokas N, Vafeiadis T, Ioannidis D, Tzovaras D (2018) Fore-\ncasting faults\
    \ of industrial equipment using machine learning\nclassifiers. In: Yildirim, T\
    \ ML (ed) 2018 IEEE (SMC) Inter-\nnational Conference on Innovations in Intelligent\
    \ Systems and\nApplications, INISTA 2018. Aristotle Univ Thessaloniki; Dem-\n\
    ocritus Univ Thrace; IEEE Systems & Cybernet Soc; IEEE; Yildiz\nTechn Univ\n47.\
    \ Amruthnath N, Gupta T (2018) Fault class prediction in\nunsupervised learning\
    \ using model-based clustering approach.\nIn: 2018 International Conference on\
    \ Information and Computer\nTechnologies, ICICT 2018. IEEE, pp 5–12\n48. Fernandes\
    \ M, Canito A, Corchado JM, Marreiros G (2020) Fault\ndetection mechanism of a\
    \ predictive maintenance system based\non autoregressive integrated moving average\
    \ models. In: Herrera\nF, Matsui K, Rodr´ıguez-Gonz´alez S (eds) Distributed Comput-\n\
    ing and Artificial Intelligence, 16th International Conference.\nSpringer International\
    \ Publishing, Cham, pp 171–180\n49. Christou IT (2019) Avoiding the Hay for the\
    \ Needle in the Stack:\nOnline Rule Pruning in Rare Events Detection. In: 2019\
    \ 16th\nInternational Symposium on Wireless Communication Systems\n(ISWCS), pp\
    \ 661–665\n50. Cheng C, Zhang B, Gao D (2019) A Predictive Maintenance\nSolution\
    \ for Bearing Production Line Based on Edge-Cloud\nCooperation. In: 2019 Chinese\
    \ Automation Congress (CAC),\npp 5885–5889\n51. Chen L-Y, Lee J-H, Yang Y-L, Yeh\
    \ M-T, Hsiao T-C (2019)\nPredicting the Remaining Useful Life of Plasma Equipment\n\
    through XCSR. In: Proceedings of the Genetic and Evolution-\nary Computation Conference\
    \ Companion, GECCO ’19. Asso-\nciation for Computing Machinery, New York, pp 1263–1270.\n\
    https://doi.org/10.1145/3319619.3326879\n52. Aremu OO, O’Reilly DO, Hyland-Wood\
    \ D, McAree PR (2019)\nKullback-leibler divergence constructed health indicator\
    \ for data-\ndriven predictive maintenance of multi-sensor systems. In: IEEE\n\
    International Conference on Industrial Informatics (INDIN). Inst\nElect & Elect\
    \ Engineers; Tampere Univ; Finnish Soc Automat;\nIEEE Ind Elect Soc, pp 1315–1320\n\
    53. Binding A, Dykeman N, Pang S (2019) Machine learning\npredictive maintenance\
    \ on data in the wild. In: IEEE 5th\nWorld Forum on Internet of Things, WF-IoT\
    \ 2019 - Conference\nProceedings. IEEE; Univ Limerick; IEEE Commun Soc; IEEE\n\
    Consumer Elect Soc; IEEE Reliabil Soc; IEEE Sensors Council;\nIEEE Signal Proc\
    \ Soc; IEEE Stand Assoc; IEEE Control Syst Soc;\nIEEE Council Elect Design Automat;\
    \ IEEE Council RFID; IEEE\nElectromagnet Compatibil Soc;, pp 507–512\n54. Farbiz\
    \ F, Miaolong Y, Yu Z (2020) A cognitive analytics based\napproach for machine\
    \ health monitoring, anomaly detection,\nand predictive maintenance. In: 2020\
    \ 15th IEEE Conference on\nIndustrial Electronics and Applications (ICIEA). IEEE,\
    \ pp 1104–\n1109\n55. Das M, Pratama M, Tjahjowidodo T (2020) A self-evolving\n\
    mutually-operative recurrent network-based model for online tool\ncondition monitoring\
    \ in delay scenario. In: Proceedings of the\n26th ACM SIGKDD International Conference\
    \ on Knowledge\nDiscovery & Data Mining, pp 2775–2783\n56. KOCA O, Kaymakci OT,\
    \ Mercimek M (2020) Advanced Predic-\ntive Maintenance with Machine Learning Failure\
    \ Estimation in\nIndustrial Packaging Robots. In: 2020 International Conference\
    \ on\nDevelopment and Application Systems (DAS), pp 1–6\n57. Li Z, Wang Y, Wang\
    \ K-S (2017) Intelligent predictive main-\ntenance for fault diagnosis and prognosis\
    \ in machine cen-\nters: Industry 4.0 scenario. Adv Manuf 5(4, SI):377–387.\n\
    https://doi.org/10.1007/s40436-017-0203-8\n58. Kim D, Lee S, Kim D (2021) An applicable\
    \ predictive\nmaintenance framework for the absence of run-to-failure data.\n\
    Appl Sci 11(11):5180. https://doi.org/10.3390/app11115180\n59. Zschech P, Heinrich\
    \ K, Bink R, Neufeld JS (2019) Prognostic\nModel Development with Missing Labels:\
    \ A Condition-Based\nMaintenance Approach Using Machine Learning. Bus Inf Syst\n\
    Eng 61(3):327–343. https://doi.org/10.1007/s12599-019-00596-1\n60. Wang H, Du\
    \ W (2020) Fast spectral correlation based on\nsparse representation self-learning\
    \ dictionary and its appli-\ncation in fault diagnosis of rotating machinery.\
    \ Complexity.\nhttps://doi.org/10.1155/2020/9857839\n61. Mohan TR, Roselyn JP,\
    \ Uthra RA, Devaraj D, Umachandran\nK (2021) Intelligent machine learning based\
    \ total productive\n1 3\nM. Fernandes et al.\n14278\nmaintenance\napproach\nfor\n\
    achieving\nzero\ndowntime\nin\nindustrial\nmachinery.\nComput\nInd\nEng\n157:107267.\n\
    https://doi.org/10.1016/j.cie.2021.107267\n62. Giordano D, Mellia M, Cerquitelli\
    \ T (2021) K-mdtsc: K-\nmulti-dimensional time-series clustering algorithm. Electronics\n\
    10(10):1166. https://doi.org/10.3390/electronics10101166\n63. Ruiz-Sarmiento J-R,\
    \ Monroy J, Moreno F-A, Galindo C,\nBonelo J-M, Gonzalez-Jimenez J (2020) A predictive\
    \ model\nfor the maintenance of industrial machinery in the con-\ntext\nof\nindustry\n\
    4.0.\nEng\nAppl\nArtif\nIntell\n87:103289.\nhttps://doi.org/10.1016/j.engappai.2019.103289,\n\
    http://www.\nsciencedirect.com/science/article/pii/S0952197619302489\n64. Chen\
    \ X, Van Hillegersberg J, Topan E, Smith S, Roberts\nM (2021) Application of data-driven\
    \ models to predictive\nmaintenance: Bearing wear prediction at tata steel. Expert\
    \ Syst\nAppl 186:115699. https://doi.org/10.1016/j.eswa.2021.115699\n65. Yu W,\
    \ Dillon T, Mostafa F, Rahayu W, Liu Y (2020) A\nGlobal Manufacturing Big Data\
    \ Ecosystem for Fault Detection\nin Predictive Maintenance. IEEE Trans Ind Inf\
    \ 16(1):183–192.\nhttps://doi.org/10.1109/TII.2019.2915846\n66. Zhai S, Gehring\
    \ B, Reinhart G (2021) Enabling predictive\nmaintenance integrated production\
    \ scheduling by operation-\nspecific health prognostics with generative deep learning.\
    \ J Manuf\nSyst. https://doi.org/10.1016/j.jmsy.2021.02.006\n67. Kolokas N, Vafeiadis\
    \ T, Ioannidis D, Tzovaras D (2020)\nA\ngeneric\nfault\nprognostics\nalgorithm\n\
    for\nmanufac-\nturing\nindustries\nusing\nunsupervised\nmachine\nlearning\nclassifiers.\n\
    Simul\nModel\nPract\nTheory\n103:102109.\nhttps://doi.org/10.1016/j.simpat.2020.102109,\n\
    http://www.\nsciencedirect.com/science/article/pii/S1569190X20300472\n68. Turkoglu\n\
    B,\nKomesli\nM,\nUnluturk\nMS\n(2019)\nApplica-\ntion of Data Mining in Failure\
    \ Estimation of Cold Forging\nMachines: An Industrial Research. Stud Inf Control\
    \ 28(1):87–94.\nhttps://doi.org/10.24846/v28i1y201909\n69. Naskos A, Gounaris\
    \ A, Metaxa I, Koechling D, Stirna J (2019)\nDetecting anomalous behavior towards\
    \ predictive maintenance.\nIn: Proper HA (ed) Advanced Information Systems Engineering\n\
    Workshops (CAISE 2019), Lecture Notes in Business Information\nProcessing, vol\
    \ 349, pp 73–82\n70. Rousopoulou V, Nizamis A, Giugliano L, Haigh P, Martins L,\n\
    Ioannidis D, Tzovaras D, Stirna J (2019) Data Analytics Towards\nPredictive Maintenance\
    \ for Industrial Ovens A Case Study Based\non Data Analysis of Various Sensors\
    \ Data. In: Proper HA (ed)\nAdvanced Information Systems Engineering Workshops\
    \ (CAISE\n2019), Lecture Notes in Business Information Processing, vol 349,\n\
    pp 83–94\n71. Bukkapatnam STS, Afrin K, Dave D, Kumara SRT (2019)\nMachine learning\
    \ and AI for long-term fault prognosis in complex\nmanufacturing systems. CIRP\
    \ Ann-Manuf Technol 68(1):459–\n462. https://doi.org/10.1016/j.cirp.2019.04.104\n\
    72. Vrabiˇc R, Kozjek D, Butala P (2017) Knowledge elicitation\nfor fault diagnostics\
    \ in plastic injection moulding: A case for\nmachine-to-machine communication.\
    \ CIRP Ann Manuf Technol\n66(1):433–436. https://doi.org/10.1016/j.cirp.2017.04.001,\
    \ http://\nwww.sciencedirect.com/science/article/pii/S000785061730001X\n73. Bampoula\
    \ X, Siaterlis G, Nikolakis N, Alexopoulos K (2021) A\ndeep learning model for\
    \ predictive maintenance in cyber-physical\nproduction systems using lstm autoencoders.\
    \ Sensors 21(3):972.\nhttps://doi.org/10.3390/s21030972\n74. Syafrudin M, Alfian\
    \ G, Fitriyani NL, Rhee J (2018) Performance\nAnalysis of IoT-Based Sensor, Big\
    \ Data Processing, and Machine\nLearning Model for Real-Time Monitoring System\
    \ in Automotive\nManufacturing. Sensors 18(9). https://doi.org/10.3390/s18092946\n\
    75. Avendano DN, Caljouw D, Deschrijver D, Van Hoecke S\n(2021) Anomaly detection\
    \ and event mining in cold forming\nmanufacturing processes. Int J Adv Manuf Technol:1–16.\n\
    https://doi.org/10.1007/s00170-020-06156-2\n76. Zhang Y, Beudaert X, Argando˜na\
    \ J, Ratchev S, Munoa\nJ\n(2020)\nA\ncpps\nbased\non\ngbdt\nfor\npredicting\n\
    failure\nevents in milling. Int J Adv Manuf Technol 111(1):341–357.\nhttps://doi.org/10.1007/s00170-020-06078-z\n\
    77. Chen\nB,\nLiu\nY,\nZhang\nC,\nWang\nZ\n(2020)\nTime\nSeries\nData\nfor\nEquipment\n\
    Reliability\nAnalysis\nWith\nDeep\nLearning.\nIEEE\nAccess\n8:105484–105493.\n\
    https://doi.org/10.1109/ACCESS.2020.3000006\n78. Kiangala KS, Wang Z (2020) An\
    \ effective predictive maintenance\nframework for conveyor motors using dual time-series\
    \ imaging\nand convolutional neural network in an industry 4.0 environment.\n\
    IEEE Access:1. https://doi.org/10.1109/ACCESS.2020.3006788\n79. Liu C, Tang D,\
    \ Zhu H, Nie Q (2021) A novel predictive\nmaintenance method based on deep adversarial\
    \ learning in the\nintelligent manufacturing system. IEEE Access 9:49557–49575.\n\
    https://doi.org/10.1109/ACCESS.2021.3069256\n80. Tran\nT,\nLundgren\nJ\n(2020)\n\
    Drill\nfault\ndiagnosis\nbased\non the scalogram and mel spectrogram of sound\
    \ signals\nusing artificial intelligence. IEEE Access 8:203655–203666.\nhttps://doi.org/10.1109/ACCESS.2020.3036769\n\
    81. Xu Y, Sun Y, Liu X, Zheng Y (2019) A Digital-Twin-Assisted\nFault Diagnosis\
    \ Using Deep Transfer Learning. IEEE Access\n7:19990–19999. https://doi.org/10.1109/ACCESS.2018.2890566\n\
    82. Kontaki\nM,\nGounaris\nA,\nPapadopoulos\nAN,\nTsichlas\nK,\nManolopoulos Y\
    \ (2016) Efficient and flexible algorithms for\nmonitoring distance-based outliers\
    \ over data streams. Inf Syst\n55:37–53. https://doi.org/10.1016/j.is.2015.07.006,\
    \ http://www.\nsciencedirect.com/science/article/pii/S0306437915001349\n83. Luxton\
    \ DD (2015) Artificial Intelligence in Behavioral and\nMental Health Care. Technical\
    \ Report, https://www.sciencedirect.\ncom/science/article/pii/B9780124202481000015\n\
    84. Mitchell\nTM\n(2006)\nThe\nDiscipline\nof\nMachine\nLearn-\ning. Technical\
    \ Report, http://www-cgi.cs.cmu.edu/∼tom/pubs/\nMachineLearningTR.pdf\n85. Meyer\
    \ B, Choppy C, Staunstrup J, van Leeuwen J (2009)\nViewpoint research evaluation\
    \ for computer science. Commun\nACM 52(4):31–34. https://doi.org/10.1145/1498765.1498780\n\
    86. (2021). History of IEEE. https://www.ieee.org/about/ieee-history.\nhtml\n\
    87. Efthymiou K, Pagoropoulos A, Papakostas N, Mourtzis D,\nChryssolouris G (2012)\
    \ Manufacturing systems complexity\nreview: Challenges and outlook. In: Procedia\
    \ CIRP, vol 3. Elsevier\nB.V., pp 644–649\n88. Garc´ıa-Mart´ın E, Rodrigues CF,\
    \ Riley G, Grahn H (2019) Estima-\ntion of energy consumption in machine learning.\
    \ J Parallel Distrib\nComput 134:75–88. https://doi.org/10.1016/j.jpdc.2019.07.007\n\
    89. Mulders M, Haarman M (2017) Predictive Maintenance 4.0,\nPredict the unpredictable\
    \ (PwC Publication). Technical Report,\nPwC and Mainnovation\n90. Gama J, ˇZliobait˙c\
    \ I, Bifet A, Pechenizkiy M, Bouchachia A\n(2014) A survey on concept drift adaptation.\
    \ ACM Comput\nSurv 46(4):1–37. https://doi.org/10.1145/2523813, http://dl.acm.\n\
    org/citation.cfm?doid=2597757.2523813\n91. Tsymbal A (2004) The problem of concept\
    \ drift: definitions\nand related work. Computer Science Department, Trinity College\n\
    Dublin.\n1 3\nMachine learning techniques applied to mechanical fault diagnosis...\n\
    14279\n92. Gomes HM, Read J, Bifet A, Barddal JP, Gama J (2019)\nMachine Learning\
    \ for Streaming Data: State of the Art,\nChallenges, and Opportunities. SIGKDD\
    \ Explor Newsl 21(2):6–\n22. https://doi.org/10.1145/3373464.3373470\n93. Krempl\
    \ G,\nˇZliobaite I, Brzezi´nski D, H¨ullermeier E, Last\nM, Lemaire V, Noack T,\
    \ Shaker A, Sievi S, Spiliopoulou\nM, Stefanowski J (2014) Open challenges for\
    \ data stream\nmining research. ACM SIGKDD Explor Newslett 16(1):1–\n10. https://doi.org/10.1145/2674026.2674028,\
    \ https://dl.acm.org/\ncitation.cfm?id=2674028\nPublisher’s note Springer Nature\
    \ remains neutral with regard to\njurisdictional claims in published maps and\
    \ institutional affiliations.\nMarta\nFernandes\nis\na\nresearcher with the Research\n\
    Group on Intelligent Engi-\nneering and Computing for\nAdvanced\nInnovation\n\
    and\nDevelopment\n(GECAD)\nin\nPorto, Portugal and has been\nan invited assistant\
    \ professor\nwith the School of Engineer-\ning, Polytechnic Institute of\nPorto\
    \ since 2017. Her research\ninterests\ninclude\nmachine\nlearning, data science,\
    \ data\nstream\nlearning,\nand\nfault\ndetection and prediction. She\nis currently\
    \ pursuing a Ph.D.\ndegree in computer science at the University of Salamanca,\
    \ Spain\nand has received a Ph.D. Research Scholarship from the Portuguese\nFoundation\
    \ for Science and Technology.\nJuan Manuel Corchado was\nthe vice president for\
    \ research\nand technology transfer from\n2013 to 2017 and the direc-\ntor of\
    \ the Science Park with\nthe University of Salamanca,\nSpain, where he was also\n\
    the director of the Doctoral\nSchool until 2017. He has\nbeen elected twice as\
    \ the Dean\nof the Faculty of Science, Uni-\nversity of Salamanca. He has\nbeen\
    \ a visiting professor with\nthe Osaka Institute of Tech-\nnology since 2015 and\
    \ a vis-\niting professor with University\nTechnology Malaysia since 2017. He\
    \ is currently the director of\nthe Bioinformatics, Intelligent Systems, and Educational\
    \ Technology\n(BISITE) Research Group, which he created in 2000. He is also the\n\
    president of the IEEE Systems, Man and Cybernetics Spanish Chapter\nand the academic\
    \ director of the Institute of Digital Art and Ani-\nmation, University of Salamanca,\
    \ where he is also a full professor.\nHe also oversees the masters programs in\
    \ digital animation, security,\nmobile technology, community management, and management\
    \ for TIC\nEnterprises with the University of Salamanca. He is also a member\n\
    of the Advisory Group on Online Terrorist Propaganda of the Euro-\npean Counter\
    \ Terrorism Centre (EUROPOL). He is also an editor and\nthe editor-in-chief of\
    \ specialized journals, such as the Advances in\nDistributed Computing and Artificial\
    \ Intelligence Journal, the Interna-\ntional Journal of Digital Contents and Applications,\
    \ and the Oriental\nJournal of Computer Science and Technology.\nGoreti Marreiros\
    \ is a pro-\nfessor in the Department of\nInformatics Engineering at the\nSchool\
    \ of Engineering of the\nPolytechnic Institute of Porto\n(ISEP). She is also subdirector\n\
    of the masters degree in engi-\nneering and artificial intelli-\ngence, director\
    \ of the post-\ngraduation in Big Data &\nDecision Making in ISEP and\nmember\
    \ of ISEPs Scientific\nCouncil. In recent years, she\nhas taught and been responsi-\n\
    ble for several courses on deci-\nsion support systems, multia-\ngent systems\
    \ and artificial intelligence at the graduate and post-\ngraduate level. She has\
    \ also participated in 15 projects as Principal\nInvestigator (8 European projects\
    \ - H2020; ITEA and ERASMUS+ -\nand 7 national projects - FCT; P2020) and in 20\
    \ projects as Researcher.\nMs. Marreiros is the vice-president of the Portuguese\
    \ Association of\nArtificial intelligence.\n1 3\nM. Fernandes et al.\n14280\n"
  inline_citation: '>'
  journal: Applied intelligence (Boston)
  limitations: '>'
  pdf_link: https://link.springer.com/content/pdf/10.1007/s10489-022-03344-3.pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'Machine learning techniques applied to mechanical fault diagnosis and fault
    prognosis in the context of real industrial manufacturing use-cases: a systematic
    literature review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/su14106199
  analysis: '>'
  authors:
  - Areej Alhothali
  - Maram Albsisi
  - Hussein Assalahi
  - Tahani I. Aldosemani
  citation_count: 20
  full_citation: '>'
  full_text: ">\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\n\
    Citation: Alhothali, A.; Albsisi, M.;\nAssalahi, H.; Aldosemani, T.\nPredicting\
    \ Student Outcomes in\nOnline Courses Using Machine\nLearning Techniques: A Review.\n\
    Sustainability 2022, 14, 6199.\nhttps://doi.org/10.3390/su14106199\nAcademic Editors:\
    \ Neil Gordon and\nHan Reichgelt\nReceived: 27 March 2022\nAccepted: 13 May 2022\n\
    Published: 19 May 2022\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional\
    \ claims in\npublished maps and institutional afﬁl-\niations.\nCopyright:\n© 2022\
    \ by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open\
    \ access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative\
    \ Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nsustainability\nReview\nPredicting Student Outcomes in Online Courses\
    \ Using Machine\nLearning Techniques: A Review\nAreej Alhothali 1,*,†\n, Maram\
    \ Albsisi 1,†\n, Hussein Assalahi 2\nand Tahani Aldosemani 3\n1\nDepartment of\
    \ Computer Science, Faculty of Computing and Information Technology,\nKing Abdulaziz\
    \ University, Jeddah 22254, Saudi Arabia; malbsisi0002@stu.kau.edu.sa\n2\nEnglish\
    \ Language Institute, King Abdulaziz University, Jeddah 22254, Saudi Arabia; hassalahi@kau.edu.sa\n\
    3\nCollege of Education, Prince Sattam bin Abdulaziz University, Al-Kharj 16278,\
    \ Saudi Arabia;\nt.aldosemani@psau.edu.sa\n*\nCorrespondence: aalhothali@kau.edu.sa\n\
    †\nThese authors contributed equally to this work.\nAbstract: Recent years have\
    \ witnessed an increased interest in online education, both massive open\nonline\
    \ courses (MOOCs) and small private online courses (SPOCs). This signiﬁcant interest\
    \ in\nonline education has raised many challenges related to student engagement,\
    \ performance, and\nretention assessments. With the increased demands and challenges\
    \ in online education, several\nresearchers have investigated ways to predict\
    \ student outcomes, such as performance and dropout in\nonline courses. This paper\
    \ presents a comprehensive review of state-of-the-art studies that examine\nonline\
    \ learners’ data to predict their outcomes using machine and deep learning techniques.\
    \ The\ncontribution of this study is to identify and categorize the features of\
    \ online courses used for learners’\noutcome prediction, determine the prediction\
    \ outputs, determine the strategies and feature extraction\nmethodologies used\
    \ to predict the outcomes, describe the metrics used for evaluation, provide a\n\
    taxonomy to analyze related studies, and provide a summary of the challenges and\
    \ limitations in\nthe ﬁeld.\nKeywords: MOOCs; SPOCs; student performance; student\
    \ dropout; machine learning; learning\nbehaviour; learning analytics\n1. Introduction\n\
    Online education has revolutionized the way people learn and has made education\n\
    more accessible and affordable to numerous people worldwide. Despite the advantages\
    \ and\nincreased interest in online and distance learning, educational institutions\
    \ are becoming\nincreasingly concerned about students’ performance and retention\
    \ rates, particularly low\ncertiﬁcation/graduation and dropout/completion rates.\
    \ Failing or dropping out of an\nonline course or program is often considered\
    \ a key parameter by institutional authorities for\nassessing program/course quality\
    \ and allocating resources. Dropout and low certiﬁcation\nrates can also pose\
    \ a potential risk to an institution’s reputation, proﬁt, and funding [1].\nThese\
    \ outcomes also have signiﬁcant consequences for a student’s self-esteem, well-being,\n\
    employment, and chances of graduating [1,2]. As a result, ﬁnding more efﬁcient\
    \ approaches\nto forecasting students’ performance as early as possible is critical\
    \ for institutions, stu-\ndents, and educators to take proactive steps toward\
    \ improving students’ online learning\nexperiences and establishing intervention\
    \ strategies that target students’ needs. With the\nincreased interest in online\
    \ education and the large amount of data produced by learners\nthrough their interactions\
    \ with online platforms, researchers have proposed methods to\nanalyze learners’\
    \ behavioral data to predict and improve educational outcomes.\nLearning analytics\
    \ (LA), more commonly known as educational data mining (EDM) [3],\nthe task of\
    \ analyzing and ﬁnding patterns in learners’ data for decision-making purposes,\n\
    has attracted many researchers in recent years. Learning analytic tools enable\
    \ institutions\nSustainability 2022, 14, 6199. https://doi.org/10.3390/su14106199\n\
    https://www.mdpi.com/journal/sustainability\nSustainability 2022, 14, 6199\n2\
    \ of 23\nto gain an understanding of their students’ status, actions and preferences\
    \ individually,\nand in relation to their peers and the targeted educational objective.\
    \ This allows the tai-\nloring of material for individual students based on the\
    \ projected outcomes and preferred\nlearning styles. In online education, LA systems\
    \ assess students’ learning behavior by\nutilizing extensive data collection of\
    \ learners’ data, including student enrollment informa-\ntion, past and current\
    \ academic records, students’ online behavior, student surveys via\nquestionnaires\
    \ concerning courses and teaching techniques, and data from online discus-\nsion\
    \ forums. Scholars have also examined various learning-behavior attributes to\
    \ predict\nlearning outcomes, such as learners’ performance and retention. To\
    \ predict and analyze\nstudents’ outcomes in online courses, researchers have\
    \ examined several machine learning\nmodels, including support vector machines\
    \ (SVMs), linear regression (LR), random forest\n(RF), and deep learning models\
    \ such as convolutional neural networks (CNNs) and long\nshort-term memory (LSTM).\n\
    1.1. Previous Reviews of Student Outcome Prediction\nSeveral studies have provided\
    \ a comprehensive analysis of the literature in the ﬁeld\nof learning analytics,\
    \ investigating studies that analyzed learners’ behavior to optimize\neducational\
    \ outcomes. Three studies presented a review analysis similar to the survey\n\
    conducted in this research. Moreno-Marcos et al. [4] presented a meta-analysis\
    \ of state-\nof-the-art predictive models based on MOOC data. The analysis provides\
    \ an overview of\nthe features, methods, and metrics used in the literature. The\
    \ authors of [5,6] presented\na survey of learning analytic studies covering related\
    \ studies and methodologies used in\nthe literature. Several studies have also\
    \ presented literature reviews on online learners’\ndropping out from massive\
    \ open online courses (MOOCs) [7]. In addition, another review\npaper [8,9] focused\
    \ on performance prediction in MOOCs and small private online courses\n(SPOCs).\n\
    1.2. Method\nThis study presents a review of studies that aim to predict student\
    \ outcomes, which we\ndeﬁne in terms of achievement, completion, and continuation\
    \ in online educational courses.\nThe study also provides an overview and taxonomy\
    \ of the current related work with a\ndetailed analysis of the features and methods\
    \ used in the literature. It also covers different\ntypes of online learning environments,\
    \ including MOOCs and SPOCs. In particular, this\nstudy aimed to answer the following\
    \ research questions:\n1.\nWhat is the process followed by researchers for learner\
    \ outcome prediction?\n2.\nWhat are the predictive variables used to predict learner\
    \ outcome?\n3.\nWhat are the learner outcomes used in the literature?\n4.\nWhat\
    \ are the online learning platforms used in the literature?\n5.\nWhat are the\
    \ machine learning methodologies used in the literature?\n6.\nWhat are the challenges\
    \ and limitations, and future directions of this ﬁeld?\nThe literature contains\
    \ numerous studies that examine online learner data to better\nunderstand learners’\
    \ progress and outcomes in online courses. We included all stud-\nies published\
    \ between 2017 and 2021 that utilized users’ learning-behaviour data and\nemployed\
    \ machine learning and deep learning techniques to predict learner outcomes\n\
    in online courses, including student dropout and student performance (students\
    \ at risk,\nstudent grade, and student certiﬁcate acquisition). We searched a\
    \ number of electronic\ndatabases, such as Scopus and Web of Science, and publishers,\
    \ such as Springer, IEEE,\nElsevier, and Sage, using the keywords and queries\
    \ shown in Table 1. The search was\nexpanded using snowball search methods to\
    \ identify additional related studies.\nSustainability 2022, 14, 6199\n3 of 23\n\
    Table 1. Search strings used in the web search engine.\nConcept\nSearch Query\n\
    Learner Performance\n(“Grade” OR “Performance” “Success” OR “Failure” OR “Certiﬁcate”\
    \ OR\n“At-risk”)\nLearner Dropout\n(“Dropout” OR “Retention” OR “Completion” OR\
    \ “Attrition” OR “With-\ndrawal”)\nOnline Learning\n(“Online learning” OR “MOOC”\
    \ OR “Online course” OR “Online Educa-\ntion”)\nMachine Learning\n(“Classiﬁcation”\
    \ OR “Prediction” OR “Machine Learning” OR \" Predictive\nmodel” OR “Deep learning”)\n\
    1.3. Study Selection\nThe search was conducted between 1 May 2021 and 30 June\
    \ 2021, and returned a total\nof 137 research studies. To exclude studies that\
    \ do not match the inclusion criteria, we\nexamined the research’s paper title,\
    \ abstract, and sometimes the proposed methodologies\nif needed. We excluded studies\
    \ that did not employ any machine learning models in\ntheir proposed methodology,\
    \ those that utilized learning features obtained from blended\nlearning or face-to-face\
    \ learning, and those that used features obtained from mobile-based\nor e-book\
    \ platforms. Studies that aimed to detect and analyze other aspects of students’\n\
    online experiences, such as sentiment, engagement, satisfaction, and learning\
    \ style, were\nalso excluded from this review. Studies that only utilized features\
    \ obtained from learning\nsystems, such as previous academic achievements and\
    \ demographic data, were excluded\nfrom the study. We also excluded studies that\
    \ only looked into linguistic features of students’\nposts or comments on discussion\
    \ boards. Studies that predict students’ performance in\nassignments, exams, or\
    \ quizzes, rather than ﬁnal grades or ﬁnal exams, were excluded\nfrom this study.\
    \ After removing irrelevant research, we were left with 67 studies. Figure 1\n\
    shows the number of dropout and performance-prediction studies after applying\
    \ the\nexclusion criteria.\nFigure 1. The number of dropout and performance-prediction\
    \ studies after applying exclusion criteria.\n1.4. Student Outcomes Prediction\
    \ Model Process\nThe problem of forecasting student outcomes using learner-interaction\
    \ data is often\nformulated as a supervised problem that requires a dataset of\
    \ pairs of values (xij, yj), in\nwhich xij denotes the ith feature or attribute\
    \ that characterizes the jth student and yj is the\nlearner outcome (e.g., 1 =\
    \ dropout or 0 = no dropout). The goal of the predictive model is to\nlearn a\
    \ function h = y(x) that estimates the relationship between the input or independent\n\
    variables and the predicted outcome, which is the dependent variable in this problem.\n\
    The dependent variables can be continuous real values (e.g., grade = 93.0), dichotomous\n\
    variables that take two possible values (e.g., fail = 1, success = 0), or polychotomous\n\
    variables that have more than two possible values (e.g., high, intermediate, and\
    \ low).\nSimilar to other problems in machine learning, predictive models in this\
    \ domain com-\nmonly follow a ﬁve-step procedure, as shown in Figure 2. The ﬁrst\
    \ step involves collecting\ndatasets from online platforms. The samples in the\
    \ dataset represent learners’ information\nand activities in one or more courses\
    \ over a period of time. A preprocessing stage is then\nperformed to extract and\
    \ select valuable features. After extracting the features, several\nmachine learning\
    \ models are trained on the training set and validated on a validation set.\n\
    Then, hyper-parameter (e.g., number of training epochs, regularization penalty)\
    \ optimiza-\nSustainability 2022, 14, 6199\n4 of 23\ntion techniques are implemented\
    \ to choose the optimal hyper-parameters of the model.\nThen, the trained model\
    \ is evaluated on an unseen testing set, and the prediction accuracy\nis estimated\
    \ using metrics in the last step. The following sections provide information\n\
    related to each of these steps.\nFigure 2. The procedure of student outcome prediction.\n\
    The remainder of the paper is organized as follows: Section 2 provides a summary\
    \ of\nthe online learning environments used in the literature; Section 3 provides\
    \ an overview of\nthe courses or subjects used in the literature; Section 4 presents\
    \ a summary of the predictive\nvariables used in the literature, with detailed\
    \ analysis of feature-extraction and feature-\nselection techniques in Sections\
    \ 5 and 6, respectively. Section 7 provides an overview of\nthe models used in\
    \ previous studies. Section 9 provides details about learner outcomes,\na summary\
    \ of the studies that predict student performance in Section 9.1 and student\n\
    dropout in Section 9.2. Finally, Sections 10 and 11, give a summary of the related\
    \ studies\nand challenges in the ﬁeld. Online courses that are delivered to a\
    \ large scale of learners\nare called massive open online courses (MOOC), while\
    \ those that target private or speciﬁc\ngroups of students are called small private\
    \ online course (SPOC) [10]. Despite the similarity\nbetween these two types of\
    \ courses, they have distinctive characteristics. The number of\nenrolled students\
    \ in SPOCs, for example, is much smaller (15 to 20 students per course)\nthan\
    \ that of students in MOOCs (up to 10, 000 per course). The small number of SPOC\n\
    enrolments has contributed to enhancing teachers’ guidance and increasing retention\
    \ rates\nin comparison to MOOCs [10].\n2. Online Learning Environment\nSeveral\
    \ MOOC and SPOC learning systems, such as Coursera, edX, Moodle, and other\nprivate\
    \ online platforms, have been used to predict student outcomes. However, because\
    \ of\ntheir large number of students and courses, the majority of studies leveraged\
    \ data from\nMOOCs. MOOCs also have a higher dropout rate; thus, a large number\
    \ of studies have\ninvestigated methods to forecast dropout in MOOCs [10]. Figure\
    \ 3 shows the number of\nstudies that uses MOOCs and SPOC in dropout and performance\
    \ prediction. In the litera-\nture, we found that datasets are often collected\
    \ from various courses that range from one\ncourse to 56 courses, with several\
    \ samples/records that range from 104 to 597, 692 records.\nThe duration of data\
    \ collection also varies among studies. The researchers considered\ntwo strategies:\
    \ collecting data over a ﬁxed period, such as four weeks or 30 days, or over\n\
    different durations, such as two, three, and four weeks.\nFigure 3. The distribution\
    \ of the studies that use MOOCs and SPOCs.\nSeveral public datasets have been\
    \ used in related studies to predict student perfor-\nmance and dropout rates.\
    \ These datasets can be considered benchmark datasets, allowing\nresearchers to\
    \ evaluate the performance of the model compared with the others. The public\n\
    datasets in this domain are the Students’ Academic Performance Dataset (SAPData)\
    \ [11],\nwhich has been utilized for grade prediction [12]; Open University Learning\
    \ Analytics\nDataset (OULAD) [13] developed by the Open University (OU) and used\
    \ for at-risk [14],\npass/fail [15], grade [16], dropout [15], and engagement\
    \ prediction [17]; Center for Ad-\nSustainability 2022, 14, 6199\n5 of 23\nvanced\
    \ Research Through Online Learning (CAROL) [18] has been used for dropout [19]\n\
    and fail/success prediction [20,21]; KDD Cup 2015 (KDDcup) [22] has been extensively\n\
    used in the literature as a whole or subset to predict dropout, such as [23,24];\
    \ and HarvardX\nand MITx dataset (HMedx) [25] have been used for dropout prediction\
    \ [26] and perfor-\nmance prediction [26,27]. Figure 4 shows the statistics of\
    \ each dataset used in previous\nstudies. Most of the studies use the KDDcup 2015\
    \ dataset, where 19 studies have utilized\nthis dataset, followed by OULAD with\
    \ 6 studies, CAROL with 3 studies, and HMedx with\n2 studies. A summary of the\
    \ public-dataset usage is provided in Table 2.\nFigure 4. The distribution of\
    \ the studies that use publicly available dataset.\nTable 2. Public learning-analytic\
    \ datasets used in the literature.\nRef.\nDataset\nPlatform\nCourses\nRecords\n\
    Features\nOutcomes\n[11]\nSAPData\nKalboard 360\n12\n480\nDemographic, Academic\n\
    Background, Interaction\nPerformance\n[13]\nOULAD\nOU VLE\n22\n32,593\nDemographic,\
    \ Registration,\nAssessment, Interaction\nPerformance\n[18]\nCAROL\nOpenEdX\n\
    5\n78,623\nInteraction, Assessment\nDropout,\nperformance\n[22]\nKDDcup\nXuetangX\n\
    39\n120,542\nEnrollment, Course,\nInteraction\nDropout\n[25]\nHMedx\nedX\n17\n\
    597,692\nAcademic Background,\nVideo Interaction,\nAssessment\nDropout,\nperformance\n\
    3. Courses\nWhen analyzing students’ learning outcomes, researchers use two approaches\
    \ with\nrespect to the course subject, focusing on subject-independent or subject-dependent\
    \ features.\nMost studies in the literature use subject-independent features by\
    \ examining/analyzing\nstudents learning behavior independent of the course subject,\
    \ such as the number of\ndownload sources. However, a few studies have focused\
    \ on analyzing features related to\nthe subject of the study, such as utilizing\
    \ variables related to learning mathematics [28].\nIn the literature, student-outcome\
    \ predictive systems are often developed based on a\nsingle course or multiple\
    \ subjects. Researchers have identiﬁed ﬁve subject categories in the\nliterature:\
    \ humanities, social sciences, natural sciences, formal sciences, professions,\
    \ and ap-\nplied sciences [29]. Table 3 provides an overview of some subjects\
    \ and their categories.\nAs shown in Figure 5, the most widely used subject categories\
    \ are formal science, and more\nspeciﬁcally, computer-related subjects such as\
    \ programming and computer networking.\nSustainability 2022, 14, 6199\n6 of 23\n\
    Table 3. Subject categories used in the literature.\nSubject Category\nExample\
    \ Subject\nNo. of Studies\nHumanities\nOnline foreign language teaching, understanding\
    \ language\n6\nSocial sciences\nGeneral sociology, Social science\n7\nNatural\
    \ sciences\nAnalytical chemistry laboratory, Physics III\n6\nFormal sciences\n\
    Assembly Language, C programming, Calculus I\n33\nMedical sciences\nFirst aid\
    \ general knowledge, Public health research\n4\nProfessions and applied sciences\n\
    Circuits and Electronics\n17\nNot mentioned\n-\n37\nFigure 5. Distribution of\
    \ subject categories used in the related studies.\n4. Predictive Variables\nExisting\
    \ methods for forecasting online learners’ outcomes, such as dropout, grade,\n\
    and completion, are based on data-mining techniques, which entail collecting attributes\n\
    from learners’ data provided during their online study and then making predictions\
    \ using\nvarious data-mining tools. Several features have been investigated to\
    \ predict and estimate\nstudents’ learning outcomes. These features can be categorized\
    \ into pre-course information,\nsuch as demographic background, previous academic\
    \ background, course information;\nin-course information, such as interaction\
    \ data and learning behavior; and post-course\ndata, such as graded assessment\
    \ and ﬁnal grade.\nDemographic information, registration, or enrollment data mainly\
    \ describe the char-\nacteristics of the learners, including their name, gender,\
    \ age, mother tongue, geogra-\nphy/origin, occupation, socioeconomic status, and\
    \ hobbies. Previous academic background\ndescribes a learner’s past academic achievements,\
    \ including information about students’\ncumulative grade point averages. Learning\
    \ behavior, also referred to as engagement data\nor log data, is the behavior\
    \ students exhibit during their interaction with an online course.\nLearning behavior\
    \ features are often represented in terms of frequency or duration, focus-\ning\
    \ on particular targets such as content, assessment, and forums. Learning behavior\
    \ also\nincludes multimedia learning behavior (i.e., video-related features),\
    \ download courseware\nbehavior, text learning behavior (i.e., browsing online\
    \ content), and exercise-related be-\nhavior. The granularity of the extracted\
    \ pattern can constitute another level, by focusing\non more ﬁne-grained interaction\
    \ data with speciﬁc events such as multimedia or video,\npractice, and participation\
    \ events. Video or multimedia interaction data, for example, can\nbe pausing,\
    \ stopping, and replaying. Studies in the literature looked at different types\
    \ of\nfeatures. The most widely used features are interaction learning behavior\
    \ features. Only a\nsmall number of studies looked into ﬁne-grained multimedia,\
    \ practice, and participation\nfeatures. A summary of the learner data used in\
    \ the related studies is provided in Table 4.\nThroughout the analysis of the\
    \ studies, behavioral (log data) followed by demographic\ndata were the most widely\
    \ used in both dropout prediction and performance prediction,\nas shown in Figure\
    \ 6.\nSustainability 2022, 14, 6199\n7 of 23\nFigure 6. Statistics of feature\
    \ categories used in dropout prediction and performance prediction.\nTable 4.\
    \ Summary of learner data used in the literature and their categories.\nCategory\n\
    Features\nDemographic features\nDate of birth, birthplace, age, gender, parent\
    \ responsible for student, nationality, mother tongue\nAcademic background\nStudying\
    \ semester, GPA, grade level, section, education\nEnrollment data\nInformation\
    \ about student’s enrollment on a course\nCourse data\nNo. of enrolled students,\
    \ drop rate, course modules\nAttendance data\nStudent absent from class\nLearning\
    \ interaction (log data)\nActivity, visit resources, downloaded resources, play\
    \ resources, access a piece of content, class participa-\ntion, logins, starting\
    \ a lesson, page navigation, page closes\nMultimedia/video interactive data\n\
    Pause, replay, stop, open, close\nPractice data\nQuestions answered, tests, tries,\
    \ assessment scores\nParticipation data\nDiscussions on forums, polls, messages\
    \ posted, messages read\n5. Features Engineering\nThe initial form of learning-behavior\
    \ data is log data or a clickstream, which contains\ntwo major variables: the\
    \ timestamp and events (e.g., opening a page). In the literature,\nfour forms\
    \ of features have been used to represent learning behavior patterns: statistical,\n\
    statistical-temporal, raw, and raw-temporal features. Most studies on this subject\
    \ have\nrelied on a coarse-grained statistical process to represent learning events\
    \ in terms of fre-\nquency, rate, or length computed and accumulated over a speciﬁc\
    \ time frame, such as a day,\nweek, or several weeks. As a result, the learner\
    \ is represented in the predictive model as a\nﬁxed-length ﬂat vector of some\
    \ continuous values as a whole or as a sequence of features\n(e.g., the total\
    \ number of activities in a day or the rate of practicing behavior over time).\n\
    Other researchers have considered the statistical-temporal approach by analyzing\n\
    learner activity over any period (weeks or days). The data of each learner are\
    \ represented\nby a dynamic-length ﬂat vector that varies in length according\
    \ to the chosen duration.\nRecent research has focused on employing more ﬁne-grained\
    \ feature analysis to extract\ntemporal properties from raw data using deep-learning\
    \ algorithms. To automatically extract\ntemporal features from raw textual or\
    \ categorical data, one ﬁrst needs to encode these\nraw data into representations,\
    \ such as the one-hot encoding scheme. Then, deep-learning\nmodels with convolutional\
    \ layers have been utilized to automatically extract the most\nsigniﬁcant features\
    \ in the prediction task. As shown in Figure 7 in both dropout prediction\nand\
    \ performance prediction tasks, statistical features have been the most commonly\
    \ used\nfeature-extraction methodology. In contrast, a small number of studies\
    \ have looked into\nraw temporal features.\nFigure 7. Statistics of feature-extraction\
    \ methodologies used in the related studies.\nSustainability 2022, 14, 6199\n\
    8 of 23\n6. Feature Selection\nTo lower the dimensionality of the feature space,\
    \ two paradigms are often employed\nin machine learning: feature selection and\
    \ dimensionality reduction. The feature-selection\napproach is a preprocessing\
    \ procedure that is performed before the learning stage to choose\nthe most relevant\
    \ features for the predictive task and increase the model’s accuracy and\nefﬁciency.\
    \ In addition to the strategy of manually selecting pertinent features adopted\
    \ by\nmost researchers in the ﬁeld, several automatic feature-selection methodologies\
    \ have also\nbeen used in the ﬁeld. These methodologies can be categorized into\
    \ correlation-based,\nwrapper-based, and ensemble-based feature selection.\nSeveral\
    \ studies have applied correlation-based feature selection approaches such as\n\
    chi-square (X2), mutual information (MI), information gain, fast correlation-based\
    \ ﬁlter\n(FCBF), relief, and Pearson’s correlation coefﬁcient [21,30,31] to select\
    \ the topmost related\nfeatures to the targeted values and discard irrelevant\
    \ and noisy attributes. This approach\nhas the beneﬁt of being adaptable to any\
    \ machine-learning model.\nWrapper methods, in contrast, search the attribute\
    \ space and use a classiﬁer to de-\ntermine the best set of features. Several\
    \ wrapper-based methods have been used in the\nliterature, including recursive\
    \ feature elimination (RFE) [32], random hill climbing [27],\nand WrapperSubsetEval\
    \ [33]. Ensemble-based feature selection methods have been exam-\nined in several\
    \ studies [21]. Dimensionality reduction approaches such as principal compo-\n\
    nent analysis (PCA) map the feature space to a lower-dimensional space. Several\
    \ studies\nhave applied this method to reduce the dimensionality of the feature\
    \ space [20,34,35].\nThe main drawback of dimensionality reduction approaches\
    \ is that the newly generated\nfeatures do not reveal much information regarding\
    \ the original feature space.\n7. Models\nResearchers have used a wide range of\
    \ machine- and deep-learning techniques to pre-\ndict online student outcomes\
    \ from their online interaction data. The machine-learning mod-\nels utilized\
    \ in the ﬁeld are categorized into probabilistic models such as Naive Bayes (NB),\n\
    linear models such as logistic regression (LR), ensemble methods such as AdaBoost,\
    \ tree-\nbased methods such as decision trees (DT), rule-based methods such as\
    \ fuzzy-logic-based\napproaches, and instance-based learning such as K-nearest\
    \ neighbor (kNN). In addition,\nsome studies have developed optimization methods\
    \ or heuristic search-based algorithms,\nsuch as the Kstar algorithm, to predict\
    \ student outcomes. Deep-learning models, such as\nconvolutional neural networks\
    \ (CNNs) and long short-term memory (LSTM), have been\ninvestigated in recent\
    \ studies to predict learners’ outcomes in online learning. Table 5\nsummarizes\
    \ the machine- and deep-learning methods used in previous studies. Figure 8\n\
    shows the predictive-model categories used in relevant studies. As shown in the\
    \ ﬁgure,\ndeep-learning models are the most widely used in the literature. It\
    \ is worth noting that\nthe term deep learning refers to models that implement\
    \ neural network models with more\nthan three layers; thus, studies that develop\
    \ a feed-forward neural network with more than\nthree hidden layers are categorized\
    \ as deep learning.\nSustainability 2022, 14, 6199\n9 of 23\nFigure 8. Distribution\
    \ of machine-learning models in the literature.\nTable 5. Summary of the machine-learning\
    \ models used in the literature and their categories.\nModel Category\nModels\n\
    Probabilistic model\nNaive Bayes, Bayes network, Bayesian generalized linear (BGL),\
    \ Bayesian belief networks,\nLinear models\nLogistic regression (LR), support\
    \ vector machine (SVM), linear discriminant analysis (LDA), general-\nized linear\
    \ model (GLM), lasso linear regression (LLG), boosted logistic regression\nEnsemble\
    \ methods\nBagging, boosting, stacking, AdaBoost, gradient boosting (GB), eXtreme\
    \ gradient boosting (xgbLin-\near), stochastic gradient boosting (SGB)\nTree-based\
    \ models\nDecision tree (DT), random forest (RF), Bayesian additive regression\
    \ trees (BART)\nRule-Based models\nRule-based classiﬁer(JRip), fuzzy set rules\n\
    Instance-based learning\nk-nearest neighbors (kNN)\nNeural network\nMultilayer\
    \ perceptron(MLP) or artiﬁcial neural network (ANN)\nSequence ML models\nConditional\
    \ random ﬁelds (CRF)\nDeep neural network\nRecurrent neural network (RNN), gated\
    \ recurrent unit (GRU), long short-term memory (LSTM),\nconvolutional neural network\
    \ (CNN), squeeze-and-excitation networks (SE-net)\nOthers\nSearch algorithms (Kstar),\
    \ optimization algorithm (pigeon-inspired optimization (PIO)), matrix\ncompletion,\
    \ unsupervised learning model (self organized map (SOM))\n8. Evaluation Metrics\n\
    Predicting student outcomes is regularly formulated as a supervised learning prob-\n\
    lem, either as a classiﬁcation or regression problem. The predictive performance\
    \ of the\nproposed classiﬁcation model has been evaluated using metrics such as\
    \ accuracy (acc.),\nsensitivity, speciﬁcity, precision, recall, f1-measure (f1),\
    \ and area under the ROC curve\n(AUC) [36,37]. The confusion matrix summarizes\
    \ the prediction outcomes for a given\nclassiﬁcation problem in terms of true\
    \ positive (TP), true negative (TN), false positive (FP),\nand false negative\
    \ (FN), which are positive examples correctly predicted by the classiﬁer,\nnegative\
    \ examples correctly classiﬁed by the model, negative instances incorrectly classiﬁed\n\
    by the model, and positive examples incorrectly classiﬁed by the model, respectively.\
    \ Accu-\nracy is the most commonly used metric in both performance prediction\
    \ (55%) and dropout\nprediction (63%), even though accuracy is not suitable for\
    \ highly imbalanced datasets.\nThe performance of the regression model is measured\
    \ using the error or difference\nbetween the actual outcome (yi) of an instance\
    \ (i) and the estimated outcome by the model\n( ˆyi). Several error measures have\
    \ been used in this task, such as mean absolute error (MAE),\nroot mean square\
    \ error (RMSE), and root mean square error of approximation (RMSEA) [38].\nIn\
    \ addition, statistical measures such as r2 (R-squared) have been used to estimate\
    \ the\nperformance of regression models. Table 6 provides the metrics used in\
    \ the literature.\nSustainability 2022, 14, 6199\n10 of 23\nTable 6. The performance\
    \ metrics used for classiﬁcation and regression.\nPrediction Task\nMetric\nFormula\n\
    Classiﬁcation\nPrecision (P)\nTP\nTP + FP\nRecall (R)\nTP\nTP + FN\nAccuracy\n\
    TP + TN\nTP + TN + FN + FP\nF-score\n2 ∗ R ∗ P\nR + P\nRegression\nMSE\n1\nm\n\
    m\n∑\ni=1\n(yi − ˆyi)2\nRMSE\n1\nm\nm\n∑\ni=1\nq\n(yi − ˆyi)2\nMAE\n1\nm\nm\n\
    ∑\ni=1\n| (yi − ˆyi) |\nR2\n1− ∑m\ni=1 (yi − ˆyi)2\n∑m\ni=1 (yi − ¯y)2\n9. Student\
    \ Outcomes\nResearchers have explored and analyzed students’ characteristics and\
    \ learning behav-\nior for various reasons, including providing personalized learning\
    \ by building students’\nproﬁles, understanding students’ learning styles, and\
    \ optimizing educational outcomes.\nThey have investigated a wide range of outcomes\
    \ that can be grouped into two major\ncategories: learners’ performance predictions\
    \ and retention and completion predictions,\nas success and retention rate are\
    \ essential variables that institutions need to measure and\nassess frequently.\
    \ Learner-performance prediction systems aim to predict students’ per-\nformance\
    \ upon the completion of a given course, such as certiﬁcate, grade category, grade\n\
    value, success/failure, and risk prediction. Student-dropout prediction has attracted\
    \ many\nresearchers in the ﬁeld in recent years. The literature has witnessed\
    \ a large increase in\ncontributions to this issue owing to the increased use\
    \ of MOOCs. Finding more efﬁcient\napproaches to mitigate online student withdrawals\
    \ is of fundamental importance to institu-\ntions, students, and staff. These\
    \ problems are often formulated as classiﬁcation (binary or\nmulti-class) or regression.\
    \ In the following subsection, a summary of the proposed studies\non student performance\
    \ and student retention/dropout is presented.\n9.1. Predicting Students Performance\n\
    To predict student performance, studies have examined four performance measures:\n\
    certiﬁcate acquisition, grade, failure/success, and at-risk prediction. Table\
    \ 7 summarizes\nthe proposed models for student-performance prediction.\nSustainability\
    \ 2022, 14, 6199\n11 of 23\nTable 7. Summary of the performance-prediction studies\
    \ that include type of online education\n(MOOC or SPOC), platform (the dataset\
    \ or the platform; blank if not mentioned), sample (the sample\nof instances in\
    \ the dataset; blank if not mentioned), courses (number of courses in the dataset;\
    \ blank\nif not stated), features (B: learning behavior or log data, D: demographic\
    \ information, V: video click\nstream, A: assessment data (grade, answer data),\
    \ C: course data, O: other, such as a forum, discussion,\nand academic background,\
    \ attendance data, features extraction (FE) method, either statistical (S),\n\
    temporal (T), or raw data (R), classes, output, best-performing model/s, and best\
    \ performance-\naccuracy scores. F1-score (F1), AUC, or regression metrics is\
    \ provided if the accuracy score was not\nreported. Scores are rounded up to two\
    \ decimal places and reported with the week if the performance\nwas estimated\
    \ during different time spans. ** and * indicate the performance using different\
    \ classes\n(binary, multi, or regression) or using different features.\nRef.\n\
    Type\nPlatform\nDataset\nSample/\nCourse\nFeatures\nFE\nModel\nClass\nOutput\n\
    Accuracy\nB\nD\nV\nA\nC\nO\nS\nT\nR\n[39]\nSPOC\n336/1\n✓\n✓\nSVM\nMulti\nGrade\n\
    0.95\n[12]\nMOOC\nSAPData\n500/12\n✓\n✓\n✓\n✓\nMLP\nMulti\nGrade\n0.84\n[40]\n\
    SPOC\n202/-\n✓\n✓\n✓\nLR\nBinary\nAt risk\nF1 = 0.66\n[41]\nMOOC\nedX\n3530/1\n\
    ✓\n✓\nRF\nBinary\nCertiﬁcate\nw5:0.95\n[27]\nMOOC\nHMedx\n597,692/15\n✓\n✓\n✓\n\
    RF\nBinary\nCertiﬁcate\n0.99\n[42]\nMOOC\n9990/1\n✓\n✓\n✓\nLSVM\nBinary\nCertiﬁcate\n\
    w7:0.99\n[43]\nSPOC\nMoodle\n-/1\n✓\n✓\nDT\nBinary\nGrade\nw7:F1 = 0.85\n[44]\n\
    MOOC\nCoursera\n-/2\n✓\n✓\n✓\nRNN\nReg.\nGrade\nRMSE = 0.058\n[45]\nMOOC\nedX\n\
    18,927/15\n✓\n✓\n✓\n✓\nBGL\nBinary\nCertiﬁcate\nw3:AUC = 0.90\n[31]\nMOOC\n603/1\n\
    ✓\n✓\n✓\n✓\nRF+\nBagging\nBinary\nCertiﬁcate\n0.79\n[46]\nMOOC\nedX\n5537/9\n\
    ✓\n✓\nLR\nBinary\nPass/fail\nw5:0.94\n[47]\nSPOC\nBlackboard\n-/-\n✓\n✓\n✓\n✓\n\
    GP\nBinary\nAt risk\n0.89\n[48]\nMOOC\n300/1\n✓\n✓\n✓\nCART\nMulti\nGrade\n0.90\n\
    [20]\nMOOC\nCAROL\n3585/1\n✓\n✓\n✓\n✓\nDL\nMulti\nPass/fail\nw8:0.98\n[49]\nSPOC\n\
    Moodle\n6119/1\n✓ **\n✓ *\n✓\nFURIA\nMulti\nGrade\n0.76 *, 0.99 **\n[50]\nSPOC\n\
    Fanya\n5542/1\n✓\n✓\n✓\n✓\n✓\nLR *\nDNN **\nReg. *\nMulti **\nGrade\nMSE = 20\
    \ *\n0.88 **\n[34]\nMOOC\nUCATx,\ncoursera\n24,789/5\n✓\n✓\n✓\nSMOTE\nSSELM\n\
    Binary\nComplete\n0.97\n[51]\nMOOC\nHMedx\nOULAD\n8000/6\n✓\n✓\n✓\nGBM\nBinary\n\
    At risk\n0.95\n[30]\nSPOC\n-/3\n✓\n✓\n✓\n✓\nRF\nBinary\nPass/fail\nw1:AUC = 0.85\n\
    [52]\nMOOC\n1528/1\n✓\n✓\n✓\nLSTM +\nDSP\nBinary\nPass/fail\n0.91\n[15]\nMOOC\n\
    OULAD\n22,437/22\n✓\n✓\n✓\n✓\nGBM\nBinary\nPass/fail\nAUC = 0.93\n[53]\nMOOC\n\
    HPU LMS\n1073/1\n✓\n✓\n✓\n✓\nSSL\nRegression\nReg.\nGrade\nMAE = 1.146\n[54]\n\
    SPOC\nedX\n124/1\n✓\n✓\n✓\n✓\nTrAdaboost\nBinary\nAt risk\nAUC = 0.70\n[55]\n\
    MOOC\nXuetangX\n12,847/1\n✓\n✓\nGRU-RNN\nReg.\nComplete\nr2 = 0.84\n[26]\nMOOC\n\
    HMedx\n641,138/-\n✓\n✓\nDNN\nBinary\nCertiﬁcate\n0.89\n[56]\nSPOC\n122/1\n✓\n\
    ✓\n✓\nRegression\nanalysis\nReg.\nGrade\n0.85\n[57]\nMOOC\nLagunita\n130,000/1\n\
    ✓\n✓\n✓\nRNN **\nRNN *\nMulti **\nReg. *\nGrade\nw5:0.55 **\nRMSE = 8.65 *\n[58]\n\
    MOOC\nHPU LMS\n1073/1\n✓\n✓\n✓\n✓\n✓\nMultiview\nSSL\nRegression\nReg.\nGrade\n\
    MAE = 1.07\n[59]\nMOOC\n1075/2\n✓\n✓\n✓\nSVT\nReg.\nGrade\nRMSE = 0.30\nSustainability\
    \ 2022, 14, 6199\n12 of 23\nTable 7. Cont.\nRef.\nType\nPlatform\nDataset\nSample/\n\
    Course\nFeatures\nFE\nModel\nClass\nOutput\nAccuracy\nB\nD\nV\nA\nC\nO\nS\nT\n\
    R\n[60]\nMOOC\nedX\n6241/2\n✓\n✓\nJRIP\nBinary\nGrade\n0.70\n[33]\nMOOC\nedX\n\
    -/3\n✓\n✓\n✓\nRF\nBinary\nGrade\n0.79\n[21]\nMOOC\nCAROL\n49,551/4\n✓\n✓\n✓\n\
    ✓\nEnsemble\nMulti\nPass/fail\nw7:0.93\n[61]\nSPOC\nMoodle\n69/1\n✓\n✓\nARM\n\
    Binary\nGrade\n[14]\nMOOC\nOULAD\n32,593/7\n✓\n✓\n✓\n✓\n✓\nRF *, GB **\nBinary\
    \ *\nMulti **\nAt\nrisk,\nGrade\n0.91 *\n0.73 **\n[62]\nMOOC\nMoodle\n66/1\n✓\n\
    ✓\nKNN\nBinary\nAt risk\n0.65\n[63]\nSPOC\n104/ 1\n✓\n✓\n✓\n✓\nNB\nMulti\nGrade\n\
    0.86\n[16]\nMOOC\nOULAD\n32,593/-\n✓\n✓\n✓\n✓\nCNN +\nLSTM\nMulti\nGrade\n0.61\n\
    [64]\nSPOC\nMoodle\n150/1\n✓\n✓\n✓\n✓\nKNN\nMulti\nGrade\n0.87\n[38]\nMOOC\nMoodle\n\
    802/4\n✓\n✓\n✓\nLR\nReg.\nGrade\nRMSEA = 0.13\n[65]\nMOOC\n2556/2\n✓\n✓\n✓\n✓\n\
    DNN\nReg.\nGrade\nw5:MAE = 6.8\n9.1.1. Certiﬁcate Acquisition Prediction\nStudent\
    \ certiﬁcate acquisition is one of the effective measures of learner performance\n\
    in professional courses. In MOOCs, learners earn a certiﬁcate of completion if\
    \ they ﬁnish\nthe course and meet the course requirements. Other specialty credentials\
    \ are available upon\nrequest and are often expensive. All research in this category\
    \ has focused on predicting the\nattainment of a certiﬁcate of completion. Thus,\
    \ predicting certiﬁcate and course completion\nattainment are considered the same.\n\
    Korosi et al. [31] developed a certiﬁcate-prediction algorithm based on features\
    \ re-\nlated to learning behavior, mouse behavior, video-watching attitudes, and\
    \ text inputs.\nThe gain-ratio feature selection approach was implemented using\
    \ different classiﬁcation al-\ngorithms, and the best performance was obtained\
    \ by random forest and bagging. Similarly,\nAl-Shabandar et al. [27] proposed\
    \ an approach that uses a random forest algorithm and\nhill climbing to automatically\
    \ select statistical features from learners’ demographical and\nbehavioral attributes.\
    \ The model was evaluated on the HMedx dataset [66] with undersam-\npling of the\
    \ majority class to solve the class imbalance problem. Imran et al. [26] proposed\
    \ a\ndeep neural network model to predict student dropout and certiﬁcate acquisition\
    \ based on\nlearner behavioral data. Liang et al. [42] proposed an approach to\
    \ improve online learning\nby offering a personalized proﬁle to guide student\
    \ behavior. The authors explored different\nclassiﬁcation models to predict whether\
    \ a student will obtain a certiﬁcate based on the\nJaccard coefﬁcient similarity\
    \ of student proﬁles and learning behavior. The model was\ntested over different\
    \ weeks (5–7 weeks), and the best results were obtained using the SVM\nmodel at\
    \ 7 weeks. Ruipérez-Valiente et al. [41] proposed an approach that uses statistical\n\
    learning behavior and progress features with several machine-learning methods\
    \ to predict\nwhether a student will obtain a certiﬁcate or not.\n9.1.2. Grade\
    \ Prediction\nAnother perspective on student success is students’ grades. Several\
    \ publications have\nproposed models for predicting student grades in various\
    \ assessments, such as the course\ngrade or ﬁnal-exam grades, quizzes, or assignments\
    \ in a course. Researchers have ap-\nproached student-grade predictions through\
    \ classiﬁcation and regression models. Several\nstudies have used binary classiﬁcation\
    \ to predict learner failure and success [33,43,54,60,61].\nOther studies categorized\
    \ grades into multiple categories, such as excellent, good, moderate,\nand fail\
    \ [48]; withdrawn, fail, pass, and distinction [14]; and good, medium, and low\
    \ [12].\nMainstream studies examined several machine-learning models with manually\
    \ se-\nlected statistical learning behavior features to predict learners’ grades.\
    \ Xiao et al. [48]\nemployed the classiﬁcation and regression tree (CART) algorithm\
    \ on statistical demo-\nSustainability 2022, 14, 6199\n13 of 23\ngraphic and learning\
    \ behavior data to classify students’ ﬁnal grades into one of four classes.\n\
    Similarly, Rahman and Islam [12] developed an approach based on demographic, academic\n\
    background, behavioral, and parents’ participation data to classify learners’\
    \ grades into\nthree categories. They employed different algorithms with ensemble\
    \ ﬁltering, and the artiﬁ-\ncial neural network (ANN) model provided the best\
    \ results. In addition, Villagra et al. [39]\nproposed a machine-learning model\
    \ that utilized normalized learning-behavior data to\nclassify students into the\
    \ ﬁve categories of the standard grading system. An SVM model\noutperformed other\
    \ baseline models in this task.\nAdnan et al. [14] employed several machine models\
    \ that use learners’ demographics,\nand learning-behavior attributes to classify\
    \ student performance into binary and multiclass cat-\negories. The results show\
    \ that gradient boosting (GB) and RF scored the highest performance\nfor multiclass\
    \ and binary classification, respectively. In addition, Singh and Sachan [64]\
    \ uti-\nlized learning behaviors, along with some secondary features, such as\
    \ intermediate quiz\nperformance and class attendance, to predict learner grade\
    \ category. SVM and kNN were\nemployed, and kNN showed better accuracy. Huang\
    \ et al. [33] proposed a binary academic-\nperformance prediction model using\
    \ behavioral data. Several machine-learning models\nwere tested, and the best\
    \ performance was achieved using RF.\nChi and Huang [63] proposed a naive Bayes\
    \ algorithm with the Laplace smoothing\nmethod to predict student performance\
    \ and produce midterm-stage warnings. Using\nassessment, attendance, and other\
    \ learning-behavior features, the model was evaluated on\na one-course dataset\
    \ and achieved the best accuracy in comparison with other techniques.\nYu [50]\
    \ developed a regression and classiﬁcation models for student performance. They\n\
    used statistical features from log data with a linear regression model (LR) to\
    \ estimate\nstudents’ grades and a deep neural network (DNN) to classify students\
    \ into three classes.\nKokoÃ˘g et al. [61] utilized assignment submission pattern\
    \ and association rule mining\n(ARM) analysis to determine whether a student will\
    \ pass or fail a course. Karlos et al. [58]\ndeveloped a multi-view semi-supervised\
    \ regression model to predict distance-learner\ngrades based on learning activities,\
    \ demographic, attendance, and assessment information.\nThe model was trained\
    \ on data obtained from Hellenic Open University (HOU) LMS.\nOther studies have\
    \ employed feature-selection strategies to select the most-relevant\nfeatures\
    \ for predictive tasks. Chen et al. [30] proposed an early predictive model that\n\
    utilized clickstream events to capture student interactions with course content\
    \ and other\nstudents in social learning networks, and classify students into\
    \ pass or fail. Feature selection\nbased on correlation analysis were employed\
    \ for each course, and different classiﬁers were\nexamined in this task. In addition,\
    \ Mourdi et al. [20] developed a multiclass classiﬁer\nthat utilized demographic,\
    \ interactive, and assessment features to determine whether\na student will pass,\
    \ fail, or dropout. The dimensionality was reduced using principal\ncomponent\
    \ analysis (PCA). In comparison to other machine-learning models, the deep\nneural\
    \ network had the greatest accuracy. Chiu et al. [46] proposed a model to predict\n\
    student performance based on behavioral data. Logistic regression and linear regression\n\
    models were employed to determine whether a student would pass or fail and estimate\n\
    their ﬁnal grade, respectively.\nRecent studies have examined temporal features\
    \ and deep-learning models to predict\nlearners’ grades. K˝orösi and Farkas [57]\
    \ proposed a recurrent neural network (RNN)\nto predict students’ ﬁnal grade scores\
    \ and category from raw log data. For both tasks,\nthe RNN model showed better\
    \ accuracy than baseline models. In addition, Qu et al. [52]\nemployed an LSTM\
    \ model trained on temporal assignment-related attributes, such as\nsubmission\
    \ order and completion time, to describe students’ learning behavior and used\
    \ a\ndiscriminative sequential pattern (DSP) adapter to predict students’ achievement.\
    \ Similarly,\nSong et al. [16] developed a CNN-LSTM model based on demographic\
    \ and engagement\npatterns to predict learners’ academic performance.\nFine-grained\
    \ behavioral features were also examined to predict learners’ grades.\nLee et\
    \ al. [65] presented a deep neural network model for student-performance prediction\n\
    based on video-watching and exercise-answering behaviors. Similarly, Lemay and\
    \ Doleck [60]\nSustainability 2022, 14, 6199\n14 of 23\npresented a grade-prediction\
    \ model for weekly assignments based on statistical video-\nviewing behavior (e.g.,\
    \ the number of fast-forwards, pauses, and playback). Different clas-\nsiﬁers\
    \ were employed, and JRip achieved the best performance. Similarly, Yang et al.\
    \ [44]\nproposed an RNN grade-prediction model utilizing video-watching data and\
    \ assessment\ngrades, such as the number of pauses, rewinds, and average quiz\
    \ grades. Fuzzy-based\napproaches have also been investigated for this problem.\
    \ Hussain et al. [49] employed\nFURIA [67], among other models, to predict learners\
    \ who will perform poorly on tests\nbased on learning activities and grades.\n\
    Several studies have considered grade prediction as a regression problem. Chunzi\
    \ et al. [56]\nexplored factors that affect college students’ online foreign-language\
    \ learning performance.\nThe author focused on learning-ability attributes reﬂected\
    \ by students’ grades of text\nrecitation and translation before and after the\
    \ course, duration of learning, login times,\nand the scores obtained in online\
    \ quizzes. Students’ online answers followed by login\ntimes were found to be\
    \ the most-signiﬁcant variables for estimating students’ grades.\nXiao et al.\
    \ [59] developed a matrix-completion-based model to predict online and ofﬂine\n\
    unit tests and ﬁnal grades. They organized the MOOC data into a matrix with missing\n\
    values and then applied the singular value thresholding (SVT) algorithm to obtain\
    \ the\nvalues of missing grades. Kostopoulos et al. [53] developed a model to\
    \ predict students’\ngrade values using semi-supervised learning (SSL) regression.\n\
    9.1.3. Students-at-Risk Prediction\nIdentifying students who are at risk of failing\
    \ a course is a prevalent objective in\nthe literature. Kondo et al. [40] used\
    \ learning interaction and the attendance attributes of\nSPOCs dataset to identify\
    \ off-task and at risk students. Cano and Leonard [47] presented\nan early-warning\
    \ system based on genetic programming (GP) rules to address students\nwith socioeconomic\
    \ disadvantages. The system automatically extracts classiﬁcation rules\nbased\
    \ on student demographic, learning interaction, academic background, registration,\n\
    and family and socioeconomic data. Similarly, Al-Shabandar et al. [51] presented\
    \ an\napproach that identiﬁes failure and withdrawal among MOOC online learners\
    \ based on\ndemographic and learning-behavior data. The synthetic minority oversampling\
    \ approach\n(SMOSE) was applied to address the class imbalance issue, and several\
    \ machine-learning\nmodels were examined, including gradient boosting machine\
    \ (GBM), which provided\nthe highest accuracy. Wan et al. [54] proposed a transfer-based\
    \ model to predict at-risk\nstudents on a weekly in-class project test. Statistical\
    \ behavioral features, such as the total\ntime spent on video resources and percentage\
    \ of total correct submissions, were extracted\nfrom the data and combined with\
    \ previously learned weights from former courses to\npredict students at risk\
    \ of failing in the current course. El Aouiﬁ et al. [62] proposed a\nmodel to\
    \ predict ﬁnal grade based on students’ interactions with pedagogical sequences\
    \ of\neducational video behaviors such as play, pause, jump forward, jump backward,\
    \ and end.\nThe author extracted sequences as features according to the pedagogical\
    \ sequences in\nwhich the student performed an action. K-nn and MLP were used,\
    \ and the best results\nwere achieved using K-nn.\n9.2. Student Dropout and Retention\
    \ Prediction\nDropping out and failing to complete a course are serious challenges\
    \ for educational\ninstitutions, which is why several studies have focused on\
    \ these topics. Various methods for\nfeature extraction have been proposed. Most\
    \ studies examined different machine-learning\nmethods trained on manually selected\
    \ statistical features to predict dropout in MOOCs [68]\nor used temporal features\
    \ obtained from raw or statistical data. Table 8 summarizes the\ndropout prediction\
    \ approaches.\n9.2.1. Statistical Features\nMost of the proposed approaches utilize\
    \ learning-behavior data with demographic\nor academic background to predict dropouts.\
    \ MonllaÃ¸s OlivÃl’ et al. [69] presented a\nSustainability 2022, 14, 6199\n15\
    \ of 23\nframework that explored multiple prediction analyzers (students, users,\
    \ or courses), classi-\nﬁcation targets (at-risk students, course effectiveness),\
    \ indicator features (e.g., the number\nof clicks made by the student), and machine-learning\
    \ models (ANN or LR). To predict\nstudent dropout, learning behavior indicators\
    \ were extracted and used to train the ANN\nmodel, which yielded a good performance.\
    \ Similarly, Zhou and Xu [24] developed a\nmulti-model stacking ensemble learning\
    \ (MMSE) approach to predict student dropout in\nMOOCs. The classiﬁcation was\
    \ performed in two stages; ﬁrst, ﬁve base classiﬁers were\nemployed and produced\
    \ an output that an XGBoost model examined to produce the ﬁnal\nprediction. Assessment\
    \ data has also been examined as indicators of learner dropout rates.\nBurgos\
    \ et al. [70] developed a course-speciﬁc logistic regression model to predict\
    \ student\ndropouts. Similarly, Jha. et al. [15] implemented several machine-learning\
    \ algorithms\nwith a different set of features to predict student dropout and\
    \ performance. The authors\nexamined demographics, assessment, and learning-interaction\
    \ data separately and learner\ninteraction provided the highest accuracy.\nSeveral\
    \ studies have incorporated course data to predict dropout in MOOCs. Cobos\nand\
    \ Olmos [45] utilized course and log data to predict dropout and certiﬁcate acquisitions.\n\
    Bayesian generalized linear (BGL) and stochastic gradient boosting (SGB) achieved\
    \ the\nbest accuracy among other classiﬁers for certiﬁcate and dropout prediction,\
    \ respectively.\nLaveti et al. [71] proposed a stacked ensemble model with features\
    \ extracted from course,\nlog, and enrollment data for dropout prediction. Similarly,\
    \ Coussement et al. [72] developed\na logit leaf model (LLM) trained on data obtained\
    \ from student demographics, cognitive,\nengagement, and academic attributes in\
    \ addition to classroom characteristics for dropout\nprediction. Hong et al. [73]\
    \ proposed a two-layer cascading classiﬁer that combines three\nclassiﬁers for\
    \ student dropout prediction in MOOCs.\nEarly predictions of learner dropout have\
    \ also been studied in the literature. Pana-\ngiotakopoulos et al. [74] presented\
    \ an early-dropout-prediction model based on students’\ndemographics, academic\
    \ background, and ﬁrst week interaction collected from a sin-\ngle MOOC course.\
    \ Several machine-learning models were examined, and LightGBM\noutperformed the\
    \ other models. In addition, Alamri et al. [75] proposed a next-week\ndropout-prediction\
    \ model that predicts students who do not access 80% of the course\ncontent in\
    \ the following week based on their learning behavior and expressed opinions in\n\
    the discussion forum. Multiple classiﬁcation models were tested and AdaBoost provided\n\
    the highest accuracy. A study by Xing and Du [76] used the weekly accumulated\
    \ statistical\nlog data with a deep neural network model to predict student dropouts\
    \ on a weekly basis.\nLikewise, Liu et al. [77] proposed a temporal-behavior features\
    \ weighting approach\nthat can be incorporated with other machine-learning models\
    \ to predict dropout at any\npoint during the course. The proposed approach was\
    \ compared with other feature genera-\ntion methodologies, such as non-stacked\
    \ [78,79], stacked [78,80,81], aggregated [79,82] and\nrecursive-aggregated [54]\
    \ approaches. The proposed method achieved a higher F-score and\nlower false-positive\
    \ rates in early-weeks prediction. Studies have also examined feature-\nselection\
    \ strategies in dropout prediction. Qiu et al. [83] developed a dropout-prediction\n\
    model that integrates ensemble feature ranking based on MI, RF, and RFE. The model\n\
    also incorporates a forward search method that gradually examines features that\
    \ improve\nprediction. The selected features were tested with a LR model. Ardchir\
    \ et al. [84] used\ndifferent machine-learning models with feature selection based\
    \ on information gain RF\nto weight features based on their importance. Neighborhood\
    \ component analysis feature\nselection was used with several machine-learning\
    \ models to predict student dropout [85].\nOptimization techniques have also been\
    \ used for dropout prediction. Jin [86] proposed\nan approach that initiates the\
    \ weight of predictive models according to the maximum neigh-\nborhood deﬁnition\
    \ of the training sample using an improved quantum particle swarm\noptimization\
    \ (IQPSO) algorithm. To overcome the class imbalance issue, Mulyani et al. [87]\n\
    proposed the SMOTE-ensemble learning (SEL) model for dropout prediction using\
    \ the syn-\nthetic minority over-sampling technique (SMOTE) for the non-dropout\
    \ class in addition to\nthe ensemble learning technique with three machine learning\
    \ algorithms, LR, KNN, and RF.\nSustainability 2022, 14, 6199\n16 of 23\nGregori\
    \ et al. [34] also used the synthetic minority oversampling (SMOTE) algorithm\
    \ with\na semi-supervised extreme learning machine (SSELM) model.\n9.2.2. Temporal\
    \ Features\nRecent studies on dropout prediction have used temporal features extracted\
    \ from raw\nor statistical data and employed deep-learning techniques to estimate\
    \ learning behavior\ndynamics over time. Lai et al. [23] developed a random vector\
    \ functional link neural\nnetwork (RVFLNN) to predict student dropout in MOOCs.\
    \ MOOCs generate periodical\ndata that varies in dimensionality; therefore, RVFLNN\
    \ were used to increase input and\nupdate the weight dynamically. Likewise, Xiong\
    \ et al. [88] proposed an RNN-LSTM model\nto predict student dropouts in MOOCs\
    \ based on statistical weekly features aggregated\nfrom students’ log data. Sun\
    \ et al. [55] developed a GRU-RNN model to predict the\npercentage of course content\
    \ completed in an entire course syllabus. Several dropout-\nprediction models\
    \ use CNNs to extract features from raw data. Wu et al. [89] proposed a\nmodel\
    \ that combined CNN, LSTM, and SVM to predict student dropout on a daily basis.\n\
    The model transforms raw data into a one-hot vector and represents each enrollment\
    \ in a\nnumber of matrices that each describe the learner’s behavior in a single\
    \ day. The proposed\nmodel outperformed other standard machine-learning methods,\
    \ such as SVM, RF, and LR,\nand temporal deep-learning techniques, such as CNN-RNN.\n\
    Several studies have developed CNN model to extract features from statistical\
    \ tem-\nporal features and predict dropout. Studies in [90,91] developed a CNN\
    \ model to analyze\nthe students’ learning behavior represented by a matrix that\
    \ encodes the daily statistical\nfeatures over a period of time. Likewise, Zheng\
    \ et al. [92] presented a model that extracts\nstatistical learning-behavior data,\
    \ performs feature weighting and selection based on a\ndecision tree, and then\
    \ uses CNN to predict dropout from students’ time-series learning\nbehavior frequencies.\
    \ Recently, several studies have proposed connectionist approaches to\nestimate\
    \ learning outcomes by combining CNN and sequence models (LSTM). Mubarak\net al.\
    \ [19] proposed a hypermodel of CNN and LSTM to extract features from the raw\
    \ data\nof MOOCs and predict student dropout. Wang et al. [93] proposed a dropout-prediction\n\
    model with automatic feature extraction from the raw MOOC data. The raw data of\
    \ each\nevent were ﬁrst converted into a one-hot vector and concatenated to produce\
    \ matrices\nrepresenting a student’s learning behavior over a number of days.\
    \ Then, a CNN and RNN\nmodel was used to extract features and utilize time-series\
    \ relationships. Similarly, Fu et al.\n[94] developed a novel approach that employs\
    \ a CNN to extract local features from raw\nclickstream data and LSTM to obtain\
    \ a time-series incorporating vector representation.\nYin et al. [95] designed\
    \ a novel neural network structure that combined the convolutional\nattention\
    \ mechanism and conditional random ﬁeld (CRF) for MOOC dropout prediction.\nThe\
    \ proposed approach was compared with several baseline classiﬁers, including CNN-\n\
    LSTM, and signiﬁcantly outperformed the baseline methods.\nSustainability 2022,\
    \ 14, 6199\n17 of 23\nTable 8. Summary of the dropout prediction studies that\
    \ include type of online education (MOOC\nor SPOC), platform (the dataset or the\
    \ platform, blank if not mentioned), sample (the sample of\ninstances in the dataset,\
    \ blank if not mentioned), courses (number of courses in the dataset, blank if\n\
    not stated), features (B: learning behavior or log data, D: demographic information,\
    \ V: video click\nstream, A: assessment data (grade, answer data), C: course data,\
    \ O: other, such as a forum, discussion,\nand academic background, attendance\
    \ data, features extraction (FE) method either statistical (S),\ntemporal (T),\
    \ and raw data(R), classes, output, best-performing model/s, and best performance\n\
    accuracy scores. F1-score (F1), AUC, or regression metrics is provided if the\
    \ accuracy score was not\nreported. Scores are rounded up to two decimal places\
    \ and reported with the week if the performance\nwas estimated during different\
    \ time spans.\nRef.\nType\nPlatform\nDataset\nSample\nCourse\nFeatures\nFE\nModel\n\
    Class\nOutput\nAccuracy\nB\nD\nV\nA\nC\nO\nS\nT\nR\n[73]\nMOOC\nKDDcup\n96,529/39\n\
    ✓\n✓\n✓\nc-RF\nBinary\nDropout\n0.93\n[71]\nMOOC\nKDDcup\n200,000/39\n✓\n✓\n✓\n\
    ✓\nStacked\nensemble\nBinary\nDropout\n0.91\nCNN + RNN\nBinary\nDropout\n0.92\n\
    [76]\nMOOC\nCanvas\n3617/1\n✓\n✓\n✓\n✓\n✓\nDL\nBinary\nDropout\nw7:0.97\n[83]\n\
    MOOC\nKDDcup\n120,542/39\n✓\n✓\n✓\nRFE + LR\nBinary\nDropout\n0.87\n[88]\nMOOC\n\
    LMS\n-/6\n✓\n✓\n✓\nRNN-LSTM\nBinary\nDropout\n0.90\n[89]\nMOOC\nKDDcup\n79,186/39\n\
    ✓\n✓\n✓\nCNN + LSTM + SVM Binary\nDropout\nF1 = 0.95\n[84]\nMOOC\nKDDcup\n112,448/39\n\
    ✓\n✓\n✓\n✓\n✓\nGB\nBinary\nDropout\nAUC = 0.89\n[15]\nMOOC\nOULAD\n32,594/22\n\
    ✓\n✓\n✓\n✓\nGBM\nBinary\nDropout\nAUC = 0.91\n[26]\nMOOC\nHMedx\n641,138/-\n✓\n\
    ✓\nDNN\nBinary\nDropout\n0.99\n[85]\nMOOC\nOULAD\n32,593/22\n✓\n✓\n✓\n✓\nANN\n\
    Binary\nDropout\nAUC = 0.93\n[72]\nMOOC\n-\n10,554/-\n✓\n✓\n✓\n✓\n✓\nLLM\nBinary\n\
    Dropout\nF1 = 0.84\n[92]\nMOOC\nKDDcup\n79,186/39\n✓\n✓\n✓\nFWTS-CNN\nBinary\n\
    Dropout\n0.87\n[77]\nMOOC\nKDDcup\n112,448/39\n✓\n✓\nGaussian NB\nBinary\nDropout\n\
    F1 = 0.85\n[96]\nMOOC\nKDDcup\n120542/39\n✓\n✓\n✓\n✓\nCNN + SE + GRU\nBinary\n\
    Dropout\n0.95\n[69]\nMOOC\nMoodle\n46,895/8\n✓\n✓\nANN\nBinary\nDropout\n0.89\n\
    [23]\nMOOC\nKDDcup\n120,542/39\n✓\n✓\n✓\nRVFLNN\nBinary\nDropout\n0.93\n[86]\n\
    MOOC\nKDDcup\n53,596/6\n✓\n✓\nSVM\nBinary\nDropout\nF1 = 0.90\n[24]\nMOOC\nKDDcup\n\
    120,542/39\n✓\n✓\n✓\nMMSE\nBinary\nDropout\n0.88\n[90]\nMOOC\nKDDcup\n120,542/39\n\
    ✓\n✓\n✓\nCNN\nBinary\nDropout\n0.88\n[95]\nMOOC\nKDDcup\n12,004/1\n✓\n✓\n✓\nAttention\
    \ + CRF\nBinary\nDropout\n0.84\n[74]\nMOOC\nMoodle\n700/1\n✓\n✓\n✓\n✓\nLightGBM\n\
    Binary\nDropout\n0.96\n[75]\nMOOC\nFuture\nLearn\n251,662/7\n✓\n✓\n✓\nRF, Adaboost\n\
    Binary\nDropout\n0.95\n10. Summary, Challenges and Limitations\nAs demonstrated\
    \ in Tables 7 and 8, most of the studies were conducted on a dataset\nobtained\
    \ from a MOOC. Only a limited number of studies considered predicting student\n\
    outcomes in SPOCs. Researchers have utilized data collected for different durations,\
    \ for ex-\nample, a month, two months, a semester, or several years. In addition,\
    \ researchers have\ndeveloped predictive models using the learner-interaction\
    \ data from a whole course or\nacross multiple course periods (e.g., three weeks,\
    \ ﬁve weeks, seven weeks). Assessing\npredictive models on different course periods\
    \ shows that models give better predictive\nresults of student outcomes when the\
    \ evaluation duration increases. In addition, the sam-\nples and number of courses\
    \ vary among the studies. Some studies worked on a subset of a\npublicly available\
    \ dataset, whereas others worked on the entire dataset. The SPOC sample\nsize\
    \ was much less than the MOOC sample size, which impacted the performance of the\n\
    Sustainability 2022, 14, 6199\n18 of 23\nSPOC-based models. Similar to any machine-learning\
    \ problem, the size of the dataset (i.e.,\nnumber of samples) can impact the performance\
    \ of the model.\nVarious sets of indicators have been used to predict student\
    \ outcomes in the literature.\nThe most widely employed group of information in\
    \ predicting student performance was\nlearning behavior (log data), followed by\
    \ assessment and demographic features. Learning\nbehavior was also the most widely\
    \ utilized set of features in predicting learner dropout;\ndemographic and assessment\
    \ features were less-frequently used. Statistically extracted\ncharacteristics\
    \ were studied more than raw features by the researchers. The number of\nstatistically\
    \ retrieved features differs among research as well. Some studies focused on\n\
    a small number of features (e.g., ﬁve features [42]), whereas others examined\
    \ a larger\nnumber of features (e.g., 79 features [83]). Those features were often\
    \ selected manually,\nand automatic feature selection was less-widely utilized.\
    \ Several machine-learning models\nwere evaluated in this task. RF and ANN were\
    \ among the best-performing machine-\nlearning models.\nSeveral limitations were\
    \ observed in the literature, which can be summarized as follows:\n–\nThere is\
    \ no consensus among researchers on the deﬁnition of dropout, success,\nand other\
    \ related terminologies. For example, some researchers considered a stu-\ndent\
    \ to be a dropout if they fail to complete a speciﬁc percentage of the assessments\n\
    (e.g., 50%) [70], or if they were not active for several consecutive days [73].\
    \ Others\nconsidered dropout to be the inability to pass a course, and some did\
    \ not provide any\nprecise deﬁnition. The inconsistency in deﬁning dropout and\
    \ other related terminolo-\ngies could be a concern to researchers, since it inﬂuences\
    \ how dropout is assessed,\naddressed, and investigated [97].\n–\nMost proposed\
    \ approaches use MOOC datasets in which students are self-motivated\nand are not\
    \ required or obligated to participate in the courses. Thus, there is an enor-\n\
    mous disparity between learners who are registered for curiosity, who, for example,\n\
    view some videos and do not complete the required assignments, and those who\n\
    are registered to ﬁnish the course, which may make identifying dropouts or failure\n\
    relatively easy tasks. Exploring other types of online environments, such as SPOCs,\n\
    might introduce more challenges, as learners may attempt to ﬁnish the course but\
    \ fail\nor withdraw due to their lack of knowledge or other reasons.\n–\nMost\
    \ studies also proposed systems that predict student outcomes when students\n\
    approach the end of the course. Only a small number of studies considered early\n\
    predictions of user outcomes, while many studies did not report the duration of\
    \ data\ncollection. This made it difﬁcult to compare the different proposed methods,\
    \ as the\nduration of the extracted features varied signiﬁcantly. In addition,\
    \ some studies used a\nsubset of publicly available datasets, making it difﬁcult\
    \ to compare different methods\non benchmark datasets.\n–\nMost studies employed\
    \ feature-engineering techniques to calculate students’ statistical\nfeatures\
    \ as a whole. These features were often chosen arbitrarily or using statistical\n\
    techniques such as correlation analysis. Some recent studies looked into methods\
    \ that\nautomatically extract temporal features from raw data by mapping raw features\
    \ into\nnumerical representations, such as one-hot encoding, and then using deep-learning\n\
    methods, in particular, convolution functions, to extract features. Despite the\
    \ useful-\nness of these techniques, the generated representations are often very\
    \ sparse and less\nuseful for comparing user behavior.\n–\nSeveral limitations\
    \ have been addressed by researchers in this ﬁeld. One of these limi-\ntations\
    \ is the problem of multi-valued instances, in which some instances containing\n\
    the same patterns have different outcomes [98]. In addition, the dropout-prediction\n\
    task is well known to be imbalanced because the proportion of the positive class\
    \ is\nmuch larger than that of the negative class. Several studies also use methods\
    \ to handle\nthe problem of class imbalance by either oversampling of the minority\
    \ class [34,87] or\nunder-sampling of the majority class [66].\nSustainability\
    \ 2022, 14, 6199\n19 of 23\n–\nOne of the observed challenges is the quality of\
    \ the training samples in which a large\nnumber of attributes are clickstream\
    \ data which might be less representative when\nlearners do not interact or engage\
    \ in learning activities. This practice is common\namong students who register\
    \ in MOOCs for curiosity [86].\n–\nLearning analytic systems are based on large\
    \ volumes of real-time data collected over a\nprotracted period. Analyzing, combining,\
    \ and linking multiple forms of learners’ data\nto forecast their outcomes or\
    \ any part of the learning process has raised many ethical\nchallenges that cannot\
    \ be properly measured using traditional ethical procedures [99].\nThe risks of\
    \ de-anonymizing learners’ identities [100] and decontextualizing data [101]\n\
    are some of the potential risks of this practice.\n11. Conclusions and Future\
    \ Directions\nWith the emergence of MOOCs and the expansion of online education\
    \ in recent years,\nthe prediction of learners’ outcomes in online environments\
    \ has attracted considerable\nattention. The spread of COVID-19 has also aided\
    \ the growth of SPOCs, blended edu-\ncation, and an interest in monitoring student\
    \ engagement and performance. Therefore,\nthis study reviewed current strategies\
    \ for predicting online-student outcomes in MOOCs\nand SPOCs. It summarized the\
    \ predictive variables, online learning platforms, feature\nextraction, selection\
    \ techniques, evaluation metrics, and the predictive models employed in\nthis\
    \ area. It also provided a thorough analysis and taxonomy for related research.\
    \ Through-\nout our analysis, we found that most studies in the ﬁeld utilized\
    \ statistical features such\nas the number of downloaded materials and duration\
    \ of video watching in a given time\nperiod. A small number of studies examined\
    \ statistical temporal and raw temporal fea-\ntures in predicting learner outcomes.\
    \ Studies conducted on benchmark datasets showed\nthat statistical temporal features\
    \ provide better results than raw features. Thus, further\ninvestigation of temporal\
    \ features will provide a valuable understanding of users’ learning\nprogress\
    \ and, eventually, their learner outcomes. Most temporally based LSTM or GRU\n\
    models learners’ time-series features. Further investigation of other recent sequence-based\n\
    models, such as the attention-based model, is required. Studies using one-hot\
    \ encoding to\nrepresent raw features and different representation techniques\
    \ for raw features are worth\ninvestigating. Different machine-learning and deep-learning\
    \ models have been used to\npredict learners’ outcomes. RF and ANN are among the\
    \ most effective machine learning\nmodels’ performance and dropout prediction,\
    \ whereas the sequence-based model provides\nthe best performance on the publicly\
    \ available dropout dataset. Further investigation of\ndeep-learning models is\
    \ recommended to predict student performance.\nAuthor Contributions: Conceptualization,\
    \ A.A., H.A., T.A., and M.A.; methodology, A.A. and M.A.;\nvalidation, A.A., M.A.,\
    \ and H.A.; formal analysis, M.A.; Data curation, M.A.; writing—original draft\n\
    preparation, M.A. and A.A.; writing—review and editing, A.A., H.A., and T.A.;\
    \ visualization, M.A.\nand A.A.; project administration, H.A. and T.A.; funding\
    \ acquisition, H.A.; All authors have read and\nagreed to the published version\
    \ of the manuscript.\nFunding: The authors extend their appreciation to the Deputyship\
    \ for Research and Innovation,\nMinistry of Education in Saudi Arabia for funding\
    \ this research work through the project number\nIFPRC-039-126-2020 and King Abdulaziz\
    \ University, DSR, Jeddah, Saudi Arabia.\nInstitutional Review Board Statement:\
    \ Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability\
    \ Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nReferences\n1.\nArce, M.E.; Crespo, B.; Míguez-Álvarez, C. Higher\
    \ Education Drop-out in Spain–Particular Case of Universities in Galicia. Int.\n\
    Educ. Stud. 2015, 8, 247–264. [CrossRef]\n2.\nXavier, M.; Meneses, J. Dropout\
    \ in Online Higher Education: A Scoping Review from 2014 to 2018; ELearn Center,\
    \ Universitat Oberta\nde Catalunya: Barcelona, Spain, 2020.\nSustainability 2022,\
    \ 14, 6199\n20 of 23\n3.\nBaker, R.S.; Inventado, P.S. Educational Data Mining\
    \ and Learning Analytics. In Learning Analytics: From Research to Practice;\n\
    Larusson, J.A., White, B., Eds.; Springer: New York, NY, USA, 2014; pp. 61–75.\
    \ [CrossRef]\n4.\nMoreno-Marcos, P.M.; Alario-Hoyos, C.; MuÃ´soz-Merino, P.J.;\
    \ Kloos, C.D. Prediction in MOOCs: A Review and Future Research\nDirections. IEEE\
    \ Trans. Learn. Technol. 2019, 12, 384–401. [CrossRef]\n5.\nRanjeeth, S.; Latchoumi,\
    \ T.; Paul, P.V. A survey on predictive models of learning analytics. Procedia\
    \ Comput. Sci. 2020, 167, 37–46.\n[CrossRef]\n6.\nHamim, T.; Benabbou, F.; Sael,\
    \ N. Survey of Machine Learning Techniques for Student Proﬁle Modelling. Int.\
    \ J. Emerg. Technol.\nLearn. 2021, 16, 136–151. [CrossRef]\n7.\nPrenkaj, B.; Velardi,\
    \ P.; Stilo, G.; Distante, D.; Faralli, S. A survey of machine learning approaches\
    \ for student dropout prediction\nin online courses. ACM Comput. Surv. (CSUR)\
    \ 2020, 53, 1–34. [CrossRef]\n8.\nGardner, J.; Brooks, C. Student success prediction\
    \ in MOOCs. User Model. User-Adapt. Interact. 2018, 28, 127–203. [CrossRef]\n\
    9.\nKatarya, R.; Gaba, J.; Garg, A.; Verma, V. A review on machine learning based\
    \ student’s academic performance prediction\nsystems. In Proceedings of the 2021\
    \ International Conference on Artiﬁcial Intelligence and Smart Systems (ICAIS),\
    \ Coimbatore,\nIndia, 25–27 March 2021; pp. 254–259. [CrossRef]\n10.\nFilius,\
    \ R.M.; Uijl, S.G. Teaching Methodologies for Scalable Online Education. In Handbook\
    \ for Online Learning Contexts: Digital,\nMobile and Open; Springer: Berlin/Heidelberg,\
    \ Germany, 2021; pp. 55–65.\n11.\nAmrieh, E.A.; Hamtini, T.M.; Aljarah, I. Mining\
    \ Educational Data to Predict Student’s academic Performance using Ensemble\n\
    Methods. Int. J. Database Theory Appl. 2016, 9, 119–136. [CrossRef]\n12.\nRahman,\
    \ M.H.; Islam, M.R. Predict Student’s Academic Performance and Evaluate the Impact\
    \ of Different Attributes on the\nPerformance Using Data Mining Techniques. In\
    \ Proceedings of the 2017 2nd International Conference on Electrical Electronic\n\
    Engineering (ICEEE), Rajshahi, Bangladesh, 27–29 December 2017; pp. 1–4. [CrossRef]\n\
    13.\nKuzilek, J.; Hlosta, M.; Zdrahal, Z. Open university learning analytics dataset.\
    \ Sci. Data 2017, 4, 170171. [CrossRef]\n14.\nAdnan, M.; Habib, A.; Ashraf, J.;\
    \ Mussadiq, S.; Raza, A.A.; Abid, M.; Bashir, M.; Khan, S.U. Predicting at-Risk\
    \ Students at\nDifferent Percentages of Course Length for Early Intervention Using\
    \ Machine Learning Models. IEEE Access 2021, 9, 7519–7539.\n[CrossRef]\n15.\n\
    Jha, N.; Ghergulescu, I.; Moldovan, A. OULAD MOOC Dropout and Result Prediction\
    \ using Ensemble, Deep Learning and\nRegression Techniques. In Proceedings of\
    \ the 11th International Conference on Computer Supported Education, Heraklion,\n\
    Greece, 2–4 May 2019; SciTePress: SetÃžbal, Portugal, 2019; Volume 2, pp. 154–164.\
    \ [CrossRef]\n16.\nSong, X.; Li, J.; Sun, S.; Yin, H.; Dawson, P.; Doss, R.R.M.\
    \ SEPN: A Sequential Engagement Based Academic Performance\nPrediction Model.\
    \ IEEE Intell. Syst. 2021, 36, 46–53. [CrossRef]\n17.\nHussain, M.; Zhu, W.; Zhang,\
    \ W.; Abidi, R. Student Engagement Predictions in an e-Learning System and Their\
    \ Impact on\nStudent Course Assessment Scores. Comput. Intell. Neurosci. 2018,\
    \ 2018, 6347186. [CrossRef] [PubMed]\n18.\nStanford, U. Center for Advanced Research\
    \ through Online Learning (CAROL). Available online: https://carol.stanford.edu\n\
    (accessed on 24 August 2021).\n19.\nMubarak, A.A.; Cao, H.; Hezam, I.M. Deep analytic\
    \ model for student dropout prediction in massive open online courses.\nComput.\
    \ Electr. Eng. 2021, 93, 107271. [CrossRef]\n20.\nMourdi, Y.; Sadgal, M.; El Kabtane,\
    \ H.; Berrada Fathi, W. A machine learning-based methodology to predict learners’\
    \ dropout,\nsuccess or failure in MOOCs. Int. J. Web Inf. Syst. 2019, 15, 489–509.\
    \ [CrossRef]\n21.\nMourdi, Y.; Sadgal, M.; Berrada Fathi, W.; El Kabtane, H. A\
    \ Machine Learning Based Approach to Enhance Mooc Users’\nClassiﬁcation. Turk.\
    \ Online J. Distance Educ. 2020, 21, 47–68. [CrossRef]\n22.\nKDD. KDD Cup 2015.\
    \ Available online: https://kdd.org/kdd-cup (accessed on 24 August 2021).\n23.\n\
    Lai, S.; Zhao, Y.; Yang, Y. Broad Learning System for Predicting Student Dropout\
    \ in Massive Open Online Courses.\nIn\nProceedings of the 2020 8th International\
    \ Conference on Information and Education Technology, Okayama, Japan, 28–30 March\n\
    2020; Association for Computing Machinery: New York, NY, USA, 2020; pp. 12–17.\
    \ [CrossRef]\n24.\nZhou, Y.; Xu, Z. Multi-Model Stacking Ensemble Learning for\
    \ Dropout Prediction in MOOCs. J. Phys. Conf. Ser. 2020, 1607, 012004.\n[CrossRef]\n\
    25.\nHo, A.D.; Reich, J.; Nesterko, S.; Seaton, D.T.; Mullaney, T.; Waldo, J.;\
    \ Chuang, I. HarvardX and MITx: The ﬁrst year of Open Online\nCourses; HarvardX\
    \ and MITx Working Paper No. 1; Harvard University: Cambridge, MA, USA, 2014.\n\
    26.\nImran, A.; Dalipi, F.; Kastrati, Z. Predicting Student Dropout in a MOOC:\
    \ An Evaluation of a Deep Neural Network Model. In\nProceedings of the 2019 5th\
    \ International Conference on Computing and Artiﬁcial Intelligence, Bali, Indonesia,\
    \ 19–22 April 2019;\npp. 190–195. [CrossRef]\n27.\nAl-Shabandar, R.; Hussain,\
    \ A.; Laws, A.; Keight, R.; Lunn, J.; Radi, N. Machine learning approaches to\
    \ predict learning outcomes\nin Massive open online courses.\nIn Proceedings of\
    \ the 2017 International Joint Conference on Neural Networks (IJCNN),\nAnchorage,\
    \ AK, USA, 14–19 May 2017; pp. 713–720. [CrossRef]\n28.\nLiu, K.F.-R.; Chen, J.-S.\
    \ Prediction and assessment of student learning outcomes in calculus a decision\
    \ support of integrating\ndata mining and Bayesian belief networks. In Proceedings\
    \ of the 2011 3rd International Conference on Computer Research and\nDevelopment,\
    \ Shanghai, China, 11–13 March 2011; Volume 1, pp. 299–303. [CrossRef]\n29.\n\
    Wu, W.H.; Jim Wu, Y.C.; Chen, C.Y.; Kao, H.Y.; Lin, C.H.; Huang, S.H. Review of\
    \ Trends from Mobile Learning Studies:\nA Meta-Analysis. Comput. Educ. 2012, 59,\
    \ 817–827. [CrossRef]\nSustainability 2022, 14, 6199\n21 of 23\n30.\nChen, W.;\
    \ Brinton, C.G.; Cao, D.; Mason-Singh, A.; Lu, C.; Chiang, M. Early Detection\
    \ Prediction of Learning Outcomes in Online\nShort-Courses via Learning Behaviors.\
    \ IEEE Trans. Learn. Technol. 2019, 12, 44–58. [CrossRef]\n31.\nKorosi, G.; Esztelecki,\
    \ P.; Farkas, R.; TÃ¸sth, K. Clickstream-Based outcome prediction in short video\
    \ MOOCs. In Proceedings of\nthe 2018 International Conference on Computer, Information\
    \ and Telecommunication Systems (CITS), Colmar, France, 11–13 July\n2018; pp.\
    \ 1–5. [CrossRef]\n32.\nPereira, F.D.; Oliveira, E.; Cristea, A.; Fernandes, D.;\
    \ Silva, L.; Aguiar, G.; Alamri, A.; Alshehri, M. Early Dropout Prediction for\n\
    Programming Courses Supported by Online Judges. In Artiﬁcial Intelligence in Education;\
    \ Isotani, S., Millán, E., Ogan, A., Hastings,\nP., McLaren, B., Luckin, R., Eds.;\
    \ Springer International Publishing: Cham, Switzerland, 2019; pp. 67–72.\n33.\n\
    Huang, A.Y.; Lu, O.H.; Huang, J.C.; Yin, C.J.; Yang, S.J. Predicting students’\
    \ academic performance by using educational big\ndata and learning analytics:\
    \ Evaluation of classiﬁcation methods and learning logs. Interact. Learn. Environ.\
    \ 2020, 28, 206–230.\n[CrossRef]\n34.\nGregori, E.B.; Zhang, J.; Galván-Fernández,\
    \ C.; de Asís Fernández-Navarro, F. Learner support in MOOCs: Identifying variables\n\
    linked to completion. Comput. Educ. 2018, 122, 153–168. [CrossRef]\n35.\nLiu,\
    \ D.; Zhang, Y.; Zhang, J.; Li, Q.; Zhang, C.; Yin, Y. Multiple Features Fusion\
    \ Attention Mechanism Enhanced Deep Knowledge\nTracing for Student Performance\
    \ Prediction. IEEE Access 2020, 8, 194894–194903. [CrossRef]\n36.\nFaraggi, D.;\
    \ Reiser, B. Estimation of the area under the ROC curve. Stat. Med. 2002, 21,\
    \ 3093–3106. [CrossRef] [PubMed]\n37.\nHutter, F.; Kotthoff, L.; Vanschoren, J.\n\
    Automated Machine Learning:\nMethods, Systems, Challenges; Springer Nature:\n\
    Berlin/Heidelberg, Germany, 2019.\n38.\nBravo-Agapito, J.; Romero, S.J.; Pamplona,\
    \ S. Early prediction of undergraduate Student’s academic performance in completely\n\
    online learning: A ﬁve-year study. Comput. Hum. Behav. 2021, 115, 106595. [CrossRef]\n\
    39.\nVillagra-Arnedo, C.J.; Gallego-Duran, F.J.; Compan, P.; Llorens Largo, F.;\
    \ Molina-Carmona, R. Predicting academic performance\nfrom Behavioural and learning\
    \ data. Int. J. Des. Nat. Ecodynamics 2016, 11, 239–249. [CrossRef]\n40.\nKondo,\
    \ N.; Okubo, M.; Hatanaka, T. Early Detection of At-Risk Students Using Machine\
    \ Learning Based on LMS Log Data. In\nProceedings of the 2017 6th IIAI International\
    \ Congress on Advanced Applied Informatics (IIAI-AAI), Shizuoka, Japan, 9–13 July\n\
    2017; pp. 198–201. [CrossRef]\n41.\nRuipérez-Valiente, J.A.; Cobos, R.; Muñoz-Merino,\
    \ P.J.; Andujar, Á.; Delgado Kloos, C. Early prediction and variable importance\n\
    of certiﬁcate accomplishment in a MOOC. In European Conference on Massive Open\
    \ Online Courses; Springer: Berlin/Heidelberg,\nGermany, 2017; pp. 263–272.\n\
    42.\nLiang, K.; Zhang, Y.; He, Y.; Zhou, Y.; Tan, W.; Li, X. Online Behavior Analysis-Based\
    \ Student Proﬁle for Intelligent E-Learning. J.\nElectr. Comput. Eng. 2017, 2017,\
    \ 9720396. [CrossRef]\n43.\nLiu, W.; Wu, J.; Gao, X.; Feng, K. An early warning\
    \ model of student achievement based on decision trees algorithm.\nIn\nProceedings\
    \ of the 2017 IEEE 6th International Conference on Teaching, Assessment, and Learning\
    \ for Engineering (TALE), Hong\nKong, China, 21–14 December 2017; pp. 517–222.\
    \ [CrossRef]\n44.\nYang, T.Y.; Brinton, C.G.; Joe-Wong, C.; Chiang, M. Behavior-Based\
    \ Grade Prediction for MOOCs Via Time Series Neural\nNetworks. IEEE J. Sel. Top.\
    \ Signal Process. 2017, 11, 716–728. [CrossRef]\n45.\nCobos, R.; Olmos, L. A Learning\
    \ Analytics Tool for Predictive Modeling of Dropout and Certiﬁcate Acquisition\
    \ on MOOCs for\nProfessional Learning. In Proceedings of the 2018 IEEE International\
    \ Conference on Industrial Engineering and Engineering\nManagement (IEEM), Bangkok,\
    \ Thailand, 16–19 December 2018; pp. 1533–1537. [CrossRef]\n46.\nChiu, Y.C.; Hsu,\
    \ H.J.; Wu, J.; Yang, D.L. Predicting student performance in MOOCs using learning\
    \ activity data. J. Inf. Sci. Eng.\n2018, 34, 1223–1235. [CrossRef]\n47.\nCano,\
    \ A.; Leonard, J.D. Interpretable Multiview Early Warning System Adapted to Underrepresented\
    \ Student Populations. IEEE\nTrans. Learn. Technol. 2019, 12, 198–211. [CrossRef]\n\
    48.\nXiao, B.; Liang, M.; Ma, J. The Application of CART Algorithm in Analyzing\
    \ Relationship of MOOC Learning Behavior and\nGrades. In Proceedings of the 2018\
    \ International Conference on Sensor Networks and Signal Processing (SNSP), Xi’an,\
    \ China,\n28–31 October 2018; pp. 250–254. [CrossRef]\n49.\nHussain, M.; Hussain,\
    \ S.; Zhang, W.; Zhu, W.; Theodorou, P.; Abidi, S.M.R. Mining Moodle Data to Detect\
    \ the Inactive and\nLow-Performance Students during the Moodle Course. In Proceedings\
    \ of the 2nd International Conference on Big Data Research,\nHangzhou, China,\
    \ 18–20 May 2018; Association for Computing Machinery: New York, NY, USA, 2018;\
    \ pp. 133–140. [CrossRef]\n50.\nYu, C. SPOC-MFLP: A multi-feature learning prediction\
    \ model for SPOC students using machine learning. J. Appl. Sci. Eng. 2018,\n21,\
    \ 279–290. [CrossRef]\n51.\nAl-Shabandar, R.; Hussain, A.J.; Liatsis, P.; Keight,\
    \ R. Detecting At-Risk Students With Early Interventions Using Machine\nLearning\
    \ Techniques. IEEE Access 2019, 7, 149464–149478. [CrossRef]\n52.\nQu, S.; Li,\
    \ K.; Wu, B.; Zhang, S.; Wang, Y. Predicting Student Achievement Based on Temporal\
    \ Learning Behavior in MOOCs.\nAppl. Sci. 2019, 9, 5539. [CrossRef]\n53.\nKostopoulos,\
    \ G.; Kotsiantis, S.; Fazakis, N.; Koutsonikos, G.; Pierrakeas, C. A semi-supervised\
    \ regression algorithm for grade\nprediction of students in distance learning\
    \ courses. Int. J. Artif. Intell. Tools 2019, 28, 1940001. [CrossRef]\n54.\nWan,\
    \ H.; Liu, K.; Yu, Q.; Gao, X. Pedagogical Intervention Practices: Improving Learning\
    \ Engagement Based on Early Prediction.\nIEEE Trans. Learn. Technol. 2019, 12,\
    \ 278–289. [CrossRef]\nSustainability 2022, 14, 6199\n22 of 23\n55.\nSun, D.;\
    \ Mao, Y.; Du, J.; Xu, P.; Zheng, Q.; Sun, H. Deep Learning for Dropout Prediction\
    \ in MOOCs. In Proceedings of the 2019\nEighth International Conference on Educational\
    \ Innovation through Technology (EITT), Biloxi, MS, USA, 27–31 October 2019;\n\
    pp. 87–90. [CrossRef]\n56.\nChunzi, S.; Xuanren, W.; Ling, L. The Application\
    \ of Big Data Analytics in Online Foreign Language Learning among College\nStudents:\
    \ Empirical Research on Monitoring the Learning Outcomes and Predicting Final\
    \ Grades. In Proceedings of the 2020 2nd\nInternational Conference on Machine\
    \ Learning, Big Data and Business Intelligence (MLBDBI), Taiyuan, China, 23–25\
    \ October\n2020; pp. 266–269. [CrossRef]\n57.\nK˝orösi, G.; Farkas, R. MOOC Performance\
    \ Prediction by Deep Learning from Raw Clickstream Data. In International Conference\n\
    on Advances in Computing and Data Sciences; Springer: Berlin/Heidelberg, Germany,\
    \ 2020; pp. 474–485. [CrossRef]\n58.\nKarlos, S.; Kostopoulos, G.; Kotsiantis,\
    \ S. Predicting and Interpreting Students’ Grades in Distance Higher Education\
    \ through a\nSemi-Regression Method. Appl. Sci. 2020, 10, 8413. [CrossRef]\n59.\n\
    Xiao, F.; Li, Q.; Huang, H.; Sun, L.; Xu, X. MOLEAS: A Multi-stage Online Learning\
    \ Effectiveness Assessment Scheme in MOOC.\nIn Proceedings of the 2020 IEEE International\
    \ Conference on Teaching, Assessment, and Learning for Engineering (TALE),\nTakamatsu,\
    \ Japan, 8–11 December 2020; pp. 31–38. [CrossRef]\n60.\nLemay, D.; Doleck, T.\
    \ Grade prediction of weekly assignments in MOOCS: Mining video-viewing behavior.\
    \ Educ. Inf. Technol.\n2020, 25, 1333–1342. [CrossRef]\n61.\nKokoÃ˘g, M.; AkÃ˘gapÄ´snar,\
    \ G.; Hasnine, M. Unfolding Students’ Online Assignment Submission Behavioral\
    \ Patterns using\nTemporal Learning Analytics. Educ. Technol. Soc. 2021, 24, 223–235.\n\
    62.\nEl Aouiﬁ, H.; El Hajji, M.; Es-saady, Y.; Hassan, D. Predicting learner’s\
    \ performance through video sequences viewing behavior\nanalysis using educational\
    \ data-mining. Educ. Inf. Technol. 2021, 26, 5799–5814. [CrossRef]\n63.\nChi,\
    \ D.; Huang, Y. Research on Application of Online Teaching Performance Prediction\
    \ Based on Data Mining Algorithm.\nIn Proceedings of the 2021 IEEE International\
    \ Conference on Consumer Electronics and Computer Engineering (ICCECE),\nGuangzhou,\
    \ China, 15–17 January 2021; pp. 394–397. [CrossRef]\n64.\nSingh, A.; Sachan,\
    \ A. Student Clickstreams Activity Based Performance of Online Course. In International\
    \ Conference on Artiﬁcial\nIntelligence and Sustainable Computing; Springer: Berlin/Heidelberg,\
    \ Germany, 2021; pp. 242–253. [CrossRef]\n65.\nLee, C.A.; Tzeng, J.W.; Huang,\
    \ N.F.; Su, Y.S. Prediction of Student Performance in Massive Open Online Courses\
    \ Using Deep\nLearning System Based on Learning Behaviors. Educ. Technol. Soc.\
    \ 2021, 24, 130–146.\n66.\nHarvardX. HarvardX Person-Course Academic Year 2013\
    \ De-Identiﬁed Dataset, Version 3.0; Harvard University: Cambridge,\nMA, USA,\
    \ 2014. [CrossRef]\n67.\nHühn, J.; Hüllermeier, E. FURIA: An algorithm for unordered\
    \ fuzzy rule induction. Data Min. Knowl. Discov. 2009, 19, 293–319.\n[CrossRef]\n\
    68.\nMubarak, A.A.; Cao, H.; Ahmed, S.A. Predictive learning analytics using deep\
    \ learning model in MOOCs’ courses videos. Educ.\nInf. Technol. 2021, 26, 371–392.\n\
    69.\nMonllaÃ¸s OlivÃl’, D.; Huynh, D.; Reynolds, M.; Dougiamas, M.; Wiese, D.\
    \ A supervised learning framework: Using assessment\nto identify students at risk\
    \ of dropping out of a MOOC. J. Comput. High. Educ. 2020, 32. [CrossRef]\n70.\n\
    Burgos, C.; Campanario, M.L.; de la Peña, D.; Lara, J.A.; Lizcano, D.; Martínez,\
    \ M.A. Data mining for modeling students’\nperformance: A tutoring action plan\
    \ to prevent academic dropout. Comput. Electr. Eng. 2018, 66, 541–556. [CrossRef]\n\
    71.\nLaveti, R.N.; Kuppili, S.; Ch, J.; Pal, S.N.; Babu, N.S.C. Implementation\
    \ of learning analytics framework for MOOCs using state-\nof-the-art in-memory\
    \ computing. In Proceedings of the 2017 5th National Conference on E-Learning\
    \ E-Learning Technologies\n(ELELTECH), Hyderabad, India, 3–4 August 2017; pp.\
    \ 1–6. [CrossRef]\n72.\nCoussement, K.; Phan, M.; De Caigny, A.; Benoit, D.F.;\
    \ Raes, A. Predicting student dropout in subscription-based online learning\n\
    environments: The beneﬁcial impact of the logit leaf model. Decis. Support Syst.\
    \ 2020, 135, 113325. [CrossRef]\n73.\nHong, B.; Wei, Z.; Yang, Y. Discovering\
    \ learning behavior patterns to predict dropout in MOOC. In Proceedings of the\
    \ 2017 12th\nInternational Conference on Computer Science and Education (ICCSE),\
    \ Houston, TX, USA, 22–25 August 2017; pp. 700–704.\n[CrossRef]\n74.\nPanagiotakopoulos,\
    \ T.; Kotsiantis, S.; Kostopoulos, G.; Iatrellis, O.; Kameas, A. Early Dropout\
    \ Prediction in MOOCs through\nSupervised Learning and Hyperparameter Optimization.\
    \ Electronics 2021, 10, 1701. [CrossRef]\n75.\nAlamri, A.; Sun, Z.; Cristea, A.I.;\
    \ Steward, C.; Pereira, F.D. MOOC next week dropout prediction: Weekly assessing\
    \ time\nand learning patterns. In International Conference on Intelligent Tutoring\
    \ Systems; Springer: Berlin/Heidelberg, Germany, 2021;\npp. 119–130.\n76.\nXing,\
    \ W.; Du, D. Dropout Prediction in MOOCs: Using Deep Learning for Personalized\
    \ Intervention. J. Educ. Comput. Res. 2018,\n57, 547–570. [CrossRef]\n77.\nLiu,\
    \ K.; Tatinati, S.; Khong, A.W.H. A Weighted Feature Extraction Technique Based\
    \ on Temporal Accumulation of Learner\nBehavior Features for Early Prediction\
    \ of Dropouts. In Proceedings of the 2020 IEEE International Conference on Teaching,\n\
    Assessment, and Learning for Engineering (TALE), Takamatsu, Japan, 8–11 December\
    \ 2020; pp. 295–302. [CrossRef]\n78.\nXing, W.; Chen, X.; Stein, J.; Marcinkowski,\
    \ M. Temporal predication of dropouts in MOOCs: Reaching the low hanging fruit\n\
    through stacking generalization. Comput. Hum. Behav. 2016, 58, 119–129. [CrossRef]\n\
    79.\nTaylor, C.; Veeramachaneni, K.; O’Reilly, U.M. Likely to stop? predicting\
    \ stopout in massive open online courses. arXiv 2014,\narXiv:1408.3382.\nSustainability\
    \ 2022, 14, 6199\n23 of 23\n80.\nKloft, M.; Stiehler, F.; Zheng, Z.; Pinkwart,\
    \ N. Predicting MOOC dropout over weeks using machine learning methods. In\nProceedings\
    \ of the EMNLP 2014 Workshop on Analysis of Large Scale Social Interaction in\
    \ MOOCs, Doha, Qatar, 25 October\n2014; Humboldt University of Berlin: Berlin,\
    \ Germany, 2014; pp. 60–65.\n81.\nHalawa, S.; Greene, D.; Mitchell, J. Dropout\
    \ prediction in MOOCs using learner activity features. Proc. Second. Eur. Mooc\n\
    Stakehold. Summit 2014, 37, 58–65.\n82.\nWhitehill, J.; Williams, J.; Lopez, G.;\
    \ Coleman, C.; Reich, J. Beyond prediction: First steps toward automatic intervention\
    \ in\nMOOC student stopout; Available at SSRN 2611750; Elsevier: Amsterdam, The\
    \ Netherlands, 2015.\n83.\nQiu, L.; Liu, Y.; Liu, Y. An Integrated Framework With\
    \ Feature Selection for Dropout Prediction in Massive Open Online Courses.\nIEEE\
    \ Access 2018, 6, 71474–71484. [CrossRef]\n84.\nArdchir, S.; Ouassit, Y.; Ounacer,\
    \ S.; Jihal, H.; EL Goumari, M.Y.; Azouazi, M. Improving Prediction of MOOCs Student\
    \ Dropout\nUsing a Feature Engineering Approach. In International Conference on\
    \ Advanced Intelligent Systems for Sustainable Development;\nSpringer: Berlin/Heidelberg,\
    \ Germany, 2019; pp. 146–156.\n85.\nNazif, A.M.; Sedky, A.A.H.; Badawy, O.M. MOOC’s\
    \ Student Results Classiﬁcation by Comparing PNN and other Classiﬁers\nwith Features\
    \ Selection. In Proceedings of the 2020 21st International Arab Conference on\
    \ Information Technology (ACIT), Giza,\nEgypt, 28–30 November 2020; pp. 1–9.\n\
    86.\nJin, C. Dropout prediction model in MOOC based on clickstream data and student\
    \ sample weight. Soft Comput. 2021, 25,\n8971–8988. [CrossRef]\n87.\nMulyani,\
    \ E.; Hidayah, I.; Fauziati, S. Dropout Prediction Optimization through SMOTE\
    \ and Ensemble Learning. In Proceedings\nof the 2019 International Seminar on\
    \ Research of Information Technology and Intelligent Systems (ISRITI), Yogyakarta,\
    \ Indonesia,\n5–6 December 2019; pp. 516–521. [CrossRef]\n88.\nXiong, F.; Zou,\
    \ K.; Liu, Z.; Wang, H. Predicting Learning Status in MOOCs Using LSTM. In Proceedings\
    \ of the ACM Turing\nCelebration Conference—China; Association for Computing Machinery:\
    \ New York, NY, USA, 2019; pp. 1–5. [CrossRef]\n89.\nWu, N.; Zhang, L.; Gao, Y.;\
    \ Zhang, M.; Sun, X.; Feng, J. CLMS-Net: Dropout Prediction in MOOCs with Deep\
    \ Learning.\nIn Proceedings of the ACM Turing Celebration Conference—China; Association\
    \ for Computing Machinery: New York, NY, USA, 2019;\npp. 1–6. [CrossRef]\n90.\n\
    Wen, Y.; Tian, Y.; Wen, B.; Zhou, Q.; Cai, G.; Liu, S. Consideration of the local\
    \ correlation of learning behaviors to predict dropouts\nfrom MOOCs. Tsinghua\
    \ Sci. Technol. 2020, 25, 336–347. [CrossRef]\n91.\nQiu, L.; Liu, Y.; Hu, Q.;\
    \ Liu, Y. Student dropout prediction in massive open online courses by convolutional\
    \ neural networks. Soft\nComput. A Fusion Found. Methodol. Appl. 2019, 23, 10287.\
    \ [CrossRef]\n92.\nZheng, Y.; Gao, Z.; Wang, Y.; Fu, Q. MOOC Dropout Prediction\
    \ Using FWTS-CNN Model Based on Fused Feature Weighting and\nTime Series. IEEE\
    \ Access 2020, 8, 225324–225335. [CrossRef]\n93.\nWang, W.; Yu, H.; Miao, C. Deep\
    \ Model for Dropout Prediction in MOOCs. In Proceedings of the 2nd International\
    \ Conference\non Crowd Science and Engineering, Beijing, China, 6–9 July 2017;\
    \ Association for Computing Machinery: New York, NY, USA,\n2017; pp. 26–32. [CrossRef]\n\
    94.\nFu, Q.; Gao, Z.; Zhou, J.; Zheng, Y. CLSA: A novel deep learning model for\
    \ MOOC dropout prediction. Comput. Electr. Eng. 2021,\n94, 107315. [CrossRef]\n\
    95.\nYin, S.; Lei, L.; Wang, H.; Chen, W. Power of Attention in MOOC Dropout Prediction.\
    \ IEEE Access 2020, 8, 202993–203002.\n[CrossRef]\n96.\nZhang, Y.; Chang, L.;\
    \ Liu, T. MOOCs Dropout Prediction Based on Hybrid Deep Neural Network. In Proceedings\
    \ of the 2020\nInternational Conference on Cyber-Enabled Distributed Computing\
    \ and Knowledge Discovery (CyberC), Chongqing, China,\n29–30 October 2020; pp.\
    \ 197–203. [CrossRef]\n97.\nAshby, A. Monitoring student retention in the Open\
    \ University: Deﬁnition, measurement, interpretation and action. Open Learn.\n\
    Open Distance-Learn. 2004, 19, 65–77. [CrossRef]\n98.\nNg, K.H.R.; Tatinati, S.;\
    \ Khong, A.W.H. Grade Prediction From Multi-Valued Click-Stream Traces via Bayesian-Regularized\
    \ Deep\nNeural Networks. IEEE Trans. Signal Process. 2021, 69, 1477–1491. [CrossRef]\n\
    99.\nHakimi, L.; Eynon, R.; Murphy, V.A. The Ethics of Using Digital Trace Data\
    \ in Education: A Thematic Review of the Research\nLandscape. Rev. Educ. Res.\
    \ 2021, 91, 671–717. [CrossRef]\n100. Oboler, A.; Welsh, K.; Cruz, L. The danger\
    \ of big data: Social media as computational social science. First Monday 2012,\
    \ 17, 3993.\n101. Eynon, R.; Fry, J.; Schroeder, R. The ethics of online research.\
    \ SAGE Handb. Online Res. Methods 2017, 2, 19–37.\n"
  inline_citation: '>'
  journal: Sustainability (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2071-1050/14/10/6199/pdf?version=1652962764
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'Predicting Student Outcomes in Online Courses Using Machine Learning Techniques:
    A Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/telecom4030028
  analysis: '>'
  authors:
  - Souad Ajjaj
  - Souad El Houssaini
  - Mustapha Hain
  - Mohammed-Alamine El Houssaini
  citation_count: 0
  full_citation: '>'
  full_text: ">\nCitation: Ajjaj, S.; El Houssaini, S.;\nHain, M.; El Houssaini, M.-A.\n\
    Incremental Online Machine\nLearning for Detecting Malicious\nNodes in Vehicular\
    \ Communications\nUsing Real-Time Monitoring. Telecom\n2023, 4, 629–648. https://doi.org/\n\
    10.3390/telecom4030028\nAcademic Editor: Barbara M. Masini\nReceived: 22 June\
    \ 2023\nRevised: 14 July 2023\nAccepted: 5 September 2023\nPublished: 11 September\
    \ 2023\nCopyright:\n© 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\n\
    This article is an open access article\ndistributed\nunder\nthe\nterms\nand\n\
    conditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nArticle\nIncremental Online Machine Learning for Detecting Malicious\n\
    Nodes in Vehicular Communications Using Real-Time\nMonitoring\nSouad Ajjaj 1,*\n\
    , Souad El Houssaini 2, Mustapha Hain 1 and Mohammed-Alamine El Houssaini 3\n\
    1\nENSAM, AICSE Laboratory, Hassan II University, Casablanca 20000, Morocco\n\
    2\nFaculty of Sciences, Chouaib Doukkali University, El Jadida 24000, Morocco\n\
    3\nESEF, Chouaib Doukkali University, El Jadida 24000, Morocco\n*\nCorrespondence:\
    \ souad.ajjaj-etu@etu.univh2c.ma\nAbstract: Detecting malicious activities in\
    \ Vehicular Ad hoc Networks (VANETs) is an important\nresearch ﬁeld as it can\
    \ prevent serious damage within the network and enhance security and privacy.\n\
    In this regard, a number of approaches based on machine learning (ML) algorithms\
    \ have been\nproposed. However, they encounter several challenges due to data\
    \ being constantly generated over\ntime; this can impact the performance of models\
    \ trained on ﬁxed datasets as well as cause the need\nfor real-time data analysis\
    \ to obtain timely responses to potential threats in the network. Therefore,\n\
    it is crucial for machine learning models to learn and improve their predictions\
    \ or decisions in real\ntime as new data become available. In this paper, we propose\
    \ a new approach for attack detection in\nVANETs based on incremental online machine\
    \ learning. This approach uses data collected from the\nmonitoring of the VANET\
    \ nodes’ behavior in real time and trains an online model using incremental\n\
    online learning algorithms. More speciﬁcally, this research addresses the detection\
    \ of black hole\nattacks that pose a signiﬁcant threat to the Ad hoc On Demand\
    \ Distance Vector (AODV) routing\nprotocol. The data used for attack detection\
    \ are gathered from simulating realistic VANET scenarios\nusing the well-known\
    \ simulators Simulation of Urban Mobility (SUMO) and Network Simulator\n(NS-3).\
    \ Further, key features which are relevant in capturing the behavior of VANET\
    \ nodes under\nblack hole attack are monitored over time. The performance of two\
    \ online incremental classiﬁers,\nAdaptive Random Forest (ARF) and K-Nearest Neighbors\
    \ (KNN), are assessed in terms of Accuracy,\nRecall, Precision, and F1-score metrics,\
    \ as well as training and testing time. The results show that ARF\ncan be successfully\
    \ applied to classify and detect black hole nodes in VANETs. ARF outperformed\n\
    KNN in all performance measures but required more time to train and test compared\
    \ to KNN. Our\nﬁndings indicate that incremental online learning, which enables\
    \ continuous and real-time learning,\ncan be a potential method for identifying\
    \ attacks in VANETs.\nKeywords: VANETs; routing attacks; detection; incremental\
    \ learning; online learning\n1. Introduction\nVehicular Ad hoc Networks (VANETs)\
    \ is a type of Mobile Ad hoc Network (MANET)\nconsisting of vehicles equipped\
    \ with advanced communication devices and sophisticated\nfeatures. VANETs provide\
    \ a platform for various applications, such as safety, trafﬁc\nefﬁciency, and\
    \ infotainment services [1]. However, VANETs are subject to several kinds\nof\
    \ attacks, including denial-of-service, jamming, black holes, and worm hole attacks,\
    \ that\nmight compromise the network’s security and privacy [2]. To detect these\
    \ attacks, various\napproaches based on machine learning algorithms are suggested\
    \ [3]. These methods are\nproven to be effective in detecting both known and unknown\
    \ attacks, but they face several\nchallenges in VANET networks [4]. First, attack\
    \ patterns may change over time, which may\naffect the performance of a model\
    \ trained on a ﬁxed dataset [5]. Second, the complexity\nof the data in VANET\
    \ networks can increase rapidly over time, making it challenging to\nTelecom 2023,\
    \ 4, 629–648. https://doi.org/10.3390/telecom4030028\nhttps://www.mdpi.com/journal/telecom\n\
    Telecom 2023, 4\n630\nscale traditional machine learning algorithms to handle\
    \ new patterns in the data. In that\nregard, incremental online learning algorithms\
    \ can be an effective solution for real-time\napplications compared to traditional\
    \ machine learning techniques, which typically require\nlarge training datasets.\
    \ Incremental online learning algorithms allows the model to learn\ncontinuously\
    \ over time, adapting to new data as they become available [6].\nThe core interest\
    \ of this study is to explore incremental online learning algorithms\nin the context\
    \ of attack detection in VANETs. More speciﬁcally, our research focuses on\nthe\
    \ detection of the black hole attack, estimated as the most damaging attacks that\
    \ might\ncompromise Ad hoc On Demand Distance Vector (AODV) routing protocol in\
    \ VANETs in\nterms of data availability [7,8]. To this end, we have used data\
    \ gathered from extensive\nsimulations of realistic VANET scenarios based on two\
    \ well-known simulators, namely\nSUMO [9] and NS-3 [10]. The collected data incorporate\
    \ essential features which are\nrelevant in capturing the behavior of VANET nodes\
    \ under a black hole attack. We selected\nmore appropriate features, namely CTRLpackets,\
    \ CountRREQ, CountRREP, CountRERR,\nThroughput, SentPckts, ReceivedPckts and Dropping\
    \ ratio. Further, the performance of\ntwo incremental online classiﬁers, namely\
    \ Adaptive Random Forest (ARF) and K-Nearest\nNeighbors (KNN), is investigated\
    \ [11,12]. Additionally, metrics such as Accuracy, Precision,\nRecall, and F1-score\
    \ are assessed in order to track the classiﬁers’ performance over time.\nThe ﬁndings\
    \ demonstrate that ARF outperforms the KNN algorithm in terms of\nAccuracy, Precision,\
    \ Recall, and F1-score. However, the latter records a higher running\ntime. Furthermore,\
    \ this study outlines the effectiveness of the proposed approach as it\nenables\
    \ the creation of constantly updated models by processing small amounts of data\
    \ at\na time, rather than the entire dataset at once, unlike traditional machine\
    \ learning methods\nthat require substantial computing resources when dealing\
    \ with large datasets. Further,\nthis approach is especially beneﬁcial in dynamic\
    \ applications such as VANET networks,\nwhere data are continuously generated\
    \ in a streaming manner. Moreover, it is adaptive,\nallowing it to adapt and improve\
    \ predictions or decisions as new data arrive, thereby\naccommodating changing\
    \ data patterns over time.\nThe main contributions of this paper are as follows:\n\
    •\nWe introduce a more dynamic approach for detecting attacks based on incremental\n\
    online machine learning algorithms trained on data generated in real-time;\n•\n\
    We collect data form VANET scenarios using a robust methodology for VANET simu-\n\
    lations based two well-known simulators, namely SUMO and NS-3;\n•\nWe select essential\
    \ features that are relevant in capturing the behavior of black hole\nnodes in\
    \ the AODV routing protocol;\n•\nWe assess the overall performance of classiﬁers\
    \ in terms of multiple performance\nmetrics, namely Accuracy, Precision, Recall\
    \ and F1-score. Further, each performance\nmetric is tracked over time to continuously\
    \ evaluate the classiﬁers;\n•\nThe complexities of both classiﬁers in terms of\
    \ training and testing time are computed\nand compared.\nThe rest of this paper\
    \ is organized as follows: in Section 2, related works on attack\ndetection in\
    \ VANETs using ML-based approaches are reviewed. Section 3 presents the\nmaterials\
    \ and methods including the proposed method, the experimental settings, the sim-\n\
    ulation environment, the data collection, as well as the incremental algorithms\
    \ considered.\nResults and discussion are provided in Section 4. Finally, we summarize\
    \ the achieved\nresults and point out future research directions.\n2. Related\
    \ Works\nMachine learning is an intensively researched area within the broader\
    \ ﬁeld of artiﬁcial\nintelligence (AI), as it has been used in a variety of real-world\
    \ applications, such as image\nidentiﬁcation, speech recognition, natural language\
    \ processing, autonomous cars, attack\ndetection, fraud detection, etc. [3]. In\
    \ the research area of attack detection in VANETs,\nseveral solutions based on\
    \ machine learning algorithms have been suggested.\nTelecom 2023, 4\n631\nIn what\
    \ follows, we present the most recent studies that addressed attack detection\
    \ in\nVANETs.\nIn the paper [13], the authors propose a novel technique for detecting\
    \ jamming in wire-\nless networks within vehicles. They utilize unsupervised machine\
    \ learning and introduce a\nnew metric called variations of the relative speed\
    \ (RSV) between the jammer’s vehicle and\nthe receiver’s vehicle. Additionally,\
    \ parameters from the wireless communication systems\non the receiving vehicle\
    \ are taken into account. By applying unsupervised learning through\nclustering,\
    \ the proposed method effectively distinguishes intentional and unintentional\n\
    jamming and identiﬁes the unique characteristics associated with each jamming\
    \ attack. The\nauthors highlight the importance of relative speed and its variations\
    \ in successful jamming\ndetection. They also demonstrate that relying solely\
    \ on conventional wireless receiver\nmetrics from the physical and network layers,\
    \ such as PDR, SINR, and RSSI, is insufﬁcient\nfor discriminating interference\
    \ from deliberate jamming events or recognizing the speciﬁc\ntraits of an attack.\n\
    In another work [14], the objective is to analyze safety messages and detect false\n\
    position information sent by misbehaving nodes. Machine learning (ML) techniques\n\
    are applied to the VeReMi dataset to detect such misbehavior. The researchers\
    \ consider\nthe ML-based approach to be a practical and efﬁcient method for identifying\
    \ improper\nbehavior in real-world VANET scenarios. The results indicate that\
    \ SVM with normalization\noutperforms logistic regression with or without normalization.\
    \ The goal of study [15] is to\nuse machine learning to identify wormhole attacks\
    \ in VANETs’ multi-hop communication.\nThe authors create a scenario of multi-hop\
    \ communication using the AODV routing protocol\non the NS3 simulator, employing\
    \ mobility traces generated by the SUMO trafﬁc simulator\nto replicate the attack\
    \ in the VANET. The gathered traces are preprocessed to enable the\nmodel to learn\
    \ about wormhole attacks, and then, K-NN and SVM algorithms are applied.\nThe\
    \ two machine learning techniques are evaluated in terms of detection accuracy\
    \ for four\ndifferent alert types.\nIn [16], various machine learning techniques\
    \ are employed to identify ﬁve different\nattacks: constant attack, random attack,\
    \ constant offset attack, eventual attack, and random\noffset attack. Initially,\
    \ binary classiﬁcation methods are used to identify each attack individ-\nually.\
    \ Subsequently, a novel method is developed for multi-classiﬁcation of attacks.\
    \ The\naccuracy obtained for each attack type varies depending on the machine\
    \ learning algorithm\nused. The Random Forest technique achieves the highest accuracy\
    \ in the case of the new\nmulti-classiﬁcation procedure. The VeReMi dataset is\
    \ used for malicious node detection in\nVANETs.\nAnother study [17] suggests a\
    \ new machine learning approach that utilizes Random\nForest and posterior detection\
    \ based on coresets to enhance the performance of Intrusion\nDetection Systems\
    \ (IDSs). The results show that, in comparison to conventional machine\nlearning\
    \ models utilized in related applications, the proposed model greatly increases\n\
    detection accuracy.\nFor attack classiﬁcation in VANETs, a hybrid optimization-based\
    \ Deep Maxout Net-\nwork (DMN) is developed in [18]. The hybrid optimization approach\
    \ is used for Cluster\nHead (CH) selection and routing operations. Feature selection\
    \ is crucial for efﬁcient clas-\nsiﬁcation, and the DMN is employed for attack\
    \ classiﬁcation with a new optimization\napproach. The optimization model based\
    \ on the DMN improves routing performance,\nenergy consumption, and trust metrics.\
    \ Precision and Recall measures are also reported.\nIn [19], an Intelligent Intrusion\
    \ Detection System (IDS) that combines deep learning and\nmachine learning techniques\
    \ is proposed. Convolutional Neural Networks (CNN) and the\nAdaptive Neuro Fuzzy\
    \ Inference System (ANFIS) are both used in the system. This strategy\novercomes\
    \ this constraint by making use of soft computing techniques and an Intelligent\n\
    IDS system, in contrast to previous systems that focus on identifying known threats\
    \ in the\nVANET environment. A hybrid technique is used to identify several attack\
    \ types, such as\nDenial-of-Service (DoS), Botnet, PortScan, and Brute Force assaults.\
    \ Real-time data from\nTelecom 2023, 4\n632\nthe CIC-IDS 2017 dataset are used\
    \ for evaluation, and the proposed methodology shows\neffectiveness compared to\
    \ other state-of-the-art techniques.\nIn study [20], a practical and effective\
    \ machine learning (ML)-based method for\ndetecting malicious behavior is suggested.\
    \ The Vehicular Reference Misbehavior Dataset\n(VeReMi) is utilized by the proposed\
    \ machine-learning-based misbehavior detection system.\nThe VeReMi dataset contains\
    \ labeled examples of ﬁve different types of position falsiﬁcation\nattacks with\
    \ varying vehicle and attacker densities. The authors propose a model that\nemploys\
    \ two sequential BSM approaches to detect these attacks. The categorization of\
    \ the\nroadside unit’s model enables the identiﬁcation and removal of harmful\
    \ nodes from the\nnetwork, reducing computational overhead on moving vehicles.\
    \ Researchers used SVM in\nthe study [21] to identify false message attacks using\
    \ the driving status, speed, acceleration,\nvehicle type, reputation, and distance\
    \ as features. Additionally, they investigated message\nsuppression strategies\
    \ involving packet loss, packet delay, and packet forwarding. In order\nto combine\
    \ various assessments, their proposed vehicle trust model calls for both a local\n\
    vehicle trust module and a central Trust Authority (TA). In another paper by [22],\
    \ authors\nprovide a machine learning (ML) mechanism that makes use of three new\
    \ features, namely\nthose that relate to the sender position, to improve the performance\
    \ of IDS against position\nfalsiﬁcation attacks. Additionally, it compares K-Nearest\
    \ Neighbor (KNN) and Random\nForest (RF), two distinct machine learning (ML) algorithms\
    \ for classiﬁcation that are used\nto identify malicious vehicles using these\
    \ features.\nAll the aforementioned studies used approaches that have proven to\
    \ be effective in\ndetecting known and unknown attacks. However, they face several\
    \ challenges in VANETs\nbecause of the evolving nature of attack patterns as well\
    \ as the complexity of the data which\ncan increase rapidly over time, making\
    \ it difﬁcult to scale traditional machine learning\nalgorithms to handle data\
    \ generated in real-time. In the next section, we introduce the\nuse of incremental\
    \ online machine learning algorithms in the context of attack detection in\nVANETs.\n\
    3. Materials and Methods\nIn this section, we ﬁrst describe the proposed approach\
    \ for attack detection after\nintroducing incremental online machine learning.\
    \ The data collection process is extensively\ndetailed including experimental\
    \ settings, simulation environment, and selected features.\nTools and frameworks\
    \ used for the implementation of incremental online algorithms are\nalso described.\n\
    3.1. Incremental Online Learning\nRecently, incremental online learning has received\
    \ more attention, especially in the\ncontext of learning from data streams where\
    \ data continually arrive, such as in IoT appli-\ncations, spam ﬁltering, attack\
    \ detection, time series forecasting, etc. [5]. This evolution\nchallenges traditional\
    \ machine learning algorithms, which assume that they have access to\nthe entire\
    \ training dataset during the learning stage [3].\nUsing traditional machine learning\
    \ algorithms consists of collecting large historical\ndata, with the model being\
    \ trained on the training set and validated on the testing set.\nIn this way,\
    \ the model is trained on a large dataset that has been collected and labeled\n\
    beforehand [23]. This approach performs well when all the data are available up\
    \ front since\nthe model is trained on a large static dataset [6].\nIn contrast,\
    \ incremental online learning algorithms involves building dynamic models\nwhich\
    \ are trained in real-time, rather than on a ﬁxed dataset all at once [4].\nFormally,\
    \ let D be a data stream including instances denoted by\n\x10→x i, yi\n\x11\n\
    , (i = 1, . . . . . . .t, . . . .)\n(1)\nwhere t represents a timestamp,\n→x i\
    \ is a vector of features, and yi is the target or the class.\nTelecom 2023, 4\n\
    633\nIncremental online learning allows the model to learn continuously from each\
    \ new\ndata instance. The goal here is to continue to learn from new incoming\
    \ data and update\nthe model continuously [24]. The updated model can be used\
    \ to predict a label yi′ for an\nunlabeled\n→x i\n′. The primary difference between\
    \ traditional and online learning is how the\ndata are presented [25]. In the\
    \ former, the dataset is static and entirely available, whereas\ndata instances\
    \ are presented sequentially over time in online learning. Incremental online\n\
    learning has a number of characteristics including:\n•\nIncremental learning allows\
    \ for the use of data as they becomes available, creating\nmodels that are constantly\
    \ up-to-date instead of having to process the entire dataset\nat once [5]. This\
    \ can be very helpful in dynamic applications like VANET networks\nwhere data\
    \ are produced in a streaming way [6];\n•\nThere is a need to process a small\
    \ amount of data at a time to learn from it [25]. This is\nin contrast to traditional\
    \ machine learning techniques, which need a lot of computing\nresources, particularly\
    \ when training on big datasets [24];\n•\nThe ability to handle streaming data\
    \ in real-time [6]. This makes it particularly useful\nin this area of research,\
    \ where data are constantly being generated and need to be\nanalyzed in real-time\
    \ [4];\n•\nAdaptive, because it can adjust to changing data patterns over time.\
    \ This suggests\nthat it is capable of learning and improving its predictions\
    \ or decisions as new data\narrive [23].\nThe proposed approach for detecting\
    \ malicious nodes in VANETs is outlined in the\nfollowing section.\n3.2. Proposed\
    \ Method\nThe process of the proposed method for attack detection based on incremental\
    \ online\nlearning is outlined in the diagram below (Figure 1):\nTelecom 2023,\
    \ 4, FOR PEER REVIEW \n5 \n \nwhere t represents a timestamp, \U0001D465⃗௜ is\
    \ a vector of features, and \U0001D466௜ is the target or the class. \nIncremental\
    \ online learning allows the model to learn continuously from each new \ndata\
    \ instance. The goal here is to continue to learn from new incoming data and update\
    \ \nthe model continuously [24]. The updated model can be used to predict a label\
    \ \U0001D466௜\nᇱ for an \nunlabeled \U0001D465⃗௜\nᇱ. The primary diﬀerence between\
    \ traditional and online learning is how the \ndata are presented [25]. In the\
    \ former, the dataset is static and entirely available, whereas \ndata instances\
    \ are presented sequentially over time in online learning. Incremental online\
    \ \nlearning has a number of characteristics including: \n• \nIncremental learning\
    \ allows for the use of data as they becomes available, creating \nmodels that\
    \ are constantly up-to-date instead of having to process the entire dataset \n\
    at once [5]. This can be very helpful in dynamic applications like VANET networks\
    \ \nwhere data are produced in a streaming way [6]; \n• \nThere is a need to process\
    \ a small amount of data at a time to learn from it [25]. This \nis in contrast\
    \ to traditional machine learning techniques, which need a lot of compu-\nting\
    \ resources, particularly when training on big datasets [24]; \n• \nThe ability\
    \ to handle streaming data in real-time [6]. This makes it particularly useful\
    \ \nin this area of research, where data are constantly being generated and need\
    \ to be \nanalyzed in real-time [4]; \n• \nAdaptive, because it can adjust to\
    \ changing data patterns over time. This suggests \nthat it is capable of learning\
    \ and improving its predictions or decisions as new data \narrive [23]. \nThe\
    \ proposed approach for detecting malicious nodes in VANETs is outlined in the\
    \ \nfollowing section. \n3.2. Proposed Method \nThe process of the proposed method\
    \ for attack detection based on incremental online \nlearning is outlined in the\
    \ diagram below (Figure 1): \n \nFigure 1. The process of the proposed approach\
    \ for attack detection using incremental online learn-\ning. \nFigure 1. The process\
    \ of the proposed approach for attack detection using incremental online learning.\n\
    The basic idea of the proposed approach is to use the data collected from the\
    \ monitor-\ning of the VANET nodes behavior in real-time and train an online model\
    \ using incremental\nonline learning algorithms. The incremental model performed\
    \ is used for classifying\nTelecom 2023, 4\n634\nVANET nodes as normal or attacker.\
    \ The primary steps of the proposed method are as\nfollows:\n•\nInitial model\
    \ training\nAn initial model is trained on initial data to build a base model\
    \ before online incre-\nmental learning is performed. This model will serve as\
    \ the starting point for the online\nincremental learning process.\n•\nIncremental\
    \ model training\nThe model is incrementally updated after processing each new\
    \ data instance provided\nin real-time. This process is repeated each time a new\
    \ data instance is available, and the\nupdating of the models occurs continually\
    \ [6]. The model is updated or reﬁned employing\nnew data rather than being trained\
    \ from scratch on the entire dataset. This ensures that the\nmodel is always up-to-date.\
    \ The updated model is used for classifying VANET nodes as\nnormal or malicious.\n\
    •\nAttack detection\nThis component performs online classiﬁcation of unlabeled\
    \ data, leveraging the up-\ndated trained model to classify new data. Online classiﬁcation\
    \ makes it possible to detect\nattacks in real-time by identifying patterns that\
    \ indicate malicious activity. If the pattern of\nthe data is matched with an\
    \ attack, an alarm is triggered.\nThe proposed attack detection method involves\
    \ data collection using a monitoring\nsystem designed to record various metrics\
    \ and behavioral parameters within each vehicle\nnode in the network. This includes\
    \ recording the most relevant features to distinguish\nbetween normal nodes and\
    \ black hole nodes in the network.\nThe following subsections provide technical\
    \ details on how the data are collected\nand preprocessed. This includes network\
    \ simulation environment, scenario-based data\ncollection, and the most relevant\
    \ features for attack detection.\n3.3. Data Collection\n3.3.1. Simulation Environment\
    \ and Scenarios\nIn this study, the steps followed to simulate the VANET network\
    \ are as follows:\nStep 1: The preparation of a road network by importing a map\
    \ from Open Street Map\n(OSM) [26]. The simulation zone is deﬁned on a map form\
    \ the Moroccan city El Jadida\n(Figure 2).\nTelecom 2023, 4, FOR PEER REVIEW \n\
    6 \n \nThe basic idea of the proposed approach is to use the data collected from\
    \ the moni-\ntoring of the VANET nodes behavior in real-time and train an online\
    \ model using incre-\nmental online learning algorithms. The incremental model\
    \ performed is used for classify-\ning VANET nodes as normal or attacker. The\
    \ primary steps of the proposed method are \nas follows: \n• \nInitial model training\
    \ \nAn initial model is trained on initial data to build a base model before online\
    \ incre-\nmental learning is performed. This model will serve as the starting\
    \ point for the online \nincremental learning process. \n• \nIncremental model\
    \ training \nThe model is incrementally updated after processing each new data\
    \ instance pro-\nvided in real-time. This process is repeated each time a new\
    \ data instance is available, and \nthe updating of the models occurs continually\
    \ [6]. The model is updated or reﬁned em-\nploying new data rather than being\
    \ trained from scratch on the entire dataset. This ensures \nthat the model is\
    \ always up-to-date. The updated model is used for classifying VANET \nnodes as\
    \ normal or malicious. \n• \nAttack detection \nThis component performs online\
    \ classiﬁcation of unlabeled data, leveraging the up-\ndated trained model to\
    \ classify new data. Online classiﬁcation makes it possible to detect \nattacks\
    \ in real-time by identifying patterns that indicate malicious activity. If the\
    \ pattern \nof the data is matched with an attack, an alarm is triggered. \nThe\
    \ proposed attack detection method involves data collection using a monitoring\
    \ \nsystem designed to record various metrics and behavioral parameters within\
    \ each vehicle \nnode in the network. This includes recording the most relevant\
    \ features to distinguish \nbetween normal nodes and black hole nodes in the network.\
    \ \nThe following subsections provide technical details on how the data are collected\
    \ and \npreprocessed. This includes network simulation environment, scenario-based\
    \ data collec-\ntion, and the most relevant features for attack detection. \n\
    3.3. Data Collection \n3.3.1. Simulation Environment and Scenarios \nIn this study,\
    \ the steps followed to simulate the VANET network are as follows: \nStep 1: The\
    \ preparation of a road network by importing a map from Open Street Map \n(OSM)\
    \ [26]. The simulation zone is deﬁned on a map form the Moroccan city El Jadida\
    \ \n(Figure 2). \n \nFigure 2. The simulation map from Open Street Map. \nStep\
    \ 2: The generation of mobility trace ﬁles using SUMO (Simulation of Urban Mo-\n\
    bility) [9] which simulate realistic vehicle movements. SUMO provides a variety\
    \ of python \nFigure 2. The simulation map from Open Street Map.\nStep 2: The\
    \ generation of mobility trace ﬁles using SUMO (Simulation of Urban\nMobility)\
    \ [9] which simulate realistic vehicle movements. SUMO provides a variety of\n\
    python tools such as netconvert, randomTrips, poly-convert, and traceExporter\
    \ used for\nthe creation, execution, and evaluation of trafﬁc simulations. The\
    \ generated mobility trace\nﬁle is then fed to the network simulator NS-3 (Figure\
    \ 3).\nTelecom 2023, 4\n635\nTelecom 2023, 4, FOR PEER REVIEW \n7 \n \ntools such\
    \ as netconvert, randomTrips, poly-convert, and traceExporter used for the crea-\n\
    tion, execution, and evaluation of traﬃc simulations. The generated mobility trace\
    \ ﬁle is \nthen fed to the network simulator NS-3 (Figure 3). \n \nFigure 3. Network\
    \ XML ﬁle edited by SUMO. \nStep 3: Simulate the network components using the\
    \ NS-3 network simulator, which \nis used to model the VANET communications’ whole\
    \ protocol stack. Simulation scenarios \nare conﬁgured in NS-3 using scripts written\
    \ in the C++ programming language. Simula-\ntions are implemented using parameters\
    \ exhibited below (Table 1). \nTable 1. Simulation settings. \nParameter \nValue\
    \ \nPlatform \nLinux, Ubuntu environment. \nSimulator of network \nNS3.29 \nSimulator\
    \ of Mobility \nSUMO-0.32.0 \nRouting protocol \nAODV \nMac/Phy Layer \nIEEE 802.11p\
    \ \nWiFichannel \nYansWifi \nPropagation model \nfriisLoss model \nTransmission\
    \ power \n33 dbm \nTransport protocol \nUDP \nTraffic type \nCBR (constant bit\
    \ rate) \nPacket size \n64 bytes \nNumber of vehicles \n50 \nRuntime \n360 s \n\
    We consider two VANET scenarios using the simulation parameters previously out-\n\
    lined in Table 1. \nIn the ﬁrst scenario, 50 vehicles are involved, with 10 random\
    \ source–destination \npairs, which produce CBR traﬃc with ﬁxed-size packets.\
    \ The AODV routing protocol is \nused to route packets considering all nodes as\
    \ legitimate vehicles. In this case, no black \nhole node is simulated. \nIn the\
    \ second simulated scenario, we implement AODV routing protocol with black \n\
    hole attacks wherein 1 to 3 nodes are selected to act as attackers and are conﬁgured\
    \ at \ndiﬀerent times to launch the black hole attacks. The remainder of the nodes\
    \ are trustwor-\nthy and have legitimate communications to the other vehicles\
    \ in the network. \n \n \nFigure 3. Network XML ﬁle edited by SUMO.\nStep 3: Simulate\
    \ the network components using the NS-3 network simulator, which is\nused to model\
    \ the VANET communications’ whole protocol stack. Simulation scenarios are\nconﬁgured\
    \ in NS-3 using scripts written in the C++ programming language. Simulations\n\
    are implemented using parameters exhibited below (Table 1).\nTable 1. Simulation\
    \ settings.\nParameter\nValue\nPlatform\nLinux, Ubuntu environment.\nSimulator\
    \ of network\nNS3.29\nSimulator of Mobility\nSUMO-0.32.0\nRouting protocol\nAODV\n\
    Mac/Phy Layer\nIEEE 802.11p\nWiFichannel\nYansWiﬁ\nPropagation model\nfriisLoss\
    \ model\nTransmission power\n33 dbm\nTransport protocol\nUDP\nTrafﬁc type\nCBR\
    \ (constant bit rate)\nPacket size\n64 bytes\nNumber of vehicles\n50\nRuntime\n\
    360 s\nWe consider two VANET scenarios using the simulation parameters previously\
    \ out-\nlined in Table 1.\nIn the ﬁrst scenario, 50 vehicles are involved, with\
    \ 10 random source–destination pairs,\nwhich produce CBR trafﬁc with ﬁxed-size\
    \ packets. The AODV routing protocol is used to\nroute packets considering all\
    \ nodes as legitimate vehicles. In this case, no black hole node\nis simulated.\n\
    In the second simulated scenario, we implement AODV routing protocol with black\n\
    hole attacks wherein 1 to 3 nodes are selected to act as attackers and are conﬁgured\
    \ at\ndifferent times to launch the black hole attacks. The remainder of the nodes\
    \ are trustworthy\nand have legitimate communications to the other vehicles in\
    \ the network.\n3.3.2. Deﬁnition of Features\nDuring the simulation, a monitoring\
    \ system is designed to record various metrics and\nbehavioral parameters within\
    \ each vehicle node during the normal scenario and the black\nhole attack scenario.\
    \ This includes recording the most relevant information to distinguish\nbetween\
    \ a normal node and a black hole node in the network.\nHereafter, a brief description\
    \ of the characteristics of AODV routing protocol [27] and\nits vulnerability\
    \ to black hole attack, which will help us selecting essential features relevant\n\
    in detecting the black hole attack [7].\nTelecom 2023, 4\n636\nAODV relies on\
    \ two fundamental mechanisms, namely route discovery and route\nmaintenance. The\
    \ protocol utilizes RREQ (Route Request) and RREP (Route Reply) mes-\nsages for\
    \ route discovery. When a source node needs to send data to a destination, it\n\
    checks its routing table for a valid route. If there is no valid route available\
    \ (possibly due to\nnon-existence, expiration, or failure), the source node broadcasts\
    \ an RREQ message to all\nneighboring nodes.\nEach intermediate node forwards\
    \ the RREQ message until it reaches either the des-\ntination or an intermediate\
    \ node that has a valid route to the destination. The node with\na valid route\
    \ responds to the source node by sending a unicast RREP message along the\nreverse\
    \ path.\nUpon receiving the RREP message, the source node proceeds with transmitting\
    \ data\npackets [27]. Regarding the mechanism of route maintenance in AODV, active\
    \ routes are\nthe only ones that are maintained. Nodes regularly communicate with\
    \ their neighbors by\nsending HELLO messages in order to analyze the status of\
    \ the links and identify whether a\npath is fresh or not.\nIf a link breaks, a\
    \ Route Error (RERR) message is sent to the source node and any other\nnodes affected\
    \ by the broken link. The nodes are informed of the link failure by this RERR\n\
    message.\nThe AODV routing protocol’s activities are illustrated in Figure 4 [7].\n\
    During the simulation, a monitoring system is designed to record various metrics\
    \ \nand behavioral parameters within each vehicle node during the normal scenario\
    \ and the \nblack hole attack scenario. This includes recording the most relevant\
    \ information to dis-\ntinguish between a normal node and a black hole node in\
    \ the network. \nHereafter, a brief description of the characteristics of AODV\
    \ routing protocol [27] and \nits vulnerability to black hole attack, which will\
    \ help us selecting essential features rele-\nvant in detecting the black hole\
    \ attack [7]. \nAODV relies on two fundamental mechanisms, namely route discovery\
    \ and route \nmaintenance. The protocol utilizes RREQ (Route Request) and RREP\
    \ (Route Reply) mes-\nsages for route discovery. When a source node needs to send\
    \ data to a destination, it \nchecks its routing table for a valid route. If there\
    \ is no valid route available (possibly due \nto non-existence, expiration, or\
    \ failure), the source node broadcasts an RREQ message to \nall neighboring nodes.\
    \ \nEach intermediate node forwards the RREQ message until it reaches either the\
    \ desti-\nnation or an intermediate node that has a valid route to the destination.\
    \ The node with a \nvalid route responds to the source node by sending a unicast\
    \ RREP message along the \nreverse path. \nUpon receiving the RREP message, the\
    \ source node proceeds with transmitting data \npackets [27]. Regarding the mechanism\
    \ of route maintenance in AODV, active routes are \nthe only ones that are maintained.\
    \ Nodes regularly communicate with their neighbors by \nsending HELLO messages\
    \ in order to analyze the status of the links and identify whether \na path is\
    \ fresh or not. \nIf a link breaks, a Route Error (RERR) message is sent to the\
    \ source node and any \nother nodes aﬀected by the broken link. The nodes are\
    \ informed of the link failure by this \nRERR message. \nThe AODV routing protocol’s\
    \ activities are illustrated in Figure 4 [7]. \n \nFigure 4. AODV routing protocol\
    \ mechanism. \nA severe kind of denial of service known as a “black hole attack”\
    \ occurs when a rogue \nnode purposefully intercepts routing request (RREQ) messages\
    \ from nearby nodes. \nInstead of sending these RREQ packets as intended to other\
    \ nodes, the malicious \nnode disrupts the route discovery process by rapidly\
    \ providing a false route reply mes-\nsage (RREP) with the highest sequence number.\
    \ \nAs a result, the malicious node receives data packets from the source node,\
    \ assuming \nit is the best route, under the erroneous idea that the route discovery\
    \ was successful. How-\never, the misbehaving node captures all the routing packets\
    \ and intentionally discards \nthem, eﬀectively preventing any successful communication\
    \ [1]. \nFigure 4. AODV routing protocol mechanism.\nA severe kind of denial of\
    \ service known as a “black hole attack” occurs when a rogue\nnode purposefully\
    \ intercepts routing request (RREQ) messages from nearby nodes.\nInstead of sending\
    \ these RREQ packets as intended to other nodes, the malicious\nnode disrupts\
    \ the route discovery process by rapidly providing a false route reply message\n\
    (RREP) with the highest sequence number.\nAs a result, the malicious node receives\
    \ data packets from the source node, assuming it\nis the best route, under the\
    \ erroneous idea that the route discovery was successful. However,\nthe misbehaving\
    \ node captures all the routing packets and intentionally discards them,\neffectively\
    \ preventing any successful communication [1].\nBased on these unique behavioral\
    \ characteristics of the black hole attack, we will\nchoose features that are\
    \ the most relevant to distinguish between a normal node and a\nblack hole node\
    \ in the VANET network.\nHere are the key considered points when recording various\
    \ metrics and behavioral\nparameters:\n•\nRouting Behavior Analysis: this involves\
    \ tracking the overall routing control packets in\nAODV and monitoring particular\
    \ packets such as RREQ (Route Request), RREP (Route\nReply), and RERR (Route Error).\
    \ These parameters can contribute to the detection\nof the black hole attack,\
    \ so they are summarized in the CTRLpackets, CountRREQ,\nCountRREP, and CountRERR\
    \ features;\nTelecom 2023, 4\n637\n•\nTrafﬁc Analysis: this involves monitoring\
    \ the trafﬁc characteristics and keeping track\nof the number of bytes that are\
    \ sent or received by each node;\n•\nDropping ratio monitoring: observing the\
    \ dropping rate of packets can help in the\nidentiﬁcation of nodes that selectively\
    \ drop or discard incoming packets, indicating\nmalicious behavior;\n•\nThroughput\
    \ monitoring: black hole attack typically drops a large amount of data,\nwhich\
    \ may decrease signiﬁcantly the throughput, making it an important metric to\n\
    monitor for detection purposes.\nThe set of features recorded in our dataset are\
    \ summarized in Table 2: CTRLpackets,\nCount RREQ, CountRREP, CountRERR, Throughput,\
    \ Sent Pckts, Received Pckts, and\nDroppingRatio.\nTable 2. Relevant features\
    \ for black hole attack detection.\nFeature Name\nFeature Description\nCTRLpackets\n\
    AODV routing control packets: a black hole node may advertise a fake\nand optimized\
    \ route to the destination. Thus, it is essential to keep track\nof the routing\
    \ control packets and detect any changes in their number\nadvertised by a node.\n\
    CountRREQ\nNumber of Route Request messages that are used by the nodes to\ndiscover\
    \ new routes to other nodes in the network. In the case of a black\nhole attack,\
    \ the malicious node may not generate any RREQ messages\nbecause it is not interested\
    \ in receiving any packets, and instead, it drops\nall the packets that it receives.\n\
    CountRREP\nNumber of Route Reply messages that are sent by nodes in response to\n\
    RREQ messages to establish a route to the destination node. In the case of\na\
    \ black hole attack, the black hole sends fake RREP; if a large number of\nRREP\
    \ messages are received from a single node, it could be an indication\nthat the\
    \ node is a potential black hole.\nCountRERR\nNumber of Route Error (RERR) Messages:\
    \ a black hole node may\ngenerate an abnormal amount of RERR messages, indicating\
    \ that the\nnode is not following the protocol’s correct path discovery mechanism.\n\
    Throughput\nThroughput measures the amount of data that can be transferred in\
    \ a\ngiven time. A sudden decrease in throughput can be an indication of a\nblack\
    \ hole attack.\nSentPckts\nThe number of sent packets. The attacker node captures\
    \ and drops all\nrouting packets, leading to a signiﬁcant decrease in the number\
    \ of\nsuccessfully sent packets.\nReceivedPckts\nThe number of received packets.\
    \ The rogue node selectively drops all the\ndata packets received from other nodes,\
    \ which may cause a decrease in\nthe number of bytes that are received.\nDroppingRatio\n\
    The dropping ratio measures the percentage of packets dropped by a\nnode, and\
    \ this feature can be used to detect if a node is dropping a higher\nnumber of\
    \ packets than normal.\n3.4. Data Preprocessing\nIn the context of our study,\
    \ data normalization is employed to scale features to a\ncommon range between\
    \ 0 and 1 or −1 and 1, which make it simpler to compare and\nevaluate the data.\n\
    Different normalization techniques can be applied, including min–max scaling,\
    \ z-score\nnormalization, and log transformation [28].\nZ-score normalization\
    \ is employed to transform each instance of the original data xi\ninto x′\ni using\
    \ the following equation:\nx′\ni = xi − µ\nσ\n(2)\nTelecom 2023, 4\n638\nwhere\
    \ µ and σ denote the mean and standard deviation, respectively.\n3.5. Incremental\
    \ Online Algorithms\nIn the current study, we selected two incremental online\
    \ classiﬁers, namely Adaptive\nRandom Forest (ARF) and K-Nearest Neighbors (KNN)\
    \ [11,12]. The algorithms are chosen\nbased on their popularity in the online\
    \ classiﬁcation area and are freely available in the\npython framework sickit-multiﬂow\
    \ [12].\nAdaptive Random Forest (ARF): a modiﬁed version of the Random Forest\
    \ (ARF)\nmethod developed to handle data streams. The algorithm includes important\
    \ extended\ntechniques that modify the way the trees are trained and sampled in\
    \ order to adapt to\nstreaming data: the online bagging and adaptive resampling\
    \ techniques. In the standard\nRandom Forest algorithm, each tree is trained on\
    \ a bootstrap sample of the training data,\nwhich is obtained by randomly sampling\
    \ with replacement from the original data. However,\nin data streams, new instances\
    \ arrive continuously, and it is not feasible to store the entire\ntraining data\
    \ in memory. The core idea underlying online bagging is to train a tree in\na\
    \ forest using a sample that is generated by randomly selecting instances from\
    \ the data\nstream with replacement. The trees in the forest differ from one another\
    \ due to the use of\nseveral samples, resulting in a varied ensemble. Adaptive\
    \ resampling is another technique\nused to adapt to changes in the stream by updating\
    \ the weight of instances according to\ntheir arrival times. Formally, instances\
    \ that are more recent are given a higher weight,\nwhich enables the classiﬁer\
    \ to adapt to concept drift over time. This technique is used to\nupdate the weights\
    \ of instances in the sample used to build the trees [6,11].\nThe pseudo code\
    \ of the ARF algorithm is given in Algorithm 1. This algorithm begins\nby initializing\
    \ a speciﬁed number of trees in an ensemble. Each new instance in the data\nstream\
    \ is then sent to each tree in the ensemble for testing and training. ARF follows\
    \ a\n“test-then-train” approach, where an incoming instance is ﬁrst used to test\
    \ the model’s\nperformance by making a prediction and estimating its accuracy.\
    \ After testing, the instance\nis used to train the model. The training process\
    \ in ARF utilizes online bagging, a technique\nthat randomly selects a subset\
    \ of features and performs splits based on these features.\nAlgorithm 1: Pseudo\
    \ Code of Adaptive Random Forest\nInputs: n_trees: the number of trees in the\
    \ ensemble, STREAM: the stream of data instances,\nf_s_size: size of the random\
    \ subset of features to select for each split.\nOutputs: arf_model: the trained\
    \ ARF model\n1.\nInitialize an ensemble with a speciﬁed number of trees.\n2.\n\
    Receive a stream of data instances.\n3.\nFor each new instance in the stream:\n\
    a.\nTest the model’s performance by making a prediction.\nb.\nEstimate the model’s\
    \ performance.\nc.\nIf the performance is below the test threshold, train the\
    \ model with the instance.\nd.\nApply online bagging and select a random subset\
    \ of features with f_s_size for tree\ntraining.\ne.\nTrain all background trees\
    \ on the current instance.\n4.\nRepeat the above steps for all instances in the\
    \ stream\n5.\nThe trained ARF model, which is the ensemble of trees, is the output.\n\
    K-Nearest Neighbors (KNN): a well-known classiﬁcation algorithm used in machine\n\
    learning that relies on the distance metric between data points. In online classiﬁcation,\n\
    the algorithm works by maintaining a set of labeled instances in memory, which\
    \ are used\nto classify new incoming instances based on their proximity in the\
    \ feature space. For\neach incoming data instance x, the algorithm computes the\
    \ distance between x and each\ninstance in the data stream using a distance metric\
    \ which is typically the Euclidean distance.\nSubsequently, the K-Nearest Neighbors\
    \ of x in D based on their distances are found. The\nclass label of x is assigned\
    \ based on the majority of its K-Nearest Neighbors. In KNN,\nTelecom 2023, 4\n\
    639\nK is a hyper parameter that determines the number of nearest neighbors to\
    \ consider for\nclassiﬁcation. As new instances are received, the algorithm updates\
    \ its set of labeled\ninstances and can change its classiﬁcation decisions accordingly.\n\
    The pseudo code below exhibits the core functionality of the KNN algorithm, which\n\
    involves updating the model with new instances and making predictions based on\
    \ the\nK-nearest neighbors.\nAlgorithm 2: Pseudo Code of K-Nearest Neighbors\n\
    Inputs: K: The number of nearest neighbors to consider, M: The memory size or\
    \ maximum number of instances to retain, STREAM:\nthe stream of data instances.\n\
    Outputs: Predicted label for each instance in the data stream\n1.\nInitialize\
    \ the KNN model:\nSet the number of nearest neighbors (K).\nSet the memory size\
    \ (M).\nInitialize an empty memory buffer.\n2.\nReceive a stream of data instances:\n\
    For each new instance in the stream:\na.\nFind the K nearest neighbors from the\
    \ instances in the memory buffer using eEuclidien distance metric.\nb.\nClassify\
    \ the new instance based on the majority class among the K-nearest neighbors.\n\
    c.\nUpdate the model and memory buffer:\nIf the memory buffer is not full (number\
    \ of instances < M), add the new instance to the buffer.\nIf the memory buffer\
    \ is full, replace the oldest instance in the buffer with the new instance.\n\
    d.\nStore the predicted label for the new instance.\n3.\nRepeat step 2 for all\
    \ instances in the data stream.\n3.6. Prequential Evaluation\nIn the performance\
    \ evaluation of machine learning models, the hold-out evaluation\nmethod is the\
    \ commonly used approach, wherein the available data are split into two\nparts:\
    \ a training set and a test set [3]. The model is trained on the training set,\
    \ and its\nperformance is evaluated on the test set to evaluate how well the model\
    \ can perform on\nnew unseen data.\nPrequential evaluation, on the other hand,\
    \ is a more recent and alternative approach to\nmodel evaluation used for data\
    \ generated continuously in time. In prequential evaluation\n(or test-then-train\
    \ evaluation), the basic idea is to use new data instances to ﬁrstly test the\n\
    model and then to train it [29]. The process of prequential evaluation involves\
    \ the following\nsteps:\n•\nPretrain the model with initial data;\n•\nTest and\
    \ train: For each incoming data instance, the model is tested on the current\n\
    instance and then trained using the same instance, using the partial_ﬁt() method;\n\
    •\nEvaluate: The model’s performance is tracked over time where metrics are updated\n\
    over time.\nFor the prequential evaluation, several parameters are tuned amongst\
    \ test step,\nmax_samples, and pretrain_size, which refer, respectively, to the\
    \ number of samples\nbetween each model test, the maximum number of samples to\
    \ process from the data stream,\nand the number of samples to use for pretraining\
    \ the model before starting the prequential\nlearning process [12].\nAn example\
    \ of pseudo code for prequential accuracy evaluation is outlined in the\nAlgorithm\
    \ 3 below:\nTelecom 2023, 4\n640\nAlgorithm 3: Prequential Accuracy for Model\
    \ Evaluation\nInputs: D: a stream of data (X,y), Arf: the classiﬁer to be evaluated,\
    \ pretrain_size: the number of\nsamples for pretraining the model, Ts: test step,\
    \ number of samples to process between each\nmodel test, max_samples: the maximum\
    \ number of samples to evaluate.\nOutputs: Acc_list: a list containing accuracy\
    \ measures after each test step.\n1.\nInitialize Acc_list to store accuracy measures\n\
    2.\nInitial counters CorrPred and TotalPred for correct predictions and total\
    \ predictions.\n3.\nPretrain the model by calling model.partial_ﬁt() on pretrain_size\
    \ samples of the data stream.\nfor i in range(pretrain_size)\n(X, y) = D.next_sample()\n\
    Arf.partial_ﬁt(X, y)\nend for\n4. Repeat for max_samples iterations:\nfor i in\
    \ range(max_samples):\n(X, y) = D.next_sample()\nprediction = model.predict(X)\n\
    Arf.partial_ﬁt(X, y)\nTotalPred+= 1\nif y == prediction:\nCorrPred+= 1\nif TotalPred\
    \ % Ts == 0:\naccuracy = CorrPred/TotalPred\nAcc_list.append(accuracy)\nCorrPred\
    \ = 0\nend if\nend for\n5. Return Acc_list\nIn this study, we use Accuracy, Precision,\
    \ Recall, and F1-score metrics computed\naccording to the following equations:\n\
    Accuracy = (tp + tn)/(tp + fp + fn + tn)\n(3)\nPrecision = tp/((tp + fp))\n(4)\n\
    Recall = tp/(tp + fn)\n(5)\nF1 − score = 2 ∗ ((Precision ∗ Recall)/(Precision\
    \ + Recall))\n(6)\nwhere tp, fp, fn, and tn are, respectively, number of instances\
    \ correctly classiﬁed as positive,\nnumber of instances incorrectly classiﬁed\
    \ as positive, number of instances incorrectly\nclassiﬁed as negative, and number\
    \ of instances correctly classiﬁed as negative.\nExperiments are performed on\
    \ Jupyter notebook, and Python 3.9.13 is used along\nwith the Matplotlib 3.5.2,\
    \ NumPy 1.20.0, and pandas 1.4.4 libraries. Implementation of\nincremental online\
    \ algorithms is performed using scikit-multiﬂow 0.5.3, a Python library\nthat\
    \ provides a range of tools, algorithms, and evaluation metrics to handle data\
    \ streams\nin real-time scenarios. Scikit-multiﬂow can be used along with other\
    \ Python libraries like\nNumpy and SciPy, pandas and scikit-learn.\nIn the subsequent\
    \ section, we will exhibit and discuss the results of evaluating the\noverall\
    \ performance of the Adaptive Random Forest (ARF) and K-Nearest Neighbors\n(KNN)\
    \ classiﬁers.\n4. Results and Discussion\nIn this section, we will ﬁrst analyze\
    \ the mean Accuracy of classiﬁers under varying\nvalues of the pretrain_size parameter,\
    \ which refers to the number of samples to use for\nTelecom 2023, 4\n641\npretraining\
    \ the model before starting the prequential learning process. We will then\ncompare\
    \ the overall performance of the classiﬁers according to Accuracy, Recall, Precision,\n\
    and F1-score metrics. Plots that monitor the performance metrics of both classiﬁers\
    \ over\ntime are also given. Finally, the running time (training time and testing\
    \ time) of both\nclassiﬁers are assessed and compared.\nThe dataset used for evaluation\
    \ includes eight features and two classes (Normal, BKH)\nwith 4492 data samples\
    \ as Normal and 4670 as black hole.\nRegarding the choice of the pretrain_size\
    \ parameter for prequential evaluation, we\nevaluated the mean Accuracy of both\
    \ classiﬁers under varying values of the pretrain_size\nin order to choose the\
    \ most appropriate value. Table 3 reports the obtained results.\nTable 3. The\
    \ mean Accuracy of classiﬁers under varying pretrain size values.\nClassiﬁer\n\
    Pretrain_Size\n400\n600\n1000\n1200\nAdaptive Random Forest (ARF)\n94.73%\n94.77%\n\
    94.83%\n94.90%\nK-Nearest Neighbors classiﬁer (KNN)\n86.86%\n87.13%\n87.30%\n\
    87.83%\nThe results exhibited in Table 3 show that both classiﬁers Adaptive Random\
    \ Forest\n(ARF) and K-Nearest Neighbors (KNN) record Accuracy above 86% for the\
    \ different values\nof Pretraining size. It is also observed that Adaptive Random\
    \ Forest (ARF) achieves higher\nAccuracy values than the KNN, which reaches almost\
    \ 95% for the different values of\nthe pretrain_size parameter. Further, we note\
    \ that the overall Accuracy increases as the\npretrain_size increases (varying\
    \ from 400, 600, 1000 to 1200) for both classiﬁers.\nThe Accuracy results of Adaptive\
    \ Random Forest (ARF) varies from 94.73% to 94.90%,\npresenting a slight increase,\
    \ while KNN improves its Accuracy from 86.86% to 87.38%\nwhen increasing the pretrain_size\
    \ from 400 to 1200.\nOverall, the results of the carried-out experiments clearly\
    \ indicate that using ARF\nachieves good Accuracy even with small pretraining\
    \ data sizes, while the KNN’s classiﬁer\nAccuracy improves as the pretrain_size\
    \ values increase.\nThis can be explained by the inherent characteristics of the\
    \ Adaptive Random Forest\n(ARF) algorithm which is based on ensemble learning.\
    \ ARF tends to leverage diverse\nmodels trained on different subsets of data to\
    \ make a decision about the class label. This\nallows ARF to perform well in situations\
    \ with a small number of pretraining samples. The\nK-Nearest Neighbors (KNN) classiﬁer,\
    \ on the other hand, relies on the nearest neighbors\nfor classiﬁcation.\nThis\
    \ suggest that when increasing the pretraining data size, the KNN algorithm can\n\
    achieve better performance.\nTherefore, considering the above ﬁndings, we adopt\
    \ {pretrain-size = 1200} for the\nevaluation of the classiﬁers in the rest of\
    \ our study.\n4.1. Performance Assessment of Classiﬁers\nThe mean Accuracy, Precision,\
    \ Recall, and F1-score of each classiﬁer are reported in\nTable 4\nTable 4. The\
    \ mean performance of ARF and KNN classiﬁers.\nIncremental Classiﬁer\nAccuracy\n\
    Precision\nRecall\nF1-Score\nAdaptive Random Forest (ARF)\n94.90%\n93.10%\n96.81%\n\
    94.92%\nKNN Classiﬁer\n87.38%\n87.58%\n86.62%\n87.09%\nFrom Table 4, we notice\
    \ that both ARF and KNN classiﬁers achieve good performance.\nHowever, Adaptive\
    \ Random Forest (ARF) outperforms the KNN classiﬁer in terms of\nAccuracy, Precision,\
    \ Recall, and F1-score.\nTelecom 2023, 4\n642\nARF reaches 94.90%, 93.10%, 96.81%\
    \ and 94.92% for Accuracy, Precision, Recall and\nF1-score, respectively, while\
    \ KNN records a lower performance: the Accuracy is 87.38%,\nPrecision is 87.58%,\
    \ Recall is 86.62%, and F1-score is 87.09%.\nThese ﬁndings suggest that Adaptive\
    \ Random Forest may be a more suitable choice for\ndetecting malicious nodes in\
    \ VANETs, offering improved predictive capabilities compared\nto the KNN algorithm.\n\
    Complementing the above results, the variations of the performance of both classiﬁers\n\
    over time according to each performance metric are plotted.\nFigures 5–8 illustrate\
    \ the monitoring over time of the performance of the classiﬁers\nregarding Accuracy,\
    \ Precision, Recall, and F1-score, respectively.\nTelecom 2023, 4, FOR PEER REVIEW\
    \ \n15 \n \nThese ﬁndings suggest that Adaptive Random Forest may be a more suitable\
    \ choice \nfor detecting malicious nodes in VANETs, oﬀering improved predictive\
    \ capabilities com-\npared to the KNN algorithm. \nComplementing the above results,\
    \ the variations of the performance of both classiﬁ-\ners over time according\
    \ to each performance metric are plotted. \nFigures 5–8 illustrate the monitoring\
    \ over time of the performance of the classiﬁers \nregarding Accuracy, Precision,\
    \ Recall, and F1-score, respectively. \n \nFigure 5. Accuracy of ARF and KNN classiﬁers\
    \ over time. \n \nFigure 6. Recall of ARF and KNN classiﬁers over time. \nFigure\
    \ 5. Accuracy of ARF and KNN classiﬁers over time.\nTelecom 2023, 4, FOR PEER\
    \ REVIEW \n15 \n \nThese ﬁndings suggest that Adaptive Random Forest may be a\
    \ more suitable choice \nfor detecting malicious nodes in VANETs, oﬀering improved\
    \ predictive capabilities com-\npared to the KNN algorithm. \nComplementing the\
    \ above results, the variations of the performance of both classiﬁ-\ners over\
    \ time according to each performance metric are plotted. \nFigures 5–8 illustrate\
    \ the monitoring over time of the performance of the classiﬁers \nregarding Accuracy,\
    \ Precision, Recall, and F1-score, respectively. \n \nFigure 5. Accuracy of ARF\
    \ and KNN classiﬁers over time. \n \nFigure 6. Recall of ARF and KNN classiﬁers\
    \ over time. \nFigure 6. Recall of ARF and KNN classiﬁers over time.\nTelecom\
    \ 2023, 4\n643\nTelecom 2023, 4, FOR PEER REVIEW \n16 \n \n \nFigure 7. Precision\
    \ of ARF and KNN classiﬁers over time. \n \nFigure 8. F1-score of ARF and KNN\
    \ classiﬁers over time. \nFor each performance metric, the mean and the current\
    \ values are plotted. We note \nthat the mean Accuracy is obtained by averaging\
    \ the Accuracy scores obtained in all test \nsteps while the current Accuracy\
    \ refers to the Accuracy of the classiﬁer on the current test \nstep. \nFor each\
    \ classiﬁer, Figures 5–8 exhibit both the mean and the current performance \n\
    over time. \nFigure 5 represents the monitoring of Accuracy for the ARF and KNN\
    \ algorithms. \nFirst, we notice that the Accuracy of ARF is higher than that\
    \ of KNN throughout the eval-\nuation time, with a mean Accuracy ranging from\
    \ 93% to 95% for ARF while KNN records \na lower performance with a mean Accuracy\
    \ between 83% and 87%. \nFurther, we observe that the plots of the mean Accuracy\
    \ for both ARF and KNN ap-\npear to be stable throughout the evaluation time.\
    \ This outcome is logical since the mean \nAccuracy is an average measure which\
    \ computes the overall performance of the classiﬁer \nover multiple test periods.\
    \ \nThese outcomes suggest that both classiﬁers show consistent Accuracy over\
    \ time. \nSimilarly, examining the plots of the current Accuracy of both ARF and\
    \ KNN, we point \nout that ARF achieves a higher Accuracy between 92% and 97%,\
    \ outperforming KNN, \nwhose current Accuracy ranged from 83% to 91%. \nFigure\
    \ 7. Precision of ARF and KNN classiﬁers over time.\nTelecom 2023, 4, FOR PEER\
    \ REVIEW \n16 \n \n \nFigure 7. Precision of ARF and KNN classiﬁers over time.\
    \ \n \nFigure 8. F1-score of ARF and KNN classiﬁers over time. \nFor each performance\
    \ metric, the mean and the current values are plotted. We note \nthat the mean\
    \ Accuracy is obtained by averaging the Accuracy scores obtained in all test \n\
    steps while the current Accuracy refers to the Accuracy of the classiﬁer on the\
    \ current test \nstep. \nFor each classiﬁer, Figures 5–8 exhibit both the mean\
    \ and the current performance \nover time. \nFigure 5 represents the monitoring\
    \ of Accuracy for the ARF and KNN algorithms. \nFirst, we notice that the Accuracy\
    \ of ARF is higher than that of KNN throughout the eval-\nuation time, with a\
    \ mean Accuracy ranging from 93% to 95% for ARF while KNN records \na lower performance\
    \ with a mean Accuracy between 83% and 87%. \nFurther, we observe that the plots\
    \ of the mean Accuracy for both ARF and KNN ap-\npear to be stable throughout\
    \ the evaluation time. This outcome is logical since the mean \nAccuracy is an\
    \ average measure which computes the overall performance of the classiﬁer \nover\
    \ multiple test periods. \nThese outcomes suggest that both classiﬁers show consistent\
    \ Accuracy over time. \nSimilarly, examining the plots of the current Accuracy\
    \ of both ARF and KNN, we point \nout that ARF achieves a higher Accuracy between\
    \ 92% and 97%, outperforming KNN, \nwhose current Accuracy ranged from 83% to\
    \ 91%. \nFigure 8. F1-score of ARF and KNN classiﬁers over time.\nFor each performance\
    \ metric, the mean and the current values are plotted. We note\nthat the mean\
    \ Accuracy is obtained by averaging the Accuracy scores obtained in all test\n\
    steps while the current Accuracy refers to the Accuracy of the classiﬁer on the\
    \ current\ntest step.\nFor each classiﬁer, Figures 5–8 exhibit both the mean and\
    \ the current performance\nover time.\nFigure 5 represents the monitoring of Accuracy\
    \ for the ARF and KNN algorithms.\nFirst, we notice that the Accuracy of ARF is\
    \ higher than that of KNN throughout the\nevaluation time, with a mean Accuracy\
    \ ranging from 93% to 95% for ARF while KNN\nrecords a lower performance with\
    \ a mean Accuracy between 83% and 87%.\nFurther, we observe that the plots of\
    \ the mean Accuracy for both ARF and KNN\nappear to be stable throughout the evaluation\
    \ time. This outcome is logical since the mean\nAccuracy is an average measure\
    \ which computes the overall performance of the classiﬁer\nover multiple test\
    \ periods.\nThese outcomes suggest that both classiﬁers show consistent Accuracy\
    \ over time.\nSimilarly, examining the plots of the current Accuracy of both ARF\
    \ and KNN, we point out\nthat ARF achieves a higher Accuracy between 92% and 97%,\
    \ outperforming KNN, whose\ncurrent Accuracy ranged from 83% to 91%.\nTelecom\
    \ 2023, 4\n644\nOverall, the plots of current Accuracy present more variations\
    \ in comparison with\nthose of mean Accuracy. This is because current Accuracy\
    \ reﬂects the performance of the\nclassiﬁer on the most recent test period of\
    \ time.\nConsidering the above results, we can conclude that monitoring both mean\
    \ and current\nAccuracy over the evaluation time can be useful in monitoring the\
    \ real-time performance\nof the classiﬁers and provide a more reliable estimate\
    \ of the classiﬁer’s ability to detect\nmisbehavior.\nSimilarly, Figures 6–8 capture\
    \ the variations of Recall, Precision, and F1-score metrics\nover time. Both the\
    \ plots of mean and current metric are monitored. ARF shows superior\nperformance\
    \ in comparison with KNN regarding all metrics, namely Recall, Precision, and\n\
    F1-score.\nIn summary, the carried-out experiments demonstrate that Adaptive Random\
    \ Forest\n(ARF) has superior performance than the K-Nearest Neighbors (KNN) algorithm\
    \ regarding\nAccuracy, Recall, Precision, and F1-score metrics in the context\
    \ of our study.\nThese ﬁndings can be attributed to the speciﬁcities of each algorithm.\
    \ On the one\nhand, Adaptive Random Forest (ARF) is based on an ensemble approach\
    \ that combines\nmultiple decision trees to make predictions, which allows for\
    \ making accurate predictions.\nFurther, ARF’s online bagging mechanism allows\
    \ reducing the impact of poor or irrele-\nvant features on the classiﬁcation process.\
    \ This technique makes ARF capable of making\naccurate predictions by handling\
    \ imbalance in data and capturing the data patterns more\naccurately [11]. On\
    \ the other hand, KNN relies on the calculation of a distance metric and\nmakes\
    \ predictions based on the majority of the K-Nearest Neighbors [12].\nIn what\
    \ follows, we aim to analyze the training and testing time of the ARF and KNN\n\
    algorithms.\n4.2. Training and Testing Time\nFigure 9 shows the variations of\
    \ ARF and KNN models in terms of total running time,\nwhich include both training\
    \ time and testing time.\nTelecom 2023, 4, FOR PEER REVIEW \n17 \n \nOverall,\
    \ the plots of current Accuracy present more variations in comparison with \n\
    those of mean Accuracy. This is because current Accuracy reﬂects the performance\
    \ of the \nclassiﬁer on the most recent test period of time. \nConsidering the\
    \ above results, we can conclude that monitoring both mean and cur-\nrent Accuracy\
    \ over the evaluation time can be useful in monitoring the real-time perfor-\n\
    mance of the classiﬁers and provide a more reliable estimate of the classiﬁer’s\
    \ ability to \ndetect misbehavior. \nSimilarly, Figures 6–8 capture the variations\
    \ of Recall, Precision, and F1-score metrics \nover time. Both the plots of mean\
    \ and current metric are monitored. ARF shows superior \nperformance in comparison\
    \ with KNN regarding all metrics, namely Recall, Precision, \nand F1-score. \n\
    In summary, the carried-out experiments demonstrate that Adaptive Random Forest\
    \ \n(ARF) has superior performance than the K-Nearest Neighbors (KNN) algorithm\
    \ regard-\ning Accuracy, Recall, Precision, and F1-score metrics in the context\
    \ of our study. \nThese ﬁndings can be attributed to the speciﬁcities of each\
    \ algorithm. On the one \nhand, Adaptive Random Forest (ARF) is based on an ensemble\
    \ approach that combines \nmultiple decision trees to make predictions, which\
    \ allows for making accurate predic-\ntions. Further, ARF’s online bagging mechanism\
    \ allows reducing the impact of poor or \nirrelevant features on the classiﬁcation\
    \ process. This technique makes ARF capable of \nmaking accurate predictions by\
    \ handling imbalance in data and capturing the data pat-\nterns more accurately\
    \ [11]. On the other hand, KNN relies on the calculation of a distance \nmetric\
    \ and makes predictions based on the majority of the K-Nearest Neighbors [12].\
    \ \nIn what follows, we aim to analyze the training and testing time of the ARF\
    \ and KNN \nalgorithms. \n4.2. Training and Testing Time \nFigure 9 shows the\
    \ variations of ARF and KNN models in terms of total running time, \nwhich include\
    \ both training time and testing time. \n \nFigure 9. Training and testing time\
    \ for ARF and KNN. \nIt is observed that ARF requires an average time of 56.53\
    \ s for training and testing, \nwhereas the average training and testing time\
    \ for KNN is 28.43 s. \nThese ﬁndings can be attributed to the fact that the two\
    \ algorithms have fundamental \ndiﬀerences. KNN relies on relatively simple techniques\
    \ based on determining the K-Near-\nest Neighbors and assigning the most common\
    \ class label as the new label for the data \npoint [30]. ARFs, on the other hand,\
    \ are ensembles of decision trees that combine decisions \nFigure 9. Training\
    \ and testing time for ARF and KNN.\nIt is observed that ARF requires an average\
    \ time of 56.53 s for training and testing,\nwhereas the average training and\
    \ testing time for KNN is 28.43 s.\nThese ﬁndings can be attributed to the fact\
    \ that the two algorithms have fundamental\ndifferences. KNN relies on relatively\
    \ simple techniques based on determining the K-\nNearest Neighbors and assigning\
    \ the most common class label as the new label for the\ndata point [30]. ARFs,\
    \ on the other hand, are ensembles of decision trees that combine\nTelecom 2023,\
    \ 4\n645\ndecisions from various trees and aggregate their results to obtain the\
    \ ﬁnal output [6,11].\nConsequently, ARF might require higher training and testing\
    \ time than KNN.\n4.3. Comparison with State-of-the-Art Methods\nIn this section,\
    \ we present a concise comparison of our proposed work with the\nstate-of-the-art\
    \ methods explored in this study as exhibited in Table 5.\nTable 5. Comparison\
    \ with state-of-the-art ML methods for attack detection.\nStudy\nApproach\nReal-Time\n\
    Dataset\nTools\nPerformance\nMetrics\nContinuous\nLearning\n[14]\nSVM and Logistic\n\
    Regression\nNo\nVeReMi\nPYTHON tools\nF1-score\nNo\n[16]\nBinary classiﬁcation\
    \ with\nNaïve Bayes, decision tree\nand Random Forest\nNo\nVeReMi\nPYTHON tools\n\
    Accuracy\nNo\n[17]\nRandom Forest and a\nposterior detection based\non coresets\n\
    No\nCICIDS2017\nMATLAB\nAccuracy\nNo\n[18]\nHybrid\noptimization-based Deep\n\
    Maxout Network (DMN)\nNo\nBoT-IoT data\nand\nNSL-KDD\ndata\nPYTHON tools\nPrecision\n\
    and Recall\nNo\n[19]\nAdaptive Neuro Fuzzy\nInference System (ANFIS)\nand Convolutional\
    \ Neural\nNetworks (CNN)\nNo\nCICIDS 2017\nPYTHON tools\nPrecision\nSensitivity\n\
    Recall\nSpeciﬁcity\nNo\n[22]\nML methods for\nclassiﬁcation, KNN and RF\nNo\n\
    VeReMi\nPYTHON tools\nAccuracy\nF1-Score\nNo\n[21]\nSVM\nNo\nGenerated\ndata\n\
    SUMO and\nOMNeT++\nPYTHON tools\nTPR, FPR, and\nACC\nNo\n[31]\nDistributed multi-layer\n\
    classiﬁer\nYes\nGenerated\ndata\nOMNET++ SUMO\nAccuracy\nNo\nOur work\nIncremental\
    \ Online\nclassiﬁcation using\nAdaptive Random Forest\n(ARF) and K-Nearest\nNeighbors\
    \ (KNN)\nclassiﬁers\nYes\nGenerated\ndata\nNS-3\nSUMO\nPython tools\nAccuracy\n\
    Recall\nPrecision\nF1-score\nTraining time\nTesting time\nYes\nThe comparison\
    \ is conducted based on several parameters such as the real-time\ndetection, the\
    \ dataset used, the involved tools, the considered evaluation metrics, and the\n\
    online learning approach.\nThe results of this comparison indicate that the current\
    \ study may offer a novel strategy,\ninvolving incremental online learning for\
    \ detecting routing attacks in VANETs.\nFirst, a key beneﬁt of our approach is\
    \ the ability to monitor the behavior of nodes\nin the network in real-time. This\
    \ allows identifying potential threats in time and acting\nappropriately.\nFurther,\
    \ we build our own dataset for attack detection, which incorporates essen-\ntial\
    \ features, which are relevant in capturing the behavior of VANET nodes under\
    \ black\nhole attacks.\nAdditionally, the adopted simulation methodology allows\
    \ reliable VANET simulations\nbuilt upon accurate mobility models that were generated\
    \ from realistic maps using popular\ntools like OSM, SUMO, and NS-3.\nTelecom\
    \ 2023, 4\n646\nFinally, our proposed approach allows continuous and real-time\
    \ learning thanks to\nthe incremental learning process, which allows for the use\
    \ of data as it becomes available,\ncreating models that are constantly up-to-date\
    \ instead of models that are trained on ﬁxed\ndatasets. This suggests that it\
    \ is capable of learning and improving its predictions or\ndecisions as new data\
    \ arrive.\n5. Conclusions\nThe current study introduced a new approach for detecting\
    \ routing attacks in VANETs,\nleveraging incremental online learning algorithms\
    \ trained on data generated in real-time.\nOur study focused on assessing the\
    \ performance of two algorithms, namely Adaptive\nRandom Forest (ARF) and K-Nearest\
    \ Neighbors, with regard to Accuracy, Precision, Recall,\nand F1-score metrics.\
    \ The training and testing time of both classiﬁers were also analyzed.\nOur research\
    \ particularly addressed the detection of black hole attacks, which pose a\nsigniﬁcant\
    \ threat to the AODV routing protocol. Data used for attack detection are collected\n\
    from simulating realistic VANET scenarios using two well-known simulators, namely\n\
    SUMO and NS-3. Further, essential features, which are relevant in capturing the\
    \ behavior\nof VANET nodes under a black hole attack, are monitored over time.\n\
    The ﬁndings show that incremental learning is a promising solution in time-critical\n\
    applications like attack detection in highly dynamic environments such as VANETs,\
    \ as it\nallows continuous and real-time learning. Further, the results show that\
    \ Adaptive Random\nForest (ARF) can be successfully applied to classify and detect\
    \ black hole nodes in VANETs.\nARF outperformed KNN with respect to all performance\
    \ measures. However, ARF required\nmore time for both training and testing in\
    \ comparison with KNN.\nWhile these conclusions have shown promising results,\
    \ the detection of intrusions\nusing online incremental machine learning remains\
    \ a challenging problem due to the\nconstantly evolving nature of attacks and\
    \ the need to continuously adapt the models to new\nthreats. It is worth noting\
    \ that the choice of features used to represent the network trafﬁc\ndata has a\
    \ signiﬁcant impact on the performance of the classiﬁer. Further, the performances\n\
    of incremental learning classiﬁers seem to vary depending on various factors like\
    \ the\ndataset used, the complexity of the problem, as well as the type of the\
    \ incremental learning\nframework used.\nAs a next step, we intend to lead an\
    \ in-depth study by tuning the algorithms’ parame-\nters in order to enhance their\
    \ performance as well as the overall required time for training\nand testing.\n\
    Finally, it is crucial to acknowledge the challenges of real-time attack detection\
    \ in\nVANETs, owing to the highly dynamic nature of the network and the rise of\
    \ sophisticated\nattack strategies. Accordingly, our next priority is to implement\
    \ a hybrid model that\nincorporates ARF with other well-known algorithms like\
    \ online SVM for more effective\nand precise detection of potential threats and\
    \ attacks in VANETs.\nAuthor Contributions: Conceptualization, S.A. and S.E.H.;\
    \ methodology, S.A. and M.-A.E.H.; vali-\ndation, M.H.; formal analysis, S.A.,\
    \ M.H. and S.E.H.; investigation, S.A. and S.E.H.; resources, S.A.\nand S.E.H.;\
    \ data curation, S.A. and S.E.H.; writing—original draft preparation, S.A., M.H.\
    \ and S.E.H.;\nwriting—review and editing, S.A., M.H. and S.E.H.; visualization,\
    \ S.A., M.-A.E.H. and S.E.H.; su-\npervision, M.H.; project administration, M.H.\
    \ and S.E.H. All authors have read and agreed to the\npublished version of the\
    \ manuscript.\nFunding: This research received no external funding.\nData Availability\
    \ Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nTelecom 2023, 4\n647\nReferences\n1.\nAjjaj, S.; El Houssaini,\
    \ S.; Hain, M.; El Houssaini, M.-A. A New Multivariate Approach for Real Time\
    \ Detection of Routing\nSecurity Attacks in VANETs. Information 2022, 13, 282.\
    \ [CrossRef]\n2.\nBanafshehvaragh, S.T.; Rahmani, A.M. Intrusion, Anomaly, and\
    \ Attack Detection in Smart Vehicles. Microprocess. Microsyst. 2023,\n96, 104726.\
    \ [CrossRef]\n3.\nMchergui, A.; Moulahi, T.; Zeadally, S. Survey on Artiﬁcial\
    \ Intelligence (AI) Techniques for Vehicular Ad-Hoc Networks (VANETs).\nVeh. Commun.\
    \ 2022, 34, 100403. [CrossRef]\n4.\nNallaperuma, D.; Nawaratne, R.; Bandaragoda,\
    \ T.; Adikari, A.; Nguyen, S.; Kempitiya, T.; De Silva, D.; Alahakoon, D.; Pothuhera,\n\
    D. Online Incremental Machine Learning Platform for Big Data-Driven Smart Trafﬁc\
    \ Management. IEEE Trans. Intell. Transp. Syst.\n2019, 20, 4679–4690. [CrossRef]\n\
    5.\nLosing, V.; Hammer, B.; Wersing, H. Incremental On-Line Learning: A Review\
    \ and Comparison of State of the Art Algorithms.\nNeurocomputing 2018, 275, 1261–1274.\
    \ [CrossRef]\n6.\nLópez, J.M. Fast and Slow Machine Learning. Ph.D. Thesis, Université\
    \ Paris-Saclay–Télécom Paristech, Paris, France, 2019.\n7.\nMalik, A.; Khan, M.Z.;\
    \ Faisal, M.; Khan, F.; Seo, J.-T. An Efﬁcient Dynamic Solution for the Detection\
    \ and Prevention of Black\nHole Attack in VANETs. Sensors 2022, 22, 1897. [CrossRef]\n\
    8.\nAjjaj, S.; El Houssaini, S.; Hain, M.; El Houssaini, M.-A. Performance Assessment\
    \ and Modeling of Routing Protocol in Vehicular\nAd Hoc Networks Using Statistical\
    \ Design of Experiments Methodology: A Comprehensive Study. ASI 2022, 5, 19. [CrossRef]\n\
    9.\nDocumentation-SUMO Documentation. Available online: https://sumo.dlr.de/docs/index.html\
    \ (accessed on 21 September 2021).\n10.\nNs-3|a Discrete-Event Network Simulator\
    \ for Internet Systems. Available online: https://www.nsnam.org/ (accessed on\
    \ 21\nSeptember 2021).\n11.\nGomes, H.M.; Bifet, A.; Read, J.; Barddal, J.P.;\
    \ Enembreck, F.; Pfharinger, B.; Holmes, G.; Abdessalem, T. Adaptive Random Forests\n\
    for Evolving Data Stream Classiﬁcation. Mach. Learn. 2017, 106, 1469–1495. [CrossRef]\n\
    12.\nMontiel, J.; Jesse, R.; Bifet, A.; Talel, A. Scikit-Multiﬂow: A Multi-Output\
    \ Streaming Framework. J. Mach. Learn. Res. 2018, 19,\n2914–2915.\n13.\nKaragiannis,\
    \ D.; Argyriou, A. Jamming Attack Detection in a Pair of RF Communicating Vehicles\
    \ Using Unsupervised Machine\nLearning. Veh. Commun. 2018, 13, 56–63. [CrossRef]\n\
    14.\nSingh, P.K.; Gupta, S.; Vashistha, R.; Nandi, S.K.; Nandi, S. Machine Learning\
    \ Based Approach to Detect Position Falsiﬁcation\nAttack in VANETs. In Security\
    \ and Privacy; Nandi, S., Jinwala, D., Singh, V., Laxmi, V., Gaur, M.S., Faruki,\
    \ P., Eds.; Springer:\nSingapore, 2019; Volume 939, pp. 166–178. ISBN 9789811375606.\n\
    15.\nSingh, P.K.; Gupta, R.R.; Nandi, S.K.; Nandi, S. Machine Learning Based Approach\
    \ to Detect Wormhole Attack in VANETs. In\nWeb, Artiﬁcial Intelligence and Network\
    \ Applications; Barolli, L., Takizawa, M., Xhafa, F., Enokido, T., Eds.; Springer\
    \ International\nPublishing: Cham, Switzerland, 2019; Volume 927, pp. 651–661.\
    \ ISBN 978-3-030-15034-1.\n16.\nSonker, A.; Gupta, R.K. A New Procedure for Misbehavior\
    \ Detection in Vehicular Ad-Hoc Networks Using Machine Learning.\nInt. J. Electr.\
    \ Comput. Eng. IJECE 2021, 11, 2535. [CrossRef]\n17.\nBangui, H.; Ge, M.; Buhnova,\
    \ B. A Hybrid Machine Learning Model for Intrusion Detection in VANET. Computing\
    \ 2022, 104,\n503–531. [CrossRef]\n18.\nKaur, G.; Kakkar, D. Hybrid Optimization\
    \ Enabled Trust-Based Secure Routing with Deep Learning-Based Attack Detection\
    \ in\nVANET. Ad Hoc Netw. 2022, 136, 102961. [CrossRef]\n19.\nKarthiga, B.; Durairaj,\
    \ D.; Nawaz, N.; Venkatasamy, T.K.; Ramasamy, G.; Hariharasudan, A. Intelligent\
    \ Intrusion Detection\nSystem for VANET Using Machine Learning and Deep Learning\
    \ Approaches. Wirel. Commun. Mob. Comput. 2022, 2022, 5069104.\n[CrossRef]\n20.\n\
    Sharma, A. Position Falsiﬁcation Detection in VANET with Consecutive BSM Approach\
    \ Using Machine Learning Algorithm.\nPh.D. Thesis, Faculty of Graduate Studies\
    \ through the School of Computer Science, Windsor, ON, Canada, 2021.\n21.\nZhang,\
    \ C.; Chen, K.; Zeng, X.; Xue, X. Misbehavior Detection Based on Support Vector\
    \ Machine and Dempster-Shafer Theory of\nEvidence in VANETs. IEEE Access 2018,\
    \ 6, 59860–59870. [CrossRef]\n22.\nErcan, S.; Ayaida, M.; Messai, N. Misbehavior\
    \ Detection for Position Falsiﬁcation Attacks in VANETs Using Machine Learning.\n\
    IEEE Access 2022, 10, 1893–1904. [CrossRef]\n23.\nRojas, J.S.; Rendon, A.; Corrales,\
    \ J.C. Consumption Behavior Analysis of over the Top Services: Incremental Learning\
    \ or Traditional\nMethods? IEEE Access 2019, 7, 136581–136591. [CrossRef]\n24.\n\
    Jin, B.; Jing, Z.; Zhao, H. Incremental and Decremental Extreme Learning Machine\
    \ Based on Generalized Inverse. IEEE Access\n2017, 5, 20852–20865. [CrossRef]\n\
    25.\nAlmeida, A.; Brás, S.; Sargento, S.; Pinto, F.C. Time Series Big Data: A\
    \ Survey on Data Stream Frameworks, Analysis and\nAlgorithms. J. Big Data 2023,\
    \ 10, 83. [CrossRef]\n26.\nOpenStreetMap. Available online: https://www.openstreetmap.org/\
    \ (accessed on 4 September 2023).\n27.\nDas, S.R.; Belding-Royer, E.M.; Perkins,\
    \ C.E. Ad Hoc On-Demand Distance Vector (AODV) Routing. Available online: https:\n\
    //tools.ietf.org/html/rfc3561 (accessed on 20 December 2020).\n28.\nSingh, D.;\
    \ Singh, B. Investigating the Impact of Data Normalization on Classiﬁcation Performance.\
    \ Appl. Soft Comput. 2020, 97,\n105524. [CrossRef]\nTelecom 2023, 4\n648\n29.\n\
    Hidalgo, J.I.G.; Maciel, B.I.F.; Barros, R.S.M. Experimenting with Prequential\
    \ Variations for Data Stream Learning Evaluation.\nComput. Intell. 2019, 35, 670–692.\
    \ [CrossRef]\n30.\nAlQabbany, A.O.; Azmi, A.M. Measuring the Effectiveness of\
    \ Adaptive Random Forest for Handling Concept Drift in Big Data\nStreams. Entropy\
    \ 2021, 23, 859. [CrossRef] [PubMed]\n31.\nRashid, K.; Saeed, Y.; Ali, A.; Jamil,\
    \ F.; Alkanhel, R.; Muthanna, A. An Adaptive Real-Time Malicious Node Detection\
    \ Framework\nUsing Machine Learning in Vehicular Ad-Hoc Networks (VANETs). Sensors\
    \ 2023, 23, 2594. [CrossRef] [PubMed]\nDisclaimer/Publisher’s Note: The statements,\
    \ opinions and data contained in all publications are solely those of the individual\n\
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or\
    \ the editor(s) disclaim responsibility for any injury to\npeople or property\
    \ resulting from any ideas, methods, instructions or products referred to in the\
    \ content.\n"
  inline_citation: '>'
  journal: Telecom (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2673-4001/4/3/28/pdf?version=1694419772
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Incremental Online Machine Learning for Detecting Malicious Nodes in Vehicular
    Communications Using Real-Time Monitoring
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/electronics9071177
  analysis: '>'
  authors:
  - Javed Asharf
  - Nour Moustafa
  - Hasnat Khurshid
  - Essam Debie
  - Waseem Haider
  - Abdul Wahab
  citation_count: 118
  full_citation: '>'
  full_text: ">\nelectronics\nReview\nA Review of Intrusion Detection Systems Using\n\
    Machine and Deep Learning in Internet of Things:\nChallenges, Solutions and Future\
    \ Directions\nJaved Asharf 1\n, Nour Moustafa 2,*\n, Hasnat Khurshid 1, Essam\
    \ Debie 2\n, Waqas Haider 2 and\nAbdul Wahab 3\n1\nMilitary College of Signals,\
    \ National University of Sciences and Technology (NUST), H-12, Islamabad 44000,\n\
    Pakistan; javed.ashraf@mcs.edu.pk (J.A.); hasnat@mcs.edu.pk (H.K.)\n2\nSchool\
    \ of Engineering and Information Technology, University of New South Wales at\
    \ the Australian\nDefence Force Academy, Canberra 2610, Australia; e.debie@unsw.edu.au\
    \ (E.D.);\nw.haider@adfa.edu.au (W.H.)\n3\nDepartment of computer Science, Riphah\
    \ University, Islamabad 44000, Pakistan; abdulwahab86@gmail.com\n*\nCorrespondence:\
    \ nour.moustafa@unsw.edu.au\nReceived: 25 May 2020; Accepted: 12 July 2020; Published:\
    \ 20 July 2020\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\
    \a\nAbstract: The Internet of Things (IoT) is poised to impact several aspects\
    \ of our lives with its fast\nproliferation in many areas such as wearable devices,\
    \ smart sensors and home appliances. IoT devices\nare characterized by their connectivity,\
    \ pervasiveness and limited processing capability. The number\nof IoT devices\
    \ in the world is increasing rapidly and it is expected that there will be 50\
    \ billion devices\nconnected to the Internet by the end of the year 2020. This\
    \ explosion of IoT devices, which can\nbe easily increased compared to desktop\
    \ computers, has led to a spike in IoT-based cyber-attack\nincidents. To alleviate\
    \ this challenge, there is a requirement to develop new techniques for detecting\n\
    attacks initiated from compromised IoT devices. Machine and deep learning techniques\
    \ are in this\ncontext the most appropriate detective control approach against\
    \ attacks generated from IoT devices.\nThis study aims to present a comprehensive\
    \ review of IoT systems-related technologies, protocols,\narchitecture and threats\
    \ emerging from compromised IoT devices along with providing an overview\nof intrusion\
    \ detection models. This work also covers the analysis of various machine learning\
    \ and\ndeep learning-based techniques suitable to detect IoT systems related to\
    \ cyber-attacks.\nKeywords: IoT security; IoT protocols; intrusion detection system;\
    \ machine learning; deep learning;\ncyber-attacks\n1. Introduction\nThe recent\
    \ development in communications and information technologies, such as the Internet\n\
    of Things (IoT), has extraordinarily surpassed the traditional sensing of nearby\
    \ environments.\nIoT technologies have facilitated the development of systems\
    \ that can improve life quality. IoT is\none of the fastest-growing technologies\
    \ in computing, with an estimated 50 billion devices by the\nend of 2020 [1].\
    \ It has been estimated that, by the year 2025, the IoT and related applications\n\
    have a potential economic impact of $3.9 trillion to $11.1 trillion per year [2].\
    \ The IoT devices can\nbecome smart objects by taking advantage of its core technologies\
    \ like communication technologies,\npervasive and ubiquitous computing, embedded\
    \ devices, Internet protocols, sensor networks,\nand Artiﬁcial Intelligence (AI)-based\
    \ applications [3].\nThe ubiquitous interconnection of physically distributed\
    \ IoT devices extends the computation\nand communication to other IoT devices\
    \ with different speciﬁcations [4]. Multiple types of sensors,\nembedded in these\
    \ devices, enable them to gather real-time data from the physical devices remotely.\n\
    Electronics 2020, 9, 1177; doi:10.3390/electronics9071177\nwww.mdpi.com/journal/electronics\n\
    Electronics 2020, 9, 1177\n2 of 45\nThe collected data from the devices allows\
    \ us to make intelligent decision systems as well as effectively\nmanaging IoT\
    \ environments. However, connecting the commonly used real-world devices to the\n\
    Internet also raises concerns about cybersecurity threats [5,6]. Therefore there\
    \ is a requirement to\ndesign and develop intelligent security solutions for the\
    \ protection of IoT devices and against attacks\ngenerated from compromised IoT\
    \ devices.\n1.1. Motivation\nWhile IoT technologies play a vital part in improving\
    \ real-life smart systems, like smart cities,\nsmart homes, smart healthcare,\
    \ the large scale and ubiquitous nature of IoT systems has introduced\nnew security\
    \ challenges [5–7]. Furthermore, since IoT devices generally work in an unattended\n\
    environment, an attacker may physically access these devices with malicious intent\
    \ [8,9]. Also,\nbecause IoT devices are connected usually over wireless networks,\
    \ eavesdropping can be used to\naccess private information from a communication\
    \ channel [10,11]. On top of these security challenges,\nIoT devices cannot afford\
    \ the implementation of advanced security features because of their restricted\n\
    energy and computation resources. Due to the interconnected and interdependent\
    \ settings of the\nIoT, new attack surfaces are emerging very regularly [12,13].\
    \ Thus, IoT systems are more vulnerable\nas compared to traditional computing\
    \ systems. This necessitates research in speciﬁc detective and\npreventive techniques\
    \ for IoT systems to protect against IoT devices based threats.\nFor protecting\
    \ IoT systems against cyber threats, another line of defense should be developed\
    \ in\nIoT networks. Intrusion Detection Systems (IDSs) fulfill this purpose [14,15].\
    \ Various surveys have\nattempted to describe machine learning-based IDSs for\
    \ protection against IoT networks or compromised\nIoT devices. The surveys cover\
    \ research work on IDSs for cloud-based IoT systems [16], Wireless sensor\nnetworks\
    \ [17–19], cyber-physical systems [20], and mobile ad hoc networks (MANETs) [21–23].\
    \ However,\ntraditional IDS methods are less effective or insufficient for the\
    \ security of IoT systems because of\ntheir peculiar characteristics mentioned\
    \ above, in particular, limited energy, ubiquitous, heterogeneity,\nlimited bandwidth\
    \ capacity and global connectivity. Machine Learning (ML) and Deep Learning\n\
    (DL) based techniques have recently gained credibility in a successful application\
    \ for the detection of\nnetwork attacks including IoT networks. This is because\
    \ ML/DL based methods can capture benign\nand anomalous behavior in IoT environments.\
    \ IoT devices and network traffic can be captured and\ninvestigated to learn normal\
    \ patterns. Any deviation from these normal learned patterns can be used to\n\
    detect anomalous behavior. Furthermore, ML/DL based methods have been tested to\
    \ predict new or\nzero-day attacks. Hence, ML/DL based algorithms provide robust\
    \ security protocols for designing the\nsecurity of IoT devices and networks.\n\
    Various surveys have discussed different techniques for designing IDS for IoT\
    \ systems, but most\nof the aforementioned surveys did not address the implementation\
    \ of ML or DL techniques as detection\nmechanisms in IoT networks and their lightweight\
    \ devices in a comprehensive manner. Some of\nthese studies published in [24–29]\
    \ revealed that the focus was on studying the issues in IoT security\ngenerally\
    \ and their classiﬁcation in different layers related to applications, network,\
    \ encryption and\nauthentication, and access controls. A comprehensive study covering\
    \ a detailed review of ML and DL\nbased techniques for IDSs in IoT networks still\
    \ needs further systematic analysis and investigation,\nwhich is a major focus\
    \ of this study.\n1.2. Scope of This Survey\nThis survey includes six important\
    \ areas related to IDSs for IoT systems and networks: (1) IoT\narchitectures and\
    \ technologies; (2) IoT threats and attack types; (3) IDS architectures and their\
    \ design;\n(4) an explanation of ML and DL techniques applied in the design of\
    \ IDSs; (5) a description of various\ndatasets available to researchers for evaluation\
    \ of their proposed IDS; and (6) future research challenges\nand directions.\n\
    Electronics 2020, 9, 1177\n3 of 45\n1.3. Main Contribution\nIn this paper, a detailed\
    \ review of network threats from IoT networks and their devices with\ncorresponding\
    \ ML and DL based attack detection techniques is presented. Table 1 summarizes\
    \ a\ncomparison of our survey with the other surveys conducted on IDSs in IoT\
    \ networks. As described in\nthe table, this survey covers all important aspects\
    \ on the subject of ML and DL based techniques used\nfor IDS in IoT networks and\
    \ their systems. The table also shows that other surveys partially cover\nsome\
    \ of the aspects and there is no single paper that explains all the aspects. The\
    \ key contributions of\nthis survey are described as follows:\n•\nDiscussion of\
    \ IoT architectures and IoT Protocols, covering their technologies, frequency\
    \ bands,\nand data rates.\n•\nExplanation of vulnerabilities, threat dimensions\
    \ and attack surfaces of IoT systems, including\nattack types related to IoT protocols,\
    \ which are discussed in detail.\n•\nReview of ML- and DL-based IDSs, involving\
    \ their design choices, pros, cons and detection\nmethods, which are covered in\
    \ detail.\n•\nDiscussion of the datasets available for network and IoT security-related\
    \ research, covering the\nadvantages and limitations of each enumerated with details.\n\
    •\nExplanation of the applications of ML and DL techniques for developing IDSs\
    \ in IoT networks\nand their systems.\n•\nPresentation of the current research\
    \ challenges and their future directions for research in this ﬁeld.\nThe organization\
    \ of the paper is presented as follows. In Section 2, recent studies conducted\n\
    related to the anomaly and intrusion detection in IoT networks are discussed.\
    \ In Section 3, an overview\nof IoT systems is presented covering IoT architecture\
    \ and reference models and IoT protocols. Section 4\ndescribes various attacks\
    \ and threats against IoT systems. Following this, Section 5 discusses IDS\narchitecture,\
    \ its design choices and various detection methods, including their ML and DL\
    \ techniques\ndescribed in Sections 6 and 7, respectively. Section 8 describes\
    \ brieﬂy the datasets that are available\nand used for testing IDS. Finally, the\
    \ future challenges and paper’s conclusion are provided in\nSections 9 and 10,\
    \ respectively.\n2. Current Reviews\nVarious survey studies have been carried\
    \ out in the ﬁeld of IoT security by describing\nvulnerabilities in IoT systems.\
    \ However, most of the existing studies on IoT security have not mainly\nfocused\
    \ on the applications of ML/DL techniques for IoT security. Table 1 summarizes\
    \ a comparison\nof our survey with the other surveys conducted on IDSs in IoT\
    \ networks. The comparison discusses\nthe contributions of each survey related\
    \ to the design of IoT-based IDSs.\nTable 1. A comparison of this survey with\
    \ others in terms of developing IoT-based Intrusion Detection\nSystems (IDSs).\n\
    IoT IDS Aspects\nSurvey Ref\nIoT\nArchitecture\nIoT\nProtocols\nIoT\nThreats\n\
    IoT IDS\nDesign\nChoices\nIoT IDS-ML\nTechniques\nIoT IDS-DL\nTechniques\nIoT\n\
    Datasets\n[30]\n✓\n✓\n✓\n✓\n×\n×\n×\n[16]\n×\n×\n✓\n✓\n✓\n×\n×\n[31]\n×\n×\n✓\n\
    ×\n×\n×\n×\n[19]\n×\n×\n×\n✓\n×\n×\n×\n[21]\n×\n×\n✓\n✓\n×\n×\n×\n[22]\n×\n×\n\
    ✓\n✓\n×\n×\n×\nElectronics 2020, 9, 1177\n4 of 45\nTable 1. Cont.\nIoT IDS Aspects\n\
    Survey Ref\nIoT\nArchitecture\nIoT\nProtocols\nIoT\nThreats\nIoT IDS\nDesign\n\
    Choices\nIoT IDS-ML\nTechniques\nIoT IDS-DL\nTechniques\nIoT\nDatasets\n[23]\n\
    ×\n×\n✓\n✓\n✓\n✓\n×\n[32]\n×\n✓\n✓\n×\n×\n×\n×\n[33]\n×\n✓\n✓\n✓\n×\n×\n×\n[34]\n\
    ×\n×\n✓\n×\n✓\n×\n×\n[35]\n×\n×\n×\n×\n✓\n✓\n✓\n[36]\n×\n×\n✓\n×\n✓\n✓\n✓\n[37]\n\
    ✓\n×\n×\n✓\n✓\n✓\n✓\n[38]\n×\n×\n✓\n✓\n×\n×\n×\nThis Survey\n✓\n✓\n✓\n✓\n✓\n✓\n\
    ✓\nIn [32], the authors studied the challenges of IoT security at the communication\
    \ layer. A study\nin [33] focused on reviewing IDSs for IoT networks. The work\
    \ in [34] covered a brief discussion of the\nML technique’s relevance in the context\
    \ of IoT security and privacy. Moreover, they identiﬁed limited\nbandwidth, computation\
    \ power and lack of adequate storage as bottlenecks in any implementation of\n\
    ML-based security solutions for IoT networks. There are other studies [35,36],\
    \ which discussed\nthe feasibility of both ML and data mining techniques to detect\
    \ intrusions in IoT networks by\nimplementing these techniques in IDSs either\
    \ through detecting anomalies or classiﬁcation of trafﬁc.\nIn [21], the authors\
    \ highlighted differentials between IDSs running over wired networks and those\n\
    running over wireless infrastructure, especially IoT networks. Due to fundamental\
    \ architectural\nvariations, the application of ML techniques in IoT IDSs needs\
    \ speciﬁc treatment related to the type of\nattacks, underlying protocols (both\
    \ in communications and networks), and application layer.\nAnother study published\
    \ in [22] discussed the implementation of IDS in the context of\nMANETs. The authors\
    \ described that there are three different types of IDS architectures feasible\n\
    in MANETs. First architecture can be a layered architecture organized in multiple\
    \ hierarchical layers.\nSecond architecture can be a ﬂat one for deploying in\
    \ a distributed and cooperative environment.\nWhile the third one can be a hybrid\
    \ of both using mobile agents. Another study [23] discussed various\nIntrusion\
    \ Detection algorithms related to IDS implementation in MANET. According to the\
    \ authors,\nthese IDS algorithms can be categorized in various categories based\
    \ on the underlying principle used\nfor the detection of an attack. These principles\
    \ can either be a rule, statistics, heuristics, signature, state,\nreputation\
    \ score, or route used. These techniques were later classiﬁed further as anomaly\
    \ detection,\nmisuse, signature-based, or hybrid techniques. There were other\
    \ classiﬁcation criteria proposed by the\nauthors [23] like real-time/ofﬂine,\
    \ attack types and effectiveness of detection (scalability, reliability,\ntimeliness,\
    \ etc.).\nAnother survey presented in [30], the authors explained a classiﬁcation\
    \ of IDS for Wireless Sensor\nNetworks (WSN) based on the deployment model of\
    \ the IDS agent. The deployment model can be\neither distributed, central, or\
    \ a hybrid mode, which is suggested as the best-suited model for WSNs.\nA similar\
    \ study [31] carried out a classiﬁcation of WSNs based on IDS using the criteria\
    \ of detection type\nused by the IDS. The classes identiﬁed included anomaly detection,\
    \ misuse detection and detection\nbased on speciﬁcations. Another aspect of cloud-based\
    \ IoT environment was discussed in [16],\nwhere the authors studied and classiﬁed\
    \ various cloud-based IDSs affecting Conﬁdentiality, Integrity,\nand Availability\
    \ (CIA) of cloud computing-based IoT networks. They explained Hypervisor-based\n\
    IDS, Host-based IDS (HIDS), Network-based IDS (NIDS) and Distributed IDS. In [30],\
    \ the authors\npresented a survey on IoT IDS with a focus on an IDS architecture.\
    \ The survey covered existing\nIoT protocols, standards and technologies, IoT\
    \ security threats, detection types and concludes by\nsuggesting proposed IoT\
    \ IDS architecture.\nElectronics 2020, 9, 1177\n5 of 45\nThe authors in [39],\
    \ proposed a novel multi-stage anomaly detection technique based on\nBoruta Fireﬂy\
    \ Aided Partitioning Density-Based Spatial Clustering of Applications with Noise\n\
    (BFA-PDBSCAN). The authors claimed that their proposed technique produced better\
    \ results in\ncomparison to the related techniques of Density-Based Spatial Clustering\
    \ of Applications with Noise\n(DBSCAN) and Hierarchical Density-Based Spatial\
    \ Clustering of Applications with Noise (HDBSCAN).\nIn [40], the authors proposed\
    \ a hybrid data processing model for network anomaly detection that\nutilizes\
    \ Grey Wolf Optimization (GWO) and Convolutional Neural Network (CNN) techniques.\n\
    The authors stated that their model achieved better accuracy and detection rate\
    \ in comparison to\nthe other state-of-the-art IDSs. In [41], an anomaly detection\
    \ method based on a deep autoencoder\nwas used to detect attacks of IoT botnets.\
    \ The method comprises extracting statistical features from\nbehavioral snapshots\
    \ of normal IoT device trafﬁc sequences and training of a DL based autoencoder\n\
    on the extracted features. The reconstruction error for trafﬁc observations is\
    \ then compared with a\nthreshold to classify them as normal or anomalous. The\
    \ authors evaluated the proposed detection\nmethod on the BASHLITE and Mirai botnets\
    \ dataset generated using commercial IoT devices. In a\nrecent survey paper published\
    \ in [37], learning-based NIDSs for IoT systems were discussed in an\noverview\
    \ of ML-based NIDSs for IoT systems.\n3. IoT System Environment\nThe adoption\
    \ of IoT throughout real-world applications, such as home automation, industrial\n\
    automation and city automation, resulted in a plethora of micro computation devices\
    \ and\nenergy-efﬁcient communication technologies, speciﬁcations and protocols.\
    \ IoT systems have been\nwidely employed in applications of military, agriculture,\
    \ power systems, education and commerce.\nDiverse areas of applications resulted\
    \ in the realization of various devices, communication standards\nand protocols.\
    \ The IoT system paradigms illustrate its various applications, where the access\
    \ network\ntechnology is presented in Figure 1 that shows a loose clustering of\
    \ various IoT communication\ntechnologies and protocols to the corresponding network.\n\
    Internet\nShort Range Wireless \nSensor Network\nRFID\nLi-Fi\nBluetooth\nBLE\n\
    ZigBee\nQR Codes\nNFC\nWiFi\nLong Range \nWireless Sensor\nVSAT\nLTE-MTC\nSigFox\n\
    NB-IoT\nLoRaWan\nMedium Range Wireless \nSensor Network\nVSAT\nHART\nWiMax\nDASH7\n\
    LTE-A\nWireless Private Area \nNetwork\nSigFox\nLoRaWan\nWireless Private Area\
    \ \nNetwork\nFiber Coax\nDSL\nEthernet\nFigure 1. IoT system environment—applications\
    \ and related access networks and protocols.\nElectronics 2020, 9, 1177\n6 of\
    \ 45\n3.1. IoT Architecture\nThe IoT architecture consists of physical objects\
    \ integrated into a communication network and\nsupported by computational equipment\
    \ to deliver smart services to users. The IoT system should be\ncapable of connecting\
    \ billions of heterogeneous devices through the Internet, so there is a need for\
    \ a\nlayered and ﬂexible architecture. There are numerous architectures and reference\
    \ models proposed by\nvarious authors and organizations but those have not yet\
    \ converged to a formally recognized reference\nmodel [3–7,42–44]. The most common\
    \ architectures and reference models (the terms “architecture”\nand “reference\
    \ model” used interchangeably by the authors) are explained as follows:\n•\nA\
    \ 3-layer architecture. The most common and basic model is a 3-layer architecture\
    \ comprising of\nthe perception, network and application layers [3,4,43], as depicted\
    \ in Figure 2, the perception\nlayer is also called ‘the device layer’ that includes\
    \ physical devices and sensors. The network\nlayer is also named ‘the transmission\
    \ layer’, which should securely transmit the telemetry data of\nsensors to processing\
    \ and data analytical systems. The application layer offers global management\n\
    of applications using the systems at the network layer.\n•\nInternational Telecommunication\
    \ Union (ITU) recommended Reference Model for IoT. ITU\nrecommends a reference\
    \ model for IoT that comprises four layers, along with security and\nmanagement\
    \ capabilities linked to the layers [45]. The layers are as follows: device layer,\
    \ network\nlayer, application support layer, service support and application layer,\
    \ as shown in Figure 3.\n•\nIoT-A Architectural Reference Model proposed by the\
    \ European Commission (FP7).\nThe European Commission within the Seventh Framework\
    \ Program (FP7) supported the project\nIoT-A proposed by Martin Bauer et al. [6].\
    \ The IoT-A model attempts to design an architecture\nthat could meet the requirements\
    \ of the industry and researchers. It offers high-level architectural\nperspectives\
    \ and views for building IoT systems. The architecture comprehensively describes\n\
    the structuring and modeling of IoT business process management, IoT services,\
    \ cross-service\norganization and virtual entities, information and functional\
    \ viewpoints, in an abstract way [46].\nAmongst these various views, a functional\
    \ view of IoT architecture is depicted in Figure 4.\n•\nAn IoT Reference Architecture\
    \ developed by Web Service Oxygen (WSO2).\nWSO2,\nan open-source technology provider,\
    \ has proposed an Architectural Reference Model based\non its skills in the IoT\
    \ solutions development.\nFigure 5 depicts the WSO2 recommended\narchitecture.\n\
    It consists of ﬁve layers:\n(1) Client/external communications—Web/Portal,\nDashboard,\
    \ Application Programming Interface (APIs), (2) Event processing and analytics\n\
    (including data storage), (3) Aggregation/bus layer—Enterprise Service Bus (ESB)\
    \ and message\nbroker, (4) Relevant transports—XMPP/CoAP/AMQP/HTTP/MQTT, etc.\
    \ and (5) Devices [47].\nThe model includes the cross-cutting layers that have\
    \ (1) a device manager, and (2) an identity\nand access management system.\n•\n\
    An IoT Reference Architecture suggested by Cisco: Cisco introduced a seven-layered\
    \ IoT\nreference model [48]. The model and its levels are illustrated in Figure\
    \ 6. The authors described\nthat control information ﬂows from level 7 to level\
    \ 1 in a control pattern. The ﬂow of information\nis the reverse in a monitoring\
    \ pattern and it is bidirectional in most systems.\nElectronics 2020, 9, 1177\n\
    7 of 45\nFigure 2. Illustration of basic IoT architecture.\nApplication Layer\
    \ \nIoT \nApplication\nService\nSupport and\nApplication\nSupport Layer\nGeneric\
    \ \nSupport \nCapabilities\nNetwork Layer\nNetworking \nCapabilities\nDevice Layer\n\
    Device \nCapabilities\nSpecific \nSupport \nCapabilities\nTransport \nCapabilities\n\
    Gateway \nCapabilities\nSecurity \nCapabilities\nGeneric Security Capabilities\n\
    Specific Security Capabilities\nManagement \nCapabilities\nGeneric Management\
    \ Capabilities\nSpecific Management Capabilities\nFigure 3. ITU-T reference model\
    \ [45].\nApplication\nDevice\nService \nOrganization\nIoT Business \nProcess \n\
    Management\nVirtual \nEntity\nIoT \nService\nCommunicatioin\nManagement\nSecurity\n\
    Figure 4. IoT-A functional view [6,46].\nElectronics 2020, 9, 1177\n8 of 45\n\
    Web/ Portal\nAPI Managment\nDashboard\nEvent Processing and Analysis\nAggression/\
    \ Bus Layer\nESB and Message Broker\nDevice Manager\nIdentity and Access Management\n\
    Communications MQTT/ HTTP\nFigure 5. Web Service Oxygen (WSO2) IoT reference architecture\
    \ [47].\n1\nEdge\nSensors, Devices, Machines\nCenter\n2\n3\n4\n6\n5\n7\nPhysical\
    \ Devices & \nControllers (The  Things  as \nIoT)\nConnectivity\n(Communication\
    \ & \nProcessing Units)\nEdge (Fog) Computing\n(Data Element analysis & \nTransformation)\n\
    Data Accumulation\n(Storage)\nData Abstraction\n(Aggregation & Access)\nApplication\n\
    (Reporting Analysis Control)\nCollaboration & Processes\n(Involving People & Business\
    \ \nProcesses)\nFigure 6. Cisco IoT reference model [48].\n3.2. IoT Protocols\n\
    Several protocols and speciﬁcations inherited from the TCP/IP model, some technologies\
    \ are\nspeciﬁcally developed for IoT systems. IEEE 802.15.4 (transmission and\
    \ communication speciﬁcation\nstandards) is not alone in the paradigm of IoT speciﬁc\
    \ technologies and standards. In Table 2,\na description of the IoT technologies\
    \ with respective frequency bands and supported data rates\nand area coverage.\n\
    Electronics 2020, 9, 1177\n9 of 45\nTable 2. IoT enabling technologies, their\
    \ frequency bands and data rates.\nTechnology\nFrequency Bands\nData Rate\nPhysical\
    \ Coverage\nWiFi 802.11\n2.5 GHz, 5 GHz\n<1 Gbps\nup to 50 m\nWiFi HaLow\n900\
    \ MHz\n0.3–234 Mbps\nup to 1 km\nWhite-Fi\n54–790 MHz\n26.7–568.9 Mbps\nup to\
    \ 100 m\nBluetooth\n2.4 GHz\n100 Kbps\nup to 100 m\nBluetooth LE\n2.4 GHz\n<1\
    \ Gbps\nup to 50 m\nZ-Wave\n686 MHz, 908 MHz, 2.4 GHz\n40 K\nup to 100 m\nZigBee\n\
    868 MHz, 915 MHz, 2.4 GHz\n20 kbps to 250 kbps\n10–75 m\nISA100.11a\n868 MHz,\
    \ 915 MHz, 2.4 GHz\n20 kbps to 250 kbps\nup to 600 m\nMiWi\n868 MHz, 915 MHz,\
    \ 2.4 GHz\n20 kbps to 250 kbps\n20–50 m\nThread\n868 MHz, 915 MHz, 2.4 GHz\n20\
    \ kbps to 250 kbps\nup to 100 m\nWirelessHART\n868 MHz, 915 MHz, 2.4 GHz\n20 kbps\
    \ to 250 kbps\n30–100 m\nLTE-A\nCellular bands\n1 G (up), 500 M (down)\nup to\
    \ 50 km\nGSM\nCellular bands\n150 Mbps\nup to 50 km\nLTE-Cat M\nCellular bands\n\
    up to 1 Mbps\n15 km\nNB-IoT\nCellular bands\n<180 kbps\n15 km\nLoRaWAN\n169/433/868/780/915\
    \ MHz ISM\n300 bit to 100 kbit/s\n2.5–15 km\nNFC\n13.56 MHz\nup to 424 kbps\n\
    <20 cm\nDASH7\n433/868/915 MHz ISM/SRD\n9.6–166.667 kbit/s\nup to 5 km\nnWave\n\
    Sub-1 GHz ISM\n100 bit/s\n10–30 km\nSigFox\n868/902 MHz ISM\n100 bit/s\n12–30\
    \ km\n4. IoT-Based Threats and Attacks\nIoT systems suffer from various security\
    \ risks as compared to conventional computing systems\ndue to several reasons\
    \ [15,47]. First, IoT systems are highly diverse with regards to devices, platforms,\n\
    communication means and protocols. Second, IoT systems comprise “things” not planned\
    \ to be\nconnected to the Internet, where control devices are used to link physical\
    \ systems. Third, there are\nno well-deﬁned boundaries in IoT systems, which regularly\
    \ change due to the mobility of users and\ndevices. Forth, IoT systems, or part\
    \ of them, would be physically insecure. Last but not least, due to\nthe limited\
    \ energy of IoT devices, it is usually very hard to deploy advanced security techniques\
    \ and\ntools on IoT devices.\nAn IoT network often contains hundreds of nodes\
    \ with assigned functions ranging from sensing of\nlight, temperature and noise\
    \ to associated control systems to regulate lighting and heating, ventilation,\n\
    and air conditioning (HVAC) systems, etc. All these sensors and control systems\
    \ communicate through\ndifferent network protocols like Bluetooth, WiFi, ZigBee,\
    \ etc. An IoT gateway is used to connect these\ndevices to the Internet. Being\
    \ composed of layers of standards, services and technologies, the IoT\nenvironment\
    \ has privacy and security concerns at each of these layers. While it seems that\
    \ the IoT\nenvironment has similar security concerns to the Internet, cloud and\
    \ mobile communication networks,\nthere are distinct characteristics that set\
    \ IoT environments, along with the applications of contemporary\nsecurity controls\
    \ [10]. These can share data, computing capacity limitation and a large number\
    \ of\nnetworked IoT devices.\nOne instance of the susceptibility of IoT devices\
    \ to attacks was demonstrated in September\n2016, where an IoT botnet built from\
    \ the Mirai malware—possibly the largest botnet on record—was\nresponsible for\
    \ a 620 Gbps attack directed towards Brian Krebs’s security blog [11]. Mirai followed\n\
    a simple strategy, where it tried a list of 62 common user credentials to get\
    \ access to digital video\nrecorders, home routers and network-enabled cameras,\
    \ which generally had fewer defenses than other\nElectronics 2020, 9, 1177\n10\
    \ of 45\nIoT devices. Later, in the same month, the French webhost OVH (On Vous\
    \ Héberge) was attacked by\nthe Mirai-based attack, which broke the record for\
    \ the largest recorded distributed denial of service\n(DDoS) attack peaking at\
    \ 1.1 Tbps [12]. The attack was made possible due to default and weak security\n\
    conﬁgurations. Similarly, in [49], the authors described the relative ease of\
    \ compromising various IoT\ndevices, due to ﬂaws in protocol implementations.\n\
    The rapid proliferation of IoT based devices is likely to make such networks susceptible\
    \ to\nattacks against privacy and security aspects. In [13], the authors identiﬁed\
    \ various security issues\nin IoT networks built with commercially available IoT\
    \ devices like sensors. One example cites a\nsmart watering system that is capable\
    \ of measuring environmental variables like temperature and\nhumidity, etc. An\
    \ actuator module was employed for functionality implementation with a web-based\n\
    user interface. The system was built on an Arduino Uno. The authors described\
    \ the exposure\nof such network to spooﬁng attacks through a software-enabled\
    \ access point (SoftAP), where an\nattacker managed all IoT devices in a network\
    \ to shut down for a while as the SoftAP broadcasts\nde-authentication packets.\n\
    Due to the limited processing capabilities of IoT devices, the hacker made all\
    \ IoT devices\nvulnerable in the network to connect to the SoftAP as it appeared\
    \ to have a stronger signal than\nthe actual access point (AP) with the same service\
    \ set identiﬁer (SSID). This allowed the compromise\nof all network communications\
    \ to eavesdropping and man in the middle (MiTM) attacks. Such attack\nscenarios\
    \ built a case for the deployment of IDSs in IoT networks to discover vulnerabilities\
    \ of IoT\ndevices. The idea of IoT revolves around the intelligent integration\
    \ of a real physical environment\nwith the Internet to enable interactivity. For\
    \ this reason, IoT environments have interconnections\nand dependencies with multiple\
    \ heterogeneous environments. This exposes each IoT system to\ncyber threats from\
    \ each connected environment [50,51]. IoT environments face threats from multiple\n\
    dimensions both from physical and virtual domains. Figure 7 illustrates multiple\
    \ threat dimensions of\nan IoT environment that would be exploited.\nIoT\nEnvironment\n\
    Each IoT is \nvulnerable to threats \nfrom multiple \ndimensions \nUser Interface/\
    \ \nApplication\nCloud\nServices\nNetwork \nService\nSensors\nOther IoT \nSystems\n\
     Attack Against Privacy\n CSRF\n XSS\n SQL Injection\n Attack Against Data\
    \ \nIntegrity\n Attack Against \nVirtualization Tech\n DoS/ DDoS\n Bluesnarfing/\
    \ \nBluejacking\n Eavesdroppin\n Malware\n Attack Against \nWireless Tech\n\
     Attack Against \nNetwork Service\n Internet Attack\n Routing Attacks\n Confidentiality\
    \ \nThreats\n  Unauthorized \nAccess\n DoS attack\n Privacy\n Location Tracking\n\
     Physical Attack\n Attack Against \nFirmware\n Device Cloning/ \nnode Capture\n\
     Theft of security \nkeys\nFigure 7. IoT environment threat dimensions.\nElectronics\
    \ 2020, 9, 1177\n11 of 45\nThough IoT Security threats can be broadly divided\
    \ into cyber and physical domains, our survey\nis mainly concerned with cyber\
    \ threats, which can take the form of either active or passive attacks.\nPassive\
    \ Attacks are characterized by a lack of any alteration to information or its\
    \ ﬂow, thereby only\ncompromising the conﬁdentiality and privacy of communications.\
    \ In some cases, a passive attack\ncan enable location tracking of IoT devices\
    \ [52–54]. Active Attacks involve active alteration and\nmodiﬁcation of information\
    \ and its ﬂow, but are not limited to device settings, control messages and\n\
    software components.\nOne active attack is when the IoT system is used as a vector\
    \ to launch massive DDoS against\nInternet systems. IoT systems are a suitable\
    \ vector for these attacks because of their large numbers and\ncomparative ease\
    \ of their compromise, due to poor security practices and weak defense mechanisms.\n\
    Mirai can be used as an example of a botnet attack through for compromising IoT\
    \ systems [11,55,56].\nIoT systems face many threat dimensions from multiple directions,\
    \ including user interface,\ncloud services, other interconnected IoT systems\
    \ associated to sensors and network services [12],\nas shown in Figure 7. A discussion\
    \ of these dimensions is presented in the following subsections.\n4.1. User Interface\n\
    Most use cases of IoT systems involve the provision of services to users by IoT\
    \ systems through\nsome sort of a user interface (mobile, desktop or web application).\
    \ The case of smart home appliances\ncan be controlled by users through mobile\
    \ applications. The rapid proliferation of smartphones has\nprovided malicious\
    \ actors to disguise malicious applications and malware as benign utility mobile\n\
    applications and publish them through applications to store without being detected\
    \ [57,58]. Also,\nsmartphones can sometimes be hacked through platform vulnerabilities\
    \ of these devices like Android\nvulnerabilities. This leads to exposing all information\
    \ stored on the phone with the possibility of\nmalware compromise. Eavesdropping,\
    \ location tracking, Denial of Service (DoS)/DDoS, bluejacking\nand bluesnarﬁng\
    \ are attacks enabled through user interface platforms [59–61].\n4.2. Cloud Services\n\
    Though Cloud services and IoT systems lie at two ends of the resource availability\
    \ spectrum,\nthe two can complement each other to produce an excellent blend of\
    \ technologies. Cloud services are\ncharacterized by ubiquitous access to computing\
    \ power and storage, etc., which can offset the resource\nlimitations of IoT systems\
    \ [62]. The potential of IoT systems can be maximized through integrated use\n\
    with cloud services to conserve energy and provide all types of services without\
    \ being constrained\nby storage and processing power limitations [63]. Likewise,\
    \ cloud services can beneﬁt from large\ndeployments of IoT systems through integrated\
    \ applications [64]. Such a distributed architecture\nopens up vulnerable points\
    \ for many attacks at multiple layers, as explained below.\n•\nAuthorization Attacks.\
    \ Through the exploitation of vulnerabilities in data security mechanisms,\nan\
    \ attacker may be able to gain unauthorized access to information on both cloud\
    \ and IoT systems.\n•\nIntegrity Attacks. Such attacks enable an attacker to compromise\
    \ the integrity of data through\nspooﬁng and bypass the authorization controls\
    \ to gain direct access to databases.\n•\nCompromise of Visualization platform.\
    \ A vulnerability in the virtualization platform can be\nexploited by an attacker\
    \ to bypass security and isolation controls between the host and the guest\noperating\
    \ system (OS), resulting in privilege escalation and pivoting attacks [65].\n\
    •\nConﬁdentiality Attacks. IoT systems, like wearable devices, are used to monitor\
    \ health-related\ndata of highly conﬁdential nature. Similarly, smart home devices\
    \ capture sensitive private data\nof the users. Privacy and conﬁdentiality concerns\
    \ overshadow the advantages of cloud services.\nMoreover, multi-tenancy and geographical\
    \ location of cloud services pose a serious threat to the\nconﬁdentiality of data\
    \ through privilege escalation and hacking [66].\nElectronics 2020, 9, 1177\n\
    12 of 45\n4.3. Connections of Multiple IoT Systems\nVarious IoT systems are designed\
    \ to work autonomously and interact with other IoT systems,\nsuch as sensors and\
    \ actuators of smart cars and smart homes, without requiring human involvement.\n\
    Such an interaction is aimed at achieving an autonomous and collaborative functionality.\
    \ Smart cars\nand smart homes can communicate with each other and provide interdependent\
    \ services and functions.\nFor instance, [67] described such a scenario where\
    \ sensing increased temperature by a temperature\nsensor, coupled with sensing\
    \ of unplugging of a smart plug, the windows of the room are automatically\nopened.\
    \ The window opening actuator would be reachable for an attack as it may manipulate\
    \ the\ntemperature sensing device through its interface and in turn that compromises\
    \ the actuator [67].\nThis example highlights the fact that the weakest part of\
    \ interdependent IoT systems can compromise\nother parts as well.\nA large number\
    \ of interconnected devices in IoT systems increases the vulnerability and also\
    \ the\nimpact of any attack, where one compromised device can lead to the compromise\
    \ of billions of devices.\nSuch a scenario can impact any externally connected\
    \ networks and systems also. One study [68]\ndemonstrated that an experimental\
    \ malware attack against Philips Hue smart lamp was so successful\nthat it compromised\
    \ all such lamps in the network, despite the presence of reliable cryptographic\n\
    authentication mechanisms against malicious ﬁrmware updates. Similar attacks could\
    \ provide the\ncontrol of lights of an entire city or their use in DDoS against\
    \ outside targets [68].\nVarious types of sensors are an essential part of IoT\
    \ systems like GPS, Radio-Frequency\nIdentiﬁcation (RFID), temperature gauge and\
    \ IP cameras. This also includes sensors and actuators\nembedded in autonomous\
    \ vehicles and the internet of vehicles (IOVs). These physical devices are\nvulnerable\
    \ to physical attacks and manipulation by malicious actors. Another component\
    \ of IoT\nsystems susceptible to such physical attacks is the actuator part, which\
    \ performs some function based\non readings of sensor devices. Both actuators\
    \ and sensors would be subjected to DoS attacks through\nﬂooding, eavesdropping,\
    \ location tracking, cloning and spooﬁng attacks [69–71].\nAn IoT system consists\
    \ of several interconnected devices using either wireless or wired networks.\n\
    A large network linked to devices would have weak security proﬁles, where sensors\
    \ and actuators\nare vulnerable to a multitude of attacks. WSNs provide information\
    \ to external entities without any\nrestriction. When they are integrated with\
    \ conventional networks services, they cause regression in\nthe security of conventional\
    \ networks [72,73].\n4.4. Protocols Level Attacks\nIoT systems are different from\
    \ traditional Internet protocols, which require lightweight protocols\nto address\
    \ issues of limited energy, data rate and computing power. A detailed description\
    \ of IoT\nprotocols based attacks can be found in [74]. Attacks of IoT technologies\
    \ are presented with threat\ntypes in Table 3.\nTable 3. Summary of attacks against\
    \ main IoT technologies (C: Confidentiality, I: Integrity, A: Availability).\n\
    Technology/Protocol\nAttack\nThreat\nThreat\nCategory\nTag Disable\nJamming\n\
    A\nTag Modiﬁcation\nUnauthorized Access and modiﬁcation of critical\ninformation\n\
    C, I\nRFID\nCloning Tags\nCounterfeiting and spooﬁng\nC, A\nReverse Engineering\n\
    Counterfeiting and spooﬁng\nC, A\nEavesdropping\nUnauthorized Access of critical\
    \ information\nC\nSnooping\nUnauthorized Access of identity and data\nC\nSkimming\n\
    Imitates the original RFID tag\nC, A\nElectronics 2020, 9, 1177\n13 of 45\nTable\
    \ 3. Cont.\nTechnology/Protocol\nAttack\nThreat\nThreat\nCategory\nReplay Attack\n\
    Deceiving readers\nC, A\nRelay Attacks\nman-in-the-middle\nI, A\nEM Interference\n\
    Jamming\nA\nFake RFID tag queries\nillicit Tracing and Tracking\nC, A\nCryptograph\
    \ Decipher attack\nPassword Decoding\nC, I\nblocker tag Attack\nDoS Attack\nA\n\
    ZigBee\nSnifﬁng\nsnifﬁng the keys\nC\nReplay attack\nMiTM\nC, I\nKillerbee Packer\n\
    Manipulation Attack Device Spooﬁng\nC\nKillerbee - zbassocﬂood\nCrash the device\n\
    A\nEavesdropping\nMiTM\nC, I\nZED Sabotage Attack\nDoS\nA\nWiFi\nFMS/KoreK/PTW/ARP\
    \ Injection/Dictionary\nAttack\nKey Retrieving Attacks\nC\nChopChop/ Fragmentation/Caffe\
    \ Latte/Hirte\nKeystream Retrieving Attacks\nC, I\nAuthentication related Attacks\n\
    DoS\nA\nAssociation related Attack\nDoS\nA\nFlooding related attacks\nDoS\nA\n\
    Honeypot\nMiTM\nC, I\nEvil Twin/Rogue AP\nMiTM\nC, I\nBluetooth\nBluebugging\n\
    Espionage\nC, I\nBluesnarﬁng\nEspionage and DoS\nC, A\nSnifﬁng Attacks\nInterception\n\
    C\nHijacking\nDoS and spooﬁng\nA, C, Identity\nFuzzing\nDoS\nA\nSpooﬁng\nSpooﬁng\n\
    Identity\nNFC\nInterception\nEavesdropping\nC\nData corruption through Interception\n\
    DoS\nI, A\nData Modiﬁcation through interception\nMiTM\nI\nData Insertion\nMiTM\n\
    I\nNFC Data Exchange Format (NDEF) attacks\nIdentity Theft and Non repudiation\n\
    C, Identity\n4.5. Radio-Frequency Identiﬁcation (RFID)\nBecause the communication\
    \ between the reader and RFID tags is made through an unprotected\nwireless channel,\
    \ the transmitted data is exposed by unauthorized readers. RFID systems face different\n\
    security threats as compared to the security threats encountered by traditional\
    \ wireless systems [75].\nVarious hacking techniques against RFID are discussed\
    \ as follows:\n•\nTag Disable. An attacker may remove the tag, delete the tag\
    \ memory by sending a kill command,\nremove the antenna, give a high energy wave\
    \ to a tag, and use a Faraday cage to block\nelectromagnetic waves.\n•\nTag Modiﬁcation.\
    \ An attacker modiﬁes or deletes valuable data from the memory of the tag.\n•\n\
    Cloning Tags. An attacker imitates or clones the tags after skimming the tag’s\
    \ information.\n•\nReverse Engineering. Using reverse engineering, an attacker\
    \ can make a copy of a tag, and using\ntag examination, the attacker may get conﬁdential\
    \ data stored within a tag.\n•\nEavesdropping. RFID systems working in ultra high\
    \ frequency (UHF) are more vulnerable to\nthis threat. An attacker gathers the\
    \ information shared between a valid tag and valid reader.\n•\nSnooping. An attacker\
    \ introduces an unauthorized reader to interact with the tag.\n•\nSkimming. An\
    \ attacker snoops data shared between a legitimate reader and legitimate tag.\n\
    •\nReplay Attack. An attacker spies to collect information about the IoT device\
    \ or node replays\neavesdropped information to achieve deception.\n•\nRelay Attacks.\
    \ An attacker places an illegitimate device between the tag and the reader to\n\
    intercept, modify and forward information directly to other systems.\nElectronics\
    \ 2020, 9, 1177\n14 of 45\n•\nElectromagnetic (EM) Interference. An attacker creates\
    \ a signal in the same range as the reader\nto preclude tags from communicating\
    \ with readers.\n•\nFake RFID Tag Query. An attacker sends queries and gets the\
    \ same response from a tag at various\nlocations to determine the location of\
    \ a speciﬁc tag.\n•\nCryptograph Decipher Attack. An attacker decodes encryption\
    \ algorithms by launching violent\nattacks and gets the plain text by deciphering\
    \ the intercepted cryptography.\n•\nBlocker tag Attack.\nUsing a blocker tag,\
    \ an attacker attempts to restrict the reader from\nreading tags.\n4.6. Zigbee\
    \ Protocol\nThe Zigbee protocol is one of the most popular IoT protocols used\
    \ for communication in IoT\ndevices because of its low cost, low power consumption\
    \ and scalability. While the importance of\nsecurity was considered during the\
    \ design of Zigbee, some trade-offs have been kept to bring the cost\nof devices\
    \ down and make them scalable at a low cost. Some of the standard security measures\
    \ could\nnot be implemented which ultimately resulted in security vulnerabilities.\
    \ The major security threats\nagainst Zigbee networks are enumerated below.\n\
    •\nSnifﬁng. Zigbee networks are exposed to snifﬁng attacks since they do not implement\
    \ encryption\ntechniques. The attacker can capture some packets to execute malicious\
    \ activities using some\nsoftware tools like KillerBess’s zbdump tool [76].\n\
    •\nReplay Attack. If an attacker is able to intercept the packets, the attacker\
    \ can sniff raw packets of\na network and could re-send the captured data as normal\
    \ trafﬁc [76].\n•\nAttaining the Link or Network key. Since keys need to be reinstalled\
    \ on the air when its objects\nrequire reﬂashing, an attacker can obtain the ZigBee\
    \ network or link keys. Also, physical attacks\ncan be used to obtain the key,\
    \ where the keys can be extracted from ZigBee devices’ ﬂash memory\nwhen the device\
    \ is physically accessed [77,78].\n•\nEavesdropping. An attacker can eavesdrop\
    \ a ZigBee network and redirect its packets using an\nMiTM attack.\n•\nZED Sabotage\
    \ Attack. Authors in [79] proposed an attack against the ZigBee protocol called\n\
    the ZigBee End-Device (ZED). The purpose of the attack is to make the ZED unavailable\
    \ by\ntransmitting a particular signal periodically to wake up the device to drain\
    \ its battery.\n4.7. Wireless Fidelity (WiFi)\nA detailed review of attacks against\
    \ various versions of the 802.11 security mechanism (i.e., WPA,\nWPA2, WEP) is\
    \ explained in [80]. The most common WiFi attacks are described below.\n•\nAttacks\
    \ Related to Retrieving Key. An attacker would monitor speciﬁc packets and then\
    \ crack\nthe key process ofﬂine. The common attacks in this category are Pyshkin,\
    \ Tews, and Weinmann\n(PTW) attacks, Fluhrer, Mantin, and Shamir (FMS) attack,\
    \ KoreK Family Attacks, Dictionary\nAttack and address resolution protocol (ARP)\
    \ Injection [80].\n•\nAttacks Related to Retrieving Keystream. An attacker only\
    \ required to monitor for speciﬁc\npackets and then go on to perform the key cracking\
    \ process ofﬂine. The common attacks in\nthis category are PTW attacks, FMS attack,\
    \ KoreK Family Attacks, Dictionary Attack and ARP\nInjection [80].\n•\nDoS or\
    \ Availability Attacks. This category of attacks includes those attacks that result\
    \ in the\nunavailability of some service or network that is commonly called a\
    \ DoS attack. These attacks\nusually target either a speciﬁc user or device, or\
    \ try to exhaust network resources (e.g., the network\nrouter or Access Point),\
    \ resulting in corrupting services for all users in that network. These attacks\n\
    mostly depend on the broadcast of forged 802.11 management messages, which are\
    \ easy to launch\nin versions of the WiFi standards up to 802.11n, as the management\
    \ messages are transmitted\nElectronics 2020, 9, 1177\n15 of 45\nunguarded [81].\
    \ Attacks in this category include: Disassociation Attack, Block ACK ﬂood,\nAuthentication\
    \ Request Flooding Attack, Deauthentication Broadcast Attack, Fake Power Saving\n\
    Attack, Beacon Flooding Attack, Probe Request and Response Flooding Attacks. A\
    \ survey of DoS\nattacks in 802.11 is covered in [82].\n4.8. Bluetooth\nMost of\
    \ the issues found in Bluetooth are related to the pairing process. Attacks can\
    \ be launched\nduring the pairing process stages, like before the completion of\
    \ the pairing process and after the pairing\nof devices is completed [83]. For\
    \ instance, based on information collected after pairing, attackers\ncan launch\
    \ man-in-the-middle attacks. A review of Bluetooth security issues is explained\
    \ in [83–85].\nThe common attacks against Bluetooth are discussed below.\n•\n\
    PIN Cracking Attack. This type of attack is performed during the pairing of the\
    \ device and the\nprocess of authentication. An attacker collects the random number\
    \ (RAND) and the Bluetooth\nDevice Address (BD_ADDR) of the targeted device using\
    \ some frequency sniffer tool. Then,\na brute-force algorithm (for example, E22\
    \ algorithm) is applied to check all possible combinations\nof the PIN with the\
    \ data collected earlier until the correct PIN is determined [84].\n•\nMAC Spooﬁng\
    \ Attack. An attack is launched during the process of link keys generation and\n\
    before encryption is established. Devices manage to authenticate each other using\
    \ generated\nlink-keys. In this, attackers can imitate another user. Attackers\
    \ can also dismiss connections or\neven alter data [84].\n•\nMan-in-the-Middle\
    \ (MIM) Attack.\nMIM attacks are launched when devices are trying to\npair [86].\
    \ After the attack is launched, devices share messages unknowingly [58]. During\
    \ this time\nauthentication is performed without the shared secret keys [58].\
    \ When the attack is successful,\nthe two devices are paired to the attacker [57,58],\
    \ while they believe the pairing was successful.\n•\nBluebugging. An attacker\
    \ exploits vulnerabilities of old devices ﬁrmware to spy on phone calls,\nsend\
    \ and receive messages, and connect to the Internet without legal users’ knowledge.\n\
    •\nBluesnarﬁng. An attacker gets unauthorized access to devices to retrieve information\
    \ and redirect\nthe incoming calls.\n•\nBluePrinting Attack.\nThis attack is launched\
    \ to capture the device model, manufacturer,\nand ﬁrmware version of the device.\
    \ This attack will work only if the target device’s BD_ADDR\nis known.\n•\nFuzzing\
    \ Attack. In a fuzzing attack, a device is forced to behave abnormally by an attacker\n\
    through sending malformed data packets to Bluetooth radio of the device.\n•\n\
    Brute-Force BD_ADDR Attack. Since the ﬁrst three bytes of BD_ADDR are ﬁxed and\
    \ known\npublicly, the brute-force attack is launched to scan on the last three\
    \ bytes [84].\n•\nWorm Attacks. In this attack, an attacker sends a malicious\
    \ software or Trojan ﬁle to available\nvulnerable Bluetooth devices. Examples\
    \ of these attacks are Sculls’ worm, Cabir worm and\nLasco worm.\n•\nDoS attacks.\n\
    These attacks target the physical layer or above layers in the protocol stack.\n\
    Some typical DoS attacks are battery exhaustion, BlueChop, BD_ADDR duplication,\
    \ BlueSmack,\nBig NAK (Negative Acknowledgement) and L2CAP guaranteed service.\n\
    4.9. Near Field Communication (NFC)\nAlthough the communication range of NFC is\
    \ restricted to a few centimeters, the International\nOrganization for Standardization\
    \ (ISO) standard does not guarantee secure communication.\nThe common attacks\
    \ against NFC technologies are brieﬂy mentioned below [87].\n•\nEavesdropping.\n\
    By using powerful and bigger antennas than those of mobile devices,\nNFC communications\
    \ can be received or intercepted by an attacker in the vicinity of the devices.\n\
    This allows an attacker to eavesdrop an NFC communication across larger distances.\n\
    Electronics 2020, 9, 1177\n16 of 45\n•\nData Corruption. An attacker can modify\
    \ data transmitted over an NFC interface. If the attacker\nalters the data into\
    \ an unrecognized format, this may result in DoS attacks.\n•\nData Modiﬁcation.\
    \ An attacker alters the actual data using amplitude modulations of\ndata transmissions.\n\
    •\nData Insertion. Malicious and undesirable data can be inserted in the form\
    \ of messages into the\ndata during the data exchange between two devices.\n•\n\
    NFC Data Exchange Format (NDEF) attacks. An attacker would exploit partial signatures,\n\
    record composition attacks and establish trust [88].\n4.10. IEEE 802.15.4\nIEEE\
    \ 802.15.4 is a technical standard, used by several IoT protocols, which describes\
    \ the operation\nof low-rate wireless personal area networks (LR-WPANs). It stipulates\
    \ the PHY layer and MAC for\nLR-WPANs. The IoT protocols based on IEEE 802.15.4\
    \ include 6LowPAN, ZigBee, Wireless HART,\nISA 100.11a, MiWi, Thread and SubNetwork\
    \ Access Protocol (SNAP). These protocols extended the\nstandard by developing\
    \ the upper layers, which are not covered in IEEE 802.15.4. The common attack\n\
    types related to the IEEE 802.15.4 standard are explained in [89–91].\n•\nRadio\
    \ interference Attack. An attacker transmits high transmission powered radio interference\n\
    signals over all channels of the related frequency band.\n•\nSymbol Flipping/\
    \ Signal Overshadowing Attack. An attacker injects wrong data into a network\n\
    by converting a legitimate data frame into an altered frame comprising information\
    \ of the\nattacker’s choice.\n•\nSteganography Attack. Adversaries would use a\
    \ hidden channel to exchange information about\nthe launching of new attacks in\
    \ the network.\n•\nNode-Speciﬁc Flooding. In this, the emission of packets is\
    \ used to cause degradation throughput\nIoT networks by ﬂooding massive fake data.\n\
    •\nBack-Off Manipulation. An attacker transmits unnecessary packets to the victim\
    \ and due to\nexcessive packet reception, the targeted nodes’ power sources are\
    \ ultimately exhausted.\n•\nBattery Life Extension (BLE) Pretense. An attacker\
    \ transmits unnecessary packets to the victim\nand due to excessive packet reception,\
    \ the targeted nodes’ power sources are ultimately exhausted.\n•\nRandom Number\
    \ Generator (RNG) Tampering. An attacker uses RNG in a way that guarantees\nthat\
    \ the back-off periods chosen by the adversary are much smaller than those selected\
    \ by\nlegitimate nodes.\n•\nBack-Off Countdown Omission. This type of attack implicates\
    \ the complete exclusion of the\nrandom back-off countdown by a malicious attacker.\n\
    •\nClear Channel Assessment (CCA) Manipulation/ Reduction/Omission.\nAn attacker\
    \ gains\nchannel access more frequently and quickly than it is done by legitimate\
    \ network nodes.\n•\nSame-Nonce Attack. An attacker obtains ciphertext keys to\
    \ gather valuable information about\ntransmitted data.\n•\nReplay-Protection Attack.\
    \ In this type of attack, frames with large sequence numbers are sent by\nattackers\
    \ to targeted legitimate nodes. This results in dropping data frames with smaller\
    \ sequence\nnumbers from other legitimate nodes.\n•\nAcknowledgment (ACK) Attack.\
    \ An attacker sends back a false ACK on behalf of the receiver\nwith the correct\
    \ expected sequence number to the sender. This prohibits data retransmission\n\
    by misleading the sender into believing that the frame has been delivered to the\
    \ receiver\nsuccessfully [89].\n•\nGuaranteed Time Slot (GTS) Attack. GTS attacks\
    \ are initiated against the network by exploiting\nthe GTS management scheme.\n\
    •\nPersonal Area Networks Identiﬁer (PANId) Conﬂict Attack. An attacker can abuse\
    \ the conﬂict\nresolution procedure by sending fake PANId conﬂict notiﬁcations\
    \ to the targeted PAN coordinator\nElectronics 2020, 9, 1177\n17 of 45\nto start\
    \ conﬂict resolution, thus temporarily preventing or delaying communications between\
    \ the\nPAN coordinator and member nodes.\n•\nPing-Pong Effect Attack. This attack\
    \ causes packet loss and service interruption, dropping node\nperformance, and\
    \ increasing consumption of energy and network load.\n•\nBootstrapping Attack.\
    \ An attacker forces a targeted network node to become unrelated with its\nPAN\
    \ at a time of the attacker’s choosing by initiating any of the MAC or PHY layer\
    \ attacks with\nthe ultimate aim of causing DoS.\n•\nSteganography Attack. An\
    \ attacker hides information within the MAC and PHY frame ﬁelds\nof the IEEE 802.15.4\
    \ protocol [89]. Data can be hidden in IEEE 802.15.4 networks by using the\nPHY\
    \ header ﬁeld of PHY frames. Similarly, Steganography attacks would also be launched\
    \ by\nhiding information within the MAC ﬁelds. Steganography attacks form a hidden\
    \ channel between\ncooperating attackers in the network, which opens up a large\
    \ number of prospects for adversaries.\n4.11. Routing Protocol for Low Power and\
    \ Lossy Network (RPL) Attack\nThe RPL protocol has been designed to allow point\
    \ to point, multiple-point to point, and point\nto multiple-point communication.\
    \ It is a distance-vector routing protocol based on IPv6. The RPL\ndevices work\
    \ on a speciﬁc topology that joins tree and mesh topologies called Destination\
    \ Oriented\nDirected Acyclic Graphs (DODAG) [74,92]. Attacks against routing protocol\
    \ can cause communication\nfailures within IoT systems [93]. The interconnection\
    \ of IoT systems to the Internet multiplies the\nvulnerabilities exponentially\
    \ through exposure to innumerable attack vectors. The main attacks against\nRPL\
    \ are discussed as follows:\n•\nSinkhole Attack. An attacker may announce a favorable\
    \ route or falsiﬁed path to entice many\nnodes to redirect their packets through\
    \ it.\n•\nSybil Attack. An attacker may use different identities in the same network\
    \ to overcome the\nredundancy techniques in scattered data storage. Also, this\
    \ can be used to attack routing algorithms.\n•\nWormhole Attack. An attacker disturbs\
    \ both trafﬁc and network topology. This attack can be\nlaunched by generating\
    \ a private channel between two attackers in the network and transmitting\nthe\
    \ selected packets through it.\n•\nBlackhole Attack. An attacker maliciously advertises\
    \ itself as the shortest path to the destination\nduring the path-discovering\
    \ mechanism and drops the data packets silently.\n•\nSelective Forward Attack.\
    \ It is a variant of the Blackhole attack, where an attacker only rejects a\n\
    speciﬁc subpart of the network trafﬁc and forwards all RPL control packets. This\
    \ attack is mainly\ntargeted to disturb routing paths; however, it can also be\
    \ used to ﬁlter any protocol [74].\n•\nHello ﬂooding attack. An attacker can announce\
    \ itself as a neighbor to many nodes, even the\ncomplete network by broadcasting\
    \ a “HELLO” message with a strong powered antenna and a\nfavorable routing metric.\
    \ This is done by an attacker in order to deceive other objects to send their\n\
    packet through it [94].\n4.12. Internet Protocol (IPv6) and Low-Power Wireless\
    \ Personal Area Networks (6LoWPAN) Based Attacks\n6LoWPAN was designed to meet\
    \ the communication requirements of connecting resource-\nconstrained, low-powered\
    \ objects and IPv6 networks. To achieve this, 6LoWPAN uses fragmentation\nat the\
    \ adaptation layer. The main attacks against 6LoWPAN are explained as follows:\n\
    •\nFragmentation Attack. IoT object communicating in IEEE 802.15.4 has a Maximum\
    \ Transmission\nUnit (MTU) of 127 bytes, as opposed to in IPv6, which has a minimum\
    \ MTU of 1280 bytes. This is\ndone using a fragmentation mechanism. Since fragmentation\
    \ is performed without using any\ntype of authentication, an attacker can inject\
    \ fragments among a fragmentation chain [95].\n•\nAuthentication Attack. In the\
    \ absence of an authentication mechanism in 6LowPAN, any malicious\nobject can\
    \ join the network and get legitimate access [92].\nElectronics 2020, 9, 1177\n\
    18 of 45\n•\nConﬁdentiality Attack. In the absence of an encryption technique\
    \ in 6loWPAN, attacks affecting\nconﬁdentiality, like eavesdropping, spooﬁng and\
    \ Man in the Middle can be launched.\n5. Intrusion Detection System (IDS)\nMost\
    \ IDSs have a common structure that includes: (1) a data gathering module collects\
    \ data,\nwhich possibly contains evidence of an attack, (2) an analysis module\
    \ detects attacks after processing\nthat data, and (3) a mechanism for reporting\
    \ an attack. In the data gathering module, the input data\nof each part of IoT\
    \ systems can be gathered and examined to ﬁnd normal behavior of interaction,\n\
    thereby detecting malicious behavior at the early stages. The Analysis module\
    \ can be implemented\nusing various techniques and methods, however, ML and DL\
    \ based methods are more suitable and\ndominant for data examination to learn\
    \ benign and anomalous behavior based on how IoT devices and\nsystems interact\
    \ with one another in IoT environments. Furthermore, ML/DL methods can predict\n\
    new attacks, which are often different from previous attacks, because ML/DL methods\
    \ can intelligently\npredict future unknown attacks through learning from existing\
    \ legitimate samples [12]. Figure 8 shows\nthe components of typical IDS based\
    \ on ML/DL methods.\n5.1. Design Choices of ML/DL Based IDS\nAs depicted in Figure\
    \ 9, the main differences in the design choices for IDSs depends on the\nfollowing\
    \ factors:\n•\nDetection methods. It could be signature-based, anomaly-based or\
    \ hybrid-based detection.\n•\nArchitecture. It can be classiﬁed as centralized\
    \ and distributed architecture.\n•\nData source. It would be host-based, network-based,\
    \ or hybrid-based data inputs.\n•\nTime of detection. It can be online or ofﬂine\
    \ detection.\n•\nEnvironment. It would be wired, wireless, ad-hoc networks.\n\
    Attack \nDetection\nAttack \nPrediction\nNormal \nTraffic\nML/ DL \nTechniques\n\
    IoT Systems \nand Threats\nFigure 8. Role of Machine Learning/Deep Learning (ML/DL)\
    \ Based IDS for IoT system.\nElectronics 2020, 9, 1177\n19 of 45\nDesign Choices\
    \ of \nIDS\n- -\n- -\n- -\n- -\nHost Based\nNetwork Based\nSignature \nBased\n\
    Online\nOffline \nData Source\nTime of Detection\nCentralized\nAnomaly \nBased\n\
    Distributed\nWired \nNetwork\nWireless \nNetwork\nAdhoc\nNetwork\nHybrid\nSpecification\
    \ \nBased\nHybrid\nEnvironment\nArchitecture\nDetection Methods\nFigure 9. Taxonomy\
    \ of design choices of IDS for IoT system.\n5.2. Detection Methods of IDSs\nThe\
    \ detection methods used for IDSs can be divided into four methodological types\
    \ [33], as shown\nin Figure 9 and explained below.\n5.2.1. Signature-Based Detection\
    \ Techniques\nSignature-based detection techniques contain a repository of attack\
    \ signatures and compares the\nnetwork trafﬁc or system actions against this repository\
    \ of signatures. As soon as any match is found,\na detection alert is raised.\
    \ Though sufﬁciently accurate against known attacks for which signatures\nexist\
    \ in the repository, this technique cannot detect zero day (new) attacks. Even\
    \ if it is not effective\nagainst mutations of an existing attack [54,96,97].\n\
    Some research, like [98], proposed means to overcome this deﬁciency of signature-based\n\
    techniques through the use of an Artiﬁcial Immune System (AIS). This technique\
    \ designed detectors\nrelying on signatures/patterns of attacks using the model\
    \ of immune cells, which can detect if a packet\nis normal or malicious through\
    \ its classiﬁcation as self or non-self element. The system has the capacity\n\
    for the adoption of new patterns from continuous monitoring of the system. However,\
    \ the feasibility\nof such a detection technique in a resource-constrained IoT\
    \ environment is questionable.\nThe authors in [99] resolved this predicament\
    \ of resource constraints in signature-based IDS\nthrough utilizing a separate\
    \ Linux machine with an adapted version of the Suricata-based signature\nIDS.\
    \ However, the authors did not provide any clues of updating attack signatures.\
    \ The authors in [100]\nextended the work published in [99] by proposing modiﬁcations\
    \ in signature matching techniques.\nAnother research by [101] tackled processing\
    \ power constraints of IoT systems through the use\nof auxiliary shift values\
    \ with a multiple pattern detection algorithm, which enables a reduction\nin the\
    \ number of matching operations required between attack signatures and network\
    \ trafﬁc\npackets. The system used signature repositories of the open-source IDS\
    \ (Snort) and the open-source\nantivirus (ClamAV).\nElectronics 2020, 9, 1177\n\
    20 of 45\n5.2.2. Anomaly-Based Detection Techniques\nAnomaly-based detection techniques\
    \ rely on a baseline normal behavior proﬁle for the monitored\nenvironment [97,102].\
    \ This normal baseline is then used for comparison of system actions at any\n\
    given moment. Any deviations out of bounds of the allowed threshold are reported\
    \ by raising an\nalert without providing any classiﬁcation for the type of attack\
    \ detected. There are also attempts of\nusing machine learning models that learn\
    \ normal and attack events as behavioral detection models,\nbut building normal\
    \ proﬁles are better than learning normal and attack events that can not include\n\
    new attack events in real-world networks. In comparison with signature-based detection\
    \ techniques,\nanomaly-based detection techniques are more effective in discovering\
    \ new attacks. One drawback of\nthis technique is the difﬁculty in building the\
    \ normal behavior baseline proﬁle, which gives rise to\nincreased false positive\
    \ rates [20,103,104]. Anomaly-based detection techniques rely on ML algorithms\n\
    to build a baseline normal proﬁle of monitored systems. The use of such ML techniques\
    \ in resource\nand energy-constrained IoT environments is still a challenge, due\
    \ to high computational resources\nneeded to train and validate ML techniques.\n\
    5.2.3. Speciﬁcation-Based Detection Techniques\nThe basic principle of both anomaly-based\
    \ detection and speciﬁcation-based detection techniques\nis the same, where the\
    \ normal behavior of a system is proﬁled through some means and is compared\n\
    against current system actions to detect out of range deviations.\nHowever, in\
    \ anomaly-based\ntechniques, normal behavior is learned through ML, whereas for\
    \ speciﬁcation-based techniques\nit needs to be manually speciﬁed through a repository\
    \ of rules and associated ranges of deviations by a\nhuman expert [105]. This\
    \ allows for lowering the false-positive rates as compared to the anomaly-based\n\
    detection techniques [20]. Having the advantage of not requiring any learning\
    \ phase after specifying a\nrule set [105], these techniques suffer from lack\
    \ of adaptability to varied environments and are liable to\nerrors in speciﬁcations\
    \ [19].\n5.2.4. Hybrid-Based Detection Techniques\nHybrid-based detection techniques\
    \ employ a mix of the earlier mentioned techniques to offset\nthe shortcomings\
    \ and optimize the advantages of detecting existing and new attacks. The authors\n\
    in [106] proposed SVELTE, which is an IDS for IP-connected IoT systems that use\
    \ RPL as a routing\nprotocol in 6LoWPAN networks. This IDS was designed using\
    \ a hybrid of anomaly and signature\nbased detection techniques to obtain a balance\
    \ between storage and processing requirements of each\nof these two techniques.\
    \ They tried to balance the storage cost of the signature-based detection and\n\
    computing cost of the anomaly-based techniques.\n6. Machine Learning (ML) Techniques\
    \ for IDS\nAs discussed in the previous section, apart from speciﬁcation-based\
    \ detection, all types of\ndetection techniques rely on some sort of ML algorithm\
    \ for the training phase of the IDS. In this\nsection, an overview of different\
    \ ML techniques used in IoT environment based IDSs is presented.\nTable 4 gives\
    \ a brief overview of ML methods, their advantages and limitations along with\
    \ reference\nto related research work conducted. In the end, Table 5 summarizes\
    \ research works conducted to\npropose IDSs using various ML methods, as detailed\
    \ below. Figure 10 illustrates the most common\nML techniques used for designing\
    \ IDSs in IoT networks.\nElectronics 2020, 9, 1177\n21 of 45\nUnsupervised \n\
    Learning\nML \nTechniques \nfor IoT IDS\nPCA\nK-Means\nSupervised \nLearning\n\
    NB\nkNN\nDT\nSVM\nRF\nEL\nFigure 10. A taxonomy of ML Techniques for IoT-based\
    \ IDSs.\n6.1. Naive Bayes (NB) Classiﬁer\nThis algorithm employs Bayes’ theorem\
    \ to predict the probability of occurrence of an event based\non previous observations\
    \ of similar events [107]. In ML scenarios, this can be used for classiﬁcation\n\
    of normal and abnormal behaviors based on previous observations in supervised\
    \ learning mode.\nThe NB classiﬁer is a commonly used supervised classiﬁer known\
    \ for its simplicity. NB calculates\nposterior probability and based on that a\
    \ labeling decision is made to classify unlabeled trafﬁc as\nnormal or anomalous.\
    \ An independent set of features of the observed trafﬁc like, status ﬂags, protocol,\n\
    latency, are used to forecast the probability of trafﬁc being normal or otherwise.\
    \ Being simple and\neasy to implement an algorithm, various IDSs have employed\
    \ an NB classiﬁer to identify anomalous\ntrafﬁc [108–111]. It requires very few\
    \ samples for training [112] and can classify in both binary and\nmulti-label\
    \ classiﬁcation. However, it fails to take into account interdependencies between\
    \ features for\nclassiﬁcation purposes, which affect its accuracy [113].\n6.2.\
    \ K-Nearest Neighbor (KNN)\nKNN does not require any parameters for its working.\
    \ Euclidean distance is used to measure the\ndistance between neighbors [114].\
    \ Figure 11 shows the basic principle behind the KNN classiﬁcation\nalgorithm,\
    \ used to classify a new data instance into already observed classes based on\
    \ its relative\ndistance to either of the classes. The green squares depict the\
    \ normal behavior class and red triangles\nshow the abnormal behavior class, any\
    \ newly observed unknown instance (blue hexagon) can\nnow be classiﬁed based on\
    \ the number of maximum nearest neighbors from either of the classes.\nAccordingly,\
    \ this new instance is classiﬁed as a known class. k is the number of nearest\
    \ neighbors used\nfor classiﬁcation.\nThe classiﬁcation will change with the value\
    \ of k. For k = 1, the red hexagon will be classiﬁed\nas an abnormal class, but\
    \ for k = 2 and k = 3, it will be classiﬁed as a normal class.\nHence,\nobtaining\
    \ the optimal value of k through testing is vital for the accuracy of this algorithm\
    \ [115].\nSome researches [116–118] have used KNN based classiﬁcation for anomaly\
    \ and intrusion detection in\ngeneral and IoT based network intrusion detection\
    \ in particular [119,120] with reasonable accuracy\nin detecting User to Root\
    \ (U2R) and Remote to Local (R2L) attacks. While KNN is simple to use,\ndetermining\
    \ the optimal value of k and identifying missing nodes are time-consuming and\
    \ costly in\nterms of accuracy.\n6.3. Decision Trees (DTs)\nDecision Trees (DTs)\
    \ work by extracting features of the samples in a dataset and then organizing\n\
    an ordered tree based on the value of a feature. Every feature is represented\
    \ by a node of the tree and\nits corresponding values are represented by the branches\
    \ originating from that node. Any feature node\nthat optimally divides the tree\
    \ in two is considered the origin node for the tree [121]. Various metrics\nElectronics\
    \ 2020, 9, 1177\n22 of 45\nare utilized for identiﬁcation of the origin node,\
    \ which optimally divides the training datasets like the\nGini index [122] and\
    \ Information Gain [123].\nAnomaly\nUknown\nNormal\nFigure 11. K-Nearest Neighbor\
    \ (KNN) classiﬁcation principle.\nFigure 12 illustrates decision tree nodes. DT\
    \ algorithms involve two processes, namely induction\nand inference, aimed at\
    \ building the model and then carrying out the classiﬁcation [124]. During the\n\
    induction process, construction of a DT starts with adding nodes and branches.\
    \ Initially, these nodes\nare unoccupied, and then through a process of feature\
    \ selection through information gain and other\nmeasures, a feature is selected\
    \ that is deemed to split the training dataset samples. This feature is then\n\
    assigned as the origin vertex of the DT.\nThe process continues to select feature\
    \ root nodes, to minimize the overlapping between different\nclasses found in\
    \ the training dataset. Resultantly, the accuracy of classiﬁer increases in identifying\n\
    distinct instances of a class. In the end, the leaves of each sub-DT are identiﬁed\
    \ and classiﬁed\naccording to their corresponding classes. After the construction\
    \ of DT, the inference process can start,\nwhere any unknown instances of classes\
    \ with features can be classiﬁed through iterative comparison\nwith constructed\
    \ DT. After the acquisition of a matching leaf node, the classiﬁcation process\
    \ for\nthe new sample is completed [124]. In context of intrusion detection DTs\
    \ have potential for use as\nclassiﬁer [125,126]. However, aspects of bigger storage\
    \ requirements and computational complexity\nmust be considered [124]. In the\
    \ IoT environment, research published in [127] used DT to detect DDoS\nattacks\
    \ through analysis of network trafﬁc for identifying malicious sources.\n6.4.\
    \ Support Vector Machines (SVMs)\nSVM is another type of classiﬁer that works\
    \ through the creation of a hyperplane in the feature\nset of two or more classes.\
    \ The splitting hyperplane is found through a maximum distance of the\nnearest\
    \ data point of each compared class [128], as shown in Figure 13. SVMs are most\
    \ appropriate for\nthe use case where classes containing large feature sets are\
    \ required to be classiﬁed based on a fewer\nnumber of data samples [35,129,130].\
    \ Based on statistical learning [128], SVMs are ideal for anomaly\ndetection where\
    \ classiﬁcation between normal and abnormal classes is required. SVMs are highly\n\
    scalable due to simplicity and are capable of performing tasks like anomaly-based\
    \ intrusion detection\nin real-time including online learning [131–133]. In [134],\
    \ authors use an optimized version of SVM to\npropose “Sec-IoV”, a multi-stage\
    \ model for anomaly detection, for detection of anomalous trafﬁc in\nvehicle-to-vehicle\
    \ (V2V) communications in Internet of Vehicles (IoV) networks.\nElectronics 2020,\
    \ 9, 1177\n23 of 45\nRoot Node\nDecision Node\nDecision Node\nDecision Node\n\
    Decision Node\nDecision Node\nDecision Node\nDecision Node\nDecision Node\nFigure\
    \ 12. Depiction of Decision Tree Structure.\nA\nB\nC\nFigure 13. Depiction of\
    \ Support Vector Machines (SVM) hyperplane splitting.\nAnother advantage of using\
    \ SVM is its use of lesser storage/memory. The use of SVM-based IDSs\nin an IoT\
    \ system has been evaluated in various research studies [135–137], where SVM showed\
    \ more\naccurate results than other ML algorithms including DTs, NB and Random\
    \ Forest. However, the use of\noptimal kernel function in SVM, which is used to\
    \ separate the data when it is not linearly separable,\nremains a challenge to\
    \ achieve the desired classiﬁcation speed.\n6.5. Ensemble Learning (EL)\nEL works\
    \ by building on strengths of various classiﬁers, through a combination of their\
    \ results\nand then generating a majority vote out for classiﬁcation, as shown\
    \ in Figure 14. This improves\nclassiﬁcation accuracy through a combination of\
    \ various homogeneous/heterogeneous classiﬁers’\noutputs [138,139]. EL is based\
    \ on the study [140], where it was found that every ML classiﬁcation\nalgorithm\
    \ depends on the application and associated data for its accuracy. Hence, no ML\
    \ algorithm can\nbe described as “one size ﬁts all solution” and for generalized\
    \ applications, EL like combinations may\nbe best suited for maximizing accuracy\
    \ through a reduction in variance and avoiding overﬁtting [141].\nThe accuracy\
    \ of EL leads to the cost of increased time complexity, due to the use of multiple\n\
    classiﬁers in parallel [142,143]. The efﬁcacy of EL for intrusion detection has\
    \ been examined in\nvarious studies [144–146]. The feasibility of EL under limited\
    \ resource environments like IoT has been\nstudied [147] with a generalized application\
    \ lightweight EL framework being proposed for online\nanomaly detection in IoT\
    \ networks. This study showed that such an EL algorithm produced better\nand accurate\
    \ results than each member classiﬁer individually [147].\nElectronics 2020, 9,\
    \ 1177\n24 of 45\nClassifier 1\nClassifier 3\nClassifier 2\nClassifier n\nEnsemble\n\
    Output\nInput \nX\nEnsemble\nh1(x)\nh2(x)\nh3(x)\nhn(x)\nFigure 14. Working of\
    \ an ensemble classiﬁer.\n6.6. Random Forest (RF)\nRFs can be categorized as a\
    \ supervised ML algorithm. An RF is built using multiple DTs to\npredict more\
    \ accurate and error resistant classiﬁcation results [148,149]. Randomly constructed\
    \ DTs are\ntrained to output classiﬁcation results based on majority voting [148].\
    \ Though DTs can be considered\nas components of RF, there are two distinct classiﬁcation\
    \ algorithms due to the reason that contrary to\nDTs, which build a rule-set during\
    \ training for subsequent classiﬁcation of new samples, RF builds\na rule-subset\
    \ using all member DTs. This results in a more robust and accurate output, which\
    \ is\nresistant to overﬁtting and requires substantially fewer inputs and does\
    \ not require the process of\nfeature selection [35]. As proposed by some studies\
    \ [150,151], RF is suitable for anomaly and intrusion\ndetection in IoT networks.\
    \ Moreover, another study [152] has shown RF to be better than KNN,\nartiﬁcial\
    \ neural network (ANN) and SVM at DDoS detection in IoT networks because it requires\
    \ fewer\ninput features and can bypass heavy computations required for feature\
    \ selection in real-time IDS [153].\n6.7. k-Means Clustering\nIt is an unsupervised\
    \ algorithm, which is based on the discovery of k clusters in the data samples.\n\
    Each instance of sample data is assigned to a particular cluster based on its\
    \ features. The samples are\ndistributed over k clusters according to their features\
    \ using the estimation of centroids as per squared\nEuclidean distance. Recalculation\
    \ of centroids of each cluster is then performed through taking the\nmean of data\
    \ points allocated to that cluster, as shown in Figure 15. The process continues\
    \ iteratively\nuntil no modiﬁcations to the clusters can be made [154,155]. Selection\
    \ of an appropriate value of k and\nthe assumption that the sample dataset will\
    \ be equally distributed over the k clusters act as limitations\nfor the k-means\
    \ clustering algorithm. Previous studies presented in [156,157] suggest the suitability\
    \ of\nk-means clustering for anomaly detection through calculating feature similarity.\
    \ The authors [158]\nsuggested combining DT with k-means clustering for anomaly\
    \ detection in IoT networks to improve\nthe performance.\n6.8. Principle Component\
    \ Analysis (PCA)\nPCA is not an anomaly detection technique, but it is commonly\
    \ used as a feature selection\nor a feature reduction technique from a large dataset.\
    \ The selected feature sets can then be used\nalong with some other ML classiﬁers\
    \ to detect anomalies in an IoT network. The PCA technique\ntransforms a large\
    \ set of variables into a reduced set of features without losing much of the information.\n\
    Various research works [159–162] used a combination of PCA with various classiﬁers\
    \ to detect\nanomalies in IoT networks.\nElectronics 2020, 9, 1177\n25 of 45\n\
    Start\nCentroid\nDistance Object to \nCentroids\nGrouping Based on \nMinimum Distance\n\
    No object changes\nits group?\nEnd\nYes\nNo\nNumber of Clusters\nFigure 15. Illustration\
    \ of k-means clustering.\nTable 4. Taxonomy of ML based methods for IoT systems\
    \ security.\nML Method\nAttack Types Handled\nPros\nCons\nKB [108,109,111,113]\n\
    HTTP attacks (Buffer\noverﬂow, Shell attacks) [111],\nDoS, Probe, R2L [109]\n\
    -It requires very few samples for\ntraining [112].\n-It can classify in both binary\
    \ and\nmulti-label classiﬁcation.\n-It shows robustness to\nirrelevant features.\n\
    It fails to take into account\ninterdependencies between features\nfor classiﬁcation\
    \ purposes, which\naffect its accuracy [113].\nKNN [116–120]\nU2R, R2L,Flooding\
    \ attacks,\nDoS, DDoS\n-Simple to use.\nDetermining optimal value of K and\nidentifying\
    \ missing nodes\nare challenging.\nDT [125–127]\nDDoS [127], U2R, R2L [125]\n\
    -Easy and simple to use method.\n-It requires bigger storage\n-It is computationally\
    \ complex\n-It is easy to use only if few DTs\nare used.\nSVM [131–133]\nScan,\
    \ DDoS (TCP, UDP\nﬂood), smurf, portsweep\n-SVMs are highly scalable due to\n\
    simplicity and are capable of\nperforming tasks like anomaly-based\nintrusion\
    \ detection in real-time\nincluding online learning.\n-SVMs are considered suitable\
    \ for data\ncontaining a large number of\nfeature attributes.\n-SVMs use lesser\
    \ storage and memory.\n-The use of optimal kernel function\nin SVM, which is used\
    \ to separate\nthe data when it is not linearly\nseparable, remains a challenge\
    \ to\nachieve desired classiﬁcation speed.\n-It is difﬁcult to understand and\n\
    interpreting SVM-based models.\nEL [144–146,163]\nDoS, Probe, R2L, U2R attacks\n\
    -It is robust to overﬁtting.\n-Performs better than a single classiﬁer.\n-It reduces\
    \ variance.\n-Increased time complexity, due to\nthe use of multiple classiﬁers\n\
    in parallel\nRF [150,151]\nDoS, Probe, R2L, U2R\n-It produces a more robust and\
    \ accurate\noutput which is resistant to overﬁtting.\n-It requires substantially\
    \ fewer inputs\nand does not require the process of\nfeature selection.\n-Since\
    \ RF constructs several DTs, its\nuse may be impractical in real-time\napplications\
    \ requiring large dataset.\nK-Means [157,158,164]\nDoS, Probe, R2L, U2R\n-k-Means\
    \ clustering does not require\nlabeled data.\n-It is less effective as compared\
    \ to\nsupervised learning technique,\nin particular detecting\nknown attacks.\n\
    PCA [159–162]\nUsed in combination with\nother ML methods\n-PCA is suitable where\
    \ the dataset\ninvolves large set of variables as PCA\ntransforms it to reduced\
    \ set of features\nwithout losing much of information.\n-Can reduce the complexity\
    \ in the data.\n-It is not an anomaly detection\nmethod, it must be used with\
    \ some\nother ML methods to design a\nsecurity model.\nElectronics 2020, 9, 1177\n\
    26 of 45\nTable 5. Comparison of studies that used ML and DL techniques in IoT\
    \ Security.\nStudy\nMachine Learning\nDeep Learning\nDataset\nThreat Detected\n\
    NBC\nKNN\nDT\nSVM\nEL\nRF\nK-Means\nRNN\nCNN\nAE\nRBM\nDBN\nEDLN\nGAN\n[110,111]\n\
    ✓\n-\n-\n✓\n-\n-\n✓\n✓\n-\n-\n-\n-\n-\n-\nKDD99\nAnomaly Detection\n[116]\n-\n\
    ✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nKDD99\napache2 udpstorm processtable mailbomb\n\
    [125]\n-\n-\n-\n-\n-\n-\n-\n✓\n✓\n-\n-\n-\n-\n-\nADFA-LD and ADFA-WD\nAdduser\
    \ Meterpreter Webshell\n[129]\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDARPA\
    \ dataset\nProbe attack, U2R attack\n[143]\n-\n-\n-\n-\n✓\n-\n-\n-\n-\n-\n-\n\
    -\n-\n-\nKDD99\nNetwork Trafﬁc anomaly detection\n[150]\n-\n-\n-\n-\n-\n✓\n-\n\
    -\n-\n-\n-\n-\n-\n-\nBoot-strapped\nWorms, Buffer overﬂows\n[157]\n-\n-\n-\n-\n\
    -\n-\n✓\n-\n-\n-\n-\n-\n-\n-\nKDD99\n-\n[165]\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n\
    -\n-\n-\n-\nISCX2012\nPROBE attacks or non-PROBE attacks\n[166]\n-\n-\n-\n-\n\
    -\n-\n-\n✓\n✓\n-\n-\n-\n-\n-\nAndroid Malware Genome project\nMalware\n[167]\n\
    -\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\n-\nOutlier Detection DataSets\nAnomaly\
    \ detection\n[168]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\n-\n-\n-\nKDD\n-\n[169]\n\
    -\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n✓\nNSL-KDD\n-\n[170]\n-\n-\n-\n-\n-\n-\n\
    -\n-\n-\n-\n-\n✓\n-\n-\n500 samples for dataset\nAnomaly detection\nElectronics\
    \ 2020, 9, 1177\n27 of 45\n7. Deep Learning (DL) Techniques for IDSs\nDL algorithms\
    \ outperform ML algorithms in applications involving large datasets. DL becomes\n\
    most relevant in IoT security applications as IoT environments are characterized\
    \ by the production of\nvast amounts and a variety of data [171]. Furthermore,\
    \ DL is capable of the automatic modeling of\ncomplex feature sets from the sample\
    \ data [171]. Another advantage of DL algorithms is their ability\nto allow deep\
    \ linking in IoT networks [172]. This enables automatic interactions between IoT-based\n\
    systems in the absence of human intervention [171] to perform assigned collaborative\
    \ functions.\nBecause of their ability to extract hierarchical feature representations\
    \ in complex deep architecture,\nDL can be classiﬁed as a branch of ML algorithms\
    \ that uses multiple non-linear layers of processing\nto extract feature sets.\
    \ These feature sets are then used for abstraction and pattern detection after\n\
    necessary transformations [173]. As shown in Figure 16, DL can be used in a generative\
    \ mode with\nunsupervised learning, discriminative mode using supervised learning,\
    \ or a hybrid approach by\ncombining both modes.\nUnsupervised \nLearning\nSupervised\
    \ \nLearning\nHybrid \nTechniques\nDL \nTechniques \nfor IoT IDS\nCNN\nRNN\nDBN\n\
    AE\nGAN\nEDLN\nRBM\nFigure 16. Taxonomy of potential DL techniques for IoT IDS.\n\
    In this section, various major DL based techniques used for designing an IDS are\
    \ discussed.\nTable 5 below summarizes research studies conducted to propose IDS\
    \ using various DL-based\nmethods. Details about each research work along with\
    \ the DL technique is explained in respective\nsub-sections below.\n7.1. Recurrent\
    \ Neural Networks (RNNs)\nRNN is a discriminative DL algorithm, which is best\
    \ suited in environments where data is to be\nprocessed sequentially. Unlike other\
    \ neural networks, its output is dependent on back-propagation\ninstead of forward\
    \ propagation [173–175]. A temporal layer is incorporated in an RNN for analyzing\n\
    data sequentially followed by learning about multi-dimensional differences in\
    \ unrevealed units of\nrecurrent components [165]. Modiﬁcations to these unrevealed\
    \ units are then made corresponding to\ndata encountered by the neural network,\
    \ causing continuous updates and the manifestation of the\ncurrent state of the\
    \ neural network.\nThe current unrevealed state of the neural network is processed\
    \ by an RNN algorithm through\nthe estimation of succeeding hidden states as triggering\
    \ of a previously unrevealed state. A simple\nexplanation of RNN functioning is\
    \ described in Figure 17. Here, outputs from neurons are sent back\nas feedback\
    \ to the neurons of the previous layer. Because IoT environments are characterized\
    \ by the\ngeneration of large amounts of sequential data like network trafﬁc ﬂows,\
    \ RNNs become relevant in IoT\nsecurity applications, especially network intrusion\
    \ detection. Previous research [176] has proposed\nthe use of an RNN for network\
    \ intrusion detection through analysis of network trafﬁc behavior and\nreported\
    \ obtaining useful results, particularly time series-based threats. Another recent\
    \ research [177]\nElectronics 2020, 9, 1177\n28 of 45\nproposes an IDS that uses\
    \ cascaded ﬁltering stages in which deep multi-layered RNN are applied for\neach\
    \ ﬁlter. RNNs are then trained to detect common attacks launched in IoT environments,\
    \ like R2L,\nDos, U2R and Probe.\nLong short-term memory (LSTM) network architectures,\
    \ which are a specialized form of RNN,\nhave also been used in the designing of\
    \ IDS. The main attribute of LSTM based RNNs is to persist\ninformation or cell\
    \ state for later use in the network. This feature makes them appropriate for\n\
    performing analysis of temporal data that changes over time. Thus, LSTM networks\
    \ are preferred\nto solve problems related to anomaly detection in time-series\
    \ sequence data. Various forms of RNN,\nincluding LSTM based RNNs, have been used\
    \ for anomaly and intrusion detection in IoT networks by\nresearchers in [178–183].\
    \ While RNNs have demonstrated promising results in predicting time series\ndata,\
    \ the detection of anomalous trafﬁc using these predictions is still challenging.\n\
    Input Layer\nHidden Layer\nOutput Layer\nX1\nX2\nX3\nXn\nOutput\nFeedback\nFigure\
    \ 17. Illustration of recurrent neural network algorithm.\n7.2. Convolutional\
    \ Neural Network (CNN)\nCNN is also a discriminative DL algorithm, which was designed\
    \ to minimize the number of data\ninputs required for a conventional artiﬁcial\
    \ neural network (ANN) through the use of equivariant\nrepresentation, sparse\
    \ interaction and sharing of parameters [184]. Thus CNN becomes more scalable\n\
    and requires less time for training. There are three-layer types in a CNN, namely\
    \ convolutional layer,\npooling layer and activation unit, as shown in Figure\
    \ 18. The convolutional layers use various kernels\nfor convoluting data inputs\
    \ [185]. The pooling layers downsize samples, thus minimizing the sizes of\nsucceeding\
    \ layers. It involves two techniques: Max pooling and average pooling, where the\
    \ former\nchooses a maximum value for every cluster of past layers after distributing\
    \ the input among distinctive\nclusters [186,187].\nThe average pooling, on the\
    \ other hand, calculates the average values of every cluster in the\nprevious\
    \ layer. The activation unit is able to trigger an activation function on every\
    \ feature in the feature\nset in a non-linear fashion [187]. CNN is best suited\
    \ for highly efﬁcient and fast feature extraction\nfrom raw data but at the same\
    \ time CNN requires high computational power [188]. Hence using\nCNN on resource-constrained\
    \ IoT devices for their security is highly challenging. This challenge is\nsomewhat\
    \ addressed through distributed architecture where a lighter version of Deep NN\
    \ is trained\nand implemented on-board with only a subset of vital output classes,\
    \ whereas, the high computational\npower of the cloud is used to perform the complete\
    \ the training of the algorithm [166]. Their use in IoT\nenvironment security\
    \ was discussed in previous research published in [189,190] for malware detection.\n\
    In [40], authors propose a hybrid data processing model for network anomaly detection\
    \ that utilizes\nElectronics 2020, 9, 1177\n29 of 45\nGrey Wolf Optimization (GWO)\
    \ and CNN techniques. Authors claim to have achieved better accuracy\nand detection\
    \ rate in comparison to other state-of-the-art IDS.\nInput\nConvolution\nPooling\n\
    Filtering\nInput Layer\nHidden Layer\nOutput Layer\nFigure 18. Illustration of\
    \ convolution neural network working.\n7.3. Deep Autoencoders (AEs)\nIt is an\
    \ unsupervised algorithm designed for the reproduction of its input at its output\
    \ through\nthe use of a decoder function and a hidden layer containing the deﬁnition\
    \ of a code utilized for\nthe representation of input [184]. The other function\
    \ in an AE neural network is called the encoder\nfunction and is responsible for\
    \ the conversion of the acquired input into code. During training,\nreconstruction\
    \ errors must be minimized [191]. One use case for AE is feature extraction from\
    \ the\ndatasets. However, these suffer from the requirement of high computational\
    \ power. Deep AEs have\nbeen used for the detection of network-based malware in\
    \ previous research with better accuracy than\nSVM and KNN [167]. Kitsune [41]\
    \ is one such study where an ensemble of deep auto-encoders was\nused to implement\
    \ an online lightweight IDS for IoT environments based on unsupervised learning\n\
    and anomaly detection where authors demonstrate better accuracy as compared to\
    \ other ML and\nDL techniques.\n7.4. Restricted Boltzmann Machine (RBM)\nIt is\
    \ an unsupervised learning-based algorithm and builds a deep generative and undirected\n\
    model [168]. There are no two nodes in any layer of an RBM that have any connection\
    \ with each other.\nVisible and hidden layers are the two types of layers making\
    \ up an RBM. Known input parameters\nare contained in the visible layer, while\
    \ the unknown potential variables are included with several\nlayers forming the\
    \ hidden layer. Working hierarchically, features extracted from a dataset are\
    \ then\npassed on to the next layer as latent variables. RBMs were used in various\
    \ research work [192,193] for\nnetwork/IoT intrusion detection systems. The challenge\
    \ of implementing RBMs is that it needs high\ncomputational resources while implementing\
    \ it on low-powered IoT devices. Furthermore, Single RBM\nlacks the capability\
    \ of feature representation. However, this limitation can be overcome by applying\n\
    two or more RBM stacked to form a Deep Belief Network (DBN).\n7.5. Deep Belief\
    \ Network (DBN)\nBeing formed by stacking two or more RBMs, DBN can be considered\
    \ as unsupervised learning\nbased generative algorithms [194]. They perform robustly\
    \ through unsupervised training for each\nlayer separately [165]. Initial features\
    \ are extracted in the pre-training phase for each layer, followed\nby a ﬁne-tuning\
    \ phase where the application of a softmax layer is executed on the top layer\
    \ [170]. It is\nmainly composed of two layers, i.e., visible layer and hidden\
    \ layer, as shown in Figure 19. Though the\nstudy in [188,195] discussed malicious\
    \ attack detection using DBNs with comparatively better results\nthan ML algorithms,\
    \ no evidence of applicability in the IoT environment was reported in the literature.\n\
    Electronics 2020, 9, 1177\n30 of 45\nBack Reference \nInput Layer\nHidden Cells\n\
    Output Cells\nFigure 19. Illustration of deep belief network working.\n7.6. Generative\
    \ Adversarial Network (GAN)\nIt is a hybrid DL method that uses both generative\
    \ and discriminative models at the same time for\ntraining [196]. Distributions\
    \ of the dataset and samples is obtained by the generative model predictions\n\
    about the authentic origination of a given sample from a training dataset and\
    \ are made by the\ndiscriminative model [196]. As shown in Figure 20, both generative\
    \ and discriminative models work as\nadversaries where the generative model attempts\
    \ deception through the generation of a sample using\nrandom noise. On the other\
    \ hand, the discriminative model attempts to authenticate real training data\n\
    samples from deceptive samples generated by the generative model. Here, D(x) represents\
    \ a binary\nclassiﬁcation giving output as real or fake (generated). The measure\
    \ of correct/incorrect classiﬁcation\ndetermines the accuracy and performance\
    \ of both the models in an inversely proportional fashion.\nThis results in models\
    \ updating in each iteration [191]. The study published in [169] discussed the\n\
    utility of the GAN algorithm for detecting anomalous behavior in IoT environments\
    \ with promising\nresults due to their ability to counter zero-day attacks through\
    \ the generation of samples mimicking\nzero-day attacks, thereby causing the discriminator\
    \ to learn different attack scenarios. However,\nthe challenge with using GAN\
    \ is that its training is difﬁcult and it produces unstable results [196,197].\n\
    Input\nGenerator\nImage\nTraining \nSample\nDiscriminator \n \nD(x)\n1\n0\nFigure\
    \ 20. Illustration of generative adversarial network (GAN) working.\n7.7. Ensemble\
    \ of DL Networks (EDLNs)\nAs discussed earlier, the ensemble of various ML classiﬁers\
    \ proves more effective than individual\nML classiﬁer results. Similarly multiple\
    \ DL algorithms can be used in parallel through organizing\nin an ensemble to\
    \ produce better results than each component DL algorithm. EDLNs can have any\n\
    Electronics 2020, 9, 1177\n31 of 45\ncombination of a discriminative, generative,\
    \ or hybrid type of DL algorithms. Best suited for solving\ncomplex issues, EDLNs\
    \ perform better in uncertain environments with a high number of features.\nA\
    \ heterogeneous EDLN has classiﬁers from the different genres, whereas a homogeneous\
    \ EDLN has\nclassiﬁers from the same genre. Both compositions are aimed at increasing\
    \ efﬁciency and producing\naccurate results [198]. Application of EDLN for IoT\
    \ security requires further study and research,\nto evaluate the possibility of\
    \ improving the performance and accuracy of the IoT security system [12].\nTable\
    \ 6 illustrates common attack types handled by corresponding DL methods along\
    \ with reference to\nrelated research. Table 6 also describes advantages and limitations\
    \ of each suggested DL method. Later,\nTable 5 below covers the comparison of\
    \ work conducted on ML and DL techniques on IoT Security.\nTable 6. Taxonomy of\
    \ DL based methods for IoT systems security.\nDL Technique\nAttack Types Handled\n\
    Pros\nCons\nRNNs [176–183]\n-R2L, DoS, U2R and\nProbe [177]\n-Botnet [176]\n-In\
    \ particular suitable to\ndetect anomalies in time\nseries data [179–183]\n-Best\
    \ suited in environments where data is\nto be processed sequentially.\n-In some\
    \ cases, IoT system environment\nproduces sequential data, hence RNNs are\nsuitable\
    \ in IoT security.\nThe major challenge in the use of RNNs is\nhandling the issue\
    \ of vanishing or\nexploding gradients, which hinders\nlearning of long data sequences.\n\
    CNNs [189,190]\nMalware attacks\n-CNN is best suited for highly efﬁcient and\n\
    fast feature extraction from raw data.\n-Since CNN can automatically learn\nbehavior\
    \ from raw network security data,\nthey have potential application in\nIoT security.\n\
    -CNN requires high computational power;\nthus using CNN on resource-constrained\n\
    IoT devices for their security is\nhighly challenging.\nDeep\nAutoencoders [41,167]\n\
    -Malware attacks\n-Botnet attacks [41]\n-AEs have been successfully used for\n\
    feature extraction and\ndimensionality reduction.\n-AEs are computationally heavy.\n\
    -May not produce desired results if the\ntraining dataset is not representative\
    \ of the\ntesting dataset.\nRBM [192,193]\n-R2L, DoS, U2R and Probe\n- Feedback\
    \ function of RMBMs facilitates\nextraction of important attributes which are\n\
    then used to capture the behavior of IoT\ntrafﬁc.\n-RBMs needs high computational\
    \ resources\nwhile implementing it on low-powered\nIoT devices.\n-Single RBM lacks\
    \ the capability of feature\nrepresentation.\nDBNs [188,195]\n-R2L, DoS, U2R and\
    \ Probe\n-Suitable for vital feature extraction with\ntraining on unlabeled data.\n\
    -DBNs require high computational costs.\nGAN [191]\n-Botnet (Mirai, Bashlite),\n\
    Scanning, MiTM\n-Ability to detect zero-day attacks\n-Generating a sample needs\
    \ only one pass\nthrough the model.\n-Its training is difﬁcult and it produces\n\
    unstable results.\nEDLNs [41,198]\n-Malware, DoS, Botnet,\nMiTM\n-Ensemble of\
    \ DL classiﬁers can achieve\nbetter model performance\n-EDLNs perform better in\
    \ uncertain\nenvironments with a high number\nof features.\n- EDLNs are computationally\
    \ heavy\nand complex.\n8. Datasets Available for IoT Security\nEvaluating the\
    \ effectiveness of any IDS entails a reliable and current dataset that contains\
    \ present\nbenign and anomalous activities. Most of the earlier research in IDS\
    \ relied on the KDD99 [199] dataset\ndue to the absence of other datasets for\
    \ about two decades. However, analysis suggests that the KDD99\ndataset negatively\
    \ affects the IDS results in [199,200] and [201]. Numerous research efforts have\
    \ been\nundertaken to address the weaknesses of KDD99 and other datasets that\
    \ appeared after that. A brief\ndescription of the most common datasets for evaluating\
    \ IDS is presented below.\n•\nKDD99. This is a modiﬁcation of the DARPA funded\
    \ DARPA98 dataset that initiated from an\nIDS program conducted at MIT’s (Massachusetts\
    \ Institute of Technology) Lincoln Laboratory\nfor evaluating IDSs that differentiate\
    \ between inbound normal and attack connections. Later on,\nthis dataset was used\
    \ in the International Knowledge Discovery and Data Mining Tools\nCompetition\
    \ [202] after some ﬁltering, resulting in what is known as the KDD CUP 99 dataset\
    \ [199].\nThis dataset has been used by most of the researchers for the last two\
    \ decades now. The absence\nof alternatives has resulted in several works directed\
    \ on the KDD CUP 99 dataset [199] as\na widespread benchmark for the accuracy\
    \ of the classiﬁer.\nHowever, KDD-99 possesses\nnumerous weaknesses, which discourage\
    \ its use in the current context, including its age,\nElectronics 2020, 9, 1177\n\
    32 of 45\nhighly skewed targets, non-stationarity between training and test datasets,\
    \ pattern redundancy,\nand irrelevant features.\n•\nNSL-KDD. NSL-KDD is an effort\
    \ by the researchers who published their work in [199] to\novercome the weaknesses\
    \ of KDD-99. It is a more balanced resampling of KDD-99 where the\nemphasis is\
    \ laid on examples that are expected to be missed by classiﬁers trained on the\
    \ basic\nKDD-99. However, as their authors acknowledge themselves, there are still\
    \ weaknesses in the\ndataset, like its non-representation of low footprint attacks\
    \ [200].\n•\nThe DEFCON dataset. DEFCON-8 dataset, generated in 2000, comprises\
    \ of port scanning and\nbuffer overﬂow attacks. Another version, the DEFCON-10\
    \ dataset, generated in 2002 uses bad\npackets, FTP by telnet protocol, administrative\
    \ privilege, port scan and sweeps attacks [203].\nThe trafﬁc produced during the\
    \ Capture the Flag (CTF) competition is dissimilar from network\ntrafﬁc of the\
    \ real world because it primarily consists of attack trafﬁc as opposed to usual\n\
    background trafﬁc, therefore its applicability for evaluating IDS is limited.\
    \ The dataset is mostly\nused to assess alert correlation techniques [204,205].\n\
    •\nThe Center of Applied Internet Data Analysis (CAIDA)—2002–2016 datasets [203,206].\n\
    This organization has three different datasets: (1) the CAIDA OC48, covering various\
    \ types of\ndata observed on an OC48 link, (2) the CAIDA DDOS, which comprises\
    \ of one-hour DDoS attack\ntrafﬁc that happened in August 2007, and (3) the CAIDA\
    \ Internet traces 2016, which is passive\ntrafﬁc traces from CAIDA’s Equinix-Chicago\
    \ monitor on the high-speed Internet backbone [207].\nThese datasets are speciﬁc\
    \ to certain events or attacks and are anonymized with their protocol\ninformation,\
    \ payload, and destination. These are not effective benchmarking datasets because\n\
    of several shortcomings, as discussed in [207], like the unavailability of ground\
    \ truth about the\nattack instances.\n•\nThe LBNL dataset contains anonymized\
    \ trafﬁc, which is comprised of only header data.\nThe dataset was generated at\
    \ the Lawrence Berkley National Laboratory, by gathering real\noutbound, inbound\
    \ and routing trafﬁc from two edge routers [206]. It lacked the labeling process\n\
    and also no extra features were created [206].\n•\nThe UNSW-NB15 is a dataset\
    \ developed at UNSW Canberra by the researchers of [208] for the\nevaluation of\
    \ IDS. The researchers used the IXIA PerfectStorm tool to generate a mixture of\
    \ attack\nand benign trafﬁc, at the Australian Center of Cyber Security (ACCS)\
    \ over two days, in sessions\nof 16 and 15 h. They generated a dataset of size\
    \ 100 GB in the form of pcap ﬁles with a substantial\nnumber of novel features.\
    \ NB15 was planned as a step-up from the KDD99 dataset discussed\nabove. It covers\
    \ 10 targets: one benign, and nine anomalous, namely: DoS, Exploits, Analysis,\n\
    Fuzzers, Worms, Reconnaissance, Generic, Shell Code and Backdoors [208]. However,\
    \ the dataset\nwas designed based on a synthetic environment for producing attack\
    \ activities.\n•\nThe ISCX datasets [207]. The Canadian Institute for Cybersecurity\
    \ has been working on the\ngeneration of numerous datasets that are used by independent\
    \ researchers, universities and\nprivate industry around the world. A few datasets\
    \ relevant to our work are IPS/IDS dataset on\nAWS (CSE-CIC-IDS2018), IPS/IDS\
    \ dataset (CICIDS2017), CIC DoS dataset (application-layer),\nISCX Botnet dataset,\
    \ ISCX IDS 2012 dataset, ISCX Android Botnet dataset, and ISCX NSL-KDD\ndataset.\
    \ Their latest dataset related to our work is CICIDS2017. This dataset covers\
    \ benign\nand the most up-to-date common attacks, which is comparable to the real-world\
    \ data [209].\nThe CICIDS2017 consists of multiple attack scenarios, with realistic\
    \ user-related background\ntrafﬁc produced by using the B-Proﬁle system. For this\
    \ dataset they built the abstract behavior\nof 25 users based on the FTP, SSH,\
    \ HTTP, HTTPS and email protocols. However, the ground\ntruth of the datasets,\
    \ which would improve the reliability of the labeling process, was not shared.\n\
    Moreover, applying the idea of proﬁling, which was used to produce these datasets,\
    \ in real\nnetworks could be problematic due to their intrinsic complexity [209].\n\
    •\nThe Tezpur University IDS (TUIDS) dataset [206]. This dataset was generated\
    \ by the professors\nfrom Tezpur University, India. This dataset features DoS,\
    \ Probing, Scan, U2R and DDoS attack\nElectronics 2020, 9, 1177\n33 of 45\nscenarios,\
    \ performed in a testbed. However, the ﬂow level data does not contain any new\
    \ features\nother than those produced by the ﬂow-capturing process [209].\n•\n\
    BoT-IoT [209] The BoT-IoT dataset was created by designing a realistic network\
    \ environment in\nthe Cyber Range Lab of The center of UNSW Canberra Cyber. The\
    \ environment incorporates a\ncombination of normal and botnet trafﬁc. Researchers\
    \ also present a testbed setting for handling\nthe existing dataset shortcomings\
    \ of capturing complete network information, correct labeling,\nand the latest\
    \ and complex attack diversity. In their work, the authors also evaluate the reliability\n\
    of the BoT-IoT dataset using different ML and statistical techniques for forensics\
    \ purposes in\ncomparison to the other datasets discussed above. The dataset’s\
    \ source ﬁles are provided in\ndifferent formats, including the original pcap\
    \ ﬁles, the generated argus ﬁles and CSV ﬁles. The ﬁles\nwere separated, based\
    \ on attack category and subcategory, to better assist in the labeling process.\n\
    The dataset contains OS and Service Scan, DoS, DDoS, Data exﬁltration and Keylogging\
    \ attacks.\nBased on the protocol used, the DDoS and DoS attacks are further organized\
    \ [209].\n•\nIoTPoT Dataset [210]. This dataset was generated through honeypots,\
    \ so there was no process for\nmanual labeling and anonymization; however, it\
    \ has restricted view of the network trafﬁc since\nonly attacks launched at the\
    \ honeypots could be observed. Authors claim that IoTPoT examines\nTelnet-based\
    \ attacks against different IoT devices running on different CPU architectures\
    \ such as\nMIPS, ARM and PPC. During 39 days of operation, authors recorded 76,605\
    \ download attempts of\nmalware binaries from 16,934 visiting IP. Authors further\
    \ claim that none of these binaries could\nhave been detected by existing honeypots\
    \ that handle the Telnet protocol, such as telnet password\nhoneypot and honeyd,\
    \ because these honeypots are not capable of handling different incoming\ncommands\
    \ initiated by the attackers [210].\n•\nN-BaIoT Dataset. The most recent dataset,\
    \ speciﬁcally related to the evaluation of IDS for IoT\nnetworks is generated\
    \ by the authors [41] as part of their research work on online network IDS.\n\
    They generated and collected trafﬁc from two networks: one, an IP camera video\
    \ surveillance\nnetwork where they launched eight different types of attacks that\
    \ affect the availability and\nintegrity of the video uplinks; two, an IoT network\
    \ comprising of three PCs and nine IoT devices,\nout of which one was infected\
    \ with the Mirai botnet malware. A detailed explanation of the attacks\nand network\
    \ topologies is available in their published papers [41]. The authors compiled\
    \ a dataset\nof extracted feature vectors for each of these nine attacks. The\
    \ attack types include: OS Scan,\nFuzzing, Video injection, ARP MiTM, Active Wiretap,\
    \ SSDP ﬂood, SYN DoS, SSL Renegotiation\nand Mirai.\n9. Challenges and Future\
    \ Research Directions\nA large number of studies and research works have been\
    \ published related to IDSs for IoT.\nHowever, there are still a large number\
    \ of open research challenges and issues, particularly in the use\nof ML and DL\
    \ techniques for anomaly and intrusion detection in IoT. The challenge is that\
    \ there exists\nno standard mechanism that guarantees validation of the proposed\
    \ systems or method. The research\nworks mostly demonstrate evaluation of their\
    \ proposed systems based on synthesized datasets and\naddress one speciﬁc problem\
    \ which may not work in the real world on real data and in the presence of\nother\
    \ problems. As evident from this and other similar studies conducted on state\
    \ of the art in IDS\nfor IoT, it is very difﬁcult to design an IDS which covers,\
    \ at least, the most important aspects of an\neffective IDS, that is it is deployable,\
    \ online, scalable, works effectively on real data and satisﬁes all\nstakeholders\
    \ requirements. Instead, most of the published work share evaluation results tested\
    \ on\ncontrived datasets, cover a single or some part of the system, and show\
    \ results using biased parameters.\nFurthermore, a proof of completeness and accuracy\
    \ of any proposed IDS is very hard to deﬁne\nor accomplish. Thus, one of the conclusions\
    \ from this study is that it is very hard to design a\ncomprehensive IDS, which\
    \ can offer good accuracy, scalability, robustness and protection against all\n\
    types of threats. Below, some of the major issues and challenges that researchers\
    \ face today and in\nthe future are described. Since the IoT security measures\
    \ are still not matured, there is enormous\nElectronics 2020, 9, 1177\n34 of 45\n\
    scope for future research in this area, particularly in anomaly and intrusion\
    \ detection using ML and\nDL techniques.\nThe most recent challenges related to\
    \ anomaly and intrusion detection in IoT networks are\ndiscussed in the following:\n\
    •\nTo test and validate proposed NIDS, a good quality dataset related to IoT IDS\
    \ is very essential.\nSuch a dataset should possess a reasonable size of network\
    \ ﬂow data covering both attack and\nnormal behavior with the corresponding label.\
    \ Furthermore, in order to capture normal behavior,\nnormal trafﬁc data from each\
    \ type of IoT device is required, other than the attack data for testing\nthe\
    \ NIDS. However, as discussed in the previous section, most of the publicly available\
    \ datasets\nlack in providing the required features, like missing labels, incomplete\
    \ network features, missing\nraw pcap ﬁles and are difﬁcult to comprehend and/or\
    \ have incomplete CSV ﬁles. Moreover,\ndatasets available only capture normal\
    \ behavior of a speciﬁc type of IoT devices, which restricts\ntraining of IDS\
    \ on those devices only. Creating a dataset that can address these issues in a\
    \ real\nenvironment will be a challenge and a potential area of research.\n•\n\
    Developing an online and real-time, anomaly-based IDS for IoT networks is very\
    \ challenging.\nThis is because such an IDS would require to learn a normal behavior\
    \ ﬁrst to detect abnormal or\nmalicious behavior. The learning phase assumes that\
    \ there is no noise or attack trafﬁc during this\nperiod which cannot be guaranteed.\
    \ Such an IDS may generate false alarms if these issues are\nnot addressed.\n\
    •\nAs also described in this paper, most of the anomaly-based NIDS tries to construct\
    \ a model\nthat captures the proﬁle of all possible behavior or patterns of normal\
    \ trafﬁc. This, however,\nis extremely challenging because it has been proven\
    \ that such models tend to bias towards the\ndominated class, that is, normal\
    \ class, resulting in high false-positive rates. Furthermore, it is\nalso not\
    \ possible to capture all possible normal observations that may be generated in\
    \ a network,\nparticularly in a heterogeneous environment of IoT networks, which\
    \ increases false-negative\nrates. Completely avoiding or minimizing false-positive\
    \ rates and false-negative rates in NIDS is\nanother research challenge.\n•\n\
    It would be interesting to develop models trained on speciﬁc types of devices.\
    \ These models\ncan be applied to IDSs in other organizations using a similar\
    \ type of device. This will assist\nother organizations, which can deploy these\
    \ models and thus save time that would have been\nrequired to collect the data\
    \ and train the IDSs. It will also help in detecting malicious IoT devices,\n\
    which are already compromised because their behavior would be different from normal\
    \ behavior\ncaptured by trained models. Developing such models is a challenging\
    \ task and a potential area\nfor future research.\n•\nDifferent stages involved\
    \ in the design and implementation of NIDS, like data-preprocessing and\nfeature\
    \ reduction, model training and deployment, in particular, ML and DL based NIDS,\
    \ increase\ncomputational complexity. Thus designing an efﬁcient NIDS that is\
    \ light on computational\nrequirements is another challenge and area for future\
    \ research.\n•\nFeature selection and dimensionality reduction methods used for\
    \ proposed IDSs are suitable to\nwork on a speciﬁc type of normal trafﬁc and to\
    \ detect a particular type of attacks which may\nnot work once the environment\
    \ of normal or attack sequences change a bit, especially under a\nfast-changing\
    \ environment of IoT devices and networks. Thus, dynamic and computationally\n\
    efﬁcient mechanism for feature selection which can work under all types of normal\
    \ and attack\ntrafﬁc is a potential research challenge.\n•\nDL and ML-based techniques\
    \ and algorithms are being widely used for training a model on a\nlarge dataset.\
    \ This has facilitated in effective handling of cyber-attacks. However, with regards\n\
    to the use of DL and ML algorithms for attack detection in IoT networks, some\
    \ challenges need\nthe attention of researchers; for example, resource constraints\
    \ issue with IoT devices limits the\nuse of DL/ML algorithms [163] for protection\
    \ of IoT networks. Another challenge with the use\nElectronics 2020, 9, 1177\n\
    35 of 45\nof ML/DL techniques in large and distributed networks, like that of\
    \ IoT networks, is that they\nface scalability issues, for example in terms of\
    \ various scenarios and choices of IDS deployment.\nOne possible solution to limitations\
    \ of individual DL or ML algorithms suggested by some of the\nauthors [211] is\
    \ the use of an ensemble of ML/DL algorithms that performed better in comparison\n\
    to an individual ML algorithm; however, such algorithms were computationally expensive\
    \ and\nthus resulted in network latency issues, which cannot be afforded in critical\
    \ systems involving\nrisks to human lives, like health and autonomous or internet\
    \ of vehicles (IoVs) systems.\n•\nThe techniques of semi-supervised learning,\
    \ transfer learning and reinforcement learning (RL) are\nstill not well explored\
    \ and experimented for designing an IDS for IoT security in order to achieve\n\
    important objectives like real-time, fast training and uniﬁed models for anomaly\
    \ detection in IoT\nand thus are potential areas of future research. Moreover,\
    \ it would be an interesting research area\nto use RL in combination with DL because\
    \ their combined use can be beneﬁcial in IoT network\nscenarios involving large\
    \ data dimensionality and non-stationary environments.\n10. Conclusions\nDuring\
    \ the last decade, the use of IoT devices has increased exponentially in all walks\
    \ of life due\nto its capacity of converting objects from different application\
    \ areas into Internet hosts. At the same\ntime, users’ privacy and security are\
    \ threatened due to IoT security vulnerabilities. Therefore, there is\na requirement\
    \ to develop more robust security solutions for IoT. Machine and deep learning-based\n\
    IDS is one of the key techniques for IoT security. In this work, a survey of ML\
    \ and DL based\nIntrusion Detection techniques used in IDS for IoT networks and\
    \ systems is presented. The IoT\narchitecture, protocols, IoT systems vulnerabilities,\
    \ and IoT protocol-level attacks have been discussed\nin detail. Then, this paper\
    \ surveyed various research work available in the literature, which suggested\n\
    IDS methodology for IoT or proposed attack detection techniques for IoT that could\
    \ be part of an\nIDS, speciﬁcally about various ML and DL techniques available\
    \ for IDS in IoT and their use by the\nresearchers. Also, a review of various\
    \ datasets available for IoT security-related research is elaborated.\nThis work\
    \ attempts to provide the researchers with the summarized but comprehensive and\
    \ useful\ninsight into the various security challenges currently being faced by\
    \ IoT systems and networks and\npossible solutions, with a focus on intrusion\
    \ detection, based on ML and DL based methods.\nAuthor Contributions: Conceptualization,\
    \ J.A., N.M. and H.K.; methodology, J.A.; software, Not applicable;\nvalidation,\
    \ N.M., E.D. and J.A.; formal analysis, J.A., N.M. and A.W.; investigation, J.A.\
    \ and W.H.; resources, W.H.\nand A.W.; data curation, J.A. and W.H.; writing—original\
    \ draft preparation, J.A. and N.M.; writing—review and\nediting, J.A. and E.D.;\
    \ visualization, J.A. and H.K.; supervision, N.M. and H.K.; project administration,\
    \ A.W.;\nfunding acquisition, Not Applicable. All authors have read and agreed\
    \ to the published version of the manuscript.\nFunding: This research received\
    \ no external funding\nConﬂicts of Interest: The authors declare no conﬂict of\
    \ interest.\nReferences\n1.\nRay, S.; Jin, Y.; Raychowdhury, A. The changing computing\
    \ paradigm with internet of things: A tutorial\nintroduction. IEEE Des. Test 2016,\
    \ 33, 76–96. [CrossRef]\n2.\nDiechmann, J.; Heineke, K.; Reinbacher, T.; Wee,\
    \ D. The Internet of Things: How to Capture the Value of IoT.\nTechnical Report,\
    \ Technical Report May. 2018, pp. 1–124. Available online: https://www.mckinsey.com/featured-\n\
    insights/internet-of-things/our-insights/the-internet-of-things-how-to-capture-the-value-of-iot#\
    \ (accessed on\n13 July 2020).\n3.\nKhan, R.; Khan, S.U.; Zaheer, R.; Khan, S.\
    \ Future internet: The internet of things architecture, possible\napplications\
    \ and key challenges. In Proceedings of the 2012 10th International Conference\
    \ on Frontiers of\nInformation Technology, Islamabad, India, 17–19 December 2012;\
    \ pp. 257–260.\n4.\nYang, Z.; Yue, Y.; Yang, Y.; Peng, Y.; Wang, X.; Liu, W. Study\
    \ and application on the architecture and\nkey technologies for IoT. In Proceedings\
    \ of the 2011 International Conference on Multimedia Technology,\nHangzhou, China,\
    \ 26–28 July 2011; pp. 747–751.\nElectronics 2020, 9, 1177\n36 of 45\n5.\nAtzori,\
    \ L.; Iera, A.; Morabito, G. The internet of things: A survey. Comput. Netw. 2010,\
    \ 54, 2787–2805.\n[CrossRef]\n6.\nTorkaman, A.; Seyyedi, M. Analyzing IoT reference\
    \ architecture models. Int. J. Comput. Sci. Softw. Eng. 2016,\n5, 154.\n7.\nChaqfeh,\
    \ M.A.; Mohamed, N. Challenges in middleware solutions for the internet of things.\
    \ In Proceedings\nof the 2012 International Conference on Collaboration Technologies\
    \ And Systems (CTS), Denver, CO, USA,\n21–25 May 2012; pp. 21–26.\n8.\nMoustafa,\
    \ N.; Creech, G.; Sitnikova, E.; Keshk, M. Collaborative anomaly detection framework\
    \ for handling\nbig data of cloud computing. In Proceedings of the 2017 Military\
    \ Communications and Information Systems\nConference (MilCIS), Canberra, Australia,\
    \ 14–16 November 2017; pp. 1–6.\n9.\nMoustafa, N.; Choo, K.K.R.; Radwan, I.; Camtepe,\
    \ S. Outlier Dirichlet mixture mechanism: Adversarial\nstatistical learning for\
    \ anomaly detection in the fog. IEEE Trans. Inf. Forensics Secur. 2019, 14, 1975–1987.\n\
    [CrossRef]\n10.\nSicari, S.; Rizzardi, A.; Grieco, L.A.; Coen-Porisini, A. Security,\
    \ privacy and trust in Internet of Things:\nThe road ahead. Comput. Netw. 2015,\
    \ 76, 146–164. [CrossRef]\n11.\nKolias, C.; Kambourakis, G.; Stavrou, A.; Voas,\
    \ J. DDoS in the IoT: Mirai and other botnets. Computer 2017,\n50, 80–84. [CrossRef]\n\
    12.\nAl-Garadi, M.A.; Mohamed, A.; Al-Ali, A.; Du, X.; Guizani, M. A survey of\
    \ machine and deep learning\nmethods for internet of things (IoT) security. arXiv\
    \ 2018, arXiv:1807.11023.\n13.\nKolias, C.; Stavrou, A.; Voas, J.; Bojanova, I.;\
    \ Kuhn, R. Learning internet-of-things security “hands-on”.\nIEEE Secur. Priv.\
    \ 2016, 14, 37–46. [CrossRef]\n14.\nMarsden, T.; Moustafa, N.; Sitnikova, E.;\
    \ Creech, G. Probability risk identiﬁcation based intrusion detection\nsystem\
    \ for SCADA systems. In International Conference on Mobile Networks and Management;\
    \ Springer: Berlin,\nGermany, 2017; pp. 353–363.\n15.\nMoustafa, N.; Misra, G.;\
    \ Slay, J. Generalized outlier gaussian mixture technique based on automated\n\
    association features for simulating and detecting web application attacks. IEEE\
    \ Trans. Sustain. Comput. 2018.\n[CrossRef]\n16.\nModi, C.; Patel, D.; Borisaniya,\
    \ B.; Patel, H.; Patel, A.; Rajarajan, M. A survey of intrusion detection\ntechniques\
    \ in cloud. J. Netw. Comput. Appl. 2013, 36, 42–57. [CrossRef]\n17.\nRizwan, R.;\
    \ Khan, F.A.; Abbas, H.; Chauhdary, S.H. Anomaly detection in wireless sensor\
    \ networks using\nimmune-based bioinspired mechanism. Int. J. Distrib. Sens. Netw.\
    \ 2015, 11, 684952. [CrossRef]\n18.\nMoustafa, N.; Creech, G.; Slay, J. Anomaly\
    \ detection system using beta mixture models and outlier detection.\nIn Progress\
    \ in Computing, Analytics and Networking; Springer: Berlin, Germany, 2018; pp.\
    \ 125–135.\n19.\nButun, I.; Morgera, S.D.; Sankar, R. A survey of intrusion detection\
    \ systems in wireless sensor networks.\nIEEE Commun. Surv. Tutor. 2013, 16, 266–282.\
    \ [CrossRef]\n20.\nMitchell, R.; Chen, I.R. A survey of intrusion detection techniques\
    \ for cyber-physical systems. ACM Comput.\nSurv. (CSUR) 2014, 46, 55. [CrossRef]\n\
    21.\nMishra, A.; Nadkarni, K.; Patcha, A. Intrusion detection in wireless ad hoc\
    \ networks. IEEE Wirel. Commun.\n2004, 11, 48–60. [CrossRef]\n22.\nAnantvalee,\
    \ T.; Wu, J. A survey on intrusion detection in mobile ad hoc networks. In Wireless\
    \ Network\nSecurity; Springer: Berlin, Germany, 2007; pp. 159–180.\n23.\nKumar,\
    \ S.; Dutta, K. Intrusion detection in mobile ad hoc networks: Techniques, systems,\
    \ and future\nchallenges. Secur. Commun. Netw. 2016, 9, 2484–2556. [CrossRef]\n\
    24.\nSfar, A.R.; Natalizio, E.; Challal, Y.; Chtourou, Z. A roadmap for security\
    \ challenges in the Internet of Things.\nDigit. Commun. Netw. 2018, 4, 118–137.\
    \ [CrossRef]\n25.\nKeshk, M.; Moustafa, N.; Sitnikova, E.; Creech, G. Privacy\
    \ preservation intrusion detection technique for\nSCADA systems. In Proceedings\
    \ of the 2017 Military Communications and Information Systems Conference\n(MilCIS),\
    \ Canberra, Australia, 14–16 November 2017; pp. 1–6.\n26.\nZhao, K.; Ge, L. A\
    \ survey on the internet of things security. In Proceedings of the 2013 Ninth\
    \ International\nConference on Computational Intelligence and Security, Leshan,\
    \ China, 14–15 December 2013; pp. 663–667.\n27.\nKumar, J.S.; Patel, D.R. A survey\
    \ on internet of things: Security and privacy issues. Int. J. Comput. Appl.\n\
    2014, 90. [CrossRef]\nElectronics 2020, 9, 1177\n37 of 45\n28.\nSuo, H.; Wan,\
    \ J.; Zou, C.; Liu, J.\nSecurity in the internet of things: A review.\nIn Proceedings\
    \ of\nthe 2012 International Conference on Computer Science and Electronics Engineering,\
    \ Hangzhou, China,\n23–25 March 2012; Volume 3, pp. 648–651.\n29.\nKouicem, D.E.;\
    \ Bouabdallah, A.; Lakhlef, H. Internet of things security: A top-down survey.\
    \ Comput. Netw.\n2018, 141, 199–221. [CrossRef]\n30.\nBenkhelifa, E.; Welsh, T.;\
    \ Hamouda, W. A critical review of practices and challenges in intrusion detection\n\
    systems for IoT: Toward universal and resilient systems. IEEE Commun. Surv. Tutor.\
    \ 2018, 20, 3496–3509.\n[CrossRef]\n31.\nAbduvaliyev, A.; Pathan, A.S.K.; Zhou,\
    \ J.; Roman, R.; Wong, W.C. On the vital areas of intrusion detection\nsystems\
    \ in wireless sensor networks. IEEE Commun. Surv. Tutor. 2013, 15, 1223–1237.\
    \ [CrossRef]\n32.\nGranjal, J.; Monteiro, E.; Silva, J.S. Security for the internet\
    \ of things: A survey of existing protocols and\nopen research issues. IEEE Commun.\
    \ Surv. Tutor. 2015, 17, 1294–1312. [CrossRef]\n33.\nZarpelao, B.B.; Miani, R.S.;\
    \ Kawakani, C.T.; de Alvarenga, S.C. A survey of intrusion detection in Internet\
    \ of\nThings. J. Netw. Comput. Appl. 2017, 84, 25–37. [CrossRef]\n34.\nXiao, L.;\
    \ Wan, X.; Lu, X.; Zhang, Y.; Wu, D. IoT security techniques based on machine\
    \ learning. arXiv 2018,\narXiv:1801.06275.\n35.\nBuczak, A.L.; Guven, E. A survey\
    \ of data mining and machine learning methods for cyber security intrusion\ndetection.\
    \ IEEE Commun. Surv. Tutor. 2015, 18, 1153–1176. [CrossRef]\n36.\nMishra, P.;\
    \ Varadharajan, V.; Tupakula, U.; Pilli, E.S. A detailed investigation and analysis\
    \ of using machine\nlearning techniques for intrusion detection. IEEE Commun.\
    \ Surv. Tutor. 2018, 21, 686–728. [CrossRef]\n37.\nChaabouni, N.; Mosbah, M.;\
    \ Zemmari, A.; Sauvignac, C.; Faruki, P. Network Intrusion Detection for IoT\n\
    Security based on Learning Techniques. IEEE Commun. Surv. Tutor. 2019, 21, 2671–2701.\
    \ [CrossRef]\n38.\nLawal, M.A.; Shaikh, R.A.; Hassan, S.R. Security analysis of\
    \ network anomalies mitigation schemes in IoT\nnetworks. IEEE Access 2020, 8,\
    \ 43355–43374. [CrossRef]\n39.\nGarg, S.; Kaur, K.; Batra, S.; Kaddoum, G.; Kumar,\
    \ N.; Boukerche, A. A multi-stage anomaly detection scheme\nfor augmenting the\
    \ security in IoT-enabled applications. Future Gener. Comput. Syst. 2020, 104,\
    \ 105–118.\n[CrossRef]\n40.\nGarg, S.; Kaur, K.; Kumar, N.; Kaddoum, G.; Zomaya,\
    \ A.Y.; Ranjan, R. A hybrid deep learning-based model\nfor anomaly detection in\
    \ cloud datacenter networks. IEEE Trans. Netw. Serv. Manag. 2019, 16, 924–935.\n\
    [CrossRef]\n41.\nMirsky, Y.; Doitshman, T.; Elovici, Y.; Shabtai, A. Kitsune:\
    \ An ensemble of autoencoders for online network\nintrusion detection. arXiv 2018,\
    \ arXiv:1802.09089.\n42.\nSethi, P.; Sarangi, S.R. Internet of things: Architectures,\
    \ protocols, and applications. J. Electr. Comput. Eng.\n2017, 2017. [CrossRef]\n\
    43.\nWu, M.; Lu, T.J.; Ling, F.Y.; Sun, J.; Du, H.Y. Research on the architecture\
    \ of Internet of Things. In Proceedings\nof the 2010 3rd International Conference\
    \ on Advanced Computer Theory and Engineering (ICACTE),\nChengdu, China, 20–22\
    \ August 2010; Volume 5.\n44.\nTan, L.; Wang, N. Future internet: The internet\
    \ of things. In Proceedings of the 2010 3rd International\nConference on Advanced\
    \ Computer Theory and Engineering (ICACTE), Chengdu, China, 20–22 August 2010;\n\
    Volume 5.\n45.\nITU, T. Telecommunication Standardization Sector of ITU. Annex\
    \ C RTP Payload Format H 1993, 261, 108–113.\n46.\nWeyrich, M.; Ebert, C. Reference\
    \ architectures for the internet of things. IEEE Softw. 2015, 33, 112–116.\n[CrossRef]\n\
    47.\nFremantle, P. A Reference Architecture for the Internet of Things. WSO2 White\
    \ Paper. 2015. Available\nonline: https://docs.huihoo.com/wso2/wso2-whitepaper-a-reference-architecture-for-the-internet-of-\n\
    things.pdf (accessed on 13 July 2020).\n48.\nGreen, J. The Internet of Things\
    \ Reference Model; Internet of Things World Forum: Geneva, Switzerland, 2014;\n\
    pp. 1–12.\n49.\nNotra, S.; Siddiqi, M.; Gharakheili, H.H.; Sivaraman, V.; Boreli,\
    \ R. An experimental study of security\nand privacy risks with emerging household\
    \ appliances. In Proceedings of the 2014 IEEE Conference on\nCommunications and\
    \ Network Security, San Francisco, CA, USA, 29–31 October 2014; pp. 79–84.\n50.\n\
    Banerjee, A.; Venkatasubramanian, K.K.; Mukherjee, T.; Gupta, S.K.S.\nEnsuring\
    \ safety, security,\nand sustainability of mission-critical cyber–physical systems.\
    \ Proc. IEEE 2011, 100, 283–299. [CrossRef]\nElectronics 2020, 9, 1177\n38 of\
    \ 45\n51.\nAlTawy, R.; Youssef, A.M. Security tradeoffs in cyber physical systems:\
    \ A case study survey on implantable\nmedical devices. IEEE Access 2016, 4, 959–979.\
    \ [CrossRef]\n52.\nWamba, S.F.; Anand, A.; Carter, L. A literature review of RFID-enabled\
    \ healthcare applications and issues.\nInt. J. Inf. Manag. 2013, 33, 875–891.\
    \ [CrossRef]\n53.\nMalasri, K.; Wang, L.\nSecuring wireless implantable devices\
    \ for healthcare: Ideas and challenges.\nIEEE Commun. Mag. 2009, 47, 74–80. [CrossRef]\n\
    54.\nKeshk, M.; Sitnikova, E.; Moustafa, N.; Hu, J.; Khalil, I. An Integrated\
    \ Framework for Privacy-Preserving\nbased Anomaly Detection for Cyber-Physical\
    \ Systems. IEEE Trans. Sustain. Comput. 2019. [CrossRef]\n55.\nBertino, E.; Islam,\
    \ N. Botnets and internet of things security. Computer 2017, 50, 76–79. [CrossRef]\n\
    56.\nNeshenko, N.; Bou-Harb, E.; Crichigno, J.; Kaddoum, G.; Ghani, N. Demystifying\
    \ IoT security: An exhaustive\nsurvey on IoT vulnerabilities and a ﬁrst empirical\
    \ look on internet-scale IoT exploitations. IEEE Commun.\nSurv. Tutor. 2019, 21,\
    \ 2702–2733. [CrossRef]\n57.\nFaruki, P.; Bharmal, A.; Laxmi, V.; Ganmoor, V.;\
    \ Gaur, M.S.; Conti, M.; Rajarajan, M. Android security:\nA survey of issues,\
    \ malware penetration, and defenses. IEEE Commun. Surv. Tutor. 2014, 17, 998–1022.\n\
    [CrossRef]\n58.\nHuang, J.; Zhang, X.; Tan, L.; Wang, P.; Liang, B. Asdroid: Detecting\
    \ stealthy behaviors in android\napplications by user interface and program behavior\
    \ contradiction. In Proceedings of the 36th International\nConference on Software\
    \ Engineering, Hyderabad, India, 31 May–7 June 2014; pp. 1036–1046.\n59.\nAlaba,\
    \ F.A.; Othman, M.; Hashem, I.A.T.; Alotaibi, F. Internet of Things security:\
    \ A survey.\nJ. Netw.\nComput. Appl. 2017, 88, 10–28. [CrossRef]\n60.\nBekara,\
    \ C. Security issues and challenges for the IoT-based smart grid. Procedia Comput.\
    \ Sci. 2014, 34, 532–537.\n[CrossRef]\n61.\nSteinhubl, S.R.; Muse, E.D.; Topol,\
    \ E.J. The emerging ﬁeld of mobile health. Sci. Transl. Med. 2015, 7, 283rv3.\n\
    [CrossRef]\n62.\nStergiou, C.; Psannis, K.E.; Kim, B.G.; Gupta, B. Secure integration\
    \ of IoT and cloud computing. Future Gener.\nComput. Syst. 2018, 78, 964–975.\
    \ [CrossRef]\n63.\nLee, K.; Murray, D.; Hughes, D.; Joosen, W. Extending sensor\
    \ networks into the cloud using amazon web\nservices. In Proceedings of the 2010\
    \ IEEE International Conference on Networked Embedded Systems for\nEnterprise\
    \ Applications, Suzhou, China, 25–26 November 2010; pp. 1–7.\n64.\nBotta, A.;\
    \ De Donato, W.; Persico, V.; Pescapé, A. Integration of cloud computing and internet\
    \ of things:\nA survey. Future Gener. Comput. Syst. 2016, 56, 684–700. [CrossRef]\n\
    65.\nBhattasali, T.; Chaki, R.; Chaki, N. Secure and trusted cloud of things.\
    \ In Proceedings of the 2013 Annual\nIEEE India Conference (INDICON), Mumbai,\
    \ India, 13–15 December 2013; pp. 1–6.\n66.\nSubashini, S.; Kavitha, V. A survey\
    \ on security issues in service delivery models of cloud computing. J. Netw.\n\
    Comput. Appl. 2011, 34, 1–11. [CrossRef]\n67.\nZhou, W.; Jia, Y.; Peng, A.; Zhang,\
    \ Y.; Liu, P. The effect of IoT new features on security and privacy:\nNew threats,\
    \ existing solutions, and challenges yet to be solved. IEEE Internet Things J.\
    \ 2018, 6, 1606–1616.\n[CrossRef]\n68.\nRonen, E.; Shamir, A.; Weingarten, A.O.;\
    \ O’Flynn, C.\nIoT goes nuclear: Creating a ZigBee chain\nreaction. In Proceedings\
    \ of the 2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA,\n\
    22–26 May 2017; pp. 195–212.\n69.\nJing, Q.; Vasilakos, A.V.; Wan, J.; Lu, J.;\
    \ Qiu, D. Security of the Internet of Things: Perspectives and challenges.\nWirel.\
    \ Netw. 2014, 20, 2481–2501. [CrossRef]\n70.\nKarlof, C.; Sastry, N.; Wagner,\
    \ D. TinySec: A link layer security architecture for wireless sensor networks.\n\
    In Proceedings of the 2nd International Conference on Embedded Networked Sensor\
    \ Systems, Baltimore,\nMD, USA, 3–5 November 2004; pp. 162–175.\n71.\nGarg, S.;\
    \ Kaur, K.; Kaddoum, G.; Ahmed, S.H.; Jayakody, D.N.K. SDN-based secure and privacy-preserving\n\
    scheme for vehicular networks: A 5G perspective. IEEE Trans. Veh. Technol. 2019,\
    \ 68, 8421–8434. [CrossRef]\n72.\nPerera, C.; Zaslavsky, A.; Christen, P.; Georgakopoulos,\
    \ D. Context aware computing for the internet of\nthings: A survey. IEEE Commun.\
    \ Surv. Tutor. 2013, 16, 414–454. [CrossRef]\n73.\nAkyildiz, I.F.; Su, W.; Sankarasubramaniam,\
    \ Y.; Cayirci, E. A survey on sensor networks. IEEE Commun. Mag.\n2002, 40, 102–114.\
    \ [CrossRef]\nElectronics 2020, 9, 1177\n39 of 45\n74.\nAbdul-Ghani, H.A.; Konstantas,\
    \ D.; Mahyoub, M.\nA comprehensive IoT attacks survey based on a\nbuilding-blocked\
    \ reference model. Int. J. Adv. Comput. Sci. Appl. 2018, 9, 355–373.\n75.\nKhattab,\
    \ A.; Jeddi, Z.; Amini, E.; Bayoumi, M. RFID Security: A Lightweight Paradigm;\
    \ Springer: Berlin,\nGermany, 2016.\n76.\nFan, X.; Susan, F.; Long, W.; Li, S.\
    \ Security Analysis of Zigbee. 2017. Available online: https://courses.csail.\n\
    mit.edu/6.857/2017/project/17.pdf (accessed on 13 July 2020).\n77.\nLee, K.; Lee,\
    \ J.; Zhang, B.; Kim, J.; Shin, Y. An enhanced Trust Center based authentication\
    \ in ZigBee\nnetworks. In International Conference on Information Security and\
    \ Assurance; Springer: Berlin, Germany, 2009;\npp. 471–484.\n78.\nDini, G.; Tiloca,\
    \ M. Considerations on security in zigbee networks.\nIn Proceedings of the 2010\
    \ IEEE\nInternational Conference on Sensor Networks, Ubiquitous, and Trustworthy\
    \ Computing, Newport Beach,\nCA, USA, 7–9 June 2010; pp. 58–65.\n79.\nVidgren,\
    \ N.; Haataja, K.; Patino-Andres, J.L.; Ramirez-Sanchis, J.J.; Toivanen, P.\n\
    Security threats in\nZigBee-enabled systems: Vulnerability evaluation, practical\
    \ experiments, countermeasures, and lessons\nlearned. In Proceedings of the 2013\
    \ 46th Hawaii International Conference on System Sciences, Maui, HI,\nUSA, 7–10\
    \ January 2013; pp. 5132–5138.\n80.\nKolias, C.; Kambourakis, G.; Stavrou, A.;\
    \ Gritzalis, S. Intrusion detection in 802.11 networks: Empirical\nevaluation\
    \ of threats and a public dataset. IEEE Commun. Surv. Tutor. 2015, 18, 184–208.\
    \ [CrossRef]\n81.\nIEEE Computer Society LAN/MAN Standards Committee. IEEE Standard\
    \ for Information Technology-\nTelecommunication and\nInformation Exchange between\n\
    Systems-Local and\nMetropolitan\nArea\nNetworks-Speciﬁc Requirements Part11: Wireless\
    \ LAN Medium Access Control (MAC) and Physical\nLayer (PHY) Speciﬁcations Amendment1:\
    \ Radio Resource Measurement of Wireless LANs. 2009. Available\nonline: http://standards.ieee.org/getieee802/download/802.11n-2009.pdf\
    \ (accessed on 13 July 2020).\n82.\nBicakci, K.; Tavli, B. Denial-of-Service attacks\
    \ and countermeasures in IEEE 802.11 wireless networks.\nComput. Stand. Interfaces\
    \ 2009, 31, 931–941. [CrossRef]\n83.\nCope, P.; Campbell, J.; Hayajneh, T. An\
    \ investigation of Bluetooth security vulnerabilities. In Proceedings of\nthe\
    \ 2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC),\
    \ Las Vegas,\nNV, USA, 9–11 January 2017; pp. 1–7.\n84.\nLonzetta, A.M.; Cope,\
    \ P.; Campbell, J.; Mohd, B.J.; Hayajneh, T. Security vulnerabilities in Bluetooth\n\
    technology as used in IoT. J. Sens. Actuator Netw. 2018, 7, 28. [CrossRef]\n85.\n\
    Hassan, S.S.; Bibon, S.D.; Hossain, M.S.; Atiquzzaman, M. Security threats in\
    \ Bluetooth technology. Comput.\nSecur. 2018, 74, 308–322. [CrossRef]\n86.\nLiu,\
    \ Y.; Cheng, C.; Gu, T.; Jiang, T.; Li, X. A lightweight authenticated communication\
    \ scheme for smart grid.\nIEEE Sens. J. 2015, 16, 836–842. [CrossRef]\n87.\nSingh,\
    \ M.M.; Adzman, K.A.A.K.; Hassan, R. Near Field Communication (NFC) Technology\
    \ Security\nVulnerabilities and Countermeasures. Int. J. Eng. Technol. 2018, 7,\
    \ 298–305.\n88.\nRoland, M.; Langer, J.; Scharinger, J.\nSecurity vulnerabilities\
    \ of the NDEF signature record type.\nIn Proceedings of the 2011 Third International\
    \ Workshop on Near Field Communication, Hagenberg,\nAustria, 22 February 2011;\
    \ pp. 65–70.\n89.\nAmin, Y.M.; Abdel-Hamid, A.T. A comprehensive taxonomy and\
    \ analysis of IEEE 802.15. 4 attacks. J. Electr.\nComput. Eng. 2016, 2016, 4.\n\
    90.\nAmin, Y.M.; Abdel-Hamid, A.T.\nClassiﬁcation and analysis of IEEE 802.15.\n\
    4 PHY layer attacks.\nIn Proceedings of the 2016 International Conference on Selected\
    \ Topics in Mobile & Wireless Networking\n(MoWNeT), Cairo, Egypt, 11–13 April\
    \ 2016; pp. 1–8.\n91.\nAmin, Y.M.; Abdel-Hamid, A.T.\nClassiﬁcation and analysis\
    \ of IEEE 802.15.\n4 MAC layer attacks.\nIn Proceedings of the 2015 11th International\
    \ Conference on Innovations in Information Technology (IIT),\nDubai, UAE, 1–3\
    \ November 2015; pp. 74–79.\n92.\nMayzaud, A.; Badonnel, R.; Chrisment, I. A Taxonomy\
    \ of Attacks in RPL-based Internet of Things. Int. J.\nNetw. Secur. 2016, 18,\
    \ 459–473.\n93.\nCao, Z.; Hu, J.; Chen, Z.; Xu, M.; Zhou, X. Feedback: Towards\
    \ dynamic behavior and secure routing\nforwireless sensor networks. In Proceedings\
    \ of the 20th International Conference on Advanced Information\nNetworking and\
    \ Applications (AINA’06), Vienna, Austria, 18–20 April 2006; Volume 2, pp. 160–164.\n\
    94.\nSen, J. Security in wireless sensor networks. Wirel. Sens. Netw. Curr. Status\
    \ Future Trends 2012, 407, 53–57.\nElectronics 2020, 9, 1177\n40 of 45\n95.\n\
    Hummen, R.; Hiller, J.; Wirtz, H.; Henze, M.; Shafagh, H.; Wehrle, K. 6LoWPAN\
    \ fragmentation attacks and\nmitigation mechanisms. In Proceedings of the Sixth\
    \ ACM Conference on Security and Privacy in Wireless\nand Mobile Networks, Budapest,\
    \ Hungary, 17–19 April 2013; pp. 55–66.\n96.\nVacca, J.R. Computer and Information\
    \ Security Handbook; Steve Elliot: Sydney, Australia, 2012.\n97.\nKeshk, M.; Turnbull,\
    \ B.; Moustafa, N.; Vatsalan, D.; Choo, K.K.R.\nA Privacy-Preserving Framework\n\
    based Blockchain and Deep Learning for Protecting Smart Power Networks. IEEE Trans.\
    \ Ind. Inf. 2020,\n16, 5110–5118. [CrossRef]\n98.\nLiu, C.; Yang, J.; Chen, R.;\
    \ Zhang, Y.; Zeng, J. Research on immunity-based intrusion detection technology\
    \ for\nthe internet of things. In Proceedings of the 2011 Seventh International\
    \ Conference on Natural Computation,\nShanghai, China, 26–28 July 2011; Volume\
    \ 1, pp. 212–216.\n99.\nKasinathan, P.; Pastrone, C.; Spirito, M.A.; Vinkovits,\
    \ M. Denial-of-Service detection in 6LoWPAN based\nInternet of Things. In Proceedings\
    \ of the 2013 IEEE 9th International Conference on Wireless and Mobile\nComputing,\
    \ Networking and Communications (WiMob), Lyon, France, 7–9 October 2013; pp. 600–607.\n\
    100. Kasinathan, P.; Costamagna, G.; Khaleel, H.; Pastrone, C.; Spirito, M.A.\
    \ An IDS framework for internet of\nthings empowered by 6LoWPAN. In Proceedings\
    \ of the 2013 ACM SIGSAC Conference on Computer &\nCommunications Security, Berlin,\
    \ Germany, 4–8 November 2013; pp. 1337–1340.\n101. Oh, D.; Kim, D.; Ro, W. A malicious\
    \ pattern detection engine for embedded security systems in the Internet\nof Things.\
    \ Sensors 2014, 14, 24188–24211. [CrossRef]\n102. Keshk, M.; Moustafa, N.; Sitnikova,\
    \ E.; Turnbull, B. Privacy-preserving big data analytics for cyber-physical\n\
    systems. Wireless Netw. 2018, 2018, 1–9. [CrossRef]\n103. Debar, H. An introduction\
    \ to intrusion-detection systems. Proc. Connect 2000, 2000.\n104. Scarfone, K.;\
    \ Mell, P. Guide to Intrusion Detection and Prevention Systems (Idps); Technical\
    \ report; National\nInstitute of Standards and Technology: Gaithersburg, MA, USA,\
    \ 2012.\n105. Amaral, J.P.; Oliveira, L.M.; Rodrigues, J.J.; Han, G.; Shu, L.\
    \ Policy and network-based intrusion detection\nsystem for IPv6-enabled wireless\
    \ sensor networks. In Proceedings of the 2014 IEEE International Conference\n\
    on Communications (ICC), Sydney, Australia, 10–14 June 2014; pp. 1796–1801.\n\
    106. Raza, S.; Wallgren, L.; Voigt, T. SVELTE: Real-time intrusion detection in\
    \ the Internet of Things. Ad Hoc Netw.\n2013, 11, 2661–2674. [CrossRef]\n107.\
    \ D’Agostini, G. A multidimensional unfolding method based on Bayes’ theorem.\
    \ Nucl. Instrum. Methods\nPhys. Res. Sect. A Accel. Spectrometers Detect. Assoc.\
    \ Equip. 1995, 362, 487–498. [CrossRef]\n108. Panda, M.; Patra, M.R. Network intrusion\
    \ detection using naive bayes. Int. J. Comput. Sci. Netw. Secur. 2007,\n7, 258–263.\n\
    109. Mukherjee, S.; Sharma, N.\nIntrusion detection using naive Bayes classiﬁer\
    \ with feature reduction.\nProcedia Technol. 2012, 4, 119–128. [CrossRef]\n110.\
    \ Agrawal, S.; Agrawal, J. Survey on anomaly detection using data mining techniques.\
    \ Procedia Comput. Sci.\n2015, 60, 708–713. [CrossRef]\n111. Swarnkar, M.; Hubballi,\
    \ N. OCPAD: One class Naive Bayes classiﬁer for payload based anomaly detection.\n\
    Expert Syst. Appl. 2016, 64, 330–339. [CrossRef]\n112. Box, G.E.; Tiao, G.C. Bayesian\
    \ Inference in Statistical Analysis; John Wiley & Sons: Hoboken, NJ, USA, 2011;\n\
    Volume 40.\n113. Ng, A.Y.; Jordan, M.I. On discriminative vs. generative classiﬁers:\
    \ A comparison of logistic regression\nand naive bayes. In Advances in Neural\
    \ Information Processing Systems; 2002; pp. 841–848. Available online:\nhttps://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf\
    \ (accessed on 13 July 2020).\n114. Soucy, P.; Mineau, G.W. A simple KNN algorithm\
    \ for text categorization. In Proceedings of the 2001 IEEE\nInternational Conference\
    \ on Data Mining, San Jose, CA, USA, 29 November–2 December 2001; pp. 647–648.\n\
    115. Deng, Z.; Zhu, X.; Cheng, D.; Zong, M.; Zhang, S. Efﬁcient kNN classiﬁcation\
    \ algorithm for big data.\nNeurocomputing 2016, 195, 143–148. [CrossRef]\n116.\
    \ Adetunmbi, A.O.; Falaki, S.O.; Adewale, O.S.; Alese, B.K. Network intrusion\
    \ detection based on rough set\nand k-nearest neighbour. Int. J. Comput. ICT Res.\
    \ 2008, 2, 60–66.\n117. Li, L.; Zhang, H.; Peng, H.; Yang, Y. Nearest neighbors\
    \ based density peaks approach to intrusion detection.\nChaos Solitons Fractals\
    \ 2018, 110, 33–40. [CrossRef]\n118. Su, M.Y. Real-time anomaly detection systems\
    \ for Denial-of-Service attacks by weighted k-nearest-neighbor\nclassiﬁers. Expert\
    \ Syst. Appl. 2011, 38, 3492–3498. [CrossRef]\nElectronics 2020, 9, 1177\n41 of\
    \ 45\n119. Pajouh, H.H.; Javidan, R.; Khayami, R.; Ali, D.; Choo, K.K.R. A two-layer\
    \ dimension reduction and two-tier\nclassiﬁcation model for anomaly-based intrusion\
    \ detection in IoT backbone networks. IEEE Trans. Emerg.\nTop. Comput. 2016. [CrossRef]\n\
    120. Li, W.; Yi, P.; Wu, Y.; Pan, L.; Li, J. A new intrusion detection system\
    \ based on KNN classiﬁcation algorithm\nin wireless sensor network. J. Electr.\
    \ Comput. Eng. 2014, 2014. [CrossRef]\n121. Kotsiantis, S.B.; Zaharakis, I.; Pintelas,\
    \ P. Supervised machine learning: A review of classiﬁcation techniques.\nEmerg.\
    \ Artif. Intell. Appl. Comput. Eng. 2007, 160, 3–24.\n122. Du, W.; Zhan, Z. Building\
    \ decision tree classiﬁer on private data. In Proceedings of the IEEE International\n\
    Conference on Privacy, Security and Data Mining; Australian Computer Society,\
    \ Inc.: Sydney, Australia, 2002;\nVolume 14, pp. 1–8.\n123. Quinlan, J.R. Induction\
    \ of decision trees. Mach. Learn. 1986, 1, 81–106. [CrossRef]\n124. Kotsiantis,\
    \ S.B. Decision trees: A recent overview. Artif. Intell. Rev. 2013, 39, 261–283.\
    \ [CrossRef]\n125. Goeschel, K. Reducing false positives in intrusion detection\
    \ systems using data-mining techniques utilizing\nsupport vector machines, decision\
    \ trees, and naive Bayes for off-line analysis.\nIn Proceedings of the\nSoutheastCon\
    \ 2016, Norfolk, VA, USA, 30 March–3 April 2016; pp. 1–6.\n126. Kim, G.; Lee,\
    \ S.; Kim, S. A novel hybrid intrusion detection method integrating anomaly detection\
    \ with\nmisuse detection. Expert Syst. Appl. 2014, 41, 1690–1700. [CrossRef]\n\
    127. Alharbi, S.; Rodriguez, P.; Maharaja, R.; Iyer, P.; Subaschandrabose, N.;\
    \ Ye, Z. Secure the internet of things\nwith challenge response authentication\
    \ in fog computing. In Proceedings of the 2017 IEEE 36th International\nPerformance\
    \ Computing and Communications Conference (IPCCC), San Diego, CA, USA, 10–12 December\n\
    2017; pp. 1–2.\n128. Tong, S.; Koller, D. Support vector machine active learning\
    \ with applications to text classiﬁcation. J. Mach.\nLearn. Res. 2001, 2, 45–66.\n\
    129. Vapnik, V. The Nature of Statistical Learning Theory; Springer Science &\
    \ Business Media: Berlin, Germany, 2013.\n130. Miranda, C.; Kaddoum, G.; Bou-Harb,\
    \ E.; Garg, S.; Kaur, K. A collaborative security framework for\nsoftware-deﬁned\
    \ wireless sensor networks. IEEE Trans. Inf. Forensics Secur. 2020, 15, 2602–2615.\
    \ [CrossRef]\n131. Liu, Y.; Pi, D. A novel kernel svm algorithm with game theory\
    \ for network intrusion detection. KSII Trans.\nInternet Inf. Syst. 2017, 11.\
    \ [CrossRef]\n132. Hu, W.; Liao, Y.; Vemuri, V.R. Robust Support Vector Machines\
    \ for Anomaly Detection in Computer Security.\nICMLA. 2003; pp. 168–174. Available\
    \ online: https://web.cs.ucdavis.edu/~vemuri/papers/rvsm.pdf\n(accessed on 13\
    \ July 2020).\n133. Wagner, C.; François, J.; Engel, T.\nMachine learning approach\
    \ for ip-ﬂow record anomaly detection.\nIn International Conference on Research\
    \ in Networking; Springer: Berlin, Germany, 2011; pp. 28–39.\n134. Garg, S.; Kaur,\
    \ K.; Kaddoum, G.; Gagnon, F.; Kumar, N.; Han, Z. Sec-IoV: A multi-stage anomaly\
    \ detection\nscheme for internet of vehicles. In Proceedings of the ACM MobiHoc\
    \ Workshop on Pervasive Systems in the\nIoT Era, Catania, Italy, 2 July 2019;\
    \ pp. 37–42.\n135. Torres, J.M.; Comesaña, C.I.; García-Nieto, P.J. Machine learning\
    \ techniques applied to cybersecurity. Int. J.\nMach. Learn. Cybern. 2019, 10,\
    \ 2823–2836. [CrossRef]\n136. Ioannou, C.; Vassiliou, V.\nClassifying Security\
    \ Attacks in IoT Networks Using Supervised Learning.\nIn Proceedings of the 2019\
    \ 15th International Conference on Distributed Computing in Sensor Systems\n(DCOSS),\
    \ Santorini, Greece, 29–31 May 2019; pp. 652–658.\n137. Lin, K.C.; Chen, S.Y.;\
    \ Hung, J.C. Botnet detection using support vector machines with artiﬁcial ﬁsh\
    \ swarm\nalgorithm. J. Appl. Math. 2014, 2014. [CrossRef]\n138. Wo´zniak, M.;\
    \ Graña, M.; Corchado, E. A survey of multiple classiﬁer systems as hybrid systems.\
    \ Inf. Fusion\n2014, 16, 3–17. [CrossRef]\n139. Illy, P.; Kaddoum, G.; Moreira,\
    \ C.M.; Kaur, K.; Garg, S. Securing fog-to-things environment using intrusion\n\
    detection system based on ensemble learning. In Proceedings of the 2019 IEEE Wireless\
    \ Communications\nand Networking Conference (WCNC), Marrakesh, Morocco, 15–18\
    \ April 2019; pp. 1–7.\n140. Domingos, P.M. A few useful things to know about\
    \ machine learning. Commun. ACM 2012, 55, 78–87.\n[CrossRef]\n141. Zhang, H.;\
    \ Liu, D.; Luo, Y.; Wang, D. Adaptive Dynamic Programming for Control: Algorithms\
    \ And Stability;\nSpringer Science & Business Media: Berlin, Germany, 2012.\n\
    Electronics 2020, 9, 1177\n42 of 45\n142. Baba, N.M.; Makhtar, M.; Fadzli, S.A.;\
    \ Awang, M.K. Current Issues in Ensemble Methods and Its Applications.\nJ. Theor.\
    \ Appl. Inf. Technol. 2015, 81, 266.\n143. Santana, L.E.; Silva, L.; Canuto, A.M.;\
    \ Pintro, F.; Vale, K.O. A comparative analysis of genetic algorithm and\nant\
    \ colony optimization to select attributes for an heterogeneous ensemble of classiﬁers.\
    \ In Proceedings of\nthe IEEE Congress on Evolutionary Computation, Barcelona,\
    \ Spain, 18–23 July 2010; pp. 1–8.\n144. Aburomman, A.A.; Reaz, M.B.I. A novel\
    \ SVM-kNN-PSO ensemble method for intrusion detection system.\nAppl. Soft Comput.\
    \ 2016, 38, 360–372. [CrossRef]\n145. Gaikwad, D.; Thool, R.C. Intrusion detection\
    \ system using bagging ensemble method of machine learning.\nIn Proceedings of\
    \ the 2015 International Conference on Computing Communication Control and Automation,\n\
    Pune, India, 26–27 February 2015; pp. 291–295.\n146. Reddy, R.R.; Ramadevi, Y.;\
    \ Sunitha, K. Enhanced anomaly detection using ensemble support vector machine.\n\
    In Proceedings of the 2017 International Conference on Big Data Analytics and\
    \ Computational Intelligence\n(ICBDAC), Chirala, India, 23–25 March 2017; pp.\
    \ 107–111.\n147. Bosman, H.H.; Iacca, G.; Tejada, A.; Wörtche, H.J.; Liotta, A.\
    \ Ensembles of incremental learners to detect\nanomalies in ad hoc sensor networks.\
    \ Ad Hoc Netw. 2015, 35, 14–36. [CrossRef]\n148. Breiman, L. Random forests. Mach.\
    \ Learn. 2001, 45, 5–32. [CrossRef]\n149. Cutler, D.R.; Edwards, T.C., Jr.; Beard,\
    \ K.H.; Cutler, A.; Hess, K.T.; Gibson, J.; Lawler, J.J. Random forests for\n\
    classiﬁcation in ecology. Ecology 2007, 88, 2783–2792. [CrossRef] [PubMed]\n150.\
    \ Chang, Y.; Li, W.; Yang, Z. Network intrusion detection based on random forest\
    \ and support vector machine.\nIn Proceedings of the 2017 IEEE International Conference\
    \ on Computational Science and Engineering (CSE)\nand IEEE International Conference\
    \ on Embedded and Ubiquitous Computing (EUC), Guangzhou, China,\n21–24 July 2017;\
    \ Volume 1, pp. 635–638.\n151. Zhang, J.; Zulkernine, M.\nA hybrid network intrusion\
    \ detection technique using random forests.\nIn Proceedings of the First International\
    \ Conference on Availability, Reliability and Security (ARES’06),\nVienna, Austria,\
    \ 20–22 April 2006; p. 8.\n152. Doshi, R.; Apthorpe, N.; Feamster, N. Machine\
    \ learning ddos detection for consumer internet of things\ndevices. In Proceedings\
    \ of the 2018 IEEE Security and Privacy Workshops (SPW), San Francisco, CA, USA,\n\
    24 May 2018; pp. 29–35.\n153. Meidan, Y.; Bohadana, M.; Shabtai, A.; Ochoa, M.;\
    \ Tippenhauer, N.O.; Guarnizo, J.D.; Elovici, Y. Detection of\nunauthorized IoT\
    \ devices using machine learning techniques. arXiv 2017, arXiv:1709.04647.\n154.\
    \ Jain, A.K. Data clustering: 50 years beyond K-means. Pattern Recognit. Lett.\
    \ 2010, 31, 651–666. [CrossRef]\n155. Hartigan, J.A.; Wong, M.A. Algorithm AS\
    \ 136: A k-means clustering algorithm. J. R. Stat. Society. Ser. C\nAppl. Stat.\
    \ 1979, 28, 100–108. [CrossRef]\n156. Bhuyan, M.H.; Bhattacharyya, D.K.; Kalita,\
    \ J.K. Network anomaly detection: Methods, systems and tools.\nIEEE Commun. Surv.\
    \ Tutor. 2013, 16, 303–336. [CrossRef]\n157. Kanjanawattana, S. A Novel Outlier\
    \ Detection Applied to an Adaptive K-Means. Int. J. Mach. Learn. Comput.\n2019,\
    \ 9. [CrossRef]\n158. Muniyandi, A.P.; Rajeswari, R.; Rajaram, R. Network anomaly\
    \ detection by cascading k-Means clustering\nand C4. 5 decision tree algorithm.\
    \ Procedia Eng. 2012, 30, 174–182. [CrossRef]\n159. Zhao, S.; Li, W.; Zia, T.;\
    \ Zomaya, A.Y. A dimension reduction model and classiﬁer for anomaly-based\nintrusion\
    \ detection in internet of things. In Proceedings of the 2017 IEEE 15th International\
    \ Conference on\nDependable, Autonomic and Secure Computing, 15th International\
    \ Conference on Pervasive Intelligence\nand Computing, 3rd International Conference\
    \ on Big Data Intelligence and Computing and Cyber Science\nand Technology Congress\
    \ (DASC/PiCom/DataCom/CyberSciTech), Orlando, FL, USA, 6–10 November\n2017; pp.\
    \ 836–843.\n160. Hoang, D.H.; Nguyen, H.D. Detecting Anomalous Network Trafﬁc\
    \ in IoT Networks. In Proceedings of\nthe 2019 21st International Conference on\
    \ Advanced Communication Technology (ICACT), Pyeong Chang,\nKorea, 17–20 February\
    \ 2019; pp. 1143–1152.\n161. Hoang, D.H.; Nguyen, H.D. A PCA-based method for\
    \ IoT network trafﬁc anomaly detection. In Proceedings\nof the 2018 20th International\
    \ Conference on Advanced Communication Technology (ICACT), Chuncheon-si,\nKorea,\
    \ 11–14 February 2018; pp. 381–386.\n162. Zhang, B.; Liu, Z.; Jia, Y.; Ren, J.;\
    \ Zhao, X. Network intrusion detection method based on PCA and Bayes\nalgorithm.\
    \ Secur. Commun. Netw. 2018, 2018. [CrossRef]\nElectronics 2020, 9, 1177\n43 of\
    \ 45\n163. Moustafa, N.; Turnbull, B.; Choo, K.K.R. An ensemble intrusion detection\
    \ technique based on proposed\nstatistical ﬂow features for protecting network\
    \ trafﬁc of internet of things. IEEE Internet Things J. 2018,\n6, 4815–4830. [CrossRef]\n\
    164. Ahmad, A.; Dey, L. A k-mean clustering algorithm for mixed numeric and categorical\
    \ data. Data Knowl. Eng.\n2007, 63, 503–527. [CrossRef]\n165. Nweke, H.F.; Teh,\
    \ Y.W.; Al-Garadi, M.A.; Alo, U.R. Deep learning algorithms for human activity\
    \ recognition\nusing mobile and wearable sensor networks: State of the art and\
    \ research challenges. Expert Syst. Appl. 2018,\n105, 233–261. [CrossRef]\n166.\
    \ De Coninck, E.; Verbelen, T.; Vankeirsbilck, B.; Bohez, S.; Simoens, P.; Demeester,\
    \ P.; Dhoedt, B. Distributed\nneural networks for Internet of Things: The Big-Little\
    \ approach. In International Internet of Things Summit;\nSpringer: Berlin, Germany,\
    \ 2015; pp. 484–492.\n167. Youseﬁ-Azar, M.; Varadharajan, V.; Hamey, L.; Tupakula,\
    \ U. Autoencoder-based feature learning for cyber\nsecurity applications.\nIn\
    \ Proceedings of the 2017 International Joint Conference on Neural Networks\n\
    (IJCNN), Anchorage, AK, USA, 14–19 May 2017; pp. 3854–3861.\n168. Hinton, G.E.\
    \ A practical guide to training restricted Boltzmann machines. In Neural Networks:\
    \ Tricks of the\nTrade; Springer: Berlin, Germany, 2012; pp. 599–619.\n169. Hiromoto,\
    \ R.E.; Haney, M.; Vakanski, A. A secure architecture for IoT with supply chain\
    \ risk management.\nIn Proceedings of the 2017 9th IEEE International Conference\
    \ on Intelligent Data Acquisition and Advanced\nComputing Systems: Technology\
    \ and Applications (IDAACS), Bucharest, Romania, 21–23 September 2017;\nVolume\
    \ 1, pp. 431–435.\n170. Zhang, Q.; Yang, L.T.; Chen, Z.; Li, P. A survey on deep\
    \ learning for big data. Inf. Fusion 2018, 42, 146–157.\n[CrossRef]\n171. Li,\
    \ H.; Ota, K.; Dong, M. Learning IoT in edge: Deep learning for the Internet of\
    \ Things with edge computing.\nIEEE Netw. 2018, 32, 96–101. [CrossRef]\n172. Fadlullah,\
    \ Z.M.; Tang, F.; Mao, B.; Kato, N.; Akashi, O.; Inoue, T.; Mizutani, K. State-of-the-art\
    \ deep learning:\nEvolving machine intelligence toward tomorrow’s intelligent\
    \ network trafﬁc control systems. IEEE Commun.\nSurv. Tutor. 2017, 19, 2432–2455.\
    \ [CrossRef]\n173. LeCun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature 2015,\
    \ 521, 436–444. [CrossRef]\n174. Hermans, M.; Schrauwen, B. Training and analysing\
    \ deep recurrent neural networks. Adv. Neural Inf.\nProcess. Syst. 2013, 26, 190–198.\n\
    175. Pascanu, R.; Gulcehre, C.; Cho, K.; Bengio, Y. How to construct deep recurrent\
    \ neural networks. arXiv 2013,\narXiv:1312.6026.\n176. Torres, P.; Catania, C.;\
    \ Garcia, S.; Garino, C.G. An analysis of recurrent neural networks for botnet\
    \ detection\nbehavior. In Proceedings of the 2016 IEEE biennial congress of Argentina\
    \ (ARGENCON), Buenos Aires,\nArgentina, 15–17 June 2016; pp. 1–6.\n177. Almiani,\
    \ M.; AbuGhazleh, A.; Al-Rahayfeh, A.; Atiewi, S.; Razaque, A. Deep recurrent\
    \ neural network for\nIoT intrusion detection system. Simul. Model. Pract. Theory\
    \ 2019, 101, 102031. [CrossRef]\n178. Guo, T.; Xu, Z.; Yao, X.; Chen, H.; Aberer,\
    \ K.; Funaya, K. Robust Online Time Series Prediction with Recurrent\nNeural Networks.\
    \ In Proceedings of the 3Rd IEEE/Acm International Conference on Data Science\
    \ and\nAdvanced Analytics, (Dsaa 2016), Montreal, QC, Canada, 17–19 October 2016;\
    \ pp. 816–825.\n179. Qin, Y.; Song, D.; Chen, H.; Cheng, W.; Jiang, G.; Cottrell,\
    \ G. A dual-stage attention-based recurrent neural\nnetwork for time series prediction.\
    \ arXiv 2017, arXiv:1704.02971.\n180. Malhotra, P.; Vig, L.; Shroff, G.; Agarwal,\
    \ P. Long Short Term Memory Networks for Anomaly Detection in Time\nSeries; Presses\
    \ Universitaires de Louvain: Louvain-la-Neuve, Belgium, 2015; Volume 89, pp. 89–94.\n\
    181. Shipmon, D.T.; Gurevitch, J.M.; Piselli, P.M.; Edwards, S.T. Time series\
    \ anomaly detection; detection of\nanomalous drops with limited features and sparse\
    \ examples in noisy highly periodic data. arXiv 2017,\narXiv:1708.03665.\n182.\
    \ Bontemps, L.; McDermott, J.; Le-Khac, N.A. Collective anomaly detection based\
    \ on long short-term memory\nrecurrent neural networks. In International Conference\
    \ on Future Data and Security Engineering; Springer:\nBerlin, Germany, 2016; pp.\
    \ 141–152.\n183. Zhu, L.; Laptev, N.\nDeep and conﬁdent prediction for time series\
    \ at uber.\nIn Proceedings of the\n2017 IEEE International Conference on Data\
    \ Mining Workshops (ICDMW), New Orleans, LA, USA,\n18–21 November 2017; pp. 103–110.\n\
    Electronics 2020, 9, 1177\n44 of 45\n184. Goodfellow, I.; Bengio, Y.; Courville,\
    \ A. Deep Learning; The MIT Press: Cambridge, MA, USA, 2016.\n185. Chen, X.W.;\
    \ Lin, X. Big data deep learning: Challenges and perspectives. IEEE Access 2014,\
    \ 2, 514–525.\n[CrossRef]\n186. Ciresan, D.C.; Meier, U.; Masci, J.; Gambardella,\
    \ L.M.; Schmidhuber, J.\nFlexible, high performance\nconvolutional neural networks\
    \ for image classiﬁcation. In Proceedings of the Twenty-Second International\n\
    Joint Conference on Artiﬁcial Intelligence, Barcelona, Spain, 16–22 July 2011.\n\
    187. Scherer, D.; Müller, A.; Behnke, S. Evaluation of pooling operations in convolutional\
    \ architectures for\nobject recognition. In International Conference on Artiﬁcial\
    \ Neural Networks; Springer: Berlin, Germany, 2010;\npp. 92–101.\n188. Chen, Y.;\
    \ Zhang, Y.; Maharjan, S.\nDeep learning for secure mobile edge computing.\narXiv\
    \ 2017,\narXiv:1709.08025.\n189. McLaughlin, N.; Martinez del Rincon, J.; Kang,\
    \ B.; Yerima, S.; Miller, P.; Sezer, S.; Safaei, Y.; Trickel, E.;\nZhao, Z.; Doupé,\
    \ A.; et al.\nDeep android malware detection.\nIn Proceedings of the Seventh ACM\n\
    on Conference on Data and Application Security and Privacy, Scottsdale, AZ, USA,\
    \ 22–24 March 2017;\npp. 301–308.\n190. Wang, W.; Zhu, M.; Zeng, X.; Ye, X.; Sheng,\
    \ Y. Malware trafﬁc classiﬁcation using convolutional neural\nnetwork for representation\
    \ learning. In Proceedings of the 2017 International Conference on Information\n\
    Networking (ICOIN), Da Nang, Vietnam, 11–13 January 2017; pp. 712–717.\n191. Mohammadi,\
    \ M.; Al-Fuqaha, A.; Sorour, S.; Guizani, M. Deep learning for IoT big data and\
    \ streaming\nanalytics: A survey. IEEE Commun. Surv. Tutor. 2018, 20, 2923–2960.\
    \ [CrossRef]\n192. Mayuranathan, M.; Murugan, M.; Dhanakoti, V. Best features\
    \ based intrusion detection system by RBM\nmodel for detecting DDoS in cloud environment.\
    \ J. Ambient Intell. Hum. Comput. 2019, 1–11. [CrossRef]\n193. Fiore, U.; Palmieri,\
    \ F.; Castiglione, A.; De Santis, A. Network anomaly detection with the restricted\
    \ Boltzmann\nmachine. Neurocomputing 2013, 122, 13–23. [CrossRef]\n194. Hinton,\
    \ G.E.; Osindero, S.; Teh, Y.W. A fast learning algorithm for deep belief nets.\
    \ Neural Comput. 2006,\n18, 1527–1554. [CrossRef]\n195. Li, Y.; Ma, R.; Jiao,\
    \ R. A hybrid malicious code detection method based on deep learning. Int. J.\
    \ Secur.\nIts Appl. 2015, 9, 205–216. [CrossRef]\n196. Goodfellow, I.; Pouget-Abadie,\
    \ J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y.\n\
    Generative adversarial nets. Adv. Neural Inf. Process. Syst. 2014, 2, 2672–2680.\n\
    197. Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; Chen,\
    \ X. Improved techniques for\ntraining gans. Adv. Neural Inf. Process. Syst. 2016,\
    \ 2234–2242.\n198. Kuncheva, L.I. Combining Pattern Classiﬁers: Methods and Algorithms;\
    \ John Wiley & Sons: Hoboken, NJ,\nUSA, 2014.\n199. Tavallaee, M.; Bagheri, E.;\
    \ Lu, W.; Ghorbani, A.A. A detailed analysis of the KDD CUP 99 data set.\nIn Proceedings\
    \ of the 2009 IEEE Symposium on Computational Intelligence for Security and Defense\n\
    Applications, Ottawa, ON, Canada, 8–10 July 2009; pp. 1–6.\n200. McHugh, J. Testing\
    \ intrusion detection systems: A critique of the 1998 and 1999 darpa intrusion\
    \ detection\nsystem evaluations as performed by lincoln laboratory. ACM Trans.\
    \ Inf. Syst. Secur. (TISSEC) 2000, 3, 262–294.\n[CrossRef]\n201. Moustafa, N.;\
    \ Slay, J. The evaluation of Network Anomaly Detection Systems: Statistical analysis\
    \ of the\nUNSW-NB15 data set and the comparison with the KDD99 data set. Inf.\
    \ Secur. J. A Glob. Perspect. 2016,\n25, 18–31. [CrossRef]\n202. Stolfo, S.J.;\
    \ Fan, W.; Lee, W.; Prodromidis, A.; Chan, P.K. Cost-based modeling for fraud\
    \ and intrusion\ndetection: Results from the JAM project. In Proceedings of the\
    \ DARPA Information Survivability Conference\nand Exposition, DISCEX’00, Hilton\
    \ Head, SC, USA, 25–27 January 2000; Volume 2, pp. 130–144.\n203. Sharafaldin,\
    \ I.; Gharib, A.; Lashkari, A.H.; Ghorbani, A.A. Towards a reliable intrusion\
    \ detection benchmark\ndataset. Softw. Netw. 2018, 2018, 177–200. [CrossRef]\n\
    204. Shiravi, A.; Shiravi, H.; Tavallaee, M.; Ghorbani, A.A. Toward developing\
    \ a systematic approach to generate\nbenchmark datasets for intrusion detection.\
    \ Comput. Secur. 2012, 31, 357–374. [CrossRef]\n205. Nehinbe, J.O. A simple method\
    \ for improving intrusion detections in corporate networks. In International\n\
    Conference on Information Security and Digital Forensics; Springer: Berlin, Germany,\
    \ 2009; pp. 111–122.\nElectronics 2020, 9, 1177\n45 of 45\n206. Bhuyan, M.H.;\
    \ Bhattacharyya, D.K.; Kalita, J.K. Towards Generating Real-life Datasets for\
    \ Network Intrusion\nDetection. IJ Netw. Secur. 2015, 17, 683–701.\n207. Sharafaldin,\
    \ I.; Lashkari, A.H.; Ghorbani, A.A. Toward Generating a New Intrusion Detection\
    \ Dataset\nand Intrusion Trafﬁc Characterization. In Proceedings of the 4th International\
    \ Conference on Information\nSystems Security and Privacy, ICISSP, Funchal, Portugal,\
    \ 22–24 January 2018; pp. 108–116. Available online:\nhttps://www.scitepress.org/Papers/2018/66398/66398.pdf\
    \ (accessed on 13 July 2020).\n208. Moustafa, N.; Slay, J. UNSW-NB15: A comprehensive\
    \ data set for network intrusion detection systems\n(UNSW-NB15 network data set).\
    \ In Proceedings of the 2015 Military Communications and Information\nSystems\
    \ Conference (MilCIS), Canberra, Australia, 10–12 November 2015; pp. 1–6.\n209.\
    \ Koroniotis, N.; Moustafa, N.; Sitnikova, E.; Turnbull, B. Towards the development\
    \ of realistic botnet dataset\nin the internet of things for network forensic\
    \ analytics: Bot-IoT dataset. Future Gener. Comput. Syst. 2019,\n100, 779–796.\
    \ [CrossRef]\n210. Pa, Y.M.P.; Suzuki, S.; Yoshioka, K.; Matsumoto, T.; Kasama,\
    \ T.; Rossow, C. IoTPOT: Analysing the rise of\nIoT compromises. In Proceedings\
    \ of the 9th USENIX Workshop on Offensive Technologies (WOOT 15),\nWashington,\
    \ DC, USA, 10–11 August 2015.\n211. Xiao, L.; Wan, X.; Lu, X.; Zhang, Y.; Wu,\
    \ D. IoT security techniques based on machine learning: How do IoT\ndevices use\
    \ AI to enhance security? IEEE Signal Process. Mag. 2018, 35, 41–49. [CrossRef]\n\
    c⃝ 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an\
    \ open access\narticle distributed under the terms and conditions of the Creative\
    \ Commons Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Electronics
  limitations: '>'
  pdf_link: https://www.mdpi.com/2079-9292/9/7/1177/pdf
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Review of Intrusion Detection Systems Using Machine and Deep Learning
    in Internet of Things: Challenges, Solutions and Future Directions'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/tvcg.2019.2934614
  analysis: '>'
  authors:
  - Luke S. Snyder
  - Yi-Hsien Lin
  - Morteza Karimzadeh
  - Dan Goldwasser
  - David S. Ebert
  citation_count: 14
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences Loading
    [MathJax]/extensions/MathMenu.js IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More
    Sites Donate Cart Create Account Personal Sign In Browse My Settings Help Access
    provided by: University of Nebraska - Lincoln Sign Out All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Transactions on Visualiz... >Volume: 26 Issue: 1 Interactive Learning for
    Identifying Relevant Tweets to Support Real-time Situational Awareness Publisher:
    IEEE Cite This PDF Luke S. Snyder; Yi-Shan Lin; Morteza Karimzadeh; Dan Goldwasser;
    David S. Ebert All Authors 16 Cites in Papers 1350 Full Text Views Abstract Document
    Sections 1 Introduction 2 Related Work 3 Interactive Learning Framework 4 SMART
    2.0 5 User Experience Show Full Outline Authors Figures References Citations Keywords
    Metrics Media Abstract: Various domain users are increasingly leveraging real-time
    social media data to gain rapid situational awareness. However, due to the high
    noise in the deluge of data, effectively determining semantically relevant information
    can be difficult, further complicated by the changing definition of relevancy
    by each end user for different events. The majority of existing methods for short
    text relevance classification fail to incorporate users'' knowledge into the classification
    process. Existing methods that incorporate interactive user feedback focus on
    historical datasets. Therefore, classifiers cannot be interactively retrained
    for specific events or user-dependent needs in real-time. This limits real-time
    situational awareness, as streaming data that is incorrectly classified cannot
    be corrected immediately, permitting the possibility for important incoming data
    to be incorrectly classified as well. We present a novel interactive learning
    framework to improve the classification process in which the user iteratively
    corrects the relevancy of tweets in real-time to train the classification model
    on-the-fly for immediate predictive improvements. We computationally evaluate
    our classification model adapted to learn at interactive rates. Our results show
    that our approach outperforms state-of-the-art machine learning models. In addition,
    we integrate our framework with the extended Social Media Analytics and Reporting
    Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework
    within a visual analytics system tailored for real-time situational awareness.
    To demonstrate our framework''s effectiveness, we provide domain expert feedback
    from first responders who used the extended SMART 2.0 system. Published in: IEEE
    Transactions on Visualization and Computer Graphics ( Volume: 26, Issue: 1, January
    2020) Page(s): 558 - 568 Date of Publication: 20 August 2019 ISSN Information:
    PubMed ID: 31442995 DOI: 10.1109/TVCG.2019.2934614 Publisher: IEEE Funding Agency:
    SECTION 1 Introduction Social media data has been used extensively in a variety
    of applications and research endeavors due to its ability to provide useful information
    on the public''s opinions and behavior. Analysts in various domains are increasingly
    using social media to gain rapid situational awareness. For instance, first responders
    are leveraging Twitter data to obtain actionable information for crisis response
    and prevention (see [29] for an extensive list of literature on this subject).
    However, the vast amounts of unstructured text make the identification of relevant
    information nontrivial, limiting situational awareness. This issue is further
    compounded by changes in topics of interest (to end users) over time, since the
    computational models built to determine relevant information for one event or
    one user group may not apply to other events or other user groups due to variations
    in diction, word structure, or user expectations. Fig. 1. Our interactive learning
    framework allows users to train text relevance classifiers in real-time to improve
    situational awareness. In this example, a real-time tweet regarding a car accident
    is incorrectly classified as “irrelevant”. Through the SMART 2.0 interface, the
    user can view its label and correct it to “relevant”, thereby retraining and improving
    the classifier for incoming streaming data. Show All Several classification approaches
    have been developed to identify relevant and irrelevant social media information,
    such as clustering [5], [6], keyword matching [45], and term-vector similarity
    [12]. However, to the best of our knowledge, no existing work in this area includes
    interactive learning with real-time data, focusing instead on improving the machine
    learning algorithms themselves [5], [16], [23], [31], [33], [38], [45], [46],
    [54] or interactively training on archived datasets [9], [18]. Continuing on our
    example of first responders, a pre-trained classifier may not fulfill first responders''
    varying needs, since one first responder may be interested in monitoring road
    closures, and another one might be interested in identifying disinformation and
    misinformation on social media in order to take counter-action. Ultimately, first
    responders'' definition of relevancy will depend on the situation at hand, which
    can vary over time. Interactively training classifiers through iterative user
    labeling can alleviate this problem. In this paper, we present a novel interactive
    framework in which the user iteratively (re)labels the relevancy of streaming
    social media data to adaptively train the underlying model to match their needs
    for improved situational awareness. We compare three different types of neural
    networks in terms of classification performance and computational efficiency for
    real-time learning. Furthermore, we optimize and computationally evaluate the
    selected models by simulating the real-time user feedback on several crisis-related
    datasets. Our results show that our interactive model outperforms state-of-the-art
    machine learning-based classification models. To incorporate our evaluated models
    into a working application, we extend an existing visual analytics system tailored
    for situational awareness called the Social Media Analytics and Reporting Toolkit
    (SMART) [52], [53], which has been successfully used by many first responder groups
    in the United States. SMART allows users to interactively explore trending topics
    on social media through integrated topic modeling and spatial, temporal, and textual
    visualizations. We call the newly extended system SMART 2.0, which incorporates
    our interactive learning framework to address the needs raised by the aforementioned
    first responder users and reduce noise in the incoming stream of data. Finally,
    we present domain-expert feedback on the usefulness of our approach as experienced
    by multiple first responders who used SMART 2.0 for crisis-related use cases.
    In addition, we include two usage scenarios of the system to illustrate its application
    to real-life situations. Overall, the major contributions of this paper are as
    follows: We present a novel interactive learning framework for classification
    of streaming text data. We compare three different types of neural networks in
    terms of performance and computational efficiency, and tune the models for learning
    at interactive rates. We further computationally evaluate the selected model on
    several disaster-related datasets. We integrate our models in SMART 2.0, a visual
    analytics application for situational awareness, and present user feedback obtained
    from domain experts using the system for crisis events. In the remainder of the
    paper, we discuss related work in section 2, the design of the framework and model
    in section 3, SMART 2.0 in section 4, evaluation of our framework in section 5,
    discussion and future work in section 6, and concluding remarks in section 7.
    SECTION 2 Related Work 2.1 Short Text Classification Researchers have presented
    many techniques to classify text documents into categories such as sentiment or
    topics [17], [35], [37], [49]. However, classifying short text, e.g. social media
    posts, is more challenging due to the lack of contextual information and loose
    adherence to standard grammar. To tackle the brevity of short text, auxiliary
    resources such as external corpora [10] or knowledge bases [20], or methods such
    as term frequency-inverse document frequency (TF-IDF) [16], have been proposed
    for improving classification. Representing words as n-dimensional vectors (i.e.
    word embedding) has become increasingly prevalent, since vectors can be used as
    inputs to machine learning models for finding semantic similarities [47], [50].
    In particular, Google''s Word2Vec [30] has been employed extensively in classification
    tasks [4], [26], [27], [32], [33] due to its impressive ability in capturing linguistic
    regularities and semantics. For instance, words frequently used together are likely
    to be closer in the Word2vec vector space than words that are not, and vector
    operations reveal meaningful semantics (e.g., the vector “King” – “Man” + “Woman”
    is close to the vector “Queen” [30]). Since pre-trained Word2vec models encode
    embeddings learned from larger web corpora, they have been increasingly used in
    short text classification tasks [4], [32], [33], [44]. Neural networks have generated
    state-of-the-art results in recent years for text classification problems [30],
    [32], [33], [43] and have also been used with Word2Vec [32], [33], [44]. Neural
    networks are well-suited for online learning processes in which training data
    is supplied iteratively since they can learn adaptively from new data [32], [33].
    Nguyen et al. [33] presented a convolutional neural network with Word2Vec that
    outperformed non-neural classifiers, and Nguyen et al. [32] proposed a new online
    learning classification algorithm for deep neural networks utilizing the log-loss
    and gradient of sequential training batches. Their methods were evaluated with
    disaster-related datasets. However, these methods were not adapted to user-guided
    learning in which time constraints are essential and the provided batches may
    be small. In particular, the online learning method designed by Nguyen et al.
    [32] was evaluated with batch sizes of 200. In our work, we assume the user needs
    to train with flexibly interactive amounts of data (10–20 samples) to view immediate
    predictive improvements for situational awareness. Classification for Situational
    Awareness Utilizing real-time social media data for situational awareness (and
    crisis prevention in particular) is a heavily researched topic [16], [23], [31],
    [33], [38], [45], [54]. However, identifying situationally-relevant information
    is nontrivial due to the high noise-to-signal ratio. Karimi et al. [23] found
    that classification methods, such as Support Vector Machine and multinomial Naïve
    Bayes, can identify disaster-related tweets, although generic features such as
    hashtag count and tweet length are preferable so that the model does not learn
    relevancy only for a specific disaster. Researchers have used clustering [5],
    [6], [46] or enhanced keyword matching [45] to detect relevant crisis and event
    information, and provided human-annotated Twitter corpora that can be used to
    train word embedding models [21]. Nazer et al. [31] developed a system to detect
    requests for help by utilizing both tweet context (e.g., geotag) and content (e.g.,
    URLs). Rudra et al. [38] designed a novel classification-summarization framework
    to classify disaster-related tweets, and then summarize the tweets by exploiting
    linguistic properties typical of disaster tweets (e.g., combinations of situational
    and non-situational information). Zoppi et al. [54] provided a relevance labeling
    strategy for crisis management that computed data relevance as a function of the
    data''s integrity (e.g., are the geo-coordinates incorrect?), statistical properties
    (e.g., can we select a subset of the data that are geographically close?), and
    clustering (e.g., what groups are present in the data?). Toriumi et al. [46] clustered
    tweets based on their retweet count in real-time to extract important topics and
    classify tweets accordingly. The methods discussed so far, however, lack user
    interactivity. In particular, these classification methods are inflexible to user-dependent
    needs that change over time as new situations and events occur. As such, their
    practical use for real-time situational awareness is limited. 2.2 Visual Analytics
    and Interactive Learning for Situational Awareness Researchers have presented
    a number of visual analytics (VA) solutions for situational awareness. Diakopoulos
    et al. [12] developed Vox Civitas, a VA application for journalistic analysis
    and user-guided filtering using social media content. Vox Civitas filters out
    unrelated data by automatically computing time-dependent term-vector similarities.
    Twit-Info [28] aggregates streamed Twitter data and automatically discovers events
    from activity peaks in real-time. The authors assign relevance to a tweet by counting
    its number of event-related keywords. Pezanowski et al. [36] designed the geovisual
    analytics system SensePlace3 to provide situational awareness by leveraging geographical
    information and place-time-theme indexing with string-based queries for exploring
    datasets. SensePlace3 primarily relies on TF-IDF for tweet retrieval in response
    to user queries. However, these tools do not employ machine learning for relevance
    classification and do not integrate user feedback to improve their underlying
    models or algorithms. Visual analytics has also been increasingly used to improve
    various machine learning processes, such as feature selection [13], attribute
    weighting [48], and labeling [8], [9], [18], and even understanding the models
    themselves [22], [39], [42]. Sacha et al. [40] proposed a framework to discuss
    the various forms of human interaction with machine learning models in visual
    analytics systems and theorized that VA tools could increase knowledge and usability
    of machine learning components. Endert et al. [15] designed a system that classifies
    archived documents through user-guided semantic interactions (e.g., moving a document
    to another group) that improve the underlying model. Our work is based on the
    same idea in that we intend to improve model performance through user feedback,
    but with real-time social media data. Heimerl et al. [18] analyzed three separate
    methods for user-guided classification of a set of archived text documents: the
    basic method, which does not employ sophisticated visuals; the visual method,
    which visually represents the labeled and unlabeled documents for user exploration;
    and the user-driven method, which provides the user with full control over the
    labeling process. The first two methods employ active learning, in which the model
    selects a data sample to be labeled by the user that most effectively helps it
    distinguish relevant from irrelevant data. This contrasts with the user deciding
    which instances they wish to label. The authors did not find any statistically
    significant differences in terms of F 1 score between the methods in their user
    study. Bosch et al. [9] developed ScatterBlogs2, a VA application that provides
    user-guided learning of filter classifiers on historical social media messages
    to support situational awareness. These two works are perhaps the most similar
    to ours, yet differ in two fundamental ways. First, they do not provide interactive
    learning in real-time, which strains the user, as they are required to visit historical
    data for additional training. Second, they do not employ neural networks, which
    are better suited for online learning environments, such as social media streaming,
    in which training data is supplied sequentially over time [32], [33]. It is important
    to note that Bosch et al. [9] allow the user to adjust a filter''s focus (i.e.,
    how precise the classification is) in real-time if it misses relevant data or
    does not sufficiently filter out irrelevant data. However, this could indicate
    that the model has not properly learned the distinction between relevant and irrelevant
    data. Since training can only be completed with historical posts, the user is
    unable to update the model immediately with the streamed data, limiting situational
    awareness. Our approach not only solves this issue by allowing the user to immediately
    train the model for improvement, but also provides the user with the ability to
    create classifiers on-the-fly to accommodate their real-time needs. SECTION 3
    Interactive Learning Framework Our framework for interactively learning relevant
    social media posts in real-time consists of two primary components. The first
    is a formalized set of design goals necessary to effectively facilitate situational
    awareness in real-time through user interactivity. The second is a detailed underlying
    model that is adapted to user-guided training with real-time streaming data. In
    section 4, we discuss our implementation of the framework that realizes the design
    goals. 3.1 Design Goals The framework''s design goals were iteratively defined
    through discussions with domain experts such as first responders who frequently
    use visual analytic social media applications for real-time situational awareness.
    In general, these experts found it necessary for the interactive framework to
    incorporate user feedback in a timely manner, as well as account for time and
    situation-dependent user needs. With their feedback, the following specific design
    goals were established: DG1 Filter and view relevant data: Filtering data by relevancy
    removes noisy data, allowing the user to more quickly find data that may require
    immediate attention or contain important information. The ability to view the
    relevant data itself is equally important for determining the urgency and content
    of relevant data. DG2 Correct incorrect classifications: Since classifiers may
    provide incorrect results, especially during the early stages of training, it
    is necessary for the user to be able to correct the label in realtime. This both
    improves the model''s performance and lowers the likelihood that incoming streamed
    data will be incorrectly classified and missed. DG3 Create new classifiers in
    real-time: The needs of the user can change dramatically over time and vary across
    users themselves. As an example, one user may wish to train a classifier to find
    data related to a specific hurricane event to expedite identification of people
    in desperate need of assistance. However, another user may wish to find data related
    to safety in general, not just a hurricane. As such, they should each be able
    to create and train their own classifiers in real-time specific to their needs
    at the time. DG4 Minimize model training time: Although it is important to design
    a high-performing model, time constraints are equally important. Specifically,
    when the model is trained by user feedback, the user should not have to wait for
    several minutes for the model to be retrained and relabel data. Previously streamed
    data labels may update with retraining, allowing the user to potentially find
    important information that they had not seen before. As such, it is necessary
    to provide these updated results as quickly as possible for real-time situational
    awareness. 3.2 Workflow Fig. 2 shows the three primary components of our framework''s
    workflow applied to streaming tweets (however, the framework can be generalized
    to other kinds of text). First, as tweets are streamed in real-time, they are
    vectorized using a word embedding model. Second, the vectorized tweets are provided
    as inputs to the neural network classifier (discussed in next section), which
    outputs a set of probabilities from the activation function of the tweet''s predicted
    relevancy and assigns an unverified relevance label. Third, the labeled tweet
    is relayed to the user through the user interface. If the user identifies tweets
    with incorrect labels, they can correct the label for the system to retrain and
    improve the model for relevance predictions. Fig. 2. High-level workflow of our
    framework with three main components: Tweet vectorization, tweet classification,
    and user feedback. Show All 3.3 Interactive Model Details In the following subsections,
    we elaborate on the underlying representations and models used to support our
    interactive learning framework. We design, optimize, and evaluate our approach
    with the key assumption that classifiers are trained (from scratch) in real-time
    using user-provided labels for streaming text. We simulate this process by adding
    training examples in small batches of 10 and evaluating against testing data,
    as explained below. All simulations were completed on a server with 128 GB RAM,
    32 TB of disk storage, and 2 Intel(R) Xeon(R) E5-2640 v4 CPUs at 2.40GHz. 3.3.1
    Model Candidates Selecting the underlying model for our framework was a key task,
    as it must be efficiently trainable with a continual stream of user-labeled data
    (DG4). As discussed in Section 2, neural networks are a natural choice for online
    learning scenarios in which training data is supplied sequentially over time [32],
    [33]. In addition, neural networks have generated impressive results with Word2Vec
    [30] embeddings [32], [33], [44]. Therefore, we employ a neural network as our
    classifier to determine text relevance based on real-time training examples provided
    by the user. To convert the text into vector inputs (of our neural network), we
    use word embeddings generated by Google''s Word2Vec skip-gram model [3], [30],
    which contains 3 million 300-dimensional word vectors pre-trained (and therefore,
    capturing word embeddings) on a subset of the Google News dataset with approximately
    1 billion words. In selecting the specific neural network model type, we experimented
    with the well-known Convolutional Neural Network (CNN) [25], Long-Short Term Memory
    (LSTM) Neural Network [19], and Recurrent Neural Network (RNN) [14] since they
    have performed well in various text classification tasks [51]. Hybrid architectures,
    such as recurrent convolutional neural networks [24], have also been proposed
    in recent years, but have not been made available in well-supported libraries.
    Therefore, we did not consider them in this paper, since our goal was to also
    support a well-tested SMART 2.0 system for end users. Our CNN model contains the
    traditional convolutional and max-pooling layers before activation [51]. Specifically,
    we apply a 1-dimensional convolutional layer, 1-dimensional max-pooling layer,
    flatten the output, and then activate it with softmax and a dense layer. The filter
    and kernel sizes of the convolutional layer are optimized during the hyperparameter
    stage (explained in Section 3.3.4). We use Hochreiter''s LSTM [19] and the traditional
    RNN [14] architectures as provided by Keras [2]. The LSTM and RNN hidden layer
    each contain 300 hidden neurons and use softmax activation. 3.3.2 Design As mentioned
    before, to enable the use of neural networks for classifying text, we convert
    the unstructured text (of the tweets) into vectors ready for consumption by the
    neural network. When using Word2Vec vectors as features for classification, a
    common approach is to convert each word in the sentence to its vector, average
    the word vectors in the sentence, and then use the resulting feature vector for
    model training [4], [43]. However, averaging the vectors results in the loss of
    syntactic information, which can negatively impact classification results [27].
    As an example, the two sentences “Only Mary will attend the ceremony.” and “Mary
    will only attend the ceremony.” would generate identical averaged sentence vectors
    since they contain the same set of words, but they differ in meaning. Therefore,
    to capture both semantic and syntactic information, we represent a sentence as
    a matrix where each row i is a 300-dimensional Word2Vec vector corresponding to
    word i in the original sentence. The input to the neural network consists of the
    matrix representing the sentence (as described above) and the output consists
    of the classification labels for the input sentence (Fig. 2). Specifically, we
    allow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can''t Decide. The
    label with the highest probability from the activation function corresponds to
    the final label given to it. The “Can''t Decide” label indicates that the tweet
    may or may not be relevant depending on the context. This is useful if the user
    finds a social media post such as “Remembering when Hurricane Irma destroyed my
    home…” that may not directly relate to the current event, but may be semantically
    relevant, and the user does not want to mark such cases as “Not Relevant”. This
    gives the user more flexibility to accommodate their needs since the definition
    of relevancy will depend on both the user and the situation. 3.3.3 Corpus for
    Model Selection and Optimization To experiment with different neural network model
    types and optimize the selected model, we used a disaster-related corpus annotated
    on the crowd-sourcing platform, Figure Eight [1]. The dataset contains 10,876
    tweets related to different types of disaster events, such as hurricanes and automobile
    accidents. The data was collected using keywords such as “ablaze” or “quarantine”,
    and therefore, covers a wide variety of disaster-related topics. Our main motivation
    for using this open dataset is its size (as well as topical relevance), enabling
    the optimization of hyperparameters and comparison of various models. In the corpus,
    each tweet is manually labeled by Figure Eight''s workers as “Relevant”, “Not
    Relevant”, or “Can''t Decide”, and the distribution of labels is unbalanced. Specifically,
    there are 4,673 “Relevant” instances, 6,187 “Not Relevant” instances, and 16 “Can''t
    Decide” instances. This dataset has been used in other tweet classification research
    projects [45]. However, the researchers of that study remove the tweets with the
    “Can''t Decide” label to improve training data quality. As explained in the previous
    section, we find the “Can''t Decide” option useful for users to apply to cases
    with insufficient context for relevance determination. We randomly shuffle the
    data and divide the dataset into 80% training, 10% validation, and 10% testing
    sets. It is important to note that we only use the Figure Eight dataset to optimize
    the hyperparameters and provide an initial evaluation of the model by simulating
    the provision of labels in real-time by the user. Since each tweet in the dataset
    contains true labels that were manually assigned by humans, it allows us to evaluate
    the model performance by comparing the model''s predictions to the true labels
    after each training iteration. Our proposed approach as well as its integration
    within the SMART 2.0 system, however, allows for the creation of the models from
    scratch (with no prior training) (DG3), leveraging real-time labels provided by
    users on streaming data for training. 3.3.4 Optimization In order to experiment
    with the different neural network model types, we ran several training simulations
    with random combinations of hyperparameters (i.e., random grid search) to see
    which model converged to the best F 1 score. The F 1 score is a metric widely
    used to evaluate the quality and performance of machine learning models and neural
    networks [41]. It is computed as the harmonic mean of precision (the proportion
    of true positive predictions compared to the total number of positive predictions)
    and recall (the proportion of true positive predictions compared to the overall
    number of positive instances) : F 1 = 2×precision×recall precision+recall . The
    F 1 score provides a balanced measure, combining these two performance aspects.
    It is therefore more informative compared to other metrics such as accuracy, especially
    when the training and testing sets are imbalanced [11], as in our case. A central
    part of our approach to the training, validation, and verification of learning
    models is simulating the interactivity of visual analytics for real-time data,
    i.e. for use cases in which training data does not exist prior to user interaction.
    We assume the user (re)labels the incoming stream of data and therefore iteratively
    trains a model, which consequently meets their real-time needs. To replicate this
    process, we computationally evaluate the model''s performance (as if it is successively
    trained by user-labeled data) by iteratively training the model with 10 new samples
    from the training dataset. We average the F 1 score obtained from each of these
    iterations and use the resulting number to measure the model''s performance. In
    addition, we introduce a new variable, window size, for our training iterations.
    Specifically, due to the considerably small amount of training data provided by
    the user, we found that an appropriately small number of epochs (one forward and
    one backward pass over the training data in the model) was necessary to reduce
    performance degradation from initial overfitting. However, we also found that
    increasing the number of epochs could lead to higher F 1 scores as more data was
    provided. Thus, we use a sliding window of 110 samples that includes the (successively
    provided) new training data (10 samples) as well as the most recently used training
    data (100 samples) to both account for small amounts of training samples and increase
    the number of total training epochs for a given sample. We use the validation
    data to optimize the hyperparameters for each of the CNN, LSTM, and RNN models.
    Specifically, after each training iteration with 10 new samples, we evaluate the
    neural network''s F 1 score on the validation set to view its simulated performance
    as if it was trained by gradual user labeling. After identifying the optimal hyperparameters
    for each of the CNN, LSTM, and RNN models, we evaluate their performance on the
    testing set. Table 1 demonstrates the results from our validation stage. Specifically,
    it lists the average F 1 score obtained during each training simulation along
    with the total CPU time required to complete the simulation (accumulated with
    each training and evaluation iteration). Although in many applications, F 1 score
    alone is sufficient to evaluate machine learning models, it is not for ours. To
    see why, note that the LSTM model yields an F 1 score of 0.75, the highest of
    any hyperparameter combination. However, the LSTM model (with the highest F 1
    score) takes approximately 4,242 seconds to complete training, whereas the CNN
    model (with the highest F 1 score) only takes 504 seconds. Thus, the LSTM model
    takes roughly eight times longer to simulate than the CNN model, but does not
    improve its F 1 score by a significant amount (LSTM: 0.75 vs. CNN: 0.74). In the
    context of interactive learning, we wish to balance the training/CPU time and
    performance such that the model both performs well and retrains in a short amount
    of time for rapid improvement (DG4). Therefore, it is necessary to consider both
    the CPU time and average F 1 score. With these optimization standards in mind,
    we chose the hyperparameters that yielded the highest F 1 scores for each model
    since the other hyperparameter combinations generated lower F 1 scores and higher
    or comparable CPU times. The selected combinations correspond to rows 1, 4, and
    7 in Table 1 with the average F 1 scores in bold. The testing process is identical
    to the validation process: after the model is trained with 10 new samples, its
    performance is measured by computing the average F 1 score on the testing set
    (using the optimized hyperparameters from the validation stage). Our results are
    summarized in Table 2. We found that the LSTM model yielded the highest F 1 score
    of 0.75. The CNN and RNN models achieved a 0.73 and 0.70 F 1 score, respectively.
    Based on these results and the previously discussed optimization standards, we
    selected the optimized CNN model for our classifier. In particular, the CNN simulation
    not only yielded a competitive average F 1 score of 0.73, but also achieved this
    score 6 to 8 times more quickly than the LSTM or RNN (Fig. 3), which is significant
    in terms of responding to user feedback in a timely manner. Table 1. Average precision,
    recall, F 1 score, and CPU time for the top three performing hyperparameter combinations
    on each of the CNN, LSTM, and RNN models. Bold numbers correspond to the highest
    F 1 scores and lowest CPU times for each of the three model types. We report the
    recall, precision, and F 1 score to four decimal places (when necessary) to distinguish
    the average F 1 scores. Table 2. Testing results with the optimal hyperparameter
    combinations for the CNN, LSTM, and RNN models. The bold numbers correspond to
    the highest F 1 score and lowest CPU time among the three models. The optimized
    CNN model yielded 0.74 and 0.73 average precision and recall scores respectively
    (Table 2, row 1). This model performance may be due to the initial lack of sufficient
    training data and difficulty in classifying certain tweets. For instance, after
    examining the testing dataset, we found that many misclassified tweets were extremely
    short (e.g., the tweet “screams internally” was misclassified as “Relevant”) or
    contained complex disaster-related diction (e.g., the tweet “emergency dispatchers
    in boone county in the hot seat” was misclassified as “Relevant”). However, as
    we demonstrate in the next section, our model still outperforms state-of-the-art
    learning models on tweet datasets. It is worth noting that we do not save the
    trained model from the validation or testing stages for evaluation in the next
    stage (or for use with SMART 2.0). We only save the optimized hyperparameters.
    This is because we assume that users start training a new model (for any event
    or topic they choose) by labeling the incoming stream of tweets. In this section,
    we optimized the model on a sufficiently large dataset that contained tweets related
    to several kinds of disasters. In the next section, we evaluate the model on datasets
    containing tweets on specific events, which is representative of cases for situational
    awareness. 3.3.5 Evaluation To further demonstrate the optimized CNN model''s
    performance, we computationally evaluated it on wildfire, bombing, and train crash
    datasets from CrisisLexT26 [34], each of which contain approximately 1,000 tweets
    collected during 2012 and 2013 from 26 major crisis situations labeled by relevance.
    We apply a similar process to evaluate our optimized CNN model on these datasets
    as we did with the Figure Eight [1] dataset. Specifically, we split the data into
    50% training and 50% testing sets (to replicate the experimental setting of To
    et al. [45], against which we will compare our results), train the model by supplying
    10 tweets from the training set at a time (to simulate user labeling of streaming
    data), evaluate the resulting model on the entire testing set, and then average
    the F 1 scores for each evaluation. We summarize our results in Table 3 and graph
    the model''s performance for retraining with 10 new incoming tweets in Fig. 4,
    5, and 6. In addition, we report the average CPU times to train the model during
    a single iteration (10 tweets) with each dataset in Table 3. Since the datasets
    vary slightly in size, we only compute the averages from the first 45 iterations
    since the smallest dataset (Boston Bombings) required 45 iterations to complete
    the simulation. We found that per-iteration training was fast and approximately
    0.5 seconds with each dataset, which meets our timing demands (DG4). Fig. 3. The
    total CPU time required for each model to complete the testing simulation. The
    CNN model is noticeably faster than both the LSTM and RNN models. Show All We
    obtained 0.71, 0.64, and 0.88 F 1 scores for the Colorado wildfires, Boston bombings,
    and NY train crash datasets, respectively. Interestingly, the variance of the
    F 1 scores over the datasets is significant. The textual data in the Boston bombings
    dataset, which yielded the lowest average F 1 score, was not as easy to separate
    into the different relevance categories by the model compared with the other two
    datasets. However, the F 1 score does eventually converge towards a higher value
    similar to the other datasets, indicating the potential presence of outliers during
    the first few training iterations. In addition, we found that the simulations
    converged to the average F 1 scores after training with approximately 190–230
    tweets, depending on the dataset, meaning that users need to label 190–230 tweets
    to achieve the reported F 1 scores. However, the CrisisLexT26 datasets also correspond
    to specific events, such as wildfires. As such, we surmise that interactively
    training the model on specific, well-defined events will reduce the amount of
    training data needed to achieve satisfactory results than with generic constraints
    on relevance (e.g., a classifier about safety in general). Finally, we compare
    our results with the learning-based algorithm employed by To et al. [45], who
    also evaluated their model''s performance with CrisisLexT26 datasets. In particular,
    their learning-based approach used Word2Vec, TF-IDF, latent semantic indexing,
    and logistic regression for classifying data as relevant or irrelevant. The authors
    of that study split the dataset into two equal parts: one for training and one
    for testing. They trained the model once (as opposed to our iterative approach)
    and evaluated on the testing set. Their algorithm was able to yield high precision
    scores between 0.85-0.95, compared to our scores of 0.64-0.86. However, their
    recall scores were approximately 0.22–0.45, considerably lower than our recall
    scores of 0.65-0.90. Therefore, our approach outperforms the learning-based model
    presented by [45], in terms of the overall F 1 score: our interactive approach
    achieves F1 scores of 0.64-0.88 (depending on the dataset) compared to 0.45-0.64
    by [45]. The authors also presented a matching-based approach that achieved a
    much higher F 1 score of 0.54-0.92, which is comparable to ours. However, they
    generate the set of hashtags to be used for matching by scanning all of the tweets
    in the dataset. Since we assume the data is streamed in real-time, and therefore,
    not available altogether, we use an iterative learning approach. Fig. 4. Optimized
    CNN F 1 score per training iteration of 10 tweets with the Colorado wildfires
    dataset (Table 3). The F 1 scores are logarithmically fitted and intersect with
    the average F 1 score (0.7134) at 228 tweets. Show All Fig. 5. Optimized CNN F
    1 score per training iteration of 10 tweets with the Boston bombings dataset (Table
    3). The F 1 scores are logarithmically fitted and intersect with the average F
    1 score (0.6410) at 184 tweets. Show All Fig. 6. Optimized CNN F 1 score per training
    iteration of 10 tweets with the NY train crash dataset (Table 3). The F 1 scores
    are logarithmically fitted and intersect with the average F 1 score (0.8792) at
    191 tweets. Show All Table 3. Average precision, recall, and F 1 score for three
    CrisisLexT26 [34] datasets. SECTION 4 SMART 2.0 4.1 SMART The Social Media Analytics
    and Reporting Toolkit (SMART) [52], [53] is a visual analytics application designed
    to support real-time situational awareness for first responders, journalists,
    government officials, and special interest groups. SMART obtains real-time publicly
    available geo-tagged data from the Twitter streaming API. The user is able to
    explore the trending and abnormal topics on various integrated visualizations,
    including spatial topic model visualization and temporal views. The tweet time
    chart and theme river visuals convey the temporal distributions of topics if the
    user wishes to determine how the content of streamed social data has changed over
    time. SMART uses string matching-based classifiers to visualize relevant data.
    Specifically, the user can either (a) select pre-defined filters, such as Safety
    or Weather (Fig. 7(c)), each using a series of related keywords for inclusion
    and exclusion of tweets in the subsequent topic-modeling (Fig. 7(f)) and (geo)visualizations
    (Fig. 7(b)), or (b) create their own filters by supplying keywords, and intersect
    or union multiple filters according to their needs. However, keyword-based matching
    is insufficient for finding relevant information as it fails to accurately capture
    semantic relevance and therefore effectively filter out noisy data. As an example,
    if the user were to apply the Safety classifier, it would be possible for the
    tweets “My house is on fire!” and “I was just fired from my job.” to pass through
    the filter since they both include the keyword fire. However, the latter is unrelated
    to the intended semantic context of Safety and thus dilutes the filter''s quality.
    To address this problem, we integrate our interactive learning framework (the
    focus of this paper) in the existing SMART application [52], [53] and seek domain
    expert feedback on the use of these models. We call the resulting extended application
    SMART 2.0. SMART 2.0 allows users to define string matching-based keyword filters
    (similar to SMART), but adds the ability for users to then iteratively refine
    and train the newly integrated models by labeling the filtered data as semantically
    relevant or not. In addition, the SMART 2.0 interface includes interactive visuals
    to facilitate user exploration, filtering, and refinement of relevant data (Fig.
    7). As with the model simulations in Section 3.3, SMART 2.0''s underlying models
    are trained with successive batches of 10 user-labeled tweets. In cases where
    model predictions conflict with user labels, user labels override the model''s
    since they represent the ground truth. In addition, users should not need to manually
    relabel the same data multiple times. Although conflicts might indicate that the
    model is not sufficiently trained, the model trains with the same data during
    several successive iterations (as discussed in Section 3.3.4), so conflicts might
    be resolved after future iterations. 4.2 SMART 2.0 Interface The extensions to
    SMART 2.0''s user interface, compared with SMART, concern the new interactive
    visuals that allow users to iteratively train machine learning models, utilize
    model predictions for rapid relevancy identification, and understand a model''s
    reliability. The SMART 2.0 interface (Fig. 7) extends the interactive features
    of SMART for relevance identification in three primary ways: Extending the tweet
    table (containing a tweet''s creation date and text) by including the predicted
    relevance label, relevance label probabilities, label modification, model training
    performance, and relevance filtering. Extending the interactive map containing
    the geo-tagged tweets whose relevancy can be individually inspected or modified.
    Altering the content of existing SMART views (e.g., topic models and spatial topic
    lenses) using either all data or only relevant data (as identified by the model
    and corrected by the user). 4.2.1 Table The SMART 2.0 table (Fig. 7(g)-(j)) is
    extended from SMART in that it not only provides a tweet''s creation date and
    text, but also provides the predicted relevance label (Fig. 7(i)) and the probabilities
    of a tweet belonging to any of the relevance classes (Fig. 7((j)) (DG1). In particular,
    the relevance of a tweet can be “Relevant”, “Not Relevant”, or “Can''t Decide”.
    The “Relevant” label is colored blue, the “Not Relevant” label red, and the “Can''t
    Decide” label gray to visually separate tweets with different relevance. SMART''s
    preexisting blue color scheme motivated us to use the blue, red, and gray diverging
    coloring for relevancy in order to maintain visual appeal and harmony. Fig. 7.
    SMART 2.0 overview: (a) The control panel provides several filters, visualizations,
    and views. (b) The content lens visualization provides the most frequently used
    words within a selected area. (c) The tweet classifier visualization provides
    keyword-based filters to help reduce noisy data. (d)(e) Clicking a tweet on the
    map with the tweet tooltip visualization displays the tweet''s time, message,
    and relevance label. (f) The topic-modeling view, based on latent dirichl et al.ocation,
    extracts trending topics and the most frequently used words associated with each
    topic among tweets with specified relevancy. (g)-(j) The message table aggregates
    the tweets for efficient exploration with (g) the model''s estimated classification
    performance ( F 1 score), (h) A drop down box to filter data by their relevance
    labels, (i) Color-coded relevance labels that can be changed by clicking on the
    label itself, and (j) Associated relevance probabilities. Tweet map symbols are
    colored orange and purple to distinguish twitter data from instagram-linked tweets,
    respectively, since the latter contains potentially useful images for situational
    awareness. Show All Users can directly click on relevance labels to correct the
    classifier''s prediction (DG2). For instance, if a tweet is incorrectly marked
    “Relevant”, clicking the label will change it to “Not Relevant” or “Can''t Decide”,
    depending on the label the user wishes to assign. Further, a drop down box is
    included at the top of the relevance label column (Fig. 7(h)), which provides
    the option to filter out data that does not have a specified relevancy (DG1).
    For example, by selecting “Relevant” from the drop down box, the table will remove
    tweets with labels “Not Relevant” and “Can''t Decide” from all views and visualizations
    in SMART, including geovisualizations and temporal views. The table also displays
    the degree (or confidence) of a tweet''s relevancy. In specific, the probabilities
    of a tweet being “Relevant”, “Not Relevant”, or “Can''t Decide” are represented
    as a horizontal segmented bar graph and sized proportional to their respective
    percentages (Fig. 7(j)). In addition, the user can sort tweets based on relevancy
    probability in ascending or descending order. We provide the relevance probabilities
    and associated sorting actions as a supplementary relevance filtering mechanism
    (DG1). In particular, it is possible for tweets to be classified as “Relevant”
    by the model, for example, but with low confidence. The probability filtering
    allows the user to specifically view high-confidence relevant data and therefore
    further reduce potentially noisy data. The table provides a performance bar that
    encodes the estimated performance ( F 1 score) of the underlying learning model
    (Fig. 7(g)), as well as the number of user-labeled tweets, to inform the user
    of the model reliability. Since labeled testing data is not available to evaluate
    the model for real-time training (because we assume the user may train on any
    type of event data and has their own specifications for relevancy), the model''s
    performance can only be estimated. Based on our evaluations in Section 3.3.5 with
    datasets typical of situational awareness scenarios (Table 3), the Colorado wildfires
    dataset generated the F 1 score (0.71) closest to the average of the three datasets
    (0.74). Therefore, we use the Colorado wildfires dataset''s logarithmic trendline
    y=0.09  log e (x)+0.22 (Fig. 4) to approximate the model''s F 1 score as a function
    of the number of user-labeled tweets. 4.2.2 Map The SMART 2.0 map is extended
    from SMART in that it includes a tweet''s relevance label (which can be modified)
    in addition to its text and creation date (Fig. 7(d)(e)). Through the Tweet Tooltip,
    the user can directly click on tweet symbols on the map to view their text and
    associated relevancy (DG1). In addition, the user can correct the classified relevance
    label (DG2) by clicking on the label itself. Map inspection can allow the user
    to view and investigate potential geographical relevancy trends. For example,
    during crisis events, relevant tweets might be closely grouped on the map, so
    it may be more beneficial for the user to view predicted relevance from the map
    itself. The interactions between the table and map are synchronized. If the user
    relabels data on the map, the associated new label will also be updated in the
    table, and vice versa. In addition, selecting a relevancy filter from the drop
    down box in the table filters the tweets on the map. 4.2.3 Integration with Existing
    Visualizations Many of SMART''s original visualizations, such as the topic-model
    views, spatial topic lenses, and temporal views help users make sense of spatiotemporal
    text data. Therefore, we integrated all of these views in SMART 2.0 with the relevance
    extensions. Users have the option to view only relevant or all the data (including
    irrelevant tweets) in various visualizations in case the interactive classifiers
    are not yet trained to desirable accuracies since, as we show in Section 3.3.5,
    classifiers typically require around 200 user-labeled tweets to achieve F 1 scores
    of 0.70-0.80. If they choose to view only relevant tweets, any relevance filtering
    action also updates the data used by other visuals. For example, the topic-modeling
    view (Fig. 7(f)) extracts the top 10 topics from the tweets and displays the most
    frequently used words for each topic. If the user filters out irrelevant tweets,
    the topic-modeling view will only be applied to the remaining relevant tweets.
    It is important to note that the majority of visualizations in SMART 2.0 require
    a minimum number of tweets in order to render. When filtered relevant data is
    scarce, visualizations do not populate, in which case users can individually inspect
    tweets. For instance, the topic-modeling view requires at least 10 tweets to extract
    topics. Overall, SMART 2.0''s suite of visualization tools can be used in combination
    with relevance interactions to further understand trends and important spatiotemporal
    characteristics of relevant data. SECTION 5 User Experience In this section, we
    provide usage scenarios and feedback from domain experts that demonstrate our
    framework''s effectiveness and usability. 5.1 Usage Scenario 1 Alice is an emergency
    dispatcher interested in identifying people in need for help or hazardous locations
    during a hurricane. She uses SMART 2.0 to find any related social media posts
    near the affected area. She adds a new filter Hurricane and provides an appropriate
    set of filter keywords such as “hurricane”, “help”, “blocked”, and “trapped”.
    After applying the Hurricane filter, she explores the filtered tweets in the table
    and finds a tweet labeled “Relevant” that says “Does anyone know how to get help
    setting up my TV?“. Since the tweet is unrelated to a hurricane, she relabels
    it as “Not Relevant”. After further browsing the table, she finds a tweet that
    says “The road near Taylor Loop is blocked from a broken tree.”, but it is labeled
    as “Not Relevant”. Since the tweet contains actionable information, she relabels
    it as “Relevant”. After labeling several more tweets for model training and noticing
    that the model predicts correctly, she decides to only view “Relevant” tweets
    and sort them by most relevant. She promptly identifies a tweet posted only a
    few minutes ago marked as highly relevant. It reads “Car just crashed into tree
    blocking road near Taylor Loop!”. Alice immediately notifies first responders
    of the location to provide assistance. By using SMART 2.0, Alice is able to identify
    important, relevant data more quickly through interactively training the model
    to remove noise and then filtering by relevance. 5.2 Usage Scenario 2 To demonstrate
    the generalizability of our framework to other domains, we applied our interactive
    framework in real-time during the Purdue vs. Virginia 2019 March Madness game
    in the Kentucky area. We assumed the role of a journalist who wanted to follow
    public discourse on the game by identifying the relevant tweets. We first constructed
    a Sports filter, which included keywords such as “Purdue”, “game”, “score”, and
    “#MarchMadness”. We then interacted with the streaming data by iteratively labeling
    the relevancy of tweets (from scratch) and found that the system correctly classified
    incoming data after roughly 80 training samples (Fig. 8). We noticed that the
    time intervals between successive trainings increased, indicating that it was
    more difficult to find incorrectly labeled data towards the end and that the model
    gradually learned from user feedback. In particular, the interval between the
    first and second training iterations was 2 minutes, whereas the interval between
    the final two was 4 minutes. 5.3 Domain Expert Feedback We piloted SMART 2.0 with
    two groups of first responders, each containing two individuals, who frequently
    use SMART during events for situational awareness in their operations. Both groups
    participated in separate 1-hour long sessions via conference call in which they
    iteratively trained a classifier from scratch and applied relevance filtering
    and visualizations to assess the implemented framework. They received a tutorial
    of SMART 2.0 30 minutes beforehand and were provided with web access to the system
    to complete the session. For both groups, we simulated the real-time use of SMART
    2.0 by feeding in a stream of historical data on events (previously collected).
    For the first group, the system presented unlabeled tweets from the Las Vegas
    shooting on October 1, 2017 in the Las Vegas area. For the second group, we used
    unlabeled tweets from the October 2017 Northern California wildfires. We used
    historical event datasets to ensure the existence of sufficient training relevant
    samples for a situational awareness scenario. The domain experts in the first
    group applied the Safety, Damage, and Security filters during the iterative training
    process, resulting in 317 tweets. They trained on the same underlying model for
    all three filters, as they considered them semantically related. In total, after
    relabeling approximately 200 tweets, they indicated that they could trust the
    model to predict accurately and were pleased that the tweets they had not seen
    before from the Security filter were labeled correctly. Their definition for relevancy
    was tweets containing actionable information. For instance, they marked tweets
    containing information about road closures, blood drive locations, or death counts
    as relevant. They labeled data with general comments regarding the shooting, such
    as “I hope everyone is safe now…terrible shooting…”, as irrelevant since they
    did not provide actionable information. Interestingly, they also marked tweets
    that would influence public opinion (and therefore may cause action) such as those
    from bots or trolls as relevant since they still contained actionable information.
    The domain experts from the second group followed a similar process in which they
    applied the Safety, Damage, and Security filters, resulting in 445 tweets, and
    trained a learning model for relevance. They found that after training on roughly
    67 tweets, the model satisfactorily predicted relevancy. As with the first group,
    these domain experts labeled tweets as relevant if they contained actionable information.
    The domain experts from both groups found SMART 2.0 to be easy to use and effective
    in identifying important data. For instance, they discovered relevant, actionable
    information after training the model: specific blood drive locations to aid shooting
    victims. Notably, the users mentioned that they felt less the need to relabel
    data as they progressed since the system provided more correct labels. They were
    pleased that they had the option to view only relevant data, but could see all
    of the data regardless of relevancy to avoid potentially missing important misclassified
    data, and that the model was responsive to user training. In addition, they found
    the relevance percentage bars to be helpful in determining the tweets that were
    potentially the most relevant. One concern the domain experts had was that SMART
    2.0 does not indicate the number of tweets that are predicted as relevant. They
    felt this extension could help them infer the occurrence of events or potential
    crises. For example, the number of relevant tweets for the Safety classifier would
    likely increase significantly during a widespread disaster. We plan to introduce
    this feature in the next development cycle. However, we have added a visualization
    of estimated model performance in SMART 2.0 (Fig. 7(g)) to help users ascertain
    the reliability of a model''s relevancy predictions. Overall, the feedback from
    the domain experts was positive and helpful, indicating the system''s practicality
    and usefulness in facilitating real-time situational awareness. In addition, they
    have asked to use SMART 2.0 in their emergency operations center. SECTION 6 Discussion
    and Future Work Our interactive learning framework and SMART 2.0 integration were
    developed with the user in mind, influencing all of our design, computational
    evaluation, and implementation choices. Our user-centered model and SMART 2.0
    application contribute to both the machine learning and visual analytics communities.
    We bridge the two fields by demonstrating how models can be interactively trained
    and evaluated while keeping the user in mind, and used to facilitate situational
    awareness for real-life, practical use. SMART 2.0 currently only collects English
    tweets, although supporting non-English languages (one at a time) with our current
    design (e.g., Spanish only) is straightforward since Word2Vec embeddings can be
    independently trained on a corpus in the target language [7]. Extending our system
    to support multilingual tweets would be a powerful asset, especially for multilingual
    users, in amplifying situational awareness by leveraging relevancy of tweets issued
    in different languages. However, the multilingual model performance evaluation
    and testing is an open area for research. In addition, determining the specifics
    of how a single relevance classifier might be trained with multilingual tweets
    requires careful attention. For instance, training iterations with Spanish tweets
    should also affect the relevancy of semantically-related English tweets. Since
    similar words in different languages likely have different vector representations
    (embeddings), multilingual mappings must be learned or training must be performed
    differently, such as with parallel corpora [7]. Multilingual support also requires
    changes in SMART 2.0''s language-dependent visualizations, such as the topic-modeling
    view (Fig. 7(f)). Translation to a unified language or extracting topics separately
    for each language are two potential solutions. Fig. 8. Tweets with correctly predicted
    relevancy from the Purdue vs. Virginia 2019 March madness game after the user
    (re)labels 80 tweets. Show All The scalability of our framework is a natural concern,
    especially since SMART and many deployed real-time visual analytics applications
    contain multiple users who require responsive interfaces while monitoring crisis
    events. We deliberately designed the framework architecture with scalability in
    mind. As mentioned in Section 3.3.4, we selected the model and optimal hyperparameters
    based on training/CPU time in an effort to maximize the model''s computational
    speed. Further, SMART 2.0 filters and views at most 800–900 tweets at a time,
    although user-specified filtering (typical in situational awareness scenarios)
    reduces the data to only a few hundred tweets, as demonstrated in Section 5.3.
    It takes only 2-3 seconds to calculate and retrieve their relevance labels over
    the network from the server where the model resides, and per-iteration training
    is fast, as established in Section 3.3.5. Training a model during a particular
    event, such as a disaster, can be straightforward due to potentially larger amounts
    of relevant data. However, social media data during periods without major events
    are likely to contain very few, if any, relevant tweets. As such, if the user
    only trains the model on irrelevant data, it will poorly predict relevant data
    since it has only be taught what is irrelevant. Although the user can improve
    the model through training during a real-life disaster, they are required to know
    when and where the disaster occurs to begin training. This can be problematic
    if the user wishes to rely on relevancy predictions to detect hazardous situations.
    To accommodate time periods in which relevant data is scarce, we plan to introduce
    an interactive feature in which users can provide example tweets or external resources
    for specific relevance labels. For instance, if the user would like to train a
    Hurricane classifier before a hurricane event, they could provide a relevant text
    such as “I''m stranded by this hurricane. Please help!”. The model could then
    detect relevant tweets once the hurricane begins as opposed to requiring user
    training during the event. In addition, we plan to provide the user with the option
    to visit specific historical data to train existing classifiers, as done by Bosch
    et al. [9]. Our interactive learning performed well on target datasets (i.e.,
    wildfire, bombing, and crash) as explained in Section 3.3.5. Specifically, it
    required the users to label approximately 200 tweets to achieve acceptable F 1
    scores. However, tweets are short, and therefore, more research is required to
    investigate the suitability of our approach for “general” classifiers, such as
    ones that learn to classify relevant data to “safety”. As safety can be affected
    by many events or situations, the model may need additional training that is typical
    of a targeted dataset. Finally, although we rigorously optimize and evaluate our
    machine learning model, the hyperparameter combinations were only tuned with the
    Figure Eight dataset [1]. Since optimal hyperparameters can depend on the dataset
    itself, it is possible our model may not be optimally tuned for different datasets,
    even though that optimization may be negligible from a user standpoint. We did
    use other datasets in our model evaluation to show the satisfactory resulting
    performance (Section 3.3.5). Given that the Figure Eight dataset classifies generic
    events as relevant or irrelevant, as opposed to specific events, we expect that
    our model performs well on many different event types. SECTION 7 Conclusion We
    presented a novel interactive framework in which users iteratively (re)train neural
    network models with streaming text data in real-time to improve the process of
    finding relevant information. We optimized and evaluated a machine learning model
    with various datasets related to situational awareness and adapted the model to
    learn at interactive rates. According to evaluation results, our model outperforms
    state-of-the-art learning models used in similar classification tasks. Finally,
    we integrated our framework with the SMART application and extended it to SMART
    2.0, allowing users to interactively explore, identify, and refine tweet relevancy
    to support real-time situational awareness. Our discussions with multiple first
    responders who use SMART 2.0 indicated positive feedback and user experience.
    In particular, their assessments demonstrated that our interactive framework significantly
    improved the time-consuming process of finding crucial information during real-time
    events. ACKNOWLEDGEMENTS This material is based upon work funded by the U.S. Department
    of Homeland Security (DHS) VACCINE Center under Award No. 2009-ST-061-CI0003,
    DHS Cooperative Agreement No. 2014-ST-061-ML0001, and DHS Science and Technology
    Directorate Award No. 70RSAT18CB0000004. Authors Figures References Citations
    Keywords Metrics Media More Like This An integrated approach of Genetic Algorithm
    and Machine Learning for generation of Worst-Case Data for Real-Time Systems 2022
    IEEE/ACM 26th International Symposium on Distributed Simulation and Real Time
    Applications (DS-RT) Published: 2022 Design of Real-Time System Based on Machine
    Learning for Snoring and OSA Detection ICASSP 2022 - 2022 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP) Published: 2022 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE transactions on visualization and computer graphics
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Interactive Learning for Identifying Relevant Tweets to Support Real-time
    Situational Awareness
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1186/s40537-022-00575-6
  analysis: '>'
  authors:
  - Tanya Nijhawan
  - Girija Attigeri
  - T. Ananthakrishna
  citation_count: 27
  full_citation: '>'
  full_text: ">\nStress detection using natural language \nprocessing and machine\
    \ learning over social \ninteractions\nTanya Nijhawan1, Girija Attigeri2* and\
    \ T. Ananthakrishna1 \nIntroduction\nCurrently, social media plays the role of\
    \ chief public opinion detector. We have over 4.2 \nbillion active worldwide social\
    \ media users. With the whirlwind expansion of Web 2.0, \npeople have developed\
    \ a liking to express their thoughts and approach over the Inter-\nnet, which\
    \ has consequently resulted in an increase of user-generated content and self-\n\
    opinionated data. Social Media Analytics (SMA) is the process of collecting information\
    \ \non various social media platforms, websites and blogs and evaluating that,\
    \ to successful \nbusiness decisions. The use of social media has become quite\
    \ commonplace in today’s \nworld. SMA is not only a collection of likes and comments\
    \ shared by people but also \na platform for many advertising brands. There are\
    \ six types of social networks where \nAbstract \nCyberspace is a vast soapbox\
    \ for people to post anything that they witness in their \nday-to-day lives. Social\
    \ media content is mostly used for review, opinion, influence, or \nsentiment\
    \ analysis. In this paper, we aim to extend sentiment and emotion analysis for\
    \ \ndetecting the stress of an individual based on the posts and comments shared\
    \ by him/\nher on social networking platforms. We leverage large-scale datasets\
    \ with tweets to \naccomplish sentiment analysis with the aid of machine learning\
    \ algorithms and a deep \nlearning model, BERT for sentiment classification. We\
    \ also adopted Latent Dirichlet \nAllocation which is an unsupervised machine\
    \ learning method for scanning a group \nof documents, recognizing the word and\
    \ phrase patterns within them, and gathering \nword groups and alike expressions\
    \ that most precisely illustrate a set of documents. \nThis helps us to predict\
    \ which topic is linked to the textual data. With the aid of these \nmodels, we\
    \ will be able to detect the emotion of users online. Further, these emotions\
    \ \ncan be used to analyze stress or depression. In conclusion, the ML models\
    \ and a BERT \nmodel have a very good detection rate. This research is useful\
    \ for the well-being of \none’s mental health. The results are evaluated using\
    \ various metrics at the macro and \nmicro levels and indicate that the trained\
    \ model detects the status of emotions based \non social interactions.\nKeywords:\
    \ Decision tree, Latent Dirichlet Algorithm, Logistic regression, Machine \nlearning,\
    \ Natural Language Processing, Random forest, Sentiment analysis, Topic \nmodelling\n\
    Open Access\n© The Author(s) 2022. Open Access This article is licensed under\
    \ a Creative Commons Attribution 4.0 International License, which permits \nuse,\
    \ sharing, adaptation, distribution and reproduction in any medium or format,\
    \ as long as you give appropriate credit to the original \nauthor(s) and the source,\
    \ provide a link to the Creative Commons licence, and indicate if changes were\
    \ made. The images or other third \nparty material in this article are included\
    \ in the article’s Creative Commons licence, unless indicated otherwise in a credit\
    \ line to the mate-\nrial. If material is not included in the article’s Creative\
    \ Commons licence and your intended use is not permitted by statutory regulation\
    \ or \nexceeds the permitted use, you will need to obtain permission directly\
    \ from the copyright holder. To view a copy of this licence, visit http:// \n\
    creat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH\nNijhawan et al. Journal\
    \ of Big Data            (2022) 9:33  \nhttps://doi.org/10.1186/s40537-022-00575-6\n\
    *Correspondence:   \nga.research10@gmail.com \n2 Depertment of Information \n\
    and Communication \nTechnology, Manipal \nInstitute of Technology, \nManipal Academy\
    \ of Higher \nEducation, Manipal, \nKarnataka, India\nFull list of author information\
    \ \nis available at the end of the \narticle\nPage 2 of 24\nNijhawan et al. Journal\
    \ of Big Data            (2022) 9:33 \npeople connect and share their interests,\
    \ opinions, experiences, and moments of life. \nBookmarking sites allow users\
    \ to have control over their resources. Social news: allows \nusers to post news\
    \ links and external articles, Media sharing: Share their videos and \nphotographs,\
    \ Microblogging: Allow users to write short written entries and Blogs and \nForums:\
    \ Allow users to produce focused content and then engage in conversations about\
    \ \nit.\nSMA is the ability to gather data from these resources and find meaning\
    \ from them, \nmake decisions and evaluate the performance of the decisions through\
    \ social media. \nFor this SMA uses the concepts such as social media intelligence,\
    \ social media listen-\ning, social media monitoring, social competitive analysis,\
    \ image analytics, sentiment \nanalysis, customer sentiment analysis. Many applications\
    \ include marketing and making \nextensive use of social data to make predictive\
    \ decisions. Some of the methods are built \nto create a hypothesis, deep penetration\
    \ of data, mapping events, etc. These calculations \ncan also be done in services\
    \ such as business, amendment, education, machine learn-\ning-based predictions,\
    \ etc. Especially now, data is controlling marketing approaches and \ntactics.\
    \ The propagation of data is only expected to rise as more people and businesses\
    \ \nplan on dispensing data about themselves on social media. It is in this material\
    \ that a \nbusiness will end up learning more about their audience, specifically\
    \ on sites like Twit-\nter, Facebook, and Instagram. With these insightful analytics,\
    \ a person fundamentally \ngains social media intelligence to inform future decisions\
    \ and actions. Currently, SMA \nis being used for influence, review, and opinion\
    \ mining. However, it can be employed in \nanalyzing the emotional state of a\
    \ person. These social factors are important indicators \nof mental health. However,\
    \ how to be quantifying and analyzing social factors is chal-\nlenging. The data\
    \ is usually unstructured and huge, which needs the techniques like Big \ndata,\
    \ Machine Learning (ML), Natural Language Processing (NLP) to get inferences for\
    \ \nstress or other mental health issues. There are studies to show that constructive\
    \ SMA to \nmeasure and quantify the social interaction along with other health\
    \ parameters are used \nin healthcare systems for stress/depression detection\
    \ [1].\nTo perform the SMA data can be collected with the help of web scraping.\
    \ Web scrap-\ning aids in extracting the underlying HTML code and, with it, the\
    \ data deposited in a \ndatabase. The scraper can then duplicate the complete\
    \ website content elsewhere. \nApart from this, with the help of applications\
    \ like lucidya and trackmyhashtag, certain \nhashtags were tracked while creating\
    \ the dataset. There are a lot of capable pre-trained \nlanguage models which\
    \ include the likes of ELMo and BERT. These models have specifi-\ncally shown\
    \ outstanding performance on aspect-based sentiment analysis problems [2]. \n\
    The pre-trained language models have the advantage to learn universal language\
    \ by pre-\ntraining on the vast unlabeled corpus to dodge overfitting on small-size\
    \ data [3]. In this \npaper, we are using a proficient deep learning model titled\
    \ BERT to resolve sentiment \nclassification tasks. Experimentations have supported\
    \ the claim that the BERT model \noutdoes other prevalent models for this task\
    \ without a complex architecture. Hence, we \nuse the BERT model to do a 5-class\
    \ emotion classification. The emotions are joy, sad-\nness, neutral, fear, and\
    \ anger.\nPage 3 of 24\nNijhawan et al. Journal of Big Data            (2022)\
    \ 9:33 \n \nTopic modeling is described as one of the most efficacious methods\
    \ for detecting use-\nful unseen structures in a corpus. It can be defined as\
    \ a method that locates a group \nof words i.e., the topic from a group of documents\
    \ that represents the information in \nthe group [4]. By leveraging the topic\
    \ modeling results we can represent, measure, and \nmodel user behavior patterns\
    \ on large-scale social networks and even use such social \ninformation for further\
    \ research. With the edge of using ML algorithms, a pre-trained \nmodel, and a\
    \ high accuracy rate, this model will give accurate and reliable results. The\
    \ \nidea of this paper is to come up with a system that not only detects stress\
    \ but also analy-\nses the topic of discussion in a particular tweet. Along with\
    \ sentiment analysis, this sys-\ntem will also accurately analyze and segregate\
    \ the user’s opinions on different topics. \nAfter carrying out in-depth studies\
    \ on pertinent datasets we will attain crucial under-\nstandings of different\
    \ correlations between social interactions and the tension/strain of \nthe user.\n\
    The contributions of the paper are as follows:\n• Binary classification of the\
    \ sentiments behind the tweets\n• Perform topic modeling with the help of LDA\
    \ which takes into consideration the \ndensity of every topic and calculates a\
    \ topic structure through an iteration process.\n• Emotion classification using\
    \ deep learning-based BERT model to detect stress.\n• Develop a Django-based web\
    \ application that receives inputs from a user and then \naccordingly generates\
    \ a prediction.\n• Develop a system in the form of a web portal that not only\
    \ detects stress but also \nanalyses the topic of discussion in a particular tweet.\n\
    • Accurately analyze and segregate the user’s opinions on different topics.\n\
    Background and literature review\nA lot of astounding contributions have been\
    \ made in the field of sentiment analysis in \nthe past few years. Initially,\
    \ sentiment analysis was proposed for a simple binary classi-\nfication that allocates\
    \ evaluations to bipolar classes. Pak and Paroubek [5] came up with \na model\
    \ that categorizes the tweets into three classes. The three classes were objective,\
    \ \npositive and negative. In their research model, they started by generating\
    \ a collection of \ndata by accumulating tweets. They took advantage of the Twitter\
    \ API and would rou-\ntinely interpret the tweets based on emoticons used. Using\
    \ that twitter corpus, they were \nable to construct a sentiment classifier. This\
    \ classifier was built on the technique—Naive \nBayes where they used N-gram and\
    \ POS-tags. They did face a drawback where the train-\ning set turned out to be\
    \ less proficient since it only contained tweets having emoticons. \nThe papers\
    \ [6–10] discuss effective data pre-processing techniques for social media con-\n\
    tent, specifically tweets. As the data contains the words which are most often\
    \ used in a \nsentence but do not contribute to the analysis, such as stop words,\
    \ symbols, punctuation \nmarks. Removing these and converting different forms\
    \ of the words to the base from is \nan essential step.\nPage 4 of 24\nNijhawan et al.\
    \ Journal of Big Data            (2022) 9:33 \nSentiment analysis\nAgarwal et al.\
    \ [11] proposed a 3-way model for categorizing sentiments in three classes. \n\
    The classes were positive, negative, and neutral. Models such as the unigram model,\
    \ a \nfeature constructed upon the model, and a tree kernel-based were used for\
    \ testing. In \nthe case of the tree kernel-centered model, tweets were chosen\
    \ to be represented in the \nform of a tree. While implementing a feature-centered\
    \ model over 100 features were \ntaken into consideration. However, in the case\
    \ of the unigram model, there were about \n10,000 features. They concluded that\
    \ features that end up combining previous polariza-\ntion of words with their\
    \ parts-of-speech (pos) tags are the most substantial. In terms of \nthe result,\
    \ the tree kernel-based model ended up performing better than the other two \n\
    models.\nCertain challenges are made by a few researchers to classify public beliefs\
    \ about mov-\nies, news, etc. from Twitter posts. V.M. Kiran et al. [12] utilized\
    \ the data from other \nwidely accessible databases like IMDB and Blippr after\
    \ appropriate alterations to benefit \nTwitter sentiment analysis in the movie\
    \ domain. Davidov et al. [13] projected a method \nto utilize Twitter user-defined\
    \ hashtags in tweets as a classification of sentiment type \nusing punctuation,\
    \ single words, and patterns as disparate feature types. They are then \ncombined\
    \ into a single feature vector for the task of sentiment classification. They\
    \ made \nuse of the K-Nearest Neighbor approach to allocate sentiment labels by\
    \ constructing \na feature vector for each example in the training and test set.\
    \ Tagging [14], in current \ntimes developed as a common way to sort out vast\
    \ and vibrant web content. It usually \nrefers to the act of correlating with\
    \ or allocating some keyword or unit to a piece of data.\nTagging aids to depict\
    \ an article and lets it be located again by perusing. Scholars \nhave established\
    \ diverse methods and procedures for tagging corpus for numerous uses. \nXiance\
    \ et al. [15] offered a flexible and practical technique for the process of the\
    \ rec-\nommendation of tags. They demonstrated documents and tags by implementing\
    \ the \ntag-LDA model. Krestel et al. [16] recommended a method to customize the\
    \ process \nof recommendation by tag. She proposed a method that amalgamates a\
    \ probabilistic \nmethod of tags from the source. In this case, the tags were\
    \ extracted from the user. She \nexamined basic language models. Additionally,\
    \ she performed LDA experimentations \non a real-world dataset. The dataset was\
    \ crawled from a vast tagging system which dis-\nplayed that personalization progresses\
    \ the process of tag recommendation.\nPre-trained language models like ELMo [17],\
    \ OpenAI GPT [18], and BERT [19] have \nproven to be extremely valuable. This\
    \ has led to Natural Language Processing (NLP) \npassing into a new era. Transfer\
    \ learning abilities permitted by pre-trained language \nmodels have helped a\
    \ lot of researchers significantly. This has allowed the pre-trained \nmodel to\
    \ play the role of the base, and this can be fine-tuned to respond to the NLP\
    \ task. \nThis process is better than performing the training of the model from\
    \ the basics [20]. \nZubair et al. [21] introduced a technique enhanced by lexicons.\
    \ It was projected to be \ncentered around a rule-based classification scheme.\
    \ It was to be carried out by assimi-\nlating emojis, modifiers, and domain-specific\
    \ terms to examine any thoughts published \non social media. However, traditional\
    \ methods emphasis on designing features has now \nreached its performance bottleneck\
    \ [22]. On the other hand, pre-trained language mod-\nels save a lot of time by\
    \ achieving the same result quickly. They are easy to incorporate \nPage 5 of\
    \ 24\nNijhawan et al. Journal of Big Data            (2022) 9:33 \n \nand there’s\
    \ not as much labeled data required. However, these techniques need to be \nincorporated\
    \ for mental health prediction with social and other parameters (Table 1).\nStress/depression\
    \ analysis\nArya and Mishra present a review of the application of machine learning\
    \ in the \nhealth sector, their limitation, predictive analysis, and challenges\
    \ in the area and need \nadvanced research and technologies. The authors reviewed\
    \ papers on mental stress \ndetection using ML that used social networking sites,\
    \ blogs, discussion forums, Ques-\ntioner technique, clinical dataset, real-time\
    \ data, Bio-signal technology (ECG, EEG), \na wireless device, and suicidal tendency.\
    \ The study shows the high potential of ML \nalgorithms in mental health [28].\
    \ Aldarwish et al used machine learning algorithms \nSVM and Naïve- Bayesian for\
    \ Predicting stress from UGC- User Generated Content \nin Social media sites (Facebook,\
    \ Twitter, Live Journal) They used social interaction \nstress datasets based\
    \ on mood and negativism and BDI- questionnaire having 6773 \nposts, 2073 depressed,\
    \ 4700 non-depressed posts (textual). They achieved an accu-\nracy of 57% from\
    \ SVM and 63% from Naïve- Bayesian. They also emphasized stress \ndetection using\
    \ big data techniques [29].\nCho et al. presented the analysis of ML algorithms\
    \ for diagnosing mental illness. \nThey studied properties of mental health, techniques\
    \ to identify, their limitations, \nand how ML algorithms are implemented. The\
    \ authors considered SVM, GBM, KNN, \nNaïve Bayesian, KNN, Random Forest. The\
    \ authors achieved 75% from the SVM clas-\nsifier [30]. Reshma et.al proposed\
    \ a Tensi Strength framework for detecting senti-\nment analysis on Twitter [31].\
    \ The authors considered SVM, NB, WSD, and n-gram \ntechniques on large social\
    \ media text for sentiment analysis and applied the Lexicon \napproach to detect\
    \ stress and relaxation in large data set. The authors achieved 65% \nprecision\
    \ and 67% recall. Deshpande and Rao presented an emotion artificial intelli-\n\
    gence technique to detect depression [32]. The authors collected 10,000 Tweet\
    \ Using \nTwitter API. They applied SVM and Naïve Bayes machine learning algorithms\
    \ and \nTable 1 Comparison of different approaches in sentiment analysis\nResearcher\n\
    Technique\nPerformances\nPak and Paroubek [5]\nNaïve Bayes Sentiment classifier\
    \ with multinomial \nfeatures\nHigh accuracy, Low decision value\nAlec et al.\
    \ [23]\nNaïve Bayes classifier, Mutual information measure for \nfeature selection\n\
    Accuracy: 81%\nBalahur et al. [24]\nWordNet-lexicon\nAccuracy: 82%\nImprovement\
    \ in the baseline 21%\nJonathon et Al. [25]\nSVM, Naïve Bayes\nAccuracy: 70%\n\
    Boiy et al. [26]\nIntegrated approach: ML, Information retrieval, NLP\nAccuracy:\
    \ 83% (English texts),70% \naccuracy (Dutch texts), 68% \n(French texts)\nLi et\
    \ al. [27]\nDependency- Sentiment, LDA, Markov chain\nAccuracy: 70.7% with on\
    \ tenfold \ncross-validation test set: 800 \nreviews\nPage 6 of 24\nNijhawan et al.\
    \ Journal of Big Data            (2022) 9:33 \nachieved F1 scores of 79% and 83%\
    \ respectively. Zucco et  al. presented a prelimi-\nnary design of an integrating\
    \ Sentiment Analysis (SA) and Affective Computing (AC) \nmethodologies for depression\
    \ conditions monitoring [33]. The authors described SA \nand AC analysis pipelines.\
    \ They also presented main challenges such as online learn-\ning and stream analytics\
    \ for real-time processing in the design and implementation of \nsuch a system.\
    \ These can be overcome by using big data technology. The authors have \nnot presented\
    \ the final system and the results testing and validation. The literature for\
    \ \nstress detection shows that the models used for prediction need improvement.\
    \ The \nmental health prediction and monitoring also need to be combined with\
    \ other health \nparameters such as eating, sleeping, physiological and other\
    \ factors.\nRole of Big data in social media analytics\nSohangir et al. emphasized\
    \ that deep Learning is a valuable tool for big data [34]. It can \nbe used to\
    \ extract remarkable information hidden in big data, considering social net-\n\
    works data. They considered the stock market as the domain. The authors aim to\
    \ build \ndeep Learning models to improve the performance of sentiment analysis\
    \ using Stock \nTweets. Authors applied neural network models such as LSTM, doc2vec,\
    \ and CNN. \nThey concluded that the deep Learning model can be used effectively\
    \ for financial senti-\nment analysis with big data analytics.\nOpinion mining\
    \ is a significant area of NLP in big data utilizing data from social \nmedia.\
    \ Applications are working on customer reviews, opinions for sentiment, influence\
    \ \nanalysis. Bandari and Bulusu used a clustering strategy for the classification\
    \ of product \nreviews based on sentiment values [35]. Big data is filled with\
    \ a volume of structured or \nunstructured data. The realization of online service\
    \ depends on data from social media \nusers, customers. Most of such data is voluminous\
    \ and unstructured, hence requiring \nadvanced techniques to handle big data such\
    \ as Hadoop. Trupthi et al. proposed a feed-\nback collection system based on\
    \ structured query language [36]. The authors employed \na decision tree for classifying\
    \ reviews. A big data approach and machine learning algo-\nrithm are required\
    \ for the efficient analysis of social media data. Hammou et al. pro-\nposed a\
    \ neural network scheme in sentiment analysis; from this, they classified the\
    \ \ncustomer emotions with high accuracy [37].\nConsidering the literature review,\
    \ the focus of the current research is to leverage social \nmedia content by applying\
    \ machine learning and deep learning techniques to predict the \nemotional state\
    \ of a user. Further, use the analysis to detect stress. These models along \n\
    with other health parameters can be used in assessing the mental health of a patient.\n\
    Research design and methodology\nThe research makes use of both secondary and\
    \ primary data sources. It is a cross-sec-\ntional study and combination of quantitative\
    \ and qualitative methodologies to know the \nimpact of social and emotions associated\
    \ with the social media data and usefulness of \nthe same. The research aims at\
    \ building models for sentiment and emotion detection \nwhich can be used for\
    \ stress management, the models are also tested on primary data. \nThe focus of\
    \ the paper is identifying the sentiment or emotions of a user concerning \ndiverse\
    \ topics or domains using Latent Dirichlet Allocation (LDA). A hybrid machine\
    \ \nPage 7 of 24\nNijhawan et al. Journal of Big Data            (2022) 9:33 \n\
    \ \nlearning and deep learning models are built and executed to deliver the sentiment\
    \ analy-\nsis using the data that incorporates a broad range of tweets. The block\
    \ diagram of the \nrecommended model is as given in Fig. 1. Before moving on to\
    \ developing the analyzer, \nwe first need to perform data cleaning by implementing\
    \ the following steps. We per-\nform tokenization, remove the unwanted patterns,\
    \ remove the stop words, and perform \nstemming. A crucial measure in developing\
    \ a classifier is determining the features of \nthe input that are pertinent.\
    \ Then proceed to understand how to encode those features. \nWe extract feature\
    \ vectors with the help of the Bag-of-words method. Once the data is \nready,\
    \ we build our machine learning model for sentiment analysis and emotion detec-\n\
    tion. These machine learning models predict sentiment or emotion. We use accuracy,\
    \ F1 \nscore, and confusion matrix throughout to assess our model’s performance.\n\
    Introduction to the dataset\nThe dataset to train our ML model for binary sentiment\
    \ analysis has 100042 tweets \n[38]. The dataset which we utilize possesses three\
    \ columns: ‘id’, ‘sentiment label’, and \n‘sentiment text’. The sentiment label\
    \ can either be 0 for positive or 1 for negative. In \nthe training dataset, we\
    \ have three columns present. First is ’id’ which is linked to \nthe tweets in\
    \ the given dataset. The next indicates the tweets collected from diverse \nFig.\
    \ 1 Sentiment Analysis Methodology\nFig. 2 Head of the sentiment analysis dataset\
    \ (training)\nPage 8 of 24\nNijhawan et al. Journal of Big Data            (2022)\
    \ 9:33 \nsources where they indicate the tweet’s polarity as positive or negative.\
    \ The last is a \ntweet label. The first 15 tweets and labels are shown in Fig. 2.\
    \ It can be observed that \nthe tweets having words love, proud, new songs, blog\
    \ are labeled as 0 and the tweets \nwith words offended, lumpy are labeled as\
    \ 1.\nThe dataset used to train the model for emotion classification has 7934\
    \ tweets [39]. \nThis dataset has 3 columns namely ‘id’, ‘emotion’, and ‘text’.\
    \ The emotions are as fol-\nlows- joy, sadness, neutral, anger, and fear. Figure \
    \ 3(a) shows the number of data \nentries in every class. Joy has the maximum\
    \ number of data entries which are 2326 \nentries. The column details and some\
    \ of the tweets with labels are shown in Fig. 3(b).\nPreprocessing of the dataset\n\
    In data pre-processing, the aim is to perform data cleaning, data integration,\
    \ data \nreduction, and data transformation. We start with removing unwanted patterns\
    \ \nfollowed by removing the stop words and performing stemming. Before eliminat-\n\
    ing stop words, we need to perform tokenization as well. Stop words are words\
    \ that \ncommonly occur in any natural language. To analyze the textual data and\
    \ construct \nnatural language processing models we need to remove stop words.\
    \ Stop words don’t \nadd much significance to the meaning of the document. Words\
    \ like “is”, “a”, “on”, and \n“the” add no meaning to the statement while parsing\
    \ it so these stop words. Now \nafter this stemming is performed. Stemming plays\
    \ a pivotal role in the pipelining \ncourse in Natural language processing. The\
    \ input to the stemmer always needs to \nbe tokenized words. This paper takes\
    \ the aid of the Bag-of-Words method for fea-\nture extraction. It is a technique\
    \ used to extract features from textual documents. \nThe features can be further\
    \ utilized for training various ML techniques. It creates \na vocabulary of all\
    \ the distinctive words present in all the documents in the train-\ning set. After\
    \ this, the first task is to split the dataset into training and validation set\
    \ \nso that the training and testing of our model can begin before applying it\
    \ to predict \nunseen and unlabeled test data.\nFig. 3 Emotion classification\
    \ dataset (a) Distribution (b) Head of the dataset\nPage 9 of 24\nNijhawan et al.\
    \ Journal of Big Data            (2022) 9:33 \n \nTopic modelling with LDA\nThe\
    \ methodology in LDA first constitutes data pre-processing as shown in Fig. 4.\
    \ A \ndictionary is created containing the number of times a word appears in the\
    \ training \nset and all the anomalies are filtered out. For every document, a\
    \ dictionary is created \nreporting how many words and how many times those words\
    \ appear. LDA has three \nimportant hyperparameters. The first one is ‘alpha’\
    \ which outlines a document-topic \ndensity factor. The second one is ‘beta’ which\
    \ denotes word density in a topic. The \nthird one is ‘k’, or the number of components\
    \ signifying the number of topics the \ndocument is to be clustered or divided.\
    \ \nBinary sentiment classification\nSupervised learning problems can branch into\
    \ two categories which are regression and \nclassification problems. The problem\
    \ which the paper addresses come under the classifi-\ncation category because\
    \ we must classify our results into either the Positive or Negative \nclass. Three\
    \ models are implemented which are Logistic Regression, Decision Trees, and \n\
    Random Forest. Pseudocodes of these algorithms are shown in Algorithm 1, 2, and\
    \ 3. \nTheir performance is compared, and the best possible model is chosen. We\
    \ used accu-\nracy, F1 score, and confusion matrix throughout to assess our model’s\
    \ performance. \nRandom Forest has the best accuracy and does well in all the\
    \ other parameters as well \nwhen in comparison to the other models.\nFig. 4 LDA\
    \ Architecture\nPage 10 of 24\nNijhawan et al. Journal of Big Data           \
    \ (2022) 9:33 \nEmotion analysis with BERT model\nAfter loading the BERT Classifier\
    \ and Tokenizer along with the Input modules. The \nconfiguration of the loaded\
    \ BERT model and the fine-tuning to make it ready to make \nfurther predictions\
    \ begins. In this paper, the BERT model has been trained using \nktrain to recognize\
    \ the emotion on text. Text classification is performed with the \nhelp of the\
    \ ktrain library. As shown in Fig. 5, BERT utilizes the features of a Trans-\n\
    former, a capable structure that studies contextual relations in a text with regards\
    \ to \nwords. In its plain arrangement, a transformer comprises two distinct mechanisms.\
    \ \nThe first mechanism is an encoder that peruses the input. The second mechanism\
    \ is a \ndecoder that induces a prediction for the respective assignment. In contrast\
    \ to direc-\ntional models, which peruse the input successively, the whole arrangement\
    \ of words \nPage 11 of 24\nNijhawan et al. Journal of Big Data            (2022)\
    \ 9:33 \n \nis delivered at once by the Transformer encoder. Hence, it is regarded\
    \ as a bidirec-\ntional model. However, it is more precise to state it non-directional.\
    \ \nResults and discussion\nIn this section, we present exploratory analysis,\
    \ results of topic modeling, binary senti-\nment analysis using ML algorithms,\
    \ and emotion detection using the BERT model.\nData exploratory analysis\nFigure 6\
    \ shows an exploratory analysis of the tweets. Figure 6 depicts the positive and\
    \ \nnegative tweets in the training dataset. Over here ‘0’ denotes positive tweet\
    \ and ‘1’ \ndenotes negative tweet. We can observe there are more than 50000 positive\
    \ tweets and \naround 40000 negative tweets in the sentiment analysis dataset.\
    \ In Fig. 7 we are check-\ning the distribution of tweets in the training and\
    \ testing dataset. The training dataset \nis shown in pink color whereas the testing\
    \ dataset is shown in orange color. This graph \ndenotes that there are more tweets\
    \ in the training dataset and the length is between 0 \nand 200 characters for\
    \ both datasets (Fig. 7). \nFig. 5 BERT Architecture\nFig. 6 Number of Negative\
    \ and Positive Tweets in the training dataset\nPage 12 of 24\nNijhawan et al.\
    \ Journal of Big Data            (2022) 9:33 \nIn the bar plot shown in Fig. 8,\
    \ we can observe the thirty most frequently occurring \nwords. We perform this\
    \ with the help of the CountVectorizer function. We can observe \nthat the word\
    \ quot occurs more than 8000 times in the dataset. The word quot is fol-\nlowed\
    \ by just, good, and like respectively. Word Cloud is the kind of visualization\
    \ where \nthe most recurrent words are showcased in bigger sizes and the less\
    \ recurrent words are \nshowcased in relatively small sizes. In Python, we have\
    \ a package for producing Word-\nCloud. In this paper, we have showcased the top\
    \ 30 most recurring words in my dataset \nwith the help of WordCloud and Bar plots.\
    \ The WordCloud in Fig. 9 shows the 30 most \nfrequently occurring words. In WordCloud\
    \ the word occurring the most commonly \nappears the largest. Since quot is most\
    \ recurring it is shown to be the biggest here.\nTopic modelling LDA\nTopic modeling\
    \ results are shown in Fig. 10. It depicts the top 10 models and the clus-\nter\
    \ of words falling each of the topics. We can observe that 0, 1 and 2 are related\
    \ the \ncollege life and some words depict the sad status. Topic modeling is beneficial,\
    \ but it’s \ntough to comprehend it just by having a glance at the combination\
    \ of words and sta-\ntistics. One of the most efficacious ways to interpret data\
    \ is done with the assistance \nFig. 7 Distribution of tweets in the training\
    \ and testing dataset\nFig. 8 Top 30 most frequently occurring words\nPage 13\
    \ of 24\nNijhawan et al. Journal of Big Data            (2022) 9:33 \n \nof visualization.\
    \ We used PyLDAvis collaborative LDA visualization python package to \nvisualize\
    \ topic modeling results. PyLDAvis permits us to comprehend the subjects in a\
    \ \ntopic model. With the assistance of this package, we get to realize the most\
    \ recurring \nwords in every individual topic along with their occurrence. Moreover,\
    \ it even demon-\nstrates how related are the topic to each other.\nEach bubble\
    \ in PyLDAvis indicates a topic. The bigger the bubble, the higher fraction \n\
    of the number of tweets in the corpus is concerning that topic. Blue bars signify\
    \ the gen-\neral frequency of each word in the corpus. If no topic is chosen,\
    \ the blue bars of the most \nused words will be shown. Red bars give the projected\
    \ number of times a given term was \nproduced by a given topic. We can conclude\
    \ from the graph shown in Fig. 11, that the \nwords sad, school, and food are\
    \ the most recurring words in the dataset. Visualization \nfor topics 1, 3, and\
    \ 5 can be seen in Figures 12, 13, and 14. This is a very good interpre-\ntation\
    \ to understand the overall orientation of the tweets and sentiments. It is a\
    \ way to \nexplore the tweets. Further, these topics can also be used to group\
    \ and label the group \nof words. With topic modeling analysis of tweets, we could\
    \ interpret that most of the \ntweets are belonging to student life with neutral\
    \ and sad emotions.\nBinary sentiment analysis\nThe sample tweets from the binary\
    \ sentiment dataset are labeled as ‘0’ or ‘1’ where 0 \nstands for positive sentiment\
    \ and 1 stands for negative sentiment as shown in Figures 15 \nand 16. For example,\
    \ the tweets having words like “love”, “thank”, “welcome” etc. have \nthe label\
    \ ‘0’ and tweets with words “insecure”, “lumpy”, “devasted” etc. are labeled as\
    \ \n‘1’. These tweets are pre-processed and are given to ML models for training.\
    \ Figure 17 \ndepicts the evaluation metrics for trained models built using logistic\
    \ regression, decision \ntree, and random forest. The ML algorithms are implemented\
    \ in 10 randomized experi-\nmental runs. We concluded that Random Forest Classifier\
    \ is a better model than Logis-\ntic Regression and Decision Trees due to a high\
    \ accuracy which was 97.78% on the test \ndataset. Hence, it is used in the final\
    \ framework. Predictions obtained for training data \nusing random Forest are\
    \ shown in Fig. 18. The tweets which are marked as negative need \nfurther analysis\
    \ for stress conditions. In the initial and predicted labels, most of the neu-\n\
    tral tweets are classified as positive, hence it will not affect identifying the\
    \ stress condi-\ntions. To confirm this, we also implemented the Vader algorithm\
    \ which classifies tweets \nFig. 9 WordCloud (a) top 30 words (b) most recurring\
    \ positive words\nPage 14 of 24\nNijhawan et al. Journal of Big Data         \
    \   (2022) 9:33 \ninto positive, negative, and neutral using a sentiment dictionary.\
    \ The output of the Vader \nsentiment analysis is shown in Fig. 19. It can be\
    \ observed that compound values are from \n−1 to 1 and values between −0.5 to\
    \ 0.5 are identified as neutral. We also tested the ran-\ndom forest model for\
    \ random tweets and one of the outputs where the tweet is classified \nas positive\
    \ is shown in Fig. 20.    \nBERT model results\nSince the emotion classifier has\
    \ 5 classes namely- joy, sad, neutral, angry, and fear. \nIt will be categorizing\
    \ the tweets in those emotions only. The training of the model \nand accuracies\
    \ obtained at different epochs are shown in Fig. 21. It can be observed \nthat\
    \ the training accuracy of the BERT model after using one cycle policy at a learn-\n\
    ing rate of 0.00002 is 94%. The evaluation of the model on the test set of 6000\
    \ tweets \nis shown in Fig. 22. The figure depicts micro evaluation metrics: accuracy,\
    \ F1 score, \nFig. 10 Topic Modelling LDA results (a) topic 1 to topic 4 (b) topic\
    \ 5 to topic 8 (c) topic 9 to topic 10\nPage 15 of 24\nNijhawan et al. Journal\
    \ of Big Data            (2022) 9:33 \n \nprecision, recall, and sensitivity for\
    \ each class. It can be observed that the model \nhas a good F1-score and accuracy\
    \ for all the classes. The model has a macro aver-\nage accuracy of 94%, and a\
    \ macro F1-score of 83%. It indicates that the model is not \noverfitting.  \n\
    The web portal is designed for topic modeling and emotion detection. The model\
    \ \nis used to classify emotions in the web portal. Figures 23, 24, 25, 26, 27\
    \ indicate the \nclassification of the emotion for the post given as input. These\
    \ depict the prediction \nafter entering the text. The figures depict the output\
    \ for each of the emotions for \ncorresponding input texts entered as input. When\
    \ the input is classified as negative. \nSadness, anger, or fear, we further analyze\
    \ more tweets to classify the user as stres\nsed.\nFig. 10 continued\nFig. 11\
    \ Representation of the top salient terms in the dataset\nPage 16 of 24\nNijhawan et al.\
    \ Journal of Big Data            (2022) 9:33 \nFig. 12 Topic 1’s most frequent\
    \ words (marked in red)\nFig. 13 Topic 3’s most frequent words (marked in red)\n\
    Fig. 14 Topic 5’s most relevant words\nPage 17 of 24\nNijhawan et al. Journal\
    \ of Big Data            (2022) 9:33 \n \nConclusion\nSentiment and emotion analysis\
    \ is an area of learning to examine opinions expressed \nin the text on numerous\
    \ social media websites. In this paper, we presented the \nexploratory analysis\
    \ of user tweets using LDA topic modeling and visualization. \nWe emphasized the\
    \ importance of data visualizations as it helps us in getting an \napt understanding\
    \ of our data. In this research work, extracted tweets are analyzed \nby using\
    \ LDA to settle on the number of topics and the percentage of a word in a \nspecific\
    \ topic. The outcomes presented that the extracted topics display a signifi-\n\
    cant structure in the data. We applied and evaluated machine learning algorithms\
    \ \nfor sentiment analysis, the BERT model for emotion analysis. Models are fine-tuned\
    \ \nfor the sentiment classification task of 5 different classes—Joy, Sadness,\
    \ Neutral, \nAnger, and Fear. We have verified the classification competence in\
    \ NLP supported \nFig. 15 Tweets with label 0 in the training set\nFig. 16 Tweets\
    \ with label ‘1’ in the training set\nPage 18 of 24\nNijhawan et al. Journal of\
    \ Big Data            (2022) 9:33 \nby deep contextual language models like BERT\
    \ with an accuracy of 94%. The paper \naimed to employ these techniques for detecting\
    \ users’ mental stress from twitter’s \nsocial media facts and figures. After\
    \ implementing 3 machine learning techniques \nin the overall of 10 randomized\
    \ experimental runs, we concluded that Random For-\nest Classifier is a better\
    \ algorithm than Logistic Regression and Decision Trees with \nan accuracy of\
    \ 97.78%. We developed a web portal that takes the text posted by the \nuser as\
    \ the input and identifies the emotion. The portal has shown accurate classi-\n\
    fications for any given tweet. The work can be further extended by combining other\
    \ \nhealth parameters to monitor mental health.\nFig. 17 Evaluation metric for\
    \ binary classification on training data for ML models\nFig. 18 Output Excel file\
    \ after Binary Sentiment Analysis using Random Forest\nPage 19 of 24\nNijhawan et al.\
    \ Journal of Big Data            (2022) 9:33 \n \nFuture work\nEffectual analysis\
    \ of policy opinionated content\nThe future scope of the paper is to develop a\
    \ system that not only detects stress but \nalso analyses the topic of discussion\
    \ in a particular tweet. This could work as a survey \nsystem. It would provide\
    \ a better solution on every debatable topic and tell the popu-\nlar choice/verdict\
    \ in areas like politics and news. This will help us efficiently analyze \nstress\
    \ and express opinions for prevailing social issues.\nDetection of spam and non‑spam\
    \ tweets\nThis paper could help analyze if a tweet is spam or non-spam. This could\
    \ potentially \nhelp naïve Twitter users be aware of spam accounts which could\
    \ be harmful to a lot \nFig. 19 Output Excel file after Binary Sentiment Analysis\
    \ using Vader Algorithm\nFig. 21 Bert model created with an accuracy of 94 percent\
    \ for overall classification (Macro)\nFig. 20 Sentiment Analysis of a tweet\n\
    Page 20 of 24\nNijhawan et al. Journal of Big Data            (2022) 9:33 \nof\
    \ Twitter users. The non–spam tweets can also be further classified to make sure\
    \ the \nones which are damaging are removed from the Twitter platform.\nImproving\
    \ sentiment word identification algorithm\nWith social media, there are a lot\
    \ of impediments. A tweet can have abbreviations, slang, \nand jargon which is\
    \ difficult to interpret. This project can be further used to perform \nanalysis\
    \ on short sentences and abbreviations to get a better idea. Additionally, people\
    \ \nshould work on the generation of a high content lexicon database. There should\
    \ also \nbe successful handling of bi-polar sentiments. All these features combined\
    \ would help \ndevelop an astounding analyzing tool.\nFig. 22 Performance evaluation\
    \ of test set using BERT model (Micro level)\nFig. 23 Neutral Tweet with the time\
    \ taken 85 s\nPage 21 of 24\nNijhawan et al. Journal of Big Data            (2022)\
    \ 9:33 \n \nDynamic topic model\nA Dynamic Topic Model will examine the fluctuations\
    \ of subjects done over time; it \nis likewise important to consider the addition\
    \ of time-varying information. Executing \na topic modeling outline that will\
    \ allow the incorporation of supplementary data will \nFig. 24 Joy Tweet with\
    \ the time taken 5 s\nFig. 25 Sadness Tweet with the time taken 5 s\nFig. 26 Anger\
    \ Tweet with the time taken 5 s\nPage 22 of 24\nNijhawan et al. Journal of Big\
    \ Data            (2022) 9:33 \nproduce an advantageous potential in the turf\
    \ of publicizing research. Additionally, \nintegrating some method of direction\
    \ during topic generation can help interpret the \nderivative solutions.\nAcknowledgements\n\
    Nil.\nAuthors’ contributions\nTN: The author has designed and implemented all\
    \ the phases of the project. GA: The author has guided throughout the \nprocess\
    \ of the project and has performed result analysis. TA: The author has finalized\
    \ the structure and content of the \nmanuscript and has done the proofreading.\
    \ All authors read and approved the final manuscript.\nAuthors’ informations\n\
    Tanya Nijhawan is a student in the Department of Electronics and Communication\
    \ Engineering, Manipal Institute of \nTechnology (MIT), MAHE, Manipal, India.\
    \ Her research interests include Data Science, Data Mining, Machine Learning\n\
    Girija Attigeri is currently Assistant Professor-Selection Grade in the Department\
    \ of Information and Communication \nTechnology, Manipal Institute of Technology,\
    \ Manipal Academy of Higher Education, Manipal, India. She has received B.E. \n\
    and M. Tech. degrees from the Visvesvaraya Technological University, Karnataka,\
    \ India. She has 15 years of experience in \nteaching and research. She has received\
    \ his Ph.D. from the Manipal Institute of Technology, Karnataka, India. His research\
    \ \ninterests span big data analytics, Machine Learning, and Data Science. She\
    \ has around 10 publications in reputed inter-\nnational conferences and journals\n\
    Ananthakrishna Thalengala has received his M.Sc. degree in 1998 in Electronic\
    \ Science from Mangalore University, India, \nM. Tech. degree in 2004 in Computer\
    \ Cognition Technology, from Mysore University, India, and a Ph.D. degree in 2019\
    \ \nfrom Manipal Academy of Higher Education (MAHE), Manipal, India. Since 2004\
    \ he is with Manipal Institute of Technol-\nogy (MIT), MAHE, Manipal, India, where\
    \ he is currently an Assistant Professor in the Department of Electronics and\
    \ \nCommunication Engineering. He is a senior member of IEEE, and his areas of\
    \ interest include Signal processing, Pattern \nclassification, and Machine learning.\n\
    Funding\nNot applicable.\n Availability of data and materials\nThe sources of\
    \ the data are cited in the paper. They are 31st and 32nd references.\nDeclarations\n\
    Ethics approval and consent to participate\nNot applicable.\nConsent for publication\n\
    Not applicable.\nCompeting interests\nNot applicable.\nFig. 27 Fear Tweet with\
    \ the time taken 5 s\nPage 23 of 24\nNijhawan et al. Journal of Big Data     \
    \       (2022) 9:33 \n \nAuthor details\n1 Depertment of Electronics and Communication\
    \ Engineering, Manipal Institute of Technology, Manipal Academy \nof Higher Education,\
    \ Manipal, Karnataka, India. 2 Depertment of Information and Communication Technology,\
    \ Manipal \nInstitute of Technology, Manipal Academy of Higher Education, Manipal,\
    \ Karnataka, India. \nReceived: 2 November 2021   Accepted: 6 February 2022\n\
    References\n 1. \nLiang Y, Zheng X, Zeng DD. A survey on big data-driven digital\
    \ phenotyping of mental health. Inform Fusion. \n2019;52(1):290–307.\n 2. \nLiu\
    \ B, Zhang L. A survey of opinion mining and sentiment analysis. Boston: Springer\
    \ US. 2012; p. 415–463.\n 3. \nMunikar M, Shakya S, Shrestha A. Fine-grained sentiment\
    \ classification using BERT. Artif Intell Transform Business \nSociety. 2019;2019:1–5.\
    \ https:// doi. org/ 10. 1109/ AITB4 8515. 2019. 89474 35.\n 4. \nWang B, Liu\
    \ Y, Liu Z, Li M, Qi M. Topic selection in latent Dirichlet allocation, 2014 11th\
    \ International Conference on \nFuzzy Systems and Knowledge Discovery (FSKD).\
    \ 2014. p. 756–760. https:// doi. org/ 10. 1109/ FSKD. 2014. 69809 31.\n 5. \n\
    Alexander P, Patrick P. Twitter as a corpus for sentiment analysis and opinion\
    \ mining. Proceedings of LREC. 2010.\n 6. \nJianqiang Z, Xiaolin G. Comparison\
    \ research on text pre-processing methods on Twitter sentiment analysis. IEEE\
    \ \nAccess. 2017;5:2870–9. https:// doi. org/ 10. 1109/ ACCESS. 2017. 26726 77.\n\
    \ 7. \nPradha S, Halgamuge MN, Vinh NQT. Effective text data preprocessing technique\
    \ for sentiment analysis in social \nmedia data, 2019 11th International Conference\
    \ on Knowledge and Systems Engineering (KSE). 2019. p. 1–8. \nhttps:// doi. org/\
    \ 10. 1109/ KSE. 2019. 89193 68.\n 8. \nDeepa DR, Tamilarasi A. Sentiment analysis\
    \ using feature extraction and dictionary-based approaches, 2019 Third \nInternational\
    \ conference on I-SMAC (IoT in Social, Mobile, Analytics, and Cloud) (I-SMAC).\
    \ 2019. p. 786–790. https:// \ndoi. org/ 10. 1109/I- SMAC4 7947. 2019. 90324 56.\n\
    \ 9. \nChaturvedi S, Mishra V, Mishra N. Sentiment analysis using machine learning\
    \ for business intelligence, 2017 IEEE \nInternational Conference on power, control,\
    \ signals, and instrumentation engineering (ICPCSI). 2017. p. 2162–2166. \nhttps://\
    \ doi. org/ 10. 1109/ ICPCSI. 2017. 83921 00.\n 10. Ho J, Ondusko D, Roy B, Hsu\
    \ DF. Sentiment analysis on tweets using machine learning and combinatorial fusion,\
    \ \n2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf\
    \ on Pervasive Intelligence and \nComputing, Intl Conf on Cloud and Big Data Computing,\
    \ Intl Conf on Cyber Science and Technology Congress \n(DASC/PiCom/CBDCom/CyberSciTech).\
    \ 2019. p. 1066–1071. https:// doi. org/ 10. 1109/ DASC/ PiCom/ CBDCom/ Cyber\
    \ \nSciTe ch. 2019. 00191.\n 11. Apoorv A, Boyi X, Ilia V, Owen R, Rebecca P.\
    \ Sentiment analysis of Twitter Data. Proceedings of the Workshop on \nLanguages\
    \ in Social Media. 2011.\n 12. Peddinti MK, Chintalapoodi P. Domain adaptation\
    \ in sentiment anlaysis of Twitter, in Analyzing Microtext Workshop, \nAAAI, 2011.\n\
    \ 13. Dmitry D, Oren T, Ari R. Enhanced sentiment learning using twitter hashtags\
    \ and smileys. Coling 2010—23rd Inter-\nnational Conference on Computational Linguistics,\
    \ Proceedings of the Conference. 2. 2010; 241–249.\n 14. Anupriya P, Karpagavalli\
    \ S. LDA based topic modeling of journal abstracts. Int Conf Adv Comput Commun\
    \ Syst. \n2015;2015:1–5. https:// doi. org/ 10. 1109/ ICACCS. 2015. 73240 58.\n\
    \ 15. Xiance S, Maosong S. Tag-LDA for scalable real-time tag recommendation.\
    \ J Comput Inform Syst. 2008;6:23.\n 16. Krestel R, Fankhauser P. Personalized\
    \ topic-based tag recommendation. Neurocomputing. 2012;76:61–70. https:// \ndoi.\
    \ org/ 10. 1016/j. neucom. 2011. 04. 034.\n 17. Peters ME, Neumann M. Deep contextualized\
    \ word representations. 2018.\n 18. Radford A, Narasimhan K. Improving language\
    \ understanding by generative pre-training. 2018.\n 19. Devlin J, Chang M, Lee\
    \ K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language\
    \ under-\nstanding, in Proceedings of the 2019 Conference of the North American\
    \ Chapter of the Association for Computa-\ntional Linguistics: Human Language\
    \ Technologies, NAACL-HLT 2019, vol 1. Minneapolis; 2019. p. 4171–4186. https://\
    \ \ndoi. org/ 10. 18653/ v1/ n19- 1423.\n 20. Jin Z, Lai X, Cao J. Multi-label\
    \ sentiment analysis base on BERT with modified TF-IDF, 2020 IEEE International\
    \ Sym-\nposium on Product Compliance Engineering-Asia (ISPCE-CN), 2020. https://\
    \ doi. org/ 10. 1109/ ISPCE- CN512 88. 2020. \n93218 61.\n 21. Zubair M, Aurangzeb\
    \ K, Shakeel A, Maria Q, Ali KI, Quan Z. Lexicon-enhanced sentiment analysis framework\
    \ using \nrule-based classification scheme. Plos One. 2017;12(2):e0171649.\n 22.\
    \ Zeng D, Dai Y, Li F, Wang J, Sangaiah AK. Aspect based sentiment analysis by\
    \ a linguistically regularized CNN with \ngated mechanism. J Intell Fuzzy Syst.\
    \ 2019;36(5):3971–80. https:// doi. org/ 10. 3233/ JIFS- 169958.\n 23. Alec G,\
    \ Richa B, Lei H. Twitter sentiment classification using distant supervision.\
    \ Processing. 2009; 150.\n 24. Alexandra B, Ralf S, Mijail K, Vanni Z, van der\
    \ Erik G, Matina H, Bruno P, Jenya B. Sentiment analysis in the news. \nproceedings\
    \ of LREC. (n-1). 2013\n 25. Jonathon Read. Using emoticons to reduce dependency\
    \ in machine learning techniques for sentiment classifica-\ntion. In Proceedings\
    \ of the ACL Student Research Workshop. Association for Computational Linguistics,\
    \ USA, 43–48. \n[N-2]. 2005.\n 26. Boiy E, Moens MF. A machine learning approach\
    \ to sentiment analysis in multilingual web texts. Inf Retrieval. \n2009;12:526–58.\
    \ https:// doi. org/ 10. 1007/ s10791- 008- 9070- z[N+1].\n 27. Li F, Huang M,\
    \ Zhu X. Sentiment analysis with global topics and local dependency. In Proceedings\
    \ of the Twenty-\nFourth AAAI Conference on Artificial Intelligence, AAAI Press.\
    \ 2010. 1371–1376.\nPage 24 of 24\nNijhawan et al. Journal of Big Data       \
    \     (2022) 9:33 \n 28. Arya V, Mishra AK. Machine learning approaches to mental\
    \ stress detection: a review. Ann Optimization Theory \nPract. 2021;31(4):55–67.\n\
    \ 29. Aldarwish MM, Ahmad HF. Predicting Depression Levels Using Social Media\
    \ Posts, 2017 IEEE 13th International \nSymposium on Autonomous Decentralized\
    \ System (ISADS), Bangkok. 2017. pp. 277–280. https:// doi. org/ 10. 1109/ \n\
    ISADS. 2017. 41.\n 30. Cho G, Yim J, Choi Y, Ko J, Lee SH. Review of machine learning\
    \ algorithms for diagnosing mental illness. Psychiatry \nInvest. 2019;16(4):262–9.\n\
    \ 31. Baheti RR, Kinariwala SA. Survey: sentiment stress identification using\
    \ tensi/strength framework. Int J Sci Res Eng \nDev. 2019;2(3):1–8.\n 32. Deshpande\
    \ M, Rao V. Depression detection using emotion artificial intelligence, 2017 International\
    \ Conference on \nIntelligent Sustainable Systems (ICISS), Palladam, India. 2017.\
    \ p. 858–862\n 33. Zucco C, Calabrese B, Cannataro M. Sentiment Analysis and Affective\
    \ Computing for Depression Monitoring. In 2017 \nIEEE international conference\
    \ on bioinformatics and biomedicine (BIBM). New York: IEEE. 2017. p. 1988–1995.\n\
    \ 34. Sohangir S, Wang D, Pomeranets A, Khoshgoftaar TM. Big Data: Deep Learning\
    \ for Financial sentiment analysis. J Big \nData. 2018;5(1):1–25.\n 35. Bandari\
    \ S, Bulusu VV. Survey on ontology-based sentiment analysis of customer reviews\
    \ for products and services. \nIn Data Engineering and Communication Technology.\
    \ Springer: Singapore. 2020; p. 91–101.\n 36. Trupthi M, Pabboju S, Gugulotu N.\
    \ Deep Sentiments Extraction for Consumer Products Using NLP-Based Technique.\
    \ \nIn Soft Computing and Signal Processing. Springer: Singapore; 2019. p. 191–201.\n\
    \ 37. Hammou BA, Lahcen AA, Mouline S. A distributed ensemble of deep convolutional\
    \ neural networks with random \nforest for big data sentiment analysis. In International\
    \ Conference on Mobile, Secure, and Programmable Network-\ning. Springer: Cham;\
    \ 2019. p. 153–162.\n 38. Vardhanapu K. Sentiment analysis, IEEE Dataport. 2020.\
    \ https:// doi. org/ 10. 21227/ e2aq- xv12.\n 39. Damian. Detecting Emotions in\
    \ Text, https:// data. world/ damof/ detec ting- emoti ons- in- text. 2021.\n\
    Publisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional\
    \ claims in published maps and institutional affiliations.\n"
  inline_citation: '>'
  journal: Journal of big data
  limitations: '>'
  pdf_link: https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-022-00575-6
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Stress detection using natural language processing and machine learning over
    social interactions
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/en14164776
  analysis: '>'
  authors:
  - Seyed Mahdi Miraftabzadeh
  - Michela Longo
  - Federica Foiadelli
  - Marco Pasetti
  - Raúl Igual
  citation_count: 42
  full_citation: '>'
  full_text: ">\nenergies\nArticle\nAdvances in the Application of Machine Learning\
    \ Techniques\nfor Power System Analytics: A Survey †\nSeyed Mahdi Miraftabzadeh\
    \ 1,*\n, Michela Longo 1,*\n, Federica Foiadelli 1, Marco Pasetti 2\nand Raul\
    \ Igual 3\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\n\
    Citation: Miraftabzadeh, S.M.;\nLongo, M.; Foiadelli, F.; Pasetti, M.;\nIgual,\
    \ R. Advances in the Application\nof Machine Learning Techniques for\nPower System\
    \ Analytics: A Survey.\nEnergies 2021, 14, 4776. https://\ndoi.org/10.3390/en14164776\n\
    Academic Editors: Don Lee,\nEklas Hossain and Ahmed Abu-Siada\nReceived: 1 June\
    \ 2021\nAccepted: 2 August 2021\nPublished: 6 August 2021\nPublisher’s Note: MDPI\
    \ stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional\
    \ afﬁl-\niations.\nCopyright: © 2021 by the authors.\nLicensee MDPI, Basel, Switzerland.\n\
    This article is an open access article\ndistributed\nunder\nthe\nterms\nand\n\
    conditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\n1\nDepartment of Energy, Politecnico di Milano, Via Lambruschini 4, 20156\
    \ Milano, Italy;\nfederica.foiadelli@polimi.it\n2\nDepartment of Information Engineering,\
    \ University of Brescia, Via Branze 38, 25123 Brescia, Italy;\nmarco.pasetti@unibs.it\n\
    3\nEduQTech, Electrical Engineering Department, EUP Teruel, Universidad de Zaragoza,\
    \ 44003 Teruel, Spain;\nrigual@unizar.es\n*\nCorrespondence: seyedmahdi.miraftabzadeh@polimi.it\
    \ (S.M.M.); michela.longo@polimi.it (M.L.);\nTel.: +39-02-2399-3759 (M.L.)\n†\n\
    This paper is an extension of the conference paper presented at IEEE EEEIC 2020,\
    \ Genova, Italy, 11–14 June 2019.\nAbstract: The recent advances in computing\
    \ technologies and the increasing availability of large\namounts of data in smart\
    \ grids and smart cities are generating new research opportunities in the\napplication\
    \ of Machine Learning (ML) for improving the observability and efﬁciency of modern\n\
    power grids. However, as the number and diversity of ML techniques increase, questions\
    \ arise about\ntheir performance and applicability, and on the most suitable ML\
    \ method depending on the speciﬁc\napplication. Trying to answer these questions,\
    \ this manuscript presents a systematic review of the\nstate-of-the-art studies\
    \ implementing ML techniques in the context of power systems, with a speciﬁc\n\
    focus on the analysis of power ﬂows, power quality, photovoltaic systems, intelligent\
    \ transportation,\nand load forecasting. The survey investigates, for each of\
    \ the selected topics, the most recent and\npromising ML techniques proposed by\
    \ the literature, by highlighting their main characteristics and\nrelevant results.\
    \ The review revealed that, when compared to traditional approaches, ML algorithms\n\
    can handle massive quantities of data with high dimensionality, by allowing the\
    \ identiﬁcation of\nhidden characteristics of (even) complex systems. In particular,\
    \ even though very different techniques\ncan be used for each application, hybrid\
    \ models generally show better performances when compared\nto single ML-based\
    \ models.\nKeywords: machine learning; power systems; smart grids; power ﬂows;\
    \ power quality; photovoltaic;\nintelligent transportation; load forecasting;\
    \ survey\n1. Introduction\nThe power system management and development have constantly\
    \ been changing\ndue to expanding complexity and distributed modern power networks.\
    \ [1]. Principally,\nthe increasing distribution of Renewable Energy Sources (RESs)\
    \ with intermittent energy\ngeneration and technological novelties in power system\
    \ management and control demand\nreliable power predictions and more precise monitoring\
    \ models [2,3]. In recent years,\nresearchers developed advanced solutions based\
    \ on Machine Learning (ML) algorithms to\nsolve the bottleneck of conventional\
    \ lumped parameter simulations. In practice, conven-\ntional traditional simulation\
    \ techniques based on deterministic methods are still dominated\nin power grids.\
    \ However, the high performance of machine learning solutions in terms of\naccuracy\
    \ computational speed, and scalability brings novelties in power grids management\n\
    and control. Therefore, it is expected to boost the adaptation of these techniques\
    \ for short-\nto medium-term forecasts of the power grid system operation to meet\
    \ this gap while getting\nbeneﬁts of advantages of traditional approaches.\nEnergies\
    \ 2021, 14, 4776. https://doi.org/10.3390/en14164776\nhttps://www.mdpi.com/journal/energies\n\
    Energies 2021, 14, 4776\n2 of 24\nAs the high-quality sensor prices–such as smart\
    \ meters, Phasor Measurement Units\n(PMUs), and Remote Terminal Units (RTUs),\
    \ or other measurement devices—are constantly\ndecreased, they are increasingly\
    \ distributed in the power system and are continuously\nacquiring a massive amount\
    \ of heterogeneous datasets. Analyzing and processing all these\ndata provides\
    \ new insights and advances in the control and operation of smart grids thanks\n\
    to innovation in ML and big data frameworks to handle structured and unstructured\
    \ data.\nTraditional time-domain methods are computationally inefﬁcient; thus,\
    \ they are not good\ncandidates for real-time applications in which response time\
    \ is a decisive concern [4,5].\nThe expected signiﬁcant penetration of Electric\
    \ Vehicles (EV) charging stations, and the\nincreasing expansion of the Internet\
    \ of Things (IoT) devices in private and public sectors\nsuch as smart buildings\
    \ introduce new challenges and opportunities for the perception\nof accurate Day-Ahead\
    \ Load Forecasts (DALFs) in micro-and smart- grids [6,7]. At the\nsame time, the\
    \ transition towards decarbonization of systems leads to the integration of\n\
    distributed energy systems which usually generate energy in an intermittent and\
    \ stochastic\nmanner, such as wind or solar energy generators with no inertia.\
    \ Consequently, the growing\ncomplexity of power ﬂow patterns requires novel approaches\
    \ to render reliable, efﬁcient,\nand economical solutions.\nIn this context, advanced\
    \ machine learning models have been shown promising results\nto provide new valuable\
    \ knowledge and insights and identify hidden data patterns, trends,\nand relationships.\
    \ In [8], the authors brieﬂy summarized the ML paradigm and presented\nthe literature\
    \ review on applications of ML methods in Power systems till the end of\n2017.\
    \ This paper continues the authors’ work presented at [8] and aims at providing\
    \ a\nsystematic review of the various machine learning algorithms used to analyze,\
    \ monitor, and\nmodel power ﬂows, power quality events, photovoltaic systems,\
    \ intelligent transportation\nsystems, and load forecasting services. The authors\
    \ selected the Journal papers for literature\nreview based on publication date,\
    \ number of citations, and novelty in contributions. The\nmain contribution of\
    \ this article is as follows:\nThe ML paradigm and well-known ML algorithms are\
    \ categorized and presented;\nThe systematic review summarizes not only the main\
    \ contributions of each article but\nalso provides information regarding the explicit\
    \ application, data source, and models, by\nmainly considering articles published\
    \ since 2018; this study used Google Scholar, Scopus,\nIEEE Xplore, and the MPDI\
    \ databases for literature review, which ended in February 2021.\nTo make a fair\
    \ comparison between models, the characteristics of a standard dataset\nfor the\
    \ testing of the reviewed models are presented.\nThe remainder of the article\
    \ is structured as follows: Section 2 explains the machine\nlearning paradigms,\
    \ well-known algorithms, and performance metrics. A literature survey\non recent\
    \ advanced machine learning applications in power ﬂow, power quality, pho-\ntovoltaic\
    \ systems, electric transportation systems, and load forecasting is presented\
    \ in\nSection 3. Section 4 discusses the results and notes achieved in the literature\
    \ review, and\nSection 5 summarizes the ﬁnal remarks and conclusions.\n2. Machine\
    \ Learning\nArtiﬁcial Intelligence (AI) deals with the broad topic related to\
    \ the perception and\nextraction of knowledge from data. AI can be divided into\
    \ two main subsets: machine\nlearning and deep learning. Machine learning is the\
    \ main subset of artiﬁcial intelligence,\nwhile deep learning can be represented\
    \ as a subset of machine learning.\nMachine learning is an interdisciplinary research\
    \ ﬁeld that consolidates expertise and\nknowledge from diverse areas and aims\
    \ at proposing solutions to given problems that\ncan be used to reply to similar\
    \ questions raised by different contexts. More speciﬁcally,\nmachine learning\
    \ is the subset of AI that deals with the extraction of knowledge from\nthe experience\
    \ by analyzing and manipulating data gathered from real-world use cases.\nThe\
    \ primary purpose of machine learning is to develop reliable active learning models\n\
    equipped with computerized patterns learning from raw data and perform fast-response\n\
    predictions applied in decision-making processes [9,10].\nEnergies 2021, 14, 4776\n\
    3 of 24\nDeep learning and neural networks are the most famous machine learning\
    \ subset.\nThanks to the use of (typically) multi-layered Artiﬁcial Neural Networks\
    \ (ANNs), deep\nlearning can handle unstructured datasets and can recognize complex\
    \ input data patterns.\nIn deep learning, different architectures can be designed\
    \ using neural unit cells in various\nlayers, unless other machine learning algorithms\
    \ are ﬁxed.\nFigure 1 illustrates artiﬁcial intelligence, machine learning, and\
    \ deep learning concepts\nin the schematic description by means of a Venn diagram.\n\
    Energies 2021, 14, x FOR PEER REVIEW \n3 of 27 \n \n \nequipped with computerized\
    \ patterns learning from raw data and perform fast-response \npredictions applied\
    \ in decision-making processes [9,10]. \nDeep learning and neural networks are\
    \ the most famous machine learning subset. \nThanks to the use of (typically)\
    \ multi-layered Artificial Neural Networks (ANNs), deep \nlearning can handle\
    \ unstructured datasets and can recognize complex input data patterns. \nIn deep\
    \ learning, different architectures can be designed using neural unit cells in\
    \ various \nlayers, unless other machine learning algorithms are fixed. \nFigure\
    \ 1 illustrates artificial intelligence, machine learning, and deep learning \n\
    concepts in the schematic description by means of a Venn diagram. \n \nFigure\
    \ 1. Artificial intelligence vs. Machine learning vs. Deep learning. \n2.1. Machine\
    \ Learning Paradigms \nIn machine learning, training a model intends to learn\
    \ the values of the parameters \n(or weights) and the bias from input data, while\
    \ in traditional methods (i.e., with \npredefined algorithms), both the model\
    \ and its parameters are given to a computer to \nperform a task. Labeled data\
    \ are samples with a sort of meaningful “tag”, “label”, or \n“class” that are\
    \ informative or desirable to know—for example, whether an Alternating \nCurrent\
    \ (AC) power signal contains harmonic distortion(s) or not. In contrast, unlabeled\
    \ \ndata are samples with no explanation; in other words, it has only row data\
    \ without any \n“tag” or “label” assigned to it—for instance, voltage and current\
    \ signals of an electric \nmotor. \nMachine learning tasks are principally arranged\
    \ into three main classes: supervised, \nunsupervised, and semi-supervised learning.\
    \ Supervised learning algorithms work with \nlabeled data with the objective of\
    \ mapping new input data to the known target output \nvalues. On the contrary,\
    \ unsupervised learning models process an unlabeled dataset, in \nwhich target\
    \ values are unknown, to draw insights by learning hidden complicated \npatterns\
    \ and structures spontaneously. Semi-supervised algorithms deal with a dataset\
    \ \nthat some samples are labeled, and more extensive samples are unlabeled. These\
    \ \nalgorithms are designed to benefit from both advantages of supervised and\
    \ unsupervised \nmethods [11]; \nSupervised learning is categorized into classification\
    \ and regression problems. A \nclassification problem predicts output variables\
    \ as a category, such as “cat” or “dog.” \nContrarily, in regression problems\
    \ output variables are numerical values [12]; \nUnsupervised learning algorithms\
    \ are generally divided into clustering or \ndimensionality reduction (or sometimes\
    \ called embedding) methods [11]. For instance, in \nanomaly detection, a clustering\
    \ algorithm is applied to data to identify false data by \nscanning outliers in\
    \ a dataset or noticing abnormal patterns; \nSemi-supervised learning makes use\
    \ of the mixture of labeled and unlabeled data as \nthe training dataset. Semi-supervised\
    \ models act as active learners [13]. There are two \nFigure 1. Artiﬁcial intelligence\
    \ vs. Machine learning vs. Deep learning.\n2.1. Machine Learning Paradigms\nIn\
    \ machine learning, training a model intends to learn the values of the parameters\
    \ (or\nweights) and the bias from input data, while in traditional methods (i.e.,\
    \ with predeﬁned\nalgorithms), both the model and its parameters are given to\
    \ a computer to perform a\ntask. Labeled data are samples with a sort of meaningful\
    \ “tag”, “label”, or “class” that\nare informative or desirable to know—for example,\
    \ whether an Alternating Current (AC)\npower signal contains harmonic distortion(s)\
    \ or not. In contrast, unlabeled data are samples\nwith no explanation; in other\
    \ words, it has only row data without any “tag” or “label”\nassigned to it—for\
    \ instance, voltage and current signals of an electric motor.\nMachine learning\
    \ tasks are principally arranged into three main classes: supervised,\nunsupervised,\
    \ and semi-supervised learning. Supervised learning algorithms work with\nlabeled\
    \ data with the objective of mapping new input data to the known target output\n\
    values. On the contrary, unsupervised learning models process an unlabeled dataset,\n\
    in which target values are unknown, to draw insights by learning hidden complicated\n\
    patterns and structures spontaneously. Semi-supervised algorithms deal with a\
    \ dataset that\nsome samples are labeled, and more extensive samples are unlabeled.\
    \ These algorithms are\ndesigned to beneﬁt from both advantages of supervised\
    \ and unsupervised methods [11];\nSupervised learning is categorized into classiﬁcation\
    \ and regression problems. A\nclassiﬁcation problem predicts output variables\
    \ as a category, such as “cat” or “dog.”\nContrarily, in regression problems output\
    \ variables are numerical values [12];\nUnsupervised learning algorithms are generally\
    \ divided into clustering or dimension-\nality reduction (or sometimes called\
    \ embedding) methods [11]. For instance, in anomaly\ndetection, a clustering algorithm\
    \ is applied to data to identify false data by scanning outliers\nin a dataset\
    \ or noticing abnormal patterns;\nSemi-supervised learning makes use of the mixture\
    \ of labeled and unlabeled data as\nthe training dataset. Semi-supervised models\
    \ act as active learners [13]. There are two\nmain semi-supervised learning algorithms,\
    \ namely reinforcement learning and Generative\nAdversarial Networks (GANs). In\
    \ reinforcement methods, if a model does a task correctly,\nit would get a reward.\
    \ The objective of reinforcement learners is to build a model to\nmaximize rewards\
    \ through an iterative process [14]. Reinforcement learning is suitable for\n\
    an interactive or dynamic environment that a model can improve itself based on\
    \ policies\nEnergies 2021, 14, 4776\n4 of 24\ndeﬁned by an expert, for example,\
    \ playing a game or self-driving cars. GANs generate\nmodels based on deep neural\
    \ learning methods to discover and learn patterns of input\ndata. Then, the generative\
    \ model can be used to create new data examples that resemble a\ntraining dataset.\
    \ For instance, GANs can create pictures that look like human faces images,\n\
    even though the faces don’t relate to any actual person.\n2.2. Machine Learning\
    \ Algorithms\nMany different machine learning techniques have been proposed in\
    \ recent years,\nparticularly consisting of hybrid ML-based models, making use\
    \ of two or more machine\nlearning techniques or even other statistical or mathematical\
    \ models. For example, en-\nsemble learning models include different weak learners\
    \ such as decision trees, support\nvector machines, and linear or logistic regression.\
    \ This section discusses the basic and most\nrelevant machine learning techniques\
    \ in each category.\n2.2.1. Classiﬁcation Algorithms\nThere are several classiﬁcation\
    \ algorithms; the most commonly used ones are pre-\nsented as follows [15]:\n\
    Logistic Regression (LR): LR is widely used for binary classiﬁcation tasks where\
    \ an\noutput belongs to one class or another (0 or 1). In this algorithm, a threshold\
    \ is deﬁned to\nindicate examples will be labeled into which class using hypothesis\
    \ and logistic function\n(usually sigmoid curve). The hypothesis determines the\
    \ likelihood of events to generate\ndata and ﬁt them into the logarithm function\
    \ that forms an S-shaped curve called sigmoid.\nThen, the logarithm function is\
    \ used to predict the class of new inputs. Even though logistic\nregression provides\
    \ better performance in binary classiﬁcation tasks, it can also be used in\nmulticlass\
    \ classiﬁcation problems, by applying the one versus all strategy [16];\nK-Nearest\
    \ Neighbors (KNN): this algorithm is one of the most basic yet broadly used\n\
    classiﬁers. It is generally used to ﬁnd data with similar characteristics and\
    \ group them\nin the same class, without making any assumptions on data distribution.\
    \ The groups are\nconstructed by considering the attributes of the neighboring\
    \ samples. It is used in real-life\nproblems in several applications such as data\
    \ mining, pattern recognition, and invasion\ndetection [17,18];\nNaïve Bayes (NB):\
    \ this technique is one of the most powerful classiﬁcation algorithms\nbased on\
    \ an extension of Bayes’ theorem, assuming each feature is independent to capture\n\
    input-output relationships. Bayes’ theorem compares the probability of an event\
    \ happening\nto what has already happened, for example, the probability of having\
    \ a ﬁre (event A)\nwhile the weather is hot (event B, which is present) [19].\
    \ The naïve algorithm is simple\nto implement and can easily predict labels of\
    \ new inputs. Additionally, when domain\nknowledge conﬁrms the feature independence,\
    \ with less data, it has a better performance\nthan other classiﬁcation algorithms\
    \ such as logistic regression. On the other hand, in real\nlife, it is not easy\
    \ to have data with entirely independent features; moreover, when there\nis an\
    \ input that was not followed up in the training phase, the algorithm assigns\
    \ zero\nprobability, and it does not classify this input in any group. This technique\
    \ is used in\nvarious applications such as text classiﬁcation and spam ﬁltering\
    \ [20];\nSupport Vector Machine (SVM): This algorithm is widely used in classiﬁcation\
    \ tasks\nand also applied in regression problems. The main idea of SVM is to transfer\
    \ data to higher\nn-dimensional space to ﬁnd an ideal hyperplane to differentiate\
    \ classes [21]. In simple\nwords, these support vectors are coordinates of a new\
    \ n-dimensional coordinate system.\nThis method is commonly used in binary classiﬁcation,\
    \ but it is computationally expensive\nand slow in the big data domain;\nDecision\
    \ Tree (DT): This algorithm is based on different hierarchical steps that lead\
    \ to\ncertain decisions. It applies a treelike structure to represent decision\
    \ paths with induction\nand pruning steps. In the induction step, the tree structure\
    \ is built, while, in the pruning\nstep, the complexities of the tree are reduced.\
    \ The inputs are mapped to outputs by travers-\ning each path through different\
    \ branches of the tree [22]. DT is a powerful classiﬁcation\nEnergies 2021, 14,\
    \ 4776\n5 of 24\ntool, simple to structure and with good performance. However,\
    \ with even small variations\nin data, DT can become unstable. Furthermore, it\
    \ can easily become overﬁtted, especially\nin a thorny tree with many branches\
    \ and conditions, thus, it does not generalize well on\nnew inputs. Regularization,\
    \ bagging, and boosting techniques are usually used to avoid\noverﬁtting problems\
    \ in the DT [23];\nRandom Forest (RF): This classiﬁer is very similar to the decision\
    \ tree. Compare to\nDT, RF uses several decision trees, instead of having only\
    \ one tree. This technique can be\napplied in massive data set to classify data\
    \ or measure the importance of each feature in\nthe ﬁnal decision. In many applications,\
    \ the random forest is preferred over the decision\ntree because it can be more\
    \ accurate and overcomes the overﬁtting issued of DT. However,\nthis technique\
    \ is not easy to implement since it has a complex structure, and it is not\nrecommended\
    \ for real-time prediction purposes because it is generally slower than other\n\
    models [24].\n2.2.2. Regression Algorithms\nSeveral regression algorithms (numerical\
    \ or continuous value prediction) have been\nintroduced in the scientiﬁc literature;\
    \ the most commonly used ones are presented in the\nfollowing:\nLinear Regression\
    \ (LR): this technique tries to ﬁnd the ﬁttest straight hyperplane to\nthe data\
    \ [25]. It is commonly used when there are linear relationships between variables,\n\
    and it can avoid overﬁtting by regularization techniques such as LASSO, Elastic-Net,\n\
    and Ridge [26]. However, it is not ﬂexible in ﬁnding the best solution for non-linear\n\
    relationships in variables and complex patterns;\nRegression Tree (RT): This technique\
    \ has the same hierarchical structure as the decision\ntree, but it takes numerical\
    \ values as input. The branching procedure not only maximizes\nthe learning gain\
    \ but also learns non-linear relationships between variables. Even if this\nmethod\
    \ is robust to outliers and easy to implement, it is prone to overﬁtting problems\
    \ [27].\nIn addition to the regression tree, random forests and Gradient Boosted\
    \ Trees (GBM), which\nare the most commonly used ensemble methods, are also applied\
    \ in numerical predictions\nand have better performance concerning overﬁtting\
    \ issues;\nDeep Neural Network (DNN): Deep neural network, or multi-layer neural\
    \ network,\nis widely used in several domains. Indeed, thanks to their ability\
    \ to capture complex\npatterns, DNNs can be used both as regression algorithms\
    \ and classiﬁers. The non-linear\nrelationships between features are learned by\
    \ non-linear activation functions and hidden\nlayers between the input and the\
    \ output [28]. There are several techniques and methods\nto improve the performance\
    \ of neural networks, as well as different advanced neural\nnetwork-based models\
    \ such as Convolutional Neural Network (CNN) or Recurrent Neural\nNetwork (RNN)\
    \ [29,30]. Different from other algorithms, in DNNs, a deep knowledge of\nhow\
    \ to tune the parameters of the neural network is required to develop a working\
    \ neural\nnetwork model. In addition, even though neural network models work well\
    \ in the big data\ndomain, they are usually very computationally expensive methods;\n\
    Extreme Learning Machine (ELM) has a wide range of applications in the data-driven\n\
    approach. ELM has been used in regression, classiﬁcation, clustering, sparse representation,\n\
    feature extraction or learning, and compression. This feedforward neural network\
    \ does not\napply the backpropagation gradient-based mechanism to update the network\
    \ weighted\nvalues; instead, it randomly assigns random values to the weight and\
    \ bias terms of the\nnetwork [31]. The main advantages of this kind of algorithm\
    \ are (i) the faster training\nphase and (ii) the better interpolation results.\
    \ On the contrary, the accuracy results of ELM\nis not promising, even if compared\
    \ to basic MLP models;\nSupport Vector Machine or kernel SVM can also be used\
    \ for regression problems, even\nthough it is mostly used in classiﬁcation problems;\n\
    XGboost, ﬁnally, is a (recently) widely used rugged decision-tree- and ensemble-\n\
    based algorithm with a framework that is designed considering a gradient boosting\
    \ proce-\ndure [32].\nEnergies 2021, 14, 4776\n6 of 24\n2.2.3. Clustering Algorithms\n\
    Clustering techniques try to group instances with the same properties in the same\n\
    cluster. These techniques are commonly used in other ﬁelds than machine learning,\
    \ such\nas image analysis, pattern recognition, data compression, and statistical\
    \ analysis. The most\nwell-known algorithms are as follows:\nK-means: this technique,\
    \ one of the simplest and intuitive machine learning algo-\nrithms, separates\
    \ instances in the k centroids or clusters with equal variance. After selecting\n\
    the number of clusters (K), the algorithm ﬁnds the best k clusters by minimizing\
    \ the cri-\nterion known as inertia through the iterative procedure and changing\
    \ the position of\ncentroids [33]. As it is simple to interpolate and scales well\
    \ to big data, it has been applied\nacross a wide range of applications in various\
    \ domains;\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise\
    \ (DBSCAN),\nwhich is widely used in data mining and machine learning, ﬁnds core\
    \ instances of high\ndensity and extends clusters with the speciﬁed radius (usually\
    \ Euclidean distance) around\nthem. Low-density regions are distinguished as outliers.\
    \ The primary problem in DBSCAN\nis selecting clustering attributes, detecting\
    \ noise with varied densities, and signiﬁcant\ndifferences of amounts of boundary\
    \ objects in opposed directions of the corresponding\nclusters [34]. The smallest\
    \ number of instances to constitute a dense region and how close\ninstances should\
    \ be to each other in the same region are deﬁned by an expert. Even though\nthis\
    \ algorithm, which is a very popular clustering technique, is widely used, it\
    \ badly\nbehaves with very sparse or high dimensional datasets;\nSpectral: this\
    \ clustering algorithm, which is also an exploratory data analysis tech-\nnique,\
    \ performs dimensionality reduction through eigenvalues (spectrum) of the similarity\n\
    of data instances, by then grouping similar data instances with reduced dimensions\
    \ into\nthe same cluster [35]. This approach is practically applied when the center\
    \ of clusters and\ntheir spread does not appropriately describe the whole cluster\
    \ (non-convex cluster), such\nas in image segmentation problems. The spectral\
    \ technique is widely used because it is a\nfast response technique and outperforms\
    \ other clustering techniques, especially in sparse\ndatasets.\n2.2.4. Embedding\
    \ Algorithms\nIn many cases, especially in the big data domain, the presence of\
    \ a large number of\nvariables or features in a dataset makes it difﬁcult to interpret\
    \ the relationship between\nthem. Training a model on the whole dataset could\
    \ easily make the model not sufﬁciently\ngeneralized on new unseen data (overﬁtting\
    \ problem). Embedded Algorithms (EAs) can\nbe applied to extract new features\
    \ from data without losing essential information before\nimplementing sophisticated\
    \ ML models. EA techniques could also be used directly for\nprediction purposes.\
    \ Embedding algorithms can be subdivided as follows:\nPrincipal Component Analysis\
    \ (PCA): the main aim of PCA is to reduce high-dimensional\ndatasets to a smaller\
    \ dimension. PCA projects each data instance onto the main compo-\nnents or ranks\
    \ while retaining as much data variation as possible. PCA techniques, such as\n\
    Singular Value Decomposition (SVD), use eigenvectors of the covariance matrix\
    \ of data to\nreduce the dimension of the dataset or making a prediction [36];\n\
    Autoencoder: this is one of the current states of the art techniques leveraging\
    \ neural\nnetworks. Autoencoders are widely used in different applications, such\
    \ as data compres-\nsion. The autoencoder learns a representation (encoding) of\
    \ the input dataset and ignores\nnoise through embedding architecture, and reconstructs\
    \ the input data as close as possible\nto its actual forms (decoder). A typical\
    \ autoencoder consists of three parts, namely: an\nencoder, a bottleneck, and\
    \ a decoder [37]. The encoder tries to compress the data to a lower\ndimension\
    \ with the best representative, the decoder attempts to regenerate an input by\n\
    eliminating the noise in the dataset, while the embedded data is stored in the\
    \ bottleneck.\nIt is possible to use the encoder part of a well-trained autoencoder\
    \ for dimensionality\nreduction, or use the whole model, for example, in anomaly\
    \ detection [38].\nEnergies 2021, 14, 4776\n7 of 24\nFigure 2 summarizes the different\
    \ machine learning paradigms and techniques used\nin power system analytics, by\
    \ providing examples for each category.\n \ncompression. The autoencoder learns\
    \ a representation (encoding) of the input dataset and \nignores noise through\
    \ embedding architecture, and reconstructs the input data as close as \npossible\
    \ to its actual forms (decoder). A typical autoencoder consists of three parts,\
    \ \nnamely: an encoder, a bottleneck, and a decoder [37]. The encoder tries to\
    \ compress the \ndata to a lower dimension with the best representative, the decoder\
    \ attempts to regenerate \nan input by eliminating the noise in the dataset, while\
    \ the embedded data is stored in the \nbottleneck. It is possible to use the encoder\
    \ part of a well-trained autoencoder for dimen-\nsionality reduction, or use the\
    \ whole model, for example, in anomaly detection [38]. \nFigure 2 summarizes the\
    \ different machine learning paradigms and techniques used \nin power system analytics,\
    \ by providing examples for each category. \n \nFigure 2. Machine learning paradigms,\
    \ algorithms, and applications in power systems. \n2.3. Model Performance Evaluation\
    \ Metrics \nThe metrics that are used in each machine learning algorithm are different\
    \ from each \nother. In Table 1 the most used metrics in discrete and continuous\
    \ cases are discussed. In \nthis table, True Positive (TP) and True Negative (TN)\
    \ are samples that are correctly pre-\ndicted as positive and negative, respectively.\
    \ In contrast, False Positive (FP) and False \nNegative (FN) are samples that\
    \ are incorrectly predicted as positive and negative, respec-\ntively. In continuous\
    \ metrics, y is the actual value, yො is the forecasted amount, and n is \nthe\
    \ number of prediction samples. \nTable 1. Model Performance Evaluation Metrics.\
    \ \nDiscrete \nContinuous \nMetric \nFormula \nMetric \nFormula \nAccuracy \n\U0001D447\
    \U0001D443 + \U0001D447\U0001D441\n\U0001D447\U0001D443 + \U0001D447\U0001D441\
    \ + \U0001D439\U0001D443 + \U0001D439\U0001D441 \nMean Square Error \n(MSE) \n\
    1\n\U0001D45B ෍(\U0001D466 − \U0001D466ො)ଶ \nFigure 2. Machine learning paradigms,\
    \ algorithms, and applications in power systems.\n2.3. Model Performance Evaluation\
    \ Metrics\nThe metrics that are used in each machine learning algorithm are different\
    \ from each\nother. In Table 1 the most used metrics in discrete and continuous\
    \ cases are discussed. In\nthis table, True Positive (TP) and True Negative (TN)\
    \ are samples that are correctly predicted\nas positive and negative, respectively.\
    \ In contrast, False Positive (FP) and False Negative\n(FN) are samples that are\
    \ incorrectly predicted as positive and negative, respectively. In\ncontinuous\
    \ metrics, y is the actual value, ˆy is the forecasted amount, and n is the number\n\
    of prediction samples.\nTable 1. Model Performance Evaluation Metrics.\nDiscrete\n\
    Continuous\nMetric\nFormula\nMetric\nFormula\nAccuracy\nTP+TN\nTP+TN+FP+FN\nMean\
    \ Square Error (MSE)\n1\nn ∑(y − ˆy)2\nError\nFN+FP\nTP+TN+FP+FN\nRoot Mean Squared\
    \ Error (RMSE)\nq\n1\nn ∑(y − ˆy)2\nPrecision\nTP\nTP+FP\nMean Absolute Error\
    \ (MAE)\n1\nn ∑|y − ˆy|\nRecall\nTP\nTP+FN\nMean Absolute Percentage Error (MAPE)\n\
    1\nn ∑\n\f\f\f y− ˆy\ny\n\f\f\f\nF1\nTP\nTP+FN\nR-squared (R2)\n1 − ∑(y− ˆy)2\n\
    ∑(y−y)2\n3. Literature Review\nMachine learning is widely applied to address various\
    \ problems to bring novel so-\nlutions or improve the performance of existing\
    \ applications. The main state-of-the-art\nEnergies 2021, 14, 4776\n8 of 24\n\
    machine learning-based applications in power systems are in power ﬂow, power quality,\n\
    photovoltaic system, intelligent transportation, and load forecasting.\n3.1. Power\
    \ Flow Applications\nCompared to traditional algorithms, machine learning technologies\
    \ make power\nﬂow problems easier to be handled. For example, algorithms like\
    \ CNN, KNN, SVM,\nreinforcement learning, and decision tree affected power ﬂow\
    \ optimization problems in\nterms of accuracy, computational speed, and response\
    \ time. Table 2 elaborates more into\ndetail the recent advancements in machine\
    \ learning applications in power ﬂow.\n3.2. Power Quality Applications\nThe power\
    \ quality, one of the most critical topics in electrical systems, has also\nbeen\
    \ affected by machine learning, which can be used to improve speed and accuracy\
    \ in\ndisturbances detection, or distortions classiﬁcation, and estimations for\
    \ future cycles. In\naddition, ML can also be used on a wide set of PQ parameters\
    \ related to load functioning\nsuch as active power, reactive power, complex power,\
    \ fundamental frequency, and power\nfactor.\nTable 3 summarizes the most recent\
    \ improvements and achievements in the use of\nML techniques in power quality\
    \ applications.\n3.3. Photovoltaic System Applications\nMachine learning algorithms\
    \ have been widely used for different purposes in Photo-\nvoltaic (PV), from forecasting\
    \ the long-, medium-, and short-term energy generation, to\nfault detection and\
    \ classiﬁcation. The most recent works in this ﬁeld are summarized in\nTable 4.\n\
    3.4. Intelligent Transportation Applications\nArtiﬁcial intelligence, especially\
    \ machine learning applications, are widely used in\nintelligent transportation,\
    \ to develop smart online trafﬁc management systems, from safety\napplications\
    \ (e.g., driving distraction detection) to optimized trafﬁc scheduling. Self-\n\
    driving cars, for instance, have been recently developed only thanks to the advancements\n\
    in machine learning.\nTable 5 provides the most recent works based on ML in the\
    \ ﬁeld of intelligent trans-\nportation.\n3.5. Load Forecasting Applications\n\
    Accurate load forecasting, both short- and long-term, is an essential task for\
    \ the\ndaily (economic) dispatching of electricity, both to prevent wasting energy\
    \ production and\nintegrating renewable energy resources.\nEnergy companies monitor,\
    \ control, and schedule load demands and power generation\nto enhance energy management\
    \ systems. However, electrical load proﬁles are becoming\nmore complicated, not\
    \ only because of the stochastic behavior of customers, but also\nbecause of the\
    \ introduction of new non-linear components in power systems, such as\nelectric\
    \ vehicles, buses, and bikes. Therefore, many researchers have been developing\
    \ both\ndeterministic and probabilistic load forecasting models to improve the\
    \ precision and speed\nof prediction models.\nTable 6 presents recent advancements\
    \ of machine learning studies in load forecasting.\nEnergies 2021, 14, 4776\n\
    9 of 24\nTable 2. Overview of Research using Machine Learning for Power Flow.\n\
    Reference\nYear\nApplication\nData\nMethod(s)\nRemark(s)/Contribution(s)\nLei\
    \ et al. [39]\n2020\nOptimal Power Flow (OPF)\nIEEE 39, 57, 118-bus, and Polish\
    \ 2383-bus (wind and\nphotovoltaic power connected)\n30,000 samples for training,\
    \ and 10,000 samples for\nvalidation\nELM\nDecompose OPF via a data-driven regression\
    \ framework with three stages\nstacked extreme learning machine (SELM); implemented\
    \ the multiple\nsupervised layers with reinforcement mode with an overall 98.71%\n\
    accuracy rate, signiﬁcantly higher than benchmarks.\nWang et al. [40]\n2020\n\
    Online Detection of\nGeomagnetically Induced\nCurrents in Power Grids\nSimulation\
    \ data based on real-life power grid\noperation (10,800 samples for training,\
    \ 1200 samples\nfor validation, and 6000 samples for testing)\nCNN\nDeveloped\
    \ hybrid feature extraction consists of pseudo-continuous\nwavelet transform (PCWT)\
    \ and short-time Fourier transform (STFT);\nimproved overall detection accuracy\
    \ to 90.15% for different noise levels\nand achieved the detection results within\
    \ 30 m\nRavikumar et al.\n[41]\n2020\nAnomaly Detection and\nMitigation (ADM)\
    \ for Wide-Area\nDamping Control\nSynchrophasor measurement data and simulated\
    \ the\ncyber-physical system (CPS) dataset (60 fps\ntransmission rate)\nKNN and\n\
    PCA\nImproved efﬁciency of ADM with domain-speciﬁc features extraction and\nselection\
    \ via Teager–Kaiser Energy Operator (TKEO), Principal Component\nAnalysis (PCA),\
    \ Wide-Area System Measures (WASMs), and Primitive\nMeasures; Proposed KNN-based\
    \ model with a 95.5% accuracy rate, better\nthan other ML-based models.\nZhang\
    \ et al. [42]\n2021\nVolt-VAR Optimization in Smart\nDistribution Systems\nUnbalanced\
    \ IEEE 13-bus and 123-bus systems (9000\noperating conditions for training, and\
    \ 13,000\noperating conditions for testing)\nReinforcement\nlearning\nThe improved\
    \ accuracy rate of voltage regulation with an average of\n99.80% compared to 90.02%\
    \ of baselines; Achieved an average executive\ntime of 21.7 and 46.2 ms for 13-bus\
    \ and 123-bus systems, respectively, and\n28.38% for the loss reduction percentage.\n\
    Baker et al. [43]\n2019\nJoint Chance Constraints in AC\nOptimal Power Flow\n\
    IEEE 37-node test feeder (5-min data from August\n2012 weekdays)\n1152 samples\
    \ for training (4 days), and 864 samples for\ntesting (3 days)\nSVM\nImproved\
    \ classic methods based on union bound (or Boole’s inequality)\nand the accuracy\
    \ rate with 0.19% and 4.73% error rate for false\nclassiﬁcation of binding and\
    \ non-binding events, respectively.\nLi et al. [44]\n2020\nTransient Stability\
    \ Assessment of\nPower System\nIEEE New England 10-machine 39-bus system, IEEE\n\
    16-machine 68-bus system, and IEEE 47-machine\n140-bus system (5984, 11,792, and\
    \ 29,520 samples,\nrespectively)\n6-s simulation time per instance, and 0.01-s\
    \ step\nXGBoost\nand FM\nProposed hybrid XGBoost-FM model robust to noise. Used\
    \ extreme\ngradient boosting (XGBoost) for automatic feature builder, and\nfactorization\
    \ machine (FM) as a classiﬁer with the enhanced detection time\nof 0.9349 s; improved\
    \ accuracy by using both original and artiﬁcial features\nto 98.21%.\nHong et\
    \ al. [45]\n2020\nState Estimation of Distribution\nNetwork\nIEEE 13-, 34-, and\
    \ 37-node test feeders (240 samples\nfor training, and 60 samples for testing)\n\
    LR, SVM,\nand FFNN\nEstimated the voltage magnitudes and angles of several successive\
    \ buses\nwith 0.01 p.u. and 0.189◦ error respectively; SVM outperformed LR and\n\
    FFNN, especially when the relationship between inputs and outputs is\nunknown,\
    \ the input bus was missing, there is a measurement error, and\nusing few adjacent\
    \ buses as input buses.\nKaragiannopoulos\net al. [46]\n2019\nOptimized Local\
    \ Control for\nActive Distribution Grids\nSeasonal historical data (30-day dataset;\
    \ 1-h time\nresolution; 7200 samples)\nSVM\nProposed Data-driven method to obtain\
    \ local Distributed Energy\nResources (DERs) controls without monitoring and communication\n\
    infrastructure; outperformed standard industry local control with an\noverall\
    \ RMSE accuracy of 0.158.\nEnergies 2021, 14, 4776\n10 of 24\nTable 2. Cont.\n\
    Reference\nYear\nApplication\nData\nMethod(s)\nRemark(s)/Contribution(s)\nZhao\
    \ et al. [47]\n2020\nReal-Time Power Grid Multi-Line\nOutage Identiﬁcation\nIEEE\
    \ 30, 118, and 300 bus systems (300,000, 800,000,\nand 2.2 million data generated,\
    \ respectively)\nANN\nGenerated a large number of samples with Monte Carlo simulation\
    \ with\nfull-blown power ﬂow models; Achieved an overall classiﬁcation accuracy\n\
    of 99%, and outstanding performance in recognizing multi-line outages in\nreal-time\
    \ with a small amount of data.\nKing et al. [48]\n2015\nAlgorithm Selection for\
    \ Power\nFlow Management\nIEEE 14-bus (10,000 states for testing), IEEE 57-bus\n\
    (10,000 states for testing), and a real 33-kV distribution\nnetworks (17,520 states\
    \ for testing -a year of\nhalf-hourly proﬁle data-)\nANN, DT\nand RF\nShown that\
    \ ML-based methods can create effective algorithm selectors for\npower ﬂow management\
    \ based on algorithms’ behavior data within 1 ms\nfor future complex networks\n\
    Labed et al. [49]\n2019\nOverloaded Power System\nAlleviation\nAlgerian (Adrar)\
    \ 22-bus system (75% of data for\ntraining, and the remaining 25% for validation)\n\
    ELM\nThe proposed method outperforms SVM and ANN learning algorithms\nwith 1.9465*10\
    \ MSE accuracy and 0.0023 s response time on the testing\nphase with generalization\
    \ performance; this fast time response minimized\nthe threat and risk of outage\
    \ and cascade failure.\nTable 3. Overview of Research Using Machine Learning for\
    \ Power Quality.\nReference\nYear\nApplication\nData\nMethod(s)\nRemark(s)/Contribution(s)\n\
    Ray et al. [50]\n2018\nPQ Disturbances Classiﬁcation in\nSolar PV Integrated Microgrid\n\
    Generated dataset from solar PV integrated microgrid\nmodel (600 samples; 5-kHz\
    \ sampling frequency)\nSVM, ICA\nProposed the independent component analysis (ICA)\
    \ and statistical feature\nextraction using SVM; ICA-SVM improved accuracy to\
    \ 99.5% compared to\n97.8% of traditional Wavelet transform-SVM\nSahani et al.\
    \ [51]\n2020\nA Real-time Power Quality\nEvents (PQEs) Recognition\nSynthetic\
    \ (50 samples for training, and 100 samples for\nvalidation) and real (100 samples\
    \ per distortion\n–validation-) power quality events data\nELM\nRobust anti-noise\
    \ online PQEs classiﬁcation; Outperform other models\nwith 98.86% accuracy rate\
    \ and 0.019 s response time\nTurovic et al. [52]\n2019\nPQ Distortions Detection\
    \ in\nDistribution Gird\nIEEE 13-bus system modiﬁed with DG (85% of the\nsamples\
    \ for training, and 15% for validation)\nANN, SVM,\nand KNN\nDetection speed comparison\
    \ between ML algorithms and traditional FFT;\nANN has the best detection’s speed\
    \ with 0.432 ms (600% more than FFT)\nwith a 99.41% accuracy rate\nLiao et al.\
    \ [53]\n2018\nVoltage Sag Estimation\nIEEE 68-bus test network (374,400 faults\
    \ simulated)\nCNN\nAutomatic system area mapping and feature extraction in the\
    \ input bus\nmatrix from various local areas in the power network; reached 99.41%\
    \ of\noverall estimation accuracy.\nVantuch et al. [54]\n2017\nPQ Forecasting\
    \ for Off-Grid\nSystem\nExperimental off-grid laboratory (141,537\none-minute-resolution\
    \ measurements, more than\n3 months) and simulated data\nRF\nMore than overall\
    \ 90% accuracy for forecasting short-term (15 min ahead)\nPQ disturbances\nBagheri\
    \ et al. [55]\n2018\nVoltage Dip Classiﬁcation\n6000 real measured voltage dips\
    \ data over different\ncountries\nOne month of recording\nCNN\nDeveloped a robust\
    \ automatic feature extraction using a space phasor\nmodel (SPM) and CNN; outperformed\
    \ the other existing models with a\n97.72% accuracy rate and 0.50% false alarm\n\
    Energies 2021, 14, 4776\n11 of 24\nTable 3. Cont.\nReference\nYear\nApplication\n\
    Data\nMethod(s)\nRemark(s)/Contribution(s)\nSahani et al. [56]\n2020\nPower Quality\
    \ Events\nRecognition\nSynthetic and laboratory PQDs (150 samples per class\n\
    −50 for training and 100 for validation-; 3.2-kHz\nsampling frequency)\nELM and\n\
    VDM\nDeveloped an automatic PQEs patterns recognition system from\nnonstationary\
    \ PQ data by using integrating variational mode\ndecomposition (VMD) and Online\
    \ P-Norm Adaptive Extreme Learning\nMachine (OPAELM); shorter event recognition\
    \ time and classiﬁcation\naccuracy rate of 99.3%.\nWang et al. [57]\n2019\nPower\
    \ Quality Disturbance\nClassiﬁcation\nSynthetic data (16 PQDs)\nIEEE Power Engineering\
    \ Society database (1000\nsamples;\nThe inﬂuence of data imbalance is eliminated\
    \ by\napplying an enhancement process)\nELM\nSelect less than 10 features out\
    \ of 4500*1280 signal matrix via discrete\nwavelet transform (DWT) feature extraction\
    \ and particle swarm\noptimization (PSO) feature selection; Proposed PSO hierarchical\
    \ ELM\n(PSO-H-ELM) classiﬁcation with automatic encoders and sparse\nconstraints;\
    \ overall classiﬁcation accuracy rate is above 95%, and high\ncalculation speed\
    \ (less than 0.169 s).\nShen et al. [58]\n2019\nDetection and Classiﬁcation of\n\
    PQDs in Wind-Grid Distribution\nSystems\nSynthetic data (2400 samples)\nSimulated\
    \ data from the standard IEEE 13 node bus\nsystem with wind-grid distribution\
    \ (5590 samples)\n10-kHz sampling frequency\nCNN and\nIPCA\nUsed Improved Principal\
    \ Component Analysis (IPCA) for extracting\nstatistical features; applied 1D-CNN\
    \ classiﬁcation, which gives 99.76%\naccuracy on average for different noise levels,\
    \ higher than other\nclassiﬁcation methods.\nDeng et al. [59]\n2019\nType Recognition\
    \ and Time\nLocation of Combined Power\nQuality Disturbance\nSynthetic data from\
    \ IEEE 1159 power quality standard\nfor training (1000 samples × 96 combinations\
    \ of PQD)\nand real data generated in a lab for testing (140\nsamples)\nGRU\n\
    Proposed bi-directional GRU model for classifying 96 different kinds of\ndisturbances\
    \ noiseless and with noise from 10 dB to 50 dB; have a 98%\naccuracy level on\
    \ real operational data and the absolute error of\nstarting-ending times location\
    \ less than 0.469 ms.\nCao et al. [60]\n2019\nTransient voltage stability\nanalysis\
    \ based on frequency,\nactive power, and reactive power\nSimulated data of different\
    \ nodes collected by phasor\nmeasurement units\nCNN and\nDeep\nLearning\nDecision\
    \ optimization algorithm based on PQ parameters implemented;\nreactive power compensation\
    \ decision based on deep learning performed\nAbed [61]\n2018\nPower factor enhance\
    \ and control\nSimulated power system\nClustering\nneural\nnetwork\nThe proposed\
    \ method allows improving power factor\nZhang et al. [62]\n2020\nReactive load\
    \ prediction\nSCADA data from a real power grid (357 busloads)\nTraining set:\n\
    data from June 1 to August 5\nTest set: data from\nAugust 6 to August 22.\n15-min\
    \ sampling period\nDeep\nlearning\nReactive power load of buses can be accurately\
    \ predicted; accuracy is better\nthan that obtained with other prediction models;\
    \ result of great signiﬁcance\nfor reactive voltage control\nNakawiro [63]\n2020\n\
    Voltage and reactive power\ncontrol\nSimulated dataset of grip operation (on-load\
    \ tap\nchanger, load, and wind power)\n1 year of operation (hourly data)\nDT and\n\
    KNN\nThe highest classiﬁcation accuracy is achieved with a DT; accuracies\nobtained\
    \ in the simulations are satisfactory for some classes; performance\nheavily relies\
    \ on the distribution of the target output and number of\nsamples per class\n\
    Energies 2021, 14, 4776\n12 of 24\nTable 3. Cont.\nReference\nYear\nApplication\n\
    Data\nMethod(s)\nRemark(s)/Contribution(s)\nMoreira et al. [64]\n2018\nPower factor\
    \ compensator (based\non PQ parameters: power factor,\nunbalance factor, harmonic\n\
    distortion, reactive power, etc.)\nTraining: Simulations characterized by a human\n\
    specialist (1,355,154 samples per disturbing load).\nReal measurements added\n\
    Test: IEEE 13-bus (111,055 samples). Three real test\nsets (disturbing loads)\n\
    DT, KNN,\nSVM, and\nANN\nPQ parameters used to analyze the functioning of a power\
    \ system; DT is\nhighly effective in classiﬁcation; 100% accuracy achieved\nValenti\
    \ et al. [65]\n2018\nNon-intrusive load monitoring\nbased on active and reactive\n\
    power\nTwo public datasets:\nTwenty-one power meters; 60-s sampling period; 2\n\
    years monitoring.\nFour different locations; multiple sampling\nfrequencies\n\
    ANN\nIntroducing reactive power increases F1 score performance from +4.9% to\n\
    +8.4%; reactive power provides signiﬁcant information for non-intrusive\nload\
    \ monitoring\nTable 4. Overview of Research Using Machine Learning for Photovoltaics\
    \ Systems.\nReference\nYear\nApplication\nData\nMethod(s)\nRemark(s)/Contribution(s)\n\
    Keerthisinghe et al.\n[66]\n2020\nPV Forecasts for Capacity\nFirming\nDataset\
    \ of 2013–2018 coming from empirical formula\nand 2019–2020 real-data of Arlington\
    \ Microgrid; input\nfrequency every half an hour for one day and two\nsamples\
    \ for output\nLSTM\nProposed encoder-decoder LSTM-based model for short-term (1-h\
    \ ahead)\nPV generation prediction resulted in reducing the yearly battery energy\n\
    throughput by 29% and the number of battery cycles with a greater than\n10% depth-of-discharge\
    \ by 51%.\nWen et al. [67]\n2019\nPV Prediction in Ship Onboard\nHistorical hourly\
    \ data of meteorological information\nalong with the ship route movement for a\
    \ year\nELM\nThe proposed ML-based model with the particle swarm optimization\
    \ (PSO)\nhas a MAPE accuracy level of 25.41% in the training phase for ﬁve-hour\n\
    ahead prediction; the difference between prediction and experimental\nresults\
    \ has 14.96% of the absolute error in the test phase, which means it\nhas a high\
    \ potential in practical cases.\nDhibi et al. [68]\n2020\nFault Detection and\
    \ Classiﬁcation\nin Grid-Tied PV System\nEmulate the operational real PV array\
    \ dataset using\nChroma 62150H-1000S programmable dc power\nsupply; 100 µs sampling\
    \ time with 1501 samples for 6\ndifferent classes for both training and testing\n\
    RF and\nK-means\nProposed two classiﬁers based on Reduced kernel RF for detecting\
    \ faults:\nEuclidean distance-based RK-RF and K-means clustering-based RK-RF\n\
    with 100% accuracy and reduced computational time 65.16% and 53.33\ncompared to\
    \ kernel RF, respectively; redundancy between samples was\nreduced by using Euclidean\
    \ distance as a dissimilarity metric; the K-means\nclustering method used to reduce\
    \ the training data amount.\nZhang et al. [69]\n2020\nDay-Ahead PV Estimation\n\
    Real datasets from Cupertino, CA, USA, from July\n2015 to December 2016; and Catania,\
    \ Sicily, Italy, from\nJanuary 2011 to December 2011 with a 15-min\nsampling rate\n\
    LSTM and\nAE\nImproved the prediction accuracy to 8.39% nRMSE compared to\nbenchmarks\
    \ with the proposed hybrid Auto Encoder (AE) LSTM model for\nthree months testing;\
    \ the proposed persistence model (PM) has a high\naccuracy of 0.72% nRMSE for\
    \ consecutive clear days; Applied the Root\nMean Squared Euclidean Distance Difference\
    \ (RMSEDD) to extract and\nselect the most valuable features to increase the model\
    \ accuracy.\nEnergies 2021, 14, 4776\n13 of 24\nTable 4. Cont.\nReference\nYear\n\
    Application\nData\nMethod(s)\nRemark(s)/Contribution(s)\nChang et al. [70]\n2020\n\
    Short-term Photovoltaic Power\nPrediction for Edge Computing\nReal PV output and\
    \ PV meteorological dataset; one\nsample every 30 min\nLightGBM\nProposed a tree-structured\
    \ self-organizing map (TS-SOM) algorithm for\nclustering weather; used Bayesian\
    \ optimization algorithm is employed for\ntemporal pattern aggregation to determine\
    \ the optimal size of time steps;\nthe proposed LightGBM outperforms other algorithms\
    \ in training and\nexecution time (0.020 s) with 35.49 RMSE accuracy, suited for\
    \ edge\ncomputing devices.\nKhan et al. [71]\n2020\nIslanding Classiﬁcation for\n\
    Grid-Connected PV\nSimulation data; total size equals 4526 samples and 7\nfeatures;\
    \ 3168 samples for training and 679 samples\nfor testing\nANN\nProposed islanding\
    \ detection model-based Wavelet transform for feature\nextraction and Multi-layered\
    \ Perceptron (MLP) for classiﬁcation with 97.8%\naccuracy under 0.2 s on unseen\
    \ conditions.\nWang et al. [72]\n2019\nKey Weather Factors from\nAnalytical Modeling\
    \ Toward\nImproved PV Forecasting\nReal hourly dataset for a year of three PV\
    \ arrays in\nAustralia from April 2012 to June 2013 with 11\nindependent variables\n\
    SVM, ANN,\nand KNN\nImproved the accuracy level for each season by using PCA for\
    \ feature\nextraction and KNN for classifying the prediction period into the historical\n\
    periods with the most similar weather situations; for example, on sunny\ndays,\
    \ with the proposed method, SVM has 3.97 instead of 8.14, ANN has\n4.09 instead\
    \ of 8.45, and weighted KNN has 8.86 instead of 9.33 nRMSE\naccuracy; this method\
    \ helps ANN converges much faster with 37.72%\ncomputational time reduction.\n\
    Gao et al. [73]\n2020\nFault Identiﬁcation Method for\nPhotovoltaic Array\nSimulation\
    \ dataset with 1320 samples and\nexperimental dataset with 1892 samples with a\
    \ ratio of\n6:2:2 for training, validation, and testing\nCNN and\nGRU\nOutperformed\
    \ benchmark methods with 98.41% accuracy in 28.1 ms\ndetection time using CNN\
    \ as automatic feature extractions and\nResidual-GRU for memorizing time-series\
    \ dynamic features; outperformed\nbenchmarks also in the presence of 10 dB to\
    \ 50 dB noise level; reached\naccuracy of 95.23% when some features are missing\
    \ (temperatures and\nirradiances).\nCatalina et al. [74]\n2020\nPV Energy Nowcasting\n\
    Hourly satellite and Numerical Weather Predictions\n(NWP) dataset with 4645 sample\
    \ size for 2015\nSVR\nProposed Gaussian SVRs models using satellite data and NWP\
    \ information\nto improve the PV energy nowcasting in the three real experimental\n\
    regions.\nRay et al. [75]\n2020\nLong-term PV Output\nForecasting\nHistorical\
    \ hourly datasets of 24 years of four different\nlocations in North Queensland\
    \ in Australia; dataset\nfrom 1990 to 2013 was used for training and 2014 for\n\
    testing\nLSTM and\nCNN\nThe proposed hybrid model, consisting of CNN and LSTM,\
    \ outperforms\nother methods with RMSE lower than 15 for all studied locations\
    \ and low\ncomputational cost (203.63 s) for training and prediction.\nYap et\
    \ al. [76]\n2020\nGrid Integration of PV\nSimulation dataset with 0.1 s sampling\
    \ time\nReinforcement\nlearning\nProposed the new virtual inertia control algorithm\
    \ for integrating PV to a\ngrid with higher frequency nadir, lower frequency deviation\
    \ (reduced by\n0.1 Hz), smaller steady-state error (reduced by 27%), faster settling\
    \ time\n(reduced by 35%), lesser active power injection or absorption, and lesser\n\
    overshooting compared to traditional approaches.\nKeerthisinghe et al.\n[77]\n\
    2019\nEnergy Management of\nPV-Storage Systems\nHistorical and one year-long simulation\
    \ datasets with\n30 min time intervals for each day\nANN\nProposed an ANN-based\
    \ model based on dynamic programming (DP),\nwhich, compared to other methods,\
    \ has better quality and faster response\ntime (27.15 s); this method reduced\
    \ a daily and yearly electric cost by more\nthan 50% for four different scenarios\
    \ considering PV output, electrical\ndemand, electricity price, and battery SOC.\n\
    Energies 2021, 14, 4776\n14 of 24\nTable 5. Overview of Research using Machine\
    \ Learning for Transportation.\nReference\nYear\nApplication\nData\nMethod(s)\n\
    Remark(s)/Contribution(s)\nAshqar et al. [78]\n2019\nTransportation Mode Recognition\n\
    Real data of GPS, accelerometer, gyroscope, and\nrotation vector sensors through\
    \ a smartphone app for\n10 travelers with 25 Hz sampling frequency\nTwo-layer\n\
    hierarchical\nframework RF-SVM\nIntroduced new extracted frequency domain features\
    \ and\nincreased accuracy rate to 97.02% compared to 95.10% of\ntraditional approaches\n\
    Jia [79]\n2019\nAnalysis of Alternative Fuel\nVehicle Adoption\nPerson-, household-,\
    \ trip- and vehicle real-dataset\n(2017 NHTS Dataset) from April 2016 to April\
    \ 2017\nRF\nExtraction inﬂuencing factors from large-scale 2017 NHTS Dataset\n\
    and Categorized them; RF outperformed other models (LR, NB,\nSVM, and DT) with\
    \ good accuracy (97.99%) and high AUC value\nfor adoption prediction.\nAksjonov\
    \ et al.\n[80]\n2019\nDetection and Evaluation of\nDriver Distraction\nSimulation\
    \ data: speed limit, a radius of the road,\nlane-keeping offset, and vehicle speed\
    \ for 18 subjects\nwith 50 Hz sampling frequency\nNonlinear regression\nbased\
    \ on Euclidean\ndistance and Fuzzy\nlogic\nThe proposed method improved the RMSE\
    \ accuracy level from\n2.1345 to 1.9992 for speed and 0.1506 to 0.1405 for distance.\n\
    Training time also decreased from 148.072 to 96.150 s compared to\nthe standard\
    \ ANFIS predictor\nNallaperuma et al.\n[81]\n2019\nOnline Smart Trafﬁc\nManagement\n\
    Real-time Bluetooth sensor network data and social\nmedia data (Twitter) from\
    \ the arterial road network in\nVictoria, Australia; 24 and 7 days data for training\
    \ and\ntesting, respectively, with data horizon equal to 15 min\nLSTM and\nReinforcement\n\
    learning\nShort-term trafﬁc ﬂow with normal ﬂuctuation prediction with\n0.0727\
    \ MAE accuracy; overcome the limitation of labeling data and\nstrict assumptions\
    \ regarding data and trafﬁc behaviors.\nGjoreski et al. [82]\n2020\nMonitoring\
    \ Driver Distractions\nReal data of 68 people through physiological sensors,\n\
    the emotional response, and facial-expression\nextraction with 1 Hz sampling frequency\n\
    Comparison of\nclassical ML and\ndeep learning\nalgorithms\nThe classical extreme\
    \ gradient boosting (XGB) outperforms the\ndeep learning method with 94% F1-score\
    \ accuracy compared to\n87% for classifying complete driving sessions.\nLi et\
    \ al. [83]\n2019\nSecurity: SQL Injection Detection\nReal-data and data augmentation\
    \ from enterprises\nand various social platforms; 36,422 real samples and\n30,000\
    \ generated samples\nDeep LSTM network\nOvercome the overﬁtting problem and increase\
    \ accuracy\n(93.47–99.58%) due to data augmentation compared to the shallow\n\
    and deep ML algorithms.\nOu et al. [84]\n2019\nReal-Time Estimation of\nDynamic\
    \ Origin-Destination ﬂow\nGenerate training dataset from real-trafﬁc dataset and\n\
    trafﬁc survey, and testing on real-time data with\n15-min intervals sampling for\
    \ 15 days in June 2017\nCNN\nCapture the dynamic mapping patterns and reconstruct\n\
    trajectories with MAPE average accuracy less than 5 (vehicle/15\nmin) on testing\n\
    Khadilkar et al.\n[85]\n2018\nScheduling Railway Lines\nReal single- and multi-track\
    \ railway data of different\nroutes with various number of trains and stations\
    \ in\nroutes\nReinforcement\nlearning\nScalable to large scale dataset due to\
    \ transfer learning; manage\nlarge, realistic problem instances in computation\
    \ times and\noutperform other traditional techniques.\nZhang et al. [86]\n2020\n\
    Short-term Passenger Demand\nPrediction\nReal taxi dataset of New York City from\
    \ January 2016\nto June 2018 for 63 zones\nMTL-TCNN\nAn automatic feature selector\
    \ algorithm; outperform other models\nwith 2.5% RMSE accuracy\nCheng et al. [87]\n\
    2018\nHigh-Speed Trains Positioning\nBeijing-Shanghai high-speed railway real-data\n\
    contains 725 groups of data\nKNN\nImprove KNN performance by applying ant colony\
    \ optimization\n(ACO) and online learning algorithms; obtain a better cluster\n\
    number of positioning data; Outperform other algorithms with\n2.21 MAE accuracy.\n\
    Alawad et al. [88]\n2019\nRailway Safely and Accidents\nReal data of accidents\
    \ and passenger information like\npassenger age and time of accident occurrence\
    \ for\n71 accidents\nDT\nDeveloped a classiﬁcation model regarding the occurrence\
    \ of\naccidents with good prediction accuracy of 88.7% on test data\nEnergies\
    \ 2021, 14, 4776\n15 of 24\nTable 6. Overview of Research Using Machine Learning\
    \ for Load forecasting.\nReference\nYear\nApplication\nData\nMethod(s)\nRemark(s)/Contribution(s)\n\
    Zhang et al. [89]\n2020\nMedium-term Load Forecasting\nTwo real-world datasets:\
    \ New York (1200 hourly\nelectricity demand values of February 2018) and\nQueensland\
    \ (1200 half-hour electric load values of\nJanuary 2017) region\nVMD, SR,\nSVR,\
    \ CBCS\nThis study proposed a novel hybrid model based on variational mode\ndecomposition\
    \ (VMD), self-recurrent (SR) mechanism, support vector\nregression (SVR), chaotic\
    \ mapping mechanism, and cuckoo search (CBCS).\nThe VMD-SR-SVRCBCS outperformed\
    \ other medium-range prediction\nmethods (240 half-hours window) in both cases\
    \ with 2.5 and 0.9 MAPE of\nNew York and Queensland, respectively.\nFeng et al.\
    \ [90]\n2020\nShort-term Load Forecasting\nReal hourly load data of University\
    \ of Texas as Dallas\nfor 2014 and 2015\nReinforcement\nlearning\nThis study proposed\
    \ a deterministic and probabilistic load prediction using\nthe two Q-learning\
    \ agents to select the best model locally from\ndeterministic load forecasting\
    \ methods and ten state-of-the-art ML-based\nmodels. The results show 50–60% accuracy\
    \ improvements compared to\nsingle-phase benchmarks models.\nAhmad et al. [91]\n\
    2019\nA-Day Ahead Load Forecasting\nin Smart Grids\nReal hourly data of two USA\
    \ grids\n(DAYTOWN, Ohio and EKPC, Kentucky) for two\nyears (2014–2015)\nANN\n\
    This study considers both accuracy and execution time to develop their\nmodel\
    \ to scale well in bigger datasets. The authors introduced the\npre-preparation,\
    \ prediction, and optimization modules. Taking advantage\nof a heuristics-based\
    \ optimization method minimized MAPE while\nreaching 98.76% accuracy, which was\
    \ relatively better than existing bi-level\ntechniques.\nZheng et al. [92]\n2017\n\
    Short- and Medium- Term Load\nPrediction\nReal hourly data of electricity load\
    \ of ISO New\nEngland (2003–2016)\nPCA, LSTM,\nXGBoost with\nK-means\nThe authors\
    \ presented a hybrid algorithm based on supervised and\nunsupervised machine learning\
    \ techniques as follows: ﬁrstly, they applied\nempirical mode decomposition (EMD)\
    \ and similar days selection days to\nextract dominant features, then, they made\
    \ predictions with LSTM\nconsidering a very rich dataset for 11 years for training,\
    \ one year for\nvalidation, and one year (2016) for testing. The similarity between\
    \ days\nachieved by XGboost-based weighted k-means. The testing results for\n\
    one-day and one-week ahead shows this hybrid method improved the\naverage accuracy\
    \ of the LSTM-based model from 5.43 to 1.08 MAPE and\n8.74 to 1.59 for a day ahead\
    \ and a week ahead, respectively.\nDabbaghjamanesh\net al. [93]\n2020\nA-Day Ahead\
    \ Load Forecasting\nfor EV Charging Station\nSynthetic dataset with hourly resolution\n\
    Reinforcement\nlearning\nThis study proposed a reinforcement learning-based model\
    \ to predict a day\nahead EV charging station load demand. The proposed Q-learning\
    \ model\noutperformed CNN and RNN models in three different scenarios\n(coordinated,\
    \ uncoordinated, and smart charging) in terms of MSE metrics.\nHigher accuracy,\
    \ higher speed, and ﬂexibility are three main advantages of\nthe proposed model.\n\
    Energies 2021, 14, 4776\n16 of 24\nTable 6. Cont.\nReference\nYear\nApplication\n\
    Data\nMethod(s)\nRemark(s)/Contribution(s)\nFarsi et al. [94]\n2021\nShort to\
    \ Long Term Ahead Load\nForecasting (1–30 days ahead)\nReal datasets of hourly\
    \ load consumption of\nMalaysia (2009 to 2010) and Germany (2012–2016)\nCNN and\n\
    LSTM\nThis article proposed a parallel LSTM-CNN Network (PLCNet). Compared\nto\
    \ others, this study’s main advantage is to use LSTM and CNN in parallel\nand\
    \ concatenate their outputs with a dense layer to make the ﬁnal\nprediction. The\
    \ proposed method outperformed statistical and machine\nlearning models with 98.23%\
    \ R-square accuracy for Malaysians and\nimproved Germany’s R-square accuracy from\
    \ 83.17 to 91.18% for a day\nahead load prediction.\nHafeez et al. [95]\n2020\n\
    A-Day Ahead Load Forecasting\nReal hourly load data of three USA power grids (FE,\n\
    EKPC, and Dayton) from 2005 to 2012\nANN\n(restricted\nBoltzmann\nmachine)\nThe\
    \ authors introduced a hybrid model based on a deep neural network\n(restricted\
    \ Boltzmann machine), modiﬁed mutual information (MMI)\ntechnique to extract features,\
    \ and proposed a genetic wind-driven (GWDO)\noptimization method to adjust the\
    \ model’s parameters. Together with their\nﬁne data engineering procedure, this\
    \ new optimization algorithm helps to\nimprove the MAPE accuracy between 4.7%\
    \ to 17.3% compared to\nbenchmarks. Moreover, their model’s average convergence\
    \ time rate is 52 s\nwhich is less than 58–102 s of benchmarks’ expectations time.\n\
    Han et al. [96]\n2019\nMedium to Long Term Load\nForecasting (a week to a year)\n\
    Two hourly real daily load datasets, Hangzhou from\nJanuary 2015 to March 2017\
    \ and Toronto from May\n2002 to July 2016.\nCNN and\nLSTM\nThe authors proposed\
    \ two methods, time-dependency convolutional\nneural network (TD-CNN) and cycle-based\
    \ long short-term memory\n(C-LSTM), that outperformed other benchmarks in terms\
    \ of accuracy and\nexecution time. Their models’ main advantages are extraction\
    \ of the\nlong-term global combined features and short-term local similar features\
    \ in\nthe LSTM-based model and conversion of load’s temporal correlation into\n\
    spatial ones in the CNN-based model.\nChen et al. [97]\n2019\nA-Day Ahead Load\
    \ Forecasting\nTwo hourly real datasets North American Utility and\nthe ISO-NE\
    \ from 1985 to 1992; the datasets of 1991 and\n1992 were used for testing\nANN\
    \ with\nresidual\nconnections\nThis study introduced a deep neural network with\
    \ residual connections,\none of the well-known techniques to overcome the problem\
    \ of lost\ninformation in earlier layers in a deep network by applying direct\
    \ links\nfrom primary layers to deeper ones. Applying ensemble strategy on the\n\
    two rich datasets provides the generalization capacity of their model. The\nproposed\
    \ model improved the MAPE error rate from 1.48 of the best\nbenchmark model to\
    \ 1.447 in the ISO-NE dataset and from 1.73 to 1.575 for\nthe North-American utility\
    \ dataset, which also implies the robustness to\ntemperature variation of the\
    \ proposed model.\nEl-Hendawi et al.\n[98]\n2020\nA-Day Ahead Load Forecasting\n\
    Real dataset of the hourly electric market of Ontario,\nCanada from 2011 to 2016\n\
    ANN\nThe authors used the wavelet transform to decompose the input data into\n\
    different levels with different frequencies to feed several neural networks.\n\
    Instead of having one model, they trained different neural-based models\nwith\
    \ part of transformed input data and made ﬁnal forecasting considering\nall models’\
    \ predictions. The proposed ensemble model improved the\nMAPE accuracy by 20%\
    \ compared to other traditional neural networks.\nEnergies 2021, 14, 4776\n17\
    \ of 24\n4. Discussion\nML-based algorithms have shown remarkable results in power\
    \ system analytics\ncompared to traditional methods. However, even if the models\
    \ proposed by the literature\nshowed to work ﬁne in real datasets, their performance\
    \ in industrial applications has not\nbeen sufﬁciently demonstrated yet, due to\
    \ cost or privacy issues. This suggests the need\nfor further investigations at\
    \ the industrial level, where the presence of input data with\ndifferent distributions\
    \ or big data properties (e.g., volume, velocity, variety, and veracity)\ncould\
    \ decrease the performance of ML models.\nRegarding the data used for system validation,\
    \ the studies generally presented cus-\ntomized datasets. They typically provided\
    \ information on the total number of samples,\nsampling frequency, recording time,\
    \ and percentage of data used for training and validation.\nAs several datasets\
    \ were synthetically generated using simulation software, only various\nstudies\
    \ reported problems with imbalanced datasets and missing items in the data. In\
    \ this\nregard, Hong et al. [45] analyzed the case in which data were missing\
    \ from one of the buses,\nconcluding that system performance decreased significantly.\
    \ Karagiannopoulos et al. [46] ex-\ntrapolated historical data and used information\
    \ from the public domain or from neighboring\nsystems to deal with missing or\
    \ noisy data. In this sense, Hafeez et al. [95] replaced missing\nvalues with\
    \ the average values of preceding days, while El-Hendawi et al. [98] replaced\
    \ miss-\ning data with the average values of the same day in previous years. Similarly,\
    \ Ray et al. [75]\nused measurements from past hours to fill in missing data and\
    \ performed data cleaning\nto exclude incorrect data from training. Jia [79],\
    \ Ou et al. [84], and Alawad et al. [88] also\nhighlighted the need to clean up\
    \ missing data, while Li et al. [44] wrote the missing features\nas zero to keep\
    \ the dimension of the matrix constant. Additionally, Gao et al. [73] presented\n\
    an ML-based fault detection system in a photovoltaic array and quantified the\
    \ impact of\nmissing PV input data (irradiance, temperature, and different combinations\
    \ of them) on\nsystem accuracy. On the other hand, Li et al. [83], Vantuch et\
    \ al. [54], and Liao et al. [53]\ndiscussed the effect of the imbalanced dataset\
    \ on performance. In this sense, Wang et al. [57]\nsolved the data imbalance problem\
    \ using an enhancement method that equalized the amount\nof data (random cropping\
    \ of existing data to generate a new dataset, increase of random\nnoise, signal\
    \ reversing, etc.). Similarly, Jia [79] applied a synthetic minority over-sampling\n\
    technique that addressed the dataset imbalance problem without overfitting the\
    \ classifier.\nThe lack of standard datasets for the testing of ML-based algorithms\
    \ also emerged as\na relevant issue. Indeed, all the models presented in the literature\
    \ are usually tested on\nnot-standard datasets, with very different characteristics\
    \ and peculiarities, thus making the\ncomparison of the performance of such methods\
    \ almost impossible. It is then apparent that,\nwhen it comes to the selection\
    \ of the most suitable ML method to be implemented in large\nscale applications,\
    \ this lack of information represents a relevant issue, that would eventually\n\
    prevent the implementation of novel (and potentially more performing) methods\
    \ in favor\nof (probably less performing) traditional ones. This, in the end,\
    \ highlights the need for the\ndeﬁnition of application-speciﬁc standard datasets,\
    \ to allow a fair comparison between\nthe very different ML methods proposed for\
    \ each application. The standardized dataset\nshould have the following properties:\n\
    Size: considering the industrial side, the dataset size should be considerably\
    \ big with\nhigh dimensionality. Although some weak learners, such as DT, showed\
    \ to work perfectly\nwith a small amount of data, they would not well generalize\
    \ in the big domain. On the\ncontrary, neural network models have better accuracy\
    \ results in the big domain;\nQuality: if the focus is only on the performance\
    \ of the machine learning model, the\ndifferent input datasets should have the\
    \ same properties. For example, some models are\nvery robust to none values or\
    \ outliers while others are not. Preparing a dataset before\nfeeding it to a model\
    \ relates to data engineering procedures rather than to the model\nperformance;\n\
    Validity: the dataset should accurately represent the phenomena or reality of\
    \ events.\nThe statistical properties of the standardized dataset should be as\
    \ much as possible close to\nreal-life scenarios to show how practical models\
    \ are;\nEnergies 2021, 14, 4776\n18 of 24\nUniqueness and completeness: the information\
    \ should be unique and not be du-\nplicated over the dataset to make sure a trained\
    \ model will generalize well enough in\nactual cases. Moreover, it should cover\
    \ all the possible occurrences or conditions. When\nconsidering, for example,\
    \ the power quality disturbance classiﬁcation, the dataset should\ninclude all\
    \ the essential distortions;\nTrain and test division: it is important the make\
    \ sure that the performance of all\nmodels are evaluated with the same train set.\
    \ Otherwise, a chosen test set probably only\nconsists of easy instances, or it\
    \ does not consist of all the possibilities;\nAccuracy metrics: authors used different\
    \ metrics to evaluate their model performance;\nhowever, it is not possible to\
    \ compare various studies when the same accuracy metrics are\nnot used. The metrics\
    \ should be proposed taking into account the nature of problems. For\nexample,\
    \ there are much fewer abnormal events in anomaly detection than normal, so the\n\
    model with 99% accuracy does not guarantee that it correctly detected all abnormal\
    \ events;\nfor such studies, F1-score or AUC should be taken into account.\nResearchers\
    \ proposed different models based on one or more techniques. Figure 3.\nshows\
    \ the frequencies of techniques presented in the literature review of this study.\
    \ In\nthis ﬁgure, ANN consists of the traditional neural network such as MLP and\
    \ Boltzmann\nmachine, SVM includes both classiﬁcation and regression, and PCA\
    \ encompasses all PCA\nmethods.\nEnergies 2021, 14, x FOR PEER REVIEW \n21 of\
    \ 27 \n \nAccuracy metrics: authors used different metrics to evaluate their model\
    \ perfor-\nmance; however, it is not possible to compare various studies when\
    \ the same accuracy \nmetrics are not used. The metrics should be proposed taking\
    \ into account the nature of \nproblems. For example, there are much fewer abnormal\
    \ events in anomaly detection than \nnormal, so the model with 99% accuracy does\
    \ not guarantee that it correctly detected all \nabnormal events; for such studies,\
    \ F1-score or AUC should be taken into account. \nResearchers proposed different\
    \ models based on one or more techniques. Figure 3. \nshows the frequencies of\
    \ techniques presented in the literature review of this study. In \nthis figure,\
    \ ANN consists of the traditional neural network such as MLP and Boltzmann \n\
    machine, SVM includes both classification and regression, and PCA encompasses\
    \ all PCA \nmethods. \n \nFigure 3. Frequencies of used techniques presented in\
    \ the literature review. \nAlternatively, it seems the hybrid models had better\
    \ performances compared to oth-\ners, particularly the one that combined feature\
    \ engineering techniques with prediction \nmodels. Reinforcement learning methods\
    \ such as Q-learning have also enhanced accuracy \nin some applications like intelligent\
    \ transportation systems and load forecasting. In some \napplications, such as\
    \ PV prediction or load forecasting, which deal with temporal datasets, \nsome\
    \ sequential techniques such as GRU or LSTM are preferred. \n5. Conclusions \n\
    When facing the challenges related to the management of smart power systems, it\
    \ \nbecame apparent that traditional techniques are no more computationally promising\
    \ so-\nlutions. One of the limitations of conventional algorithms is their inadequate\
    \ capacity to \nhandle a large amount of data—consisting of chunks of heterogeneous\
    \ datasets—collect-\ning from measurement devices such as phasor measurement units\
    \ and smart meters. As \na result, many researchers developed high-level, efficient,\
    \ and reliable solutions based on \nstate-of-the-art intelligent learning algorithms\
    \ to provide innovative solutions or promote \nthe overall performance of current\
    \ models in various power system fields. In this context, \nthe ML paradigm and\
    \ modern ML algorithms are categorized and presented in this arti-\ncle. Furthermore,\
    \ this study provided a systematic overview of the latest machine learning \n\
    techniques and models employed to bring new resolutions in power flows, power\
    \ quality \nevents, power quality parameters, photovoltaic systems, intelligent\
    \ transportation sys-\nFigure 3. Frequencies of used techniques presented in the\
    \ literature review.\nAlternatively, it seems the hybrid models had better performances\
    \ compared to others,\nparticularly the one that combined feature engineering\
    \ techniques with prediction models.\nReinforcement learning methods such as Q-learning\
    \ have also enhanced accuracy in\nsome applications like intelligent transportation\
    \ systems and load forecasting. In some\napplications, such as PV prediction or\
    \ load forecasting, which deal with temporal datasets,\nsome sequential techniques\
    \ such as GRU or LSTM are preferred.\n5. Conclusions\nWhen facing the challenges\
    \ related to the management of smart power systems, it\nbecame apparent that traditional\
    \ techniques are no more computationally promising so-\nlutions. One of the limitations\
    \ of conventional algorithms is their inadequate capacity to\nhandle a large amount\
    \ of data—consisting of chunks of heterogeneous datasets—collecting\nfrom measurement\
    \ devices such as phasor measurement units and smart meters. As a\nresult, many\
    \ researchers developed high-level, efﬁcient, and reliable solutions based on\n\
    state-of-the-art intelligent learning algorithms to provide innovative solutions\
    \ or promote\nthe overall performance of current models in various power system\
    \ ﬁelds. In this context,\nEnergies 2021, 14, 4776\n19 of 24\nthe ML paradigm\
    \ and modern ML algorithms are categorized and presented in this article.\nFurthermore,\
    \ this study provided a systematic overview of the latest machine learning\ntechniques\
    \ and models employed to bring new resolutions in power ﬂows, power quality\n\
    events, power quality parameters, photovoltaic systems, intelligent transportation\
    \ systems,\nand load forecasting services. The authors also suggested the properties\
    \ of a standard\ndataset for testing and reviewing the ML-based models to make\
    \ a fair comparison between\nthe performances of proposed models for each topic.\
    \ However, the literature analysis\nimplies that hybrid models based on supervised\
    \ machine learning algorithms are applied\nmore exceeding than unsupervised or\
    \ semi-supervised techniques. Thus, it can be high-\nlighting that supervised\
    \ algorithms convey more beneﬁts to problems typically faced by\nelectrical power\
    \ engineers. Finally, it can also be concluded that the application of machine\n\
    learning methods in electrical systems simpliﬁes complex issues and ensures more\
    \ reliable\nand accurate results. As numerous works proposed solutions based on\
    \ ML techniques,\nthe authors limited their research to well-known newly published\
    \ articles. Accordingly, in\nfuture work, the authors focus on and review articles\
    \ related to each topic separately to\nprovide an informative survey.\nAuthor\
    \ Contributions: Conceptualization, S.M.M., F.F. and M.L.; methodology, S.M.M.\
    \ and M.P.;\nformal analysis, S.M.M.; investigation, S.M.M.; resources, S.M.M.;\
    \ writing—original draft preparation,\nS.M.M. and M.P.; writing—review and editing,\
    \ S.M.M., M.P., M.L, R.I. and F.F.; visualization, S.M.M.;\nsupervision, M.L.\
    \ and F.F.; project administration, M.L. All authors have read and agreed to the\n\
    published version of the manuscript.\nFunding: This research received no external\
    \ funding.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\n\
    Abbreviation\nAbbreviation\nMeaning\nACO\nAnt colony optimization\nAdrar\nAlgerian\n\
    AE\nAutoencoder\nAI\nArtiﬁcial intelligence\nANFIS\nAdaptive neuro-fuzzy inference\
    \ system\nANN\nArtiﬁcial neural network\nAUC\nArea under curve\nCBCS\nChaotic\
    \ mapping mechanism, and cuckoo search\nCNN\nConvolutional neural network\nCPS\n\
    Cyber-physical system\nD\nDimensional\nDALFs\nDay-Ahead Load Forecasts\ndB\nDecibels\n\
    DBSCAN\nDensity-based spatial clustering of applications with noise\nDERs\nDistributed\
    \ energy resources\nDNN\nDeep neural network\nDP\nDynamic Programming\nDT\nDecision\
    \ Tree\nDWT\nDiscrete wavelet transform\nELM\nExtreme learning machine\nEMD\n\
    Empirical mode decomposition\nEV\nElectric vehicles\nFFT\nFast Fourier transform\n\
    FM\nDactorization machine\nFN\nFalse negative\nEnergies 2021, 14, 4776\n20 of\
    \ 24\nFP\nFalse positive\nGANs\nGenerative adversarial networks\nGBM\nGradient\
    \ boosted trees\nGPS\nGlobal positioning system\nGRU\nGated recurrent Unit\nICA\n\
    Independent component analysis\nIEEE\nInstitute of electrical and electronics\
    \ engineers\nIoT\nInternet of things\nIPCA\nImproved principal component analysis\n\
    KNN\nK-nearest neighbors\nLASSO\nLeast absolute shrinkage and selection operator\n\
    LR\nLogistic regression/Linear regression\nLSTM\nLong short term memory\nMAE\n\
    Mean absolute error\nMAPE\nMean absolute percentage error\nML\nMachine learning\n\
    MLP\nMulti-layered perceptron\nms\nmilliseconds\nMSE\nMean square error\nNHTS\n\
    National household travel survey\nOPAELM\nOnline p-norm adaptive extreme learning\
    \ machine\nOPF\nOptimal power ﬂow\nPCA\nPrinciple component analysis\nPCWT\nPseudo-continuous\
    \ wavelet transform\np.u.\nPer unit\nPM\nPersistence model\nPMU\nPhasor measurement\
    \ units\nPQ\nPower quality\nPQEs\nPower quality events\nPSO\nParticle swarm optimization\n\
    PSO-H-ELM\nPSO hierarchical ELM\nPV\nPhotovoltaic\nR2\nR-squared\nRESs\nRenewable\
    \ energy sources\nRF\nRandom forest\nRK\nReduced kernel\nRMSE\nRoot mean squared\
    \ error\nRMSEDD\nRoot mean squared Euclidean distance difference\nRNN\nRecurrent\
    \ Neural Network\nROC curve\nReceiver operating characteristic curve\nRTU\nRemote\
    \ terminal units\nSELM\nStacked extreme learning machine\nSoC\nState of charge\n\
    SPM\nSpace phasor model\nSR\nSelf-recurrent mechanism\nSTFT\nShort-time Fourier\
    \ transform\nSVM\nSupport vector machine\nSVR\nSupport vector regression\nTKEO\n\
    Teager–Kaiser energy operator\nTN\nTrue negative\nTP\nTrue positive\nTS-SOM\n\
    Tree-structured self-organizing map\nVMD\nVariational mode decomposition\nWASMs\n\
    Wide-area system measures\nXGB/XGboost\nExtreme gradient boosting\nEnergies 2021,\
    \ 14, 4776\n21 of 24\nReferences\n1.\nHowell, S.; Rezgui, Y.; Hippolyte, J.L.;\
    \ Jayan, B.; Li, H. Towards the next generation of smart grids: Semantic and holonic\n\
    multi-agent management of distributed energy resources. Renew. Sustain. Energy\
    \ Rev. 2017, 77, 193–214. [CrossRef]\n2.\nMemon, A.A.; Kauhaniemi, K. A critical\
    \ review of AC Microgrid protection issues and available solutions. Electr. Power\
    \ Syst. Res.\n2015, 129, 23–31. [CrossRef]\n3.\nHu, J.; Vasilakos, A.V. Energy\
    \ Big Data Analytics and Security: Challenges and Opportunities. IEEE Trans. Smart\
    \ Grid 2016, 7,\n2423–2436. [CrossRef]\n4.\nHong, T.; Chen, C.; Huang, J.; Lu,\
    \ N.; Xie, L.; Zareipour, H. Guest Editorial Big Data Analytics for Grid Modernization.\
    \ IEEE\nTrans. Smart Grid 2016, 7, 2395–2396. [CrossRef]\n5.\nWang, B.; Fang,\
    \ B.; Wang, Y.; Liu, H.; Liu, Y. Power System Transient Stability Assessment Based\
    \ on Big Data and the Core Vector\nMachine. IEEE Trans. Smart Grid 2016, 7, 2561–2570.\
    \ [CrossRef]\n6.\nPasetti, M.; Ferrari, P.; Silva, D.R.C.; Silva, I.; Sisinni,\
    \ E. On the Use of LoRaWAN for the Monitoring and Control of Distributed\nEnergy\
    \ Resources in a Smart Campus. Appl. Sci. 2020, 10, 320. [CrossRef]\n7.\nPasetti,\
    \ M.; Rinaldi, S.; Flammini, A.; Longo, M.; Foiadelli, F. Assessment of electric\
    \ vehicle charging costs in presence of\ndistributed photovoltaic generation and\
    \ variable electricity tariffs. Energies 2019, 12, 499. [CrossRef]\n8.\nMiraftabzadeh,\
    \ S.M.; Foiadelli, F.; Longo, M.; Pasetti, M. A Survey of Machine Learning Applications\
    \ for Power System Analytics.\nIn Proceedings of the 2019 IEEE International Conference\
    \ on Environment and Electrical Engineering and 2019 IEEE Industrial\nand Commercial\
    \ Power Systems Europe (EEEIC/I CPS Europe), Genova, Italy, 10–14 June 2019; pp.\
    \ 1–5.\n9.\nHastie, T.; Tibshirani, R.; Friedman, J. Springer Series in Statistics.\
    \ In The Elements of Statistical Learning, 2nd ed.; Springer: New\nYork, NY, USA,\
    \ 2009; ISBN 978-0-387-84858-7.\n10.\nAgneeswaran, V.S.; Tonpay, P.; Tiwary, J.\
    \ Paradigms for realizing machine learning algorithms. Big Data 2013, 1, 207–214.\n\
    [CrossRef]\n11.\nHuang, G.; Song, S.; Gupta, J.N.D.; Wu, C. Semi-supervised and\
    \ unsupervised extreme learning machines. IEEE Trans. Cybern.\n2014, 44, 2405–2417.\
    \ [CrossRef]\n12.\nBreiman, L.; Friedman, J.H.; Olshen, R.A.; Stone, C.J. Chapman\
    \ and Hall/CRC. In Classiﬁcation and Regression Trees; Routledge:\nAbingdon, UK,\
    \ 1984; ISBN 9780412048418.\n13.\nQiu, J.; Wu, Q.; Ding, G.; Xu, Y.; Feng, S.\
    \ A survey of machine learning for big data processing. EURASIP J. Adv. Signal\
    \ Process.\n2016, 2016, 1–16. [CrossRef]\n14.\nWiering, M.A.; van Hasselt, H.\
    \ Ensemble Algorithms in Reinforcement Learning. IEEE Trans. Syst. Man Cybern.\
    \ Part B 2008, 38,\n930–936. [CrossRef] [PubMed]\n15.\nBartlett, P.L.; Jordan,\
    \ M.I.; McAuliffe, J.D. Convexity, classiﬁcation, and risk bounds. J. Am. Stat.\
    \ Assoc. 2006, 101, 138–156.\n[CrossRef]\n16.\nKleinbaum, D.G.; Klein, M. Statistics\
    \ for Biology and Health. In Logistic Regression, 2nd ed.; Springer: New York,\
    \ NY, USA, 2002;\nISBN 978-0-387-21647-8.\n17.\nPeterson, L. K-nearest neighbor.\
    \ Scholarpedia 2009, 4, 1883. [CrossRef]\n18.\nKeller, J.M.; Gray, M.R. A Fuzzy\
    \ K-Nearest Neighbor Algorithm. IEEE Trans. Syst. Man Cybern. 1985, SMC-15, 580–585.\
    \ [CrossRef]\n19.\nDevroye, L.; Györﬁ, L.; Lugosi, G. Stochastic Modelling and\
    \ Applied Probability. In A Probabilistic Theory of Pattern Recognition,\n1st\
    \ ed.; Springer: New York, NY, USA, 1996; Volume 31, ISBN 978-1-4612-0711-5.\n\
    20.\nRish, I. An empirical study of the naive Bayes classiﬁer. In Proceedings\
    \ of the Seventeenth International Joint Conference on\nArtiﬁcial Intelligence\
    \ (IJCAI 2001), Seattle, WA, USA, 4–10 August 2001; American Association for Artiﬁcial\
    \ Intelligence: Seattle,\nWA, USA, 2001; Volume 3, pp. 41–46.\n21.\nNoble, W.S.\
    \ What is a support vector machine? Nat. Biotechnol. 2006, 24, 1565–1567. [CrossRef]\n\
    22.\nSafavian, S.R.; Landgrebe, D. A Survey of Decision Tree Classiﬁer Methodology.\
    \ IEEE Trans. Syst. Man Cybern. 1991, 21, 660–674.\n[CrossRef]\n23.\nSong, Y.Y.;\
    \ Lu, Y. Decision tree methods: Applications for classiﬁcation and prediction.\
    \ Shanghai Arch. Psychiatry 2015, 27, 130–135.\n[CrossRef] [PubMed]\n24.\nStrobl,\
    \ C.; Boulesteix, A.L.; Zeileis, A.; Hothorn, T. Bias in random forest variable\
    \ importance measures: Illustrations, sources and\na solution. BMC Bioinform.\
    \ 2007, 8, 1–21. [CrossRef]\n25.\nMontgomery, D.C.; Peck, E.A.; Vining, G.G. Probability\
    \ and Statistics. In Introduction to Linear Regression Analysis, 6th ed.; John\n\
    Wiley & Sons: Hoboken, NJ, USA, 2021; ISBN 978-1-119-57875-8.\n26.\nSeber, G.A.F.;\
    \ Lee, A.J. Linear Regression Analysis; John Wiley & Sons: Hoboken, NJ, USA, 2003;\
    \ ISBN 978-0-471-41540-4.\n27.\nPrasad, A.M.; Iverson, L.R.; Liaw, A. Newer classiﬁcation\
    \ and regression tree techniques: Bagging and random forests for\necological prediction.\
    \ Ecosystems 2006, 9, 181–199. [CrossRef]\n28.\nAnthony, M.; Bartlett, P.L. Neural\
    \ Network Learning: Theoretical Foundations; Cambridge University Press: Cambridge,\
    \ UK, 2009;\nISBN 9780521118620.\n29.\nKalchbrenner, N.; Grefenstette, E.; Blunsom,\
    \ P. A Convolutional Neural Network for Modelling Sentences. In Proceedings of\
    \ the\n52nd Annual Meeting of the Association for Computational Linguistics (Volume\
    \ 1: Long Papers), Baltimore, MD, USA, 22–27\nJune 2014; pp. 655–665.\nEnergies\
    \ 2021, 14, 4776\n22 of 24\n30.\nGers, F.A.; Schmidhuber, J.; Cummins, F. Learning\
    \ to forget: Continual prediction with LSTM. Neural Comput. 2000, 12, 2451–2471.\n\
    [CrossRef]\n31.\nHuang, G.B.; Zhu, Q.Y.; Siew, C.K. Extreme learning machine:\
    \ Theory and applications. Neurocomputing 2006, 70, 489–501.\n[CrossRef]\n32.\n\
    Chen, T.; Guestrin, C. XGBoost: A scalable tree boosting system. In Proceedings\
    \ of the ACM SIGKDD International Conference\non Knowledge Discovery and Data\
    \ Mining, San Francisco, CA, USA, 13–17 August 2016; ACM: San Francisco, CA, USA,\
    \ 2016;\nVolume 13–17, pp. 785–794.\n33.\nLikas, A.; Vlassis, N.; Verbeek, J.J.\
    \ The global k-means clustering algorithm. Pattern Recognit. 2003, 36, 451–461.\
    \ [CrossRef]\n34.\nBirant, D.; Kut, A. ST-DBSCAN: An algorithm for clustering\
    \ spatial-temporal data. Data Knowl. Eng. 2007, 60, 208–221. [CrossRef]\n35.\n\
    Bendat, J.S.; Piersol, A.G. Engineering Applications of Correlation and Spectral\
    \ Analysis, 2nd ed.; John Wiley & Sons: New York, NY,\nUSA, 2013; ISBN 978-0-471-57055-4.\n\
    36.\nShlens, J. A Tutorial on Principal Component Analysis. arXiv 2014, arXiv:1404.1100.\n\
    37.\nTschannen, M.; Bachem, O.; Lucic, M. Recent advances in autoencoder-based\
    \ representation learning. In Proceedings of the Third\nworkshop on Bayesian Deep\
    \ Learning (NeurIPS 2018), Montréal, QC, Canada, 7 December 2018.\n38.\nAygun,\
    \ R.C.; Yavuz, A.G. Network Anomaly Detection with Stochastically Improved Autoencoder\
    \ Based Models. In Proceedings\nof the 4th IEEE International Conference on Cyber\
    \ Security and Cloud Computing (CSCloud 2017), New York, NY, USA, 26–28\nJune\
    \ 2017; IEEE: New York, NY, USA, 2017; pp. 193–198.\n39.\nLei, X.; Yang, Z.; Yu,\
    \ J.; Zhao, J.; Gao, Q.; Yu, H. Data-Driven Optimal Power Flow: A Physics-Informed\
    \ Machine Learning\nApproach. IEEE Trans. Power Syst. 2021, 36, 346–354. [CrossRef]\n\
    40.\nWang, S.; Dehghanian, P.; Li, L.; Wang, B. A Machine Learning Approach to\
    \ Detection of Geomagnetically Induced Currents in\nPower Grids. IEEE Trans. Ind.\
    \ Appl. 2020, 56, 1098–1106. [CrossRef]\n41.\nRavikumar, G.; Govindarasu, M. Anomaly\
    \ Detection and Mitigation for Wide-Area Damping Control using Machine Learning.\n\
    IEEE Trans. Smart Grid 2020, 1. [CrossRef]\n42.\nZhang, Y.; Wang, X.; Wang, J.;\
    \ Zhang, Y. Deep Reinforcement Learning Based Volt-VAR Optimization in Smart Distribution\n\
    Systems. IEEE Trans. Smart Grid 2021, 12, 361–371. [CrossRef]\n43.\nBaker, K.;\
    \ Bernstein, A. Joint Chance Constraints in AC Optimal Power Flow: Improving Bounds\
    \ through Learning. IEEE Trans.\nSmart Grid 2019, 10, 6376–6385. [CrossRef]\n\
    44.\nLi, N.; Li, B.; Gao, L. Transient Stability Assessment of Power System Based\
    \ on XGBoost and Factorization Machine. IEEE Access\n2020, 8, 28403–28414. [CrossRef]\n\
    45.\nHong, G.; Kim, Y.S. Supervised Learning Approach for State Estimation of\
    \ Unmeasured Points of Distribution Network. IEEE\nAccess 2020, 8, 113918–113931.\
    \ [CrossRef]\n46.\nKaragiannopoulos, S.; Aristidou, P.; Hug, G. Data-Driven Local\
    \ Control Design for Active Distribution Grids Using Off-Line\nOptimal Power Flow\
    \ and Machine Learning Techniques. IEEE Trans. Smart Grid 2019, 10, 6461–6471.\
    \ [CrossRef]\n47.\nZhao, Y.; Chen, J.; Poor, H.V. A Learning-to-Infer Method for\
    \ Real-Time Power Grid Multi-Line Outage Identiﬁcation. IEEE Trans.\nSmart Grid\
    \ 2020, 11, 555–564. [CrossRef]\n48.\nKing, J.E.; Jupe, S.C.E.; Taylor, P.C. Network\
    \ State-Based Algorithm Selection for Power Flow Management Using Machine\nLearning.\
    \ IEEE Trans. Power Syst. 2015, 30, 2657–2664. [CrossRef]\n49.\nLabed, I.; Labed,\
    \ D. Extreme learning machine-based alleviation for overloaded power system. IET\
    \ Gener. Transm. Distrib. 2019,\n13, 5058–5070. [CrossRef]\n50.\nRay, P.K.; Mohanty,\
    \ A.; Panigrahi, T. Power quality analysis in solar PV integrated microgrid using\
    \ independent component\nanalysis and support vector machine. Optik (Stuttg.)\
    \ 2019, 180, 691–698. [CrossRef]\n51.\nSahani, M.; Dash, P.K.; Samal, D. A real-time\
    \ power quality events recognition using variational mode decomposition and\n\
    online-sequential extreme learning machine. Measurement 2020, 157, 107597. [CrossRef]\n\
    52.\nTurovic, R.; Stanisavljevic, A.; Dragan, D.; Katic, V. Machine learning for\
    \ application in distribution grids for power quality\napplications. In Proceedings\
    \ of the 2019 20th International Symposium on Power Electronics (Ee 2019), Novi\
    \ Sad, Serbia, 23–16\nOctober 2019; IEEE: Novi Sad, Serbia, 2019; pp. 1–6.\n53.\n\
    Liao, H.; Milanovic, J.V.; Rodrigues, M.; Shenﬁeld, A. Voltage Sag Estimation\
    \ in Sparsely Monitored Power Systems Based on\nDeep Learning and System Area\
    \ Mapping. IEEE Trans. Power Deliv. 2018, 33, 3162–3172. [CrossRef]\n54.\nVantuch,\
    \ T.; Misak, S.; Jezowicz, T.; Burianek, T.; Snasel, V. The Power Quality Forecasting\
    \ Model for Off-Grid System Supported\nby Multiobjective Optimization. IEEE Trans.\
    \ Ind. Electron. 2017, 64, 9507–9516. [CrossRef]\n55.\nBagheri, A.; Gu, I.Y.H.;\
    \ Bollen, M.H.J.; Balouji, E. A Robust Transform-Domain Deep Convolutional Network\
    \ for Voltage Dip\nClassiﬁcation. IEEE Trans. Power Deliv. 2018, 33, 2794–2802.\
    \ [CrossRef]\n56.\nSahani, M.; Dash, P.K. Automatic Power Quality Events Recognition\
    \ Using Modes Decomposition Based Online P-Norm Adaptive\nExtreme Learning Machine.\
    \ IEEE Trans. Ind. Inform. 2020, 16, 4355–4364. [CrossRef]\n57.\nWang, J.; Xu,\
    \ Z.; Che, Y. Power quality disturbance classiﬁcation based on DWT and multilayer\
    \ perceptron extreme learning\nmachine. Appl. Sci. 2019, 9, 2315. [CrossRef]\n\
    58.\nShen, Y.; Abubakar, M.; Liu, H.; Hussain, F. Power Quality Disturbance Monitoring\
    \ and Classiﬁcation Based on Improved PCA\nand Convolution Neural Network for\
    \ Wind-Grid Distribution Systems. Energies 2019, 12, 1280. [CrossRef]\nEnergies\
    \ 2021, 14, 4776\n23 of 24\n59.\nDeng, Y.; Wang, L.; Jia, H.; Tong, X.; Li, F.\
    \ A Sequence-to-Sequence Deep Learning Architecture Based on Bidirectional GRU\
    \ for\nType Recognition and Time Location of Combined Power Quality Disturbance.\
    \ IEEE Trans. Ind. Inform. 2019, 15, 4481–4493.\n[CrossRef]\n60.\nCao, J.; Zhang,\
    \ W.; Xiao, Z.; Hua, H. Reactive Power Optimization for Transient Voltage Stability\
    \ in Energy Internet via Deep\nReinforcement Learning Approach. Energies 2019,\
    \ 12, 1556. [CrossRef]\n61.\nAbed, A. Improved Power Factor of Electrical Generation\
    \ by using Clustering Neural Network. Int. J. Appl. Eng. Res. 2018, 13,\n4633–4636.\n\
    62.\nZhang, X.; Wang, Y.; Zheng, Y.; Ding, R.; Chen, Y.; Wang, Y.; Cheng, X.;\
    \ Yue, S. Reactive Load Prediction Based on a Long\nShort-Term Memory Neural Network.\
    \ IEEE Access 2020, 8, 90969–90977. [CrossRef]\n63.\nNakawiro, W. A Machine Learning\
    \ Approach for Coordinated Voltage and Reactive Power Control. ECTI Trans. Electr.\
    \ Eng.\nElectron. Commun. 2020, 18, 54–60. [CrossRef]\n64.\nMoreira, A.C.; Paredes,\
    \ H.K.M.; de Souza, W.A.; Nardelli, P.H.J.; Marafão, F.P.; da Silva, L.C.P. Evaluation\
    \ of Pattern Recognition\nAlgorithms for Applications on Power Factor Compensation.\
    \ J. Control Autom. Electr. Syst. 2018, 29, 75–90. [CrossRef]\n65.\nValenti, M.;\
    \ Bonﬁgli, R.; Principi, E.; Squartini, S. Exploiting the Reactive Power in Deep\
    \ Neural Models for Non-Intrusive Load\nMonitoring. In Proceedings of the 2018\
    \ International Joint Conference on Neural Networks (IJCNN), Rio de Janeiro, Brazil,\
    \ 8–13\nJuly 2018; pp. 1–8.\n66.\nKeerthisinghe, C.; Mickelson, E.; Kirschen,\
    \ D.S.; Shih, N.; Gibson, S. Improved PV Forecasts for Capacity Firming. IEEE\
    \ Access\n2020, 8, 152173–152182. [CrossRef]\n67.\nWen, S.; Zhang, C.; Lan, H.;\
    \ Xu, Y.; Tang, Y.; Huang, Y. A hybrid ensemble model for interval prediction\
    \ of solar power output in\nship onboard power systems. IEEE Trans. Sustain. Energy\
    \ 2021, 12, 14–24. [CrossRef]\n68.\nDhibi, K.; Fezai, R.; Mansouri, M.; Trabelsi,\
    \ M.; Kouadri, A.; Bouzara, K.; Nounou, H.; Nounou, M. Reduced Kernel Random\n\
    Forest Technique for Fault Detection and Classiﬁcation in Grid-Tied PV Systems.\
    \ IEEE J. Photovolt. 2020, 10, 1864–1871. [CrossRef]\n69.\nZhang, Y.; Qin, C.;\
    \ Srivastava, A.K.; Jin, C.; Sharma, R.K. Data-Driven Day-Ahead PV Estimation\
    \ Using Autoencoder-LSTM and\nPersistence Model. IEEE Trans. Ind. Appl. 2020,\
    \ 56, 7185–7192. [CrossRef]\n70.\nChang, X.; Li, W.; Zomaya, A.Y. A Lightweight\
    \ Short-Term Photovoltaic Power Prediction for Edge Computing. IEEE Trans. Green\n\
    Commun. Netw. 2020, 4, 946–955. [CrossRef]\n71.\nKhan, M.A.; Kurukuru, V.S.B.;\
    \ Haque, A.; Mekhilef, S. Islanding Classiﬁcation Mechanism for Grid-Connected\
    \ Photovoltaic\nSystems. IEEE J. Emerg. Sel. Top. Power Electron. 2020, 9, 1966–1975.\
    \ [CrossRef]\n72.\nWang, J.; Zhong, H.; Lai, X.; Xia, Q.; Wang, Y.; Kang, C. Exploring\
    \ key weather factors from analytical modeling toward improved\nsolar power forecasting.\
    \ IEEE Trans. Smart Grid 2019, 10, 1417–1427. [CrossRef]\n73.\nGao, W.; Wai, R.J.\
    \ A Novel Fault Identiﬁcation Method for Photovoltaic Array via Convolutional\
    \ Neural Network and Residual\nGated Recurrent Unit. IEEE Access 2020, 8, 159493–159510.\
    \ [CrossRef]\n74.\nCatalina, A.; Alaiz, C.M.; Dorronsoro, J.R. Combining Numerical\
    \ Weather Predictions and Satellite Data for PV Energy Nowcast-\ning. IEEE Trans.\
    \ Sustain. Energy 2020, 11, 1930–1937. [CrossRef]\n75.\nRay, B.; Shah, R.; Islam,\
    \ M.R.; Islam, S. A New Data Driven Long-Term Solar Yield Analysis Model of Photovoltaic\
    \ Power Plants.\nIEEE Access 2020, 8, 136223–136233. [CrossRef]\n76.\nYap, K.Y.;\
    \ Sarimuthu, C.R.; Lim, J.M.Y. Grid Integration of Solar Photovoltaic System Using\
    \ Machine Learning-Based Virtual\nInertia Synthetization in Synchronverter. IEEE\
    \ Access 2020, 8, 49961–49976. [CrossRef]\n77.\nKeerthisinghe, C.; Chapman, A.C.;\
    \ Verbiˇc, G. Energy Management of PV-Storage Systems: Policy Approximations Using\
    \ Machine\nLearning. IEEE Trans. Ind. Inform. 2019, 15, 257–265. [CrossRef]\n\
    78.\nAshqar, H.I.; Almannaa, M.H.; Elhenawy, M.; Rakha, H.A.; House, L. Smartphone\
    \ transportation mode recognition using a\nhierarchical machine learning classiﬁer\
    \ and pooled features from time and frequency domains. IEEE Trans. Intell. Transp.\
    \ Syst.\n2019, 20, 244–252. [CrossRef]\n79.\nJia, J. Analysis of Alternative Fuel\
    \ Vehicle (AFV) Adoption Utilizing Different Machine Learning Methods: A Case\
    \ Study of 2017\nNHTS. IEEE Access 2019, 7, 112726–112735. [CrossRef]\n80.\nAksjonov,\
    \ A.; Nedoma, P.; Vodovozov, V.; Petlenkov, E.; Herrmann, M. Detection and Evaluation\
    \ of Driver Distraction Using\nMachine Learning and Fuzzy Logic. IEEE Trans. Intell.\
    \ Transp. Syst. 2019, 20, 2048–2059. [CrossRef]\n81.\nNallaperuma, D.; Nawaratne,\
    \ R.; Bandaragoda, T.; Adikari, A.; Nguyen, S.; Kempitiya, T.; De Silva, D.; Alahakoon,\
    \ D.; Pothuhera,\nD. Online Incremental Machine Learning Platform for Big Data-Driven\
    \ Smart Trafﬁc Management. IEEE Trans. Intell. Transp. Syst.\n2019, 20, 4679–4690.\
    \ [CrossRef]\n82.\nGjoreski, M.; Gams, M.Z.; Luštrek, M.; Genc, P.; Garbas, J.U.;\
    \ Hassan, T. Machine Learning and End-to-End Deep Learning for\nMonitoring Driver\
    \ Distractions from Physiological and Visual Signals. IEEE Access 2020, 8, 70590–70603.\
    \ [CrossRef]\n83.\nLi, Q.; Wang, F.; Wang, J.; Li, W. LSTM-Based SQL Injection\
    \ Detection Method for Intelligent Transportation System. IEEE Trans.\nVeh. Technol.\
    \ 2019, 68, 4182–4191. [CrossRef]\n84.\nOu, J.; Lu, J.; Xia, J.; An, C.; Lu, Z.\
    \ Learn, Assign, and Search: Real-Time Estimation of Dynamic Origin-Destination\
    \ Flows Using\nMachine Learning Algorithms. IEEE Access 2019, 7, 26967–26983.\
    \ [CrossRef]\n85.\nKhadilkar, H. A Scalable Reinforcement Learning Algorithm for\
    \ Scheduling Railway Lines. IEEE Trans. Intell. Transp. Syst. 2019,\n20, 727–736.\
    \ [CrossRef]\nEnergies 2021, 14, 4776\n24 of 24\n86.\nZhang, K.; Liu, Z.; Zheng,\
    \ L. Short-Term Prediction of Passenger Demand in Multi-Zone Level: Temporal Convolutional\
    \ Neural\nNetwork with Multi-Task Learning. IEEE Trans. Intell. Transp. Syst.\
    \ 2020, 21, 1480–1490. [CrossRef]\n87.\nCheng, R.; Song, Y.; Chen, D.; Ma, X.\
    \ Intelligent Positioning Approach for High Speed Trains Based on Ant Colony Optimization\n\
    and Machine Learning Algorithms. IEEE Trans. Intell. Transp. Syst. 2019, 20, 3737–3746.\
    \ [CrossRef]\n88.\nAlawad, H.; Kaewunruen, S.; An, M. Learning from Accidents:\
    \ Machine Learning for Safety at Railway Stations. IEEE Access\n2020, 8, 633–648.\
    \ [CrossRef]\n89.\nZhang, Z.; Hong, W.C.; Li, J. Electric Load Forecasting by\
    \ Hybrid Self-Recurrent Support Vector Regression Model with Variational\nMode\
    \ Decomposition and Improved Cuckoo Search algorithm. IEEE Access 2020, 8, 14642–14658.\
    \ [CrossRef]\n90.\nFeng, C.; Sun, M.; Zhang, J. Reinforced Deterministic and Probabilistic\
    \ Load Forecasting via Q -Learning Dynamic Model\nSelection. IEEE Trans. Smart\
    \ Grid 2020, 11, 1377–1386. [CrossRef]\n91.\nAhmad, A.; Javaid, N.; Mateen, A.;\
    \ Awais, M.; Khan, Z.A. Short-Term load forecasting in smart grids: An intelligent\
    \ modular\napproach. Energies 2019, 12, 164. [CrossRef]\n92.\nZheng, H.; Yuan,\
    \ J.; Chen, L. Short-Term Load Forecasting Using EMD-LSTM neural networks with\
    \ a xgboost algorithm for\nfeature importance evaluation. Energies 2017, 10, 1168.\
    \ [CrossRef]\n93.\nDabbaghjamanesh, M.; Moeini, A.; Kavousi-Fard, A. Reinforcement\
    \ Learning-based Load Forecasting of Electric Vehicle Charging\nStation Using\
    \ Q-LearningTechnique. IEEE Trans. Ind. Inform. 2020, 17, 4229–4237. [CrossRef]\n\
    94.\nFarsi, B.; Amayri, M.; Bouguila, N.; Eicker, U. On Short-Term Load Forecasting\
    \ Using Machine Learning Techniques and a Novel\nParallel Deep LSTM-CNN Approach.\
    \ IEEE Access 2021, 9, 31191–31212. [CrossRef]\n95.\nHafeez, G.; Alimgeer, K.S.;\
    \ Khan, I. Electric load forecasting based on deep learning and optimized by heuristic\
    \ algorithm in\nsmart grid. Appl. Energy 2020, 269, 114915. [CrossRef]\n96.\n\
    Han, L.; Peng, Y.; Li, Y.; Yong, B.; Zhou, Q.; Shu, L. Enhanced deep networks\
    \ for short-term and medium-term load forecasting.\nIEEE Access 2019, 7, 4045–4055.\
    \ [CrossRef]\n97.\nChen, K.; Chen, K.; Wang, Q.; He, Z.; Hu, J.; He, J. Short-Term\
    \ Load Forecasting with Deep Residual Networks. IEEE Trans. Smart\nGrid 2019,\
    \ 10, 3943–3952. [CrossRef]\n98.\nEl-Hendawi, M.; Wang, Z. An ensemble method\
    \ of full wavelet packet transform and neural network for short term electrical\n\
    load forecasting. Electr. Power Syst. Res. 2020, 182, 106265. [CrossRef]\n"
  inline_citation: '>'
  journal: Energies
  limitations: '>'
  pdf_link: https://www.mdpi.com/1996-1073/14/16/4776/pdf?version=1628228231
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Advances in the Application of Machine Learning Techniques for Power System
    Analytics: A Survey'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2021.3054833
  analysis: '>'
  authors:
  - Jagadeesha R. Bhat
  - Salman A. AlQahtani
  citation_count: 119
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 9 6G Ecosystem:
    Current Status and Future Perspective Publisher: IEEE Cite This PDF Jagadeesha
    R. Bhat; Salman A. Alqahtani All Authors 122 Cites in Papers 15063 Full Text Views
    Open Access Comment(s) Under a Creative Commons License Abstract Document Sections
    I. Introduction II. Scope of Future Networks III. Literature Review IV. 6G Enabling
    Technologies V. Architectures for 6G Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: Next-generation of the cellular network will
    attempt to overcome the limitations of the current Fifth Generation (5G) networks
    and equip itself to address the challenges which become obvious in the future.
    Currently, academia and industry have focused their attention on the Sixth Generation
    (6G) network, which is anticipated to be the next big game-changer in the telecom
    industry. The outbreak of COVID''19 has made the whole world to opt for virtual
    meetings, live video interactions ranging from healthcare, business to education.
    However, we miss an immersive experience due to the lack of supporting technology.
    Experts have anticipated that starting from the post-pandemic age, the performance
    requirements of technology for virtual and real-time communication, the rise of
    several verticals such as industrial automation, robotics, and autonomous driving
    will increase tremendously, and will skyrocket during the next decade. In this
    manuscript, we study the latest perspectives and future megatrends that are most
    likely to drive 6G. Initially, we describe the instances that lead us to the vision
    of 6G. Later, we narrate some of the use cases and the KPIs essential to meet
    their performance requirement. Further, we highlight the key requirements of 6G
    based on contemporary research such as UN sustainability goals, business model,
    edge intelligence, digital divide, and the trends in machine learning for 6G.
    Technologies that enable 6G comprising of digital, physical, and biological spaces.
    Published in: IEEE Access ( Volume: 9) Page(s): 43134 - 43167 Date of Publication:
    26 January 2021 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2021.3054833 Publisher:
    IEEE Funding Agency: SECTION I. Introduction Recently after the launch of Fifth
    Generation (5G) mobile networks, several data-intensive applications and verticals
    which require specific network parameters have undergone a deep sense of relief
    due to the availability of high data rate, bandwidth, and reliability. In 2020,
    the 5G deployments by different network providers either as Non-Stand-Alone (NSA),
    or as Stand-Alone (SA), will address the need for mobile internet by high data
    rate (e.g., enhanced Mobile Broadband (eMBB)), ultra-reliable low latency (uRLLC),
    and connectivity between massively dense deployment of smart devices (e.g., massive
    Machine Type Communication (mMTC)) [1]. Further, 5G networks can reduce the network
    scale-up time drastically due to its virtualized core. Consequently, the cost
    reduces. All these features have made 5G networks predominantly different from
    its predecessors in terms of network capacity, and range of applications it would
    support [1]. It is envisioned that by 2030, there will be 97 billion machine type
    devices, resulting in an astonishing increase in the global mobile generated data
    traffic to 5.016 Zeta Bytes (ZB) per month from 0.062 ZB as in 2020 [2]. Further,
    a survey has anticipated that by 2030, the number of people living in 43 megacities
    across the globe will increase to 10 million [19]. Consequently, information communication
    technologies (ICT) should be agile and robust with respect to the massive data
    generated by the mobile devices due to urbanization and from smart city ecosystems
    such as smart transportation, smart healthcare, and smart buildings. Moreover,
    the data generated will be fueled by the emerging applications in the area of
    Artificial Intelligence (AI), Robotics, Industry 4.0, and Internet of Everything
    (IoE) to name but a few, that generates a colossal amount of data. In such a context,
    cellular network infrastructure will be one of the key drivers to support extreme
    data rate, high bandwidth, and quality of service (QoS) for these verticals [3],
    [4]. Let us consider a few future internet applications such as self-driving cars
    with simultaneous communication capability. In such a case, each communication
    link demands extreme network parameters, for instance, connectivity with a delivery
    drone to assist in the delivery of emergency medical facilities that requires
    extremely low latency (<1ms), on the other hand, with the roadside infrastructure
    (V2X) that requires high reliability (~99.9999999%) to prevent accidents [48].
    Similarly, remote robotic surgery in a smart hospital, remote holographic image
    transmission of a live public concert, data communication from ultra-high dense
    IoT (uHDIoT) and wearable devices, haptic online games, and haptic meetings are
    some other scenarios. In 2030, these use cases will be the key thrust areas which
    require a high degree of automation and intelligence to address diversified requirements
    [5]–[8]. To address these requirements, we need data rate up to 1 Tbps, ultra-high
    reliability of 10 −9 and ultra-low latency of 0.1 ms or less [2]–[4]. It is evident
    from the above facts that 5G telecom networks will drastically fail to meet the
    said requirements, as 5G’s capacity is relatively lower and unified [8]. As a
    result, there is a definite need for a next-generation system that support new
    services, and technologies while maintaining backward compatibility. Altogether,
    the new technology components and architecture for beyond 5G (B5G) networks is
    necessary to address a plethora of user applications [3]–[9]. Several experts
    have articulated the speculated features of Sixth Generation (6G) networks which
    are considered to be the successor of 5G networks [1]–[9]. As expected, 6G network
    shall overcome the major limitations of the predecessor networks; alongside, it
    will further extend the three key features of the 5G network. In Fig. 1(a), the
    intersection of these factors (classes of use cases) is shown. Certain scenarios
    may require multiple use cases to be met simultaneously (uRLLC and mMTC) as shown
    in Fig 1. (a) with intersecting circles. Specifically, 6G network will further
    enhance Ultra-Mobile Broadband (feUMBB), ultra-High Sensing Low Latency Communications
    (uHSLLC), ultra-High Density Data (uHDD) services, ultra-High Energy Efficiency
    (uHEE), ultra-High Reliability and Sensing (uHRS), ultra-High Reliability and
    User experience (uHRUx), ultra-Low Latency Reliability and Secure (uLLRS), ultra-High
    Security (uHS), ultra-High Sensing and Localization (uHSLo) and several other
    combinations of these Key Performance Indicators (KPIs) and use cases [9], [10].
    Refer Fig. 1(b), for an exemplary use case of 6G for different verticals. As shown
    in the figure, certain verticals require a combination of use cases which is indicated
    by the blue arrow at the intersection. For instance, distance robotic surgery
    will require low latency (uRLLC) and high security of the data (uHS). This interdependency
    between multiple use cases is shown as uLLRS (low latency, reliability, and security)
    by an arrow in Fig 1(b). In the similar way, the other arrows at the intersection
    (uHRS), (uHRUx) of multiple use cases are shown. FIGURE 1. A comparison of 5G
    and 6G using support for verticals (a) 5G and (b) 6G. Show All Currently, 5G network
    is yet to be fully deployed worldwide; as a result, its real-time performance
    on every use case is not well studied. Nevertheless, when we see the evolution
    of the telecom industry, it is evident from the keen observation that each generation
    took a decade from setting a vision, R&D, standardization to market launch. Expecting
    the same trend, the next generation (i.e., 6G) shall be made realizable for customers’
    use around 2030. At this juncture, academia and industry may only have the visionary
    groundwork regarding 6G networks, as it is too early to realize the full potential
    of it. Motivated by the current research on 6G networks and its technological
    components, in this manuscript, we will discuss various aspects of future networks.
    Several kinds of literature have shed light on vision, requirements [1]–[9], technologies
    [10]–[15], use cases [96]–[98], of 6G networks. In this article, we outline the
    key points from the previous research and provide insight into the future research
    trends in 6G networks. The main contributions of this manuscript are as follows:
    First, we discuss the expected key features of future networks to give a direction
    to the research community. We highlight most of the KPIs for 6G networks as applicable
    to different use cases (Table 1) and we provide an outline of the 6G ecosystem
    by considering all the stakeholders. We provide an exhaustive categorized survey
    of exclusive literature on 6G and discuss their research directions (Table 2).
    Unlike the existing literature, we discuss various architectures for 6G networks
    from the literature. Moreover, we proposed network architecture for 6G with a
    layer-wise approach for simplicity; and improved latency, reliability through
    AI-enabled distributed cloud features. Further, we summarize the future megatrends
    in 6G networks, namely, UN Sustainable Development Goals, Digital divide, Edge
    intelligence and Machine learning, and Business models. The remaining part of
    the manuscript is organized as follows: TABLE 1 List of KPI for 6G and Typical
    Values [46], [48], [96], [118] TABLE 2 List of Literatures on 6G and Their Key
    Contributions Section II illustrates the scope of the future networks, KPIs, and
    use cases of 6G networks. In section III, we classify the recent literatures,
    and highlight their contributions to the research community. Section IV discusses
    various technology components of 6G networks and their main features. Furthermore,
    section V presents the architectures of 6G networks, while section VI provides
    the future trends in 6G networks. In section VII, the global research initiatives
    are highlighted, and section VIII narrates the open research areas for future
    exploration. Finally, section IX concludes this article. SECTION II. Scope of
    Future Networks The study of future networks enables us to equip ourselves with
    necessary facilities for the industry verticals required during 2030. The in-depth
    analysis of the current mobile networks reveals the existence of a large gap between
    user application’s expectations and services offered by the network providers
    [38]. For example, user expectations on immersive multimedia, personalized holograms,
    multi-sense haptic service, etc. remain as a gap which the current networks shall
    not support [12]. Further, technology components including hardware, software
    and overall ecosystem (devices, spectrum and standards, security, data management
    algorithms, cloud-based core and access solutions, and applications) need tremendous
    upgrade [19]–[23], [28]. A. Transition from 5G to 6G: Gaps and Recommendations
    Implementation of 5G has escalated in most of the mega- cities worldwide due to
    5G’s ability to support current industrial use cases. Albeit, there would emerge
    several use cases, societal requirements, and technological evolution in the future
    requiring exploration beyond the abilities of 5G. Currently, terrestrial communication
    is one of the key requirements in 5G networks; however, it would extend in 6G
    networks, say from terrestrial to underwater and aerial in several folds of 5G’s
    capacity [12]–[14]. This degree of freedom in all-round connectivity will encourage
    the rise of several verticals such as flying cars, ultra-real human- computer
    interactions, holographic telepresence, and underwater recreation having challenging
    constraints. To address the demands of these verticals such as 3D connectivity,
    multi-dimensional video (~16K resolution) [46], extremely high data rate (Tbps)
    and bandwidth (~10GHz) [9], and so on, the communication networks must be much
    more intelligent than now. Also, the existing internet cannot support these requirements.
    Therefore, a new KPIs will be necessary to handle the use cases with integrated
    performance requirements. For instance, real-time remote robotic surgery will
    need simultaneous ultra-low latency, massive data rate, and security to work effectively
    [17], [18]. Furthermore, user applications do not have direct control in terms
    of latency, security, congestion, and reliability, leading to poor user experience.
    Similarly, a state of unpreparedness exists to launch holographic data that requires
    several Gbps to Tbps data rate with extremely low latency and ability to connect
    to millions of users in real-time. The time difference between the occurrence
    of events and generation of response should be minimal which is lacking in the
    existing cellular networks. For instance, in a remote robotic surgery sending
    a command to control the robotic limbs and the video transmissions of the surgery
    must have a time precision (latency) of a fraction of milliseconds. Again, sparse
    infrastructure and interfaces for all-round network connectivity for a packet
    to traverse when we consider air, underwater, and terrestrial communication. Further,
    intelligence has been merely considered at the network edge (as in 5G), but we
    lack intelligence at individual layers of network or when network elements are
    distributed. 1) Recommendations for Future Networks We recommend the following
    key point for the future networks: End-to-End connectivity: Future networks shall
    provide sufficient assistance such as virtualization, intelligent decision, network
    automation, and slicing to applications in order to make them attain reliability,
    security, network capacity along with guaranteed and timely delivery from the
    origin to the destination. Therefore, a holistic approach toward full end-to-end
    connectivity and integrity of data transfer is necessary to make the future applications
    a reality [23], [38], [49]. Interoperability: The network will interoperate between
    heterogeneous, small/ large, and private/ public networks where each mobile node
    will have multiple radio interfaces to provide all-round connectivity. Compatibility:
    It should allow new protocols, network architectures, nodes, and services to coexist
    with the existing technologies. The data transfer protocol must be agile to send
    packets over different network interfaces with the vision of end-to-end realization
    of services. Dedicated timely service: There should be autonomous services such
    as Industry 4.0, autonomous driving, and robotic surgery to improve the service
    quality and scale up the production, time plays a critical role. Therefore, the
    network must have exclusive support for time-critical services [22]–[24]. Edge
    computing capability: To reduce the delay during media-rich services and provide
    local services efficiently at a closer location to the user devices, the data
    from the user will be processed at the edge instead of only at the cloud. This
    increases reliability, scalability, and privacy [42], [97]. Intelligent network:
    Starting from the physical layer to the applications, the AI will be predominant
    and distributed across different network entities such as core, access network,
    and terminal users. Thus, 6G will transit from smart network (as in 5G) to an
    intelligent network [36], [81]. Ultra-smart devices: The existing hand-held smart-
    phones will become obsolete and they will be replaced by either smart glasses
    that would deliver ubiquitous XR experiences or the phone functions that will
    integrate a smart wearable device/ smart patch offering high resolution, and holographic
    experiences [19]. We also envision that the features of the phone will be found
    in a distributed connected intelligent device. Thus, alleviating the need for
    a hand-held dedicated device. Cell-free networking: The mobile device shall connect
    to the radio access network seamlessly without depending on a specific type of
    access point to provide infinite mobility and QoS [31]. The network should be
    flexible enough to allow devices to connect through a range of access networks
    such as THz, mm-wave, and visual light communication (VLC). In addition, a user
    may get served by multiple antennas mounted on various base stations without the
    restriction of cell boundaries; thus, reducing the inter cell interferences [28],
    [40]. Support diverse media: Even if 5G supports diverse multimedia such as text,
    image, video, voice, augmented reality, and multi-dimensional holograms, in 6G
    the applications would demand the multimedia at lowest latency, precise location,
    high data rate, etc., which requires much improved KPIs than 5G [106]. Amalgamated
    sensing, communication, and positioning: Some verticals need a combination of
    ultra-high-speed, ultra-low latency, exact positioning, along with precise sensing
    of the surroundings all being met together. To realize such requirements, future
    networks shall provide resources, energy, hardware, and computational support
    [108]. Multi-level architecture: The storage, communication, and computing should
    be distributed at the user, edge, and cloud levels, which will operate as a centralized
    or as a distributed model to support scalability [108], [114]. B. 6G Ecosystem
    The 6G ecosystem consists of all stakeholders ranging from an equipment manufacturer
    to application developer. A brief diagram of the 6G ecosystem has been presented
    in Fig. 2. The 6G chipset manufacturers will deal with the design of new hardware
    and electronic components such as radio, modulator to accommodate new technology.
    Next, the mobile manufacturers and network equipment vendors (radio access network
    installer) will base their products to support the underlying technology from
    the chipset manufacturers. Next, all these technologies will assist the mobile
    network operators to launch their mobile services to users. Moreover, the edge
    devices at the boundary of radio access and the core network, cloud servers, data
    centers, software modules such as core network virtualization, slice providers,
    content delivers, will exchange data and services with the application developers.
    Since 6G will be centered around the users, the mobile devices, network operators,
    and other service providers of the network shall also revolve around the users.
    The discrete arrows indicate that the mobile users (or IoT devices that need service)
    will directly be supported by the network provider, device manufacturer, and applications.
    However, all the remaining components of the ecosystem will coordinate the services.
    Such coordination, and interdependency/ interplay between each component of the
    ecosystem is shown by the bold arrows. FIGURE 2. 6G ecosystem. Show All Further,
    as shown in Fig. 2, security, and intelligence will be pivotal in the 6G ecosystem.
    The security, and AI as shown, shall be a part of every component such as at the
    device level, at the mobile operator, user, software, edge or cloud level, and
    at the applications. C. Vision and KPIs of 6G Vision and quest for 6G network:
    After discussing the technology gaps and recommendations for 6G networks, we highlight
    the crux of the vision and the need for 6G networks. There are several reasons
    we put forward, which necessitate the need for 6G networks. First, considering
    the broad scope of United Nation’s Sustainability Development Goals (SDGs) for
    2030, there is need for several essential technologies that will satisfy the objectives,
    communication challenges, and new application requirements of the SDGs [44]. Second,
    5G network has matured enough to address the performance requirements of the existing
    verticals. However, when these verticals or market demands grow to the next level,
    say Industry 5.0, Society 5.0, Transport 4.0, and many more that are human-centric,
    ubiquitous, fully automated, and driven by AI, will demand specialized resources
    which burden 5G networks [19]. Third, considering the business models for 2030
    and beyond, the technologies like telepresence, holographic and haptic communications,
    Brain-Computer Interface (BCI), 4D imaging, Extended Reality (ER), and Internet
    of Everything (IoE) will drive the telecommunication industry [39]. The requirements
    of the aforementioned industries and applications cannot be met by 5G networks.
    Thus, leading to the contemplation of 6G networks as an ultimate ecosystem of
    communication technology solutions [34]–[37]. It should be noted that the 6G network
    is not just about exploring new frequency bands, architecture, and access schemes
    to support high data rates. Even though 5G network covers the requirements of
    future networks to a minimum extent, the 6G network must comply with the requirements
    such as extreme reliability, real-time ultra-low latency, and massive connectivity,
    simultaneously. The authors in [9] have categorized the 6G vision as four types,
    namely, intelligent, ubiquitous, deep, and holographic connectivity. In general,
    we suggest that as an ecosystem, 6G networks must envision to support the new
    application requirements of industry and society, especially with regards to the
    following seven metrics during 2030. ( i ) Data rate: This is the ability to provide
    quality service for verticals ranging from autonomous high precision industry
    to immersive virtual/mixed reality applications, (ii) Latency: This is the time-sensitive
    data delivery, (iii) Super Coverage: It means a three-dimensionally covering infrastructure
    that encompasses aerial, terrestrial, and underwater connectivity, (iv) Sense
    of Feel: This involves the support for holographic personal communication to tactile
    internet applications, ( v ) Extremely low power consumption: This stands for
    mobile devices that operate for a long time without requiring to charge, (vi)
    high network density: It means connecting of devices ranging up to 10 million/
    sq.km, and (vii) high security with precise location information ranging from
    10cm to 1cm [39], [53], [55]. TABLE 1 shows the KPIs that mandate the various
    verticals and use cases that will be supported by 6G. D. Use Cases of 6G Networks
    Let’s look at some of the prime use cases of 6G networks along with the KPI requirements,
    as shown in Fig 3. These use cases will require a combination of KPIs simultaneously
    to deliver their services. Hence, they are challenging. The following scenarios
    will be more realistic in 2030. FIGURE 3. Use cases of 6G. Show All 1) Ultra Smart
    Cities 5G networks will provide the user and application-specific QoS and quality
    of experience (QoE) through many verticals. For instance, telemedicine, smart
    agriculture, and smart industry can get the data service at specific data rate,
    latency, or priority. However, when we contemplate the future scenarios in an
    ultra-smart city, that may require, for example, a data rate of the order 1Tbps,
    3D connectivity, localization within 1cm, and reliability of 99.99999999% for
    automated transportation, smart healthcare, or smart industry [46]. The KPIs needed
    for these applications in a smart city cannot be addressed by 5G networks [48],
    [108]. We shall consider a few more examples to visualize the scenarios. The mobility
    support requirements of 6G will typically vary from 240km/hr to 1200km/hr. A self-driving
    car needs to communicate with roadside sensors and other vehicles in the adjacent
    lanes to coordinate while moving at high speeds. Furthermore, the delivery drones
    on the fly may need to communicate with the ambulance on the ground to collect
    the medical supplies in the urban setting and transport it to remote locations.
    In another instance, drones may have to follow the cars to act as floating base
    stations or as relays to communicate the information as a part of inter-vehicular
    communication. It is worthy to note that the two scenarios involve radio communication
    between multiple entities under high mobility (say 500 km/hr). The 6G networks
    should support extremely low latency communication in the above scenarios, which
    is one of the key requirements of autonomous driving and decision making. These
    examples cover a few scenarios of future smart city and their communication requirements.
    But in general, high energy efficiency, high data rate, reliability, latency,
    precise sensing, and localization are essential for a smart city, as shown in
    Fig 3. 2) Multi-Dimensional Reality Human-computer interactions that deal with
    the ultra-high-definition graphical contents such as online games based on Augmented
    Reality (AR) or Virtual Reality (VR), generates a massive amount of data. We would
    soon witness 3D games or multi-dimensional video that interacts with all the five
    sense organs of the body to create an illusion of real-world by combining VR and
    AR to render a true virtual gaming experience. These applications will be tangible
    within a decade as the computer technology, computing power, and storage space
    of the mobile devices multifold in their capacity. Nevertheless, when these augmented
    data to be transmitted through a wireless channel, we need extreme bandwidth,
    reliability, and data rate that 6G networks will offer. In other words, we need
    ultra-high reliability, low latency, ultra-high data density, and user experience,
    as shown in Fig. 3. 3) Haptic Communication Let us consider a smart healthcare
    system where an injured patient can only express her emotions by visualizing in
    her mind. In that situation, a smart headband can reconstruct the brain signals
    and represent it as a 3D video of the patient’s imagination and communicate to
    the caregiver in real-time through mobile networks. In addition, a group of people
    who do not have a common language to communicate can use their imaginations, and
    disabled people get access to open the door, or to control the gadgets can use
    haptic communication. These haptic ways of communication will enable them to express
    the information by the sense of touch. It is one of the expected use cases of
    6G networks, where the network supports high data rate which is much greater than
    5G could support. The other scenarios of haptic communication include the brain-controlled
    computer interactions where people interact with their surroundings through haptics
    and control the environment through digital gadgets such as brain-embedded wireless
    chip that respond to human emotions [5], [105]. 4) Remote Surgeries and Telemedicine
    5G networks can provide ultra-low latency of nearly 1 ms to critical applications.
    However, remote surgeries are extremely sensitive, with a latency requirement
    of much less than 1 ms (nearly 0.1ms) [149]. The remote robotic surgeries will
    require ultra-high precision and reliability of the data, high data rate to exchange
    data, and control signals between two remote health care facilities through the
    mobile network. The emergence of 6G networks will be a game-changer when telemedicine
    and remote healthcare will be taken into account by diminishing the space and
    time constraints [31]. Since 6G’s vision is to provide at least 99.99999% data
    reliability and a 1 Tbps data rate, it will be the most suitable candidate to
    meet the above requirements. The main concern in the case of remote surgeries
    is the latency requirements. The 6G should target both minimum and maximum latency
    requirements, unlike the previous generation networks. That is because, during
    the remote surgery, some aspects of the transmitted data must arrive at the destination
    within a specified maximum latency, whereas some events should follow minimum
    arrival latency. These minimum and maximum latencies together will render coherent
    data at the receiver, which is the key requisite for remote surgery [31]. 5) Holographic
    Communication With the maturity of AR/VR applications, we will soon realize that
    the virtual experience is not serving us all the aspects of reality, and we need
    more. Recently, due to the outbreak of COVID’19 pandemics, virtual presence (telepresence)
    has gained high prominence over real physical meetings. This kind of task requires
    advanced VR techniques, bandwidth, and computations to project an object or a
    person in real-time remotely. As a future trend, each mobile phone will be equipped
    with more than five ultra- high definition cameras to capture an event and render
    multiple dimensional videos that give an immersive experience of the event being
    captured to all the human senses. In other words, a video may be a multi-dimensional
    real-time projection with the audio-visual effect of the person or object being
    telecasted in a virtual meeting. For instance, videos of 16 K resolution, 240
    Hz scanning rate, and spherical coverage (360°) need to be transmitted as a hologram
    for a fully immersive VR experience [46], [115]–[117]. To transmit these videos
    or holograms that require several Gbps data rates, the existing 5G’s KPIs such
    as data date, bandwidth, and reliability becomes insignificant [12], [39]. These
    transmissions involve a large volume of data. Besides, the angle of projection,
    response time are critical as well. For instance, if the audio or video response
    from the site of holographic projection has to be sent back to the source, then
    a precise and well-coordinated synchronization between the source and destination
    with respect to different layers of the image is essential. In another instance,
    consider a public concert where a remote artist can be rendered as a holographic
    presence (virtual presence) to entertain people from his local existence. Similarly,
    remote and hard to reach areas such as mines and deep ocean terminals will benefit
    from holographic applications where excavation activities and workforce training
    can be undertaken through holographic communication. 6) Tactile Internet In 6G
    networks, connectivity between various devices will be highly interactive in real-time
    (responsive), including the transfer of data, control, and feedback in real-time
    with a sense of touch [71]. Here one could transmit touch, feelings (sense), along
    with the information to give a live experience of the things virtually related
    to the information being communicated. To be specific, tactile internetworking
    involves a feeling of touch or taste along with an audio, video, or other forms
    of responses [9]. For instance, training the astronauts on space facilities, accessing
    underwater vessels/containers, and remote surgeries with the help of virtualized
    holographic models requires even a sense of touch to execute remote training and
    perform repairs with ultra-low latency. Furthermore, the transmission of smell
    and taste to enhance users’ experience will be a major target of food industry
    to digitize users’ experience of food access. As an example, while transmitting
    an advertisement of a particular type of food, its smell, texture, and taste shall
    be transmitted together with the help of advanced sensor technology and Apps to
    give a real feeling of the food. The 6G network will serve the need of these verticals
    due to its ultra-high data rate and low latency [18], [75], [88]. In summary,
    when we consider holographic tactile internet communication, different aspects
    of the hologram in the case of remote transmission may require varying amount
    of latencies (the minimum and maximum latency requirements), reliability, and
    user experience which need to meet by 6G networks to render a synchronized immersive
    effect to the human senses. SECTION III. Literature Review In this section, we
    will review the most recent literature that motivated our research. First, we
    categorize the publications based on the topic of research, and later, we present
    their contributions. Altogether, we have considered 158 recent articles, specifically
    on 6G networks and its components. For easy reference, we have listed them categorically
    in Table 2. 6G vision, technology, prospectus, and challenges: First, the vision
    statement of the 6G networks, as defined by all researchers, pointed out that
    6G networks will be a breakthrough in transforming the smart cellular network
    into an intelligent network. These studies have anticipated the emergence of 6G
    networks by 2030 and have listed several interesting technologies, applications,
    and use cases that will benefit from the 6G networks [1]–[5], [9], [19], [31].
    The technologies such as AI, Intelligent surface, holographic radio, blockchain,
    three-dimensional connectivity, cell-less architecture, quantum computing, and
    wireless power transfer will revolutionize future networks [19], [28], [31], [43].
    Furthermore, the authors elaborated on the targets and features necessary to meet
    the requirements of the use cases. In general, most of the technology components
    of 6G networks are in the infancy stage. Therefore, there will be several challenges,
    such as training the AI models, security issues, lack of architecture, signal
    modeling, and computational facilities [96]–[98]. In addition, the authors have
    proposed several KPIs for future networks, compared them with 5G networks, and
    listed several technology gaps in the inception of 6G networks [35], [39], [40],
    [107]. Next, we review photonics and VLC. We noticed that VLC and photonics will
    be the primary components of 6G’s terra hertz communication [10]. Consequently,
    authors in [12], [17] have drawn a roadmap to VLC for its inception in 6G networks.
    Another key aspect of 6G network will be the AI and edge intelligence. AI will
    be used by 6G networks at all levels starting from the PHY for channel selection,
    MAC for achieving power efficiency, and at the application level for context awareness
    [13], [42], [47]. Moreover, AI will spread across various network entities in
    6G ecosystem, such as the sensing, edge, and cloud devices, in a distributed way
    to manage the small data generated locally and big data to be processed centrally
    to minimize the latency to the minimal level [36], [60], [61], [72]. In addition,
    the authors at [13], [34], [42] have discussed various ML algorithms that would
    benefit from the 6G networks at the various operational levels of the network.
    During 2030 and later, 6G networks will open doors for several verticals and markets
    due to its performance metrics. These business models for selected verticals have
    been discussed in [16], [23], [24], [50]. Another technology component of 6G networks
    that will revolutionize wireless communication will be reconfigurable intelligent
    surfaces building using meta- materials or smart objects [29]. These surfaces
    will essentially direct the data toward the destination using their electromagnetic
    reflection property to achieve high-quality reception without any external power
    sources [59], [70]. Similarly, another category is machine type communication
    and ultra-dense IoT networks. The authors in [7] have exclusively discussed the
    machine type communication and its components for the 6G networks. The other category
    in our review is an interesting technology component, i.e., quantum communication
    (QC). In [71], the authors detailed a state of the art work on QC for 6G networks,
    including the scope of machine learning in QC. Furthermore, we have discussed,
    all the architectures and their key features, the issues involved with the digital
    divide, and rural connectivity, the scope of business models and THz communication,
    cell-less architecture, ecosystem and its components, etc. Please see Table 2,
    for a detailed discussion on the various literature on different categories of
    6G technology, and their contributions. SECTION IV. 6G Enabling Technologies In
    this section, we will discuss the prime features, their types, need, challenges,
    etc., of various technologies that enable 6G communication. In Fig. 4, we have
    depicted these technologies. We reckon that 6G will integrate three aspects namely
    physical, biological, and digital world as shown in Fig. 4. This intuition shall
    be elaborated as in the case of 6G networks along with the typical radio frequency
    communication, it will include robots, digital twins, artificial intelligence,
    emotion-driven devices, smart communicating surfaces, communication through brain
    implanted chips or brain-machine interface [136] to enable all-round cyber-physical-biological
    communication experience. Consequently, 6G will be much more than the present
    smart connected networks, where the network components will largely integrate
    intelligence to bring in a paradigm shift from smart to intelligent network as
    shown in Fig. 4. Further, we have discussed eight enabling technologies for 6G
    namely, artificial intelligence, THz communication, 3D connectivity, Visible light
    communication, blockchain, quantum communication, intelligent surfaces, and digital
    twins. FIGURE 4. Technology enablers of 6G. Show All A. Artificial Intelligence
    Why AI for 6G: In 5G networks, the network orchestration functions lead to a flexible
    network slicing feature. Consequently, the specific requirements of most of the
    verticals could be met without depending on AI to a greater extent. However, we
    address the applications that are beyond the service limits of 5G networks in
    6G networks. We reckon that due to the following reasons, AI is essential for
    the 6G networks [60]–[61]. ( i ) We anticipate that in 6G networks, there will
    be a plethora of heterogeneous network components that interconnect via multiple
    numerologies (3D connectivity) to serve diversified verticals, process a large
    amount of data, and they demand varying levels of QoS. Addressing these tasks
    require efficient analysis, optimization, and decision skills. Therefore, at every
    level of the communication system, say user terminal nodes to edge processor,
    and core network, intelligence has been deeply embedded and integrated to offer
    end-to-end services. In addition, in such contexts, relying on existing architecture
    or time-consuming mathematical models for optimization of performance metrics
    may not lead to feasible solutions [7], [56]. Moreover, these user applications
    give the idea that the 6G network is much complex in terms of network structure
    and dynamics than the earlier generations. Consequently, it requires assistance
    from smart, adaptive, and intelligent AI agents to self-learn from the network
    inputs to adjust the offered services with dynamism and optimize. Generally, this
    scenario suggests that 6G architecture must target automation through AI. Precisely,
    those tasks such as resource allocation, reaching the targeted KPIs, mobility
    management and handover, policy and billing, services for various verticals, orchestration,
    and quality of service ought to be AI-driven by considering the volume, the heterogeneity
    of data, and its analysis to improve the performance [2]. Thus, making the network
    self-operative, manageable, and self-sustained under any network conditions [36],
    [61]. (ii) Similarly, at the physical layer, sensing and detection from sensors
    require the spectrum sensing to address spectrum scarcity problem, whereas interference
    detection requires a dynamic and massive volume of data collection. In this context,
    AI techniques such as Support Vector Machine (SVM) for real-time spectrum sensing
    and Convolutional Neural Network (CNN) for cooperative sensing would be proved
    to be effective solutions [36]. In addition, when we consider physical layer modeling
    in the presence of channel non-linearity, AI methods perform better than simple
    mathematical models, which may have unacceptable time complexity. For instance,
    Deep Neural Network (DNN) and CNN-based supervised algorithms have been proved
    to better than traditional methods [61]. 1) Current Trends in AI There are different
    types of AI systems (say ML) for wireless networking, which seems promising for
    6G networks, namely, supervised, unsupervised, and reinforcement learning. Currently,
    a large body of research focuses on the use of AI and its derivatives, such as
    deep learning and machine learning to optimize the wireless networks’ performance
    of [13], [34], [72]. AI will be pivotal in Radio Access Network (RAN) and the
    virtualized core network of 6G networks, including edge computing, resource allocation,
    slicing, control, and applications. However, how exactly AI would support the
    core 6G networks has to be realized in the future as a finalized architecture
    for 6G networks is yet to be devised. Tariq et al. [3] has opined that AI will
    be pervasive in distributed 6G architecture, where various network components
    would adopt federated learning. AI will be operating in distributed training agents
    at different network entities that will support individuals and the overall network’s
    benefit by a collective operation. The AI scenarios, specifically deep learning
    algorithms, will suit the analysis of big data generated by several IoT nodes
    when processed centrally in the network. In addition, to enhance network performance,
    the wearable sensors will incorporate the computing and data aggregation features
    locally along with sensing and communication. That means 6G networks necessitate
    another context, where data processing is done locally at the edge/fog devices
    in a distributed way for delay-sensitive applications, which could be done using
    reinforcement techniques [5], [42]. The authors in [60] have endorsed that locally
    generated data by various sub-networks of the 6G network might be processed locally
    using AI techniques to attain efficiency in terms of reduced latency and overhead.
    Furthermore, AI could be integrated into the radio access network to infuse cognitive
    capabilities for efficient channel selection from the physical channels such as
    THz band, visible light communication, Wi-Fi, satellite link, transmission power,
    and modulation scheme selections. AI at the application layer can manage various
    smart processes such as automated driving, smart healthcare, and improved performance
    by suitable data learning methods. On the same note, the study in [4] has identified
    three categories of AI systems for 6G networks. (i) AI at the network edge supports
    low latency applications with real-time data processing capability instead of
    a cloud-based centralized AI system. (ii) AI-enabled radio that decouples physical
    hardware (i.e., transceiver) from the software with control functions to enable
    AI-driven dynamic and intelligent decision making using the data received from
    the transceiver. For instance, interference avoidance, cell selection, channel
    estimation, etc. (iii) Distributed AI, where each network entity will be capable
    of running the AI algorithms in parallel, using the local data in a distributed
    manner before being sent to the centralized cloud [43], [102]. 2) AI Architectures
    6G In the literature, there exist two AI models for 6G network architecture. We
    will review both of them briefly in the following paragraph. AI Model 1: Let us
    consider how AI finds its place at different levels of a network. An AI-based
    layered architecture for 6G networks, as defined in [36], has the following realizations.
    Layer 1: This first layer occurs at the RAN, where several sensors such as robots,
    cameras, mobile devices, and connected vehicles sense real-time data and communicate
    to the Access Point (AP). In this context, efficient environmental data collection,
    intelligent channel estimation and interference management (including the scenario
    of cognitive radio), ultra-low latency, and high reliability shall be possible
    by enabling AI techniques in these devices. Otherwise, data collection in a rapidly
    varying environment and channel selection during multiple devices’ simultaneous
    operation will pose challenges. For efficient real-time sensing, SVM, DNN, and
    CNN methods are recommended. Layer 2: Next, as the data arrives at the Access
    Point (AP) level from layer 1, the massive data collected from the sensors require
    analysis (temporal or special), processing (to reduce the redundancy), computing,
    and storage. In particular, data analysis and mining filter the redundant content
    to tailor the data set to a reasonable quantity. At this point, well-known AI
    techniques such as Principal Component Analysis (PCA) and Isometric Mapping (ISOMAP)
    can now be employed to compress the raw data. With intelligent data analytics,
    critical network data patterns can be studied to improve the overall performance.
    Layer 3: In the third level, data from the AP, or e-NB is sent to cloud computing
    and storage. These clouds or edge computing facilities will be NFV and SDN controlled
    virtualized network entities. At this level, the controllers equipped with intelligent
    agents will make decisions regarding learning and decision making by the knowledge
    obtained from layers below it. It includes resource allocation functions, power
    control by considering the network slicing requirements of various applications.
    Further, AI agents control handover and optimize the physical layer parameters
    such as deciding the precoding matrix, rank index in massive MIMO transmissions,
    and edge computing by learning from the knowledge obtained from the previous lower
    levels to meet the high-quality service requirements. The main AI methods for
    efficient decision making regarding optimal parameter selection in massive MIMO
    or smart reflecting surface (polarization, precoding matrix, phase, and rank indicator,
    etc.,) in 6G network can be performed through the Markov decision process, reinforcement
    learning methods. Layer 4: Finally, at the application level, AI will assist in
    delivering specific services based on the needs of each application or verticals.
    For instance, data service for the 6G eMBB user and ultra-low latency for robotic-assisted
    smart healthcare can use AI techniques to automate the network functionality,
    self-management, and self-organizing features [36]. AI Model II: Now, let us see
    the O-RAN Alliance, which is an Open RAN alliance formed by five global telecom
    operators for B5G networks, though not specifically for 6G [61]. Albeit, it provides
    a framework for integrating AI into the future cellular network. We summarize
    the key points as follows. Here RAN is coupled with AI features to handle non-real-time
    (latency less than 1 s) and near-real-time (latency 10 ms–1 s), and real-time
    services. This architecture has a multi-radio protocol stack to support heterogeneous
    radio interfaces. i) At the physical layer, deploying AI will enhance channel
    estimation for MIMO, receiver symbol detection without much regard to channel
    state information (CSI), and channel decoding. As discussed earlier, AI techniques
    such as learning-based DNN for channel estimation, and detection will prove fruitful.
    ii) Further, at the MAC layer, spectrum access in the outset of multiple radio
    interfaces will be the key requirement. However, employing spectrum access schemes
    for individual access technologies (as in existing practice) will prove inefficient
    as 6G networks will have heterogeneous access technologies. Therefore, to facilitate
    distributed channel access for multiple radio access technologies, learning based
    dynamic spectrum access technologies, such as deep reinforcement learning-based
    distributed spectrum access method shall be adopted [61]. This architecture defines
    two interfaces, namely, A1 and E1 to support non-real-time and near-real-time
    access layer, respectively. iii) Similarly, at the network layer in 6G network,
    the resource allocation and management tasks are herculean task by traditional
    methods due to the high density of connected devices. Therefore, it is recommended
    to use methods such as AI-driven trouble shooting, knowledge data for fault recovery,
    and root cause fault analysis by an AI model. Second, the management of virtualized
    network functions at different virtual networks, tuning the MIMO parameters for
    setting the beam width during mobility can be achieved with reinforcement learning
    algorithms [61]. Additionally, the authors in [60], have proposed an intelligent
    radio for 6G networks that enables the future upgrade of radio receiver, introducing
    an operating system between transceiver hardware and its software. The operating
    system will be capable of reconfiguring its algorithm to improve the performance
    by sensing the transceiver parameters and other inputs with the help of an interface.
    The DNN-based implementation of the intelligent radio will be low cost, and flexible.
    B. THz Communication Spectrum The 6G communication will span across microwave
    (300 MHz–300 GHz), infrared (300 GHz–400 THz), and visible light (400–800 THz)
    frequencies, ranging from long distance to short distance coverage, while coexisting
    with the previous generations. However, 300 GHz–3 THz expanding up to 10 THz which
    is popularly known as Terahertz (THz) spectrum is a new frequency band for cellular
    communication, which 6G networks will exclusively target to offer extremely high
    data rates [4], [75]. 1) Need of THz Spectrum Spectrum is the fundamental and
    sparsely available resource for wireless communication. From the discussion of
    6G’s plausible use cases, and expected KPIs, it directly boils down to the point
    of bandwidth and spectrum resources to support these data applications. The 5G
    network will explore sub-GHz, and frequencies up to 30–300 GHz, (with a maximum
    bandwidth of 400 MHz) to offer millimeter wave communications. However, the offered
    bandwidth may not suffice to meet the services that have dual requirements like
    ultra-low latency combined with high data rate or vice versa, because that strains
    the bandwidth [10], [57]. Anyway, 6G network being the successor of 5G network,
    will continue to use the microwave, mm-wave frequencies along with the future
    THz frequency bands, where tens of GHz chunk of frequency (spectrum) are available
    [9]. Recently, along the same lines, 6G research has been triggered by the US
    Federal Communications Commission (FCC) by announcing an experimental license
    for 95 GHz to 3 THz spectra, along with granting 21.2 GHz of spectrum for unlicensed
    communication [24]. This major step will motivate many researchers to explore
    THz band in-depth. The need for a new spectrum arose as the lower GHz band will
    be much occupied (5G family). As a result, providing a large chunk of frequencies
    to next-generation networks to support high data rate will not be feasible. Therefore,
    ITU-R has foreseen the need for an exclusive spectrum for B5G networks and has
    recommended 275 GHz – 3 THz frequencies that include the microwave and optical
    frequency bands. This Terahertz frequency band will offer a nearly 60 GHz unoccupied
    spectrum, sufficient to provide a Tbps data rate [3]. 2) Opportunities THz communication
    offers several opportunities due to its unique properties. As a result, 6G networks
    will prefer it for short-range high-speed communication and space communication
    between satellite and ground stations. Meanwhile, the availability of wider bandwidth
    in THz frequencies makes the high data rate feasible. Next, high-frequency communication
    leads to miniaturization of the circuit (device). As a result, multiple antennas,
    an array of antennas can be accommodated into the user devices. This facilitates
    the use of efficient multi-antenna MIMO schemes, beamforming, and interference
    suppression to enhance the quality of communication [107]. Furthermore, these
    high-frequency signals will follow the line of sight (LoS), and are highly directional,
    resulting in less susceptible to jamming and are secure. In addition, the frequencies
    above 100 GHz are least affected by rain/moisture. The THz range will give rise
    to extremely tiny cells (say 10’s of meters) to support a high data rate mobile
    broadband communication in local space for special applications in the future.
    Short wavelength and pulses with well resolute time make it an ideal candidate
    for super-precise positioning [24]. With regard to the space communication, certain
    wavelengths like 600–870 micrometer are well suited for long-range communication
    at low noise and low power [111], [120], [122]. 3) Challenges THz waves will significantly
    affect large-scale fading and shadowing. It has been reported that signals below
    100 GHz face large attenuation due to moisture in the air [120125]. Further, it
    has a relatively higher free space loss compared to lower frequencies. At 1 THz,
    the radio signal will experience high absorption from water molecules, oxygen
    in the air [28]. In addition, shadowing will immensely influence propagation.
    A study says the human body may attenuate the signal by 20–35 dB [9]. Another
    challenge will be at high frequencies design and fabrication of transceivers for
    mobile devices is extremely complicated [127]. Besides, the power required to
    handle the high frequency processing such as Analog to Digital Conversion (ADC)
    for sampling is yet to be known [28], [132]. There is currently a lack of study
    on the channel characterization of THz frequency signals, the support for inter-frequency
    mobility, handover and the physical layer protocols [5]. C. Ubiquitous 3D Communication
    One of the significant requirements of the 6G networks, as different from the
    previous generations is to have all round global connectivity such as high altitude,
    underwater, and terrestrial connectivity to adeptly accommodate a wide range of
    verticals. With this vision, 6G networks target to achieve extended and continuous
    communication between humans and smart things, machine-machine such as underwater
    vehicles, UAVs, or spacecraft, and robots [75], [121]. In 2030, communication
    between terrestrial, aircraft and satellites, and ships will become more obvious
    [9]. Let us see the scenario more in detail. Refer Fig. 5 where we have pictorially
    represented the scenario. Concerning the network densification, 6G networks is
    anticipated to see an un-presidential increase in the number of user terminals,
    where users will be capable of networking with other terminals either in the same
    level (terrestrial-terrestrial) or at different levels (terrestrial-aerial, terrestrial-underwater,
    or aerial-underwater) via multiple radio access networks. Furthermore, it will
    be a common scenario where a user will have network connectivity with multiple
    cells with resources being shared among those cells by mutual coordination. As
    a result, the concept of a cell will diminish i.e., the cell-less architecture
    will emerge to have infinite freedom with multiple RAN connectivity feature to
    endorse all round connectivity [134]. The emergence of cell-free in 6G networks
    will bring in the need of heterogeneous technologies such as satellite, UAV, submarine,
    and deep-sea connectivity to coexist and integrate with terrestrial THz RAN to
    enhance the data rate, coverage and seamless, flexible connectivity aspects. The
    proposed feature shall address the issues of present networks such as delayed
    handovers, mobility, data loss, etc., to offer better quality of experience to
    the users [31], [142], [124]. In the following paragraph, we will visit the various
    components that constitutes 3D networking. FIGURE 5. 3D communication scenario
    in 6G. Show All 1) Space Communication The three main patterns of space communication
    consist of ( i ) communication between the earth satellite transponders and low
    earth orbit (LEO) satellites, (ii) 6G cellular base station, cellular users, and
    UAVs, and (iii) UAVs, aeroplanes, and 6G base stations. It is a well-known fact
    that the existing communication satellites mainly belong to the Geo Stationary
    Orbits (GEO) that impose large delay for terrestrial mobile communications. Consequently,
    theoretical studies in the literature have envisioned mobile broadband amalgamation
    with LEO satellites [121]. It has been reported that several countries like US,
    UK, and China have planned their satellite missions. For instance, China has planned
    to launch nine such LEO satellites (Hongyan) by 2025, into its cluster of 320
    satellites [4]. These satellites may provide direct coverage to the terrestrial
    station (transponders) or to ships. This is the first pattern of space communication.
    These LEO satellites will offer extended coverage, high-speed data, low latency
    communication support to terrestrial communication in conjunction with GEO satellites
    that acts as backbone network [22], [149]. 2) UAV Based Communication Recently,
    UAV’s have been extensively used as relay nodes to extend cellular coverage. At
    a little lower altitude than satellites, UAVs functions as either mobile base
    stations while being afloat in space to cover a large footprint of terrestrial
    mobile users or act as relays (hotspots) to extend the coverage of base stations
    where the mobile infrastructure is hard to setup and thereby serve a greater number
    of users [18]. One of the foreseen applications of UAV communication in 6G networks
    will be to provide rural connectivity at a reduced cost [76], [151]. 3) Features
    of UAV Communication Easy deployment and network setup are critical features of
    UAV to enable flexible range extension or coverage enhancement of wireless communication.
    For instance, during a calamity, hurricane, tsunami, desert, where the existing
    infrastructure has damaged or does not exist, the UAV may function as a floating
    base station. However, there are challenges when 2D communication has to be extended
    to 3D due to the altitude and added degree of freedom [31]. The aerial channel
    modeling study should attain maturity before UAV can be integrated as a part of
    6G [9], [113]. Further, optimization of route, power, number of UAV’s to provide
    coverage requires special attention as these devices have power constraints [76].
    4) Underwater Communication Let us consider a scenario of deep-sea exploration,
    for instance, the study of underwater habitats, deep water ecosystems, natural
    resources, or recreational tour, and military communication; all these activities
    involve the transmission of video, data to a ground station. Furthermore, the
    communication during any rescue operation through a submarine or UAV, far from
    the coast are challenging due to an unpredictable underwater environment, where
    RF, visible light, and acoustic signals should provide communication. Later, it
    has to be integrated into the terrestrial or at times to space communication [112].
    Incorporating this challenge as a part of 6G network with different frequencies
    to communicate for underwater (low frequency) and terrestrial will meet the key
    requirement of the future networks. The acoustic communication mainly spans between
    acoustic waves, RF, and optical wireless frequencies; a thorough channel modeling
    for all three cases is an intricate task [127], [9]. In general, to provide 3D
    coverage, 6G network will virtualize its hardware access (i.e., physical and medium-access)
    to enable the network service for more users and verticals. D. Visible Light Communication
    In earlier telecom networks, connectivity between cell tower and mobile switching
    centers were either microwave (RF) or fiber optic links (non-RF). However, since
    4G onwards, free-space optical (FSO) links have started to attain prominence to
    connect the backhaul due to its simplicity, license-free operation, high data
    rates, and security. Due to these features, VLC will be on the main contender
    for backhaul networking in 6G network, along with RF links [2]. In dense 6G networks,
    a very high bandwidth technology will be mandatory at the backhaul to manage the
    data due to the massive amount of data generation. In this regard, the VLC and
    THz communications will be the two main candidates for 6G cellular communication
    at the physical layer [31]. The VLC as a complementary technology to RF occupies
    the higher scale of THz frequency ranging from 400 THz–800 THz. Note that VLC
    offers hundreds of THz bandwidth (unlicensed), meanwhile, the THz frequency communication
    has only up to a hundred THz bandwidth (licensed) available [4]. Even though 5G
    networks did not consider VLC in its ecosystem, VLC deployments for short-range
    communications offer several Gbps data rate [9]. Thus, in the next decade when
    LED technologies mature, especially with the advent of micro-LED sources, and
    laser diodes, 6G’s data rate requirements can be met. VLC as a family of FSO has
    several merits over RF communication. First, the optical spectrum is license free.
    Second, the VLC in the frequency range 430–790 THz uses white light LED as a source
    to transmit the encoded data. The data encoding is much simplified in VLC, where
    by modulating the brightness of the light emitted by LED, different encoding levels
    can be achieved easily. Third, it is anticipated that VLC link will offer several
    THz bandwidths and up 100 Gbps data rate, and may grow to Tbps in future [4].
    As a result, the use of multi transmission links can offer enough data rate for
    6G use cases [3]. In particular, laser diode which has higher efficiency and illumination
    than LEDs, can offer a 100 Gbps of data rate for critical applications of the
    6G networks [28]. In general, even indoor communications remain secure within
    a confined space as light sources cannot pass through the obstacles [17]. VLC
    will be the most suitable candidate for the line of sight inter-UAV communication
    in 6G networks. Since the communication system is simple and flexible, two or
    more aerial vehicles can communicate through optical frequencies generated by
    the LED sources without being distracted by the obstacles above the ground level.
    In addition, the interference issues in VLC are also minimal. As a result, underwater
    communications between submarines, personal healthcare devices, or industries
    susceptible to electromagnetic radiation are the other scenarios that will find
    VLC the most suitable [9]. Nevertheless, VLC is challenging in the outdoor setting,
    as external light (ex: sunlight) will influence the communication beam. So, aerial
    communication requires a very strong light beam to overcome this limitation. Thus,
    VLC is more suitable for indoor short-range communication without this constraint.
    E. Blockchain Security Blockchain is a distributed ledger that will play a pivot
    role in the maintenance of data security and transparency when numerous devices
    share the data in a decentralized way in B5G networks. 1) Why Blockchain for 6G
    We know that 6G networks will have a massive network of IoT and MTC devices that
    connect homes, cities, and factories with several data transactions between the
    networked entities. Moreover, a survey predicts that there will be 50 billion
    connected devices IoE during the 6G network era [3]. In that context, there should
    have trust among the devices to facilitate secure data transfer. To build such
    trust and security, the blockchain allows maintaining a sequential ledger (chain
    of blocks) by each node wherein if any user does the modification of data within
    a block, it will be visible to and shall be authenticated by everyone else to
    prevent falsification. 2) Features of Blockchain These modifications are imprinted
    into the data block with a private hash key, and data remains unaltered without
    the consent of the other devices [2]. That means, when a data block is created,
    it must be verified with respect to the previous blocks by the other participating
    users to maintain transparency. In this way, the reliability factor is introduced
    among all users; however, with the compromise of privacy, as modifications will
    be visible to all [14]. The decentralized operation of blockchain brings flexibility
    with reduced management cost. The blockchain even offers several advantages in
    a highly connected mesh network, such as security, reliability, trust, and scalability
    [2]. One of such instances of blockchain in 6G networks could be a video conference
    call, or mixed reality-based video streaming, where the network along with providing
    connectivity, also requires all the parties to authenticate themselves and the
    data being communicated through dedicated blockchains. This will bring in the
    knowledge of data properties to analyze the abnormal behaviors in real-time [42].
    However, there are other challenges in 6G networks besides the massive device
    connections for blockchain implementation, such as resource restrictions on the
    devices limit the scope of use of cryptographic security algorithms on the device.
    Similarly, data packets must undergo an intensive audit to evaluate the risk,
    which becomes a herculean task when the number of devices is very big. Due to
    the lack of third party verification, security attacks are more accessible by
    compromising half of the total participants. The authors in [14] have suggested
    using cryptographic and quantum computing based algorithms for privacy issues
    in blockchain. Altogether, the blockchain has a major role contributing to security,
    scalability, and reliability factors in the 6G network [30]. In the future, for
    6G, blockchain shall provide security to mobile edge computing nodes when several
    devices wish to store their data in the edge device. Similarly, in case of device-to-device
    communication, the cooperative data caching among the users also shall use distributed
    security. In addition, when we consider network virtualization, it is obvious
    that the network capacity will enhance due to provisioning more slices. In this
    regard, while implementing blockchain, it will provide authenticity of data in
    every slices along with immutability features to efficiently manage the virtual
    network [33], [14]. F. Quantum Communication (QC) It is one of the enablers of
    the 6G network; especially, it will support 6G network in achieving ( i ) extremely
    high data rate requirements at the backhaul, (ii) data security, and (iii) long-range
    transmissions [3]–[5]. In general, the existing protocols of mobile networks can
    be significantly enhanced by utilizing the principles of quantum theories to attain
    higher degrees of freedom [71], [18]. Foreseen by the role of QC in the 6G and
    future networks, the Government of UK, and New York University have invested heavily
    on the research of quantum communication and computing [78]. In the case of QC,
    data that encoded using photons which cannot be decoded or copied (cloned) without
    tampering as quantum particles (photons) will be highly intertwined and correlated.
    In addition, data will be represented as ‘qubits’ a unique notion of multi-level
    description of data, where qubit is the fundamental unit of quantum data [4].
    Therefore, the underlying principle of QC, includes quantum superposition, quantum
    entanglement (no-cloning) theorem [71]. However, when we move our attention toward
    data rate improvement in QC, we use the concept of a quantum superposition of
    qubits. Basically, it does the parallel processing of multi-dimensional large-sized
    data to offer seamless data rate, unlike the binary data processing in conventional
    computing systems. For instance, a qubit can represent binary ‘0’ and ‘1’ simultaneously.
    In addition, n bits can represent 2 n bit patterns at the same time, instead of
    anyone combination of n bits as in digital communication. In the case of QC, communication
    security is another key feature to mention by randomization. The communication
    is impossible for eavesdropping as the data will be represented in an encoded
    quantum state using photons and cannot be meddled without the intertwined patterns.
    Here, QC makes use of quantum entanglement, or the no-cloning principle [3]. One
    of the future applications of QC in security is the quantum key distribution (QKD).
    This application provides vast security to physical layer due to quantum mechanics
    based key distribution in future networks [19]. Furthermore, long range communication
    is supported with the aid of quantum repeaters (relay) that retransmits the photons.
    It has been reported that QC can be integrated with satellite communication to
    offer global coverage [4]. Meanwhile, quantum repeaters and switches are difficult
    to build due to no-cloning theorem [71]. An interesting paradigm for the future
    networks will be combining QC with ML to solve classification and regression problems
    more efficiently than the classical ML solutions, which is termed quantum machine
    learning [40]. Here, quantum supervised and unsupervised learning, quantum deep
    learning, and quantum reinforcement learning techniques would satisfy the complex
    computation requirements of 6G networks [71]. All these features make QC a suitable
    candidate for 6G networks. G. Ultra-Massive MIMO and Intelligent Communication
    Surfaces Due to the availability of large bandwidth in THz frequency, by the use
    of MIMO transmission technique, large spectral efficiency can be achieved. MIMO
    technique which exploits the spatial diversity of the communication medium several
    antennas at the transmitter and/or the receiver, and offers enhanced spectral
    efficiency and gain with the given channel bandwidth [9]. For instance, in a 64-beam
    array plane, there are 64 unique predetermined angular directions. During transmission
    to a user, a single angular beam will be chosen to expect each user to be well
    apart from the rest and should have a line of sight path [48]. This limitation
    of 5G massive MIMO should be overcome in 6G networks, where each antenna array
    creates a beam in multiple directional vectors with each beam having different
    angular directivity [48]. The massive MIMO transmission will enable the receiver
    to overcome the degradation effect on the THz waves caused due to rain and atmosphere
    in dense urban settings [9]. However, looking at challenges, such as energy efficiency
    of massive MIMO and inter-cell interference, efficient solutions are yet to be
    determined [28], [40]. In the context of 6G networks, it is more appropriate to
    use the term ultra-massive MIMO. To fill the gap concerning the physical layer,
    the combination of THz and several antenna arrays (massive MIMO) shall be used
    in 6G networks to support user’s mirage of data hungry applications. One of such
    use cases will be beamforming, where multiple antennas are pointed toward a user
    with the help of directional beams for complete utilization of spatial dimensions.
    Therefore, 6G networks must target physically large panels accommodating more
    antennas, and narrow the beam width (increased spatial resolution) to increase
    the beamforming gain. The efficiency of beamforming can be achieved using continuous
    aperture antennas where each antenna is discretely spaced. This increases the
    cost and power consumption. In this context, holography radio is a solution that
    uses meta-materials on which antennas can be densely spaced on a small area [48].
    In 6G, we will overcome the limitations of 5G networks by using large antenna
    reflecting surfaces and holographic communication, where holographic beamforming
    with the help of Smart antennas renders holographic 3D videos [28]. Unlike the
    5G massive MIMO, 6G networks would use smart intelligent surfaces (IRS) which
    reflect the data received from a base station toward the receiver [59]. 1) Intelligent
    Communication Smart Surfaces The existing massive MIMO technologies for 5G networks
    provide spatially, efficient ways of enhancing communication performance. However,
    the complex signal processing, increased power consumption, hardware design, and
    THz wave’s propagation properties (ex: blockage due to obstacles) pose a real
    challenge. Furthermore, at THz frequencies, due to low scattering very few scattering
    paths exist. Thus, using antenna arrays becomes a challenge due to the shrinking
    size of the antenna elements. Therefore, an alternative to ultra-massive MIMO
    is essential. As we know, the hardware components will be driven by intelligence
    to adapt to the changes in the surrounding environment in the 6G network. For
    instance, the performance of the cognitive spectrum access, modulation, and coding,
    beamforming can be modified as the environment changes [70]. One such intelligent
    component that enables wireless communication is an Intelligent surface. The intelligent
    surface is also known as Reconfigurable Intelligent Surface (RIS). An intelligent
    surface will act as a boon to enhance overall communication between transmitter
    and receiver in a cost effective and energy-efficient manner [28]. An intelligent
    surface is a passive reflective surface of electromagnetic signals without requiring
    a dedicated power source. These are generally made of reflective arrays, liquid
    crystals, or software defined meta-surfaces. These surfaces will manipulate and
    reflect the incident RF signal from different sources and direct them toward the
    receiver to assist in wireless communication [40], [48]. The modification of the
    incident RF signals is done by programming these meta-material surfaces, similar
    to a software defined radio. Therefore, the RIS is also denoted as software-defined
    surface (SDS) [59], [119]. 2) Features of Intelligent Surfaces Since these surfaces
    are passive and do not use power amplifiers or analog for the digital converter,
    they prove to be power efficient than massive MIMO technology as the latter uses
    active elements to re-transmit the signal. Further, the amount of radio wave reflection
    depends on the surfaces area of these intelligent surface. Another key property
    of the intelligent surface is that they intelligently reconfigure the modulation
    or phase of the reflected signal with respect to the channel condition such as
    fading and path loss. Further, the intelligent surface can aggrandize beamforming
    of massive MIMO in the 6G networks due to the software programmable feature [28].
    With the help of reflecting surface elements, it is easy to mimic the effect of
    beamforming at a reduced complexity. Studies have investigated the potential of
    intelligent reflecting surface for index modulation for 6G networks [59]. 3) Holographic
    MIMO Holographic MIMO is an interesting paradigm of intelligent surface enabled
    communication [70]. Here, a large intelligent surface can be configured to act
    either as transmitter, or as receiver using the principle of optical holography.
    When there is a power source used for the operation, i.e., in the active mode
    of operation it is explicitly termed as H-MIMO surface. However, in passive mode
    (no power source) it is majorly a reflecting surface that modifies and reflects
    the incident RF signal as a intelligent reflecting surface [70], [43]. Now, let
    us consider the challenges associated with the Intelligent surfaces: ( i ) In
    a wireless communication system assisted by RIS, the surface should acquire enough
    CSI feedback from the transmitter and the receiver. However, acquiring the CSI
    is a burden for passive devices which cannot transmit the pilot signals for channel
    estimation, as in the conventional systems. (ii) The passive surface needs to
    transfer information, such as control signals to synchronize with the transceiver
    and real-time environmental parameters. (iii) Resource allocation issues such
    as lack of analytical models and computation cost [29]. These H-MIMO surfaces
    have a benefit of reusability, easy customization, and lower latency due to software
    programmability of the meta materials. In passive modes the thermal noise is absent.
    It is also spectrum efficient. Furthermore, intelligent surfaces will act as range
    extenders when there is no direct link from the transmitter (gNB) to the user.
    Due to simplified beamforming, the RF signals can be focused to the desired location
    that reduces the change of eavesdropping, loss of signal due to attenuation, and
    facilitates wireless power transfer along with the information to the intended
    receiver [70]. In 6G network, RIS will find its applications apart from physical
    layer communication, as RIS enabled Edge computing, wireless power transfer, device-to-device
    communication, and positioning and localization [2991]. H. Digital Twins A digital
    twin is a digital representation of a real world object, person, place, or event
    projected virtually in a cyber-physical world, without regard to time or space
    restrictions [90]. It is expected that with the digital twins, we will experience
    the reality (i.e., of the original object) virtually (as a twin) [86]. The advancements
    in sensor technology, AI, and communication has led to several applications of
    digital twins. To render the twin of an object digitally at the remote site end,
    the system must process a high definition data of the original object, analyse,
    and decode to reproduce it virtually. In this context, AI’s supervised or unsupervised
    algorithms will be much suitable as they can accurately analyse the data from
    the surroundings. Digital twins have several use cases, as mentioned in [89],
    [90]. For instance, in Industry 4.0, a digital twin of a machine will help the
    operator to predict future failures or malfunctions much earlier by analyzing
    the twin data as there will be continuous feedback between the original machine
    and the twin for better performance analysis. A similar intension has been exploited
    by the automotive giant Tesla company in their digital twin of the cars. Another
    foreseen application will be for e-healthcare. With the aid of the digital twin
    of a human, it will be easy to analyze the body parameters in real-time or shall
    study the impact of a drug, pre-surgical study of the area to be operated [89].
    Samsung has expressed that digital twins will be one of the technology components
    that will drive 6G networks [86]. In [15], authors have proposed the digital twin
    model for the 6G network architecture. Since 6G networks will have extremely high
    network density, the network will have distributed its architecture for better
    management. These digital twins (a.k.a. cyber twin) can become part of the network
    by acting as assistant, data logger, and digital asset owner of the objects. In
    this scenario, the 6G’s network architecture shall consist of four entities, namely,
    (i) end entities (human, things) who receive the service, (ii) edge to cloud entity
    that connects the end-user to the core cloud through the edge devices, and does
    initial edge processing. (iii) Cyber twins located at the edge server which are
    the replica of an end-user that provides services such as communication, data
    logging, and even act as the owner of assets. It will replace the end-to-end communication
    model with an end-to-cloud model. (iv) The cloud network where edge clouds and
    different other clouds will be interconnected. This architecture will provide
    scalability, security, and flexibility to future generation networks. 1) Challenges
    To replicate an object or human in real-time requires extremely high data rates
    and perfect synchronization (say 1Tbps and 100 ms). Besides, there are issues
    such as privacy, security, ethical issues, and cost of development. The technical
    community is hopeful that by 2030, digital twins will very well support 6G networks
    when there will be more advancements in technology [90]. I. Challenges of 6G Enablers
    Even though 6G enablers will enhance the performance of 6G networks several times
    more than the 5G networks, the path to achieve the gain is arduous. In this subsection,
    we describe the various challenges associated with the 6G network technology components,
    as given in Table 3. TABLE 3 Technological Challenges of 6G SECTION V. Architectures
    for 6G While roadmap, and standardization activities are yet to be initiated,
    a few researchers have proposed new architectures of 6G networks. In this subsection,
    we describe briefly various architectures from the literature. In the earlier
    section (IV), we narrated the critical features of two AI-based 6G architectures.
    Now, let us discuss the other architectures. A. Cyber-Twin Architecture The authors
    proposed two architectures for 6G networks, namely, (i) cloud-centric internet
    model and (ii) a decoupled RAN model [104]. In the cloud-centric internet model,
    the existing IP architecture is slightly retained with certain modifications.
    First, the users are connected to the RAN, and the data from the RAN enters the
    edge cloud layer where cyber-twins accept the data. These cyber-twins act as data
    loggers, assent owners, or virtual representation of the user. Edge clouds, in
    turn, are connected to the cloud layer, where multiple clouds are interconnected
    to form the center of the network architecture. The cloud layer consists of resource
    scheduler, orchestrator, communication, computing, and caching functions. This
    cloud layer enables applications to provide services in the edge and cloud at
    reasonable cost and QoS. B. Decoupled RAN Architecture Here are distinct APs for
    handling the control and user plane data different from the earlier cellular generations.
    The control plane BS will be a macro BS, which a user shall connect to exchange
    the control information. These control BSs will be connected to the user plane
    (data) BSs for high-level signaling. In addition, the uplink and downlink traffic
    are also decoupled and handled by separate BSs to the users. Here, the uplink
    BS can be a micro-cell dense deployment near the user to collect the user data
    at low power. The uplink and downlink base stations will have internal coordination
    for efficient communication [104], [105]. C. Generalized Architecture for 6G A
    generalized 6G architecture for IoT and vehicular networks is presented in [139].
    It consists of three levels, namely, user level composed of smart devices with
    caching ability. These devices send the sensed data to the base station level,
    where base stations or APs have edge servers to perform scheduling and resource
    allocation. Finally, a central server will do the slicing, handover actions at
    the network level. This architecture will reduce the delay for critical services
    (autonomous cars) by processing and storing at the edge level. However, due to
    its partially centralized control plane, some non-critical tasks still use cloud
    processing. D. A High-Level Architecture of 6G A three-level 6G network architecture
    consists of AI plane, user plane, and control plane [140]. In this case, storage,
    compute, and networking are done at the same level (in user plane) to eliminate
    the hierarchy. The user plane is flat and it is defined between an access network
    and the internet. The control plane and AI planes are distributed and virtualized
    for various services. Further, the transport network is virtualized and isolated
    from the rest by software defined virtualization. The core network functions are
    made as micro-services and accessible by server-less systems. E. Multi-Level Architecture
    for 6G A three tier architecture of 6G consists of smart users, edge devices,
    and the cloud [114]. The users implement intelligent decision making techniques
    such as data-driven or model-based techniques to predict mobility and traffic
    patterns. At edge intelligence level, the mobile edge devices use deep reinforcement
    learning, or deep neural networks to optimize the scheduler for resource allocation
    to mobile users based on the CSI. Moreover, at the cloud level, having a high
    capacity central control can train the system with a numerical platform that presents
    the labeled samples for optimization algorithms. These can be used to train deep
    neural networks and later be implemented in the control plane. F. Three-Dimensional
    6G Architecture [4] The FG-NET 2030 has envisioned a 3D architecture for 6G networks,
    covering three key aspects, namely, communication (infrastructure view), intelligence
    (control view), and management (network view). It has different communication
    layers at the infrastructure view ranging from terrestrial, underwater, aerial,
    and satellite to enhance the range of communication in 6G networks. In addition,
    the authors at the control view presented how 6G networks will include intelligence
    to control and optimize the overall functions of sensing, spectrum access, communication,
    storage, and processing with the help of AI, deep learning, and ML. It recommends
    that the intelligence will be distributed across various network entities. Finally,
    in the network view, the functions of the overall network have been divided as
    sublayers. They include the application sublayer, routing, management, spectrum
    access, and physical medium, which resembles the layered IP stack. A simplified
    version of the same architecture for 6G has been proposed in [108]. G. Neuroscience
    Based 6G Architecture This architecture is more of a framework that tries to integrate
    neuro signals to emulate the wireless signals to be applied in 6G networks [136].
    Here, human brain’s intelligence and radiating properties of wireless signals
    shall be integrated to enable an intelligent communication between the human and
    computers. Recent advances in bio-IoT and implantable- communication devices have
    made us bold to envision short range 6G network communication, where wireless
    modules implanted inside the human brain acquire intelligence from the brain and
    communicate directly with the outer world wireless devices and base station or
    with another human with similar capability. H. Proposed Architecture Our proposed
    6G architecture consists of devices at the outer layer as shown in Fig. 6. The
    devices in this layer gather data, events in the network and communicate to the
    next level. Here, IoT nodes, cellular users, smart devices, etc., which have been
    connected to the radio access network may have a certain level of intelligence
    by default. Consequently, interference, channel selection, sensing shall be better
    managed. We have also depicted the paradigm of the cell-free network, small cells,
    and massive MIMO enabled by intelligent surfaces operating side-by-side as will
    be the scenario in 6G. The arrows in this layer indicate the wireless links. Further,
    in the next layer, we have edge devices and cloud that utilize AI for resource
    allocation, user management, and other optimization tasks. As proposed in [42],
    we considered a distributed edge intelligent architecture which provides process,
    store, compute and decision-making facility for independent physical networks.
    The intelligent agents at these edge devices learns from the devices to assist
    in better management of the network below it at layer 1. For instance, it could
    be the selection of wireless channel and tuning the parameters accordingly. Here,
    a centralized edge node provides backhaul service to all the physical networks
    through these edge clouds, that is shown by the discrete lines. It also includes
    cloud-RAN and backhaul resources for various use cases. However, all these resources
    are virtualized. Further, we have various clouds that interconnect the network
    elements. Finally, at the innermost layer, we have the applications for various
    verticals which either provides or receives services from the layers below it.
    Thus, the proposed network architecture in Fig. 6, provides the abstract view
    of 6G. However, several other features such as quantum computing, fog nodes, security,
    etc., have not been shown exclusively and shall be the potential candidates of
    6G architecture. FIGURE 6. Proposed network level 6G architecture. Show All SECTION
    VI. Future Trends in 6G The 6GFlagship project has identified 11 key areas of
    6G pilot project, and have published white papers in June 2020 [36], [42], [44]–[55],
    [107]. In this section, we summarize the overall ambition of some of the key areas
    such as localization and sensing, trust and security, UN sustainable development
    goals, rural connectivity and networking, edge intelligence, machine learning,
    and 6G business scope as shown in Fig. 7. FIGURE 7. Disruptive future trends in
    6G. Show All A. Localization in 6G Traditional sensing and localization using
    GPS or cell coordination will become obsolete in future high dense smart urban
    settings. In 2030 future networks, KPI related to accuracy must include user’s
    environment information to facilitate accurate positioning. For instance, a use
    case of 6G networks such as autonomous driving, where high resolution positioning
    at the cm level is necessary along the driving lane or track details to achieve
    traffic safety [19]. Similarly, mobile sensor-based applications that use location
    information should include power consumption per unit distance coverage; integrity
    and privacy-related KPI should include location details, malfunctioning nodes,
    and alarms [53]. These requirements ensure that future technologies or devices
    for 6G networks must be accompanied by additional information such as power consumption
    and types of alarms generated during any sensor failure for exact location sensing
    with emphasis on privacy. In addition, localization and sensing of information
    have several applications in future, including robotic surgery, contact tracing
    during pandemics, VR based games, social networking and dating apps, food delivery,
    context-aware marketing, autonomous driving, personal navigation, and animal tracking.
    Specifically, in the case of the context-aware services, 6G network will automatically
    learn user requirements by sensing the environment or location and providing the
    best QoS and other settings [124]. Consequently, 6G systems will integrate intelligence,
    pervasive networking, along with precision localization and high-resolution sensing
    to serve specific use cases that emerge during 2030, requiring accuracy being
    fine-tuned to cm level. This will enable seamless connectivity, localization,
    and sensing for context-aware services [19]. The 5G NR and mm-wave will offer
    localization and sensing features different from the previous generation of cellular
    networks. Moreover, 6G networks will further enhance the localization accuracy
    leading to high definition imaging by the use of THz communication, massive antennas,
    and RIS [53], [91]. As the beamwidth reduces at lower wavelengths ( μm -wave),
    the positioning accuracy increases. Next, the inverse relationship between frequency
    and device size (antenna) will enable dense packing of multiple antennas, which
    facilities precise angular and direction estimation. In this regard, better channel
    estimation algorithms are necessary at THz frequencies to sense and localize.
    Further, a higher data rate of 6G network will enable the sharing of maps between
    the devices much easier. With the large IRSs, the reflected wave can provide better
    localization service by exploiting the near-field effect and analyzing the wavefront.
    In addition, it will remove the need of synchronization between reference stations.
    However, more precise methods are necessary for the real-time localization and
    positioning if the intelligent surfaces are smaller in size [91]. It should be
    noted that the existing localization techniques using ML heavily depend on fingerprinting,
    regression, and classification methods. However, more intelligent localization
    and mapping methods will be necessary for the 6G network [53]. By AI- and ML-based
    frameworks, the sensing system can be trained to learn from raw data, where vital
    information (time and space-based) present with the noisy radio signal, or weak
    sensor data can be extracted to analyse the location and sensing patterns for
    the 6G network system. Albeit, most of the ML- and DL-based techniques need a
    large volume of structured data for training, which may not be available (noisy,
    random) in tiny sensor-based applications. Therefore, advanced analysis methods
    should be introduced to assist in trained localization. As mentioned earlier,
    localization is also associated with another set of tasks i.e., imaging and sensing
    which involves high frequency signals (60GHz). These signals when reflected by
    the objects, the size and dimension of the objects can be measured accurately
    [127]. Besides, localization also includes imaging using sensors. Here, passive
    sensors will capture the images and active sensors will transmit signals that
    represent range, angle, and Doppler with a high resolution and accuracy. For instance,
    in case of self-driving cars, radar imaging adds range and Doppler to the multi-dimensional
    image [53]. B. Trustworthy 6G How security in 6G different from 5G: The number
    of connected devices (IoT, MTC, and mobile users) in 6G networks will increase
    at a galloping rate and density-wise ten million devices in a unit square km area
    [2]. These devices will inherit artificial intelligence and span across diversified
    applications such as banking, industry, healthcare, government, transportation,
    and many more. Due to the dependence on many verticals, 6G networks must enforce
    a higher-level security in every stage of its network to prevent all forms of
    security attacks. In another instance, we may imagine a hyper connected world,
    where a minor compromise in the security settings may lead to fatality in an automated
    industry, robotic surgery performing incorrect incision, etc., [55], [50]. Currently,
    in 5G networks, traditional cryptography will serve most of the needs to secure
    the data communication across cloud and edge architectures. However, a new era
    of security will begin by the inception of quantum computing in 6G network. For
    instance, QKD systems facilitate easier eavesdropping detection that classical
    cryptography. Furthermore, upgrading the level of security is possible with quantum
    secure direct communications (QSDC), which provides better security over the quantum
    channel than the QKD or classical methods [120]. In another view, automation of
    security functions with ML algorithms at different levels will be essential because
    of network densification. However, there is a threat of reversed ML based security
    attacks, as the network can learn all parameters minutely. Therefore, 6G network
    needs exclusive holistic network security architecture [55]. 1) Required Security
    Features The key features should include: especially, future SIM cards and their
    security aspects, and use of asymmetric cryptography, Transport Layer Security
    (TLS) using elliptic curve cryptography (ECC), software and AI-driven security
    as applicable to SDN and NFV. For example, the security model in 6G networks will
    consist of a deep learning enabled VNF gateway that monitors the ingress and egress
    traffic and apply filtering policies to detect and prevent potential security
    attacks. In summary, a trust model must define the rules to collect, process,
    distribute, and filter data in the network. C. 6G for Sustainability Development
    Goals (SDG) Digitization due to 5G networks will currently address many social
    issues such as education, environment protection, and hunger. Nonetheless, there
    exist several problems that require extensive urbanization and connectivity. Since
    6G networks will bring in user experience-based hyperdata connectivity, several
    global issues especially, with regard to the United Nation’s SDGs, shall be mitigated
    [86]. The 6G network through its multi-faceted communication capability (long
    and short-range, 3D connectivity), has the potential to promote global sustainability,
    which is mainly due to its seamless connectivity and support for a plethora of
    services. Let’s see some of the future scopes of 6G networks as regards the promotion
    of sustainable development goals. For instance, online banking will enable anyone
    to access financial services with much ease, which will help to eradicate poverty.
    Similarly, online healthcare services will reduce the travel time and will easily
    reach the needy whenever necessary to promote better healthcare delivery. By having
    all-round network connectivity, farmers in rural places can make use of digital
    transactions for their agricultural, and home products. We shall implement IoT
    in food production to improve the yield and to obliterate hunger [93]. With the
    aid of 6G networks, all these issues shall be addressed on a massive global scale.
    When we consider zero energy, 6G network will target miniaturization of the devices
    to increase the power efficiency and energy harvesting, thus improving environmental
    performance. For instance, these devices may sustain the power generated through
    everyday activities such as walking, jogging, and household work of the device
    user. This energy shall support the personal information devices like wristband,
    smart patch which monitor a person’s vital body signs from time to time. In addition,
    in another scenario, these could cater to information and entertainment needs
    through over the top connectivity. To support the United Nation’s SDGs, 6G network
    has set its vision to provide an opportunity for the global society and economy
    to accomplish its digital dreams [44]. It includes data connectivity, a strong
    economy, facilitates smart healthcare, energy efficiency to name a few among the
    17 SDGs. The 6G vision targets three underpinnings to implement SDGs through various
    services. They are as follows. (i) Addressing the problems of people and society
    by providing suitable digital infrastructure to empower them. (ii) Context-awareness
    based on the environment which is possible by highly precise localization and
    sensing capabilities, online monitoring of resources, and so on. (iii) All-round
    uplifting of the ecosystem of SDGs. Besides these requirements, the fundamental
    necessity to achieve the goal will be the all-round connectivity and access to
    the internet. 1) How 6G Supports UN-SDGs Since SDGs spans all sectors of life,
    the vision of 6G networks should be multi-disciplinary. As a result, an open and
    co-innovation platform for 6G’s technology standardization activities which support
    human life and environment is crucial to create an ecosystem that will benefit
    all stakeholders of the UN-SDGs. The 6G ecosystem will contribute to the progress
    and well-being of various communities in society. For instance, 6G will provide
    wireless networking to connect patient, doctor, and medical equipment together
    in a remote smart healthcare system. With the aid of 6G network, several digital
    services could be extended such as education, food distribution services, banking,
    and industry to advance on a massive scale. These eventually result in the empowerment
    of human life toward achieving a better SDGs. Having said this, there will be
    concerns while targeting the SDGs such as inclusiveness, data security. For instance,
    when it comes to data collection, all stakeholders such as hospitals, society,
    and government must handle data with utmost privacy rules for the integrity of
    the system. Now, 6G networks play a role in addressing the privacy and security
    of data communication. In summary, 6G networks must implement actions that promote
    sustainability to different verticals, encourage growth with minimum energy consumption,
    and implement zero waste when people, machines, and resources are interconnected.
    Due to the prevalence of the intelligence in 6G network, it will improve efficiency
    and environmental sustainability by using low energy consuming technologies or
    devices that derive the energy from natural activities for their functioning.
    We have summarized the set of KPIs for all 17 SDGs and their possible use cases,
    as in Table 4. TABLE 4 KPI for UN SDG and Their Use Cases 2) Opportunity for 6G
    The use cases of 6G networks must be aligned with the intentions of SDGs, especially
    the SDGs which can be well supported by ICT infrastructure. The common aspects
    such as online financial activities, including online banking and online business
    will promote economic growth. Furthermore, telemedicine, remote mapping of natural
    resources such as minerals and water bodies are some use cases that improves social
    and healthcare sustainability. When we look at 6G’s diverse range of wireless
    connectivity, edge intelligence, localization, sensing, and ultra-low energy consumption
    will strongly support rural connectivity, environmental context, and energy aspects
    of SDGs. In Table 4, we summarize the KPIs with regard to the 17 SDGs and the
    use cases, as intended in [44]. D. How 6G Will Wipeout Digital Divide? There are
    several reasons for lack of connectivity in many remote areas of the world such
    as absence of infrastructure, low income, rough geography, and many more [92].
    The 6G network has a strong vision on breaking the digital divide which will in
    turn promotes sustainability alongside the connectivity. It is estimated that
    nearly 3.7 billion of world population is far from reach of internet connectivity
    [44]. The digital divide gives rise to several other problems such as retarded
    economic growth, education and awareness, inhibits the adoption of remote, and
    advanced healthcare technologies. It also causes low growth in modern industrial
    and new age transportation sectors. The digital divide has not left any geographical
    regions untouched. In reality, even though, first world nations have high-quality
    mobile internet connectivity, there are remote areas of USA, and Europe that experience
    a low data rate of 0.4–1Mbps through wired/wireless connectivity. The situation
    is much worse in Africa, Brazil, and India [52]. 1) Scope of 6G in Rural Connectivity
    Connectivity in tough terrains, away from the main cities should be simple, low
    cost, and easily achievable. Therefore, mobile broadband is the best solution
    when compared to wired internet [48]. Providing last-mile connectivity through
    the LEO satellite will cover a large area. However, it will not be a reasonable
    solution, considering the throughput, latency, and cost. Instead, usage of large
    power transmitters (mega cells) in low dense rural areas will be a viable solution.
    To achieve this target, new safety rules and power regulations will be necessary.
    Alongside, floating mobile base stations like UAV or balloons will promote mobile
    data services while acting as relays or data cache [52]. Since remote regions
    have financial constraints, while providing good QoS, special emphasis on affordability
    (cost-wise) is mandatory. The use of natural power sources such as solar and wind
    energy for power grids that operate the backhauls will enable cost reduction.
    In addition, cost reduction and resource efficiency could be achieved by confining
    the contents to the rural region generated within the locality. This requires
    caching the locally generated contents in local servers itself and provisioning
    network slicing. The local caching will enable cost reduction by minimizing the
    need for backhaul connections. Moreover, several data serving schemes can be brought
    into picture through network slicing to enhance connectivity with limited resources
    [76]. Regarding the backhaul connectivity, even though visible light or microwave
    seems to be the right candidates, they may face challenges in rural areas. Therefore,
    Integrated Access and Backhaul (IAB) seems much promising where the operator uses
    the part of the spectrum for backhaul and the remaining part for cellular access
    at a reasonably lower cost than the former candidates [52]. Thus, 6G networks
    shall rely on IAB to reduce the deployment cost and improve the resource utilization
    [107]. With IAB, only a subset of base stations will carry the backhaul traffic
    through connected links (fiber/wire); whereas, the remaining base stations will
    be connected through wireless links to transmit the backhaul traffic through multi-hop
    relaying. It is much possible in 6G due to the availability of large bandwidth
    in THz bands where access and backhaul traffic can be multiplexed by suitable
    scheduling schemes. Thereby, reducing the infrastructure cost. The IAB will provide
    service in sub-6GHz and mm-wave. Similarly, Low orbit Satellites, UAV, and balloons
    find useful solutions to extend the range seamlessly. Based on the communication
    need, these terminals can be made operational (discontinuous trans-reception)
    as and at when required: thus, reducing the power consumption. Altogether, non-terrestrial
    communication will play a key role in 6G networks to reduce the digital divide
    [52]. In recent trends, edge computing and the caching of local data for local
    usage, installing micro telecom infrastructure and integrating it with mobile
    network operator, and their maintenance have attained prominence in rural regions.
    Albeit, all these have several challenges, such as financial models, frequency
    regulations, propagation delays, and co-existence issues [48]. E. What is the
    Scope of Edge Intelligence in 6G The rise of edge computing and ultra-dense node
    deployment or mobile devices led to the emergence of mobile edge computing to
    optimize the computations and performance in the network. Further, the very nature
    of mobile devices requires the computations to be distributed. A limiting factor
    of 5G network is the lack of intelligence in mobile edge computing, which will
    otherwise facilitate an easy transition into new verticals and services such as
    Industry 4.0, smart mega cities, and intelligent gadgets. One of the reasons for
    the setback is that existing AI algorithms do consume enormous time and resources.
    Not only that, the production of hardware and AI solutions are yet to achieve
    synchronism. In total, when 5G network is considered to use cloud driven AI, 6G
    network will use edge driven AI [80], [81]. The Edge Intelligence (EI) is an extreme
    capability to process the data at the edge. Today, mobile devices have the high
    computational capability, storage space, and run petty AI applications. Subsequently,
    AI will become an integral part of the 6G network with distributed intelligence
    to run standalone and automated operations replacing most human interactions.
    For instance, verticals such as autonomous cars, ultra-smart healthcare, and modern
    industries involve complicated tasks with regard to data acquisition and computation.
    Therefore, distributed intelligence at different network entities will be required
    to offer optimized services. This forms the foundation for the 6G internet of
    intelligent things [86]. EI can be viewed from different directions: data analytics
    point of view refers to data analysis and development of solutions at or near
    the site where the data is generated. the network perspective mainly targets the
    intelligent functions that are deployed at the edge of the network, comprising
    the user, tenant, or the network boundary [42]. 1) Need for Software for AI Edge
    Currently, we lack software fluidity; without that, precisely locating intelligence
    in the network entity and sharing knowledge will be impaired. EI will be useful
    in several verticals to offer better QoS. For instance, when we consider the scenario
    of self-driving vehicles, these vehicles need to compute and communicate many
    factors, such as inter vehicular distance, roadside entities, traffic signs, and
    vehicles in the nearby lanes. These data will be communicated to the EI enabled
    base station or access points located along the roadside to quickly (low latency)
    store and process the data and assist in deciding the load on available frequency
    bands (spectrum) to facilitate the data transfer from these vehicles dynamically.
    Noticeably, these EI units will also have access to the central cloud database,
    which maintains the overall data from several regional EI systems. Further, smart
    city applications, mobile extended reality (XR) which requires large data processing
    and computations, can be benefited by EI due to local data processing, reduced
    delay, resulting in higher data rate, task and sharing, and so on [42]. Nevertheless,
    the hardware technology to accommodate the intelligent processing capabilities
    on a small edge device with resource constraints is yet to mature. Similarly,
    software with intelligent, real-time, and independent decision capabilities with
    regard to training and learning from the data to meet the requirements of 6G networks
    will require time to evolve. 2) Challenges (i) When we consider a distributed
    environment, providing intelligence across the edge devices by gathering all parameters
    related to resource usage, training, and learning models will be a challenge.
    (ii) Furthermore, the optimum deployment of edge nodes in a high-density network
    and resource allocation is complex. (iii) At the edge devices, the action of AI
    is largely influenced by the type of precision of input data, especially when
    the available data is huge, vivid, and needs classification. Therefore, training
    models must consider these challenging factors and rely on synthetic data generated
    from generalized adversarial networks to manage the data consistency. (iv) With
    respect to the distributed edge devices that handle AI, say mobile phones, the
    AI algorithms must be lightweight, and applications must be partitioned between
    user devices and edge layer for efficient resource management. (v) The software
    packages for EI contain edge applications that are deployed on the edge devices
    along with virtualization infrastructure. In this context, the selection of system
    policies, billing, etc., play a key role. (vi) In EI devices, real-time feedback
    is essential to attain better performance. Consequently, it is necessary to re-define
    the real-time feedback by considering the online learning and pre-trained models.
    The training algorithms should be distributed instead of centralized and must
    adopt online training models by considering the edge device limitations [61],
    [42], [80]. F. How Machine Learning Will Rule 6G Machine Learning (ML) is a sub-branch
    of AI which assists in the prediction and classification of data based on the
    trained data. In 6G network, ML will find its application at different instances
    of the network, especially, at the PHY, MAC, Network, and Application layers.
    We shall discuss these applications briefly in this section. From the earlier
    discussions on use cases during 6G era, we realize that interconnected intelligent
    network components require ML as a fundamental network requirement to offer value
    addition to the services, zero-touch optimization, and improve the performance.
    This is because relying purely on mathematical modeling of the wireless system
    will make the whole system computationally complex. However, ML can efficiently
    model the wireless system that could not be wrapped under mathematical equations
    to suit the requirements of 6G networks. Interestingly, ML will act on the vast
    real-time data received from different use cases such as self-driven cars, holographic
    telepresence, and intelligently control 6G network [47]. At the PHY layer, many
    of the functionalities can be envisioned by mathematical models. However, certain
    non-linear phenomenon such as interference detection and channel prediction can
    be efficiently addressed with ML. Furthermore, the discrete modules such as modulator,
    demodulator, and the filter will optimize the performance as a whole by suitable
    learning methods in ML. When we consider the channel coding process, the transmitter
    and receiver must implement certain coding schemes to safeguard the data against
    channel disparities. Deep learning is one of the promising methods for predicting
    the appropriate coding length by learning the channel code-word length. Next,
    synchronization is another aspect which certainly depends on the channel variations,
    mobility, and frequency deviations. In this regard, deep neural networks will
    prove to be beneficial. However, further research is necessary to comment more
    on this. Another aspect of ML is the use of deep neural networks for positioning.
    Since the existing positioning methods will popup accuracy issues when there is
    no line of sight path due to their dependency on pure mathematical modeling. In
    deep learning, fingerprint method uses CSI and received signal strength as learning
    data. Altogether, several physical layer optimizations such as beamforming, channel
    estimation, and throughput maximization are non-convex and do not yield a good
    performance with heuristic algorithms. Consequently, deep learning (CNN and DNN)
    appears as a boon to solve the physical layer optimization issues of 6G networks
    in real time, while achieving a good tradeoff between performance and computational
    time. 1) ML at the MAC ML will find its application in optimal solutions to the
    MAC layer tasks such as resource allocation, modulation and coding scheme (MCS)
    selection, handover, and uplink power control. When the channel conditions vary
    due to user movement, channel variation the deep reinforcement learning methods
    could determine optimal solutions by learning from the varying inputs. (i) ML
    for resource allocation: In most IoT scenarios the data transmission is predictable
    as they follow a specific pattern. As a result, it is easy to predict such patterns
    and decide the resources to be allocated efficiently with the aid of ML. This
    reduces the network latency, and uplink random access for the devices. Similarly,
    by allocation of time, the frequency resources for cell users can be optimized
    with ML. It can predict the data traces in the cell, and network load and allocate
    time frequency slots in a flexible manner (flexible duplex) to optimize the resources
    and avoid interferences. (ii) Power management: 6G network devices must have seamless
    battery management capacity. Since MAC layer controls the access to medium, the
    radio power is the key factor to be managed. ML can analyze the real-time traffic
    patterns inside the cell and data targeted for the neighbor cells to adjust the
    load. By this, the network can schedule users between transmission and sleep states
    in a power efficient way. (ii) Security with ML: Due to the high profile nature
    of the 6G network such as high data rate, all-round connectivity, and extremely
    low latency, the data generation will go skyrocket. As a result, to address the
    various security threats that would arise due to the massive data, the network
    security must be automated. In this context, proactive and self-adaptive features
    of ML will be promising solutions to automate the security. 2) ML at the Application
    Layer 6G networks will be intelligent by integrating context awareness to offer
    better user services and multi-agent reinforcement learning to enhance the overall
    efficiency. Using a rule-based approach to determine the context configuration
    for each service class will be tedious. However, ML can predict the context configurations
    meaningfully with the help of past service choices and modify the configuration
    to suit the new services from the applications. For instance, lets us consider
    an application that monitors the network parameters automatically. We know that
    KPIs need to be maintained within the threshold level. Therefore, ML can be used
    to detect anomaly with respect to network management, security, and many more.
    Future network architectures must be end-to-end service providers, where there
    will be dynamic cooperation between network segments and communication entities.
    To ensure all-round ubiquitous connectivity, interoperability between heterogeneous
    networks is mandatory. Furthermore, efficiency in terms of cost, power, system
    performance, communication, and connectivity, spectrum usage are necessary. The
    core network to provide on demand service deployment consists of SBA, where network
    functions (NF) will subscribe to network repository function and it offers services
    to one or more NFs. By, each NF can be independently upgraded or removed to provide
    network slicing. G. Business Model for 6G 5G’s market will revolve around, providing
    the three key metrics for various verticals. Furthermore, in 5G networks, business
    models changed from monopolized network operators to distributed local edge micro
    operators, virtualized and software network service providers, and slice managers.
    Unlike the 5G network, the 6G network will provide ultimate connectivity between
    cyber-physical world in real-time and several upfront KPIs. This will motivate
    numerous verticals, technology developers, product and application developers
    for the future society 5.0. These market advances will disrupt the conventional
    business and invite new stakeholders and investors with regard to 6G’s communication,
    storage, and control applications [50]. In addition, 6G network will intertwine
    with intelligence, virtualization, local edge operation, spectrum sharing, and
    a variety of sensing services; altogether, it will influence our personal lives
    and will be an integral part of society in 2030. As a consequence, it will bring
    in several newer forms of business models and opportunities to generate revenue
    [16]. 1) Key Trends and Uncertainties Smart grid system for distribution of electricity,
    or internet will be foreseeing extreme connectivity and self-driven. Considering
    these applications, revenue models can be planned with the involvement of public-private-partnership.
    Next, over the top (OTT) companies will offer cloud storage and cognitive services
    (AI, UAV as a service, and context aware services) along with primary features
    such as calling and data connectivity to compete with the traditional MNOs. This
    trend will boost innovative business models to attract customers and improve their
    profit. The business models along with increasing the revenue must even target
    sustainability goals for the benefit of humanity. For instance, 6G’s vision includes
    providing connectivity for all humans; in this context, at least providing mobile
    internet to everyone in rural areas will solve the problem of digital divide.
    When people around the globe are connected, they will have the digital identity
    that will enable economic growth [157]. In this regard, data services will attract
    monetization. Further, another interesting business opportunity will be industry
    automation. When we consider Industry 5.0 and beyond, all sensors and machines
    that communicate with each other or operated by robots, will require a private
    and secure communication network which will offer extreme reliability, zero touch
    assistance, and operate as standalone networks. More importantly, 6Gs spectrum
    sharing, and policy regulations will generate prime revenue to the government
    and agencies. In addition to this, digital twins, and FinTech industry will revolutionize
    the business pattern with 6G networks. This is because, 6G networks will use green,
    zero energy, and zero emission technologies such as intelligent reflecting surface
    and high lifetime wireless nodes in a manner that will bring new business around
    building these technologies [23]. 2) Scenarios It is anticipated that the following
    scenarios will open up new business opportunities for 6G [50]. Edge computing,
    IoT, Robotics, and AI will become the common technologies that every electronic
    gadget will incorporate and will be reasonably priced for customers to afford.
    Therefore, these devices could be used to collect a variety of data from users
    and process them locally before remote transmission. Next, network slicing will
    allow up to 10000 slices under a network operator. This will gear-up revenue generation
    [23]. Furthermore, the development of sustainable solutions will empower them
    to operate as standalone and simplify the lives of people. In this regard, to
    find locally sustainable and economical solutions, public-private partnership
    projects, distributed cooperative models, and peer-peer business models will be
    more suitable against the monopolized market at the local level [50]. In addition,
    due to the sparse spread of technology in rural areas, manufacturing cheap and
    inexpensive solutions at the micro level could be an acceptable approach. However,
    6G will overcome the digital divide to promote technology sharing in which a 3D
    design of a machine can be shared in real-time with another underdeveloped city
    that is deprived of research facilities to manufacture it using its cheap labor
    and market locally as well as in the place of origin at a reasonable price than
    otherwise. This will support a circular economy and creates a better society both
    in the rural and urban areas. H. Additional Trends 1) Cell-Free Access Cell-less
    architecture is a concept in future networks, where the existing paradigm of connecting
    a user to an access point (gNB) confined to a cell will no longer be the case.
    Instead, a cellular user will get connectivity from multiple BS without the restriction
    of cell boundaries. In a traditional cellular architecture, the users who are
    at the edge of the cell will have poor connectivity, which will be addressed in
    the cell-free architecture. This feature requires tweaking the traditional cell
    search, synchronization, random access, and resource allocation procedures, to
    suit the new cell-less access [107], [112]. Furthermore, in 6G due to the extremely
    high density of mobile users or intelligent devices that require high data rates,
    the coverage of the gNB will be limited since they operate at very high frequencies,
    which reduces the interference. In this context, cell-less architecture is very
    useful to provide seamless connectivity. The cell-less system consists of several
    gNBs distributed in a large geographical region provides connectivity to all users
    located using the same frequency. These gNBs will be connected to a central processing
    unit (CPU), which controls them. Normally, a cell-free system is associated with
    MIMO to improve the spatial connectivity. The cell-free architecture has several
    benefits, namely, (i) scalable signal processing, (ii) scalable power control,
    (iii) spectral and energy efficiency, and (iv) simple signal processing and economical
    system [120]. 2) Backscatter Communication (BsC) It is an interesting paradigm
    for 6G network to achieve extremely high energy efficiency (EHEE) of the network
    when there are an enormous number of connected devices. For massively dense distribution
    of wireless nodes, the use of the battery as a power source will not be feasible
    due to the challenges involved in recharging or replacing the battery. In this
    scenario, the wireless nodes shall receive radio signals from the ambience passively,
    leading to battery-free operation [123]. The fundamental idea is to use the RF
    energy that is incident from the ambience and reflect it back after modulation.
    This method in fact, reduces the burden on the constrained device and increases
    the spectrum efficiency. It has applications in short-range and low power communication
    scenarios. 3) Wireless Power Transfer While connecting the massive devices, obtaining
    the channel state information (CSI) to know their channel status becomes a tedious
    task and consumes a lot of energy. As a result, the quality of transmission may
    get affected if CSI is omitted. However, a tradeoff between data communication
    and power can be achieved by wireless power transfer (WPT) and CSI-free transmissions.
    Further, wireless power transfer will be a possible method to promote sustainability,
    low emission, and zero energy consumption among devices in 6G networks [112].
    In this context, as applicable to massive IoT nodes, the nods can be made energy
    harvesting from the surrounding. It could be by the use of solar energy, wind,
    hydroelectric, or RF signals, etc. The wireless power transfer using RF involves
    remotely powering (charging) the wireless devices using the RF field. For instance,
    when BS transfer to a user, then the neighbor of that user may use the energy
    in the RF signal to charge itself. It uses far-field radiation properties of electromagnetic
    waves to transmit within a short distance. This has several benefits, such as
    significant increase in the durability and life time of the nodes, reduced energy
    footprint, auto-charging, and contactless charging. A few well-known solutions
    for wireless energy transfer involve (i) energy beamforming, where a group of
    high beam antennas are focused to transmit the energy to a specific set of nodes
    using narrow beams. (ii) distributed antenna systems to replenish the energy loss
    during energy propagation toward the nodes by using power beacons. However, there
    are several challenges involved in energy transfer. They are (i) ineffectiveness
    of the support for non-line of sight energy transfer, (ii) power transfer for
    non-stationary (mobile) users, (iii) radiation hazards, to list a few. SECTION
    VII. 6G Research Worldwide Hitherto While the 5G network is being deployed worldwide,
    the research and development activities on 6G networks are gearing up both in
    industry and academia. In March 2020, 3GPP completed the 5G standard release 16,
    which will be followed by release 17 to support all three scenarios in 2021 [46],
    [99]. Therefore, in the next couple of years, 3GPP will initiate the 6G research.
    It is anticipated that by 2027, the 5G infrastructure market will increase to
    47.75bn USD by 2027. The top five players in the 5G infrastructure market include
    Huawei, Ericsson, Samsung, Nokia, and ZTE. They have currently shifted their attention
    toward the 6G network due to the enormous potential and benefit that the 6G network
    is anticipated to offer over the 5G network. Along the same lines, FCC has decided
    to promote 6G network research and trials in the THz band by opening a 95 GHz
    –3 THz frequency band for research [99]. Samsung has anticipated that ITU-R will
    define the official vision of the 6G network in mid-2021, and the initial commercialization
    of 6G network will begin by the end of 2028 [86]. Recently, from the past 18 months
    after the first 6G wireless summit in March 2019, the academia has become aggressive
    in the early research on the vision, technologies, challenges, and the future
    directions of 6G networks. The global telecom companies have even endorsed this.
    In the following paragraph, we will list the major activities in the research
    and standardization efforts by various organizations worldwide. A. Finland The
    6G Flagship program (6Genesis) of the University of Oulu in association with other
    industries and academic institutes such as Nokia, VTT research center, Business
    Oulu, and Keysight technologies are at the forefront of research groups. They
    have initiated two 6G network summits and have released 12 white papers on the
    key research areas of the 6G network [112]. Moreover, Mediatek has started its
    research on 6G chipset along with Nokia in Finland. B. China The major players
    in the 6G network in China include Huawei and ZTE. They have their independent
    research units aside government-sponsored 6G R&D promotion and expert group. Their
    research mainly includes THz, AI, and blockchain for 6G networks along with other
    operators such as China Mobile and China Unicom. C. USA In Feb 2019, US President
    Donald Trump US telecom companies gave a call to intensify their research to launch
    6G network at the earliest. From academia, The New York University wireless center,
    headed by Prof. T. Rappaport, has highly engaged in developing the THz channel
    modeling and has achieved a 100 Gbps data rates in its trials [57]. D. South Korea
    One of the key telecom players SK Telecom, has undertaken 6G network research
    in the areas of THz communication, ultra-massive MIMO, and aerial communication.
    Further, they have collaborated with Ericsson and Nokia in 6G equipment manufacturing
    technology development. Samsung has recently released the white paper on 6G vision
    emphasizing three aspects: holographic communication, truly immersive XR, and
    digital twins [86]. Moreover, LG and KAIST University have developed a 6G network
    research laboratory to jointly conduct research in the technology areas of the
    6G network. E. Japan NTT-Docomo has released its white paper indicating the vision
    of future 2030 network (6G). It has focused its research on AI and Cyber physical
    system to promote 6G networks. In the process, NTT has demonstrated 100 Gbps at
    28 GHz band. In addition, Japan Govt., released 2.04 bn USD to promote R&D. Apart
    from these, Toshiba and Tokyo University have initiated 6G networks research [149].
    Similarly, Sony, Nippon, and Intel have planned to work together in different
    fields of 6G technology. F. Europe In Europe, besides Finland, several universities
    namely, University of Dresden and Deutsche Telekom (Germany) are involved in the
    research of Tactile internet, HCI technologies. Next, University of Padua, Italy
    and NYU wireless group are also involved in the 6G network research. SECTION VIII.
    Future Explorations After discussing the current trends and future directions,
    it is essential to lead the research community with hints for further exploration.
    In this section, we will ponder upon some key directions and opportunities. A.
    How to Provide Security In 6G network, massive connectivity, heterogeneity, and
    multi-hop routing for communication of user data arises the question of privacy
    of data. This is because, in the said context, data will be exposed to multiple
    entities, while ML requires the use of private data for training the data, which
    increases the vulnerability of data. The proactive security mechanisms for dynamic
    networks (high mobility vehicular networks) will increase the cost of control
    signaling. However, 6G networks cannot afford to look for reactive security methods
    that are slower [159]. Therefore, cost effective proactive methods need to be
    invented. Even though, quantum security seems to be promising, a vibrant research
    in this direction is yet to be seen. Therefore, 6G networks research shall devote
    more efforts on these security aspects. B. Virtualization of Radio Access Interface
    Even though 6G network is distributed, an intelligent supports SDN and visualization
    are needed. The crucial questions to answer, include Will every network element
    supports various simultaneous verticals requirements such as resources, latency,
    QoS, and cloud computing needs? and how do we achieve that? In addition, virtualization
    in 6G, needs to support all radio access interfaces such as THz radio, smart surface,
    and quantum radio, which is a great challenge to address. C. Vertical Edge Caching
    When we consider the terrestrial, aerial, and space networks (vertically), the
    satellite networks will introduce a large delay. Therefore, aerial network elements
    such as balloons or HAPs shall act as data caching and edge computing facilities
    between terrestrial and satellite networks to reduce the computations of satellite
    networks and support critical services from there itself at a reduced delay. This
    scenario has a wide scope for future research with regard to radio resource allocation,
    localization, power efficiency, etc. D. Mobility and Localization Similarly, cell-free
    networking requires precise localization and synchronization between heterogeneous
    networks along with beamforming and accommodation of reflecting surfaces, which
    opens another area for future research [158]. Next, high precision beamforming
    under high mobility is infeasible. Therefore, the location-based steering of the
    beam looks promising. Albeit, efficient location-based mobility tracking for beamforming
    requires research attention. Moreover, during the mobility between different access
    networks with various protocols, to maintain active communication and localization,
    an integrated protocol design is necessary, which we lack at the moment. E. Rural
    Connectivity After the outburst of COVID’19 pandemics, the world has gone almost
    virtual. We noticed that the urban areas sparsely connect to the rural areas,
    and most of the rural areas have local networks without having connectivity to
    the backbone. Therefore, to bridge the gap of the digital divide, policy makers,
    technology developers, and service providers shall work together to facilitate
    the availability of modern ICT tools to the unconnected. Furthermore, there is
    a need for awareness among the people about using AI, XR, and cloud-based technologies
    to promote better living and healthcare. Next, with respect to optical connectivity
    to alleviate the digital divide, new standards are necessary for VLC, and the
    equipment shall have dual connectivity comprising of VLC and radio transceivers
    [147]. The VLC radio shall also target to provide cost effective services to suit
    the needs of the underprivileged population. By considering these potentials of
    VLC regarding the digital divide, major research is necessary to support long
    range communication, with relatively lower cost than the radio spectrum. F. Analysis
    of Meta Materials and Reflecting Surfaces Undoubtedly, these reflecting surfaces
    form the key component of 6G networks due to their energy saving and harvesting
    features. Albeit, analytical and electromagnetic field modeling to study the non-linear
    behavior of the phase response, channel capacity, and spectrum efficiency of these
    smart reflecting transmitters, needs in-depth study in the near future [144].
    Furthermore, the multicarrier modulation, multi-antenna transmissions, practical
    direct demodulation of the reflected wave at the receiver, and the prototype development
    are the key areas for future exploration. G. EI When heterogeneous devices are
    connected, and all the intelligence has pushed to the edge, managing various data
    types, address resource allocation, and modeling the network behavior that requires
    data-driven multi-level distributed algorithms shall be well studied. In addition,
    light-weight AI solutions will be necessary to enhance distributed operations.
    Moreover, another interesting domain to explore is selecting different machine
    learning models for various verticals as each vertical will have multiple KPI
    requirements. H. Sustainable Goals One may readily agree that the 6G network will
    greatly support SDGs. However, measurement of social impact due to the penetration
    of the 6G networks requires an in-depth analysis and mapping between 6G KPIs and
    the 17 SDGs. First, KPI values for SDGs should be determined. Second, a tremendous
    amount of work is required to bring equality in society, gender roles, and data
    privacy to invent new use cases of the 6G network successfully in order to promote
    SDGs. SECTION IX. Conclusion This article discusses the trajectory of the 6G network
    and its network components along with the current status, merits, use cases, and
    challenges. Further, we highlighted the current research and areas for future
    explorations. The vision of the 6G network and the next-generation network requirements
    indicate that the 6G network will immensely outperform the 5G network due to its
    ability to serve the extreme needs of future use cases such as autonomous vehicles,
    tele healthcare, industry automation, and the rise of several verticals. With
    a positive outlook, we described some of the possible use cases of the 6G network
    that will largely influence the society in 2030 and later. Further, it is evident
    from the KPIs that every aspect of the 6G network communication will wisely support
    various verticals. The main outline of this paper’s discussion stems from how
    6G network being a 3D connected network, and an AI-driven network will bridge
    the contemporary technology gap to efficiently achieve the SDGs. In addition,
    we provided a brief discussion by highlighting the merits of the key areas of
    the 6G network mentioned in the 6GFlagship project. We envision that AI, Intelligent
    surfaces, cell-free architecture, digital twins, and quantum computing will likely
    become the 6G technology candidates. Albeit, these technologies are not market
    ready. In this regard, international collaborations in industry and academia for
    research, technology development, and commercialization are necessary for the
    early inception of the 6G network. Authors Figures References Citations Keywords
    Metrics More Like This AF Relaying Secrecy Performance Prediction for 6G Mobile
    Communication Networks in Industry 5.0 IEEE Transactions on Industrial Informatics
    Published: 2022 Security, Reliability and Test Aspects of the RISC-V Ecosystem
    2021 IEEE European Test Symposium (ETS) Published: 2021 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/9312710/09335927.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: '6G Ecosystem: Current Status and Future Perspective'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1155/2018/8738613
  analysis: '>'
  authors:
  - Jiyang Xie
  - Zeyu Song
  - Yupeng Li
  - Yanting Zhang
  - Hong Yu
  - Jinnan Zhan
  - Zhanyu Ma
  - Yuanyuan Qiao
  - Jianhua Zhang
  - Jun Guo
  citation_count: 30
  full_citation: '>'
  full_text: '>

    This website stores data such as cookies to enable essential site functionality,
    as well as marketing, personalization, and analytics. By remaining on this website
    you indicate your consent. Cookie Policy Journals Publish with us Publishing partnerships
    About us Blog Wireless Communications and Mobile Computing Journal overview For
    authors For reviewers For editors Table of Contents Special Issues Wireless Communications
    and Mobile Computing/ 2018/ Article On this page Abstract Introduction Conclusions
    Conflicts of Interest Acknowledgments References Copyright Related Articles Special
    Issue Channel Characterization and Modeling for 5G and Future Wireless System
    Based on Big Data View this Special Issue Review Article | Open Access Volume
    2018 | Article ID 8738613 | https://doi.org/10.1155/2018/8738613 Show citation
    A Survey on Machine Learning-Based Mobile Big Data Analysis: Challenges and Applications
    Jiyang Xie ,1Zeyu Song,1Yupeng Li,2Yanting Zhang,3Hong Yu,1Jinnan Zhan,2Zhanyu
    Ma ,1Yuanyuan Qiao,3Jianhua Zhang ,2and Jun Guo1 Show more Academic Editor: Liu
    Liu Received 09 Apr 2018 Accepted 07 Jun 2018 Published 01 Aug 2018 Abstract This
    paper attempts to identify the requirement and the development of machine learning-based
    mobile big data (MBD) analysis through discussing the insights of challenges in
    the mobile big data. Furthermore, it reviews the state-of-the-art applications
    of data analysis in the area of MBD. Firstly, we introduce the development of
    MBD. Secondly, the frequently applied data analysis methods are reviewed. Three
    typical applications of MBD analysis, namely, wireless channel modeling, human
    online and offline behavior analysis, and speech recognition in the Internet of
    Vehicles, are introduced, respectively. Finally, we summarize the main challenges
    and future development directions of mobile big data analysis. 1. Introduction
    With the success of wireless local access network (WLAN) technology (a.k.a. Wi-Fi)
    and the second/third/fourth generation (2G/3G/4G) mobile network, the number of
    mobile phones, which is 7.74 billion, 103.5 per 100 inhabitants all over the world
    in 2017, is rising dramatically [ 1]. Nowadays, mobile phone can not only send
    voice and text messages, but also easily and conveniently access the Internet
    which has been recognized as the most revolutionary development of mobile Internet
    (M-Internet). Meanwhile, worldwide active mobile-broadband subscriptions in 2017
    have increased to 4.22 billion, which is 9.21% higher than that in 2016 [ 1].
    Figure 1 shows the numbers of mobile-cellular telephone and active mobile-broadband
    subscriptions of the world and main districts from 2010 to 2017. The numbers which
    are up to the bars are the mobile-cellular telephone or active mobile-broadband
    subscriptions (million) in the world of the year which increase each year. Under
    the M-Internet, various kinds of content (image, voice, video, etc.) can be sent
    and received everywhere and the related applications emerge to satisfy people’s
    requirements, including working, study, daily life, entertainment, education,
    and healthcare. In China, mobile applications giants, i.e., Baidu, Alibaba, and
    Tencent, held 78% of M-Internet online time per day in apps which was about 2,412
    minutes in 2017 [ 2]. This figure indicates that M-Internet has entered a rapid
    growth stage.   (a)     (a)  (b)     Figure 1  Mobile-cellular telephone subscriptions
    (million) in (a) and active mobile-broadband subscriptions (million) in (b) of
    the world and main districts [ 1]. Nowadays, more than 1 billion smartphones are
    in use and producing a great quantity of data every day. This situation brings
    far-reaching impacts on society and social interaction and increases great opportunities
    for business. Meanwhile, with the rapid development of the Internet-of-Things
    (IoT), much more data is automatically generated by millions of machine nodes
    with growing mobility, for example, sensors carried by moving objects or vehicles.
    The volume, velocity, and variety of these data are increasing extremely fast,
    and soon they will become the new criterion for data analytics of enterprises
    and researchers. Therefore, mobile big data (MBD) has been already in our lives
    and is being enriched rapidly. The trend for explosively increased data volume
    with the increasing bandwidth and data rate in the M-Internet has followed the
    same exponential increase as Moore’s Law for semiconductors [ 3]. The prediction
    [ 2] about the global data volume will grow up to 47 zettabytes () by 2020 and
    163 zettabytes by 2025. For M-Internet, 3.7 exabytes () data have been generated
    per month from the mobile data traffic in 2015 [ 4], 7.2 exabytes in 2016 [ 5],
    24 exabytes by 2019 on forecasting [ 5], and 49 exabytes by 2021 on forecasting
    [ 5]. According to the statistical and prediction results, a concept called MBD
    has appeared. The MBD can be considered as a huge quantity of mobile data which
    are generated from a massive number of mobile devices and cannot be processed
    and analyzed by a single machine [ 6, 7]. MBD is playing and will play a more
    important role than ever before by the popularization of mobile devices including
    smartphones and IoT gadgets especially in the era of 4G and the forthcoming the
    fifth generation (5G) [ 4, 8]. With the rapid development of information technologies,
    various data generated from different technical fields are showing explosive growth
    trends [ 9]. Big data has broad application prospects in many fields and has become
    important national strategic resources [ 10]. In the era of big data, many data
    analysis systems are facing big challenges as the volume of data increases. Therefore,
    analysis for MBD is currently a highly focused topic. The importance of MBD analysis
    is determined by its role in developing complex mobile systems which supports
    a variety of intelligently interactive services, for example, healthcare, intelligent
    energy networks, smart buildings, and online entertainments [ 4]. MBD analysis
    can be defined as mining terabyte-level or petabyte-level data collected from
    mobile users and wireless devices at the network-level or the app-level to discover
    unknown, latent, and meaningful patterns and knowledge with large-scale machine
    learning methods [ 11]. Present requirements of MBD are based on software-defined
    in order to be more scalable and flexible. M-Internet environment in the future
    will be even more complex and interconnected [ 12]. For this purpose, data centers
    of MBD need to collect user statistics information of millions of users and obtain
    meaningful results by proper MBD analysis methods. For the decreasing price of
    data storage and widely accessible high performance computers, an expansion of
    machine learning has come into not only theoretical researches, but also various
    application areas of big data. Even though, there is a long way to go for the
    machine learning-based MBD analysis. Machine learning technology has been used
    by many Internet companies in their services: from web searches [ 13, 14] to content
    filtering [ 15] and recommendation [ 16, 17] on online social communities, shopping
    websites, or contend distribution platforms. Furthermore, it is also frequently
    appearing in products like smart cellphones, laptop computers, and smart furniture.
    Machine learning systems are used to detect and classify objects, return most
    relevant searching results, understand voice commands, and analyze using habits.
    In recent years, big data machine learning has become a hot spot [ 18]. Some conventional
    machine learning methods based on Bayesian framework [ 19– 22], distributed optimization
    [ 23– 26], and matrix factorization [ 27] can be applied into the aforementioned
    applications and have obtained good performances in small data sets. On this foundation,
    researchers have always been trying to fill their machine learning model with
    more and more data [ 28]. Furthermore, the data we got is not only big but also
    has features such as multisource, dynamic and sparse value; these features make
    it harder to analyze MBD with conventional machine learning methods. Therefore,
    the aforementioned applications implemented with conventional machine learning
    methods have fallen in a bottleneck period for low accuracy and generalization.
    Recently, a class of novel techniques, called deep learning, is applied in order
    to make the effort to solve the problems and has obtained good performances [
    29]. Machine learning, especially deep learning, has been an essential technique
    in order to use big data effectively. Most conventional machine learning methods
    are shallow learning structures with one or none hidden layers. These methods
    performed well in practical use and were precisely analyzed theoretically. But
    when dealing with high-dimensional or complicated data, shallow machine learning
    methods show their weakness. Deep learning methods are developed to learn better
    representations automatically with deep structure by using supervised or unsupervised
    strategies [ 30, 31]. The features extracted by deep hidden layers are used for
    regression, classification, or visualization. Deep learning uses more hidden layers
    and parameters to fit functions which could extract high level features from complex
    data; the parameters will be set automatically using large amount of unsupervised
    data [ 32, 33]. The hidden layers of deep learning algorithms help the model learn
    better representation of data; the higher layers learn specific and abstract features
    from global features learned by lower layers. Many surveys show that nonlinear
    feature extractors that are linked up as stacks such as deep learning methods
    always perform better in machine learning tasks, for example, a more accurate
    classification method [ 34], better learning of data probabilistic models [ 35],
    and the extraction of robust features [ 36]. Deep learning methods have proved
    useful in data mining, natural language processing, and computer vison applications.
    A more detailed introduction of deep learning is presented in Section 3.1.4. Artificial
    Intelligence (AI) is a technology that develops theories, methods, techniques,
    and applications that simulate or extend human brain abilities. The research of
    observing, learning, and decision-making process in human brain motivates the
    development of deep learning, which was first designed aiming to emulate the human
    brain’s neural structures. Further observation on neural signals processing and
    the effect on brain mechanisms [ 37– 39] inspired the architecture design of deep
    learning network, using layers and neuron connections to generalize globally.
    Conventional methods such as support vector machines, decision trees, and case-based
    reasoning which are based on statistics or logic knowledge of human may fall short
    when facing complex structure or relationships of data. Deep learning methods
    can learn patterns and relationships from hidden layers and may benefit the signal
    processing study in human brain with visualization methods of neural network.
    Deep learning has attracted much attention from AI researchers recently because
    of its state-of-the-art performance in machine learning domains including no only
    the aforementioned natural language processing (NLP), but also speech recognition
    [ 40, 41], collaborative filtering [ 42], and computer vision [ 43, 44]. Deep
    learning has been successfully used in industry products which have access to
    big data from users. Companies in United States such as Google, Apple, Facebook,
    and Chinese companies like Baidu, Alibaba, and Tencent have been collecting and
    analyzing data from millions of users and pushing forward deep learning based
    applications. For example, Tencent YouTu Lab has developed identification (ID)
    card identification and bank card identification systems. These systems can read
    information from card images to check user information while registering and bank
    information while purchasing. The identification systems are based on deep learning
    model and large volume of user data provided by Tencent. Apple develops Siri,
    a virtual intelligent assistant in iPhones, to answer questions about weather,
    location, news according to voice commands and dial numbers or send text messages.
    Siri also utilizes deep learning methods and uses data from apple services [ 45].
    Google uses deep learning on Google translation service with massive data collected
    by Google search engine. MBD contains a large variety of information of offline
    data and online real-time data stream generated from smart mobile terminals, sensors,
    and services and hastens various applications based on the advancement of data
    analysis technologies, such as collaborative filtering-based recommendation [
    46, 47], user social behavior characteristics analysis [ 48– 51], vehicle communications
    in the Internet of Vehicles (IoV) [ 52], online smart healthcare [ 53], and city
    residents’ activity analysis [ 6]. Although the machine learning-based methods
    are widely applied in the MBD fields and obtain good performances in real data
    test, the present methods still need to be further developed. Therefore, five
    main challenges facing MBD analysis regarding the machine learning-based methods
    include large-scale and high-speed M-Internet, overfitting and underfitting problems,
    generalization problem, cross-modal learning, and extended channel dimensions
    and should be considered. This paper attempts to identify the requirement and
    the development of machine learning-based mobile big data analysis through discussing
    the insights of challenges in the MBD and reviewing state-of-the-art applications
    of data analysis in the area of MBD. The remainder of the paper is organized as
    follows. Section 2 introduces the development of data collection and properties
    of MBD. The frequently adopted methods of data analysis and typical applications
    are reviewed in Section 3. Section 4 summarizes the future challenges of MBD analysis
    and provides suggestions. 2. Development and Collection of the Mobile Big Data
    2.1. Data Collection Data collection is the foundation of a data processing and
    analysis system. Data are collected from mobile smart terminals and Internet services,
    or called mobile Internet devices (MIDs) generally, which are multimedia-capable
    mobile devices providing wireless Internet access and contain smartphones, wearable
    computers, laptop computers, wireless sensors, etc. [ 54]. MBD can be divided
    into two hierarchical data form: transmission and application data, from bottom
    to top. The transmission data focus on solving channel modeling [ 55, 56] and
    user access problems corresponding to the physical transmission system of M-Internet.
    On this foundation, application data focus on the applications based on the MBD
    including social networks analysis [ 57– 59], user behavior analysis [ 48, 50,
    60], speech analysis and decision in IoV [ 61– 66], smart grid [ 67, 68], networked
    healthcare [ 53, 69, 70], finance services [ 46, 71], etc. Due to the heterogeneity
    of the M-Internet and the variety of the access devices, the collected data are
    unstructured and usually in many categories and formats, which make data preprocessing
    become an essential part of a data processing and analysis system in order to
    ensure the input data complete and reliable [ 72]. Data preprocessing can be divided
    into three steps which are data cleaning, generation of implicit ratings, and
    data integration [ 46]. (1) Data Cleaning. Due to possible equipment failures,
    transmission errors, or human factor, raw data are “dirty data” which cannot be
    directly used, generally [ 46]. Therefore, data cleaning methods including outlier
    detection and denoising are applied in the data preprocessing to obtain the data
    meet required quality. Manual removal of error data is difficult and impossible
    to accomplish in MBD due to the massive volume. Common data cleaning methods can
    alleviate the dirty data problem to some extent by training support vector regression
    (SVR) classifiers [ 73], multiple linear regression models [ 74], autoencoder
    [ 75], Bayesian methods [ 76– 78], unsupervised methods [ 79], or information-theoretic
    models [ 79]. (2) Generation of Implicit Ratings. Generation of implicit ratings
    is mainly applied in recommend systems. The volume of rating data increases rapidly
    by analyzing specific user behaviors to solve data sparsity problem with machine
    learning algorithms, for example, neural networks and decision trees [ 46]. (3)
    Data Integration. Data integration is a step to integrate data from different
    resources with different formats and categories and to handle missing data fields
    [ 7]. Figure 2 represents the procedures of data collection and preprocessing.    Figure
    2  The procedures of data collection and preprocessing. 2.2. Properties of Mobile
    Big Data The MBD brings a massive amount of new challenges to conventional data
    analysis methods for its high dimensionality, heterogeneity, and other complex
    features from applications, such as planning, operation and maintenance, optimization,
    and marketing [ 57]. This section discusses the five Vs (short for volume, velocity,
    variety, value, and veracity) features [ 80] deriving from big data towards the
    MBD. The five Vs features have been improved in M-Internet, while it makes users
    access Internet anytime and anywhere [ 81]. (1) Volume: Large Number of MIDs,
    Exabyte-Level Data, and High-Dimensional Data Space. Volume is the most obvious
    feature of MBD. In the forthcoming 5G network and the era of MBD, conventional
    store and analysis methods are incapable of processing the 1000x or more wireless
    traffic volume [ 7, 82]. It is of great urgency to improve present MBD analysis
    methods and propose new ones. The methods should be simple and cost-effective
    to be implemented for MBD processing and analysis. Moreover, they should also
    be effective enough without requiring a massive amount of data for model training.
    Finally, they are precise to be applied in various fields [ 81]. (2) Velocity:
    Real-Time Data Streams and Efficiency Requirement. Velocity can be considered
    as the speed at which data are transmitted and analyzed [ 83]. The data is now
    continuously streaming into the servers in real-time and makes the original batch
    process break down [ 84]. Due to the high generating rate of MBD, velocity is
    the efficiency requirement of MBD analysis since real-time data processing and
    analysis are extremely important in order to maximize the value of MBD streams
    [ 7]. (3) Variety: Heterogeneous and Nonstructured Mobile Multimedia Contents.
    Due to the heterogeneity of MBD which means that mobile data traffic comes from
    spatially distributed data resources (i.e., MIDs), the variety of MBD arises and
    makes the MBD more complex [ 4]. Meanwhile, the nonstructured MBD also causes
    the variety. The MBD can be divided into structured data, semistructured data,
    and unstructured data. Here, unstructured data are usually collected in new applications
    and have random data fields and contents [ 7]; therefore, they are difficult to
    analyze before data cleaning and integration. (4) Value: Mining Hidden Knowledge
    and Patterns from Low Density Value Data. Value, or low density value of MBD,
    is caused by a large amount of useless or repeated information in the MBD. Therefore,
    we need to mine the big value by MBD analyzing which is hidden knowledge and patterns
    extraction. The purified data can provide comprehensive information to conduct
    more effectively analysis results about user demands, user behaviors, and user
    habits [ 85] and to achieve better system management and more accurate demand
    prediction and decision-making [ 86]. (5) Veracity: Consistency, Trustworthiness,
    and Security of MBD. The veracity of MBD includes two parts: data consistency
    and trustworthiness [ 80]. It can also be summarized as data quality. MBD quality
    is not guaranteed due to the noise of transmission channel, the equipment malfunctioning,
    and the uncalibrated sensors of MIDs or the human factor (for instance, malicious
    invasion) resulting in low-quality data points [ 4]. Veracity of MBD ensures that
    the data used in analysis process are authentic and protected from unauthorized
    access and modification [ 80]. 3. Applications of Machine Learning Methods in
    the Mobile Big Data Analysis 3.1. Development of Data Analysis Methods In this
    section, we present some recent achievements in data analysis from four different
    perspectives. 3.1.1. Divide-and-Conquer Strategy and Sampling of Big Data The
    strategies dividing and conquering big data is a computing paradigm dealing with
    big data problems. The development of distributed and parallel computing makes
    divide-and-conquer strategy particularly important. Generally speaking, whether
    the diversity of samples in learning data benefits the training results varies.
    Some redundant and noisy data can cause a large amount of storage cost as well
    as reducing the efficiency of the learning algorithm and affecting the learning
    accuracy. Therefore, it is more preferable to select representative samples to
    form a subset of original sample space according to a certain performance standard,
    such as maintaining the distribution of samples, topological structure, and keeping
    classification accuracy. Then learning method will be constructed on previous
    formed subset to finish the learning task. In this way, we can maintain or even
    improve the performance of big data analyzing algorithm with minimum computing
    and stock resources. The need to learn with big data demands on sample selection
    methods. But most of the sample selection method is only suitable for smaller
    data sets, such as the traditional condensed nearest neighbor [ 93], the reduced
    nearest neighbor [ 94], and the edited nearest neighbor [ 95]; the core concept
    of these methods is to find the minimum consistent subset. To find the minimum
    consistent subset, we need to test every sample and the result is very sensitive
    to the initialization of the subset and samples setting order. Li et al. [ 96]
    proposed a method to select the classification and edge boundary samples based
    on local geometry and probability distribution. They keep the space information
    of the original data but need to calculate k-means for each sample. Angiulli et
    al. [ 97, 98] proposed a fast condensation nearest neighbor (FCNN) algorithm based
    on condensed nearest neighbor, which tends to choose the classification boundary
    samples. Jordan [ 99] proposed statistical inference method for big data. When
    dealing with statistical inference with divide-and-conquer algorithm, we need
    to get confidence intervals from huge data sets. By data resampling and then calculating
    confidence interval, the Bootstrap theory aims to obtain the fluctuation of the
    evaluation value. But it does not fit big data. The incomplete sampling of data
    can lead to erroneous range fluctuations. Data sampling should be correct in order
    to provide statistical inference calibration. An algorithm named Bag of Little
    Bootstraps was proposed, which can not only avoid this problem, but also has many
    advantages on computation. Another problem discussed in [ 99] is massive matrix
    calculation. The divide-and-conquer strategy is heuristic, which has a good effect
    in practical application. However, new theoretical problems arise when trying
    to describe the statistical properties of partition algorithm. To this end, the
    support concentration theorem based on the theory of random matrices has been
    proposed. In conclusion, data partition and parallel processing strategy is the
    basic strategy to deal with big data. But the current partition and parallel processing
    strategy uses little data distribution knowledge, which has influence on the load
    balancing and the calculation efficiency of big data processing. Hence, there
    exists an urgent requirement to solve the problem about how to learn the distribution
    of big data for the optimization of load balancing. 3.1.2. Feature Selection of
    Big Data In the field of data mining, such as document classification and indexing,
    the dataset is always large, which contains a large number of records and features.
    This leads to the low efficiency of algorithm. By feature selection, we can eliminate
    the irrelevant features and increase the speed of task analysis. Thus, we can
    get a better preformed model with less running time. Big data processing faces
    a huge challenge on how to deal with high-dimensional and sparse data. Traffic
    network, smartphone communication records, and information shared on Internet
    provide a large number of high-dimensional data, using tensor (such as a multidimensional
    array) as natural representation. Tensor decomposition, in this condition, becomes
    an important tool for summary and analysis. Kolda [ 100] proposed an efficient
    use of the memory of the Tucker decomposition method named as memory-efficient
    Tucker (MET) decomposition decreasing time and space cost which traditional tensor
    decomposition algorithm cannot do. MET adaptively selects execution strategy based
    on available memory in the process of decomposition. The algorithm maximizes the
    speed of computation in the premise of using the available memory. MET avoid dealing
    with the large number of sporadic intermediate results proceeded during the calculation
    process. The adaptive selections of operation sequence not only eliminate the
    intermediate overflow problem, but also save memory without reducing the precision.
    On the other hand, Wahba [ 101] proposed two approaches to the statistical machine
    learning model which involve discrete, noisy, and incomplete data. These two methods
    are regularized kernel estimation (RKE) and robust manifold unfolding (RMU). These
    methods use dissimilarity between training information to get nonnegative low
    rank definite matrix. The matrix will then be embedded into a low dimensional
    Euclidean space, which coordinate can be used as features of various learning
    modes. Similarly, most online learning research needs to access all features of
    training instances. Such classic scenario is not always suitable for practical
    applications when facing high-dimensional data instances or expensive feature
    sets. In order to break through this limit, Hoi et al. [ 102] propose an efficient
    algorithm to predict online feature solving problem using some active features
    based on their study of sparse regularization and truncation technique. They also
    test the proposed algorithm in some public data sets for feature selection performance.
    The traditional self-organizing map (SOM) can be used for feature extraction.
    But the low speed of SOM limits its usage on large data sets. Sagheer [ 103] proposed
    a fast self-organizing map (FSOM) to solve this problem. The goal of this method
    is to find a feature space where data is mainly distributed in. If there exits
    such area, data can be extracted in these areas instead of information extraction
    in overall feature spaces. In this way, we can greatly reduce extraction time.
    Anaraki [ 104] proposed a threshold method of fuzzy rough set feature selection
    based on fuzzy lower approximation. This method adds a threshold to limit the
    QuickReduct feature selection. The results of the experiment prove that this method
    can also help the accuracy of feature extraction with lower running time. Gheyas
    et al. [ 105] proposed a hybrid algorithm of simulated annealing and genetic algorithm
    (SAGA), combining the advantages of simulated annealing algorithm, genetic algorithm,
    greedy algorithm, and neural network algorithm, to solve the NP-hard problem of
    selecting optimal feature subset. The experiment shows that this algorithm can
    find better optimal feature subset, reducing the time cost sharply. Gheyas pointed
    in as conclusion that there is seldom a single algorithm which can solve all the
    problems; the combination of algorithms can effectively raise the overall affect.
    To sum up, because of the complexity, high dimensionality, and uncertain characteristics
    of big data, it is an urgent problem to solve how to reduce the difficulty of
    big data processing by using dimension reduction and feature selection technology.
    3.1.3. Big Data Classification Supervised learning (classification) faces a new
    challenge of how to deal with big data. Currently, classification problems involving
    large-scale data are ubiquitous, but the traditional classification algorithms
    do not fit big data processing properly. (1) Support Vector Machine (SVM). Traditional
    statistical machine learning method has two main problems when facing big data.
    (1) Traditional statistical machine learning methods are always involving intensive
    computing which makes it hard to apply on big data sets. (2) The prediction of
    model that fits the robust and nonparameter confidence interval is unknown. Lau
    et al. [ 106] proposed an online support vector machine (SVM) learning algorithm
    to deal with the classification problem for sequentially provided input data.
    The classification algorithm is faster, with less support vectors, and has better
    generalization ability. Laskov et al. [ 107] proposed a rapid, stable, and robust
    numerical incremental support vector machine learning method. Chang et al. [ 108]
    developed an open source package called LIBSVM as a library for SVM code implementation.
    In addition, Huang et al. [ 109] present a large margin classifier M4. Unlike
    other large margin classifiers which locally or globally constructed separation
    hyperplane, this model can learn both local and global decision boundary. SVM
    and minimax probability machine (MPM) has a close connection with the model. The
    model has important theoretical significance and furthermore, the optimization
    problem of maxi-min margin machine (M4) can be solved in polynomial time. (2)
    Decision Tree (DT). Traditional decision tree (DT), as a classic classification
    learning algorithm, has a large memory requirement problem when processing big
    data. Franco-Arcega et al. [ 110] put forward a method of constructing DT from
    big data, which overcomes some weakness of algorithms in use. Furthermore, it
    can use all training data without saving them in memory. Experimental results
    showed that this method is faster than current decision tree algorithm on large-scale
    problems. Yang et al. [ 111] proposed a fast incremental optimization decision
    tree algorithm for large data processing with noise. Compared with former decision
    tree data mining algorithm, this method has a major advantage on real-time speed
    for data mining, which is quite suitable when dealing with continuous data from
    mobile devices. The most valuable feature of this model is that it can prevent
    explosive growth of the decision tree size and the decrease of prediction accuracy
    when the data packet contains noise. The model can generate compact decision tree
    and predict accuracy even with highly noisy data. Ben-Haim et al. [ 112] proposed
    an algorithm of building parallel decision tree classifier. The algorithm runs
    in distributed environment and is suitable for large amount and streaming data.
    Compared with serial decision tree, the algorithm can improve efficiency under
    the premise of accuracy error approximation. (3) Neural Network and Extreme Learning
    Machine (ELM). Traditional feedforward neural networks usually use gradient descent
    algorithm to tune weight parameters. Generally speaking, slow learning speed and
    poor generalization performance are the bottlenecks that restrict the application
    of feedforward neural network. Huang et al. [ 113] discarded the iterative adjustment
    strategy of the gradient descent algorithm and proposed extreme learning machine
    (ELM). This method randomly assigns the input weights and the deviations of the
    single hidden layer neural network. It can analyze the output weights of the network
    by one step calculation. Compared to the traditional feedforward neural network
    training algorithm, the network weights can be determined by multiple iterations,
    and the training speed of ELM is significantly improved. However, due to the limitation
    of computing resource and computational complexity, it is a difficult problem
    to train a single ELM on big data. There are usually two ways to solve this problem:
    (1) training ELM [ 114] based with divide-and-conquer strategy; (2) introducing
    parallel mechanism [ 115] to train a single ELM. It is shown in [ 116, 117] that
    a single ELM has strong function approximation ability. Whether it is possible
    to extend this approximation capability to ELM based on divide-and-conquer strategy
    is a key index to evaluate the possibility that ELM can be applied to big data.
    Some of the related studies also include effective learning to solve such problem
    [ 118]. In summary, the traditional classification method of machine learning
    is difficult to apply to the analysis of big data directly. The study of parallel
    or improved strategies of different classification algorithms has become the new
    direction. 3.1.4. Big Data Deep Learning With the unprecedentedly large and rapidly
    growing volumes of data, it is hard for us to get hidden information from big
    data with ordinary machine learning methods. The shallow-structured learning architectures
    of most conventional learning methods are not fit for the complex structures and
    relationships in these input data. Big data deep learning algorithm, with its
    deep architectures and globally feature extracting ability, can learn complex
    patterns and hidden connections beyond big data [ 37, 119]. It has had state-of-the-art
    performances in many benchmarks and also been applied in industry products. In
    this section, we will introduce some deep learning methods in big data analytics.
    Big data deep learning has some problems: (1) the hidden layers of deep network
    make it difficult to learn from a given data vector, (2) the gradient descent
    method for parameters learning makes the initialization time increasing sharply
    as the number of parameters arises, and (3) the approximations at the deepest
    hidden layer may be poor. Hinton et al. [ 32] proposed a deep architecture: deep
    belief network (DBN) which can learn from both labeled and unlabeled data by using
    unsupervised pretraining method to learn unlabeled data distributions and a supervised
    fine-tune method to construct the models, and solved part of the aforementioned
    problems. Meanwhile, subsequent researches, for example, [ 120], improved the
    DBN trying to solve the problems. Convolutional neural network (CNN) [ 121] is
    another popular deep learning network structure for big data analyzing. A CNN
    has three common features including local receptive fields, shared weights, and
    spatial or temporal subsampling, and two typical types of layers [ 122, 123].
    Convolutional layers are key parts of CNN structure aiming to extract features
    from image. Subsampling layers, which are also called pooling layers, adjust outputs
    from convolutional layer to get translation invariance. CNN is mainly applied
    in computer vision field for big data, for example, image classification [ 124,
    125] and image segmentation [ 126]. Document (or textual) representation, also
    part of NLP, is the basic method for information retrieval and important to understand
    natural language. Document representation finds specific or important information
    from the documents by analyzing document structure and content. The unique information
    could be document topic or a set of labels highly related to the document. Shallow
    models for document representation only focus on small part of the text and get
    simple connection between words and sentences. Using deep learning can get global
    representation of the document because of its large receptive field and hidden
    layers which could extract more meaningful information. The deep learning methods
    for document representation make it possible to obtain features from high-dimensional
    textual data. Hinton et al. [ 127] proposed deep generative model to learn binary
    codes for documents which make documents easy to store up. Socher et al. [ 128]
    proposed a recursive neural network on analyzing natural language and contexts,
    achieving state-of-the-art results on segmentation and understanding of natural
    language processing. Kumer et al. [ 129] proposed recurrent neural networks (RNN)
    which construct search space from large amount of textual data. With the rapid
    growth and complexity of academic and industry data sets, how to train deep learning
    models with large amount of parameters has been a major problem. The works in
    [ 40, 41, 43, 130– 133] proposed effective and stable parameter updating methods
    for training deep models. Researchers focus on large-scale deep learning that
    can be implemented in parallel including improved optimizers [ 131] and new structures
    [ 121, 133– 135]. In conclusion, big data deep learning methods are the key methods
    of data mining. They use complex structure to learn patterns from big data sets
    and multimodal data. The development of data storage and computing technology
    promotes the development of deep learning methods and makes it easier to use in
    practical situations. 3.2. Wireless Channel Modeling As is well known, wireless
    communication transmits information through electromagnetic waves between a transmitting
    antenna and a receiving antenna, which is deemed as a wireless channel. In the
    past few decades, the channel dimension has been extended to space, time, and
    frequency, which means the channel property is comprehensively discovered. Another
    development is that channel characteristics can be accurately described by different
    methods, such as channel modeling [ 136]. Liang et al. [ 137] used machine learning
    to predict channel state information so as to decease the pilot overhead. Especially
    for 5G, wireless big data emerges and its related technologies are employed to
    traditional communication research to meet the demand of 5G. However, the wireless
    channel is essentially a physical electromagnetic wave, and the current 5G channel
    model research follows the traditional way. Zhang [ 138] proposed an interdisciplinary
    study of big data and wireless channels, which is a cluster-based channel model.
    In the cluster-nuclei based channel model, the multipath components (MPCs) are
    aggregated into a traditional stochastically channel model. At the same time,
    the scene is discerned by the computer and the environment is rebuilt by machine
    learning methods. Then, by matching the real propagation objects with the clusters,
    the cluster-nuclei, which are the key factors in contacting deterministic environment
    and stochastic clusters, can be easily found. There are two main steps employing
    the machine learning methods in the cluster-nuclei based channel model. The recent
    progress is shown as follows. 3.2.1. A Gaussian Mixture Model (GMM) Based Channel
    MPCs Clustering Method The MPCs are clustered with the Gaussian mixture model
    (GMM) [ 87, 139]. Using sufficient statistic characteristics of channel multipath,
    the GMM can get clusters corresponding to the multipath propagation characteristics.
    The GMM assumes that all the MPCs consist of several Gaussian distributions in
    varying proportions. Given a set of channel multipath , the log-likelihood of
    the Gaussian mixture model is where is the set of all the parameters and is the
    prior probability satisfying the constraint . To estimate the GMM parameters,
    expectation maximization (EM) algorithm is employed to solve the log-likelihood
    function of GMM [ 87]. Figure 3 illustrates the simulation result of GMM clustering
    algorithm.    Figure 3  Clustering results of GMM [ 87]. As seen in Figure 3,
    the GMM clustering obtains clearly compact clusters. As scattering property of
    the channel multipath obeys Gaussian distribution, the compact clusters can accord
    with the multipath scattering property. Moreover, corresponding to the clustering
    mechanism of GMM, paper [ 87] proposed a compact index (CI) to evaluate the clustering
    results shown as follows: where is the variance of the kth cluster and and are
    given as where is the number of multipaths corresponding to the kth cluster. Both
    the means and variances of the clusters are considered in CI. Considering sufficient
    statistics characteristics, CI can uncover the inherent information of multipath
    parameters and provide appropriate explanation to the clustering result. Besides,
    considering sufficient statistics characteristics, the CI can evaluate the clustering
    results more reasonably. 3.2.2. Identifying the Scatters with the Simultaneous
    Localization and Mapping Algorithm (SLAM) In order to reconstruct three-dimensional
    (3D) propagation environment and to find the main deterministic objects, simultaneous
    localization and mapping (SLAM) algorithm is used to identify the texture from
    the measurement scenario picture [ 140, 141]. Figure 4 illustrates our indoor
    reconstruction result with SLAM algorithm.   (a)     (a)  (b)     Figure 4  Recognition
    of multiobjects with SLAM algorithm: (a) real indoor scene and (b) reconstruction
    result with SLAM algorithm. The texture of propagation environment can be used
    to search for the main scatters in the propagation environment. Then, the three-dimensional
    propagation environment can be reconstructed with the deep learning method. Then
    the mechanism to form the cluster-nuclei is clear. The channel impulse response
    can be produced by machine learning with a limited number of cluster-nuclei, i.e.,
    decision tree [ 142], neural network [ 143], and mixture model [ 144]. Based on
    the database from various scenarios, antenna configurations, and frequency, channel
    changing rules can be explored and then input into the cluster-nuclei based modeling.
    Finally, the predication of channel impulse response in various scenarios and
    configuration can be realized [ 138]. 3.3. Analyses of Human Online and Offline
    Behavior Based on Mobile Big Data The advances of wireless networks and increasing
    mobile applications bring about explosion of mobile traffic data. It is a good
    source of knowledge to obtain the individuals’ movement regularity and acquire
    the mobility dynamics of populations of millions [ 145]. Previous researches have
    described how individuals visit geographical locations and employed mobile traffic
    data to analyze human offline mobility patterns. Representative works like [ 146,
    147] explore the mobility of users in terms of the number of base stations they
    visited, which turned out to be a heavy tail distribution. Authors in [ 146, 148,
    149] also reveal that a few important locations are frequently visited by users.
    In particular, these preferred locations are usually related to home and work
    places. Moreover, through defining a measure of entropy, Song et al. [ 150] believe
    that 93% of individual movements are potentially predictable. Thus, various models
    have been applied to describe the human offline mobility behavior [ 151]. Passively
    collecting human mobile traffic data while users are accessing the mobile Internet
    has many advantages like low energy consumption. In general, the mobile big data
    covers a wide range and a great number of populations with fine time granularity,
    which gives us an opportunity to study human mobility at a scale that other data
    sources are very hard to reach [ 152]. Novel offline user mobility models developed
    based on the mobile big data are expected to benefit many fields, including urban
    planning, road traffic engineering, telecommunication network construction, and
    human sociology [ 145]. Online browsing behavior is another important facet regarding
    user behavior when it comes to network resource consumption. A variety of applications
    are now available on smart devices, covering all aspects of our daily life and
    providing convenience. For example, we can order taxies, shop, and book hotels
    using mobile phones. Yang et al. [ 49] provide a comprehensive study on user behaviors
    in exploiting the mobile Internet. It has been found that many factors, such as
    data usage and mobility pattern, may impact people’s online behavior on mobile
    devices. It is discovered that the more the number of distinct cells a user visit,
    the more diverse applications user has visited. Zheng et al. [ 153] analyze the
    longitudinal impact of proximity density, personality, and location on smartphone
    traffic consumption. In particular, location has been proven to have strong influences
    on what kinds of apps users prefer to use [ 149, 153]. The aforementioned observations
    point out that there is a close relationship between online browsing behavior
    and offline mobility behavior. Figure 5(a) is an example of how browsed applications
    and current location related to each other from the view of temporal and spatial
    regularity. It has been found that the mobility behaviors have strong influences
    on online browsing behavior [ 149, 153, 154]. Similar trends can also be observed
    for crowds at crowd gathering places, as is shown in Figure 5(b); i.e., certain
    apps are favored at places that group people together and provide some specific
    functions. The authors in [ 50] tried to measure the relationship between human
    mobility and app usage behavior. In particular, the authors proposed a rating
    framework which can forecast the online app usage behavior for individuals and
    crowds. Building the bridge between human offline mobility and online mobile Internet
    behavior can tell us what people really need in daily life. Content providers
    can leverage this knowledge to appropriately recommend content for mobile users.
    At the same time, Internet service providers (ISPs) can use this knowledge to
    optimize networks for better end-user experiences.   (a) App usage behavior of
    Bob in temporal and spatial dimension     (a) App usage behavior of Bob in temporal
    and spatial dimension  (b) App usage behavior of crowds at crowd gathering place     Figure
    5  App usage behavior in daily life: (a) the app usage behavior of an individual
    and (b) app usage behavior of crowds at crowd gathering places [ 50]. In order
    to make full use of users’ online and offline information, some researchers begin
    to quantize the interplay between online social network and offline social network
    and investigate network dynamics from the view of mobile traffic data [ 155– 158].
    Specifically, the online and offline social networks are, respectively, constructed
    based on online interest based and location based social network among mobile
    users. The two different networks are grouped into layers of a multilayer social
    network , as shown in Figure 6. and depict offline and online social network separately.
    In each layer, the graph is described as , where and , respectively, represent
    node sets and edge sets. Nodes, such as , represent users. Edges exist among users
    when users share similar object-based interests [ 88]. Combining information from
    manifold networks in a multilayer structure provides a new insight into user interactions
    between virtual and physical worlds. It sheds light on the link generation process
    from multiple views, which will improve social bootstrapping and friend recommendations
    in various valuable applications by a large margin [ 158].    Figure 6  Multilayer
    model of a network [ 88]. So far, we have summarized some representative works
    related to human online and offline behaviors. It is meaningful to note that owing
    to the highly spatial-temporal and nonhomogeneous nature of mobile traffic data,
    a pervasive framework is challenging yet indispensable to realize the collection,
    processing, and analyses of massive data, reducing resource consumption and improving
    Quality of Experience (QoE). The seminal work by Qiao et al. [ 60] proposes a
    framework for MBD (FMBD). It provides comprehensive functions on data collection,
    storage, processing, analyzing, and management to monitor and analyze the massive
    data. Figure 7(a) displays the architecture of FMBD, while Figure 7(b) shows the
    considered mobile networks framework. With the interaction between user equipment
    and 2G/3G/4G network, real massive mobile data can be collected by traffic monitoring
    equipment (TME). The implementation modules are employed based on Apache software
    [ 159]. FMBD builds a security environment and easy-to-use platform both for operators
    and data analysts, showing good performance on energy efficiency, portability,
    extensibility, usability, security, and stability. In order to meet the increasing
    demands on traffic monitoring and analyzing, the framework provides a solution
    to deal with large-scale mobile big data.    Figure 7  The overall architecture
    of framework for mobile big data (FMBD) and our considered mobile networks architecture
    [ 60]. In conclusion, the prosperity of continuously emerging mobile applications
    and users’ increasing demands on accessing Internet all bring about challenges
    for current and future mobile networks. This section surveys the literature on
    analyses of human online and offline behavior based on the mobile traffic data.
    Moreover, a framework has also been investigated, in order to meet the higher
    requirement of dealing with dramatically increased mobile traffic data. The analyses
    based on the big data will provide valuable information for the ISPs on network
    deployment, resource management, and the design of future mobile network architectures.
    3.4. Speech Recognition and Verification for the Internet of Vehicles With the
    significant development of smart vehicle produces, intelligent vehicle based Internet
    of Vehicle (IoV) technologies have received widespread attention of many giant
    Internet businesses [ 160– 162]. The IoV technologies include the communication
    between different vehicles and vehicles to sensors, roads, and humans. These communications
    can help the IoV system sharing and the gathering information on vehicles and
    their surrounds. One of the challenges in the real-life applications of smart
    vehicles and IoV systems is how to design a robust interactive method between
    drivers and the IoV system [ 163]. The level of focusing on driving will directly
    affect the danger of driver and passengers; hence, the attention of drivers should
    be paid on the complex road situation in order to avoid accidents during an intense
    driving. So, using the voices transfer information to the IoV systems is an effective
    solution for assistant and cooperative driving. By building a speech recognition
    interactive system, the driver can check traffic jams near the destination or
    order a lunch in the restaurant near the rest stop through the IoV system by using
    voice-based interaction. The speech recognition interactive system for IoV system
    can reduce the risk of vehicle accident, and the drivers do not need to touch
    the control panels or any buttons. A useful speech recognition system in IoV can
    simplify the life of the drivers and passengers in vehicles [ 164]. In the IoV
    system, drivers want to use their own voice commands to control the driving vehicles,
    and the IoV system must recognize the difference between an authorized and unauthorized
    user. Therefore, an automatic speaker verification system is necessary in IoV,
    which can protect the vehicle from the imposters. Recently, many deep learning
    methods have been applied in the speech recognition and speaker verification systems
    [ 41, 165– 167], and published results show that speech processing methods driven
    by MBD and deep learning can obviously improve the performance of the existing
    speech recognition and speaker verification system [ 40, 168, 169]. In the IoV
    systems, millions of sensors collect abundant vehicles and environmental noises
    from engines and streets will significantly reduce the accuracy of speech processing
    system, while the traditional speech enhancement methods, for example, Wiener
    filtering [ 170] and minimum mean-square error estimation (MMSE) [ 171] which
    focus on advancing signal noise ratio (SNR), do not take full advantage of a priori
    distribution of noises around vehicles. With the help of machine learning and
    deep learning methods, we can use a priori knowledge of the noises to improve
    the robustness of speech processing systems. For speech recognition task, deep-neural-network
    (DNN) can be applied to train an effective monophone classifier, instead of the
    traditional GMM based classifier. Moreover, the deep-neural-network hidden Markov
    model (DNN-HMM) speech recognition model can significantly improve the performance
    of Gaussian mixture model hidden Markov model (GMM-HMM) models [ 172– 174]. As
    shown in Figure 8, making full use of the self-adaption power of DNN, we can use
    the multitraining methods to improve the robustness of DNN monophone classifier
    by adding noise into the training data [ 89]. The experimental results in [ 89,
    175] show that the multitraining method can build a matched training and testing
    condition which can improve the accuracy of noisy speech recognition, especially
    for the prior knowledge of noise types that we can easily obtain in vehicles.    Figure
    8  Multitraining DNN [ 89]. As shown in Figure 9, a DNN can also be used to train
    a feature mapping network (FMN) which uses noisy features as input and corresponding
    clean features as training target. Enhanced features extracted by the FMN can
    improve the performance of speech recognition systems. Han et al. [ 176] used
    FMN to extract one enhanced Mel-frequency cepstral coefficient (MFCC) frame from
    15 noisy MFCCs frames. Xu et al. [ 90] built a FMN which learned the mapping from
    a log spectrogram to a log Mel filter bank. The enhanced feature can remarkably
    reduce the word error rate in speech recognition.    Figure 9  DNN used for feature
    mapping [ 90]. Besides getting the mapping feature directly, the DNN can also
    be used to train an ideal binary mask (IBM) which can be used to separate the
    clean speech from background noise as shown in Figure 10 [ 91, 177, 178]. With
    a priori knowledge of noise types and SNR, we can generate IBMs as training targets
    and use noisy power spectral as training data. In the test phase, we can use the
    learned IBMs to get enhanced features which can improve the robustness of speech
    recognition.    Figure 10  DNN used for IBMs learning [ 91]. In speaker verification
    tasks, the classical GMM based methods, for example, Gaussian mixture model universal
    background model (GMM-UBM) [ 179] and i-vector systems [ 180], need to build a
    background GMM, firstly, using a large quantity of speaker independent speeches.
    Then, by computing the statistics information on each GMM component of enrollment
    speakers, we can get speaker models or speaker i-vectors. However, a trained monophone
    classification DNN can replace the function of GMM by computing the statistics
    information on each monophone instead of on GMM components. Many published papers
    [ 181– 184] show that the DNN-i-vector based speaker verification systems work
    better than the GMM-i-vector method on detection accuracy and robustness. Unlike
    in the speech recognition tasks where the DNNs are used to get enhanced features
    from noisy features, researchers more prefer to use a DNN or convolutional neural
    network (CNN) to generate noise robustness bottleneck feature directly in speaker
    verification tasks [ 185– 187]. As shown in Figure 11, acoustic features or feature
    maps are used to train a DNN/CNN with a bottleneck layer which has less nodes
    and closes to the output layer. Speaker ID, noise types, monophone labels, or
    combination of these labels are used as training targets. Outputs of bottleneck
    layers include abundant differentiated information and can be used as speaker
    verification features which improve the performance of classical speaker verification
    methods such as the aforementioned GMM-UBM and i-vector. Similar to the multitraining
    method, adding noisy speeches into the training data can also improve the robustness
    of extracted bottleneck features [ 65, 92].    Figure 11  DNN/CNN used for extracting
    bottleneck feature [ 92]. Recently, some adversarial training methods are introduced
    to extract noise invariant bottleneck features [ 64, 188]. As shown in Figure
    12, the adversarial network includes two parts, i.e., an encoding network (EN)
    which can extract noise invariant features and a discriminative network (DN) which
    can judge noise types of the noise invariant feature generated from EN. Therefore,
    we can get robustness noise invariant features from EN which can improve the performance
    of speaker verification system by adversarial training these two parts in turn
    [ 64, 188].    Figure 12  Adversarial training network for noise invariant bottleneck
    feature extraction [ 64]. In conclusion, using DNN and machine learning methods
    can make full use of the MBD collected from the IoV systems. Moreover, it improves
    the performance of speech recognition and speaker verification methods applied
    in the voice interactive systems. 4. Conclusions and Future Challenges Although
    the machine learning-based methods introduced in Section 3 are widely applied
    in the MBD fields and obtain good performances in real data test, the present
    methods still need to be further developed. Therefore, five main challenges facing
    MBD analysis regarding the machine learning-based methods should be considered
    as follows. (1) Large-Scale and High-Speed M-Internet. Due to the growth of MIDs
    and high speed of M-Internet, increasingly various mobile data traffic is introduced
    and results in a heavy load to the wireless transmission system, which leads us
    to improve wireless communication technologies including WLAN and cellular mobile
    communication. In addition, the requirement of real-time services and applications
    depends on the development of machine learning-based MBD analysis methods towards
    high efficiency and precision. (2) Overfitting and Underfitting Problems. A benefit
    of MBD to machine learning and deep learning lies in the fact that the risk of
    overfitting becomes smaller with more and more data available for training [ 28].
    However, underfitting is another problem for the oversize data volume. In this
    condition, a larger model might be a better selection, while the model can express
    more hidden information of the data. Nevertheless, larger model which generally
    implies a deeper structure increases runtime of the model which affects the real-time
    performance. Therefore, the model size in machine learning and deep learning,
    which represents number of parameters, should be balanced to model performance
    and runtime. (3) Generalization Problem. As the massive scale of MBD, it is impossible
    to gain entire data even if they are only in a specific field. Therefore, the
    generalization ability which can be defined as suitable of different data subspace,
    or called scalability, of a trained machine learning or deep learning model is
    of great importance for evaluating the performance. (4) Cross-Modal Learning.
    The variety of MBD causes multiple modalities of data (for example, images, audios,
    personal location, web documents, and temperature) generated from multiple sensors
    (correspondingly, cameras, sound recorders, position sensor, and temperature sensor).
    Multimodal learning should learn from multimodal and heterogeneous input data
    with machine learning and deep learning [ 4, 189] and obtain hidden knowledge
    and meaningful patterns; however, it is quite difficult to discover. (5) Extended
    Channel Dimensions. The channel dimensions have been extended to three domains,
    i.e., space, time, and frequency, which means that the channel property is comprehensively
    discovered. Meanwhile, the increasing antenna number, high bandwidth, and various
    application scenarios bring the big data of channel measurements and estimations,
    especially for 5G. The finding channel characteristics need to be precisely described
    by more advanced channel modeling methodologies. In this paper, the applications
    and challenges of machine learning-based MBD analysis in the M-Internet have been
    reviewed and discussed. The development of MBD in various application scenarios
    requires more advanced data analysis technologies especially machine learning-based
    methods. Three typical applications of MBD analysis focus on wireless channel
    modeling, human online and offline behavior analysis, and speech recognition and
    verification in the Internet of Vehicles, respectively, and the machine learning-based
    methods used are widely applied in many other fields. In order to meet the aforementioned
    future challenges, three main study aims, i.e., accuracy, feasibility, and scalability
    [ 28], are highlighted for present and future MBD analysis research. In future
    work, accuracy improving will be also the primary task on the basis of a feasible
    architecture for MBD analysis. In addition, as the aforementioned discussion of
    the generalization problem, scalability has obtained more and more attentions
    especially in a classification or recognition problem where scalability also includes
    the increase in the number of inferred classes. It is of great importance to improve
    the scalability of the methods with the high accuracy and feasibility in order
    to face the analysis requirements of MBD. Conflicts of Interest The authors declare
    that they have no conflicts of interest. Acknowledgments This paper was supported
    in part by the National Natural Science Foundation of China (NSFC) [Grant no.
    61773071]; in part by the Beijing Nova Program Interdisciplinary Cooperation Project
    [Grant no. Z181100006218137]; in part by the Beijing Nova Program [Grant no. Z171100001117049];
    in part by the Beijing Natural Science Foundation (BNSF) [Grant no. 4162044];
    in part by the Funds of Beijing Laboratory of Advanced Information Networks of
    BUPT; in part by the Funds of Beijing Key Laboratory of Network System Architecture
    and Convergence of BUPT; and in part by BUPT Excellent Ph.D. Students Foundation
    [Grant no. XTCX201804]. References International Telecommunication Union (ITU),
    “ICT Facts and Figures 2017,” https://www.itu.int/en/ITU-D/Statistics/Pages/facts/default.aspx,
    2017. View at: Google Scholar Meeker, “Internet Trend 2017,” http://www.kpcb.com/internet-trends,
    2017. View at: Google Scholar G. Fettweis and S. Alamouti, “5G: personal mobile
    internet beyond what cellular did to telephony,” IEEE Communications Magazine,
    vol. 52, no. 2, pp. 140–145, 2014. View at: Publisher Site | Google Scholar M.
    A. Alsheikh, D. Niyato, S. Lin, H.-P. Tan, and Z. Han, “Mobile big data analytics
    using deep learning and apache spark,” IEEE Network, vol. 30, no. 3, pp. 22–29,
    2016. View at: Publisher Site | Google Scholar Cisco, “Cisco Visual Networking
    Index: Global Mobile Data Traffic Forecast Update, 2016-2021 White Paper,” https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/mobile-white-paper-c11-520862.html,
    2017. View at: Google Scholar Y. Guo, J. Zhang, and Y. Zhang, “An algorithm for
    analyzing the city residents'' activity information through mobile big data mining,”
    in Proceedings of the Joint 15th IEEE International Conference on Trust, Security
    and Privacy in Computing and Communications, 10th IEEE International Conference
    on Big Data Science and Engineering and 14th IEEE International Symposium on Parallel
    and Distributed Processing with Applications, IEEE TrustCom/BigDataSE/ISPA 2016,
    pp. 2133–2138, China, August 2016. View at: Google Scholar Z. Liao, Q. Yin, Y.
    Huang, and L. Sheng, “Management and application of mobile big data,” International
    Journal of Embedded Systems, vol. 7, no. 1, pp. 63–70, 2015. View at: Publisher
    Site | Google Scholar M. Agiwal, A. Roy, and N. Saxena, “Next generation 5G wireless
    networks: a comprehensive survey,” IEEE Communications Surveys & Tutorials, vol.
    18, no. 3, pp. 1617–1655, 2016. View at: Publisher Site | Google Scholar W. Li
    and Z. Zhou, “Learning to hash for big data: current status and future trends,”
    Chinese Science Bulletin (Chinese Version), vol. 60, no. 5-6, p. 485, 2015. View
    at: Publisher Site | Google Scholar V. Mayerschönberger and K. Cukier, Big Data:
    A Revolution That Will Transform How We Live, Work, and Think, Eamon Do-lan/Houghton
    Mifflin Harcourt, Boston, 2013. D. Z. Yazti and S. Krishnaswamy, “Mobile big data
    analytics: research, practice, and opportunities,” in Proceedings of the 15th
    IEEE International Conference on Mobile Data Management, IEEE MDM 2014, pp. 1-2,
    Australia, July 2014. View at: Google Scholar E. Zeydan, E. Bastug, M. Bennis
    et al., “Big data caching for networking: moving from cloud to edge,” IEEE Communications
    Magazine, vol. 54, no. 9, pp. 36–42, 2016. View at: Publisher Site | Google Scholar
    Z. Liu, Y. Qi, Z. Ma et al., “Sentiment analysis by exploring large scale web-based
    Chinese short text,” in Proceedings of the International Conference on Computer
    Science and Application Engineering (CSAE), pp. 21–23, 2017. View at: Google Scholar
    Z. Wang, Y. Qi, J. Liu, and Z. Ma, “User intention understanding from scratch,”
    in Proceedings of the 1st International Workshop on Sensing, Processing and Learning
    for Intelligent Machines, SPLINE 2016, Denmark, July 2016. View at: Google Scholar
    C. Zhang, Z. Si, Z. Ma, X. Xi, and Y. Yin, “Mining sequential update summarization
    with hierarchical text analysis,” Mobile Information Systems, vol. 2016, Article
    ID 1340973, 10 pages, 2016. View at: Publisher Site | Google Scholar C. Zhang,
    Y. Zhang, W. Xu, Z. Ma, Y. Leng, and J. Guo, “Mining activation force defined
    dependency patterns for relation extraction,” Knowledge-Based Systems, vol. 86,
    pp. 278–287, 2015. View at: Publisher Site | Google Scholar C. Zhang, W. Xu, Z.
    Ma, S. Gao, Q. Li, and J. Guo, “Construction of semantic bootstrapping models
    for relation extraction,” Knowledge-Based Systems, vol. 83, pp. 128–137, 2015.
    View at: Publisher Site | Google Scholar M. Jordan, “Message from the president:
    the era of big data,” ISBA Bull, vol. 18, pp. 1–3, 2011. View at: Google Scholar
    W. Chen, D. Wipf, Y. Wang, Y. Liu, and I. J. Wassell, “Simultaneous Bayesian sparse
    approximation with structured sparse models,” IEEE Transactions on Signal Processing,
    vol. 64, no. 23, pp. 6145–6159, 2016. View at: Publisher Site | Google Scholar
    | MathSciNet W. Chen, M. R. D. Rodrigues, and I. J. Wassell, “Projection design
    for statistical compressive sensing: a tight frame based approach,” IEEE Transactions
    on Signal Processing, vol. 61, no. 8, pp. 2016–2029, 2013. View at: Publisher
    Site | Google Scholar H. Yong, D. Meng, W. Zuo, and L. Zhang, “Robust online matrix
    factorization for dynamic background subtraction,” IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 2017. View at: Google Scholar Q. Xie, D. Zeng,
    Q. Zhao et al., “Robust low-dose CT sinogram prepocessing via exploiting noise-generating
    mechanism,” IEEE Transactions on Medical Imaging, vol. 36, no. 12, pp. 2487–2498,
    2017. View at: Google Scholar M. O''Connor, G. Zhang, W. B. Kleijn, and T. D.
    Abhayapala, “Function splitting and quadratic approximation of the primal-dual
    method of multipliers for distributed optimization over graphs,” IEEE Transactions
    on Signal and Information Processing over Networks, pp. 1–1, 2018. View at: Publisher
    Site | Google Scholar G. Zhang and R. Heusdens, “Distributed optimization using
    the primal-dual method of multipliers,” IEEE Transactions on Signal and Information
    Processing over Networks, vol. 4, no. 1, pp. 173–187, 2018. View at: Publisher
    Site | Google Scholar | MathSciNet G. Zhang and R. Heusdens, “Linear coordinate-descent
    message passing for quadratic optimization,” in Proceedings of the International
    Conference on Acoustics, Speech, and Signal Processing, pp. 20055–2008, 2012.
    View at: Publisher Site | Google Scholar | MathSciNet G. Zhang, R. Heusdens, and
    W. B. Kleijn, “Large scale LP decoding with low complexity,” IEEE Communications
    Letters, vol. 17, no. 11, pp. 2152–2155, 2013. View at: Publisher Site | Google
    Scholar Z. Ma, A. E. Teschendorff, A. Leijon, Y. Qiao, H. Zhang, and J. Guo, “Variational
    bayesian matrix factorization for bounded support data,” IEEE Transactions on
    Pattern Analysis and Machine Intelligence, vol. 37, no. 4, pp. 876–889, 2015.
    View at: Publisher Site | Google Scholar Z.-H. Zhou, N. V. Chawla, Y. Jin, and
    G. J. Williams, “Big data opportunities and challenges: discussions from data
    analytics perspectives,” IEEE Computational Intelligence Magazine, vol. 9, no.
    4, pp. 62–74, 2014. View at: Publisher Site | Google Scholar Y. LeCun, Y. Bengio,
    and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015.
    View at: Publisher Site | Google Scholar Y. Bengio and S. Bengio, “Modeling high-dimensional
    discrete data with multi-layer neural networks,” in Proceedings of the 13th Annual
    Neural Information Processing Systems Conference, NIPS 1999, pp. 400–406, USA,
    December 1999. View at: Google Scholar M. Ranzato, Y.-L. Boureau, and Y. Le Cun,
    “Sparse feature learning for deep belief networks,” in Advances in Neural Information
    Processing Systems, pp. 1185–1192, 2008. View at: Google Scholar G. E. Hinton,
    S. Osindero, and Y. Teh, “A fast learning algorithm for deep belief nets,” Neural
    Computation, vol. 18, no. 7, pp. 1527–1554, 2006. View at: Publisher Site | Google
    Scholar | MathSciNet Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy
    layer-wise training of deep networks,” in Proceedings of the 20th Annual Conference
    on Neural Information Processing Systems (NIPS ''06), pp. 153–160, Cambridge,
    Mass, USA, December 2006. View at: Google Scholar H. Larochelle, Y. Bengio, J.
    Louradour, and P. Lamblin, “Exploring strategies for training deep neural networks,”
    Journal of Machine Learning Research, vol. 10, pp. 1–40, 2009. View at: Google
    Scholar R. Salakhutdinov and G. Hinton, “Deep boltzmann machines,” in Proceedings
    of the International Conference on Artificial Intelligence and Statistics, vol.
    24, pp. 448–455, 2009. View at: Publisher Site | Google Scholar I. Goodfellow,
    H. Lee, and Q. V. Le, “Measuring invariances in deep networks,” Neural Information
    Processing Systems, pp. 646–654, 2009. View at: Google Scholar Y. Bengio and Y.
    LeCun, “Scaling learning algorithms towards, AI,” Large Scale Kernel Machines,
    vol. 34, pp. 321–360, 2007. View at: Google Scholar Y. Bengio, A. Courville, and
    P. Vincent, “Representation learning: a review and new perspectives,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798–1828, 2013.
    View at: Publisher Site | Google Scholar I. Arel, D. C. Rose, and T. P. Karnowski,
    “Deep machine learning—a new frontier in artificial intelligence research,” IEEE
    Computational Intelligence Magazine, vol. 5, no. 4, pp. 13–18, 2010. View at:
    Publisher Site | Google Scholar G. E. Dahl, D. Yu, L. Deng et al., “Context-dependent
    pre-trained deep neural networks for large-vocabulary speech recognition,” IEEE
    Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30–42,
    2012. View at: Google Scholar G. Hinton, L. Deng, D. Yu et al., “Deep neural networks
    for acoustic modeling in speech recognition: the shared views of four research
    groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012. View
    at: Publisher Site | Google Scholar R. Salakhutdinov, A. Mnih, and G. Hinton,
    “Restricted Boltzmann machines for collaborative filtering,” in Proceedings of
    the 24th International Conference on Machine learning (ICML ''07), vol. 227, pp.
    791–798, Corvallis, Oregon, June 2007. View at: Publisher Site | Google Scholar
    D. C. Cireşan, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Deep, big, simple
    neural nets for handwritten digit recognition,” Neural Computation, vol. 22, no.
    12, pp. 3207–3220, 2010. View at: Publisher Site | Google Scholar M. D. Zeiler,
    G. W. Taylor, and R. Fergus, “Adaptive deconvolutional networks for mid and high
    level feature learning,” in Proceedings of the 2011 IEEE International Conference
    on Computer Vision, ICCV 2011, pp. 2018–2025, Spain, November 2011. View at: Google
    Scholar A. Efrati, “How deep learning works at Apple, beyond,” https://www.theinformation.com/How-Deep-Learning-Works-at-Apple-Beyond,
    2013. View at: Google Scholar Z. Yang, B. Wu, K. Zheng, X. Wang, and L. Lei, “A
    survey of collaborative filtering-based recommender systems for mobile internet
    applications,” IEEE Access, vol. 4, pp. 3273–3287, 2016. View at: Publisher Site
    | Google Scholar K. Zhu, L. Zhang, and A. Pattavina, “Learning geographical and
    mobility factors for mobile application recommendation,” IEEE Intelligent Systems,
    vol. 32, no. 3, pp. 36–44, 2017. View at: Publisher Site | Google Scholar S. Jiang,
    B. Wei, T. Wang, Z. Zhao, and X. Zhang, “Big data enabled user behavior characteristics
    in mobile internet,” in Proceedings of the 2017 9th International Conference on
    Wireless Communications and Signal Processing (WCSP), pp. 1–5, Nanjing, October
    2017. View at: Publisher Site | Google Scholar J. Yang, Y. Qiao, X. Zhang, H.
    He, F. Liu, and G. Cheng, “Characterizing user behavior in mobile internet,” IEEE
    Transactions on Emerging Topics in Computing, vol. 3, no. 1, pp. 95–106, 2015.
    View at: Publisher Site | Google Scholar Y. Qiao, X. Zhao, J. Yang, and J. Liu,
    “Mobile big-data-driven rating framework: measuring the relationship between human
    mobility and app usage behavior,” IEEE Network, vol. 30, no. 3, pp. 14–21, 2016.
    View at: Publisher Site | Google Scholar Y. Qiao, J. Yang, H. He, Y. Cheng, and
    Z. Ma, “User location prediction with energy efficiency model in the Long Term-Evolution
    network,” International Journal of Communication Systems, vol. 29, no. 14, pp.
    2169–2187, 2016. View at: Publisher Site | Google Scholar M. Gerla and L. Kleinrock,
    “Vehicular networks and the future of the mobile internet,” Computer Networks,
    vol. 55, no. 2, pp. 457–469, 2011. View at: Publisher Site | Google Scholar M.
    M. Islam, M. A. Razzaque, M. M. Hassan, W. N. Ismail, and B. Song, “Mobile cloud-based
    big healthcare data processing in smart cities,” IEEE Access, vol. 5, pp. 11887–11899,
    2017. View at: Publisher Site | Google Scholar Texas Instruments, “Wireless Handset
    Solutions: Mobile Internet Device,” http://www.ti.com/solution/handset_smartphone,
    2008. View at: Google Scholar X. Ma, J. Zhang, Y. Zhang, and Z. Ma, “Data scheme-based
    wireless channel modeling method: motivation, principle and performance,” Journal
    of Communications and Information Networks, vol. 2, no. 3, pp. 41–51, 2017. View
    at: Publisher Site | Google Scholar X. Ma, J. Zhang, Y. Zhang, Z. Ma, and Y. Zhang,
    “A PCA-based modeling method for wireless MIMO channel,” in Proceedings of the
    2017 IEEE Conference on Computer Communications: Workshops (INFOCOM WKSHPS), pp.
    874–879, Atlanta, GA, May 2017. View at: Publisher Site | Google Scholar X. Zhang,
    Z. Yi, Z. Yan et al., “Social computing for mobile big data,” The Computer Journal,
    vol. 49, no. 9, pp. 86–90, 2016. View at: Publisher Site | Google Scholar K. Zhu,
    Z. Chen, L. Zhang, Y. Zhang, and S. Kim, “Geo-cascading and community-cascading
    in social networks: comparative analysis and its implications to edge caching,”
    Information Sciences, vol. 436-437, pp. 1–12, 2018. View at: Publisher Site |
    Google Scholar S. Gao, H. Luo, D. Chen et al., “A cross-domain recommendation
    model for cyber-physical systems,” IEEE Transactions on Emerging Topics in Computing,
    vol. 1, no. 2, pp. 384–393, 2013. View at: Publisher Site | Google Scholar Y.
    Qiao, Z. Xing, Z. M. Fadlullah, J. Yang, and N. Kato, “Characterizing flow, application,
    and user behavior in mobile networks: a framework for mobile big data,” IEEE Wireless
    Communications Magazine, vol. 25, no. 1, pp. 40–49, 2018. View at: Publisher Site
    | Google Scholar H. Yu, Z. Tan, Z. Ma, R. Martin, and J. Guo, “Spoofing detection
    in automatic speaker verification systems using DNN classifiers and dynamic acoustic
    features,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–12.
    View at: Publisher Site | Google Scholar H. Yu, Z.-H. Tan, Y. Zhang, Z. Ma, and
    J. Guo, “DNN filter bank cepstral coefficients for spoofing detection,” IEEE Access,
    vol. 5, pp. 4779–4787, 2017. View at: Publisher Site | Google Scholar Z. Ma, H.
    Yu, Z.-H. Tan, and J. Guo, “Text-independent speaker identification using the
    histogram transform model,” IEEE Access, vol. 4, pp. 9733–9739, 2016. View at:
    Publisher Site | Google Scholar H. Yu, Z.-H. Tan, Z. Ma, and J. Guo, “Adversarial
    network bottleneck features for noise robust speaker verification,” in Proceedings
    of the 18th Annual Conference of the International Speech Communication Association,
    INTERSPEECH 2017, pp. 1492–1496, Sweden, August 2017. View at: Google Scholar
    H. Yu, A. Sarkar, D. A. L. Thomsen, Z.-H. Tan, Z. Ma, and J. Guo, “Effect of multi-condition
    training and speech enhancement methods on spoofing detection,” in Proceedings
    of the 1st International Workshop on Sensing, Processing and Learning for Intelligent
    Machines, SPLINE 2016, Denmark, July 2016. View at: Google Scholar H. Yu, Z. Ma,
    and M. Li, “Histogram transform model Using MFCC features for text-independent
    speaker identification,” in Proceedings of the IEEE Asilomar Conference on Signals,
    Systems, pp. 500–504, 2014. View at: Google Scholar Z. Ma, J. Xie, H. Li et al.,
    “The role of data analysis in the development of intelligent energy networks,”
    IEEE Network, vol. 31, no. 5, pp. 88–95, 2017. View at: Publisher Site | Google
    Scholar Z. Ma, H. Li, Q. Sun, C. Wang, A. Yan, and F. Starfelt, “Statistical analysis
    of energy consumption patterns on the heat demand of buildings in district heating
    systems,” Energy and Buildings, vol. 85, pp. 464–472, 2014. View at: Publisher
    Site | Google Scholar D. West, “How mobile devices are transforming healthcare,”
    Issues in Technology Innovation, vol. 18, no. 1, pp. 1–11, 2012. View at: Google
    Scholar L. A. Tawalbeh, R. Mehmood, E. Benkhlifa, and H. Song, “Mobile cloud computing
    model and big data analysis for healthcare applications,” IEEE Access, vol. 4,
    pp. 6171–6180, 2016. View at: Publisher Site | Google Scholar S. Sagiroglu and
    D. Sinanc, “Big data: a review,” in Proceedings of the International Conference
    on Collaboration Technologies and Systems (CTS ''13), pp. 42–47, IEEE, San Diego,
    Calif, USA, May 2013. View at: Publisher Site | Google Scholar K. Zheng, L. Hou,
    H. Meng, Q. Zheng, N. Lu, and L. Lei, “Soft-defined heterogeneous vehicular network:
    architecture and challenges,” IEEE Network, vol. 30, no. 4, pp. 72–80, 2016. View
    at: Publisher Site | Google Scholar H. Hsieh, V. Klyuev, Q. Zhao, and S. Wu, “SVR-based
    outlier detection and its application to hotel ranking,” in Proceedings of the
    2014 IEEE 6th International Conference on Awareness Science and Technology (iCAST),
    pp. 1–6, Paris, France, October 2014. View at: Publisher Site | Google Scholar
    S. Rahman, M. Sathik, and K. Kannan, “Multiple linear regression models in outlier
    detection,” International Journal of Research in Computer Science, vol. 2, no.
    2, pp. 23–28, 2012. View at: Publisher Site | Google Scholar H. A. Dau, V. Ciesielski,
    and A. Song, “Anomaly Detection using replicator neural networks trained on examples
    of one class,” in Simulated Evolution and Learning, vol. 8886 of Lecture Notes
    in Computer Science, pp. 311–322, Springer International Publishing, Cham, 2014.
    View at: Publisher Site | Google Scholar Z. Ma, J.-H. Xue, A. Leijon, Z.-H. Tan,
    Z. Yang, and J. Guo, “Decorrelation of neutral vector variables: theory and applications,”
    IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 1, pp.
    129–143, 2018. View at: Publisher Site | Google Scholar | MathSciNet Z. Ma, S.
    Chatterjee, W. B. Kleijn, and J. Guo, “Dirichlet mixture modeling to estimate
    an empirical lower bound for LSF quantization,” Signal Processing, vol. 104, pp.
    291–295, 2014. View at: Publisher Site | Google Scholar Z. Ma and A. Leijon, “Bayesian
    estimation of beta mixture models with variational inference,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 33, no. 11, pp. 2160–2173,
    2011. View at: Publisher Site | Google Scholar C. C. Aggarwal, “Outlier analysis,”
    in Data Mining, Springer, 2015. View at: Google Scholar | MathSciNet Y. Demchenko,
    P. Grosso, C. de Laat, and P. Membrey, “Addressing big data issues in scientific
    data infrastructure,” in Proceedings of the IEEE International Conference on Collaboration
    Technologies and Systems (CTS ''13), pp. 48–55, May 2013. View at: Publisher Site
    | Google Scholar C. Zhou, H. Jiang, Y. Chen, L. Wu, and S. Yi, “User interest
    acquisition by adding home and work related contexts on mobile big data analysis,”
    in Proceedings of the IEEE INFOCOM 2016 - IEEE Conference on Computer Communications
    Workshops (INFOCOM WKSHPS), pp. 201–206, San Francisco, CA, USA, April 2016. View
    at: Publisher Site | Google Scholar X. Ge, H. Cheng, M. Guizani, and T. Han, “5G
    wireless backhaul networks: challenges and research advances,” IEEE Network, vol.
    28, no. 6, pp. 6–11, 2014. View at: Publisher Site | Google Scholar S. Landset,
    T. M. Khoshgoftaar, A. N. Richter, and T. Hasanin, “A survey of open source tools
    for machine learning with big data in the Hadoop ecosystem,” Journal of Big Data,
    vol. 2, no. 1, pp. 24–59, 2015. View at: Publisher Site | Google Scholar D. Soubra,
    “The 3Vs that define Big Data,” http://www.datasciencecentral.com/forum/topics/the-3vs-that-define-big-data,
    2012. View at: Google Scholar L. Ma, F. Nie, and Q. Lu, “An analysis of supply
    chain restructuring based on big data and mobile internet—a case study of warehouse-type
    supermarkets,” in Proceedings of the IEEE International Conference on Grey Systems
    and Intelligent Services, GSIS 2015, pp. 446–451, UK, August 2015. View at: Google
    Scholar A. McAfee and E. Brynjolfsson, “Big data: the management revolution,”
    Harvard Business Review, vol. 90, no. 10, pp. 60–128, 2012. View at: Google Scholar
    Y. Li, J. Zhang, and Z. Ma, “Clustering in wireless propagation channel with a
    statistics-based framework,” in Proceedings of the 2018 IEEE Wireless Communications
    and Networking Conference (WCNC), pp. 1–6, Barcelona, April 2018. View at: Publisher
    Site | Google Scholar P. Kazienko, K. Musiał, and T. Kajdanowicz, “Multidimensional
    social network in the social recommender system,” IEEE Transactions on Systems,
    Man, and Cybernetics: Systems, vol. 41, no. 4, pp. 746–759, 2011. View at: Publisher
    Site | Google Scholar A. Abe, K. Yamamoto, and S. Nakagawa, “Robust speech recognition
    using DNN-HMM acoustic model combining noise-aware training with spectral subtraction,”
    in Proceedings of the 16th Annual Conference of the International Speech Communication
    Association, INTERSPEECH 2015, pp. 2849–2853, Germany, September 2015. View at:
    Google Scholar Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “An experimental study
    on speech enhancement based on deep neural networks,” IEEE Signal Processing Letters,
    vol. 21, no. 1, pp. 65–68, 2014. View at: Publisher Site | Google Scholar A. Narayanan
    and D. Wang, “Ideal ratio mask estimation using deep neural networks for robust
    speech recognition,” in Proceedings of the 2013 38th IEEE International Conference
    on Acoustics, Speech, and Signal Processing, ICASSP 2013, pp. 7092–7096, Canada,
    May 2013. View at: Google Scholar D. Serdyuk, K. Audhkhasi, and P. Brakel, “Invariant
    representations for noisy speech recognition,” Computation and Language, 2016,
    arXiv:1612.01928. View at: Google Scholar P. E. Hart, “The condensed nearest neighbor
    rule,” IEEE Transactions on Information Theory, vol. 14, no. 3, pp. 515-516, 1968.
    View at: Publisher Site | Google Scholar G. Gates, “The reduced nearest neighbor
    rule,” IEEE Transactions on Information Theory, vol. 18, no. 3, pp. 431–433, 1972.
    View at: Publisher Site | Google Scholar H. Brighton and C. Mellish, “Advances
    in instance selection for instance-based learning algorithms,” Data Mining and
    Knowledge Discovery, vol. 6, no. 2, pp. 153–172, 2002. View at: Publisher Site
    | Google Scholar | MathSciNet Y. Li and L. Maguire, “Selecting critical patterns
    based on local geometrical and statistical information,” IEEE Transactions on
    Pattern Analysis and Machine Intelligence, vol. 33, no. 6, pp. 1189–1201, 2011.
    View at: Publisher Site | Google Scholar F. Angiulli, “Fast nearest neighbor condensation
    for large data sets classification,” IEEE Transactions on Knowledge and Data Engineering,
    vol. 19, no. 11, pp. 1450–1464, 2007. View at: Publisher Site | Google Scholar
    F. Angiulli and G. Folino, “Distributed nearest neighbor-based condensation of
    very large data sets,” IEEE Transactions on Knowledge and Data Engineering, vol.
    19, no. 12, pp. 1593–1606, 2007. View at: Publisher Site | Google Scholar M. I.
    Jordan, “Divide-and-conquer and statistical inference for big data,” in Proceedings
    of the the 18th ACM SIGKDD international conference, p. 4, Beijing, China, August
    2012. View at: Publisher Site | Google Scholar T. G. Kolda and J. Sun, “Scalable
    tensor decompositions for multi-aspect data mining,” in Proceedings of the 8th
    IEEE International Conference on Data Mining, ICDM 2008, pp. 363–372, Italy, December
    2008. View at: Google Scholar G. Wahba, “Dissimilarity data in statistical model
    building and machine learning,” in Proceedings of the 5th International Congress
    of Chinese Mathematicians, pp. 785–809, 2012. View at: Google Scholar | MathSciNet
    S. C. Hoi, J. Wang, P. Zhao, and R. Jin, “Online feature selection for mining
    big data,” in Proceedings of the 1st International Workshop on Big Data, Streams
    and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications,
    pp. 93–100, Beijing, China, August 2012. View at: Publisher Site | Google Scholar
    A. Sagheer, N. Tsuruta, R.-I. Taniguchi, D. Arita, and S. Maeda, “Fast feature
    extraction approach for multi-dimension feature space problems,” in Proceedings
    of the 18th International Conference on Pattern Recognition, ICPR 2006, pp. 417–420,
    China, August 2006. View at: Google Scholar J. R. Anaraki and M. Eftekhari, “Improving
    fuzzy-rough quick reduct for feature selection,” in Proceedings of the 2011 19th
    Iranian Conference on Electrical Engineering, ICEE 2011, Iran, May 2011. View
    at: Google Scholar I. A. Gheyas and L. S. Smith, “Feature subset selection in
    large dimensionality domains,” Pattern Recognition, vol. 43, no. 1, pp. 5–13,
    2010. View at: Publisher Site | Google Scholar K. W. Lau and Q. H. Wu, “Online
    training of support vector classifier,” Pattern Recognition, vol. 36, no. 8, pp.
    1913–1920, 2003. View at: Publisher Site | Google Scholar P. Laskov, C. Gehl,
    S. Krüger, and K.-R. Müller, “Incremental support vector learning: analysis, implementation
    and applications,” Journal of Machine Learning Research, vol. 7, pp. 1909–1936,
    2006. View at: Google Scholar | MathSciNet C. Chang and C. Lin, “LIBSVM: a Library
    for support vector machines,” ACM Transactions on Intelligent Systems and Technology,
    vol. 2, no. 3, article 27, 2011. View at: Publisher Site | Google Scholar K. Huang,
    H. Yang, I. King, and M. R. Lyu, “Maxi-min margin machine: learning large margin
    classifiers locally and globally,” IEEE Transactions on Neural Networks and Learning
    Systems, vol. 19, no. 2, pp. 260–272, 2008. View at: Publisher Site | Google Scholar
    A. Franco-Arcega, J. A. Carrasco-Ochoa, G. Snchez-Daz et al., “Building fast decision
    trees from large training sets,” Intelligent Data Analysis, vol. 16, no. 4, pp.
    649–664, 2012. View at: Google Scholar H. Yang and S. Fong, “Incrementally optimized
    decision tree for noisy big data,” in Proceedings of the 1st International Workshop
    on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming
    Models and Applications (BigMine ''12), pp. 36–44, Beijing, China, August 2012.
    View at: Publisher Site | Google Scholar Y. Ben-Haim and E. Tom-Tov, “A streaming
    parallel decision tree algorithm,” Journal of Machine Learning Research (JMLR),
    vol. 11, pp. 849–872, 2010. View at: Google Scholar | MathSciNet G. B. Huang,
    Q. Y. Zhu, and C. K. Siew, “Extreme learning machine: theory and applications,”
    Neurocomputing, vol. 70, no. 1–3, pp. 489–501, 2006. View at: Publisher Site |
    Google Scholar N. Liu and H. Wang, “Ensemble based extreme learning machine,”
    IEEE Signal Processing Letters, vol. 17, no. 8, pp. 754–757, 2010. View at: Publisher
    Site | Google Scholar Q. He, T. Shang, F. Zhuang, and Z. Shi, “Parallel extreme
    learning machine for regression based on mapReduce,” Neurocomputing, vol. 102,
    pp. 52–58, 2013. View at: Publisher Site | Google Scholar R. Zhang, Y. Lan, G.-B.
    Huang, and Z.-B. Xu, “Universal approximation of extreme learning machine with
    adaptive growth of hidden nodes,” IEEE Transactions on Neural Networks and Learning
    Systems, vol. 23, no. 2, pp. 365–371, 2012. View at: Publisher Site | Google Scholar
    H.-J. Rong, G.-B. Huang, N. Sundararajan, P. Saratchandran, and H.-J. Rong, “Online
    sequential fuzzy extreme learning machine for function approximation and classification
    problems,” IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics,
    vol. 39, no. 4, pp. 1067–1072, 2009. View at: Publisher Site | Google Scholar
    Y. Yang, Y. Wang, and X. Yuan, “Bidirectional extreme learning machine for regression
    problem and its learning effectiveness,” IEEE Transactions on Neural Networks
    and Learning Systems, vol. 23, no. 9, pp. 1498–1505, 2012. View at: Publisher
    Site | Google Scholar W. X. Chen and X. Lin, “Big data deep learning: challenges
    and perspectives,” IEEE Access, vol. 2, pp. 514–525, 2014. View at: Publisher
    Site | Google Scholar G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality
    of data with neural networks,” The American Association for the Advancement of
    Science: Science, vol. 313, no. 5786, pp. 504–507, 2006. View at: Publisher Site
    | Google Scholar | MathSciNet Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,
    “Gradient-based learning applied to document recognition,” Proceedings of the
    IEEE, vol. 86, no. 11, pp. 2278–2323, 1998. View at: Publisher Site | Google Scholar
    D. C. Ciresan, U. Meier, and J. Masci, “Flexible, high performance convolutional
    neural networks for image classification,” in Proceedings of the International
    Joint Conference on Artificial Intelligence, pp. 1237–1242, 2011. View at: Google
    Scholar D. Scherer, A. Müller, and S. Behnke, “Evaluation of pooling operations
    in convolutional architectures for object recognition,” in Proceedings of the
    International Conference on Artificial Neural Networks, pp. 92–101, 2010. View
    at: Google Scholar A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in Proceedings of the 26th Annual Conference
    on Neural Information Processing Systems (NIPS ''12), pp. 1097–1105, Lake Tahoe,
    Nev, USA, December 2012. View at: Google Scholar J. Dean, G. Corrado, and R. Monga,
    “Large scale distributed deep networks,” in Neural Information Processing Systems,
    pp. 1223–1231, 2012. View at: Google Scholar G. Papandreou, L.-C. Chen, K. P.
    Murphy, and A. L. Yuille, “Weakly-and semi-supervised learning of a deep convolutional
    network for semantic image segmentation,” in Proceedings of the 15th IEEE International
    Conference on Computer Vision, ICCV 2015, pp. 1742–1750, Chile, December 2015.
    View at: Google Scholar G. Hinton and R. Salakhutdinov, “Discovering binary codes
    for documents by learning deep generative models,” Topics in Cognitive Science,
    vol. 3, no. 1, pp. 74–91, 2011. View at: Publisher Site | Google Scholar R. Socher,
    C. C.-Y. Lin, C. D. Manning, and A. Y. Ng, “Parsing natural scenes and natural
    language with recursive neural networks,” in Proceedings of the 28th International
    Conference on Machine Learning (ICML ''11), pp. 129–136, Bellevue, Wash, USA,
    June 2011. View at: Google Scholar R. Kumar, J. O. Talton, and S. Ahmad, “Data-driven
    web design,” in Proceedings of the International Conference on Machine Learning,
    pp. 3-4, 2012. View at: Google Scholar R. Raina, A. Madhavan, and A. Y. Ng, “Large-scale
    deep unsupervised learning using graphics processors,” in Proceedings of the 26th
    International Conference On Machine Learning, ICML 2009, pp. 873–880, Canada,
    June 2009. View at: Google Scholar J. Martens, “Deep learning via Hessian-free
    optimization,” in Proceedings of the 27th International Conference on Machine
    Learning (ICML ''10), pp. 735–742, June 2010. View at: Google Scholar K. Zhang
    and X.-W. Chen, “Large-scale deep belief nets with mapreduce,” IEEE Access, vol.
    2, pp. 395–403, 2014. View at: Publisher Site | Google Scholar L. Deng, D. Yu,
    and J. Platt, “Scalable stacking and learning for building deep architectures,”
    in Proceedings of the 2012 IEEE International Conference on Acoustics, Speech,
    and Signal Processing (ICASSP ''12), pp. 2133–2136, Kyoto, Japan, March 2012.
    View at: Publisher Site | Google Scholar K. Kavukcuoglu, M. Ranzato, R. Fergus,
    and Y. LeCun, “Learning invariant features through topographic filter maps,” in
    Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition Workshops, CVPR Workshops 2009, pp. 1605–1612, USA, June 2009.
    View at: Google Scholar B. Hutchinson, L. Deng, and D. Yu, “Tensor deep stacking
    networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
    35, no. 8, pp. 1944–1957, 2013. View at: Publisher Site | Google Scholar J. Zhang,
    “Review of wideband MIMO channel measurement and modeling for IMT-Advanced systems,”
    Chinese Science Bulletin, vol. 57, no. 19, pp. 2387–2400, 2012. View at: Publisher
    Site | Google Scholar C. Liang, H. Li, Y. Li, S. Zhou, and J. Wang, “A learning-based
    channel model for synergetic transmission technology,” China Communications, vol.
    12, no. 9, pp. 83–92, 2015. View at: Publisher Site | Google Scholar J. Zhang,
    “The interdisciplinary research of big data and wireless channel: a cluster-nuclei
    based channel model,” China Communications, vol. 13, no. supplement 2, Article
    ID 7833457, pp. 14–26, 2016. View at: Publisher Site | Google Scholar Y. Li, J.
    Zhang, Z. Ma, and Y. Zhang, “Clustering analysis in the wireless propagation channel
    with a variational gaussian mixture model,” IEEE Transactions on Big Data, pp.
    1–1, 2018. View at: Publisher Site | Google Scholar F. Bai, T. Vidal-Calleja,
    and S. Huang, “Robust incremental SLAM under constrained optimization formulation,”
    IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 1–8, 2018. View at: Publisher
    Site | Google Scholar I. Z. Ibragimov and I. M. Afanasyev, “Comparison of ROS-based
    visual SLAM methods in homogeneous indoor environment,” in Proceedings of the
    2017 14th Workshop on Positioning, Navigation and Communications (WPNC), pp. 1–6,
    Bremen, October 2017. View at: Publisher Site | Google Scholar U. M. Fayyad, On
    the Induction of Decision Trees for Multiple Concept Learning, University of Michigan,
    1992. G. Cybenko, “Approximation by Superpositions of a sigmoidal function,” Mathematics
    of Control Signals & Systems, vol. 2, no. 4, pp. 303–314, 1989. View at: Google
    Scholar | MathSciNet Z. Ma, P. K. Rana, J. Taghia, M. Flierl, and A. Leijon, “Bayesian
    estimation of dirichlet mixture model with variational inference,” Pattern Recognition,
    vol. 47, no. 9, pp. 3143–3157, 2014. View at: Publisher Site | Google Scholar
    D. Naboulsi, M. Fiore, S. Ribot, and R. Stanica, “Large-scale mobile traffic analysis:
    a survey,” IEEE Communications Surveys & Tutorials, vol. 18, no. 1, pp. 124–161,
    2016. View at: Publisher Site | Google Scholar E. Halepovic and C. Williamson,
    “Characterizing and modeling user mobility in a cellular data network,” in Proceedings
    of the PE-WASUN''05 - Second ACM International Workshop on Performance Evaluation
    of Wireless Ad Hoc, Sensor, and Ubiquitous Networks, pp. 71–78, Canada, October
    2005. View at: Google Scholar S. Scepanovic, P. Hui, and A. Yla-Jaaski, “Revealing
    the pulse of human dynamics in a country from mobile phone data,” NetMob D4D Challenge,
    pp. 1–15, 2013. View at: Google Scholar S. Isaacman, R. A. Becker, and R. Caceres,
    “Identifying important places in people''s lives from cellular network data,”
    in Proceedings of the International Conference on Pervasive Computing, pp. 133–151,
    2011. View at: Google Scholar I. Trestian, S. Ranjan, A. Kuzmanovic, and A. Nucci,
    “Measuring serendipity: connecting people, locations and interests in a mobile
    3G network,” in Proceedings of the 2009 9th ACM SIGCOMM Internet Measurement Conference,
    IMC 2009, pp. 267–279, USA, November 2009. View at: Google Scholar C. Song, Z.
    Qu, N. Blumm, and A.-L. Barabási, “Limits of predictability in human mobility,”
    Science, vol. 327, no. 5968, pp. 1018–1021, 2010. View at: Publisher Site | Google
    Scholar Q. Lv, Y. Qiao, N. Ansari, J. Liu, and J. Yang, “Big data driven hidden
    markov model based individual mobility prediction at points of interest,” IEEE
    Transactions on Vehicular Technology, vol. 66, no. 6, pp. 5204–5216, 2017. View
    at: Publisher Site | Google Scholar Y. Qiao, Y. Cheng, J. Yang, J. Liu, and N.
    Kato, “A mobility analytical framework for big mobile data in densely populated
    area,” IEEE Transactions on Vehicular Technology, vol. 66, no. 2, pp. 1443–1455,
    2017. View at: Publisher Site | Google Scholar L. Meng, S. Liu, and A. Striegel,
    “Analyzing the longitudinal impact of proximity, location, and personality on
    smartphone usage,” Computational Social Networks, vol. 1, no. 1, 2014. View at:
    Publisher Site | Google Scholar M. Böhmer, B. Hecht, J. Schöning, A. Krüger, and
    G. Bauer, “Falling asleep with angry birds, facebook and kindle: a large scale
    study on mobile application usage,” in Proceedings of the 13th International Conference
    on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ''11),
    pp. 47–56, September 2011. View at: Publisher Site | Google Scholar D. Hristova,
    M. Musolesi, and C. Mascolo, “Keep your friends close and your facebook friends
    closer: a multiplex network approach to the analysis of offline and online social
    Ties,” in Proceedings of the 8th International Conference on Weblogs and Social
    Media, ICWSM 2014, pp. 206–215, USA, June 2014. View at: Google Scholar R. I.
    M. Dunbar, V. Arnaboldi, M. Conti, and A. Passarella, “The structure of online
    social networks mirrors those in the offline world,” Social Networks, vol. 43,
    pp. 39–47, 2015. View at: Publisher Site | Google Scholar D. Hristova, M. J. Williams,
    M. Musolesi, P. Panzarasa, and C. Mascolo, “Measuring urban social diversity using
    interconnected geo-social networks,” in Proceedings of the the 25th International
    Conference, pp. 21–30, Canada, April 2016. View at: Publisher Site | Google Scholar
    D. Hristova, A. Noulas, C. Brown, M. Musolesi, and C. Mascolo, “A multilayer approach
    to multiplexity and link prediction in online geo-social networks,” EPJ Data Science,
    vol. 5, no. 1, 2016. View at: Publisher Site | Google Scholar Apache, “Apache
    software foundation,” http://apache.org, 2017. View at: Google Scholar M. Gerla,
    E.-K. Lee, G. Pau, and U. Lee, “Internet of vehicles: from intelligent grid to
    autonomous cars and vehicular clouds,” in Proceedings of the IEEE World Forum
    on Internet of Things (WF-IoT ''14), pp. 241–246, March 2014. View at: Publisher
    Site | Google Scholar F. Yang, S. Wang, J. Li, Z. Liu, and Q. Sun, “An overview
    of internet of vehicles,” China Communications, vol. 11, no. 10, pp. 1–15, 2014.
    View at: Publisher Site | Google Scholar K. M. Alam, M. Saini, and A. El Saddik,
    “Toward social internet of vehicles: concept, architecture, and applications,”
    IEEE Access, vol. 3, pp. 343–357, 2015. View at: Publisher Site | Google Scholar
    J. D. Lee, B. Caven, S. Haake, and T. L. Brown, “Speech-based interaction with
    in-vehicle computers: the effect of speech-based e-mail on drivers'' attention
    to the roadway,” Human Factors: The Journal of the Human Factors and Ergonomics
    Society, vol. 43, no. 4, pp. 631–640, 2001. View at: Publisher Site | Google Scholar
    C. Y. Loh, K. L. Boey, and K. S. Hong, “Speech recognition interactive system
    for vehicle,” in Proceedings of the 13th IEEE International Colloquium on Signal
    Processing and its Applications, CSPA 2017, pp. 85–88, Malaysia, March 2017. View
    at: Google Scholar D. Amodei, S. Ananthanarayanan, and R. Anubhai, “Deep speech
    2: end-to-end speech recognition in English and mandarin,” in Proceedings of the
    International Conference on Machine Learning, pp. 173–182, 2016. View at: Google
    Scholar E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-Dominguez,
    “Deep neural networks for small footprint text-dependent speaker verification,”
    in Proceedings of the 2014 IEEE International Conference on Acoustics, Speech,
    and Signal Processing, ICASSP 2014, pp. 4052–4056, Italy, May 2014. View at: Google
    Scholar K. Chen and A. Salman, “Learning speaker-specific characteristics with
    a deep neural architecture,” IEEE Transactions on Neural Networks and Learning
    Systems, vol. 22, no. 11, pp. 1744–1756, 2011. View at: Publisher Site | Google
    Scholar L. Deng, G. E. Hinton, and B. Kingsbury, “New types of deep neural network
    learning for speech recognition and related applications: an overview,” in Proceedings
    of the 38th IEEE International Conference on Acoustics, Speech, and Signal Processing
    (ICASSP ''13), pp. 8599–8603, IEEE, Vancouver, Canada, May 2013. View at: Publisher
    Site | Google Scholar A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition
    with deep recurrent neural networks,” in Proceedings of the 38th IEEE International
    Conference on Acoustics, Speech, and Signal Processing (ICASSP ''13), pp. 6645–6649,
    May 2013. View at: Publisher Site | Google Scholar J. Meyer and K. U. Simmer,
    “Multi-channel speech enhancement in a car environment using Wiener filtering
    and spectral subtraction,” in Proceedings of the 1997 IEEE International Conference
    on Acoustics, Speech, and Signal Processing, ICASSP. Part 1 (of 5), pp. 1167–1170,
    April 1997. View at: Google Scholar R. C. Hendriks, R. Heusdens, and J. Jensen,
    “MMSE based noise PSD tracking with low complexity,” in Proceedings of the 2010
    IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP
    2010, pp. 4266–4269, USA, March 2010. View at: Google Scholar F. Seide, G. Li,
    and D. Yu, “Conversational speech transcription using context-dependent deep neural
    networks,” in Proceedings of the 12th Annual Conference of the International Speech
    Communication Association (INTERSPEECH ''11), vol. 33, pp. 437–440, August 2011.
    View at: Google Scholar H. Ze, A. Senior, and M. Schuster, “Statistical parametric
    speech synthesis using deep neural networks,” in Proceedings of the 38th IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP ''13),
    pp. 7962–7966, IEEE, Vancouver, Canada, May 2013. View at: Publisher Site | Google
    Scholar G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks
    for LVCSR using rectified linear units and dropout,” in Proceedings of the 38th
    IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP
    ''13), pp. 8609–8613, May 2013. View at: Publisher Site | Google Scholar Y. Qian,
    M. Bi, T. Tan, and K. Yu, “Very deep convolutional neural networks for noise robust
    speech recognition,” IEEE/ACM Transactions on Audio, Speech and Language Processing,
    vol. 24, no. 12, pp. 2263–2276, 2016. View at: Publisher Site | Google Scholar
    K. Han, Y. He, D. Bagchi et al., “Deep neural network based spectral feature mapping
    for robust speech recognition,” INTERSPEECH, pp. 2484–2488, 2015. View at: Google
    Scholar B. Li and K. C. Sim, “Improving robustness of deep neural networks via
    spectral masking for automatic speech recognition,” in Proceedings of the 2013
    IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013, pp.
    279–284, Czech Republic, December 2013. View at: Google Scholar Y. Xu, J. Du,
    L.-R. Dai, and C.-H. Lee, “A regression approach to speech enhancement based on
    deep neural networks,” IEEE/ACM Transactions on Audio, Speech and Language Processing,
    vol. 23, no. 1, pp. 7–19, 2015. View at: Publisher Site | Google Scholar D. A.
    Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker verification using adapted
    Gaussian mixture models,” Digital Signal Processing, vol. 10, no. 1, pp. 19–41,
    2000. View at: Publisher Site | Google Scholar N. Dehak, P. J. Kenny, R. Dehak,
    P. Dumouchel, and P. Ouellet, “Front-end factor analysis for speaker verification,”
    IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp.
    788–798, 2011. View at: Publisher Site | Google Scholar O. Abdel-Hamid, A.-R.
    Mohamed, H. Jiang, and G. Penn, “Applying convolutional neural networks concepts
    to hybrid NN-HMM model for speech recognition,” in Proceedings of the IEEE International
    Conference on Acoustics, Speech, and Signal Processing (ICASSP ''12), pp. 4277–4280,
    IEEE, March 2012. View at: Publisher Site | Google Scholar M. McLaren, Y. Lei,
    and L. Ferrer, “Advances in deep neural network approaches to speaker recognition,”
    in Proceedings of the 40th IEEE International Conference on Acoustics, Speech,
    and Signal Processing, ICASSP 2015, pp. 4814–4818, Australia, April 2014. View
    at: Google Scholar C. Yu, A. Ogawa, M. Delcroix, T. Yoshioka, T. Nakatani, and
    J. H. L. Hansen, “Robust i-vector extraction for neural network adaptation in
    noisy environment,” in Proceedings of the 16th Annual Conference of the International
    Speech Communication Association, INTERSPEECH 2015, pp. 2854–2857, Germany, September
    2015. View at: Google Scholar N. Li, M.-W. Mak, and J.-T. Chien, “Deep neural
    network driven mixture of PLDA for robust i-vector speaker verification,” in Proceedings
    of the 2016 IEEE Workshop on Spoken Language Technology, SLT 2016, pp. 186–191,
    USA, December 2016. View at: Google Scholar Z. Zhang, L. Wang, A. Kai, T. Yamada,
    W. Li, and M. Iwahashi, “Deep neural network-based bottleneck feature and denoising
    autoencoder-based dereverberation for distant-talking speaker identification,”
    EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, p. 12,
    2015. View at: Google Scholar M. McLaren, Y. Lei, and N. Scheffer, “Application
    of convolutional neural networks to speaker recognition in noisy conditions,”
    in Proceedings of the Fifteenth Annual Conference of the International Speech
    Communication Association, 2014. View at: Google Scholar T. N. Sainath, B. Kingsbury,
    and B. Ramabhadran, “Auto-encoder bottleneck features using deep belief networks,”
    in Proceedings of the 2012 IEEE International Conference on Acoustics, Speech,
    and Signal Processing, ICASSP 2012, pp. 4153–4156, Japan, March 2012. View at:
    Google Scholar Y. Shinohara, “Adversarial multi-task learning of deep neural networks
    for robust speech recognition,” INTERSPEECH, pp. 2369–2372, 2016. View at: Google
    Scholar N. D. Lane and P. Georgiev, “Can deep learning revolutionize mobile sensing?”
    in Proceedings of the the 16th International Workshop, pp. 117–122, Santa Fe,
    NM, USA, Feburary 2015. View at: Publisher Site | Google Scholar Copyright Copyright
    © 2018 Jiyang Xie et al. This is an open access article distributed under the
    Creative Commons Attribution License, which permits unrestricted use, distribution,
    and reproduction in any medium, provided the original work is properly cited.
    PDF Download Citation Download other formats Order printed copies Views 6447 Downloads
    2334 Citations 36 About Us Contact us Partnerships Blog Journals Article Processing
    Charges Print editions Authors Editors Reviewers Partnerships Hindawi XML Corpus
    Open Archives Initiative Fraud prevention Follow us: Privacy PolicyTerms of ServiceResponsible
    Disclosure PolicyCookie PolicyCopyrightModern slavery statementCookie Preferences'
  inline_citation: '>'
  journal: Wireless Communications and Mobile Computing
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Survey on Machine Learning-Based Mobile Big Data Analysis: Challenges
    and Applications'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/app10249112
  analysis: '>'
  authors:
  - Соломія Федушко
  - Taras Ustyianovych
  - Yuriy Syerov
  - Tomáš Peráček
  citation_count: 19
  full_citation: '>'
  full_text: ">\napplied  \nsciences\nArticle\nUser-Engagement Score and SLIs/SLOs/SLAs\n\
    Measurements Correlation of E-Business Projects\nThrough Big Data Analysis\nSolomiia\
    \ Fedushko 1,*\n, Taras Ustyianovych 1\n, Yuriy Syerov 1\nand Tomas Peracek 2\n\
    1\nSocial Communication and Information Activity Department, Lviv Polytechnic\
    \ National University,\n79000 Lviv, Ukraine; taras.ustyianovych.dk.2017@lpnu.ua\
    \ (T.U.); yurii.o.sierov@lpnu.ua (Y.S.)\n2\nFaculty of Management, Comenius University\
    \ in Bratislava, 814 99 Bratislava, Slovakia;\ntomas.peracek@fm.uniba.sk\n*\n\
    Correspondence: solomiia.s.fedushko@lpnu.ua\nReceived: 25 November 2020; Accepted:\
    \ 17 December 2020; Published: 20 December 2020\n\x01\x02\x03\x01\x04\x05\x06\a\
    \b\x01\n\x01\x02\x03\x04\x05\x06\a\nAbstract: The Covid-19 crisis lockdown caused\
    \ rapid transformation to remote working/learning\nmodes and the need for e-commerce-,\
    \ web-education-related projects development, and maintenance.\nHowever, an increase\
    \ in internet traﬃc has a direct impact on infrastructure and software performance.\n\
    We study the problem of accurate and quick web-project infrastructure issues/bottleneck/overload\n\
    identiﬁcation. The research aims to achieve and ensure the reliability and availability\
    \ of a commerce/\neducational web project by providing system observability and\
    \ Site Reliability Engineering (SRE)\nmethods. In this research, we propose methods\
    \ for technical condition assessment by applying the\ncorrelation of user-engagement\
    \ score and Service Level Indicators (SLIs)/Service Level Objectives\n(SLOs)/Service\
    \ Level Agreements (SLAs) measurements to identify user satisfaction types along\n\
    with the infrastructure state. Our solution helps to improve content quality and,\
    \ mainly, detect\nabnormal system behavior and poor infrastructure conditions.\
    \ A straightforward interpretation of\npotential performance bottlenecks and vulnerabilities\
    \ is achieved with the developed contingency\ntable and correlation matrix for\
    \ that purpose. We identify big data and system logs and metrics as the\ncentral\
    \ sources that have performance issues during web-project usage. Throughout the\
    \ analysis of\nan educational platform dataset, we found the main features of\
    \ web-project content that have high\nuser-engagement and provide value to services’\
    \ customers. According to our study, the usage and\ncorrelation of SLOs/SLAs with\
    \ other critical metrics, such as user satisfaction or engagement improves\nearly\
    \ indication of potential system issues and avoids having users face them. These\
    \ ﬁndings\ncorrespond to the concepts of SRE that focus on maintaining high service\
    \ availability.\nKeywords:\neducational web-projects; real-time data analysis;\
    \ big data; system performance;\nApplication Performance Monitoring (APM); business-plan;\
    \ strategy; Service Level Objectives\n(SLO); Service Level Agreement (SLA); Service\
    \ Level Indicator (SLI)\n1. Introduction\nHardware and software technical assessment\
    \ of an educational web project in the face of the\nincreased need for their use\
    \ not only creates many challenges, but also requires fast and objective\ndata-driven\
    \ operations and decisions. This need is especially relevant during the Covid-19\
    \ crisis,\nas it forces educational and business institutions to make the necessary\
    \ migration to the online mode.\nEducational institutions have faced the need\
    \ to provide teachers with a ﬂexible IT infrastructure that\nempowers eﬃcient\
    \ deployment of educational materials and courses both in regular times and in\
    \ a\nstate of emergency [1]. Educational institutions’ activities become almost\
    \ impossible and less valuable\nAppl. Sci. 2020, 10, 9112; doi:10.3390/app10249112\n\
    www.mdpi.com/journal/applsci\nAppl. Sci. 2020, 10, 9112\n2 of 16\nwithout the\
    \ availability of electronic educational web projects, in comparison to modern\
    \ competitive\norganizations that provide similar services. Many universities\
    \ in Ukraine and around the world are\nalready eﬀectively using online learning\
    \ as one of the leading strategies for building and developing\neducational institution\
    \ services. In addition, it was proven that the use of web-based resources for\n\
    online learning is more eﬀective compared to traditional learning methods [2].\n\
    The transition to the online mode allows eﬃcient and user-friendly use of e-learning\
    \ technologies,\nwhich are deﬁned as eﬀective multimedia learning using e-educational\
    \ technology [3]. Thus, we state\nthat e-learning is a component of educational\
    \ technology, the further development of which is\ncurrently underway by many\
    \ technical giants of the business world. In particular, the article by R.\nLakshminarayanan,\
    \ B. Kumar, and M. Raju [4] considered how companies oﬀering cloud services and\n\
    technologies, in particular, Amazon Web Services (AWS), Microsoft, and Google\
    \ allow educational\ninstitutions to take advantage of certain products. In the\
    \ study, a comparative and features analysis of\ncertain product usage was made.\n\
    The course of events related to the Covid-19 pandemic outbreak has led to a closer\
    \ look at the\nneed for digitalization and the establishment of so-called e-learning\
    \ within certain institutions and\norganizations. We display in Figure 1, which\
    \ was generated using the Google Trends service [5],\nthat with the onset of the\
    \ Covid-19 pandemic, the increased rates of searches related to online learning\n\
    projects vary from 15% to 43%. The values show the popularity of a search term\
    \ relative to the highest\npoint on the graph for a particular region and time\
    \ period. 100 is the peak popularity of a search term,\nwhile 50 means that the\
    \ popularity of the term is half as popular during the speciﬁed period.\nAppl.\
    \ Sci. 2020, 10, x FOR PEER REVIEW \n2 of 16 \nwithout the availability of electronic\
    \ educational web projects, in comparison to modern competitive \norganizations\
    \ that provide similar services. Many universities in Ukraine and around the world\
    \ are \nalready effectively using online learning as one of the leading strategies\
    \ for building and developing \neducational institution services. In addition,\
    \ it was proven that the use of web-based resources for \nonline learning is more\
    \ effective compared to traditional learning methods [2]. \nThe transition to\
    \ the online mode allows efficient and user-friendly use of e-learning \ntechnologies,\
    \ which are defined as effective multimedia learning using e-educational technology\
    \ [3]. \nThus, we state that e-learning is a component of educational technology,\
    \ the further development of \nwhich is currently underway by many technical giants\
    \ of the business world. In particular, the article \nby R. Lakshminarayanan,\
    \ B. Kumar, and M. Raju [4] considered how companies offering cloud \nservices\
    \ and technologies, in particular, Amazon Web Services (AWS), Microsoft, and Google\
    \ allow \neducational institutions to take advantage of certain products. In the\
    \ study, a comparative and \nfeatures analysis of certain product usage was made.\
    \ \nThe course of events related to the Covid-19 pandemic outbreak has led to\
    \ a closer look at the \nneed for digitalization and the establishment of so-called\
    \ e-learning within certain institutions and \norganizations. We display in Figure\
    \ 1, which was generated using the Google Trends service [5], that \nwith the\
    \ onset of the Covid-19 pandemic, the increased rates of searches related to online\
    \ learning \nprojects vary from 15% to 43%. The values show the popularity of\
    \ a search term relative to the highest \npoint on the graph for a particular\
    \ region and time period. 100 is the peak popularity of a search term, \nwhile\
    \ 50 means that the popularity of the term is half as popular during the specified\
    \ period. \n \nFigure 1. Google Trends statistics of online learning services\
    \ search and usage. \nHowever, many educational institutions have not been sufficiently\
    \ prepared for the switching to \nonline task mode due to various reasons:. \n\
    1. \nlack of internal infrastructure or subscriptions to external online projects\
    \ to provide educational \nservices; \n2. \ninsufficient reliability of institutions’\
    \ infrastructural and technical support; \n3. \ncomplete or partial lack of the\
    \ necessary teaching materials and resources for online classes; \n4. \nthe lack\
    \ of the university’s strategy for web projects implementation to support students’\
    \ e-\nlearning needs. \nIn this article, we decided to solve the following problem:\
    \ web project technical control and \nevaluation of an educational institution\
    \ or e-commerce, which faces the problem of high load on \nhardware and web-software\
    \ during remote learning and work; identify the cause-and-effect \nrelationships\
    \ of particular infrastructure type issues and improve site reliability methods\
    \ usage. The \nFigure 1. Google Trends statistics of online learning services\
    \ search and usage.\nHowever, many educational institutions have not been suﬃciently\
    \ prepared for the switching to\nonline task mode due to various reasons:\n1.\n\
    lack of internal infrastructure or subscriptions to external online projects to\
    \ provide\neducational services;\n2.\ninsuﬃcient reliability of institutions’\
    \ infrastructural and technical support;\n3.\ncomplete or partial lack of the\
    \ necessary teaching materials and resources for online classes;\n4.\nthe lack\
    \ of the university’s strategy for web projects implementation to support students’\n\
    e-learning needs.\nAppl. Sci. 2020, 10, 9112\n3 of 16\nIn this article, we decided\
    \ to solve the following problem: web project technical control and\nevaluation\
    \ of an educational institution or e-commerce, which faces the problem of high\
    \ load\non hardware and web-software during remote learning and work; identify\
    \ the cause-and-eﬀect\nrelationships of particular infrastructure type issues\
    \ and improve site reliability methods usage.\nThe need for clear external or\
    \ internal root-cause problem identiﬁcation is essential because this will\nallow\
    \ the formation of qualitative Service Level Objectives (SLOs).\nThe study objectives\
    \ are as follows: (1) increase infrastructure visibility through correlation\n\
    and use of a user-engagement score with Service Level Objectives (SLOs)/Service\
    \ Level Agreement\n(SLA)/Service Level Indicator (SLI) as the main indicators\
    \ of the web project technical equipment quality;\n(2) improve methods for data\
    \ analysis of virtual environment performance metrics; (3) interactively\nmonitor\
    \ various web service processes and evaluate the technical characteristics of\
    \ servers, applications,\netc. based on indicators and goals; (4) provide statistical\
    \ methods for meaningful review of web\nproject goals based on metrics coming\
    \ from the data sources in order to improve all the processes\ndescribed above;\
    \ (5) develop methods to increase availability and reliability of educational\
    \ web projects;\n(6) use metrics in real-time to indicate system resource shortages\
    \ and bottlenecks instead of critical\nuser responses; (7) increase the eﬃciency\
    \ of a university’s online educational service and objectively\ncreate the requirements\
    \ for scaling, creating eﬀective solutions and architectures through monitoring;\n\
    (8) improve decision-making processes regarding the architecture and IT operations\
    \ of an online\neducational project. Consequently, a project designed for e-learning,\
    \ as well as the particular university\nor institution’s educational technology\
    \ must follow data-driven decision making.\n2. Literature Review\nMany methodologies\
    \ and frameworks have already been developed for technical assessments and\nmonitoring.\
    \ For example, M. Bashirov’s study used mathematical modeling and an electromagnetic-\n\
    acoustic eﬀect to determine the defect and reliability of pipelines. The predictive\
    \ model usage at an\nearly stage is determined to increase the probability of\
    \ defect identiﬁcation [6]. A certain percentage\nof web-project technical equipment\
    \ (hardware) or software in e-learning or e-commerce is outdated,\nin which case\
    \ its monitoring and reliability assessment is diﬃcult and limited. The main reason\n\
    is that there are diﬃculties in establishing connection with the legacy devices\
    \ themselves; fewer\nmetrics and logs are generated, and data processing and transfers\
    \ are slower compared to modern\nIT solutions. A study by Bednarski et al. showed\
    \ the results of the historical structure’s technical\ncondition assessment. Physical\
    \ quantity measurements were important for the crack state of a historic\nchurch\
    \ in Jangrot, the assessment allowed identifying the kind of components which\
    \ needed to be\nreplaced. The study mainly emphasized the need for environmental\
    \ data collection and its correlation\nwith information about a particular equipment\
    \ unit’s condition and reliability. The need for advances\nin technical measurement\
    \ data collection and the development of new means for quick data ingestion\n\
    was emphasized as well [7–9]. The most focus is on the problem of infrastructure\
    \ insecurity and\nunreliability, which can lead to web project instability and\
    \ the inability to provide educational services\nby institutions in real-time.\
    \ The educational web projects’ reliability and availability is a prerequisite\n\
    for providing qualitative services in higher education institutions and schools.\
    \ It plays a particularly\nimportant role in isolation and quarantine conditions,\
    \ for example, Covid-19, as educational institutions\nare transitioning to online\
    \ teaching and e-learning. An equally important problem faced by educational\n\
    institutions during the period of abnormal load on the online project technology\
    \ infrastructure is the\nlack of visibility and real-time monitoring, which makes\
    \ it impossible to make objective decisions\nregarding system scaling and troubleshooting.\
    \ Accordingly, if we take into account the technological\nneeds and scale of a\
    \ particular online project, the need for monitoring, site reliability engineering,\n\
    and operational intelligence methods becomes increasingly clear [10]. Belforte\
    \ et al. provide an\nexample distributed across more than 60 computing centers\
    \ worldwide, with CMS management and\nmonitoring using custom and traditional\
    \ machine reliability metrics. In addition, an algorithm to\nautomate the performance\
    \ of distributed resources is described, which is very valuable for an online\n\
    Appl. Sci. 2020, 10, 9112\n4 of 16\nsystem that uses load-balancing between clusters\
    \ [11]. The implementation of this algorithm can\nfacilitate and improve monitoring\
    \ processes.\nA correct SLA deﬁnition allows a strong user/customer understanding.\
    \ Operational intelligence\nand real-time monitoring techniques involve the implementation\
    \ of user behavior analytics.\nThis process, in turn, allows not only understanding\
    \ the users and monitoring their behavior in\nreal-time, but also to determine\
    \ certain performance indicators for an individual user, to set objective\nSLAs.\
    \ In the article by Alﬁan et al., big data methods were used to collect browsing\
    \ history and\ntransaction data for real-time analysis of user behavior interacting\
    \ with e-services in diﬀerent locations.\nThis study allows web projects to improve\
    \ service quality and to establish the optimal service level\nagreement, which\
    \ will be beneﬁcial for all parties involved. Equally essential is the real-time\
    \ monitoring\nof personalized diabetic patients’ health. The study is valuable\
    \ because it reﬂects the use of the Bluetooth\nLow Energy method to reduce the\
    \ cost of data collection, as well as to provide high-quality advice\nto patients.\
    \ Machine learning predicts the likelihood of detecting diabetes in patients based\
    \ on the\ncollected data. This technical solution can be used not only for medical\
    \ data collection, but also for\nthe equipment and infrastructure data transmission\
    \ to a centralized logging environment [12,13].\nWeb-analytics tools are used\
    \ for real-time mouse tracking, which helps to collect data about users\nand their\
    \ interaction with an e-service. Accordingly, the data correlate with transactions\
    \ and queries’\ncompletion rates, which are necessary to monitor availability\
    \ and reliability. The article by Cegan and\nFilip describes how user behavior\
    \ monitoring allows detecting bottlenecks in the web environment,\nas well as\
    \ in technical equipment and infrastructure components. The authors propose a\
    \ new method\nfor collecting mouse-clicking data based on real-time data transformation\
    \ to convert discrete position\ndata to system functions to optimize compression\
    \ and analysis [14]. The usage of functional tests\nto assess the infrastructure\
    \ and site reliability as a way to validate site operations and optimization\n\
    techniques is described in Elmsheuser et al.’s study [15].\nMachine learning and\
    \ artiﬁcial intelligence are also widely applied for technical condition\nassessment.\
    \ A recent study conducted by Kaminski et al. showed the usage of artiﬁcial neural\n\
    networks, namely the multilayer perceptron, to assess the technical condition\
    \ of a water supply system.\nThe results proved that the use of artiﬁcial intelligence\
    \ in such tasks can increase the eﬃciency of\ndetecting defects in pipes with\
    \ a distributed water supply chain and is an example of human-machine\ninteraction\
    \ [16]. An improved support function machine model indicated that the pattern\
    \ recognition\nmethod based on an improved kernel function support vector machine\
    \ is eﬃcient for validating\ntechnical conditions of the recoil mechanism [17].\
    \ The embedded system’s usage greatly simpliﬁes\nreal-time monitoring, as it allows\
    \ more metrics to collect than are collected during traditional system\nmonitoring.\
    \ Studies by Bosse and Lehmhus provide a model for data collection using a structural\n\
    monitoring and tactile sensing system to obtain data from the lowest system levels.\
    \ It allows an IT team\nto assess and monitor the technical condition of the equipment\
    \ with great accuracy [18]. This research\nis crucial for our study, because in\
    \ order to form new metrics and ﬁnd infrastructure issues’ causal\nrelationships,\
    \ and create an optimal SLO/SLA deﬁnition, it is necessary to collect data from\
    \ all possible\nlevels of web project applications.\nDespite the growing need\
    \ for e-learning, many research and educational institutions are not\nready to\
    \ move to a full-ﬂedged online mode due to insuﬃciently reliable technical equipment\
    \ and/or\ninfrastructure. This leads to the issue that many information-educational\
    \ web projects during the\nCovid-19 crisis are not able to maintain stability\
    \ when the load of materials, users and downtimes\noccur; in addition, the transactions\
    \ might not be processed in a proper way. The problem remains\nunresolved, as\
    \ only a small percentage of Ukrainian universities were ready to move to remote\
    \ teaching\nand learning courses when the lockdown started. The main signals of\
    \ an e-learning or e-business\nproject problem should be data and metrics that\
    \ show unsatisfactory performance indicators, but not\nnegative user feedback\
    \ and/or open incidents for the e-project support team.\nA study by Feldmann et\
    \ al.\nfound that internet traﬃc grew by about 15–20% within one\nweek of the\
    \ Covid-19 crisis due to the increased use of online resources, namely: web conferencing,\n\
    Appl. Sci. 2020, 10, 9112\n5 of 16\nVPN, e-commerce, e-learning, and gaming. These\
    \ ﬁndings are similar to the insights shown by\nmobility reports published by\
    \ Google and prove the increased digital demand during Covid-19 [19,20].\nIn addition,\
    \ ensuring the infrastructure reliability requires the implementation of system\
    \ quality\nmonitoring methods, setting certain goals to warrant highly reliable\
    \ and uninterrupted IT operation,\nas well as scalability, which might previously\
    \ have been lacking. The scarcity of real-time data analytics\ndeprives a project\
    \ of observability and does not allow accurate estimation of the actual educational\n\
    web project needs for handling end-to-end operations and measuring the load on\
    \ them.\nThe research by Canizo et al describes the implementation of a monitoring\
    \ solution on a real\nindustrial use case that includes several industrial press\
    \ machines. The eﬀectiveness and its scalability\nfactors are proved. A big data\
    \ architecture for industrial Cyber-Physical System (CPS) monitoring is\nproposed,\
    \ considering four main data factors during the implementation, namely: data acquisition,\n\
    processing, persistence and server availability. The data collection process is\
    \ implemented using\nprogrammable logic controllers. Message streaming and parallel\
    \ processing tools are used to transfer\nand transform the data.\nThe research\
    \ is valuable because of Signe and multiple data anomaly\ndetections that are\
    \ applied as calculation frameworks to detect issues and anomalies. The anomaly\n\
    detection algorithms are mainly based on checking previous and current system\
    \ states. Nevertheless,\nthis implementation addresses all the main issues that\
    \ a CPS faces. The proposed solution has increased\nthe overall equipment eﬀectiveness\
    \ [21].\nThe state-of-the-art real-time big data processing technologies that\
    \ are used for anomaly detection,\nabnormal system behavior and vital machine\
    \ learning algorithm features are studied by Habeeb et al.\nIn the research they\
    \ describe frameworks to handle big data processing in real-time in order to identify\n\
    system issues and security vulnerabilities; a survey of big data techniques was\
    \ conducted. The research\nalso provides comprehensive big data techniques to\
    \ monitor network data [22].\nThe deﬁnition of Service Level Objectives and usage\
    \ is necessary for reliability monitoring,\nresource utilization reduction, and\
    \ performing computationally inexpensive calculations. The article\ndescribes\
    \ performance modeling with proﬁling to ensure low system performance usage so\
    \ that the\nresources used for e-commerce can be decreased by three times [23].\
    \ This framework can be eﬀectively\nautomated and applied to universities’ e-learning\
    \ projects in order to ensure high reliability and\nto reduce costs of infrastructure\
    \ maintenance. In addition, it proves that monitoring and control\nof Service\
    \ Level Objectives can increase project eﬃciency, and therefore their usage within\
    \ EdTech\nremains necessary.\n3. Materials and Methods\nEdTech and e-business\
    \ project availability and reliability are important to ensure qualitative\nservice\
    \ delivery and product distribution. This need is especially necessary and noticeable\
    \ during\nthe period of remote work or study. Service unavailability might cause\
    \ ﬁnancial losses and also\ndoes not allow for a quality educational process.\
    \ Research by Melo et al. [24] helps to estimate how\nmuch money can be saved\
    \ by increasing system availability using SLA; it also presents analysis of\n\
    various system architectures to ensure a beneﬁcial cost-beneﬁt relationship. Research\
    \ from Fortune\n1000 companies shows the downtime value for business-critical\
    \ metrics. For example, on average the\ntotal cost of an unplanned system downtime\
    \ per year is about $1.25 billion and up to $2.5 billion [25].\nAdditionally,\
    \ making objective data-driven decisions [26–28] about a particular service [29,30],\
    \ user [31],\nand technical condition [32] is a key prerequisite for ensuring\
    \ equipment quality and reliability.\nThe modern monitoring tools usage will simplify\
    \ the task of the equipment’s technical assessment\nand control. According to\
    \ the Gartner Share Analysis Report for 2019, digital products focus is\nincreasingly\
    \ on end-user experience monitoring, which is very important because it provides\
    \ a clear\nuser-to-web project interaction understanding. This has a critical\
    \ impact on business income and the\nuser’s desire to continue using the service\
    \ in the future. Especially notable is the end-user experience\nin the period\
    \ of the increased need to use online education and business web projects, such\
    \ as in the\nperiod of COVID-19. Also, the ITOM (IT Operations Management) performance\
    \ analysis software\nAppl. Sci. 2020, 10, 9112\n6 of 16\nmarket grew by around\
    \ 11% compared to 2018. The AI (Artiﬁcial intelligence) usage, namely AIOps\n\
    (Artiﬁcial Intelligence Operations), ITIM (IT Infrastructure Monitoring) and other\
    \ monitoring solutions\nhold around 45% of the performance market demand, whereas\
    \ the APM and network monitoring\nsegments [33] have decreased to demands of 34%\
    \ and 21%, respectively.\nDowntime and minimalizations in spending are essential\
    \ to address, as well as insuﬃcient\nhardware and software viability/availability.\
    \ The SLO deﬁnition and usage is not capable of solving the\nsystem technical\
    \ condition assessment problem without the help of SLI/SLO/SLA data and metadata.\n\
    The use of SLOs is a popular trend today; diﬀerent-sized enterprises that run\
    \ electronic services\nare increasingly using it. SLOs and the SLA serve as motivating\
    \ factors that provide a goal setting\nprocess. It encourages an organization\
    \ to achieve the goals, increase the threshold and overcome it\nagain; measure\
    \ user satisfaction level with and without an Apdex score correlation; accept\
    \ limiting\nthe threshold of system availability; and help understand at what\
    \ infrastructure improvement stage\na web project can improve performance in the\
    \ future. There are many tools for SLO computations\nand monitoring on the market,\
    \ and they can be easily applied to solve various problems with very\ncomplex\
    \ demands.\nWe used the following methods to conduct and obtain the research results:\
    \ data engineering;\ndata collection and logging; mathematical and statistical\
    \ methods to calculate KPIs, user-engagement,\nand correlation analysis; site\
    \ reliability engineering methods; exploratory and descriptive methods\nfor prescriptive\
    \ data analysis; incidents management analysis; data visualization; business-plan\
    \ and\nlong/short-term strategy for infrastructure improvement formation.\nTo\
    \ obtain the required data for the algorithm implementation and SLO deﬁnition,\
    \ it is necessary\nto monitor the infrastructure of the electronic environment\
    \ from the log ﬁles stored in the system and\ncollect web project user behavior\
    \ and activity statistics.\nMoreover, SLO/SLA adherence metadata should be correlated\
    \ with other important metrics for\ne-education and e-business, namely: Customer\
    \ Proﬁtability Score (CPS), Net Proﬁt Margin, Conversion\nRate, Net Promoter Score\
    \ (NPS), and relative market share. These and other metrics need to be\nmonitored\
    \ on interactive panels, in dashboards for operational intelligence monitoring.\n\
    Before determining the service level objectives, the following questions should\
    \ be answered: what\npercentage of web project performance increase should be\
    \ met; whether an increase in availability will\naﬀect a raise in proﬁts, and\
    \ if yes, then how much; what interdependence level between user-engagement\n\
    score and service level indicators (SLI) is observed.\n4. Results\nThe short-\
    \ and long-term SLO deﬁnition is fundamental in monitoring. The former is vital\n\
    for systems engineers, while the latter is necessary for the strategy development\
    \ and management\ndepartments of a particular web project. However, in terms of\
    \ infrastructure technical condition\nassessment and investigating web environment\
    \ impact on hardware, both SLO types are useful and\nneed to be analyzed.\n4.1.\
    \ User-Engagement Calculations to Assess Educational Web Project Parts Interaction\n\
    Both reliability and availability metrics are valuable in the application of performance\
    \ management\nand monitoring. However, both are diﬀerent from each other, because\
    \ a piece of technical equipment\nmay be available but not reliable. For example,\
    \ we could consider a case where we suppose we do have\nequipment X, which has\
    \ a frequent connection loss for 6 min every hour. That means 90% availability,\n\
    but less than 1 h of reliability, which is a poor indicator value for e-commerce.\n\
    Self-education e-resources have been in demand especially since the start of the\
    \ Covid-19\ncrisis. For instance, registration on the popular educational platform\
    \ Coursera, which underwent a\npartnership with various universities during the\
    \ Covid-19 quarantine period, is up by 173% in March\n2020, while course enrollment\
    \ increased by 145% compared to February the same year [34].\nAppl. Sci. 2020,\
    \ 10, 9112\n7 of 16\nIn the study, we analyzed the 2017 business and ﬁnance courses\
    \ data from an educational platform\nUdemy in order to obtain the user-engagement\
    \ score and perform association tests for the obtained\nvalues with the subscription\
    \ and review count, prices, and course duration. This allowed us to validate\n\
    the user-engagement score as an unbiased metric for web project assessment and\
    \ correlation with\ntechnical condition data. We investigated how the user-engagement\
    \ score might aﬀect the increase in\neﬃciency of careful Service Level Objectives\
    \ deﬁnition and computation. Also, we evaluated various\nindicators and strategies\
    \ to assess infrastructure technical conditions based on the correlation between\n\
    SLO/SLA and user-engagement scores. The developed framework is to help identify\
    \ possible system\nissues and improve regular downtimes management. According\
    \ to Google Trends statistics, the peak\nof popularity of the platform from 2017\
    \ to 2020 inclusive was reached in late March–early April 2020\nand was 100%.\
    \ Thus, the losses due to the unavailability of the service can be much greater\
    \ than in the\nnormal period. Therefore, the implementation of technical condition\
    \ assessment using site reliability\nand SLO methods is necessary for the full\
    \ services and information product provision.\nWe found that in 2017 Udemy business\
    \ and ﬁnance domain courses data, there are 1195 observations\nout of which 94.85%\
    \ entities have a label “paid”, although, the user enrollment in paid courses\
    \ is\n75.58%. The average cost of a studied online course was $120, and the median\
    \ and the mode were $124\nand $200, respectively. In the majority of cases, online\
    \ classes that were published earlier in the year\nhad more students signed up\
    \ by the end of the year. In addition, 0.03% of free courses had a higher\nthan\
    \ average number of subscribers and even exceeded subscriptions to paid courses.\
    \ This indicates\nthat if the web project training is free and the user believes\
    \ that its material is well-structured enough,\nit is more likely that a person\
    \ will register for the class, and not for a paid one. In turn, this will create\
    \ a\nload on the infrastructure, induce more course enrollments, but can be less\
    \ proﬁtable to an e-learning\nservice provider.\nThe paid course duration takes\
    \ longer compared to free ones. We determined the total sum of\nhours required\
    \ for each registered participant to complete the course. The number of subscribers\
    \ is\nmultiplied by the course duration to calculate this. The total number of\
    \ subscribers was 668,938, so the\nentire time for all users’ course completions\
    \ was 3,754,806 h.\nOn average, one hour of paid content costs a user $18.6. Assume\
    \ that a 1% loss of availability can\nblock users from enrolling or attending\
    \ the selected class, a loss in proﬁt will be about $123,700 per\nhour, according\
    \ to our data.\nA detailed data examination revealed that the most expensive courses\
    \ are at the expert level,\nand in terms of duration they are in the interval\
    \ of 1–100 h, while classes with a longer span (~200–300 h)\nare only beginner\
    \ level and cost less by about $20 than those in less long courses. Thus, the\
    \ shorter the\ncourse duration (Figure 2) —the less load on the infrastructure\
    \ is created, and more proﬁt is made not\nonly due to the higher e-training cost,\
    \ but also lower costs associated with the service disposal. This is\nimportant\
    \ to consider.\nUser-engagement calculation and monitoring along with correlation\
    \ of the SLAs compliance\nlevels will allow us to assess the technical equipment,\
    \ infrastructure condition, as well as to determine\nwhich categories of users\
    \ face particular online project bottlenecks. An equation to determine an\nonline\
    \ project user-engagement score with respect to n number of content entities has\
    \ been developed.\nOur formula oﬀers the possibility of its application both to\
    \ a web project separate element and the\nservice entirely; input parameters are\
    \ the most important metrics for understanding the web project\nand its participants’\
    \ interactions; the weights are calculated on a comparative basis of all project\n\
    components. Below is Equation (1) for speciﬁc content i.\nUi = max(Ri, C)\nmax(Vi,\
    \ C) Wi,\n(1)\nwhere Ui is the user-engagement value. Its range depends on the\
    \ minimum and maximum values\navailable in the dataset for the arguments Vi and\
    \ Ri, Vi is the number of views/visitors/subscriptions\non a certain online project\
    \ topic. Ri is the number of reactions on a certain online project topic, where\
    \ 0\nAppl. Sci. 2020, 10, 9112\n8 of 16\n≤ Ri ≤ Vi. C is a constant used to avoid\
    \ getting zero during the division. In our research, we set it to 0.5\nbecause\
    \ it is an artiﬁcially created value that does not count as a real user review\
    \ Ri value. Wi is the\nweight parameter, which is diﬀerent for each topic and\
    \ its user-engagement Ui value.\np\ny\n,\n,\nthe entire time for all users’ course\
    \ completions was 3,754,806 h. \nOn average, one hour of paid content costs a\
    \ user $18.6. Assume that a 1% loss of availability \ncan block users from enrolling\
    \ or attending the selected class, a loss in profit will be about $123,700 \n\
    per hour, according to our data. \nA detailed data examination revealed that the\
    \ most expensive courses are at the expert level, \nand in terms of duration they\
    \ are in the interval of 1–100 h, while classes with a longer span (~200–\n300\
    \ h) are only beginner level and cost less by about $20 than those in less long\
    \ courses. Thus, the \nshorter the course duration (Figure 2) —the less load on\
    \ the infrastructure is created, and more profit \nis made not only due to the\
    \ higher e-training cost, but also lower costs associated with the service \n\
    disposal. This is important to consider. \n \nFigure 2. Business and finance course\
    \ price and duration grouped by competency levels. \nUser-engagement calculation\
    \ and monitoring along with correlation of the SLAs compliance \nlevels will allow\
    \ us to assess the technical equipment, infrastructure condition, as well as to\
    \ determine \nwhich categories of users face particular online project bottlenecks.\
    \ An equation to determine an \nonline project user-engagement score with respect\
    \ to n number of content entities has been \ndeveloped. Our formula offers the\
    \ possibility of its application both to a web project separate element \nFigure\
    \ 2. Business and ﬁnance course price and duration grouped by competency levels.\n\
    Max functions are used to obtain the maximum value between Vi or Ri and the constant.\
    \ We found\nthat more than 9% of courses do not have any user reviews, but the\
    \ number of subscribers ranged\nfrom 0 to 1600. This is quite unclear and at the\
    \ same time captivating because it means that there was\ninsuﬃcient user-engagement,\
    \ probably due to a lack of a user- and content-understanding. Apart from\nthat,\
    \ we consider that users might have faced technical issues when viewing the content\
    \ of these\ncourses. This ﬁnding needs further study to determine what leads to\
    \ this kind of user-behavior. Table 1\nshows the number of courses according to\
    \ the total number of subscribers (X-axis).\nTable 1. Total subscribers count\
    \ interval per Udemy business and ﬁnance courses.\nSubscribers Count Interval\n\
    Number of Courses\n0–5000\n1119\n5000–10,000\n52\n10,000–15,000\n14\n15,000–20,000\n\
    6\n20,000–30,000\n5\n50,000–70,000\n3\nWe propose the following Equation (2) to\
    \ calculate the weight value:\nWi =\nXk\nj=1 fj,\n(2)\nwhere fi is the relative\
    \ frequency of a certain factor related to a web-resource category or topic,\n\
    for instance, user interaction, total time spent for page visit, page load time,\
    \ views, reviews,\nsubscriptions, and comments left. We might also face an issue\
    \ when the Wi equals zero. Then the\nuser-engagement is equal to zero as well.\
    \ This might happen in rare cases, because if the Ui has a high\nvalue without\
    \ multiplying it by the weight coeﬃcient, it is very likely that Wi will be greater\
    \ than 0.\nAppl. Sci. 2020, 10, 9112\n9 of 16\nAs mentioned above, the output\
    \ range of the user-engagement score is dependent on the arguments\nVi and Ri;\
    \ in order to standardize it, adjusted to the range from 0 to 1, and receive a\
    \ user-engagement\nscore value for a topic i, we propose the following simple\
    \ Equation (3) below:\nUstandardi =\nUi\nmax(Ui),\n(3)\nwhere we get the ratio\
    \ of a certain category user-engagement to the max user-engagement scores.\n4.2.\
    \ SLO/SLA and the Obtained User-Engagement Score Correlation\nHaving the user-engagement\
    \ score calculated, the service level objectives and agreements deﬁnition\nprocedure\
    \ gets improved. By calculating the appropriate user-engagement scores for educational\n\
    or e-business project components, we can ﬁlter these components according to particular\
    \ criteria,\nwhich will let the web project parties ﬁnd the aggregated user-engagement\
    \ values (mean, median,\npercentiles) only for those units that interest us most,\
    \ and compare them with other groups. Also,\nbased on multiple entities of the\
    \ user-engagement scores, we can ﬁnd the overall scores using statistical\nand\
    \ mathematical functions and monitor them over time. Based on the values obtained\
    \ from these\ncomputations and SLOs/SLAs data, we propose a correlation matrix\
    \ to show how the user-engagement\nscore can aﬀect them and the technical equipment,\
    \ and facilitate its assessment. Thus, applying\ntechnical condition assessment\
    \ is necessary to increase the availability of an e-commerce or e-learning\nproject,\
    \ especially when there is a high demand for it. Using the SLO and the SLA allows\
    \ us to\nassess the infrastructure condition and make objective decisions about\
    \ the application architecture.\nWe should take into account that the SLO setting\
    \ process requires a preliminary system and potential\ninfrastructure risks understanding\
    \ as well as performing user-behavior analysis.\nThe following is a contingency\
    \ table that helps to quantify user-engagement score intervals\nfrequency with\
    \ data about the SLO/SLAs adherence. The user-engagement score can be generic\
    \ for the\nwhole platform, as well as applying for individual web project components.\
    \ Based on the contingency\ntable data, we can ﬁnd the association between a user\
    \ and the online–resource interaction, how positive\nit has been, and what might\
    \ be improved. For example, if the SLO for availability is deﬁned, which is\n\
    directly related to the technical equipment condition, it is profoundly observable\
    \ how it associates with\na certain web–resource user-engagement or interaction.\
    \ It is thus possible to display the cases when the\nSLO or the SLA has been high\
    \ (Table 2), even though the user-engagement has not been or vice versa.\nTable\
    \ 2. Contingency table for the SLO/SLA data and user-engagement scores correlation.\n\
    User-Engagement Standardized Score\nSLO/SLA\n0–0.25\n0.25–0.5\n0.5–0.75\n0.75–0.95\n\
    0.95–1\nTotal\n0–25%\nX11\nX12\nX13\nX14\nX15\nX1+\n25–50%\nX21\nX22\nX23\nX24\n\
    X25\nX2+\n50–75%\nX31\nX32\nX33\nX34\nX35\nX3+\n75–95%\nX41\nX42\nX43\nX44\nX45\n\
    X4+\n95–100%\nX51\nX52\nX53\nX54\nX55\nX5+\nTotal\nX+1\nX+2\nX+3\nX+4\nX+5\nX\n\
    The relative frequencies usage is also recommended for a clear contingency table\
    \ results\ninterpretation.\nWe developed a 3 × 3 matrix based on the SLO/SLA compliance\
    \ levels and\nuser-engagement score. The levels (low, moderate, high) are custom\
    \ for each web project. In our case we\ndeﬁne the following ranges (0–0.05), (0.05–0.65),\
    \ (0.65–1) as low, moderate and high, correspondingly.\nThe SLO/SLA levels depend\
    \ on their deﬁnition documents. For instance, the current published target\nfor\
    \ Google Compute Engine availability is 99.95% availability [35]. If this target\
    \ is met, we deﬁne the\nSLO/SLA level as high (Table 3).\nAppl. Sci. 2020, 10,\
    \ 9112\n10 of 16\nTable 3. Interpretation table for the SLO/SLA and the user-engagement\
    \ score levels correlation.\nLow User-Engagement\nModerate User-Engagement\nHigh\
    \ User-Engagement\nLow SLO/SLA\nPoor technical\nequipment condition.\nCritical\
    \ infrastructure\nproblems might arise.\nUsers face potential issues.\nWeb-service\
    \ needs to be improved,\nand the logs might be analyzed.\nUsers face many issues\n\
    during interaction with a\nservice. The bounce rate\n(single page visit) is\n\
    likely to be high.\nModerate SLO/SLA\nA service can face\npotential issues and\n\
    problems. Even with a\nlow load, the SLO/SLA is\nnot high enough to be\nmet.\n\
    Changes and improvements to the\nonline-resource are necessary as\nwell as user-monitoring\n\
    implementation.\nA service has a high\nthroughput and users\nactively interact\
    \ with it.\nThe technical conditions\nneed to be improved.\nHigh SLO/SLA\nA service\
    \ met its\nobjectives and is reliable,\nhowever, due to low\nuser-engagement,\
    \ it is\nquite diﬃcult to predict\nonline-resource behavior,\nand the SLO/SLA\
    \ levels\nthemselves when the\nload increases.\nA system\ninfrastructure/network/technical\n\
    component performs well. The\nuser-interaction needs to be\nimproved and its future\
    \ increase\noutcomes must be observed.\nA service works well,\nusers actively\
    \ interact\nwith a platform. The\ntechnical condition\nmeets the desired\nrequirements,\
    \ so features\nand improvements can\nbe added to maintain\nhigh scores.\nAccordingly,\
    \ the higher the user-engagement score level, the more active users interact with\
    \ the\nplatform, the more they expect this platform to meet the SLAs. If the user-engagement\
    \ score level\nis low and the SLO/SLA is not met, this is a sign of a web project\
    \ with critical problems, even with\nhardware-related ones. If the user-engagement\
    \ score level is low, but the SLO/SLA is met frequently\nand is determined by\
    \ our table as high, the service and equipment under certain conditions are quite\n\
    reliable and cope with the load, but with user ﬂow and load increase, it is diﬃcult\
    \ to predict the web\nservice operations performance. In this situation, we recommend\
    \ testing the platform and executing\nincreased user ﬂow simulations.\nA moderate\
    \ user-engagement and the same average SLO/SLA levels indicate minor problems\
    \ with\ntechnical equipment and infrastructure, as well as the need to improve\
    \ the web project user interaction.\nWith high SLO/SLA adherence stats and a top\
    \ user-engagement score, the technical equipment works\nwith stability, the development\
    \ team can test and release new features for the web project and gradually\nadd\
    \ improvements, making it more attractive to users than before.\nBased on the\
    \ Udemy business and ﬁnance courses dataset, the user-engagement score was\ncalculated,\
    \ and obtained values are represented in a table view (Table 4).\nTable 4. The\
    \ standardized user-engagement score distribution for Udemy business and ﬁnance\
    \ courses.\nStandardized User-Engagement Score Interval\nValue\n(0–0.1)\n1165\n\
    (0.1–0.2)\n20\n(0.2–0.3)\n7\n(0.3–0.4)\n2\n(0.4–0.5)\n0\n(0.5–0.6)\n3\n(0.6–0.7)\n\
    1\n(0.7–0.8)\n0\n(0.8–0.9)\n0\n(0.9–1)\n1\nTo compute it, we used such parameters\
    \ as the number of subscribers, the number of users who\nleft feedback on the\
    \ course, and the number of published course materials. We can see that only a\
    \ small\nnumber of Udemy courses (2.8%) crossed the standardized user-engagement\
    \ score of 0.1, where the\nminimum and maximum values are 0 and 1, respectively.\
    \ In these courses with the value above 0.1,\nAppl. Sci. 2020, 10, 9112\n11 of\
    \ 16\nthe duration ranges from 1 to 6 h; more than 61% are paid, the price varies\
    \ between $150–200, while the\nminimum price among this course category is $20,\
    \ and the mean is $120; the expertise level for this\ncategory is mostly beginner\
    \ or suitable for all levels, although some of them have the intermediate\nexpertise\
    \ level.\nThereafter, we found that courses with a low user-engagement score (<0.1)\
    \ not only have few\nsubscribers and feedback but also focus on all levels or\
    \ beginner level as well. The duration of these\ncourses usually varies from 1\
    \ to 3 h, meaning the courses are not long-term, and most of them are paid\n(92.87%),\
    \ the prices vary from $20 to $200, while the median and mean are $102 and $104,\
    \ respectively.\nWe used Pearson’s correlation coeﬃcient to study the association\
    \ among user-engagement scores,\nthe number of subscribers, and the number of\
    \ reviews of a speciﬁc course. We obtained a very robust\nassociation between\
    \ the score and the number of reviews (1), and a strong association between the\n\
    score and number of subscribers (0.77). Also, a correlation between the number\
    \ of subscribers and the\nnumber of reviews exists (0.79) and is strong as well\
    \ (Figure 3).\nAppl. Sci. 2020, 10, x FOR PEER REVIEW \n11 of 16 \nfor this category\
    \ is mostly beginner or suitable for all levels, although some of them have the\
    \ \nintermediate expertise level. \nThereafter, we found that courses with a low\
    \ user-engagement score (<0.1) not only have few \nsubscribers and feedback but\
    \ also focus on all levels or beginner level as well. The duration of these \n\
    courses usually varies from 1 to 3 h, meaning the courses are not long-term, and\
    \ most of them are \npaid (92.87%), the prices vary from $20 to $200, while the\
    \ median and mean are $102 and $104, \nrespectively. We used Pearson’s correlation\
    \ coefficient to study the association among user-\nengagement scores, the number\
    \ of subscribers, and the number of reviews of a specific course. We \nobtained\
    \ a very robust association between the score and the number of reviews (1), and\
    \ a strong \nassociation between the score and number of subscribers (0.77). Also,\
    \ a correlation between the \nnumber of subscribers and the number of reviews\
    \ exists (0.79) and is strong as well (Figure 3). \n \nFigure 3. Pearson’s association\
    \ coefficients and plots for the user-engagement score, number of \nsubscribers\
    \ and reviews to the Udemy Business and Finance courses. \nThis strong association\
    \ between user-engagement score and reviews/subscribers count exists \nbecause\
    \ they are used as the main arguments for the equation. We consider correlation\
    \ of other \ndataset features with the obtained user-engagement score, so that\
    \ more insights can be received. To \nstudy the dataset covariance and obtain\
    \ the most critical features for dimensionality reduction, we \nperformed Principal\
    \ Component Analysis (PCA). We used the singular value decomposition method, \n\
    which examines the covariances between individuals in the dataset. This statistical\
    \ method allowed \nus to simplify the correlation observation and define features\
    \ that have the highest value for web-\nprojects user-engagement and popularity.\
    \ Figure 4 shows that the first dimension (PC1), with \neigenvalue and variance\
    \ out of all principal components equals 35%. We observe that such features \n\
    as the number of subscribers, number of reviews, weights, user-engagement, boolean\
    \ value for \nfree/paid entity, and content activeness time (how long a course\
    \ is available on the platform) are \nincluded in this component. \nFigure 3.\
    \ Pearson’s association coeﬃcients and plots for the user-engagement score, number\
    \ of\nsubscribers and reviews to the Udemy Business and Finance courses.\nThis\
    \ strong association between user-engagement score and reviews/subscribers count\
    \ exists\nbecause they are used as the main arguments for the equation. We consider\
    \ correlation of other\ndataset features with the obtained user-engagement score,\
    \ so that more insights can be received.\nTo study the dataset covariance and\
    \ obtain the most critical features for dimensionality reduction,\nwe performed\
    \ Principal Component Analysis (PCA). We used the singular value decomposition\n\
    method, which examines the covariances between individuals in the dataset. This\
    \ statistical method\nallowed us to simplify the correlation observation and deﬁne\
    \ features that have the highest value\nfor web-projects user-engagement and popularity.\
    \ Figure 4 shows that the ﬁrst dimension (PC1),\nwith eigenvalue and variance\
    \ out of all principal components equals 35%. We observe that such\nfeatures as\
    \ the number of subscribers, number of reviews, weights, user-engagement, boolean\
    \ value\nfor free/paid entity, and content activeness time (how long a course\
    \ is available on the platform) are\nincluded in this component.\nAppl. Sci. 2020,\
    \ 10, 9112\n12 of 16\nAppl. Sci. 2020, 10, x FOR PEER REVIEW \n12 of 16 \n \n\
    Figure 4. PCA squared coordinates correlation plot. \nThe PCA biplot (Figure 5)\
    \ proved again that user-engagement, the number of reviews, and the \nnumber of\
    \ subscribers are positively correlated. We also found that the higher the price,\
    \ the greater \nthe number of published lectures, and the longer the course duration,\
    \ which is obvious. Moreover, \nthe time since a course is available correlates\
    \ with the user-engagement score and corresponding to \nits values. We assume\
    \ that the longer a web-project content is available, the larger the chance to\
    \ obtain \nhigh user-engagement and many subscribers/reviews. A negative correlation\
    \ between the course \npayment option (free/paid) and the number of subscribers,\
    \ which has significance for the user-\nengagement score, is examined. Correspondingly,\
    \ if a web-project content is free of charge, it has a \nhigher probability to\
    \ obtain high user-engagement and attention than the paid ones. \n \nFigure 5.\
    \ PCA biplot of business and finance course generated numerical features. \nFigure\
    \ 4. PCA squared coordinates correlation plot.\nThe PCA biplot (Figure 5) proved\
    \ again that user-engagement, the number of reviews, and the\nnumber of subscribers\
    \ are positively correlated. We also found that the higher the price, the greater\
    \ the\nnumber of published lectures, and the longer the course duration, which\
    \ is obvious. Moreover, the time\nsince a course is available correlates with\
    \ the user-engagement score and corresponding to its values.\nWe assume that the\
    \ longer a web-project content is available, the larger the chance to obtain high\n\
    user-engagement and many subscribers/reviews. A negative correlation between the\
    \ course payment\noption (free/paid) and the number of subscribers, which has\
    \ signiﬁcance for the user-engagement score,\nis examined. Correspondingly, if\
    \ a web-project content is free of charge, it has a higher probability to\nobtain\
    \ high user-engagement and attention than the paid ones.\nAppl. Sci. 2020, 10,\
    \ x FOR PEER REVIEW \n12 of 16 \n \nFigure 4. PCA squared coordinates correlation\
    \ plot. \nThe PCA biplot (Figure 5) proved again that user-engagement, the number\
    \ of reviews, and the \nnumber of subscribers are positively correlated. We also\
    \ found that the higher the price, the greater \nthe number of published lectures,\
    \ and the longer the course duration, which is obvious. Moreover, \nthe time since\
    \ a course is available correlates with the user-engagement score and corresponding\
    \ to \nits values. We assume that the longer a web-project content is available,\
    \ the larger the chance to obtain \nhigh user-engagement and many subscribers/reviews.\
    \ A negative correlation between the course \npayment option (free/paid) and the\
    \ number of subscribers, which has significance for the user-\nengagement score,\
    \ is examined. Correspondingly, if a web-project content is free of charge, it\
    \ has a \nhigher probability to obtain high user-engagement and attention than\
    \ the paid ones. \n \nFigure 5. PCA biplot of business and finance course generated\
    \ numerical features. \nFigure 5. PCA biplot of business and ﬁnance course generated\
    \ numerical features.\nAppl. Sci. 2020, 10, 9112\n13 of 16\nThe above analyses\
    \ can help to obtain new knowledge about the data and determine types of\nweb-project\
    \ content that have high user-engagement and are attractive for customers. The\
    \ main\nfeatures that correlate with the user-engagement score are deﬁned. The\
    \ PCA results can be applied for\nthe development of predictive machine learning\
    \ models to solve various tasks in the ﬁelds of e-learning\nand e-commerce.\n\
    5. Conclusions\nWe conclude that commerce and educational project representatives,\
    \ who still do not use electronic\nand EdTech resources, experience many losses\
    \ during the period of the urgent need for digitalization\nand remote working/teaching\
    \ activities. The article provides a framework for improving technical\nequipment\
    \ reliability and availability, and detection of insuﬃcient resource allocation,\
    \ which can\nlead to proﬁt, users, or customers’ loss, and harms business competition,\
    \ especially during a crisis.\nWe propose the application of user-engagement and\
    \ Site Reliability Engineering tools with the concept\nof Service Level Objectives/Service\
    \ Level Agreement in an eﬃcient way using real-time monitoring,\ndue to the fact\
    \ that it allows organizations to make web-project infrastructure observable and\
    \ achieve\ndata-driven decision making. The presence of operational intelligence\
    \ and performance monitoring\nis necessary for data research and to provide high-quality\
    \ service in the remote work and learning\nmodes. We claim that log management\
    \ of a web-project facilitates eﬃcient Service Level Objectives\ndeﬁnition as\
    \ well as the possibility of task automation in the future with intelligent methods.\n\
    In this article, we provided an equation for the user-engagement score calculation,\
    \ which was\napplied to the Udemy business and ﬁnance educational content dataset.\
    \ Our user-engagement\nscore is valuable for determining user-behavior and learning\
    \ trends from topics with their diﬀerent\nvalues. The developed contingency table\
    \ will simplify the study of the relationship between SLO/SLA\nadherence and user-engagement\
    \ data. Accordingly, to calculate user-engagement, we propose to use\nmore than\
    \ one metric, as well as to use weights that are independent and reﬂect a speciﬁc\
    \ web project\nunit in its total spectrum (relative frequency, percentage, ratings).\
    \ We would like to pay special attention\nto the need for user rating score presence\
    \ in the dataset, which can broaden the user-engagement study\nas well as its\
    \ correlation with the obtained SLO/SLA. It is necessary to develop a strategy\
    \ for collecting\nthe necessary business/machine data, as well as a business plan\
    \ to determine the desired SLI thresholds\nso that the SLO/SLA calculation can\
    \ be done in an eﬃcient manner.\nThe limitation of the study is the analysis of\
    \ static data generated in 2017 when the demand\nfor web-educational content was\
    \ high but not so great as opposed to the 2020 period of lockdown\nand the remote\
    \ working and learning modes. Also, we analyzed just the business and ﬁnance\n\
    Udemy educational web-service content, however, the platform contains other popular\
    \ educational\ncontent and user groups to study, where the customer preferences\
    \ might diﬀer, as well as being\non other platforms. Our study can lead to certain\
    \ kinds of social implications—to an increase in\ncommerce/educational web-project\
    \ proﬁtability due to being available and reliable as well as to allow\npeople\
    \ to access speciﬁc content. That might also enforce web-projects to adjust the\
    \ content in order\nto increase user-engagement and meet the SLOs/SLAs. A tendency\
    \ for user-friendly web-projects\nand improvements in zero downtime system [36]\
    \ can evolve. However, we should also consider that\norganizations will need to\
    \ collect more data about users than before, so security and privacy concerns\n\
    might arise. Correspondingly, the more data that will be collected, the more eﬃcient\
    \ techniques for data\nhandling and storage need to be developed, and organizations\
    \ need to adopt these new IT solutions.\nIn the article, we proposed an SLO/SLA\
    \ and user-engagement levels matrix that improves the\ninfrastructure technical\
    \ condition interpretation and speeds up the above-mentioned contingency table\n\
    formation. Our ﬁndings associated with educational/commerce web projects show\
    \ that factors such as\nthe web project service costs, required knowledge level,\
    \ and class duration aﬀects user-engagement.\nA data lake, which will contain\
    \ raw data, and logs that are of signiﬁcance to the e-learning and\ne-commerce\
    \ strategies, can be developed in further research.\nAppl. Sci. 2020, 10, 9112\n\
    14 of 16\nFor further studies, we propose to perform Natural Language Processing\
    \ (NLP) of the text data in\nthe studied dataset and correlation with our user-engagement\
    \ score. In this way, it will be possible\nto ﬁnd out whether there is a relationship\
    \ between the name or description of an educational web\nproject element and the\
    \ interaction frequency. The most valuable might be the task to identify which\n\
    words and phrases are key to the audience interested in educational/commerce web\
    \ projects by their\nincrease/decrease. Also, hypotheses regarding the ratio of\
    \ user-engagement score, business metrics,\nuser evaluation, and SLO/SLA adherence\
    \ can be developed and conﬁrmed or rejected.\nAuthor Contributions: Conceptualization,\
    \ T.U., S.F. and Y.S.; methodology, T.U. and S.F.; validation, T.U., S.F.\nand\
    \ Y.S.; formal analysis, T.U., S.F. and Y.S.; investigation, T.U., S.F., T.P.\
    \ and Y.S.; resources, T.U., S.F., T.P. and Y.S.;\ndata curation, T.U., S.F. and\
    \ Y.S.; writing—original draft preparation, T.U., S.F., Y.S. and T.P.; writing—review\
    \ and\nediting, T.U., S.F., Y.S. and T.P.; visualization, T.U.; project administration,\
    \ S.F. All authors have read and agreed to\nthe published version of the manuscript.\n\
    Funding: This research is supported by National Research Foundation of Ukraine\
    \ within the project “Methods of\nmanaging the web community in terms of psychological,\
    \ social and economic inﬂuences on society during the\nCOVID-19 pandemic”, grant\
    \ number 94/01-2020.\nConﬂicts of Interest: The authors declare no conﬂict of\
    \ interest.\nReferences\n1.\nBojovi´c, Ž.; Bojovi´c, P.D.; Vujoševi´c, D.; Šuh,\
    \ J. Education in times of crisis: Rapid transition to distance\nlearning. Comput.\
    \ Appl. Eng. Educ. 2020, 28, 1467–1489. [CrossRef]\n2.\nBenta, D.; Bologa, G.;\
    \ Dzitac, S.; Dzitac, I. University Level Learning and Teaching via E-Learning\
    \ Platforms.\nProcedia Comput. Sci. 2015, 55, 1366–1373. [CrossRef]\n3.\nMoreno,\
    \ R.; Mayer, R. Interactive multimodal learning environments. Educ. Psychol. Rev.\
    \ 2007, 19, 309–326.\n[CrossRef]\n4.\nLakshminarayanan, R.; Kumar, B.; Raju, M.\
    \ Cloud Computing Beneﬁts for Educational Institutions. In Second\nInternational\
    \ Conference of the Omani Society for Educational Technology.\nAvailable online:\
    \ https:\n//arxiv.org/abs/1305.2616 (accessed on 12 May 2013).\n5.\nGoogle Trends.\
    \ Available online: https://www.sciencedirect.com/science/article/pii/S1877050915015987\n\
    (accessed on 2 July 2020).\n6.\nBashirov, M.G.; Bashirova, E.M.; Khusnutdinova,\
    \ I.G.; Luneva, N.N. The technical condition assessment\nand the resource of safe\
    \ operation of technological pipelines using electromagnetic-acoustic eﬀect. In\
    \ IOP\nConference Series: Materials Science and Engineering; IOP Publishing: Bristol,\
    \ UK, 2020; Volume 734, p. 012191.\n[CrossRef]\n7.\nBednarski, L.; Sie´nko, R.;\
    \ Howiacki, T. Supporting historical structures technical condition assessment\
    \ by\nmonitoring of selected physical quantities. Procedia Eng. 2017, 195, 32–39.\
    \ [CrossRef]\n8.\nBelodedenko, S.V.; Yatsuba, A.V.; Klimenko, Y.M. Technical condition\
    \ assessment and prediction of the\nsurvivability of the mill rolls. Metall. Min.\
    \ Ind. 2015, 7, 85–94.\n9.\nBłachnio, J. Analysis of technical condition assessment\
    \ of gas turbine blades with non-destructive methods.\nActa Mech. Autom. 2013,\
    \ 7, 203–208. [CrossRef]\n10.\nFedushko, S.; Ustyianovych, T.; Gregus, M. Real-time\
    \ high-load infrastructure transaction status output\nprediction using operational\
    \ intelligence and big data technologies. Electronics 2020, 9, 668. [CrossRef]\n\
    11.\nBelforte, S.; Fisk, I.; Flix, J.; Hernández, M.; Klem, J.; Letts, J.; Magini,\
    \ N.; Saiz, P.; Sciaba, A. The commissioning\nof CMS sites: Improving the site\
    \ reliability. J. Phys. 2010, 219, 062047. [CrossRef]\n12.\nAlﬁan, G.; Ijaz, M.F.;\
    \ Syafrudin, M.; Syaekhoni, M.A.; Fitriyani, N.L.; Rhee, J. Customer behavior\
    \ analysis\nusing real-time data processing: A case study of digital signage-based\
    \ online stores. Asia Pac. J. Mark. and\nLogist. 2019, 31, 265–290. [CrossRef]\n\
    13.\nAlﬁan, G.; Syafrudin, M.; Ijaz, M.F.; Syaekhoni, M.A.; Fitriyani, N.L.; Rhee,\
    \ J. A personalized healthcare\nmonitoring system for diabetic patients by utilizing\
    \ BLE-based sensors and real-time data processing. Sensors\n2018, 18, 2183. [CrossRef]\n\
    14.\nCegan, L.; Filip, P. Advanced web analytics tool for mouse tracking and real-time\
    \ data processing.\nIn Proceedings of the IEEE 14th International Scientiﬁc Conference\
    \ on Informatics, Informatics 2017–2018,\nPoprad, Slovakia, 14–16 November 2017;\
    \ pp. 431–435.\nAppl. Sci. 2020, 10, 9112\n15 of 16\n15.\nElmsheuser, J.; Legger,\
    \ F.; Medrano Llamas, R.; Sciacca, G.; Van Der Ster, D. Improving ATLAS grid site\n\
    reliability with functional tests using Hammer Cloud. J. Phys. 2012, 396, 032066.\
    \ [CrossRef]\n16.\nKami´nski, K.; Kami´nski, W.; Mizerski, T. Application of artiﬁcial\
    \ neural networks to the technical condition\nassessment of water supply systems.\
    \ Ecol. Chem. Eng. 2017, 24, 31–40. [CrossRef]\n17.\nSu, Z.; Yang, Z.; Zhang,\
    \ X. Tank gun recoil mechanism technical condition assessment based on improved\n\
    kernel function SVM. In Proceedings of the IEEE 10th International Conference\
    \ on Electronic Measurement\nand Instruments, ICEMI 2011, Chengdu, China, 16–19\
    \ August 2011; Volume 4, pp. 361–363. [CrossRef]\n18.\nBosse, S.; Lehmhus, D.\
    \ Digital real-time data processing with embedded systems. Mater. Integr. Intell.\
    \ Syst.\nTechnol. Appl. 2016, 281–300. [CrossRef]\n19.\nFeldmann, A.; Gasser,\
    \ O.; Lichtblau, F.; Pujol, E.; Poese, I.; Dietzel, C.; Wagner, D.; Wichtlhuber,\
    \ M.; Tapidor, J.;\nVallina-Rodriguez, N.; et al. The Lockdown Eﬀect: Implications\
    \ of the COVID-19 Pandemic on Internet Traﬃc.\nIn Proceedings of the ACM Internet\
    \ Measurement Conference, Pittsburgh, PA, USA, 27–29 October 2020.\n20.\nGoogle.\
    \ COVID-19 Community Mobility Report. 2020. Available online: https://www.google.com/covid19/\n\
    mobility/ (accessed on 15 December 2020).\n21.\nCanizo, M.; Conde, A.; Charramendieta,\
    \ S.; Minon, R.; Cid-Fuentes, R.G.; Onieva, E. Implementation of\na large-scale\
    \ platform for cyber-physical system real-time monitoring. IEEE Access 2019, 7,\
    \ 52455–52466.\n[CrossRef]\n22.\nHabeeb, R.A.A.; Nasaruddin, F.; Gani, A.; Hashem,\
    \ I.A.T.; Ahmed, E.; Imran, M. Real-time big data processing\nfor anomaly detection:\
    \ A survey. Int. J. Inf. Manag. 2019, 45, 289–307. [CrossRef]\n23.\nChen, Y.;\
    \ Iyer, S.; Liu, X.; Milojicic, D.; Sahai, A. SLA decomposition: Translating service\
    \ level objectives\nto system level thresholds. In Proceedings of the 4th International\
    \ Conference on Autonomic Computing\n(ICAC’07), Jacksonville, FL, USA, 11–15 June\
    \ 2007.\n24.\nMelo, C.; Dantas, J.; Fé, I.; Oliveira, A.; Maciel, P. Synchronization\
    \ server infrastructure: A relationship\nbetween system downtime and deployment\
    \ cost. In Proceedings of the International Conference on Systems,\nMan, and Cybernetics\
    \ (SMC), Banﬀ, AB, Canada, 5–8 October 2017; pp. 1250–1255. [CrossRef]\n25.\n\
    Elliot, S. DevOps and the Cost of Downtime: Fortune 1000 Best Practice Metrics\
    \ Quantified; International Data\nCorporation (IDC): Framingham, MA, USA, 2014;\
    \ Available online: https://kapost-files-prod.s3.amazonaws.\ncom/published/54ef73ef2592468e25000438/idc-devops-and-the-cost-of-downtime-fortune-1000-best-practice-\n\
    metrics-quantified.pdf (accessed on 15 December 2014).\n26.\nIzonin, I.; Tkachenko,\
    \ R.; Kryvinska, N.; Zub, K.; Mishchuk, O.; Lisovych, T. Recovery of Incomplete\
    \ IoT\nSensed Data using High-Performance Extended-Input Neural-Like Structure.\
    \ Procedia Comput. Sci. 2019,\n160, 521–526. [CrossRef]\n27.\nIzonin, I.; Kryvinska,\
    \ N.; Vitynskyi, P.; Tkachenko, R.; Zub, K. GRNN Approach Towards Missing Data\n\
    Recovery Between IoT Systems. In Proceedings of the Advances in Intelligent Networking\
    \ and Collaborative\nSystems; Springer: Cham, Switzerland, 2020; pp. 445–453.\n\
    28.\nBeshley, M.; Kryvinska, N.; Seliuchenko, M.; Beshley, H.; Shakshuki, E.M.;\
    \ Yasar, A. End-to-End QoS “Smart\nQueue” Management Algorithms and Traﬃc Prioritization\
    \ Mechanisms for Narrow-Band Internet of Things\nServices in 4G/5G Networks. Sensors\
    \ 2020, 20, 2324. [CrossRef]\n29.\nPoniszewska-Maranda, A.; Matusiak, R.; Kryvinska,\
    \ N.; Yasar, A.-U.-H. A real-time service system in the\ncloud. J. Ambient. Intell.\
    \ Humaniz. Comput. 2020, 11, 961–977. [CrossRef]\n30.\nKryvinska, N.; Bickel,\
    \ L. Scenario-Based analysis of IT enterprises servitization as a part of digital\n\
    transformation of modern economy. Appl. Sci. 2020, 10, 1076. [CrossRef]\n31.\n\
    Markovets, O.; Pazderska, R.; Horpyniuk, O.; Syerov, Y. Informational support\
    \ of eﬀective work of the\ncommunity manager with web communities. CEUR Workshop\
    \ Proc. 2020, 2654, 710–722. Available online:\nhttp://ceur-ws.org/Vol-2654/paper55.pdf\
    \ (accessed on 17 June 2020).\n32.\nKaminskyj, R.; Shakhovska, N.; Gregus, M.;\
    \ Ladanivskyy, B.; Savkiv, L. An Express Algorithm for Transient\nElectromagnetic\
    \ Data Interpretation. Electron. 2020, 9, 354. [CrossRef]\n33.\nWurster, L.; Baul,\
    \ S. Market Share Analysis: ITOM. Perform. Anal. Softw. Worldw. 2019. Available\
    \ online:\nhttps://www.gartner.com/doc/reprints?id=1-1ZA4D838&ct=200619&st=sb\
    \ (accessed on 17 June 2020).\n34.\nChaudhary, V. Covid-19 & e-learning: Coursera\
    \ sees massive uptake in courses. April 2020. Available\nonline: https://www.ﬁnancialexpress.com/education-2/covid-19-e-learning-coursera-sees-massive-uptake-\n\
    in-courses/1920127/ (accessed on 6 April 2020).\nAppl. Sci. 2020, 10, 9112\n16\
    \ of 16\n35.\nBeyer, B.; Jones, C.; Petoﬀ, J.; Murphy, N.R. Site Reliability Engineering:\
    \ How Google Runs Production Systems;\nO’Reilly Media, Inc.: Sebastopol, CA, USA,\
    \ 2016; p. 552.\n36.\nNaseer, U.; Niccolini, L.; Pant, U.; Frindell, A.; Dasineni,\
    \ R.; Benson, T.A. Zero Downtime Release:\nDisruption-free Load Balancing of a\
    \ Multi-Billion User Website. In Proceedings of the Annual Conference of the\n\
    ACM Special Interest Group on Data Communication on the Applications, Technologies,\
    \ Architectures, and Protocols\nfor Computer Communication; Association for Computing\
    \ Machinery: New York, NY, USA, 2020; pp. 529–541.\nPublisher’s Note: MDPI stays\
    \ neutral with regard to jurisdictional claims in published maps and institutional\n\
    aﬃliations.\n© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article\
    \ is an open access\narticle distributed under the terms and conditions of the\
    \ Creative Commons Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Applied sciences (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2076-3417/10/24/9112/pdf?version=1608471288
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: User-Engagement Score and SLIs/SLOs/SLAs Measurements Correlation of E-Business
    Projects Through Big Data Analysis
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/su15032603
  analysis: '>'
  authors:
  - Tehseen Mazhar
  - Rizwana Naz Asif
  - Muhammad Amir Malik
  - Muhammad Adnan
  - Inayatul Haq
  - Muhammad Iqbal
  - Muhammad Kamran
  - Shahzad Ashraf
  citation_count: 24
  full_citation: '>'
  full_text: ">\nCitation: Mazhar, T.; Asif, R.N.;\nMalik, M.A.; Nadeem, M.A.; Haq,\
    \ I.;\nIqbal, M.; Kamran, M.; Ashraf, S.\nElectric Vehicle Charging System in\n\
    the Smart Grid Using Different\nMachine Learning Methods.\nSustainability 2023,\
    \ 15, 2603. https://\ndoi.org/10.3390/su15032603\nAcademic Editors: Francesco\
    \ Calise,\nMaria Vicidomini and Francesco\nLiberato Cappiello\nReceived: 9 December\
    \ 2022\nRevised: 9 January 2023\nAccepted: 10 January 2023\nPublished: 1 February\
    \ 2023\nCopyright:\n© 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\n\
    This article is an open access article\ndistributed\nunder\nthe\nterms\nand\n\
    conditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nsustainability\nArticle\nElectric Vehicle Charging System in the Smart\
    \ Grid Using\nDifferent Machine Learning Methods\nTehseen Mazhar 1\n, Rizwana\
    \ Naz Asif 2, Muhammad Amir Malik 3\n, Muhammad Asgher Nadeem 4,\nInayatul Haq\
    \ 5\n, Muhammad Iqbal 1, Muhammad Kamran 6 and Shahzad Ashraf 7,*\n1\nDepartment\
    \ of Computer Science, Virtual University of Pakistan, Lahore 54000, Pakistan\n\
    2\nSchool of Computer Science, National College of Business Administration & Economics,\n\
    Lahore 54000, Pakistan\n3\nDepartment of Computer Science and Software Engineering,\
    \ Islamic International University,\nIslamabad 44000, Pakistan\n4\nDepartment\
    \ of Computer Science, University of Sargodha, Sargodha 40100, Pakistan\n5\nSchool\
    \ of Information Engineering, Zhengzhou University, Zhengzhou 450001, China\n\
    6\nDepartment of Computer Science, NCBA&E Multan, Multan 60650, Pakistan\n7\n\
    NFC Institute of Engineering and Technology, Multan 60650, Pakistan\n*\nCorrespondence:\
    \ nfc.iet@hotmail.com\nAbstract: Smart cities require the development of information\
    \ and communication technology to\nbecome a reality (ICT). A “smart city” is built\
    \ on top of a “smart grid”. The implementation of\nnumerous smart systems that\
    \ are advantageous to the environment and improve the quality of life for\nthe\
    \ residents is one of the main goals of the new smart cities. In order to improve\
    \ the reliability and\nsustainability of the transportation system, changes are\
    \ being made to the way electric vehicles (EVs)\nare used. As EV use has increased,\
    \ several problems have arisen, including the requirement to build\na charging\
    \ infrastructure, and forecast peak loads. Management must consider how challenging\n\
    the situation is. There have been many original solutions to these problems. These\
    \ heavily rely on\nautomata models, machine learning, and the Internet of Things.\
    \ Over time, there have been more EV\ndrivers. Electric vehicle charging at a\
    \ large scale negatively impacts the power grid. Transformers\nmay face additional\
    \ voltage ﬂuctuations, power loss, and heat if already operating at full capacity.\n\
    Without EV management, these challenges cannot be solved. A machine-learning (ML)-based\
    \ charge\nmanagement system considers conventional charging, rapid charging, and\
    \ vehicle-to-grid (V2G)\ntechnologies while guiding electric cars (EVs) to charging\
    \ stations. This operation reduces the\nexpenses associated with charging, high\
    \ voltages, load ﬂuctuation, and power loss. The effectiveness\nof various machine\
    \ learning (ML) approaches is evaluated and compared. These techniques include\n\
    Deep Neural Networks (DNN), K-Nearest Neighbors (KNN), Long Short-Term Memory\
    \ (LSTM),\nRandom Forest (RF), Support Vector Machine (SVM), and Decision Tree\
    \ (DT) (DNN). According to\nthe results, LSTM might be used to give EV control\
    \ in certain circumstances. The LSTM model’s peak\nvoltage, power losses, and\
    \ voltage stability may all be improved by compressing the load curve. In\naddition,\
    \ we keep our billing costs to a minimum, as well.\nKeywords: electric vehicles;\
    \ smart grid; load forecasting; signal processing; machine learning\n1. Introduction\n\
    Electric vehicles (EVs) have grown in importance as the auto industry has developed.\n\
    EV sales will reach 2.1 million in 2019, a 40% annual growth rate [1]. Electric\
    \ vehicle (EV)\nchargers are now an essential part of the world’s infrastructure,\
    \ with 7.3 million chargers\ninstalled globally in 2019 and a 60% increase in\
    \ the number of public charging stations\ninstalled in 2019 compared to [1]. Additionally,\
    \ 43 million electric vehicles are expected\nto have been sold globally by 2030,\
    \ accounting for 30% of all vehicles [2]. Fast-changing\ntechnologies, like DC-DC\
    \ converters with improved performance, have greatly contributed\nSustainability\
    \ 2023, 15, 2603. https://doi.org/10.3390/su15032603\nhttps://www.mdpi.com/journal/sustainability\n\
    Sustainability 2023, 15, 2603\n2 of 26\nto this [3]. EVs must be managed properly\
    \ as soon as possible. Due to the high amount\nof energy required to charge these\
    \ vehicles, the high number of EVs on the road places\nsigniﬁcant stress on the\
    \ distribution grid. As new driving techniques are developed to\nhelp drivers\
    \ lower their operating costs, demand for these vehicles is also anticipated to\n\
    rise. More electricity is needed to power their charging stations as more electric\
    \ vehicles\nare on the road. When more EVs are on the road, the load curve rises,\
    \ which increases\nthe stress on the transformer and the rest of the distribution\
    \ grid. A strong and reliable\nmanagement system is necessary for the distribution\
    \ grid to run efﬁciently and dependably.\nFor electric vehicles, Apple Inc. has\
    \ developed a method that only considers the amount\nof battery charge needed\
    \ to locate the closest charging station [4]. However, neither the\ndemands on\
    \ the distribution grid nor the drawbacks of charging the EV are considered.\n\
    The development of ICT greatly helps the achievement of smart cities. A framework\
    \ made\nup of ICT is referred to as a “smart city.” It is used to promote and\
    \ develop sustainable\npractices to handle the many problems that urban environments\
    \ present. A smart city\nis made up of an intelligent network of machines and\
    \ objects that are linked together\nusing cloud and wireless technology. The Internet\
    \ of Things manages and analyzes the\ndata they receive in real time to assist\
    \ citizens, towns, and businesses in making the best\ndecisions to improve living\
    \ standards [5]. Integrating devices and data with a city’s physical\ninfrastructure\
    \ can lower living expenses and promote sustainability. Electric vehicle (EV)\n\
    charging stations and parking meters can easily be reached by connected cars.\
    \ A smart\ncity combines the physical infrastructure with ICT to provide beneﬁts\
    \ such as improved\nmobility, convenience, air and water quality, and energy conservation.\
    \ Sensors, motors,\ncentralized units, networks, interfaces, and intelligent metering\
    \ infrastructure will all be\nused in smart buildings in a smart city [6]. Governmental\
    \ organizations are attempting\nto utilize cellular and Low Power Wide Area technologies\
    \ linked to the infrastructure to\nincrease visitor and resident convenience and\
    \ efﬁciency. To minimize energy consumption,\nsmart cities must implement a smart\
    \ grid concept into their energy infrastructure. An\nintelligent metering infrastructure\
    \ creates two-way communication in all grid nodes [7].\nCustomers can take active\
    \ or passive actions to increase the grid’s reliability and energy\nefﬁciency.\
    \ The smart grid can also lessen environmental pollution by facilitating the efﬁcient\n\
    integration of EVs and renewable energy sources into the grid. Government agencies\
    \ could\ninteract with the public, create infrastructure, and track operations\
    \ and development thanks\nto smart city technologies. IoT devices are used in\
    \ smart cities to enhance operations, service\ndelivery, and citizen involvement.\
    \ According to recent research, smart cities are the best\nsolution to population\
    \ pressure in developing and developed nations. Congestion, housing,\npollution,\
    \ administration, availability of power, etc. ICT is utilized to boost productivity,\n\
    interact with municipal or urban services, and enhance the products and services\
    \ provided\nby city authorities. Better citizen-government relationships save\
    \ expenses and resource use.\nThey are intended to respond to issues in a realistic,\
    \ proactive manner [8]. This literature\nreview aims to inform policymakers and\
    \ smart city planners about how to take the needs\nand well-being of the community\
    \ into account when making plans or decisions. The need\nfor eco-friendly vehicle\
    \ technologies is explained by the decline in global CO2 emissions\nand the rising\
    \ price of carbon fuels. In contrast to conventional vehicles, modern electrical\n\
    vehicles (EVs) will improve air quality by reducing carbon emissions. The main\
    \ issues\nbrought on by using traditional vehicles will be solved if electric\
    \ vehicles are successfully\nintegrated. At this time, the penetration of electric\
    \ vehicles has not shown a signiﬁcant\neffect on the grid’s demand for electricity.\
    \ Electric vehicles will eventually become more\nwidely used and more affordable.\
    \ As a result, they will have a big impact on how the\nsmart grid operates and\
    \ how much energy is needed. To lower the failures related to power\nallocation\
    \ and electricity ﬂow across the smart grid, intelligent management systems are\n\
    required. The performance and dependability of various ML techniques were compared\
    \ to\noptimize the distribution grid and reduce charging costs.\nDecision Tree\
    \ (DT), Random Forest (RF), Support Vector Machine (SVM), K-Nearest\nNeighbors\
    \ (KNN), Deep Neural Networks (DNN), and Long Short-Term Memory are the\nSustainability\
    \ 2023, 15, 2603\n3 of 26\nmachine learning (ML) techniques used in this paper\
    \ (LSTM). An ML-based solution for\nmanaging and routing an EV ﬂeet is also provided\
    \ in this paper. The following are the\npaper’s main contributions:\nWe offer\
    \ a management system for managing electric vehicle (EV) charging adaptable\n\
    to the load data uncertainty that can arise with the ML system’s input data. This\
    \ is carried\nout to ensure the dependability and functionality of the system.\
    \ This system combines V2G\ntechnology with both standard and rapid charging.\n\
    The subject of our second analysis is the effectiveness of various machine learning\n\
    (ML) techniques in managing the charging and reconﬁguring of an electric vehicle\
    \ ﬂeet.\nThen, to reduce load changes, power losses, voltage ﬂuctuations, and\
    \ charging costs, we\noptimize using the best machine learning algorithm.\n1.1.\
    \ Intelligent Information Management in Smart Cities Promotes Energy and\nGovernance\
    \ Sustainability\nCity and urban planners and the government should think carefully\
    \ about how to\nproduce, acquire, and sustainably use the power before making\
    \ any major decisions.\nRobotic computing and artiﬁcial agencies are two new technologies\
    \ for solving these\nproblems [9]. Cities and urban areas consume signiﬁcantly\
    \ more energy than rural areas.\nAs a result, it will be very challenging to stop\
    \ environmental pollution and global warming.\nThe most efﬁcient way to reduce\
    \ pollution is to generate and manage urban electricity using\ncircuit technology.\
    \ Engineers can better understand human behavior, energy consumption,\nmachine\
    \ learning, and the Internet of Things. The need for standardized data on smart\n\
    energy, the integration of energy across smart grids, and behavioral analytics\
    \ are just a\nfew risks.\nThe power efﬁciency art grid will be maximized by smart\
    \ metering. This is especially\nimportant given the complicated sustainability\
    \ issues that cities and urban areas are cur-\nrently dealing with. It seems that\
    \ using smart technology ideas and knowledge may be a\ngood idea based on the\
    \ ﬁndings of this analysis. They facilitate carrier machine multiple\nkernel learning\
    \ in NILM energy breakdown using SVMs and a genetic algorithm [5]. The\nurban\
    \ use of digital technology has helped the developing world’s growing population\n\
    and lack of services. Due to the expense of ensuring that infrastructure will\
    \ last for a\nlong time, a sizable informal economy, and numerous social and political\
    \ pressures in the\nmodern world, the idea of smart citieshas not taken off quickly.\
    \ They are considering that\nsmart cities are political for many developing countries\
    \ to support social and economic\ndevelopment.\nIn the early 1800s, the idea of\
    \ “smart growth”, which emphasizes infrastructure like\npublic transportation\
    \ and well-planned cities, ﬁrst appeared. Emerging economies can\nhave technologically\
    \ advanced smart cities if economic, human resource, policy-making,\nand legal\
    \ reforms are fully integrated into development plans [10]. Asserts that for this\n\
    to happen, government priorities and spending priorities must also change, in\
    \ addition\nto the general public becoming aware of and understanding technological\
    \ advancements.\nThis area of study has become more well-known recently as people\
    \ have realized how\nclosely related economic and political issues are to the\
    \ idea of a smart city. In developing\nnations around the world, building smart\
    \ cities is challenging due to education and literacy\nissues. People in cities\
    \ must learn how to use new technologies to avoid being left out of\nsociety because\
    \ they cannot use information. IT experts and information managers must\nconsider\
    \ how the data they collect will be used. Smart services, like smart cards, necessitate\n\
    people to learn, understand, and view things from various angles. These problems\
    \ will\nbe resolved, and smart cities will become sustainable [5]. The challenges\
    \ presented by the\ngrowth of smart cities and the movement of people from rural\
    \ to urban areas are strongly\nintertwined with the well-being of the respective\
    \ populations. Safety, open government,\nhigh-quality education, and creative\
    \ problem-solving are all crucial [5]. These elements\nwill determine whether\
    \ smart cities are successful or unsuccessful, as well as the direction\nof research\
    \ in all areas, ultimately resulting in comprehensive solutions for the issues\
    \ that\nSustainability 2023, 15, 2603\n4 of 26\nplague these urban areas. To understand\
    \ user groups and preferences, you must examine\ndata from all academic disciplines.\
    \ The happiness and well-being of the inhabitants may\nat ﬁrst seem simple and\
    \ unimportant, but it is crucial to consider these when designing\nsmart cities,\
    \ smart services, and smart technology [11]. Figure 1 shows the Type of Electric\n\
    Vehicles and their power source.\nnecessitate people to learn, understand, and\
    \ view things from various angles. These prob-\nlems will be resolved, and smart\
    \ cities will become sustainable [5]. The challenges pre-\nsented by the growth\
    \ of smart cities and the movement of people from rural to urban areas \nare strongly\
    \ intertwined with the well-being of the respective populations. Safety, open\
    \ \ngovernment, high-quality education, and creative problem-solving are all crucial\
    \ [5]. \nThese elements will determine whether smart cities are successful or\
    \ unsuccessful, as well \nas the direction of research in all areas, ultimately\
    \ resulting in comprehensive solutions \nfor the issues that plague these urban\
    \ areas. To understand user groups and preferences, \nyou must examine data from\
    \ all academic disciplines. The happiness and well-being of \nthe inhabitants\
    \ may at first seem simple and unimportant, but it is crucial to consider these\
    \ \nwhen designing smart cities, smart services, and smart technology [11]. Figure\
    \ 1 shows \nthe Type of Electric Vehicles and their power source. \n \nFigure\
    \ 1. The type of Electric Vehicles [12]. \nAn electric vehicle’s battery can account\
    \ for 25 to 50% of the overall cost. According \nto predictions, batteries for\
    \ electric vehicles are expected to cost about 225 Euros per kilo-\nwatt-hour\
    \ by 2025. The cost of manufacturing Li-Ion batteries for electric vehicles de-\n\
    creased by about 50% between 2007 and 2014. These significant price drops mean\
    \ that \nbuying an EV will soon be comparable to purchasing a gas-powered car.\
    \ Currently, three \ntypes of electric vehicles are available: battery, plug-in,\
    \ and hybrid (HEVs). The launch \nsystem of the vehicle determines these classifications.\
    \ The energy needed for power pro-\nduction is stored and released by batteries\
    \ in BEVs. Plug-in hybrids (PHEVs) have an in-\nternal combustion unit that can\
    \ run on gasoline and other fuels and a battery pack that \npowers the electric\
    \ motor (ICE). The battery pack in an HEV can be charged without elec-\ntricity.\
    \ The batteries are continuously charged due largely to the internal combustion\
    \ en-\ngine and brake system. BEVs do not emit greenhouse gases, so they help\
    \ to keep the air \nclean. The electric motor works with the gasoline engine in\
    \ PHEVs and HEVs to minimize \nsize and emissions. The electric battery, the power\
    \ source for the different EVs shown in \nFigure 1, is denoted by the letter EB.\
    \ \n1.2. The Vehicle to Grid (V2G) Network Is a Crucial Development for the Smart\
    \ Grid \nFigure 1. The type of Electric Vehicles [12].\nAn electric vehicle’s\
    \ battery can account for 25 to 50% of the overall cost. According to\npredictions,\
    \ batteries for electric vehicles are expected to cost about 225 Euros per kilowatt-\n\
    hour by 2025. The cost of manufacturing Li-Ion batteries for electric vehicles\
    \ decreased\nby about 50% between 2007 and 2014. These signiﬁcant price drops\
    \ mean that buying an\nEV will soon be comparable to purchasing a gas-powered\
    \ car. Currently, three types of\nelectric vehicles are available: battery, plug-in,\
    \ and hybrid (HEVs). The launch system\nof the vehicle determines these classiﬁcations.\
    \ The energy needed for power production\nis stored and released by batteries\
    \ in BEVs. Plug-in hybrids (PHEVs) have an internal\ncombustion unit that can\
    \ run on gasoline and other fuels and a battery pack that powers\nthe electric\
    \ motor (ICE). The battery pack in an HEV can be charged without electricity.\n\
    The batteries are continuously charged due largely to the internal combustion\
    \ engine and\nbrake system. BEVs do not emit greenhouse gases, so they help to\
    \ keep the air clean. The\nelectric motor works with the gasoline engine in PHEVs\
    \ and HEVs to minimize size and\nemissions. The electric battery, the power source\
    \ for the different EVs shown in Figure 1, is\ndenoted by the letter EB.\n1.2.\
    \ The Vehicle to Grid (V2G) Network Is a Crucial Development for the Smart Grid\n\
    Enhancing the smart grid network’s capacity to balance power supply and demand\n\
    and enabling mobile batteries to store energy reduces the negative impacts of\
    \ using non-\nrenewable energy sources [13]. EVs are harder to integrate without\
    \ an EDMS to control\nenergy consumption. EDMS is looking for user feedback to\
    \ make charging electric vehicles\nas quick and easy as possible. By taking into\
    \ account a range of technical and ﬁnancial vari-\nables, such as the location\
    \ and charging status of electric vehicles (EVs), user preferences,\npredicted\
    \ energy demand, and the state of the distribution network at the time, the Energy\n\
    Distribution Management System (EDMS) will choose the best answer to this question.\n\
    Predicting EV demand and impact is one of EDMS’s strongest features. Achieve the\
    \ highest\nQuality of Service (quality of service) levels possible; this is crucial\
    \ [14]. The EVs Intelligent\nCharging (EVIC) speciﬁcation includes the EDMS concept,\
    \ as shown in Figure 2.\nSustainability 2023, 15, 2603\n5 of 26\nvariables, such\
    \ as the location and charging status of electric vehicles (EVs), user prefer-\n\
    ences, predicted energy demand, and the state of the distribution network at the\
    \ time, the \nEnergy Distribution Management System (EDMS) will choose the best\
    \ answer to this \nquestion. Predicting EV demand and impact is one of EDMS’s\
    \ strongest features. Achieve \nthe highest Quality of Service (quality of service)\
    \ levels possible; this is crucial [14]. The \nEVs Intelligent Charging (EVIC)\
    \ specification includes the EDMS concept, as shown in \nFigure 2. \n \nFigure\
    \ 2. Electric Vehicle powertrain energy management [15]. \nFor the electric vehicle\
    \ management system (EDMS) to operate as designed, each \nelectric automobile\
    \ of the future will need to be equipped with an EB that is synchronized \nwith\
    \ a data-gathering device. The onboard computer will be provided with a GNSS re-\n\
    ceiver, an inertial measurement unit (IMU), and a Wi-Fi wireless LAN interface.\
    \ All three \nof these components have the potential to be used in the process\
    \ of determining the loca-\ntion of the vehicle [16]. The EV’s Onboard Unit has\
    \ a localization unit that can constantly \naccurately identify its exact location.\
    \ A built-in phone modem that can send data to the \ninternet is present in the\
    \ vehicle. The smart city’s communications network will be \ngrounded. Wi-Fi roadside\
    \ units that can be installed anywhere in the city and are compat-\nible with\
    \ electric vehicle charging stations will make up this infrastructure. The EDMS\
    \ \nwill process the location and charging method of electric vehicles.  \n1.3.\
    \ Integration of EVs in Smart Grids \nBecause electric cars utilize power in so\
    \ many ways, systems that rely on them are \nprone to problems. After it has been\
    \ determined that the electrical grid has been built \nappropriately to minimize\
    \ larger penetration strains, the load profile for electric vehicles \nmay be\
    \ forecast. The amount of energy that can be used to power the load on an electric\
    \ \nvehicle depends on the battery’s capacity. The Nissan Leaf’s built-in battery\
    \ has a 30-kil-\nowatt-hour energy storage capacity. These batteries can hold\
    \ four full days’ worth of \nlights for the average house. Several approaches\
    \ may be used to create accurate estima-\ntions regarding EV load profiles. To\
    \ make more precise estimates of the load needs for \nEVs, probabilistic models\
    \ for plug-in electric cars make use of the fact that the charging of \nbatteries\
    \ does not follow a linear pattern. The object principle is the cornerstone of\
    \ the \nMarkov chain approach, often known as a Markov or hybrid grey model, which\
    \ forecasts \nFigure 2. Electric Vehicle powertrain energy management [15].\n\
    For the electric vehicle management system (EDMS) to operate as designed, each\n\
    electric automobile of the future will need to be equipped with an EB that is\
    \ synchronized\nwith a data-gathering device. The onboard computer will be provided\
    \ with a GNSS receiver,\nan inertial measurement unit (IMU), and a Wi-Fi wireless\
    \ LAN interface. All three of these\ncomponents have the potential to be used\
    \ in the process of determining the location of the\nvehicle [16]. The EV’s Onboard\
    \ Unit has a localization unit that can constantly accurately\nidentify its exact\
    \ location. A built-in phone modem that can send data to the internet is\npresent\
    \ in the vehicle. The smart city’s communications network will be grounded. Wi-Fi\n\
    roadside units that can be installed anywhere in the city and are compatible with\
    \ electric\nvehicle charging stations will make up this infrastructure. The EDMS\
    \ will process the\nlocation and charging method of electric vehicles.\n1.3. Integration\
    \ of EVs in Smart Grids\nBecause electric cars utilize power in so many ways,\
    \ systems that rely on them are\nprone to problems. After it has been determined\
    \ that the electrical grid has been built\nappropriately to minimize larger penetration\
    \ strains, the load proﬁle for electric vehicles\nmay be forecast. The amount\
    \ of energy that can be used to power the load on an elec-\ntric vehicle depends\
    \ on the battery’s capacity. The Nissan Leaf’s built-in battery has a\n30-kilowatt-hour\
    \ energy storage capacity. These batteries can hold four full days’ worth of\n\
    lights for the average house. Several approaches may be used to create accurate\
    \ estimations\nregarding EV load proﬁles. To make more precise estimates of the\
    \ load needs for EVs,\nprobabilistic models for plug-in electric cars make use\
    \ of the fact that the charging of\nbatteries does not follow a linear pattern.\
    \ The object principle is the cornerstone of the\nMarkov chain approach, often\
    \ known as a Markov or hybrid grey model, which forecasts\nEV demand. This strategy\
    \ considers both predicted deviations and recurring trends. Artiﬁ-\ncial neural\
    \ networks, support vector machines, and decision trees, among other machine\n\
    learning techniques, may be used to estimate the number of EVs presently in use\
    \ on the\nroadways at any time [17]. The initial dataset is split during the data\
    \ conﬁguration stage\ninto training and test datasets. One uses a training dataset\
    \ to train the model and ﬁnd\n“hidden” correlations and patterns between the target\
    \ values and attribute values. The\neffectiveness of the data mining strategy\
    \ is assessed using the test dataset. Information\nfrom the training dataset is\
    \ used during the testing and grading phases. The training\nand evaluation process\
    \ is repeated about 20 times to ﬁnd the ideal set of parameters. A\nmodel’s performance\
    \ can be assessed using r-correlation, root-mean-squared error (RMSE),\nand root-mean-square\
    \ absolute error (MAPE). Time and training metrics were used to\nevaluate the\
    \ performance. According to the study, SVM offers the most accurate forecasts\n\
    (r = 98.09). SVM training takes longer than other training approaches. In such\
    \ a short\nperiod, ANN achieved amazing results. The MAPE criterion was the most\
    \ effective method\nfor evaluating the precision of load forecasts. Values are\
    \ most effective when they are less\nthan 5%. Applying the RMSE index severely\
    \ wants to punish large absolute errors. It\nSustainability 2023, 15, 2603\n6\
    \ of 26\nis possible to develop a model of the requirements and routines of EV\
    \ drivers using the\nMarkov model. The existing prediction charging model was\
    \ initially designed to support a\nsingle EV, but it has since been changed to\
    \ support charging multiple EVs simultaneously.\nStudies on origin-destination\
    \ (OD) and trafﬁc have also made it possible to predict the load\nproﬁle for electric\
    \ vehicles. Integration models for Electric Vehicles (EVs) into the smart\ngrid,\
    \ both centralized and decentralized is presented in Figure 3.\nused to evaluate\
    \ the performance. According to the study, SVM offers the most accurate \nforecasts\
    \ (r = 98.09). SVM training takes longer than other training approaches. In such\
    \ a \nshort period, ANN achieved amazing results. The MAPE criterion was the most\
    \ effective \nmethod for evaluating the precision of load forecasts. Values are\
    \ most effective when they \nare less than 5%. Applying the RMSE index severely\
    \ wants to punish large absolute errors. \nIt is possible to develop a model of\
    \ the requirements and routines of EV drivers using the \nMarkov model. The existing\
    \ prediction charging model was initially designed to support \na single EV, but\
    \ it has since been changed to support charging multiple EVs simultane-\nously.\
    \ Studies on origin-destination (OD) and traffic have also made it possible to\
    \ predict \nthe load profile for electric vehicles. Integration models for Electric\
    \ Vehicles (EVs) into the \nsmart grid, both centralized and decentralized is\
    \ presented in Figure 3. \n \nFigure 3. Integration models for Electric Vehicles\
    \ (EVs) into the smart grid, both centralized and \ndecentralized [18]. \nThe\
    \ cheap automated decision making is crucial for saving money on maintenance \n\
    and energy. Authors in [19] were able to increase discharge rates and lower charging\
    \ costs \nby utilizing cyber insurance. The short-term price, coverage, and insurance\
    \ options can be \ndetermined by combining the proposed cost function with a Cartesian\
    \ product. When an \nelevation is viewed as an idealized cost function with parameters,\
    \ a learning algorithm \ncan decide the best course of action. The authors in\
    \ [20] looked at how to forecast the load \non the distribution grid from electric\
    \ vehicles in the upcoming years using an enhanced \ngrey theory prediction model.\
    \ An intelligent, machine learning-based technique for charg-\ning electric automobiles\
    \ is presented by [21]. In the presence of renewable energy sources, \n[22] looked\
    \ at how charging electric vehicles changed the demand profile of the smart \n\
    Figure 3. Integration models for Electric Vehicles (EVs) into the smart grid,\
    \ both centralized and\ndecentralized [18].\nThe cheap automated decision making\
    \ is crucial for saving money on maintenance\nand energy. Authors in [19] were\
    \ able to increase discharge rates and lower charging costs\nby utilizing cyber\
    \ insurance. The short-term price, coverage, and insurance options can be\ndetermined\
    \ by combining the proposed cost function with a Cartesian product. When an\n\
    elevation is viewed as an idealized cost function with parameters, a learning\
    \ algorithm can\ndecide the best course of action. The authors in [20] looked\
    \ at how to forecast the load on\nthe distribution grid from electric vehicles\
    \ in the upcoming years using an enhanced grey\ntheory prediction model. An intelligent,\
    \ machine learning-based technique for charging\nelectric automobiles is presented\
    \ by [21]. In the presence of renewable energy sources, [22]\nlooked at how charging\
    \ electric vehicles changed the demand proﬁle of the smart grid. They\ncame up\
    \ with solutions to the problems. According to [23], using artiﬁcial intelligence\
    \ and\nsmart grids have been used to operate electric cars (EVs). The prediction\
    \ method of [24] was\nused to project the demand for electric vehicles shortly\
    \ (2012). To forecast the variable need\nfor charging electric vehicles and the\
    \ overall number of vehicles on the road [22] created\na cellular agent automaton.\
    \ The use of machine learning in this approach was crucial.\nSmart city development\
    \ can reduce population pressures, but overall population growth\nis predictable,\
    \ especially in developing countries. Because it would lead to new political,\n\
    economic, and social problems, including technological exclusion and discrimination,\
    \ it is\nnot possible. It involves extensive research and consideration from all\
    \ parties involved, and\nthe best chance of success comes from a plan that looks\
    \ at the situation from a variety of\nangles [25]. Research on smart cities suggests\
    \ incorporating attempts to reduce computer\nprocesses into the provision of social\
    \ services, with an emphasis on boosting end-user\ncomprehension. Infrastructure\
    \ like the Global Positioning System (GPS), Galileo, and\nthe European Geostationary\
    \ Navigation Overlay Service is necessary for electric vehicle\ncompatibility\
    \ with the smart grid (EGNOS). Instead of satellites, Wi-Fi will be used to\n\
    identify our location [26]. The various ways that EVs can be integrated into smart\
    \ grids\nand smart cities are shown in Figure 4. Electric vehicles are introduced\
    \ in a smart city using\nWi-Fi and GPS positioning systems.\nSustainability 2023,\
    \ 15, 2603\n7 of 26\nattempts to reduce computer processes into the provision\
    \ of social services, with an em-\nphasis on boosting end-user comprehension.\
    \ Infrastructure like the Global Positioning \nSystem (GPS), Galileo, and the\
    \ European Geostationary Navigation Overlay Service is \nnecessary for electric\
    \ vehicle compatibility with the smart grid (EGNOS). Instead of satel-\nlites,\
    \ Wi-Fi will be used to identify our location [26]. The various ways that EVs\
    \ can be \nintegrated into smart grids and smart cities are shown in Figure 4.\
    \ Electric vehicles are \nintroduced in a smart city using Wi-Fi and GPS positioning\
    \ systems. \n \nFigure 4. EVs integration with smart grid and smart houses [27].\
    \ \nVarious elements, such as energy storage technologies, electric cars, and\
    \ renewable \nenergy sources, were considered while studying the ideal energy\
    \ management frame-\nwork. To estimate the activity for the next day that would\
    \ result in the lowest overall cost \nfor the energy that was bought, the authors\
    \ solved a linear programming problem. Using \na case study encompassing three\
    \ separate buildings, it was shown that the suggested \nmethod reduced energy\
    \ costs while simultaneously boosting grid dependability. They de-\nveloped a\
    \ domestic energy consumption management system that [28] identified the ideal\
    \ \nprocedure for scheduling various loads and supplies across different price\
    \ levels using a \nmulti-objective optimization problem. Results showed that the\
    \ system could decrease \ntransformer losses for the utility provider while still\
    \ meeting customer demand. Various \ntechniques have been used to study optimization,\
    \ like dynamic and quadratic program-\nming. Sequential and probabilistic reasoning\
    \ techniques for lowering distribution system \nFigure 4. EVs integration with\
    \ smart grid and smart houses [27].\nVarious elements, such as energy storage\
    \ technologies, electric cars, and renewable\nenergy sources, were considered\
    \ while studying the ideal energy management framework.\nTo estimate the activity\
    \ for the next day that would result in the lowest overall cost for\nthe energy\
    \ that was bought, the authors solved a linear programming problem. Using\na case\
    \ study encompassing three separate buildings, it was shown that the suggested\n\
    method reduced energy costs while simultaneously boosting grid dependability.\
    \ They\ndeveloped a domestic energy consumption management system that [28] identiﬁed\
    \ the\nideal procedure for scheduling various loads and supplies across different\
    \ price levels\nusing a multi-objective optimization problem. Results showed that\
    \ the system could\ndecrease transformer losses for the utility provider while\
    \ still meeting customer demand.\nVarious techniques have been used to study optimization,\
    \ like dynamic and quadratic\nprogramming. Sequential and probabilistic reasoning\
    \ techniques for lowering distribution\nsystem losses were studied by [29]. Based\
    \ on the outcomes, the developed method showed\nthat minimizing load variation\
    \ is more advantageous than minimizing losses because\nit produces the same overall\
    \ result in a shorter time. In addition, it works with any\nconﬁguration of the\
    \ distribution grid, regardless of type. [29] investigated how improving\nthe\
    \ voltage proﬁle of a real-time coordination system could be achieved by lowering\
    \ the\ntotal cost of energy production and power losses. Assuming that EVs are\
    \ randomly plugged\nin, the algorithm uses the maximum sensitivities selection\
    \ optimization method. With the\nhelp of a real-time system, the distribution\
    \ grid can be more effective, and its load can\nbe decreased [30] investigated\
    \ load variance optimization for a single-home micro grid.\nQuadratic programming\
    \ is used in a micro grid that serves a single house to minimize load\nvariation.\
    \ The system lowers the variance in individual loads, which reduces the conﬂict\
    \ in\nloads on the distribution grid. This shows that the system beneﬁts the entire\
    \ grid, according\nto the data. However, because some parameters, like the load\
    \ curve, were taken as givens,\nthe system is not as effective as it could be.\
    \ In the real world, this might not be accurate.\nA coordinated charging method\
    \ for EVs based on power factor correction was developed\nby [31] to decrease\
    \ power losses and enhance the voltage proﬁle. Users’ favorite charging\nmethods\
    \ were considered using a priority selection algorithm. The system reduced peak\n\
    demand and improved the performance of the electrical grid. Researchers in [32]\
    \ proposed\ncharging many electric vehicles simultaneously using a decentralized\
    \ topology system. The\nsuggested approach employs collectively pointless games.\
    \ The system decreases the peak-\nto-valley gap by ﬁlling the valley of the load\
    \ curve—costs for producing power decrease\nSustainability 2023, 15, 2603\n8 of\
    \ 26\nas a result. Less two-way communication is needed between command centers\
    \ and EVs.\nRegardless of power loss or voltage variations, the optimization challenge\
    \ ensures that EVs\ncan be completely charged using the least amount of energy.\
    \ The approach assumes that all\nloads, not just those from EVs, are predictable.\n\
    In the design of power architectures, energy storage methods, micro-grid control\n\
    systems, and energy management optimization, key problems and limitations are\
    \ covered\nin the article. A summary of current research on EV charging stations\
    \ is also provided. An\nadded beneﬁt is that the infrastructure was built to support\
    \ various degrees of micro grids\nfor charging electric automobiles. In order\
    \ to make EV charging stations as efﬁcient as\npossible, a lot of research is\
    \ done on the coordinated control systems, energy management\nplans, and machine\
    \ learning methods utilized by these stations. To analyze and compare\nthe system’s\
    \ performance, many machine learning techniques were used. In the end, this\n\
    approach is extensively used, and it found how to add and remove constraints and\
    \ obstacles\nin the most reliable way possible\n2. Literature Review\n2.1. Managed\
    \ Charging of Electric Vehicle\nUnplanned EV charging harms the distribution grid,\
    \ as shown by several studies. Re-\nsearchers [33,34] examined how electric vehicles\
    \ are charged. Without proper coordination,\nit has been seen that charging EVs\
    \ causes signiﬁcant voltage disturbances and energy loss.\nOne of the most popular\
    \ methods of frequency control frequency droop control is\ncurrently being successfully\
    \ applied to electric cars. The major technique for controlling\nthe frequency\
    \ now is to regulate the output of generators connected to the main power\ngrid.\
    \ As conventional power plants are phased out, electric vehicles are a great alternative\n\
    since their batteries may be charged or discharged in response to frequency deviation\n\
    alarms. As a result, we investigate frequency regulation in a power grid model\
    \ with loads,\ntraditional producers, and a sizable amount of EVs. By providing\
    \ grid-related services,\nthese last devices, in particular the FDC, autonomously\
    \ optimize the grid. Two new control\nalgorithms that may be used to manage electric\
    \ car batteries in the best possible way during\nfrequency regulation service.\
    \ The control processes, on the one hand, make sure that\nthe power balance and\
    \ frequency management of the main grid are stable. On the other\nhand, these\
    \ approaches can meet a range of EV charging needs. The available methods\nare\
    \ designed to reduce the failure rates of battery-powered devices. This contrasts\
    \ with\nthe EV literature, which frequently concentrates on determining the ideal\
    \ charge level.\nThe performance of the solutions is then contrasted with that\
    \ of other state-of-the-art\nV2G control systems. The results of numerical experiments\
    \ carried out using an accurate\nrepresentation of the power grid show that the\
    \ suggested methods perform well in actual\noperational circumstances. According\
    \ to [33], these detrimental effects might become more\nnoticeable as more EVs\
    \ are on the road. Renewable energy may be able to solve these\nproblems, but\
    \ [34] analysis shows that power quality regulation is still a problem. Authors\n\
    in [35] investigated the dangers of using the distribution grid to charge EVs.\
    \ It follows that\nV2G technologies can enhance the grid’s functionality. V2G\
    \ technologies, according to [36],\nhave many beneﬁts, such as power regulation,\
    \ load balancing, and current harmonics\nﬁltering. V2G technologies, however,\
    \ might lead to EVs discharging deeply. In order\nto charge an electric vehicle\
    \ ﬂeet as efﬁciently as possible while the total capacity of the\npower grid is\
    \ constrained, a new distributed control technique is proposed in this study.\n\
    The most economical way to charge for a proﬁle of total energy use can be found\
    \ by\nsolving a scheduling problem. As a result, the optimization issue that arises\
    \ is a quadratic\nprogramming issue with choice variables linked to both the objective\
    \ function and the\nconstraint. In our model, people only interact with their\
    \ immediate neighbors and reach\nout to higher-ups when making decisions. The\
    \ answer was discovered using a distributed\niterative approach that takes into\
    \ account duality, proximity, and consensus theory. An\nillustration case study\
    \ shows how the tactic can result in the best solution for everyone [37].\nThis\
    \ work proposes a new distributed control technique for optimally charging an\
    \ electric\nSustainability 2023, 15, 2603\n9 of 26\nvehicle (EV) ﬂeet when the\
    \ overall capacity of the power grid is constrained. By resolving a\nscheduling\
    \ issue, the most cost-effective method of charging for a proﬁle of total energy\n\
    use can be determined. Consequently, the resulting optimization problem is expressed\n\
    as a quadratic programming problem with choice variables that are connected to\
    \ both\nthe objective function and the constraint. In our concept, individuals\
    \ just communicate\nwith their close neighbors and make decisions without contacting\
    \ anyone above them. A\ndistributed iterative method that considers duality, proximity,\
    \ and consensus theory was\nutilized to obtain the solution. An example case study\
    \ demonstrates that the strategy can\nlead to the optimal answer for everyone.\n\
    The batteries in electric vehicles (EVs) degrade over time, shortening their life\
    \ cycle\nand lowering customer satisfaction, so this comes at a cost. Because\
    \ it only considers the\npower grid’s needs and ignores those of the end users,\
    \ it is not thought to be the best option.\nThe impact of various charging techniques\
    \ on the price of charging and the rate of battery\ndecrease was also studied\
    \ by [38]; the effectiveness of non-coordinated, one-way, and two-\nway charging\
    \ was examined. Battery life can be signiﬁcantly increased at a lower cost when\n\
    intelligent charging techniques and time-of-use electricity prices are combined.\
    \ Most of the\nresearch on coordinated charging of electric vehicles conducted\
    \ by [39] was user-focused.\nWithin the limitations of the power grid, an optimization\
    \ model that reduces costs has been\ncreated. The beneﬁts of the system become\
    \ clearer as the number of EVs rises. The system\ncan cut costs by up to 50% compared\
    \ to uncoordinated billing. The system requires smart\nmeters to gather real-time\
    \ data on how electric vehicles are being charged and does not\nconsider fast\
    \ charging. It can be challenging to charge electric vehicles (EVs) because they\n\
    consume a lot of energy, and renewable energy generation is not always reliable,\
    \ according\nto [39], who also looked into this issue. Signiﬁcant cost savings\
    \ of up to 8% were found\nwhen comparing the results of the charging scheme to\
    \ those of uncoordinated charging.\nRecharging an electric vehicle reduces the\
    \ amount of carbon dioxide released into the\natmosphere and is better for the\
    \ environment. In agreement with [39], smart charging\ninfrastructure is crucial\
    \ for deploying EVs. Inductive and conductive charging is used for\nelectric and\
    \ plug-in hybrid vehicles. Conducted charging systems demand a power station\n\
    that is physically connected to an electric vehicle compared to inductive charging\
    \ systems.\nA scheme for electrical conversion may also include a high-to-low-frequency\
    \ converter\nand power factor correction (PFC). Either an internal or external\
    \ charging system can be\nused to charge the device. The battery and inverter\
    \ current regulators and their power\nsupplies are built to house inside the vehicle\
    \ in onboard chargers. Off-board chargers are\nlocated outside the car. Conductive\
    \ chargers need not be able to transfer power in order to\nbe considered such.\
    \ We might also add more standards. The AC level 1 charger is one of\nmany different\
    \ charger types. Figure 5 shows the Static WCS for EVs.\nPower grids are experiencing\
    \ a loss of transmission and a decrease in their ability to\nperform main frequency\
    \ control as traditional generators are being phased out in favor of\nrenewable\
    \ energy sources. The issue is made signiﬁcantly more challenging by the steadily\n\
    increasing number of electric vehicles (EVs), which necessitates the creation\
    \ of trying to cut\nmethods for the management of grid operations. Rather than\
    \ being an issue, the growth of\nelectric vehicles could end up being a solution\
    \ to a number of issues that have arisen with\nthe electricity grid. In this context,\
    \ the so-called vehicle-to-grid (V2G) mode of operation\nis crucial and is one\
    \ of the main operating modes for electric vehicles. This mode can\noffer auxiliary\
    \ services to the power grid such peak clipping, load shifting, and frequency\n\
    management. To be more precise, frequency droop management, one of the more traditional\n\
    techniques for frequency regulation, has just lately begun to be successfully\
    \ applied to\nelectric vehicles. This is the primary frequency regulation, which\
    \ is done by controlling\nthe current active power of the largest grid generators.\
    \ Due to the decommissioning\nof conventional power plants, electric vehicles\
    \ are viewed as particularly advantageous\nalternatives. This is because electric\
    \ vehicles (EVs) have the ability to respond to alarms\nabout frequency deviations\
    \ by either charging or discharging their batteries.\nSustainability 2023, 15,\
    \ 2603\n10 of 26\nnal charging system can be used to charge the device. The battery\
    \ and inverter current \nregulators and their power supplies are built to house\
    \ inside the vehicle in onboard \nchargers. Off-board chargers are located outside\
    \ the car. Conductive chargers need not be \nable to transfer power in order to\
    \ be considered such. We might also add more standards. \nThe AC level 1 charger\
    \ is one of many different charger types. Figure 5 shows the Static \nWCS for\
    \ EVs. \n \nFigure 5. Static WCS for EVs [40]. \nPower grids are experiencing\
    \ a loss of transmission  and a decrease in their ability \nto perform main frequency\
    \ control as traditional generators are being phased out in favor \nof renewable\
    \ energy sources. The issue is made significantly more challenging by the \nsteadily\
    \ increasing number of electric vehicles (EVs), which necessitates the creation\
    \ of \ntrying to cut methods for the management of grid operations. Rather than\
    \ being an issue, \nthe growth of electric vehicles could end up being a solution\
    \ to a number of issues that \nhave arisen with the electricity grid. In this\
    \ context, the so-called vehicle-to-grid (V2G) \nmode of operation is crucial\
    \ and is one of the main operating modes for electric vehicles. \nThis mode can\
    \ offer auxiliary services to the power grid such peak clipping, load shifting,\
    \ \nand frequency management. To be more precise, frequency droop management,\
    \ one of \nthe more traditional techniques for frequency regulation, has just\
    \ lately begun to be suc-\ncessfully applied to electric vehicles. This is the\
    \ primary frequency regulation, which is \ndone by controlling the current active\
    \ power of the largest grid generators. Due to the \ndecommissioning of conventional\
    \ power plants, electric vehicles are viewed as particu-\nlarly advantageous alternatives.\
    \ This is because electric vehicles (EVs) have the ability to \nrespond to alarms\
    \ about frequency deviations by either charging or discharging their bat-\nteries.\
    \  \nThis study focuses on modifying individual loads to conform to system constraints.\
    \ \nThis research was conducted because it is challenging to charge a significant\
    \ number of \nelectric vehicles with our current infrastructure. Since distributed\
    \ methods may extend \nFigure 5. Static WCS for EVs [40].\nThis study focuses\
    \ on modifying individual loads to conform to system constraints.\nThis research\
    \ was conducted because it is challenging to charge a signiﬁcant number of\nelectric\
    \ vehicles with our current infrastructure. Since distributed methods may extend\n\
    and communicate more efﬁciently than earlier approaches, they are being evaluated\
    \ as\na potential solution. We show that distributed linear optimization and communication\n\
    networks can be used to ensure relative average fairness while maximizing utilization.\n\
    Here, we outline these techniques. The outcomes of our research and simulation\
    \ are\npresented to show how highly useful these techniques are. When used in\
    \ test scenarios,\nthe algorithm often performs within 5% of the ideal centralized\
    \ conﬁguration. Scaling up,\nhowever, is easier and requires less communication\
    \ than in the perfect situation. The end\nuser, in this case, the owner of the\
    \ vehicle, is the one who is looking at battery charging\nin this article. We\
    \ describe a number of distributed algorithms to accomplish speciﬁc\npolicy goals\
    \ due to the fact that vehicle owners’ priorities can change over time. Our\n\
    main focus is on the many types of distributed charges. Dispersed systems stand\
    \ out\nin terms of recharging electric vehicles for a number of reasons. First,\
    \ when individual\ncomponents fail, decentralized algorithms are often more stable.\
    \ Second, it is anticipated\nthat in charging conditions, the number of electric\
    \ vehicles competing for the same amount\nof energy will vary quickly, indicating\
    \ a random quality. It would be nice to have self-\nmanaging distributed optimization\
    \ approaches in this kind of circumstance. In conclusion,\na distributed approach\
    \ makes it possible to implement a set of rules with little risk of data\nloss\
    \ [41].\nAs a result, we investigate frequency regulation in a power grid model\
    \ that includes\nloads, traditional generators, and lots of EVs. By offering auxiliary\
    \ services to the grid,\nthe latter, most notably the FDC, can take an independent\
    \ part in the process of grid\noptimization. For the purpose of optimizing the\
    \ control of electric vehicle batteries while the\nservice is being provided to\
    \ regulate frequency, we provide two unique control algorithms.\nOn the one hand,\
    \ the control strategies make sure that the main grid’s power balance\nand frequency\
    \ stability are upheld. On the other hand, the techniques are adaptable\nenough\
    \ to meet a variety of electric vehicle charging requirements (EVs). The suggested\n\
    approaches’ main goal is to prevent battery device degradation, which is in contrast\
    \ to\nthe pertinent literature’s frequent discussion of obtaining the ideal charge\
    \ level in relation\nto electric vehicles. The proposed solutions are last evaluated\
    \ in comparison to various\ncutting V2G control systems and their outcomes are\
    \ contrasted. The solutions proposed are\nsuccessful when used in real operational\
    \ settings, as shown by the outcomes of numerical\ntests performed using a realistic\
    \ model of a power grid.\nSustainability 2023, 15, 2603\n11 of 26\nBecause batteries\
    \ lose power while linked to the grid, it is difﬁcult to determine the\ncost of\
    \ an Energy Storage System for frequency management. As a result, researchers\
    \ are\nlooking at how real-world conditions affect Li-ion batteries’ capacity\
    \ to store energy. Both a\ncontrol method for Li-ion ESS that helps with frequency\
    \ management and a cost accounting\nmodel for frequency regulation that takes\
    \ the impact of shorter battery life into account\nare created. We estimate the\
    \ expected lifetime and the average annual cost of the Li-ion\nESS for different\
    \ dead bands and SOC set-points. Case studies show that the Li-ion ESS’s\nestimated\
    \ operating life under standard and full discharge settings is much shorter than\n\
    the manufacturer’s reported nominal life. In this paper, a precise method for\
    \ calculating\nthe cost of ESS that take part in grid frequency regulation is\
    \ presented. This is carried out\nto aid ESS’s growth in the auxiliary services\
    \ sector [42].\nBecause batteries degrade while the system is linked to the grid,\
    \ it is challenging to\nestimate the cost of an Energy Storage System (ESS) for\
    \ frequency regulation. Researchers\nare examining how the stresses of practical\
    \ use affect the Li-ion cells in the battery energy\nstorage system to ﬁnd a solution\
    \ to this issue. We construct a control method for Li-ion ESS\nthat can take part\
    \ in grid frequency management, and we develop a cost accounting model\nfor frequency\
    \ regulation that takes the effect of battery life loss into account. We determine\n\
    the anticipated working life and annual average cost of the Li-ion ESS for various\
    \ dead\nbands and SOC thresholds. Case studies demonstrate that the nominal lifetime\
    \ speciﬁed by\nthe manufacturer for standard and full discharge settings for the\
    \ Li-ion ESS is signiﬁcantly\nshorter than the predicted lifetime under practical\
    \ working conditions. This study offers an\naccurate costing method for ESS participating\
    \ in grid frequency regulation, which will aid\nin the expansion of ESS participation\
    \ in the market for peripheral services.\nThe controller’s charging cost can be\
    \ reduced numerically and repeatedly due to the\nmodel’s efﬁciency and simplicity.\
    \ The developed EV charge features indicate a balance\nbetween the following four\
    \ trends: (1) charging during times of low electricity prices;\n(2) charging slowly;\
    \ (3) charging near the end of the authorized charge period; and\n(4) preventing\
    \ vehicles from sending power back to the grid. The result shows that batteries\n\
    charged using optimized methods exceed those charged using standard methods by\
    \ a\nsigniﬁcant margin, using data from real hybrid electric vehicles. This suggests\
    \ that smaller\nbatteries could be used to satisfy the needs of vehicle life.\
    \ It has been shown that identical\npatterns hold true for batteries of different\
    \ sizes, allowing them to be used in both plug-in\nhybrids and pure electric vehicles\
    \ [43].\nNext, we go over a distributed water ﬁlling technique that can be applied\
    \ to networked\ncontrol systems where nodes communicate data with distant nodes.\
    \ Water ﬁlling is a well-\nknown method of communication system optimization.\
    \ It has assisted in solving real-world\ncontrol engineering and decision-making\
    \ problems. In a system with multiple control\npoints, the decentralized approach\
    \ of ﬁlling water tanks is described in this study. To do\nthis, we take into\
    \ account the water levels of a number of users who only interact with their\n\
    immediate neighbors and decide as a group. We create two versions of a new distributed\n\
    algorithm that combines consensus, proximity, and ﬁxed-point mapping theory (exact\
    \ and\napproximation), and we show that both converge. A charging station for\
    \ a ﬂeet of electric\nvehicles is used as an example to show how it works.\nThis\
    \ study shows how to charge a car’s battery in the most economical way possible,\n\
    taking into consideration the cost of power and the predicted cost as the battery\
    \ ages. It\ndoes this by using a simpliﬁed lithium-ion battery lifetime model.\
    \ The fundamental battery\nlife model shown below takes temperature, battery charge\
    \ level, and daily power loss\ninto account. This model’s correctness was demonstrated\
    \ by comparing it to a precise\nmodel developed at the National Renewable Energy\
    \ Laboratory. Comparing this model to\nexperimental results proved its validity.\
    \ The charger controller can use iterative numerical\ncharge cost minimization\
    \ because the basic model runs quickly. Electric car charging\nproﬁles strike\
    \ a compromise between four trends: quick charging, charging at the end\nof the\
    \ permitted charge period, charging at low-cost intervals, and prohibiting vehicles\n\
    from sending electricity back to the grid. Simulations using data from actual\
    \ Prius plug-in\nSustainability 2023, 15, 2603\n12 of 26\nhybrid EV show that\
    \ fully charged batteries may be used for a lot longer than batteries that\nhave\
    \ only been partially charged. This suggests that the requisite vehicle lifespan\
    \ could be\nmet with smaller batteries. These patterns have been shown to hold\
    \ true across a range of\nbattery sizes. They therefore apply to both plug-in\
    \ hybrid and pure electric vehicles.\n2.2. Types of Contactless EVS\nMagnetic\
    \ coupling coefﬁcients measure the effectiveness of the connection between\nthe\
    \ secondary and primary coils. Coupling coefﬁcients with high values are necessary\
    \ to\nmove a lot of power. The static wireless fast charger, which uses more than\
    \ 20 kW of power,\nis standardized. The OLEV and DWC initiatives’ main goals were\
    \ to make the technology\nmore efﬁcient and commercially viable. The OLEV project\
    \ raised the air gap to 20 cm and\nretained an efﬁciency of 83% while achieving\
    \ the needed high frequency of 20 kHz for\n60 kW.\nAdditionally, the horizontal\
    \ sensitivity was around 24 cm. The ﬁfth-generation diesel\nwater heater (DWC)\
    \ OLEV got its fuel supply from an S-type power station rail. Despite\nonly having\
    \ a 20 cm air gap tolerance and a side-to-side deviation tolerance of 30 cm, it\n\
    was nevertheless able to transmit 22 kW of electricity.\nA dynamic WPT can move\
    \ the 820 kW MPT for the high-speed train being built in\nKorea through the 5\
    \ cm air gap, with an efﬁciency of 83%. The roughly 16-km-long Bus\nRoute 16 in\
    \ Malaga, Spain has been overseen by a WPT since December 2014.\nOne of the most\
    \ popular methods of frequency control frequency droop control is\ncurrently being\
    \ successfully applied to electric cars. The major technique for controlling\n\
    the frequency now is to regulate the output of generators connected to the main\
    \ power\ngrid. As conventional power plants are phased out, electric vehicles\
    \ are a great alternative\nsince their batteries may be charged or discharged\
    \ in response to frequency deviation\nalarms. As a result, we investigate frequency\
    \ regulation in a power grid model with loads,\ntraditional producers, and a sizable\
    \ amount of EVs. By providing grid-related services,\nthese last devices, in particular\
    \ the FDC, autonomously optimize the grid. Two new control\nalgorithms that may\
    \ be used to manage electric car batteries in the best possible way during\nfrequency\
    \ regulation service. The control processes, on the one hand, make sure that\n\
    the power balance and frequency management of the main grid are stable. On the\
    \ other\nhand, these approaches can meet a range of EV charging needs. The available\
    \ methods\nare designed to reduce the failure rates of battery-powered devices.\
    \ This contrasts with\nthe EV literature, which frequently concentrates on determining\
    \ the ideal charge level.\nThe performance of the solutions is then contrasted\
    \ with that of other state-of-the-art\nV2G control systems. The results of numerical\
    \ experiments carried out using an accurate\nrepresentation of the power grid\
    \ show that the suggested methods perform well in actual\noperational circumstances\
    \ [44].\nResearchers are working to create integrated infrastructure solutions\
    \ that calculate the\namount of power transmitted through the system using a U-shaped\
    \ power station rail and\na frequency of 35 kHz. The near ﬁeld, where there is\
    \ no radiation, and the far ﬁeld, where\nthere is radiation, are two separate\
    \ areas in the electromagnetic ﬁelds created by the antenna\nof a moving electric\
    \ charge. The area surrounding the transmitter (T x) where the energy\nlevel is\
    \ constant is referred to as the “close ﬁeld.” If there is no receiver to receive\
    \ the energy,\nT x cannot transmit it. The size and shape of the transmitter and\
    \ receiver have an impact\non the near-ﬁeld ranges. Magnetic and electric ﬁelds\
    \ exist separately in non-radiating areas.\nElectrodes and coils can transmit\
    \ power through electric and magnetic ﬁelds. The force\ndecreases at a rate of\
    \ 1/r3 as the distance r between the transmitter and receiver grows,\nbut the\
    \ energy does not change. The rapid power loss rate results in the WPT electric\n\
    ﬁeld’s shorter range. The WPT magnetic ﬁeld can transmit power over greater distances\n\
    than alternative methods because magnetic ﬁelds can easily pass through solid\
    \ objects like\nfurniture, people, and walls. An electric vehicle’s air gap and\
    \ efﬁciency must be increased\nto improve WCS. The system’s size and surface area\
    \ can be reduced by altering its operating\nSustainability 2023, 15, 2603\n13\
    \ of 26\nfrequency. The WPT’s efﬁciency at a speciﬁc power level rises as its\
    \ frequency does as well.\nBusinesses, academic institutions, and research institutions\
    \ come in a wide variety.\nWe now have some responses after looking into wireless\
    \ charging. In reality, the WPT\nsystem rises by about 1 MHz before reaching a\
    \ high frequency. One option is a system that\nruns between 100 and 200 kHz. The\
    \ ﬁnal expression is T0 = M/R0. The mutual inductance\n(M) between the receiver\
    \ and transmitter (T-to-T) and equivalent resistance make up T0\n(R-to-zero).\
    \ To get the mouse a high TQ. The ideal conditions for maximizing TQ are high\n\
    driving frequency, high mutual inductance, and low equivalent resistance. The\
    \ inductance\nL and capacitance C are needed to determine the resonant frequency,\
    \ which equals 1/LC.\nThe resonant frequency will drop if L or C is increased.\
    \ When the frequency is raised to such\na high level, the conversion encounters\
    \ the switching problem. The WPT system is limited\nin its ability to operate\
    \ at high frequencies by a low coupling coefﬁcient. Because it is more\npractical,\
    \ the frequency is ﬁxed. When frequency standardization is used, the frequencies\n\
    are chosen to ensure that the system performs at its best. To promote wireless\
    \ electric\ncar charging systems, additional study is required on efﬁciency requirements,\
    \ operation\nfrequency, power level, electromagnetic interference (EMI), safety,\
    \ and testing technology\n(WEVCSs).\nA novel distributed control approach to charge\
    \ a ﬂeet of electric vehicles (EVs) as\neffectively as feasible when the overall\
    \ capacity of the power grid is limited. By resolving a\nscheduling issue, the\
    \ ideal charging can be discovered. Obtaining a cost-effective proﬁle of\nthe\
    \ entire quantity of energy utilized is the objective. In the resulting optimization\
    \ problem,\nthe decision variables are connected to both the objective function\
    \ and the constraint\nvariables. In our approach, people just communicate with\
    \ their near neighbors and take\ndecisions without seeking advice from those in\
    \ positions of authority. To ﬁnd a solution, a\ndistributed iterative algorithm\
    \ founded on the ideas of duality, closeness, and consensus is\nemployed. The\
    \ global optimum can be reached using this strategy, as demonstrated by a\nsimulated\
    \ case study.\nThe water ﬁlling technique can be used in networked control systems\
    \ with node-\nto-node communication that is independent. Water ﬁlling is a well-known\
    \ method of\ncommunication system optimization. It has assisted in solving real-world\
    \ control engi-\nneering and decision-making problems. In a system with multiple\
    \ control points, the\ndecentralized approach of ﬁlling water tanks may be used.\
    \ A study of interconnected water\nlevels among users who do not involve a central\
    \ authority structure and simply use them\nto communicate with their immediate\
    \ neighbors was conducted. In this study, we develop\na unique distributed algorithm\
    \ that combines ﬁxed point mapping theory, proximity theory,\nand consensus theory.\
    \ The algorithms’ exact and approximation implementations provide\nthe same outcome.\
    \ The charging of a ﬂeet of electric vehicles is used to illustrate how the\n\
    system functions [45].\nThe difﬁculty of charging a large number of electric vehicles\
    \ with infrastructure that\nhas a limited capacity led to the need for this study,\
    \ which examines the issue of individual\nload adjustment under overall capacity\
    \ limits. Distributed solutions to this challenge are\nbeing looked into for scalability\
    \ and communication ease. We discuss a number of dis-\ntributed algorithms for\
    \ maximizing utilization and relative average fairness using concepts\nfrom communication\
    \ networks (AIMD algorithms) and distributed convex optimization.\nWe give analytical\
    \ and simulation ﬁndings to demonstrate the effectiveness of these algo-\nrithms.\
    \ The algorithm’s performance in the analyzed circumstances is often within 5%\
    \ of\nthe ideal centralized case’s performance, but with substantially better\
    \ scalability and fewer\ncommunication requirements.\n2.3. Machine Learning Techniques\n\
    The many uses of ML algorithms have been widely covered in writing. The authors\n\
    in [4] investigated how SVM could be used to solve problems with more than two\
    \ classes.\nThe algorithm was developed so that it could be used to solve problems\
    \ involving multiple\ncategories by using various normalization techniques. The\
    \ algorithm passed thorough\nSustainability 2023, 15, 2603\n14 of 26\ntesting\
    \ and discovered that it was effective with a wide range of normalization techniques.\n\
    Its multiclass classiﬁcation accuracy was also quite impressive.\nDNN was also\
    \ examined by [46] for classifying multi-type images. As a result, the\nproposed\
    \ algorithm has a wide range of applications, even for famously challenging\n\
    to-label classes that are frequently misclassiﬁed by other ML techniques using\
    \ a DNN-\nimproved class identiﬁcation. The capacity of the algorithm to forecast\
    \ travel times was\nalso investigated by [47]. Its regressive predictions were\
    \ the most accurate and reliable of\nall the tested algorithms. One of the best\
    \ regression algorithms, especially for time series,\nis LSTM. Additionally, LSTM\
    \ helps classify issues because it can be used to create classes\nbased on number\
    \ intervals. Authors in [48] looked into using radio waves to estimate the\nrequired\
    \ energy and power (RF). RF was easy to use and produced accurate results. The\n\
    energy consumption model had a mean absolute percentage error of only 16%, making\
    \ it\nmore accurate than the autoregressive model. Creating a multiclass classiﬁcation\
    \ problem\nwas necessary for managing EV ﬂeets. As a result, many ML techniques\
    \ previously shown\nto be the most successful for classiﬁcation problems were\
    \ used. We assessed each model’s\naccuracy before contrasting and comparing its\
    \ performance to see if there were any viable\noptions for increasing the effectiveness\
    \ of the distribution network. The ML algorithm\nsummary is presented in Table\
    \ 1.\nTable 1. ML algorithm summary.\nAlgorithm\nAdvantages\nDisadvantages\nDecision\
    \ Tree (DT)\nData does not need to be sized or\nnormalized. Regression and classiﬁcation\n\
    analyses beneﬁt from it—highly accurate\npredictions and understanding.\nTraining\
    \ takes a while since even little\nchanges to the dataset could greatly impact\n\
    the ﬁnal structure.\nRandom Forest (RF)\nCapable of processing large datasets\
    \ with\nvarious factors and handling uncertainty\nwhile putting out the mean or\
    \ mode of\nseveral decision trees.\nThe problem is made harder by how many\ntrees\
    \ are created. Training typically\nrequires a sizable amount of time.\nSupport\
    \ Vector Machine (SVM)\nClassiﬁcation and regression are possible\nuses; the classiﬁer’s\
    \ performance\nis minimized.\nThe training procedure takes longer with\nlarge\
    \ datasets—poor performance when\nthere are more features than\ntraining samples.\n\
    K-Nearest Neighbors (KNN)\nBoth classiﬁcation and regression analysis\ncan beneﬁt\
    \ from its use. Shorter training\nsessions. Clear use of ideas.\nWe are having\
    \ poor performance when\ndealing with huge datasets. Poor\nperformance when dealing\
    \ with a large\nnumber of inputs. Poor performance when\ndealing with datasets\
    \ that are not balanced\nDeep Neural Network (DNN)\nA model that may be utilized\
    \ for various\ntasks, like classiﬁcation and regression.\nNeed enormous data sets\
    \ very prone to\nover ﬁtting. The ideal width and depth\ncannot be determined\
    \ using a\ngeneral principle.\nLong Short-Term Memory (LSTM)\nTime series detection,\
    \ accurate forecasting\nand adaptability to various applications\n(classiﬁcation\
    \ and regression).\nThere is no established methodology for\ndetermining the optimum\
    \ breadth and\ndepth. There is no theory for selecting\noptimal hyper parameters.\n\
    3. Methodology\n3.1. Machine Learning\nMachine learning studies focus on building\
    \ automatons that can learn new skills\nthrough observation and exposure to new\
    \ data. This area of research aims to create al-\ngorithms that allow machines\
    \ to remember and operate independently without human\nintervention. Data is provided\
    \ to a generic algorithm in machine learning, which builds\nits logic based on\
    \ the data rather than being preprogrammed. Supervised learning, unsu-\npervised\
    \ learning, and reinforcement learning are a few machine learning techniques [49]\n\
    Computers are used to make predictions in computational statistics. It is closely\
    \ related\nSustainability 2023, 15, 2603\n15 of 26\nto machine learning and frequently\
    \ crosses over with it. Mathematical optimization is\nknown to study how to make\
    \ optimization processes, theories, and applications better.\nMathematical optimization\
    \ is related to machine learning, as well. Unsupervised learning,\nalso known\
    \ as exploratory data analysis, is frequently highlighted when combining ma-\n\
    chine learning and data mining [50]. Unsupervised machine learning is a powerful\
    \ tool for\nunderstanding the typical behavior of many organisms and observing\
    \ constant movement\nfrom these patterns. Predictive analytics, or machine learning,\
    \ is a method for developing\ncomplicated algorithms and models that can be used\
    \ to forecast outcomes based on data\nanalysis. Even then, AI falls far short\
    \ of machine learning’s capabilities. When artiﬁcial\nintelligence (AI) was ﬁrst\
    \ developed, some scientists programmed computers to learn from\ntheir mistakes.\
    \ They looked for a solution using symbolic techniques like neural networks.\n\
    But as rational and knowledge-based approaches gain more attention, there is a\
    \ clear dis-\ntinction between AI and machine learning. Data collection and presentation\
    \ in probability\nsystems had theoretical and practical problems. Statistics lost\
    \ their value after 1980 when\ncyber systems had already exceeded artiﬁcial intelligence.\
    \ However, the statistical focus of\nother research was seen outside of AI in\
    \ pattern recognition and information retrieval. In\ncontrast, work on knowledge-based\
    \ learning continued inside AI and resulted in inductive\nlogic programming. Academics\
    \ in AI and CS started to ignore neural network research\nsimultaneously. Under\
    \ the umbrella of “connectionism”, researchers outside AI/CS, like\nHopﬁeld [51],\
    \ made comparable attempts. They had great success with backpropagation,\nwhich\
    \ they used in the middle of the 1980s [52]. Data mining and machine learning\
    \ are\nvery similar and use many methods. Machine learning entails making predictions\
    \ based\non previously discovered properties from training data. On the other\
    \ hand, data mining\nentails identifying novel properties (the step of analyzing\
    \ knowledge extraction in the\ndatabase). Machine learning frequently uses data\
    \ mining techniques for “unsupervised\nlearning” or as a step before learning\
    \ to increase accuracy. Except for the ECML PKDD,\nmost of the distinctions between\
    \ the two ﬁelds, which frequently have separate conferences\nand journals, are\
    \ founded on those assumptions. Discovering new information is typically\nhow\
    \ knowledge extraction and data mining (KDD) is judged effective. The effectiveness\n\
    of machine learning is generally assessed by how well it can replicate existing\
    \ data. Due\nto a lack of training data, supervised methods cannot be used in\
    \ a typical KDD task. Still,\nan unsupervised manner, also known as an “uninformed\
    \ method”, can relatively easily\noutperform other monitored methods. Machine\
    \ learning techniques that divide data into\ntraining and test sets, such as the\
    \ holdout method, enable extremely precise estimation\nof classiﬁcation models\
    \ (usually two-thirds of the data in the training set and one-third).\nIt also\
    \ assesses how successfully the model was trained using real test data. The N-fold\n\
    cross-validation method, in contrast, randomly selects k subsets from the data,\
    \ of which\nk1 is used to train the model, and k1 is used to assess the model’s\
    \ predictive capabil-\nity. The bootstrap technique, which uses a copy-and-paste\
    \ method to choose n random\nsamples from the data set, can be used in addition\
    \ to the holdout and cross-validation\ntechniques [52] to evaluate the model accurately\n\
    3.2. Learning Algorithms\nThe Q-learning algorithm was used by [53] in 2012 to\
    \ improve the power management\nsystem of electric and hybrid bicycles. In terms\
    \ of power management, these researchers\naimed to increase rider comfort and\
    \ safety and utilize battery power more effectively. Sim-\nulated results from\
    \ this study showed that the proposed power management system could\nincrease\
    \ riding comfort by 24% and energy efﬁciency by 50%. Since then, reinforcement\n\
    learning algorithms have been used in several studies to replace control optimization\
    \ theory\nfor HESS energy management. To ﬁnd the best way to maintain an HEV’s\
    \ battery at the\nideal charge level. The author in [53] used the Q-learning algorithm.\
    \ By combining this\nstrategy with a long-term plan, you can strike a balance\
    \ between maximum effectiveness\nand the capacity to act now. Atheros in [54]\
    \ employs “reverse reinforcement learning” to\ndevelop a probabilistic driving\
    \ path prediction system that determines the ideal engine-to-\nSustainability\
    \ 2023, 15, 2603\n16 of 26\nbattery power ratio based on the expected behavior\
    \ of the driver. Several papers on the\napplication of reinforcement learning\
    \ to control the power ﬂow in hybrid transmission\nsystems have recently been\
    \ published [54]. First, the adjustment, effectiveness, and learning\ncapacity\
    \ of a Q-learning-based energy management strategy for a hybrid tracked vehicle\n\
    was assessed [55]. The researchers then developed online Q-learning-based recursive\
    \ al-\ngorithms to allow real-time updates to control methods for hybrid transmission\
    \ systems.\nWhen the driver’s actions, location, and conditions on the road all\
    \ vary, these algorithms’\neffectiveness may likely decline.\n3.3. Proposed Methods\n\
    Driving cycle data could be used to ﬁnd ﬂexible energy management strategies using\n\
    advanced reinforcement learning. A comparison of rule-based energy management\
    \ strate-\ngies and those learned through DRL and online learning was done. The\
    \ examples show\nboth transactional and ad hoc neural networks. Machine learning\
    \ can be used to overcome\nthis issue in ﬁve different ways. A block diagram of\
    \ an electric vehicle’s static wireless\ncharging system is shown in Figure 6.\
    \ The receiver coil and transmitter coil are placed on\ntop of one another to\
    \ show how well the EVs work. Over time, using this approach will\nlessen pollution\
    \ and conserve our ﬁnite supply of conventional energy. Figure 7 shows\nproposed\
    \ methodology.\nSustainability 2023, 15, x FOR PEER REVIEW \n17 of 27 \n \nimum\
    \ effectiveness and the capacity to act now. Atheros in [54] employs “reverse\
    \ rein-\nforcement learning” to develop a probabilistic driving path prediction\
    \ system that deter-\nmines the ideal engine-to-battery power ratio based on the\
    \ expected behavior of the \ndriver. Several papers on the application of reinforcement\
    \ learning to control the power \nflow in hybrid transmission systems have recently\
    \ been published [54]. First, the adjust-\nment, effectiveness, and learning capacity\
    \ of a Q-learning-based energy management \nstrategy for a hybrid tracked vehicle\
    \ was assessed [55]. The researchers then developed \nonline Q-learning-based\
    \ recursive algorithms to allow real-time updates to control meth-\nods for hybrid\
    \ transmission systems. When the driver’s actions, location, and conditions \n\
    on the road all vary, these algorithms’ effectiveness may likely decline. \n3.3.\
    \ Proposed Methods \nDriving cycle data could be used to find flexible energy\
    \ management strategies using \nadvanced reinforcement learning. A comparison\
    \ of rule-based energy management strat-\negies and those learned through DRL\
    \ and online learning was done. The examples show \nboth transactional and ad\
    \ hoc neural networks. Machine learning can be used to overcome \nthis issue in\
    \ five different ways. A block diagram of an electric vehicle’s static wireless\
    \ \ncharging system is shown in Figure 6. The receiver coil and transmitter coil\
    \ are placed on \ntop of one another to show how well the EVs work. Over time,\
    \ using this approach will \nlessen pollution and conserve our finite supply of\
    \ conventional energy. Figure 7 shows \nproposed methodology. \n \nFigure 6. Contactless\
    \ charging types [56]. \nFigure 6. Contactless charging types [56].\nSustainability\
    \ 2023, 15, 2603\n17 of 26\nSustainability 2023, 15, x FOR PEER REVIEW \n18 of\
    \ 27 \n \n \nFigure 7. Proposed methodology. \n4. Data Acquisition \nWe discovered\
    \ a dataset about charging electric vehicles after searching for it on \nKaggle.\
    \ Details include the overall cost of the energy utilized, the date, and the length\
    \ of \neach session. In this dataset, 3395 EV charging sessions are covered in\
    \ great. A workplace \ncharging initiative involved 85 distinct EV drivers who\
    \ used 105 station sessions spread \nover 25 different sites. There are 24 resources\
    \ and 3395 specifics on the car. The effective-\nness of various machine learning\
    \ techniques is assessed and contrasted. The techniques \nused include deep neural\
    \ networks, k-nearest neighbors, long short-term memory, ran-\ndom forest, support\
    \ vector machines, and decision trees. All machine learning methods \nwere compared\
    \ using the same dataset to see which produced the most accurate results. \nAccording\
    \ to the results, it seemed that LSTM could help with EV control in some circum-\n\
    stances. The peak voltage, power losses, and voltage stability of the LSTM model\
    \ can all \nbe improved by flattening the load curve. We can lower our billing\
    \ costs by predicting \nincoming data. The proposed Smart Grid Electric Vehicle\
    \ Charging and Hybrid Energy \nStorage Management System’s dataset is described\
    \ in Table 2. \nTable 2. Summary of data set (Electric Vehicle Charging Dataset\
    \ Kaggle). \n1. \nSession ID \n13. \nstation Id \n2. \nKWH Total \n14. \nlocation\
    \ Id \n3. \nDollars \n15. \nmanager Vehicle \n4. \nCreated \n16. \nfacility Type\
    \ \nFigure 7. Proposed methodology.\n4. Data Acquisition\nWe discovered a dataset\
    \ about charging electric vehicles after searching for it on\nKaggle. Details\
    \ include the overall cost of the energy utilized, the date, and the length of\n\
    each session. In this dataset, 3395 EV charging sessions are covered in great.\
    \ A workplace\ncharging initiative involved 85 distinct EV drivers who used 105\
    \ station sessions spread\nover 25 different sites. There are 24 resources and\
    \ 3395 speciﬁcs on the car. The effectiveness\nof various machine learning techniques\
    \ is assessed and contrasted. The techniques used\ninclude deep neural networks,\
    \ k-nearest neighbors, long short-term memory, random forest,\nsupport vector\
    \ machines, and decision trees. All machine learning methods were compared\nusing\
    \ the same dataset to see which produced the most accurate results. According\
    \ to the\nresults, it seemed that LSTM could help with EV control in some circumstances.\
    \ The peak\nvoltage, power losses, and voltage stability of the LSTM model can\
    \ all be improved by\nﬂattening the load curve. We can lower our billing costs\
    \ by predicting incoming data. The\nproposed Smart Grid Electric Vehicle Charging\
    \ and Hybrid Energy Storage Management\nSystem’s dataset is described in Table\
    \ 2.\nA decision was made regarding how much data should be shared between Train\
    \ and\nTest (80–20%) for each neural network after considering the ﬁndings for\
    \ DNN, KNN, SVM,\nRF, DT and LSTM. Tests and simulations are performed using Mat\
    \ lab 2021a. It has an\n8.00 GB RAM, a hard drive, and an 11th Gen Intel(R) Core(TM)\
    \ i5-1135G7 @ 2.40 GHz\nhigh-performance PC CPU.\nSustainability 2023, 15, 2603\n\
    18 of 26\nTable 2. Summary of data set (Electric Vehicle Charging Dataset Kaggle).\n\
    1.\nSession ID\n13.\nstation Id\n2.\nKWH Total\n14.\nlocation Id\n3.\nDollars\n\
    15.\nmanager Vehicle\n4.\nCreated\n16.\nfacility Type\n5.\nEnded\n17.\nMon\n6.\n\
    Start Time\n18.\ntues\n7.\nEnd Time\n19.\nwed\n8.\nCharge Time Hrs\n20.\nThurs\n\
    9.\nWeek Day\n21.\nFri\n10.\nPlatform\n22.\nsat\n11.\nDistance\n23.\nsun\n12.\n\
    User ID\n24.\nreported zip\n5. Results\n5.1. Machine Learning Models:\n5.1.1.\
    \ Charging Station Classiﬁcation Results\nAccording to the charging station classiﬁcation,\
    \ the network has twelve different types\nof charging stations. Table 3 shows\
    \ how well other models direct EVs to the most effective\ncharging locations,\
    \ maximizing the effectiveness of the distribution network and lowering\nthe cost\
    \ of charging.\nTable 3. Accuracies of ML classiﬁer for charging station classiﬁcation.\n\
    Machine Learning Model\nAccuracy\nDecision Tree\n93%\nRandom Forest\n94%\nSVM\n\
    29%\nKNN\n41%\nDNN\n77%\nLSTM\n94%\nSVM and KNN cannot be used for EV routing,\
    \ as shown in Table 3 by their incredibly\nlow precision. The most accurate models,\
    \ with a 94% accuracy rate, are RF and LSTM.\nThe 93% precision of DT is identical\
    \ to that of RF and LSTM. DNN has a remarkable 77%\naccuracy rate. Expanding the\
    \ datasets and changing the hyper parameters, such as the\nnumber of layers and\
    \ nodes in each layer, can improve the system’s performance. These\nresults explain\
    \ that RF and LSTM perform better at multiclass classiﬁcation problems than\n\
    algorithms like SVM and KNN.\nFigure 8 shows the accuracies of ML classiﬁer for\
    \ charging station classiﬁcation.\n5.1.2. Classiﬁcation Based on Charging\nThree\
    \ categories of charging speeds have been created to reﬂect the range of uses\
    \ for\ncharging stations (fast charging, conventional charging, and V2G). How\
    \ well ML models\ncould see the ideal charging rate is shown in Table 4.\nSustainability\
    \ 2023, 15, 2603\n19 of 26\nSustainability 2023, 15, x FOR PEER REVIEW \n20 of\
    \ 27 \n \n \nFigure 8. Accuracies of ML classifier for charging station classification.\
    \ \n5.1.2. Classification Based on Charging \nThree categories of charging speeds\
    \ have been created to reflect the range of uses for \ncharging stations (fast\
    \ charging, conventional charging, and V2G). How well ML models \ncould see the\
    \ ideal charging rate is shown in Table 4. \nTable 4 demonstrates that only SVM\
    \ has a low level of precision. DT, KNN, and DNN \nhave accuracy rates of about\
    \ 83%, 83%, and 84%, respectively. The most accurate results \nare produced by\
    \ RF and LSTM models. LSTM is 4% more precise in determining the ideal \ncharging\
    \ rates than RF. The best ML model for classifying charging stations and vehicle\
    \ \nspeeds is LSTM. Because it addresses both classification issues, it is the\
    \ best strategy for \nmanaging an EV fleet. Its ability to recognize temporal\
    \ patterns in data about power con-\nsumption is one of the reasons it is so accurate.\
    \ \nTable 4. Accuracies of ML classifier for charging speed classification. \n\
    Machine Learning Model \nAccuracy \nDecision Tree \n83% \nRandom Forest  \n89%\
    \ \nSVM \n56% \nKNN \n83% \nDNN \n84% \nLSTM \n93% \nFigure 9 shows the accuracies\
    \ of ML classifier for charging speed classification. \nFigure 8. Accuracies of\
    \ ML classiﬁer for charging station classiﬁcation.\nTable 4. Accuracies of ML\
    \ classiﬁer for charging speed classiﬁcation.\nMachine Learning Model\nAccuracy\n\
    Decision Tree\n83%\nRandom Forest\n89%\nSVM\n56%\nKNN\n83%\nDNN\n84%\nLSTM\n93%\n\
    Table 4 demonstrates that only SVM has a low level of precision. DT, KNN, and\n\
    DNN have accuracy rates of about 83%, 83%, and 84%, respectively. The most accurate\n\
    results are produced by RF and LSTM models. LSTM is 4% more precise in determining\n\
    the ideal charging rates than RF. The best ML model for classifying charging stations\
    \ and\nvehicle speeds is LSTM. Because it addresses both classiﬁcation issues,\
    \ it is the best strategy\nfor managing an EV ﬂeet. Its ability to recognize temporal\
    \ patterns in data about power\nconsumption is one of the reasons it is so accurate.\n\
    Figure 9 shows the accuracies of ML classiﬁer for charging speed classiﬁcation.\n\
    Sustainability 2023, 15, x FOR PEER REVIEW \n21 of 27 \n \n \nFigure 9. Accuracies\
    \ of ML classifier for charging speed classification. \n5.2. The Impact of Uncertain\
    \ Load Data on the EV Management System \nThe load data is augmented with various\
    \ concentrations of Gaussian white noise \n(GWN) to simulate the data’s unpredictability\
    \ and add different degrees of uncertainty. \nWe evaluate previously successful\
    \ ML methods to see how well they can handle new \ntypes of luck. Thus, the impact\
    \ of tension on the system is researched, especially how it \naffects power losses\
    \ and load curves. \nFigure 9. Accuracies of ML classiﬁer for charging speed classiﬁcation.\n\
    Sustainability 2023, 15, 2603\n20 of 26\n5.2. The Impact of Uncertain Load Data\
    \ on the EV Management System\nThe load data is augmented with various concentrations\
    \ of Gaussian white noise\n(GWN) to simulate the data’s unpredictability and add\
    \ different degrees of uncertainty. We\nevaluate previously successful ML methods\
    \ to see how well they can handle new types\nof luck. Thus, the impact of tension\
    \ on the system is researched, especially how it affects\npower losses and load\
    \ curves.\nVariation in Load Data and Its Impact on Machine Learning Accuracy\n\
    The results of including uncertainty to the load data used to group charging stations\n\
    are shown in Table 5. By contrasting precision before and after adding delay,\
    \ we can\nidentify the change in precision. When there is uncertainty, DT’s accuracy\
    \ drops to 77%.\nAlthough still not as good as DT’s, RF’s accuracy drops to 86%.\
    \ The best model, LSTM,\nremained accurate 95% of the time and was unaffected\
    \ by uncertainty.\nTable 5. Classiﬁcation accuracy of a machine learning classiﬁer\
    \ using a 10% global weighted network\nfor Electric Vehicles charging stations.\n\
    Machine Learning Model\nAccuracy\nAccuracy Change\nDT\n76%\n−17%\nRF\n85%\n−9%\n\
    LSTM\n94%\n0\nThe impact of adding uncertainty to load data on the effectiveness\
    \ of the machine\nlearning algorithms used to categorize charging speeds is shown\
    \ in Table 3. In RF and\nDNN, accuracy has fallen by 1% and 2%, respectively.\
    \ The accuracy of DT, KNN, and LSTM\nremained unchanged when GWN was added. The\
    \ outcomes show that LSTM is adaptable\neven in the presence of GWN in the load\
    \ data and performs excellently both before and\nafter the introduction of uncertainty.\
    \ Machine Learning Classiﬁcation of Pricing Structures:\nAccuracy of Prediction:\
    \ 10% Standardization of Weights around the World is presented in\nTable 6.\n\
    Table 6. Machine learning classiﬁcation of pricing structures: Accuracy of prediction:\
    \ 10% standard-\nization of weights around the world.\nMachine Learning Model\n\
    Accuracy\nAccuracy Change\nDT\n83%\n0%\nRF\n88%\n−1%\nKNN\n84%\n0\nDNN\n82%\n\
    −2%\nLSTM\n93%\n0%\nML implementation Results for EV-HESMS is presented in Table\
    \ 6. In the proposed\nEV-HESMS model, 80% for training and 20% data for testing\
    \ taken to ﬁnd the Gradient\nLoss, Action Error and MSE.\nTable 7 shows how the\
    \ EV-HESMS Model behaves in terms of gradient Loss, MSE,\ntraining, and testing.\
    \ Efﬁciency by itself is insufﬁcient. The system’s loss and error for each\nmachine\
    \ learning technique using Gradient loss and Event error must also be determined.\n\
    The dataset and the capabilities of the system can be considered to a great extent\
    \ as the\ngradient loss, action error, and MSE are taken into account. Additionally,\
    \ LSTM is already\nthought to be more effective than other methods. In reality,\
    \ LSTM also considerably reduces\nerrors in this situation.\nSustainability 2023,\
    \ 15, 2603\n21 of 26\nTable 7. ML implementation Results for Proposed EV-HESMS\
    \ Model.\nMethods\nTrain (80%)\nTest (20%)\nGradient Loss\nAction Error\nMSE\n\
    1. Random Forest\n2716\n679\n0.366\n0.304\n0.342\n2. SVM\n2716\n679\n0.359\n0.449\n\
    0.303\n3. KNN\n2716\n679\n0.426\n0.402\n0.333\n4. DNN\n2716\n679\n0.390\n0.329\n\
    0.475\n5. LSTM\n2716\n679\n0.386\n0.310\n0.258\n5.3. Mathematically Model to Different\
    \ Calculated Parameters:\nControlling parameters can be changed in real-time in\
    \ a system with real-time con-\ntrol [34]. The method’s practicality was shown\
    \ by simulated experiments on a hardware\noperating system based on rechargeable\
    \ batteries and super capacitors. bmin is added to the\nestimate to get the minimum\
    \ battery load (w). The following defines the cost function: The\nedge is deleted\
    \ as unneeded if the sum of the values bmin(v) = bmin(w) + cost(e) is more than\
    \ M.\nC+\ne (bw) =\n\x1ACos(e)bw ≤ M − Cost(e)\n∞Otherwise\n(1)\nIf the original\
    \ edge, e = (v, m), has negative costs, we subtract the value of the price\nfrom\
    \ bmin (w). We obtain bmin(v). The cost function is given by:\nC−\ne (bw) =\n\x1A\
    −bwbw < −Cost(e)\nCost(e)Otherwise\n(2)\nMAPE =\nq\n∑N\ni=1\n(|xi−yi|)1\nxi\n\
    N\n× 100%\n(3)\nRMSE =\nv\nu\nu\nu\nu\nt\nN\n∑\ni=1\n(xi − yi)2\nN\n(4)\nr =\n\
    ∑N\ni=1(Xi − X′)(Yi − Y′)1\nq\n∑N\ni=1(Xi − X′)1 q\n∑N\ni=1(Yi − Y′)2\n(5)\nNumerous\
    \ solutions must be developed to deal with problems like excesses and\noverload\
    \ when electric vehicles are connected to a smart grid. This does not happen until\n\
    the most economical energy has been acquired. Using the deﬁned criteria, electric\
    \ vehicles\ncan compete for reduced electricity rates in a given area. They are\
    \ using price signals to\ndiscourage the charging of vehicles in crowded places\
    \ and make money through electric\ncar sharing. EV software uses a variety of\
    \ machine-learning techniques. Some of the most\npopular methods for analyzing\
    \ data and identifying its relevance include decision trees,\nANNs, SVMs, GRNNs,\
    \ and k-nearest neighbors (KNN). Usually, MAPE, r, and RMSE\nare used to evaluate\
    \ a forecast’s degree of accuracy. R is a common abbreviation for the\ncorrelation\
    \ coefﬁcient. Measures of how far off an estimate is from reality include the\n\
    root-mean-square error and mean absolute percentage error (MAPE). You can ﬁnd\
    \ the\ntools RMSE, r, and MAPE in EQU. In the ﬁrst case, the true value is represented\
    \ by Xi,\nwhile YI represents the close approximation. There are intriguing differences\
    \ between the\nmeans of the true value vector’s X and the projected value vector’s\
    \ Y. The letter N stands\nfor the whole cast set of values. Figure 10 show the\
    \ classiﬁcation accuracy of a machine\nlearning classiﬁer.\nSustainability 2023,\
    \ 15, 2603\n22 of 26\nmean-square error and mean absolute percentage error (MAPE).\
    \ You can find the tools \nRMSE, r, and MAPE in EQU. In the first case, the true\
    \ value is represented by Xi, while YI \nrepresents the close approximation. There\
    \ are intriguing differences between the means \nof the true value vector’s X\
    \ and the projected value vector’s Y. The letter N stands for the \nwhole cast\
    \ set of values. Figure 10 show the classification accuracy of a machine learning\
    \ \nclassifier.  \n \nFigure 10. Classification accuracy of a machine learning\
    \ classifier. \nFigure 11 shows the accuracy of prediction: 10% standardization\
    \ of weights around \nthe world. \n \nFigure 11. Accuracy of prediction: 10% standardization\
    \ of weights around the world. \nFigure 10. Classiﬁcation accuracy of a machine\
    \ learning classiﬁer.\nFigure 11 shows the accuracy of prediction: 10% standardization\
    \ of weights around\nthe world.\nrelation coefficient. Measures of how far off\
    \ an estimate is from reality include the root-\nmean-square error and mean absolute\
    \ percentage error (MAPE). You can find the tools \nRMSE, r, and MAPE in EQU.\
    \ In the first case, the true value is represented by Xi, while YI \nrepresents\
    \ the close approximation. There are intriguing differences between the means\
    \ \nof the true value vector’s X and the projected value vector’s Y. The letter\
    \ N stands for the \nwhole cast set of values. Figure 10 show the classification\
    \ accuracy of a machine learning \nclassifier.  \n \nFigure 10. Classification\
    \ accuracy of a machine learning classifier. \nFigure 11 shows the accuracy of\
    \ prediction: 10% standardization of weights around \nthe world. \n \nFigure 11.\
    \ Accuracy of prediction: 10% standardization of weights around the world. \n\
    Figure 11. Accuracy of prediction: 10% standardization of weights around the world.\n\
    Figure 12 shows the ML implementation results for proposed EV-HESMS model.\n5.4.\
    \ Limitations of the Proposed System\n(1)\nMore advanced deep learning, reinforcement\
    \ learning, and federated learning tech-\nniques can be used to centralize this\
    \ system, enabling more effective vehicle utilization\nregulation. Obviously,\
    \ if there are more providers vying for fewer stations, the prob-\nlem will improve.\n\
    (2)\nThe dataset should contain more parameters. Electric vehicle (EV) sales have\
    \ been\nincreasing, and this trend is likely to continue as prices fall and ranges\
    \ increase.\nCurrently, the percentage of electric cars (EVs) in use is quite\
    \ modest. Any company\nthat has a parking lot for clients or staff will be impacted\
    \ by this. Therefore, it is\ncrucial for businesses to consider how their energy\
    \ policy will change as a result of\nplanned changes to EVs and EV charging. Particularly\
    \ for businesses, having multiple\nchargers or fast-charging equipment on hand\
    \ is essential.\nSustainability 2023, 15, 2603\n23 of 26\n(3)\nAs automakers work\
    \ to reduce the amount of time needed to charge an EV so that\nthe “refueling”\
    \ process is more equivalent to that of a normal vehicle, chargers need\nmore\
    \ power.\nSustainability 2023, 15, x FOR PEER REVIEW \n24 of 27 \n \nFigure 12\
    \ shows the ML implementation results for proposed EV-HESMS model. \n \nFigure\
    \ 12. ML implementation results for proposed EV-HESMS model. \n5.4. Limitations\
    \ of the Proposed System \n(1) More advanced deep learning, reinforcement learning,\
    \ and federated learning tech-\nniques can be used to centralize this system,\
    \ enabling more effective vehicle utiliza-\ntion regulation. Obviously, if there\
    \ are more providers vying for fewer stations, the \nproblem will improve. \n\
    (2) The dataset should contain more parameters. Electric vehicle (EV) sales have\
    \ been \nincreasing, and this trend is likely to continue as prices fall and ranges\
    \ increase. Cur-\nrently, the percentage of electric cars (EVs) in use is quite\
    \ modest. Any company that \nhas a parking lot for clients or staff will be impacted\
    \ by this. Therefore, it is crucial \nfor businesses to consider how their energy\
    \ policy will change as a result of planned \nchanges to EVs and EV charging.\
    \ Particularly for businesses, having multiple \nchargers or fast-charging equipment\
    \ on hand is essential. \n(3) As automakers work to reduce the amount of time\
    \ needed to charge an EV so that \nthe “refueling” process is more equivalent\
    \ to that of a normal vehicle, chargers need \nmore power. \n6. Conclusions \n\
    Information and communication technology development is crucial to the world-\n\
    wide spread of smart cities (ICT). Every developing city needs a smart grid. Government\
    \ \nand corporate organizations are promoting using electric vehicles (EVs) to\
    \ reduce green-\nhouse gas emissions and combat climate change. Electric automobiles\
    \ have sparked sev-\neral previously unforeseen problems due to their greater\
    \ presence in contemporary, so-\nphisticated power networks. Two significant issues\
    \ are implementing cost-saving tech-\nnology for controlling energy supply and\
    \ demand and creating more efficient techniques \nfor invoicing clients. This\
    \ issue has received various possible solutions that have been put \nforward.\
    \ This entails a thorough investigation of charging procedures, industry stand-\n\
    ards, and different data-driven models and machine-learning techniques to facilitate\
    \ the \nseamless integration of electric vehicles into the smart grid. We examine\
    \ the most recent \ndevelopments in smart grid-based energy management services\
    \ and applications and the \ngrowing appeal of electric vehicles. This indicates\
    \ that individuals with a say in infrastruc-\nFigure 12. ML implementation results\
    \ for proposed EV-HESMS model.\n6. Conclusions\nInformation and communication\
    \ technology development is crucial to the worldwide\nspread of smart cities (ICT).\
    \ Every developing city needs a smart grid. Government and\ncorporate organizations\
    \ are promoting using electric vehicles (EVs) to reduce greenhouse\ngas emissions\
    \ and combat climate change. Electric automobiles have sparked several\npreviously\
    \ unforeseen problems due to their greater presence in contemporary, sophisti-\n\
    cated power networks. Two signiﬁcant issues are implementing cost-saving technology\
    \ for\ncontrolling energy supply and demand and creating more efﬁcient techniques\
    \ for invoicing\nclients. This issue has received various possible solutions that\
    \ have been put forward. This\nentails a thorough investigation of charging procedures,\
    \ industry standards, and different\ndata-driven models and machine-learning techniques\
    \ to facilitate the seamless integration\nof electric vehicles into the smart\
    \ grid. We examine the most recent developments in\nsmart grid-based energy management\
    \ services and applications and the growing appeal of\nelectric vehicles. This\
    \ indicates that individuals with a say in infrastructure development\nmust consider\
    \ the health of communities, public safety, access to electricity and information,\n\
    the provision of services, and other issues [11]. This kind of smart city and\
    \ technology\ndevelopment will be effective when all stakeholders and important\
    \ factors are considered.\nHurdles and opportunities have emerged as technology\
    \ have advanced toward long-term\nsolutions. We talk about conductive and inductive\
    \ charging for electric cars. The most\nimportant studies that have been done\
    \ on electric vehicles, connector hybrid electric vehicle\ntypes, charging rates,\
    \ and battery capacity, and these topics are reviewed. This paper\nanalyzes current\
    \ advancements in ﬁxed and portable wireless charging technology. Al-\nthough\
    \ there are international standards for wirelessly charging electric vehicles,\
    \ different\nwireless charging systems employ a range of frequencies. The efﬁcacy\
    \ of contemporary\nmachine learning algorithms and robotic models is being assessed\
    \ to integrate the smart\ngrid. We examine the techniques presently used to calculate\
    \ the driving range, charging\ntime, and trafﬁc impact of electric vehicles. It\
    \ is necessary to study the effects of the public\ninfrastructure’s energy efﬁciency,\
    \ robustness, and dependability on the economy, society,\nand environment to make\
    \ widespread usage of electric vehicles feasible. Discussions and\nanalyses will\
    \ focus on wireless power transmission systems in the future. These systems,\n\
    Sustainability 2023, 15, 2603\n24 of 26\nwhich include those that carry energy\
    \ inside cars and those that do so between autos and\nthe grid, will be of great\
    \ importance. Research on mobile energy storage and delivery\ninfrastructures\
    \ based on electric vehicles may help us better understand how to incorporate\n\
    renewable energy sources into distributed micro grids.\nAuthor Contributions:\
    \ Conceptualization, T.M.; methodology, T.M., R.N.A.; software, T.M., R.N.A.,\n\
    M.A.M. and S.A.; validation, T.M., M.I. and M.K.; formal analysis, S.A.; investigation,\
    \ T.M and I.H.;\nresources, T.M. and I.H.; data curation, T.M., M.A.N.; writing—original\
    \ draft preparation, T.M.;\nwriting—review and editing, T.M., M.A.M. and M.A.N.;\
    \ visualization, T.M. All authors have read\nand agreed to the published version\
    \ of the manuscript.\nFunding: This research received no external funding.\nInstitutional\
    \ Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\n\
    Data Availability Statement: Not applicable.\nConﬂicts of Interest: The authors\
    \ declare no conﬂict of interest.\nReferences\n1.\nIEA, C. Global EV Outlook 2020.\
    \ 2020. Available online: https://www.iea.org/reports/global-ev-outlook-2020 (accessed\
    \ on\n1 February 2022).\n2.\nDhakal, T.; Kangwon National University; Min, K.-S.\
    \ Macro Study of Global Electric Vehicle Expansion. Foresight STI Gov. 2021,\n\
    15, 67–73. [CrossRef]\n3.\nElmenshawy, M.; Massoud, A. Modular Isolated DC-DC\
    \ Converters for Ultra-Fast EV Chargers: A Generalized Modeling and\nControl Approach.\
    \ Energies 2020, 13, 2540. [CrossRef]\n4.\nShibl, M.; Ismail, L.; Massoud, A.\
    \ Electric Vehicles Charging Management Using Machine Learning Considering Fast\
    \ Charging\nand Vehicle-to-Grid Operation. Energies 2021, 14, 6199. [CrossRef]\n\
    5.\nLytras, M.D.; Visvizi, A.; Jussila, J. Social media mining for smart cities\
    \ and smart villages research. Soft Comput. 2020, 24,\n10983–10987. [CrossRef]\n\
    6.\nMorvaj, B.; Lugaric, L.; Krajcar, S. Demonstrating smart buildings and smart\
    \ grid features in a smart energy city. In Proceedings\nof the 2011 3rd International\
    \ Youth Conference on Energetics (IYCE), Leiria, Portugal, 7–9 July 2011.\n7.\n\
    Qaisar, S.M.; Alyamani, N. A Review of Charging Schemes and Machine Learning Techniques\
    \ for Intelligent Management of\nElectric Vehicles in Smart Grid. Manag. Smart\
    \ Cities 2022, 51–71. [CrossRef]\n8.\nAxelsson, K.; Granath, M. Stakeholders’\
    \ stake and relation to smartness in smart city development: Insights from a Swedish\
    \ city\nplanning project. Gov. Inf. Q. 2018, 35, 693–702. [CrossRef]\n9.\nLytras,\
    \ M.D.; Chui, K.T. The Recent Development of Artiﬁcial Intelligence for Smart\
    \ and Sustainable Energy Systems and\nApplications. Energies 2019, 12, 3108. [CrossRef]\n\
    10.\nLiu, Z.; Wu, Q.; Huang, S.; Wang, L.; Shahidehpour, M.; Xue, Y. Optimal Day-Ahead\
    \ Charging Scheduling of Electric Vehicles\nThrough an Aggregative Game Model.\
    \ IEEE Trans. Smart Grid 2017, 9, 5173–5184. [CrossRef]\n11.\nWang, T.; Liang,\
    \ Y.; Jia, W.; Arif, M.; Liu, A.; Xie, M. Coupling resource management based on\
    \ fog computing in smart city\nsystems. J. Netw. Comput. Appl. 2019, 135, 11–19.\
    \ [CrossRef]\n12.\nDericio˘glu, C.; Yirik, E.; Ünal, E.; Cuma, M.U.; Onur, B.;\
    \ Tümay, M. A Review of Charging Technologies for Commercial Electric\nVehicles.\
    \ Int. J. Adv. Automot. Technol. 2018, 2, 61–70. [CrossRef]\n13.\nWang, S.; Wan,\
    \ J.; Li, D.; Zhang, C. Implementing smart factory of industrie 4. 0: An outlook.\
    \ Int. J. Distrib. Sens. Netw. 2016,\n12, 3159805.\n14.\nDanish, M.S.S.; Bhattacharya,\
    \ A.; Stepanova, D.; Mikhaylov, A.; Grilli, M.L.; Khosravy, M.; Senjyu, T. A Systematic\
    \ Review of\nMetal Oxide Applications for Energy and Environmental Sustainability.\
    \ Metals 2020, 10, 1604. [CrossRef]\n15.\nEhsani, M.; Gao, Y.; Gay, S.E.; Emadi,\
    \ A. Modern Electric, Hybrid Electric, and Fuel Cell Vehicles; CRC Press: Boca\
    \ Raton, FL, USA, 2018.\n16.\nCattaruzza, D.; Absi, N.; Feillet, D.; González-Feliu,\
    \ J. Vehicle routing problems for city logistics. EURO J. Transp. Logist. 2017,\
    \ 6,\n51–79. [CrossRef]\n17.\nXydas, E.S.; Marmaras, C.E.; Cipcigan, L.M.; Hassan,\
    \ A.S.; Jenkins, N. Forecasting electric vehicle charging demand using support\n\
    vector machines. In Proceedings of the 2013 48th International Universities’ Power\
    \ Engineering Conference (UPEC), Dublin,\nIreland, 2–5 September 2013.\n18.\n\
    Sadeghian, O.; Oshnoei, A.; Mohammadi-Ivatloo, B.; Vahidinasab, V.; Anvari-Moghaddam,\
    \ A. A comprehensive review on\nelectric vehicles smart charging: Solutions, strategies,\
    \ technologies, and challenges. J. Energy Storage 2022, 54, 105241. [CrossRef]\n\
    19.\nVu, Q.V.; Dinh, A.H.; Van Thien, N.; Tran, H.T.; Le Xuan, H.; Van Hung, P.;\
    \ Kim, D.T.; Nguyen, L. An Adaptive Hierarchical\nSliding Mode Controller for\
    \ Autonomous Underwater Vehicles. Electronics 2021, 10, 2316. [CrossRef]\nSustainability\
    \ 2023, 15, 2603\n25 of 26\n20.\nHuang, X.; Shi, J.; Gao, B.; Tai, Y.; Chen, Z.;\
    \ Zhang, J. Forecasting Hourly Solar Irradiance Using Hybrid Wavelet Transformation\n\
    and Elman Model in Smart Grid. IEEE Access 2019, 7, 139909–139923. [CrossRef]\n\
    21.\nQuesada, J.A.; Lopez-Pineda, A.; Gil-Guillén, V.F.; Durazo-Arvizu, R.; Orozco-Beltrán,\
    \ D.; López-Domenech, A.; Carratalá-\nMunuera, C. Machine learning to predict\
    \ cardiovascular risk. Int. J. Clin. Pr. 2019, 73, e13389. [CrossRef]\n22.\nZhai,\
    \ Z.; Su, S.; Liu, R.; Yang, C.; Liu, C. Agent–cellular automata model for the\
    \ dynamic ﬂuctuation of EV trafﬁc and charging\ndemands based on machine learning\
    \ algorithm. Neural Comput. Appl. 2018, 31, 4639–4652. [CrossRef]\n23.\nRigas,\
    \ E.S.; Ramchurn, S.D.; Bassiliades, N. Managing Electric Vehicles in the Smart\
    \ Grid Using Artiﬁcial Intelligence: A Survey.\nIEEE Trans. Intell. Transp. Syst.\
    \ 2014, 16, 1619–1635. [CrossRef]\n24.\nSangdehi, S.M.M.; Hamidifar, S.; Kar,\
    \ N.C. A Novel Bidirectional DC/AC Stacked Matrix Converter Design for Electriﬁed\
    \ Vehicle\nApplications. IEEE Trans. Veh. Technol. 2014, 63, 3038–3050. [CrossRef]\n\
    25.\nSultana, B.; Mustafa, M. Impact of reconﬁguration and demand response program\
    \ considering electrical vehicles in smart\ndistribution network. In Proceedings\
    \ of the 3rd International Electrical Engineering Conference (IEEC 2018), Karachi,\
    \ Pakistan,\n9–10 February 2018.\n26.\nRamirez-Vazquez, R.; Gonzalez-Rubio, J.;\
    \ Escobar, I.; Suarez Rodriguez, C.D.P.; Arribas, E. Personal exposure assessment\
    \ to Wi-Fi\nradiofrequency electromagnetic ﬁelds in Mexican microenvironments.\
    \ Int. J. Environ. Res. Public Health 2021, 18, 1857. [CrossRef]\n[PubMed]\n27.\n\
    Lund, P.D.; Byrne, J.; Haas, R.; Flynn, D. (Eds.) The role of electric vehicles\
    \ in smart grids. In Advances in Energy Systems: The\nLarge-scale Renewable Energy\
    \ Integration Challenge; John Wiley & Sons: Hoboken, NJ, USA, 2019; pp. 245–264.\n\
    28.\nAmer, A.; Shaban, K.; Gaouda, A.; Massoud, A. Home Energy Management System\
    \ Embedded with a Multi-Objective Demand\nResponse Optimization Model to Beneﬁt\
    \ Customers and Operators. Energies 2021, 14, 257. [CrossRef]\n29.\nAshraf, S.\
    \ Culminate Coverage for Sensor Network Through Bodacious-instance Mechanism.\
    \ In I-Manager’s Journal on Wireless\nCommunication Networks; I-Manager Publications:\
    \ Nagercoil, India, 2019; Volume 8, p. 1. [CrossRef]\n30.\nJian, L.; Xue, H.;\
    \ Xu, G.; Zhu, X.; Zhao, D.; Shao, Z.Y. Regulated Charging of Plug-in Hybrid Electric\
    \ Vehicles for Minimizing\nLoad Variance in Household Smart Microgrid. IEEE Trans.\
    \ Ind. Electron. 2012, 60, 3218–3226. [CrossRef]\n31.\nMoghaddam, Z.; Ahmad, I.;\
    \ Habibi, D.; Phung, Q.V. Smart Charging Strategy for Electric Vehicle Charging\
    \ Stations. IEEE Trans.\nTransp. Electriﬁcation 2017, 4, 76–88. [CrossRef]\n32.\n\
    Hu, J.; Morais, H.; Sousa, T.; Lind, M. Electric vehicle ﬂeet management in smart\
    \ grids: A review of services, optimization and\ncontrol aspects. Renew. Sustain.\
    \ Energy Rev. 2016, 56, 1207–1226. [CrossRef]\n33.\nKarmaker, A.K.; Roy, S.; Ahmed,\
    \ R. Analysis of the impact of electric vehicle charging station on power quality\
    \ issues. In\nProceedings of the 2019 International Conference on Electrical,\
    \ Computer and Communication Engineering (ECCE), Chittagong,\nBangladesh, 7–9\
    \ February 2019.\n34.\nAshraf, S.; Alfandi, O.; Ahmad, A.; Khattak, A.M.; Hayat,\
    \ B.; Kim, K.H.; Ullah, A. Bodacious-Instance Coverage Mechanism for\nWireless\
    \ Sensor Network. Wirel. Commun. Mob. Comput. 2020, 2020, 8833767. [CrossRef]\n\
    35.\nDileep, G. A survey on smart grid technologies and applications. Renew. Energy\
    \ 2019, 146, 2589–2625. [CrossRef]\n36.\nYilmaz, M.; Krein, P.T. Review of the\
    \ Impact of Vehicle-to-Grid Technologies on Distribution Systems and Utility Interfaces.\
    \ IEEE\nTrans. Power Electron. 2012, 28, 5673–5689. [CrossRef]\n37.\nCarli, R.;\
    \ Dotoli, M. A Distributed Control Algorithm for Optimal Charging of Electric\
    \ Vehicle Fleets with Congestion Management.\nIFAC-PapersOnLine 2018, 51, 373–378.\
    \ [CrossRef]\n38.\nYong, J.Y.; Ramachandaramurthy, V.K.; Tan, K.M.; Mithulananthan,\
    \ N. A review on the state-of-the-art technologies of electric\nvehicle, its impacts\
    \ and prospects. Renew. Sustain. Energy Rev. 2015, 49, 365–385. [CrossRef]\n39.\n\
    Clairand, J.-M.; Guerra-Terán, P.; Serrano-Guerrero, X.; González-Rodríguez, M.;\
    \ Escrivá-Escrivá, G. Electric Vehicles for Public\nTransportation in Power Systems:\
    \ A Review of Methodologies. Energies 2019, 12, 3114. [CrossRef]\n40.\nPanchal,\
    \ C.; Stegen, S.; Lu, J. Review of static and dynamic wireless electric vehicle\
    \ charging system. Eng. Sci. Technol. Int. J. 2018,\n21, 922–937. [CrossRef]\n\
    41.\nStüdli, S.; Crisostomi, E.; Middleton, R.; Shorten, R. A ﬂexible distributed\
    \ framework for realising electric and plug-in hybrid\nvehicle charging policies.\
    \ Int. J. Control. 2012, 85, 1130–1145. [CrossRef]\n42.\nAshraf, S.; Ahmed, T.;\
    \ Saleem, S. NRSM: Node redeployment shrewd mechanism for wireless sensor network.\
    \ Iran J. Comput. Sci.\n2020, 4, 171–183. [CrossRef]\n43.\nHoke, A.; Brissette,\
    \ A.; Smith, K.; Pratt, A.; Maksimovic, D. Accounting for Lithium-Ion Battery\
    \ Degradation in Electric Vehicle\nCharging Optimization. IEEE J. Emerg. Sel.\
    \ Top. Power Electron. 2014, 2, 691–700. [CrossRef]\n44.\nScarabaggio, P.; Carli,\
    \ R.; Cavone, G.; Dotoli, M. Smart control strategies for primary frequency regulation\
    \ through electric\nvehicles: A battery degradation perspective. Energies 2020,\
    \ 13, 4586. [CrossRef]\n45.\nCarli, R.; Dotoli, M. A Distributed Control Algorithm\
    \ for Waterﬁlling of Networked Control Systems via Consensus. IEEE Control\nSyst.\
    \ Lett. 2017, 1, 334–339. [CrossRef]\n46.\nMurthy, V.N.; Singh, V.; Chen, T.;\
    \ Manmatha, R.; Comaniciu, D. Deep decision network for multi-class image classiﬁcation.\
    \ In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\
    \ Las Vegas, NV, USA, 27–29 June 2016.\n47.\nMa, X.; Tao, Z.; Wang, Y.; Yu, H.;\
    \ Wang, Y. Long short-term memory neural network for trafﬁc speed prediction using\
    \ remote\nmicrowave sensor data. Transp. Res. Part C Emerg. Technol. 2015, 54,\
    \ 187–197. [CrossRef]\nSustainability 2023, 15, 2603\n26 of 26\n48.\nDeligiannis,\
    \ P.; Koutroubinas, S.; Koronias, G. Predicting Energy Consumption Through Machine\
    \ Learning Using a Smart-\nMetering Architecture. IEEE Potentials 2019, 38, 29–34.\
    \ [CrossRef]\n49.\nManishimwe, A.; Alexander, H.; Kaluuma, H.; Dida, M.A. Integrated\
    \ Mobile Application Based on Machine Learning for East\nAfrica Stock Market.\
    \ J. Inf. Syst. Eng. Manag. 2021, 6, em0143. [CrossRef]\n50.\nFernandes, A.; Salazar,\
    \ L.H.; Dazzi, R.; Garcia, N.; Leithardt, V.R.Q. Using Different Models of Machine\
    \ Learning to Predict\nAttendance at Medical Appointments. J. Inf. Syst. Eng.\
    \ Manag. 2020, 5, em0122. [CrossRef] [PubMed]\n51.\nRumelhart, D.E.; Smolensky,\
    \ P.; McClelland, J.L.; Hinton, G.E. is achieved. Prior to stabilization, neural\
    \ networks do not jump\naround between points in activation space. Stabiliza-tion\
    \ is the process whereby a network ﬁrst generates a de-terminate\nactivation pattern,\
    \ and thereby arrives at a point in activation space. Behav. Brain Sci. 2004,\
    \ 27, 2.\n52.\nSarker, I.H. Machine Learning: Algorithms, Real-World Applications\
    \ and Research Directions. SN Comput. Sci. 2021, 2, 1–21.\n[CrossRef] [PubMed]\n\
    53.\nHsu, R.C.; Lin, T.-H.; Su, P.-C. Dynamic Energy Management for Perpetual\
    \ Operation of Energy Harvesting Wireless Sensor\nNode Using Fuzzy Q-Learning.\
    \ Energies 2022, 15, 3117. [CrossRef]\n54.\nGui, T.; Liu, P.; Zhang, Q.; Zhu,\
    \ L.; Peng, M.; Zhou, Y.; Huang, X. Mention recommendation in Twitter with cooperative\
    \ multi-\nagent reinforcement learning. In Proceedings of the 42nd International\
    \ ACM SIGIR Conference on Research and Development in\nInformation Retrieval,\
    \ Paris, France, 21–25 July 2019.\n55.\nLiu, T.; Zou, Y.; Liu, D.; Sun, F. Reinforcement\
    \ Learning of Adaptive Energy Management with Transition Probability for a Hybrid\n\
    Electric Tracked Vehicle. IEEE Trans. Ind. Electron. 2015, 62, 7837–7846. [CrossRef]\n\
    56.\nAlsharif, A.; Wei, T.C.; Ayop, R.; Lau, K.Y.; Bukar, A.L. A Review of the\
    \ Smart Grid Communication Technologies in Contactless\nCharging with Vehicle\
    \ to Grid Integration Technology. J. Integr. Adv. Eng. 2021, 1, 11–20. [CrossRef]\n\
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all\
    \ publications are solely those of the individual\nauthor(s) and contributor(s)\
    \ and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility\
    \ for any injury to\npeople or property resulting from any ideas, methods, instructions\
    \ or products referred to in the content.\n"
  inline_citation: '>'
  journal: Sustainability (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2071-1050/15/3/2603/pdf?version=1675240701
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Electric Vehicle Charging System in the Smart Grid Using Different Machine
    Learning Methods
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1287/trsc.2021.1097
  analysis: '>'
  authors:
  - Karsten Schroer
  - Wolfgang Ketter
  - Thomas Y. Lee
  - Alok Gupta
  - Micha Kahlen
  citation_count: 11
  full_citation: '>'
  full_text: '>

    This website stores cookies on your computer. These cookies are used to collect
    information about how you interact with our website and allow us to remember you.
    We use this information in order to improve and customize your browsing experience
    and for analytics and metrics about our visitors both on this website and other
    media. To find out more about the cookies we use, see our Privacy Policy. If you
    decline, your information won’t be tracked when you visit this website. A single
    cookie will be used in your browser to remember your preference not to be tracked.
    Accept Decline INFORMS.org Certified Analytics Professional INFORMS Connect Career
    Center INFORMS Analytics Conference 2024 brought to you byUniversity of Nebraska
    Log In Help Skip main navigation JOURNALS MAGAZINES PUBLICATIONS PRICING & SUBSCRIPTIONS  Advanced
    Search JOURNAL HOMEARTICLES IN ADVANCECURRENT ISSUEARCHIVESABOUT SUBMIT SUBSCRIBE
    View PDF Tools Share Go to Section Abstract HomeTransportation ScienceVol. 56,
    No. 1 Data-Driven Competitor-Aware Positioning in On-Demand Vehicle Rental Networks
    Karsten Schroer , Wolfgang Ketter , Thomas Y. Lee, Alok Gupta , Micha Kahlen Published
    Online:3 Dec 2021https://doi.org/10.1287/trsc.2021.1097 Abstract We study a novel
    operational problem that considers vehicle positioning in on-demand rental networks,
    such as car sharing in the wider context of a competitive market in which users
    select vehicles based on access. Existing approaches consider networks in isolation;
    our competitor-aware model takes supply situations of competing networks into
    account. We combine online machine learning to predict market-level demand and
    supply with dynamic mixed integer nonlinear programming. For evaluation, we use
    discrete event simulation based on real-world data from Car2Go and DriveNow. Our
    model outperforms conventional models that consider the fleet in isolation by
    a factor of two in terms of profit improvements. In the case we study, the highest
    theoretical profit improvements of 7.5% are achieved with a dynamic model. Operators
    of on-demand rental networks can use our model under existing market conditions
    to build a profitable competitive advantage by optimizing access for consumers
    without the need for fleet expansion. Model effectiveness increases further in
    realistic scenarios of fleet expansion and demand growth. Our model accommodates
    rising demand, defends against competitors’ fleet expansion, and enhances the
    profitability of own fleet expansions. Previous Back to Top Next Figures References
    Related Information Volume 56, Issue 1 January-February 2022 Pages 1-264, C2 Article
    Information Supplemental Materials Metrics Downloaded 113 times in the past 12
    months Cited 11 times Information Received:October 20, 2020 Accepted:August 27,
    2021 Published Online:December 03, 2021 Copyright © 2021, INFORMS Cite as Karsten
    Schroer, Wolfgang Ketter, Thomas Y. Lee, Alok Gupta, Micha Kahlen (2021) Data-Driven
    Competitor-Aware Positioning in On-Demand Vehicle Rental Networks. Transportation
    Science 56(1):182-200. https://doi.org/10.1287/trsc.2021.1097 Keywords machine
    learning online optimization optimal positioning sharing economy Car2Go PDF download
    Sign Up for INFORMS Publications Updates and News SIGN UP Partners The Institute
    for Operations Research and the Management Sciences 5521 Research Park Drive,
    Suite 200 Catonsville, MD 21228 USA phone 1 443-757-3500 phone 2 800-4INFORMS
    (800-446-3676) fax 443-757-3515 email informs@informs.org Get the Latest Updates
    Discover INFORMS Explore OR & Analytics Get Involved Impact Join Us Recognizing
    Excellence Professional Development Resource Center Meetings & Conferences Publications
    About INFORMS Communities PubsOnLine Regional Analytics 2023 Certified Analytics
    Professional Career Center INFORMS Connect Copyright 2024 INFORMS. All Rights
    Reserved INFORMS Code of Conduct Terms of Use Privacy Contact INFORMS Sitemap
    Follow INFORMS on: Share on Twitter Facebook Linked In INFORMS site uses cookies
    to store information on your computer. Some are essential to make our site work;
    Others help us improve the user experience. By using this site, you consent to
    the placement of these cookies. Please read our Privacy Statement to learn more.
    Agree'
  inline_citation: '>'
  journal: Transportation science
  limitations: '>'
  pdf_link: null
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Data-Driven Competitor-Aware Positioning in On-Demand Vehicle Rental Networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/bdcc6010018
  analysis: '>'
  authors:
  - Hafiz Suliman Munawar
  - Fahim Ullah
  - Siddra Qayyum
  - Danish Shahzad
  citation_count: 35
  full_citation: '>'
  full_text: ">\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\n\
    Citation: Munawar, H.S.; Ullah, F.;\nQayyum, S.; Shahzad, D. Big Data in\nConstruction:\
    \ Current Applications\nand Future Opportunities. Big Data\nCogn. Comput. 2022,\
    \ 6, 18. https://\ndoi.org/10.3390/bdcc6010018\nAcademic Editors: Domenico Talia\n\
    and Fabrizio Marozzo\nReceived: 6 December 2021\nAccepted: 3 February 2022\nPublished:\
    \ 6 February 2022\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional\
    \ claims in\npublished maps and institutional afﬁl-\niations.\nCopyright:\n© 2022\
    \ by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open\
    \ access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative\
    \ Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nbig data and \ncognitive computing\nReview\nBig Data in Construction:\
    \ Current Applications and\nFuture Opportunities\nHaﬁz Suliman Munawar 1\n, Fahim\
    \ Ullah 2,*\n, Siddra Qayyum 1\nand Danish Shahzad 3\n1\nSchool of the Built Environment,\
    \ University of New South Wales, Sydney, NSW 2052, Australia;\nh.munawar@unsw.edu.au\
    \ (H.S.M.); s.qayyum@unsw.edu.au (S.Q.)\n2\nSchool of Surveying and Built Environment,\
    \ University of Southern Queensland,\nSpringﬁeld Central, QLD 4300, Australia\n\
    3\nDepartment of Visual Computing, University of Saarland, 66123 Saarbrücken,\
    \ Germany;\ndani.shahzad87@gmail.com\n*\nCorrespondence: fahim.ullah@usq.edu.au\n\
    Abstract: Big data have become an integral part of various research ﬁelds due\
    \ to the rapid advance-\nments in the digital technologies available for dealing\
    \ with data. The construction industry is no\nexception and has seen a spike in\
    \ the data being generated due to the introduction of various digital\ndisruptive\
    \ technologies. However, despite the availability of data and the introduction\
    \ of such tech-\nnologies, the construction industry is lagging in harnessing\
    \ big data. This paper critically explores\nliterature published since 2010 to\
    \ identify the data trends and how the construction industry can\nbeneﬁt from\
    \ big data. The presence of tools such as computer-aided drawing (CAD) and building\n\
    information modelling (BIM) provide a great opportunity for researchers in the\
    \ construction industry\nto further improve how infrastructure can be developed,\
    \ monitored, or improved in the future. The\ngaps in the existing research data\
    \ have been explored and a detailed analysis was carried out to iden-\ntify the\
    \ different ways in which big data analysis and storage work in relevance to the\
    \ construction\nindustry. Big data engineering (BDE) and statistics are among\
    \ the most crucial steps for integrating\nbig data technology in construction.\
    \ The results of this study suggest that while the existing research\nstudies\
    \ have set the stage for improving big data research, the integration of the associated\
    \ digital\ntechnologies into the construction industry is not very clear. Among\
    \ the future opportunities, big\ndata research into construction safety, site\
    \ management, heritage conservation, and project waste\nminimization and quality\
    \ improvements are key areas.\nKeywords: big data; big data engineering; construction\
    \ big data; digital technologies; construction industry\n1. Introduction\nBig\
    \ data are increasingly becoming an integral part of almost all ﬁelds. The rapidity\n\
    with which data is generated and piled up in the era of disruptive digital technologies\
    \ is\nastounding [1]. Such big data have necessitated the need for efﬁcient data\
    \ management\ntools and techniques to deal with the bulk of data. Recently, a\
    \ great deal of focus has\nbeen dedicated to using, storing, and managing big\
    \ data in various ﬁelds [2]. The rise of\ninterest in big data is associated with\
    \ the easy availability of technology such as smart-\nphones and computers across\
    \ the globe [3]. The bulk of data generated daily through\nthese technologies\
    \ has made various researchers interested in using the data for innova-\ntive\
    \ purposes and moving away from traditional time-consuming questionnaire-based\n\
    approaches for data collection to more digital data management. Algorithm development,\n\
    machine learning (ML), statistical analysis, and computational model development\
    \ are\namong the various techniques that depend on data that can be easily gathered\
    \ by day-to-\nday usage gadgets [4,5]. The presence of bulks of data makes it\
    \ possible for researchers to\nmake informed decisions and conduct relevant analyses\
    \ for their ﬁeld of study.\nBig Data Cogn. Comput. 2022, 6, 18. https://doi.org/10.3390/bdcc6010018\n\
    https://www.mdpi.com/journal/bdcc\nBig Data Cogn. Comput. 2022, 6, 18\n2 of 27\n\
    Construction is a data-intensive sector where the bulk of data is generated and\
    \ not\ncapitalized on adequately due to slow technology adoption [6]. Accordingly,\
    \ it is not\nsurprising to see the construction sector lagging behind the technology\
    \ curve by more than\nﬁve years which is rather slow considering the day-to-day\
    \ innovations and disruptions\nbrought about by the booming information technology\
    \ industry [7]. Moreover, big data, a\nrelatively new technology, are not properly\
    \ adopted by construction. In fact, construction\nbig data management is in its\
    \ nascency and has a long way to go to mature. However,\nmultiple studies [6,8]\
    \ show that the potential is enormous if construction big data are\nfully utilized.\n\
    There are various steps involved in using big data, including data acquisition,\
    \ stor-\nage, classiﬁcation, and reﬁning [8]. These steps are handled through\
    \ various software\nprograms to reﬁne the associated big data and make it usable\
    \ for research and practical\npurposes [9–11]. The biggest challenge in big data\
    \ management is identifying which data is\nuseful and vice versa through data\
    \ reﬁnement [12,13]. The immense amounts of data easily\navailable make it hard\
    \ to identify the datasets used for a particular purpose. Moreover,\nthe available\
    \ data format may not be ready for use or easily readable for the intended\npurpose\
    \ [14,15]. These barriers to accessing, understanding, and utilizing big data\
    \ make\nit important to develop systems for extracting key information and analyzing\
    \ it [16]. In\naddition, the strategic sorting and analysis of big data have opened\
    \ up new avenues of\nresearch by widening the need to use data appropriately [17].\
    \ In the case of construction,\nsome barriers to big data adoption include latency,\
    \ data privacy, data availability, data\ngovernance, poor broadband connectivity\
    \ at construction sites, and cost implication for\nlong-term use. For instance,\
    \ big data adoption in construction may have latency issues with\nlower transfer\
    \ rate and response time required due to software issues or network problems\n\
    which may be a hurdle for some time-sensitive construction applications [18].\n\
    Furthermore, there is an increase in vulnerability in technology adoption due\
    \ to the\nﬂuidity of security parameters. Storing construction design and ﬁnancial\
    \ information in\nshared resources concerns the construction industry [19]. Afolabi\
    \ et al. [20] assessed the\neconomies of big data in project delivery and included\
    \ poor network connect among the\nthreats to adoption by the construction industry.\n\
    Sorting big data requires developing database designs that would automate picking\n\
    the most useful data for a given purpose [21]. Identifying a design that works\
    \ best for data\nsorting is an entire research area on its own and has helped\
    \ expand big data research by a\ngreat deal [22]. Currently, the biggest question\
    \ concerning researchers in the ﬁeld of big\ndata is to ﬁnd a way that creates\
    \ seamless coordination between database systems such\nthat they can hold big\
    \ data, help process it, and possibly lead to an error-free statistical\nanalysis\
    \ [23]. Removing the current limitations in understanding big data will enable\n\
    scientists to utilize the readily available data and make better decisions.\n\
    The construction industry is also beneﬁting from big data in a way that has revolu-\n\
    tionized its traditional operational methods to a more automated process. The\
    \ presence of\ndigital tools and technologies for designing and executing construction\
    \ projects has made\nthe construction industry take enormous leaps in the last\
    \ two decades. The possibility of\nmodeling building structures and identifying\
    \ the functionality of those structures before\nthey are built has led to industrial\
    \ investments in big data and related technologies [24,25].\nComputer-aided design\
    \ (CAD), such as building information modelling (BIM), is a term\nnow synonymous\
    \ with the construction industry [26]. The three-dimensional modeling\nof buildings\
    \ and other construction infrastructures leads to the generation of digital ﬁles\n\
    which can be stored in various formats, leading to a bulk of data generation [27].\
    \ Other\ndigital innovations such as digital twins, 3D laser scanning, and advanced\
    \ wearable gadgets\nincorporated in hats, shoes, gloves, and other sensor-based\
    \ tools have revolutionized the\nconstruction industry and helped generate useful\
    \ big data.\nBig data in the construction industry can accumulate quickly and\
    \ become storage\nheavy due to the large size of the 3D modeling ﬁles and a huge\
    \ amount of daily data\ngenerated by wearable gadgets [28]. Management of such\
    \ big data is a hectic but essential\nBig Data Cogn. Comput. 2022, 6, 18\n3 of\
    \ 27\ntask as the usefulness of the models lies in ensuring that they are available\
    \ for viewing and\nleveraging as and when needed. Apart from providing the ease\
    \ of modeling infrastructure,\nbig data also provide the opportunity to develop\
    \ sustainable structures by using test models\nbefore actual constructions. These\
    \ are made possible by using digital twins, geographical\ninformation systems\
    \ (GIS)-based 3D point cloud structures, and other cloud-based scanning\nsystems.\
    \ Furthermore, the software that enables CAD and BIM further feeds into the\n\
    databases and contributes to big data. All these variables lead to the possibility\
    \ of utilizing\ntechnology for sustainable construction and associated development\
    \ in line with the United\nNations sustainable development goals and other local\
    \ development initiatives.\nThe applications of big data in the construction industry\
    \ are immense. Identifying\nhow big data can be applied to the construction industry\
    \ remains the real challenge.\nSince each construction project leads to more data\
    \ generation, it is crucial to analyze\nand sort the data accordingly. Some of\
    \ the key features within the construction industry\nthat can beneﬁt from big\
    \ data include construction safety, efﬁciency, waste minimization,\nproductivity,\
    \ competitive advantage, and pollution management [29]. The strategic and\noperational\
    \ beneﬁts of big data in the construction industry have further been explored\
    \ by\nAtuahene et al. [30]. The major beneﬁts of big data were found to be project\
    \ management,\nmanagement of claims, and procurement. These aspects of big data\
    \ application are crucial\nfor managing construction projects. However, many other\
    \ aspects and applications of big\ndata within the construction industry still\
    \ need to be explored. While these different aspects\nof construction projects\
    \ beneﬁt from big data, it is important to understand how big data can\nbe analyzed\
    \ and utilized for different projects. Furthermore, the algorithms and frameworks\n\
    that can integrate big data in the construction industry remain largely unexplored.\n\
    Today, studies on construction and its management in relation to big data are\
    \ scarce,\npresenting a gap in research. This provides opportunities for further\
    \ research that can\ngreatly beneﬁt the construction industry in the long run.\
    \ This gap is targeted in the current\nstudy, where the papers published in construction\
    \ ﬁelds focused on big data since 2010 are\nstudied. The key takeaways of these\
    \ studies are presented here to help the construction\nresearchers build upon\
    \ these studies and advance the state of research related to big data\nin construction.\n\
    In terms of implications, this study will help both the construction researchers\
    \ and\npractitioners, where the former will have the current state of research\
    \ on big data and\ncan see opportunities for further research. Similarly, the\
    \ practitioners can ascertain the\nsoftware and hardware requirements for incorporating\
    \ big-data-based opportunities in\nconstruction and create implementation models\
    \ and gadgets. This paper is divided into\nsections exploring big data engineering\
    \ (BDE), databases, use of big data in construction,\nthe application of big-data-based\
    \ statistics in construction, and future opportunities for big\ndata in construction.\n\
    Research Questions\nThis study aims to identify ways in which big data can be\
    \ used for construction and\nits management based on the review of existing literature.\
    \ The existing literature on big\ndata does not provide detailed solutions for\
    \ construction management, which creates a gap\nin the literature concerning the\
    \ use of big data in the construction industry. The research\nquestions set for\
    \ this study are as follows:\n•\nHow can we use big data for research in construction\
    \ engineering and management?\n•\nHow is construction big data managed and stored?\n\
    •\nHow can big data be used for planning construction projects in a futuristic\
    \ way?\nThe rest of the paper is organized as follows. Section 2 presents the\
    \ method and\nmaterials used in the study. Section 3 presents the preliminary\
    \ analyses conducted in the\nstudy. This is followed by Section 4, where the BDE\
    \ and its subcomponents, including\nbig data processing, big data storage, and\
    \ big data analytics (BDA), are presented and\ndiscussed. Similarly, the 10 vs.\
    \ of big data and ML techniques are also presented in this\nsection. Section 5\
    \ presents the future opportunities for big data in construction. Finally,\nBig\
    \ Data Cogn. Comput. 2022, 6, 18\n4 of 27\nSection 6 concludes the study and presents\
    \ the key takeaways, limitations, and future\nexpansion directions based on the\
    \ current study.\n2. Materials and Methods\nThis study follows a multi-stepped\
    \ approach for reviewing the studies on big data\nin construction. First, a comprehensive\
    \ literature retrieval mechanism is adopted from\npublished literature and modiﬁed\
    \ accordingly to retrieve pertinent literature on big data\nin construction. This\
    \ is followed by analyses of the retrieved articles in the shape of\npreliminary\
    \ analyses, BDE, processing, storage, analytics, and statistical and data mining\n\
    approaches in relation to the construction industry. These steps are subsequently\
    \ explained.\nAn extensive literature search was carried out to identify peer-reviewed\
    \ papers re-\nlated to big data and construction since 2010, following the approaches\
    \ adopted in recent\nstudies [31,32]. This was conducted in order to keep a recent\
    \ focus and study current\narticles on big data in construction. Some preliminary\
    \ analyses, as subsequently discussed,\nhighlighted that big data in construction\
    \ received more attention in 2010 and onwards;\nhence, the review period of 2010\
    \ and onwards makes sense. A number of scholarly research\nplatforms, including\
    \ Google Scholar, Scopus, Science Direct, Springer, Elsevier, and IEEE\nExplore,\
    \ were consulted for literature search based on the high volume of high-quality\n\
    research papers available on these platforms following recent studies [33–35].\
    \ Once the\nsearch engines were selected, a combination of different keywords\
    \ was developed to iden-\ntify the most useful publications for this study in\
    \ the next step. The keyword combinations\nwere developed in a tier-based approach,\
    \ such that terms related to big data, such as\n“big data”, “big data analysis”,\
    \ “big data volume”, and “big data analysis tools” fell into\ncategory 1 (S1).\n\
    Similarly, all keywords pertaining to construction, such as “construction”, “construc-\n\
    tion management”, and “construction industry”, were classiﬁed into category 2\
    \ (S2). Differ-\nent combinations of keywords from both categories were used to\
    \ retrieve the most relevant\npublications. Examples of keyword combinations include\
    \ big data in construction, big data\nfor construction management, construction\
    \ management, and big data, etc.\nSearch category was further restricted by including\
    \ only those papers that were\npublished in 2010 or later years. Since big data\
    \ technology was used robustly in the last\ndecade, research publications prior\
    \ to 2010 were left out. Concept papers, editorials, notes,\nperspectives, closures,\
    \ discussions, conference papers, and others were also excluded from\nthe search\
    \ to ensure the inclusion of original research papers only. Other publications\n\
    dealing with classical deﬁnitions were also excluded.\nUsing different combinations\
    \ of the keywords to identify papers published from 2010\nonwards led to a total\
    \ of more than 10,000 papers being retrieved from the mentioned\nsearch engines.\
    \ The list of articles was narrowed down using the detailed inclusion criteria\n\
    set for this study. This included removing duplicates and other exclusions, as\
    \ previously\nmentioned, which brought the search results down to around 4000\
    \ papers. This was further\nnarrowed down in a stepwise manner to ensure that\
    \ only those papers were included that\nﬁt the scope of the current study. In\
    \ the ﬁnal step, the content of the papers was analyzed\nto determine their suitability\
    \ for this study, resulting in a total of 156 papers.\nFigure 1 shows an overview\
    \ of the different ways in which research studies have\naddressed the use of big\
    \ data in construction. There has been a rise in the interest in big\ndata usage\
    \ for the construction industry since 2016. However, the interest has been limited\n\
    in terms of analyses scope as the trends have remained steady. As shown in Figure\
    \ 1,\nthe publications on this topic have followed similar terms and research\
    \ themes over the\nlast few years, leading to gradual evolution. For example,\
    \ in 2016, most papers related\nto big data and construction focused on the use\
    \ of cloud computing, while 2017 saw a\ntrend of developing models and frameworks\
    \ for implementing big data in the construction\nindustry. Similarly, in 2018\
    \ and 2019, researchers have mainly explored how different big\ndata models could\
    \ be implemented within the construction industry. Recently, the research\nBig\
    \ Data Cogn. Comput. 2022, 6, 18\n5 of 27\nfocus has shifted to using big data\
    \ in real-time construction projects and identifying how\nthese technologies could\
    \ be harnessed for developing futuristic construction projects.\nBig Data Cogn.\
    \ Comput. 2022, 6, 18 \n5 of 29 \n \nindustry. Similarly, in 2018 and 2019, researchers\
    \ have mainly explored how different big \ndata models could be implemented within\
    \ the construction industry. Recently, the re-\nsearch focus has shifted to using\
    \ big data in real-time construction projects and identifying \nhow these technologies\
    \ could be harnessed for developing futuristic construction projects. \n \nFigure\
    \ 1. Funnel diagrams showing trends in big data research in construction since\
    \ 2016. \nIn addition to big data, some other technologies and methods have been\
    \ researched \nin the last couple of years for improving the construction industry.\
    \ There is a great overlap \nin the types of technologies studied simultaneously\
    \ for developing models that could \nguide future research in the construction\
    \ industry. Figure 2 shows the overlapping tools \nand technologies identified\
    \ from recent literature. It can be observed that big data is not \nstandalone;\
    \ rather, it depends on other tools and methods, including data analytics, ML,\
    \ \npattern recognition, statistics, deep learning, and artificial intelligence\
    \ (AI). All these tools \nand technologies are used in different combinations\
    \ for developing models that could be \nused in real time for construction projects.\
    \ The reliance of all these tools on each other is \nan important factor to consider\
    \ when developing construction projects as the computa-\ntional aspects of the\
    \ project can only be as good and true and the depth of research is \nperformed\
    \ for developing and testing the algorithms and frameworks. The construction \n\
    industry greatly benefits from the overlapping fields of big data technologies.\
    \ The use of \nbig data requires data mining which generates enormous datasets.\
    \ The bulk of construc-\ntion-related data makes the use of statistics inevitable.\
    \ \nAlong with data management, statistical analysis, and big data analytics,\
    \ several dif-\nferent techniques and resources come into use. For example, machine\
    \ learning tools and \nartificial intelligence play a crucial role in the construction\
    \ industry in conjunction with \nbig data. The overlap of all the different fields\
    \ shown in Figure 2 shows how the field of \nconstruction is laden with the use\
    \ of different technologies, each of which is somehow \nassociated with big data.\
    \ The use of computational models, databases, deep learning, pat-\ntern recognition,\
    \ virtual reality, bots, and augmented reality contributes to the application\
    \ \nof big data in the construction industry. An in-depth analysis of the big\
    \ data applications \nand the use of technology in the construction industry results\
    \ in a much more complex \noverlap than shown here. However, the core aim of using\
    \ different technologies is to sim-\nplify how datasets can be used to guide future\
    \ construction projects. Recognizing data \nFigure 1. Funnel diagrams showing\
    \ trends in big data research in construction since 2016.\nIn addition to big\
    \ data, some other technologies and methods have been researched in\nthe last\
    \ couple of years for improving the construction industry. There is a great overlap\n\
    in the types of technologies studied simultaneously for developing models that\
    \ could\nguide future research in the construction industry. Figure 2 shows the\
    \ overlapping tools\nand technologies identiﬁed from recent literature. It can\
    \ be observed that big data is not\nstandalone; rather, it depends on other tools\
    \ and methods, including data analytics, ML,\npattern recognition, statistics,\
    \ deep learning, and artiﬁcial intelligence (AI). All these tools\nand technologies\
    \ are used in different combinations for developing models that could be\nused\
    \ in real time for construction projects. The reliance of all these tools on each\
    \ other is an\nimportant factor to consider when developing construction projects\
    \ as the computational\naspects of the project can only be as good and true and\
    \ the depth of research is performed\nfor developing and testing the algorithms\
    \ and frameworks. The construction industry\ngreatly beneﬁts from the overlapping\
    \ ﬁelds of big data technologies. The use of big data\nrequires data mining which\
    \ generates enormous datasets. The bulk of construction-related\ndata makes the\
    \ use of statistics inevitable.\nBig Data Cogn. Comput. 2022, 6, 18 \n6 of 29\
    \ \n \npatterns and understanding how each dataset fits the needs of a construction\
    \ project is \nonly possible if the dataset has been analyzed, critically appraised,\
    \ and classified for its \nspecific usage. The guiding principle here is to use\
    \ modern technology to upgrade and \nupdate the ways in which information could\
    \ be streamlined for the benefit of different \nprojects. For example, identifying\
    \ the materials that best suit a particular structure, devel-\noping project timelines,\
    \ and streamlining the resources can become much more straight-\nforward if the\
    \ construction projects are developed with the help of big data technologies.\
    \ \nAs shown in Figure 2, different technologies in the construction industry\
    \ overlap in \ndifferent ways. Integrating big data in the construction industry\
    \ is possible through the \ncombined use of other technologies such as machine\
    \ learning, AI, VR, AR, pattern recog-\nnition, and other such methods. \n \n\
    Figure 2. Overlapping fields of research contributing to big data. \n3. Preliminary\
    \ Analyses \nAs mentioned in the method, some preliminary analyses were conducted\
    \ on the re-\ntrieved articles, including the keywords analysis and the countries\
    \ of origin of the articles \nfollowing recently published articles [31,35]. Before\
    \ this, a basic Google Trend (r) search \nwas conducted using trends.google.com\
    \ (accessed on 20 November 2021). A comparison \nwas made for three iterations\
    \ of the keywords previously mentioned. These included con-\nstruction big data\
    \ (keyword 1) big data in construction (keyword 2) and big data for\nFigure 2.\
    \ Overlapping ﬁelds of research contributing to big data.\nBig Data Cogn. Comput.\
    \ 2022, 6, 18\n6 of 27\nAlong with data management, statistical analysis, and\
    \ big data analytics, several\ndifferent techniques and resources come into use.\
    \ For example, machine learning tools\nand artiﬁcial intelligence play a crucial\
    \ role in the construction industry in conjunction\nwith big data. The overlap\
    \ of all the different ﬁelds shown in Figure 2 shows how the\nﬁeld of construction\
    \ is laden with the use of different technologies, each of which is\nsomehow associated\
    \ with big data. The use of computational models, databases, deep\nlearning, pattern\
    \ recognition, virtual reality, bots, and augmented reality contributes to the\n\
    application of big data in the construction industry. An in-depth analysis of\
    \ the big data\napplications and the use of technology in the construction industry\
    \ results in a much more\ncomplex overlap than shown here. However, the core aim\
    \ of using different technologies is\nto simplify how datasets can be used to\
    \ guide future construction projects. Recognizing data\npatterns and understanding\
    \ how each dataset ﬁts the needs of a construction project is only\npossible if\
    \ the dataset has been analyzed, critically appraised, and classiﬁed for its speciﬁc\n\
    usage. The guiding principle here is to use modern technology to upgrade and update\
    \ the\nways in which information could be streamlined for the beneﬁt of different\
    \ projects. For\nexample, identifying the materials that best suit a particular\
    \ structure, developing project\ntimelines, and streamlining the resources can\
    \ become much more straightforward if the\nconstruction projects are developed\
    \ with the help of big data technologies.\nAs shown in Figure 2, different technologies\
    \ in the construction industry overlap\nin different ways. Integrating big data\
    \ in the construction industry is possible through\nthe combined use of other\
    \ technologies such as machine learning, AI, VR, AR, pattern\nrecognition, and\
    \ other such methods.\n3. Preliminary Analyses\nAs mentioned in the method, some\
    \ preliminary analyses were conducted on the\nretrieved articles, including the\
    \ keywords analysis and the countries of origin of the articles\nfollowing recently\
    \ published articles [31,35]. Before this, a basic Google Trend (r) search\nwas\
    \ conducted using trends.google.com (accessed on 20 November 2021). A comparison\n\
    was made for three iterations of the keywords previously mentioned. These included\n\
    construction big data (keyword 1), big data in construction (keyword 2), and big\
    \ data for\nconstruction management (keyword 3). As shown in Figure 3, the earliest\
    \ attention paid to\nbig data in construction was reported in 2010. This was reported\
    \ for keyword 1, followed\nby keyword 2 in 2013 and keyword 3 in 2014. Two clusters\
    \ are clearly visible from Figure 3.\nThe initial interest cluster showed when\
    \ big data focused on construction and the spike\nin interest cluster. The ﬁrst\
    \ cluster is evident in 2010–2014, whereas the spike in interest\ncluster started\
    \ in 2016. This shows the hotness or relevance of the topic under investigation\n\
    in the current study.\nAfter the Google Trend analyses, the retrieved articles\
    \ were analyzed using Vos\nViewer® tool. The ﬁrst analysis was that of keywords.\
    \ The natural distribution of keywords\nretrieved from the articles shows ﬁve\
    \ distinct clusters: education, city and region, disaster\nand human interactions,\
    \ knowledge management, and technology management in relation\nto construction,\
    \ as given in Figure 4. The overall top keywords in order of priority retrieved\n\
    from these articles included big data, information management, AI, data mining,\
    \ internet of\nthings, ML, advanced analytics, data technologies, students, data\
    \ handling, digital storage,\ncolleges and universities, smart city, decision\
    \ making, cloud computing, construction\nindustry, and others. These are based\
    \ on the appearance of the keywords in the titles,\nabstract, and keywords of\
    \ a minimum of 30 papers. These keywords are in line with the\nnatural clusters\
    \ highlighted in Figure 4.\nBig Data Cogn. Comput. 2022, 6, 18\n7 of 27\nBig Data\
    \ Cogn. Comput. 2022, 6, 18 \n7 of 29 \n \n \nFigure 3. Google Trends analyses\
    \ for big data in construction, showing interest development and \nspike in interest.\
    \ \nAfter the Google Trend analyses, the retrieved articles were analyzed using\
    \ Vos \nViewer® tool. The first analysis was that of keywords. The natural distribution\
    \ of key-\nwords retrieved from the articles shows five distinct clusters: education,\
    \ city and region, \ndisaster and human interactions, knowledge management, and\
    \ technology management \nin relation to construction, as given in Figure 4. The\
    \ overall top keywords in order of pri-\nority retrieved from these articles included\
    \ big data, information management, AI, data \nmining, internet of things, ML,\
    \ advanced analytics, data technologies, students, data han-\ndling, digital storage,\
    \ colleges and universities, smart city, decision making, cloud com-\nputing,\
    \ construction industry, and others. These are based on the appearance of the\
    \ key-\nwords in the titles, abstract, and keywords of a minimum of 30 papers.\
    \ These keywords \nare in line with the natural clusters highlighted in Figure\
    \ 4. \nFigure 3. Google Trends analyses for big data in construction, showing\
    \ interest development and\nspike in interest.\nBig Data Cogn. Comput. 2022, 6,\
    \ 18 \n8 of 29 \n \n \nFigure 4. Five key clusters of big data in construction\
    \ based on keywords reported in the reviewed \nliterature. \nIn another analysis,\
    \ the top 10 contributing countries to big data research in construc-\ntion were\
    \ investigated. These are China, United States, United Kingdom, Russian Feder-\n\
    ation, Australia, India, South Korea, Germany, Spain, and Italy in terms of the\
    \ number of \ncontributions as shown in Figure 5. The colors in the country box\
    \ show the countries with \nthe strongest collaborations, whereas the size of\
    \ the box refers to the number of papers. \nFigure 4.\nFive key clusters of big\
    \ data in construction based on keywords reported in the\nreviewed literature.\n\
    Big Data Cogn. Comput. 2022, 6, 18\n8 of 27\nIn another analysis, the top 10 contributing\
    \ countries to big data research in con-\nstruction were investigated. These are\
    \ China, United States, United Kingdom, Russian\nFederation, Australia, India,\
    \ South Korea, Germany, Spain, and Italy in terms of the number\nof contributions\
    \ as shown in Figure 5. The colors in the country box show the countries\nwith\
    \ the strongest collaborations, whereas the size of the box refers to the number\
    \ of papers.\nFor example, most of the papers authored by Chinese authors are\
    \ in collaboration with\nauthors from Australia, New Zealand, and Indonesia.\n\
    Big Data Cogn. Comput. 2022, 6, 18 \n9 of 29 \n \n \nFigure 5. Countries conducting\
    \ big data research in construction based on reviewed literature. \n4. Big Data\
    \ Engineering (BDE) \nBig data analytics (BDA) is supported by BDE that provides\
    \ a framework to conduct \nit. BDE has tremendous applications in construction.\
    \ It has been used for BIM to improve \nproject management [36]. It has also been\
    \ used to improve building design and for effec-\ntive performance monitoring\
    \ [37], project management, safety, energy management, de-\ncision-making design\
    \ frameworks, resource management [38], quality management, \nwaste management,\
    \ and others [24]. \nTo understand BDE, it is important to discuss big data platforms.\
    \ These platforms \nare divided into two groups based on variations in their inherent\
    \ characteristics. These \ninclude horizontal scaling platforms (HSP) and vertical\
    \ scaling platforms (VSP). HSP uti-\nlizes multiple servers by distributing processing\
    \ across them and bringing new machines \ninto the cluster. VSPs are single-server-based\
    \ configurations that achieve the scaling by \nupgrading the hardware of the related\
    \ server. In construction, HSPs have been used for \nwaste management [25], profitability\
    \ performance [39], smart road construction, and oth-\ners [40]. Similarly, VSPs\
    \ have been reported in one-off construction projects [41], trans-\nportation\
    \ [42], and others. This paper focuses on HSPs, particularly Berkeley Data Ana-\n\
    lytics Stack (BDAS) and Hadoop. \nRecently, BDAS has been in the limelight since\
    \ it has greater performance gains over \nFigure 5. Countries conducting big data\
    \ research in construction based on reviewed literature.\n4. Big Data Engineering\
    \ (BDE)\nBig data analytics (BDA) is supported by BDE that provides a framework\
    \ to conduct\nit. BDE has tremendous applications in construction. It has been\
    \ used for BIM to improve\nproject management [36]. It has also been used to improve\
    \ building design and for ef-\nfective performance monitoring [37], project management,\
    \ safety, energy management,\ndecision-making design frameworks, resource management\
    \ [38], quality management,\nwaste management, and others [24].\nTo understand\
    \ BDE, it is important to discuss big data platforms. These platforms\nare divided\
    \ into two groups based on variations in their inherent characteristics. These\n\
    include horizontal scaling platforms (HSP) and vertical scaling platforms (VSP).\
    \ HSP uti-\nlizes multiple servers by distributing processing across them and\
    \ bringing new machines\ninto the cluster. VSPs are single-server-based conﬁgurations\
    \ that achieve the scaling by up-\ngrading the hardware of the related server.\
    \ In construction, HSPs have been used for waste\nmanagement [25], proﬁtability\
    \ performance [39], smart road construction, and others [40].\nBig Data Cogn.\
    \ Comput. 2022, 6, 18\n9 of 27\nSimilarly, VSPs have been reported in one-off\
    \ construction projects [41], transportation [42],\nand others. This paper focuses\
    \ on HSPs, particularly Berkeley Data Analytics Stack (BDAS)\nand Hadoop.\nRecently,\
    \ BDAS has been in the limelight since it has greater performance gains\nover\
    \ Hadoop. However, as it is quite recent, it suffers the drawback of limitation\
    \ in\navailable supporting tools. On the other side, Hadoop has been widely utilized\
    \ in big data\napplications. The tools offered by these platforms are useful in\
    \ the storage and processing\nof big data. For instance, Bilal et al. [39] investigated\
    \ the proﬁtability performance of\nconstruction projects using big data and used\
    \ Hadoop Distributed File System (HDFS)\nfor managing the data within the staging\
    \ area while employing Resource Description\nFramework (RDF)-enabled Network Data\
    \ Model (NDM) for storing the persistent data.\nSimilarly, Jun Ying et al. [43]\
    \ investigated the development and implementation of BDAS\nby the relevant building\
    \ authorities in Singapore, which has enhanced knowledge and\nexpertise in buildability.\
    \ An overview of big data classiﬁcation into BDE and BDA is shown\nin Figure 6\
    \ and subsequently explained.\nBig Data Cogn. Comput. 2022, 6, 18 \n11 of 29 \n\
    \ \n \nFigure 6. Classification of big data into its key domains. \nAs shown in\
    \ Figure 6, the two major big data domains rely on statistics, data pro-\ncessing,\
    \ and data management. All these features, in turn, are heavily dependent on ML\
    \ \ntools and methods. For example, BDE requires data processing and storage,\
    \ which in turn \nrequire regression models, NoSQL, and MapReduce, all of which\
    \ are different types of \ncomputational tools that enable the different applications\
    \ of big data management. Simi-\nlarly, BDA heavily depends on ML tools that can\
    \ use data and statistics to provide orga-\nnized data solutions. The use of tree-based\
    \ analysis, Bayesian analysis, and shrinkage are \nall examples of ML integration\
    \ in the field of BDA. A wide variety of ML tools have been \nexplored over the\
    \ years and have been directly or indirectly associated with big data man-\nagement\
    \ and analysis. Tools such as linear regression, vector machines, KNN models,\
    \ \nclustering, and decision tree regression are among the few examples which\
    \ enable the use \nof big data coherently. Furthermore, the classification tree\
    \ of big data is likely to be further \nexpanded as the ML algorithms are further\
    \ developed and more analysis methods are \nadded to the list. Therefore, the\
    \ constant expansion of the big data analysis tools can ena-\nble the use of these\
    \ tools in the construction industry for improving construction projects \nFigure\
    \ 6. Classiﬁcation of big data into its key domains.\nBig data are classiﬁed into\
    \ two major domains: BDE and BDA. These two main\ndomains are further divided\
    \ into many classes and subclasses. A third domain that comes\nunder the canopy\
    \ of big data is ML. The use of ML is inevitable in big data as the data\nneed\
    \ to be organized, analyzed, and used through ML tools and models such as deep\n\
    learning and neural networks. Some of the key ML tools and models associated with\
    \ big\ndata directly or indirectly include regression analysis, clustering, classiﬁcation,\
    \ information\nretrieved (R), and natural language processing (NLP). Some examples\
    \ of ML in construction\ninclude deep-learning-based ﬂood detection and damage\
    \ assessment [44], projects delay\nrisk prediction [45], construction site safety\
    \ [46], construction site monitoring [47], neural\nnetwork models to predict concrete\
    \ properties [48], and others.\nBig Data Cogn. Comput. 2022, 6, 18\n10 of 27\n\
    The various algorithms and methods shown in Figure 6 all contribute towards big\
    \ data\nin some way. The use of supervised and unsupervised learning approaches\
    \ is determined\ndepending on the type of datasets available. The major difference\
    \ between supervised\nlearning and unsupervised learning is that the algorithm\
    \ for supervised learning utilizes\nlabeled datasets while the unsupervised data\
    \ do not use labeled data. The supervised\nand unsupervised algorithms further\
    \ have different methods and examples. For instance,\nregression, linear regression,\
    \ neural network regression, random forest, Naïve Bayes, and\nlasso regression\
    \ are examples of supervised learning.\nSimilarly, clustering, Natural Language\
    \ Processing (NLP), and KNN are examples\nof unsupervised learning. The applications\
    \ of each of these algorithms can differ and\nhence their integration in the construction\
    \ industry can vary. The regression models are\nused in engineering for analyzing\
    \ trends and correlations between different variables.\nIn the construction industry,\
    \ these models play a crucial role as the statistical analysis\nand correlation\
    \ development between different variables are made easy through linear\nregression\
    \ and other similar algorithms. Similarly, machine learning models have made\n\
    it possible to ensure that construction projects are developed considering safety,\
    \ time\nmanagement, and quality.\nAs shown in Figure 6, the two major big data\
    \ domains rely on statistics, data pro-\ncessing, and data management. All these\
    \ features, in turn, are heavily dependent on\nML tools and methods. For example,\
    \ BDE requires data processing and storage, which\nin turn require regression\
    \ models, NoSQL, and MapReduce, all of which are different\ntypes of computational\
    \ tools that enable the different applications of big data management.\nSimilarly,\
    \ BDA heavily depends on ML tools that can use data and statistics to provide\n\
    organized data solutions. The use of tree-based analysis, Bayesian analysis, and\
    \ shrinkage\nare all examples of ML integration in the ﬁeld of BDA. A wide variety\
    \ of ML tools have\nbeen explored over the years and have been directly or indirectly\
    \ associated with big data\nmanagement and analysis. Tools such as linear regression,\
    \ vector machines, KNN models,\nclustering, and decision tree regression are among\
    \ the few examples which enable the use\nof big data coherently. Furthermore,\
    \ the classiﬁcation tree of big data is likely to be further\nexpanded as the\
    \ ML algorithms are further developed and more analysis methods are\nadded to\
    \ the list. Therefore, the constant expansion of the big data analysis tools can\
    \ enable\nthe use of these tools in the construction industry for improving construction\
    \ projects in the\nfuture. Yang and Yu [49] investigated the application of heterogeneous\
    \ networks oriented\nto NoSQL database in optimal post-evaluation indexes of construction\
    \ projects. NoSQL\ndatabase is scalable with a powerful and ﬂexible data model\
    \ and a large amount of data\nand has increasing application potential in the\
    \ memory ﬁeld. Sanni-Anibire et al. [50]\ninvestigated the increase in delays\
    \ and abandonment of tall buildings and developed a\nmachine learning model for\
    \ delay risk assessment. Methods such as K-Nearest Neighbors\n(KNN), Artiﬁcial\
    \ Neural Networks (ANN), Support Vector Machines (SVM), and Ensemble\nmethods\
    \ were considered. The model developed for predicting the risk of delay was based\n\
    on ANN with a classiﬁcation accuracy of 93.75%. The key components of big data\
    \ from\nFigure 6 for its management are discussed below.\n4.1. Big Data Processing\n\
    Distributed and parallel computation is present in the core of BDE. In construction,\
    \ big\ndata processing has been utilized for waste management [51], prefabricated\
    \ construction\nproject management [52], proﬁtability analyses, and other construction\
    \ management appli-\ncations [39]. For processing information, a considerable\
    \ number of models are developed.\nSome of the key big data models are discussed\
    \ below.\n4.1.1. MapReduce (MR)\nMapReduce was developed for the handling of big\
    \ data. It utilizes a distributed\nprocessing model in which two functions, as\
    \ indicated by the name itself, map and reduce,\nare employed to write analytical\
    \ tasks. Mappers and reducers are the processes that collect\nBig Data Cogn. Comput.\
    \ 2022, 6, 18\n11 of 27\nthe data from these functions for further processing.\
    \ Initially, mappers collect and read the\ninput information to process it for\
    \ subsequent results generation. The output of mappers is\nused by reducers which\
    \ give the results that are ultimately stored in the ﬁle system. MR has\nbeen\
    \ used by Jiao et al. [53] to develop an augmented framework for BIM. Similarly,\
    \ it has\nalso been used in construction knowledge maps [54] and other big data\
    \ applications [54].\nThe use of MapReduce in the construction industry is inevitable\
    \ due to the big data\napplications within the construction industry. The usability\
    \ of the MapReduce framework\nin the construction industry relies on the management\
    \ of big data in a particular way.\nAccordingly, the datasets are analyzed and\
    \ divided into categories to reduce clutter and\npresent an easy-to-understand\
    \ data output. The basic framework of MapReduce includes\ndata input, data chunks,\
    \ decomposition mappers, decomposed output, linear mappers,\nlinear reducers,\
    \ and combined output. The exact series and number of components in the\nframework\
    \ can vary depending on the version used. However, the overall features and\n\
    application of MapReduce remain the same, i.e., reduction of data into manageable\
    \ chunks.\nThe use of MapReduce not only distributes data into smaller chunks\
    \ but also helps develop\ndatasets that present a more analytic view of big data.\
    \ Having organized datasets within\nthe construction industry is of key importance\
    \ as it can greatly increase the efﬁciency of\ndata management and decision making\
    \ based on data analysis.\nHadoop was the popular and ﬁrst big data platform that\
    \ introduced and made it easy\nfor people to work on MR by executing its programs\
    \ successfully. For tasks requiring batch\nprocessing, MR proved itself to be\
    \ an effective tool as a typical cluster contains interlinked\nmappers and reducers\
    \ that assist by running MR programs side by side at the same time.\nThough it\
    \ has its beneﬁts, these are not devoid of the drawbacks. These drawbacks include\n\
    running some applications for graph generation and real-time and iterative processing.\
    \ By\ndissociating the rest of the ecosystem from the processing of MR, Hadoop’s\
    \ latest versions\nhave tried to sort out the problem. Yet another resource negotiator\
    \ (YARN) has also been\nintroduced, which functions by providing resource management\
    \ and scheduling related\nfunctions of MR and has made it easy to implement innovative\
    \ applications by Hadoop.\nHadoop models have been used in construction for smart\
    \ buildings and disaster\nmanagement [55], failure prediction of construction\
    \ ﬁrms [56], workers’ safe behaviors in\na metro construction project [57], and\
    \ other relevant applications. The overall platform\ndesign architecture of Hadoop\
    \ offers high reliability; adopt cluster technology, multi-copy\ntechnology, independent\
    \ backup technology, and other means to reduce the data failure\nrate effectively\
    \ and build a reliable data application service platform. First, the processing\n\
    of big data into batches and simultaneous reduction and reﬁning of the data are\
    \ carried out\nusing MR. Next, data are batched into similar items to streamline\
    \ the analyses. This step\nfurther reduces noise or datasets that do not align\
    \ with a particular batch of data. Finally, a\ndataset is obtained, which is reﬁned\
    \ and aligned with the original search purpose.\n4.1.2. Directed Acyclic Graph\n\
    Big data platforms also use Directed Acyclic Graph (DAG) which is an alternative\n\
    processing model. In comparison with MR, DAG works by relaxing map-then-reduce,\n\
    the style of MR, which is supported by Spark. Spark is widely accepted for reactive\
    \ and\niterative applications due to its supremacy over MR in high expressiveness\
    \ and in-memory\ncomputation. Disk-resident and memory-resident tasks are conducted\
    \ ten and one hundred\ntimes faster using Spark than MR. DAGs show relationships\
    \ among variables, making them\neasier to understand. DAGs provide major advantages\
    \ that enable experts and researchers\nto construct complex causal relationships\
    \ in which nodes represent stochastic variables,\nand directed edges (arrows)\
    \ indicate direct probabilistic dependencies among the relevant\nvariables. DAGs\
    \ are also able to encode deterministic as well as probabilistic relationships\n\
    among the variables. The usage of Spark and associated DAGs has been reported\
    \ for\nconstruction proﬁtability analysis [39], waste management [25], energy\
    \ monitoring service\non smart campuses [58], and others.\nBig Data Cogn. Comput.\
    \ 2022, 6, 18\n12 of 27\nSpark and Hadoop are among the ML tools with enormous\
    \ potential in construction\nengineering and management. Figure 7 compares the\
    \ two tools that can inform research in\nconstruction. The speed of both these\
    \ systems is better than other algorithms and ML tools\ncurrently in use in the\
    \ construction industry. Moreover, fault tolerance in both these systems\nis also\
    \ high and has greater scalability than existing models. The data storage in these\n\
    systems is slightly different in that Spark uses a memory system while Hadoop\
    \ utilizes a\ndisk for data storage. The language for both these tools is also\
    \ different since Spark is written\nin Scala while Hadoop has been developed using\
    \ JavaScript. Despite the slight differences,\nboth these tools provide the opportunity\
    \ to process data in the form of batches and at a\nhigher speed than previously\
    \ existing models, making them potential tools for futuristic\nmodel developments\
    \ in construction engineering and management. JavaScript has been\nused in construction\
    \ to anticipate building material reuse [59], automated progress control\ncoupled\
    \ with laser scanning [60], shared virtual reality for design and management [61],\n\
    construction information mining [62], and others. Similarly, Scala has been used\
    \ for the\nprocess information modeling concept for on-site construction management\
    \ [63].\ndred times faster using Spark than MR. DAGs show relationships among\
    \ variables, mak-\ning them easier to understand. DAGs provide major advantages\
    \ that enable experts and \nresearchers to construct complex causal relationships\
    \ in which nodes represent stochastic \nvariables, and directed edges (arrows)\
    \ indicate direct probabilistic dependencies among \nthe relevant variables. DAGs\
    \ are also able to encode deterministic as well as probabilistic \nrelationships\
    \ among the variables. The usage of Spark and associated DAGs has been re-\nported\
    \ for construction profitability analysis [39], waste management [25], energy\
    \ moni-\ntoring service on smart campuses [58], and others. \nSpark and Hadoop\
    \ are among the ML tools with enormous potential in construction \nengineering\
    \ and management. Figure 7 compares the two tools that can inform research \n\
    in construction. The speed of both these systems is better than other algorithms\
    \ and ML \ntools currently in use in the construction industry. Moreover, fault\
    \ tolerance in both these \nsystems is also high and has greater scalability than\
    \ existing models. The data storage in \nthese systems is slightly different in\
    \ that Spark uses a memory system while Hadoop uti-\nlizes a disk for data storage.\
    \ The language for both these tools is also different since Spark \nis written\
    \ in Scala while Hadoop has been developed using JavaScript. Despite the slight\
    \ \ndifferences, both these tools provide the opportunity to process data in the\
    \ form of batches \nand at a higher speed than previously existing models, making\
    \ them potential tools for \nfuturistic model developments in construction engineering\
    \ and management. JavaScript \nhas been used in construction to anticipate building\
    \ material reuse [59], automated pro-\ngress control coupled with laser scanning\
    \ [60], shared virtual reality for design and man-\nagement [61], construction\
    \ information mining [62], and others. Similarly, Scala has been \nused for the\
    \ process information modeling concept for on-site construction management \n\
    [63]. \n \nFigure 7. Components of Spark and Hadoop. A side-by-side comparison\
    \ of Spark and Hadoop pro-\nvides insights about the usability and applications\
    \ of each. \n4.1.3. Big Data Processing in Construction \nBig data processing\
    \ has been effectively utilized in the construction industry for fail-\nure prediction\
    \ data [56], construction waste analytics [25], profitability data [39], modular\
    \ \nand prefabricated construction [52], fire incident management [64], smart\
    \ campus energy \nFigure 7. Components of Spark and Hadoop. A side-by-side comparison\
    \ of Spark and Hadoop\nprovides insights about the usability and applications\
    \ of each.\n4.1.3. Big Data Processing in Construction\nBig data processing has\
    \ been effectively utilized in the construction industry for failure\nprediction\
    \ data [56], construction waste analytics [25], proﬁtability data [39], modular\n\
    and prefabricated construction [52], ﬁre incident management [64], smart campus\
    \ energy\nmonitoring [58], healthier cities management [28], smart road management\
    \ [40], and others.\nThough MR and Spark have their own signiﬁcance, these are\
    \ less frequently employed\nin the construction industry to process big data such\
    \ as BIM-associated data. Partial BIM\nmodels’ retrieval was optimized by MR by\
    \ Bilal, et al. [65] and Chang and Tsai [66]. The\nauthors found a loop in the\
    \ Hadoop MR logic of data distribution. For overcoming the\nquery problem, a few\
    \ steps of prepartitioning and processing are introduced for relevant\nBIM data\
    \ parts that are later stored in Hadoop clusters. Node multi-threading during\n\
    data analysis helped by making the CPU work its maximum. This helped in customizing\n\
    Hadoop for BIM data while the YARN application implemented querying components.\n\
    YARN applications are further utilized to develop a BIM system for quantity estimation\
    \ and\nclash detection that can execute required tasks with the performance improved\
    \ many-fold.\nAnother research group worked for naive and expert BIM users by\
    \ developing a\nsystem for BIM data storage and retrieval [67]. The authors developed\
    \ a system for\ncloud BIM to retrieve and represent big data intelligently. This\
    \ system helped develop an\ninteractive interface to maximize the usability and\
    \ utility of construction big data. Complex\nBIM data are retrieved by processing\
    \ proposed natural languages after reformulating user\nBig Data Cogn. Comput.\
    \ 2022, 6, 18\n13 of 27\nqueries. This data are then visualized by mapping on\
    \ various visualizations. Before query\nevaluation, two BIM collections are merged\
    \ to optimize the process of query execution.\nUsing this technology, a 40% reduction\
    \ in response time has been witnessed compared\nto other traditional technologies.\
    \ Currently, the utilization of BIM is limited across the\nconstruction and facilities\
    \ management stages. The real intent of BIM could only be\nachieved once applied\
    \ at each stage of the building lifecycle.\n4.2. Big Data Storage\nBig data storage\
    \ is also an important aspect of BDE. In construction, big data storage\nhas been\
    \ explored for forecasting the success of construction projects [68], smart buildings\n\
    data storage [69], tender price evaluation [70], and others. Despite the availability\
    \ of BIM\ndata storage, the current applications in construction still require\
    \ successful implemen-\ntation. Social BIM, proposed by Das et al. [71], captures\
    \ building models and the social\ninteractions among the users. The authors developed\
    \ BIMCloud based on the distributed\nBIM framework.\nSimilarly, a two-tiered hybrid\
    \ data infrastructure was proposed by Jeong et al. [72]\nfor data management and\
    \ monitoring of bridges. In this model, the client tier efﬁciently\ncompletes\
    \ some analytical tasks by storing structured data momentarily using MongoDB,\n\
    while the central tier stores sensor data permanently using Apache Cassandra.\
    \ Lin et al. [67]\nalso used MongoDB to store BIM data obtained through building\
    \ models.\nOverall big data storage is provided by either emerging NoSQL databases\
    \ or dis-\ntributed ﬁle systems, as explained subsequently.\n4.2.1. Distributed\
    \ File Systems\nThe distributed ﬁle systems consist of Hadoop Distributed File\
    \ System (HDFS) and\nTachyon. HDFS is designed to deal with large and complex\
    \ databases such as those related\nto BIM, waste, and other construction big data\
    \ sources. It operates with the commodity\nservers grouped together in a cluster.\
    \ As it utilizes several servers, the probability of\nhardware failure also increases.\
    \ To overcome this problem, HDFS introduces fault tolerance\nachieved through\
    \ the distribution of data and their replication. However, in situations\nwhere\
    \ low-latency data access is required, HDFS is not a suitable option as it shows\n\
    inferior performance. Moreover, it is also troublesome to save many small ﬁles\
    \ due to\nissues in managing meta-data. Moreover, it is not useful if modiﬁcations\
    \ must be made\nconcurrently at random locations in the data. Nevertheless, HDFS\
    \ has been utilized by\nconstruction researchers for observing construction workers’\
    \ behavior [73], improving road\nperformance [39], and investigating proﬁtability\
    \ performance [39]. Furthermore, based on\nthe distributed input from HDFS, it\
    \ facilitates building predictive models for conducting\nbuilding simulations\
    \ that give output in a predictive model markup language.\nTachyon is a distributed\
    \ ﬁle system designed to extend HDFS beneﬁts by providing\naccess to the distributed\
    \ data across the cluster at memory speed. It provides better\nperformance through\
    \ in-memory data caching and backward compatibility allows MR and\nSpark tasks\
    \ to run without changing the codes required in those programs. Tachyon has\n\
    been utilized in construction for handling unstructured documents [65] and ﬁle\
    \ storage [74].\nThe Tachyon performs better than HDFS, is backward compatible\
    \ and can handle the\nMapReduce jobs without any further modiﬁcations.\n4.2.2.\
    \ NoSQL Databases\nRelational databases have been common for data management in\
    \ past decades. How-\never, new applications were designed for better performance,\
    \ scalability, and ﬂexibility\nas the technology emerged. Relational databases\
    \ lag because of their special processing\nand storage needs. As a result, new\
    \ systems were devised to ﬁll this technology gap. One\nsuch system is the “Not\
    \ only SQL” system that has optimized data management in several\nways. For achieving\
    \ ﬂexibility, it supports schemaless storage rather than schema-oriented\nstorage.\
    \ NoSQL has been widely used in different industries, including construction,\
    \ due\nBig Data Cogn. Comput. 2022, 6, 18\n14 of 27\nto its fragmented nature.\
    \ Some examples of NoSQL in construction include integration\nof lessons learned\
    \ knowledge in BIM [75], web service framework for construction sup-\nply chain\
    \ collaboration and management [76], and Social BIMCloud implementation [71].\n\
    NoSQL systems store schemaless data in a non-relational model. It does not set\
    \ too many\nrestrictions on value and allows easy product determination. Generally,\
    \ when NoSQL\ndatabases are set to key values, they carry out only speciﬁc tasks\
    \ without evaluating speciﬁc\nvalues. The key-value database is mainly tailored\
    \ to the business accessed through the\nprimary key. These systems have four data\
    \ models that are brieﬂy discussed below.\n•\nKey-value\nThis is the simplest\
    \ data model used for unstructured data storage. However, the data\nlack self-description.\
    \ It has been used for knowledge management in construction [77] and\nintegration\
    \ of lessons learned knowledge in BIM [75]. BIM provides positive outcomes\non\
    \ project success, such as cost and time reduction, communication and coordination\n\
    improvement, and increased quality. Big data utilization in BIM can be beneﬁcial\
    \ to discover\nroot causes of poor building performance, perform real-time data\
    \ queries, improve the\ndecision-making process, improve productivity, and reveal\
    \ new designs and services in the\nconstruction industry, as is the case in every\
    \ industry.\n•\nDocument\nThis model can store self-describing data. However,\
    \ this model can lag in terms of efﬁ-\nciency. It has been used for uniﬁed lifecycle\
    \ data management in architecture, engineering,\nconstruction, and facilities\
    \ management through BIM integration [78].\n•\nColumnar\nAggregated columns, grouped\
    \ sub-columns, and sparse data can be stored by using\nthis model. It has been\
    \ used for integrating digital construction through the internet of\nthings [79]\
    \ and smart archiving of energy and petroleum construction projects [80].\n•\n\
    Graph\nThis model works well for property-graph-based huge datasets in relationship\
    \ traver-\nsal. It has been used for the 4D construction management information\
    \ model of prefab-\nricated buildings [81] and the development of a BIM-enabled\
    \ software tool for facility\nmanagement [82].\nDatabases concerning big data\
    \ storage and management are widely used worldwide\nfor research on various topics.\
    \ The construction industry also relies on big data sources\nand databases, observed\
    \ throughout the last ﬁve years to a decade. As shown in Figure 8,\nthe search\
    \ engine is among the most widely searched database in the last ﬁve years,\nfollowed\
    \ by relational and graph DBMS. Until the time of analyzing data for this review,\n\
    i.e., November 2021, other heavily used databases for extracting and using big\
    \ data for the\nconstruction industry include document stores, native XML, key-value\
    \ stores, and wide\ncolumn stores. Object-oriented DBMS and multivalued DBMS search\
    \ are considerably\nlower than relational DBMS and graph DBMS, whereas the search\
    \ engines outperform all\nother DBMS. These different databases provide data sources\
    \ for BIM and computational\nsources for developing structures that could guide\
    \ larger construction projects. The rising\ntrend in using big data sources shows\
    \ the increasing interest among the construction\nindustries in big data. For\
    \ example, exchanging and reusing information is critical for\nengineering and\
    \ construction project management. The issues pertaining to data exchange\nhave\
    \ been minimized with the Extensible Markup Language (XML) application. Such\n\
    an XML-based Distributed Construction Estimating System (XDCES) has been helpful\
    \ to\nreduce the overload of cost-estimating information exchange. Similarly,\
    \ construction-based\nDBMS enables all construction companies to build and maintain\
    \ a database easily. It allows\nsupervisors and workers to capture information\
    \ using a mobile or tablet device, and then\nall of that information is stored\
    \ in the cloud and accessible via a desktop version.\nBig Data Cogn. Comput. 2022,\
    \ 6, 18\n15 of 27\nBig Data Cogn. Comput. 2022, 6, 18 \n17 of 29 \n \n \nFigure\
    \ 8. Database popularity in 2016–2021 based on search trends. \n4.3. Big Data\
    \ Analytics (BDA) \nBDA gathers information from a variety of disciplines. All\
    \ these disciplines have one \nthing in common: to find out data patterns. Some\
    \ of these related disciplines are data min-\ning, statistics, business analytics,\
    \ predictive analytics, data analytics, knowledge discov-\nery from data, and\
    \ the most recent one, big data. Big data use the previous techniques to \nbroaden\
    \ the field of data analytics. For BDA, some of the ML-based tools are developed.\
    \ \nIn construction projects, BDA has been used for improving building design\
    \ and effective \nperformance monitoring [37], project safety, energy, resource,\
    \ overall management and \ndecision-making frameworks [38], and quality and waste\
    \ management [24]. Big data ana-\nlytics has been taken a step further by developing\
    \ predictive analysis techniques. Ngo et \nal. [83] used a factor-based big data\
    \ predictive analytic tool for analyzing the capacity of \nFigure 8. Database\
    \ popularity in 2016–2021 based on search trends.\n4.3. Big Data Analytics (BDA)\n\
    BDA gathers information from a variety of disciplines. All these disciplines have\n\
    one thing in common: to ﬁnd out data patterns. Some of these related disciplines\
    \ are\ndata mining, statistics, business analytics, predictive analytics, data\
    \ analytics, knowledge\ndiscovery from data, and the most recent one, big data.\
    \ Big data use the previous tech-\nniques to broaden the ﬁeld of data analytics.\
    \ For BDA, some of the ML-based tools are\ndeveloped. In construction projects,\
    \ BDA has been used for improving building design\nand effective performance monitoring\
    \ [37], project safety, energy, resource, overall man-\nagement and decision-making\
    \ frameworks [38], and quality and waste management [24].\nBig data analytics\
    \ has been taken a step further by developing predictive analysis tech-\nniques.\
    \ Ngo et al. [83] used a factor-based big data predictive analytic tool for analyzing\n\
    the capacity of construction industries to deal with big data. This tool was tested\
    \ and\nvalidated on four different construction organizations to ensure that the\
    \ predictive analytic\nBig Data Cogn. Comput. 2022, 6, 18\n16 of 27\nmethod could\
    \ improve how the construction industry can use big data. The integration of\n\
    big data in the construction industry remains an avenue that requires further\
    \ research in\nterms of big data analytics. The gaps in this area were explored\
    \ by Atuahene et al. [30] and\nAtuahene et al. [84]. It was identiﬁed that the\
    \ management and processing of data by ﬁrms\nled to the generation of more data,\
    \ which made data analysis an uphill task. Developing an\nintegrated framework\
    \ for managing big data and sorting the useful datasets can greatly\nincrease\
    \ the usability and application of big data in the construction industry. Overall,\n\
    data analytics is conducted through statistical, data mining, and regression techniques,\
    \ as\nexplained below.\n4.3.1. Statistics\nStatistics has wide applications in\
    \ the construction industry. Statistical techniques\nincluding Monte Carlo simulation,\
    \ Gaussian distribution, non-Bayesian methods, correla-\ntion analysis, factor\
    \ analysis, decision trees, Naïve Bayes, and others have been reported\nby various\
    \ studies in construction [85,86]. Some of the areas that beneﬁtted from statis-\n\
    tics include learning from post-project reviews, identifying causes of construction\
    \ delays,\nanalyzing buildings for structural damages, construction litigation,\
    \ and identifying and\nrecognizing heavy machinery and workers. Other examples\
    \ of statistics in construction\nare those of bidding statistics to predict completed\
    \ construction cost [87], accidents statis-\ntics [88], quality control [89],\
    \ and six sigma for project success [90,91]. From measuring\nthe bid-to-win ratio\
    \ to how much a project is over budget or schedule, and KPIs, the more\nnumbers\
    \ you can put behind your work, the better. Data not only allow for more visibility\n\
    into the state of a particular project, but relevant industry statistics and facts\
    \ can provide\nvaluable information needed to make important future decisions\
    \ regarding preconstruction\nand planning, productivity tools, risk assessment,\
    \ and workforce and operational efﬁciency.\nTable 1 presents some uses of statistical\
    \ models in construction.\nTable 1. Use of statistical models in construction.\n\
    Purpose\nTechniques\nReferences\nDamage detection in buildings\nMonte Carlo simulation\n\
    Gaussian distribution\n[85]\nConstruction time scheduling\nGaussian distribution\n\
    [86]\nPredicting project delays\nMonte Carlo simulation\nNon-Bayesian methods\n\
    Correlation analysis\nFactor analysis\n[92–94]\nDecision making\nDecision trees\n\
    Naïve Bayes\n[95,96]\n4.3.2. Data Mining\nData mining is used to extract meaningful\
    \ patterns in the data. It has been an inte-\ngral part of all big data management\
    \ systems. It employs the techniques used in pattern\nrecognition, ML, and statistics.\
    \ Several models are assessed, and the ones with the best\ntolerance and high\
    \ accuracy are selected and used for obtaining predictive results. In\nconstruction,\
    \ data mining has been reported in waste management [97], BIM-based con-\nstruction\
    \ engineering quality management [98], and other relevant areas. Data mining\n\
    detects useful regularities and information necessary for decision making for\
    \ construction\nmanagement projects. A data mining method such as cluster analysis\
    \ is important for\nthe construction industry, as it combines different construction\
    \ objects into homogeneous\ngroups and investigates them.\nData mining is supported\
    \ through data warehousing. Specially structured data is\nstored in data warehousing\
    \ for querying and analysis. Extract, transform and load (ETL) is\na program that\
    \ allows the collation of transactional data and operational data. Warehouse\n\
    Big Data Cogn. Comput. 2022, 6, 18\n17 of 27\ndata analysis is conducted using\
    \ Online Analytical Processing (OLAP), which performs\nbetter than SQL in computing\
    \ breakdown and data summaries. OLAP has been used for\ncost data management in\
    \ construction cost estimates by Moon et al. [99]. OLAP technology\ndeals with\
    \ the operational data and data obtained using big data technology. OLAP is\n\
    presented as a multidimensional cube that rapidly processes datasets.\nSimilarly,\
    \ different data mining techniques have been used to identify construction\ndelays.\
    \ For analyzing construction datasets, Kim et al. [12] presented a framework of\
    \ knowl-\nedge discovery in databases (KDD). In the KDD, the most time-consuming\
    \ and challenging\nstep is data preprocessing. Nevertheless, KDD is a powerful\
    \ tool for identifying casual\nrelationships in construction projects and reducing\
    \ construction variability by identifying\nand eliminating causes for possible\
    \ deviations. With the application of KDD, randomness\nof construction projects\
    \ and novel patterns can be determined. Other techniques include\ndimensional\
    \ matrix analysis, link analysis, and text analysis [100]. Other datasets with\n\
    information related to delay causes, BIM-based knowledge discovery, intelligent\
    \ learn-\ning, and the prevention of occupational injuries can be easily extended\
    \ in the domain of\ndata mining.\n4.3.3. Regression Techniques\nBased on an input\
    \ variable, regression predicts the value of the target variable. It is a\nsupervised\
    \ ML method. Regression is categorized into simple linear and multiple linear\n\
    regression based on explanatory variables. In simple linear regression, the relationship\n\
    between two variables (an explanatory variable x and a dependent variable y) is\
    \ modeled\nusing ML. While in multiple linear regression, two or more explanatory\
    \ variables are\nused and their relationship with the dependent variable is modeled.\
    \ The more common\nregression technique is multiple linear regression.\nRegression\
    \ has been extensively used in construction research. For example, it has been\n\
    used to predict properties of concrete cured under hot weather [48], predicting\
    \ ﬁnal cost for\ncompetitive bids on construction projects [101], determining\
    \ contingency in international\nconstruction projects [102], estimating performance\
    \ time for construction projects [103],\nand others. Moreover, regression has\
    \ been used for cost estimation, which is a difﬁcult task\nin the early stages\
    \ of the project. Adoption of parametric methods such as regression and\nmultiple\
    \ regression can be applied as both analytical and predictive techniques to estimate\n\
    the overall reliability of the cost estimation.\n4.4. The 10 vs. of Big Data\n\
    The bulk and variety of big data gathering enormously each day make it virtually\n\
    impossible to deal with the data sources seamlessly. On the other hand, the enormity\
    \ of\nbig data gives it many characteristics that further expand the potential\
    \ of big data and\nits applications in different research ﬁelds. Figure 9 provides\
    \ an overview of some of the\ncrucial characteristics of big data, also known\
    \ as the vs. of big data. The 10 vs. of big data\nhave been discussed in Figure\
    \ 9. Understanding these characteristics of big data enables\nthe identiﬁcation\
    \ of opportunities and challenges. The most crucial properties of big data\ninclude\
    \ their value, volume, velocity, variety, veracity, volatility, validity, variability,\
    \ vulner-\nability, and visualization, also known as the 10 vs. of big data [104].\
    \ These characteristics of\nvs. are used to guide research in different areas\
    \ and ﬁelds.\nIn terms of the use of big data in the ﬁeld of construction, analyzing\
    \ the vs. can help\nexplore how big data can be used for developing better construction\
    \ models in the future.\nFirstly, big data provide great value using various databases\
    \ and sources that inform the\nresearch studies and algorithm developments related\
    \ to computational models of different\nbuilding structures. In addition to the\
    \ value of research, big data also provide a bulk of\ninformation needed for research\
    \ simply through the ever-increasing volume of data that\nbecomes available each\
    \ day. Furthermore, the velocity with which databases expand each\nday adds variety\
    \ to the sort of data available for utilization in ﬁelds like construction. The\n\
    variety of data present is not varying just in terms of the data sources but also\
    \ the types\nBig Data Cogn. Comput. 2022, 6, 18\n18 of 27\nof data. For example,\
    \ big data can be present in the form of written text, graphs, pictures,\nand\
    \ various other formats to help manage construction project schedules and progress\n\
    reporting. The increasing amounts of data make the visualization process quite\
    \ complex.\nTherefore, it is crucial to develop new ways for data visualization\
    \ and analysis to keep with\nthe volatility of big data.\nBig Data Cogn. Comput.\
    \ 2022, 6, 18 \n21 of 29 \n \nvalidating information related to construction projects.\
    \ The ability to visualize big data, \nkeep up with the variety of data, and accept\
    \ the volatility, vulnerability, and variability \nthat come with the veracity\
    \ of data helps ensure that big data could be truly applicable in \nthe construction\
    \ industry. Therefore, the value of big data in the construction industry is \n\
    high and it helps guide future projects. \n \nFigure 9. The 10 vs. of big data.\
    \ \n4.5. Machine Learning Techniques \nOne AI subdomain is ML which can be used\
    \ to learn from the data using computa-\ntional systems. The tools used for big\
    \ data ML are presented in Table 2. ML is further \ncategorized into: (i) supervised\
    \ learning; (ii) unsupervised learning; (iii) association; and \n(iv) numeric\
    \ prediction. ML has several applications in the construction industry. It uses\
    \ \ndifferent approaches, including rule-based learning approaches, case-based\
    \ reasoning \ntechniques, artificial neural networks, and hybrid methodologies.\
    \ \nML has immense potential as a tool in the field of construction. Over the\
    \ last two \ndecades, several ML algorithms have been proposed to aid and improve\
    \ the overall pro-\ncess of construction. For example, ML has been used to predict\
    \ properties of concrete [48], \ncontract management [109], site safety and injury\
    \ prediction [46], delay risks management \n[45], BIM integrated on-demand site\
    \ monitoring [47], and other areas of construction en-\ngineering and management.\
    \ \nVarious ML tools are integrated at different steps along with the construction\
    \ man-\nagement processes. Different ML interfaces such as PyTorch and Keras.io\
    \ help develop \ncomputational models based on existing data for building futuristic\
    \ construction models. \nBIM can also be improved by using big data and ML tools,\
    \ as these technologies allow the \nopportunity to explore how technology could\
    \ be applied to the construction industry \n[110]. Over the last few years, different\
    \ algorithms have been explored to predict various \nproject phases and guide\
    \ construction projects from inception to closure [111]. Firstly, de-\ncision\
    \ trees and similar tools are used for developing an overall project timeline\
    \ to predict \nor determine construction project performance in various phases.\
    \ Secondly, statistical \nFigure 9. The 10 vs. of big data.\nThe 10 vs. of big\
    \ data are among the crucial characteristics representing the true picture\nof\
    \ big data as a ﬁeld of research. The applications of big data in the construction\
    \ industry\nare innumerable and they can all be categorized and managed through\
    \ understanding the\ncharacteristic features (or Vs) of big data. The construction\
    \ industry beneﬁts immensely\nas a business by integrating big data technologies.\
    \ The correlation with the business side\nof the construction industry has been\
    \ explored in light of the 10 vs. of big data and it\nhas been found that these\
    \ characteristics provide an immense business growth potential.\nStarting from\
    \ the core attributes of volume, variety and velocity, big data have come a\n\
    long way in terms of their applications and trends. Today, there are 10 characteristics\
    \ that\ndeﬁne big data and are also crucial for implementing big data into different\
    \ ﬁelds. It is\ncrucial to understand that these 10 vs. of big data can be explained\
    \ in a context-dependent\nmanner considering the ﬁeld of research. As for the\
    \ construction industry, the variety and\nvolume of big data are immense, but\
    \ there is also a great deal of variability in the data\npresent. For example,\
    \ the choice of building materials and the suitability of the selected\nmaterials\
    \ in different projects depend on several different factors. In this case, analyzing\n\
    the applicability of big data is possible through data-visualizing techniques\
    \ that can help\ndeal with the volatility and variability of big data. Similarly,\
    \ the validity and veracity of\nbig data in construction can be judged only after\
    \ analyzing the value that the data sources\nbring and the authenticity that these\
    \ sources present. Therefore, the increasing velocity of\nbig data is not useful\
    \ as an independent factor. Instead, the application of big data in the\nconstruction\
    \ industry depends on the 10 different characteristics (Vs) which are associated\n\
    with big data and are explained in Figure 9.\nBig Data Cogn. Comput. 2022, 6,\
    \ 18\n19 of 27\nSimilarly, these data types can be reﬁned and unstructured, further\
    \ adding variety\nto the type of data present for various reporting and research\
    \ purposes. Veracity refers to\nthe reliability of big data. This is guided by\
    \ statistics as the enormity of big data makes\nit hard to identify reliable data\
    \ sources. Therefore, validating data sources and ensuring\nthat they can be reliably\
    \ used to guide construction project developments is crucial for\nresearch. The\
    \ veracity of data sources leads to another important characteristic of big data:\n\
    variability. It is crucial to understand that big data can be highly variable\
    \ depending on the\nsources used for extracting the datasets. Understanding these\
    \ characteristics of big data\nand analyzing these characteristics given the use\
    \ of big data in the construction industry\ncan greatly enhance the potential\
    \ of future construction projects.\nOverall, multiple construction-related studies\
    \ have reported the usage of vs. of big\ndata. For example, velocity has been\
    \ reported for high-speed construction data process-\ning [105]. Value has been\
    \ reported for smarter universities and campuses [106]. Volume\nhas been reported\
    \ for mass level offsite construction material and component produc-\ntion [107].\
    \ Variety has been reported for investigating the proﬁtability performance of\n\
    construction projects [39]. Veracity has been reported for forecasting the success\
    \ of con-\nstruction projects [68]. Similarly, variability has been reported for\
    \ modeling occupational\naccidents in construction projects [108].\nBig data necessitate\
    \ cost-effective, innovative information processing forms for\nenhanced insights\
    \ and decision making. Construction companies can analyze histori-\ncal datasets\
    \ and carry out predictive analytics to forecast future events. Data-driven\n\
    decision making has the potential to reshape the entire business. Together, the\
    \ 10 attributes\nor 10 vs. of big data play a crucial role in the construction\
    \ industry. The volume of data\nand the velocity through which data are produced\
    \ at high speed lead to the possibility of\nvalidating information related to\
    \ construction projects. The ability to visualize big data,\nkeep up with the\
    \ variety of data, and accept the volatility, vulnerability, and variability that\n\
    come with the veracity of data helps ensure that big data could be truly applicable\
    \ in the\nconstruction industry. Therefore, the value of big data in the construction\
    \ industry is high\nand it helps guide future projects.\n4.5. Machine Learning\
    \ Techniques\nOne AI subdomain is ML which can be used to learn from the data\
    \ using computational\nsystems. The tools used for big data ML are presented in\
    \ Table 2. ML is further categorized\ninto: (i) supervised learning; (ii) unsupervised\
    \ learning; (iii) association; and (iv) numeric\nprediction. ML has several applications\
    \ in the construction industry. It uses different\napproaches, including rule-based\
    \ learning approaches, case-based reasoning techniques,\nartiﬁcial neural networks,\
    \ and hybrid methodologies.\nML has immense potential as a tool in the ﬁeld of\
    \ construction. Over the last two\ndecades, several ML algorithms have been proposed\
    \ to aid and improve the overall process\nof construction. For example, ML has\
    \ been used to predict properties of concrete [48],\ncontract management [109],\
    \ site safety and injury prediction [46], delay risks manage-\nment [45], BIM\
    \ integrated on-demand site monitoring [47], and other areas of construction\n\
    engineering and management.\nVarious ML tools are integrated at different steps\
    \ along with the construction man-\nagement processes. Different ML interfaces\
    \ such as PyTorch and Keras.io help develop\ncomputational models based on existing\
    \ data for building futuristic construction models.\nBIM can also be improved\
    \ by using big data and ML tools, as these technologies allow the\nopportunity\
    \ to explore how technology could be applied to the construction industry [110].\n\
    Over the last few years, different algorithms have been explored to predict various\
    \ project\nphases and guide construction projects from inception to closure [111].\
    \ Firstly, decision\ntrees and similar tools are used for developing an overall\
    \ project timeline to predict or\ndetermine construction project performance in\
    \ various phases. Secondly, statistical analysis\ntools are used for analyzing\
    \ previous projects and choosing guiding principles for future\nprojects [112].\
    \ Finally, design tools are integrated with ML algorithms to build 3D con-\nBig\
    \ Data Cogn. Comput. 2022, 6, 18\n20 of 27\nstruction models and graphics for\
    \ building models. These computational models enable\nanalyzing construction projects\
    \ by planning through look-up schedules and looking for\nways to improve buildings\
    \ and other structures [113].\nThe combined use of big data, ML, and AI holds\
    \ the potential to develop seamless\nconstruction projects and enable the development\
    \ of structures that can withhold severe\nweather conditions and disasters. For\
    \ example, one of the key uses of ML tools in futuristic\nconstruction projects\
    \ can be the development of structures that can stand through natural\ndisasters\
    \ and provide safety nets to communities during ﬂoods and other disasters [114].\n\
    Similarly, post-disaster evacuation and rescue of individuals can also be carried\
    \ out more\neasily if the area contains structures such as roads and buildings\
    \ built through the use of\nstatistical modeling, thus providing safe routes for\
    \ people [115]. Although the automation\nof construction projects remains a future\
    \ goal, the integration of different ML algorithms\nis already underway. Managing\
    \ costs, timelines, and human resources on a construction\nproject are areas guided\
    \ by various algorithms and computational models [116]. The ML\napproach can also\
    \ be applied to develop leading indicators to classify sites according to\ntheir\
    \ safety risk in construction projects.\nTable 2. Machine learning tools used\
    \ for big data.\nNo.\nTool\nDescription\nSupported Algorithm\nLanguages\nApplications\
    \ in Construction\nRef.\n1\nPyTorch\nPyTorch is a free tool available\nfor Windows,\
    \ Mac OS, and\nLinux for developing\nML programs\nRegression\nClassiﬁcation\n\
    Clustering\nDimensionality reduction\nPreprocessing\nC, C++,\nPython\nObject detection,\
    \ analyzing\nbuildings and other structures\nto develop better models\n[117]\n\
    2\nApache\nMahout\nAn open-source tool that allows\nhigh-performing and scalable\n\
    applications using ML\nDistributed Linear\nAlgebra\nClustering\nRegression\nPreprocessing\n\
    Java, Scala\nProcessing big data for the\ndevelopment of building\nmodels and\
    \ appropriate\nalgorithms\n[118,119]\n3\nShogun\nA diverse ML platform\nsupporting\
    \ various languages\nand platforms. Works well with\nWindows, Linux, and Mac OS\n\
    Classiﬁcation\nRegression\nDimensionality reduction\nOnline learning\nSupport\
    \ vector machines\nC++\nProvides a platform for\nanalyzing data and\ndeveloping\
    \ strategies for\nconstruction projects using\navailable information in the\n\
    form of big data\n[120]\n4\nSciKit\nLearn\nA free, machine-learning tool\nthat\
    \ supports Windows, Mac OS,\nand Linux\nRegression\nClassiﬁcation\nPreprocessing\n\
    Clustering\nModel selection\nC, C++,\nCython,\nPython\nEnables statistical analysis\
    \ for\nconstruction projects,\nparticularly using existing\ndata for developing\
    \ suitable\nconstruction models\n[121]\n5\nKeras.io\nAn ML software that can be\n\
    used across different platforms\nAPI for neural networks\nPython\nProvides training\
    \ models\nwhich can be harnessed for\nimproving BIM and creating\nconﬁdent models\
    \ for\nconstruction projects\n[122]\n5. Future Opportunities of Big Data in Construction\n\
    There is immense potential for the use of big data in the construction industry.\
    \ The\nuse of big data and ML can enable construction automation. These tools\
    \ can also enhance\nthe overall project by removing various hurdles and roadblocks\
    \ that tend to slow down\ndifferent projects. The construction industry is quite\
    \ dynamic and demanding, with the\nneed for labor strength and human resources\
    \ to ensure the smooth running of projects.\nThe constant challenge of keeping\
    \ projects on track and ensuring that new buildings and\nstructures are made up\
    \ to modern standards puts much strain on the project management\nteams. These\
    \ roadblocks can greatly be reduced with the use of big data and ML. The\ncore\
    \ aim of using big data in the construction industry is to enhance the project\
    \ planning\nphases and speed up the overall construction process by predicting\
    \ the possible timelines\nBig Data Cogn. Comput. 2022, 6, 18\n21 of 27\nfor particular\
    \ projects and identifying what factors can be worked on to improve the overall\n\
    process [123].\nThe automation of the construction projects will require the combined\
    \ use of big data,\ndeep learning, and ML tools. One of the major concerns with\
    \ such projects is ensuring\nworkers’ safety and developing strategies for overcoming\
    \ potential threats to the overall\nprocess. Safety of the workers and the structures\
    \ is essential for the smoother development\nof construction projects. The use\
    \ of big data and related tools can ensure that existing data\nand information\
    \ can be used for drafting guiding principles and then building computa-\ntional\
    \ models accordingly. For example, using sensor-based wearable personal protective\n\
    equipment, the big data of near misses, onsite accidents, hazards, and other issues\
    \ can be\ngenerated for developing safety plans and management techniques. Similarly,\
    \ big data,\nBIM, and cloud-powered simulations can help minimize project waste\
    \ and help produce\nsuperior quality constructed facilities. Further, big data\
    \ artifacts generated by 3D scanners\nfor as-built drawing development are another\
    \ key advantage whereby the rehabilitation\nplans of ancient heritage sites can\
    \ be developed.\nThe future holds great potential for the construction industry\
    \ through big data in-\ntegration. Some of the key opportunities for the construction\
    \ industries lie in using big\ndata for business and environmental sustainability.\
    \ The current roadblocks faced by the\nconstruction industry can be overcome in\
    \ the future through the integration of information\nextracted through big data.\
    \ The use of information gathered from past and present projects\ncan help develop\
    \ sustainable infrastructure in the long term. It is possible to avoid past\n\
    mistakes and use better quality products guided by the information found through\
    \ big data\nin construction. Future research directions in the ﬁeld of construction\
    \ rely heavily on big\ndata as the presence of information sources can help in\
    \ building better infrastructure and\ngreatly improve building designs and the\
    \ overall construction business. The construction\nindustry must move towards\
    \ automation and build upon the integration of technology\nto make the future\
    \ use of big data seamless and hassle-free. The use of big data tools,\nBIM, and\
    \ CAD can only be possible if the relevant support and integration systems are\n\
    present [107]. Hence, the future of the construction industry depends on upgrading\
    \ the\npresent environment gradually.\nOverall, the role of big data in enabling\
    \ the entire process of futuristic construction\nprojects is undeniable. Data\
    \ play a crucial role in developing training models and smoothly\nenabling the\
    \ process of construction. Future developments in this ﬁeld will also include\n\
    the generation and use of more algorithms and models that rely on big data, owing\
    \ to the\nneed to train the models reliably.\n6. Conclusions\nThe construction\
    \ industry is yet to reap the true beneﬁts of using big data aptly. Over\nthe\
    \ last two decades, the rapid growth of big data technologies has caused a spike\
    \ in\nthe number of models and platforms that have been developed for increasing\
    \ digitaliza-\ntion across different ﬁelds. However, the same level of digitalization\
    \ has not truly been\nharnessed or integrated by the construction industry. A\
    \ critical overview of the existing\nliterature points towards the bulk of existing\
    \ resources and platforms that can easily be\napplied for construction management.\
    \ However, the state of implantation of adoption in\nconstruction is below par.\
    \ Therefore, the utilization and commercialization of big data to\nbeneﬁt the\
    \ construction industry are crucial. An extensive literature review enabled us\
    \ to\nidentify the potential of big data in construction as the industry generates\
    \ huge amounts\nof data daily and can greatly improve using the latest technologies.\
    \ The development of\nonline tools and software which enable infrastructure modeling\
    \ and CAD is a crucial step\nin the right direction for futuristic constructions.\
    \ Having explored the existing ML tools, we\nfound that these tools, coupled with\
    \ big data, can be applied in the construction industry.\nIn this paper, we have\
    \ discussed the existing tools used in big data, the use of statistics, big\n\
    data storage, and BDE. Overlap between these variables further creates complications\
    \ in\nthat more data are present and the ﬁeld of big data is ever-expanding.\n\
    Big Data Cogn. Comput. 2022, 6, 18\n22 of 27\nThe current study contributes to\
    \ the body of knowledge by providing a state-of-the-\nart review of relevant articles\
    \ focused on big data applications in construction published\nbetween 2010 and\
    \ 2021. It further provides various current applications and future oppor-\ntunities\
    \ of big data in the construction industry for practitioners and researchers to\
    \ ponder\nupon and initiates the necessary debate around practical implementation\
    \ and adoption of\nbig data applications in construction.\nThere are currently\
    \ various gaps and pitfalls that act as barriers to using big data to its\nfull\
    \ potential. Firstly, data generation is much faster than the tools available\
    \ for processing\nit. Moreover, big data integration into the construction industry\
    \ is quite an uphill task even\nwith the existing data processing tools.\nThe\
    \ current study is limited to the literature published in the last decade and\
    \ may\nnot include all the available papers due to speciﬁc selection criteria\
    \ developed in this\nstudy. Similarly, the search terms may not be holistic and\
    \ thus not exhaustive; a study\nconducted in the future with slightly different\
    \ search strings may produce different results.\nIn the future, the researchers\
    \ can expand upon and explore the ﬁve clusters identiﬁed in\nFigure 4. The individual\
    \ relations and adoption frameworks for big data in these clusters\ncan be explored.\n\
    Author Contributions: Conceptualization, H.S.M. and F.U.; methodology, H.S.M.,\
    \ F.U. and S.Q.;\nsoftware, H.S.M. and F.U.; validation, H.S.M., F.U., S.Q. and\
    \ D.S.; formal analysis, H.S.M. and F.U.;\ninvestigation, H.S.M., F.U. and S.Q.;\
    \ resources, H.S.M. and F.U.; data curation, H.S.M., F.U., S.Q. and\nD.S.; writing—original\
    \ draft preparation, H.S.M. and F.U.; writing—review and editing, H.S.M., F.U.,\n\
    S.Q. and D.S.; visualization, H.S.M. and F.U.; supervision, F.U.; project administration,\
    \ H.S.M. and\nF.U.; funding acquisition, H.S.M. and F.U. All authors have read\
    \ and agreed to the published version\nof the manuscript.\nFunding: This research\
    \ received no external funding.\nInstitutional Review Board Statement: Not applicable.\n\
    Informed Consent Statement: Not applicable.\nData Availability Statement: Data\
    \ are available with the ﬁrst author and can be shared upon\nreasonable request.\n\
    Conﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n\
    1.\nVillars, R.L.; Olofson, C.W.; Eastwood, M. Big data: What it is and why you\
    \ should care. White Pap. IDC 2011, 14, 1–14.\n2.\nSiddiqa, A.; Karim, A.; Gani,\
    \ A. Big data storage technologies: A survey. Front. Inf. Technol. Electron. Eng.\
    \ 2017, 18, 1040–1070.\n[CrossRef]\n3.\nPhaneendra, S.V.; Reddy, E.M. Big Data-solutions\
    \ for RDBMS problems-A survey. In Proceedings of the 12th IEEE/IFIP Network\n\
    Operations & Management Symposium (NOMS 2010), Osaka, Japan, 19–23 April 2013.\n\
    4.\nHenry, R.; Venkatraman, S. Big Data Analytics the Next Big Learning Opportunity.\
    \ J. Manag. Inf. Decis. Sci. 2015, 18, 17–29.\n5.\nXu, W.; Sun, J.; Ma, J.; Du,\
    \ W. A personalized information recommendation system for R&D project opportunity\
    \ ﬁnding in big\ndata contexts. J. Netw. Comput. Appl. 2016, 59, 362–369.\n6.\n\
    Sepasgozar, S.M.; Davis, S. Construction technology adoption cube: An investigation\
    \ on process, factors, barriers, drivers and\ndecision makers using NVivo and\
    \ AHP analysis. Buildings 2018, 8, 74. [CrossRef]\n7.\nUllah, F.; Sepasgozar,\
    \ S.M.; Wang, C. A systematic review of smart real estate technology: Drivers\
    \ of, and barriers to, the use of\ndigital disruptive technologies and online\
    \ platforms. Sustainability 2018, 10, 3142. [CrossRef]\n8.\nKwon, O.; Lee, N.;\
    \ Shin, B. Data quality management, data usage experience and acquisition intention\
    \ of big data analytics. Int. J.\nInf. Manag. 2014, 34, 387–394. [CrossRef]\n\
    9.\nCui, L.; Yu, F.R.; Yan, Q. When big data meets software-deﬁned networking:\
    \ SDN for big data and big data for SDN. IEEE Netw.\n2016, 30, 58–65. [CrossRef]\n\
    10.\nChaudhary, R.; Aujla, G.S.; Kumar, N.; Rodrigues, J.J. Optimized big data\
    \ management across multi-cloud data centers: Software-\ndeﬁned-network-based\
    \ analysis. IEEE Commun. Mag. 2018, 56, 118–126. [CrossRef]\n11.\nSimmhan, Y.;\
    \ Aman, S.; Kumbhare, A.; Liu, R.; Stevens, S.; Zhou, Q.; Prasanna, V. Cloud-based\
    \ software platform for big data\nanalytics in smart grids. Comput. Sci. Eng.\
    \ 2013, 15, 38–47. [CrossRef]\n12.\nKim, K.Y. Business intelligence and marketing\
    \ insights in an era of big data: The q-sorting approach. KSII Trans. Internet\
    \ Inf. Syst.\n(TIIS) 2014, 8, 567–582.\nBig Data Cogn. Comput. 2022, 6, 18\n23\
    \ of 27\n13.\nHu, X. Sorting big data by revealed preference with application\
    \ to college ranking. J. Big Data 2020, 7, 1–26. [CrossRef]\n14.\nCusters, B.;\
    \ Uršiˇc, H. Big data and data reuse: A taxonomy of data reuse for balancing big\
    \ data beneﬁts and personal data\nprotection. Int. Data Priv. Law 2016, 6, 4–15.\
    \ [CrossRef]\n15.\nMajumdar, J.; Naraseeyappa, S.; Ankalaki, S. Analysis of agriculture\
    \ data using data mining techniques: Application of big data.\nJ. Big Data 2017,\
    \ 4, 1–15. [CrossRef]\n16.\nShadroo, S.; Rahmani, A.M. Systematic survey of big\
    \ data and data mining in internet of things. Comput. Netw. 2018, 139, 19–47.\n\
    [CrossRef]\n17.\nZhou, R.; Liu, M.; Li, T. Characterizing the efﬁciency of data\
    \ deduplication for big data storage management. In Proceedings\nof the 2013 IEEE\
    \ international symposium on workload characterization (IISWC), Portland, OR,\
    \ USA, 22–24 September 2013;\npp. 98–108.\n18.\nPetri, I.; Rana, O.; Beach, T.;\
    \ Rezgui, Y.; Sutton, A. Clouds4Coordination: Managing project collaboration in\
    \ federated clouds. In\nProceedings of the 2015 IEEE/ACM 8th International Conference\
    \ on Utility and Cloud Computing (UCC), Limassol, Cyprus,\n7–10 December 2015;\
    \ pp. 494–499.\n19.\nHay, B.; Nance, K.; Bishop, M. Storm clouds rising: Security\
    \ challenges for IaaS cloud computing. In Proceedings of the 2011 44th\nHawaii\
    \ International Conference on System Sciences, Washington, DC, USA, 4–7 January\
    \ 2011; pp. 1–7.\n20.\nAfolabi, A.; Ojelabi, R.A.; Fagbenle, O.I.; Mosaku, T.\
    \ The economics of cloud-based computing technologies in construction\nproject\
    \ delivery. Int. J. Civ. Eng. Technol. (IJCIET) 2017, 8, 232–242.\n21.\nMoniruzzaman,\
    \ A.; Hossain, S.A. Nosql database: New era of databases for big data analytics-classiﬁcation,\
    \ characteristics and\ncomparison. arXiv 2013, arXiv:1307.0191.\n22.\nKouanou,\
    \ A.T.; Tchiotsop, D.; Kengne, R.; Zephirin, D.T.; Armele, N.M.A.; Tchinda, R.\
    \ An optimal big data workﬂow for\nbiomedical image analysis. Inform. Med. Unlocked\
    \ 2018, 11, 68–74. [CrossRef]\n23.\nRodrigues, M.; Santos, M.Y.; Bernardino, J.\
    \ Big data processing tools: An experimental performance evaluation. Wiley Interdiscip.\n\
    Rev. Data Min. Knowl. Discov. 2019, 9, e1297. [CrossRef]\n24.\nWang, D.; Fan,\
    \ J.; Fu, H.; Zhang, B. Research on optimization of big data construction engineering\
    \ quality management based on\nRNN-LSTM. Complexity 2018, 2018, 9691868. [CrossRef]\n\
    25.\nBilal, M.; Oyedele, L.O.; Akinade, O.O.; Ajayi, S.O.; Alaka, H.A.; Owolabi,\
    \ H.A.; Qadir, J.; Pasha, M.; Bello, S.A. Big data\narchitecture for construction\
    \ waste analytics (CWA): A conceptual framework. J. Build. Eng. 2016, 6, 144–156.\
    \ [CrossRef]\n26.\nMunawar, H.S.; Qayyum, S.; Ullah, F.; Sepasgozar, S. Big data\
    \ and its applications in smart real estate and the disaster management\nlife\
    \ cycle: A systematic analysis. Big Data Cogn. Comput. 2020, 4, 4. [CrossRef]\n\
    27.\nQadir, Z.; Khan, S.I.; Khalaji, E.; Munawar, H.S.; Al-Turjman, F.; Mahmud,\
    \ M.P.; Kouzani, A.Z.; Le, K. Predicting the energy\noutput of hybrid PV–wind\
    \ renewable energy system using feature selection technique for smart grids. Energy\
    \ Rep. 2021, 7,\n8465–8475. [CrossRef]\n28.\nMiller, H.J.; Tolle, K. Big data\
    \ for healthy cities: Using location-aware technologies, open data and 3D urban\
    \ models to design\nhealthier built environments. Built Environ. 2016, 42, 441–456.\
    \ [CrossRef]\n29.\nChen, X.; Lu, W. Scenarios for Applying Big Data in Boosting\
    \ Construction: A Review. In Proceedings of the 21st International\nSymposium\
    \ on Advancement of Construction Management and Real Estate, Guiyang, China, 24–27\
    \ August 2018; pp. 1299–1306.\n30.\nAtuahene, B.T.; Kanjanabootra, S.; Gajendran,\
    \ T. Towards an integrated framework of big data capabilities in the construction\n\
    industry: A systematic literature review. In Proceedings of the 34th Association\
    \ of Researchers in Construction Management\n(ARCOM), Belfast, UK, 3–5 September\
    \ 2018; p. 547.\n31.\nUllah, F. A beginner’s guide to developing review-based\
    \ conceptual frameworks in the built environment. Architecture 2021, 1,\n5–24.\
    \ [CrossRef]\n32.\nUllah, F.; Al-Turjman, F. A conceptual framework for blockchain\
    \ smart contract adoption to manage real estate deals in smart\ncities. Neural\
    \ Comput. Appl. 2021, 1–22. [CrossRef]\n33.\nUllah, F. Developing a Novel Technology\
    \ Adoption Framework for Real Estate Online Platforms: Users’ Perception and Adoption\
    \ Barriers;\nUniversity of New South Wales: Sidney, Australia, 2021.\n34.\nUllah,\
    \ F.; Qayyum, S.; Thaheem, M.J.; Al-Turjman, F.; Sepasgozar, S.M. Risk management\
    \ in sustainable smart cities governance:\nA TOE framework. Technol. Forecast.\
    \ Soc. Change 2021, 167, 120743. [CrossRef]\n35.\nQayyum, S.; Ullah, F.; Al-Turjman,\
    \ F.; Mojtahedi, M. Managing smart cities through six sigma DMADICV method: A\
    \ review-based\nconceptual framework. Sustain. Cities Soc. 2021, 72, 103022. [CrossRef]\n\
    36.\nHuang, X. Application of BIM Big Data in Construction Engineering Cost. J.\
    \ Phys. Conf. Ser. 2021, 1865, 032016. [CrossRef]\n37.\nLoyola, M. Big data in\
    \ building design: A review. J. Inf. Technol. Constr. 2018, 23, 259–284.\n38.\n\
    Ismail, S.A.; Bandi, S.; Maaz, Z.N. An appraisal into the potential application\
    \ of big data in the construction industry. Int. J. Built\nEnviron. Sustain. 2018,\
    \ 5, 145–154. [CrossRef]\n39.\nBilal, M.; Oyedele, L.O.; Kusimo, H.O.; Owolabi,\
    \ H.A.; Akanbi, L.A.; Ajayi, A.O.; Akinade, O.O.; Delgado, J.M.D. Investigating\n\
    proﬁtability performance of construction projects using big data: A project analytics\
    \ approach. J. Build. Eng. 2019, 26, 100850.\n[CrossRef]\n40.\nSharif, M.; Mercelis,\
    \ S.; Van Den Bergh, W.; Hellinckx, P. Towards real-time smart road construction:\
    \ Efﬁcient process management\nthrough the implementation of internet of things.\
    \ In Proceedings of the International Conference on Big Data and Internet of\n\
    Thing, London, UK, 20–22 December 2017; pp. 174–180.\nBig Data Cogn. Comput. 2022,\
    \ 6, 18\n24 of 27\n41.\nCurtis, C. Architecture at Scale: Reimagining One-Off\
    \ Projects as Building Platforms. Archit. Des. 2020, 90, 96–103. [CrossRef]\n\
    42.\nShtern, M.; Mian, R.; Litoiu, M.; Zareian, S.; Abdelgawad, H.; Tizghadam,\
    \ A. Towards a multi-cluster analytical engine for\ntransportation data. In Proceedings\
    \ of the 2014 International Conference on Cloud and Autonomic Computing, London,\
    \ UK,\n8–12 September 2014; pp. 249–257.\n43.\nYing, L.J.; Pheng, L.S. Enhancing\
    \ buildability in China’s construction industry using Singapore’s buildable design\
    \ appraisal\nsystem. J. Technol. Manag. China 2007, 2, 264–278. [CrossRef]\n44.\n\
    Munawar, H.S.; Ullah, F.; Qayyum, S.; Heravi, A. Application of Deep Learning\
    \ on UAV-Based Aerial Images for Flood Detection.\nSmart Cities 2021, 4, 1220–1242.\
    \ [CrossRef]\n45.\nGondia, A.; Siam, A.; El-Dakhakhni, W.; Nassar, A.H. Machine\
    \ learning algorithms for construction projects delay risk prediction.\nJ. Constr.\
    \ Eng. Manag. 2020, 146, 04019085. [CrossRef]\n46.\nTixier, A.J.-P.; Hallowell,\
    \ M.R.; Rajagopalan, B.; Bowman, D. Application of machine learning to construction\
    \ injury prediction.\nAutom. Constr. 2016, 69, 102–114. [CrossRef]\n47.\nRahimian,\
    \ F.P.; Seyedzadeh, S.; Oliver, S.; Rodriguez, S.; Dawood, N. On-demand monitoring\
    \ of construction projects through a\ngame-like hybrid application of BIM and\
    \ machine learning. Autom. Constr. 2020, 110, 103012. [CrossRef]\n48.\nMaqsoom,\
    \ A.; Aslam, B.; Gul, M.E.; Ullah, F.; Kouzani, A.Z.; Mahmud, M.; Nawaz, A. Using\
    \ Multivariate Regression and ANN\nModels to Predict Properties of Concrete Cured\
    \ under Hot Weather. Sustainability 2021, 13, 10164. [CrossRef]\n49.\nYang, A.;\
    \ Yu, G. Application of Heterogeneous Network Oriented to NoSQL Database in Optimal\
    \ Postevaluation Indexes of\nConstruction Projects. Discret. Dyn. Nat. Soc. 2022,\
    \ 2022, 4817300. [CrossRef]\n50.\nSanni-Anibire, M.O.; Zin, R.M.; Olatunji, S.O.\
    \ Machine learning model for delay risk assessment in tall building projects.\
    \ Int. J.\nConstr. Manag. 2020, 1–10. [CrossRef]\n51.\nLu, W.; Chen, X.; Ho, D.C.;\
    \ Wang, H. Analysis of the construction waste management performance in Hong Kong:\
    \ The public\nand private sectors compared using big data. J. Clean. Prod. 2016,\
    \ 112, 521–531. [CrossRef]\n52.\nHan, Z.; Wang, Y. The applied exploration of\
    \ big data technology in prefabricated construction project management. ICCREM\n\
    2017, 2017, 71–78.\n53.\nJiao, Y.; Zhang, S.; Li, Y.; Wang, Y.; Yang, B.; Wang,\
    \ L. An augmented MapReduce framework for building information modeling\napplications.\
    \ In Proceedings of the 2014 IEEE 18th International Conference on Computer Supported\
    \ Cooperative Work in Design\n(CSCWD), Hsinchu, Taiwan, 21–23 May 2014; pp. 283–288.\n\
    54.\nYu, T.; Liang, X.; Wang, Y. Factors affecting the utilization of big data\
    \ in construction projects. J. Constr. Eng. Manag. 2020,\n146, 04020032. [CrossRef]\n\
    55.\nQadir, Z.; Ullah, F.; Munawar, H.S.; Al-Turjman, F. Addressing disasters\
    \ in smart cities through UAVs path planning and 5G\ncommunications: A systematic\
    \ review. Comput. Commun. 2021, 168, 114–135. [CrossRef]\n56.\nAlaka, H.A.; Oyedele,\
    \ L.O.; Owolabi, H.A.; Bilal, M.; Ajayi, S.O.; Akinade, O.O. A framework for big\
    \ data analytics approach to\nfailure prediction of construction ﬁrms. Appl. Comput.\
    \ Inform. 2018, 16, 207–222. [CrossRef]\n57.\nAsadianfam, S.; Shamsi, M.; Kenari,\
    \ A.R. Hadoop Deep Neural Network for offending drivers. J. Ambient. Intell. Humaniz.\n\
    Comput. 2021, 13, 659–671. [CrossRef]\n58.\nLiu, R.-H.; Kuo, C.-F.; Yang, C.-T.;\
    \ Chen, S.-T.; Liu, J.-C. On construction of an energy monitoring service using\
    \ big data technology\nfor smart campus. In Proceedings of the 2016 7th International\
    \ Conference on Cloud Computing and Big Data (CCBD), Macau,\nChina, 16–18 November\
    \ 2016; pp. 81–86.\n59.\nSong, Y.; Clayton, M.J.; Johnson, R.E. Anticipating reuse:\
    \ Documenting buildings for operations using web technology. Autom.\nConstr. 2002,\
    \ 11, 185–197. [CrossRef]\n60.\nZhang, C.; Arditi, D. Automated progress control\
    \ using laser scanning technology. Autom. Constr. 2013, 36, 108–116. [CrossRef]\n\
    61.\nCaneparo, L. Shared virtual reality for design and management: The Porta\
    \ Susa project. Autom. Constr. 2001, 10, 217–228.\n[CrossRef]\n62.\nPalaneeswaran,\
    \ E.; Kumaraswamy, M.M. Knowledge mining of information sources for research in\
    \ construction management.\nJ. Constr. Eng. Manag. 2003, 129, 182–191. [CrossRef]\n\
    63.\nPan, W.; Ilhan, B.; Bock, T. Process information modelling (PIM) concept\
    \ for on-site construction management: Hong Kong case.\nPeriod. Polytech. Archit.\
    \ 2018, 49, 165–175. [CrossRef]\n64.\nKim, J.-S.; Kim, B.-S. Analysis of ﬁre-accident\
    \ factors using big-data analysis method for construction areas. KSCE J. Civil\
    \ Eng.\n2018, 22, 1535–1543. [CrossRef]\n65.\nBilal, M.; Oyedele, L.O.; Qadir,\
    \ J.; Munir, K.; Ajayi, S.O.; Akinade, O.O.; Owolabi, H.A.; Alaka, H.A.; Pasha,\
    \ M. Big Data in\nthe construction industry: A review of present status, opportunities,\
    \ and future trends. Adv. Eng. Inform. 2016, 30, 500–521.\n[CrossRef]\n66.\nChang,\
    \ C.-Y.; Tsai, M.-D. Knowledge-based navigation system for building health diagnosis.\
    \ Adv. Eng. Inform. 2013, 27, 246–260.\n[CrossRef]\n67.\nLin, J.R.; Hu, Z.Z.;\
    \ Zhang, J.P.; Yu, F.Q. A natural-language-based approach to intelligent data\
    \ retrieval and representation for\ncloud BIM. Comput.-Aided Civ. Infrastruct.\
    \ Eng. 2016, 31, 18–33. [CrossRef]\n68.\nNarayan, S.; Tan, H.C. Adopting big data\
    \ to forecast success of construction projects: A review. Malays. Constr. Res.\
    \ J. 2019, 6,\n132–143.\nBig Data Cogn. Comput. 2022, 6, 18\n25 of 27\n69.\nLinder,\
    \ L.; Vionnet, D.; Bacher, J.-P.; Hennebert, J. Big building data—A big data platform\
    \ for smart buildings. Energy Procedia\n2017, 122, 589–594. [CrossRef]\n70.\n\
    Zhang, Y.; Luo, H.; He, Y. A system for tender price evaluation of construction\
    \ project based on big data. Procedia Eng. 2015, 123,\n606–614. [CrossRef]\n71.\n\
    Das, M.; Cheng, J.C.; Kumar, S.S. Social BIMCloud: A distributed cloud-based BIM\
    \ platform for object-based lifecycle information\nexchange. Vis. Eng. 2015, 3,\
    \ 1–20. [CrossRef]\n72.\nJeong, S.; Byun, J.; Kim, D.; Sohn, H.; Bae, I.H.; Law,\
    \ K.H. A data management infrastructure for bridge monitoring. In Proceedings\n\
    of the Sensors and Smart Structures Technologies for Civil, Mechanical, and Aerospace\
    \ Systems 2015, San Diego, CA, USA, 9–12\nMarch 2015; p. 94350.\n73.\nGuo, S.;\
    \ Ding, L.; Luo, H.; Jiang, X. A Big-Data-based platform of workers’ behavior:\
    \ Observations from the ﬁeld. Accid. Anal. Prev.\n2016, 93, 299–309. [CrossRef]\n\
    74.\nRam, J.; Afridi, N.K.; Khan, K.A. Adoption of Big Data analytics in construction:\
    \ Development of a conceptual model.\nBuilt Environ. Proj. Asset Manag. 2019,\
    \ 9, 564–579. [CrossRef]\n75.\nOti, A.; Tah, J.; Abanda, F. Integration of lessons\
    \ learned knowledge in building information modeling. J. Constr. Eng. Manag.\n\
    2018, 144, 04018081. [CrossRef]\n76.\nDas, M.; Cheng, J.C.; Law, K.H. An ontology-based\
    \ web service framework for construction supply chain collaboration and\nmanagement.\
    \ Eng. Constr. Archit. Manag. 2015, 22, 551–572. [CrossRef]\n77.\nJing, Y.; Wang,\
    \ Y.-C.; Wang, Z. Knowledge management in construction—The framework of high value\
    \ density knowledge\ndiscovery with graph database. In Civil, Architecture and\
    \ Environmental Engineering; CRC Press: Boca Raton, FL, USA, 2017;\npp. 712–715.\n\
    78.\nJiao, Y.; Wang, Y.; Zhang, S.; Li, Y.; Yang, B.; Yuan, L. A cloud approach\
    \ to uniﬁed lifecycle data management in architecture,\nengineering, construction\
    \ and facilities management: Integrating BIMs and SNS. Adv. Eng. Inform. 2013,\
    \ 27, 173–188. [CrossRef]\n79.\nWoodhead, R.; Stephenson, P.; Morrey, D. Digital\
    \ construction: From point solutions to IoT ecosystem. Autom. Constr. 2018, 93,\n\
    35–46. [CrossRef]\n80.\nElZahed, M.; Marzouk, M. Smart archiving of energy and\
    \ petroleum projects utilizing big data analytics. Autom. Constr. 2022,\n133,\
    \ 104005. [CrossRef]\n81.\nYang, B.; Dong, M.; Wang, C.; Liu, B.; Wang, Z.; Zhang,\
    \ B. IFC-based 4D construction management information model of\nprefabricated\
    \ buildings and its application in graph database. Appl. Sci. 2021, 11, 7270.\
    \ [CrossRef]\n82.\nZibion, D. Development of a BIM-Enabled Software Tool for Facility\
    \ Management Using Interactive Floor Plans, Graph-Based\nData Management and Granular\
    \ Information Retrieval. Master’s Thesis, Aalto University, Espoo, Finland, 2018.\n\
    83.\nNgo, J.; Hwang, B.-G.; Zhang, C. Factor-based big data and predictive analytics\
    \ capability assessment tool for the construction\nindustry. Autom. Constr. 2020,\
    \ 110, 103042. [CrossRef]\n84.\nAtuahene, B.T.; Kanjanabootra, S.; Gajendra, T.\
    \ Beneﬁts of Big Data Application Experienced in the Construction Industry:\n\
    A Case of an Australian Construction Company. In Proceedings of the 36th Annual\
    \ Association of Researchers in Construction\nManagement (ARCOM) Conference, Virtual\
    \ Conference, Leeds, UK, 7–8 September 2020.\n85.\nLam, H.F.; Yang, J.H.; Au,\
    \ S.K. Markov chain Monte Carlo-based Bayesian method for structural model updating\
    \ and damage\ndetection. Struct. Control Health Monit. 2018, 25, e2140. [CrossRef]\n\
    86.\nAra, J.; Ali, S.; Shah, I. Monitoring schedule time using exponentially modiﬁed\
    \ Gaussian distribution. Qual. Technol. Quant. Manag.\n2020, 17, 448–469. [CrossRef]\n\
    87.\nWright, M.G.; Williams, T.P. Using bidding statistics to predict completed\
    \ construction cost. Eng. Econ. 2001, 46, 114–128.\n[CrossRef]\n88.\nAbdullah,\
    \ D.; Wern, G.C.M. An analysis of accidents statistics in Malaysian construction\
    \ sector. In Proceedings of the International\nConference on E-business, Management\
    \ and Economics, Dubai, United Arab Emirates, 28–30 December 2011; pp. 1–4.\n\
    89.\nMunawar, H.S.; Ullah, F.; Heravi, A.; Thaheem, M.J.; Maqsoom, A. Inspecting\
    \ Buildings Using Drones and Computer Vision:\nA Machine Learning Approach to\
    \ Detect Cracks and Damages. Drones 2022, 6, 5. [CrossRef]\n90.\nSiddiqui, S.Q.;\
    \ Ullah, F.; Thaheem, M.J.; Gabriel, H.F. Six Sigma in construction: A review\
    \ of critical success factors.\nInt. J. Lean Six Sigma 2016, 7, 171–186. [CrossRef]\n\
    91.\nUllah, F.; Thaheem, M.J.; Siddiqui, S.Q.; Khurshid, M.B. Inﬂuence of Six\
    \ Sigma on project success in construction industry of\nPakistan. TQM J. 2017,\
    \ 29, 276–309. [CrossRef]\n92.\nShirowzhan, S.; Lim, S. Autocorrelation statistics-based\
    \ algorithms for automatic ground and non-ground classiﬁcation of Lidar\ndata.\
    \ In Proceedings of the ISARC, International Symposium on Automation and Robotics\
    \ in Construction, Sydney, Australia,\n9–11 July 2014; p. 1.\n93.\nSepasgozar,\
    \ S.M.; Karimi, R.; Shirowzhan, S.; Mojtahedi, M.; Ebrahimzadeh, S.; McCarthy,\
    \ D. Delay causes and emerging digital\ntools: A novel model of delay analysis,\
    \ including integrated project delivery and PMBOK. Buildings 2019, 9, 191. [CrossRef]\n\
    94.\nDoloi, H.; Sawhney, A.; Iyer, K. Structural equation model for investigating\
    \ factors affecting delay in Indian construction projects.\nConstr. Manag. Econ.\
    \ 2012, 30, 869–884. [CrossRef]\n95.\nBaker, H.R.; Smith, S.D.; Masterton, G.;\
    \ Hewlett, B. Failures in construction: Learning from everyday forensic engineering.\
    \ In\nForensic Engineering 2018: Forging Forensic Frontiers; American Society\
    \ of Civil Engineers: Reston, VA, USA, 2018; pp. 648–658.\nBig Data Cogn. Comput.\
    \ 2022, 6, 18\n26 of 27\n96.\nAlipour, M.; Harris, D.K.; Barnes, L.E.; Ozbulut,\
    \ O.E.; Carroll, J. Load-capacity rating of bridge populations through machine\n\
    learning: Application of decision trees and random forests. J. Bridge Eng. 2017,\
    \ 22, 04017076. [CrossRef]\n97.\nLu, W.; Chen, X.; Peng, Y.; Shen, L. Benchmarking\
    \ construction waste management performance using big data.\nResour. Conserv.\
    \ Recycl. 2015, 105, 49–58. [CrossRef]\n98.\nSun, H.; Wang, L.; Yang, Z.; Xie,\
    \ J. Research on Construction Engineering Quality Management Based on Building\
    \ Information\nModel and Computer Big Data Mining. Arab. J. Sci. Eng. 2021, 1–11.\
    \ [CrossRef]\n99.\nMoon, S.; Kim, J.; Kwon, K. Effectiveness of OLAP-based cost\
    \ data management in construction cost estimate. Autom. Constr.\n2007, 16, 336–344.\
    \ [CrossRef]\n100. Carrillo, P.; Harding, J.; Choudhary, A. Knowledge discovery\
    \ from post-project reviews. Constr. Manag. Econ. 2011, 29, 713–723.\n[CrossRef]\n\
    101. Williams, T.P. Predicting ﬁnal cost for competitively bid construction projects\
    \ using regression models. Int. J. Proj. Manag. 2003,\n21, 593–599. [CrossRef]\n\
    102. Polat, G.; Bingol, B.N. A comparison of fuzzy logic and multiple regression\
    \ analysis models in determining contingency in\ninternational construction projects.\
    \ Constr. Innov. 2013, 13, 445–462. [CrossRef]\n103. Hoffman, G.J.; Thal, A.E.,\
    \ Jr.; Webb, T.S.; Weir, J.D. Estimating performance time for construction projects.\
    \ J. Manag. Eng. 2007, 23,\n193–199. [CrossRef]\n104. Bukowski, L. Reliable, Secure\
    \ and Resilient Logistics Networks; Springer: Cham, Switzerland, 2019.\n105. Konikov,\
    \ A.; Konikov, G. Big Data is a powerful tool for environmental improvements in\
    \ the construction business. In IOP\nConference Series: Earth and Environmental\
    \ Science; IOP Publishing: Bristol, UK, 2017; p. 012184.\n106. Williamson, B.\
    \ The hidden architecture of higher education: Building a big data infrastructure\
    \ for the ‘smarter university’. Int. J.\nEduc. Technol. High. Educ. 2018, 15,\
    \ 1–26. [CrossRef]\n107. Gbadamosi, A.-Q.; Oyedele, L.; Mahamadu, A.-M.; Kusimo,\
    \ H.; Bilal, M.; Delgado, J.M.D.; Muhammed-Yakubu, N. Big data for\nDesign Options\
    \ Repository: Towards a DFMA approach for offsite construction. Autom. Constr.\
    \ 2020, 120, 103388. [CrossRef]\n108. Ajayi, A.; Oyedele, L.; Akinade, O.; Bilal,\
    \ M.; Owolabi, H.; Akanbi, L.; Delgado, J.M.D. Optimised big data analytics for\
    \ health\nand safety hazards prediction in power infrastructure operations. Saf.\
    \ Sci. 2020, 125, 104656. [CrossRef]\n109. Valpeters, M.; Kireev, I.; Ivanov,\
    \ N. Application of machine learning methods in big data analytics at management\
    \ of contracts\nin the construction industry. In Proceedings of the MATEC Web\
    \ of Conferences, St. Petersburg, Russia, 20–22 December 2017;\np. 01106.\n110.\
    \ Braun, A.; Borrmann, A. Combining inverse photogrammetry and BIM for automated\
    \ labeling of construction site images for\nmachine learning. Autom. Constr. 2019,\
    \ 106, 102879. [CrossRef]\n111. Huang, M.; Nini´c, J.; Zhang, Q. BIM, machine\
    \ learning and computer vision techniques in underground construction: Current\
    \ sta-\ntus and future perspectives. Tunn. Undergr. Space Technol. 2021, 108,\
    \ 103677. [CrossRef]\n112. Cheng, J.C.; Chen, W.; Chen, K.; Wang, Q. Data-driven\
    \ predictive maintenance planning framework for MEP components based\non BIM and\
    \ IoT using machine learning algorithms. Autom. Constr. 2020, 112, 103087. [CrossRef]\n\
    113. Bloch, T.; Sacks, R. Comparing machine learning and rule-based inferencing\
    \ for semantic enrichment of BIM models.\nAutom. Constr. 2018, 91, 256–272. [CrossRef]\n\
    114. Munawar, H.S.; Hammad, A.W.; Waller, S.T. A review on ﬂood management technologies\
    \ related to image processing and\nmachine learning. Autom. Constr. 2021, 132,\
    \ 103916. [CrossRef]\n115. Munawar, H.S.; Hammad, A.; Ullah, F.; Ali, T.H. After\
    \ the ﬂood: A novel application of image processing and machine learning\nfor\
    \ post-ﬂood disaster management. In Proceedings of the 2nd International Conference\
    \ on Sustainable Development in Civil\nEngineering (ICSDC 2019), Jamshoro, Pakistan,\
    \ 5–7 December 2019; pp. 5–7.\n116. Qureshi, A.H.; Alaloul, W.S.; Manzoor, B.;\
    \ Musarat, M.A.; Saad, S.; Ammad, S. Implications of machine learning integrated\n\
    technologies for construction progress detection under industry 4.0 (IR 4.0).\
    \ In Proceedings of the 2020 Second International\nSustainability and Resilience\
    \ Conference: Technology and Innovation in Building Designs (51154), Sakheer,\
    \ Bahrain, 11–12\nNovember 2020; pp. 1–6.\n117. Rozemberczki, B.; Scherer, P.;\
    \ He, Y.; Panagopoulos, G.; Riedel, A.; Astefanoaei, M.; Kiss, O.; Beres, F.;\
    \ López, G.; Collignon, N.\nPytorch geometric temporal: Spatiotemporal signal\
    \ processing with neural machine learning models. In Proceedings of the\n30th\
    \ ACM International Conference on Information & Knowledge Management, Gold Coast,\
    \ Australia, 1–5 November 2021;\npp. 4564–4573.\n118. Eluri, V.R.; Ramesh, M.;\
    \ Al-Jabri, A.S.M.; Jane, M. A comparative study of various clustering techniques\
    \ on big data sets\nusing Apache Mahout. In Proceedings of the 2016 3rd MEC International\
    \ Conference on Big Data and Smart City (ICBDSC),\nMuscat, Oman, 15–16 March 2016;\
    \ pp. 1–4.\n119. Solanki, R.; Ravilla, S.H.; Bein, D. Study of distributed framework\
    \ hadoop and overview of machine learning using apache\nmahout. In Proceedings\
    \ of the 2019 IEEE 9th Annual Computing and Communication Workshop and Conference\
    \ (CCWC),\nLas Vegas, NV, USA, 7–9 January 2019; pp. 0252–0257.\n120. Sonnenburg,\
    \ S.; Rätsch, G.; Henschel, S.; Widmer, C.; Behr, J.; Zien, A.; Bona, F.d.; Binder,\
    \ A.; Gehl, C.; Franc, V. The SHOGUN\nmachine learning toolbox. J. Mach. Learn.\
    \ Res. 2010, 11, 1799–1802.\n121. Jain, A. Scikit-learn: Machine learning in Python.\
    \ J. Mach. Learn. Res. 2011, 12, 2825–2830.\nBig Data Cogn. Comput. 2022, 6, 18\n\
    27 of 27\n122. Majumder, G.; Jain, R. A Comparative Study and Analysis of Classiﬁcation\
    \ Methods in Machine Learning. Think India J. 2019, 22,\n709–718.\n123. Liu, H.;\
    \ Lang, B. Machine learning and deep learning methods for intrusion detection\
    \ systems: A survey. Appl. Sci. 2019, 9, 4396.\n[CrossRef]\n"
  inline_citation: '>'
  journal: Big data and cognitive computing
  limitations: '>'
  pdf_link: https://www.mdpi.com/2504-2289/6/1/18/pdf?version=1644300250
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'Big Data in Construction: Current Applications and Future Opportunities'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
