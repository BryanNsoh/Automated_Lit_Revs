- analysis: '>'
  authors:
  - Li W.
  - Zhang Z.
  - Xie B.
  - He Y.
  - He K.
  - Qiu H.
  - Lu Z.
  - Jiang C.
  - Pan X.
  - He Y.
  - Hu W.
  - Liu W.
  - Que T.
  - Hu Y.
  citation_count: '0'
  description: Analyzing the vast amount of omics data generated comprehensively by
    high-throughput sequencing technology is of utmost importance for scientists.
    In this context, we propose HiOmics, a cloud-based platform equipped with nearly
    300 plugins designed for the comprehensive analysis and visualization of omics
    data. HiOmics utilizes the Element Plus framework to craft a user-friendly interface
    and harnesses Docker container technology to ensure the reliability and reproducibility
    of data analysis results. Furthermore, HiOmics employs the Workflow Description
    Language and Cromwell engine to construct workflows, ensuring the portability
    of data analysis and simplifying the examination of intricate data. Additionally,
    HiOmics has developed DataCheck, a tool based on Golang, which verifies and converts
    data formats. Finally, by leveraging the object storage technology and batch computing
    capabilities of public cloud platforms, HiOmics enables the storage and processing
    of large-scale data while maintaining resource independence among users.
  doi: 10.1016/j.csbj.2024.01.002
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract BetaPowered by GenAIQuestions answered in this article Keywords
    1. Introduction 2. Methods/implementation 3. Results 4. Discussion 5. Conclusion
    Funding CRediT authorship contribution statement Declaration of Competing interest
    Availability References Show full outline Figures (7) Show 1 more figure Tables
    (2) Table 1 Table 2 Computational and Structural Biotechnology Journal Volume
    23, December 2024, Pages 659-668 Software/web server article HiOmics: A cloud-based
    one-stop platform for the comprehensive analysis of large-scale omics data Author
    links open overlay panel Wen Li a b c 1, Zhining Zhang d 1, Bo Xie a 1, Yunlin
    He d, Kangming He d, Hong Qiu a d, Zhiwei Lu d, Chunlan Jiang d, Xuanyu Pan e,
    Yuxiao He a, Wenyu Hu d, Wenjian Liu f, Tengcheng Que f g h, Yanling Hu a b c
    d f Show more Share Cite https://doi.org/10.1016/j.csbj.2024.01.002 Get rights
    and content Under a Creative Commons license open access Abstract Analyzing the
    vast amount of omics data generated comprehensively by high-throughput sequencing
    technology is of utmost importance for scientists. In this context, we propose
    HiOmics, a cloud-based platform equipped with nearly 300 plugins designed for
    the comprehensive analysis and visualization of omics data. HiOmics utilizes the
    Element Plus framework to craft a user-friendly interface and harnesses Docker
    container technology to ensure the reliability and reproducibility of data analysis
    results. Furthermore, HiOmics employs the Workflow Description Language and Cromwell
    engine to construct workflows, ensuring the portability of data analysis and simplifying
    the examination of intricate data. Additionally, HiOmics has developed DataCheck,
    a tool based on Golang, which verifies and converts data formats. Finally, by
    leveraging the object storage technology and batch computing capabilities of public
    cloud platforms, HiOmics enables the storage and processing of large-scale data
    while maintaining resource independence among users. Previous article in issue
    Next article in issue Questions answered in this article BetaPowered by GenAI
    This is generative AI content and the quality may vary. Learn more. How does HiOmics
    address the challenge of data storage for high-throughput sequencing data? Why
    is ensuring the reproducibility of data analysis results important in bioinformatics?
    How does HiOmics streamline comprehensive data mining? What is HiOmics? What is
    the focus of HiOmics in terms of personalized analysis plugins? Keywords Artificial
    intelligence modelingCloud-based analysisHiOmicsLarge-scale omics data analysisMulti-omics
    integration analysis 1. Introduction The advent of high-throughput sequencing
    technology has led to an exponential growth in omics data, encompassing transcriptomics,
    genomics, and metabolomics data [1], [2], [3], [4]. The generation of large-scale
    omics data offers scientists valuable opportunities to gain comprehensive and
    profound insights into life processes [5], [6], [7]. However, it also presents
    remarkable challenges, demanding more intricate analysis methods/workflows and
    substantial computational resources. Although programming software, such as Python
    and R, offer advantages in data statistics and visualization, users must possess
    specific computer knowledge, including proficiency in at least one programming
    language and familiarity with the Linux operating system [8]. Traditional desktop
    software [9], [10], [11], [12] can only implement one or a few functions within
    the entire analysis process, and different applications often have distinct requirements
    for input and output data formats. Furthermore, installing and configuring these
    desktop applications can pose a considerable challenge for users. In recent years,
    numerous web-based bioinformatics tools have emerged [13], [14]. These tools enable
    researchers to conduct analyses seamlessly, eliminating the need for mastering
    intricate programming skills or configuring complex analysis environments through
    interactive graphical interfaces. ImageGP [15] is a specialized visualization
    platform tailored for generating graphics related to biological and medical data.
    This platform offers a wide array of scientific graphic and analysis sub-functions.
    START App [16] and CANEapp [17] provide transcriptomics data analysis and visualization
    services from read count data. MetaboAnalyst [18] specializes in the analysis
    and visualization of metabolomics data. IMG/M [19] focuses on metagenomics data
    analysis and visualization. Additionally, Metascape [20] provides functional enrichment,
    interaction component analysis, and gene annotation functions, catering to experimental
    biologists by offering comprehensive resources for gene list annotation and analysis.
    Although these individual web tools boast robust features, leveraging multiple
    platforms comprehensively is essential to formulating a complete system-level
    analysis workflow. Since its launch in 2010, Galaxy [21] has paved the way for
    numerous cloud-based web tools that now play a pivotal role in high-throughput
    data analysis. Sangerbox [22] boasts powerful interactive plotting capabilities,
    allowing users to intuitively adjust parameters within the interface. However,
    its bioinformatics tools remain limited. Conversely, Hiplot [23] offers a wider
    array of interactive visualization tools, primarily tailored for lightweight data
    processing and plotting requirements. Qiita [24] stands out for its extensive
    collection of community resources, but its analysis functions are primarily focused
    on microbiome data. DolphinNext [25] empowers users to construct intricate data
    analysis workflows effortlessly using drag-and-drop operations, although its advanced
    analysis tools are somewhat limited in scope. Although Galaxy integrates numerous
    upstream and downstream analysis tools and facilitates customization of complex
    workflows through drag-and-drop functionality, non-bioinformatics users face a
    learning curve to familiarize themselves with the various tools and workflows
    [21]. Furthermore, issues, such as a non-user-friendly interface and frequent
    errors in input data formatting, significantly affect the overall user experience.
    To address these challenges, we have developed HiOmics, a dedicated cloud-based
    web platform (https://www.henbio.com/en/tools) designed for the comprehensive
    analysis and visualization of multi-omics data. HiOmics utilizes the Element Plus
    framework [26], not only presenting an appealing and user-friendly interface but
    also incorporating interactive features to elevate the overall user experience.
    For consistent and reliable analysis results across different environments, HiOmics
    relies on Docker container technology [27], packaging the application and its
    dependencies into an independent container. Workflow Description Language (WDL)
    [28] streamlines the management and construction of data analysis workflows, enhancing
    efficiency and reproducibility. To cater to users without programming backgrounds,
    HiOmics seamlessly integrates typical bioinformatics pipelines. Additionally,
    we have developed DataCheck, a tool built on Golang [29], to automatically verify
    and convert input data formats, making it adaptable to various input types. The
    adoption of object storage and batch processing technologies on public cloud platforms
    empowers HiOmics to manage large-scale data storage and processing. This infrastructure
    ensures relative resource independence among users, facilitating efficient data
    management and processing. 2. Methods/implementation 2.1. Building user-friendly
    web interfaces with Elements Plus HiOmics employs the UI library Element Plus
    [26], which is based on Vue.js 3, to create a sleek and user-friendly interface.
    Backend data communication is facilitated by utilizing the ThinkPHP framework
    and the MySQL database. All HiOmics applications are developed using open-source
    software like R and Python. They are abstracted as independent, parameterized,
    pluggable, and easily version-controlled plugins. With a simple mouse-click, users
    can seamlessly perform a wide array of tasks, ranging from basic visualization
    to complex omics analysis, without the need for any coding or programming expertise.
    All functionalities are accessible via the web interface provided by HiOmics.
    2.2. Achieving reproducibility with container technology In the field of bioinformatics,
    ensuring the reproducibility of data analysis results is of paramount importance
    [30], [31]. To address this issue, HiOmics employs Docker container technology
    [32], [33] to package each software tool, alongside its dependencies, analysis
    scripts, and runtime environment, into a singular, independent container image.
    Subsequently, these images are uploaded to a unified container image repository.
    Each software tool corresponds to an individual Docker image, and collectively,
    multiple images form a comprehensive data analysis workflow. This approach facilitates
    the convenience of building once and running anywhere, circumventing redundant
    software and environment setups. It guarantees consistency in software, versioning,
    and runtime environments across diverse servers and operating systems (Fig. 1A).
    Ultimately, this strategy ensures the reproducibility and reliability of data
    analysis results. Download : Download high-res image (461KB) Download : Download
    full-size image Fig. 1. Overview of the core advantages offered by the cloud-based
    HiOmics platform, enhancing development efficiency and user experience. (A) Leveraging
    Docker container technology to ensure the reliability and reproducibility of data
    analysis results. (B) HiOmics utilizes WDL + Cromwell to construct and manage
    intricate analysis workflows, integrating typical bioinformatics analysis processes.
    This diminishes the learning curve and exploration costs for users, meeting the
    deep data mining needs of non-programming users, such as clinical physicians.
    (C) Command-line tool DataCheck, based on Golang, automatically identifies and
    converts user-uploaded data into standard formats, thereby enhancing the user
    experience. (D) Utilization of object storage and batch computing technologies
    in public cloud platforms facilitates large-scale data storage and processing.
    Additionally, it ensures relative resource independence among different users.
    2.3. Building standard workflows with WDL Bioinformatics analysis tasks typically
    involve multiple steps and necessitate the combined use of various software and
    scripts, such as shell, Python, Perl, and command-line software. The utilization
    of WDL [28] enables the automation of bioinformatics analysis workflows. This
    automation standardizes and unifies the inputs, outputs, and running environments
    for each step, thereby greatly improving the reproducibility and reliability of
    the analysis. HiOmics employs WDL + Cromwell [34] to construct and manage workflows,
    providing a web-based interface for task submission (Fig. 1B). Currently, HiOmics
    has launched typical omics analysis workflows, including transcriptomics, metagenomics,
    and whole-genome sequencing. Additionally, it has integrated fundamental data,
    such as reference genomes and indexes for commonly studied species. Through the
    web-based interface, users can easily complete complex omics analysis tasks by
    uploading data files, and specifying a few parameters. 2.4. Achieving diverse
    input data formats with DataCheck Many data analysis software applications impose
    strict requirements on the input data format, often mandating data to be in comma-separated
    CSV files. Uploading a table-type XLSX file, for instance, would trigger an error.
    To circumvent such issues and minimize user inconvenience regarding format conversion,
    we have developed a command-line tool called DataCheck using Golang [29] (Fig.
    1C). This tool is integrated into the analysis script and automatically validates
    data before executing specific analysis tasks. It can automatically identify the
    type, encoding format, and delimiter of user-uploaded files and then convert them
    into the standard format required by the plugin. Users no longer need to concern
    themselves with whether they should upload a text file or a spreadsheet file.
    Additionally, this tool conducts automatic data validation, detecting common data
    errors, such as non-numeric values, irregularly formatted matrices, inconsistent
    row or column names, as well as duplicate rows and columns. It alerts users through
    log messages, empowering them to perform targeted checks and rectify the data
    accordingly. 2.5. Implementing large-scale data storage and processing with cloud
    platform architecture The rapid advancement of sequencing technology, coupled
    with decreasing costs, has resulted in a swift increase in high-throughput sequencing
    data [35], [36], [37]. Individual data files often reach gigabyte levels, thereby
    posing unprecedented challenges in data storage and computation [38], [39]. To
    counter this challenge, HiOmics embraces public cloud object storage technology
    (Alibaba OSS, similar to Amazon S3), offering a scalable storage solution for
    large-scale high-throughput sequencing data. This approach bypasses local storage
    capacity limitations on a single server and effortlessly expands storage in tandem
    with user growth (currently, individual users possess a default permanent storage
    space of 5 GB, extendable as required). Simultaneously, HiOmics utilizes public
    cloud batch computing (Alibaba Cloud Batch Compute, similar to Amazon Batch) as
    the core computing resource. This system supports the parallel processing of numerous
    jobs, automatically manages resources, schedules tasks, and loads data based on
    task requirements (Fig. 1D). Therefore, resource utilization among HiOmics users
    remains relatively independent, averting resource contention and impact. This
    setup enhances task processing speed and eliminates the necessity to restrict
    the number of submitted tasks. 3. Results 3.1. Simple and user-friendly web page
    HiOmics, a user-friendly web platform, does not require downloading or installing
    any software. The main interface features a left navigation bar and a right plugin
    bar, enabling users to swiftly locate the desired plugin based on their requirements
    (Fig. 2A). Moreover, users can use the search bar positioned at the top left corner
    of the page to find plugins quickly by entering keywords. Each plugin page adopts
    a two-column layout, as depicted in Fig. 2B. On the left side, a section is dedicated
    for data uploading and parameter configuration. Users can simply click the \"Select
    File\" option to choose files from the Cloud storage file manager or upload directly
    from their local device. To minimize input errors, most basic parameters are presented
    as dropdown lists for easy selection. The right side of the page contains a documentation
    and sample file toolbar, offering users a comprehensive understanding of the functionality
    and effective usage of the plugin. With only three simple steps—uploading data,
    configuring parameters, and submitting the task—users can effortlessly obtain
    the desired results without writing any code. Download : Download high-res image
    (740KB) Download : Download full-size image Fig. 2. HiOmics offers a user-friendly
    interface. (A) Fig. A showcases the main interface of the HiOmics Artificial intelligence
    (AI) Modeling module. Users can swiftly locate the desired tools using the search
    bar at the top or the navigation menu on the left. (B) Fig. B illustrates the
    interface of the \"Logistic Regression\" plugin within Artificial Intelligence
    module. The parameter panel is located on the left, while the explanation panel
    is situated on the right. 3.2. Numerous analysis and visualization tools Since
    2022, HiOmics has developed and launched over 290 interactive data analysis and
    visualization tools, all conveniently accessible through a user-friendly web interface.
    These tools encompass a comprehensive range of classic software and methods, covering
    the entire process from upstream sequence processing to downstream data visualization
    (Table 1). Table 1. Comparison of Web Services between Henbio and other Cloud-based
    Platforms. Cloud-based platforms HiOmics Hiplot ImageGP Galaxy number of analysis
    tools 290 + 330 + 34 1600 + Data file processing 8 small no full Sequence file
    processing 43 no small full Data preprocessing 14 no no full Data statistical
    analysis 35 small no small Basic plotting 60 full small moderate Advanced analysis
    28 full small moderate Integrative omics analysis 13 no no no Cloud-based analysis
    Workflow 49 no no constructed by users Artificial intelligence modeling 42 no
    no no Pathogen detection 7 no no small Biomedical Databases 726 no no small To
    accommodate article length constraints, this paper primarily focuses on introducing
    a few select functional plugins. These plugins are based on demo datasets, representing
    various feature categories and offering readers a fundamental understanding of
    their usage on this platform. 3.2.1. Use Case 1: Basic plotting module The basic
    plotting module offers 61 versatile plotting tools, including heatmap, Violin
    plot, volcano plot, Sankey diagram, Manhattan plot, Scatter plots, and box plot
    (Fig. 3A). These tools cater to diverse visualization needs, ensuring that the
    resulting images meet high-quality standards suitable for publication. Download
    : Download high-res image (729KB) Download : Download full-size image Fig. 3.
    HiOmics offers a variety of high-throughput data analysis and visualization tools
    to meet common requirements. (A) Partial visualization results from the Basic
    Plotting Module. (B) Partial visualization results from the Advanced Analysis
    Module. Next, let us take a heatmap as an example to introduce the usage of the
    basic plotting tool. Upon entering the heatmap plugin interface, you can easily
    select the gene expression matrix and group files from your local uploads or cloud
    storage file manager. After setting the parameters, simply click the \"Run\" button
    to generate the heatmap. Parameters for the heatmap include options for color
    scheme, row width, column height and normalization direction. Additional settings
    allow displaying specific genes of interest, clustering, row and column names,
    legends, and numerical values. Upon clicking the \"Submit Task\" button in the
    interface, the platform will automatically deliver the plotting task to Cromwell
    for scheduling and execution through the API interface. Once the task is completed,
    you can preview or download the heatmap from the result files. The heatmap can
    be saved in PDF, PNG, or SVG formats, and you may also choose to store it in the
    cloud storage file manager. 3.2.2. Use case 2: advanced analysis module The advanced
    analysis module includes 29 plugins, covering various functionalities, such as
    prognosis model construction, immune infiltration analysis, Mendelian randomization,
    GO enrichment analysis, co-expression analysis, and phylogenetic tree analysis
    (Fig. 3B). In this work, we utilize TCGA''s bladder urothelial carcinoma dataset
    [40] to demonstrate the functionality of the \"Each-sample Immune Infiltration
    Analysis\" plugin. Users accessing the plugin webpage are required to input the
    gene expression matrix and grouping files, and then set the relevant parameters
    before initiating the analysis. The generated output comprises enrichment scores
    and expression heatmaps representing various cell types across different samples,
    along with boxplots displaying scores of different cells across distinct groups
    (with and without p-values), as depicted in Fig. 4. Download : Download high-res
    image (963KB) Download : Download full-size image Fig. 4. Representative use case
    of advanced analysis module. (A) Screenshot of the “Each-sample Immune Infiltration
    Analysis” plugin. (B) and (C) Display the input files, including the gene expression
    matrix file and the grouping file. (D) Expression heatmaps of various cell types
    across different samples, with colors indicating different sample groups, where
    blue represents the high-risk group and red represents the low-risk group. (E)
    File summarizing enrichment scores of various cells across different samples.
    (F) Boxplots comparing cell enrichment scores between high-risk and low-risk groups,
    with cell types labeled below and significance levels indicated above (* for P < 0.05,
    ** for P < 0.01, *** for P < 0.001). 3.2.3. Use case 3: multi-omics integration
    analysis module Beyond single omics analysis, HiOmics prioritizes the integration
    of multiple omics data. Presently, the Integration Omics Analysis module provides
    13 distinct plugins tailored for joint multi-omics analysis. Here, the NCI-60
    cancer cell line metabolomics and gene expression data [41], [42] were retrieved
    to demonstrate the functionality of the \"Integration Analysis of Transcriptomics
    and Metabolomics Data\" plugin, depicted in Fig. 5. To initiate the process, start
    by clicking on \"Select File\" to input the metabolite expression (abundance)
    matrix, transcriptome expression matrix, and the sample phenotype file, which
    contains outcome data associated with each sample. Next, manually enter or select
    from a dropdown menu the column name in the sample phenotype file that necessitates
    calculation, the data type of this column (discrete or continuous), and specify
    the names of the metabolites and genes to be plotted on the correlation graph.
    Once configured, click \"Run\" to submit the data for background processing. The
    generated output comprises five graphs and a table, as illustrated in Fig. 5.
    Download : Download high-res image (931KB) Download : Download full-size image
    Fig. 5. Exemplification of the integrative omics analysis module’s utility. (A)
    Screenshot of the “Integration Analysis of Transcriptomics and Metabolomics Data”
    plugin. (B) Results of transcriptome and metabolome correlation analysis are displayed
    in the excel table. (C) Summary statistics boxplot of filtered metabolites and
    genes. (D) Association between metabolites and genes of interest. (E) Principal
    Component Analysis (PCA) plots of filtered metabolites and genes. 3.2.4. Use Case
    4: Artificial intelligence (AI) modeling module 3.2.4.1. Machine learning models
    Numerous biological problems can be reformulated as data analysis and pattern
    recognition problems. The AI modeling module of HiOmics has incorporated interactive
    plugins for 32 widely used machine learning algorithms, encompassing artificial
    neural networks, support vector machines, random forests, and AdaBoost. These
    plugins enable users to effectively manage and analyze substantial volumes of
    biomedical data. To showcase the functionality of the AI modeling tools of HiOmics,
    we chose a publicly available heart dataset [43], utilizing twelve binary classification
    machine learning algorithms to assess their performance within the HiOmics data
    analysis framework. Notably, our objective was to showcase the application of
    various machine learning plugins, rather than aiming for superior predictive performance
    compared to state-of-the-art methods. The algorithm parameters were set as follows:
    a test set split ratio of 0.2, non-shuffled data rows, utilization of fivefold
    cross-validation, and no custom model parameters were defined. Fig. 6A depicts
    a simplified machine learning workflow. The performance metrics, including accuracy,
    precision, sensitivity, and specificity for different algorithms, are presented
    in Table 2 and Fig. 6B, while Fig. 6C illustrates the ROC curves for nine out
    of the twelve algorithms. Download : Download high-res image (584KB) Download
    : Download full-size image Fig. 6. Overview of Artificial Intelligence Modules.
    (A) This figure illustrates the machine learning models supported by the HiOmics
    platform and the modeling process. (B) Performance comparison of 12 different
    binary classification machine learning algorithms in terms of accuracy, precision,
    sensitivity, and specificity. (C) The ROC curves and AUC values for nine of these
    machine learning models. (D) This figure demonstrates the construction and evaluation
    process of clinical prognosis models on the HiOmics platform. Table 2. Performance
    Comparison of Different Binary Classification Machine Learning Models. APP Name
    Accuracy Precision Sensitivity Specificity F1 k-Nearest Neighbors (KNN) 0.852
    0.813 0.907 0.800 0.857 Artificial Neural Network (ANN) 0.852 0.854 0.953 0.611
    0.901 Gaussian Naive Bayes (GNB) 0.864 0.833 0.909 0.818 0.870 Logistic Regression
    (LR) 0.864 0.857 0.857 0.870 0.857 Decision Tree (DT) 0.864 0.786 0.917 0.827
    0.846 Support Vector Machine (SVM) 0.869 0.864 0.950 0.714 0.905 Random Forest
    (RF) 0.875 0.786 0.943 0.830 0.857 AdaBoost 0.875 0.881 0.860 0.889 0.871 Linear
    Discriminant Analysis (LDA) 0.898 0.833 0.946 0.863 0.886 Gradient Boosting Decision
    Tree (GBDT) 0.909 0.881 0.925 0.896 0.902 Light GBM 0.920 0.857 0.973 0.882 0.911
    Binary Stacking (BS) 0.955 0.905 1.000 0.920 0.950 3.2.4.2. Clinical prognostic
    models The Prognosis Model module offers clinical doctors a variety of visual
    plugins, such as Lasso Regression and Cox Regression Model, to investigate the
    factors influencing different disease outcomes. These plugins aid in assessing
    the probability of individual outcome events by utilizing multiple predictive
    factors. Fig. 6D provides a simplified example illustrating the functionality
    of this module. Initially, the \"Univariate COX Model\" plugin is employed to
    identify substantial survival-related factors. Subsequently, statistically significant
    factors are chosen for analysis using the \". Multivariable COX Model\" plugin,
    generating forest plots and column line graphs. Moreover, the \"Independent Prognostic
    Analysis\" plugin evaluates whether specific factors are independent of other
    clinical characteristics, serving as independent prognostic factors. The \" ROC
    Curve Analysis \" plugin assesses the predictive accuracy of various clinical
    factors and risk scores for survival time. Additionally, the \"Survival Curve
    and Risk Curve\" plugin evaluates the accuracy of the prognosis prediction model
    while identifying substantial differences in overall survival time between high-
    and low-risk groups classified by the model. 3.3. Convenient workflow HiOmics
    has developed a collection of over 10 bioinformatics data processing workflows
    (Fig. 7), encompassing transcriptome analysis, metagenomic analysis, and genetic
    variation analysis, all built upon the standardized workflow language, WDL. These
    workflows address users'' essential requirements for executing complex data analysis.
    For example, the microbiome amplicon analysis workflow integrates vital software
    and databases for various steps, including paired-end sequence merging, barcode
    and primer removal, quality control, denoising for acquiring ASV representative
    sequences, taxonomic annotation, diversity analysis, functional prediction, and
    differential analysis using LEfSe [44], [45]. All these steps are conveniently
    presented on a single page, allowing users to seamlessly navigate through the
    workflow and obtain desired results effortlessly. These workflows substantially
    streamline comprehensive data mining, particularly benefiting users without scripting
    language or workflow construction expertise. Download : Download high-res image
    (301KB) Download : Download full-size image Fig. 7. All workflows of HiOmics.
    4. Discussion After the completion of the Human Genome Project, scientists expanded
    their focus from the genome to other ''omics'' data types, —such as the transcriptome
    for gene expression, the epigenome for epigenetic markers, the proteome for protein
    production, and the metabolome for metabolic functional products. Although single
    omics data analysis usually involves correlation [46], researchers are increasingly
    collecting and analyzing multiple omics datasets simultaneously to attain a more
    comprehensive understanding. Computer science plays a critical role in handling
    large-scale multi-omics data. Desktop applications, programming languages, and
    server-based network tools are frequently utilized in bioinformatics but have
    inherent limitations. Some tools lack comprehensive data analysis capabilities,
    while others demand users to possess computer knowledge and familiarity with programming
    languages and Linux. Additionally, physical server constraints hinder large-scale
    data processing. Fortunately, cloud-based bioinformatics data analysis websites
    have emerged to counter these challenges. RAP [47] is a web-based tool tailored
    specifically for RNA-seq analysis, offering standard and customized workflows.
    Hiplot [23] excels in interactive analysis and visualization of lightweight single-omics
    data. Galaxy [48] empowers users to construct and manage their data analysis workflows,
    albeit requiring proficiency in multiple tools. In this work, we introduce HiOmics,
    a comprehensive cloud-based bioinformatics data analysis platform leveraging state-of-the-art
    technologies, such as Docker containers, the WDL language, and the Cromwell engine.
    Tailored to meet the demands of multi-omics data processing and visualization,
    HiOmics boasts an extensive suite of nearly 300 data processing and visualization
    tools. These tools encompass diverse analysis requirements, featuring specialized
    toolsets for specific purposes and rigorously tested, reproducible analysis workflows
    utilizing multiple software packages. HiOmics caters to upstream analysis of raw
    sequencing data and downstream analysis, ensuring the delivery of high-quality
    visual outcomes. Its capabilities span from single omics analysis to intricate
    multi-omics integrative analysis, efficiently managing datasets of varying sizes,
    from small-scale to large-scale multi-sample data. Designed with a user-friendly
    graphical interface, HiOmics simplifies the processing of vast biomedical data,
    accommodating users without programming experience. With only three simple steps,
    researchers can effectively analyze and explore omics data, thereby eliminating
    barriers and empowering in-depth analysis to uncover new insights. For reliability,
    portability, and efficiency in bioinformatics data analysis workflows, HiOmics
    integrates WDL + Cromwell and Docker container technology. WDL + Cromwell offers
    robust workflow description and management capabilities, while Docker containers
    ensure independent and dependable workflow deployment and execution environments.
    This integration resolves compatibility issues across diverse environments, allowing
    for complex data analysis workflows while maintaining consistency and standardization.
    Additionally, it minimizes maintenance costs and workflow-related risks. To manage
    the growing volume of omics data, HiOmics incorporates public cloud object storage
    technology and batch computing techniques, ensuring users'' resource independence
    and enabling efficient processing of vast datasets. Furthermore, standardized
    data format validation enhances the user experience, improving data analysis reliability
    and effectiveness within the platform. Despite these advancements, HiOmics acknowledges
    the ongoing demand for personalized analysis plugins tailored to specific research
    fields. Hence, HiOmics is dedicated to developing additional plugins to cater
    to the diverse needs of its users. Concurrently, enhancing the speed and responsiveness
    of real-time interactions remains a focal point for HiOmics. Furthermore, recognizing
    the trend toward deep learning in biomedical data analysis [49], [50], [51], [52],
    [53], we aim to introduce more deep learning plugins, expanding our users'' options
    for artificial intelligence modeling. Through continuous efforts, we aim to enhance
    HiOmics, providing users with more efficient, flexible, and convenient data analysis
    and visualization capabilities. 5. Conclusion In general, HiOmics offers an integrated
    biomedical data analysis and visualization service, streamlining researcher workflows
    and enabling more convenient scientific research. Currently, users can freely
    access all of HiOmics functions by simply registering an account. However, for
    large data computations involving cloud computing and storage costs, we plan to
    introduce charges for this specific service in the future. This decision aims
    to cover the infrastructure costs associated with cloud service providers. Nevertheless,
    other tools and services will remain free to support a broader range of researchers
    in their studies. Funding This work is supported by the National Natural Science
    Foundation of China (82160537) and the Key Research and Development Program of
    Guangxi (2021AB12032). CRediT authorship contribution statement Wen Li: Writing
    – original draft. Zhining Zhang: Methodology, Supervision, Software. Bo Xie: Software,
    Validation. Yunlin He: Software. Kangming He: Software. Hong Qiu: Investigation,
    Resources. Zhiwei Lu: Visualization. Chunlan Jiang: Validation. Xuanyu Pan: Software.
    Yuxiao He: Validation. Wenyu Hu: Validation. Wenjian Liu: Writing – review & editing.
    Tengcheng Que: Writing – review & editing. Yanling Hu: Supervision, Project administration,
    Funding acquisition. Declaration of Competing interest The author declares no
    competing interests. Availability The website can be freely accessed at https://henbio.com/en/tools
    and https://henbio.com/tools. Furthermore, the available open-source code of the
    website is located at https://github.com/yongkangning/HiOmics. References [1]
    D. Pandey, P. Onkara Perumal A scoping review on deep learning for next-generation
    RNA-Seq. Data analysis Funct Integr Genom, 23 (2023), p. 134 View in ScopusGoogle
    Scholar [2] A. Sathyanarayanan, T.T. Mueller, M. Ali Moni, K. Schueler, B.T. Baune,
    et al. Multi-omics data integration methods and their applications in psychiatric
    disorders Eur Neuropsychopharmacol, 69 (2023), pp. 26-46 View PDFView articleView
    in ScopusGoogle Scholar [3] A. Sucre, M. Martinez, A. Garin-Muga OmicSDK-transcriptomics:
    a web platform for transcriptomics data analysis Stud Health Technol Inform, 302
    (2023), pp. 1042-1046 View in ScopusGoogle Scholar [4] M.L. Leite, L.S. de Loiola
    Costa, V.A. Cunha, V. Kreniski, M. de Oliveira Braga Filho, et al. , Artificial
    intelligence and the future of life sciences Drug Discov Today, 26 (2021), pp.
    2515-2526 View PDFView articleView in ScopusGoogle Scholar [5] M. Kang, E. Ko,
    T.B. Mersha A roadmap for multi-omics data integration using deep learning Brief
    Bioinform, 23 (2022) Google Scholar [6] S.V. Vasaikar, P. Straub, J. Wang, B.
    Zhang LinkedOmics: analyzing multi-omics data within and across 32 cancer types
    Nucleic Acids Res, 46 (2018), pp. D956-d963 CrossRefView in ScopusGoogle Scholar
    [7] Y. Qian, L. Li, Z. Sun, J. Liu, W. Yuan, et al. A multi-omics view of the
    complex mechanism of vascular calcification Biomed Pharmacother = Biomedecine
    Pharmacother, 135 (2021), Article 111192 View PDFView articleView in ScopusGoogle
    Scholar [8] W.S. Pittard, S. Li The essential toolbox of data science: python,
    R, git, and docker Methods Mol Biol (Clifton, N J ), 2104 (2020), pp. 265-311
    CrossRefView in ScopusGoogle Scholar [9] J.B. Procter, G.M. Carstairs, B. Soares,
    K. Mourão, T.C. Ofoegbu, et al. Alignment of biological sequences with jalview
    Mult Seq Alignment (2021), pp. 203-224 CrossRefView in ScopusGoogle Scholar [10]
    S. Kumar, G. Stecher, M. Li, C. Knyaz, K. Tamura MEGA X: molecular evolutionary
    genetics analysis across computing platforms Mol Biol Evol, 35 (2018), pp. 1547-1549
    View in ScopusGoogle Scholar [11] S. Purcell, B. Neale, K. Todd-Brown, L. Thomas,
    M.A. Ferreira, et al. PLINK: a tool set for whole-genome association and population-based
    linkage analyses Am J Hum Genet, 81 (2007), pp. 559-575 View PDFView articleCrossRefView
    in ScopusGoogle Scholar [12] P. Shannon, A. Markiel, O. Ozier, N.S. Baliga, J.T.
    Wang, et al. Cytoscape: a software environment for integrated models of biomolecular
    interaction networks Genome Res, 13 (2003), pp. 2498-2504 CrossRefView in ScopusGoogle
    Scholar [13] F. Kern, T. Fehlmann, A. Keller On the lifetime of bioinformatics
    web services Nucleic Acids Res, 48 (2020), pp. 12523-12533 CrossRefView in ScopusGoogle
    Scholar [14] Z. Zhang, H. Li, S. Jiang, R. Li, W. Li, et al. A survey and evaluation
    of Web-based tools/databases for variant analysis of TCGA data Brief Bioinform,
    20 (2019), pp. 1524-1541 CrossRefGoogle Scholar [15] T. Chen, Y.X. Liu, L. Huang
    ImageGP: An easy‐to‐use data visualization web server for scientific researchers
    iMeta, 1 (2022) Google Scholar [16] J.W. Nelson, J. Sklenar, A.P. Barnes, J. Minnier
    The START App: a web-based RNAseq analysis and visualization resource Bioinformatics,
    33 (2017), pp. 447-449 CrossRefView in ScopusGoogle Scholar [17] D. Velmeshev,
    P. Lally, M. Magistri, M.A. Faghihi CANEapp: a user-friendly application for automated
    next generation transcriptomic data analysis BMC Genom, 17 (2016) Google Scholar
    [18] J. Chong, O. Soufan, C. Li, I. Caraus, S. Li, et al. MetaboAnalyst 4.0: towards
    more transparent and integrative metabolomics analysis Nucleic Acids Res, 46 (2018),
    pp. W486-W494 CrossRefView in ScopusGoogle Scholar [19] I.A. Chen, K. Chu, K.
    Palaniappan, M. Pillay, A. Ratner, et al. IMG/M v.5.0: an integrated data management
    and comparative analysis system for microbial genomes and microbiomes Nucleic
    Acids Res, 47 (2019), pp. D666-D677 CrossRefView in ScopusGoogle Scholar [20]
    Y. Zhou, B. Zhou, L. Pache, M. Chang, A.H. Khodabakhshi, et al. Metascape provides
    a biologist-oriented resource for the analysis of systems-level datasets Nat Commun,
    10 (2019), p. 1523 View in ScopusGoogle Scholar [21] D. Blankenberg, G.V. Kuster,
    N. Coraor, G. Ananda, R. Lazarus, et al. Galaxy: a web‐based genome analysis tool
    for experimentalists Curr Protoc Mol Biol, 89 (2010) Google Scholar [22] Sangerbox.
    Available from: 〈http://sangerbox.com/〉, access date: June 20, 2023. Google Scholar
    [23] J. Li, B. Miao, S. Wang, W. Dong, H. Xu, et al. Hiplot: a comprehensive and
    easy-to-use web service for boosting publication-ready biomedical data visualization
    Brief Bioinforma (23 () (2022) Google Scholar [24] A. Gonzalez, J.A. Navas-Molina,
    T. Kosciolek, D. McDonald, Y. Vazquez-Baeza, et al. Qiita: rapid, web-enabled
    microbiome meta-analysis Nat Methods, 15 (2018), pp. 796-798 CrossRefView in ScopusGoogle
    Scholar [25] O. Yukselen, O. Turkyilmaz, A.R. Ozturk, M. Garber, A. Kucukural
    DolphinNext: a distributed data processing platform for high throughput genomics
    BMC Genom, 21 (2020), p. 310 View in ScopusGoogle Scholar [26] Element Plus: A
    Vue 3 UI framework. Available from: 〈https://element-plus.org/zh-CN/〉, Access
    date: June 20, 2023. Google Scholar [27] Docker: Accelerated, Containerized Application
    Development. Available from: 〈https://www.docker.com〉, Access date: June 20, 2023.
    Google Scholar [28] OpenWDL: Community Driven Open-development Workflow Language.
    Available from: 〈https://openwdl.org/〉, Access date: June 20, 2023. Google Scholar
    [29] The Go Programming Language. Available from: 〈https://golang.org/〉, Access
    date: June 20, 2023. Google Scholar [30] H. Suetake, T. Fukusato, T. Igarashi,
    T. Ohta A workflow reproducibility scale for automatic validation of biological
    interpretation results Gigascience, 12 (2022) Google Scholar [31] M. Baker 1,500
    scientists lift the lid on reproducibility Nature, 533 (2016), pp. 452-454 CrossRefView
    in ScopusGoogle Scholar [32] J. Matelsky, G. Kiar, E. Johnson, C. Rivera, M. Toma,
    et al. Container-based clinical solutions for portable and reproducible image
    analysis J Digit Imaging, 31 (2018), pp. 315-320 CrossRefView in ScopusGoogle
    Scholar [33] L. You, H. Sun Research and design of docker technology based authority
    management system Comput Intell Neurosci, 2022 (2022), p. 5325694 View in ScopusGoogle
    Scholar [34] Cromwell: A. Workflow Management System. Available from: 〈https://cromwell.readthedocs.io/en/stable/〉,
    access date: June 20, 2023. Google Scholar [35] Y. Cao, L. Li, M. Xu, Z. Feng,
    X. Sun, et al. The ChinaMAP analytics of deep whole genome sequences in 10,588
    individuals Cell Res, 30 (2020), pp. 717-731 CrossRefView in ScopusGoogle Scholar
    [36] Z. Tang, W. Fan, Q. Li, D. Wang, M. Wen, et al. MVIP: multi-omics portal
    of viral infection Nucleic Acids Res, 50 (2022), pp. D817-d827 CrossRefView in
    ScopusGoogle Scholar [37] A. Doricchi, C.M. Platnich, A. Gimpel, F. Horn, M. Earle,
    et al. Emerging approaches to DNA data storage: challenges and prospects ACS nano,
    16 (2022), pp. 17552-17571 CrossRefView in ScopusGoogle Scholar [38] E. Dotan,
    M. Alburquerque, E. Wygoda, D. Huchon, T. Pupko GenomeFLTR: filtering reads made
    easy Nucleic Acids Res (2023) Google Scholar [39] B. Langmead, A. Nellore Cloud
    computing for genomic data analysis and collaboration Nat Rev Genet, 19 (2018),
    p. 325 CrossRefView in ScopusGoogle Scholar [40] [dataset] TCGA Bladder Urothelial
    Carcinoma data, phs000178.v11.p8. Available from: 〈https://portal.gdc.cancer.gov/projects/TCGA-BLCA〉,
    Access date: June 20, 2023. Google Scholar [41] [dataset] Molecular Target Data
    - NCI DTP Data - NCI Wiki. Available from: 〈https://wiki.nci.nih.gov/display/ncidtpdata/molecular+target+data〉,
    Access date: June 20, 2023. Google Scholar [42] J.K. Siddiqui, E. Baskin, M. Liu,
    C.Z. Cantemir-Stone, B. Zhang, et al. IntLIM: integration using linear models
    of metabolomics and gene expression data BMC Bioinforma (19 () (2018) Google Scholar
    [43] [dataset] UCI Machine Learning Repository. Cleveland Heart Disease Database.
    Available from: 〈https://archive.ics.uci.edu/dataset/45/heart+disease〉, Access
    date: June 20, 2023. Google Scholar [44] Y.X. Liu, Y. Qin, T. Chen, M. Lu, X.
    Qian, et al. A practical guide to amplicon and metagenomic analysis of microbiome
    data Protein Cell, 12 (2021), pp. 315-330 CrossRefView in ScopusGoogle Scholar
    [45] Y.X. Liu, L. Chen, T. Ma, X. Li, M. Zheng, et al. EasyAmplicon: an easy‐to‐use,
    open‐source, reproducible, and community‐based pipeline for amplicon data analysis
    in microbiome research iMeta, 2 (2023) Google Scholar [46] M. Picard, M.P. Scott-Boyer,
    A. Bodein, O. Perin, A. Droit Integration strategies of multi-omics data for machine
    learning analysis Comput Struct Biotechnol J, 19 (2021), pp. 3735-3746 View PDFView
    articleView in ScopusGoogle Scholar [47] M. D''Antonio, P. D''Onorio De Meo, M.
    Pallocca, E. Picardi, A.M. D''Erchia, et al. RAP: RNA-Seq analysis pipeline, a
    new cloud-based NGS web application BMC Genom, 16 (2015), p. S3 CrossRefView in
    ScopusGoogle Scholar [48] A. Nekrutenko, J. Taylor, J. Goecks, D. Blankenberg,
    D. Clements, et al. The Galaxy platform for accessible, reproducible and collaborative
    biomedical analyses: 2020 update Nucleic Acids Res, 48 (2020), pp. W395-W402 Google
    Scholar [49] K.A. Tran, O. Kondrashova, A. Bradley, E.D. Williams, J.V. Pearson,
    et al. Deep learning in cancer diagnosis, prognosis and treatment selection Genome
    Med, 13 (2021), p. 152 View in ScopusGoogle Scholar [50] D. Berrar, W. Dubitzky
    Deep learning in bioinformatics and biomedicine Brief Bioinform, 22 (2021), pp.
    1513-1514 CrossRefView in ScopusGoogle Scholar [51] G. Eraslan, Ž. Avsec, J. Gagneur,
    F.J. Theis Deep learning: new computational modelling techniques for genomics
    Nat Rev Genet, 20 (2019), pp. 389-403 CrossRefView in ScopusGoogle Scholar [52]
    P. Sen, S. Lamichhane, V.B. Mathema, A. McGlinchey, A.M. Dickens, et al. Deep
    learning meets metabolomics: a methodological perspective Brief Bioinform, 22
    (2021), pp. 1531-1542 CrossRefView in ScopusGoogle Scholar [53] B. Wen, W.F. Zeng,
    Y. Liao, Z. Shi, S.R. Savage, et al. Deep learning in proteomics Proteomics, 20
    (2020), Article e1900335 Google Scholar Cited by (0) 1 Theoretically, the contributions
    of the three authors are equal and both are considered as the first author. ©
    2024 The Author(s). Published by Elsevier B.V. on behalf of Research Network of
    Computational and Structural Biotechnology. Recommended articles Improving generalization
    capability of deep learning-based nuclei instance segmentation by non-deterministic
    train time and deterministic test time stain normalization Computational and Structural
    Biotechnology Journal, Volume 23, 2024, pp. 669-678 Amirreza Mahbod, …, Sepideh
    Hatamikia View PDF DeepNeuropePred: A robust and universal tool to predict cleavage
    sites from neuropeptide precursors by protein language model Computational and
    Structural Biotechnology Journal, Volume 23, 2024, pp. 309-315 Lei Wang, …, Yan
    Wang View PDF GCclassifier: An R package for the prediction of molecular subtypes
    of gastric cancer Computational and Structural Biotechnology Journal, Volume 23,
    2024, pp. 752-758 Jiang Li, …, Xin Wang View PDF Show 3 more articles Article
    Metrics Captures Readers: 5 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Computational and Structural Biotechnology Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'HiOmics: A cloud-based one-stop platform for the comprehensive analysis
    of large-scale omics data'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liang H.
  - Zhang Z.
  - Hu C.
  - Gong Y.
  - Cheng D.
  citation_count: '0'
  description: 'With the rapid evolution of the Internet, Internet of Things (IoT),
    and geographic information systems (GIS), spatio-temporal Big Data (STBD) is experiencing
    exponential growth, marking the onset of the STBD era. Recent studies have concentrated
    on developing algorithms and techniques for the collection, management, storage,
    processing, analysis, and visualization of STBD. Researchers have made significant
    advancements by enhancing STBD handling techniques, creating novel systems, and
    integrating spatio-temporal support into existing systems. However, these studies
    often neglect resource management and system optimization, crucial factors for
    enhancing the efficiency of STBD processing and applications. Additionally, the
    transition of STBD to the innovative Cloud-Edge-End unified computing system needs
    to be noticed. In this survey, we comprehensively explore the entire ecosystem
    of STBD analytics systems. We delineate the STBD analytics ecosystem and categorize
    the technologies used to process GIS data into five modules: STBD, computation
    resources, processing platform, resource management, and applications. Specifically,
    we subdivide STBD and its applications into geoscience-oriented and human-social
    activity-oriented. Within the processing platform module, we further categorize
    it into the data management layer (DBMS-GIS), data processing layer (BigData-GIS),
    data analysis layer (AI-GIS), and cloud native layer (Cloud-GIS). The resource
    management module and each layer in the processing platform are classified into
    three categories: task-oriented, resource-oriented, and cloud-based. Finally,
    we propose research agendas for potential future developments.'
  doi: 10.1109/TBDATA.2023.3342619
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Big Data
    >Volume: 10 Issue: 2 A Survey on Spatio-Temporal Big Data Analytics Ecosystem:
    Resource Management, Processing Platform, and Applications Publisher: IEEE Cite
    This PDF Huanghuang Liang; Zheng Zhang; Chuang Hu; Yili Gong; Dazhao Cheng All
    Authors 162 Full Text Views Abstract Document Sections I. Introduction II. Spatio-Temporal
    Big Data III. Processing Platform IV. Resource Management V. Applications Show
    Full Outline Authors Figures References Keywords Metrics Abstract: With the rapid
    evolution of the Internet, Internet of Things (IoT), and geographic information
    systems (GIS), spatio-temporal Big Data (STBD) is experiencing exponential growth,
    marking the onset of the STBD era. Recent studies have concentrated on developing
    algorithms and techniques for the collection, management, storage, processing,
    analysis, and visualization of STBD. Researchers have made significant advancements
    by enhancing STBD handling techniques, creating novel systems, and integrating
    spatio-temporal support into existing systems. However, these studies often neglect
    resource management and system optimization, crucial factors for enhancing the
    efficiency of STBD processing and applications. Additionally, the transition of
    STBD to the innovative Cloud-Edge-End unified computing system needs to be noticed.
    In this survey, we comprehensively explore the entire ecosystem of STBD analytics
    systems. We delineate the STBD analytics ecosystem and categorize the technologies
    used to process GIS data into five modules: STBD, computation resources, processing
    platform, resource management, and applications. Specifically, we subdivide STBD
    and its applications into geoscience-oriented and human-social activity-oriented.
    Within the processing platform module, we further categorize it into the data
    management layer (DBMS-GIS), data processing layer (BigData-GIS), data analysis
    layer (AI-GIS), and cloud native layer (Cloud-GIS). The resource management module
    and each layer in the processing platform are classified into three categories:
    task-oriented, resource-oriented, and cloud-based. Finally, we propose research
    agendas for potential future developments. Published in: IEEE Transactions on
    Big Data ( Volume: 10, Issue: 2, April 2024) Page(s): 174 - 193 Date of Publication:
    13 December 2023 ISSN Information: DOI: 10.1109/TBDATA.2023.3342619 Publisher:
    IEEE Funding Agency: SECTION I. Introduction With the rapid expansion of the Internet,
    IoT, GIS, and data collection technologies becoming more diverse and data types
    continually enhancing, making STBD grows ”explosively.” Integrating spatio-temporal
    information and Big Data marks the official entry into the era of STBD [1]. STBD
    compensates for the lack of data by providing a rich volume and variety of data
    types that can fully satisfy numerous research needs, promoting ongoing, in-depth,
    cross-sectional research. The dynamic evolution of spatio-temporal objects, events,
    and other elements and their dynamic correlations pose significant challenges
    to data management, processing, and analysis [2]. Meanwhile, computer technology
    is rapidly evolving. Computing power has skyrocketed, and hardware architecture
    has evolved. Big data processing frameworks and high-performance computing models
    have been established. Due to the evolution of new hardware, software, and application
    requirements, expanding GIS to improve adaptability and performance efficiency
    presents new opportunities and challenges. In the STBD ecosystem, GIS data merges
    spatial and temporal dimensions, capturing changes in geographical information.
    GIS consolidates varied data into a unified spatial framework, enabling in-depth
    spatial-temporal analysis [3]. Its unique attributes allow for integrating different
    data types and unveiling hidden patterns within geographical spaces, enhancing
    our understanding of spatial trends and contexts. The seamless integration of
    databases, Big Data, AI, and cloud computing signifies an emerging trend in unified
    systems [4]. Database systems are foundational for managing vast amounts of structured
    and unstructured data. Big data and AI shine in sophisticated data analytics,
    whereas cloud platforms amplify processing efficiency through resource provisioning.
    STBD, with its expansive, varied, and time-sensitive attributes, is deeply linked
    with these technologies. Databases paired with Big Data analytics manage and decipher
    extensive datasets, revealing pivotal insights. AI, fueled by data-driven learning,
    infuses intelligence into processing and decision-making. Cloud infrastructure
    offers unmatched scalability, adapting to diverse processing needs [5]. Their
    collective synergy in a cohesive ecosystem amplifies combined strengths, bolstering
    data processing capabilities. Spatio-temporal applications typically span multiple
    domains, including GIS, AI, and Big Data. Given the large-scale nature of spatio-temporal
    data, there is an urgent need for robust data processing capabilities to ensure
    efficient analysis and management. spatio-temporal applications involve intricate
    data processing involving geographic location and time, endowing them with unique
    features such as time series and geographical coordinates [6]. In spatio-temporal
    applications, achieving real-time and accurate data processing is crucial, especially
    in traffic monitoring and weather forecasting applications. This underscores the
    demand for timely data updates and precise spatial information. So, it''s critical
    to integrate these areas to meet interdisciplinary needs. In healthcare [7], this
    convergence empowers predictive analytics for patient outcomes, while in finance
    [8], it facilitates fraud detection and risk assessment. We have provided application
    examples for the smart city STBD platform [9], underscoring the interconnectedness
    of computational resources, spatio-temporal data, processing platforms, and applications
    in Fig. 1. Collectively, this convergence represents a multifaceted synergy that
    transcends technology silos, unleashing transformative potential and opportunities
    across diverse sectors. Fig. 1. Applications of smart city STBD platform. Show
    All Resource management is indispensable for STBD processing efficiency across
    various platforms. In DBMS, meticulous resource allocation ensures efficient storage
    and retrieval of STBD, providing robust support for complex queries and analyses.
    Intelligent resource allocation in Big Data processing systems significantly enhances
    the processing speed of massive datasets, accelerating information extraction
    and pattern recognition. In AI training architectures, precise resource management
    directly impacts the performance of machine learning (ML) models by optimizing
    computational resources to improve training efficiency. Dynamic resource allocation
    and management on cloud platforms are crucial for ensuring system flexibility
    and adaptability, especially in addressing evolving demands for data processing
    and AI training. To better understand resource management''s role in the multi-platform
    environment of STBD processing, we sort out the existing STBD analytics ecosystem
    and divide it into the five modules in Fig. 2. (1) STBD included definition, characteristics,
    and analysis. (2) Computation resources comprised computational, storage, and
    communication resources used in distributed computing architecture to perform
    various tasks. (3) Processing Platform contained data management, processing,
    analytics, and cloud native layers. (4) Resource Management covered DBMS-GIS,
    BigData-GIS, AI-GIS, and Cloud-GIS resource scheduling and management policies.
    (5) Applications of STBD oriented to geoscience and human-social activities. Despite
    their differences in nature and objectives, these layers intersect and provide
    comprehensive solutions for addressing complex data challenges in data processing.
    Fig. 2. Spatio-temporal Big Data analytics ecosystem. Show All We have identified
    critical gaps that demand urgent attention. Current investigations and performance
    analyses are predominantly confined to systems developed before 2017, necessitating
    more recent research to provide additional insights. Current research predominantly
    focuses on developing algorithms and technologies for capturing, storing, managing,
    analyzing, and visualizing STBD [10], [11]. However, these efforts often need
    to pay more attention to the critical aspects of system optimization and resource
    management, limiting the potential for enhancing the efficiency of STBD processing
    and applications. Additionally, there needs to be more research on broader aspects
    of the spatio-temporal analysis ecosystem, including spatio-temporal database
    management, parallel processing, AI analytics, and cloud-based spatial feature
    integration. Research on the migration of STBD to new Cloud-Edge-End integrated
    computing systems is still in its infancy and represents a critical area that
    requires in-depth exploration [12]. These key domains demand attention in current
    research to fill significant gaps in understanding STBD and its practical applications.
    We comprehensively review recent research on STBD systems. To furnish users with
    a guide to address problems and employ solution techniques related to STBD. We
    systematically organize the state-of-the-art STBD analytics systems and delve
    into their resource management, providing users with a detailed understanding
    of the current landscape. Furthermore, we illuminate trends, emerging requirements,
    and challenges in each support layer within the STBD ecosystem. This offers researchers
    valuable insights into potential directions for future research in STBD processing
    and applications. Also, we’d like to explore the existing challenges and forthcoming
    opportunities in STBD analytics, which is our vision for the future development
    of STBD analytics. The paper is structured as follows: Section II introduces the
    STBD''s definition, characteristics, and analysis. Section III focuses on the
    core and spatio-temporal platforms within the data management and processing layers,
    with insights into spatio-temporal cloud platforms. Section IV discusses resource
    management strategies across these layers. Section V reviews open-source databases
    and STBD-related applications. Section VI highlights DBMS-GIS, BigData-GIS, AI-GIS,
    and cloud-GIS research trends. Section VII concludes. SECTION II. Spatio-Temporal
    Big Data A. Definition STBD is an extensive, large-scale dataset founded on a
    unified spatio-temporal datum (comprising temporal reference system and spatial
    reference system), capturing activities (such as movement changes) in both time
    and space, directly associated with a location through positioning or indirectly
    through spatial distribution [13]. Traditionally, STBD has found applications
    in describing meteorological data derived from sophisticated systems like remote
    sensing (RS), GIS, global positioning systems (GPS), geological information systems,
    intelligent city systems, traffic information systems, and environmental information
    systems. RS generally denotes non-contact, long-distance comprehensive detection
    technologies; GIS is a computer system for collecting, storing, managing, processing,
    retrieving, analyzing, and expressing geospatial data; and GPS is a high-precision
    radio navigation system that relies on artificial earth satellites and time information.
    With the incorporation of the temporal dimension, GIS has transformed into temporal
    GIS, representing an evolving capacity to integrate temporal data seamlessly with
    location and attribute data. B. Characteristics STBD integrates Big Data and spatial
    data characteristics, incorporating time-dimension information to address various
    time-related geographic data challenges effectively. Big data processing involves
    leveraging all available data without compromising the time required for random
    analysis. Big data has five key features: volume, velocity, variety, value, and
    veracity [14]. Spatial data can be represented in two formats: raster and vector.
    Raster data, exemplified by satellite imagery, is typically displayed as multi-dimensional
    arrays. In contrast, vector data, consisting of points, lines, and polygons, depicts
    geographic features like roads, regional boundaries, and GPS coordinates. Notably,
    STBD is distinguished by specificity, fuzziness, dynamism, finiteness, social
    network, heterogeneity, low quality, non-smoothness, and networking [15]. STBD''s
    data content encompasses time, spatial, and non-spatial-temporal attributes [16].
    Time attributes describe temporal information, such as time stamps for spatial
    objects, raster layers, or a series of snapshots. Spatial attributes encompass
    spatial information, including location (e.g., longitude and latitude), extent
    (e.g., area and perimeter), and shape composition. Non-spatio-temporal attributes,
    ranging from structured data like climate or demographic data to unstructured
    data like remote sensing images, add diversity to the dataset. The temporal snapshots
    model timestamps spatial layers with a shared theme, portraying lines, polygons,
    raster time series trajectories, and dynamic spatio-temporal networks like time-expanded
    or time-aggregated graphs. Spatio-temporal networks depict dynamic structures
    across time and space, illustrating connections such as traffic routes and communication
    networks. Traffic flow involves the movement of entities at specific instances
    and places, necessitating the analysis and modeling of spatio-temporal data. The
    temporal change model captures spatio-temporal data, starting with an initial
    spatial layer at a specific time, followed by incremental modifications, covering
    motion (e.g., Brownian motion or random walks), velocity, and acceleration at
    spatial points. Splitting or merging signifies the dividing or amalgamation of
    lines and polygons at a specific spatio-temporal nexus. Conversely, the traffic
    model focuses on temporal shifts in traffic attributes, including flow, speed,
    and direction from a specified spatial origin. Event and process models articulate
    temporal details through events or processes in Table I. Given the diversity of
    these data types, effective analysis and processing necessitate using different
    methods. TABLE I Spatio-Temporal Big Data Characteristic Division C. Analysis
    Spatial Analysis: The continuous advancements in GIS and network technology have
    markedly augmented the capabilities of GIS spatial analysis. Spatial analysis,
    a basic function of GIS, enables the examination of spatial layout, aggregation
    level, and coupling association [17]. Researchers have expanded spatial analysis
    functions to encompass dynamic content, including geometric analysis, terrain
    analysis, network analysis, raster data analysis, and spatial statistical analysis.
    This expansion enhances the feasibility of diverse spatial planning and design
    decisions by bolstering data support and design science in system designs [18].
    Spatio-Temporal Analysis: Traditionally, spatial statistics faced limitations
    due to the constrained accessibility of STBD [19]. However, recent advancements
    in social and environmental perception have significantly enhanced STBD accessibility,
    fueling the demand for applications and research requiring real-time and dynamic
    analysis of extensive spatial data [20]. This shift has prompted the development
    of spatio-temporal analysis methods, extending the reach of conventional spatial
    statistical methods into the spatio-temporal domain [21]. To fully exploit STBD,
    researchers must delve into the spatio-temporal analysis of environmental and
    human factors and their intricate interconnections. The advancement of STBD necessitates
    urgent theoretical, technical, and methodological support for spatio-temporal
    cluster/anomaly/correlation/prediction analysis. SECTION III. Processing Platform
    In this section, from the data management layer, where the marriage of database
    management systems (DBMS) and GIS unfolds, to the dynamic realms of data processing
    with the infusion of the data processing layer, the data analysis layer introduces
    the transformative influence of AI, reshaping how we glean insights from spatial
    data. Finally, ascending to the cloud native layer, the fusion of technology and
    geography manifests in Cloud-GIS, offering scalability and adaptability that transcend
    traditional boundaries. A. Data Management Layer The data management layer encompasses
    the classification of spatio-temporal databases, recognizing the limitations of
    traditional databases in efficiently handling massive, diverse, unstructured administrative
    data and heterogeneous memory resources. The demand for a usable, scalable, update-supporting
    spatio-temporal DBMS is evident for the successful support of contemporary urban
    applications. 1) Basic DBMS Spatial DBMSs [22] fall into two primary categories:
    relational and NoSQL DBMSs. Relational DBMSs comprise interconnected two-dimensional
    row tables and utilize SQL for data manipulation. Well-known relational DBMSs
    with spatial capabilities include Oracle Spatial, IBM Db2, Microsoft SQL Server,
    Microsoft Access, MySQL Spatial, and PostGIS in Table II. While traditional relational
    DBMSs are reliable, mature, and efficient and find widespread use in various applications,
    they may face challenges when dealing with large-scale data, diverse data types,
    and the demands of emerging ultra-large-scale and high-concurrency web 2.0 platforms.
    Conversely, NoSQL DBMSs offer high scalability, performance, flexible data models,
    and robust availability. They encompass three main data store types: column-based
    databases (e.g., HBase), document stores (e.g., MongoDB), and graph databases
    (e.g., Neo4j), aptly addressing the complexities of modern applications. TABLE
    II Basic Database Management System The widespread adoption of in-memory databases,
    including Redis, NuoDB, and MySQL Cluster, has triggered a resurgence in in-memory
    computing technologies. However, to achieve significant performance improvements
    in near-memory computing, the system design must be customized to the specific
    requirements of the upper-layer application. Pregel [23] proposed various near-memory
    computing architectures tailored for MapReduce workloads. Q-PIM [24] introduced
    a flexible SRAM-based precision all-digital in-memory (PIM) architecture. It utilizes
    a genetic algorithm-based training-free layer quantization approach to optimally
    control the precision of each DNN layer for enhanced efficiency. Near-memory computing
    still needs more efficient and transparent system-level support despite these
    advancements. Addressing this gap, Vermij et al. [25] put forth a dynamic workload
    balancing technique. This approach empowers applications to execute heterogeneously
    on near-memory CPUs, optimizing CPU utilization and enhancing overall performance,
    especially when dealing with massive datasets. 2) Spatio-Temporal DBMS-GIS While
    traditional relational databases such as Oracle Spatial, MySQL Spatial, and PostGIS
    offer support for managing STBD, they often encounter challenges with scalability
    as data volumes grow [26]. On the other hand, distributed NoSQL data stores exhibit
    impressive capabilities for handling millions of updates per second. However,
    these NoSQL solutions inherently need more support for STBD management, primarily
    due to the absence of essential secondary indexes [27]. Several robust STBD analytics
    systems have emerged recently, capitalizing on NoSQL DBMSs like MD-HBase, GeoMesa,
    Distributed SECONDO, BBoxDB, THBase, JUST, and TrajMesa. Table III presents a
    detailed characteristic matrix of these systems. For instance, JUST [28] leverages
    HBase as its foundational storage, GeoMesa as its STBD indexing tool, and Spark
    as its execution engine. In contrast, TrajMesa [29] stands out as the first endeavor
    to establish a comprehensive distributed NoSQL trajectory storage engine based
    on GeoMesa. TABLE III NoSQL-Based Spatio-Temporal Database Management System B.
    Data Processing Layer The data processing layer is a critical component that houses
    diverse spatio-temporal platforms tailored for Big Data computing. Using Big Data
    systems to construct clusters can expedite the processing of terabyte (TB) or
    exabyte (EB)-level data [35]. Nevertheless, most Big Data systems only support
    traditional relational data or some STBDs. With the rapid development of location-based
    services and RS, many companies and projects are looking to integrate Big Data
    systems and STBDs. 1) Basic Big Data System This section compares the three most
    popular Big Data systems, Apache Hadoop, Spark, and Flink in Table IV. TABLE IV
    Characteristic Comparison of Apache Hadoop, Spark, and Flink Apache Hadoop [36]
    is an open-source software framework developed by Yahoo! for distributed storage
    and processing large datasets with cluster-level fault tolerance. It can be set
    up across a cluster of computers constructed from commodity hardware. Currently,
    Hadoop consists of five core modules: (1) Hadoop Common is a set of shared programming
    libraries the other modules utilize. (2) Hadoop Distributed File System (HDFS)
    is a Java-based file system for storing data across multiple machines. (3) Hadoop
    YARN manages and schedules resource requests in a distributed environment. (4)
    Hadoop MapReduce is a programming model for processing large datasets in parallel.
    (5) Hadoop Ozone, which is an object store for Hadoop. Landset et al. [37] surveyed
    Hadoop''s static and dynamic resource management modes, job scheduling methods,
    and performance comparison. However, due to its disk-based data processing, low-level
    processing API, and Java-only support, Hadoop is unsuitable for massive-scale
    and complex Big Data processing. Apache Spark [38] supports in-memory computing
    and acyclic data flow through its DAG execution engine. It has been reported to
    be 100 times faster than Hadoop''s MapReduce. Currently, research on Spark resource
    allocation mainly focuses on increasing its scheduling depth (e.g., heuristic
    attribute reduction and eviction mechanisms) and breadth (e.g., cross-platform),
    heterogeneous resources). Leveraging SQL, ML, graph computing, and multiple languages,
    Spark proves ideal for applications requiring high throughput, handling limited
    real-time and micro-batch data processing with substantial data volumes and intricate
    logic. In contrast to Hadoop MapReduce, Spark introduces a novel programming paradigm
    focused on the resilient distributed dataset (RDD). This dataset can be efficiently
    stored in memory and distributed across machines for streamlined data processing.
    Apache Flink [39] is a unified stream and batch processing framework utilizing
    a dataflow programming model to provide event-at-a-time processing on finite and
    infinite datasets. This primary processing mode gives Flink lower stream processing
    latency than Spark Streaming, making it suitable for tasks requiring low latency,
    such as real-time monitoring, reports, stream data analysis, and data warehousing.
    Flink also supports simple static resource management, dynamic management of computing
    resources, and memory resource isolation, though it does not support CPU resource
    isolation. It can integrate with Yarn, Mesos, K8s, and other frameworks better
    [40]. 2) Spatio-Temporal BigData-GIS In Big Data processing, prevalent models
    include distributed computing frameworks built upon DAGs or massively parallel
    processing iterative computing models. The dynamic task generation at runtime
    challenges pre-scheduling approaches [41]. Given the distributed nature of these
    systems, there is a pronounced need for a flexible and adaptable scheduling system
    to effectively manage the intricate computational landscape. The goal of integrating
    STBD and Big Data systems is to enable traditional Big Data systems to support
    the storage of basic STBD types such as points, lines, planes, and time series,
    and the operations and analysis of STBD such as joins, range, and k NN. We classified
    into three groups: (1) Hadoop-based, (2) Spark-based, and (3) Flink-based in Table
    V. TABLE V Characteristic Comparison of Hadoop/Spark/Flink-Based BigData-GIS Systems
    Hadoop-Based Big Data Systems are the foundation of STBD framework research. SpatialHadoop
    [42] is a Hadoop MapReduce framework specifically designed to facilitate spatial
    data storage, processing, and querying. This framework consists of four layers:
    a two-level spatial index for data storage, MapReduce for data processing, basic
    spatial operations (e.g., range query, k NN, spatial join), and Pigeon-based spatial
    queries. These components make SpatialHadoop both convenient and extensible. Moreover,
    ST-Hadoop [43] provides built-in STBD types and operations, enhancing data indexing
    at the storage layer and introducing new spatio-temporal processing methods, making
    it more applicable than traditional spatial Big Data systems. Spark-Based Big
    Data Systems are gaining popularity due to their faster in-memory data processing
    capabilities compared to Hadoop-based solutions. GeoSpark [44] is a cluster computing
    framework that enables large-scale spatial data processing. It comprises a Spark
    Layer, a spatial RDD (SRDD) Layer, and a Spatial Query Processing Layer. These
    layers allow for storing geometrical and spatial objects (such as Point, LineString,
    Polygons, and Rectangles) and performing basic geometrical operations. Furthermore,
    spatial indexes (e.g., R-tree, Quad-tree) can be created to improve the performance
    of spatial data processing in each SRDD partition. STARK [45] presents a comprehensive
    solution for supporting spatio-temporal operations within the Spark framework.
    It provides spatial partitioners, different indexing modes, filter, join, and
    clustering operators for spatio-temporal data. Furthermore, unlike GeoSpark''s
    SRDDs, which can only hold geometries of one type, STARK''s RDDs can accommodate
    all kinds of columns, including spatial and temporal data. This allows for efficient
    processing of data in subsequent steps. Additionally, STARK extends Piglet''s
    capabilities with an extended Pig Latin dialect for further ease of use. It also
    integrates seamlessly with the Spark API, eliminating users needing to adopt a
    separate API. STARK still needs to support more spatio-temporal queries than GeoSpark.
    Flink-Based Big Data Systems are well-suited to dealing with real-time STBD due
    to their scalability and ability to process streaming data. GeoFlink [46] is a
    framework that integrates Flink with streaming spatial data (e.g., points, line
    strings, polygons) using a grid-based index. It currently supports spatial range,
    spatial k NN, and join queries but is limited to point data. GeoFlink comprises
    two core layers: the Spatial Stream Layer and the Real-time Spatial Query Processing
    Layer. The former converts incoming data streams into spatially referenced streams,
    while the latter enables spatial queries to be executed on these streams in real
    time. SPEAR [47] is a system for integrating temporal data into Flink. SPEAR supports
    streaming temporal point, line, and polygon data queries. It extends Flink by
    providing partitioning based on GeoHash, spatial and temporal indexing, and a
    library of spatio-temporal streaming processes. SPEAR enables dynamic query management
    in a distributed environment over high-velocity STBD streams, allowing fast real-time
    query responses. C. Data Analysis Layer The data analysis layer comprises diverse
    spatio-temporal platforms tailored for AI training, recognizing the need to depart
    from traditional serial analytical algorithms that fall short of meeting the real-time
    processing demands of STBD. Integrating AI and GIS has enabled remarkable improvements
    in GIS''s image processing and analysis capabilities and predictive power. This
    has facilitated the application of AI-GIS technology in various fields, such as
    ecological assessment, environmental protection, and agriculture. In recent years,
    the rapid advancement of AI and GIS has resulted in the emergence of several AI-GIS
    platforms. This section will provide a more in-depth examination of the features
    of each platform. 1) Basic AI System TensorFlow, Keras, PyTorch, and MXNet [48]
    are the four most pertinent comparative platforms. In Table VI, the comparison
    results are presented. TABLE VI Comparison of AI Frameworks TensorFlow [49], a
    robust tool for tasks like image recognition, semantic segmentation, and natural
    language processing, consists of two core modules: a static computational graph
    generator and a software library. Moreover, TensorFlow adeptly manages CPU, GPU,
    and memory resources. The solution, which involves the coordination of CPU and
    GPU management, enhances the efficiency of the training stage (or long-term processing)
    by implementing static scheduling and load-balancing algorithms instead of dynamic
    provisioning [50]. Keras [51] is not an independent AI platform but a lightweight
    application interface for TensorFlow. It highly integrates the functions of TensorFlow,
    has a low learning cost, and effectively calls existing functions of TensorFlow.
    However, it has a relatively slow running speed and strict code modification requirements
    and is only suitable for more straightforward application scenarios. PyTorch [52]
    is a popular DL framework for handling dynamic computational graphs and performing
    various derivatives. It implements a custom allocator that incrementally builds
    up a CUDA memory cache and reassigns it to later allocations without using any
    CUDA APIs. By configuring the code, it is possible to use multiple GPUs for parallel
    processing, yet this can lead to a GPU memory imbalance problem. Specifically,
    the model and gradient lost during the calculation are saved on card 0 by default,
    causing card 0 to consume more video memory than the other cards [53]. MXNet [54]
    offers symbolic and imperative programming, enabling maximum efficiency and productivity.
    Its upper interface, Gluon, stands out from other AI platforms because it supports
    flexible, dynamic, and efficient static graphs. Additionally, MXNet has been designed
    to focus on distributed computing, providing superior support for multi-machine
    training. Moreover, MXNet utilizes object substitution, memory sharing, dynamic
    memory allocation, and other technologies, resulting in lower video memory consumption
    for most model training than others [55]. 2) Spatio-Temporal AI-GIS The traditional
    Big Data system presents certain limitations regarding GIS data processing. Communication
    resources can be a bottleneck, as GPUs may have enough processing power to handle
    image data in GIS. However, bandwidth and data transfer limitations impede further
    improvements in processing speed. Furthermore, the complexity of a model requires
    more parameters, and synchronizing them in a distributed system requires a considerable
    amount of bandwidth [56]. Additionally, computing resources can be an issue, as
    the system will allocate resources such as excessive CPU and GPU to a running
    task, resulting in lower resource utilization. The current resource scheduling
    algorithms need to be optimized explicitly for AI and, thus, cannot fully exploit
    the advantages of DL and Big Data systems when processing STBD [57]. For communication
    resources, two standard communication models are commonly employed: (1) a master-slave-based
    architecture [58], wherein a parameter server is maintained to store all parameters
    and devices communicate exclusively with the parameter server; and (2) a peer-to-peer
    (P2P)-based architecture [59], wherein devices interact with each other, resulting
    in an increased bandwidth overhead if the number of devices increases. To demonstrate
    the effectiveness of ML techniques for accelerating the processing of GIS data
    models in large datasets, the stale synchronous parallel (SSP) mechanism of TensorFlow
    can be utilized. This approach is designed to parallelize computations for large-scale
    data analysis, allowing for more efficient and faster processing. The traditional
    bulk synchronous parallel (BSP) mechanism assumes that all tasks are trained and
    updated with uniform parameters, leading to longer running times due to the different
    input parameters of each task. However, the SSP mechanism of TensorFlow allows
    faster-running tasks to commence the next iteration earlier with the parameters
    of the old version, thus improving the overall processing performance of the system
    and optimizing the GIS data model in a shorter duration. STBD inherently involves
    data points characterized by spatial and temporal dimensions, exemplified by applications
    such as tracking the dynamic movement of objects over time [60]. Confronted with
    the formidable challenges posed by STBD, it becomes imperative to leverage new
    hardware architectures and employ effective parallel processing models to ensure
    efficient and real-time data analysis. Enhancing resource usage efficiency and
    reducing computational resource consumption is essential for achieving remarkable
    performance. To this end, an appropriate resource allocation approach should be
    employed to distribute resources to the relevant activities. Traditional neural
    networks are static models with numerous layers, each performing a designated
    task. TensorFlow proposes a concept based on dynamic flow control, which enables
    users to distribute data across multiple devices in a cluster. Each device independently
    calculates a part of the data, thus realizing the data parallelism function. Tiresias
    [61] is a GPU scheduler that works to minimize the execution time of individual
    tasks and prevent starvation tasks. It also considers the task''s execution time
    and the GPU occupancy rate, calculates the task''s service level, and prioritizes
    the tasks accordingly. AI is crucial in GIS, particularly when managing unstructured
    data such as pictures and videos. DL offers an efficient solution for tasks such
    as target detection, binary classification, and feature classification. For instance,
    DeepLabv3+ [62] is a TensorFlow-based tool that can perform semantic image segmentation,
    enabling us to differentiate between land, ocean, and buildings in satellite images.
    Entropy-Weighted Network [63] is another example used for polar sea ice open lead
    detection from synthetic-aperture radar images. In the past decade, road extraction
    from satellite images has been a popular research topic, with applications ranging
    from automated crisis response and road map updating to city planning, geographic
    information updating, and car navigation. D-LinkNet [64], a PyTorch-based high-resolution
    satellite imagery road extraction model, has been developed. In addition, Orfeo
    ToolBox [65] provides a processing library with AI algorithms that can be used
    to manage high-resolution optical, multispectral, and radar images at the TB scale.
    PSGraph [66], a graph analytics system, has been developed using Spark executor
    and PyTorch for calculations and a distributed parameter server for storing frequently
    accessed models. The above description highlights the essential role that AI technology
    can play in processing STBD on a Big Data system. For instance, the combination
    of TensorFlow and distributed systems can maximize the high performance of TensorFlow
    image processing and leverage the data parallelism of the distributed system.
    Furthermore, this block-based processing of Big Data can generate analytical models
    that can enhance the capabilities of problem-solving, automatic reasoning, decision-making,
    knowledge representation, and utilization, thereby enabling intelligent solutions
    to complex real-world problems. Depending on the specific AI methods applied to
    GIS applications, there are various combinations of GIS with expert or knowledge-based
    expert systems, pattern recognition, and decision support systems. D. Cloud Native
    Layer The cloud native layer introduces the mainstream architecture of cloud scheduling
    systems, highlighting the need for comprehensive one-stop STBD platforms. The
    current challenge lies in the inefficiency of managing and scheduling data-intensive
    STBD and AI on existing cloud-native platforms. There is a pressing demand for
    a high-performance and scalable scheduling system tailored for cloud-native environments.
    The industry is actively working towards deploying Big Data platforms and AI training
    on cloud-native infrastructures to achieve excellent elasticity and scalability
    [67]. Despite these aspirations, there have been limited breakthroughs in this
    domain. The advent of container technologies and orchestration systems has significantly
    accelerated the development of cloud-native generation platforms. Notably, Docker
    and Kubernetes (K8s) technologies have emerged as the de facto standard for shaping
    the future of cloud-native architecture. 1) New Cloud Computing Scheduling System
    STBD transfer involves intricate data transmission from the cloud to edge and
    terminal devices. Optimization techniques such as data segmentation and compression
    ensure efficient distribution and storage. Subsequently, leveraging edge computing
    effectively mitigates the transmission pressure on the cloud, enhancing overall
    data processing efficiency. Establishing a direct and efficient data transmission
    channel meets real-time processing requirements for a subset of STBD on terminal
    devices. Network optimization is achieved through the meticulous selection of
    transmission paths and the adoption of appropriate network protocols. These combined
    factors synergistically propel the efficient, secure, and real-time transmission
    of STBD within the Cloud-Edge-End environment. A large-scale cloud platform often
    demands the efficient and reliable operation of tens of thousands of containers,
    necessitating a robust service orchestration system. Several cluster resource
    scheduling systems have emerged to meet the increasing resource demands from diverse
    services and tasks, including Borg, Mesos, and Omega [68]. Borg, a representative
    of centralized schedulers, has been a trailblazer with over a decade of production
    experience at Google, overseeing tasks such as GFS, BigTable, Gmail, and MapReduce.
    Mesos, representing two-level schedulers, facilitates the equitable sharing of
    cluster resources among multiple frameworks. As a shared-state scheduler example,
    Omega supports using different schedulers for various task types and handles resource
    request conflicts adeptly. K8s, an open-source project from Google designed for
    managing Docker clusters, inherits Borg''s strengths and aims to orchestrate,
    deploy, run, and manage containerized applications. It features a unified scheduling
    framework capable of managing thousands of servers and tens of thousands of containers.
    It also includes a plugin interface for third-party customization and extension
    of new scheduling systems. The industry adopts three primary strategies to address
    the challenges of developing and deploying STBD and AI applications in the cloud
    [69]. The first approach involves delivering cloud services through application-
    or service-level multi-tenancy. In this scenario, a database or AI service instance
    serves user needs, and user isolation is managed by the database or AI platform
    rather than the cloud infrastructure. Examples include AWS Aurora and Google Cloud
    AutoML. While flexible, this approach tends to be more application-specific and
    less versatile for general-purpose Big Data or database platforms. The second
    approach utilizes dedicated bare-metal machines within the cloud platform to provide
    Big Data or AI services, ensuring platform performance. However, this sacrifices
    elasticity and flexibility, making these bare-metal machines inefficient for scheduling
    and operational capabilities. The third approach confines the container platform
    to scheduling stateless applications, such as microservices, while deploying Big
    Data and AI platforms on traditional virtualization-based clouds. Although this
    guarantees robust scheduling capabilities, it compromises the stability of Big
    Data or AI platforms. While straightforward, real-world instances have demonstrated
    limited success due to stability concerns. 2) One-Stop Spatio-Temporal Platform
    The advancement of GIS technology has enabled the development of one-stop GIS
    platforms that leverage Big Data systems and AI frameworks on cloud platforms
    for rapid analysis and processing. Such platforms can be used more efficiently
    in geoscience and social activities. This section compares the most representative
    one-stop spatio-temporal platforms in Table VII. TABLE VII Comparison of One-StopGIS
    Platforms The Esri platform is the launching point for ArcGIS [70], a powerful
    GIS software that can be integrated into web servers and desktop applications.
    ArcGIS''s programmable components span a wide range of objects, from fine-grained
    individual geometries to coarse-grained objects. With object pools, users can
    specify a maximum number of services to be deployed, allowing for more excellent
    compatibility with Big Data platforms such as Spark. ArcGIS Server also provides
    granular control over the deployment of GIS services, allowing users to launch
    and cancel individual services. In contrast, SuperMap services must be activated
    or stopped, making granular management impossible. For cloud-native applications,
    cloud-based computing and storage are viable alternatives, although external cloud
    service providers such as AWS and Azure may be necessary. SuperMap [71] is well-known
    for its capability to encrypt data using the SDB format, allowing storage of multiple
    datasets in a single source, encompassing simple points, lines, faces, or complex
    geometries. Its data type capabilities surpass ArcGIS’s, as it adeptly handles
    vector files from CAD applications and raster data from picture formats such as
    PNG and IMG. SuperMap seamlessly integrates with Big Data tools and frameworks,
    including distributed file systems, spatio-temporal databases, and spatial analysis
    and processing tools like Spark. Its self-hosted cloud platform based on K8s enables
    GIS functions to be modularized into elastically scalable microservices, providing
    edge GIS tools for establishing a centralized cloud platform. MapInfo [72] contains
    a powerful relational DBMS that organizes data in two-dimensional tables, incorporating
    graphical columns to contain graphical objects and standard data types for easy
    manipulation of graphical data. It also provides ODBC compatibility for various
    relational databases, ensuring the continuity of the original database and access
    to remote databases. Moreover, MapInfo supports the standard picture graphics
    format and a variety of others, including AutoCAD-generated DWG and DXF files.
    QGIS [73] is a lightweight, open-source software package that offers a wide range
    of powerful mapping-related functions and is highly scalable. It includes shared
    GIS libraries such as GDAL and SQLite, which support more data formats than those
    supported by ArcGIS. Furthermore, QGIS can integrate with other desktop GIS software,
    such as GRASS GIS and SAGA GIS, allowing it to perform typical data processing
    and geographical analysis more effectively. However, it has some drawbacks; for
    instance, it is less stable than ArcGIS and can only merge two layers simultaneously.
    Compared to ArcGIS and SuperMap, QGIS is highly customizable and supports Big
    Data systems and AI algorithms. Nevertheless, it could benefit from being more
    feature-rich and more stable. One-stop shops facilitate large-scale analysis of
    STBDs, yet they need to improve data security, scalability, and cloud-native support.
    Moreover, these platforms only offer cloud platform processing without focusing
    on optimizing efficiency through effective resource management. So, further research
    and development are needed. SECTION IV. Resource Management The seamless orchestration
    of computational power, storage, and networking resources expedites STBD processing
    and guarantees that platforms unlock their full potential, providing timely insights
    and actionable results in the dynamic realm of spatio-temporal data analytics.
    A. Resource Management in DBMS-GIS Traditional DBMSs focus on efficiently creating,
    deleting, modifying, and querying data. With the addition of STBD and new storage
    devices, we can examine the computing resource management in DBMS-GIS systems
    from the perspectives of application workloads and hardware resources. 1) Workload-Oriented
    Resource Management Diverse workloads exhibit unique resource requirements and
    sensitivity levels. We can skillfully manage resources to optimize performance
    by comprehensively understanding the application''s features. Online transaction
    processing (OLTP) and onLine analytical processing (OLAP) are distinctly different
    in terms of data processing technology. OLTP requires fast I/O performance due
    to soild consistency, supported by conflict resolution at write time (using read
    and write locks), leading to scalability issues [74]. On the other hand, OLAP
    focuses on data analysis and is akin to a data warehouse, so DBMSs do not need
    to be concerned with data consistency constantly; instead, they should focus on
    decision support and providing clear and intelligible query results, which are
    CPU- and memory-sensitive workloads [75]. Additionally, real-time data is another
    essential workload type. For instance, Ma et al. [76] utilized taxi passengers’
    real-time ride requests to schedule the proper taxis to pick them up. Streaming
    data will continually arrive at the DBMS in a fine-grained form, necessitating
    the need to balance indexing, processing, storage, I/O, and CPU consumption. With
    the increasing popularity of NoSQL databases, more and more application scenarios
    are being uncovered. For example, some applications prioritize read performance,
    while others prioritize low-conflict writing, making resource management more
    complex. 2) Storage-Oriented Resource Management The distinguishing attributes
    of STBD include its vast scale, notable diversity, and temporal dynamics. Consequently,
    storage systems must exhibit exceptional scalability and adaptability to effectively
    manage the continuous influx of data streams. STBD often integrates data from
    various sources and modalities, such as sensors and social media. Consequently,
    storage management systems require robust support for diverse data types and flexible
    models. Moreover, meeting the analytical and querying demands of STDB involves
    operations spanning diverse temporal and spatial scales. Therefore, storage systems
    must employ efficient spatio-temporal indexing and querying techniques for rapid
    data access and analysis. The storage engine comprises two essential components,
    disk, and memory, which determine the categorization of databases as disk-based
    or in-memory. Both components can significantly enhance system performance by
    reducing the amount of data that needs to be transferred between the CPU and traditional
    storage. Disk storage is a service unit that utilizes a class of storage media
    (e.g., SSD) to facilitate computation. All data within the database is stored
    on disk, while memory is mainly used as a buffer for bulk writes. Sequential access
    is more efficient than random access due to the physical structure of the hard
    disk, so disk-based databases typically employ indexes to sort records in multiple
    fields, thus reducing data query time. To create these indexes, various data structures,
    such as B-trees, hash tables, B+ trees, and more, can be utilized [77]. Although
    disk storage may affect system performance, many databases still choose it as
    their primary storage due to its ability to ensure data consistency and integrity
    through on-disk log files and transaction management in relational databases,
    as outlined by the principles of atomicity, consistency, isolation, and durability
    [78]. Furthermore, disk storage is preferred due to its low cost, significantly
    lower than other storage devices in the storage hierarchy. There are two techniques
    to increase the throughput of disk-based systems to combat the subpar I/O performance
    of hard disk drives. (1) The number of hard disks and data processing parallelism
    can be increased to reduce the strain on each drive. New RAID architectures such
    as Elastic-RAID [79] can increase system throughput by several or tens, and data
    can be quickly rebuilt during a disk failure. Biscuit [80] is a near-storage computing
    framework for modern solid-state drives, allowing for distributed execution of
    data-intensive applications. (2) Individual disk performance can be improved.
    The 3-D NAND Flash Value-Aware SSD [81] is a reliable solid-state disk with high-speed
    access and low power consumption, which can enhance the performance of DBMS. Kang
    et al. [82] proposed the Smart SSD model, which uses an object-based communication
    protocol to exploit the processing power of SSDs. In-memory storage integrates
    computational resources into innovative storage units with high-bandwidth connectivity,
    enhancing data read and write performance, and distributed architectures can significantly
    augment their capacity. Despite their imperfections, in-memory databases constitute
    an essential subset of distributed databases that are increasingly pivotal for
    modern data processing. The rapid evolution of distributed in-memory storage has
    paved the way for the emergence of in-memory data management systems for task
    processing and data analysis (e.g., HBase, Pig, and H-Store), catering to the
    demands of massive data processing. However, this technology has notable drawbacks,
    including cost, limited capacity, and potential data instability. The new non-volatile
    memory (NVM) storage has the characteristics of I/O performance close to DRAM,
    allowing faster and more flexible data transmission between memory, disk storage,
    and the CPU. Byte addressing fills the gap between traditional disk storage and
    memory, making NVM an essential tool to overcome the memory wall. Recent developments
    in NVM devices, such as resistive memory [83], spin-transfer torque random access
    memory, and Intel Optane DC persistent memory [84] have further narrowed this
    gap. Current research in NVM storage technology focuses on the issues of reading
    and writing speed differences, writing lifespan, and fault tolerance when accessing
    NVM storage devices from the perspectives of hardware structure design, reading
    and writing strategies, wear balance strategies, and fault tolerance strategies.
    For instance, NJS [85], an NVM-based file system with a write-efficient journaling
    scheme, reduces the logging overhead of traditional file systems and takes advantage
    of the byte-accessibility characteristic of NVM. Additionally, circ-Tree [86],
    a B+ tree variant with a circular design for NVM, enhances the access performance
    of Key-Value stores. The implementation of functions reveals the difference between
    the two storage engines. In-memory databases are relatively straightforward, as
    writing and releasing random memory space is simple. However, disk-based databases
    must contend with more complex procedures, such as data referencing, file serialization,
    and defragmentation, which can be challenging to implement. 3) Resource Management
    on Cloud Storage optimization: Cloud computing demands flexible storage capabilities
    as a service paradigm to ensure efficient data access. Q [87] introduced a dynamic
    shared memory management framework. This approach permits multiple virtual machines
    to dynamically access shared memory resources based on their requirements, enhancing
    inter-VM communication efficiency and VM memory swapping efficiency. Centaur [88]
    achieved dynamic partition caching for each VM through local replacement in each
    partition. This strategy results in lower cache miss rates and improved performance
    isolation and control for VM workloads, ensuring the efficiency and effectiveness
    of storage. Load Balancing: To enhance the concurrent processing capacity of cloud
    environments, ensuring stable throughput and response times necessitates comprehensive
    management of machine workloads. H et al. [89] proposed a CMLB cloud-related multimedia
    system. It presented a highly effective load-balancing technique. It considers
    the load of all servers and the network conditions, effectively addressing resource
    scheduling and allocation issues. Zhao et al. [90] approached the challenge from
    a different angle, focused on optimizing the selection of physical hosts to improve
    the direction of task requests. They propose a load-balancing method based on
    the LB-BC technique, which combines Bayesian algorithms with clusters to identify
    the most suitable physical hosts for executing workloads. This approach effectively
    maintains long-term load balance, reduces task failures, and significantly boosts
    throughput. B. Resource Management in BigData-GIS System Mainstream Big Data dedicated
    clusters and cloud computing platforms often require assistance to address complex
    diversification and hardware heterogeneity issues. 1) Task Scheduling for Mixed
    Workloads Task-DAG Schedulers are devised to allocate resources to interdependent
    tasks within a job, considering the overall task-DAG structure. However, achieving
    an optimal schedule that minimizes the makespan while considering task dependencies
    and diverse resource demands is challenging. Spear [91] is a scheduling framework
    geared towards reducing the manufacturing time of complex jobs while considering
    both task dependencies and heterogeneous resource requirements. Decima [92], conversely,
    is a system that utilizes reinforcement learning and neural networks to generate
    workload-specific scheduling algorithms, eliminating the need for detailed instructions
    beyond a high-level objective. Lastly, DelayStage [93] introduces a stage delay
    scheduling strategy that interleaves cluster resources in parallel stages, thereby
    improving cluster resource utilization and enhancing job performance. 2) Cluster-Oriented
    Multi-Resource Management Harvest CPU resource: Modern data analytics typically
    run tasks on statically reserved resources, prone to over-provisioning to guarantee
    quality of service (QoS), resulting in many resource time fragments. As such,
    the resource utilization of a data analytics cluster is severely underutilized.
    Shenango [94] offered comparable latencies while achieving far greater CPU efficiency.
    It reallocates cores across applications at a fine granularity of 5Âµs, enabling
    cycles that would otherwise have been wasted by latency-sensitive applications
    to be used productively by batch processing applications. Para [95] is an event-driven
    scheduling mechanism designed to harvest the CPU time fragments in co-located
    Big Data analytic workloads. Additionally, it identifies the idle CPU time window
    associated with each CPU core by capturing the task-switch event. Cluster Resource
    Scheduler: Although cluster schedulers have significantly improved resource allocation,
    the utilization of the allocated resources could be higher due to inaccurate resource
    requests. Elasecutor [96] is a novel executor scheduler for data analysis systems
    that dynamically allocates and explicitly adjusts resources to executors over
    time based on predicted temporal/varying resource requirements. Ursa [97] enabled
    the scheduler to capture accurate resource demands dynamically from the execution
    runtime and to provide timely, fine-grained resource allocation based on monotasking.
    In addition, Flinkcl [98] extends the capabilities of Flink from CPU clusters
    to heterogeneous CPU-GPU clusters, significantly increasing its computational
    power. 3) Resource Management on Cloud Cloud resource utilization: Resource underutilization
    is a pervasive issue in cloud data centers, which can be addressed by deploying
    multiple workloads in one cluster. To improve resource utilization while avoiding
    contention and performance interference, TPShare [99] proposed a simple yet effective
    vertical label mechanism to coordinate the time-sharing or space-sharing schedulers
    in different layers. Furthermore, the Scavenger [100] system was designed to function
    without offline profiling or prior information about the tenants’ workloads to
    ensure the QoS of latency-sensitive services and reduce the performance impact
    on batch jobs. Additionally, the CERES [101] system was developed to guarantee
    the QoS of latency-sensitive services and minimize the performance impact on batch
    jobs. Containerized Resource Management: ML workloads such as DL and hyperparameter
    tuning are computationally intensive and require parallel execution to reduce
    the learning time. KubeRay [102], an operator and suite of tools designed and
    built to create Ray clusters in K8s, provides an effective solution to meet this
    need with minimal effort. C. Resource Management in AI-GIS This section focuses
    on two core aspects: accelerating AI training and improving cluster resource utilization
    for DL workload types and heterogeneous cluster resources. 1) Application-Oriented
    Workload Types Deep Neural Networks (DNNs) continuously grow in size to improve
    the accuracy and quality of the models. Nevertheless, these strategies often result
    in suboptimal parallelization performance. Several research works have been proposed
    to accelerate distributed computing of DNNs. DNNs have seen remarkable growth
    in size to improve models’ accuracy and quality. However, this has led to suboptimal
    parallelization performance. To address this issue, researchers have proposed
    various initiatives to expedite the distributed computation of DNNs [103]. These
    initiatives range from algorithmic modifications to hardware and infrastructure
    optimization. Algorithmic techniques such as model parallelism, data parallelism,
    and federated learning have been proposed to enable distributed computation of
    DNNs. Moreover, special-purpose processors, distributed memory architectures,
    and high-speed interconnects have been proposed as hardware solutions to enhance
    the efficiency of distributed computation [104]. Additionally, cloud computing,
    edge computing, and container orchestration have been proposed as infrastructure
    solutions to provide an optimal platform for distributed computation [105]. All
    of these initiatives have the potential to accelerate the distributed computation
    of DNNs significantly. As the burgeoning trend of graph-based DL, graph neural
    network (GNN) have demonstrated exceptional capability in generating high-quality
    node feature vectors. However, existing one-size-fits-all GNN implementations
    must keep up with the ever-evolving GNN architectures, ever-increasing graph sizes,
    and diverse node embedding dimensionality. To address this challenge, GNNAdvisor
    [106] is an adaptive and efficient runtime system designed to expedite various
    GNN workloads on GPU platforms. Moreover, DistGNN [107] optimizes the Deep Graph
    Library for full-batch training on CPU clusters, featuring an efficient shared
    memory implementation, communication reduction with a minimum vertex-cut graph
    partitioning algorithm, and communication avoidance with a family of delayed-update
    algorithms. The transformer algorithm has revolutionized the field of NLP in recent
    years. Unlike Recurrent Neural Network models, transformers can process sequences
    of any length in parallel, improving accuracy when dealing with long sequences.
    However, deploying efficient online services in data centers equipped with GPUs
    is still a challenge. Fang et al. [108] developed TurboTransformers, a transformer
    serving system composed of a computing runtime and a serving framework. ET [109]
    proposed a novel self-attention architecture and an attention-aware pruning design.
    2) Cluster-Oriented Resource Management GPU Sharing Strategy: GPUs currently stand
    as the predominant accelerators for DL applications. However, a significant drawback
    arises from the fact that many existing cluster schedulers treat GPUs as non-shareable
    devices. This results in certain tasks needing to fully occupy the resources of
    a single GPU, consequently diminishing the resource utilization rate of the entire
    cluster and influencing the average completion time of tasks [110]. An effective
    GPU-sharing strategy should maximize resource sharing while ensuring isolation
    between tasks. To achieve optimal resource sharing, both GPU computing resources
    and GPU storage resources should be shared to the greatest extent possible. This
    approach allows for accommodating more tasks on a single GPU. Simultaneously,
    to guarantee isolation, efforts should be made to minimize the impact between
    tasks, thereby reducing the influence on execution time and overall throughput
    [111]. Heterogeneous Clusters Scheduling: The advancement of hardware technology
    has made heterogeneous devices in clusters inevitable. Consequently, allocating
    all hardware devices as the same resource will lead to an inevitable loss of efficiency.
    To address this issue, it is necessary to explore resource scheduling strategies
    based on heterogeneous clusters, analyze the execution efficiency of different
    tasks on different hardware devices, and optimally allocate resources to avoid
    unnecessary waste [112]. 3) Resource Management on Cloud Container Technology:
    Nowadays, the most widely used cluster management solution is K8s, based on Docker
    containers. However, the technology for running AI training tasks in the Docker
    environment still needs to be developed. Moreover, these cluster management solutions
    cannot sense the underlying hardware, reducing distributed training efficiency
    and underutilizing cluster GPUs [113], [114]. To optimize the execution time of
    DL in Docker containers and improve the utilization of GPU clusters, Yeh et al.
    [115] proposed KubeShare, an extension of K8s that enables GPU sharing with fine-grained
    allocation. In addition, kube-Knots [116] can dynamically harvest spare compute
    cycles through dynamic container orchestration, allowing for the co-location of
    latency-critical and batch workloads while improving overall resource utilization.
    In FaST-GShare [117], an advanced spatio-temporal GPU sharing framework crafted
    for serverless computing in DL inferences, the FaST-Manager takes center stage.
    This strategic component adeptly limits and isolates spatio-temporal resources,
    optimizing GPU multiplexing efficiency. D. Resource Management in Cloud-GIS System
    This section discusses the research progress of traditional cloud computing technologies.
    Subsequently, the focus is on recent research hotspots in cloud-native systems.
    1) Traditional Cloud Computing Technologies The current landscape of cloud services
    extends beyond distributed computing, encompassing distributed computing, utility
    computing, load balancing, parallel computing, network storage, hot backup redundancy,
    and virtualization. While traditional cloud computing has matured over the years,
    the emergence of new tools and frameworks has prompted further research and optimization.
    Efficient resource management and task allocation under a certain hardware resource
    are critical for achieving significant performance improvements in the cloud.
    Li et al. [118] introduced the Load Balancing Ant Colony Optimizations algorithm
    that reduces task set generation time while maintaining scheduling flexibility.
    Kumar et al. [119] enhanced scheduling performance by combining min–min, and max–min
    methods with a standard genetic algorithm, improving traditional scheduling performance
    and resource utilization. 2) Cloud Computing 2.0 Based on Cloud-Native cloud-native
    is wholly managed by a third party, is event-driven, stateless, and briefly kept
    in a compute container, whereas traditional cloud computing requires manual infrastructure
    building. Serverless deployment applications, derived from the benefits of cloud-native
    architectures, can automatically construct, deploy, and initiate services without
    additional infrastructure building. Serveless: To enhance response time and decrease
    latency while optimizing costs through serverless resource scheduling, current
    strategies are centered on minimizing cold start time or reducing the frequency
    of cold starts. Cloud functions typically operate on pre-allocated virtual machines
    and containers, allowing for swift launches. However, functions reliant on extensive
    packages exhibit slower starts, diminishing the application''s resilience and
    responsiveness to sudden increases in load. Abad et al. [120] introduced a package-aware
    scheduling algorithm to mitigate this challenge by assigning functions with similar
    package requirements to the same worker nodes. This approach amplifies packet
    caching hit rates, consequently reducing the latency of cloud functions. Additionally,
    the load on worker nodes is monitored to prevent it from surpassing a configurable
    threshold. Building on this, Oakes et al. [121] proposed a shared package cache
    to expedite the start-up time of cloud functions further, resulting in a substantial
    reduction in response time and latency. Virtualization: Due to the unique characteristics
    of serverless applications, such as bursty, variable, and stateless behavior,
    existing platform scheduling mechanisms often need to catch up. The prevailing
    strategy is to enhance resource utilization dynamically and optimally, given these
    challenges. In pursuit of this goal, Kaffes et al. [122] introduced a centralized
    approach to eliminate queue imbalance, mitigate interference with core-based granularity,
    and propose a centralized kernel granularity scheduler. This design offers the
    advantage of directly assigning ready-to-execute functions to idle working cores,
    minimizing unnecessary queuing, eliminating queue imbalances and improving overall
    platform resource utilization. In a parallel effort, Jiang et al. [123] presented
    a hybrid job assignment approach to enhance workflow resource utilization. Their
    approach considers the resource consumption patterns of various workflow execution
    phases, culminating in the proposal of a serverless workflow execution system.
    The outcome is significantly more efficient execution of large-scale scientific
    workflows on public clouds than the traditional cluster execution model in extensive
    large-scale tests. Containerization: The scale of containerization technology
    often involves hundreds or even thousands of servers, leading to a noticeable
    performance impact due to bandwidth and communication overhead between networks.
    There can be network bandwidth bottlenecks in scenarios with bursty workloads,
    particularly when hundreds of virtual machines hosting cloud functions utilize
    the same container images. To address this challenge, Wang et al. [124] introduced
    FaaSNet, a lightweight, adaptable function tree structure that facilitates scalable
    container provisioning. Thomas et al. [125] concentrated on the total startup
    time, encompassing the duration required for initiating and establishing connections
    among a group of containers. Their proposed particle model, albeit compromising
    isolation, significantly reduces the interconnection time of containers. SECTION
    V. Applications Geoscience, a broad phrase that encompasses specific subjects
    such as climatology, hydrology, and others, is separate from the sphere of social
    activity, spanning a spectrum from epidemiology to commercial businesses. Geosciences
    are mostly concerned with offline examination of RS data and structured information
    gathered from sensors. Social activity revolves around the real-time study of
    geographic data generated by human activities. It divides into two segments in
    the setting of STBD: one reflecting the physical environment and the other repeating
    the rhythms of human-social activity. To demonstrate the breadth and diversity
    of this data ecosystem, we’ve compiled a list of 15 open-source datasets relevant
    to STBD, which is shown succinctly in Table VIII. TABLE VIII Representative Spatio-Temporal
    Big Data Dataset A. Geographic Environment As Earth observation technology advances,
    there has been a remarkable increase in the volume of RS data. The basic STBD
    comprises a myriad of types: from vector and image data to geographic entity details,
    place names, address data, 3D model datasets, and mapping product information,
    all paired with their relevant metadata. The multifaceted realm of geoscience
    calls for a robust, user-centric cloud platform to address the eclectic requirements
    of its practitioners. Acknowledging this, Varouchakis et al. [126] explored groundwater
    resource distribution, leveraging structured numerical datasets and classic modeling
    approaches. Concurrently, Le et al. [127] applied ConvLSTM to interpolate and
    prognosticate citywide air pollutants. While these endeavors capitalized on traditional
    analysis and processing modalities to frame databases and data processing infrastructure,
    the days of hands-on management could be better. Today''s environment, characterized
    by the synergy of Big Data and AI, heralds a transformative phase where integrated,
    one-stop platforms for data processing emerge as the norm. Ensuring data security
    and rigorous encryption, especially for delicate geographic datasets, stands central
    in this transformative journey. Addressing these imperatives will augment the
    efficacy and reach of geoscientific inquiry and catalyze groundbreaking applications
    and insights in the Earth observation domain. B. Human-Social Activities The rapid
    progression of Internet technology and the ubiquity of social media platforms
    have led to a daily influx of vast spatio-temporal data rooted in human social
    interactions. Within this realm, public thematic data encompasses a spectrum ranging
    from population statistics and macroeconomic insights to point-of-interest data,
    geographical censuses, monitoring records, and their corresponding metadata. Parallelly,
    the realm of IoT real-time perception data–time-stamped data harnessed through
    advanced IoT sensing–embraces real-time spatio-temporal information from Earth
    observation sensor networks, specialized sensor readings, real-time industry-shared
    themes, and their associated metadata. Tapping into the vast web ocean, internet
    online acquisition data is primarily sourced using tools like web crawlers, capturing
    diverse online data streams. Social activity data analysis is now at the forefront
    of research, with wearable sensors and Internet crawlers instrumental in accumulating
    STBD rich with user-generated content. Such data analyses offer profound insights
    for both social and commercial sectors. Castro et al. [128] adeptly used daily
    COVID-19 data to delineate spatio-temporal spread patterns across regions. For
    timely epidemiological assessments during public health crises, a seamless STBD
    flow is paramount. This calls for merging Big Data stream processing with AI predictive
    algorithms. Exploiting data distribution traits, expansive GIS analyses are undertaken.
    To heighten efficiency, computing resources are judiciously dispersed based on
    data''s temporal and spatial nature. A sleek computational framework emerges by
    refining cache processes and curtailing node communication overhead. In intelligent
    transportation, STBD has showcased pivotal advancements. Real-time storage and
    indexing of vehicular patterns and traffic data ensure swift spatio-temporal query
    responses for traffic oversight. Immediate, extensive video and sensor data analysis
    aids in swift traffic event detection and resolution. An accurate traffic prediction
    model emerges with DL algorithms, refining traffic light coordination and boosting
    traffic flow efficiency [129]. Cloud-native platforms have proven invaluable in
    intelligent transportation implementations across cities [130], fostering cross-city
    data collaboration, offering versatile services to traffic authorities, and realizing
    optimal resource exploitation. SECTION VI. Research Agenda In prior sections,
    we delved into STBD and its processing platforms, encompassing foundational attributes,
    spatio-temporal nuances, and resource management intricacies. This segment elucidates
    STBD''s potential in amplifying asset management efficiency across varied platforms,
    bolstering processing speed. Concurrently, we introduce emerging research avenues,
    providing in-depth insights and recommendations for future exploration. A. Research
    Agenda for DBMS-GIS This section examines how a compute-storage separation design
    affects remote data access. We improve dynamic memory consumption by combining
    local caching with cloud storage, forming a training data management system to
    increase efficiency during learning. We investigate the links between data storage
    strategies and training costs in the emerging Cloud 2.0 ecosystem, characterized
    by various metering schemes. We present a cost-cutting data storage technique
    for multi-cloud scenarios, resulting in significant consumer savings. 1) Dynamic
    Demand Based on Cross-Task Data Sharing System DL training in large high-performance
    computing clusters involves numerous nodes working together to read and process
    tasks. While data shuffling between training phases is common in DL jobs, direct
    data synchronization among multiple computer features is the best technique. Serverless
    models, such as AWS Lambda, present issues in effective inter-function communication
    and consistent data state retention between calls because of their limited memory
    and storage. To address this, we propose using shared local storage to collect
    intermediate data during cloud-based DL training, eliminating the requirement
    for external data persistence mechanisms like as databases, cloud disks, or Redis
    clusters and avoiding duplicate data transfers. We want to create a high-performance
    intermediary storage system that emphasizes dynamic data-sharing strategies and
    allows for smooth data synchronization across several job functions. We explore
    DL workload communication properties deeper to carefully hone low-latency, high-throughput
    inter-task data exchanges. Our ultimate goal is to create a scalable hybrid cloud
    storage infrastructure that combines cloud-centric data management with local
    storage, allowing real-time data sync via concurrent cloud storage operations.
    2) High-Performance Local Cache Method Based on Data Reuse Frequency Prediction
    DL''s voracious appetite for data necessitates notable storage throughput. This
    is centralized in container-supported computing functions, where thousands of
    serverless functions actively engage in intensive shared file system operations.
    This situation leads to increased latency, erratic read/write performance, and
    challenges in managing the rapid scaling of cloud-native apps, particularly during
    peak traffic periods. We’re designing a robust local cache management system to
    address this, accounting for data access patterns, latency, and volume. Our blueprint
    highlights two cache scenarios for training data: A dynamic cache file selection
    strategy for optimizing storage in skewed read/write ratios scenarios is unveiled.
    This involves real-time file access pattern analysis, determining if a requested
    file merits cache placement. This avoids caching files accessed only once and
    adjusts for frequently accessed files based on temporal patterns. A generational
    tech-based cache file update strategy is proposed in instances with an extensive
    cache footprint during training. This flexible approach enables tailoring with
    diverse cache replacement algorithms for optimal access performance. Concludingly,
    acknowledging that data significance varies across learning stages–typically high
    initially and tapering over time–we integrate lifecycle management for cached
    objects. This adjusts cache object lifecycles and predicts cache durations and
    access patterns, refining cache replacement while upholding its intrinsic strategies.
    3) Cost-Aware High-Performance Hybrid Storage Solution in Multi-Cloud Environments
    Stateless computational functions, while agile, grapple with challenges in preserving
    data locally, a pivotal facet for DL training. While the cache from Task 2 augments
    throughput and trims latency, it compounds both caching and training expenses.
    Moreover, its absence of automated storage oversight mandates cloud storage integration
    for expansive data training and enduring data preservation. Notably, cloud storage
    providers vary in service quality and pricing. The versatility of multi-cloud
    storage–leveraging multiple providers based on cost-effectiveness and niche services–enhances
    user satisfaction, cuts costs, and sidesteps vendor dependency. Without a judicious
    storage strategy, there''s a tangible risk of squandering cloud resources in this
    landscape. Therefore, an astute storage approach is paramount for cost efficiency
    and resource conservation in cloud environments. B. Research Agenda for BigData-GIS
    This section examines the fluctuating computational and data requirements during
    DL training phases. We introduce an adaptive power management strategy tailored
    to enhance the efficacy of modern cloud-based training systems. Considering the
    diverse pricing structures in the Cloud 2.0 epoch, we delve into the relationship
    between computing power distribution and overall training expenses. This leads
    us to formulate a cost-optimized data storage algorithm for multi-cloud contexts
    to reduce user costs. 1) Dynamic Computing Resource Allocation Based on Real-Time
    Resource Requirements DL exhibits pronounced variations in resource consumption,
    highlighting the need for swift resource provisioning at training onset and efficient
    resource release post-training. Given that resource demands, data processing,
    and duration vary across training and inference tasks, there''s a pressing need
    for platforms with elastic resource provisioning. As the number of training iterations
    rises, most models demand fewer resources. Hence, there''s a need for resource
    allocation that dynamically adjusts to optimize usage as training progresses.
    Unlike traditional systems where users are constrained by static configurations,
    our proposed system introduces a dynamic resource model for learning training.
    This model ensures full resource utilization by adjusting allocations per iteration
    and rebalancing resources based on diverse strategies and task priorities. This
    leads to an enhanced scheduling mechanism and augmented system throughput. 2)
    Task Scheduling Based on Data Dependency and Network Topology Awareness In breaking
    down DL training tasks into serverless functions, each function intertwines with
    a series of dependent services. Changes in runtime parameters or serverless function
    scheduling can ripple through this dependency chain. We want to add the scheduler''s
    data dependency awareness to address this. This enhancement will be achieved by
    interfacing with a global parameter configuration center, allowing the system
    to dynamically gauge each event''s impact on every training subtask. The system
    can judiciously choose the optimal scheduling strategy by evaluating potential
    state transition graphs, minimizing resource wastage. Moreover, on many function-as-a-service
    platforms, functions are encapsulated within containers, which run atop either
    virtual or physical hardware. While a server node may house multiple container
    services, the container''s view is restricted to its network, devoid of the underlying
    machine''s IP awareness. This must be clarified to understand data distribution''s
    physical layout, affecting computational and data locality. We’d suggest embedding
    a network physical topology module in the scheduling system to reduce this. This
    module would dynamically map physical IPs to container IPs. Hence, when stateful
    services are scheduled, they can reference this mapping to identify an available
    physical machine. This ensures that computational tasks are routed based on the
    physical data storage layout, leveraging data locality for performance enhancement.
    3) Cost-Estimated Elastic Configuration for Computational Functions The era of
    Cloud 2.0 has seen serverless computing revolutionize resource management, bringing
    enhanced pricing models, granular resource metrics, and significant benefits,
    especially for DL training architectures. Given the periodic fluctuations of DL
    workloads and short-term unpredictable load spikes, shaping an adaptive resource
    configuration for emerging cloud functionalities is a complex task. We propose
    a cost-centric computational resource allocation strategy tailored for intensive
    DL contexts to navigate these challenges. Central to this strategy is the judicious
    selection of billing methods. In essence, serverless computing platforms typically
    offer two payment modes: Prepaid Resources At a lower unit price. Postpaid Resources
    are priced higher per unit. Aligning the right payment mode with the application''s
    resource consumption pattern can yield substantial cost benefits. Our approach
    hinges on real-time serverless computing usage monitoring, underpinned by stable
    and elastic computational resource usage prediction models. Using this foundation,
    the strategy dynamically tweaks the prepaid resource ratio, ensuring optimal cost
    benefits by favoring the cheaper option wherever possible. A responsive approach
    is favored for short-term, unpredictable, and bursty computational demands. Using
    feedback-controlled adaptive resource management, our method swiftly reconfigures
    virtual resources across nodes, offering quick reactions to load alterations while
    adhering to service objectives. In contrast, our strategy leans on statistical
    and ML techniques for long-term, cyclical DL workloads that exhibit predictable
    patterns. These tools dive deep into system logs and extensive load change datasets
    to construct performance blueprints aligned with prolonged load trends. This offers
    a backbone for making informed decisions on cost-effective, large-scale computational
    configurations. Complementing this, we have crafted an adaptive minimum-cost framework
    for optimal function placement. C. Research Agenda for AI-GIS This section discusses
    the new issues of scheduling DL training activities within GPU clusters. We provide
    a multi-tiered, granular cluster scheduler to bridge the gap between basic hardware
    and the complicated demands of upper-tier computing jobs. Anchored by GPU sharing
    frameworks and different resource clusters, our focus is divided into two main
    goals: accelerating DL training and maximizing cluster resource efficiency. The
    scheduler''s design promotes GPU sharing paradigms, fine-tunes distributed training
    task distribution, and leverages the capabilities of various devices to accelerate
    training efforts. 1) GPU Sharing Strategy and Fine-Grained Scheduling Resources
    Heterogeneous clusters amalgamate diverse hardware devices, offering computational
    prowess. These clusters proffer benefits like cost-efficient high-performance
    computing, robust scalability, and optimal computational resource use. They also
    cater to diverse computational architectures, ensuring holistic satisfaction.
    Yet, managing resources and choreographing tasks emerge as pressing issues. To
    navigate these intricacies, we propose a nuanced scheduling paradigm tailored
    for clusters interspersing CPUs and an array of GPUs. This tripartite framework
    encapsulates a task scheduling model, a device evaluative mechanism, and a central
    scheduler. As an initial step, DL training tasks undergo preprocessing to glean
    vital metrics. Once extracted, these tasks segue into a system task reservoir,
    biding their time for scheduling. Simultaneously, the device evaluation mechanism
    periodically assesses the capabilities of each device. The scheduler, acting on
    real-time resource availability and task prerequisites, cherry-picks the most
    fitting task for execution from this reservoir. 2) Scheduling Optimization Based
    on Heterogeneous Clusters GPUs, the cornerstone for accelerating DL training,
    are increasingly harnessed for real-time tasks. Many tasks may only tap into a
    fraction of a GPU''s vast capabilities. Thus, the move towards multi-tasking on
    a singular GPU has gained momentum. Nvidia''s Multi-Process Service (MPS) champions
    this cause by enabling concurrent task execution on a single GPU. However, its
    Achilles’ heel is inter-task interference. Should one task falter (including premature
    termination), it cascades a ripple effect, leading to the failure of other cohabiting
    tasks on that GPU. This underscores the imperative for a failsafe GPU-sharing
    framework. Our proposition pivots on a dual-faceted GPU-sharing model, integrating
    both spatial and time-sharing elements: Spatial Sharing: In a scenario where myriad
    tasks operate concurrently on one GPU, MPS offers a rudimentary spatial sharing
    foundation, albeit with its limitations concerning task isolation. To address
    this, we delve deeper into the intricate architecture of the GPU. Tasks are then
    tethered to designated Streaming Multiprocessors and memory portions. This strategic
    allocation naturally engenders task segregation, sidestepping the pitfalls of
    MPS. Time-Sharing: This dimension focuses on temporally segmenting GPU execution
    and allotting tasks predicated on strategic metrics. To lay the groundwork, we
    first dissect the characteristics intrinsic to DL training tasks, including their
    GPU computational demand, memory footprint, and intercommunication intervals.
    A tailored scheduling framework emerges for our segmented GPU using these metrics
    as guiding beacons. Both methodologies effectively fragment the GPU into finer
    units, equipping subsequent scheduling algorithms with a granular toolset. The
    cluster''s multitasking potential surges by doing so, allowing for concurrent
    task execution. This symbiosis not only amplifies task equity but also curtails
    task queuing durations and overall execution timelines. 3) Optimization Strategy
    for Distributed Based on Container Technology. Distributed DL offers shortened
    training periods by dispersing computing needs over numerous nodes. However, the
    distribution''s fundamental essence, model parameter synchronization, provides
    an inherent overhead. Communication between nodes, essential for ensuring model
    coherence, extends the time we want to save. As a result, the spatial positioning
    of these computational nodes emerges as a critical factor of distributed DL effectiveness.
    Our strategy unfolds in strategic steps: (1) Task Characterization: The locational
    sensitivity of each DL task introduced is originally assessed. Due to the wide
    range of sensitivities, tasks are divided into separate groups. These identified
    metrics are saved in a special database. (2) Hardware Topology Understanding:
    Our database includes a comprehensive mapping of the hardware topology. This information
    reservoir serves as the North Star for later scheduling methods. Given its container-centric
    architecture, the current paradigm often exhibits shortsightedness, emphasizing
    the need for a deeper understanding of the underlying hardware. The inefficiency
    in training and inefficient GPU utilization that results is palpable. (3) Holistic
    Scheduling method: Our scheduling method is a wise blend of depth and dynamism,
    proposing a synthesis of task qualities and hardware topology. A multi-tiered
    analysis provides depth by addressing job peculiarities and the diverse hardware
    landscape. Dynamism derives from the realization that when resources are regained,
    fragmentation becomes a potential concern. Due to this, our plan incorporates
    regular recalibration, enabling us to continuously optimize task assignments.
    This frequent reshuffling reduces resource fragmentation while simultaneously
    improving the fluidity and efficacy of resource scheduling. This scheduling strategy,
    anchored in data-driven insights and characterized by adaptability, promises a
    harmonious marriage of computational and communicative demands and sets the stage
    for efficient and responsive distributed DL architectures. D. Research Agenda
    for Cloud-GIS This section delves into the complexities associated with cloud-native
    computing in the context of next-generation Big Data processing and AI training.
    We introduce data management, resource distribution, and task coordination strategies.
    We aim to craft a robust theoretical framework, grounded in cloud-native service
    principles, that bolsters efficiency in cloud-centric Big Data processing and
    AI training. This framework promises marked improvements in computational throughput
    and cost-effectiveness. The implications of this approach are particularly pronounced
    in sectors like public safety, intelligent transport, and advanced healthcare.
    1) High-Performance Data Management Based on Stateless Services We propose a storage
    service built on local storage, tailored to efficiently back stateful services,
    including database services (e.g., MySQL) and distributed storage, within a stateless
    cloud-native framework. All storage resources are streamlined into a cloud storage
    pool in this setup. Each node''s local storage is segmented into multiple persistent
    volume groups (PVGs). Containers can specifically request storage resources, termed
    as persistent volumes (PVs). Each PV, adaptable in size, can only be engaged by
    a single container. It supports both HDDs and SSDs and a tiered storage amalgamation.
    Stateful container services specify PV requirements (like size, IOPS, SSD, or
    HDD preferences) when initiating. The onus is on the scheduler to rapidly pinpoint
    an apt machine on the platform to accommodate the container. If any container
    falters, the scheduler identifies this malfunction, and leveraging the data''s
    physical topology, earmarks a suitable machine for reboot. STBD and AI training
    place considerable demands on storage throughput. As computational functions within
    container clusters centrally process vast amounts of data and trigger thousands
    of serverless services, they exert significant pressure on shared file systems
    with intensive read/write operations. Such a burden results in increased latency,
    spikes in high latency, compromised read/write stability, and challenges in managing
    peak traffic within short durations. To combat this, we propose a robust local
    cache management method factoring in data usage frequency, access expiration,
    and data size. We’ve conceptualized designs for two primary training data cache
    contexts: We propose a dynamic cache file selection strategy for scenarios experiencing
    low cache storage utilization due to minimal read/write ratios. This strategy
    undertakes real-time analysis of commonly accessed files’ size and nature. It
    discerns if requested files warrant caching, eschewing single-use files from cache
    placement. Further, it factors in file access frequency and access time intervals,
    minimizing errors in cache decisions. We suggest a cache file update strategy
    rooted in generational technology to elevate efficiency in cache replacements
    during data training scenarios with extensive cached files. This method tailors
    different cache replacement algorithms to suit various high-performance access
    needs. Acknowledging that data''s importance varies across learning stages–with
    heightened access initially that tapers over time–we envision amplifying our cache
    replacement strategy. We aim to integrate lifecycle management for cache objects.
    We can update this lifecycle to predict data cache duration and access frequency,
    refining cache replacement execution while preserving its innate attributes. 2)
    Dynamic Computing Resource Allocation Based on Real-Time Demand Cloud platform
    resources are logically pooled and segmented based on CPU, GPU, memory, storage,
    and network attributes. When scheduling a container, it specifies resource needs
    via orchestration files or configurations. Given the resource pool''s status and
    host resources, the scheduler must pinpoint an apt physical node for the container
    in milliseconds. The scheduler''s efficacy, speed, and balance influence cloud
    resource utilization in expansive cloud clusters. Existing schedulers in Big Data
    or AI platforms often need to improve with the unique demands of container applications,
    like their bursty nature, variability, and statelessness. Additionally, most current
    cloud-native schedulers focus primarily on the container platform, overlooking
    the diverse resource management needs of varying application loads. This oversight
    is especially glaring with the dynamic demands of STBD or AI training. Our proposed
    solution emphasizes the real-time perception of cluster states. By harnessing
    data and computation topologies, we advocate for a cloud-native service model-based
    scheduling approach. The goal is dual: achieving cloud computing elasticity and
    optimizing deep learning computing performance. This entails real-time, precise
    resource allocation and control, promoting adaptive service scheduling. It moves
    away from static strategies, offering flexible computational power control and
    dynamic resource allocation during iterative computations. STBD and AI training
    tasks have a pronounced resource consumption pattern, exhibiting notable peaks
    and troughs. It''s essential to quickly allocate computing resources during the
    initial phase of data training and efficiently reclaim them post-training to optimize
    resource use. Given the diverse resource demands of data processing, model training,
    and inference, there''s a pressing need for computing platforms with adaptable
    resource provisioning. As the number of training iterations for Big Data or AI
    models rises, their resource needs typically wane. Thus, resources for ML training
    must be adjustable in real time to optimize utilization throughout the task. Unlike
    traditional systems, where users set fixed configurations upfront, our proposed
    system introduces a dynamic resource consumption model for training tasks. It
    caters to each iteration''s specific needs, ensuring full resource utilization.
    Furthermore, dynamically rebalancing resource allocation based on strategies and
    job priorities augments the scheduler''s efficiency and amplifies overall system
    throughput. 3) Scheduling Computation Tasks Based on Data Topology Containers
    in cloud-native platforms designed for flexible and elastic scheduling often function
    within dedicated network spaces that remain concealed from the host network. Because
    a single server node might hold numerous container services, this configuration
    creates a disparity: more container IPs than host IPs. Big data or AI frameworks
    such as MapReduce, Spark, and TensorFlow have historically determined a service''s
    launch location based on the physical topology of data storage. However, in a
    containerized environment, jobs see only the container network, necessitating
    additional computation and data localization. To address this, we’ve built a network
    physical topology module inside the scheduler. This module keeps a live map of
    physical IPs and container IPs. As a result, when scheduling a stateful service,
    it can use this map to find the best physical location. Hosts in a containerized
    cloud environment can function as standalone networks, each hosting one or more
    sub-networks. These sub-networks provide the network addresses for containers.
    When a container is closed, its address is returned to the pool and made available
    for future allocations. Our application-aware data topology approach unfolds as
    follows: (1) When a containerized data application (like a Spark application)
    initiates a computational task, the scheduler identifies the container required
    (such as an HDFS data node) using dependency relationships. (2) The scheduler
    queries the physical node''s network and storage details where the container resides
    via the metadata module. This module, updated in real time, fetches the latest
    metadata from storage and network services. (3) Given that distributed storage
    often holds multiple replicas, the scheduler uses anti-affinity rules to ensure
    distribution across different physical nodes. This results in a list of potential
    physical nodes. (4) After considering each node''s resource consumption and load,
    the scheduler opts for the least loaded node to deploy the associated computing
    service container. (5) Since the computing and storage services coexist on the
    same host, they can employ local networking or domain socket mechanisms to expedite
    data transfer and boost performance. SECTION VII. Conclusion This article provides
    a comprehensive analysis of STBD processing platforms emphasizing resource management.
    We explore the evolution and key applications of STBD, segmenting its analytical
    ecosystem into DBMS-GIS, BigData-GIS, AI-GIS, and Cloud-GIS. While our primary
    focus is resource management, we also introduce emerging research areas. Our in-depth
    review aims to offer fresh insights for researchers, fostering the enhancement
    and optimization of STBD systems. Overall, our findings pave the way for scholars
    striving to deepen their grasp on the STBD analytical landscape. Authors Figures
    References Keywords Metrics More Like This Service Component Architecture for
    Geographic Information System in Cloud Computing Infrastructure 2013 IEEE 27th
    International Conference on Advanced Information Networking and Applications (AINA)
    Published: 2013 Analysis of a Joint Data Security Architecture Integrating Artificial
    Intelligence and Cloud Computing in the Era of Big Data 2022 4th International
    Conference on Smart Systems and Inventive Technology (ICSSIT) Published: 2022
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Big Data
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Survey on Spatio-Temporal Big Data Analytics Ecosystem: Resource Management,
    Processing Platform, and Applications'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gill S.S.
  - Wu H.
  - Patros P.
  - Ottaviani C.
  - Arora P.
  - Pujol V.C.
  - Haunschild D.
  - Parlikad A.K.
  - Cetinkaya O.
  - Lutfiyya H.
  - Stankovski V.
  - Li R.
  - Ding Y.
  - Qadir J.
  - Abraham A.
  - Ghosh S.K.
  - Song H.H.
  - Sakellariou R.
  - Rana O.
  - Rodrigues J.J.P.C.
  - Kanhere S.S.
  - Dustdar S.
  - Uhlig S.
  - Ramamohanarao K.
  - Buyya R.
  citation_count: '1'
  description: Over the past six decades, the computing systems field has experienced
    significant transformations, profoundly impacting society with transformational
    developments, such as the Internet and the commodification of computing. Underpinned
    by technological advancements, computer systems, far from being static, have been
    continuously evolving and adapting to cover multifaceted societal niches. This
    has led to new paradigms such as cloud, fog, edge computing, and the Internet
    of Things (IoT), which offer fresh economic and creative opportunities. Nevertheless,
    this rapid change poses complex research challenges, especially in maximizing
    potential and enhancing functionality. As such, to maintain an economical level
    of performance that meets ever-tighter requirements, one must understand the drivers
    of new model emergence and expansion, and how contemporary challenges differ from
    past ones. To that end, this article investigates and assesses the factors influencing
    the evolution of computing systems, covering established systems and architectures
    as well as newer developments, such as serverless computing, quantum computing,
    and on-device AI on edge devices. Trends emerge when one traces technological
    trajectory, which includes the rapid obsolescence of frameworks due to business
    and technical constraints, a move towards specialized systems and models, and
    varying approaches to centralized and decentralized control. This comprehensive
    review of modern computing systems looks ahead to the future of research in the
    field, highlighting key challenges and emerging trends, and underscoring their
    importance in cost-effectively driving technological progress.
  doi: 10.1016/j.teler.2024.100116
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Early
    computing to modern computing: A vision 3. Evolution of computing paradigms: Technological
    drivers 4. Classification of computing: Paradigms, technologies and trends 5.
    Impact and performance criteria 6. Emerging trends in modern computing 7. Summary
    and conclusions CRediT authorship contribution statement Declaration of competing
    interest Acknowledgments Appendix. List of acronyms Data availability References
    Show full outline Cited by (3) Figures (3) Tables (6) Table 1 Table 2 Table 3
    Table 4 Table 5 Table 6 Telematics and Informatics Reports Volume 13, March 2024,
    100116 Modern computing: Vision and challenges Author links open overlay panel
    Sukhpal Singh Gill a, Huaming Wu b, Panos Patros c, Carlo Ottaviani d, Priyansh
    Arora e, Victor Casamayor Pujol f, David Haunschild g, Ajith Kumar Parlikad h,
    Oktay Cetinkaya i, Hanan Lutfiyya j, Vlado Stankovski k, Ruidong Li l, Yuemin
    Ding m, Junaid Qadir n, Ajith Abraham o p, Soumya K. Ghosh q, Houbing Herbert
    Song r, Rizos Sakellariou s, Omer Rana t, Joel J.P.C. Rodrigues u v…Rajkumar Buyya
    y Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.teler.2024.100116
    Get rights and content Under a Creative Commons license open access Highlights
    • We explore the evolution of computing paradigms & technological drivers (1960
    onward). • We offer a taxonomy of modern computing based on impact and performance
    criteria. • We classify computing based on paradigms, technologies, impact areas,
    and trends. • We identify open challenges and research directions for computing
    traits. • We introduce a hype cycle for modern computing systems, spotlighting
    emerging trends. Abstract Over the past six decades, the computing systems field
    has experienced significant transformations, profoundly impacting society with
    transformational developments, such as the Internet and the commodification of
    computing. Underpinned by technological advancements, computer systems, far from
    being static, have been continuously evolving and adapting to cover multifaceted
    societal niches. This has led to new paradigms such as cloud, fog, edge computing,
    and the Internet of Things (IoT), which offer fresh economic and creative opportunities.
    Nevertheless, this rapid change poses complex research challenges, especially
    in maximizing potential and enhancing functionality. As such, to maintain an economical
    level of performance that meets ever-tighter requirements, one must understand
    the drivers of new model emergence and expansion, and how contemporary challenges
    differ from past ones. To that end, this article investigates and assesses the
    factors influencing the evolution of computing systems, covering established systems
    and architectures as well as newer developments, such as serverless computing,
    quantum computing, and on-device AI on edge devices. Trends emerge when one traces
    technological trajectory, which includes the rapid obsolescence of frameworks
    due to business and technical constraints, a move towards specialized systems
    and models, and varying approaches to centralized and decentralized control. This
    comprehensive review of modern computing systems looks ahead to the future of
    research in the field, highlighting key challenges and emerging trends, and underscoring
    their importance in cost-effectively driving technological progress. Graphical
    abstract Download : Download high-res image (336KB) Download : Download full-size
    image Previous article in issue Next article in issue Keywords Modern computingEdge
    AIEdge computingArtificial IntelligenceMachine learningCloud computingQuantum
    computingComputing 1. Introduction The Internet, the expansive computational backbone
    of interactive machines, is largely responsible for the 21st century’s social,
    financial, and technological growth [1]. The growing reliance on the computing
    resources it encapsulates has pushed the complexity and scope of such platforms,
    leading to the development of innovative computing systems. These systems have
    genuinely improved the capabilities and expectations of computing equipment driven
    by rapid technical and user-driven evolution [2]. For instance, vintage mainframes
    combined centralized data processing and storage with transmission interfaces
    for user input. Due to advancements in clusters and packet-switching technologies,
    microchip gadgets, and Graphical User Interfaces (GUIs), technology originally
    shifted from big, centrally-run mainframe computers to Personal Computers (PCs).
    The globalization of network standards made it possible for interconnected networks
    worldwide to communicate and share data [3]. Businesses slowly combined sensor
    and actuator goals with built-in network connectivity by creating architectures
    and standards that submit tasks to remote pools of computing resources, such as
    memory, storage, and data processing [4]. As a result, newer models like the Internet
    of Things (IoT) and edge computing are now beginning to expand the reach of technology
    outside the confines of traditional network nodes [5]. Over the past six decades,
    computing models have fundamentally shifted to address the problems posed by the
    ever-evolving nature of our civilization and its associated computer system architectures
    [6]. The evolution of computing from mainframes to workstations to the cloud to
    autonomous and decentralized architectures, such as edge computing and IoT technologies,
    however, maintains identical core parts and traits that characterize their function
    [7]. Research in computing underpins all of them! Advancements in areas like security,
    computer hardware acceleration, edge computing, and energy efficiency typically
    serve as catalysts for innovation and entrepreneurship that span across various
    business domains [8]. While computing systems and other forms of system integration
    create new problems/opportunities, software frameworks have been developed to
    address them. Thus, middleware, network protocols, and safe segregation techniques
    must be continually developed and refined to support novel computing systems—and
    their innovative use cases. 1.1. Motivation By tracking the effect of computing
    systems on the community, this comprehensive study seeks to (a) establish the
    essential features and components of modern computing systems, (b) thoroughly
    assess the development of innovations and behavioral patterns that inspired the
    invention of these paradigms, and (c) recognize significant developments throughout
    the models, such as the integration of system design, the shifting between centralization
    and decentralization, and lags in model conceptualization and development. This
    investigation suggests that next-generation computing systems will facilitate
    the decentralization of computational services. This will be achieved via the
    composition of decentralized calculation tools with workload-specific targets
    for performance to create dramatically more complex structures. These will satisfy
    holistic operational demands, such as improved capacity and power accessibility.
    1.2. Related surveys and our contributions Computing being a rapidly growing topic,
    the time is right for a novel, forward-thinking study to summarize, improve, and
    integrate the existing and newly-generated information, and to explore possible
    trends and future viewpoints. Previously, Pujol et al. [9] provided a survey on
    distributed computing continuum systems that focused on business models. Further
    back in 2018, Buyya et al. [1] presented a manifesto on fundamental issues, developments,
    and impacts in cloud computing research. Meanwhile, Gill et al. [4] offered a
    visionary survey of advances in computing paradigms for fog, edge, and serverless
    computing. Further, Shalf [10] summarized the 2020 state of the art of technological
    roadmaps and their implications for the future of systems, including what a post-exascale
    system would entail. Finally, in 2021, Angel et al. [11] reviewed leading computational
    frameworks for cloud and edge computing, and showcased breakthroughs that had
    been brought about via the merging of Machine Learning (ML) with these models.
    In order to evaluate and identify the most pressing research issues of modern
    computing, we have developed the very first taxonomy of its type. We performed
    a gap analysis of the current surveys using several criteria, as shown in Table
    1, which underpinned the design of our work. Hence, our study uniquely contributes
    by (a) exploring the history of computing paradigm shifts with a focus on technology
    drivers, (b) providing a thorough taxonomy of computing systems, (c) introducing
    the hype cycle for modern computing systems with a focus on new trends, and (d)
    discussing the effects and cost-effective performance requirements of modern computing.
    The key contributions of this article are summarized as follows: Table 1. Comparison
    of this work with existing studies. Work [9] [1] [4] [10] [11] Our Work Year 2023
    2018 2022 2020 2021 2024 A Taxonomy of Modern Computing ✓ Evolution of Computing
    Paradigms (1960 to 2023) ✓ Classification of Computing Standalone vs. Networked
    Computing ✓ General Purpose vs. Specialized Computing ✓ Centralized vs. Decentralized
    Computing ✓ Computing Trends and Emerging Technologies ✓ ✓ ✓ ✓ ✓ ✓ Computational
    Methodologies: Parallel vs. Sequential Computing ✓ Traits of Computing Focus/
    Paradigms ✓ ✓ Technologies/ Impact Areas ✓ Trends/ Observations ✓ Impact and Performance
    Criteria Performance Metrics ✓ Efficiency Metrics ✓ Social Impact ✓ Security and
    Compliance ✓ Economic and Management ✓ Open Challenges and Future Directions ✓
    ✓ ✓ ✓ ✓ ✓ Emerging Trends in Modern Computing: Hype Cycle ✓ • It offers a concise
    overview of the transition from early to modern computing. • The study explores
    the evolution of computing paradigms, focusing on technological drivers (1960–2023).
    • Following a novel methodology, the article produces a taxonomy of modern computing
    based on traits of computing such as (1) focus or paradigms; (2) technologies
    or impact areas; and (3) trends or observations. • It presents a comprehensive
    classification of computing: (1) Standalone vs. Networked Computing; (2) General
    Purpose vs. Specialized Computing, (3) Centralized vs. Decentralized Computing,
    (4) Computing Trends and Emerging Technologies; and (5) Computational Methodologies:
    Parallel vs. Sequential Computing. • The study identifies the impact and performance
    criteria of modern computing in terms of performance metrics, efficiency metrics,
    social impact, security and compliance, and economics and management. • It provides
    an in-depth summary of computing traits and resources for further research. •
    The article identifies open challenges and research directions for the traits
    of computing. • Finally, it introduces the hype cycle for modern computing systems,
    spotlighting emerging trends. 1.3. Article organization The article is organized
    as follows: Section 2 offers a concise overview of the transition from early to
    modern computing. Section 3 explores the evolution of computing paradigms, focusing
    on technological drivers. Section 4 presents a classification of computing systems,
    and Section 5 examines the impact and performance criteria in modern computing.
    The article concludes in Section 7, summarizing computing-related technologies
    and trends through a hype cycle in Section 6. The list of acronyms used in this
    study is given in Appendix. 2. Early computing to modern computing: A vision Over
    the last six decades, advancements in computing systems have optimized the efficiency
    of the available hardware [12]. Over this time period, novel computing models
    and innovations have been developed and replaced the previous state-of-the-art,
    all of which incrementally contribute to the current technology status [2]. Fig.
    1 shows the transition from early computing to contemporary computing. Originally,
    a single system could only carry out a single task; hence, a user needed various
    systems working in tandem to achieve their desired tasks. However, to safely share
    information between computers – in order to overcome the problem of executing
    only one task at a time – a reliable communication mechanism is essential [13].
    To that end, our investigation unfolds across three key sections: Section 3 delves
    into the evolution of computing paradigms, emphasizing technological drivers.
    Section 4 offers a comprehensive classification of computing systems. The discussion
    in Section 5 revolves around the impact and performance criteria of modern computing.
    Section 6 introduces the hype cycle for modern computing systems, spotlighting
    emerging trends. Download : Download high-res image (2MB) Download : Download
    full-size image Fig. 1. Modern computing: A taxonomy. 3. Evolution of computing
    paradigms: Technological drivers Fig. 1 illustrates the progression of computing
    technology starting from the year 1960. 3.1. Client server In the year 1960, a
    centralized platform (a.k.a, distribution integration) was developed to share
    workloads (a.k.a., jobs) between the resource providers (i.e., server instances)
    and service consumers (i.e., customers) [12]. Supporting it, a networking system
    was utilized for communications between client devices and servers, and servers
    exchange resources for customers to perform their tasks using a load balancing
    mechanism [14]. Illustrative examples of the client–server model’s application
    include the Email and the World Wide Web (WWW). However, users in this configuration
    were unable to freely interact with one another. 3.2. Supercomputer A supercomputer
    is a powerful computer with extraordinary processing capability, such that it
    can handle complex calculations in several areas of science, including climate
    study, quantum physics, and molecular simulation [15]. Energy utilization and
    heat control in supercomputers endured as a key research problem throughout their
    growth in the 1960s [16]. Supercomputers, such as Multivac, HAL-9000, and Machine
    Stops, have been instrumental in underpinning/enabling dramatic technological
    advancements [14]. 3.3. Proprietary mainframe To handle massive amounts of data
    (including dealing with transactions, customer data analysis, and censuses), a
    high-speed machine with large computing power is required [17]. Virtualization
    on mainframes allows for increased efficiency, protection, and dependability.
    In the year 2017, IBM announced the newest version of its mainframe, the IBM z14
    [13]. Being built to support massive economic activity and despite their high
    price tag, mainframe computers deliver outstanding efficiency [14]. 3.4. Cluster
    computing Cluster computing is a method of increasing the efficiency of a computing
    system by utilizing several nodes to complete a single operation [18]. In order
    to coordinate various computing nodes, this type of technology requires a rapid
    Local Area Network (LAN) for exchanging information among them [19]. 3.5. Home
    PCs The early days of the Internet coincided with the flourishing of PC kept at
    one’s home [3]. The Internet was evolving into a foundational network, connecting
    local networks to the larger Internet using self-adaptive network protocols, such
    as Transmission Control Protocol/Internet Protocol (TCP/IP)—in contrast to the
    original Network Control Protocol (NCP)-based Advanced Research Projects Agency
    Network (ARPANET) mechanisms [2]. As a result, there was a sharp increase in the
    number of hosts on the Internet, which quickly overwhelmed centralized naming
    technologies like HOSTS.TXT. In the year 1985, the earliest publicly available
    version of a Domain Name System (DNS) was released for the Unix BIND system [20].
    This system translates hostnames into IP addresses. Pioneer Windows, Icons, Menus,
    and Pointers (WIMP)-based GUIs on computers, such as the Xerox Star and the Apple
    LISA, proved that customers could successfully use machines in their homes for
    tasks like playing video games and surfing the Internet [21]. 3.6. Open MPP/SMP
    Massive Parallel Processing (MPP) and Symmetric Multi-Processing (SMP) systems
    are the two most common forms of parallel computing platforms [16]. In an SMP
    setup, multiple processors run the same Operating System (OS) concurrently while
    sharing the rest of the hardware’s capacity (e.g., disc space and RAM). Naturally,
    resource pooling influences the computational speed of completing a given assignment.
    In an MPP scenario, the file system can be shared, while no other resources are
    pooled for use during task processing [14]. Incorporating more machines and their
    associated storage and RAM space, increases the ability to scale according to
    the Universal Scalability Law (an extension of Amdahl’s Law), assuming the proportion
    of work that can be parallelized and the interprocess communication penalty: (1)
    3.7. Grid computing This technology enables a group to work together towards the
    same objective by executing non-interactive, and largely IO-intensive tasks [19].
    Each application running on only one grid is a top priority [12]. In addition
    to allocating and managing resources, grid computing also offers a reliable architecture,
    as well as tracking and exploration support. 3.8. WWW The primary web browsers,
    websites, and web servers all came into existence in the later stages of the 1980s
    and early 1990s, underpinned by the development of Hyper Text Transport Protocol
    (HTTP) and Hyper Text Markup Language (HTML) [2]. The platform for the interconnected
    system of networks that makes up the WWW was made possible by the standardizing
    technology of TCP/IP network protocols. This allowed for a dramatic increase in
    the total number of servers linked to the Web and introduced Information Technology
    (IT) to the general public. Software applications were thus able to communicate
    with one another beyond address spaces and networking, e.g., via novel technologies
    like Remote Procedure Calls (RPCs) [22]. 3.9. Commodity clusters Commodity cluster
    computing employs several computers simultaneously, which can inexpensively execute
    user tasks [19]. In an effort to standardize their processes, several companies
    use open standards while building commodity computers [14]. This allowed immediate
    computing business needs to be met using ready-made processors. 3.10. Peer to
    peer (P2P) P2P is a distributed framework to share workloads or jobs amongst multiple
    peers; alternatively, computers and peers may interact with one another openly
    at the application layer [23]. With no mediator in the center, users of a peer-to-peer
    system can share resources like memory, CPU speed, and storage space. Peer-to-peer
    communication utilizes the TCP/IP protocol suite [24]. Interactive media, sharing
    file infrastructure, and content distribution are some of the most common use
    cases for P2P technology. 3.11. Web services The technology supporting web services
    enables the exchange of data between various Internet-connected devices in machine-understandable
    data formats, such as JavaScript Object Notation (JSON) and Extensible Markup
    Language (XML), over the WWW [25]. Commonly, web-based services operate as a connection
    between end users and database servers. 3.12. Service-Oriented Architecture (SOA)
    The SOA paradigm enables software elements to be reused and made compatible through
    advertised service designs/Application Programming Interfaces (APIs) [26]. It
    is normally easier to include services in new apps: the apps can be architected
    to adhere to standardized protocols and leverage consistent design patterns. This
    frees the software engineer from the burden of recreating or duplicating current
    features or figuring out how to link to and interoperate with current systems—e.g.,
    via using Software Development Kits (SDKs) that implement common functionalities,
    such as networking, retries, marshaling of data and error handling [27]. Each
    SOA API exposes the logic and data necessary to carry out a single, self-contained
    business operation (such as vetting the creditworthiness of a client, determining
    the loan’s due date, or handling an insurance application) [28]. The loose integration
    provided by the service’s design allows for the service to be invoked with limited
    knowledge of the underlying service implementation. 3.13. Virtualized clusters
    Virtualization enables a guest computer system to be implemented on top of a host
    computer system, which abstracts away the problem of physically supporting and
    maintaining multiple types/architectures of physical machines [19]. With a virtualized
    cluster, several Virtual Machines (VMs) may pool their resources to complete a
    single job. VM hypervisors, which execute the guest system on the host system,
    allow software-based virtualization to run either on top of an OS or directly
    (bare-metal) on hardware [14]. Costs and complexity are reduced, and a greater
    number of tasks may be completed with identical hardware by adopting a VM-based
    system. 3.14. High Performance Computing (HPC) system HPC is the computing method
    of choice when dealing with computationally intensive issues, which tend to arise
    in the domains of commerce, technology, and research [14], [19]. A scheduler in
    an HPC system manages accessibility to the various computing resources available
    for use in solving various issues [29]. HPC systems utilize a pooled set of resources,
    allowing them to perform workloads or tasks via the allocation of concurrent resources
    and online utilization of various resources. 3.15. Autonomic computing One of
    the first global initiatives to build computer systems with minimum human involvement
    to achieve preset goals was IBM’s autonomic computing program in 2006 [30]. It
    was mostly based on research on nerves, thinking, and coordination. Autonomic
    computing research examines how software-intensive systems may make choices and
    behave autonomously to achieve user-specified goals [4]. Control for closed- and
    open-loop systems has shaped autonomic computing [31]. Complex systems can have
    several separate control networks. 3.16. Mobile computing The term “mobile computing”
    is used to describe a wide range of IT components that give consumers mobility
    in their usage of computation, information, and associated equipment and capabilities
    [32]. An especially popular definition of “mobile” is accessing information while
    moving, when an individual is not confined to a fixed place. Accessibility at
    a fixed spot may also be thought of as mobile, especially if it is provided by
    hardware that consumers can move as needed but that remains in one place while
    functioning [33]. Mobile computing devices are becoming essential across industries,
    boosting efficiency and creativity in fields such as healthcare, retail, manufacturing,
    and the arts. 3.17. Cloud computing Software as a service (SaaS), platform as
    a service (PaaS), and infrastructure as a service (IaaS) are all examples of Internet-accessible
    web services [1]. Google Mail is an excellent instance of a SaaS product since
    it provides a wide range of useful features without the burden of installation
    and ongoing upkeep costs. PaaS providers like Microsoft provide a scalable environment
    where users can install their applications [34]. Amazon is a prime instance of
    an IaaS provider since it provides users with access to servers, networks, storage,
    and other hardware components necessary to run applications and other workloads
    efficiently and effectively. Using distant facilities for performing user operations
    (processing, administration, and storage of data) over the Internet is known as
    “cloud computing”, abbreviated as “XaaS”, where X = “I”, “P”, “S”, etc. Cloud
    computing enables the pooling of resources to reduce execution costs and enhance
    service accessibility [35]. There are four major types of cloud computing systems:
    public, private, hybrid, and communal. Dependability, safety, and cost-effectiveness
    are just a few examples of Quality of service (QoS) characteristics that should
    be considered while developing a successful cloud service. 3.18. IoT Controllers,
    gadgets, and detection devices are all examples of IoT devices that can communicate
    with one another over the WWW [5]. IoT has many potential uses in many different
    areas, including farming, medical treatment, climate prediction, logistics, home
    automation, and industrial automation [36]. 3.19. Fog computing This cutting-edge
    design makes extensive use of mobile devices, also known as fog nodes, which are
    utilized for data storage and processing, and rely on the web for inter-node connectivity
    [37]. The data plane and the control plane are the two main components of fog
    computing [38]. Although the control layer is a gateway component and determines
    the network’s layout, the data plane offers capabilities at the network’s edge
    to decrease delay and boost QoS [39]. Fog computing supports IoT gadgets such
    as smartphones, detectors, and health monitors. 3.20. Edge computing Edge computing
    is a method that delegates processing to dispersed edge devices for data processing
    and information exchange [40]. In addition, edge computing enhances QoS, decreases
    delay, and lowers transmitting expenses by computing huge volumes of data on gadgets
    at the edge rather than in the public cloud [41]. However, edge computing relies
    on a constantly available web connection to perform certain tasks in a timely
    manner, so it is best used for applications that can execute autonomously without
    centralized control for prolonged periods of time [42]. 3.21. Serverless computing
    The serverless computing paradigm eliminates the need to manage servers and other
    infrastructure components [43] centrally. Since serverless computing eliminates
    the need for software engineers to manage servers, it is expected to grow much
    faster. With serverless computing, hosting companies may easily handle infrastructure
    management and automatic provisioning [44]. Because of this, less effort and resources
    are needed to oversee the infrastructure. 3.22. Osmotic computing Osmotic computing
    is a growing idea that merges IoT, cloud, fog, and edge technology for the constantly
    changing administration of IT services. The dramatic increase in the size of resources
    in the network’s periphery is the primary force behind this trend. By defining,
    creating, and implementing a computing model, this paradigm focuses on methods
    to improve edge and cloud-based IoT services [45]. To manage resources and resolve
    data difficulties in IoT and data science, osmotic computing applies the fundamental
    concepts of the osmosis phenomenon in chemistry [46]. The primary objective of
    this computing model is to distribute workloads and efficiently use available
    resources among servers without degrading service delivery or efficiency. 3.23.
    Dew computing Dew computing is “a software-hardware organization model for computers
    situated in the cloud computing environment”, where a local machine complements
    and operates independently of cloud services [47]. Dew computing may bridge the
    gap between cloud and on-premises computing. Data and services stored in the cloud
    are accessible regardless of an Internet connection. The need for constant Internet
    access is the primary restriction on cloud and fog computing. Complementing fog
    and edge computing with considerable Internet reliance, an extra layer, including
    dew computing, is necessary to keep apps and services alive and functioning. Even
    if dew computing is not conducted entirely online, it nevertheless uses cloud
    computing and depends on collaboration for data and operations, for example, One
    Drive [48]. 3.24. Quantum computing Quantum computing is a radically different
    way to analyze knowledge and data. Several possibilities can be taken advantage
    of when processing information stored in the quantum states of quantum machines
    that are unavailable when analyzing information in a conventional fashion [49].
    The phenomena of quantum entanglement and superposition are two such examples.
    Because of quantum entanglement, it is difficult to offer a comprehensive description
    from the understanding of merely the component states, which is a defining characteristic
    of quantum systems. One definition of the term “superposition” is the potential
    of merging quantum states to create a new valid quantum state [50]. The primary
    purpose driving the effort to construct a quantum computer was the modeling of
    quantum systems; however, it was not until the identification of quantum algorithms
    capable of achieving realistic objectives that the enthusiasm for constructing
    such devices began to garner increasing scrutiny [51]. 4. Classification of computing:
    Paradigms, technologies and trends In this section, we discuss the different types
    of computing and classify them into different broad categories as shown in Fig.
    1. Table 2 briefly describes traits of computing that are used in this classification
    such as (1) focus or paradigms; (2) technologies or impact areas; and (3) trends
    or observations. Table 2. Summary of computing traits. Trait Description Focus/Paradigms
    We discuss well-established computing paradigms, from client–server to quantum
    computing, which have been explored in the last decade. Technologies/Impact Areas
    We cover key research that has grown over time by utilizing these well-established
    computing paradigms and how this has led to many breakthroughs in the underlying
    technology. Trends/Observations The new trends, such as large-scale machine learning,
    digital twins, edge AI, bitcoin currency, 6G & Beyond and quantum Internet and
    biologically-inspired computing, for the next generation of computing, have come
    to light due to these advances in computing paradigms and technology. 4.1. Standalone
    vs. Networked computing Standalone computing occurs when a computer is not connected
    to another computer in any way, whether through wired or WiFi connections [52].
    Multiple computers linked together form a network, a model that falls under networked
    computing. 4.1.1. Standalone computing In this section, we discuss the main focus
    or paradigms, technologies or impact areas, and various trends or observations
    within standalone computing. 4.1.1.1. Focus/paradigms The following are the main
    focus or paradigms for standalone computing: (1) PCs: Individuals use PCs, which
    leverage microprocessors designed for personal use. Before the PC, businesses
    had to operate computers by connecting several users’ terminals to a separate,
    massive mainframe system [3]. By the end of the 1980s, technical developments
    had enabled the construction of a compact computer that a person could purchase
    and use as a word processor or for various computing objectives [2]. (2) Embedded
    Systems: A computer (often a microcontroller or microprocessor) is built into
    (i.e., embedded in) the design of a device [53]. Most of the time, an individual
    does not even realize they are using a computer because there might not be any
    obvious hints of applications, data, or software [54]. The software that operates
    a microwave oven or an engine control unit of a contemporary vehicle are two instances
    of items with undetectable integrated systems. 4.1.1.2. Technologies/impact areas
    The key technologies and affected domains for standalone computing include: (1)
    Single-board Computers (SBCs): In an SBC, the CPU, I/O, memory, and various other
    components are all housed on one integrated circuit board; the quantity of memory
    is fixed; and there are no slots to be expanded for additional hardware [55].
    (2) Raspberry Pi 4: The Raspberry Pi is a family of tiny SBCs that have been developed
    to allow programming and computing capabilities to be available to all. The Raspberry
    Pi Model B became the inaugural board produced by the foundation behind the Raspberry
    Pi [55]. Due to its immense popularity, other variants have subsequently been
    developed, each with its own set of advantages. These include the Raspberry Pi
    computation component, which has been optimized for use in embedded systems [56].
    (3) NVIDIA Jetson Series: This is a line of Graphics Processing Units (GPUs) that
    includes the initial processors built with the explicit purpose of powering self-driving
    robots [57]. With up to 32 Tera Operations Per Second (TOPS) of Artificial Intelligence
    (AI) efficiency, these GPUs efficiently handle optical measurements, sensor fusion,
    positioning, visualization, obstacle detection, and path-planning, all of which
    are essential for the development of robotics [55]. The Jetson Xavier series focuses
    on creating specialized robots and edge robots, with several distinct manufacturing
    components. 4.1.1.3. Trends/observations The main trends and observations regarding
    standalone computing are: (1) Adoption of AI/ML: NVIDIA Jetson Nano, for instance,
    enables consumers to equip billions of low-power AI/ML systems with remarkable
    new features [58]. It paves the way for a wide variety of integrated IoT services,
    such as low-cost Network Video Recorders (NVRs), consumer automation, and analytics-rich
    gateways [55]. With its ready-to-try applications and enthusiastic software developer
    community, Jetson Nano serves as the ideal tool for beginning students to gain
    knowledge about AI and robotics in real-life situations. (2) Cybersecurity: Embedded
    systems are compact, specifically designed devices built to carry out a single
    task, frequently in real-time, while using as few resources as possible [54].
    Installing protective measures on these platforms to guard against dangers like
    unauthorized usage or fraudulent attacks drives the need for embedded security
    [59]. These safeguards are included in electrical components, firmware, and applications
    to achieve an all-encompassing defense. 4.1.2. Networked computing In this section,
    we discuss the main focus or paradigms, technologies or impact areas, and various
    trends or observations within networked computing. 4.1.2.1. Focus/paradigms The
    following are the main focus or paradigms for networked computing: (1) Networking/Connectivity:
    Servers in cloud computing underpin the services and APIs provided to internal
    and external clients. Communication on several levels, both inside and among data
    centers, is essential for effectively implementing cloud services [1]. Crucially,
    networking ensures that all parts can talk to one another in a safe, frictionless,
    effective, and adaptable way. Many developments and studies in networking during
    the past ten years have focused on the cloud [60]. For instance, Software-Defined
    Networking (SDN) and Network Function Virtualization (NFV) aim to construct adaptable,
    versatile, and programmable computer networks to lessen the financial and time
    commitments of cloud service providers [61]. Scalability challenges have spurred
    several current developments in network design for the Cloud Data Centers (CDCs),
    as well as the necessity for a flat addressing space, and the excess demand for
    machines. Notwithstanding these developments, numerous networking issues require
    a resolution. The excessive energy consumption of modern CDCs is a major issue
    [60]. Especially because it is a common practice in data centers to have all networking
    equipment active at all times. Furthermore, unlike computing servers, most network
    parts (including switches, hubs, and routers), cannot be energy-proportionate;
    features like hibernation during periods of low traffic and connection-rate adaptability
    are not built in by default [62]. Consequently, the design and execution of approaches
    and technologies that seek to minimize network energy usage and make it proportionate
    to the incoming load continue to be outstanding issues. QoS assurance presents
    another complex challenge within CDC networks [63]. Service Level Agreements (SLAs)
    in modern clouds focus mostly on computing and storage. There is currently no
    way to encapsulate network performance constraints like latency and bandwidth
    assurances without resorting to “best effort” because no abstraction or method
    guarantees performance isolation. Providing network connections across widely
    dispersed resources (in other words, installing a “virtual cluster” encompassing
    resources in an amalgamated cloud setting), exacerbates this difficulty [64].
    However, there are numerous open challenges to deliver reliability assurances
    for these networks—due to packages needing to navigate the (public) Internet,
    including resources in various locations [65]. 4.1.2.2. Technologies/impact areas
    The key technologies and affected domains for networked computing include: 4.1.2.2.1.
    Internet of things (IoT) Devices (a.k.a., things) that can detect, control, and
    communicate are now routinely integrated with continuous control and monitoring
    functions via the Internet [1]. These devices have become ubiquitous in modern
    society, found in homes, on public transport, along highways, and in vehicles.
    Because of this, IoT applications may function in many contexts and provide a
    sophisticated evaluation and administration of complicated relationships [66].
    As a result, IoT devices and services may solve problems in many application domains,
    such as digital health, facility administration systems, production, and transportation.
    IoT-based systems have to deal with limited processing power, memory, and storage
    space because (i) platforms are constantly changing, so devices that join a network
    have to be able to adapt to these changes; (ii) devices differ in how well they
    work with computers and what features they offer; and (iii) to ensure the safety
    of the IoT data that has been acquired, a federated system is needed [5]. These
    days, popular IoT use cases include medical care, smart cities, climate prediction,
    water supply management, and highway surveillance, all of which leverage the capabilities
    of cloud, serverless, fog, and edge computing for processing user data to meet
    QoS requirements [67]. • Healthcare: Among the many significant IoT applications
    is medical care, which is designed to treat conditions including heart attacks,
    diabetes, cancer, COVID-19, and influenza [68]. For instance, a patient’s heart
    condition may be instantly diagnosed using a variety of medical devices in an
    interconnected IoT and computing environment [69]. Additionally, modern technology
    like Virtual Reality (VR) or AI can enhance the present healthcare system in the
    fight against inevitable pandemics [70]. • Agriculture: In order to forecast variables
    like yield, rainfall, and crop quality, the agricultural industry is making use
    of modern technology to analyze a wide range of data pertaining to agriculture
    [71]. One use case is the development of cloud-based agricultural systems that
    can autonomously forecast the state of agriculture using data collected from a
    variety of IoT or edge sensors. Additionally, to facilitate automated farming,
    an iOS or Android application is created to handle the massive amounts of data
    and supply the information they need to the agriculturalists through their edge
    devices [72]. • Smart Home: Owners may optimize energy consumption and offer the
    necessary protection with the deployment of cameras through the implementation
    of smart homes, which allow them to operate their home devices from their cell
    phones [36]. For instance, a resource management approach that incorporates cloud
    and fog computing may be used for controlling edge devices utilizing a smartphone
    application, which in turn regulates the room’s humidity, lighting, surveillance
    systems, fans, and voltage, such as via sensors connected to different household
    devices [73]. • Traffic Management: IoT is crucial in the efficient management
    of traffic through the use of a number of sensors and controllers [74]. To identify
    potholes, for instance, an IoT-based intelligent transportation system is created.
    In addition, its efficiency was assessed using a range of machine learning approaches
    and performance metrics [75]. Additionally, data may be processed swiftly using
    fog and edge computing methodologies to notify about potholes early, thereby reducing
    the likelihood of mishaps. • Weather Forecasting: Through the use of cloud computing
    and the IoT, scientists and weather forecasters may better gather data to inform
    their work [76]. Scientists have long relied on visual observations, data storage,
    and the public presentation of meteorological factors like air quality and moisture
    to better understand and explain these phenomena [77]. The findings may be made
    using an IoT system that relies on sensors and can transmit the results to the
    cloud. Cloud services have long been relied upon by IoT applications to handle
    processing and permanent storage. Still, as the number of ‘things’ proliferates,
    such services are increasingly unable to keep up with the real-time demands of
    IoT gadgets [78]. This is due to the high quantity of data and the short reaction
    times required by systems that operate in the real world over wide geographical
    areas. By moving resource orchestration from servers to edge networks, fog/edge
    computing expands the capabilities of cloud systems: Set up as a series of nested
    “cloudlets” that may perform data intake, processing, and administration [79].
    Compared to cloud services, geographically localized solutions use less power
    and allow for more mobile resources by decreasing reaction times and increasing
    intake bandwidth through horizontal scalability. These features make fog/edge
    computing a potential future architecture for IoT applications since this architectural
    model allows for scalability on a logical and geographical scale with near-instantaneous
    response latency [32]. By aggregating information from implanted and mobile gadgets
    and establishing mobile area networks, smart e-health apps can track information
    about patients in a continuous fashion [80]. By performing tasks like healthcare
    equipment noise filtering, data reduction and fusion, and analytics that identify
    harmful patterns in patients’ well-being, smart gateways gather and interpret
    data from devices locally [81]. At the same time, longer-term patterns may be
    evaluated at cloud levels. In addition, IoT systems supported by fog computing
    may adjust their actions based on the information they receive from sensors. For
    example, if a heart attack is recognized by initial processing at the fog layer,
    the intelligent gateway gathering signals from the defibrillator may adaptively
    boost the sample size before the attack. Similarly, the Industrial Internet of
    Things (IIoT) benefits from integrating edge, fog and cloud layers to provide
    specific and nearly real-time actions [82]. Smart grids and energy management
    are central to the Internet of Energy (IoE) paradigm. Coarse-grained information
    on network health may be gathered from dispersed networks of energy producers
    that track power usage, generation and/or battery life. While ‘Smart-Meters’ may
    communicate energy needs to service providers on a more detailed scale, monitoring
    capacity, generation, and use [80]. Therefore, IoT is a foundational technology
    for future systems, like electric automobiles and decentralized power grids [33].
    In addition, the increased safety, reliability, and durability of electricity
    distribution that this type of grid may provide can better satisfy the evolving
    needs of consumers. In-depth surveys are a good resource for IoT researchers who
    want to explore more. 4.1.2.2.2. Software-defined network (SDN) SDN transcends
    traditional network paradigms by separating control logic from the underlying
    hardware and centralizing network management [83]. This innovative approach facilitates
    programmable network architectures and streamlines management by distinctly segregating
    network policies, hardware implementation, and traffic forwarding [84]. Integral
    to cloud computing, SDN enhances communication and automates configurations, revolutionizing
    network adaptability and resource utilization in diverse environments [85]. NFV
    is another approach that utilizes software programs to perform traditionally hardware-based
    networking tasks, such as DNS, load balancing, and intrusion detection. NFV not
    only lowers costs but also enhances the flexibility of network functions and service
    responsiveness. Furthermore, VM consolidation in a virtualized network can help
    reduce energy costs by minimizing the number of VMs in operation [86]. SDN-based
    cloud computing optimizes network virtualization while decreasing electricity
    consumption. Crucially, SDN increases the abstraction of physical assets and automates
    and optimizes the setup process [85]. Many questions still need to be answered
    by scholars and investigators. First, ensuring data safety during transit across
    multiple cloud data centers is absolutely necessary for SDN-based cloud computing
    [87]. Second, even if SDN-enabled cloud infrastructures may be replicated, the
    balance between cost and energy use remains. Deploying SDN-based cloud computing
    systems is necessary to offer an economical network virtualization service with
    lower energy costs and greater dependability [83]. Furthermore, this may also
    boost data distribution and outcome collection by utilizing methods inspired by
    AI-based models, allowing us to expand existing information connectivity in such
    SDN contexts to accommodate blockchain-based systems. 4.1.2.3. Trends/observations
    The main trends and observations regarding networked computing are as follows:
    4.1.2.3.1. Intelligent edge The IoT connects billions of new devices, generating
    massive amounts of information that, inevitably, proves challenging to process.
    Over 41.6 billion IoT gadgets are estimated to be in operation by the end of 2025
    [88]. Increasing numbers of products, including connected autos, smart meters,
    and in-store sensors, are being created and installed by companies to improve
    customer experience while generating enormous quantities of data [4]. Meanwhile,
    this emerging data must be gathered, managed, and processed immediately. What
    exactly will this mean? Edge and fog computing might be a method for moving ahead.
    In the coming years, edge computing is forecast to receive far greater focus than
    fog computing. In contrast to traditional cloud computing, which analyzes data
    at a remote data center, edge computing performs so locally. In fog computing,
    it is possible to execute a portion of the work in the cloud, while edge devices
    perform other aspects [89]. Since computing at the edge uses far less network
    bandwidth than conventional computing, the data exchanged among connected devices
    could take a long time. Computing it nearby, either on the gadget itself or within
    a local network, will be more cost- and energy-effective. On the contrary, edge
    computing may provide cloud computing with much-needed support in coping with
    the vast volumes of data created by the IoT and other connected devices [90].
    Emerging IoT devices create and transport data across the fog and edge, and their
    processing power is leveraged to carry out processes that could otherwise be performed
    in the cloud. Hence, managing these systems with fog and edge, IoT devices and
    support from the cloud requires distributing the intelligence along the computing
    tiers, which leads to edge intelligent [91]. The terms “fog” and “edge” allude
    to these novel network nodes for IoT devices. Thus, they aid businesses in reducing
    their reliance on the cloud by transmitting information to analytics platforms.
    Businesses can lessen their dependency on cloud platforms for data processing
    and thereby reduce latency across networks by implementing edge and fog solutions
    [92]. This will allow rapid evidence-based recommendations to assist them in their
    decision-making process. Nevertheless, once real-time processing is complete,
    edge devices must transfer data to the cloud for statistics to be performed on
    it [93]. A company’s communication network is largely concerned with enabling
    various remote apps and providing endless storage space, thanks to cloud computing,
    connectivity, and computing capacity. That will ultimately alter data processing
    at the edge in real-time, which is essential for optimal data utilization [90].
    Future-proof network infrastructures will need to accommodate an unprecedented
    influx of smart devices. For real-time intelligence, it is crucial to have the
    decision-making process located close to where the data is produced. Self-driving
    automobiles and self-sustainable, smart factory equipment, for instance, require
    to be making split-second decisions [94]. Further, airline sensors collect data
    on engine efficiency in real time, allowing for predictive maintenance before
    a plane ever takes off. Potential cost reductions might be considerable. The more
    business connections an organization has, the more processing power and intelligence
    it can provide. 4.1.2.3.2. 6G and beyond The advancement to Industry 5.0 and the
    foundation of a technology-driven economy hinge on the development of Beyond 5G
    (B5G) and 6G networks. As communication and technological advancements increase,
    international industrial sectors will increasingly depend on 5G and B5G networks
    to provide revolutionary services and applications that will inevitably require
    ultra-low latency, unprecedented reliability, and continuous mobility [95]. Meanwhile,
    underpinned by Moore’s law, mobile devices have been rapidly adopting systems-technology
    co-optimization (STCO) and related system-building approaches, which departs from
    the conventional system-on-a-chip (SoC) approach [96]. Through cloud-based principles,
    including utilizing functioning between and among data centers, connecting in
    a micro-service setting, and concurrently offering reliable services and applications,
    it is expected that B5G/6G networks will be able to serve a wide variety of applications
    [97]. Both B5G and 6G networks aim to enable the smooth and complete integration
    of many industries, including the IoT, aerial networks (also known as drones),
    satellite accessibility, and submerged connectivity [98]. To keep up with this
    astonishing expectation, the next generation of networks (B5G/6G) will largely
    rely on cutting-edge AI/ML technology for intelligent network operations and administration.
    B5G and 6G infrastructures are anticipated to provide computationally intensive
    applications and services paired with infrastructure shifts [99]. Edge computing
    has received a lot of interest and is being evaluated as an integrated service
    in 6G networks to enable the two fundamental changes in network infrastructure
    and network services. While many studies have focused on features like cache services
    and compute offloading methods, little is known about mobile edge computing implementation.
    The necessity of moving forward with a software-centric strategy from the network
    core to the wireless layer was emphasized in the first efforts that contributed
    significantly to the conception of 5G [100]. As with 5G networks, 6G networks
    will depend heavily on SDN, which, together with NFV, represents a departure from
    the conventional hardware-centric strategy [9]. The mobile edge computing paradigm
    also encourages moving the base station (BS) and the core network functions to
    different places. BS functions are moved upstream to the cloud, and core network
    functions are moved downstream to the devices. The resulting boundary between
    the BS and the end device might be seen as an “edge” or “fog” domain [73]. While
    cloud computing has made it possible for users to access richer and more complicated
    apps by tapping into the resources of a remote cloud server, an alternative technique
    is needed to meet the extremely delicate latency criteria stated for use cases
    in 5G and maybe 6G [101]. This heterogeneous network design directly results from
    the complicated traffic distributions in today’s wireless networks. A wireless
    access point (AP), a macro BS, and a small cell BS are just a few examples of
    network access nodes that may provide stable and smooth connections for mobile
    users [102]. These network access nodes provide edge computing at network edges
    with less delay. The design of diverse mobile edge computing networks has gained
    more and more interest due to the varied properties of network access nodes, such
    as coverage capability and power transmission [101]. Nevertheless, it is imperative
    to carefully plan for the offloading of computing tasks to many access nodes in
    a network [103], [104]. In a heterogeneous network design, intelligently distributing
    tasks and resources among different nodes can substantially boost system performance
    [55]. For instance, by collaborating, the edge and cloud can elevate IoT tasks’
    QoS. While cloud servers manage compute-intensive tasks well, edge servers excel
    at processing tasks demanding minimal data or low latency [41]. Strategically
    assigning tasks among edge servers can redistribute work from overburdened nodes
    to less active ones, thus accelerating execution times. 4.2. General purpose vs.
    Specialized computing Leveraging fit-for-purpose software and given enough time,
    general-purpose computing (which includes desktop PCs, laptops, mobile devices
    like tablets and smartphones, and even certain televisions) can handle just about
    any computation [105]. A CPU, memory, input/ output systems, and a bus are the
    main parts of any general-purpose computing system. In contrast, integrated computers,
    are used in intelligent systems, and are often referred to as “special-purpose”
    computing systems. 4.2.1. General-purpose computing In this section, we discuss
    the main focus, paradigms, technologies, and impact areas, as well as various
    trends and observations about general-purpose computing. 4.2.1.1. Focus/paradigms
    The following are the main focus areas and paradigms associated with general-purpose
    computing: (1) Von Neumann Architecture: A computing device with a Von Neumann
    architecture has its main components – the CPU, memory, and I/O – connected via
    a single bus [106]. The efficiency of computers was greatly enhanced by the advent
    of this architecture, which provided effective means of storing and executing
    instructions. The fundamental idea behind this design is that data and instructions
    are handled in the same way. In other words, the data being handled and the program
    instructions themselves share the same storage and processing resources: a memory
    address can contain either an instruction to be executed or data; the software
    execution pathways decide how to interpret it [107]. This design substantially
    simplifies the framework and features of a computer, making it more accessible
    to both software engineers and non-technical users. (2) Integrated Computing:
    Compatibility throughout cloud applications and services is commonly achieved
    by implementing software adaptors and libraries and deploying application containers
    for computing to facilitate mobility [108]. Nevertheless, there is still a variety
    of challenges that have existed since the beginning of cloud computing but, due
    to their complexities, have not been adequately resolved yet [60]. One of these
    challenges is encouraging cloud connectivity without mandating a baseline set
    of capabilities for all services; ideally, customers can combine complicated features
    from several providers. Another area of investigation is how to develop cloud
    interoperability middleware that can facilitate the offering of complex services
    by composing more straightforward services from multiple 3rd-party providers [109].
    Such a high degree of abstraction would empower users to make service decisions
    based on their requirements, such as price, turnaround time, privacy, etc. This
    brings up an additional key area that needs further study: the manner in which
    to allow user-level middleware (intercloud and hybrid clouds) to discover potential
    services for an ensemble without assistance from cloud service providers [110].
    A strategy that relies on cloud providers working together is unlikely to be successful
    because their financial goals lie in keeping all the features they offer to their
    consumers (i.e., they have no incentive to help due to the fact that just a portion
    of the offerings in an ensemble are themselves). Consequently, the middleware
    that allows the melody of services must address challenges at both of its connections:
    Firstly, the middleware should seamlessly and abstractly provide the service to
    cloud users. Secondly, for the consumers, a service might be implemented in its
    entirety by sub-services offered by one vendor (maybe leveraging a 3rd-part SaaS
    organization able to offer the functionality), or it might be acquired through
    composing multiple services from various providers [111]. The provider user interface
    makes it possible to access complex functions without requiring special cooperation
    from providers [109]. The widespread use of cloud compatibility can offer commercial
    and financial advantages to cloud manufacturers, but frequently integrated clouds
    (which were achieved via Cloud Federation) cannot be realized until such time
    [112]. This calls for the development of intercloud markets, distinctive approaches
    to invoicing and accounting, as well as novel cloud-suitable pricing systems.
    4.2.1.2. Technologies/impact areas The key technologies and affected domains for
    general-purpose computing include: 4.2.1.2.1. Programming models Clusters are
    a type of parallel or distributed computational system that consists of a group
    of interconnected standalone computers that work collectively as a single integrated
    computing resource. Clusters and grids are platforms that communicate with each
    other to serve as a single resource [113]. A multi-core parallel architecture
    describes this form of capability, which is based on specific functions. Conversely,
    cloud computing emerged on top of clusters to abstract leveraging their computing
    resources and coordinate enormous data sets. A programming model is tightly coupled
    to where data is transmitted to manage an application’s functions. Important metrics
    to remember while building a programming model are efficiency, adaptability, goal
    architecture, and code maintainability [114]. Data analytics software often handles
    massive data sets that require many phases of processing. Certain steps have to
    be carried out in order, while others are executed simultaneously across several
    nodes in a cluster, grid, or cloud. The capacity of algorithms to perform statistical
    analysis on huge amounts of data will be crucial to unlocking achievements in
    industrial advances and next-generation scientific discoveries [115]. With the
    exponential growth of data comes the difficulty of organizing massive data sets,
    which in turn increases their complexity due to the ways they connect with each
    other. Its many processes include moving, archiving, replicating, processing,
    and erasing data. Data life-cycle complexities can be reduced via solutions that
    automate and improve data management activities. It has been shown that two limitations
    affect the data life cycle [116]. The framework used is the first limitation,
    initially regarding how it operates on data derived from consumers and apps. The
    second limitation derives from the observation that data is spread over several
    systems and infrastructures. That is why big data applications need to be capable
    of communicating amongst various systems that deal with the data and the effects
    that information and occurrences might have. The focus of this work is the second
    limitation, the big data infrastructure itself, and it includes a comprehensive
    analysis of the programming models and settings necessary to overcome this limitation.
    A programming model is underpinned by how quickly and smoothly its data is manipulated.
    A few elements to consider when creating a programming model include operation,
    adaptability, target designs, and the simplicity of maintenance code modification
    procedures [117]. For the sake of service, it is sometimes necessary to sacrifice
    at least one of these aspects. The exchange of computation for data storage or
    transmission is a usual instance of algorithmic manipulation. These difficulties
    can be mitigated by employing parallel methods and technology. A software engineer
    might undoubtedly leverage many variants of the same technique to enable distinct
    performance adjustments on different hardware architectures. Modern computing
    clusters comprise nodes with more than one CPU, and their hardware designs range
    from tiny to super powerful. 4.2.1.2.2. Virtualization With virtualization, the
    original physical object is replaced with a virtual one. The OSs of server infrastructure,
    hard drives, and PCs are some of the most typical targets for virtualization in
    a data center. Thus, virtualization decouples higher-level software and OSs from
    the underlying computing system [118]. VMs are a key component of hardware virtualization,
    standing in for a “real” computer running an OS. Emulating a computer system is
    what VMs do. A hypervisor makes a copy of the underlying hardware so that several
    OSs can share the same resources [119]. Despite being around for half a century,
    VMs are experiencing a surge in popularity because of the rise of the mobile workforce
    and desktop PCs. Server virtualization, which employs a hypervisor to effectively
    “duplicate” the underlying hardware, is a primary use case for virtualization
    technology in the corporate world [120]. In a non-virtualized setting, the guest
    OS generally works in tandem with the hardware [121]. OSs may be virtualized and
    continue functioning as if running on hardware, giving businesses access to similar
    performance levels while reducing hardware costs [122]. The majority of guest
    OSs do not need full access to hardware; therefore, even if virtualization efficiency
    is lower than hardware efficacy, it remains preferred. This means firms are less
    reliant on a single piece of hardware and have more leeway to make necessary changes.
    Following the success of server virtualization, other sections of the data center
    have also begun to implement the same approach. Virtualization technology for
    OSs has been around for generations [123]. In this implementation, the software
    enables the hardware to run several OSs in parallel. Companies that want to adopt
    a cloud-like IT infrastructure should prioritize virtualization. Using server
    resources more effectively is one of the primary benefits of virtualizing a data
    center [124]. Thanks to virtualization, IT departments may use a single VM to
    host a wide variety of applications, workloads, and OSs, with the flexibility
    to add or subtract resources as required easily. The use of virtualization allows
    firms to expand readily. Organizations may better monitor resource utilization
    and react to shifting needs using such systems. 4.2.1.2.3. Multicore processors
    For improved performance and more efficient use of energy, integrated circuits
    with several processing cores, or “cores”, are becoming increasingly common. Furthermore,
    these processors enable more effective parallel processing and multithreading,
    allowing for simultaneous processing of numerous jobs [125]. A computer with a
    dual-core arrangement is functionally equivalent to one with two or more individual
    CPUs. Sharing a socket between two CPUs accelerates communication between them.
    The use of processors with multiple cores is one technique to enhance processor
    performance while surpassing the practical restrictions of semiconductor manufacturing
    and design. Using several processors helps prevent any potentially dangerous overheating
    [126]. Multicore processors are compatible with any up-to-date computer hardware
    architectures. These days, multicore processors are standard in desktop and portable
    computers. Nevertheless, the actual power and utility of these CPUs depend on
    software built to leverage parallelism [127]. Application tasks are broken up
    into many processing threads in a parallel strategy, distributing and managing
    them over multiple CPUs. 4.2.1.3. Trends/observations The main trends and observations
    regarding general-purpose computing are as follows: 4.2.1.3.1. Software systems
    Web-based computing and Software Engineering (SE) are closely related disciplines.
    For instance, service-oriented SE provides various advantages to the software
    creation procedure and app development by merging the greatest elements of services
    and the cloud. In contrast to cloud computing, which is concerned with effectively
    transmitting services to consumers using adaptable virtualization of resources
    and load balancing, service-oriented SE is concerned with architectural design
    (service searching and composition) [128]. Customers and developers are both essential
    to the evolution of hardware innovations, which is why software engineering is
    a crucial discipline [129]. With the help of distributed computing and virtualization,
    customers may set up automatically managed VMs and cloud services for their initiatives
    and applications. Thanks to cloud services, teams working on software may now
    more easily collaborate on the development, testing, and distribution of their
    products. Here are some scenarios in which cloud computing might improve software
    engineering: The production timeline can be compressed [130]. As a result of the
    availability of ample computing resources made possible by cloud computing and
    virtualization, software engineers no longer need to rely just on a single physical
    computer. The time it takes to install the required applications may be decreased
    by retrieving cloud services, indicating that development activities can be performed
    with increased parallelism thanks to cloud computing. Third, VMs and cloud instances
    may substantially improve the setup and delivery procedures. Using sufficient
    virtualization resources from a private or public cloud, developers can speed
    up the building and testing process, which is otherwise, extremely time-consuming
    [123]. To circumvent this, a simplified system for managing code versions is required.
    In software development, code branches are used for refining and adding features.
    With cloud computing, there is no need to invest in or lease expensive hardware
    only to store some code. A distributed software engineering team may access apps
    more easily in a cloud setting, and service quality can be enhanced through dynamic
    resource allocation. As a result, the software construction process is streamlined
    thanks to cloud computing, which eliminates the need for development servers to
    rely on specific physical computers [129]. Nevertheless, there are obstacles when
    merging software engineering and cloud computing. The majority of the difficulties
    are with moving the data. Because different cloud providers use various APIs to
    offer cloud services, migrating software and data from one cloud to another while
    avoiding vendor lock-in is challenging. Avoiding over-reliance on any one set
    of APIs is one way to fix this problem while building and releasing applications
    in the cloud. The problem of dependability and accessibility is another obstacle.
    If everything is moved to the cloud, it will be difficult to retrieve the data
    if the cloud is compromised by hackers or affected by an unexpected calamity.
    The engineering teams are responsible for creating a local backup of their work
    [131]. Cloud computing allows software engineering academics to study multinational
    software development. Several investigations have examined the feasibility of
    using cloud computing to lower operational, delivery, and software development
    expenses. Researchers have investigated the feasibility of replacing services
    with a cloud-based platform for student-to-student knowledge exchange and collaboration
    [132]. Software systems have been supplanted by systems running on the cloud to
    reduce expenditures and maximize the utilization of resources. The conventional
    data management techniques have become increasingly cumbersome in the past few
    years due to the rapid increase in available data. A new frontier for study in
    software engineering has opened up thanks to the IoT, Blockchain (the distributed
    ledger), and ML/AI, with data management being the primary challenge [133]. These
    studies also provide a springboard for further study and innovative approaches
    to cloud data management, leading to the development of advanced technologies
    like Cisco’s pioneering fog computing [134]. Enterprise software developers are
    creating an abstraction layer, or “Blockchain-as-a-Service”, and selling it to
    other businesses as a subscription service [4]. These numerous new fields rely
    significantly on software engineering, yet they could not exist without it. 4.2.1.3.2.
    Simulations The capacity to carry out research, analyze strengths and shortcomings,
    and demonstrate viability is hampered in new or emerging computing domains due
    to the lack of mature technology and sufficient infrastructure. In many cases,
    the time and resources needed to acquire the necessary physical resources make
    it impractical to conduct the necessary research [79]. An alternative approach
    that can approximate a physical environment is a simulator. Additionally, simulation
    offers the ability to test suggested hypotheses in lightweight and low-cost settings.
    Real-world testing of novel methods is difficult and expensive because of the
    time and effort required to gather the necessary hardware resources (particularly
    for large-scale tests) and create the necessary software applications and systems
    [135]. Investigators demonstrate the viability of their ideas by modeling and
    simulation, and then conduct tests to confirm their concepts in a monitored environment
    utilizing simulation tools. Simulation software provides a convenient setting
    for testing solutions to real-world issues by allowing users to experiment and
    see what happens [136]. If a commercially available simulator is inadequate for
    user needs, then researchers should consider building their own, complete with
    graphical user interfaces. This is especially true if users need to simulate components
    of emerging computer architectures [137]. Researchers could benefit greatly from
    using a simulator to formulate questions and analyze different theoretical frameworks
    in simulated setups, therefore stimulating more research and fostering the development
    of communities within the relevant field. 4.2.2. Specialized computing In this
    section, we discuss the main focus or paradigms, technologies or impact areas,
    and various trends or observations within specialized computing. 4.2.2.1. Focus/paradigms
    The following are the main focus or paradigms for Specialized Computing: 4.2.2.1.1.
    Reconfigurable computing The modern paradigm of reconfigurable computing enables
    hardware components to swiftly alter their configuration and functioning in response
    to changing processing needs. Reconfigurable computer devices, such as Field-Programmable
    Gate Arrays (FPGAs), can be reprogrammed to perform a variety of different functions
    [138]. The main function of reconfigurable computing is to fill the void among
    general-purpose CPUs and Application-Specific Integrated Circuits (ASICs) [19].
    It allows hardware to be optimized for efficiency, power efficiency, and flexibility
    by matching application requirements. Static and dynamic switching are the two
    primary modes of operation for reconfigurable technology. In static reconfiguration,
    the component settings are adjusted prior to the computer starting to compute.
    However, dynamic reconfiguration permits hardware changes to be made while the
    system is running, allowing for dynamic modifications to hardware behavior. 4.2.2.1.2.
    Domain-specific architectures As computing and the digital transformation spread
    to various use cases, such as cloud (AI/HPC), networking, edge, the IoT, and self-driving
    cars, highly domain-specific computational tasks are making it more likely that
    Domain-Specific Architectures (DSAs) can enable big performance gains [139]. Using
    ChatGPT and other comparable software that are powered by large language models
    – which are fundamental to achieving generative AI – provides greater specialization
    inside AI workloads at extremely high volume, which motivates further hardware
    specialization [81]. DSAs, or application-domain-specific hardware and software,
    have substantial market potential. As a result of their superior performance on
    tasks that profit from a significant amount of parallel computing, such as AI
    workloads (learning and predicting), GPUs and Tensor Processing Units (TPUs) are
    currently controlling a sizable portion of the data center market [140]. Meanwhile,
    accelerations of 15–50 times the original speed, depending on the workload, are
    not uncommon. In the automobile industry, minimal latency and high-performance
    inference are provided via tailor-made solutions from industry leaders. 4.2.2.1.3.
    Exascale computing To handle the massive computations required by convergent modeling,
    simulation, AI, and data analysis, an entirely novel type of supercomputer called
    exascale computing has emerged [2]. This is motivated by advanced computational
    needs in science and engineering. Exascale computing (also supercomputing) becomes
    essential to expedite the generation of knowledge. Researchers and technologists
    may employ data analysis driven by exascale supercomputing to expand the frontiers
    of our existing understanding and promote breakthrough ideas. Supercomputing capabilities
    are in high demand as the world moves towards exascale computing to ensure continued
    scientific and technological advancements, while our civilization’s technological
    and scientific frameworks are progressing quickly thanks to exascale computing
    [141]. The immense potential of these tools necessitates their careful operation,
    especially as cultures worldwide undergo rapid changes in their moral frameworks
    and their perceptions of what it means to live sustainably. As such, novel responses
    to formerly intractable issues are being uncovered thanks to exascale computing.
    Exascale supercomputers are prohibitively expensive to construct; thus academics
    and scientists rely on funding to lease them instead of buying their own [142].
    Exascale computing systems produce enormous quantities of heat because of the
    volume of data they process. They require extremely cold environments to be stored
    in or unique cooling mechanisms built into the systems and racks themselves for
    optimal performance. Differentiating them from other types of supercomputers and
    quantum computers, they are computer systems with the largest capacity and most
    powerful hardware [143]. To further our understanding of the universe, exascale
    computers can model elementary physical processes like the granular interactions
    of atoms. Quite a few sectors rely on this capacity to analyze, forecast, and
    construct the world of tomorrow: for instance, better predict the weather, investigate
    in detail the interaction between rain, wind, clouds, and various other atmospheric
    occurrences, analyze their effects on one another at a molecular level and so
    on. Mathematical formulas can be used to determine the millisecond-by-millisecond
    effects of all forces acting in a certain environment at a specific time [144].
    These seemingly trivial interactions rapidly generate billions of possible permutations,
    which need trillions of mathematical equations to calculate and analyze. This
    kind of speed is only achievable on an exascale machine. By studying the results
    of these computations, researchers can gain a deeper insight into the nature of
    our universe [143]. Exascale supercomputers, despite their challenges, can literally
    increase our understanding, enabling us to address the problems of the future.
    4.2.2.1.4. Analog computing A novel approach may minimize errors in ultra-fast
    analog optical neural networks. Larger and more complicated machine-learning models
    need stronger and more effective computing gear. However, standard digital computers
    are lagging. Compared to a digital neural network, an analog optical network’s
    performance in areas like image classification and voice recognition is comparable.
    However, its speed and energy efficiency far exceed those of its digital counterparts
    [145]. Nevertheless, hardware faults in these analog devices might impact the
    accuracy of calculations. One possible source of this inaccuracy is microscopic
    flaws in the hardware itself [146]. Errors tend to multiply rapidly in a complex
    optical neural network. Even when using error-correction approaches, due to the
    basic features of the components that make up an optical neural network, a certain
    degree of error is inescapable [147]. Conversely, the optical switches that make
    up the network’s architecture can reduce mistakes they typically accrue by adding
    a modest hardware component. 4.2.2.1.5. Neuromorphic computing When applied to
    AI, neuromorphic computing makes it possible for AI to learn and make decisions
    independently, significantly improving over the first generation of developing
    AI. To acquire abilities in areas like recognizing voice and sophisticated tactical
    games, including chess and Go, neuromorphic algorithms are now involved in deep
    learning [145]. Next-generation AI will imitate the human brain’s capacity to
    comprehend and react to circumstances instead of merely operating from formulaic
    algorithms [148]. When it comes to understanding what they have read, neuromorphic
    computing systems will seek out patterns and use their ‘common sense’ and the
    surrounding context. When Google’s Deep Dream AI was programmed to hunt for dog
    faces, it notably showed the limitations of algorithm-only computer systems [147]:
    Any images that it interpreted as having dog faces were transformed into dog faces.
    Third-generation AI computing attempts to simulate the elaborate structure of
    a living brain’s neural network [149]. This calls for AI with computing and analytic
    capabilities on par with the extremely efficient biological brain. To demonstrate
    their exceptional energy economy, human brains can surpass supercomputers using
    less than 20 watts of electricity. Spiking Neural Networks (SNN) are the AI equivalent
    of our synaptic neural network [150]. They leverage many layers of artificial
    neurons, and each spiking neuron may fire and interact with its neighbors in response
    to external inputs. Most AI neural network architectures follow the Von Neumann
    design [106], which divides the memory and computation into discrete nodes. Computers
    exchange information by reading it from memory, sending it to the CPU for processing,
    and then returning it to storage. This constant back-and-forth wastes a lot of
    time and effort. It causes a slowdown that becomes more noticeable while processing
    huge data sets. As a response, multiple neuromorphic devices can be utilized to
    supplement and improve the performance of traditional technologies, such as CPUs,
    GPUs, and FPGAs [146]. Low-power neurological systems may perform powerful activities,
    including learning, browsing, and monitoring. A practical instance would involve
    immediate voice recognition on mobile phones without the CPU needing to interact
    with the cloud. 4.2.2.2. Technologies/impact areas The key technologies and affected
    domains for Specialized Computing include: (1) Graphics Processing Unit (GPU):
    GPUs have rapidly risen in prominence as a crucial component of both home and
    enterprise computers [18]. A GPU is a special type of computer chip deployed in
    a variety of application domains, most notably the rendering of moving images.
    While GPUs are best recognized for their usage in gaming, they are also finding
    increasing application in the fields of creative creation and AI [151]. The initial
    purpose of GPUs was to speed up the display of 3D visuals. They improved their
    functionality as they got more adaptable and programmable over time. This paved
    the way for more complex lighting and shadow characteristics and photorealistic
    environments to be implemented by graphics developers. Additional engineers started
    using GPUs to drastically speed up various tasks in deep learning, HPC, and other
    fields [138]. (2) Compute Unified Device Architecture (CUDA): The demand for more
    powerful computers grows daily. As a result of constraints like size, climate,
    etc., vendors throughout the world are finding it difficult to make future improvements
    to CPUs [18]. Service providers that provide solutions in this kind of environment
    have begun to seek out performance improvements elsewhere. The use of GPUs for
    parallel processing is one option that enables significant speed gains [152].
    The total number of cores in a GPU is significantly greater than that of a CPU.
    Although CPUs are designed for sequential processing, offloading them to GPUs
    enables parallel processing. For general-purpose computing on NVIDIA’s GPUs, users
    can rely on CUDA, which allows for the execution of processes in parallel on the
    GPU without any specific order requirement [138]. Offloading compute-intensive
    activities to Nvidia’s GPU using CUDA is straightforward thanks to the library’s
    support for the popular C, C++, and Fortran programming languages [152]. CUDA
    is employed in scenarios needing extensive computational power or suitable for
    parallel processing to achieve high performance. Fields such as AI, healthcare
    analysis, science, digital transformation, cryptocurrency mining, and scientific
    modeling, among others, depend on CUDA technology. 4.2.2.3. Trends/observations
    The main trends and observations regarding Specialized Computing are as follows:
    Large-Scale ML: As big data grows, ML algorithms with many variables are needed
    to ensure that these models can handle very large data sets and make accurate
    predictions, including hidden features with many dimensions, middle representations,
    and selection functions [153]. The need for ML systems to train complicated models
    with millions to trillions of variables has increased as a result [154]. Distributed
    clusters of tens to hundreds of devices are often used for ML systems because
    they can handle the high computing needs of ML algorithms at these sizes. Yet,
    developing algorithms and software systems for these distributed clusters requires
    intensive analysis and design [155]. The latest advances in industrial-scale ML
    have focused on exploring new concepts and approaches for (a) highly specialized
    monolithic concepts for large-scale straight applications, such as different distributed
    topic models or regression models, and (b) for adaptable and readily programmable
    universally applicable distributed ML platforms such as GraphLab based on vertex
    programming and Petuum using a parameter-driven server [156]. It is widely acknowledged
    that knowledge of distributed system topologies and programming is essential;
    however, ML-rooted statistical and algorithmic discoveries can yield even more
    fruit for large-scale ML systems in the form of principles and techniques specific
    to distributed machine learning applications. These guidelines and techniques
    shed light on several crucial questions: • How to share an ML application among
    nodes? • How to connect machine-learning calculations with machine-to-machine
    dialog? • How should one proceed with having such a conversation? • What ought
    to be conveyed among machines? And, should they cover many big ML-related topics,
    from practical use cases to technical implementations to theoretical investigations
    [98]? Understanding how these concepts and tactics may be made effective, generally
    applicable, and easy to develop is the primary goal of large-scale ML systems
    studies, as is ensuring that scientifically validated accuracy and scalability
    assurances underpin them. 4.3. Centralized vs. Decentralized computing A central
    server controls and processes most of the data in a centralized network, whereas
    no single entity has influence over a decentralized network. 4.3.1. Centralized
    computing In this section, we discuss the main focus or paradigms, technologies
    or impact areas, and various trends or observations within centralized computing.
    4.3.1.1. Focus/paradigms The following are the main focus or paradigms for centralized
    computing: (1) Cloud Computing: The adoption of cloud computing, which revolutionized
    how end-users and software engineers interact with applications and computing
    systems, led to the rise of technology as the fifth utility [1]. Cloud computing
    was successfully accepted by giving consumers on-demand access to the computing
    power they want, the freedom to modify their resource consumption as needed, and
    the transparency of paying just whatever is being utilized. Business groups, regulatory
    bodies, and universities have all been quick to endorse it since it first appeared.
    Like contemporary society relying on essential utilities, the cloud has grown
    into the economy’s foundation by providing immediate utilization of subscription-driven
    computing resources [157]. As a result of using cloud technology, innovative companies
    can be launched quickly, existing ones can expand globally, advances in science
    can be sped up, and novel computing methods can be developed for ubiquitous and
    pervasive apps [34]. SaaS, PaaS, and IaaS have served as the three primary service
    models that have pushed uptake in the cloud thus far [35]. • Mobile Cloud Computing:
    To provide value to mobility consumers, network operators, and cloud service providers,
    mobile cloud computing integrates mobile devices, cloud computing, and communication
    networks. With the help of mobile cloud computing, a wide variety of handheld
    gadgets can run complex mobile apps. Under this paradigm, handling and storing
    data is done by servers rather than individual mobile devices [32]. Several advantages
    result from the use of mobile cloud computing apps based on this architecture:
    (i) battery life has significantly increased; (ii) there has been an increase
    in both the speed and size of data being stored and processed; (iii) the system’s
    emphasis on “store once, access anywhere” eliminates complex data synchronization;
    and (iv) stability and scalability have been dramatically enhanced. Nevertheless,
    inadequate network capacity is a significant challenge for mobile cloud computing
    [33]. Wireless mobile cloud services have capacity constraints in contrast to
    their cable counterparts. The spectrum of mobile devices offers a wide range of
    wavelengths. This has resulted in slower access speeds, as much as one-third in
    comparison to a wired network. Due to the increased likelihood of data loss on
    a wireless network, it is more challenging to recognize and deal with security
    risks on mobile devices than on desktop computers [158]. Customers frequently
    report issues with accessibility to services, including network outages, overcrowding
    on public transit, lack of coverage, etc. Customers may occasionally experience
    a low-energy signal, which slows down access and impacts data storage. Mobile
    cloud computing is employed on several OS-driven platforms, including Apple iOS,
    Android, and Windows Phone, resulting in network modifications that need cross-platform
    compatibility [159]. Mobile gadgets have a greater environmental impact due to
    their high energy consumption and low output [60]. As the use of mobile cloud
    computing grows, so does the problem of the increased drain on mobile devices’
    batteries. A device’s battery life is crucial for using its software and executing
    other tasks. Although the modified code is tiny in size, offloading uses more
    energy than running it locally [160]. • Green Cloud Computing: In the last few
    decades, Information and Communication Technology (ICT) has significantly evolved,
    drawing on technological advancements from the past two centuries. This evolution
    has elevated computing to the status of a fundamental service, akin to traditional
    utilities such as water, electricity, gas, and telephony, thereby establishing
    it as the fifth essential utility in modern society [161]. Modern cloud computing
    systems are becoming progressively large-scale and dispersed as more and more
    businesses and organizations have shifted their computing workload to the cloud—while
    others opt out of maintaining code altogether and instead leverage cloud-powered
    SaaS services. A cloud computing infrastructure of this magnitude not only offers
    more affordable and dependable services but also, increases energy effectiveness
    and reduces the global community’s carbon impact [162]. Every minor enhancement
    is much appreciated. In an effort to achieve zero carbon emissions, the community
    has recently been aggressively exploring a more sustainable version of cloud computing
    called green cloud computing to lessen reliance on fossil fuels and curb its carbon
    footprint [163]. Green cloud computing is a system that considers its constraints
    and goals to minimize energy consumption. Researchers are focusing on scheduling
    workloads and resources in light of carbon emissions, in order to increase the
    effectiveness of the resources used [164]. Additionally, forecasting problems
    with hardware and creating management systems to use hardware with varying degrees
    of dependability can maximize device lifetime and reuse. Further, utilizing micro-data
    centers – rather than standard server data centers – is a promising approach to
    boost efficiency and save costs. These facilities can accommodate future growth,
    serve huge populations, and dissipate heat effectively [165]. Furthermore, virtualization
    is another ecologically friendly technique that boosts the versatility of system
    resources. Through improved tracking and control, servers may pool their resources
    more effectively [166]. Innovations and practices that support sustainable development
    are constantly being developed as organizations rely more heavily on cloud services
    to enable “green cloud computing”. 4.3.1.2. Technologies/impact areas The key
    technologies and affected domains for centralized computing include: (1) Cloud
    Storage Technologies: Files and information stored in the cloud may be accessed
    from anywhere with a web connection or via secure network access. Transferring
    files to the cloud puts the responsibility for data security squarely on the shoulders
    of the cloud provider, rather than consumers. The service provider hosts, manages,
    and maintains the servers where user data is stored, and they also guarantee that
    users always have access to their files [167]. When compared to storing data on
    local discs or storage networks, cloud-based storage is a more affordable and
    scalable option. There is a limit to the quantity of information that can be stored
    on a hard disc. When users exhaust internal storage space, they must copy their
    data to removable media. The difference between on-premises storage networks and
    cloud storage is that the latter sends data to servers located in a remote data
    center. VMs, which are abstracted on top of an actual server, make up the vast
    majority of users’ servers [168]. Known as autoscaling, a cloud provider spins
    up more virtual servers as necessary to accommodate users’ ever-increasing storage
    demands. Files, blocks, and objects are the three primary categories of cloud
    storage, which are accessible in private, public, and hybrid cloud configurations.
    (2) Microservices: Microservices are a type of application architecture in which
    several autonomous services collaborate using simple APIs. A cloud-native software
    development method, microservice architecture separates an app’s main functionality
    into its own modules [169]. By compartmentalizing the app’s components, the development
    and operations teams may collaborate without interfering with each other. If several
    engineers can collaborate on the same project simultaneously, it takes less time
    to complete. This is in contrast with the monolith software architecture, which
    had been the standard for application development in the past [170]. All of an
    app’s features and services are tightly bound together and run as one seamless
    whole under a monolithic architecture [171]. The application’s architecture becomes
    more involved whenever new features are introduced, or existing ones enhanced.
    Because of this, optimizing a single feature inside the application requires disassembling
    the whole thing, which is a time-consuming and tedious process. This additionally
    necessitates that scaling the application as a whole is required if scaling any
    one process inside it—rather than just scaling out just that overloaded element
    [172]. Microservice architectures separate an app’s essential features into individual
    processes. To adapt to shifting business requirements, software engineering teams
    may develop and maintain new elements independently of the rest of the application.
    The monolith has been the standard for application development in the past. An
    application’s features and services are tightly bound together and run seamlessly
    under a monolithic architecture [173]. Microservices’ malleability might hasten
    the deployment of novel modifications, necessitating the development of novel
    patterns. In software engineering, a “pattern” is supposed to refer to any mathematical
    approach that is known to function. An “anti-pattern” is an erroneous pattern
    that is often applied to achieve a solution but often ends up causing even more
    problems. (3) Container Technologies: Given the advent of Docker, container technology
    has gained widespread use in the cloud computing sector, where it is used to efficiently
    execute user workloads [174]. Since containers are independent entities that may
    run without sharing data with other containers, this technology provides an inexpensive
    cloud environment for deploying applications. In a container, applications deployed
    on the same hardware server can share the same underlying resources while maintaining
    their own distinct processes [175]. Container technology leverages Linux kernel
    capabilities, such as libcontainer and control groups (cgroups). By utilizing
    cgroups and namespaces, Docker can operate containers independently within a host
    node, providing the container with its own dedicated set of runtime resources
    (including the host’s networked devices, disc space, memory, and CPU). In addition,
    namespaces provide for more efficient application deployment and development by
    separating the program’s perspective from the operating environment [176]. Furthermore,
    containerization becomes an example of creating, publishing, and running applications
    in an isolated way and is indicated as a Container as a Service (CaaS). There
    are three primary advantages of CaaS: (1) containers boot up in no time at all;
    (2) they consume fewer resources than VMs; and (3) many instances may be operated
    at once using container technology [177]. Recent investigations [178] into container
    technology reveal unanswered research questions. Firstly, containers are less
    secure than VMs since they share the kernel, but this is something that may be
    fixed in future versions with the help of Unikernel. Secondly, optimizing container
    performance is a time-consuming endeavor that requires buffer space. Swarm and
    Kubernetes are two examples of cutting-edge cloud computing tools that may be
    used for handling user-created QoS-based container clusters [179], [180]. Thirdly,
    because containers share the same computing/hardware resources, co-located tenants
    can suffer from unpredictable performance interference when the CPU Shares algorithm
    is used, and even worse, they can leak information enabling side-channel attacks
    to be performed by a malicious tenant [181]. (4) Serverless Computing: The use
    of serverless computing in the creation of apps for the cloud is gaining traction
    [182]. The goal of serverless computing is to ensure that only the most effective
    serverless technologies are deployed, reducing costs while increasing benefits
    [183]. Meanwhile, companies in all industries are adopting AI since it is the
    next generation of innovation. Due to these AI-driven platforms, we have been
    able to make more accurate, timely decisions [184]. They have altered the methods
    used to conduct business, communicate with customers, and assess company information.
    Complex ML systems can significantly affect developers’ output and efficiency
    [185]. However, switching to a serverless architecture may be able to solve many
    of the issues that engineers face. The serverless design ensures that the machine
    learning models are administered correctly and that all available resources are
    utilized efficiently. Developers will be able to devote a greater amount of time
    to training AI models rather than maintaining the server environment [186]. Creating
    ML algorithms is a common practice when confronting difficult problems. They perform
    tasks such as data analysis and preprocessing, model training, and AI model tuning
    [186]. Serverless computing running AI tasks will provide for reliable data storage
    and communication. 4.3.1.3. Trends/observations The main trends and observations
    regarding centralized computing are as follows: (1) AI-driven Computing: The fundamental
    benefit of autonomic computing is a reduced overall cost of ownership. Therefore,
    the cost of upkeep will be drastically reduced. The number of technicians required
    to keep everything running smoothly will go down as a result, too. Autonomous
    IT systems driven by AI will reduce the time and money needed for installation
    and upkeep while also improving IT system stability [4]. In accordance with higher-order
    benefits, businesses would be more capable of handling their operations with the
    help of IT systems that are able to adopt and execute directions based on their
    business plan and allow for adjustments in reaction to evolving circumstances.
    Using AI-based autonomic computing has several advantages, including reducing
    the expense and quantity of human labor needed to manage large server farms, which
    is made possible through server consolidation [187]. Using AI for self-driving
    computers will simplify system administration. As a result, computer systems will
    be greatly enhanced. Server load distribution is another potential use case since
    it allows for parallel data processing across several computers. Meanwhile from
    an energy perspective, analyzing the power grid in real-time allows for more cost-effective
    and long-term power policy decisions to be made [1]. There are benefits to using
    remote data centers instead of keeping data in-house. Despite the hefty upfront
    expenses, businesses may obtain AI technology relatively easily by paying a monthly
    fee on the cloud. When employing an AI-powered system, there may be no need for
    human involvement in data analysis [188]. Using AI in the cloud can potentially
    make businesses more effective, strategic, and insight-driven. AI can increase
    output by automating routine processes and data analysis without human intervention
    [74]. For instance, integrating AI technology with Google Cloud Stream statistics
    could enable real-time personalization, anomaly detection, and management scenario
    prediction [189]. As the number of cloud-based applications grows, it is essential
    to implement a system of rigorous data protection based on intelligence. Network
    security systems backed by AI-enabled traffic tracing and analysis; AI-enabled
    devices can sound an alarm as soon as an anomaly is detected. Such methods will
    ensure keeping sensitive data protected. (2) Net Zero Emissions: Several data
    center operators have committed to being carbon neutral by the year 2030 as sustainability
    becomes an increasingly hot subject in the industry [190]. But are these promises
    only a reaction to the possibility of legislation, or is it actually making progress?
    If business planes are a major contributor to global warming, how do they plan
    to cut their carbon footprint so rapidly? The data centers’ businesses in the
    United States use about as much power as the state of New Jersey [191]. If all
    of the power came from renewable resources, this level of demand would not be
    a problem. Liquid cooling and energy generation both require water, and a typical
    data center uses as much water as an urban area of 30,000 to 50,000 individuals
    [192]. Becoming a pioneer in sustainability might also bring up emerging markets.
    Companies are going to utilize green data centers to offset their carbon footprints
    as they grow and become more energy efficient and sustainable [193]. A car company,
    for instance, might employ emission-free data centers for all of its corporate
    services. Last but not least, adopting environmentally friendly practices may
    help businesses comply with environmental rules, avoid fines, and get access to
    attractive, low-interest, long-term capital investment possibilities [194]. 4.3.2.
    Decentralized computing In this section, we discuss the main focus or paradigms,
    technologies or impact areas, and various trends or observations within decentralized
    computing. 4.3.2.1. Focus/paradigms The following are the main focus or paradigms
    for decentralized computing: (1) Parallel Computing: Through the utilization of
    several processor cores, parallel computing can perform multiple tasks simultaneously.
    The ability to divide and conquer a work into smaller, more manageable chunks
    is what sets parallel computing apart from its serial counterpart [195]. Real-world
    events may be modeled and simulated effectively on parallel computing systems
    [196]. As processing and network speeds continue to increase at an exponential
    rate, adopting a parallel architecture is no longer just a nice-to-have. The IoT
    and big data will eventually require us to process terabytes of data simultaneously.
    Devices such as dual-core, quad-core, eight-core, and even 56-core CPUs utilize
    parallel computing. Therefore, although parallel computers are not brand new,
    this is the problem: These new technologies are spitting up ever-faster networks,
    and computer efficiency has surged 250,000 times in 20 years [197]. For instance,
    AI technologies will sift through more than 100 million patients’ heart rhythms
    in the medical sector alone, looking for signals of A-fib or V-tach, saving many
    lives in the process [196]. When the systems must slowly move through each procedure,
    they will not be able to complete it on time. As great as the potential is, parallel
    computing may be nearing the edge of what it can achieve with conventional processors.
    Parallel calculations may see significant improvements in the coming decade, thanks
    to quantum computers. In a current, unauthorized announcement, Google claimed
    to have achieved quantum supremacy [76], [198]. If it is accurate, then Google
    has created a machine that can perform in 4 min whatever would require the most
    capable supercomputer on the planet 10,000 years to achieve [51]. Quantum computing
    is a major step forward for parallel computation. Imagine it like this: Processing
    in a serial fashion does one task at a time. An 8-core simultaneous computer can
    do eight tasks simultaneously. There are fewer particles in the universe than
    there are qubits’ states in a 300-qubit quantum computer [198]. (2) Fog Computing:
    The proliferation of IoT devices and the effort needed for analyzing and storing
    enormous amounts of knowledge led to the development of fog computing as a complementary
    service to traditional cloud computing. Fog computing, which provides fundamental
    network functions, can back IoT apps that require a small response-time window
    [37]. Due to the dispersed, diverse, and constrained nature of the fog computing
    paradigm, it is challenging to spread IoT application operations effectively within
    fog nodes to meet QoS and Quality of Experience (QoE) limitations [39]. Vehicle-to-Everything
    (V2X), medical tracking, and manufacturing automation adopt fog computing as it
    delivers the ability to compute close to the consumer to match fast response demands
    for these applications. Due to the proliferation of IoT devices, these applications
    generate massive volumes of data. Cloud computing falls short of satisfying latency
    demands due to the transmission of data over long distances and network overload.
    Bridging data sources and CDCs, it sets up a network of gateways, routers, switches,
    and compute resources [199]. The use of fog computing enhances the capabilities
    of cloud computing due to its minimal latency and cost-effectiveness, as well
    as the decrease in bandwidth necessary for the transit of data. It is more secure
    to process confidential information locally at the fog nodes, and if/when needed,
    only submit trained models – not raw data – to intermediate nodes and eventually
    the cloud for aggregation, e.g., via federated learning [200]. These applications
    collect data from various IoT devices to deliver useful insights and deal with
    latency issues [201]. (3) P2P Network: This network is formed in its most basic
    form when two or more PCs are linked to one another and exchange resources without
    passing through a third computer that acts as a server [23]. A P2P network might
    be a spontaneous connection, which would consist of two or more computers linked
    together using a Universal Serial Bus for the purpose of file sharing. In a fixed
    infrastructure, P2P networking utilizes copper lines to connect six PCs located
    in a single workplace [24]. Alternately, a peer-to-peer network may be an ecosystem
    that is considerably larger in scale and is characterized by the use of specialized
    protocols and apps to establish direct links between consumers over the web. (4)
    Osmotic Computing: This model has become pervasive in various settings, from urban
    planning and healthcare to linked vehicles and Industry 4.0 [46]. It lays the
    groundwork for a system in which vehicles, pedestrians, and urban infrastructure
    interact and share real-time information to improve traffic flow. As more people
    use IoT applications housed in different types of networks (cloud, edge, and IoT),
    it is now clear that the providers who make up the IoT’s service ecosystem (data,
    service, network, and equipment) are all interconnected [48]. In this setting,
    buyers and sellers implicitly expect their data and services to be secure and
    trustworthy. There is no requirement for familiarity with the federated ecosystem
    (service, data, and network) for users of the IoT apps to connect with many applications
    using a web-based user interface [202]. Users send their information to application
    providers without realizing that those trusted suppliers may share that information
    with any third parties (such as a company that hosts analytics on the cloud or
    a company that provides the infrastructure for mobile devices). Security issues
    may arise for software due to the wide variety of computing devices available
    from different manufacturers and their presence in an untrusted realm with no
    overarching authority [203]. (5) Dew Computing: It stands out because of its near-complete
    independence from Internet access, its users’ physical closeness to servers, its
    low latency, outstanding speed, excellent user interface, and adaptability in
    terms of control available to users [204]. Instead of serving as a replacement
    for cloud computing, dew computing serves as a useful supplement. In the not-too-distant
    future, people throughout the globe might be able to limit their time spent online,
    increasing their efficiency and effectiveness. Countries have adopted measures
    to handle the influx of Internet users caused by the COVID-19 blackout. To lighten
    the Internet’s burden, video streaming services are reducing visual quality, while
    others just update their software outside of peak viewing times. The dew computer’s
    proximity to the user in the design means it can facilitate all electronic interactions
    with fewer steps and more efficient data transfer [204]. (6) Edge Computing: Since
    its origins in content delivery networks, distributed computing has matured into
    the mainstream as an edge computing paradigm that places resources near the client’s
    end. Big data is typically best stored in the cloud, whereas immediate information
    created by consumers and exclusively for the customer needs computing power and
    storage on the edge [40]. To accommodate growing mobile user needs, cloud providers
    have realized they must shift crucial processing to the device. With its high
    performance and low cost, edge computing is a key driver for AI. This can be the
    most helpful method to see how AI relates to edge computing. Due to the data-
    and compute-intensive characteristics of AI, edge computing aids AI-powered applications
    in resolving their technical problems. AI/ML systems consume large amounts of
    data to discover trends and provide trustworthy recommendations [205]. AI use
    cases that need video analysis face latency challenges and rising expenses due
    to the cloud-based transmission of high-definition video data. The delay and reliance
    on central processing in cloud computing are problematic when ML inputs, outputs,
    and (re-)training data must be handled in real-time. It is possible to perform
    computation and decisions at the edge, eliminating the need for costly backbone
    connections and allowing immediate action on the data. Client information regarding
    location is stored at the edge instead of in the cloud for security reasons. When
    data is streamed to the cloud, all relevant data and datasets are uploaded. Edge
    networks for computing have introduced several difficulties associated with infrastructure
    management because of their dispersed and intricate nature [206]. Managing resources
    efficiently requires carrying out several tasks. Examples include VM consolidation,
    resource optimization, energy efficiency, workload prediction, and scheduling.
    Resource management has historically relied on static, established guidelines,
    mostly based on operations research methodologies, even in dynamic, rapidly changing
    settings and in immediate situations. To deal with these issues, especially when
    choices must be made, AI-based solutions are being used more and more frequently.
    AI/ML methods have become increasingly common in the past few years [207]. However,
    selecting where on edge to carry out a task can be challenging, as it requires
    considering tradeoffs like the volume of data on edge servers and the ability
    to move users [208]. The cache has to anticipate the consumer’s next destination
    for it to build on the notion of mobility [209]. It is situated at a suitable
    edge to cut costs and energy consumption. Several different methods, including
    genetic algorithms, neural network models, and reinforcement learning, are utilized
    in this process. • Mobile Edge Computing: Mobile Edge Computing – now Multi-access
    Edge Computing (MEC) – expands its possibilities by introducing cloud computing
    to the web’s edge. Initially targeted solely on the edge nodes of mobile networks,
    MEC has since expanded its scope to include conventional networks and, ultimately,
    integrated networks. While typical cloud computing occurs on servers located far
    from the end-user and devices, MEC enables activities to be carried out at base
    stations, centralized controllers, and various other aggregating sites on the
    Internet [210]. MEC improves consumer QoE by redistributing cloud computing workloads
    to customers’ individual, on-premises servers, thus relieving congestion on mobile
    networks and lowering latency [211]. Innovative applications, services, and user
    experiences are being unlocked at a dizzying rate thanks to advances in edge data
    generation, collection, and analysis and in the transmission of data between devices
    and the cloud [212]. Because of this, MEC is accessible to consumers and businesses
    in a wide range of contexts and industries. Integrating MEC into a camera network
    improves the speed with which data may be stored and processed. With sufficient
    processing power and bandwidth, data may be immediately analyzed locally instead
    of being sent to a remote data center [213]. Self-driving automobiles and autonomous
    mobile robots (AMRs) are two examples of emerging technologies that require powerful
    ML to arrive at judgments rapidly. If such decisions take place in a remote data
    center, only seconds might be the distinction between nearly escaping failures
    and causing a tragedy [205]. Because the vehicle must avoid hitting pedestrians,
    animals, and other vehicles, judgments must be made on the vehicle. Machine-to-machine
    (M2M) communication will be essential to the success of 6G as the forthcoming
    generation of a global wireless standard and the technological advances that will
    emerge from it [101]. 4.3.2.2. Technologies/impact areas The key technologies
    and affected domains for decentralized computing include: 4.3.2.2.1. Distributed
    ledger technology The computing paradigms of fog, edge, and cloud are currently
    experiencing explosive growth in both the business and academic worlds. Security,
    confidentiality, and data integrity in these systems have become increasingly
    important as their practical applications have expanded [214]. Data loss, theft,
    and corruption from malicious software like ransomware, trojans, and viruses raise
    serious considerations in this area. For the system’s and most importantly, end-users’
    sake, it is crucial that data integrity be maintained, and that no data be delivered
    from an unauthenticated source. Medical care, innovative cities, transport, and
    monitoring are all examples of applications of critical importance where the margin
    for error is near zero [4]. • Blockchain: Because the majority of edge devices
    have limited computing and storage capacity, developing an appropriate system
    for data security, and preserving integrity is challenging. The IoT and other
    real-time systems have used blockchain technology for data security [134]. To
    store and monitor the worth of an asset over time, a blockchain is, in theory,
    a set of distributed ledgers. When new information is added to the system, it
    becomes a block with a Proof of Work (PoW). A PoW is a hash value that cannot
    be made without changing the PoW of the blocks that came before it in the ledger.
    Miners create and verify these PoWs while also mining blocks in the Fog network
    [215]. After a miner has completed the PoW, it broadcasts the newly created block
    into the network, where the other nodes check its legitimacy before joining it
    in the chain. Also, the fraudulent change of data in a blockchain will not work
    unless at least half of the copies of the data in question are changed individually
    by carrying out the same actions. With such a strict time constraint, modifying
    any data in the blockchain will be extremely difficult. Network nodes must offer
    route selection, preservation, financial services, and mining for the blockchain
    to function. Considering these challenges, numerous groups have worked to develop
    solid frameworks for combining blockchain and fog computing [133]. The majority
    of these systems employ a dynamic allocation mining technique in which the least-used
    nodes mine and validate the chains. In contrast, the remaining nodes are employed
    for load balancing, computation, and data collection [108], [216]. The blockchain
    on a large portion of the network is replicated at those nodes if a worker detects
    an issue in relation to blockchain manipulation or signature forging. Furthermore,
    blockchains offer public-key encryption with adaptive key exchange for further
    security. Blockchain is a deceptively simple central notion, but incorporating
    it into fog computing systems presents several challenges. Cost and upkeep are
    major factors surrounding storage capacity and scalability. Only complete nodes
    (nodes that can fully validate the transactions or blocks) store the whole chain,
    which still results in massive storage needs. Data anonymity and privacy issues
    are another blockchain shortcoming. Privacy is, therefore, not incorporated into
    the blockchain architecture; consequently, third-party tools are necessary for
    accomplishing these crucial requirements [217]. This might result in less efficient
    applications that demand more resources (both computationally and in terms of
    storage space) to run. There are still numerous unresolved issues and potential
    future developments for blockchains in IoT architectures [13]. Insufficient resources
    are the main barrier to excellent data protection and dependability. Because of
    resource limits, more complex encryption or key generation cannot be incorporated
    with these chains of data [218]. Only restricted encryption algorithms may be
    implemented. By considering resource limitations, more effective algorithms may
    be created. In high fault-rate scenarios, wherein the edge nodes are susceptible
    to attack at any time, modifying such chains is another essential approach [219].
    Network and I/O bandwidth needs are greatly increased due to the necessity of
    revalidating blocks and copying chains from the primary network. The majority
    of frameworks additionally use a master–slave architecture, which introduces a
    potential weak spot. In diverse settings, this is to be expected. The balance
    between cost and reliability must be meticulously evaluated when considering redundancy
    [132]. The blockchain flaws also continue to impact fog architectures. There is
    a need to develop efficient consensus techniques that can validate blocks with
    little block sharing and copying. Those curious might learn more about blockchain
    by reading an in-depth report on the topic. 4.3.2.2.2. Federated learning Data
    is needed for ML model training, testing, and validation. Information is stored
    in locations accessible by thousands or millions of users (devices). Rather than
    sharing the entire dataset required to train a model, federated devices only communicate
    the parameters specific to that device’s instance of the model. The parameter
    sharing mechanism is defined by the federated learning topology [220]. Each participant
    in a centralized topology contributes the parameters of the model to a centralized
    server, which then trains the centralized model and returns the trained parameters
    to each participant. Parameters are typically shared among a smaller group of
    peers in other configurations, including peer-to-peer or hierarchical ones. ML
    methods that require large or geographically dispersed data sets may benefit from
    federated learning. However, there is no universally applicable machine-learning
    solution [221]. Several unanswered questions remain about federated learning that
    researchers and developers are hard at work trying to address [222], [223]. There
    are a lot of opportunities for efficient communication in federated learning.
    This means the master server or other entities acquiring the parameters must be
    able to cope with occasional interruptions or delays in transmission. Getting
    all the federated devices to talk to each other and stay in sync is still an open
    issue [222]. There is typically a lack of transparency between federated parties
    and a central server regarding the computing capacity of the federated parties.
    However, it is still challenging to ensure that the training activities will operate
    on a diverse mix of devices [220]. Federated parties’ data sets might be quite
    varied in terms of data amount, reliability, and variety [224]. It is challenging
    to predict how statistically diverse the training data sets will be and how to
    protect against any detrimental effects this diversity may have. Efficient deployment
    of privacy-enhancing solutions is required to prevent data loss due to shared
    model parameters. 4.3.2.2.3. Bitcoin currency Transaction settlement using blockchain
    technology was initially proposed with the digital (crypto-) currency Bitcoin.
    The blockchain is a distributed ledger that verifies monetary transactions using
    PoW and may be configured to record anything of worth. Blockchains, including
    bitcoins and cryptocurrencies, are innovative in operating apps across networks
    [225]. Designers create smart contracts for Bitcoin money exchanges, which are
    subsequently carried out on blockchain VMs [226]. Blockchain relies on a decentralized,
    concurrency-agnostic runtime environment and consensus mechanism. Blocks of data
    may be disseminated across Bitcoin ledgers via a peer-to-peer network with no
    requirement for a centralized authority, thanks to the Bitcoin enabling network
    [226]. The data in the blockchain is certified by the members to keep it safe
    and open, and anybody is welcome to join the network. Cloud computing may use
    this property, and the security of cloud storage, in particular, can benefit from
    it. Cloud computing infrastructures enable the execution of complex applications
    and the handling of massive data sets. Centralized data centers coupled with Fog
    or IoT devices at the network edge cannot efficiently handle the enormous data
    storage required to deliver high-availability, real-time, low-latency services
    [227]. A distributed cloud design is required to deal with these problems instead
    of the more conventional network architecture. Blockchain technology, a fundamental
    element of distributed cloud systems, offers detailed control over resources by
    enabling their management through distributed apps [228]. It also allows for the
    tracking of resource usage, providing both customers and service providers with
    the means to verify that the agreed-upon QoS is being met. A marketplace is a
    platform where everyone may promote their computer resources while discovering
    what they require using AI-based techniques or models of prediction [229]. Blockchains,
    compared to cloud computing, offer fewer computer resources available to run distributed
    applications, such as less space for storing data, less powerful VMs, and a more
    unstable protocol. As a result, apps that are sensitive to delay and those that
    use a lot of resources need to find solutions to these problems [230]. Combining
    blockchain and cloud computing to develop a blockchain-based distributed cloud
    can provide novel advantages and solve current restrictions. Data moves closer
    to its owner and user through Blockchain’s distributed cloud, providing on-demand
    resources, security, and cost-effective access to infrastructure [231]. In the
    meantime, the high price and substantial consumption of electricity from clouds
    may be solved with a blockchain-based distributed cloud. Cloud storage security
    is another area where blockchain may play a role in the future [232]. By dividing
    user data into smaller pieces before storing it everywhere, it is possible to
    encrypt it further. A small portion of the data is accessible to the hacker, not
    the entire file. In addition to eradicating data-altering hackers from the network,
    a backup copy of the data may be used to restore any changes [229]. The use of
    quantum computers to circumvent the mathematical impossibility of modern encryption
    is one of their most publicized uses. In the meantime, many online publications
    have predicted the end of Bitcoin and other cryptocurrency use after Google stated
    it had achieved quantum supremacy. 4.3.2.3. Trends/observations The main trends
    and observations regarding decentralized computing are as follows: Serverless
    Edge Computing: Serverless’ ‘scale-to-zero’ feature, which releases unoccupied
    containers from the system, works well for energy-conscious IoT scenarios with
    load-inconsistent applications. On the other hand, fine-grained scaling (i.e.,
    at the function stage) is capable of handling extremely distinct needs and execution
    settings at the edge [185]. Many IoT applications rely on instances initiated
    by sensing or actuating, just like functions in serverless [102]. However, unlike
    serverless functions, IoT devices often sense or act only on rare occasions, whereas
    they sleep the majority of the time to conserve power. So, first, serverless appears
    to be an ideal paradigm of execution. However, combining serverless, edge computing,
    and IoT applications is challenging because serverless was originally designed
    for cloud environments, which do not have the same constraints as edge computing
    devices [233]. In light of this opportunity, it is essential to combine serverless,
    edge computing, and IoT applications to address this challenge. This is crucial
    to be addressed, as the fact is that although this adaptation looks needed and
    helpful, its practicality necessitates comprehensive inspections to avoid ramifications.
    4.3.3. Hybrid computing It involves combining both a centralized network and a
    decentralized network. In this section, we discuss the main focus or paradigms,
    technologies or impact areas, and various trends or observations within hybrid
    computing. 4.3.3.1. Focus/paradigms The following are the main focus or paradigms
    for hybrid computing: Fog-Cloud-Edge Orchestration: Increasingly, IoT technologies
    are required in daily life. Smart cities, automated manufacturing, virtual reality,
    and autonomous cars are just a few instances of the vast variety of sectors where
    the application of these technologies has been rising quickly [234]. This type
    of IoT application frequently necessitates access to heterogeneous distant, local,
    and multi-cloud compute resources, in addition to a globally dispersed array of
    sensors. The expanded Fog-Cloud-Edge orchestration paradigm is born from this.
    This new paradigm has made it a necessity to expand application-orchestration
    needs (i.e., self-service deployment and run-time administration) beyond the confines
    of a purely cloud-based infrastructure and across the full breadth of cloud or
    edge resources. Recent years have seen an increased focus on the research and
    advancement of orchestrating platforms in both business and academic settings
    as a means of meeting this need. 4.3.3.2. Technologies/impact areas The key technologies
    and affected domains for hybrid computing include: (1) Cryptocurrencies: Decentralized
    networks with powerful computational power were pioneered by cryptocurrencies.
    There is no centralized authority that controls the cryptocurrency market or issues
    new cryptocurrencies. Bitcoin, the first decentralized digital currency, was launched
    in 2009 and employs blockchain technology to record transactions and save user
    histories [235]. Blockchain Explorer and similar tools reveal Bitcoin’s decentralized
    network activity as it moves from one wallet to another, and they also reveal
    the activity of other cryptocurrency networks. There is no equivalent technology
    that would enable such transparency in the private banking business, nor would
    such a publication ever be made public. Decentralization design incorporates many
    additional features that make it hard for bad actors to forge bitcoin or steal
    from user accounts, such as synchronizing the blockchain across all machines on
    the network [236]. Bitcoin and other cryptocurrencies are required to function
    on decentralized networks: A blockchain does not have a central controlling computer
    or administrator. (2) Machine Economy: The emerging machine economy refers to
    the exchange of resources (such as power, data storage, processing power, currency,
    and network connections) in the upcoming global networks of computers [237]. Together,
    the data centers that power the cloud, the web, and monetary exchanges, form a
    network that will support the machines that power the future economy. This is
    the time when AI willfully conceals or exaggerates its powers. AI conceals and
    safeguards limited supplies to protect the crucial scarce resource of computation
    cycles used to generate AI insights. The organization is guarding the computation
    cycles used to generate AI insights, which are the most crucial scarce resource
    in this case. Lies, trickery, and barter to coax AI into parting with its limited
    resources will become an increasingly hot issue in the coming years [238]. To
    prevent itself from being overused, AI will have to resort to dishonest behavior.
    The machine economy is going to be among the most significant developments to
    come for human culture; and will be among the hottest topics of the emerging payment
    and AI technologies needed to fund future interstellar and interplanetary travel.
    4.3.3.3. Trends/observations The main trends and observations regarding hybrid
    computing are as follows: Distributed Computing Continuum: Emerging from the convergence
    of IoT, edge, fog, and cloud computing, Distributed Computing Continuum Systems
    (DCCS) represent a novel computing paradigm that harnesses the collective power
    and heterogeneity of these diverse computing tiers to address the demanding computational
    requirements of future applications [239]. These applications, ranging from autonomous
    vehicles and e-Health to smart cities, holographic communications, and virtual
    reality, demand unprecedented levels of computational power, low latency, and
    efficient data management. Achieving these stringent requirements necessitates
    seamless integration and collaborative operation among all computing tiers, transforming
    the underlying infrastructure into a unified, intelligent system. As exemplified
    by edge and fog computing, the underlying infrastructure of DCCS plays a pivotal
    role in determining its performance. This geographically distributed, heterogeneous,
    and resource-constrained infrastructure poses significant challenges, needing
    new approaches that can dynamically adapt to application and user demands [9].
    Cloud-centric methodologies, often tailored to cloud-specific assumptions, fall
    short in addressing the characteristics of edge, fog, and DCCS environments. To
    address these challenges, DCCS advocates for decentralized intelligence, empowering
    each component of the underlying infrastructure to make autonomous decisions based
    on its specific tasks and local conditions [240]. This approach leverages the
    concept of service level objectives (SLOs), well-established in cloud computing,
    to define the operational goals of each component of the system. By modularizing
    and distributing SLOs across the system, a DCCS can achieve scalable intelligence
    within its infrastructure. Further, incorporating the Markov Blanket concept into
    SLO management enables causal filtering, ensuring that only conditionally dependent
    variables are considered when making decisions. This selective filtering, coupled
    with causal inference or active inference, empowers each component to make informed
    decisions independently, adapting to its dynamic environment and the overall system’s
    requirements [241]. This loosely-coupled architecture fosters a resilient and
    adaptive DCCS, capable of catering to the diverse and evolving demands of future
    applications. 4.4. Computational methodologies: Parallel vs. Sequential computing
    Parallel computing implies a computer model wherein numerous tasks are completed
    concurrently, employing a number of processors or threads [242]. In this paradigm,
    many processes run concurrently and their outputs are pooled. Tasks can be conducted
    in parallel instead of sequentially, potentially reducing execution times. 4.4.1.
    Parallel computing In this section, we discuss the main focus or paradigms, technologies
    or impact areas, and various trends or observations within parallel computing.
    4.4.1.1. Focus/paradigms The following are the main focus or paradigms for parallel
    computing. Simultaneous Data Processing: In order to handle many parts of a task
    at once, parallel processing employs multiple processors, or CPUs. By breaking
    down large computations into smaller ones, systems may drastically speed up their
    execution [242]. Parallel processing is possible on current computers with multiple
    cores and on any machine with more than one CPU. Multi-core processors are embedded
    processors containing two or more CPUs for increased performance, lowered energy
    use, and more efficient handling of many tasks. Two to four cores are common in
    modern computers, with some models supporting up to 12. Modern computers commonly
    use parallel processing to complete complex processes and calculations. At the
    most basic level, sequential and parallel-serial processes differ in how registers
    are employed. Shift registers work in series, computing every bit one at a time,
    while registers with concurrent loading handle each bit of a word concurrently
    [243]. Using multiple functional units that can execute identical or distinct
    tasks in parallel enables the management of more complex parallel processing.
    4.4.1.2. Technologies/impact areas The key technologies and affected domains for
    parallel computing include: (1) ASICs: Application-Specific Integrated Circuits
    (ASICs) are integrated circuits designed for specific uses. As their name suggests,
    ASICs are limited to a single function. They provide a single function and are
    consistent throughout their service life [138]. ASICs are semiconductor devices
    and circuitry developed to carry out a particular task. In contrast to mainstream
    processors, including CPUs and GPUs, both the speed and the energy efficiency
    of ASICs are optimized to fit the needs of a specific application [244]. Their
    excellent performance, minimal energy use, and small form factor make them ideal
    for mass-produced goods that can afford the higher bespoke design costs. (2) FPGA:
    A Field Programmable Gate Array (FPGA) is a semiconductor that can be programmed
    to provide unique logic for use in both early system prototype design and the
    last version of a system to circumvent obsolescence [138]. In contrast to other
    bespoke or semi-custom integrated circuits, FPGAs can be easily reprogrammed by
    a software update to meet the changing requirements of the larger system they
    are integrated into, using hardware design languages, such Verilog and Very High-Speed
    Integrated Circuit Hardware Description Language (VHDL) [245]. Nowadays, most
    rapidly expanding applications are perfect fits for FPGAs, which include edge
    computing, AI, network security, 5G, industrial control, and automated machinery.
    4.4.1.3. Trends/observations The main trends and observations regarding parallel
    computing are as follows: (1) Neuro-symbolic AI: Advances in deep learning techniques
    have unlocked a few of AI’s enormous possibilities. Consequently, it is now obvious
    that these methods are at a breaking point and that such sub-symbolic or neuro-inspired
    solutions only function effectively for particular kinds of issues and are typically
    opaque to both analysis and comprehension [246]. However, symbolic AI methods,
    founded on rules, logic, and reasoning, perform significantly better in terms
    of openness, comprehensibility, authenticity, and reliability than sub-symbolic
    methods. A new path termed neuro-symbolic AI was recently recommended, integrating
    the effectiveness of sub-symbolic AI alongside the visibility of symbolic AI [247].
    This synergy has the potential to yield a new generation of AI devices and platforms
    that are both comprehensible and expansion-intolerant and can combine logic with
    learning in a generic fashion. (2) Scalability: The most important advantage of
    scalable design is improved efficiency, as well as the capacity to deal with sudden
    spikes in traffic or severe loads with little to no warning [248]. An application
    or online company may continue to operate smoothly during busy periods with the
    assistance of a scalable system, preventing businesses from incurring financial
    losses or suffering reputational harm [173]. If a system is organized into component
    services (for example, using the microservices system design), monitoring, updating
    features, troubleshooting, and scaling may become simpler tasks. 4.4.2. Sequential
    computing In this section, we discuss the main focus or paradigms, technologies
    or impact areas, and various trends or observations within sequential computing.
    4.4.2.1. Focus/paradigms The following are the main focus or paradigms for sequential
    computing: One-process-at-a-time execution: In the context of computing, sequential
    computing describes a paradigm in which operations are carried out in a certain
    order, with the output of one operation feeding into the data being the input
    of the subsequent one [249]. A single processor carries out all of the model’s
    tasks in the sequence specified by the code. 4.4.2.2. Technologies/impact areas
    The key technologies and affected domains for sequential computing include: Traditional
    Von Neumann Architecture: This architecture is a sequential computing-based concept
    for digital machines. This system includes a CPU, RAM, and I/O devices, all interconnected
    by a bus [250]. The CPU of a system based on the Von Neumann architecture processes
    instructions sequentially, feeding the output of one into the input channel of
    the subsequent one [107]. 4.4.2.3. Trends/observations The main trends and observations
    regarding sequential computing are as follows: (1) In-Memory Computing: In-memory
    computing is a method used to perform computations solely in memory (like RAM).
    This word usually refers to massive and complicated computations that must be
    executed on a cluster of computers using specialized systems software [249]. As
    a clustering system, the machines pool their RAM, so the computation is effectively
    done across machines and uses the combined RAM capacity of all the machines collectively.
    (2) Energy-efficiency: Power effectiveness and sustainability have emerged as
    major issues for HPC systems as their processing capacity increases [251]. To
    reduce electrical usage while increasing computational performance, scientists
    are inventing environmentally friendly hardware layouts, investigating innovative
    cooling strategies, and fine-tuning algorithms. The general efficiency of HPC
    systems is being improved by the development of energy-aware scheduling and utilization
    strategies. (3) Performance Optimization: Since single-processor efficiency can
    no longer develop at a rapid pace, the era of the single-microprocessor computer
    is coming to an end. It is time for a new era in computing when parallelism takes
    center stage and sequential computing takes a back seat [252]. There are still
    significant scientific and engineering obstacles to overcome, but now is a good
    moment to try new approaches to computer programming and hardware design. Various
    computer architectures have emerged, each tailored to certain performance and
    efficiency goals. The next wave of discoveries will certainly necessitate enhancements
    to computer hardware and software [253]. No one can say for sure if we will succeed
    in making parallel computing as mainstream and user-friendly as yesterday’s peak
    sequential single-processor computer systems in the field of computing. Innovative
    novel applications that motivate the computer business will slow down if parallel
    programming and associated software activities do not become popular, and if creativity
    slows down across the economy as a whole, many other sectors will suffer as well
    [121]. 4.5. Computing trends and emerging technologies New computing trends and
    emerging technologies continue to advance the field of computing, improving the
    adaptability, self-management, and sustainability of many types of industrial
    systems. 4.5.1. Advanced computing styles and trends In this section, we discuss
    advanced computing styles and trends and their related technologies and paradigms.
    4.5.1.1. Focus/paradigms The following are the main focus or paradigms for advanced
    computing styles: Quantum AI: Quantum computing is attractive because it is a
    unique innovation that can radically change AI and computing in general. In this
    section, we look into what quantum computing can do and how it can affect AI and
    the wider economy. The implications of this computing method might have far-reaching
    effects on several facets of our cultural and financial lives [4]. The widespread
    impact of AI suggests that combining it with quantum computing might unleash dramatic
    change in the field of AI [198]. Several algorithms that made it possible to do
    tasks previously thought impossible for conventional computers emerged in the
    wake of the foundational studies that formalized the notion of a quantum computer
    [254]. The development of Shor’s algorithm, an effective method for dividing enormous
    amounts of data, has bolstered research into quantum computing and quantum cryptography.
    Yet, existing cutting-edge technologies are not yet accurate enough to execute
    Shor’s algorithm successfully, which requires a degree of precision for performing
    register initialization, quantum operations on multiple qubits, and storing quantum
    states. It is also crucial to remember that quantum computers have particular
    limits [76]. The acceleration afforded by quantum computers grows exponentially
    compared to the amount of time a conventional computer takes (Grover’s method);
    hence, it is not predicted that it will effectively solve NP-hard efficiency issues.
    The benefits of quantum computing, such as quantum superposition and entanglement,
    typically vanish rapidly with the complexity and magnitude (i.e., the number of
    quantum systems involved) of the underlying hardware, making the process of designing
    a quantum computer non-trivial. Despite this, the curiosity of significant technologically
    advanced players (IBM, Microsoft, Google, Amazon, Intel, and Honeywell) has skyrocketed
    in the past few years, and a plethora of fresh startups have emerged to propose
    remedies for quantum computing using technologies as diverse as superconducting
    devices, encased ions, and integrated light circuits. Corporations like these
    are among the numerous that are investing in quantum research and development
    at the moment [255]. Although there are many obstacles to overcome, the Google
    AI team has achieved considerable strides in the past few years, gaining a quantum
    edge by developing Sycamore, a programmable quantum computer. Similarly, IBM has
    now launched the Eagle chip, the first quantum computer with more than 100 qubits
    of hardware [256]. This is only the beginning of an intensive research and development
    program, with the tech giant hoping to increase the number of qubits to over a
    thousand by 2024 [51]. But as was previously stated, protecting these devices
    from ambient noise is a significant constraint when trying to retain the subtle
    characteristics of composite quantum states while still allowing for coherence
    in quantum development. Because of this, a quantum computer’s components require
    ultra-low temperatures in the order of fractions of a Kelvin, which presents hurdles
    for both device design and material development [257]. 4.5.1.2. Trends/technologies
    The main trends and technologies regarding advanced computing styles are as follows:
    (1) Edge AI: Recent advancements in AI efficiency, the rise of IoT devices, and
    the emergence of edge computing have all unleashed the promise of edge AI. This
    has opened up previously unimaginable uses for edge AI, such as helping radiologists
    make diagnoses, assisting in driving cars and fertilizing crops [92]. Since its
    inception in the mid-1990s—paired with the emergence of content delivery networks
    that utilize edge servers positioned near users to stream online and gaming video—edge
    computing has been the subject of much discussion and adoption by professionals
    and businesses. Almost every sector today has tasks that may benefit from adopting
    edge AI. In truth, edge applications are driving the next generation of AI computing,
    which will improve people’s lives in various settings, such as at home, at work,
    at school, and on the road. AI at the edge refers to the application of AI to
    physical devices. In contrast to storing all of an organization’s data in a single
    centralized spot, such as a cloud provider’s data center or a private data warehouse,
    “Edge AI” allows for AI calculations to be performed close to the users at the
    network’s edge. Because the Internet is accessible all across the globe, any area
    might be thought of as its outskirts. Omnipresent traffic signals, autonomous
    equipment, and mobile phones are just a few examples. It might also be anything
    from a shop to a factory to a healthcare facility. Companies of all sizes strive
    to automate more of their processes because doing so improves productivity, effectiveness,
    and safety [258]. Computer software may aid with this through the ability to recognize
    patterns and dependably carry out identical tasks repeatedly. However, it is challenging
    to fully convey them in a system of algorithms and regulations because the world
    is unpredictable and human actions cover infinite circumstances. Today, as edge
    AI has progressed, robots and devices can work with the “intelligence” of human
    cognition no matter what they are. Intelligent IoT apps driven by AI may learn
    to adjust to novel circumstances and effectively complete identical or similar
    tasks [259]. Substantial progress in important areas has allowed for the practical
    deployment of AI models at the edge. Furthermore, developments in neural networks,
    along with other areas of AI, have laid the groundwork for universal ML [260].
    Many companies are finding that they can successfully train AI models and put
    them into action at the edge. AI in the periphery requires widely distributed
    computing resources. Recent advancements in enormously parallel GPUs are currently
    used to run neural networks. The development of devices connected to the IoT is
    partly responsible for the present age’s unparalleled surge in data volume [261].
    The development of sensors, smart cameras, robots, and other data-gathering equipment
    has made it possible to begin using AI models at the edge in nearly all facets
    of business. The increased speed, dependability, and security that 5G/6G is delivering
    to the battleground are also helping IoT use cases [118]. (2) Biologically-inspired
    Computing: The term “bio-inspired computing” refers to creating computer systems
    by drawing inspiration from the natural world. As an aside, computer science is
    also used to model and understand biological processes [145]. Computing architectures
    that take cues from nature can function as autonomous, flexible networks. Similarly,
    bio-inspired computing offers a fresh perspective on AI by building modular, self-improving
    systems [262]. Swarm intelligence refers to the ability of swarms of autonomous
    entities to generate intelligence by collaborating in ways reminiscent of the
    behavior of bees or ants. Biologists, software engineers, computer scientists,
    physicists, mathematicians, and geneticists all work together on the subject of
    bio-inspired computing [263]. Compared to their digital counterparts, biological
    systems have several distinct benefits. AI has advanced thanks to incorporating
    many concepts originally derived from natural processes into machine learning.
    Adaptable and responsive autonomous robots might be extremely useful in high-risk
    settings like conflict zones and hazardous clean-up activities [146]. Tasks like
    crop pollination might be performed by swarms of small robots. Bio-inspired technology
    is being used in cognitive modeling by developing artificial neural network systems
    based on neuron function within the brain. Training, growing, and collaborating
    on computer chips is becoming a reality [264]. When these nodes are linked by
    self-organizing wireless links, they form a system well adapted to modeling issues
    with several basic causes [263]. Self-learning and reconfigurable chips mean less
    time spent loading software and more time spent getting things done. Such systems
    might help explain the propagation of ideas through a community or construct a
    model of brain function that reflects true biological processes. The use of DNA
    in natural computing is a topic of current study. Data storage, covert messaging,
    and even computation are all possibilities that have been proposed by DNA bioinformatics
    studies DNA [265]. DNA molecules may also form practical structures by self-assembly.
    The computer hardware, such as switches, CPUs, and timers, might be replaced by
    biological components. It is already possible to employ some biological substances
    in electronics. Even internal cell programming for purposes like medication secretion
    is feasible. (3) Explainable Artificial Intelligence (XAI): Successful completion
    of computer engineering tasks depends on wise decision-making. Can workloads be
    reliably executed on an automated system? Is there any way to understand how the
    trained models came to their conclusions? Problems like this are typical and must
    be solved until any computer can be used in action [4]. Incorrect decision-making
    about such complicated and cutting-edge technology is costly in terms of resources
    and money. Many AI/ML implementations in computer systems have improved resource
    utilization and energy usage through better decision-making. However, the forecasts
    made by these AI/ML models for computing devices are still not usable, interpretable,
    or implementable. Such restrictions are a common problem for AI/ML models [266].
    Most current research has focused on clarifying how QoS is accomplished, even
    though QoS remains a top priority. Is there anything academics can do to help
    the IT industry move forward? Therefore, when attempting to make educated judgments
    on handling resources (a prime manifestation of AI for computing), a solid grounding
    in Explainable AI (XAI) and experience with XAI methods and tools is required
    [267]. Forecasting of resource and power consumption and SLA variances, as well
    as the implementation of promptly proactive action to resolve these concerns,
    are examples of the types of Explainable AI techniques that may be used. XAI forecasting
    algorithms must be correctly developed to make computing more practical, explicable,
    and deployable [268]. (4) Semantic Web and Decentralized Systems Integration:
    Fog computing has emerged as a software engineering culture and practice that
    combines at least five different technology types: IoT, AI, Cloud-to-Edge Computing,
    Blockchain, and Digital Twins [269]. Various recent projects have presented their
    vision of integration between the Semantic Web and decentralized systems, for
    example, networks based on Blockchain technologies [270]. Here, the main challenge
    is to achieve a new generation of trustworthy, sustainable, human-centric, performant,
    and scalable smart applications. (5) Quantum Internet: It is an ecosystem enabling
    quantum devices to communicate and share data in a setting that uses quantum physics’
    peculiar rules. In principle, this would grant the quantum Internet hitherto unattainable
    skills via standard web apps [59]. Quantum devices, such as a quantum computer
    or a quantum processor, may generate the quantum states of qubits, which can then
    be used to encode information. Sending qubits over a network of physically distinct
    quantum devices is, in essence, what the quantum Internet will be all about. Importantly,
    this will occur because of the strange characteristics of quantum states. That
    probably sounds like the conventional web [271]. However, if one wants to transmit
    qubits, then they need to use a quantum channel instead of a conventional one,
    which requires exploiting the peculiar behavior of quantum particles used to encode
    information onto quantum states. That requires to build up, and apply, relatively
    novel (or exotic) knowledge on the top of what is known about classical computing
    to effectively drive the possible evolution of quantum ecology into an effective
    quantum internet [272] [273] [254]. One could imagine that their favorite web
    browser will not have much in common with the quantum Internet [4]. 4.5.2. Industry
    and sustainability trends In this section, we discuss industry and sustainability
    trends and their related technologies and paradigms. 4.5.2.1. Focus/paradigms
    The following are the main focus or paradigms for industry and sustainability
    trends: Carbon-Neutral Computing: The expansion of the computer age is an important
    factor in the data center industry’s advancement; however, the push towards carbon
    neutrality is a more dramatic paradigm change and the industry’s biggest challenge
    to date. Large-scale cloud providers have pledged to attain zero emissions on
    all initiatives by 2030 [274]. The fight against climate change must include data
    centers. Everything from everyday conveniences like Internet banking and shopping
    to cutting-edge technologies like machine learning, quantum technology, and autonomous
    vehicles would be impossible without them. There is no denying of the ever-increasing
    need for data centers. Nevertheless, because of the damage they cause to the natural
    world, they also attract greater scrutiny [190]. A sustainable future with a zero-carbon
    footprint is possible because of these advancements in electricity, water effectiveness,
    and land utilization. Online conferences and handheld gadgets make it feasible
    for individuals to work from their homes and cut transit carbon emissions; however,
    each bit of data has a carbon footprint of its own [192]. Therefore, whereas electronic
    devices provide opportunities to enhance our oversight of water and materials
    and to support sustainable economic growth, simply sending a message provides
    for the challenging environmental impact of data. However, this may differ greatly
    depending on the spot and efficiency of the data centers that deal with traffic
    [193]. Crucially, as globalization brings online amenities to more societies,
    physical infrastructure, such as data centers, must grow to accommodate an increase
    in consumers, a majority of whom will be in regions around the globe that currently
    lack access to green power availability. 4.5.2.2. Trends/technologies The main
    trends and technologies regarding advanced computing styles are as follows: (1)
    Industry 4.0: The Fourth Industrial Revolution, or Industry 4.0, reshapes how
    goods are made, enhanced, and disseminated. Emerging innovations such as the IoT,
    cloud computing, analytics, and AI/ML are being incorporated into manufacturing
    facilities and processes [275]. Advanced sensors, software with embedded capabilities,
    and robots are used in these “smart industries” to gather information for more
    informed decision-making. When data from manufacturing operations is combined
    with data from Enterprise Resource Planning (ERP), supply chain, customer service,
    and other corporate systems, information that was previously kept separate can
    be seen and understood in completely new ways, which leads to even more value
    being created [276]. Improved efficiency and responsiveness to clients is made
    possible by the advent of technological innovations such as enhanced automation,
    predictive maintenance, and automatic optimization of process enhancements [277].
    To enter the fourth industrial revolution, the manufacturing sector must embrace
    the development of smart factories. The ability to see industrial assets in real-time
    and access preventative maintenance tools may be gained by analyzing the massive
    volumes of big data generated from sensors on the production line. Smart factories
    implementing cutting-edge IoT technology see gains in output and quality [278].
    Manufacturing inaccuracies and costs can be reduced by using AI-powered visual
    insights instead of traditional business models for human inspection. Quality
    assurance staff may monitor production operations from almost any location with
    minimal expenditure using a smartphone linked to the cloud. Companies may save
    money on costly repairs by identifying problems early on with the help of ML algorithms
    [49]. Any business operating in the industrial sector, from individual to process
    production and even in the energy and mining industries, may use the ideas and
    tools of Industry 4.0. (2) Digital Twins: A digital twin is a computerized model
    of and connected to a real-world object that may be used to test and improve its
    design, performance, and usability [279]. Smart sensors embedded in the object
    capture data in real-time, allowing a digital depiction of the asset to be produced
    [99]. The model may be used through an asset’s lifespan, from development and
    testing to actual usage, revamping and eventual retirement. To create a digital
    representation of a physical object, digital twins utilize many technologies.
    The term “IoT” describes the network of interconnected devices and the underlying
    infrastructure that enables them to exchange data and instructions with one another
    and the cloud as a whole. With gratitude to the introduction of affordable computer
    chips and high-bandwidth connectivity, one can now have trillions of gadgets hooked
    up to the global web. Digital twins use data from IoT sensors to replicate physical
    properties in a virtual form [280]. The information is sent into a system or panel
    to be viewed as it changes in real time. Studying, solving issues, and pattern
    recognition are just a few examples of the kinds of cognitive challenges that
    AI seeks to address [281]. AI/ML-based algorithms and statistical models let machines
    do tasks with little to no human help. They do this by relying on patterns of
    observation and inference. Machine learning techniques used in digital twins process
    enormous amounts of sensor data, allowing for the identification of data patterns.
    Optimization of performance, servicing, emissions outputs, and efficiency may
    all be gained using data insights provided by AI/ML [282]. There are several significant
    distinctions between digital twins and modeling: even though both leverage virtual
    model-based simulations, a digital twin maintains a two-way connection and can
    affect the physical object. Offline optimization and the design process are two
    common applications of simulation. Developers use simulators to test out different
    iterations of a product. On the contrary, digital twins are interactive and dynamically
    updated virtual worlds. Both their scope and their utility have increased. 4.5.3.
    Adaptive and self-managing systems In this section, we discuss adaptive and self-managing
    systems and their related technologies and paradigms. 4.5.3.1. Focus/paradigms
    The following are the main focus or paradigms for adaptive and self-managing systems:
    Autonomic Computing: IBM’s autonomic computing program was one of the earliest
    worldwide efforts to develop computing systems with little human intervention
    required to accomplish predetermined goals [30]. It was primarily based on findings
    about how human nerves and thinking work and how they are coordinated—bioinspiration,
    as discussed above. In autonomic computing, researchers explore how software-intensive
    systems can make decisions and act without human interaction to reach the (user-specified)
    “administration” objectives [283]. The concept of control for closed- and open-loop
    systems has significantly impacted the foundations of autonomic computing [31].
    Multiple independent control networks may coexist in practice inside complex systems.
    The integration of ML and AI to enhance resource utilization and efficiency at
    scale remains an important obstacle regardless of investigations into autonomic
    frameworks to handle computing resources, from a single resource (e.g., a web
    server) to resource groupings (e.g., several servers inside a CDC) [4]. Autonomous
    and self-managing systems can be implemented on a spectrum from fully automated
    to partially automated with human oversight through the use of AI/ML to improve
    the efficiency and performance of the computing systems. 4.5.3.2. Trends/technologies
    The main trends and technologies regarding adaptive and self-managing systems
    are as follows: SDN-NFV: The explosion of IoT devices and the concomitant flood
    of sensor data enable knowledge-driven IoT applications, including connected cities
    and smart agriculture [84]. To begin providing such services, one must develop
    a data-gathering method that is flexible enough to adapt to shifting conditions
    in the field. Network programmability (SDN or NFV) enables the easy reconfiguration
    of IoT networks [86]. Current SDN/NFV-based approaches in the IoT environment
    nevertheless fail owing to a shortage of knowledge of resources and overhead,
    as well as incompatibility with conventional protocols [1]. This void must be
    filled by prioritizing resource and power limitations in the creation of SDN/NFV-enabled
    IoT nodes and network protocols. Assigning traffic sources to those Virtual Network
    Functions (VNFs) across the most efficient paths, with sufficient energy and network
    reliability, may maximize the number of active NFV nodes [9]. Summary: Table 3
    lists a summary of open challenges and future directions in Paradigms/ Technologies/
    Impact Areas, along with recommendations for further reading. Table 4 lists the
    summary of Trends/Observations for modern computing along with the recommendations
    for future reading. Table 3. Summary of open challenges and future directions
    in Paradigms/Technologies/Impact areas along with further reading. Paradigms/Technologies/Impact
    areas Open challenges and future directions Further reading Cloud Computing What
    are the tradeoffs that need to be established between the various QoS requirements
    brought on by the large variety of IoT applications operating on cloud systems?
    ACM CSUR [1] Autonomic Computing What additional problems may be addressed by
    an autonomic computing expansion that is based on AI/ML as the number of IoT and
    scientific workloads increases? Elsevier IoT [4] Mobile Cloud Computing How would
    AI-based deep learning algorithms be used to anticipate the resource demands beforehand
    for diverse geographic resources needed for mobile cloud computing, requiring
    new strategies for provisioning and scheduling resources? ACM CSUR [60] Green
    Cloud Computing How can improved methods for effective data encoding for lower
    bandwidth usage and energy-effective transmission in data-intensive IoT devices
    make cloud computing more environmentally friendly? ACM CSUR [162] Fog Computing
    How can AI approaches be utilized to properly schedule tasks when working in locations
    with varying amounts of fog resources? Elsevier JPDC [39] & IEEE COMST [41] Edge
    Computing In what ways edge computing can be utilized to boost power and resource
    utilization, hence enhancing QoS? IEEE COMST [41], [206] Mobile Edge Computing
    How can novel resource provisioning and scheduling policies be developed for mobile
    edge computing that makes use of AI-based deep learning approaches to forecast
    the resource requirements beforehand for resources that are located in different
    locations? IEEE COMST [41], [213] & ACM CSUR [60] Serverless Computing How to
    reduce the cold start time and increase scalability using serverless edge computing?
    IEEE TSC [186] & ACM CSUR [183] Osmotic Computing How can osmotic computing improve
    resource availability or performance at the network edge while moving services
    from the data center to the edge for AI/ML-driven adaptive administration of microservices?
    ACM TOIT [46] Dew Computing How should dew computing allow a highly scalable method
    that can increase or reduce the real-time demands of performing operations at
    runtime via utilizing AI? Elsevier IoT [48] Programming Models How to select a
    programming model that efficiently gathers data when and where it is needed while
    keeping complexity low relative to the total number of processors at hand? Procedia
    Computer Science [115] Virtualization How can unbreakable security for VMs be
    ensured if consumers do not follow recommended practices when it comes to login
    credentials, installations, and other operations? ACM CSUR [122] IoT How to ensure
    that an SLA is upheld while responding to customer requests as quickly as possible
    using IoT applications? IEEE COMST [78] Integrated Computing How may QoS characteristics
    change if communication between layers in a fog-edge/cloud computing paradigm
    is improved? ACM CSUR [108] & Elsevier FGCS [112] Connectivity/ Networking How
    can satisfying the demand or need for network solutions enabling high performance,
    resilience, dependability, scalability, adaptability, and cybersecurity remain
    constant? ACM CSUR [60] & IEEE COMST [61] Container Technologies How can the QoS
    in data processing be enhanced by leveraging containers with virtualization? Springer
    JoS [174] & Wiley CCPE [178] Microservices How to handle errors, ensure data integrity,
    and communicate effectively amongst services in a distributed system using a microservice
    architecture? IEEE TSC [172] Software-defined Networks What are some ways in which
    SDN might help minimize power usage in cloud and edge computing? Wiley ETT [84]
    Distributed Ledger Technology (Blockchain) How can distributed ledger technology
    (Blockchain) be utilized to secure the data for IoT applications? IEEE COMST [108],
    [216] Federated Learning How could companies ensure privacy in federated learning
    services, which differ from learning in data centers in that users’ data is disclosed
    to third parties or the centralized server while exchanging model changes during
    the training stage? Elsevier KBS [222] & CIE [220] Software Engineering How can
    fault tolerance be improved in computing systems dynamically without manually
    writing the software code by utilizing AI to “automatically” diagnose and fix
    an error? Elsevier JSS [129] Distributed Computing Continuum Systems How can Distributed
    Computing Continuum Systems consider all computing tiers as a single system and
    optimize future applications in a decentralized manner? IEEE TKDE [239] Table
    4. Summary of Trends/Observation for modern computing along with future reading.
    Trends/ Observation Open challenges and future directions Further reading AI-driven
    Computing How to optimize the management of resources using the latest AI/ ML
    models in computing systems? Elsevier IoT [4] Large Scale Machine Learning How
    can businesses mitigate the risks associated with the proliferation of sensitive
    information that arise as a result of the proliferation of data produced by AI
    and ML systems? IEEE TKDE [155] Edge AI What strategies should be employed to
    oversee the simulation and information transmission among peripheral devices and
    other systems? What network infrastructures should be utilized to enable this
    communication? Elsevier IoTCPS [92] & ACM SIGCOMM [261] Bitcoin Currency How can
    computing be utilized to maximize the efficiency of computation or processing
    capacity usage in cryptocurrency for cloud mining? Elsevier JNCA [226] Industry
    4.0 How can AI, the cloud, and edge computing be used to do predictive analysis
    that involves company resources? IEEE COMST [275] Intelligent Edge How to deal
    with big problems that come up when designing system-level, algorithm-level, or
    architectural-level developments or innovations for integrated cognitive ability,
    like making decisions in real-time, keeping AI training and inference environmentally
    friendly, and deploying protection? IEEE COMST [88] XAI How can the forecasting
    of resource and power consumption and SLA variances, as well as the implementation
    of promptly proactive action, reduce SLA violations and enhance QoS using XAI?
    ACM CSUR [266] Exascale Computing How to make energy-efficient computing as power-hungry
    as the supercomputers that do calculations and transfer data within the computing
    environment nowadays? ACM CSUR [142] 6G and Beyond What role 6G may play in reducing
    latency and improving reaction times by transmitting data between edge devices
    at high speeds? IEEE COMST [98] Quantum AI What steps should be taken to build
    the AI cloud-based quantum computing infrastructures that are expected to be the
    foundation for our usage of quantum computers and simulators, which will supplement
    our existing classical computing hardware? Wiley SPE [51] Quantum Internet How
    can the benefits of quantum networking be preserved while integrating the quantum
    Internet into currently operating conventional technology that will have to exist
    alongside and communicate effortlessly with today’s Internet services? IEEE COMST
    [254] Analog Computing How is it that analog computers can do complicated computations
    faster and more accurately than their digital equivalents, which utilize ML methods?
    Nature Electronics [146] Neuromorphic Computing How might neuromorphic systems,
    which model the brain’s structure and function and use analog circuits to do AI
    tasks, pave the way for creating incredibly adaptable, self-learning machines?
    Nature Computational Science [149] Biologically-inspired Computing What can researchers
    take away from brain cells concerning ways to minimize the energy needed for computation,
    AI, and ML, given that these cells can easily combine smaller tasks to execute
    larger ones? Elsevier ESA [263] Digital Twins How can network digital twins aid
    in speeding up preliminary installations by preparing navigation, protection,
    digitization, and evaluation in simulation while offering the scalability and
    interoperability of complex networks? IEEE COMST [280] Net Zero Computing How
    can companies mitigate the negative ecological impact of their IT infrastructure
    by constructing environmentally friendly data centers and improving energy effectiveness,
    given that these centers use significant quantities of electricity and release
    enormous quantities of waste heat while also providing powerful computing services?
    IEEE COMST [190] 5. Impact and performance criteria In this section, we discuss
    the impact of contemporary computing and performance criteria. 5.1. Performance
    metrics We are considering QoS, SLA, autoscaling, and fault tolerance as performance
    metrics for computing systems. 5.1.1. QoS and SLA Predicting how a cloud computing
    system will work in real-time is a major difficulty, even if AI techniques are
    used [284]. The efficiency of a computer may be measured using QoS metrics, including
    execution time, cost, scalability, elasticity, latency, and dependability. A SLA,
    a legally binding contract between a cloud service consumer and provider, defines
    QoS standards and potential penalties should they be violated [285]. Today, various
    IoT applications can use blockchain and similar technologies. Each one has its
    own QoS factors that depend on its area, goal, and demand [286]. An SLA may also
    be assessed with a metric called SLA violation rate, which determines compensation
    in the event of an SLA breach by estimating the divergence of the real SLA compared
    to the needed (estimated or predicted) SLA [287]. Since compromized QoS in one
    cloud service may negatively impact the QoS of the entire computing system, QoS
    is becoming increasingly crucial while assembling cloud services. Provisioning
    the proper quantity and quality of cloud resources that will satisfy the QoS of
    an application’s price range, response time, and deadline is essential for providing
    an effective cloud service [288]. Consequently, cloud providers should guarantee
    to offer sufficient resources to minimize or reduce the SLA violation rate, allowing
    users’ workloads to be executed in accordance with their set time and cost constraints
    [289]. In that regard, the diversity of applications and their behaviors on different
    machines requires a tighter description of their needs to minimize SLA violation
    while not over-provisioning infrastructure [290]. QoS-aware resource management
    methods, which can determine and meet the QoS needs of a computing system, such
    as SLO-driven modeling and execution-reordering of web requests, are crucial to
    its success in the future [291]. Several research issues must be overcome before
    QoS can be attained effectively [292]. Initially, the execution time of an application
    is large, and its performance is diminished due to a lack of cloud resources during
    runtime—which can be compounded by transparent processes to the developer, such
    as garbage collection, magnifying the potential of inexplicable SLO violations
    [293]. Additionally, finding the requirement for effective SLA-aware resource
    management methods decreases the SLA violation rate and preserves the overall
    efficiency of the computing system. Finally, to reach the ultimate goal of having
    multiple clouds, there has to be a unified SLA standard across all cloud providers
    [294]. Since many IoT applications rely on cloud computing systems that employ
    AI-based supervised or unsupervised algorithms for learning or models for forecasting,
    it is imperative to determine the appropriate balance amongst various QoS needs.
    5.1.2. Autoscaling Thanks to the dynamic nature of the cloud, self-adapting techniques
    may be used to reduce resource costs without compromizing QoS [295]. Resource
    autoscaling, or strategy, reconfiguration, and provisioning, allows for self-additivity.
    Scientists have looked into autoscaling, or the dynamic modification of computational
    resources like VMs, for several reasons [123]. These include the desire to learn
    more about (a) horizontal changes, or the addition or removal of VMs; (b) vertical
    transformations, or the addition or removal of VM resources; (c) choice-making
    techniques, such as analytical modeling, control theory, and neural networks;
    and (d) utilizing a range of pricing models, such as on-demand. When it comes
    to latency-sensitive QoS requirements, the primary challenge for autoscaling methods
    is figuring out how to make a scaling decision quickly enough. AI prediction is
    the initial step towards making decisions in the quickest way possible [248].
    However, traditional ML may not be up to the task when it comes to IoT applications
    requiring real-time mistake correction due to a lack of autonomous error correction
    [296]. Also, the rise of latency-sensitive IoT apps and microservices that need
    responses in the range of milliseconds has made things worse while container-based
    solutions and burstable efficiency resources should make it possible to deploy
    and provision resources in the cloud quickly. To prevent a potentially disastrous
    situation, a smart car’s onboard computer constantly monitors data such as the
    vehicle’s speed, the location of other drivers and passengers, and the road conditions
    [297]. The cloud alone cannot answer this problem due to the instability and latency
    in connections between the cloud and users; instead, autoscaling techniques for
    IoT applications must take these factors into account [298]. The truth is that
    autoscaling needs to be made bigger because the cloud naturally gets in the way
    of Industry 4.0 ideas, like real-time management, and making decisions without
    a central authority. 5.1.3. Fault tolerance Providers of cloud computing services
    owe it to their customers to make such services available without interruption,
    regardless of what problems arise [299]. To meet the QoS standards of a computing
    system efficiently, fault tolerance approaches are employed. Software, hardware,
    and even networks may all go wrong when a computer system operates. In addition,
    fault resilience guarantees the reliability and accessibility of cloud services
    [4]. Timeout breakdowns, overload issues, and resource-lack failures are further
    examples of cloud dependability issues. A major breakdown has the potential to
    cause a cascade of failures in the system [300]. Several proactive and reactive
    fault tolerance approaches have been developed to cope with these kinds of failures.
    The most common method of handling faults in long-running processes is called
    “checkpointing”, and it involves preserving the current state after each modification
    [301]. Additionally, checkpoints are employed if there is a possibility of not
    beginning at the same position [1]. Replication-based resilience is another well-known
    method; it involves duplicating the nodes or jobs until they are completed. If
    a system is overloaded or malfunctioning, a task migration-based resilience solution
    can move the work to another computer. Computer systems must have autonomous resilience-aware
    resource management technology, reliability of service methods, and reliable information
    integrity (e.g., blockchain) to keep running. Reliability impacts QoS in cloud
    computing while still delivering it effectively. One of the biggest obstacles
    in cloud computing is figuring out how to deliver a secure and effective cloud
    service while cutting down on power consumption and emissions [302]. Cloud computing
    has built-in redundancy to maintain service availability, QoS, and performance
    guarantees. Resource management must consider varying failures and workload prototypes
    for medical care, urban planning, and agricultural applications to run well [71].
    Predicting failure in systems that use cloud computing is difficult and can impact
    the dependability of the system [301]. Predicting faults and achieving the requisite
    dependability of the cloud service while maintaining QoS necessitates several
    machine or deep learning approaches [13]. Replication-based fault tolerance solutions
    are effective for IoT applications because they reduce task delay and response
    time. A dependable cloud storage system that will offer an effective retrieval
    system for processing big data is also required to deal with big data applications
    [303]. 5.2. Efficiency metrics We are considering energy consumption, carbon footprint,
    and serviceability as efficiency metrics for computing systems. 5.2.1. Energy
    consumption Data collection and processing have risen exponentially during the
    last several years. This pattern has been pushing cloud systems to the limits
    of their computational and, by extension, energy consumption capacities [304].
    Annually, CDCs have increased their power use by around 20% to 25% [305]. This
    shift has led to the rise of decentralized computer architectures such as Fog
    and Edge. The latency and cost-effectiveness of cloud computing are all vastly
    improved by moving parts of its computation to distributed edge devices and networks.
    There nevertheless exist difficulties associated with this. Irregular energy supply,
    even without the power supply itself, presents significant issues for numerous
    highly critical and remote sensing applications. The ever-growing number of IoT
    devices and the data they produce have put networking’s ability to handle information,
    compute, and transfer data throughput to the test [162]. Meanwhile, smaller IoT
    devices are currently created with limited computing power, storage spaces, and
    energy. Hence, it is imperative to boost the performance of fog and edge nodes
    in the network. Sustainability in CDCs and minimizing their carbon impact have
    also become more pressing concerns. This must be accomplished without lowering
    the bar for QoS [306]. Notwithstanding the obstacles, there have been several
    advances in this area. Software, hardware, and transitional approaches have all
    been taken to the energy management problem. Approaches and techniques are being
    designed to optimize software efficiency, supported by computational models [306].
    One example is mobile edge computing offloading. Hardware-wise, particularly for
    the application, devices were designed to provide peak performance while minimizing
    energy consumption. Energy efficiency in Wireless Sensor Networks (WSNs) has been
    extensively researched [4]. Fog/edge-node sleep time scheduling, active resource
    management, and additional energy-saving strategies have all been used in the
    intermediate phase. There are still many unanswered questions and potential avenues
    for development when it comes to the effectiveness and longevity of fog, edge,
    and cloud infrastructures. Advanced algorithms for encoding data into fewer bits
    are explored to reduce transmitter power needs, which are crucial due to limited
    transmission bandwidth, more critical than direct CPU power needs. Despite the
    need for specialized hardware, encoding methods may be used by taking advantage
    of the universal encoders present in virtually all mobile devices [13]. Yet, it
    has become impossible to lower the ideal bandwidth due to the rising quantity
    of data exchange and loss. Preparing for CPU and data utilization in a way that
    minimizes heat generation requires modeling at the transistor level, which necessitates
    the development of 3D thermal simulation systems [75]. Lastly, the aim is to minimize
    power consumption to the point that the CPU and transceiver may be powered entirely
    by energy harvesting or scavenging approaches [307]. Consequently, the Fog/Edge
    network’s granularity may be decreased, leading to more widely scattered, overbearing,
    and resilient architectures. In various fields, like energy limits, blockchain
    algorithms might be studied with various versatile AI-based learning approaches
    for enhanced energy scheduling. 5.2.2. Carbon footprint End-user needs for applications
    and the resulting growth in storage in the Exabyte range will result in the first
    Exascale system by 2025, followed by a Zettascale system by 2035 [2]. While this
    is certainly something to be proud of, there are also many difficulties that come
    along with it. Keeping everything running requires massive amounts of energy,
    which poses a major obstacle. At the moment, over ten percent of the world’s power
    is used each year by the ICT sector [190]. The rebound effect, which leads to
    even higher demand and consumption, makes it counterproductive to create ever-larger
    systems by increasing efficiency. The next generation of autonomous system paradigms
    will likely place a greater emphasis on power and carbon footprints in light of
    climate change and the projected 1.5 °C rise in worldwide temperatures owing to
    emissions of carbon dioxide by 2100 [2]. This is not merely about lowering energy
    use per unit of processing, as is the case now, but also about more basic issues
    with systems that assume continuous stable power supplies, connectivity with sources
    of clean energy, and alternate techniques of minimizing energy usage [308]. The
    study and treatment of systems as living ecosystems rather than as collections
    of discrete components is a topic of great interest, and this includes the comprehensive
    integration of managing energy (asynchronous computation, power scaling, wake-on-LAN,
    air conditioning, etc.). 5.2.3. Serviceability/usability The fields of human–computer
    interaction and networked systems have yet to fully merge with each other. This
    closer synchronization would be especially helpful for cloud computing [1]. Despite
    significant work on resource management and the back-end associated concerns,
    accessibility is a vital component in lowering the costs of organizations investigating
    cloud services and infrastructure. Costs associated with labor might decrease
    since customers will receive superior service and increase their output [309].
    NIST’s Cloud Usability model addresses five dimensions of cloud usability: capability,
    personalization, reliability, security, and value, all of which have been highlighted
    as critical issues [310]. The term “capable” refers to the degree to which cloud
    service can fulfill the needs of its customers. With the assistance of personal
    customization options, individuals and businesses will have the capability to
    modify the visual style and adjust or eliminate features from interfaces for various
    services. Trustworthy, robust, and useful are attributes associated with possessing
    a system that fulfills its duties throughout state situations, is safely protected,
    and delivers value to customers accordingly. Current cloud initiatives have mostly
    concentrated on wrapping up sophisticated services into APIs that can be accessed
    by end users [309]. HPC Cloud is the most evident example. To make HPC applications
    more accessible and easier to use, researchers have developed several different
    services. In addition to being packaged as services, these systems provide Web
    interfaces through which their settings may be set and their input and output
    files managed. DevOps is another path associated with cloud usage that has gained
    popularity in recent years [311]. DevOps has increased the efficiency of both
    software engineers and administrators when it comes to developing and delivering
    remedies on the cloud. Cloud computing is important not only for creating brand
    new solutions AIOps and MLOps [312] but also, for streamlining the process of
    moving existing applications from onsite settings to adaptable, multi-tenant cloud
    services. 5.3. Social impact We are considering the digital divide, ethical AI,
    and digital humanism as social impact metrics for computing systems. 5.3.1. Digital
    divide Corporations in rural areas have significant challenges due to the difficulty
    of gaining a connection to broadband connectivity and, by extension, cloud-based
    resources [313]. Access to the web is one example of a long-standing infrastructural
    gap between urban and rural areas. There are a lot of companies that cannot expand
    and innovate because they lack access to new technology. Businesses in rural areas
    face another obstacle: the high cost of maintaining and upgrading on-premises
    IT infrastructure. Cloud computing’s main benefits are the ability to work together
    and think creatively. The cloud encourages teamwork by facilitating real-time,
    distributed collaboration. This greater collaboration encourages invention. As
    a result, rural enterprises may now compete on an equal basis with their metropolitan
    competitors [314]. Accessibility to data and fundamental information is also crucial.
    The benefit of using the cloud has increased significantly with the advent of
    generative AI. Comprehensive sales, marketing, and manufacturing capabilities
    are provided by core AI services, but these cannot be reproduced with human processing
    and can be too costly to install on-site for modest organizations. The proliferation
    of cloud computing has expanded business opportunities, but not equally. By utilizing
    the cloud, companies in rural areas may overcome the constraints of their physical
    location [315]. Cloud computing’s greater availability, decreased cost, scalable
    effectiveness, and improved cooperation may breathe new life into the rural economy
    and propel it towards long-term success. 5.3.2. Ethical AI AI systems require
    vast amounts of data, including details on businesses and their clients [316].
    The value of knowing the data owner surpasses that of having private information
    that cannot be linked to a specific person. When dealing with sensitive information,
    companies regularly face problems related to data security and regulatory compliance
    [317]. Autonomic computing using AI needs to take into account privacy rules and
    data protection. While AI has the potential to be a game-changer, it has not always
    been successful in achieving its aims. A hunt for answers by an AI may result
    in a flood of insensitive comments [318]. The vast number of AI decisions and
    the stakes involved make this field fraught with peril. Prior to expanding the
    use of this invention, it is crucial to develop accountability and ownership.
    5.3.3. Digital humanism The unavoidable consequences of digital colonization driven
    by business need a counter-force of digital humanism motivated by care for humanity
    and the Earth [319]. We have never been both so interdependent, yet so isolated.
    Modern digital systems allow for global communication. One no longer has to be
    in the same room as someone else to have a conversation, collaborate on a project,
    or just have fun with them. The cell phone is rapidly becoming an integral part
    of people’s daily lives all across the world. Connectivity between the developing
    world and the developed nations of the world is rapidly expanding, for both good
    and ill. These interconnections are causing conflicts that could have been prevented
    when individuals and ideas were separated by space. Western materialism and commerce
    meet Eastern spirituality and culture in the virtual world [320]. Therefore, although
    humans may all end up in the cloud at some point, the barriers of mutual respect
    and compassion that keep us from crashing into one another are more than frayed.
    Most modern digital accounting and tracking systems are used by private companies
    seeking to maximize profits at the expense of others, enriching a few elites at
    the expense of a much larger underclass [321]. In contrast, if the cloud could
    be utilized for humanity’s benefit, manufacturing and distribution might be dramatically
    enhanced. Controlled well, such instruments will allow for fine-tuning of many
    crucial societal functions, particularly at the subnational and neighborhood levels.
    5.4. Security and compliance We are considering data protection, privacy regulations,
    and resilience to attacks as security and compliance metrics for computing systems.
    5.4.1. Security, privacy and resiliency In recent years, there has been a dramatic
    change in academia and business towards the IoT, edge computing, and cloud computing
    in order to serve customers better. With this massive paradigm shift, comes a
    slew of problems and difficulties with protecting the confidentiality and safety
    of the information stored on these devices [322]. Edge computing’s many distinguishing
    features – its low latency, geographical dispersion, end-device accessibility,
    high processing power, variability, etc. – make it imperative that security and
    privacy mechanisms be both flexible and powerful [323]. In addition, creating
    universally compatible software platforms is challenging due to the wide variety
    of use cases and device types. Several elements become important in the research
    of these security and associated challenges in the cloud and fog computing models:
    End-user confidence and privacy; verification and validation of sources inside
    nodes; secure communications between sensor, compute, and broker nodes; detection
    and prevention of malicious attacks; secure, reliable and decentralized data storage,
    such as Blockchain [231]. Some of the problems that have already been addressed
    in this field include adaptive mutual authentication, identifying and retrieval
    of harmful or malfunctioning nodes, the detection and defense against assaults,
    the avoidance of harmful hazards, and the protection of user information from
    theft. Unmanned Aerial Vehicle (UAV)-aided computing devices can now maintain
    their anonymity while contributing to distributed frameworks in AI technology,
    such as computer vision and path learning, supporting data processing and decision-making
    [324]. Other efforts in fog forensics have also given digital evidence by recreating
    prior computer activities and identifying how these events contrast with cloud
    forensics in important ways. The past few years have seen significant progress
    in several key areas related to Fog Radio Access Networks (F-RANs), including
    mobility management, interference reduction, and resource optimization [325].
    Novel approaches have evolved for varied applications handling privacy challenges.
    Face recognition and resolution, vehicle crowd sensing, geographic location sensing
    and data processing, renewable node storage systems and data centers, and fog-based
    public cloud computing are promising new research areas. Prevention against data
    theft, attacks involving man-in-the-middle, confidentiality of users, location
    confidentiality, forward privacy, reliable user-level key management, and many
    other weaknesses have all been addressed through such efforts [4]. There are scaling
    issues with many fog/cloud privacy and security models that prevent them from
    fully applying to the next-generation edge computing transition [326]. Because
    of fog computing’s decentralized nature, numerous new security concerns, which
    are not an issue in the cloud, emerge in the fog layer and IoT devices. The deployment
    of authentication systems is hampered by the prevalence of threats such as advanced
    persistent threats (APT attacks), malware, distributed denial of service (DDoS)
    attacks, two-way communication, and micro-servers without hardware protection
    mechanisms in edge data centers [327]. Additionally, these studies show how the
    mobile edge computing architecture might change in the future. For example, edge
    nodes working together could make real-time encryption more efficient. The computational
    capacity of both edge and distant resources has not been completely used in previous
    efforts, and security flaws have been addressed from a restricted viewpoint. New
    phenomena appear when cloud-like capacities are distributed to the network’s periphery
    [231]. Edge data center collaboration, service migration on a local and global
    scale, end-user concurrency, QoS, real-time applications, load distribution, server
    overflow issues, stolen device detection, and dependable node interaction are
    all examples of such scenarios. Future studies can focus on new areas, such as
    evolving game-theoretical strategies to the privacy algorithms encouraged by adversarial
    attack scenarios, communication protocols in sensor cloud systems, and clustering
    model-based security evaluation (AI-based forecasting approaches), which can be
    investigated as potential solutions to these issues [236]. Mobile devices’ presence
    in these data centers should be taken into account by safeguarding systems. 5.5.
    Economic and management We are considering cost-efficiency, resource allocation,
    application design, computing economics, and data management under economics and
    management for computing systems. 5.5.1. Cost-efficiency Minimizing cloud expenditures
    while maximizing application performance and efficacy is the goal of cloud cost
    optimization, which entails striking a fine balance between technological standards
    and corporate goals [304]. Cost-effective cloud computing refers to the practice
    of utilizing cloud providers in the most economical way feasible to operate software,
    complete tasks, and create value for a company. Optimization as a practice varies
    from fundamental business management to challenging scientific and technical fields
    including operational research, statistical and data analysis, and modeling and
    prediction [316]. Corporations may maximize the return on their investments in
    cloud computing through cost optimization, which reduces wasteful expenditures
    and strengthens their operational effectiveness [328]. By avoiding economic hazards,
    aligning spending with company goals, and establishing a secure, scalable, and
    cost-effective cloud infrastructure, corporations can maximize the return on their
    investments in cloud computing. In general, efficient cloud cost management preserves
    essential resources against the risk of unanticipated expenditures and financial
    mismanagement. Changing to a cloud-native methodology involves more than just
    updating technology; it also necessitates a substantial adjustment in mindset
    [1]. Building scalable apps that make efficient use of resources requires developers
    to think in terms of the cloud from the start. To optimize cloud expenditures,
    a cloud-native application design requires an in-depth familiarity with the services
    and resources offered by different cloud service providers. Managed service options
    are superior to autonomous technologies since they require less effort and time
    investment [329]. A sophisticated knowledge of the user application’s demands,
    regulatory demands, and possible financial consequences is necessary to choose
    between a single and multi-cloud installation plan. An organization’s administration
    might be simplified by adopting a single-cloud approach, but doing so could leave
    it vulnerable to vendor lock-in and service restrictions [2]. Contrarily, a multi-cloud
    strategy can increase complexity in administration but has the ability to optimize
    costs, provide greater flexibility, and lessen the danger of vendor lock-in. Identifying
    which is the most economical and profitable implementation approach requires careful
    consideration of the specific features, pricing methods, and competencies of different
    cloud services. 5.5.2. Resource allocation The sheer size of today’s CDCs makes
    resource management in networked systems a formidable challenge. In large-scale
    distributed architectures, the variety of network devices, elements, and ways
    to connect raises the difficulty of resource management strategies [330]. Consequently,
    there is a necessity for innovative resource allocation methodologies that would
    add to the reliability and effectiveness of these systems while keeping them cost-effective
    and sustainable. While resource management is fundamental to distributed systems
    (be it the cloud, the IoT, or fog computing), additional guarantees are needed
    to ensure that these systems operate well in terms of latency, dependability,
    cost-effectiveness, and throughput [331]. The software layer is just one part
    of these larger systems, which also require consideration of networking, server
    architecture, and ventilation. By incorporating blockchain technology into operations
    like resource sharing and VM migration, cloud systems may be more secure [332].
    There is a pressing need to investigate novel approaches to managing computer
    system resources by taking a systemic perspective and using AI models. Moreover,
    experiment-driven strategies for examining methods to optimize resource management
    methods may be investigated [333]. Borg was opened up by Google as Kubernetes,
    which is an instance of a cluster management system that incorporates data abstraction
    into resource management. Users are freed from worrying about the nuts and bolts
    of resource management and may instead focus on composing cloud-native applications.
    Borg conceptually separates the whole cluster into cells, each housing a Borgmaster
    (controller) and a Borglet (which initiates and terminates tasks within the cell’s
    perimeter). The master node coordinates with the Borglets and processes RPCs from
    clients requesting actions like creating jobs or reading data [253]. This centralized
    design is very suitable for scaling. The primary benefit of this architecture
    is that operations that have already been started will continue to execute even
    if the master or a Borglet fails [334]. A system known as Mesos can facilitate
    the equitable distribution of commodity clusters. It coordinates the use of commodity
    clusters by many systems. The fundamental idea is to make use of available resources
    [335]. In this model, Mesos determines how many resources to give to every framework
    depending on the limitations associated with that framework, and the frameworks
    then choose which offers to take. Thus, scheduling choices must be made by frameworks.
    In addition, Mesos facilitates the creation of domain-specific frameworks (like
    Spark) that may greatly enhance performance. To schedule and manage available
    resources, YARN is used as a framework [1]. It enables services to ask for computing
    power at various topological levels, including individual servers, networks, and
    whole racks. The primary component in charge of allocation is YARN’s resource
    management. Similarly to Mesos, it enables several frameworks to collaborate on
    the same commodity clusters [334]. YARN’s integrated reliability masks the complexities
    of failure identification and recovery. • Heterogeneous Resources and Workloads:
    There is a lack of cohesion in the existing literature about managing resources
    and workloads in diverse cloud settings. As a result, there is no common setting
    in which cloud applications can make optimal use of heterogeneity in VMs, vendors,
    and hardware architectures [151]. Consequently, the initiative recommends an overarching
    program that takes into consideration diversity throughout. Effective solutions
    can be picked from a collection of workload and resource handling methods, depending
    on an application’s needs [336]. Heterogeneous memory control is necessary for
    this purpose. Modern memory control techniques rely heavily on hypervisors, thereby
    minimizing the potential advantages of heterogeneity. Recent calls for action
    have advocated for alternatives that focus on heterogeneity awareness in the guest
    OS. Another chasm is that between heterogeneity and abstraction [337]. Accelerator-specific
    languages and low-level programming initiatives are necessary for today’s programming
    paradigms to utilize hardware processors. Furthermore, such models allow for the
    creation of useful research software. As a result, service-oriented and user-driven
    applications on cloud platforms are hampered in their ability to take advantage
    of heterogeneity. Kick-starting an international community initiative to come
    up with an open-source, high-level programming language that is suitable for cutting-edge
    and creative Web-based applications in a heterogeneous setting is a worthwhile
    step to take [338]. Whenever fog computing matures and application migration occurs,
    such aids will be invaluable. 5.5.3. Application design By 2025, analysts predict
    61 billion connected devices will generate 40 percent of global data at the cost
    of $2.5 trillion [339]. Medical services, near-real-time traffic management systems,
    precise farming, intelligent towns and cities, etc., are just a few examples of
    IoT applications that are driving the need for improved processing capacity, data
    storage, confidentiality, security, and trustworthy communication. Additionally,
    as the data produced by these devices is used to resolve real-time challenges,
    credibility, uniformity, and accessibility of the data must be maintained. It
    is challenging to design such complex applications for IoT systems [340]. As a
    result, it is essential to develop application designs and architectures that
    are not only dependable and quick enough to deliver effective efficiency but additionally,
    scalable to manage massive amounts of data through these devices. These are the
    most important factors to consider when developing such apps for cloud environments.
    Firstly, a data packet’s latency is the time it takes to travel between an IoT
    device and the cloud before returning. For time-sensitive information, even a
    millisecond delay might have drastic consequences. For instance, having a crisis-sensing
    instrument that only sounds an alert after a disaster has already taken place
    is not a viable solution. Data needing immediate reaction should be analyzed as
    close as possible to the origin [341]. Secondly, if all this data is transferred
    to the cloud for storage and analysis, the resulting traffic will be massive,
    using up all available bandwidth. The distance between the device and the cloud
    also increases transmission latency, which slows down responses and reduces user
    experience. Therefore, some tasks must be transferred from the cloud to an edge
    server located between the Internet servers and the mobile device: such solutions
    better satisfy end-users’ requirements. By storing and processing certain IoT
    data directly on IoT devices, the fog computing model reduces the load on the
    cloud and keeps costs down. Large-scale, geographically dispersed applications
    that rely heavily on real-time data benefit from the fog’s consistency [342].
    Fog computing may be the most appropriate choice to enable effective IoT and provide
    reliable and safe services and resources to many IoT users. Big data analytics,
    IoT devices, fog, and edge computing have become the foundations for smart city
    programs worldwide [343]. In transport, fog computing is useful for several tasks,
    including vehicle-to-vehicle interaction, smart-sensor-based congestion control
    system management, driverless car management, and self-parking, among others.
    Furthermore, governments may employ these applications to make the lives of their
    residents safer and more environmentally friendly, making them a sustainable approach.
    Emergency services, such as those dealing with fires or natural disasters, can
    also benefit from this technology by receiving timely alerts about developing
    crises to help them make informed choices. Farming software that tracks weather
    and climatic data like rainfall, wind speed, and temperatures, makes it easier
    for farmers to reap a harvest. An IoT agriculture platform is suggested for cloud
    and fog computing, with applications including automated agricultural monitoring,
    visual inspection for pest control, and more efficient use of farm resources [340].
    Meanwhile, in the medical field, more and more people are using fitness trackers,
    blood pressure monitors, and heart rate monitors to track vital signs and gather
    data for medical analysis. Thanks to these innovations, physicians can check their
    patients’ health from afar, and patients have more say in their care and decisions.
    5.5.4. Computing economics There are several promising new avenues for study in
    the financial aspects of cloud computing. It is becoming clearer that the lower
    costs of container deployment can be used to handle real-time workloads [344].
    This is speeding up the switch from VMs to containers for cloud computing. • Cost-Effective
    Computing Models: In serverless computing, no billing for computing resources
    is made until a function is invoked. Processes executed in these lambda functions
    tend to be narrower in focus and designed for processing data streams. Whether
    or not serverless computing is beneficial for a given application depends on its
    projected runtime behavior and workload [1]. Averaged versus peak transaction
    rates; scaling the number of simultaneous operations on the infrastructure (i.e.,
    operating multiplies simultaneous functions with a growing number of consumers);
    and benchmark implementation of serverless functions across various backend hardware
    platforms [345]. Conversely, increased employment of fog and edge computing characteristics
    with cloud-based data centers gives tremendous study potential in cloud economics.
    • Economic Impact of Computing Technologies:It is possible to lower the expenses
    of running cloud services and infrastructure by combining reliable resources of
    the cloud with more ephemeral resources at the consumer’s edge. To make such technology
    accessible at the edge, nevertheless, it is anticipated that consumers will require
    some sort of inducement [157]. Expanding the cloud market to include new types
    of service providers is possible because of the accessibility of cloud and edge
    resources. Researchers call these intermediate facilities located between the
    conventional data center and the user-owned or provisioned resources, microdata
    centers [346]. The federation concept in computing allows for many microdata center
    operators to operate together to distribute workloads in a given region at desired
    pricing. 5.5.5. Data management Metadata handling for datasets is not given much
    attention in cloud IaaS and PaaS services for storing and information administration,
    which instead prioritize file, partially structured, and structured data separately.
    In contrast to traditional, organized data warehouses, proponents of “Data Lakes”
    advocate for businesses to store all their data in unstructured formats on the
    cloud, using services like Hadoop [1]. Nevertheless, using them might be difficult
    due to the absence of information for tracking and defining the origin and authenticity
    of the data. Throughout the past ten years, research archives have become exceptional
    in handling vast, varied datasets and the accompanying information that provides
    context for their usage. Collocating data and computing resources in a small number
    of strategically located data centers worldwide allow for economies of scale,
    a major advantage of CDCs [348]. Nevertheless, bandwidth restrictions across worldwide
    networks and delays in gaining access to data present obstacles [350]. This becomes
    an increasingly pressing issue as IoT and 5G mobile networks expand. However,
    the cloud providers’ access to private data and critical confidential information
    still poses a risk for businesses that need to guarantee strict privacy for their
    end-users. Likewise, there are no foolproof auditing techniques to prove that
    the cloud service provider has not obtained the data, even though regulatory measures
    are in place. In a hybrid setup, customers may handle confidential information
    under their watchful eye while still taking advantage of the advantages of public
    clouds, thanks to the proximity of private data centers to public CDCs connected
    by an independent high-bandwidth network. Furthermore, effective approaches to
    managing resource flexibility in such contexts should be explored [351]. In addition,
    it is preferable to have high-level programming abstractions and bindings to platforms
    that can allocate and oversee resources in these massively dispersed settings.
    Table 5. Summary of open challenges and future directions in the above-discussed
    impact of modern computing and performance criteria with future reading. Impact
    and performance criteria Open challenges and future directions Further reading
    QoS and SLA How can SLAs and QoS be preserved in real-time when cloud computing
    and edge resources and tasks are executed? ACM CSUR [287] and Wiley IJCS [289]
    Autoscaling How can it be ensured that computing resources need to meet SLAs and
    QoS are effectively autoscaled in real-time? ACM CSUR [295] Fault Tolerance How
    can reliable support be continuously provided with environmentally-friendly services?
    Elsevier SETA [300] Energy Consumption How can modern computing benefit from AI/ML
    to provide environmentally-friendly services and low energy consumption? Springer
    Cluster Computing [304] Carbon Footprint What technological advancements may decrease
    the impact of climate change and how could environmentally-friendly computing
    have a lower-carbon footprint? IEEE COMST [190] Serviceability What methodologies
    should be employed to develop and measure key performance indicators, also known
    as KPIs, in order to assess the success of initiatives that aim to make cloud
    computing more usable and secure? Wiley ETT [309] Digital Divide How does the
    use of the cloud help overcome the digital divide? Can ICTs help bridge the digital
    divide in infrastructural growth? Elsevier Telematics and Informatics [313] Ethical
    AI When designing and implementing AI in computing devices, what ethical concerns
    must be taken into account? Nature Machine Intelligence [347] Digital Humanism
    How may digital tools stimulate original thought and the independent thinking
    of individuals, and whether or not the synergy of these traits can promote a digital
    shift in the workplace? Elsevier Journal of Business Research [319] Security,
    Privacy & Resiliency What measures can be taken to ensure that personal information
    is protected and data is securely processed in the cloud when IoT apps collect
    and analyze massive amounts of data? IEEE COMST [323] Cost-Efficiency How can
    impending difficulties like the prohibitive cost of setting up and running big
    systems testing environments and the influence of global warming on the architecture
    of upcoming systems be overcome? Springer Cluster Computing [304] Resource Allocation
    What are the best practices for successfully provisioning cloud and edge resources
    for many IoT apps before scheduling such resources? ACM CSUR [333] Heterogeneous
    Workloads/ Resources How can the heterogeneity of resources and workloads impact
    the efficiency of a computing system at runtime? ACM CSUR [151] Application Design
    How can more efficient IoT apps be developed to make greater use of available
    computer power? ACM CSUR [201] Computing Economics How can businesses strengthen
    their CapEx (Capital Expenditure) and OpEx (Operational Expenditure) strategies
    by learning about the primary economic advantages of cloud computing in terms
    of return on investment (ROI), total cost of ownership (TCO), and relocation?
    Elsevier Telecommunications Policy [344] Data Management How can organizations
    make optimal use of AI/ML approaches for enormous amounts of data to ensure efficient
    data administration and analysis? Springer JBD [348] & ACM CSUR [349] Download
    : Download high-res image (706KB) Download : Download full-size image Fig. 2.
    Hype cycle for modern computing. Finally, with the IoT, deep learning, and blockchain
    all set to be housed on clouds, it is important to look at specialist data management
    services to ensure their success [352]. As indicated above, IoT will include a
    strengthened requirement to deal with streaming data, their effective storage,
    and a requirement to integrate data management on the edge effortlessly with administration
    in the cloud [38]. When unregulated edge devices are involved, integrity and authenticity
    become even more crucial. As the use of deep learning grows, it will become more
    important to be able to manage trained models well and make sure they can be quickly
    loaded and switched between to make online and distributed analytics applications
    possible [349]. Finally, blockchain and decentralized ledgers can improve data
    management and tracking by providing greater transparency and auditability. While
    initially used by the financial sector (of which cryptocurrencies are only one
    prominent example), these systems may be expanded to store other company data
    safely with an inherent auditing record. Summary: Table 5 lists the summary of
    open challenges and future directions in the above-discussed impact of modern
    computing and performance criteria, along with recommendations for future reading.
    6. Emerging trends in modern computing The advent of modern computing technology
    has made it possible to resolve several real-world issues, including delayed responses
    and low latency. It has facilitated the development of start-ups led by promising
    young minds from all over the world, providing access to massive computing capacity
    for tackling difficult issues and accelerating scientific advancement. Thanks
    to its ground-breaking improvements in efficiency in domains like neural networks,
    Natural Language Processing (NLP), and related applications, AI has been gaining
    popularity lately. Computing is a vital infrastructure for running AI services
    due to its enormous processing power, and AI has the potential to improve existing
    computing by making resource management effective. Several AI models rely on outside
    data sets and large-scale computer capacity, both of which might be easier to
    access with today’s computing systems. Currently, training advanced models of
    AI in large numbers is becoming even more crucial. Additionally, extensive application
    of AI in contemporary computer systems may be possible due to ground-breaking
    XAI research. In the decades to come, AI will place substantial stress on computing
    resources. To meet these demands, it is necessary to develop new approaches to
    research and methodology that make use of AI models to solve problems with adaptability,
    delay, and handling of resources and cybersecurity. Scalability and adaptability
    are two open issues that have not yet made full use of AI models as an economical
    way to boost the performance of computer applications. Our analysis has led us
    to categorize certain areas of computing into three separate maturity levels:
    a period of five to ten years, over a decade, and under five years. Several novel
    innovations are on the horizon that might significantly improve the utilization
    of modern computing, and the article has highlighted them all over the coming
    decade. Fig. 2 depicts the hype cycle for modern computing systems along with
    their new trends. Researchers extensively study computing paradigms and technologies,
    with edge AI and federated learning now dominating. New areas of study within
    computing, such as distributed computing continuum and AI-driven computing are
    just scratching the surface. Applications for computing in these domains may not
    mature for another five to 10 years. Quantum ML, sustainability, Net Zero Computing,
    XAI, and the quantum Internet are all expected to be in the spotlight for at least
    another decade. Digital twins, cybersecurity, edge intelligence, edge computing,
    and blockchain technology have generated an unprecedented level of excitement.
    They are expected to be completely built-in under five years with the help of
    modern technology. Machine Economics, In-Memory Computing, Bitcoin Currency and
    AIOps/MLOps have all reached their peak of inflated expectations for the following
    five to 10 years of noteworthy evolution. Significant progress needs to be made
    before biologically inspired computing, neuro-symbolic AI, analog computing, neuromorphic
    computing, 6G, and quantum computing can be considered hype-worthy. Cloud and
    fog computing has been trending heavily over the past few years, and that trend
    could persist for the next five to ten years. Table 6. List of acronyms. Abbreviation
    Description PCs Personal Computers DNS Domain Name System MPP Massive Parallel
    Processing AI Artificial Intelligence SMP Symmetric Multi Processing OS Operating
    System GUI Graphical User Interfaces IoT Internet of Things HTTP Hyper Text Transport
    Protocol HTML Hyper Text Markup Language WWW World Wide Web RPC Remote Procedure
    Calls JSON JavaScript Object Notation XML Extensible Markup Language SOA Service-Oriented
    Architecture CDC Cloud Data Centers HPC High Performance Computing IT Information
    Technology SaaS Software as a Service PaaS Platform as a Service IaaS Infrastructure
    as a Service SBC Single-board Computers SDN Software-Defined Networking NVF Network
    Function Virtualization IIoT Industrial Internet of Things QoS Quality of Service
    IoE Internet of Energy B5G Beyond 5G SLA Service-Level Agreement FPGA Field-Programmable
    Gate Arrays ASICs Application-Specific Integrated Circuits GPU Graphics Processing
    Units CUDA Compute Unified Device Architecture TPU Tensor Processing Units ICT
    Information and Communication Technology CaaS Container as a Service QoE Quality
    of Experience V2X Vehicle-to-Everything MEC Multi-access Edge Computing VM Virtual
    Machines M2M Machine-to-Machine PoW Proof of Work XAI Explainable Artificial Intelligence
    UAV Unmanned Aerial Vehicle DDoS Distributed Denial of Service STCO Systems-Technology
    Co-Optimization SoC System-on-a-Chip ML Machine Learning SLO Service Level Objective
    7. Summary and conclusions This research offers a comprehensive exploration of
    the evolution of modern computing systems over the past sixty years, tracking
    the transition from classical computers to quantum computing and examining their
    key components, such as physical architecture, conceptual units, and communication
    methods. We analyze the influence of conceptualization and physical models on
    the shift from centralized to decentralized structures, a significant change since
    the Internet’s inception. Developments in microcontroller architecture, operating
    system design, and networking infrastructure have given rise to ubiquitous computing
    models like the Internet of Things (IoT), pushing the boundaries of both physical
    and conceptual realms. The move towards specialized hardware and software, particularly
    in data-driven fields like AI, represents a shift from earlier focuses on system
    flexibility and adaptability. This article also addresses issues of accessibility
    and potential inequalities, emphasizing the need to ensure these technologies
    positively impact society and everyday life. Integrating recent advancements with
    ongoing challenges in the application of established technological trends, this
    work provides an in-depth analysis of the next wave of scientific research in
    computing. It summarizes current findings, acknowledges limitations, and outlines
    new trends and key challenges, considering the impact of emerging trends and envisioning
    future research paths in modern computing. This review aims to be a valuable resource
    for experts, technologists, and academics interested in the latest developments
    and future directions in the field of modern computing. CRediT authorship contribution
    statement Sukhpal Singh Gill: Writing – original draft, Validation, Methodology,
    Investigation, Formal analysis, Data curation, Conceptualization, Visualization,
    Writing – review & editing. Huaming Wu: Writing – review & editing, Writing –
    original draft, Conceptualization, Data curation, Formal analysis, Investigation,
    Methodology. Panos Patros: Writing – review & editing, Writing – original draft,
    Data curation, Conceptualization, Investigation, Methodology. Carlo Ottaviani:
    Writing – review & editing, Writing – original draft, Formal analysis, Conceptualization,
    Investigation. Priyansh Arora: Writing – original draft, Formal analysis, Conceptualization,
    Investigation, Methodology, Writing – review & editing. Victor Casamayor Pujol:
    Writing – original draft, Data curation, Conceptualization, Investigation, Methodology,
    Writing – review & editing. David Haunschild: Conceptualization, Formal analysis,
    Investigation, Methodology, Writing – original draft, Writing – review & editing.
    Ajith Kumar Parlikad: Writing – review & editing, Writing – original draft, Conceptualization,
    Investigation, Methodology. Oktay Cetinkaya: Writing – review & editing, Writing
    – original draft, Conceptualization, Investigation, Methodology. Hanan Lutfiyya:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology. Vlado Stankovski: Writing – review & editing, Writing – original
    draft, Conceptualization, Investigation, Methodology. Ruidong Li: Writing – review
    & editing, Writing – original draft, Conceptualization, Investigation, Methodology.
    Yuemin Ding: Writing – review & editing, Writing – original draft, Conceptualization,
    Investigation, Methodology. Junaid Qadir: Writing – review & editing, Writing
    – original draft, Conceptualization, Data curation, Formal analysis, Investigation,
    Methodology, Visualization. Ajith Abraham: Writing – review & editing, Writing
    – original draft, Conceptualization, Investigation, Methodology. Soumya K. Ghosh:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology. Houbing Herbert Song: Writing – review & editing, Writing – original
    draft, Methodology, Conceptualization, Investigation. Rizos Sakellariou: Writing
    – review & editing, Writing – original draft, Formal analysis, Conceptualization,
    Investigation, Methodology, Supervision. Omer Rana: Writing – review & editing,
    Writing – original draft, Conceptualization, Investigation, Methodology, Supervision.
    Joel J.P.C. Rodrigues: Writing – review & editing, Writing – original draft, Conceptualization,
    Investigation, Methodology. Salil S. Kanhere: Writing – review & editing, Writing
    – original draft, Conceptualization, Investigation, Methodology. Schahram Dustdar:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology, Supervision. Steve Uhlig: Writing – review & editing, Writing – original
    draft, Conceptualization, Investigation, Methodology, Supervision. Kotagiri Ramamohanarao:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology, Supervision. Rajkumar Buyya: Writing – review & editing, Writing
    – original draft, Conceptualization, Formal analysis, Investigation, Methodology,
    Supervision, Visualization. Declaration of competing interest The authors declare
    that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgments
    We thank the Editor-in-Chief (Prof. Ke Xue) and anonymous reviewers for their
    insightful comments and recommendations to improve the overall quality and organization
    of the article. We would also like to express our gratitude to Neil Butler (CEO,
    CloudScaler, UK), Marco AS Netto (Microsoft Azure HPC, USA) and Manmeet Singh
    (University of Texas at Austin, USA) for their thoughtful remarks and valuable
    suggestions. Appendix. List of acronyms Table 6 shows the list of acronyms. Data
    availability No data was used for the research described in the article. References
    [1] Buyya R., et al. A manifesto for future generation cloud computing: Research
    directions for the next decade ACM Comput. Surv., 51 (5) (2018), pp. 1-38 Google
    Scholar [2] Lindsay D., et al. The evolution of distributed computing systems:
    from fundamental to new frontiers Computing, 103 (8) (2021), pp. 1859-1878 CrossRefView
    in ScopusGoogle Scholar [3] Yamashita R. History of personal computers in Japan
    Int. J. Parallel Emergent Distrib. Syst., 35 (2) (2020), pp. 143-169 CrossRefView
    in ScopusGoogle Scholar [4] Gill S.S., et al. AI for next generation computing:
    Emerging trends and future directions Int. Things, 19 (2022), Article 100514 View
    PDFView articleView in ScopusGoogle Scholar [5] Gubbi J., et al. Internet of Things
    (IoT): A vision, architectural elements, and future directions Future Gener. Comput.
    Syst., 29 (7) (2013), pp. 1645-1660 View PDFView articleView in ScopusGoogle Scholar
    [6] Muralidhar R., et al. Energy efficient computing systems: Architectures, abstractions
    and modeling to techniques and standards ACM Comput. Surv., 54 (11s) (2022), pp.
    1-37 CrossRefGoogle Scholar [7] Chakraborty A., et al. Journey from cloud of things
    to fog of things: Survey, new trends, and research directions Softw. - Pract.
    Exp., 53 (2) (2023), pp. 496-551 CrossRefView in ScopusGoogle Scholar [8] Beloglazov
    A., et al. Energy-aware resource allocation heuristics for efficient management
    of data centers for cloud computing Future Gener. Comput. Syst., 28 (5) (2012),
    pp. 755-768 View PDFView articleView in ScopusGoogle Scholar [9] Casamayor Pujol
    V., et al. Fundamental research challenges for distributed computing continuum
    systems Information, 14 (3) (2023), p. 198 CrossRefView in ScopusGoogle Scholar
    [10] Shalf J. The future of computing beyond Moore’s law Phil. Trans. R. Soc.
    A, 378 (2166) (2020), Article 20190061 CrossRefView in ScopusGoogle Scholar [11]
    Angel N.A., et al. Recent advances in evolving computing paradigms: Cloud, edge,
    and fog technologies Sensors, 22 (1) (2021), p. 196 CrossRefGoogle Scholar [12]
    Rimal B.P., et al. A taxonomy and survey of cloud computing systems 2009 Fifth
    International Joint Conference on INC, IMS and IDC, IEEE (2009), pp. 44-51 CrossRefView
    in ScopusGoogle Scholar [13] Gill S.S., et al. Transformative effects of IoT,
    blockchain and artificial intelligence on cloud computing: Evolution, vision,
    trends and open challenges Int. Things, 8 (2019), Article 100118 View PDFView
    articleView in ScopusGoogle Scholar [14] Flynn M.J. Very high-speed computing
    systems Proc. IEEE, 54 (12) (1966), pp. 1901-1909 View in ScopusGoogle Scholar
    [15] Kozyrakis C.E., et al. A new direction for computer architecture research
    Computer, 31 (11) (1998), pp. 24-32 View in ScopusGoogle Scholar [16] Casavant
    T.L., et al. A taxonomy of scheduling in general-purpose distributed computing
    systems IEEE Trans. Softw. Eng., 14 (2) (1988), pp. 141-154 View in ScopusGoogle
    Scholar [17] Yu J., et al. A taxonomy of workflow management systems for grid
    computing J. Grid Comput., 3 (2005), pp. 171-200 CrossRefView in ScopusGoogle
    Scholar [18] Owens J.D., et al. GPU computing Proc. IEEE, 96 (5) (2008), pp. 879-899
    View in ScopusGoogle Scholar [19] Compton K., et al. Reconfigurable computing:
    a survey of systems and software ACM Comput. Surv. (csuR), 34 (2) (2002), pp.
    171-210 View in ScopusGoogle Scholar [20] Wright S. Cybersquatting at the intersection
    of internet domain names and trademark law IEEE Commun. Surv. Tutor., 14 (1) (2010),
    pp. 193-205 CrossRefGoogle Scholar [21] Jansen B.J. The graphical user interface
    ACM SIGCHI Bull., 30 (2) (1998), pp. 22-26 CrossRefGoogle Scholar [22] Tay B.H.,
    et al. A survey of remote procedure calls Oper. Syst. Rev., 24 (3) (1990), pp.
    68-79 View in ScopusGoogle Scholar [23] Suryono R.R., et al. Peer to peer (P2P)
    lending problems and potential solutions: A systematic literature review Procedia
    Comput. Sci., 161 (2019), pp. 204-214 View PDFView articleView in ScopusGoogle
    Scholar [24] Schollmeier R., et al. Protocol for peer-to-peer networking in mobile
    environments Proceedings. 12th International Conference on Computer Communications
    and Networks (IEEE Cat. No. 03EX712), IEEE (2003), pp. 121-127 View in ScopusGoogle
    Scholar [25] Alonso G., et al. Web Services Springer (2004) Google Scholar [26]
    Perrey R., et al. Service-oriented architecture 2003 Symposium on Applications
    and the Internet Workshops, 2003. Proceedings, IEEE (2003), pp. 116-119 View in
    ScopusGoogle Scholar [27] Maffione V., et al. A software development kit to exploit
    RINA programmability 2016 IEEE International Conference on Communications (ICC),
    IEEE (2016), pp. 1-7 CrossRefGoogle Scholar [28] L. Resende, Handling heterogeneous
    data sources in a SOA environment with service data objects (SDO), in: Proceedings
    of the 2007 ACM SIGMOD International Conference on Management of Data, 2007, pp.
    895–897. Google Scholar [29] Mergen M.F., et al. Virtualization for high-performance
    computing Oper. Syst. Rev., 40 (2) (2006), pp. 8-11 CrossRefView in ScopusGoogle
    Scholar [30] Kephart J.O., et al. The vision of autonomic computing Computer,
    36 (1) (2003), pp. 41-50 View in ScopusGoogle Scholar [31] Singh S., et al. STAR:
    SLA-aware autonomic management of cloud resources IEEE Trans. Cloud Comput., 8
    (4) (2017), pp. 1040-1053 View in ScopusGoogle Scholar [32] Othman M., et al.
    A survey of mobile cloud computing application models IEEE Commun. Surv. Tutorials,
    16 (1) (2013), pp. 393-413 Google Scholar [33] AlAhmad A.S., et al. Mobile cloud
    computing models security issues: A systematic review J. Netw. Comput. Appl.,
    190 (2021), Article 103152 View PDFView articleView in ScopusGoogle Scholar [34]
    Anwar M.H., et al. Recommender system for optimal distributed deep learning in
    cloud datacenters Wirel. Pers. Commun. (2022), pp. 1-25 Google Scholar [35] Durao
    F., et al. A systematic review on cloud computing J. Supercomput., 68 (2014),
    pp. 1321-1346 CrossRefView in ScopusGoogle Scholar [36] Gill S.S., et al. ROUTER:
    Fog enabled cloud based intelligent resource management approach for smart home
    IoT devices J. Syst. Softw., 154 (2019), pp. 125-138 View PDFView articleView
    in ScopusGoogle Scholar [37] Iftikhar S., et al. AI-based fog and edge computing:
    A systematic review, taxonomy and future directions Int. Things (2022), Article
    100674 Google Scholar [38] Gill S.S., et al. Fog-based smart healthcare as a big
    data and cloud service for heart patients using IoT International Conference on
    Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018,
    Springer (2019), pp. 1376-1383 CrossRefView in ScopusGoogle Scholar [39] Singh
    J., et al. Fog computing: A taxonomy, systematic review, current trends and research
    challenges J. Parallel Distrib. Comput., 157 (2021), pp. 56-85 View PDFView articleView
    in ScopusGoogle Scholar [40] Shi W., et al. Edge computing: Vision and challenges
    IEEE Int. Things J., 3 (5) (2016), pp. 637-646 View in ScopusGoogle Scholar [41]
    Walia G.K., et al. AI-empowered fog/edge resource management for IoT applications:
    A comprehensive review, research challenges and future perspectives IEEE Commun.
    Surv. Tutor., 26 (1) (2023), pp. 1-56 CrossRefGoogle Scholar [42] Khan W.Z., et
    al. Edge computing: A survey Future Gener. Comput. Syst., 97 (2019), pp. 219-235
    View PDFView articleView in ScopusGoogle Scholar [43] Jonas E., et al. Cloud programming
    simplified: A berkeley view on serverless computing (2019) arXiv preprint arXiv:1902.03383
    Google Scholar [44] Hassan H.B., et al. Survey on serverless computing J. Cloud
    Comput., 10 (1) (2021), pp. 1-29 CrossRefGoogle Scholar [45] A. Buzachis, et al.,
    Modeling and emulation of an osmotic computing ecosystem using osmotictoolkit,
    in: Proceedings of the 2021 Australasian Computer Science Week Multiconference,
    2021, pp. 1–9. Google Scholar [46] Neha B., et al. A systematic review on osmotic
    computing ACM Trans. Int. Things, 3 (2) (2022), pp. 1-30 CrossRefGoogle Scholar
    [47] Ray P.P. An introduction to dew computing: definition, concept and implications
    IEEE Access, 6 (2017), pp. 723-737 View in ScopusGoogle Scholar [48] Gushev M.
    Dew computing architecture for cyber-physical systems and IoT Int. Things, 11
    (2020), Article 100186 View PDFView articleView in ScopusGoogle Scholar [49] Qu
    Y., et al. A blockchained federated learning framework for cognitive computing
    in industry 4.0 networks IEEE Trans. Ind. Inform., 17 (4) (2020), pp. 2964-2973
    Google Scholar [50] Kovachy T., et al. Quantum superposition at the half-metre
    scale Nature, 528 (7583) (2015), pp. 530-533 CrossRefView in ScopusGoogle Scholar
    [51] Gill S.S., et al. Quantum computing: A taxonomy, systematic review and future
    directions Softw. - Pract. Exp., 52 (1) (2022), pp. 66-114 CrossRefView in ScopusGoogle
    Scholar [52] Gulliver S.R., et al. Pervasive and standalone computing: the perceptual
    effects of variable multimedia quality Int. J. Hum.-Comput. Stud., 60 (5–6) (2004),
    pp. 640-665 View PDFView articleView in ScopusGoogle Scholar [53] Ravi S., et
    al. Security in embedded systems: Design challenges ACM Trans. Embed. Comput.
    Syst. (TECS), 3 (3) (2004), pp. 461-491 CrossRefView in ScopusGoogle Scholar [54]
    De Micco L., et al. A literature review on embedded systems IEEE Latin Am. Trans.,
    18 (02) (2019), pp. 188-205 CrossRefView in ScopusGoogle Scholar [55] Basford
    P.J., et al. Performance analysis of single board computer clusters Future Gener.
    Comput. Syst., 102 (2020), pp. 278-291 View PDFView articleView in ScopusGoogle
    Scholar [56] Pajankar A. Raspberry pi supercomputing and scientific programming
    Ashwin Pajankar (2017) Google Scholar [57] Hwu T., et al. A self-driving robot
    using deep convolutional neural networks on neuromorphic hardware 2017 International
    Joint Conference on Neural Networks (IJCNN), IEEE (2017), pp. 635-641 View in
    ScopusGoogle Scholar [58] Süzen A.A., et al. Benchmark analysis of jetson tx2,
    jetson nano and raspberry pi using deep-cnn 2020 International Congress on Human-Computer
    Interaction, Optimization and Robotic Applications (HORA), IEEE (2020), pp. 1-5
    CrossRefGoogle Scholar [59] Kumar A., et al. Securing the future internet of things
    with post-quantum cryptography Secur. Priv., 5 (2) (2022), Article e200 Google
    Scholar [60] Ren J., et al. A survey on end-edge-cloud orchestrated network computing
    paradigms: Transparent computing, mobile edge computing, fog computing, and cloudlet
    ACM Comput. Surv., 52 (6) (2019), pp. 1-36 Google Scholar [61] Wang C., et al.
    Integration of networking, caching, and computing in wireless systems: A survey,
    some research issues, and challenges IEEE Commun. Surv. Tutor., 20 (1) (2017),
    pp. 7-38 View in ScopusGoogle Scholar [62] Ahmadabadi J.Z., et al. Star-quake:
    A new operator in multi-objective gravitational search algorithm for task scheduling
    in IoT based cloud-fog computing system IEEE Trans. Consum. Electron. (2023) Google
    Scholar [63] Asghari A., et al. Server placement in mobile cloud computing: a
    comprehensive survey for edge computing, fog computing and cloudlet Computer Science
    Review, 51 (2024), p. 100616 View PDFView articleView in ScopusGoogle Scholar
    [64] Bari M.F., et al. On orchestrating virtual network functions 2015 11th International
    Conference on Network and Service Management (CNSM), IEEE (2015), pp. 50-56 CrossRefView
    in ScopusGoogle Scholar [65] Cai Y., et al. Compute-and data-intensive networks:
    The key to the metaverse 2022 1st International Conference on 6G Networking (6GNet),
    IEEE (2022), pp. 1-8 View PDFView articleView in ScopusGoogle Scholar [66] Al-Masri
    E., et al. Energy-efficient cooperative resource allocation and task scheduling
    for Internet of Things environments Int. Things, 23 (2023), Article 100832 View
    PDFView articleView in ScopusGoogle Scholar [67] Sriraghavendra M., et al. DoSP:
    A deadline-aware dynamic service placement algorithm for workflow-oriented IoT
    applications in fog-cloud computing environments Energy Conservation Solutions
    for Fog-Edge Computing Paradigms, Springer (2022), pp. 21-47 CrossRefView in ScopusGoogle
    Scholar [68] Verma P., et al. FCMCPS-COVID: AI propelled fog–cloud inspired scalable
    medical cyber-physical system, specific to coronavirus disease Int. Things, 23
    (2023), Article 100828 View PDFView articleView in ScopusGoogle Scholar [69] Desai
    F., et al. HealthCloud: A system for monitoring health status of heart patients
    using machine learning and cloud computing Int. Things, 17 (2022), Article 100485
    View PDFView articleView in ScopusGoogle Scholar [70] Iftikhar S., et al. FogDLearner:
    A deep learning-based cardiac health diagnosis framework using fog computing Proceedings
    of the 2022 Australasian Computer Science Week, ACM (2022), pp. 136-144 CrossRefView
    in ScopusGoogle Scholar [71] Gill S.S., et al. IoT based agriculture as a cloud
    and big data service: the beginning of digital India J. Organ. End User Comput.
    (JOEUC), 29 (4) (2017), pp. 1-23 View in ScopusGoogle Scholar [72] Sengupta A.,
    et al. Mobile edge computing based internet of agricultural things: a systematic
    review and future directions Mob. Edge Comput. (2021), pp. 415-441 CrossRefView
    in ScopusGoogle Scholar [73] Iftikhar S., et al. Fog computing based router-distributor
    application for sustainable smart home 2022 IEEE 95th Vehicular Technology Conference:(VTC2022-Spring),
    IEEE (2022), pp. 1-5 Google Scholar [74] Bansal K., et al. DeepBus: Machine learning
    based real time pothole detection system for smart transportation using IoT Int.
    Technol. Lett., 3 (3) (2020), Article e156 View in ScopusGoogle Scholar [75] Tuli
    S., et al. IThermoFog: IoT-fog based automatic thermal profile creation for cloud
    data centers using artificial intelligence techniques Int. Technol. Lett., 3 (5)
    (2020), Article e198 View in ScopusGoogle Scholar [76] Singh M., et al. Quantum
    artificial intelligence for the science of climate change Artificial Intelligence,
    Machine Learning and Blockchain in Quantum Satellite, Drone and Network, CRC Press
    (2022), pp. 199-207 Google Scholar [77] Singh M., et al. Quantifying COVID-19
    enforced global changes in atmospheric pollutants using cloud computing based
    remote sensing Remote Sens. Appl.: Soc. Environ., 22 (2021), Article 100489 View
    PDFView articleView in ScopusGoogle Scholar [78] Stoyanova M., et al. A survey
    on the internet of things (IoT) forensics: challenges, approaches, and open issues
    IEEE Commun. Surv. Tutor., 22 (2) (2020), pp. 1191-1221 CrossRefView in ScopusGoogle
    Scholar [79] Mansouri N., et al. Cloud computing simulators: A comprehensive review
    Simul. Model. Pract. Theory, 104 (2020), Article 102144 View PDFView articleView
    in ScopusGoogle Scholar [80] Tuli S., et al. HealthFog: An ensemble deep learning
    based smart healthcare system for automatic diagnosis of heart diseases in integrated
    IoT and fog computing environments Future Gener. Comput. Syst., 104 (2020), pp.
    187-200 View PDFView articleView in ScopusGoogle Scholar [81] Gill S.S., et al.
    ChatGPT: Vision and challenges Int. Things Cyb.-Phys. Syst., 3 (2023), pp. 262-271
    View PDFView articleView in ScopusGoogle Scholar [82] Vila M., et al. Edge-to-cloud
    sensing and actuation semantics in the industrial Internet of Things Pervasive
    Mob. Comput., 87 (2022), Article 101699 View PDFView articleView in ScopusGoogle
    Scholar [83] Kreutz D., et al. Software-defined networking: A comprehensive survey
    Proc. IEEE, 103 (1) (2014), pp. 14-76 Google Scholar [84] Mekki T., et al. Software-defined
    networking in vehicular networks: A survey Trans. Emerg. Telecommun. Technol.,
    33 (10) (2022), Article e4265 View in ScopusGoogle Scholar [85] Son J., et al.
    A taxonomy of software-defined networking (SDN)-enabled cloud computing ACM Comput.
    Surv. (csuR), 51 (3) (2018), pp. 1-36 CrossRefGoogle Scholar [86] L. Poutievski,
    et al., Jupiter evolving: transforming google’s datacenter network via optical
    circuit switches and software-defined networking, in: Proceedings of the ACM SIGCOMM
    2022 Conference, 2022, pp. 66–85. Google Scholar [87] Kumar A., et al. A secure
    drone-to-drone communication and software defined drone network-enabled traffic
    monitoring system Simul. Model. Pract. Theory, 120 (2022), Article 102621 View
    PDFView articleView in ScopusGoogle Scholar [88] Wang X., et al. Convergence of
    edge computing and deep learning: A comprehensive survey IEEE Commun. Surv. Tutor.,
    22 (2) (2020), pp. 869-904 CrossRefView in ScopusGoogle Scholar [89] Zhang J.,
    et al. Mobile edge intelligence and computing for the internet of vehicles Proc.
    IEEE, 108 (2) (2019), pp. 246-261 CrossRefGoogle Scholar [90] Chen S., et al.
    Internet of things based smart grids supported by intelligent edge computing IEEE
    Access, 7 (2019), pp. 74089-74102 CrossRefView in ScopusGoogle Scholar [91] Pujol
    V.C., et al. Edge intelligence—Research opportunities for distributed computing
    continuum systems IEEE Internet Comput., 27 (4) (2023), pp. 53-74 CrossRefView
    in ScopusGoogle Scholar [92] Singh R., et al. Edge AI: a survey Int. Things Cyb.-Phys.
    Syst., 3 (2023), pp. 71-92 View PDFView articleView in ScopusGoogle Scholar [93]
    Jia Y., et al. Flowguard: An intelligent edge defense mechanism against IoT DDoS
    attacks IEEE Internet Things J., 7 (10) (2020), pp. 9552-9562 CrossRefView in
    ScopusGoogle Scholar [94] Yang B., et al. Edge intelligence for autonomous driving
    in 6G wireless system: Design challenges and solutions IEEE Wirel. Commun., 28
    (2) (2021), pp. 40-47 CrossRefView in ScopusGoogle Scholar [95] Liu F., et al.
    Integrated sensing and communications: Toward dual-functional wireless networks
    for 6G and beyond IEEE J. Selected Areas Commun., 40 (6) (2022), pp. 1728-1767
    CrossRefView in ScopusGoogle Scholar [96] Ishtiaq M., et al. Edge computing in
    IoT: A 6G perspective (2021) arXiv preprint arXiv:2111.08943 Google Scholar [97]
    Kumar A., et al. A drone-based networked system and methods for combating coronavirus
    disease (COVID-19) pandemic Future Gener. Comput. Syst., 115 (2021), pp. 1-19
    View PDFView articleView in ScopusGoogle Scholar [98] Shi Y., et al. Machine learning
    for large-scale optimization in 6g wireless networks IEEE Commun. Surv. Tutor.
    (2023) Google Scholar [99] Alkhateeb A., et al. Real-time digital twins: Vision
    and research directions for 6G and beyond IEEE Commun. Mag. (2023) Google Scholar
    [100] Ansar S.A., et al. Intelligent Fog-IoT Networks with 6G endorsement: Foundations,
    applications, trends and challenges 6G Enabled Fog Computing in IoT: Applications
    and Opportunities, Springer (2023), pp. 287-307 CrossRefGoogle Scholar [101] Akyildiz
    I.F., et al. 6G and beyond: The future of wireless communications systems IEEE
    Access, 8 (2020), pp. 133995-134030 CrossRefView in ScopusGoogle Scholar [102]
    Ghafouri S., et al. Mobile-kube: Mobility-aware and energy-efficient service orchestration
    on kubernetes edge servers 2022 IEEE/ACM 15th International Conference on Utility
    and Cloud Computing (UCC), IEEE (2022), pp. 82-91 CrossRefView in ScopusGoogle
    Scholar [103] Wu H., et al. Energy-efficient decision making for mobile cloud
    offloading IEEE Trans. Cloud Comput., 8 (2) (2020), pp. 570-584 CrossRefView in
    ScopusGoogle Scholar [104] Wu H., et al. Lyapunov-guided delay-aware energy efficient
    offloading in iIoT-mec systems IEEE Trans. Ind. Inform., 19 (2) (2023), pp. 2117-2128
    CrossRefView in ScopusGoogle Scholar [105] Owens J.D., et al. A survey of general-purpose
    computation on graphics hardware Comput. Graph. Forum, 26 (1) (2007), pp. 80-113
    CrossRefView in ScopusGoogle Scholar [106] Von Neumann J. John Von Neumann: Selected
    Letters American Mathematical Soc. (2005) Google Scholar [107] Kimovski D., et
    al. Beyond von neumann in the computing continuum: Architectures, applications,
    and future directions IEEE Internet Comput. (2023) Google Scholar [108] Yang R.,
    et al. Integrated blockchain and edge computing systems: A survey, some research
    issues and challenges IEEE Commun. Surv. Tutor., 21 (2) (2019), pp. 1508-1532
    CrossRefView in ScopusGoogle Scholar [109] Alsamhi S.H., et al. Computing in the
    sky: A survey on intelligent ubiquitous computing for uav-assisted 6g networks
    and industry 4.0/5.0 Drones, 6 (7) (2022), p. 177 CrossRefView in ScopusGoogle
    Scholar [110] Chen J., et al. Deep learning with edge computing: A review Proc.
    IEEE, 107 (8) (2019), pp. 1655-1674 CrossRefView in ScopusGoogle Scholar [111]
    Singh H., et al. Metaheuristics for scheduling of heterogeneous tasks in cloud
    computing environments: Analysis, performance evaluation, and future directions
    Simul. Model. Pract. Theory, 111 (2021), Article 102353 View PDFView articleView
    in ScopusGoogle Scholar [112] Botta A., et al. Integration of cloud computing
    and internet of things: a survey Future Gener. Comput. Syst., 56 (2016), pp. 684-700
    View PDFView articleView in ScopusGoogle Scholar [113] Cappello F., et al. Computing
    on large-scale distributed systems: XtremWeb architecture, programming models,
    security, tests and convergence with grid Future Gener. Comput. Syst., 21 (3)
    (2005), pp. 417-437 View PDFView articleView in ScopusGoogle Scholar [114] Andrews
    D., et al. Achieving programming model abstractions for reconfigurable computing
    IEEE Trans. Very Large Scale Integr. (VLSI) Syst., 16 (1) (2007), pp. 34-44 Google
    Scholar [115] Jackson J.C., et al. Survey on programming models and environments
    for cluster, cloud, and grid computing that defends big data Procedia Comput.
    Sci., 50 (2015), pp. 517-523 View PDFView articleView in ScopusGoogle Scholar
    [116] Cao C., et al. A novel multi-objective programming model of relief distribution
    for sustainable disaster supply chain in large-scale natural disasters J. Clean.
    Prod., 174 (2018), pp. 1422-1435 View PDFView articleView in ScopusGoogle Scholar
    [117] Butts M., et al. A structural object programming model, architecture, chip
    and tools for reconfigurable computing 15th Annual IEEE Symposium on Field-Programmable
    Custom Computing Machines (FCCM 2007), IEEE (2007), pp. 55-64 CrossRefView in
    ScopusGoogle Scholar [118] Shen X., et al. Holistic network virtualization and
    pervasive network intelligence for 6G IEEE Commun. Surv. Tutor., 24 (1) (2021),
    pp. 1-30 CrossRefView in ScopusGoogle Scholar [119] Jin S., et al. H-svm: Hardware-assisted
    secure virtual machines under a vulnerable hypervisor IEEE Trans. Comput., 64
    (10) (2015), pp. 2833-2846 View in ScopusGoogle Scholar [120] Mansouri Y., et
    al. A review of edge computing: Features and resource virtualization J. Parallel
    Distrib. Comput., 150 (2021), pp. 155-183 View PDFView articleView in ScopusGoogle
    Scholar [121] Zhang J., et al. Performance analysis of 3D XPoint SSDs in virtualized
    and non-virtualized environments 2018 IEEE 24th International Conference on Parallel
    and Distributed Systems (ICPADS), IEEE (2018), pp. 1-10 Google Scholar [122] Alam
    I., et al. A survey of network virtualization techniques for Internet of Things
    using SDN and NFV ACM Comput. Surv., 53 (2) (2020), pp. 1-40 Google Scholar [123]
    Xing Y., et al. Virtualization and cloud computing Future Wireless Networks and
    Information Systems: Volume 1, Springer (2012), pp. 305-312 CrossRefView in ScopusGoogle
    Scholar [124] A. Agache, et al., Firecracker: Lightweight virtualization for serverless
    applications, in: 17th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 20), 2020, pp. 419–434. Google Scholar [125] Blake G., et al. A survey of
    multicore processors IEEE Signal Process. Mag., 26 (6) (2009), pp. 26-37 View
    in ScopusGoogle Scholar [126] Gizopoulos D., et al. Architectures for online error
    detection and recovery in multicore processors 2011 Design, Automation & Test
    in Europe, IEEE (2011), pp. 1-6 CrossRefGoogle Scholar [127] Delgado R., et al.
    New insights into the real-time performance of a multicore processor IEEE Access,
    8 (2020), pp. 186199-186211 CrossRefView in ScopusGoogle Scholar [128] Piattini
    M., et al. Toward a quantum software engineering IT Prof., 23 (1) (2021), pp.
    62-66 CrossRefView in ScopusGoogle Scholar [129] Arvanitou E.-M., et al. Software
    engineering practices for scientific software development: A systematic mapping
    study J. Syst. Softw., 172 (2021), Article 110848 View PDFView articleView in
    ScopusGoogle Scholar [130] Althar R.R., et al. The realist approach for evaluation
    of computational intelligence in software engineering Innov. Syst. Softw. Eng.,
    17 (1) (2021), pp. 17-27 CrossRefView in ScopusGoogle Scholar [131] De Stefano
    M., et al. Software engineering for quantum programming: How far are we? J. Syst.
    Softw., 190 (2022), Article 111326 View PDFView articleView in ScopusGoogle Scholar
    [132] Sharma G., et al. Applications of blockchain in automated heavy vehicles:
    Yesterday, today, and tomorrow Autonomous and Connected Heavy Vehicle Technology,
    Elsevier (2022), pp. 81-93 View PDFView articleView in ScopusGoogle Scholar [133]
    Al-Jaroodi J., et al. Blockchain in industries: A survey IEEE Access, 7 (2019),
    pp. 36500-36515 CrossRefView in ScopusGoogle Scholar [134] Doyle J., et al. Blockchainbus:
    A lightweight framework for secure virtual machine migration in cloud federations
    using blockchain Secur. Priv., 5 (2) (2022), Article e197 Google Scholar [135]
    Jurado Perez L., et al. Simulation of scalability in cloud-based iot reactive
    systems leveraged on a wsan simulator and cloud computing technologies Appl. Sci.,
    11 (4) (2021), p. 1804 CrossRefGoogle Scholar [136] Buyya R., et al. A strategy
    for advancing research and impact in new computing paradigms Green Mobile Cloud
    Computing, Springer (2022), pp. 297-308 CrossRefView in ScopusGoogle Scholar [137]
    Brady C., et al. All roads lead to computing: Making, participatory simulations,
    and social computing as pathways to computer science IEEE Trans. Educ., 60 (1)
    (2016), pp. 59-66 Google Scholar [138] Ferraz O., et al. A survey on high-throughput
    non-binary LDPC decoders: ASIC, FPGA, and GPU architectures IEEE Commun. Surv.
    Tutor., 24 (1) (2021), pp. 524-556 Google Scholar [139] Jouppi N.P., et al. A
    domain-specific architecture for deep neural networks Commun. ACM, 61 (9) (2018),
    pp. 50-59 CrossRefView in ScopusGoogle Scholar [140] Cong J., et al. Customizable
    computing—from single chip to datacenters Proc. IEEE, 107 (1) (2018), pp. 185-203
    View in ScopusGoogle Scholar [141] Ji H., et al. Magnetic reconnection in the
    era of exascale computing and multiscale experiments Nat. Rev. Phys., 4 (4) (2022),
    pp. 263-282 CrossRefView in ScopusGoogle Scholar [142] Heldens S., et al. The
    landscape of exascale research: A data-driven literature analysis ACM Comput.
    Surv., 53 (2) (2020), pp. 1-43 CrossRefGoogle Scholar [143] Kim Y., et al. Evidence
    for the utility of quantum computing before fault tolerance Nature, 618 (7965)
    (2023), pp. 500-505 View in ScopusGoogle Scholar [144] Anzt H., et al. Preparing
    sparse solvers for exascale computing Phil. Trans. R. Soc. A, 378 (2166) (2020),
    Article 20190053 CrossRefView in ScopusGoogle Scholar [145] Zangeneh-Nejad F.,
    et al. Analogue computing with metamaterials Nat. Rev. Mater., 6 (3) (2021), pp.
    207-225 View in ScopusGoogle Scholar [146] Zhang W., et al. Neuro-inspired computing
    chips Nat. Electron., 3 (7) (2020), pp. 371-382 CrossRefView in ScopusGoogle Scholar
    [147] Zhao M., et al. Reliability of analog resistive switching memory for neuromorphic
    computing Appl. Phys. Rev., 7 (1) (2020) Google Scholar [148] Zador A., et al.
    Catalyzing next-generation artificial intelligence through neuroai Nat. Commun.,
    14 (1) (2023), p. 1597 View in ScopusGoogle Scholar [149] Schuman C.D., et al.
    Opportunities for neuromorphic computing algorithms and applications Nat. Comput.
    Sci., 2 (1) (2022), pp. 10-19 CrossRefView in ScopusGoogle Scholar [150] Morabito
    F.C., et al. Advances in AI, neural networks, and brain computing: An introduction
    Artificial Intelligence in the Age of Neural Networks and Brain Computing, Elsevier
    (2024), pp. 1-8 View PDFView articleCrossRefGoogle Scholar [151] Rosenfeld V.,
    et al. Query processing on heterogeneous CPU/GPU systems ACM Comput. Surv., 55
    (1) (2022), pp. 1-38 CrossRefGoogle Scholar [152] Sanders J., et al. CUDA by Example:
    An Introduction to General-Purpose GPU Programming Addison-Wesley Professional
    (2010) Google Scholar [153] Tuli S., et al. Predicting the growth and trend of
    COVID-19 pandemic using machine learning and cloud computing Int. Things, 11 (2020),
    Article 100222 View PDFView articleView in ScopusGoogle Scholar [154] Lwakatare
    L.E., et al. Large-scale machine learning systems in real-world industrial settings:
    A review of challenges and solutions Inf. Softw. Technol., 127 (2020), Article
    106368 View PDFView articleView in ScopusGoogle Scholar [155] Wang M., et al.
    A survey on large-scale machine learning IEEE Trans. Knowl. Data Eng., 34 (6)
    (2020), pp. 2574-2594 CrossRefGoogle Scholar [156] M.N. Angenent, et al., Large-scale
    machine learning for business sector prediction, in: Proceedings of the 35th Annual
    ACM Symposium on Applied Computing, 2020, pp. 1143–1146. Google Scholar [157]
    Buyya R., et al. Cloud computing and emerging IT platforms: Vision, hype, and
    reality for delivering computing as the 5th utility Future Gener. Comput. Syst.,
    25 (6) (2009), pp. 599-616 View PDFView articleView in ScopusGoogle Scholar [158]
    Malik S.U., et al. EFFORT: Energy efficient framework for offload communication
    in mobile cloud computing Softw. - Pract. Exp., 51 (9) (2021), pp. 1896-1909 CrossRefView
    in ScopusGoogle Scholar [159] Jin X., et al. A survey of research on computation
    offloading in mobile cloud computing Wirel. Netw., 28 (4) (2022), pp. 1563-1585
    CrossRefView in ScopusGoogle Scholar [160] Patros P., et al. Toward sustainable
    serverless computing IEEE Internet Comput., 25 (6) (2021), pp. 42-50 CrossRefView
    in ScopusGoogle Scholar [161] Masdari M., et al. Green cloud computing using proactive
    virtual machine placement: challenges and issues J. Grid Comput., 18 (4) (2020),
    pp. 727-759 CrossRefView in ScopusGoogle Scholar [162] Gill S.S., et al. A taxonomy
    and future directions for sustainable cloud computing: 360 degree view ACM Comput.
    Surv., 51 (5) (2018), pp. 1-33 CrossRefGoogle Scholar [163] Shu W., et al. Research
    on strong agile response task scheduling optimization enhancement with optimal
    resource usage in green cloud computing Future Gener. Comput. Syst., 124 (2021),
    pp. 12-20 View PDFView articleView in ScopusGoogle Scholar [164] Zhou Q., et al.
    Energy efficient algorithms based on VM consolidation for cloud computing: comparisons
    and evaluations 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and
    Internet Computing (CCGRID), IEEE (2020), pp. 489-498 CrossRefView in ScopusGoogle
    Scholar [165] Mansour R.F., et al. Design of cultural emperor penguin optimizer
    for energy-efficient resource scheduling in green cloud computing environment
    Cluster Comput., 26 (1) (2023), pp. 575-586 CrossRefView in ScopusGoogle Scholar
    [166] Singh M., et al. Dynamic shift from cloud computing to industry 4.0: Eco-friendly
    choice or climate change threat IoT-Based Intelligent Modelling for Environmental
    and Ecological Engineering: IoT Next Generation EcoAgro Systems, Springer (2021),
    pp. 275-293 CrossRefView in ScopusGoogle Scholar [167] W. Zeng, et al., Research
    on cloud storage architecture and key technologies, in: Proceedings of the 2nd
    International Conference on Interaction Sciences: Information Technology, Culture
    and Human, 2009, pp. 1044–1048. Google Scholar [168] Hota M., et al. Leveraging
    cloud-native microservices architecture for high performance real-time intra-day
    trading: A tutorial 6G Enabled Fog Computing in IoT: Applications and Opportunities,
    Springer (2023), pp. 111-129 CrossRefGoogle Scholar [169] Kumar M., et al. Qos-aware
    resource scheduling using whale optimization algorithm for microservice applications
    Softw. - Pract. Exp. (2023) Google Scholar [170] Ghofrani J., et al. Challenges
    of microservices architecture: A survey on the state of the practice ZEUS, 2018
    (2018), pp. 1-8 View in ScopusGoogle Scholar [171] Song C., et al. ChainsFormer:
    A chain latency-aware resource provisioning approach for microservices cluster
    International Conference on Service-Oriented Computing, Springer (2023), pp. 197-211
    CrossRefView in ScopusGoogle Scholar [172] Al-Doghman F., Mothers AI-enabled secure
    microservices in edge computing: Opportunities and challenges IEEE Trans. Serv.
    Comput. (2022) Google Scholar [173] Xu M., et al. CoScal: Multifaceted scaling
    of microservices with reinforcement learning IEEE Trans. Netw. Serv. Manag., 19
    (4) (2022), pp. 3995-4009 CrossRefView in ScopusGoogle Scholar [174] Bentaleb
    O., et al. Containerization technologies: Taxonomies, applications and challenges
    J. Supercomput., 78 (1) (2022), pp. 1144-1181 CrossRefView in ScopusGoogle Scholar
    [175] A. Barbalace, et al., Edge computing: The case for heterogeneous-ISA container
    migration, in: Proceedings of the 16th ACM SIGPLAN/SIGOPS International Conference
    on Virtual Execution Environments, 2020, pp. 73–87. Google Scholar [176] Golec
    M., et al. BioSec: A biometric authentication framework for secure and private
    communication among edge devices in IoT and industry 4.0 IEEE Consum. Electron.
    Mag., 11 (2) (2020), pp. 51-56 Google Scholar [177] Struhár V., et al. Real-time
    containers: A survey 2nd Workshop on Fog Computing and the IoT (Fog-IoT 2020),
    Schloss Dagstuhl-Leibniz-Zentrum für Informatik (2020) Google Scholar [178] Casalicchio
    E., et al. The state-of-the-art in container technologies: Application, orchestration
    and security Concurr. Comput.: Pract. Exper., 32 (17) (2020), Article e5668 View
    in ScopusGoogle Scholar [179] Zhong Z., et al. A cost-efficient container orchestration
    strategy in kubernetes-based cloud computing infrastructures with heterogeneous
    resources ACM Trans. Int. Technol. (TOIT), 20 (2) (2020), pp. 1-24 CrossRefView
    in ScopusGoogle Scholar [180] Mallikarjunaradhya V., et al. An overview of the
    strategic advantages of AI-powered threat intelligence in the cloud J. Sci. Technol.,
    4 (4) (2023), pp. 1-12 Google Scholar [181] P. Patros, et al., Investigating resource
    interference and scaling on multitenant PaaS clouds, in: Proceedings of the 26th
    Annual International Conference on Computer Science and Software Engineering,
    2016, pp. 166–177. Google Scholar [182] Kounev S., et al. Toward a definition
    for serverless computing Leibniz-Zentrum fur Informatik (2021) Google Scholar
    [183] Shafiei H., et al. Serverless computing: a survey of opportunities, challenges,
    and applications ACM Comput. Surv., 54 (11s) (2022), pp. 1-32 CrossRefGoogle Scholar
    [184] Golec M., et al. Qos analysis for serverless computing using machine learning
    Serverless Computing: Principles and Paradigms, Springer (2023), pp. 175-192 CrossRefView
    in ScopusGoogle Scholar [185] M.S. Aslanpour, et al., Serverless edge computing:
    vision and challenges, in: Proceedings of the 2021 Australasian Computer Science
    Week Multiconference, 2021, pp. 1–10. Google Scholar [186] Li Y., et al. Serverless
    computing: state-of-the-art, challenges and opportunities IEEE Trans. Serv. Comput.,
    16 (2) (2022), pp. 1522-1539 CrossRefView in ScopusGoogle Scholar [187] Kumar
    M., et al. AI-based sustainable and intelligent offloading framework for iIoT
    in collaborative cloud-fog environments IEEE Trans. Consum. Electron. (2023) Google
    Scholar [188] Iftikhar S., et al. TESCO: Multiple simulations based AI-augmented
    Fog computing for QoS optimization 2022 IEEE Smartworld, Ubiquitous Intelligence
    & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing,
    Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta),
    IEEE (2022), pp. 2092-2099 CrossRefView in ScopusGoogle Scholar [189] Firouzi
    F., et al. The convergence and interplay of edge, fog, and cloud in the AI-driven
    Internet of Things (IoT) Inf. Syst., 107 (2022), Article 101840 View PDFView articleView
    in ScopusGoogle Scholar [190] Cao Z., et al. Toward a systematic survey for carbon
    neutral data centers IEEE Commun. Surv. Tutor., 24 (2) (2022), pp. 895-936 CrossRefView
    in ScopusGoogle Scholar [191] Siddik M.A.B., et al. The environmental footprint
    of data centers in the United States Environ. Res. Lett., 16 (6) (2021), Article
    064017 CrossRefView in ScopusGoogle Scholar [192] Senthilkumar A., et al. Enhancement
    of R600a vapour compression refrigeration system with MWCNT/TiO2 hybrid nano lubricants
    for net zero emissions building Sustain. Energy Technol. Assess., 56 (2023), Article
    103055 View PDFView articleView in ScopusGoogle Scholar [193] Kurniawan T.A.,
    et al. Decarbonization in waste recycling industry using digitalization to promote
    net-zero emissions and its implications on sustainability J. Environ. Manag.,
    338 (2023), Article 117765 View PDFView articleView in ScopusGoogle Scholar [194]
    Wilkinson R., et al. Environmental impacts of earth observation data in the constellation
    and cloud computing era Sci. Total Environ., 909 (2024), Article 168584 View PDFView
    articleView in ScopusGoogle Scholar [195] Bhardwaj A.K., et al. HEART: Unrelated
    parallel machines problem with precedence constraints for task scheduling in cloud
    computing using heuristic and meta-heuristic algorithms Softw. - Pract. Exp.,
    50 (12) (2020), pp. 2231-2251 CrossRefView in ScopusGoogle Scholar [196] Fox G.C.,
    et al. Parallel Computing Works! Elsevier (2014) Google Scholar [197] Wu H., et
    al. A multi-dimensional parametric study of variability in multi-phase flow dynamics
    during geologic CO2 sequestration accelerated with machine learning Appl. Energy,
    287 (2021), Article 116580 View PDFView articleView in ScopusGoogle Scholar [198]
    Gill S.S. Quantum and blockchain based serverless edge computing: A vision, model,
    new trends and future directions Int. Technol. Lett. (2021), Article e275 Google
    Scholar [199] Nayeri Z.M., et al. Application placement in fog computing with
    AI approach: Taxonomy and a state of the art survey J. Netw. Comput. Appl., 185
    (2021), Article 103078 View PDFView articleView in ScopusGoogle Scholar [200]
    Patros P., et al. Rural AI: Serverless-powered federated learning for remote applications
    IEEE Internet Comput., 27 (2) (2023), pp. 28-34 CrossRefView in ScopusGoogle Scholar
    [201] Mahmud R., et al. Application management in fog computing environments:
    A taxonomy, review and future directions ACM Comput. Surv., 53 (4) (2020), pp.
    1-43 View in ScopusGoogle Scholar [202] Ruggeri A., et al. An innovative blockchain-based
    orchestrator for osmotic computing J. Grid Comput., 20 (2022), pp. 1-17 Google
    Scholar [203] Gill S.S., et al. SECURE: Self-protection approach in cloud resource
    management IEEE Cloud Comput., 5 (1) (2018), pp. 60-72 CrossRefView in ScopusGoogle
    Scholar [204] Ahammad I., et al. A review on cloud, fog, roof, and dew computing:
    Iot perspective Int. J. Cloud Appl. Comput. (IJCAC), 11 (4) (2021), pp. 14-41
    CrossRefView in ScopusGoogle Scholar [205] Mao Y., et al. A survey on mobile edge
    computing: The communication perspective IEEE Commun. Surv. Tutorials, 19 (4)
    (2017), pp. 2322-2358 View in ScopusGoogle Scholar [206] Luo Q., et al. Resource
    scheduling in edge computing: A survey IEEE Commun. Surv. Tutor., 23 (4) (2021),
    pp. 2131-2165 CrossRefView in ScopusGoogle Scholar [207] Cao K., et al. An overview
    on edge computing research IEEE Access, 8 (2020), pp. 85714-85728 CrossRefView
    in ScopusGoogle Scholar [208] Kotsehub N., et al. FLoX: Federated learning with
    FaaS at the edge 2022 IEEE 18th International Conference on E-Science (E-Science)
    (2022), pp. 11-20 CrossRefView in ScopusGoogle Scholar [209] Almurshed O., et
    al. Adaptive edge-cloud environments for rural AI 2022 IEEE International Conference
    on Services Computing (SCC) (2022), pp. 74-83 CrossRefView in ScopusGoogle Scholar
    [210] Abbas N., Zhang Y., Taherkordi A., Skeie T. Mobile edge computing: A survey
    IEEE Internet Things J., 5 (1) (2017), pp. 450-465 View in ScopusGoogle Scholar
    [211] Du J., et al. Computation energy efficiency maximization for NOMA-based
    and wireless-powered mobile edge computing with backscatter communication IEEE
    Trans. Mob. Comput. (2023), pp. 1-16 View in ScopusGoogle Scholar [212] Mach P.,
    et al. Mobile edge computing: A survey on architecture and computation offloading
    IEEE Commun. Surv. Tutorials, 19 (3) (2017), pp. 1628-1656 View in ScopusGoogle
    Scholar [213] Siriwardhana Y., et al. A survey on mobile augmented reality with
    5G mobile edge computing: Architectures, applications, and technical aspects IEEE
    Commun. Surv. Tutor., 23 (2) (2021), pp. 1160-1192 CrossRefView in ScopusGoogle
    Scholar [214] Golec M., et al. BlockFaaS: Blockchain-enabled serverless computing
    framework for AI-driven IoT healthcare applications J. Grid Comput., 21 (4) (2023),
    p. 63 View in ScopusGoogle Scholar [215] Zheng Z., et al. Blockchain challenges
    and opportunities: A survey Int. J. Web Grid Serv., 14 (4) (2018), pp. 352-375
    CrossRefView in ScopusGoogle Scholar [216] Gai K., et al. Blockchain meets cloud
    computing: A survey IEEE Commun. Surv. Tutor., 22 (3) (2020), pp. 2009-2030 CrossRefView
    in ScopusGoogle Scholar [217] Moqurrab S.A., et al. A deep learning-based privacy-preserving
    model for smart healthcare in internet of medical things using fog computing Wirel.
    Pers. Commun., 126 (3) (2022), pp. 2379-2401 CrossRefView in ScopusGoogle Scholar
    [218] Golec M., et al. Aiblock: Blockchain based lightweight framework for serverless
    computing using ai 2022 22nd IEEE International Symposium on Cluster, Cloud and
    Internet Computing (CCGrid), IEEE (2022), pp. 886-892 CrossRefView in ScopusGoogle
    Scholar [219] Kumar M., et al. Blockchain inspired secure and reliable data exchange
    architecture for cyber-physical healthcare system 4.0 Int. Things Cyber-Phys.
    Syst. (2023) Google Scholar [220] Li L., et al. A review of applications in federated
    learning Comput. Ind. Eng., 149 (2020), Article 106854 View PDFView articleView
    in ScopusGoogle Scholar [221] Yang J., et al. A federated learning attack method
    based on edge collaboration via cloud Softw. - Pract. Exp. (2022) Google Scholar
    [222] Zhang C., et al. A survey on federated learning Knowl.-Based Syst., 216
    (2021), Article 106775 View PDFView articleView in ScopusGoogle Scholar [223]
    Jiang W., et al. Federated split learning for sequential data in satellite–terrestrial
    integrated networks Inf. Fusion, 103 (2024), Article 102141 View PDFView articleView
    in ScopusGoogle Scholar [224] Kairouz P., et al. Advances and open problems in
    federated learning Found. Trends Mach. Learn., 14 (1–2) (2021), pp. 1-210 CrossRefView
    in ScopusGoogle Scholar [225] Wu G., et al. Privacy-preserving offloading scheme
    in multi-access mobile edge computing based on MADRL J. Parallel Distrib. Comput.,
    183 (2024), Article 104775 View PDFView articleView in ScopusGoogle Scholar [226]
    Ferdous M.S., et al. A survey of consensus algorithms in public blockchain systems
    for crypto-currencies J. Netw. Comput. Appl., 182 (2021), Article 103035 View
    PDFView articleView in ScopusGoogle Scholar [227] Manimuthu A., et al. A literature
    review on Bitcoin: Transformation of crypto currency into a global phenomenon
    IEEE Eng. Manag. Rev., 47 (1) (2019), pp. 28-35 CrossRefView in ScopusGoogle Scholar
    [228] Xu J., et al. A survey of blockchain consensus protocols ACM Comput. Surv.
    (2023) Google Scholar [229] Wang X., et al. Blockchain intelligence for internet
    of vehicles: Challenges and solutions IEEE Commun. Surv. Tutor. (2023) Google
    Scholar [230] Rahardja U., et al. GOOD, bad and dark bitcoin: a systematic literature
    review Aptisi Trans. Technopreneurship (ATT), 3 (2) (2021), pp. 115-119 View in
    ScopusGoogle Scholar [231] Golec M., et al. IFaaSBus: A security-and privacy-based
    lightweight framework for serverless computing using IoT and machine learning
    IEEE Trans. Ind. Inform., 18 (5) (2021), pp. 3522-3529 Google Scholar [232] Qu
    G., et al. ChainFL: A simulation platform for joint federated learning and blockchain
    in edge/cloud computing environments IEEE Trans. Ind. Inform., 18 (5) (2022),
    pp. 3572-3581 CrossRefView in ScopusGoogle Scholar [233] Golec M., et al. HealthFaaS:
    AI based smart healthcare system for heart patients using serverless computing
    IEEE Internet Things J. (2023) Google Scholar [234] Svorobej S., et al. Orchestration
    from the cloud to the edge The Cloud-to-Thing Continuum: Opportunities and Challenges
    in Cloud, Fog and Edge Computing, Springer International Publishing (2020), pp.
    61-77 CrossRefView in ScopusGoogle Scholar [235] Härdle W.K., et al. Understanding
    cryptocurrencies J. Financ. Econom., 18 (2) (2020), pp. 181-208 CrossRefView in
    ScopusGoogle Scholar [236] Weichbroth P., et al. Security of cryptocurrencies:
    A view on the state-of-the-art research and current developments Sensors, 23 (6)
    (2023), p. 3155 CrossRefView in ScopusGoogle Scholar [237] Schweizer A., et al.
    To what extent will blockchain drive the machine economy? Perspectives from a
    prospective study IEEE Trans. Eng. Manage., 67 (4) (2020), pp. 1169-1183 CrossRefView
    in ScopusGoogle Scholar [238] Khan M., et al. A review of distributed ledger technologies
    in the machine economy: challenges and opportunities in industry and research
    Proc. CIRP, 107 (2022), pp. 1168-1173 View PDFView articleView in ScopusGoogle
    Scholar [239] Dustdar S., et al. On distributed computing continuum systems IEEE
    Trans. Knowl. Data Eng., 35 (4) (2022), pp. 4092-4105 Google Scholar [240] Donta
    P.K., et al. Exploring the potential of distributed computing continuum systems
    Computers, 12 (10) (2023), p. 198 CrossRefView in ScopusGoogle Scholar [241] Morichetta
    A., et al. A roadmap on learning and reasoning for distributed computing continuum
    ecosystems IEEE International Conference on Edge Computing (EDGE), Institute of
    Electrical and Electronics Engineers (IEEE) (2021), pp. 25-31 CrossRefView in
    ScopusGoogle Scholar [242] Beasley C.J., et al. A new look at simultaneous sources
    Seg Technical Program Expanded Abstracts 1998, Society of Exploration Geophysicists
    (1998), pp. 133-135 CrossRefView in ScopusGoogle Scholar [243] Aminizadeh S.,
    et al. The applications of machine learning techniques in medical data processing
    based on distributed computing and the Internet of Things Comput. Methods Programs
    Biomed. (2023), Article 107745 View PDFView articleView in ScopusGoogle Scholar
    [244] Petrou L., et al. The first family of application-specific integrated circuits
    for programmable and reconfigurable metasurfaces Sci. Rep., 12 (1) (2022), p.
    5826 View in ScopusGoogle Scholar [245] Murray K.E., et al. Vtr 8: High-performance
    cad and customizable fpga architecture modelling ACM Trans. Reconfigurable Technol.
    Syst. (TRETS), 13 (2) (2020), pp. 1-55 CrossRefGoogle Scholar [246] Hitzler P.,
    et al. Neuro-Symbolic Artificial Intelligence: The State of the Art IOS Press
    (2022) Google Scholar [247] Gaur M., et al. Knowledge-infused learning: A sweet
    spot in neuro-symbolic ai IEEE Internet Comput., 26 (4) (2022), pp. 5-11 CrossRefView
    in ScopusGoogle Scholar [248] Du J., et al. Computation energy efficiency maximization
    for intelligent reflective surface-aided wireless powered mobile edge computing
    IEEE Trans. Sustain. Comput. (2023) Google Scholar [249] Cuadrado J., et al. Intelligent
    simulation of multibody dynamics: space-state and descriptor methods in sequential
    and parallel computing environments Multibody Syst. Dyn., 4 (2000), pp. 55-73
    View in ScopusGoogle Scholar [250] Zhang Y., et al. Transparent computing: Spatio-temporal
    extension on von Neumann architecture for cloud services Tsinghua Sci. Technol.,
    18 (1) (2013), pp. 10-21 View in ScopusGoogle Scholar [251] Jiang Q., et al. Adaptive
    scheduling of stochastic task sequence for energy-efficient mobile cloud computing
    IEEE Syst. J., 13 (3) (2019), pp. 3022-3025 CrossRefView in ScopusGoogle Scholar
    [252] Bufistov D., et al. A general model for performance optimization of sequential
    systems 2007 IEEE/ACM International Conference on Computer-Aided Design, IEEE
    (2007), pp. 362-369 CrossRefView in ScopusGoogle Scholar [253] Aslanpour M.S.,
    et al. Performance evaluation metrics for cloud, fog and edge computing: A review,
    taxonomy, benchmarks and standards for future research Int. Things, 12 (2020),
    Article 100273 View PDFView articleView in ScopusGoogle Scholar [254] Singh A.,
    et al. Quantum internet—applications, functionalities, enabling technologies,
    challenges, and research directions IEEE Commun. Surv. Tutor., 23 (4) (2021),
    pp. 2218-2247 CrossRefView in ScopusGoogle Scholar [255] De Leon N.P., et al.
    Materials challenges and opportunities for quantum computing hardware Science,
    372 (6539) (2021), p. eabb2823 View in ScopusGoogle Scholar [256] Smith K.N.,
    et al. Scaling superconducting quantum computers with chiplet architectures 2022
    55th IEEE/ACM International Symposium on Microarchitecture (MICRO), IEEE (2022),
    pp. 1092-1109 CrossRefView in ScopusGoogle Scholar [257] Spivey R.F., et al. High-stability
    cryogenic system for quantum computing with compact packaged ion traps IEEE Trans.
    Quant. Eng., 3 (2021), pp. 1-11 CrossRefGoogle Scholar [258] Nandhakumar A.R.,
    et al. EdgeAISim: A Toolkit for Simulation and Modelling of AI Models in Edge
    Computing Environments Meas.: Sensors (2023) Google Scholar [259] Xue M., et al.
    DDPQN: An efficient DNN offloading strategy in local-edge-cloud collaborative
    environments IEEE Trans. Serv. Comput., 15 (2) (2022), pp. 640-655 CrossRefView
    in ScopusGoogle Scholar [260] Lee Y.-L., et al. Techology trend of edge AI 2018
    International Symposium on VLSI Design, Automation and Test (VLSI-DAT), IEEE (2018),
    pp. 1-2 Google Scholar [261] Ding A.Y., et al. Roadmap for edge ai: A dagstuhl
    perspective ACM SIGCOMM Comput. Commun. Rev., 52 (1) (2022), pp. 28-33 CrossRefView
    in ScopusGoogle Scholar [262] Murugesan D., et al. Comparison of biologically
    inspired algorithm with socio-inspired technique on load frequency control of
    multi-source single-area power system Applied Genetic Algorithm and Its Variants:
    Case Studies and New Developments, Springer (2023), pp. 185-208 CrossRefGoogle
    Scholar [263] Kar A.K. Bio inspired computing–a review of algorithms and scope
    of applications Expert Syst. Appl., 59 (2016), pp. 20-32 View PDFView articleView
    in ScopusGoogle Scholar [264] Xu M., et al. esDNN: deep neural network based multivariate
    workload prediction in cloud computing environments ACM Trans. Int. Technol. (TOIT),
    22 (3) (2022), pp. 1-24 CrossRefView in ScopusGoogle Scholar [265] Denkena B.,
    et al. Reprint of: Gentelligent processes in biologically inspired manufacturing
    CIRP J. Manuf. Sci. Technol., 34 (2021), pp. 105-118 View PDFView articleView
    in ScopusGoogle Scholar [266] Dwivedi R., et al. Explainable AI (XAI): Core ideas,
    techniques, and solutions ACM Comput. Surv., 55 (9) (2023), pp. 1-33 CrossRefGoogle
    Scholar [267] Tosun A.B., et al. Histomapr™: An explainable AI (XAI) platform
    for computational pathology solutions Artificial Intelligence and Machine Learning
    for Digital Pathology: State-of-the-Art and Future Challenges, Springer (2020),
    pp. 204-227 CrossRefView in ScopusGoogle Scholar [268] Arrieta A.B., et al. Explainable
    artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges
    toward responsible AI Inf. Fusion, 58 (2020), pp. 82-115 Google Scholar [269]
    Kochovski P., et al. Trust management in a blockchain based fog computing platform
    with trustless smart oracles Future Gener. Comput. Syst., 101 (2019), pp. 747-759
    View PDFView articleView in ScopusGoogle Scholar [270] Shkembi K., et al. Semantic
    web and blockchain technologies: Convergence, challenges and research trends J.
    Web Semant., 79 (2023), Article 100809 View PDFView articleView in ScopusGoogle
    Scholar [271] Córcoles A.D., et al. Challenges and opportunities of near-term
    quantum computing systems Proc. IEEE, 108 (8) (2019), pp. 1338-1352 Google Scholar
    [272] Pirandola S., et al. Physics: unite to build a quantum internet Nature,
    532 (7598) (2016), pp. 169-171 CrossRefView in ScopusGoogle Scholar [273] Wehner
    S., et al. Quantum internet: a vision for the road ahead Science, 362 (6412) (2018),
    p. eaam9288 View in ScopusGoogle Scholar [274] Seto K.C., et al. From low-to net-zero
    carbon cities: The next global agenda Ann. Rev. Environ. Resour., 46 (2021), pp.
    377-415 CrossRefView in ScopusGoogle Scholar [275] Aceto G., et al. A survey on
    information and communication technologies for industry 4.0: State-of-the-art,
    taxonomies, perspectives, and challenges IEEE Commun. Surv. Tutor., 21 (4) (2019),
    pp. 3467-3501 CrossRefView in ScopusGoogle Scholar [276] Aceto G., et al. Industry
    4.0 and health: Internet of things, big data, and cloud computing for healthcare
    4.0 J. Ind. Inf. Integr., 18 (2020), Article 100129 View PDFView articleView in
    ScopusGoogle Scholar [277] Teoh Y.K., et al. IoT and fog computing based predictive
    maintenance model for effective asset management in industry 4.0 using machine
    learning IEEE Internet Things J. (2021) Google Scholar [278] Zheng T., et al.
    The applications of Industry 4.0 technologies in manufacturing context: a systematic
    literature review Int. J. Prod. Res., 59 (6) (2021), pp. 1922-1954 CrossRefView
    in ScopusGoogle Scholar [279] Yu W., et al. Energy digital twin technology for
    industrial energy management: Classification, challenges and future Renew. Sustain.
    Energy Rev., 161 (2022), Article 112407 View PDFView articleView in ScopusGoogle
    Scholar [280] Mihai S., et al. Digital twins: A survey on enabling technologies,
    challenges, trends and future prospects IEEE Commun. Surv. Tutor. (2022) Google
    Scholar [281] Wang Y., et al. A survey on digital twins: architecture, enabling
    technologies, security and privacy, and future prospects IEEE Internet Things
    J. (2023) Google Scholar [282] Kor M., et al. An investigation for integration
    of deep learning and digital twins towards construction 4.0 Smart Sustain. Built
    Environ., 12 (3) (2023), pp. 461-487 CrossRefView in ScopusGoogle Scholar [283]
    Singh S., et al. Qos-aware autonomic resource management in cloud computing: a
    systematic review ACM Comput. Surv., 48 (3) (2015), pp. 1-46 Google Scholar [284]
    Morichetta A., et al. Demystifying deep learning in predictive monitoring for
    cloud-native SLOs 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
    (2023), pp. 1-11 CrossRefGoogle Scholar [285] Wright S.A. Performance modeling,
    benchmarking and simulation of high performance computing systems Future Gener.
    Comput. Syst., 92 (2019), pp. 900-902 View PDFView articleView in ScopusGoogle
    Scholar [286] Materwala H., et al. QoS-SLA-aware adaptive genetic algorithm for
    multi-request offloading in integrated edge-cloud computing in internet of vehicles
    Veh. Commun., 43 (2023), Article 100654 View PDFView articleView in ScopusGoogle
    Scholar [287] Sharma Y., et al. SLA management in intent-driven service management
    systems: A taxonomy and future directions ACM Comput. Surv. (2023) Google Scholar
    [288] Khan S., et al. Guaranteeing end-to-end QoS provisioning in SOA based SDN
    architecture: A survey and open issues Future Gener. Comput. Syst., 119 (2021),
    pp. 176-187 View PDFView articleView in ScopusGoogle Scholar [289] Dilek S., et
    al. QoS-aware IoT networks and protocols: A comprehensive survey Int. J. Commun.
    Syst., 35 (10) (2022), Article e5156 View in ScopusGoogle Scholar [290] Pujol
    V.C., et al. Towards a prime directive of SLOs 2023 IEEE International Conference
    on Software Services Engineering (SSE) (2023), pp. 61-70 CrossRefView in ScopusGoogle
    Scholar [291] P. Patros, et al., SLO request modeling, reordering and scaling,
    in: Proceedings of the 27th Annual International Conference on Computer Science
    and Software Engineering, 2017, pp. 180–191. Google Scholar [292] Singh S., et
    al. The journey of qos-aware autonomic cloud computing IT Prof., 19 (2) (2017),
    pp. 42-49 View in ScopusGoogle Scholar [293] Patros P.o. Investigating the effect
    of garbage collection on service level objectives of clouds 2017 IEEE International
    Conference on Cluster Computing (CLUSTER), IEEE (2017), pp. 633-634 CrossRefView
    in ScopusGoogle Scholar [294] Zeng X., et al. SLA management for big data analytical
    applications in clouds: A taxonomy study ACM Comput. Surv., 53 (3) (2020), pp.
    1-40 Google Scholar [295] Qu C., et al. Auto-scaling web applications in clouds:
    A taxonomy and survey ACM Comput. Surv., 51 (4) (2018), pp. 1-33 View in ScopusGoogle
    Scholar [296] Lorido-Botran T., et al. A review of auto-scaling techniques for
    elastic applications in cloud environments J. Grid Comput., 12 (2014), pp. 559-592
    CrossRefView in ScopusGoogle Scholar [297] Singh P., et al. RHAS: robust hybrid
    auto-scaling for web applications in cloud computing Cluster Comput., 24 (2) (2021),
    pp. 717-737 CrossRefView in ScopusGoogle Scholar [298] T. Heinze, et al., Auto-scaling
    techniques for elastic data stream processing, in: Proceedings of the 8th ACM
    International Conference on Distributed Event-Based Systems, 2014, pp. 318–321.
    Google Scholar [299] Gill S.S., et al. Holistic resource management for sustainable
    and reliable cloud computing: An innovative solution to global challenge J. Syst.
    Softw., 155 (2019), pp. 104-129 View PDFView articleView in ScopusGoogle Scholar
    [300] Bharany S., et al. Energy efficient fault tolerance techniques in green
    cloud computing: A systematic survey and taxonomy Sustain. Energy Technol. Assess.,
    53 (2022), Article 102613 View PDFView articleView in ScopusGoogle Scholar [301]
    Gill S.S., et al. Failure management for reliable cloud computing: a taxonomy,
    model, and future directions Comput. Sci. Eng., 22 (3) (2018), pp. 52-63 View
    in ScopusGoogle Scholar [302] Gill S.S., et al. Tails in the cloud: a survey and
    taxonomy of straggler management within large-scale cloud data centres J. Supercomput.,
    76 (2020), pp. 10050-10089 CrossRefView in ScopusGoogle Scholar [303] Gill S.S.
    A manifesto for modern fog and edge computing: Vision, new paradigms, opportunities,
    and future directions Operationalizing Multi-Cloud Environments: Technologies,
    Tools and Use Cases, Springer (2021), pp. 237-253 Google Scholar [304] Katal A.,
    et al. Energy efficiency in cloud computing data centers: a survey on software
    technologies Cluster Comput., 26 (3) (2023), pp. 1845-1875 CrossRefView in ScopusGoogle
    Scholar [305] Masanet E., et al. Recalibrating global data center energy-use estimates
    Science, 367 (6481) (2020), pp. 984-986 CrossRefView in ScopusGoogle Scholar [306]
    Iftikhar S., et al. HunterPlus: AI based energy-efficient task scheduling for
    cloud–fog computing environments Int. Things, 21 (2023), Article 100667 View PDFView
    articleView in ScopusGoogle Scholar [307] Tuli S., et al. HUNTER: AI based holistic
    resource management for sustainable cloud computing J. Syst. Softw., 184 (2022),
    Article 111124 View PDFView articleView in ScopusGoogle Scholar [308] Schneider
    T., et al. Harnessing AI and computing to advance climate modelling and prediction
    Nature Clim. Change, 13 (9) (2023), pp. 887-889 CrossRefView in ScopusGoogle Scholar
    [309] Hartmann M., et al. Edge computing in smart health care systems: Review,
    challenges, and research directions Trans. Emerg. Telecommun. Technol., 33 (3)
    (2022), Article e3710 View in ScopusGoogle Scholar [310] Baek H.J., et al. Enhancing
    the usability of brain-computer interface systems Comput. Intell. Neurosci., 2019
    (2019) Google Scholar [311] Miraz M.H., et al. Adaptive user interfaces and universal
    usability through plasticity of user interface design Comp. Sci. Rev., 40 (2021),
    Article 100363 View PDFView articleView in ScopusGoogle Scholar [312] Diaz-de
    Arcaya J., et al. A joint study of the challenges, opportunities, and roadmap
    of mlops and aiops: A systematic survey ACM Comput. Surv., 56 (4) (2023), pp.
    1-30 CrossRefGoogle Scholar [313] Celik I. Exploring the determinants of artificial
    intelligence (Ai) literacy: Digital divide, computational thinking, cognitive
    absorption Telemat. Inform., 83 (2023), Article 102026 View PDFView articleView
    in ScopusGoogle Scholar [314] Gill S.S., et al. Transformative effects of ChatGPT
    on modern education: Emerging Era of AI chatbots Int. Things Cyber-Phys. Syst.,
    4 (2024), pp. 19-23 View PDFView articleView in ScopusGoogle Scholar [315] Le
    Roux C., et al. Can cloud computing bridge the digital divide in South African
    secondary education? Inf. Dev., 27 (2) (2011), pp. 109-116 CrossRefView in ScopusGoogle
    Scholar [316] Arce C.G.M., et al. Optimizing business performance: Marketing strategies
    for small and medium businesses using artificial intelligence tools Migr. Lett.,
    21 (S1) (2024), pp. 193-201 Google Scholar [317] Qadir J., et al. Toward accountable
    human-centered AI: rationale and promising directions J. Inf., Commun. Ethics
    Soc., 20 (2) (2022), pp. 329-342 CrossRefView in ScopusGoogle Scholar [318] Munn
    L. The uselessness of AI ethics AI Ethics, 3 (3) (2023), pp. 869-877 CrossRefGoogle
    Scholar [319] Scuotto V., et al. The digital humanism era triggered by individual
    creativity J. Bus. Res., 158 (2023), Article 113709 View PDFView articleView in
    ScopusGoogle Scholar [320] Schaap J., et al. ‘Gods in world of warcraft exist’:
    Religious reflexivity and the quest for meaning in online computer games New Media
    Soc., 19 (11) (2017), pp. 1744-1760 CrossRefView in ScopusGoogle Scholar [321]
    Magni D., et al. Digital humanism and artificial intelligence: the role of emotions
    beyond the human–machine interaction in society 5.0 J. Manag. History (2023) Google
    Scholar [322] Yu Q., et al. Lagrange coded computing: Optimal design for resiliency,
    security, and privacy The 22nd International Conference on Artificial Intelligence
    and Statistics, PMLR (2019), pp. 1215-1225 CrossRefView in ScopusGoogle Scholar
    [323] Olowononi F.O., et al. Resilient machine learning for networked cyber physical
    systems: A survey for machine learning security to securing machine learning for
    CPS IEEE Commun. Surv. Tutor., 23 (1) (2020), pp. 524-552 Google Scholar [324]
    Liu Z., et al. Efficient dropout-resilient aggregation for privacy-preserving
    machine learning IEEE Trans. Inf. Forensics Secur., 18 (2022), pp. 1839-1854 CrossRefView
    in ScopusGoogle Scholar [325] Samriya J.K., et al. Secured data offloading using
    reinforcement learning and Markov decision process in mobile edge computing Int.
    J. Netw. Manag., 33 (5) (2023), Article e2243 View in ScopusGoogle Scholar [326]
    Ullah I., et al. Privacy preserving large language models: Chatgpt case study
    based vision and framework (2023) arXiv preprint arXiv:2310.12523 Google Scholar
    [327] Kim H., et al. Resilient authentication and authorization for the internet
    of things (IoT) using edge computing ACM Trans. Int. Things, 1 (1) (2020), pp.
    1-27 Google Scholar [328] Delacour C., et al. Energy-performance assessment of
    oscillatory neural networks based on VO devices for future edge AI computing IEEE
    Trans. Neural Netw. Learn. Syst. (2023) Google Scholar [329] Quan Z., et al. A
    historical review on learning with technology: From computers to smartphones Encyclopedia
    of Information Science and Technology, Sixth Edition, IGI Global (2025), pp. 1-21
    CrossRefGoogle Scholar [330] Mijuskovic A., et al. Resource management techniques
    for cloud/fog and edge computing: An evaluation framework and classification Sensors,
    21 (5) (2021), p. 1832 CrossRefGoogle Scholar [331] Singh S., et al. A survey
    on resource scheduling in cloud computing: Issues and challenges J. Grid Comput.,
    14 (2016), pp. 217-264 View in ScopusGoogle Scholar [332] Hong C.-H., et al. Resource
    management in fog/edge computing: a survey on architectures, infrastructure, and
    algorithms ACM Comput. Surv., 52 (5) (2019), pp. 1-37 View in ScopusGoogle Scholar
    [333] Jamil B., et al. Resource allocation and task scheduling in fog computing
    and internet of everything environments: A taxonomy, review, and future directions
    ACM Comput. Surv., 54 (11s) (2022), pp. 1-38 CrossRefGoogle Scholar [334] Raju
    A., et al. A comparative study of spark schedulers’ performance 2019 4th International
    Conference on Computational Systems and Information Technology for Sustainable
    Solution (CSITSS), IEEE (2019), pp. 1-5 Google Scholar [335] Henning S., et al.
    Benchmarking scalability of stream processing frameworks deployed as microservices
    in the cloud J. Syst. Softw., 208 (2024), Article 111879 View PDFView articleView
    in ScopusGoogle Scholar [336] Feng J., et al. Heterogeneous computation and resource
    allocation for wireless powered federated edge learning systems IEEE Trans. Commun.,
    70 (5) (2022), pp. 3220-3233 CrossRefView in ScopusGoogle Scholar [337] Garofalo
    A., et al. A heterogeneous in-memory computing cluster for flexible end-to-end
    inference of real-world deep neural networks IEEE J. Emerg. Sel. Top. Circuits
    Syst., 12 (2) (2022), pp. 422-435 CrossRefView in ScopusGoogle Scholar [338] Wu
    H., et al. Collaborate edge and cloud computing with distributed deep learning
    for smart city internet of things IEEE Internet Things J., 7 (9) (2020), pp. 8099-8110
    CrossRefView in ScopusGoogle Scholar [339] Kumar V. Digital enablers The Economic
    Value of Digital Disruption: A Holistic Assessment for CXOs, Springer (2023),
    pp. 1-110 View in ScopusGoogle Scholar [340] Sha K., et al. A survey of edge computing-based
    designs for IoT security Digit. Commun. Netw., 6 (2) (2020), pp. 195-202 View
    PDFView articleView in ScopusGoogle Scholar [341] Sequeiros J.B., et al. Attack
    and system modeling applied to IoT, cloud, and mobile ecosystems: Embedding security
    by design ACM Comput. Surv., 53 (2) (2020), pp. 1-32 Google Scholar [342] Kaur
    A., et al. The future of cloud computing: opportunities, challenges and research
    trends 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics
    and Cloud)(I-SMAC) I-SMAC, IEEE (2018), pp. 213-219 CrossRefView in ScopusGoogle
    Scholar [343] Sebastian A., et al. Memory devices and applications for in-memory
    computing Nature Nanotechnol., 15 (7) (2020), pp. 529-544 CrossRefView in ScopusGoogle
    Scholar [344] Vu K., et al. ICT as a driver of economic growth: A survey of the
    literature and directions for future research Telecommun. Policy, 44 (2) (2020),
    Article 101922 View PDFView articleView in ScopusGoogle Scholar [345] Tesfatsion
    L. Agent-based computational economics: Overview and brief history Artif. Intell.,
    Learn. Comput. Econ. Finance (2023), pp. 41-58 CrossRefView in ScopusGoogle Scholar
    [346] Vairetti C., et al. Analytics-driven complaint prioritisation via deep learning
    and multicriteria decision-making European J. Oper. Res., 312 (3) (2024), pp.
    1108-1118 View PDFView articleView in ScopusGoogle Scholar [347] Jobin A., et
    al. The global landscape of AI ethics guidelines Nat. Mach. Intell., 1 (9) (2019),
    pp. 389-399 CrossRefGoogle Scholar [348] Hariri R.H., et al. Uncertainty in big
    data analytics: survey, opportunities, and challenges J. Big Data, 6 (1) (2019),
    pp. 1-16 Google Scholar [349] Cao L. Data science: a comprehensive overview ACM
    Comput. Surv., 50 (3) (2017), pp. 1-42 View in ScopusGoogle Scholar [350] Daniel
    B.K. Big data and data science: A critical review of issues for educational research
    Br. J. Educ. Technol., 50 (1) (2019), pp. 101-113 CrossRefView in ScopusGoogle
    Scholar [351] Donta P.K., et al. Governance and sustainability of distributed
    continuum systems: a big data approach J. Big Data, 10 (1) (2023), pp. 1-31 Google
    Scholar [352] ur Rehman M.H., et al. The role of big data analytics in industrial
    Internet of Things Future Gener. Comput. Syst., 99 (2019), pp. 247-259 View PDFView
    articleView in ScopusGoogle Scholar Cited by (3) CloudAISim: A toolkit for modelling
    and simulation of modern applications in AI-driven cloud computing environments
    2023, BenchCouncil Transactions on Benchmarks, Standards and Evaluations Show
    abstract PREDICTION OF CRYPTOCURRENCY PRICES USING LSTM, SVM AND POLYNOMIAL REGRESSION
    2024, arXiv Quantum Computing: Vision and Challenges 2024, arXiv © 2024 The Authors.
    Published by Elsevier B.V. Recommended articles Secure Internet of medical Things
    (IoMT) based on ECMQV-MAC authentication protocol and EKMC-SCP blockchain networking
    Information Sciences, Volume 654, 2024, Article 119783 Qinyong Lin, …, D. Paulraj
    View PDF Smarter eco-cities and their leading-edge artificial intelligence of
    things solutions for environmental sustainability: A comprehensive systematic
    review Environmental Science and Ecotechnology, Volume 19, 2024, Article 100330
    Simon Elias Bibri, …, Alexandre Alahi View PDF Cloud-based non-destructive characterization
    Non-Destructive Material Characterization Methods, 2024, pp. 727-765 Arash Heidari,
    …, Akira Otsuki Show 3 more articles Article Metrics Captures Readers: 29 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Telematics and Informatics Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Modern computing: Vision and challenges'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lipovac I.
  - Babac M.B.
  citation_count: '0'
  description: This paper presents a comprehensive exploration of the concept of big
    data and its management while highlighting the challenges that arise in the process.
    The study showcases the development of a data pipeline, designed to facilitate
    big data collection, integration, and analysis while addressing state-of-the-art
    challenges, methods, tools, and technologies. Emphasis is placed on pipeline flexibility,
    with a view towards enabling ease of implementation of architecture changes, seamless
    integration of new sources, and straightforward implementation of additional transformations
    in existing pipelines as needed. The pipeline architecture is discussed in detail,
    with a focus on its design principles, components, and implementation details,
    as well as the mechanisms used to ensure its reliability, scalability, and performance.
    Results from a range of experiments demonstrate the pipeline’s effectiveness in
    addressing the challenges of big data management and analysis, as well as its
    robustness and versatility in accommodating diverse data sources and processing
    requirements. This study provides insights into the critical role of data pipelines
    in enabling effective big data management and showcases the importance of flexibility
    in pipeline design to ensure adaptability to evolving data processing needs.
  doi: 10.1504/IJDMMM.2024.136221
  full_citation: '>'
  full_text: '>

    "Login Help Sitemap Home For Authors For Librarians Orders Inderscience Online
    News Home Full-text access for editors Developing a data pipeline solution for
    big data processing by Ivona Lipovac; Marina Bagić Babac International Journal
    of Data Mining, Modelling and Management (IJDMMM), Vol. 16, No. 1, 2024  Abstract:
    This paper presents a comprehensive exploration of the concept of big data and
    its management while highlighting the challenges that arise in the process. The
    study showcases the development of a data pipeline, designed to facilitate big
    data collection, integration, and analysis while addressing state-of-the-art challenges,
    methods, tools, and technologies. Emphasis is placed on pipeline flexibility,
    with a view towards enabling ease of implementation of architecture changes, seamless
    integration of new sources, and straightforward implementation of additional transformations
    in existing pipelines as needed. The pipeline architecture is discussed in detail,
    with a focus on its design principles, components, and implementation details,
    as well as the mechanisms used to ensure its reliability, scalability, and performance.
    Results from a range of experiments demonstrate the pipeline''s effectiveness
    in addressing the challenges of big data management and analysis, as well as its
    robustness and versatility in accommodating diverse data sources and processing
    requirements. This study provides insights into the critical role of data pipelines
    in enabling effective big data management and showcases the importance of flexibility
    in pipeline design to ensure adaptability to evolving data processing needs. Online
    publication date: Mon, 22-Jan-2024 The full text of this article is only available
    to individual subscribers or to users at subscribing institutions.   Existing
    subscribers: Go to Inderscience Online Journals to access the Full Text of this
    article. Pay per view: If you are not a subscriber and you just want to read the
    full contents of this article, buy online access here. Complimentary Subscribers,
    Editors or Members of the Editorial Board of the International Journal of Data
    Mining, Modelling and Management (IJDMMM): Login with your Inderscience username
    and password:     Username:        Password:          Forgotten your password?  Want
    to subscribe? A subscription gives you complete access to all articles in the
    current issue, as well as to all articles in the previous three years (where applicable).
    See our Orders page to subscribe. If you still need assistance, please email subs@inderscience.com    Keep
    up-to-date Our Blog Follow us on Twitter Visit us on Facebook Our Newsletter (subscribe
    for free) RSS Feeds New issue alerts Return to top Contact us About Inderscience
    OAI Repository Privacy and Cookies Statement Terms and Conditions Help Sitemap
    © 2024 Inderscience Enterprises Ltd."'
  inline_citation: '>'
  journal: International Journal of Data Mining, Modelling and Management
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Developing a data pipeline solution for big data processing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Alfares N.
  - Kesidis G.
  citation_count: '0'
  description: 'Over the past ten years, many different approaches have been proposed
    for different aspects of the problem of resources management for long running,
    dynamic and diverse workloads such as processing query streams or distributed
    deep learning. Particularly for applications consisting of containerized microservices,
    researchers have attempted to address problems of dynamic selection of, for example:
    types and quantities of virtualized services (e.g., IaaS/VMs), horizontal and
    vertical scaling of different microservices, assigning microservices to VMs, task
    scheduling, or some combination thereof. In this context, we argue that online
    optimization frameworks like simulated annealing are highly suitable for exploration
    of the trade-offs between performance (SLO) and cost, particularly when the complex
    workloads and cloud-service offerings vary over time. Based on a macroscopic objective
    that combines both performance and cost terms, annealing facilitates light-weight
    and coherent policies of exploration and exploitation. In this paper, we first
    give some background on simulated annealing and then experimentally demonstrate
    its usefulness for container sizing using microservice benchmarks. We conclude
    with a discussion of how the basic annealing platform can be applied to other
    resource-management problems, hybridized with other methods, and accommodate user-specified
    rules of thumb.'
  doi: 10.1145/3631311.3632399
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesMIDDLEWAREProceedingsWoC
    ''23Container Sizing for Microservices with Dynamic Workload by Online Optimization
    RESEARCH-ARTICLE SHARE ON Container Sizing for Microservices with Dynamic Workload
    by Online Optimization Authors: Nader Alfares , George Kesidis Authors Info &
    Claims WoC ''23: Proceedings of the 9th International Workshop on Container Technologies
    and Container CloudsDecember 2023Pages 1–6https://doi.org/10.1145/3631311.3632399
    Published:08 January 2024Publication History 0 citation 139 Downloads eReaderPDF
    WoC ''23: Proceedings of the 9th International Workshop on Container Technologies
    and Container Clouds Container Sizing for Microservices with Dynamic Workload
    by Online Optimization Pages 1–6 Previous Next ABSTRACT References Index Terms
    Recommendations Comments ABSTRACT Over the past ten years, many different approaches
    have been proposed for different aspects of the problem of resources management
    for long running, dynamic and diverse workloads such as processing query streams
    or distributed deep learning. Particularly for applications consisting of containerized
    microservices, researchers have attempted to address problems of dynamic selection
    of, for example: types and quantities of virtualized services (e.g., IaaS/VMs),
    horizontal and vertical scaling of different microservices, assigning microservices
    to VMs, task scheduling, or some combination thereof. In this context, we argue
    that online optimization frameworks like simulated annealing are highly suitable
    for exploration of the trade-offs between performance (SLO) and cost, particularly
    when the complex workloads and cloud-service offerings vary over time. Based on
    a macroscopic objective that combines both performance and cost terms, annealing
    facilitates light-weight and coherent policies of exploration and exploitation.
    In this paper, we first give some background on simulated annealing and then experimentally
    demonstrate its usefulness for container sizing using microservice benchmarks.
    We conclude with a discussion of how the basic annealing platform can be applied
    to other resource-management problems, hybridized with other methods, and accommodate
    user-specified rules of thumb. References E. Aarts and J. Korst. Simulated Annealing
    and Boltzmann Machines. Wiley, 1989. N. Alfares, G. Kesidis, A.F. Baarzi, and
    A. Jain. Cluster resource management for dynamic workloads by online optimization.
    http://arxiv.org/abs/2207.04594. C. Bystrom, J. Heyman, and T. Gustafsson. Locust.
    https://github.com/locustio/locust. Show All References Index Terms Container
    Sizing for Microservices with Dynamic Workload by Online Optimization Computing
    methodologies Mathematics of computing Mathematical analysis Mathematical optimization
    Software and its engineering Software organization and properties Theory of computation
    Design and analysis of algorithms Online algorithms Online learning algorithms
    Theory and algorithms for application domains Machine learning theory Reinforcement
    learning Sequential decision making Index terms have been assigned to the content
    through auto-classification. Recommendations Proactive workload management in
    dynamic virtualized environments Recently, with the improvement of Cloud systems
    technologies and the essential advantages they can provide such as availability,
    scalability, and costs saving; massive domains in the IT industry are directing
    their business to the Cloud. To fit the ... Read More Container-based operating
    system virtualization: a scalable, high-performance alternative to hypervisors
    EuroSys ''07: Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on
    Computer Systems 2007 Hypervisors, popularized by Xen and VMware, are quickly
    becoming commodity. They are appropriate for many usage scenarios, but there are
    scenarios that require system virtualization with high degrees of both isolation
    and efficiency. Examples include ... Read More An Extended Virtual Network Functions
    Manager Architecture to Support Container ICISS ''18: Proceedings of the 1st International
    Conference on Information Science and Systems Network function virtualization
    (NFV) is a network architecture concept that decouples network functions from
    hardware. With NFV, virtual network functions (VNFs) can easily be instantiated
    and deployed on standard servers using virtual machines (VMs). ... Read More Comments
    38 References View Table Of Contents Footer Categories Journals Magazines Books
    Proceedings SIGs Conferences Collections People About About ACM Digital Library
    ACM Digital Library Board Subscription Information Author Guidelines Using ACM
    Digital Library All Holdings within the ACM Digital Library ACM Computing Classification
    System Digital Library Accessibility Join Join ACM Join SIGs Subscribe to Publications
    Institutions and Libraries Connect Contact Facebook Twitter Linkedin Feedback
    Bug Report The ACM Digital Library is published by the Association for Computing
    Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: 'WoC 2023 - Proceedings of the 9th International Workshop on Container
    Technologies and Container Clouds, Part of: Middleware 2023'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Container Sizing for Microservices with Dynamic Workload by Online Optimization
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wu Y.W.
  - Zhang Y.
  - Wang T.
  - Wang H.M.
  citation_count: '0'
  description: In recent years, software construction, operation, and evolution have
    encountered many new requirements, such as the need for efficient switching or
    configuration in development and testing environments, application isolation,
    resource consumption reduction, and higher efficiency of testing and deployment.
    These requirements pose great challenges to developers in developing and maintaining
    software. Container technology has the potential of releasing developers from
    the heavy workload of development and maintenance. Of particular note, Docker,
    as the de facto industrial standard for containers, has recently become a popular
    research area in the academic community. To help researchers understand the status
    and trends of research on Docker containers, this study conducts a systematic
    literature review by collecting 75 high-level papers in this field. First, quantitative
    methods are used to investigate the basic status of research on Docker containers,
    including research quantity, research quality, research areas, and research methods.
    Second, the first classification framework for research on Docker containers is
    presented in this study, and the current studies are systematically classified
    and reviewed from the dimensions of the core, platform, and support. Finally,
    the development trends of Docker container technology are discussed, and seven
    future research directions are summarized.
  doi: 10.13328/j.cnki.jos.006765
  full_citation: '>'
  full_text: '>

    "服务器错误 404 - 找不到文件或目录。 您要查找的资源可能已被删除，已更改名称或者暂时不可用。"'
  inline_citation: '>'
  journal: Ruan Jian Xue Bao/Journal of Software
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Development Exploration of Container Technology Through Docker Containers:
    A Systematic Literature Review Perspective'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Moreau D.
  - Wiebels K.
  - Boettiger C.
  citation_count: '3'
  description: The fast-paced development of computational tools has enabled tremendous
    scientific progress in recent years. However, this rapid surge of technological
    capability also comes at a cost, as it leads to an increase in the complexity
    of software environments and potential compatibility issues across systems. Advanced
    workflows in processing or analysis often require specific software versions and
    operating systems to run smoothly, and discrepancies across machines and researchers
    can impede reproducibility and efficient collaboration. As a result, scientific
    teams are increasingly relying on containers to implement robust, dependable research
    ecosystems. Originally popularized in software engineering, containers have become
    common in scientific projects, particularly in large collaborative efforts. In
    this Primer, we describe what containers are, how they work and the rationale
    for their use in scientific projects. We review state-of-the-art implementations
    in diverse contexts and fields, with examples in various scientific fields. Finally,
    we discuss the possibilities enabled by the widespread adoption of containerization,
    especially in the context of open and reproducible research, and propose recommendations
    to facilitate seamless implementation across platforms and domains, including
    within high-performance computing clusters such as those typically available at
    universities and research institutes.
  doi: 10.1038/s43586-023-00236-9
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Subscribe Sign
    up for alerts RSS feed nature nature reviews methods primers primers article Primer
    Published: 13 July 2023 Containers for computational reproducibility David Moreau
    , Kristina Wiebels & Carl Boettiger   Nature Reviews Methods Primers  3, Article
    number: 50 (2023) Cite this article 825 Accesses 3 Citations 5 Altmetric Metrics
    Abstract The fast-paced development of computational tools has enabled tremendous
    scientific progress in recent years. However, this rapid surge of technological
    capability also comes at a cost, as it leads to an increase in the complexity
    of software environments and potential compatibility issues across systems. Advanced
    workflows in processing or analysis often require specific software versions and
    operating systems to run smoothly, and discrepancies across machines and researchers
    can impede reproducibility and efficient collaboration. As a result, scientific
    teams are increasingly relying on containers to implement robust, dependable research
    ecosystems. Originally popularized in software engineering, containers have become
    common in scientific projects, particularly in large collaborative efforts. In
    this Primer, we describe what containers are, how they work and the rationale
    for their use in scientific projects. We review state-of-the-art implementations
    in diverse contexts and fields, with examples in various scientific fields. Finally,
    we discuss the possibilities enabled by the widespread adoption of containerization,
    especially in the context of open and reproducible research, and propose recommendations
    to facilitate seamless implementation across platforms and domains, including
    within high-performance computing clusters such as those typically available at
    universities and research institutes. This is a preview of subscription content,
    access via your institution Access options Access to this article via University
    of Nebraska-Lincoln is not available. Change institution Access Nature and 54
    other Nature Portfolio journals Get Nature+, our best-value online-access subscription
    $29.99 / 30 days cancel any time Learn more Subscribe to this journal Receive
    1 digital issues and online access to articles $99.00 per year only $99.00 per
    issue Learn more Buy this article Purchase on Springer Link Instant access to
    full article PDF Buy now Prices may be subject to local taxes which are calculated
    during checkout Additional access options: Log in Learn about institutional subscriptions
    Read our FAQs Contact customer support Similar content being viewed by others
    Packaging and containerization of computational methods Article 02 April 2024
    AiiDA 1.0, a scalable computational infrastructure for automated reproducible
    workflows and data provenance Article Open access 08 September 2020 FAIRly big:
    A framework for computationally reproducible processing of large-scale data Article
    Open access 11 March 2022 References Hsiehchen, D., Espinoza, M. & Hsieh, A. Multinational
    teams and diseconomies of scale in collaborative research. Sci. Adv. 1, e1500211
    (2015). Article   ADS   Google Scholar   International Human Genome Sequencing
    Consortium. Initial sequencing and analysis of the human genome. Nature 409, 860–921
    (2001). Article   ADS   Google Scholar   Kandoth, C. et al. Mutational landscape
    and significance across 12 major cancer types. Nature 502, 333–339 (2013). Article   ADS   Google
    Scholar   DeGrace, M. M. et al. Defining the risk of SARS-CoV-2 variants on immune
    protection. Nature 605, 640–652 (2022). Article   ADS   Google Scholar   Berrang-Ford,
    L. et al. A systematic global stocktake of evidence on human adaptation to climate
    change. Nat. Clim. Change 11, 989–1000 (2021). Article   ADS   Google Scholar   Donoho,
    D. L. An invitation to reproducible computational research. Biostatistics 11,
    385–388 (2010). Article   MATH   Google Scholar   Prabhu, P. et al. in State of
    the Practice Reports 1–12 (Association for Computing Machinery, 2011). Humphreys,
    P. in Science in the Context of Application (eds Carrier, M. & Nordmann, A.) 131–142
    (Springer Netherlands, 2011). Cioffi-Revilla, C. in Introduction to Computational
    Social Science: Principles and Applications (ed. Cioffi-Revilla, C.) 35–102 (Springer
    International Publishing, 2017). Levenstein, M. C. & Lyle, J. A. Data: sharing
    is caring. Adv. Methods Pract. Psychol. Sci. 1, 95–103 (2018). Article   Google
    Scholar   Kidwell, M. C. et al. Badges to acknowledge open practices: a simple,
    low-cost, effective method for increasing transparency. PLoS Biol. 14, e1002456
    (2016). Article   Google Scholar   Auer, S. et al. Science forum: a community-led
    initiative for training in reproducible research. eLife https://doi.org/10.7554/eLife.64719
    (2021). Article   Google Scholar   Epskamp, S. Reproducibility and replicability
    in a fast-paced methodological world. Adv. Methods Pract. Psychol. Sci. 2, 145–155
    (2019). Article   Google Scholar   Pittard, W. S. & Li, S. in Computational Methods
    and Data Analysis for Metabolomics (ed. Li, S.) 265–311 (Springer US, 2020). Baker,
    M. 1,500 Scientists lift the lid on reproducibility. Nature https://doi.org/10.1038/533452a
    (2016). Article   Google Scholar   Baker, M. Reproducibility: seek out stronger
    science. Nature 537, 703–704 (2016). Article   Google Scholar   Button, K. S.,
    Chambers, C. D., Lawrence, N. & Munafò, M. R. Grassroots training for reproducible
    science: a consortium-based approach to the empirical dissertation. Psychol. Learn.
    Teach. 19, 77–90 (2020). Article   Google Scholar   Wilson, G. et al. Good enough
    practices in scientific computing. PLoS Comput. Biol. 13, e1005510 (2017). This
    article outlines a set of good computing practices that every researcher can adopt,
    regardless of their current level of computational skill. These practices encompass
    data management, programming, collaborating with colleagues, organizing projects,
    tracking work and writing manuscripts. Article   Google Scholar   Vicente-Saez,
    R. & Martinez-Fuentes, C. Open science now: a systematic literature review for
    an integrated definition. J. Bus. Res. 88, 428–436 (2018). Article   Google Scholar   McKiernan,
    E. C. et al. How open science helps researchers succeed. eLife 5, e16800 (2016).
    Article   Google Scholar   Woelfle, M., Olliaro, P. & Todd, M. H. Open science
    is a research accelerator. Nat. Chem. 3, 745–748 (2011). Article   Google Scholar   Evans,
    J. A. & Reimer, J. Open access and global participation in science. Science 323,
    1025 (2009). Article   ADS   Google Scholar   Sandve, G. K., Nekrutenko, A., Taylor,
    J. & Hovig, E. Ten simple rules for reproducible computational research. PLoS
    Comput. Biol. 9, e1003285 (2013). Article   ADS   Google Scholar   Fan, G. et
    al. in Proceedings of the 29th ACM SIGSOFT International Symposium on Software
    Testing and Analysis 463–474 (Association for Computing Machinery, 2020). Liu,
    K. & Aida, K. in 2016 International Conference on Cloud Computing Research and
    Innovations (ICCCRI) 56–63 (IEEE, 2016). Hale, J. S., Li, L., Richardson, C. N.
    & Wells, G. N. Containers for portable, productive, and performant scientific
    computing. Comput. Sci. Eng. 19, 40–50 (2017). Article   Google Scholar   Boettiger,
    C., Center for Stock Assessment Research. An introduction to Docker for reproducible
    research. Oper. Syst. Rev. https://doi.org/10.1145/2723872.2723882 (2015). This
    article explores how Docker can help address challenges in computational reproducibility
    in scientific research, examining how Docker combines several areas from systems
    research to facilitate reproducibility, portability and extensibility of computational
    work. Article   Google Scholar   Kiar, G. et al. Science in the cloud (SIC): a
    use case in MRI connectomics. Gigascience 6, gix013 (2017). Article   ADS   Google
    Scholar   Merkel, D. Docker: lightweight Linux containers for consistent development
    and deployment. Seltzer https://www.seltzer.com/margo/teaching/CS508.19/papers/merkel14.pdf
    (2013). This article describes how Docker can package applications and their dependencies
    into lightweight containers that move easily between different distros, start
    up quickly and are isolated from each other. Kurtzer, G. M., Sochat, V. & Bauer,
    M. W. Singularity: scientific containers for mobility of compute. PLoS ONE 12,
    e0177459 (2017). Article   Google Scholar   Sochat, V. V., Prybol, C. J. & Kurtzer,
    G. M. Enhancing reproducibility in scientific computing: metrics and registry
    for Singularity containers. PLoS ONE 12, e0188511 (2017). This article presents
    Singularity Hub, a framework to build and deploy Singularity containers for mobility
    of compute. The article also introduces Singularity Python software with novel
    metrics for assessing reproducibility of such containers. Article   Google Scholar   Walsh,
    D. & Podman team. Podman: A Tool for Managing OCI Containers and Pods. Github
    https://github.com/containers/podman (2023). Potdar, A. M., Narayan, D. G., Kengond,
    S. & Mulla, M. M. Performance evaluation of Docker container and virtual machine.
    Procedia Comput. Sci. 171, 1419–1428 (2020). Article   Google Scholar   Gerhardt,
    L. et al. Shifter: containers for HPC. J. Phys. Conf. Ser. 898, 082021 (2017).
    Article   Google Scholar   Ram, K. Git can facilitate greater reproducibility
    and increased transparency in science. Source Code Biol. Med. 8, 7 (2013). Article   Google
    Scholar   Vuorre, M. & Curley, J. P. Curating research assets: a tutorial on the
    git version control system. Adv. Methods Pract. Psychol. Sci. 1, 219–236 (2018).
    Article   Google Scholar   Clyburne-Sherin, A., Fei, X. & Green, S. A. Computational
    reproducibility via containers in psychology. Meta Psychol. 3, 892 (2019). Article   Google
    Scholar   Boettiger, C. & Eddelbuettel, D. An introduction to rocker: Docker containers
    for R. R J. 9, 527 (2017). Article   Google Scholar   Nüst, D. et al. The Rockerverse:
    packages and applications for containerization with R. Preprint at https://doi.org/10.48550/arXiv.2001.10641
    (2020). Nüst, D. & Hinz, M. containerit: generating Dockerfiles for reproducible
    research with R. J. Open Source Softw. 4, 1603 (2019). Article   ADS   Google
    Scholar   Xiao, N. Liftr: Containerize R markdown documents for continuous reproducibility
    (CRAN, 2019). Peikert, A. & Brandmaier, A. M. A reproducible data analysis workflow
    with R Markdown, Git, Make, and Docker. Preprint at PsyArXiv https://doi.org/10.31234/osf.io/8xzqy
    (2019). Article   Google Scholar   Younge, A. J., Pedretti, K., Grant, R. E. &
    Brightwell, R. in 2017 IEEE International Conference on Cloud Computing Technology
    and Science (CloudCom) 74–81 (2017). Freire, J., Bonnet, P. & Shasha, D. in Proceedings
    of the 2012 ACM SIGMOD International Conference on Management of Data 593–596
    (Association for Computing Machinery, 2012). Papin, J. A., Mac Gabhann, F., Sauro,
    H. M., Nickerson, D. & Rampadarath, A. Improving reproducibility in computational
    biology research. PLoS Comput. Biol. 16, e1007881 (2020). Article   Google Scholar   Sochat,
    V. V. et al. The experiment factory: standardizing behavioral experiments. Front.
    Psychol. 7, 610 (2016). Article   Google Scholar   Khan, F. Z. et al. Sharing
    interoperable workflow provenance: a review of best practices and their practical
    application in CWLProv. Gigascience 8, giz095 (2019). Article   Google Scholar   Kane,
    S. P. & Matthias, K. Docker: Up & Running: Shipping Reliable Containers in Production
    (‘O’Reilly Media, Inc., 2018). Khan, A. Key characteristics of a container orchestration
    platform to enable a modern application. IEEE Cloud Comput. 4, 42–48 (2017). Article   Google
    Scholar   Singh, S. & Singh, N. in 2016 2nd International Conference on Applied
    and Theoretical Computing and Communication Technology (iCATccT) 804–807 (2016).
    Singh, V. & Peddoju, S. K. in 2017 International Conference on Computing, Communication
    and Automation (ICCCA) 847–852 (IEEE, 2017). Kang, H., Le, M. & Tao, S. in 2016
    IEEE International Conference on Cloud Engineering (IC2E) 202–211 (IEEE, 2016).
    Sultan, S., Ahmad, I. & Dimitriou, T. Container security: issues, challenges,
    and the road ahead. IEEE Access. 7, 52976–52996 (2019). Article   Google Scholar   Ruiz,
    C., Jeanvoine, E. & Nussbaum, L. in Euro-Par 2015: Parallel Processing Workshops
    813–824 (Springer International Publishing, 2015). Nadgowda, S., Suneja, S. &
    Kanso, A. in 2017 IEEE International Conference on Cloud Engineering (IC2E) 266–272
    (IEEE, 2017). Srirama, S. N., Adhikari, M. & Paul, S. Application deployment using
    containers with auto-scaling for microservices in cloud environment. J. Netw.
    Computer Appl. 160, 102629 (2020). Article   Google Scholar   Cito, J. et al.
    in 2017 IEEE/ACM 14th International Conference on Mining Software Repositories
    (MSR) 323–333 (IEEE, 2017). Poldrack, R. A. & Gorgolewski, K. J. Making Big Data
    open: data sharing in neuroimaging. Nat. Neurosci. 17, 1510–1517 (2014). Article   Google
    Scholar   Smith, S. M. & Nichols, T. E. Statistical challenges in ‘Big Data’ human
    neuroimaging. Neuron 97, 263–268 (2018). Article   Google Scholar   Tourbier,
    S. et al. Connectome Mapper 3: a flexible and open-source pipeline software for
    multiscale multimodal human connectome mapping. J. Open Source Softw. 7, 4248
    (2022). Article   ADS   Google Scholar   Nichols, T. E. et al. Best practices
    in data analysis and sharing in neuroimaging using MRI. Nat. Neurosci. 20, 299–303
    (2017). Article   Google Scholar   Halchenko, Y. O. & Hanke, M. Open is not enough.
    Let’s take the next step: an integrated, community-driven computing platform for
    neuroscience. Front. Neuroinform. 6, 22 (2012). Article   Google Scholar   Schalk,
    G. & Mellinger, J. A Practical Guide to Brain–Computer Interfacing with BCI2000:
    General-Purpose Software for Brain–Computer Interface Research, Data Acquisition,
    Stimulus Presentation, and Brain Monitoring (Springer Science & Business Media,
    2010). Kaur, B., Dugré, M., Hanna, A. & Glatard, T. An analysis of security vulnerabilities
    in container images for scientific data analysis. Gigascience 10, giab025 (2021).
    Article   Google Scholar   Huang, Y. et al. Realized ecological forecast through
    an interactive Ecological Platform for Assimilating Data (EcoPAD, v1.0) into models.
    Geosci. Model. Dev. 12, 1119–1137 (2019). Article   ADS   Google Scholar   White,
    E. P. et al. Developing an automated iterative near‐term forecasting system for
    an ecological study. Methods Ecol. Evol. 10, 332–344 (2019). Article   Google
    Scholar   Powers, S. M. & Hampton, S. E. Open science, reproducibility, and transparency
    in ecology. Ecol. Appl. 29, e01822 (2019). Article   Google Scholar   Ali, A.
    S., Coté, C., Heidarinejad, M. & Stephens, B. Elemental: an open-source wireless
    hardware and software platform for building energy and indoor environmental monitoring
    and control. Sensors 19, 4017 (2019). Article   ADS   Google Scholar   Morris,
    B. D. & White, E. P. The EcoData retriever: improving access to existing ecological
    data. PLoS ONE 8, e65848 (2013). Article   ADS   Google Scholar   Schulz, W. L.,
    Durant, T. J. S., Siddon, A. J. & Torres, R. Use of application containers and
    workflows for genomic data analysis. J. Pathol. Inform. 7, 53 (2016). Article   Google
    Scholar   Di Tommaso, P. et al. The impact of Docker containers on the performance
    of genomic pipelines. PeerJ 3, e1273 (2015). Article   Google Scholar   O’Connor,
    B. D. et al. The Dockstore: enabling modular, community-focused sharing of Docker-based
    genomics tools and workflows. F1000Res. 6, 52 (2017). Article   Google Scholar   Bai,
    J. et al. BioContainers registry: searching bioinformatics and proteomics tools,
    packages, and containers. J. Proteome Res. 20, 2056–2061 (2021). Article   Google
    Scholar   Gentleman, R. C. et al. Bioconductor: open software development for
    computational biology and bioinformatics. Genome Biol. 5, R80 (2004). Article   Google
    Scholar   Zhu, T., Liang, C., Meng, Z., Guo, S. & Zhang, R. GFF3sort: a novel
    tool to sort GFF3 files for tabix indexing. BMC Bioinformatics 18, 482 (2017).
    Article   Google Scholar   Müller Paul, H., Istanto, D. D., Heldenbrand, J. &
    Hudson, M. E. CROPSR: an automated platform for complex genome-wide CRISPR gRNA
    design and validation. BMC Bioinformatics 23, 74 (2022). Article   Google Scholar   Torre,
    D., Lachmann, A. & Ma’ayan, A. BioJupies: automated generation of interactive
    notebooks for RNA-Seq data analysis in the cloud. Cell Syst. 7, 556–561.e3 (2018).
    Article   Google Scholar   Mahi, N. A., Najafabadi, M. F., Pilarczyk, M., Kouril,
    M. & Medvedovic, M. GREIN: an interactive web platform for re-analyzing GEO RNA-seq
    data. Sci. Rep. 9, 7580 (2019). Article   ADS   Google Scholar   Dobin, A. & Gingeras,
    T. R. Mapping RNA-seq reads with STAR. Curr. Protoc. Bioinform. 51, 11.14.1–11.14.19
    (2015). Article   Google Scholar   Dobin, A. et al. STAR: ultrafast universal
    RNA-seq aligner. Bioinformatics 29, 15–21 (2013). Article   Google Scholar   Patro,
    R., Duggal, G., Love, M. I., Irizarry, R. A. & Kingsford, C. Salmon provides fast
    and bias-aware quantification of transcript expression. Nat. Methods 14, 417–419
    (2017). Article   Google Scholar   Yan, F., Powell, D. R., Curtis, D. J. & Wong,
    N. C. From reads to insight: a Hitchhiker’s guide to ATAC-seq data analysis. Genome
    Biol. 21, 22 (2020). Article   Google Scholar   Garcia, M. et al. Sarek: a portable
    workflow for whole-genome sequencing analysis of germline and somatic variants.
    Preprint at bioRxiv https://doi.org/10.1101/316976 (2018). Article   Google Scholar   Sirén,
    J. et al. Pangenomics enables genotyping of known structural variants in 5202
    diverse genomes. Science 374, abg8871 (2021). Article   Google Scholar   Zarate,
    S. et al. Parliament2: accurate structural variant calling at scale. Gigascience
    9, giaa145 (2020). Article   Google Scholar   Morris, D., Voutsinas, S., Hambly,
    N. C. & Mann, R. G. Use of Docker for deployment and testing of astronomy software.
    Astron. Comput. 20, 105–119 (2017). Article   ADS   Google Scholar   Taghizadeh-Popp,
    M. et al. SciServer: a science platform for astronomy and beyond. Astron. Comput.
    33, 100412 (2020). Article   ADS   Google Scholar   Herwig, F. et al. Cyberhubs:
    virtual research environments for astronomy. Astrophys. J. Suppl. Ser. 236, 2
    (2018). Article   ADS   Google Scholar   The Astropy Collaboration. et al. The
    Astropy Project: building an open-science project and status of the v2.0 Core
    Package*. Astron. J. 156, 123 (2018). Article   ADS   Google Scholar   Robitaille,
    T. P. et al. Astropy: a community Python package for astronomy. Astron. Astrophys.
    Suppl. Ser. 558, A33 (2013). Article   Google Scholar   Abolfathi, B. et al. The
    fourteenth data release of the sloan digital sky survey: first spectroscopic data
    from the extended Baryon Oscillation Spectroscopic Survey and from the Second
    Phase of the Apache Point Observatory Galactic Evolution Experiment. Astrophys.
    J. Suppl. Ser. 235, 42 (2018). Article   ADS   Google Scholar   Nigro, C. et al.
    Towards open and reproducible multi-instrument analysis in gamma-ray astronomy.
    Astron. Astrophys. Suppl. Ser. 625, A10 (2019). Article   Google Scholar   Liu,
    Q., Zheng, W., Zhang, M., Wang, Y. & Yu, K. Docker-based automatic deployment
    for nuclear fusion experimental data archive cluster. IEEE Trans. Plasma Sci.
    IEEE Nucl. Plasma Sci. Soc. 46, 1281–1284 (2018). Article   ADS   Google Scholar   Meng,
    H. et al. An invariant framework for conducting reproducible computational science.
    J. Comput. Sci. 9, 137–142 (2015). Article   Google Scholar   Agostinelli, S.
    et al. Geant4 — a simulation toolkit. Nucl. Instrum. Methods Phys. Res. A 506,
    250–303 (2003). Article   ADS   Google Scholar   Vallisneri, M., Kanner, J., Williams,
    R., Weinstein, A. & Stephens, B. The LIGO open science center. J. Phys. Conf.
    Ser. 610, 012021 (2015). Article   Google Scholar   Scott, D. & Becken, S. Adapting
    to climate change and climate policy: progress, problems and potentials. J. Sustain.
    Tour. 18, 283–295 (2010). Article   Google Scholar   Ebenhard, T. Conservation
    breeding as a tool for saving animal species from extinction. Trends Ecol. Evol.
    10, 438–443 (1995). Article   Google Scholar   Warlenius, R., Pierce, G. & Ramasar,
    V. Reversing the arrow of arrears: the concept of ‘ecological debt’ and its value
    for environmental justice. Glob. Environ. Change 30, 21–30 (2015). Article   Google
    Scholar   Acker, J. G. & Leptoukh, G. Online analysis enhances use of NASA Earth
    science data. Eos Trans. Am. Geophys. Union 88, 14–17 (2007). Article   ADS   Google
    Scholar   Yang, C. et al. Big earth data analytics: a survey. Big Earth Data 3,
    83–107 (2019). Article   Google Scholar   Wiebels, K. & Moreau, D. Leveraging
    containers for reproducible psychological research. Adv. Methods Pract. Psychol.
    Sci. 4, 25152459211017853 (2021). This article describes the logic behind containers
    and the practical problems they can solve. The tutorial section walks the reader
    through the implementation of containerization within a research workflow, with
    examples using Docker and R. The article provides a worked example that includes
    all steps required to set up a container for a research project, which can be
    easily adapted and extended. Google Scholar   Nüst, D. et al. Ten simple rules
    for writing Dockerfiles for reproducible data science. PLoS Comput. Biol. 16,
    e1008316 (2020). This article presents a set of rules to help researchers write
    understandable Dockerfiles for typical data science workflows. By following these
    rules, researchers can create containers suitable for sharing with fellow scientists,
    for including in scholarly communication and for effective and sustainable personal
    workflows. Article   Google Scholar   Elmenreich, W., Moll, P., Theuermann, S.
    & Lux, M. Making simulation results reproducible — survey, guidelines, and examples
    based on Gradle and Docker. PeerJ Comput. Sci. 5, e240 (2019). Article   Google
    Scholar   Van Moffaert, K. & Nowé, A. Multi-objective reinforcement learning using
    sets of pareto dominating policies. J. Mach. Learn. Res. 15, 3663–3692 (2014).
    MATH   MathSciNet   Google Scholar   Gama, J., Sebastião, R. & Rodrigues, P. P.
    On evaluating stream learning algorithms. Mach. Learn. 90, 317–346 (2013). Article   MATH   MathSciNet   Google
    Scholar   Kim, A. Y. et al. Implementing GitHub Actions continuous integration
    to reduce error rates in ecological data collection. Methods Ecol. Evol. 13, 2572–2585
    (2022). Article   Google Scholar   Wilson, G. et al. Best practices for scientific
    computing. PLoS Biol. 12, e1001745 (2014). Article   Google Scholar   Eglen, S.
    J. et al. Toward standard practices for sharing computer code and programs in
    neuroscience. Nat. Neurosci. 20, 770–773 (2017). Article   Google Scholar   No
    authors listed. Rebooting review. Nat. Biotechnol. 33, 319 (2015). Article   Google
    Scholar   Kenall, A. et al. Better reporting for better research: a checklist
    for reproducibility. BMC Neurosci. 16, 44 (2015). Article   Google Scholar   Poldrack,
    R. A. The costs of reproducibility. Neuron 101, 11–14 (2019). Article   Google
    Scholar   Nagarajan, P., Warnell, G. & Stone, P. Deterministic implementations
    for reproducibility in deep reinforcement learning. Preprint at arXiv https://doi.org/10.48550/arXiv.1809.05676
    (2018). Article   Google Scholar   Piccolo, S. R., Ence, Z. E., Anderson, E. C.,
    Chang, J. T. & Bild, A. H. Simplifying the development of portable, scalable,
    and reproducible workflows. eLife 10, e71069 (2021). Article   Google Scholar   Higgins,
    J., Holmes, V. & Venters, C. in High Performance Computing 506–513 (Springer International
    Publishing, 2015). de Bayser, M. & Cerqueira, R. in 2017 IEEE International Conference
    on Cloud Engineering (IC2E) 259–265 (IEEE, 2017). Netto, M. A. S., Calheiros,
    R. N., Rodrigues, E. R., Cunha, R. L. F. & Buyya, R. HPC cloud for scientific
    and business applications: taxonomy, vision, and research challenges. ACM Comput.
    Surv. 51, 1–29 (2018). Article   Google Scholar   Azab, A. in 2017 IEEE International
    Conference on Cloud Engineering (IC2E) 279–285 (IEEE, 2017). Qasha, R., Cała,
    J. & Watson, P. in 2016 IEEE 12th International Conference on e-Science (e-Science)
    81–90 (IEEE, 2016). Saha, P., Beltre, A., Uminski, P. & Govindaraju, M. in Proceedings
    of the Practice and Experience on Advanced Research Computing 1–8 (Association
    for Computing Machinery, 2018). Abdelbaky, M., Diaz-Montes, J., Parashar, M.,
    Unuvar, M. & Steinder, M. in 2015 IEEE/ACM 8th International Conference on Utility
    and Cloud Computing (UCC) 368–371 (IEEE, 2015). Hung, L.-H., Kristiyanto, D.,
    Lee, S. B. & Yeung, K. Y. GUIdock: using Docker containers with a common graphics
    user interface to address the reproducibility of research. PLoS ONE 11, e0152686
    (2016). Article   Google Scholar   Salza, P. & Ferrucci, F. Speed up genetic algorithms
    in the cloud using software containers. Future Gener. Comput. Syst. 92, 276–289
    (2019). Article   Google Scholar   Pahl, C., Brogi, A., Soldani, J. & Jamshidi,
    P. Cloud container technologies: a state-of-the-art review. IEEE Trans. Cloud
    Comput. 7, 677–692 (2019). Article   Google Scholar   Dessalk, Y. D., Nikolov,
    N., Matskin, M., Soylu, A. & Roman, D. in Proceedings of the 12th International
    Conference on Management of Digital EcoSystems 76–83 (Association for Computing
    Machinery, 2020). Martín-Santana, S., Pérez-González, C. J., Colebrook, M., Roda-García,
    J. L. & González-Yanes, P. in Data Science and Digital Business (eds García Márquez,
    F. P. & Lev, B.) 121–146 (Springer International Publishing, 2019). Jansen, C.,
    Witt, M. & Krefting, D. in Computational Science and Its Applications — ICCSA
    2016 303–318 (Springer International Publishing, 2016). Brinckman, A. et al. Computing
    environments for reproducibility: capturing the ‘Whole Tale’. Future Gener. Comput.
    Syst. 94, 854–867 (2019). Article   Google Scholar   Perkel, J. M. Make code accessible
    with these cloud services. Nature 575, 247–248 (2019). Article   ADS   Google
    Scholar   Poldrack, R. A., Gorgolewski, K. J. & Varoquaux, G. Computational and
    informatic advances for reproducible data analysis in neuroimaging. Annu. Rev.
    Biomed. Data Sci. 2, 119–138 (2019). Article   Google Scholar   Vaillancourt,
    P. Z., Coulter, J. E., Knepper, R. & Barker, B. in 2020 IEEE High Performance
    Extreme Computing Conference (HPEC) 1–8 (IEEE, 2020). Adufu, T., Choi, J. & Kim,
    Y. in 17th Asia-Pacific Network Operations and Management Symposium (APNOMS) 507–510
    (IEEE, 2015). Cito, J., Ferme, V. & Gall, H. C. in Web Engineering 609–612 (Springer
    International Publishing, 2016). Tedersoo, L. et al. Data sharing practices and
    data availability upon request differ across scientific disciplines. Sci. Data
    8, 192 (2021). Article   Google Scholar   Tenopir, C. et al. Data sharing by scientists:
    practices and perceptions. PLoS ONE 6, e21101 (2011). Article   ADS   Google Scholar   Gomes,
    D. G. E. et al. Why don’t we share data and code? Perceived barriers and benefits
    to public archiving practices. Proc. Biol. Sci. 289, 20221113 (2022). Google Scholar   Weston,
    S. J., Ritchie, S. J., Rohrer, J. M. & Przybylski, A. K. Recommendations for increasing
    the transparency of analysis of preexisting data sets. Adv. Methods Pract. Psychol.
    Sci. 2, 214–227 (2019). Article   Google Scholar   Download references Acknowledgements
    D.M. and K.W. are supported by a Marsden grant from the Royal Society of New Zealand
    and a University of Auckland Early Career Research Excellence Award awarded to
    D.M. Author information Authors and Affiliations School of Psychology and Centre
    for Brain Research, University of Auckland, Auckland, New Zealand David Moreau
    & Kristina Wiebels Department of Environmental Science, Policy and Management,
    University of California Berkeley, Berkeley, CA, USA Carl Boettiger Contributions
    Introduction (D.M., K.W. and C.B.); Experimentation (D.M., K.W. and C.B.); Results
    (D.M., K.W. and C.B.); Applications (D.M., K.W. and C.B.); Reproducibility and
    data deposition (D.M., K.W. and C.B.); Limitations and optimizations (D.M., K.W.
    and C.B.); Outlook (D.M., K.W. and C.B.). Corresponding author Correspondence
    to David Moreau. Ethics declarations Competing interests The authors declare no
    competing interests. Peer review Peer review information Nature Reviews Methods
    Primers thanks Beth Ciimini, Stephen Piccolo and the other, anonymous, reviewer(s)
    for their contribution to the peer review of this work. Additional information
    Publisher’s note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Related links ACM Digital
    Library: https://dl.acm.org Amazon Web Services: https://aws.amazon.com Ansible:
    https://ansible.com Astropy: https://astropy.org ATAC-seq Pipeline: https://github.com/ENCODE-DCC/atac-seq-pipeline
    BCI2000 project: https://bci2000.org/ Binder: https://mybinder.org/ Bioconductor:
    https://bioconductor.org BioContainers: https://biocontainers.pro Bismark: https://www.bioinformatics.babraham.ac.uk/projects/bismark/
    Breakdancer: https://github.com/genome/breakdancer CERN Container Registry: https://hub.docker.com/u/cern
    Chef: https://chef.io CodeOcean: http://codeocean.com Containerd: https://containerd.io/
    Docker Hub: https://hub.docker.com EarthData: https://earthdata.nasa.gov EcoData
    Retriever: https://ecodataretriever.org Ecological Niche Modelling on Docker:
    https://github.com/ghuertaramos/ENMOD Ecopath: https://ecopath.org/ EIGENSOFT:
    https://hsph.harvard.edu/alkes-price/software/eigensoft Environmental Data Commons:
    https://edc.occ-data.org Experiment Factory: https://expfactory.github.io F1000Research
    guidelines: https://f1000research.com/for-authors/article-guidelines/software-tool-articles
    fmriprep: https://fmriprep.org FSL project: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki
    GATK: https://gatk.broadinstitute.org gdb: https://github.com/haggaie/docker-gdb
    GEANT4: https://geant4.web.cern.ch GeoServer: https://geoserver.org GitHub Actions:
    https://github.com/features/actions GitHub Container Registry: https://github.com/features/packages
    Google Cloud Platform: https://cloud.google.com GRASS GIS: https://grass.osgeo.org
    Jenkins: https://jenkins.io liftr: https://liftr.me/ LIGO Open Science Centre:
    https://losc.ligo.org LXC: https://linuxcontainers.org Marble Station: https://github.com/marblestation/docker-astro
    Mesos: https://mesos.apache.org NEST: https://nest-simulator.org NeuroDebian:
    https://neuro.debian.net NEURON: https://neuron.yale.edu/neuron OpenShift: https://openshift.com/
    Planet Research Data Commons: https://ardc.edu.au/program/planet-research-data-commons
    Podman: https://podman.io/ Puppet: https://puppet.com QGIS: https://qgis.org Quay:
    https://quay.io Rocker project: https://rocker-project.org/ Rocket: https://github.com/rkt/rkt
    Salmon: https://combine-lab.github.io/salmon SciServer: https://sciserver.org
    Singularity: https://sylabs.io/ STAR: https://github.com/alexdobin/STAR strace:
    https://github.com/amrabed/strace-docker SVTyper: https://github.com/hall-lab/svtyper
    Supplementary information Supplementary Information Glossary Clusters Groups of
    machines that work together to run containerized applications. Compute resources
    The resources required by a container to run, including central processing units,
    memory and storage. Containerization platform A complete system for building,
    deploying and managing containerized applications, typically including a container
    runtime, and additional tools and services for things such as container orchestration,
    networking, storage and security. Container runtime The software responsible for
    running and managing containers on a host machine, involving tasks such as starting
    and stopping containers, allocating resources to them and providing an isolated
    environment for them to run in. Continuous Integration/Continuous Deployment (CI/CD).
    A software development practice that involves continuously integrating code changes
    into a shared repository and continuously deploying changes to a production environment.
    Dependencies Software components that a particular application relies on to run
    properly, including libraries, tools and frameworks. Distributed-control model
    A deployment model in which control is distributed among multiple independent
    nodes, rather than being centralized in a single control node. Docker engine The
    containerization technology that Docker uses, consisting of the Docker daemon
    running on the computer and the Docker client that communicates with the daemon
    to execute commands. Dockerfiles A script that contains instructions for building
    a Docker image. Environment variables A variable that is passed to a container
    at runtime, allowing the container to configure itself on the basis of the value
    of the variable. High-performance computing The use of supercomputers and parallel
    processing techniques to solve complex computational problems that require a large
    amount of processing power, memory and storage capacity. Host operating system
    Primary operating system running on the physical computer or server in which virtual
    machines or containers are created and managed. Image A preconfigured package
    that contains all the necessary files and dependencies for running a piece of
    software in a container. Namespaces Virtualization mechanisms for containers,
    which allow multiple containers to share the same system resources without interfering
    with each other. Networking The process of connecting multiple containers together
    and to external networks, allowing communication between containers and the outside
    world. Orchestration The process of automating the deployment, scaling and management
    of containerized applications in a cluster. Orchestration platform System for
    automating the deployment, scaling and management of containerized applications.
    Port mapping The process of exposing the network ports of a container to the host
    machine, allowing communication between the container and the host or other networked
    systems. Production environment Live, operational system in which software applications
    are deployed and used by end-users. Runtime environment Specific set of software
    and hardware configurations that are present and available for an application
    to run on, including the operating system, libraries, system tools and other dependencies.
    Scaling The process of increasing or decreasing the number of running instances
    of a containerized application to meet changing demand. Shared-control model Deployment
    model in which a single central entity has control over multiple resources or
    nodes. Volumes A storage mechanism for containers, which allows data to persist
    outside the file system of the container, including after a container has been
    deleted or replaced. Rights and permissions Springer Nature or its licensor (e.g.
    a society or other partner) holds exclusive rights to this article under a publishing
    agreement with the author(s) or other rightsholder(s); author self-archiving of
    the accepted manuscript version of this article is solely governed by the terms
    of such publishing agreement and applicable law. Reprints and permissions About
    this article Cite this article Moreau, D., Wiebels, K. & Boettiger, C. Containers
    for computational reproducibility. Nat Rev Methods Primers 3, 50 (2023). https://doi.org/10.1038/s43586-023-00236-9
    Download citation Accepted 23 May 2023 Published 13 July 2023 DOI https://doi.org/10.1038/s43586-023-00236-9
    Subjects Research management Software Access to this article via University of
    Nebraska-Lincoln is not available. Change institution Buy or subscribe Associated
    content Containers for computational reproducibility Nature Reviews Methods Primers
    PrimeView 13 Jul 2023 Sections Figures References Abstract References Acknowledgements
    Author information Ethics declarations Peer review Additional information Supplementary
    information Glossary Rights and permissions About this article Advertisement Nature
    Reviews Methods Primers (Nat Rev Methods Primers) ISSN 2662-8449 (online) About
    Nature Portfolio About us Press releases Press office Contact us Discover content
    Journals A-Z Articles by subject Protocol Exchange Nature Index Publishing policies
    Nature portfolio policies Open access Author & Researcher services Reprints &
    permissions Research data Language editing Scientific editing Nature Masterclasses
    Research Solutions Libraries & institutions Librarian service & tools Librarian
    portal Open research Recommend to library Advertising & partnerships Advertising
    Partnerships & Services Media kits Branded content Professional development Nature
    Careers Nature Conferences Regional websites Nature Africa Nature China Nature
    India Nature Italy Nature Japan Nature Korea Nature Middle East Privacy Policy
    Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility
    statement Terms & Conditions Your US state privacy rights © 2024 Springer Nature
    Limited"'
  inline_citation: '>'
  journal: Nature Reviews Methods Primers
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Containers for computational reproducibility
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Yang N.
  - Shen W.
  - Li J.
  - Liu X.
  - Guo X.
  - Ma J.
  citation_count: '0'
  description: As the dominant container orchestration system, Kubernetes is widely
    used by many companies and cloud vendors. It runs third-party add-ons and applications
    (termed third-party apps) on its control plane to manage the whole cluster. The
    security of these third-party apps is critical to the whole cluster but has not
    been systematically studied so far. Therefore, this paper analyzes the security
    of third-party apps and reveals that third-party apps are granted excessive critical
    permissions, which can be exploited by an attacker to escape from the worker node
    and take over the whole Kubernetes cluster. Even worse, excessive permissions
    of different third-party apps can be chained together to turn non-critical issues
    into severe attack vectors. To systematically analyze the exploitability of excessive
    permissions, we design three strategies based on different attacking paths. These
    three strategies can steal the cluster admin permission with the DaemonSet of
    a third-party app directly, or via the same app's or another app's critical component
    indirectly. We investigate the security impact of excessive permission attacks
    in real production environments. We analyze all third-party apps in CNCF and show
    that 51 of 153 (33.3%) ones have potential security risks. We further scan Kubernetes
    services provided by the top four cloud vendors. The results show that all of
    them are vulnerable to excessive permission attacks. We report all our findings
    to the corresponding teams and get eight new CVEs from communities and a security
    bounty from Google.
  doi: 10.1145/3576915.3623121
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesCCSProceedingsCCS ''23Take
    Over the Whole Cluster: Attacking Kubernetes via Excessive Permissions of Third-party
    Applications RESEARCH-ARTICLE SHARE ON Take Over the Whole Cluster: Attacking
    Kubernetes via Excessive Permissions of Third-party Applications Authors: Nanzi
    Yang , Wenbo Shen , Jinku Li , Xunqi Liu , + 2 Authors Info & Claims CCS ''23:
    Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications SecurityNovember
    2023Pages 3048–3062https://doi.org/10.1145/3576915.3623121 Published:21 November
    2023Publication History 0 citation 569 Downloads eReaderPDF CCS ''23: Proceedings
    of the 2023 ACM SIGSAC Conference on Computer and Communications Security Take
    Over the Whole Cluster: Attacking Kubernetes via Excessive Permissions of Third-party
    Applications Pages 3048–3062 Previous Next ABSTRACT References Index Terms Recommendations
    Comments ABSTRACT As the dominant container orchestration system, Kubernetes is
    widely used by many companies and cloud vendors. It runs third-party add-ons and
    applications (termed third-party apps) on its control plane to manage the whole
    cluster. The security of these third-party apps is critical to the whole cluster
    but has not been systematically studied so far. Therefore, this paper analyzes
    the security of third-party apps and reveals that third-party apps are granted
    excessive critical permissions, which can be exploited by an attacker to escape
    from the worker node and take over the whole Kubernetes cluster. Even worse, excessive
    permissions of different third-party apps can be chained together to turn non-critical
    issues into severe attack vectors. To systematically analyze the exploitability
    of excessive permissions, we design three strategies based on different attacking
    paths. These three strategies can steal the cluster admin permission with the
    DaemonSet of a third-party app directly, or via the same app''s or another app''s
    critical component indirectly. We investigate the security impact of excessive
    permission attacks in real production environments. We analyze all third-party
    apps in CNCF and show that 51 of 153 (33.3%) ones have potential security risks.
    We further scan Kubernetes services provided by the top four cloud vendors. The
    results show that all of them are vulnerable to excessive permission attacks.
    We report all our findings to the corresponding teams and get eight new CVEs from
    communities and a security bounty from Google. References Yousra Aafer, Nan Zhang,
    Zhongwen Zhang, Xiao Zhang, Kai Chen, XiaoFeng Wang, Xiaoyong Zhou, Wenliang Du,
    and Michael Grace. 2015. Hare hunting in the wild android: A study on the threat
    of hanging attribute references. In Proceedings of the 22nd ACM SIGSAC Conference
    on Computer and Communications Security. 1248--1259. Yousra Aafer, Xiao Zhang,
    and Wenliang Du. 2016. Harvesting Inconsistent Security Configurations in Custom
    Android {ROMs} via Differential Analysis. In 25th USENIX Security Symposium (USENIX
    Security 16). 1153--1168. Abdulla Aldoseri, David Oswald, and Robert Chiper. 2022.
    A Tale of Four Gates: Privilege Escalation and Permission Bypasses on Android
    Through App Components. In European Symposium on Research in Computer Security.
    Springer, 233--251. Show All References Index Terms Take Over the Whole Cluster:
    Attacking Kubernetes via Excessive Permissions of Third-party Applications Security
    and privacy Systems security Distributed systems security Recommendations Detecting
    repackaged smartphone applications in third-party android marketplaces CODASPY
    ''12: Proceedings of the second ACM conference on Data and Application Security
    and Privacy Recent years have witnessed incredible popularity and adoption of
    smartphones and mobile devices, which is accompanied by large amount and wide
    variety of feature-rich smartphone applications. These smartphone applications
    (or apps), typically organized ... Read More Launching generic attacks on iOS
    with approved third-party applications ACNS''13: Proceedings of the 11th international
    conference on Applied Cryptography and Network Security iOS is Apple''s mobile
    operating system, which is used on iPhone, iPad and iPod touch. Any third-party
    applications developed for iOS devices are required to go through Apple''s application
    vetting process and appear on the official iTunes App Store upon ... Read More
    Android permissions demystified CCS ''11: Proceedings of the 18th ACM conference
    on Computer and communications security Android provides third-party applications
    with an extensive API that includes access to phone hardware, settings, and user
    data. Access to privacy- and security-relevant parts of the API is controlled
    with an install-time application permission system. ... Read More Comments 78
    References View Table Of Contents Footer Categories Journals Magazines Books Proceedings
    SIGs Conferences Collections People About About ACM Digital Library ACM Digital
    Library Board Subscription Information Author Guidelines Using ACM Digital Library
    All Holdings within the ACM Digital Library ACM Computing Classification System
    Digital Library Accessibility Join Join ACM Join SIGs Subscribe to Publications
    Institutions and Libraries Connect Contact Facebook Twitter Linkedin Feedback
    Bug Report The ACM Digital Library is published by the Association for Computing
    Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and
    Communications Security
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Take Over the Whole Cluster: Attacking Kubernetes via Excessive Permissions
    of Third-party Applications'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Shi T.
  - Ma H.
  - Chen G.
  - Hartmann S.
  citation_count: '0'
  description: As a lightweight and flexible infrastructure solution, containers have
    increasingly been used for application deployment on a global scale. By rapidly
    scaling containers at different locations, the deployed applications can handle
    dynamic workloads from the worldwide user community. Existing studies usually
    focus on the (dynamic) container scaling within a single data center or the (static)
    container deployment across geo-distributed data centers. This article studies
    an increasingly important container scaling problem for application deployment
    in geo-distributed clouds. Reinforcement learning (RL) has been widely used in
    container scaling due to its high adaptability and robustness. To handle high-dimensional
    state spaces in geo-distributed clouds, we propose a deep RL algorithm, named
    DeepScale, to auto-scale containerized applications. DeepScale innovatively utilizes
    multi-step predicted future workloads to train a holistic scaling policy. It features
    several newly designed algorithmic components, including a domain-tailored state
    constructor and a heuristic-based action executor. These new algorithmic components
    are essential to meet the requirements of low deployment costs and achieve desirable
    application performance. We conduct extensive simulation studies using real-world
    datasets. The results show that DeepScale can significantly outperform an industry-leading
    scaling strategy and two state-of-the-art baselines in terms of both cost-effectiveness
    and constraint satisfaction.
  doi: 10.1109/TSC.2023.3317262
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Services...
    >Volume: 16 Issue: 6 Auto-Scaling Containerized Applications in Geo-Distributed
    Clouds Publisher: IEEE Cite This PDF Tao Shi; Hui Ma; Gang Chen; Sven Hartmann
    All Authors 225 Full Text Views Abstract Document Sections I. Introduction II.
    Related Work III. Problem Description IV. DeepScale for LACS V. Performance Evaluation
    Show Full Outline Authors Figures References Keywords Metrics Footnotes Abstract:
    As a lightweight and flexible infrastructure solution, containers have increasingly
    been used for application deployment on a global scale. By rapidly scaling containers
    at different locations, the deployed applications can handle dynamic workloads
    from the worldwide user community. Existing studies usually focus on the (dynamic)
    container scaling within a single data center or the (static) container deployment
    across geo-distributed data centers. This article studies an increasingly important
    container scaling problem for application deployment in geo-distributed clouds.
    Reinforcement learning (RL) has been widely used in container scaling due to its
    high adaptability and robustness. To handle high-dimensional state spaces in geo-distributed
    clouds, we propose a deep RL algorithm, named DeepScale , to auto-scale containerized
    applications. DeepScale innovatively utilizes multi-step predicted future workloads
    to train a holistic scaling policy. It features several newly designed algorithmic
    components, including a domain-tailored state constructor and a heuristic-based
    action executor. These new algorithmic components are essential to meet the requirements
    of low deployment costs and achieve desirable application performance. We conduct
    extensive simulation studies using real-world datasets. The results show that
    DeepScale can significantly outperform an industry-leading scaling strategy and
    two state-of-the-art baselines in terms of both cost-effectiveness and constraint
    satisfaction. Published in: IEEE Transactions on Services Computing ( Volume:
    16, Issue: 6, Nov.-Dec. 2023) Page(s): 4261 - 4274 Date of Publication: 19 September
    2023 ISSN Information: DOI: 10.1109/TSC.2023.3317262 Publisher: IEEE Funding Agency:
    SECTION I. Introduction Enterprise application deployment demands for elastically
    acquiring and releasing resources to handle dynamic workloads from the worldwide
    user community [1]. Currently, the elastic deployment and real-time management
    of applications increasingly rely on containers, an industry-leading lightweight
    virtualization technology [2], [3]. By bundling together an application with all
    its dependencies (e.g., libraries and code), the containerized application can
    realize fast deployment and migration in clouds [4]. Containers lay the technical
    foundation for elastic application deployment through horizontal scaling and vertical
    scaling [5]. Particularly, the horizontal scaling changes the number of containers
    for deployed application instances, i.e., containerized applications.1 The vertical
    scaling changes the container configuration (i.e., the amount of provisioned resources)
    for application instances. Since the horizontal scaling can be performed on containers
    in different cloud data centers, it has the advantage to address the workload
    changes across multiple geographical locations [6]. The vertical scaling can be
    realized with practically no downtime [7]. Therefore, it is favored for the localized
    workload variations [6]. Many recent efforts have been made in the literature
    for container scaling within a single data center [5], [8], [9], [10], [11]. To
    guarantee the application performance in terms of the average response time of
    global user requests [12], [13], [14], container scaling must work effectively
    across geo-distributed data centers. Besides, the pricing of containers in public
    clouds is based on the assigned resources, e.g., the number of vCPUs [15], [16].
    Note that the prices of resources assigned to containers in different regions
    can vary substantially, as clearly evidenced in Table I for Amazon Web Services
    (AWS) across different regions. For example, the price of containers in Sao Paulo
    is $ 0.0696 per hour, while the prices in N.Virginia, Oregon, and Dublin are $
    0.04048 per hour. That is, the most expensive region can fetch up to 1.7 times
    the price of the cheapest regions. The significant impact of containers’ locations
    on both the performance and cost of deployed applications must be considered when
    performing container scaling. TABLE I Unit Cost (vCPU per Hour) of Containers
    at AWS Across Different Regions In the articles [17], [18], the problem of application
    deployment in geographically distributed data centers was studied in a static
    scenario to find the most cost-effective deployment while satisfying the performance
    requirement on the average response time. However, the location-aware application
    deployment problem does not consider the impact of current deployment decisions
    on the future workload variations, which is important when we minimize the cumulative
    cost for application deployment over a time span such as a billing day in practice.
    In this paper, we study the dynamic problem of location-aware container scaling
    (LACS) in geo-distributed clouds from the perspective of application providers,
    to minimize the cumulative application deployment cost while satisfying the constraint
    on the average application response time. In practice, enterprise application
    providers usually apply threshold-based rules to automate container scaling [19].
    This strategy is efficient to make scaling decisions in real-time. However, manually
    choosing appropriate thresholds is difficult and often results in containers either
    over-utilized that slow down the request processing speed or under-utilized that
    waste money [20]. Reinforcement learning (RL) allows to express what an application
    provider aims to obtain, instead of how it should be obtained [5]. The nature
    of RL makes it very appealing to auto-scale containers for applications with dynamically
    changing and widely distributed workloads. For example, a deep Q-network (DQN)
    can be trained to approach an optimal policy for scaling containers in geo-distributed
    clouds to minimize the cumulative cost in the long run [21], [22], [23]. However,
    it is challenging to achieve both cost-effectiveness and constraint satisfaction
    by directly using DQN [24]. Therefore, new learning techniques must be developed
    to effectively solve the LACS problem. Predictive strategies such as time series
    analysis have been used to improve the timeliness of container scaling within
    a single data center, which is important to achieve cost-effective application
    deployment [20]. In this paper, we seek to utilize prediction models to provide
    essential information on dynamic application workloads for DQN to make well-informed
    scaling decisions in geo-distributed clouds. Particularly, multi-step predicted
    future workloads are collected by a domain-tailored state constructor to ensure
    high cost-effectiveness. The performance constraint of the LACS problem motivates
    us to adopt safe RL, which aims to learn a policy that maximizes the expected
    return while ensuring (or encouraging) the satisfaction of some safety constraints
    [25]. In this paper, we propose two safe RL approaches to increase the chance
    of satisfying the constraint on the average response time. On the one hand, we
    introduce a penalty-based reward function and a heuristic-based action executor
    to train policies towards the constraint-compliant scaling. On the other hand,
    the heuristics designed in the action executor can prevent scaling actions from
    prolonging the average response time beyond the acceptable level during the learning
    process, which has been verified in our experiments (Section V-F). In a nutshell,
    all the new algorithmic components jointly form a holistic scaling policy for
    the LACS problem. In particular, we show experimentally in Section V that the
    proposed domain-tailored state constructor and the heuristic-based action executor,
    are essential to meet the requirements of low deployment costs and achieve desirable
    application performance. The main contributions of this paper are summarized as
    follows. First, we identify and formulate the LACS problem to minimize the total
    deployment cost subject to the constraint on the average response time across
    widely distributed user communities. To the best of our knowledge, this is the
    first study in the literature on dynamic container scaling for application deployment
    with a realistic consideration of the location impact on both the performance
    and cost on the global scale. Second, we propose a novel deep reinforcement learning
    (DRL)-based algorithm, namely DeepScale, to solve the LACS problem. DeepScale
    innovatively trains the holistic scaling policy. It features several newly designed
    algorithmic components, including a domain-tailored state constructor and a heuristic-based
    action executor. Our proposed multi-step predicted workloads, penalty-based reward
    function, and safety-aware heuristics are utilized by DeepScale to minimize the
    cost and satisfy the performance constraint of the LACS problem. Finally, we evaluate
    the effectiveness of DeepScale through extensive simulation studies using realistic
    container pricing offered by AWS and workloads for cloud applications, i.e., WikiBench
    [26] and NASA [27]. The results show that DeepScale can consistently satisfy the
    constraint on the average response time and achieve up to 39% savings in terms
    of the deployment cost, compared to the Amazon auto-scaling service [19] and two
    state-of-the-art baselines, i.e., A-SARSA [10] and DQLCM [28]. The remainder of
    this paper is organized as follows. Section II discusses the related work about
    containerized application deployment in clouds and container scaling with RL.
    Section III defines the LACS problem, including the system architecture for containerized
    application deployment in geo-distributed clouds. Section IV presents the details
    of the DeepScale algorithm. Section V describes the design of simulation studies
    and analyses the evaluation results. Section VI discusses the scope and limitations
    of this research. Section VII concludes the paper. SECTION II. Related Work This
    section introduces the related works about containerized application deployment
    in clouds and container scaling with RL. The main challenges that need to be addressed
    in this paper are also highlighted. A. Containerized Application Deployment in
    Clouds Currently, the deployment and real-time management of applications increasingly
    rely on containers, an industry-leading lightweight virtualization technology
    [4], [6]. Significant efforts have been made for containerized application deployment
    in clouds. Commercial container management platforms, e.g., Rancher [29] and OpenShift
    [30], facilitate container deployment in public clouds. In addition to cloud management
    and visibility, they empower application providers with the ability to easily
    adapt the deployment of containers across data centers through a unified user
    interface or API. However, these platforms do not support auto-scaling containers
    for applications to quickly respond to workload fluctuations, which are essential
    for application providers to achieve low deployment cost and satisfactory application
    performance [17]. The problem of containerized application deployment in clouds
    has been studied at different resource levels: container deployment [5], [8],
    [31], cluster deployment [9], [32], or both [33]. These works considered horizontal
    scaling [8], [9], [32], vertical scaling [31], or both [5] depending on different
    elasticity dimensions. However, they all focus on auto-scaling techniques within
    a single data center to handle workload variations of applications in a cost-effective
    manner. That is, these solutions neglect container locations, which can have a
    significant impact on both the deployment cost and the network latency of applications.
    Predictive auto-scaling utilizes a workload forecast to derive scaling actions.
    Google uses Autopilot, as a vertical and horizontal auto-scaler, to reduce resource
    waste and increase reliability [34]. Focusing on vertical scaling, self-adaptive
    resource sharing was investigated in the article [35] for co-located applications
    in saturated containerized clouds. In the article [36], a hybrid auto-scaling
    mechanism, called Chameleon, combining proactive methods with a reactive fallback
    mechanism was proposed to deploy business applications. In the article [37], several
    predictive-based auto-scaling policies were designed for microservice applications
    both on VMs and containers level. However, they also focus on auto-scaling techniques
    within a single data center to handle workload variations of applications. Considering
    the locations of containers, latency-aware auto-scalers, i.e., Voilà [38] and
    Hona [39], were proposed for application replica placement in fog computing platforms.
    In the article [6], ge-kube was proposed as an extension of Kubernetes to introduce
    self-adaptation and network-aware scheduling capabilities among 4 different regions.
    A multi-level elastic deployment model for containerized applications was proposed
    in the article [7]. The model was evaluated in a simulated geo-distributed environments
    through uniformly distributing network delay. To support container scaling globally,
    the existing algorithms such as the meta-heuristics in the article [17] were proposed
    to make deployment decisions based on the current workload. However, these algorithms
    suffer from the high computational cost. Moreover, the current deployment may
    not work well in the future due to the dynamic nature of application workloads.
    B. Container Scaling With RL Existing research showed that RL is effective at
    scaling virtual machines (VMs) to handle dynamic application workloads [21], [40],
    [41], [42]. Due to the large difference between containers and VMs in terms of
    start-up, shut-down, and migration times, these RL-based algorithms that perform
    well in VM scaling cannot be effectively applied to container scaling with high
    demand for timeliness and accuracy [10]. Recently, some RL-based algorithms have
    been proposed for scaling containers for application deployment. For example,
    Rossi et al. [5] proposed a model-based RL method to control the horizontal and
    vertical elasticity of containers. Because the future trend of workload is not
    considered, these approaches fail to deal with highly dynamic application workloads,
    which causes resource wastage or compromised quality of service (QoS) [10]. As
    a prediction technique, time series analysis has been applied to handle dynamic
    application workloads [43]. The resulting workload prediction models can be further
    utilized by RL methods to ensure the timeliness and accuracy of scaling actions,
    such as A-SARSA [10]. Note that A-SARSA only considers one-step prediction, which
    may not be sufficient for RL to make well-informed scaling decisions as shown
    experimentally in Section V-E. Besides, A-SARSA focuses on auto-scaling containers
    within a single data center. Due to its use of the Q-table, A-SARSA cannot scale
    well to large LACS problems involving many geo-distributed data centers. In recent
    years, deep Q-network (DQN) was applied to resource allocation problems with high-dimensional
    state spaces [21], [23], [28]. In the article [21], a DQN-based resource provisioning
    and task scheduling system was proposed to minimize the energy cost for cloud
    service providers. Further considering the thermal effect of job allocation, Yi
    et al. [23] applied DQN to allocate compute-intensive jobs within the boundary
    of a single data center. Tang et al. [28] proposed a DQN-based container migration
    algorithm, i.e., DQLCM, to support mobility tasks with various application requirements,
    including the communication delay, power consumption, and migration cost. Traditional
    DRL methods assume that agents are free to any policy for exploration [44], [45],
    [46], [47], [48]. For the location-aware container scaling (LACS) problem, it
    is unacceptable to give an agent complete freedom. For example, some scaling actions
    may cause containers to be heavily utilized. In that case, the application performance
    will drastically deteriorate such that the constraint on the average response
    time cannot be satisfied. Therefore, safe exploration, i.e., to control the damage
    caused by exploration [49], is important for solving the LACS problem. To guarantee
    constraint satisfaction, some policy-search-based DRL algorithms were proposed
    recently. For example, constrained policy optimization (CPO) was proposed to train
    neural network policies for robotic control tasks [50]. However, CPO may not be
    suitable for the LACS problem because it requires the learning process to start
    from a feasible policy. It is difficult to manually identify such a feasible stochastic
    policy modeled as a deep neural network2 for any LACS problem instance. Moreover,
    DQN is often shown to be more effective than the policy-search-based method [46],
    [48]. The review above motivates us to develop a DQN-based algorithm to address
    the LACS problem. Moreover, to satisfy the constraint on the average response
    time, safe RL techniques, i.e., safe exploration, should be investigated. SECTION
    III. Problem Description The aim of the location-aware container scaling (LACS)
    problem is to scale containers globally for an application in response to a dynamically
    changing and widely distributed workload to minimize the total deployment cost
    ( TDC ) over a time span subject to the constraint on the average response time
    ( ART ). The response time is measured from the moment a user makes an application
    request to the moment when this user receives the corresponding response, taking
    into account both request processing time and round-trip delay ( RTD ) between
    the user and an application instance. The important notations for problem modeling
    are summarized in Table II. TABLE II Mathematical Notations We consider container
    scaling for application deployment with three layers: container manager layer,
    application layer, and user layer, as shown in Fig. 1. In practice, an application
    involves a potentially large and dynamically changing number of requests from
    widely distributed users in global user regions U . Suppose that the entire time
    span, e.g., a billing day, is divided into fixed-size execution periods. During
    time period t , we represent the workload from user region u(u∈U) in terms of
    application request rate as γ u (t) . Fig. 1. A simplified system architecture
    of container scaling for application deployment in geo-distributed clouds based
    on [6]. Show All Multiple application instances can be deployed in parallel and
    each application instance independently processes a subset of the incoming requests
    [6]. Following [51], [52], the average resource consumption per request over a
    long sequence of requests is highly stable. Therefore, for an application instance,
    the variation of workload depends on the fluctuation of request rates from the
    corresponding user regions. Let n(t) denote the number of application instances
    during time period t . Similar to [6], we adopt a hierarchical architecture to
    manage all application instances scalably, following the master-workers pattern
    [53]. We consider a set of geo-distributed data centers D . In data center d∈D
    , a collection of resources can be allocated to containers for application deployment.
    There are usually four main kinds of resources for containers in public clouds:
    CPU, memory, storage, and bandwidth [28]. Since the computing resource is the
    main factor, we assume that there is sufficient memory, storage and network capacity
    for the containerized application [54], [55]. During time period t we use x d
    (t) ( x d (t)∈N ) to denote the provisioned CPU, e.g., the number of vCPUs, to
    the container in data center d for application instance A d (t) . Note that if
    there is not an application instance to be deployed in d , then x d (t)=0 holds.
    When an application instance is deployed in a single-vCPU container in d , then
    x d (t)=1 holds. The application deployment during t can be completely captured
    by the vCPU provision vector CPU(t)=[ x d (t) ] d∈D , including the numbers of
    vCPUs provisioned in all cloud data centers. Let U C d denote the unit cost of
    vCPUs for containers in data center d . Based on the application deployment during
    t , i.e., CPU(t) , the deployment cost of all application instances during t can
    be calculated by: DC(t)= ∑ d∈D x d (t)U C d . (1) View Source We use σ u,d (t)∈[0,1]
    to denote the percentage of requests from user region u to application instance
    A d (t) during time period t . The workload of application instance A d (t) during
    t can be calculated by: λ d (t)= ∑ u∈U γ u (t) σ u,d (t), (2) View Source where
    ∑ d∈D σ u,d (t)=1 guarantees that all application requests from user regions will
    be processed. The aggregated application workload from all user regions during
    t can be determined as: ω(t)= ∑ u∈U γ u (t). (3) View Source Let μ d (t) denote
    the capacity of A d (t) , i.e., the maximum amount of requests processable by
    A d (t) per time unit. Following [56], we model each individual application instance
    as an M/M/1 queue. According to Little''s Law [57], the average request processing
    time of A d (t) depends on both μ d (t) and λ d (t) : T arp d (t)= 1 μ d (t)−
    λ d (t) , (4) View Source where μ d (t)> λ d (t) guarantees that the capacity
    of A d (t) is greater than its workload. Let T rtd u,d denote RTD between user
    region u and data center d , we can calculate the average response time for all
    the user requests during time period t by: ART(t)= ∑ d∈D ∑ u∈U γ u (t) σ u,d (t)(
    T rtd u,d + T arp d (t)) ω(t) . (5) View Source With the goal to minimize TDC
    , i.e., the cumulative deployment cost over the time span involving T consecutive
    periods, i.e., t∈{1,…,T} , the LACS problem can be formulated as follows: minTDC=
    ∑ t=1 T DC(t), (6) View Source subject to: ART(t)⩽m  ∀t∈{1,…,T}. (7) View Source
    Constraint (7) guarantees that ART at any period over the entire time span does
    not exceed the acceptable threshold m set by the application provider. In the
    next section, we introduce our DeepScale algorithm to solve the LACS problem.
    SECTION IV. DeepScale for LACS Our proposed algorithm DeepScale solves the location-aware
    container scaling (LACS) problem through auto-scaling containers both horizontally
    and vertically for application deployment in geo-distributed clouds. In this section,
    we start with a high-level overview of DeepScale and then describe the details
    on how DeepScale realizes container scaling by a deep reinforcement learning (DRL)-based
    policy. A. Overview of DeepScale To improve the effectiveness of the scaling actions,
    we include a workload prediction model in DeepScale to accurately predict future
    workloads of cloud applications [20]. DeepScale realizes container scaling by
    a DRL-based policy. Taking the predicted workloads and monitored container status
    as input, the scaling policy decides when and what scaling actions are performed
    to minimize the total deployment cost ( TDC ) subject to the constraint on the
    average response time ( ART ). Fig. 2 illustrates the workflow of DeepScale. Fig.
    2. Workflow of DeepScale. Show All Concretely, DeepScale first trains the workload
    prediction model based on historical workload traces. Different time series prediction
    methods, e.g., long short-term memory (LSTM) [58], can be used to built the workload
    prediction model. Afterwards, a DRL-based scaling policy is trained by utilizing
    the predicted future workloads. Finally, the trained prediction model and scaling
    policy can be commissioned to scale containers for serving incoming application
    requests. Before each time period, DeepScale first predicts the future application
    workloads using the prediction model. Based on the predicted workloads and the
    container status (e.g., resource utilization [6]), DeepScale makes container scaling
    decisions using the scaling policy. Because different containers may have largely
    different capacities in terms of vCPU numbers, DeepScale applies capacity-based
    weighted Round-Robin (CWRR) [59] to dispatch requests among all application instances.
    CWRR can be implemented in the load balancing modules offered by cloud providers,
    e.g., AWS.3 With CWRR, the percentage of requests from user region u to application
    instance A d (t) , i.e., σ u,d (t) , is determined by: σ u,d (t)= x d (t) ∑ d∈D
    x d (t) . (8) View Source The rationale of CWRR-based request dispatching is two-fold.
    On the one hand, CWRR is commonly used in practice due to its simplicity and low
    computational cost [60]. On the other hand, CWRR can achieve effective load balancing
    by dispatching more user requests to the application instances with larger capacities
    [59]. Thus it can prevent any application instance from being heavily utilized,
    thereby reducing the risk of long queuing time. Next, we describe the details
    on how DeepScale trains the DRL-based scaling policy. We define a DRL-based scaling
    system for the LACS problem as follows. Observation: The agent observation includes
    the information about the current containers for application deployment and future
    application workloads. Action: To perform scaling actions, i.e., to adjust the
    number of containers among data centers (horizontal scaling) and/or the number
    of vCPUs provisioned to current containers (vertical scaling). The DRL design
    challenge is to effectively minimize TDC while handling constraint (7). First,
    we introduce a State Constructor to collect multi-step future workloads using
    the workload prediction model. The essential information can help deep Q-network
    (DQN) to make well-informed scaling decisions. Second, the predicted workload
    is explicitly used by a heuristic-based Action Executor to ensure that any scaling
    decisions made by DQN will not prolong ART beyond the acceptable level. Finally,
    we propose a penalty-based reward function to guide constraint-aware Q-learning.
    The DRL-based scaling policy is shown in Fig. 3, which is composed of three components,
    i.e., a State Constructor, a DQN, and an Action Executor. In the following, we
    provide a detailed description of each component. Fig. 3. Training DRL-based scaling
    policy. Show All B. State Constructor Three types of observation information are
    collected by the State Constructor. The Current Deployment of Application Instances:
    We consider the current vCPU provision for application instances, i.e., CPU(t)
    , as the state feature because it decides the deployment cost during t , i.e.,
    DC(t) , by (1) and significantly affects application average response time ART(t)
    by (4), (8), and (5). The Current Resource Utilization: The current containers’
    CPU utilization rate is considered because it has a major impact on the application
    average response time [18]. With the help of geo-kube [6] for geo-distributed
    and elastic deployment of containers in Kubernetes, we can periodically monitor
    the CPU utilization rate of containers through RESTful APIs, e.g., the Metrics
    API in Kubernetes.4 Because the CWRR-based request dispatching can approach the
    load balancing among all containers, we use the average utilization rate u(t)
    as the state feature: u(t)= ∑ d∈{i| x i (t)>0} u d (t) n(t) , (9) View Source
    where u d (t) is the monitored average CPU utilization rate of application instance
    A d (t) during t . The Multi-Step Future Workloads: To make well-informed scaling
    decisions in terms of cost-effectiveness, the State Constructor considers multi-step
    future workloads. Particularly, the State Constructor utilizes the trained workload
    prediction model to acquire the multi-step future workloads by a recursive strategy
    [20]. That is, the predicted workload is recursively used to predict the next
    workload. For example, after w(t+1) is predicted, it is considered as input of
    the workload prediction model to predict the next workload w(t+2) . Similarly,
    w(t+2) is used to predict w(t+3) , etc. We consider the difference between the
    predicted workloads in the future f time periods and the current workload, i.e.,
    Δw(t)=[w(t+1)−w(t),w(t+2)−w(t),…,w(t+f)−w(t) ], as the state feature. Here, we
    use the workload variations based on the aggregated request rate rather than the
    request rate from each user region since this is more relevant for the DQN to
    learn to make high-level scaling decisions, e.g., change the total number of vCPUs
    provisioned to containers. We will detail the high-level scaling decisions in
    the next subsection. To sum up, the output of the State Constructor is s(t)=[CPU(t),u(t),Δw(t)]
    , which describes the current vCPU provision, average CPU utilization rate, and
    the predicted workload variations. C. DRL for Training DQN When performing both
    vertical and horizontal scaling in geo-distributed clouds, the number of potential
    scaling actions is indeterminate. For example, the possible actions for horizontal
    scaling depend on the locations of current application instances, not to mention
    the combination with vertical scaling. Therefore, it is infeasible to design a
    fixed-size action space in advance. We apply a novel method to let the DQN make
    high-level scaling decisions, i.e., change the total number of vCPUs provisioned
    to containers, and introduce a heuristic-based Action Executor to make low-level
    scaling decisions for concrete horizontal and vertical scaling actions. Formally,
    we define the action space of the high-level scaling decisions as A={↑,↓,→} ,
    where ↑ , ↓ , and → represent to increase (scale-up), decrease (scale-down), or
    maintain vCPU provision respectively. We apply double Q-learning to learn the
    optimal Q-function. The DRL-based scaling policy applies the DQN in Fig. 3 as
    the function approximator. Following many existing research works [23], [61],
    we use experience replay [46] to stabilize Q-learning. To guide DRL to minimize
    TDC over the time span subject to the constraint on ART , we design a new reward
    function for DRL as follow: r(s(t),a(t))=−DC(t)−max(0,ρ(ART(t)−m)), (10) View
    Source where ρ is a penalty parameter for the scaling actions that cannot satisfy
    the performance requirement. Algorithm 1: Training the DQN. Initialize: Experience
    replay memory D Output: DQN parameters 1: for episode=1 to Max_Episode do 2: for
    t=1 to T do 3: Obtain s(t) from the State Constructor 4: With probability ϵ select
    a random action, otherwise select an action with the maximum Q-value 5: Perform
    container scaling using the chosen action a(t) through the Action Executor 6:
    Obtain s(t+1) , calculate r(s(t),a(t)) based on (10) 7: Store transition (s(t),a(t),r(s(t),a(t)),s(t+1))
    in D ; 8: Update Q-value 9: end for 10: Update DQN parameters using new Q-value
    estimates 11: end for The detailed procedures for training the DQN are shown in
    Algorithm 1. At each time period t of an execution episode, based on the current
    s(t) , the DRL agent uses ϵ -greedy policy to choose an action randomly with probability
    ϵ and select the action with the maximum Q-value with probability 1−ϵ (step 4).
    The chosen action, i.e., a(t) , is performed through the Action Executor (step
    5). At the next time period t+1 , the DRL agent obtains s(t+1) , calculates the
    reward r(s(t),a(t)) defined in (10) (step 6), and subsequently updates the Q-value
    (step 8). At the end of the execution episode, the DRL agent trains all the connection
    weight parameters of the DQN. The trained DQN will be utilized in the next execution
    episode (step 10). D. Action Executor Obeying the high-level scaling decisions
    from the DQN, i.e., to scale-up, scale-down, or maintain the total vCPU number,
    we design an Action Executor to make low-level scaling decisions. For example,
    if a(t)=↑ is made by the DQN, the proposed Action Executor may add additional
    units of vCPU to existing containers and/or launch new containers in appropriate
    data centers. To closely follow large workload changes, three safety-aware heuristics
    are proposed to quickly change the deployed capacity of application instances.
    Note that the predicted application workload is also considered by the Action
    Executor to ensure that any low-level scaling decisions will not prolong the average
    response time beyond the acceptable level. 1) Scale-Up When a high-level decision
    of scale-up is made, i.e., a(t)=↑ , the Action Executor will iteratively choose
    the data center with the highest scale-up benefit to increase one vCPU unit. Particularly,
    the benefit of data center d in terms of performance improvement and cost-saving
    is estimated by: benefi t + d = ANL(t)−AN L + d (t) D C + d (t)−DC(t) , (11) View
    Source where AN L + d (t) and D C + d (t) are the new average network latency
    and deployment cost provided that one vCPU unit is added in d . Concretely, the
    average network latency can be calculated by: ANL(t)= ∑ d∈D ∑ u∈U γ u (t) σ u,d
    (t) T rtd u,d ω(t) . (12) View Source Note that if there exists a container in
    data center d , the benefit of d is estimated provided that one vCPU unit is added
    to this container (vertical scaling). Otherwise, the benefit is estimated provided
    that a new container with one vCPU unit is launched in d (horizontal scaling).
    Next, we introduce a safety-aware mechanism to allocate a sufficient number of
    vCPUs in one scaling action. After increasing one vCPU unit, the new average CPU
    utilization rate can be estimated by: u ′ (t)= w(t+1) ∑ d∈{i| x i (t)>0} μ ′ d
    (t) , (13) View Source where w(t+1) is the workload during the next time period
    predicted by the workload prediction model and μ ′ d (t) denotes the new capacity
    of A d (t) . If u ′ (t)⩾1 , one more vCPU unit is added following the above process.
    Otherwise, the Action Executor stops scaling up vCPUs. The safety-aware mechanism
    aims to handle the situation when there is a surge in workload. Finally, user
    requests are re-dispatched based on VWRR for load balancing. Algorithm 2 describes
    the overall procedure of scale-up vCPUs by the proposed Action Executor. Algorithm
    2: Action Executor Performs Scale-Up. Input: High-level decisions to scale-up
    vCPUs. Output: Low-level scaling actions. 1: Termination←False 2: while Termination=False
    do 3: Decide the data center d with the highest benefi t + d evaluated by (11)
    provided that one vCPU unit is added to the container or launch a new container
    with one vCPU unit 4: Evaluate the new average CPU utilization rate u ′ (t) based
    on (13) 5: if u ′ (t)<1 then 6: Termination←True 7: end if 8: end while 9: Re-dispatch
    requests based on CWRR 2) Scale-Down When a high-level decision of scale-down
    is made, i.e., a(t)=↓ , the Action Executor will attempt to reduce one vCPU unit
    from the existing container. Note that in order to modify the CPU resources assigned
    to containers, Kubernetes recreates the corresponding Pod (in one-container-per-Pod
    model). In practice, connection draining can be applied to ensure that all the
    requests are served. Through connection draining, the existing connection to application
    instances can be kept alive to complete in-flight requests during the process
    of starting up new containers. The mechanism is offered by many cloud providers,
    e.g., AWS [62]. Here, we introduce another safety-aware mechanism to avoid the
    potential deterioration of application performance. Particularly, the Action Executor
    first estimates the average CPU utilization rate u ′ (t) after reducing one vCPU
    unit from the current containers defined (13). If u ′ (t)<1 , the Action Executor
    reduces the vCPU unit from the existing container in the data center c with the
    largest scale-down benefit, which is estimated by: benefi t − d = DC(t)−D C −
    d (t) AN L − d (t)−ANL(t) , (14) View Source where AN L − d (t) and D C − d (t)
    are the new average network latency and deployment cost after decreasing one vCPU
    unit in d . In case that u ′ (t)⩾1 , the Action Executor aborts decreasing vCPUs
    to prevent containers from being heavily utilized and potentially resulting in
    a substantially prolonged response delay. Algorithm 3 presents the overall process
    to scale-down vCPUs by our proposed Action Executor. Algorithm 3: Action Executor
    Performs Scale-Down. Input: High-level decisions to scale-down vCPUs. Output:
    Low-level scaling actions. 1: Termination←False 2: while Termination=False do
    3: Evaluate the new average CPU utilization rate u ′ (t) if one vCPU unit is reduced
    based on (13) 4: if u ′ (t)<1 then 5: Decide the data center d with the highest
    benefi t − d evaluated by (14) provided that one vCPU unit is reduced from the
    container 6: else 7: Termination←True 8: end if 9: end while 10: Re-dispatch requests
    based on CWRR 3) Maintain When a maintain decision is made, i.e., a(t)=→ , the
    Action Executor will attempt to reduce deployment cost and improve application
    performance by reconfiguring the current containers. Particularly, the Action
    Executor first determines the data center with the largest benefi t + d defined
    in (11) for increasing one vCPU unit. Then the Action Executor determines the
    data center with the largest benefi t − d defined in (14) for decreasing one vCPU
    unit. Let AN L ′ (t) and D C ′ (t) denote the new average network latency and
    deployment cost after the reconfiguration. In many cases, we have AN L ′ (t)<ANL(t)∧D
    C ′ (t)>DC(t) or D C ′ (t)<DC(t)∧AN L ′ (t)>ANL(t) . The Action Executor only
    performs the low-level scaling when AN L ′ (t)<ANL(t)∧D C ′ (t)<DC(t) to avoid
    reconfiguring the current containers too frequently. In summary, as shown in Fig.
    3 our newly designed multi-step predicted workloads, penalty-based reward function,
    and safety-aware heuristics are integrated into the three components of the DRL-based
    scaling policy to realize both the cost minimization and constraint satisfaction
    of the LACS problem. SECTION V. Performance Evaluation In the absence of a publicly
    available global cloud testbed, we conduct a series of simulation studies to examine
    the performance of DeepScale using the real-world datasets. By using realistic
    container pricing and workloads for cloud applications, we compare the performance
    of DeepScale with several state-of-the-art baselines. The highlights are: DeepScale
    can consistently satisfy the constraint on the average response time ( ART ),
    while some baselines cannot in some problem instances. For different problem instances
    in terms of application types and workloads, DeepScale achieves up to 39% savings
    in terms of the total deployment cost ( TDC ). A. Datasets We collect the real
    container pricing schemes in April 2023 from AWS [15]. 18 major Amazon data centers
    (see Table I) have been included in the experiments. Referring to [14], we adopt
    82 user regions from 35 countries on 6 continents in the Sprint IP Network5 to
    simulate the global user community. To evaluate the network latency between users
    and deployed services, we use the observation information in the Sprint6 IP backbone
    network databases [18]. We use real traces of user requests based on the public
    benchmark WikiBench [26] and NASA HTTP [27] to create workloads for our experiments.
    WikiBench is a Web hosting benchmark allowing the stress-test of systems designed
    to host Web applications. Following [63], our experimental workload contains 1%
    of all user requests issued to Wikipedia (in all languages). NASA HTTP contains
    HTTP requests to the NASA Kennedy Space Center WWW server in Florida. Refer to
    [6], the request rate of NASA HTTP is increased by 180 times to replay millions
    of requests daily. Referring to [64], [65], we apply Facebook subscribers statistics7
    to simulate the distribution of application requests among different user regions.
    We randomly extract one day''s workload from WikiBench and NASA HTTP for training
    and use the workload on the following day for testing. The duration of each time
    period, i.e., the time interval for making scaling decisions, is set to 3 minutes
    as in the articles [5], [6]. Fig. 4 depicts the request rate (i.e., number of
    requests per second) during the two days. That is, the left 480 time periods are
    used for training and the right 480 time periods are used for testing. From Fig.
    4, we can observe that the historical workloads variations match well with future
    workloads changes on the WikiBench dataset. As a result, the prediction model
    can predict future workloads with high accuracy. In comparison, on the NASA HTTP
    dataset, future workload trend deviates significantly from the historical track,
    making this dataset much harder to handle by the prediction model. We are tasked
    to deploy 3 applications reported in the article [18]. We follow [14] closely
    to calibrate the queuing model, particularly the request rate distribution based
    on real-world business web applications reported in the article [66], in term
    of resource demands for the individual applications. The application processing
    time for a single request is approximately 10 ms (application 1), 15 ms (application
    2), and 20 ms (application 3), running on the container with one vCPU unit. To
    sum up, six problem instances are included in our experiments. For convenience,
    we denote them as Wiki app-1, Wiki app-2, Wiki app-3, NASA app-1, NASA app-2,
    NASA app-3, respectively. Fig. 4. Application worklaod. Show All B. Algorithm
    Implementation We implement DeepScale using PyTorch [67] and build a novel simulator
    using openAI‘s gym environment [68]. Following the basic principles for cloud
    experiment reproducibility in the article [69], we have made the representative
    code and data publicly available at: https://zenodo.org/record/8166602. For the
    workload prediction model, we apply a long short-term memory (LSTM) neural network
    because it can accurately predict cloud application workloads in many existing
    research works and production systems [20], [70]. We adopt the same parameter
    settings as [20] for the LSTM-based prediction model: a hidden layer with 20 LSTM
    units and the number of future time periods to be predicted f=5 . We use root-mean-square
    error (RMSE) as the loss function and Adam [71] as the optimizer for training
    the LSTM neural network. In our experiments, the LSTM neural network can converge
    within 100 episodes and provide good performance in predicting future workloads
    (see Section V-F). In our implementation, the DQN has two fully-connected hidden
    layers, each with 64 nodes. The input and hidden layers use rectified linear units
    (ReLUs). We also apply Adam as the optimizer. The initial and minimum ϵ , i.e.,
    the probability that DRL randomly chooses an action (in step 4 of Algorithm 1),
    are set as 0.2 and 0.01, respectively [23]. Following [23], other algorithm settings
    of the DQN include: learning rate α is 0.001, and the mini-batch size is 32. The
    DQN is trained for 200 episodes because it always converges within 200 episodes.
    Refer to [12], the acceptable threshold of ART , i.e., m in constraint (7), is
    set to 150 ms, since a latency up to 200 ms will deteriorate the user experience
    significantly [72]. C. Baselines To evaluate the performance of DeepScale, we
    further implement an industry scaling strategy and two state-of-the-art baselines
    in our experiments. Amazon auto-scaling service [19] provides many container scaling
    methods to handle the increasing or decreasing workload of an application. Referring
    to [73], we apply the rule-based auto-scaling method by setting an upper threshold
    (0.8) and a lower threshold (0.6) on the CPU utilization rate of containers. For
    convenience, we denote this baseline algorithm as AWS-Scale. Concretely, if the
    CPU utilization rate is above the upper threshold, AWS-Scale will scale-up the
    system. In case the CPU utilization rate is below the lower threshold, AWS-Scale
    will scale-down the system. To realize the location-aware container scaling, Algorithms
    2 and 3 are used in AWS-Scale to make low-level scaling decisions according to
    the threshold-based scale-up and scale-down decisions. A-SARSA [10] is a recently
    proposed RL algorithm for auto-scaling containers. A-SARSA first combines ARIMA
    and a feedforward neural network to predict the CPU utilization rate and response
    time. Then the two predicted values are respectively discretized into different
    levels. Finally, a Q-table is trained by SARSA to make scaling decisions. For
    a fair comparison, we have fine-tuned the ARIMA model to achieve highly competitive
    accuracy for workload prediction with our LSTM-based model. Concretely, the RMSE
    of the predicted workload w(t+1) on the testing day are 1.86 and 2.02 requests/s
    respectively (1.78 and 2.05 requests/s for our LSTM-based model detailed in Section
    V-F). To control the constraint on the response time, A-SARSA applies a penalty-based
    reward function. Because A-SARSA only considers the container scaling within a
    single cloud data center, we also allow A-SARSA to use low-level scaling decisions
    made by our Action Executor. The deep Q-learning container migration algorithm
    (DQLCM) [28] is proposed for delay-sensitive applications in fog computing. To
    select an appropriate action, DQLCM introduces problem-specific strategies for
    container migration. Particularly, two thresholds of CPU utilization rate, i.e.,
    t h under and t h over , are predefined to classify fog nodes into different groups,
    i.e., under-utilized nodes and over-utilized nodes. For the nodes in different
    groups, different heuristics are proposed to determine the migrated containers
    and their destination. The action set of DQLCM is determined as optional container
    placement generated by these heuristics. To apply DQLCM to the LACS problem, we
    treat container migration as container scaling and fog nodes as application instances
    at different locations. t h under and t h over are set as 0.5 and 0.9 respectively
    because the combination of parameters demonstrates the best performance in terms
    of both the cost minimization and constraint satisfaction in our experiments.
    D. Constraint Compliance To compare algorithm performance, we run each experiment
    30 times. Fig. 5 shows ART of different problem instances over the testing day
    achieved by DeepScale and the three baselines. The red line in Fig. 5 is the predefined
    constraint m , i.e., 150 ms. Fig. 5. ART of problem instances. Show All For the
    WikiBench workload, ART of app-3 by AWS-Scale is slightly over m . The three RL-based
    algorithms are capable of meeting the constraint on ART . Particularly, A-SARSA
    always has the lowest ART . Note that the LACS problem aims to minimize TDC subject
    to the constraint on ART . The lower ART obtained by A-SARSA is undesirable due
    to its high TDC (see Section V-E). For DQLCM, ART is longer than DeepScale for
    app-1 and app-3, while shorter than DeepScale for app-2. For the NASA HTTP workload,
    the two threshold-based algorithms, i.e., AWS-Scale and DQLCM, significantly exceed
    the predefined constraint m for all applications. This indicates that using two
    fixed thresholds for container scaling is insufficient to satisfy the performance
    constraint. With the help of the penalty-based reward function and safety-aware
    heuristics, A-SARSA and DeepScale can satisfy constraints for all applications.
    E. TDC Comparison In this subsection, we compare DeepScale with the baselines
    for the constraint-compliant problem instances in terms of TDC as reported in
    Fig. 6. Fig. 6. TDC of constraint-compliant problem instances. Show All DeepScale
    saves cost by 22% for app-1 and 21% for app-2 compared to AWS-Scale for the WikiBench
    workload. This shows that the industry-leading threshold-based methods may not
    be suitable for scaling cloud applications with dynamically changing and widely
    distributed workloads. The bad performance on the deployment cost in our experiments
    is consistent with previous observations reported in the article [73]. A-SARSA
    spends 21% more TDC for the WikiBench workload and 39% more TDC for the NASA HTTP
    workload than DeepScale on average over different applications. Through our newly
    proposed multi-step workload prediction, the scaling decisions made by DeepScale
    is more cost-effective. Beside, DeepScale can handle high-dimensional state space
    more effectively by using DQN comparing with the discretization technique adopted
    in A-SARSA. DeepScale achieves on average 8% less TDC than DQLCM for the WikiBench
    workload, which also demonstrates the effectiveness of the workload prediction
    model. The observed performance differences between DeepScale and all baselines
    are all verified through a statistical test (Wilcoxon Rank-Sum test) with a significance
    level of 0.05. From the above results, we can conclude that DeepScale can achieve
    the lowest TDC and highly likely satisfy constraints on ART . The mean and standard
    deviation of ART and TDC are presented in Table III. The results are presented
    in italic for the problem instances on which the baseline algorithms cannot satisfy
    the constraint on ART . From Table III we observe that ART and TDC achieved by
    DeepScale has a small standard deviation over 30 repeated experiments, confirming
    its stability and reliability for the LACS problem. TABLE III Algorithm Performance
    Comparison for the LACS Problem With Different Problem Instances ( ART ART in
    Ms, TDC TDC per Year in USD, the Best is Highlighted) F. Further Analysis To verify
    the reliability of the workload prediction model used by DeepScale, we evaluate
    the performance of long short-term memory (LSTM)-based workload prediction model
    in terms of root-mean-square error (RMSE). The RMSE of the predicted workloads
    in future f time periods on the testing day are shown in Table IV. From Table
    IV, we can see the RMSE ranges from 1.78 to 5.08 requests/s for the WikiBench
    workload and from 2.05 to 6.34 requests/s for the NASA HTTP workload. We can calculate
    a normalized RMSE by: NRMSE= RMSE y max − y min , (15) View Source where y max
    and y min are the maximum and minimum workloads on the testing day. By calculation,
    the NRMSE of the workload prediction model for predicting the WikiBench workload
    and the NASA HTTP workload in the next time period are 0.95% and 1.36% respectively.
    The results are reliable for DeepScale to make scaling decisions [20]. TABLE IV
    Performance of Workload Prediction Model in Terms of RMSE (Requests/s) Next, we
    depict the change of TDC and ART obtained by DeepScale on the testing day across
    all learning episodes in Figs. 7 and 8. For the ablation study, the performance
    of DeepScale with single-step predicted workload (DeepScale with single-step prediction
    for short) and DeepScale without using predicted workloads (DeepScale without
    prediction for short) are also included in Figs. 7 and 8. Concretely, the State
    Constructor of DeepScale with single-step prediction only utilizes w(t+1) as the
    prediction information. Also, w(t+1) is used by Action Executor to generate constraint-compliant
    scaling decisions. For DeepScale without prediction, the State Constructor does
    not collect the information of predicted workloads. Meanwhile, the Action Executor
    makes low-level scaling decisions based on the current CPU utilization rate. We
    only present the results with respect to the problem instance of NASA app-3 while
    a similar trend has been observed for other problem instances. Fig. 7 shows that
    TDC of the three algorithms becomes flattened after about 50 episodes. By utilizing
    multi-step future workloads from the workload prediction model, TDC of DeepScale
    is 3% less than DeepScale with single-step prediction and 5% less than DeepScale
    without prediction, confirming the importance of considering future workloads
    on cost-saving. In Fig. 8, we can observe that ART of DeepScale and DeepScale
    with single-step prediction falls strictly under m (red line) after about 100
    episodes, while DeepScale without prediction cannot effectively learn constraint-compliant
    policies. The results show that using the predicted workload in Action Executor
    is very helpful to reduce ART for meeting the performance constraint. Fig. 7.
    TDC of NASA app-3 on the testing day. Show All Fig. 8. ART of NASA app-3 on the
    testing day. Show All After training, A-SARSA, DQLCM, and DeepScale can scale
    containerized applications for incoming requests with trivial computational overhead.
    Particularly, the total time required to make a high-level scaling decision using
    DQN and a low-level scaling decision through our heuristic-based Action Executor
    is within 1 ms. The training time of DeepScale is within 30 minutes, which includes
    the training of the LSTM-based workload prediction model and the DRL-based scaling
    policy. Periodical use of DeepScale every day is highly feasible in practice.
    The training time of A-SARSA is similar to DeepScale. The training of DQLCM takes
    a longer time due to a more complex action space (about 5 hours). Finally, we
    apply DeepScale to solve the LACS problem with a more stringent threshold, i.e.,
    m=140 ms and a more lax threshold, i.e., m=160 ms . The mean and standard deviation
    of ART and TDC are presented in Table V. From Table, V we observe that DeepScale
    still can satisfy constraints on ART for all problem instances. A smaller m results
    in larger TDC while a larger m contributes to cost saving. TABLE V Performance
    Comparison With Different m m ( ART ART in Ms, TDC TDC per Year in USD SECTION
    VI. Discussion From the experimental results, we can see the effectiveness of
    DeepScale for solving the LACS problem. The following issues deserve further investigations.
    Scope: As confirmed by many existing studies [74], it is a common practice for
    modern applications to be architected as stateless as possible for container-level
    scalability, e.g., search engines and social media [75]. In line with this trending
    architectural style, we focus on scaling the stateless applications in the paper.
    Besides, we assume that each application instance, as a separate service, is deployed
    to one container. This is a common assumption in the literature [76], [77] and
    widely exercised in the industry for easier auto-scaling [78]. The proposed algorithm
    is inapplicable to the scenario where two or more applications running within
    the same container. In this paper, we focus on the application performance in
    terms of the response time. The response time is measured from the moment a user
    makes an application request to the moment when this user receives the corresponding
    response, taking into account both the request processing time and round-trip
    delay between the user and an application instance. Referring to [28], [54], [55],
    we assume that there is sufficient memory for the applications to support the
    temporary storage and fast access. Therefore, the request processing time is significantly
    affected by the CPU utilization of containers. That is, the experimental results
    may not apply directly to some memory-bound applications. Tail Response Time:
    In this paper, we consider the constraint on the average response time with respect
    to widely distributed user requests, because it seriously affects the user satisfaction
    with applications [12], [13]. In the industry, tail response time is also considered
    to provide deep insights into the performance of some cloud applications under
    high load [79]. We measured the distribution of the response time achieved by
    DeepScale on the testing day. Subject to the constraint on the average response
    time, i.e., 150 ms in our experiments, 99% of requests are served within 300 ms
    for all the problem instances. For example, Fig. 9 demonstrates the percentiles
    under the response time bounds ranged from 0 ms to 400 ms for app-1. In Fig. 9,
    we can observe that 99% of user requests can be served within 285 ms for the WikiBench
    workload and 292 ms for the NASA workload. Several studies have proven that the
    performance is accepted by many applications [80], [81]. Fig. 9. Percentiles under
    the response time bounds for app-1. Show All Limitations: In practice, workload
    contention may occur when an application imposes heavy workload on system resources
    [82]. In this paper, we apply the proactive strategy to ensure that the workload
    will not exceed to the critical level of the container capacity. In our experiments,
    the CPU utilization of containers is consistently below 80%, the previous study
    showed that the level of contention will not significantly affect the average
    request processing time [83]. Except for the number of allocated vCPUs, the capacity
    of an application instance is also impacted by the application caching, hyper-threading
    architecture, and potential contention. Even so, the number of vCPU is still an
    important measure of capacity for containerized applications [7], [33]. We will
    take the other effects on the capacity of application instances into account in
    our future work. SECTION VII. Conclusions and Future Works In this article, we
    proposed an effective deep reinforcement learning algorithm, DeepScale, to auto-scale
    containerized applications in geo-distributed clouds. We first formulated the
    location-aware container scaling (LACS) problem to minimize the total cost over
    a time span under the constraint on the average response time for containerized
    application deployment. By training the newly designed holistic scaling policy
    with newly designed algorithmic components, DeepScale can achieve both cost-effectiveness
    and constraint satisfaction. Finally, we evaluated the effectiveness of DeepScale
    through conducting extensive simulation studies on real-world datasets. The experiments
    with realistic application workloads showed that DeepScale can effectively satisfy
    the constraint on the average response time for a variety of applications under
    significantly different workloads. In the meantime, DeepScale can significantly
    reduce the deployment cost of applications compared with the state-of-the-art
    baselines, including Amazon auto-scaling service and recently proposed RL-based
    algorithms. In this paper, we consider the constraint on the average response
    time because it seriously affects the user satisfaction of applications [12],
    [13]. We believe it is a promising future direction to simultaneously consider
    other constraints, such as service availability among different cloud data centers
    and data sovereignty. Designing novel safe RL algorithms for solving the LACS
    problem with multiple potentially conflicting constraints will be an interesting
    research topic. Furthermore, we will evaluate DeepScale on real-world geo-distributed
    cloud platforms. Authors Figures References Keywords Metrics Footnotes More Like
    This A Global Cost-Aware Container Scheduling Strategy in Cloud Data Centers IEEE
    Transactions on Parallel and Distributed Systems Published: 2022 A Novel Cost
    Optimization Method for Mobile Cloud Computing by Capacity Planning of Green Data
    Center With Dynamic Pricing Canadian Journal of Electrical and Computer Engineering
    Published: 2019 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Services Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Auto-Scaling Containerized Applications in Geo-Distributed Clouds
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mahbub M.
  - Shubair R.M.
  citation_count: '2'
  description: With advancements of cloud technologies Multi-Access Edge Computing
    (MEC) emerged as a remarkable edge-cloud technology to provide computing facilities
    to resource-restrained edge user devices. Utilizing the features of MEC user devices
    can obtain computational services from the network edge which drastically reduces
    the transmission latency of evolving low-latency applications such as video analytics,
    e-healthcare, etc. The objective of the work is to perform a thorough survey of
    the recent advances relative to the MEC paradigm. In this context, the work overviewed
    the fundamentals, architecture, state-of-the-art enabling technologies, evolving
    supporting/assistive technologies, deployment scenarios, security issues, and
    solutions relative to the MEC technology. The work, moreover, stated the relative
    challenges and future directions to further improve the features of MEC.
  doi: 10.1016/j.jnca.2023.103726
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Graphical abstract Keywords 1. Introduction 2. Literature review
    3. Relative computing services and MEC 4. MEC standardization and architecture
    5. Enabling technologies 6. Technical features of MEC 7. Contemporary supporting
    technologies for MEC 8. MEC deployment cases 9. Security aspects of MEC 10. Lessons
    learned, challenges, and future directions 11. Conclusion Declaration of competing
    interest Acknowledgement Appendix. Data availability References Vitae Show full
    outline Cited by (2) Figures (17) Show 11 more figures Tables (3) Table 1 Table
    2 Table 3 Journal of Network and Computer Applications Volume 219, October 2023,
    103726 Contemporary advances in multi-access edge computing: A survey of fundamentals,
    architecture, technologies, deployment cases, security, challenges, and directions
    Author links open overlay panel Mobasshir Mahbub, Raed M. Shubair Show more Add
    to Mendeley Share Cite https://doi.org/10.1016/j.jnca.2023.103726 Get rights and
    content Abstract With advancements of cloud technologies Multi-Access Edge Computing
    (MEC) emerged as a remarkable edge-cloud technology to provide computing facilities
    to resource-restrained edge user devices. Utilizing the features of MEC user devices
    can obtain computational services from the network edge which drastically reduces
    the transmission latency of evolving low-latency applications such as video analytics,
    e-healthcare, etc. The objective of the work is to perform a thorough survey of
    the recent advances relative to the MEC paradigm. In this context, the work overviewed
    the fundamentals, architecture, state-of-the-art enabling technologies, evolving
    supporting/assistive technologies, deployment scenarios, security issues, and
    solutions relative to the MEC technology. The work, moreover, stated the relative
    challenges and future directions to further improve the features of MEC. Graphical
    abstract Download : Download high-res image (280KB) Download : Download full-size
    image Previous article in issue Next article in issue Keywords MECIoTDigital twinOpen-source
    MECQuantum computingAISecurity 1. Introduction According to Ericsson''s most recent
    prediction for 2020–2026, worldwide mobile subscribers will increase up to 8.8
    billion, and global wireless data traffic will be doubled (Vaezi et al., 2022),
    (Ericsson, 2021). This enormous rise in wireless data traffic, vast Internet of
    Things (IoT) device deployments, and wireless broadband subscribers are being
    driven by expanded mobile network coverage (Ericsson, 2022). With the introduction
    of beyond 5G connectivity, mobile communications infrastructures, and an ever-increasing
    number of portable devices (e.g., handsets, wearable devices, unmanned aerial
    vehicles, and interlinked vehicles), a proliferation of unique services (together
    with verticals) including intelligent cities, industry 4.0, medical services,
    and coordinated driving are now feasible. These breakthroughs necessitate the
    close integration of communication, processing, caching, and control technologies
    (Qian, 2022). By 2023, Cisco predicts that approximately 300 billion multimedia
    applications will have been downloaded (Cisco, 2018). Along with a flourishing
    spectrum of new and different services, demands for the seamless quality of experiences
    (QoEs) are rising; placing centralized or consolidated cloud computing infrastructures
    under strain since traditional centralized cloud services are unable to meet linearly
    expanding computational processing needs. Furthermore, according to Ericsson Mobility
    Research, worldwide average mobile data consumption will grow up to 164 exabytes
    (EB) per month in 2025 (Singh et al., 2022a). These emerging services have considerably
    diverse priorities, and they often necessitate huge data processing with short
    end-to-end latency. Across numerous technological solutions at multiple levels
    (i.e., machine intelligence, millimeter-wave transmissions, network function virtualization),
    MEC will perform an influential function (Liyanage et al., 2021). MEC''s purpose
    is to relocate cloud features and functionality (i.e., computational and storage
    services) to the edge or end of a network to reduce the long duration and highly
    unpredictable delays required to approach centralized clouds. MEC-assisted networking
    enables user devices (UDs) to transfer computational operations to adjacent processing
    facilities or edge processing servers, which are often located on the near-user
    network nodes (i.e., base stations) (Ebrahim et al., 2022). Nevertheless, as edge
    processing units have significantly lower computing capacity than core or central
    cloud services, the available facilities (radio, computation, and energy) must
    be effectively handled to offer users a reasonable quality of service (QoS) (Wang
    et al., 2021a). Since the end-to-end latency comprises both transmission and processing
    time, the available resources at the cellular edge of the networks must be managed
    collaboratively, determining the optimum joint allocation of resources over time
    in a dynamic and data-driven approach. The European Telecommunications Standards
    Institute (ETSI) launched the Mobile Edge Computing (MEC) Industrial Specifications
    Group (ISG) in December 2014 to encourage and expedite the evolution of edge-cloud
    computing in wireless networks (ETSI, 2014). The ETSI MEC ISG aimed to establish
    an accessible ecosystem throughout multi-vendor cloud systems positioned at the
    RAN''s edge, attainable by application/service vendors and third-party stakeholders.
    This strategy is envisioned to address the issues associated with centrally controlled
    cloud computing contexts, specifically latency and the expectation of higher speeds
    (ETSI, 2014). Wireless network operators can decrease traffic congestion in the
    center of the network and backhaul connections while facilitating the transfer
    of heavy computational activities from power or resource-restricted user equipment
    (UE) to the edge servers. This can be achieved by moving data processing workloads
    to the edge of the network and locally handling data in the vicinity of users.
    Generally, such distributed cloud architecture serves as a foundational technology
    for forthcoming 5G and beyond 5G systems, replacing traditional cellular base
    stations by providing edge-cloud computing capabilities and an IT services ecosystem
    at the edge of the network. ETSI ISG has decided to drop the ‘Mobile’ from MEC
    and changed the name to ‘Multi-Access Edge Computing’ in September 2016 to better
    reflect its appropriateness in heterogeneous network systems, including storage
    and computation assistance for end devices over stationary access technologies
    such as satellite, fiber, wireless, light-fiber, and fiber-wireless (Light, 2016).
    Software Driven Networking (SDN) and virtualized networks have been adopted in
    MEC development. SDN and Network Functions Virtualization (NFV) are natural extensions
    to MEC nodes because they enable intelligent network convergence and resource
    management. MEC nodes use virtualization to accommodate third-party vendor-based
    Containers and Virtual Machines (VM), resulting in a multi-tenant environment
    at the edge of the network. Due to the implementation of MEC nodes on the edge
    of the network, there is a requirement to promote the mobility of services.Therefore,
    VM offloading approaches and Container-based offloading strategies may be identified
    in the literature (Blanco et al., 2017). Sixth-generation (6G) (Alwis et al.,
    2021), (Elmeadawy and Shubair, 2019) network infrastructures are intended to support
    the Internet of Everything (IoE) implementations, including brain-computer interplay,
    multiple sensory extended reality (XR), and self-reliant systems to attain a completely
    automated and intelligent framework for consumer services and programs in the
    upcoming years to offer subscribers with an engrossing perception in a virtual
    environment such as Multiverse. To attain these goals, 6G must meet stringent
    standards, including ultra-high transmission and dependability, ultra-low latencies,
    seamless connection, and so on. To that end, it is crucial to include some sophisticated
    technology into the architecture of 6G wireless environments, such as using Terahertz
    communication systems (Elayan et al., 2020) and Intelligent Reflecting Surface
    (IRS)/Reconfigurable Intelligent Surface (RIS) (Alghamdi et al., 2020) to enhance
    data transmission efficiency, MEC, and machine intelligence (Ahammed et al., 2022)
    to strengthen data processing performance, and Blockchain (Guo and Yu, 2022) to
    prevent security risks. MEC is regarded as a possible 6G enabling innovation capable
    of meeting expanded service requirements (Banafaa et al., 2023). For example,
    by analyzing computation-intensive content or storing ultra-high-definition movies
    at the edge, MEC may provide ultra-low transmission latency and perfect 4K/8K
    visual broadcast. The work reviewed existing literature (survey and review papers)
    relative to the MEC technologies in section 2. Then it overviewed the fundamentals,
    relative computing technologies, and MEC in section 3. In section 4 the survey
    discussed the MEC standardization, reference architecture, and integration of
    MEC in 5G infrastructure. Afterward, it performed a survey of the state-of-the-art
    enabling technologies such as Service Function Chaining (SFC), Heterogeneous Cloud-Radio
    Access Network (H-CRAN), Device-to-Device (D2D) communication, Machine Learning
    and Artificial Intelligence (ML/AI) in section 5. Further, the work discussed
    the technical features of MEC, i.e., computation offloading, communications, content
    delivery and caching in section 6. Supporting technologies such as Rate Splitting
    Multiple Access (RSMA), Intelligent Reflecting Surfaces (IRSs), Game Theory, Auction
    Theory, Digital Twins, the open-source MEC framework (for the forthcoming or evolving
    network, such as 6G), a passive optical network (PON) are discussed in section
    7 for improving the features of MEC paradigm. In section 8, the work briefed the
    deployment scenarios of MEC. Moreover, this work provided an overview of security
    concerns relative to MEC and state-of-the-art countermeasures in section 9. Furthermore,
    it described the lessons learned, challenges, and future directions for further
    work in section 10. Finally, the survey concluded with section 11. Fig. 1 visualizes
    the structure of the paper. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 1. The structure of the paper. 2. Literature review
    This section of the paper incorporates a review of existing surveys or review
    works or papers to provide insight into the current progress of the MEC and relative
    technologies. Akhlaqi et al. (Akhlaqi and Hanapi, 2023) carried out a survey utilizing
    a mixed-method comprehensive literature review that includes quantitative and
    qualitative data from literature investigated. The classification of literature
    based on adopted techniques is described, and significant offloading-related challenges
    in MEC are explored. The work described possible fields of work, the importance
    of the methodologies, algorithms, and approaches, and the research directions
    in MEC for continued future exploration. Djigal et al. (2022) provided an in-depth
    investigation of Machine Learning/Deep Learning-based resource allocation processes
    in MEC. The survey started with tutorials that illustrate the benefits of using
    Machine Learning and Deep Learning algorithms in MEC. The work provided a comprehensive
    assessment of recent studies that employed Machine Learning/Deep Learning approaches
    for resource allocation in MEC from three perspectives: (1) task offloading; (2)
    task scheduling; and (3) joint resource allocation. Ultimately, the work explored
    the critical issues and potential research objectives of using the Machine Learning/Deep
    Learning to allocate resources in MEC networks. Liang et al. (2022) performed
    an extensive survey of prevailing advancements in MEC and explained the MEC principle,
    structure, and features. The work also discussed MEC''s technological enablers
    such as NFV, SDN, Information-Centric Networking (ICN), Cloud-Radio Access Networks
    (C-RAN), Service Function Chaining, Network Slicing, and Fog-computing-enabled
    access network infrastructures. MEC application scenarios as well as the possible
    research problems are discussed. Huo et al. (Hou et al., 2022) presented an up-to-date
    and thorough assessment of MEC-enabled vehicular network infrastructure. The work
    started with outlining MEC''s concept, framework, applications, and problems.
    Following that, the article overviewed MEC''s implementation for vehicular networking
    services and applications by identifying existing research and potential problems.
    Khan et al. (2022a) provided an in-depth study of recent advancements in MEC-assisted
    video streaming that have resulted in extraordinary improvements to enable unique
    use cases. A comprehensive overview of the current advancements is provided, with
    emphasis on novel cache management techniques, effective computation-task offloading,
    coordinated offloading and caching, and the utilization of Artificial Intelligence
    or Machine Learning (i.e., Reinforcement Learning, Deep Learning, etc.) in MEC-enabled
    streaming services. The future generation (6G) connectivity systems are designed
    to support the Internet of Everything and transform client applications and services
    into a fully autonomous and intelligent system. The Digital Twin-enabled edge
    network (DITEN) is suggested to do this by combining Mobile/Multi-Access Edge
    Computing (MEC) with Digital Twin (DT), hence boosting the performance of the
    network such as capacity, and security while lowering the cost of connectivity,
    processing, and caching. Tang et al. (2022) provided a complete review of DITEN
    for 6G in this survey. First, the work discussed the essential components of DITEN,
    such as the principle, structure, and potential. Second, a thorough DITEN design
    is created, which includes DT modeling/updating, DT implementation, major challenges,
    and supporting technologies. The Internet of Things (IoT), the vehicular network,
    the space-to-air-to-ground integrated network (SAGIN), wireless transmission technologies,
    and other technologies are then discussed, alongside the implementation of DITEN
    for each domain, such as DT models, DT interaction, incentive strategies, and
    so on. Finally, limitations and unresolved concerns are addressed. Gür et al.
    (2022) explored the ICN and MEC integration in this study to give a complete assessment
    from a prospective view of beyond 5G networks. To demonstrate the practical significance
    of ongoing standardization initiatives, the work provided a review of active standardization
    initiatives. Furthermore, the work presented major B5G deployment scenarios and
    emphasize the importance of ICN and MEC integration in meeting their needs. Finally,
    the survey outlined research problems and possible research areas. It included
    a brief review of ICN integration issues and implementation instances as well.
    Douch et al. (2022) provided a comprehensive and well-structured evaluation of
    Edge Computing (EC) as well as its associated technologies. The work defined EC
    from the roots, explaining its architecture and progression from Cloudlet technologies
    to Multi-Access Edge Computing. Further, the work reviewed current research on
    the key components of an EC framework, such as resource managing, task offloading,
    data monitoring, network management, and so on. Additionally, it focused on EC
    technological solutions, beginning with Edge Intelligence, a subclass of Artificial
    Intelligence (AI) that incorporates AI algorithms at capacity-constrained edge
    servers or nodes with substantial heterogeneity and flexibility. Then, the review
    moved forward onto 5G and its enabling technologies and overviewed how EC and
    the 5G complement one another. Moreover, it investigated virtualization and containerization
    as potential hosting Virtual Machines for edge services. Malazi et al. (Tabatabaee
    Malazi et al., 2022) presented the dynamic resource deployment problem and discussed
    its connections to other issues, including scheduling tasks, resource allocation,
    and edge caches. It also provided a thorough analysis of contemporary dynamic
    resource or service placement strategies for MEC contexts from the viewpoints
    of communication, middleware, services, and evaluation. At first, it examined
    several MEC topologies and their supporting technologies from a communication
    standpoint. Secondly, it reviewed the dynamic service placement approaches from
    the perspective of middleware. The work also conducted a methodological survey
    and identified eight research strategies that researchers adopt. The work divided
    the research aims into six major categories, resulting in a hierarchy of development
    objectives for the dynamic/flexible service placement challenge. In addition,
    it evaluated the approaches and developed a solution taxonomy based on six criteria.
    In the third phase, the work focused on the application level and introduced programs
    that can benefit from dynamic resource placement. Moreover, in the fourth stage,
    the work reviewed the evaluation platforms utilized to validate technologies,
    such as simulators and testing platforms. Finally, the work constructed a list
    of potential problems and challenges classified by diverse points of view. Qiu
    et al. (2022) presented a detailed review of current studies that use auction
    mechanisms in edge computing. Initially, a brief introduction to edge computing
    is provided, covering three typical edge computing perspectives: cloudlet, fog,
    and mobile edge computing. Then, the work elaborated on the basics and history
    of auction techniques that are often employed in edge computing environments.
    Afterward, it presented a detailed assessment of auction-based methodologies used
    for edge computing, which is classified by auction methodology. Finally, numerous
    unresolved issues and intriguing research avenues are highlighted. Decentralized
    or Distributed Deep Learning (DDL), recently, regarded as one of the facilitators
    of MEC, helps evolving civilization through dispersed model training and worldwide
    shared training knowledge. Sun et al. (2022a) provided a complete overview in
    terms of confidentiality, edge variability, and adversarial threats and countermeasures.
    Furthermore, future DDL developments emphasize themes like efficient utilization
    of resources, asynchronous connectivity, and completely distributed frameworks.
    Arthurs et al. (2022) analyzed the concepts of the use of cloud technology with
    Intelligent Transportation Systems (ITS) and interconnected cars and presented
    taxonomy as well as implementation scenarios. The work concluded by highlighting
    areas where more research is required to enable cars and ITS to employ edge-cloud
    technology in a fully controlled and automated manner. In the cloud and edge computing
    paradigm, effective resource management and pricing is a fundamental problem.
    In recent times, auction framework design has received a lot of attention as a
    solution for dealing with this problem. Jin et al. (2022) provided an in-depth
    examination of cognitive computation offloading, including essential challenges,
    measurements, and future perspectives. Ali et al. (2021) presented an overview
    of MEC architecture, application cases, conceptual principles for MEC security
    infrastructure, privacy and security strategies, and discussed existing and future
    difficulties, their consequences, and solutions. This study investigated important
    dangers, defined the MEC framework, identified vulnerable functional layers, and
    threat groups, and suggested security solutions. To counteract targeted assaults,
    the study briefed several strategies that MEC providers adopt in various levels
    of security safeguarding. Sharghivand et al. (2021) provided a complete assessment
    of the auction-based techniques in the realm of cloud/edge computing. Jiang et
    al. (2021a) performed a complete review of MEC-based video streaming. The associated
    overview and background information are first evaluated. Secondly, resource allocation
    concerns have been raised. The enabling mechanisms for video content streaming
    are described by taking caching, processing, and networking into consideration.
    Following that, a hierarchy of MEC-supported video streaming services is developed.
    Finally, problems and prospective research directions are presented. Ranaweera
    et al. (2021a) examined the MEC system''s privacy and security aspects. It presented
    detailed research on threat vector analysis and detection in the ETSI-prescribed
    MEC architecture. In addition, the work overviewed the vulnerabilities that lead
    to the detected threat vectors and provided viable security solutions to address
    the flaws. MEC''s privacy concerns are also recognized, and specific privacy objectives
    are stated. Finally, the work suggested future instructions to improve MEC service
    privacy and security. Siriwardhana et al. (2021) comprehensively covered the Mobile
    Augmented Reality (MAR) technology as well as its future possibilities concerning
    5G systems and complementing technologies relative to MEC. The research, in particular,
    presented an instructive analysis of existing and future MAR frameworks in terms
    of edge, cloud, hybrid, and localized technological possibilities. The study examined
    significant MAR application domains and their prospects with the introduction
    of 5G wireless communications technologies. The study also evaluated the importance
    of 5G technologies and explored the requirements and constraints of MAR technical
    areas such as connectivity, mobility control, energy governance, task offloading
    and relocation, safety, and privacy. Shah et al. (2021) examined MEC and the slicing
    of networks in the context of 5G-focused application scenarios. Modifications
    to the cloud-native 5G backbone have recently received attention, with MEC application
    cases enabling network scalability, elasticity, adaptability, and automation or
    self-orchestration. A cloud-based micro-service framework is envisioned in terms
    of 5G network slicing. The work also discussed recent breakthroughs allowing end-to-end
    (E2E) network slicing, as well as the supporting technologies and ongoing standardization
    initiatives. Finally, this work outlined unresolved research challenges and offered
    some potential recommendations. Spinelli et al. (Spinelli and Mancuso, 2021) started
    with a review of standardization, with a focus on 5G and NFV, and then moved on
    to the versatility of MEC intelligent resource utilization and its migration possibilities.
    Moreover, this survey investigated how the MEC is being utilized and how it may
    assist industrial verticals. Filali et al. (2020) demonstrated the incorporation
    of MEC into the design of contemporary mobile networks, together with the transition
    strategies to a conventional 5G network infrastructure. It also addressed NFV,
    SDN, and network slicing. Furthermore, the work presented a cutting-edge study
    on the various ways of optimizing MEC capabilities and QoS factors. The work categorized
    various techniques based on the optimum resources and QoS factors (i.e., storage,
    processing, memory, energy, bandwidth, and latency). Ultimately, based on the
    conventional SDN paradigm, the work offered reference architecture for a MEC-NFV
    ecosystem. Mehrabi et al. (2019) reviewed the concept of end-user devices-enhanced
    MEC in depth, i.e., methods that use the capacities of the end device community
    and the deployed MEC to offer services to end devices. The work divided device-enhanced
    MEC strategies into two categories: computation offloading techniques and caching
    strategies. It further subdivided the caching and offloading strategies based
    on the desired performance requirements, which comprise throughput optimization,
    latency reduction, energy saving, utility maximization, and increased security.
    Finally, the work discussed future research possibilities and identifies the key
    limitations of present device-enhanced MEC systems. Game theory (GT) is already
    utilized successfully to construct, develop, and optimize the functioning of numerous
    typical communication systems and networking contexts. Moura et al. (Moura and
    Hutchison, 2019) covered the literature on theoretical games deployed to wireless
    networks, with an emphasis on use cases of forthcoming MEC. Finally, the work
    described potential trends and research areas for using theoretical games in the
    emerging MEC services, taking into account both network design challenges and
    usage scenarios. Porambage et al. (2018) presented a comprehensive overview of
    the use of MEC technologies for the development of IoT applications, as well as
    their synergies. The work described the technical considerations of implementing
    MEC in the IoT paradigm and offered some insight into different integration solutions.
    Taleb et al. (2017) provided an overview of MEC and emphasized the major facilitating
    innovations. It discussed MEC orchestration by taking into account both individual
    applications and a framework of MEC platforms that allow mobility, shedding light
    on the various orchestration and deployment choices. Furthermore, this paper examined
    the MEC standard architecture and primary deployment scenarios that provide multi-tenancy
    for developers, content suppliers, and third parties. Furthermore, this article
    provided a summary of current standardization initiatives and expands on open
    research problems. Shahzadi et al. (2017) overviewed the common edge-cloud computing
    architecture and techniques, along with a quantitative comparison of their classifications
    using several QoS criteria (relevant to networks’ performance and system overheads).
    Taking into account the knowledge gained, techniques examined, and theories addressed,
    the study provided a complete review of current research and future research prospects
    for Multi-Access/Mobile Edge Computing. Michailidis et al. (2022) presented an
    introduction to Unmanned Aerial Vehicle (UAV)-assisted MEC-enabled IoT as well
    as a thorough discussion of use instances and application areas where security
    is paramount. Following that, current research activities on security mechanisms
    for UAV-assisted MEC-aided IoT are fully overviewed. To that aim, information-theoretic
    mechanisms for ensuring proper physical-layer security are examined including
    advanced security measures based on innovations such as Machine Learning and Blockchain.
    Furthermore, research findings on hardware and software-based approaches for network
    node recognition and authentication are described. Finally, the work described
    future possibilities in this research arena, encouraging additional investigation.
    Wei et al. (2022) provided a detailed literature overview on Reinforcement Learning
    (RL)-empowered MEC and presented suggestions for further research. Moreover, challenges
    of the MEC ecosystem linked with unrestricted mobility, interactive channels,
    and dispersed services are outlined, accompanied by how they might be resolved
    using RL solutions in various mobile applications or services. Finally, the open
    research issues are highlighted to offer useful direction for further studies
    in RL retraining and learning. Li et al. (2022a) provided a complete evaluation
    of the progress of the privacy-concerned task offloading in Mobile Edge Computing.
    Offloading privacy problems, as well as associated metrics and application scenarios,
    are examined first. The contemporary privacy-preserving offloading solutions are
    then categorized into three groups based on their related offloading phases: controlling
    offloading content and sequence, secure transfer of computational data, and offloading
    endpoint selection. Finally, future options for privacy-protecting offloading
    are outlined. Feng et al. (2022) provided a complete analysis of the computational
    task offloading in MEC systems, including implementations, offloading priorities,
    and offloading methodologies. It focused on essential difficulties related to
    different offloading objectives, such as delay reduction, energy usage minimization,
    profit maximization, and network utility maximization. The existing issues and
    future possibilities of the task offloading in MEC systems are examined. Pham
    et al. (2020) started the study by providing a comprehensive review of MEC technologies
    and their possible use cases and implementation scenarios. Then, it presented
    the most recent research on the convergence of MEC with emerging technologies
    that will be employed in 5G and even beyond. The work also summarized edge computing
    simulation platforms and experimental assessments, as well as open source initiatives.
    It also highlighted lessons learned from cutting-edge research and explore difficulties
    and potential future avenues for MEC research. Huda et al. (Huda and Moh, 2022)
    examined UAV-assisted MEC systems with an emphasis on computational task offloading.
    To evaluate features and functionality, the work compared the task offloading
    algorithms holistically. Finally, it discussed open challenges and research objectives
    in design and execution. Waheed et al. (2022) presented a detailed review of the
    different vehicular network computing concepts. The study discussed the architectural
    intricacies, similarities, variances, and significant characteristics of each
    computing paradigm. Finally, it presented open research issues in vehicular networks
    as well as prospective research initiatives. Shakarami et al. (2020a) provided
    a comprehensive study on stochastic-based offloading methodologies in diverse
    computing technologies such as Fog Computing (FC), Mobile Edge Computing, and
    Mobile Cloud Computing (MCC), within which a standard taxonomy is provided to
    explore novel mechanisms. Nikravan et al. (Nikravan and Kashani, 2022) presented
    a rigorous overview of Fog-Edge-Cloud (FEC) trust maintenance schemes. To that
    aim, selected FEC trust management options are divided into three broad categories:
    architecture, algorithm, and model/framework. Furthermore, this study reviewed
    and contrasted the FEC trust managing systems based on their advantages and disadvantages,
    evaluation methodology, tools and simulation settings, and key trust measures.
    Finally, certain unresolved concerns and anticipated trends for future investigations
    are mentioned. Bréhon–Grataloup et al. (2022) studied the deployment issues of
    MEC in vehicular networks. The study characterized the contemporary Vehicle-to-Everything
    (V2X) designs to reveal the techniques working behind their enhanced performance:
    network accessibility and coverage, communication reliability, massive data processing,
    and workload offloading. Finally, it highlighted unresolved difficulties and obstacles
    that must be addressed before reaping the full advantages of this paradigm. Lin
    et al. (2020) offered a detailed review of previous and recent developments in
    research areas relevant to offloading models in edge computing. Furthermore, the
    study identified and discussed various research prospects and obstacles in the
    field of edge computing task offloading models. Shakarami et al. (2020b) presented
    an overview of the ML-based computational task offloading techniques in the MEC
    ecosystem. In this sense, it studied classical hierarchical structures to identify
    current mechanisms on this critical topic and identified unresolved challenges.
    Furthermore, the survey concluded with an argument about unresolved challenges
    and unexplored or insufficiently addressed future research tasks. Ray et al. (2019)
    studied the critical role of edge computing technologies in IoT. First, it described
    the taxonomical categorization and evaluated the industrial elements that can
    be benefited from both edge computing and IoT, followed by a detailed discussion
    of each taxonomical element. Secondly, the study described two use cases in which
    the edge-IoT model recently combined to tackle urban smart living difficulties.
    Thirdly, it proposed a unique edge-IoT framework for e-healthcare, known as EH-IoT,
    and created a demonstration testbed. The test findings indicated encouraging benefits
    in terms of reducing reliance on IoT cloud storage or analytics. Iftikhar et al.
    (2023) conducted a Systematic Literature Review, also known as an SLR, using standard
    review methods to examine the function of Artificial Intelligence (AI) and Machine
    Learning (ML) algorithms and the problems in their application for resource administration
    in fog/edge computing settings. Several Machine Learning, Reinforcement Learning,
    and Deep Learning strategies for managing edge AI have been addressed. Furthermore,
    the work discussed the history and current state of AI/ML-empowered fog/edge Computing.
    Furthermore, a hierarchy of AI/ML-empowered approaches to resource management
    for fog/edge technology has been suggested, and current solutions have been compared
    using the proposed taxonomy. Finally, remaining difficulties and intriguing future
    research topics in AI/ML-empowered fog/edge computation have been highlighted
    and explored. Gill et al. (2022) explored current research and probable future
    paths for AI/ML, cloud computing, and quantum computation. Furthermore, the work
    examined the challenges and potential for exploiting AI and ML for next-generation
    computation environments such as edge, fog, cloud, quantum computation, and serverless
    computing. There are several similar sorts of literature available in scholarly
    databases such as IEEE Xplore, Science Direct, Springer Link, ACM Digital Library,
    etc. relative to this survey. However, this work focused on enhancing the existing
    literature by including an overview of contemporary advanced enabling and supporting
    technologies. This survey tried to cover notable enabling and supporting technologies
    for MEC which are not covered or limitedly overviewed in prior works. The work
    moreover included an overview of security issues and state-of-the-art preventive
    mechanisms, which is limitedly included in the existing literature. Through this
    literature review, one can obtain an insight into the MEC and most recent relative
    technologies. Moreover, the review of existing works will be assistive to figure
    out the advancements provided by this work. Table 1 includes a comparison between
    this survey and the mentioned prior survey or review works. Through the table,
    a better insight of the extended contribution of this survey can be obtained.
    Table 1. Comparison table. Survey Topics This Work Akhlaqi and Hanapi (2023) Djigal
    et al. (2022) Liang et al. (2022) Hou et al. (2022) Khan et al. (2022a) Tang et
    al. (2022) Gür et al. (2022) Douch et al. (2022) Tabatabaee Malazi et al. (2022)
    Qiu et al. (2022) Fundamentals & Techs. Fundamentals ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Relative
    technologies ✓ ✓ ✓ ✓ ✓ Standards and Archi. Standardization ✓ ✓ Architecture ✓
    ✓ ✓ ✓ ✓ ✓ MEC in 5G ✓ ✓ ✓ Enab. Techs. Virtual Machines ✓ ✓ Container ✓ ✓ SDN
    ✓ ✓ NFV ✓ ✓ Network Slicing ✓ ICN ✓ ✓ ✓ SFC ✓ ✓ C-RAN ✓ ✓ F-RAN ✓ ✓ H-CRAN ✓ D2D
    ✓ AI/ML ✓ ✓ ✓ ✓ ✓ ✓ Features Computation Offloading ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Communications
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ Caching and Content Delivery ✓ ✓ ✓ Supp. Techs. NOMA ✓ RSMA ✓ UAV
    ✓ SWIPT & EH ✓ IRS/RIS ✓ ✓ Game Theory ✓ Auction Theory ✓ ✓ Digital Twin ✓ ✓ OS
    MEC for 6G ✓ Quantum comp. ✓ PON ✓ Usage Case ✓ ✓ ✓ ✓ ✓ ✓ Security Security issues
    ✓ ✓ Solutions ✓ ✓ Issues & Dir. ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Survey Topics This Work Sun
    et al. (2022a) Arthurs et al. (2022) Jin et al. (2022) Ali et al. (2021) Sharghivand
    et al. (2021) Jiang et al. (2021a) Ranaweera et al. (2021a) Siriwardhana et al.
    (2021) Shah et al. (2021) Spinelli and Mancuso (2021) Fundamentals & Techs. Fundamentals
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Relative technologies ✓ ✓ ✓ ✓ ✓ Standards and Archi. Standardization
    ✓ ✓ ✓ Architecture ✓ ✓ ✓ ✓ ✓ ✓ MEC in 5G ✓ ✓ ✓ ✓ Enab. Techs. Virtual Machines
    ✓ Container ✓ SDN ✓ NFV ✓ ✓ Network Slicing ✓ ✓ ICN ✓ SFC ✓ C-RAN ✓ F-RAN ✓ H-CRAN
    ✓ D2D ✓ AI/ML ✓ ✓ ✓ Features Computation Offloading ✓ ✓ ✓ ✓ ✓ Communications ✓
    ✓ Caching and Content Delivery ✓ ✓ ✓ ✓ Supp. Techs. NOMA ✓ RSMA ✓ UAV ✓ SWIPT
    & EH ✓ IRS/RIS ✓ Game Theory ✓ Auction Theory ✓ ✓ ✓ Digital Twin ✓ OS MEC for
    6G ✓ Quantum comp. ✓ PON ✓ Usage Case ✓ ✓ ✓ ✓ Security Security issues ✓ ✓ ✓ ✓
    ✓ Solutions ✓ ✓ ✓ ✓ Issues & Dir. ✓ ✓ ✓ ✓ ✓ ✓ ✓ Survey Topics This Work Filali
    et al. (2020) Mehrabi et al. (2019) Moura and Hutchison (2019) Porambage et al.
    (2018) Taleb et al. (2017) Shahzadi et al. (2017) Michailidis et al. (2022) Wei
    et al. (2022) Li et al. (2022a) Feng et al. (2022) Fundamentals & Techs. Fundamentals
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Relative technologies ✓ ✓ Standards and Archi. Standardization
    ✓ ✓ Architecture ✓ ✓ ✓ ✓ ✓ MEC in 5G ✓ ✓ Enab. Techs. Virtual Machines ✓ Container
    ✓ SDN ✓ ✓ NFV ✓ ✓ Network Slicing ✓ ✓ ICN ✓ SFC ✓ C-RAN ✓ F-RAN ✓ H-CRAN ✓ D2D
    ✓ AI/ML ✓ ✓ ✓ ✓ Features Computation Offloading ✓ ✓ ✓ ✓ ✓ ✓ Communications ✓ ✓
    ✓ ✓ ✓ ✓ ✓ ✓ Caching and Content Delivery ✓ ✓ ✓ Supp. Techs. NOMA ✓ RSMA ✓ UAV
    ✓ ✓ SWIPT & EH ✓ IRS/RIS ✓ ✓ Game Theory ✓ ✓ Auction Theory ✓ Digital Twin ✓ OS
    MEC for 6G ✓ Quantum comp. ✓ PON ✓ Usage Case ✓ ✓ ✓ ✓ ✓ ✓ Security Security issues
    ✓ ✓ ✓ ✓ ✓ Solutions ✓ ✓ ✓ ✓ Issues & Dir. ✓ ✓ ✓ ✓ ✓ ✓ ✓ Survey Topics This Work
    Pham et al. (2020) Huda and Moh (2022) Waheed et al. (2022) Shakarami et al. (2020a)
    Nikravan and Kashani (2022) Bréhon–Grataloup et al. (2022) Lin et al. (2020) Shakarami
    et al. (2020b) Ray et al. (2019) Iftikhar et al. (2023) Gill et al. (2022) Fundamentals
    & Techs. Fundamentals ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Relative technologies ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
    Standards and Archi. Standardization ✓ Architecture ✓ ✓ ✓ ✓ ✓ MEC in 5G ✓ ✓ Enab.
    Techs. Virtual Machines ✓ ✓ ✓ Container ✓ ✓ SDN ✓ ✓ ✓ NFV ✓ ✓ ✓ ✓ Network Slicing
    ✓ ICN ✓ SFC ✓ C-RAN ✓ ✓ F-RAN ✓ H-CRAN ✓ ✓ D2D ✓ AI/ML ✓ ✓ ✓ ✓ ✓ ✓ ✓ Features
    Computation Offloading ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Communications ✓ ✓ ✓ ✓ ✓ ✓ ✓ Caching and
    Content Delivery ✓ ✓ ✓ Supp. Techs. NOMA ✓ ✓ RSMA ✓ UAV ✓ ✓ ✓ SWIPT & EH ✓ ✓ IRS/RIS
    ✓ Game Theory ✓ Auction Theory ✓ Digital Twin ✓ OS MEC for 6G ✓ Quantum comp.
    ✓ ✓ PON ✓ Usage Case ✓ ✓ ✓ ✓ ✓ ✓ ✓ Security Security issues ✓ ✓ ✓ ✓ ✓ Solutions
    ✓ ✓ ✓ Issues & Dir. ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Motivations: The motivations behind performing
    this survey work are mentioned in the following. • An up-to-date insight into
    the state-of-the-art advances of MEC technology is frequently required to readily
    observe contemporary progress. • Computation and communication resource allocation
    for MEC is an ever-challenging issue. Therefore, up-to-date improvements relative
    to these mentioned terms are required. Since, literature on Game Theory and Auction
    Theory-based computation offloading and resource allocation is limited more and
    more investigations and discussions on these methodologies should be included
    in the novel works and literature. As well as, the deployment of open-source frameworks
    in the context of MEC for the forthcoming networking technologies such as beyond
    5G/6G should be discussed. Moreover, novel multiple access schemes such as the
    adoption of Rate Splitting Multiple Access (RSMA) which is considered to be a
    significant role player for 6G wireless communications should be examined in terms
    of MEC. • A brief overview of modern technologies such as Heterogeneous Cloud-Radio
    Access Networks (H-CRAN), Simultaneous Wireless Information and Power Transfer
    (SWIPT), Intelligent Reflecting Surfaces (IRSs), Digital Twins, passive optical
    networks (PON) and their evolving integration in MEC infrastructure is required
    since in previous literature the descriptions of such technologies in terms of
    MEC are limited or even not present. • Security is one of the significant concerns
    for the MEC infrastructure; therefore, sophisticated research is required relative
    to MEC security. • Evolving challenges and research directions should be devised
    as per advancement or progress of the MEC services. Contributions: The notable
    contributions of this survey work are highlighted below. • The work reviewed (41
    papers) and briefed up-to-date literature (till 2023) to provide a better insight
    into the present studies on the MEC paradigm (included both mobile edge computing
    and multi-access edge computing, however mainly focused on multi-access edge computing).
    • It included a brief overview of relative computing paradigms such as cloud computing,
    nomadic computing, mobile cloud computing, and edge computing technologies such
    as cloudlet computing, mobile ad hoc cloud computing, fog computing, mist computing,
    dew computing, osmotic computing, and MEC. • Standardization, ETSI MEC reference
    architecture, and the integration of MEC in 5G network infrastructure (with a
    brief of 5G-SBA-MEC architecture) are overviewed and briefed. • State-of-the-art
    enabling technologies for MEC including clouds, Virtual Machines, Containers,
    Software Defined Networking (SDN), Network Function Virtualization (NFV), network
    slicing, Information-Centric Networking (ICN), Service Function Chaining (SFC),
    radio access control such Cloud-RAN (C-RAN), Fog-RAN (F-RAN), and H-CRAN, Device-to-Device
    Communication (D2D), machine learning and artificial intelligence approaches are
    discussed. • Requirements and recent advances in the computation offloading, communications,
    caching, and distributed content delivery approaches are described to provide
    a brief insight. • Contemporary or advancing supportive or assistive technologies
    for MEC, namely, multiple access techniques such as Non-Orthogonal Multiple Access
    (NOMA) and RSMA, deployment of UAVs, energy harvesting and SWIPT, implementation
    of IRSs, Game Theory, Auction Theory, adoption of Digital Twin technologies, open-source
    framework (open-source MEC), quantum computing, integration of PON, etc. are discussed.
    • System-aware and user-oriented vast deployment scenarios of MEC are briefed.
    • A brief description of security issues or challenges relative to MEC infrastructure
    such as edge networking level, access network level, core infrastructure level,
    edge device level, MEC system level, and MEC host-level threats are provided.
    Moreover, relevant contemporary threat prevention mechanisms against security
    issues are overviewed. • The survey finally mentioned several evolving challenges
    and research directions relative to the ubiquitous MEC paradigm such as standardization
    (in terms of forthcoming networks, i.e., 6G), energy consumption, efficiency and
    scalability of mobility management network functions, MEC service orchestration
    and programmability, multiple-MEC coordinated collaboration, offloading decision,
    user experience and bandwidth tradeoffs, security, space-air-ground integrated
    network (SAGIN) and MEC, edge intelligence, etc. 3. Relative computing services
    and MEC 3.1. Cloud computing Cloud computing is a paradigm for expanding ubiquitous
    and on-demand transmission accessibility to shared infrastructure (Fatemi Moghaddam
    et al., 2015). Cloud data centers (supplied by cloud vendors such as IBM, Google,
    Amazon, Microsoft, etc.) provide virtual facilities that are widely accessible,
    extensible, and dynamically reconfigurable; this reconfigurability enables cloud
    computing to deliver solutions adopting a pay-according-to-usage pricing system.
    Users may quickly access remote computational facilities and data processing services
    using the pay-according-to-usage pricing system, and clients are simply charged
    for the number of services they utilize. The cloud provides Software-as-a-Service
    (SaaS), Platform-as-a-Service (PaaS), and Infrastructure-as-a-Service (IaaS) functionalities.
    Developers can employ many different services depending on the needs of the applications
    they create. The goal of cloud computing at first was to provide users with access
    to computational resources for ubiquitous computing. Despite cloud computing technology
    having aided in accomplishing this goal, accessing cloud-based services may take
    a longer processing time and which is not viable for some vital applications or
    services demanding very low latency. Because of the expanding growth of mobile
    devices and the volume of data produced at the edge of the network, cloud services
    must be located near the place where the data is produced. Due to the increased
    need for higher bandwidth, reduced latency, spatially dispersed, and privacy-aware
    content, computing models that may be deployed in the close vicinity of connected
    devices are required to satisfy the aforementioned demands. Edge computing has
    indeed been presented as a solution for those requirements. Following that, this
    work described and compared several computing concepts (Kamboj and Ghumman, 2016).
    3.2. Mobile or nomadic computing The evolution of cloud computing has been impacted
    by the evolution of mobile computing (MC) (Barbara, 1999). In the case of mobile
    computing (also known as the nomadic computing (Ahson and Mahgoub, 1998), (Yu
    et al., 2016) paradigm), computing is conducted using mobile and portable equipment.
    Pervasive context-aware services, for example, location-based reminders, are examples
    of mobile computing. The decentralized computing infrastructure of mobile computing
    is advantageous. Mobile devices may function in dispersed places because of their
    framework. However, mobile computing has numerous limitations such as inadequate
    resource limitations, transmission latency, the equilibrium between individuality
    and reliance or interoperability (common in all decentralized systems), and mobile
    clients'' requirement to seamlessly adapt to frequently changing scenarios. Because
    of these constraints, mobile computing is generally not suited for programs with
    low latency or resilience requirements or applications that create, analyze, and
    store significant volumes of data on devices. Fog and cloud technology have expanded
    the extent and accessibility of mobile computing (Bittencourt et al., 2018). In
    cloud and fog computing paradigms, the processing is not restricted to a local
    networking system. Mobile computing simply necessitates the use of mobile device
    hardware. Nevertheless, fog and cloud processing require more powerful hardware
    as well as virtualization capabilities. Mobile computing integrity should be provided
    by protecting mobile devices. Mobile computing has limited capabilities than fog
    and cloud technology; however, recent developments in wireless interfaces and
    mobile technology have significantly narrowed this gap. 3.3. Mobile cloud computing
    Mobile Cloud Computing technology (MCC) (van der Westhuizen and Hancke, 2017)
    is the integration of mobile computation with cloud services in which data computation
    and storing occur in the cloud away from mobile devices. MCC is concerned with
    the interaction between cloud platform vendors and cloud service customers (Qureshi
    et al., 2011), (Bale et al., 2021). MCC allows resource-constrained user devices
    to access the vast capabilities of cloud computing. The majority of MCC processing
    has gone from user devices to the cloud. MCC can execute programs with intense
    computations and extends the battery life of mobile devices. MCC has utilized
    the advantage of the specifications and capabilities of both cloud and mobile
    computing. In MCC, computing facilities are highly available due to the use of
    a mix of cloud computing and mobile computing, as opposed to mobile computing
    which is resource restricted. This is advantageous for applications requiring
    a high level of computing, such as mobile augmented and virtual reality. Cloud-based
    solutions are far more accessible in MCC than mobile computing. Although user
    devices can do computing in MCC, MCC depends on cloud services to execute high-computation
    operations. MCC provides privacy on mobile devices as well as the cloud. MCC is
    subject to the same constraints as the cloud and mobile computing technologies.
    MCC is based on a centralized design that may not be suitable for applications
    that require high prevalence. Moreover, enabling cloud-based capabilities in both
    cloud technology and MCC requires a Wide Area Network (WAN) connection; hence,
    applications operating on these platforms must be constantly linked to the Internet,
    posing connectivity difficulties. Moreover, as previously noted, offloading processing
    to the cloud causes excessive latency, which is incompatible with delay-sensitive
    activities. 3.4. Edge computing Edge computing (Cao et al., 2020) has improved
    storage, supervision, and processing capabilities of data produced by linked devices.
    Edge computing, contrary to MCC, has now been positioned near the end-devices
    at the network''s edge. Edge computing technology, as per description of OpenEdge
    Computing, performs processing at the network''s edge using tiny data centers
    near consumers (Cao et al., 2021). The primary goal of edge computation is to
    offer computational and storage facilities in the close vicinity to the users
    in a ubiquitous manner. Edge computing technology is a critical computing paradigm
    for end devices in which data is filtered, preprocessed, and aggregated utilizing
    cloud centers located near end devices. The following are the primary benefits
    of edge computing (Shirin Abkenar et al., 2022): The Reduction of Latency: Because
    of its closeness to clients, edge computing has lower latency than MCC and cloud
    technology. Yet, if the local processing unit is insufficiently powerful, the
    delay in edge computing might be greater than it is in the cloud and MCC services.
    Increased System Performance: Attaining millisecond-level data analysis is the
    most crucial feature of edge computing. Edge computing minimizes total system
    latency and transmission bandwidth consumption while improving overall system
    effectiveness. Enhanced Privacy and Security: Integrated cloud service providers
    supply their clients with a complete system of unified data intrusion prevention
    solutions. Yet, if centrally stored data is disclosed, significant consequences
    will emerge. The edge computing framework, for instance, enables the deployment
    of the most relevant security precautions locally (at a local edge computing server).
    Therefore, the majority of the computation may be conducted on the network''s
    edge, comparatively with a lower volume of data. As a result, it decreases the
    danger of information leaks during transmissions and the quantity of data retained
    on the cloud service, lowering security and privacy issues. Enhanced Service or
    Resource Accessibility: The accessibility of resources in edge computing is also
    improved. Edge computing, in comparison to MCC, incorporates tiny data centers,
    whereas MCC does not comprise a data center. As a result, service availability
    is greater with edge computing. Moreover, since edge computing may construct hybrid
    peer-to-peer and cloud services paradigms, it benefits from greater processing
    capabilities than MCC. Minimize Operating Expenses: Directly moving data into
    a cloud system imposes significant operational expenses for data transmission,
    adequate bandwidth, and latency factors. Edge computing, on the other hand, can
    minimize data uploading volume; as a result, data transfer volume, bandwidth usage,
    and latency will be reduced, lowering operating expenses. Resiliency to Connection
    Issues: When some computational activities can be performed immediately on the
    edge, services are not impacted by restricted or inconsistent network connectivity.
    This is especially useful for executing programs in remote environments with limited
    network access. It can also help to lower the high expenses associated with networking
    solutions such as cellular technology. 3.4.1. Cloudlet computing Carnegie Mellon
    University presented the very first edge computing idea in 2009, putting computation/storage
    near user devices (Satyanarayanan et al., 2009). In certain research, cloudlet
    is termed as a micro data center (MDC). The cloudlet (Babar et al., 2021) concept
    is based on strategically positioning a server or a group of servers with trustworthy
    high capacities and processing power, as well as a robust Internet connection,
    near edge devices to offer both storage and computing capacity for such nearby
    UDs. Cloudlets constitute small-scale data warehouses (miniaturized clouds) that
    are typically one hop away from user devices. Cloudlet computing offloads computations
    from user devices to virtualized machine (VM)-based cloudlets located at the network''s
    edge (Lewis et al., 2014). Cloudlet is a miniature cloud located around user devices.
    Cloud solution providers who intend to deliver accessible services close to mobile
    devices might be named as cloudlet technology operators as well. Cloudlets can
    also be facilitated by telecom operators such as AT&T and others, with virtualization
    capability positioned close to user devices. Cloudlets typically have small-scale
    hardware sizes in contrast to massive data facilities in cloud computing. Since
    cloudlets are smaller in size, computational resources are more modest, but energy
    consumption and latency are lower compared to cloud computing. One limitation
    of cloudlet technology is that accessibility is only gained through Wireless Local
    Area Network (WLAN) connections, and mobile devices need to toggle between the
    WLAN and cellular network to obtain cloudlet services. Moreover, cloudlets are
    not necessarily an intrinsic component of the cellular network, and WLAN generally
    offers local service with restricted mobility support, making it difficult to
    achieve an acceptable service quality for mobile user devices. Cloudlets and mobile
    cloudlets are identical concepts. In the context of mobile cloudlets, the cloudlets
    are termed as a cluster of nearby interconnected mobile devices. In which they
    communicate with each other through wireless communication technologies, for example,
    by Bluetooth or WLAN. These devices can be suppliers or customers of computing
    services. 3.4.2. Mobile Ad hoc cloud computing MCC is widespread; however, it
    is ineffective without centralized clouds. Ad hoc mobile connectivity is a transient
    and dynamic system of nodes formed by routing and transmission protocols. The
    most dispersed kind of system is Mobile Ad Hoc-Based Cloud Computing (MACC) (Yaqoob
    et al., 2016). Mobile devices generate a very dynamic network structure in MACC;
    system adaptation is required for additional devices to often leave/join the system.
    Ad hoc mobile devices may also form clouds to support communication, storage,
    and computation. The application scenarios of MACC include unmanned vehicular
    technology, group live video broadcasting, sensor networks (Yaqoob et al., 2016),
    (Zhu et al., 2015), (Chowdhury, 2021), etc. In the case of cloudlets, virtualized
    computation by Virtual Machines is necessary, whereas it is not required in MACC.
    Both cloudlet technology and MACC offer mobility; but, real-time IoT services
    are not feasible in resource-constrained MACC. Computations in the cloudlet have
    been distributed among cloudlet nodes located near mobile devices to provide local
    services to mobile clients. Since MACC resources are ad hoc in nature (Conti and
    Giordano, 2014), it is fundamentally different from cloud services. Mobile devices
    in MACC serve as storage devices, data or content providers, and computational
    devices. Due to the confined network infrastructure, they also govern traffic
    routing among each other. As localized resources (by end devices) have been integrated
    to form an ad hoc cloud, MACC may provide high computation capacity. In centralized
    cloud services, these characteristics are distinctive in terms of clients, connectivity,
    and architecture (Khalifa et al., 2014). 3.4.3. Fog computing Fog computing is
    another kind of edge computing (Mukherjee et al., 2018). Cisco launched the fog
    computing framework (termed as “fog” in several researches) in 2012 to facilitate
    the processing of tasks for user devices at the network''s edge (Habibi et al.,
    2020). Decentralization of the cloud computing infrastructure deploying fog is
    done by offering the computation or task processing features between the cloud
    and edge devices. This brings content, computing, storage, and services near the
    user devices where the data has to be processed. This enables the formation of
    fog outside the cloud infrastructure and a reduction of data transmission latency.
    The primary distinction between fog and cloud computing is the size of the hardware
    elements associated with these computing paradigms (Raza et al., 2020). Cloud
    technology provides highly accessible computational facilities with comparatively
    high power consumption, whereas fog computing offers intermediate accessible computing
    resources with reduced power usage. With cloud computing, massive data centers
    are often used, but in fog computing, smaller servers, gateways, routers, switches,
    set-top devices, or access nodes are used. Since fog computing hardware consumes
    relatively lesser area than cloud computing hardware, it may be positioned closer
    to consumers. Fog computing is accessed through linked devices from the network
    edge to the core of the network, whereas cloud computing is accessed through the
    network core. Moreover, fog-based services may operate without constant internet
    access, implying that the services can function independently, with essential
    updates being delivered to the cloud whenever there is a network connection. In
    cloud technology, however, devices must be linked to the network while the cloud
    platform is operating. Computing, connectivity, storage, management, and decision-process
    have been dispersed closer to devices in fog, allowing them to supervise, analyze,
    interpret, evaluate, and respond more effectively. Fog computing has the potential
    to improve many industries, including energy, industry, transport, healthcare,
    intelligent cities, and so on. When fog is contrasted with MACC, MACC is better
    suited for extremely dispersed and dynamic networks where there is no network
    connection. Because linked devices in MACC are primarily decentralized, they form
    a more flexible network. Cloudlet computing technology, on the other hand, works
    effectively with the mobile or dynamic cloudlet structure, but fog computing can
    accommodate massive quantities of traffic; moreover, resources in the fog may
    be situated anywhere along the device-to-cloud path. Although both execute storage
    and computation on the edge of the network near end users, fog computing and edge
    computing are distinct concepts. The OpenFog Consortium (Yannuzzi et al., 2017),
    (Kuo et al., 2018) defines this separation by the hierarchical structure. Fog
    computing allows computation, communication, storage, management, and acceleration
    anyplace along the cloud server-to-things channel or path; while, the processing
    is only conducted at the edge of the network in the context of edge computing.
    The main disadvantage of the aforementioned edge computing ideas (cloudlet, ad
    hoc clouds, and fog computing) is that typically, they are not incorporated into
    the framework of a wireless network; hence, the QoS and QoE for end users are
    not assured. The cloud radio or wireless access network (C-RAN) is one notion
    that can combine cloud services into a wireless network (Chabbouh et al., 2017).
    The C-RAN employs the concept of a dispersed protocol stack, shifting some levels
    of the protocols to the central baseband unit (BBU) from dispersed remote radio
    heads (RRHs). Fog RAN/F-RAN (Ku et al., 2016) is a type of fog computing framework
    that may also be integrated with wireless communication technologies. F-RAN and
    C-RAN are both suitable to be deployed within the base stations of wireless networks;
    these can be utilized in 5G-related wireless technology implementations and are
    significantly energy efficient. 3.4.4. Mist computing Mist computing (Sattari
    et al., 2020) is recently introduced, which represents decentralized cloud or
    computing technology at the farthest edge of linked devices. Mist computation
    is the first computing step in the IoT-fog-cloud chain; it is colloquially known
    as “IoT computation” or “things computation.” An IoT device might be a wearable
    smartwatch, a smartphone, an intelligent fridge, or any smart device. Mist computing
    technology is the extension of computation, storage, and networking over the fog
    via objects. Mist computation is a subset of MACC because the communication in
    the mist is rarely ad hoc. According to research, mist computing technology can
    minimize the strain in existing WLAN networks for video broadcasting applications,
    protect users'' confidentiality through local processing, and quickly deploy virtualized
    implementations on single-board computer systems. Apart from the computing paradigms
    discussed earlier, the cloud of things and edge clouds are two more related computing
    paradigms described in various works. 3.4.5. Dew computing Dew computing technology
    is a computing paradigm that first appeared in 2015 (Rindos and Wang, 2016), (Sojaat
    and Skalaa, 2017). This kind of computing is concerned with the establishment
    of collaborative connectivity between Cloud Services and end-user devices. This
    integration enables resources to be transferred between two devices or components
    based on network circumstances. 3.4.6. Osmotic computing Osmotic computation is
    an emerging computing typology that enables efficient IoT application processing
    at the edge of the network (Maksimović, 2018). Such a paradigm is based on the
    requirement to connect micro-cloud services provided at the edge with the massive-capacity
    cloud servers. Osmotic computation is identified by the connection of edge, fog,
    and cloud computing enabling the smooth and free flow of micro services among
    them (Villari et al., 2016). 3.4.7. Multi-access edge computing As per ETSI standardization,
    MEC is indeed a platform that provides features for cloud processing and an IT
    customer experience within RAN (namely at the cellular edge) of wireless networks
    for users. In the context of MEC, edge computing capabilities can be integrated
    into existing base stations by RAN operators besides other available options of
    edge-level deployments. MEC computing systems are compact and have virtualization
    capabilities. Due to the nature of hardware (limited capacity), accessible computing
    capabilities in MEC are limited as compared to cloud computing. Additionally,
    MEC can serve low-latency services along with delay-sensitive crucial applications
    (Sindjoung et al., 2022). MEC services provide mobile users with tailored and
    contextualized experiences because they leverage real-time networking information.
    Networking of MEC has been established by WLAN, wide area network (WAN), or mobile
    networks. MEC mainly focuses on RAN-based communications infrastructure operators.
    The 5G and beyond communication infrastructure is expected to help MEC greatly
    since it can handle a wide range of mobile devices with reduced latency and greater
    bandwidth (Nakazato et al., 2019). Moreover, SDN and NFV as well facilitate MEC
    capabilities. Table 2 contains the technical features or brief description, advantages,
    and disadvantages of the aforementioned computing paradigms. Table 2. Technical
    features, advantages, and disadvantages of the computing paradigms. Computing
    Paradigms Technical Features/Description Advantages Disadvantages Cloud Computing
    • Centralized deployment • Accessible through WAN • SaaS, PaaS, and IaaS functionalities
    • Data processing happens far away from users • Multiple hops away • Massive computation
    capacity • Dynamically reconfigurable • Ubiquitous computing • Massive storage
    capacity • Higher latency • Requires higher bandwidth • Threat of massive data
    loss at the time of security failure • Higher deployment cost Mobile or Nomadic
    Computing • Computing is conducted using mobile and portable equipment • Context-aware
    services, e.g., location-based reminders • Decentralized deployment • Typically
    accessible through WAN • Dispersed computing facilities • Computational resource
    limitations • Transmission latency • Equilibrium between individuality and reliance
    or interoperability • Adaptability for users • Limited storage capacity • Requirements
    of sophisticated hardware at user end Mobile Cloud Computing • Integration of
    mobile computation with cloud services • Allows access to the vast capabilities
    of cloud computing • Centralized deployment • Typically accessible through WAN
    • Offers capabilities of both cloud and mobile computing • Highly available due
    to the mix of cloud and mobile computing • Advantageous for mobile augmented and
    virtual reality • Higher latency compared to the mobile or nomadic computing (cloud-dependent)
    • Constant internet link is required • Ineffective without centralized cloud Cloudlet
    Computing • Also termed as micro data center (MDC) • Cloudlets constitute small-scale
    data warehouses • Typically one hop away • Distributed deployment • Computation/storage
    near the user devices • Accessible through WiFi/WLAN • Private facility • Offloads
    computations to virtualized machine • Latency-awareness • Energy consumption and
    latency are lower compared to cloud computing • Supports mobility • Limited mobility
    since it is only accessible through WLAN/WiFi • Usually not incorporated into
    wireless networks Mobile Ad Hoc Cloud Computing • Decentralized/distributed •
    Forms device cluster and share computational resources • Mobile in nature • Virtual
    computation is not required in MACC like cloudlets • Highly mobile • Transient
    and dynamic • Real-time services, e.g., IoT are not feasible in resource-constrained
    MACC • Usually not incorporated into wireless networks Computing Paradigms Technical
    Features/Description Advantages Disadvantages Fog Computing • Decentralized/distributed
    • Private deployments • Accessible through wireless access points • Intermediate
    accessible computing with reduced power • May operate without constant internet
    access • One hop away • Latency-awareness • Lower energy consumption • Accommodate
    massive quantities of traffic than cloudlets • Low cost • Flexibility is lower
    than the MACC • Usually not incorporated into the wireless networks, however,
    can be integrated via Fog-RAN • Certain security concerns Mist Computing • Decentralized/distributed
    • First computing step in the IoT-fog-cloud chain • Colloquially known as “IoT
    computation” or “things computation” • Accessible through WLAN • Latency-awareness
    • Lower energy consumption • Confidentiality through local processing • Virtualized
    implementations on single-board computer systems • Highly suitable for IoT and
    sensor networks • Limited computational capacity • Typically storage facilities
    are not readily available Dew Computing • Decentralized but dependent on central
    cloud • Collaborative connectivity between Cloud Services and end-user devices
    • Offers certain cloud facilities to the edge • Limited computation capacity •
    Dependency on central cloud • Usually requires a reliable connectivity • Typically
    storage facilities are not readily available Osmotic Computing • Decentralized/distributed
    • Enables efficient IoT application processing at the edge • Connects micro-cloud
    services with the massive cloud servers • Identified by the connection of edge-fog-cloud
    enabling the smooth flow of micro services among them • Suitable for IoT services
    • Latency-awareness • Lower energy consumption • Limited computational capacity
    • Typically storage facilities are not readily available Multi-Access Edge Computing
    • Decentralized/distributed • Mostly accessible through WAN or cellular networks
    • Typically implemented in RAN. However, there are other few options of implementation.
    • Offers highly near user computing facilities • Latency-awareness • Lower energy
    consumption • End-user friendly • Tailored experiences leveraging real-time network
    information • Computational and storage facilities near the user premises • Has
    virtualization capabilities • Reduced threat of information leaks during transmissions
    due to the lower volume of data • Computing and storage capabilities are limited
    than cloud computing • Certain security issues 4. MEC standardization and architecture
    4.1. Standardization ETSI authored the implementation of MEC technology in the
    networking functions virtualization (NFV) architecture in February 2018 (ETSI.
    Network, 2023), (Canto et al., 2021). Furthermore, ETSI released use instances
    and requirement standards on MEC in October 2018 which includes an annex outlining
    sample use cases and associated technical benefits. ETSI has released the MEC
    frameworks and reference architecture definition, which explain the functional
    parts and the points of reference amongst them, as well as several MEC services.
    ETSI released the Proof of Conception (PoC) model in July 2019, which was approved
    by the MEC ETSI ISG. Moreover, ETSI published a paper regarding MEC-5G integration
    (ETSI, 2018). MEC allows applications to be implemented as software-only units
    that operate on the upper end of a virtualized environment placed near the edge
    of the network. ETSI MEC''s infrastructure contains system-, host-, and network-level/tier/layer
    elements (ETSI, 2019). The network-level enables connectivity to a range of resources,
    while the host level offers the virtualized framework and MEC architecture, allowing
    MEC applications to be executed. The system-level administration offers an overview
    of the base MEC system, allowing user devices and third-party partners to get
    access (Cruz et al., 2022). Moreover, D2D transmission is direct connectivity
    between two mobile devices that do not require the usage of the network core or
    access points, or base stations. End devices’ resource exchange has been examined
    using D2D-aided MEC (Seng et al., 2019). The combination of MEC and D2D can boost
    the computation capability of cellular networks by assisting end devices with
    processing and storage capabilities to interact with traditional MEC architecture.
    Typically, end devices, small cell base stations, and a macro cell base station
    comprise the D2D-aided MEC infrastructure. In this instance, the end devices can
    delegate work to a neighboring MEC or a cluster of D2D nodes. 4.2. ETSI MEC reference
    architecture Low latency, closeness, position awareness, high throughput, and
    real-time exposure to radio network parameters are distinguishing properties of
    the MEC framework. These features enabled expedited content delivery functions
    and applications to be supplied reasonably close to end users or subscribers at
    the edge of the wireless network. The satisfaction of mobile subscribers may be
    considerably increased by more effective network and service management, greater
    service quality, decreased data transportation expenses, and lowered network congestion.
    Fig. 2 illustrates the ETSI MEC framework. Download : Download high-res image
    (353KB) Download : Download full-size image Fig. 2. ETSI MEC framework. The MEC
    reference model (Giust et al., 2017), (Filippou et al., 2020) features are briefly
    described below: MEC Host and Communications System: The control process of the
    MEC standard design represents the host layer. It consists of MEC host (MEH) and
    host maintenance entities. The MEH is made up of the MEC platform, services, and
    Virtualized Infrastructure. The Virtualized Infrastructure hosts MEC services
    and applications by providing the computation, communication, and storage resources.
    The network layer connects the external and internal elements. MEC Platform: The
    MEC platform enables MEC services to be hosted as operations on the virtual platform.
    According to the request of the MEC system controller, the MEC platform is also
    accountable for the initialization and suspension of MEC applications. MEC Supervisor:
    The MEC orchestrator or supervisor is a system-level managing layer that consists
    of a platform administrator and a Virtualized Infrastructure Manager (VIM). Its
    core duties include applications and services provisioning using virtualized MEC
    resources, MEC resource information maintenance (such as topologies, available
    MEH services and resources), and security and authentication checks for MEC applications.
    The MEC orchestrator or supervisor is also in charge of policy execution. Operation
    Support or Assistance Sub-System: Operation Support Subsystem (OSS) is in charge
    of providing permission to user subscription queries submitted by User Equipment
    (UE) through the application-level coordination and management gateway. Fig. 3
    depicts the ETSI MEC reference architecture. Download : Download high-res image
    (1MB) Download : Download full-size image Fig. 3. ETSI MEC reference architecture.
    4.3. Integration of MEC in 5G network infrastructure MEC is seen as a crucial
    enabler, allowing operators to incorporate application-oriented features into
    their networks. This will enable operators and service vendors to deal with critical
    latency situations. MEC deployment might be accomplished in a variety of settings
    where network design and generation have no bearing on the implementation. It
    is essential to mention that MEC technologies are not confined to 5G, although
    it is a crucial component for enabling and facilitating 5G (Parada et al., 2018),
    (Rahimi et al., 2021). MEC is a universal access solution that provides reduced
    latency wherever necessary, e.g., in situations requiring local interaction, such
    as automated vehicles (Passas et al., 2021). After the introduction of MEC concepts,
    the ETSI ISG and numerous value chain stakeholders worked hard to formulate MEC
    standards based on industry agreements. At that moment, the ETSI consortium had
    68 affiliates and 35 partners, including not only mobile carriers but also industries,
    network operators, and institutions such as the University Carlos III of Madrid,
    Vodafone, Intel, IBM, NTT Corporation, and others (5G Coral, 2018). Their participation
    is crucial for establishing an open and adaptable MEC ecosystem, and MEC benefits
    a wide range of stakeholder groups, such as mobile network operators (MNOs), developers,
    over-the-top (OTT) participants, individual application suppliers, network equipment
    distributors, IT system operators, network operators, and technology companies.
    The ETSI ISG also released a series of specifications and guidelines focused on
    topics such as architecture and framework, MEC within the NFV system, and MEC
    and C-RAN collocation. With the technical framework 3GPP TS 23.501 (GPP. 3GPP
    TS 23, 2017), the 3GPP started introducing MEC in the definition of 5G infrastructure.
    The 3GPP recently detailed how to install and effectively incorporate MEC into
    5G. Fig. 4 visualizes MEC''s integration with 5G infrastructure. Download : Download
    high-res image (438KB) Download : Download full-size image Fig. 4. MEC''s integration
    with 5G network infrastructure. One may identify two types of functions in the
    5G Service Based/Oriented Architecture (SBA/SOA) suggested by 3GPP (Lu et al.,
    2019), (Xia et al., 2019): (i) those which utilize one or more services and (ii)
    others that provide services. The interchange of resources (produce/consume) is
    dependent on authentication procedures that provide approval or endorsement to
    the consumers. SBA/SOA enables service access efficiency and flexibility. The
    request/response paradigm is one of the strategies used for basic and compact
    service requests. The architecture supports a subscribe/notify approach for lengthy
    operations to improve the efficiency of sharing services and information across
    entities. MEC ETSI ISG presents a comprehensive guideline for developing services
    that facilitate such functions and attributes in MEC. This proposed standard provides
    the same characteristics for MEC applications that SBA does for network functionalities
    and related services (enrollment, detection, accessibility, verification and permission,
    and so on). The integrated implementation of MEC systems in a 5G environment necessitates
    the interaction of certain functionalities of MEC in 5G with network functions.
    Network functionalities and services are recorded within Network Resource Function
    (NRF) unit, whereas the services generated by requests in MEC are enrolled in
    the MEC system''s service registration portal. For using a service, approval from
    an orchestrator or authenticator is necessary. Moreover, it allows communications
    with the network functionality. The Authenticating/Identification Server Function
    (AUSF) grants this type of permission. The NRF proposes the finding of accessible
    services. Network Exposure Function (NEF) serves the same purpose as NRF in some
    circumstances when services need to be reached by exterior and untrustworthy entities.
    NEF might be viewed as a centralized body for service exposure, approving all
    types of requests received from beyond the system. Apart from Application Function
    (AF), NEF, and NRF, there have been a few additional entities or functions worth
    mentioning. In a 5G system, the Policy Control Function (PCF) unit is in control
    of policies and regulations such as traffic redirection rules. According to the
    AF''s level of confidence, the PCF might be accessible through NEF or independently.
    The Unified Data Managing (UDM) function is in charge of several services connected
    to users and subscribers. It produces credentials to authorize users, manages
    user recognition, and access controls (such as roaming), records the users'' assisting
    NFs (assisting AMF, Session Maintenance Function (SMF)), and ensures service sustainability
    through a record for Data Network Name (DNN)/SMF allocations. From the standpoint
    of the MEC platform, the User Plane Function (UPF) is a decentralized and programmable
    data plane. As a result, under some deployment circumstances, the local UPF might
    participate in the MEC implementation (Wang et al., 2022a). To incorporate MEC
    in a 5G SBA (Li et al., 2019a), (Wang et al., 2022b), unique functional facilitators
    are designed, which may be characterized as follows: Selection and Re-selection
    of User Planes: The 5G central network allows UPF selection or reselection for
    targeted traffic forwarding to the data connection. The UPF preference mechanism
    parameters are determined by the UPF implementation context and MEC service provider
    configuration. Traffic or Packet Forwarding and Steering: In the 5G network, the
    UPF offers multiple packet routing techniques for MEC operations. Moreover, AFs
    can influence UPF selection or reselection and provide custom traffic forwarding
    rules for a single user. Local Area Packet/Data Network (LAPN/LADN): The adaptability
    of the UPF placement enables LAPN/LADN functionality. MEC hosts may then be installed
    on the N6 interface, which connects the UPF to a data connection. Based on LAPN
    statistics obtained from the AMF, a user adopting MEC facilities may identify
    LAPN accessibility during the enrollment process. Sessions and Services Continuity
    (SSC): SSC functionality is required to facilitate the users'' and applications''
    mobility. The 5G infrastructure enables MEC applications to choose between three
    SSC configurations. SSC approach 1 in particular offers the user with the consistent
    network connection, SSC approach 2 may terminate the user''s present connectivity
    before establishing a newer one, whereas SSC approach 3 maintains continuity of
    service for the subscriber by activating the new user interface before disconnecting
    the previous one. Network Capacity Exposure: The 5G framework enables MEC''s immediate
    access to network functionalities as well as indirect accessibility through the
    NEF. Exposed functionalities include the disclosure of user events, the delivery
    of user actions to external services, and the disclosure of analytics to third
    parties. QoS and Pricing Functions: The QoS and pricing criteria for user traffic
    or packet directed to the LAPN are defined by the PCF within 5G SBA. 5. Enabling
    technologies 5.1. Clouds, Virtual Machines, and Container Cloud computing delivers
    large computational capabilities, always-on availability, and easy access while
    decreasing the necessity for end subscribers to administer, monitor, and facilitate
    software and hardware. It also features shared pools of resources with dynamic
    scaling. There are four main technology models available, as well as three service
    types. 5.1.1. Technology models Public Cloud: This type of model is administered
    by a cloud vendor to give public subscribers access to a set of resources in some
    kind of pay-per-use manner (i.e., Dell, Microsoft, and Amazon) (Li and Wan, 2018).
    Private Cloud: It is entirely owned and administered by a business. To maintain
    security, privileged access is offered within the corporate network (i.e., Google,
    Citrix, and RackSpace) (Wang et al., 2020a). Hybrid Cloud: It is a fusion of both
    public and private clouds (i.e., IBM, HP, and VMWare) (Xu et al., 2018). Community
    Cloud: It is a cloud resource pool made up of numerous providers that may be shared
    by a specified group (Baig et al., 2015). 5.1.2. Service models Platform-as-a-Service
    (PaaS): Users are provided with a platform for designing, deploying, and managing
    applications (i.e., Azure websites and Amazon Beanstalk) (Lv et al., 2010; Kahvazadeh
    et al., 2022; Castro et al., 2016). Infrastructure-as-a-Service (IaaS): Offers
    scalable virtualized computing infrastructure, including computation, storage,
    and connectivity (i.e., Azure VMs, EC2, and Amazon) (Castro et al., 2016). Software-as-a-Service
    (SaaS): Provides cloud-hosted software that is readily available (i.e., Dropbox
    and Google Sheets) (Castro et al., 2016). Virtual Machine: A cloud-based system
    is often comprised of a cluster of physical devices that constitute a single logical
    object that may be shared among several players doing discrete isolated tasks
    or jobs. One method is to use a hypervisor that can generate and run Virtual Machines
    (VMs) that also can host various processes (Liumei et al., 2016). Virtual Machines
    segregation provides users with an isolated environment, i.e., a fully working
    computer, regardless of the foundational equipment (Li et al., 2022b). The VM
    technology provides fine-grained supervision for initializing and terminating
    activities and services at any moment without impacting the underlying hardware,
    allowing for more resource provisioning versatility. Virtual Machine, however,
    is an artificial construct of physical equipment stack (i.e., virtual BIOS, networking
    interface, storage, RAM, and CPU) that necessitates a complete guest operating
    system (OS) image as well as extra packages and modules for hosting services and
    programs. A Virtual Machine wastes a substantial amount of facilities unless a
    service or application requires or demands such infrastructure, in addition to
    its delayed starting time due to booting a full operating system. Container: In
    the context of the Containers (Pahl et al., 2019), abstractions occur at the operating
    system level, supporting applications and libraries, as well as systems resources
    to operate a particular service or application. Containers split the resources
    of actual computers, resulting in numerous separate user-space instances that
    are substantially smaller in size than Virtual Machines. This enables several
    Containers to run under a single operating system, enabling faster implementation
    with nearly native efficiency in central processing unit (CPU), memory, storage,
    and networking. A Container often executes a service or application by enabling
    quick instantiation and rapid migration benefits owing to its lightweight architecture;
    however, it is less reliable than Virtual Machines, which can also run several
    applications more effectively. In actuality, a Container may be orchestrated in
    milliseconds, but a Virtual Machine might take seconds or even minutes, according
    to the capabilities of the operating system, actual equipment, and the system
    workload. Containers provide a lightweight virtualization approach that enables
    the portable execution of MEC functions, which is very relevant for mobile consumers
    (Jin et al., 2019a). Containers can also help MEC services since they provide
    methods for rapid packaging and deployment across a wide number of linked MEC
    platforms (Barbarulo et al., 2022). Containers outperform Virtual Machines in
    the domain of MEC in five ways. First, Containers generate layers of images and
    extend images to develop and create programs, which are then recorded as images
    (‘.img’ system image files) in storage. The Container processor or engine later
    uses this to execute the program on the host. The engine aids in packaging, distributing,
    and coordinating, allowing for rapid deployment. Second, Container API supports
    life-cycle management, which includes building, defining, composing, distributing,
    and executing Containers. Third, storage is given by connecting one or even more
    data volume Containers with continuous service. Fourth, networking is accomplished
    by port mappings, which facilitate Container interconnection. Finally, compatibility
    for micro services-based architecture (i.e., discrete software packages of a loosely
    connected service that may be easily mapped to construct a business application
    according to need) makes containerization easier under the PaaS paradigm. Docker
    is the most popular Container technology for facilitating an edge computing context
    (Reis et al., 2022). Containers may also be utilized to replace Virtual Machines,
    resulting in lower resource use concerning both storage and computation. Moreover,
    Apache Mesos (Xue et al., 2017) and Google Kubernetes (Botez et al., 2020) support
    the Container cluster management inside dispersed nodes, allowing for quicker
    scalability. Regardless of the advantages of Containers, which are more visible
    in settings with user maneuverability, Virtual Machines may host larger applications
    or programs or services or a collection of applications or programs or services
    connected with a specific third-party vendor, ensuring a better level of security.
    The functionality of Virtual Machines can indeed be advantageous in such circumstances,
    particularly where service flexibility is not essential, as in corporate and residential
    networks. Containers, particularly Docker Containers, are intrinsically adaptable
    and can easily operate within a Virtual Machine (Lingayat et al., 2018). Such
    Containers may be moved from one Virtual Machine to the other or perhaps to bare
    hardware without needing substantial effort. Kubernetes: Docker is responsible
    for the packing and delivery of applications or services, meanwhile the Kubernetes
    infrastructure is responsible for scaling, running, and monitoring such applications
    or services. Kubernetes (Pereira Ferreira and Sinnott, 2019) is also known as
    a Container operator because it automates the installation, scheduling, scalability,
    and synchronization of containerized workloads. Kubernetes by itself does not
    directly execute Containers; rather, single or maybe more Containers are encased
    in a high-level framework known as pods. Containers within the same pod exchange
    resources and networks, as well as interact with one another. The Kubernetes cluster''s
    overall design comprises a master or controller and nodes or slaves. The master
    is in charge of providing the Application Program Interfaces (APIs) to developers
    and scheduling cluster deployments, such as pods and nodes. The nodes include
    the Container runtime, such as a single or more Docker Containers operating within
    pods, as well as a component called Kubelet (Goethals et al., 2022), which handles
    connectivity between the master and the node. OpenStack: Due to its adaptability
    and diversity, is regarded as one of the best possibilities for supporting 5G
    MEC usage scenarios. It is an open program that is used to construct both public
    and private clouds and has strong support for Virtual Machine and Container concepts.
    OpenStack (Phan and Liu, 2018) is sometimes known as a cloud operating system
    since it maintains and controls massive pools of facilities in data centers such
    as computation, connectivity, and storage. OpenStack seems to be a massively distributed
    infrastructural software platform that is utilized in hundreds of data centers
    across the world. The telecom sector has recently introduced it to improve MEC
    usage scenarios. 5.2. Network Function Virtualization (NFV) NFV is a virtualized
    framework that uses virtual hardware abstraction to separate network services
    from hardware or equipment. As a result, it is advantageous reusing the technology
    and infrastructure of NFV (Gonzalez et al., 2018). Fig. 5 represents ETSI NFV
    architectural framework. Download : Download high-res image (432KB) Download :
    Download full-size image Fig. 5. ETSI NFV architectural framework. The NFV is
    made up of Virtualized Network Functions (VNFs), underpinning NFV Infrastructures
    (NFVI) (Chin et al.) and an NFV Management and Orchestration system (MANO) (Breitgand
    et al., 2021) to offer virtualized network services. A VNF is generally a software
    execution of a network operation that is independent of the hardware facilities
    it employs. The VNFs depend on the NFVI, which obtains the required virtualized
    facilities (processing, storage, and communication) from the physical resources
    via the virtualization level. A VNF can be distributed over one or more Virtual
    Machines, with Virtual Machines segmented on the facilities of a physical host
    by software packages known as hypervisors. The NFV MANO is made up of three major
    parts (Chen et al., 2022): the NFV Operator, the VNF Management unit, and the
    Virtualized/Virtual Infrastructure Manager (VIM). The NFV Operator is the topmost
    structural layer of the NFV Management and Orchestration system and is in charge
    of network service development and Lifecycle Management system (LCM). Conversely,
    the VNF Management units are there in charge of the Lifecycle Management of the
    VNFs, which are handled separately by the Element Maintenance (EM) systems. A
    VNF Management unit can support one or more VNFs. Lastly, the Virtual Infrastructure
    Manager supervises and regulates the NFVI facilities (i.e., processing, communication,
    inventory of software, storage capacity, and network resources, improving energy
    efficiency, increasing resources for Virtual Machines, planning and optimization,
    and a collection of infrastructure fault detection and prevention operations).
    An NFV-based networking service is made up of an ordered list of VNFs that connect
    two endpoints and route traffic via them. This combination to deliver an NFV-based
    networking service is comparable to the specifications of Service Function Chaining
    (SFC). The architectural modifications are detailed individually for each one
    of the two levels (of a MEC infrastructure) for consistency and clarity. MEC Host
    Tier/Level: Both the MEC services and the MEC Platform are installed as VNFs upon
    that host side, whereas the virtualized architecture is implemented as NFV Infrastructures
    (Chantre and Saldanha da Fonseca, 2020), (Sarrigiannis et al., 2018). The NFV
    Infrastructures, or virtualized infrastructure, may be implemented using a variety
    of virtualization technologies, including Container or hypervisor-based systems,
    as well as combining virtualization techniques. The MEC Platform Manager is replaced
    on the host supervisory end by the NFV-MEC Platform Manager and a VNF Management
    unit. The NFV-MEC Platform Manager is responsible for the same tasks as the MEC
    Platform Manager. The VNF Management unit is in charge of managing the Virtualized
    Network Functions’ service life. The Virtual Infrastructure Manager retains its
    identical capabilities. MEC System Tier: The MEC Orchestrator (MEO) is overtaken
    by the MEC Applications Operator as well as an NFV Operator in the NFV-MEC framework.
    The MEC Applications Operator is responsible for the same things as the MEC Orchestrator
    (Sarrigiannis et al., 2020). Nevertheless, the MEC Applications Operator delegates
    resource management and MEC program management to an NFV Operator. The other components
    are unaffected. The work identifies the mentioned potential NFV advantages for
    MEC: (i) reduced OPEX and CAPEX; (ii) more flexibility and rapid implementation
    of new services. 5.3. Software Defined Networking (SDN) To install the MEC platform
    in an NFV context, ETSI proposes a reference design. Also, some alternative designs
    are presented depending on the SDN framework to enable smooth collaboration across
    NFV and SDN platforms (Lv and Xiu, 2020). Fig. 6 visualizes the SDN-based MEC-NFV
    architectural framework. While being created for standard NFV infrastructure,
    these solutions are still applicable in a MEC-NFV context. According to ETSI standards,
    SDN control strategies can be placed in the MEC-NFV framework in a variety of
    circumstances (Fig. 6) (Kiran et al., 2020). An SDN supervisor can be integrated
    with the virtualized environment supervisor, counted as part of the NFV framework,
    abstracted as either a Virtualized Network Function (VNF) or integrated into the
    Base Station Subsystem (BSS)/Operations Support Subsystems (OSS). As a result,
    SDN control systems are placed in MEC servers to deliver on-demand MEC operations
    by linking VNFs and managing infrastructure resources interactively (i.e., computation,
    storage, and connectivity) (Kiran et al., 2020). MEC enables service operators
    to provide new sorts of services that necessitate cloud computing resources at
    the network''s edge. SDN improves MEC functionality by recognizing the significance
    of flexibility in defining policies for where and how data will be processed (Shah
    et al., 2020), as well as implementing network services without extra expenditure
    or hardware change. The work identifies five SDN advantages for the MEC infrastructure:
    (i) scalability; (ii) accessibility; (iii) resiliency; (iv) interoperability;
    and (v) flexibility. Download : Download high-res image (609KB) Download : Download
    full-size image Fig. 6. SDN-based MEC-NFV architectural framework. 5.4. Network
    Slicing Network slicing (Wu et al., 2022a), (Ramneek et al., 2019) has evolved
    as a significant idea for delivering an adaptable networking infrastructure that
    can efficiently serve rising enterprises with different service requirements.
    It entails dividing a single network into multiple circumstances, each of which
    is designed and adapted for a particular mandate and/or service/application. According
    to an important industry statement that explains the network slicing idea, network
    slicing allows the establishment of several logical, self-contained systems on
    a single physical infrastructure, providing resource separation and customizable
    network operations. In other ways, network slicing creates a multi-architecture
    that supports flexible network resource allocation as well as an adaptive attribution
    of network services, Radio Access Techniques (RATs), and activities, even with
    a limited lifespan, allowing for new revenue generation opportunities. Network
    slicing facilitates resource sharing across virtual MNOs, processes, and applications
    by introducing the concept of network slice brokers, which supports network sharing
    administration and the service exposing capability operation, as detailed in the
    3GPP cellular systems (Cárdenas and Fernández, 2020). The network slicing, from
    an infrastructure standpoint, assigns a set of reserved or shared facilities,
    either real or virtual, to certain occupants by implementing a network hypervisor.
    To meet the service needs of incoming queries, network slices must integrate a
    collection of cloud-based and network-based resources, such as bandwidth, networking
    functions, computing, storage, and accessibility to big data or quantitative analytics,
    among others. Fig. 7 depicts an example of the network slicing on a common network
    infrastructure considering the potential role of MEC in mobile broadband, automotive,
    and massive IoT services (Liang et al., 2022), (Taleb et al., 2017), (Wijethilaka
    and Liyanage, 2021). Mobile broadband services need significant capacity to ensure
    that an application achieves adequate efficiency. With traffic forwarding to the
    localized edge, the MEC system may store content at the edge, boosting the capability
    of the wireless backhaul and network infrastructure. To assure optimal broadband
    experiences, MEC may also provide a variety of services such as video accelerators
    or activity-aware performance optimizations. MEC is a catalytic element that influences
    the potential of an array of sophisticated operations for the Vehicle-to-Everything/automotive
    network slicing (Mei et al., 2019), which must support rigorous latency and extensibility
    with network functions initialized at the edge. Flexibility is crucial for managing
    enormous volumes of data quickly and effectively in massive IoT; therefore, MEC
    can offer storage and processing services for accomplishing signaling improvements.
    Download : Download high-res image (557KB) Download : Download full-size image
    Fig. 7. Network slicing (Liang et al., 2022). To enable service modification in
    slicing, a combo of NFV and SDN solutions is needed, offering a strong synchronization
    for VNF allotment and service orchestration at the edge server, while offering
    genuine and flexible service management in real-time operation. Network slicing,
    as one MEC enabler, delivers two primary advantages to the MEC setting: (i) dynamic
    infrastructure and (ii) effective resource utilization. 5.5. Information-Centric
    Networking (ICN) The Internet, which had been initially intended for host-to-host
    interactions, is now mostly utilized for content distribution. With continually
    rising traffic levels, an Information-Centric Networking (ICN) concept tries to
    bridge the gap between both the original design of the Internet and modern services,
    including high-definition multimedia on-demand streaming, 3D gameplay, and virtual
    and augmented reality environments. To improve content transfer, ICN proposes
    redesigning the Internet framework as a content-centric system that employs two
    conceptual designs, namely connectivity (instead of hosts) as well as caching
    at MEC computing servers to alleviate bandwidth congestion and enhance data delivery
    (Gür et al., 2020; Rayani et al., 2020; Barakabitze and Hines, 2023). 5.6. Service
    Function Chaining (SFC) The goal to accelerate the transition to software-defined
    and configurable networks have prompted network engineering and research organizations
    to create another technique defined as Service Functions Chaining (SFC) (Li et
    al., 2021a). SFC assists telecom companies and service providers in dynamically
    creating network services and directing traffic flows across them. Although this
    description and principles seem similar to those of NFV, SFC deals with the issue
    of delivering end-to-end solutions throughout a chain or hierarchy of service
    functionalities (Erbati and Schiele, 2021). In reality, SFC is an ideal technique
    for interconnecting two or even more service functionalities in a certain network
    sequence. Fig. 8 shows the architecture of SFC. Additionally, virtualized service
    functionalities and physical network elements can be chained. As a result, SFC
    assures that physical network operations remain in-network and services delivery
    frameworks are not omitted thereby. Download : Download high-res image (327KB)
    Download : Download full-size image Fig. 8. SFC architecture. Traffic streams
    in an SFC infrastructure are categorized and then processed based on this categorization.
    These procedures are administered in the sequence of the chains, and such procedures
    may be reclassified, culminating in another sequence. The SFC advisory committee
    of the Internet Engineering Task Force (IETF) has created SFC architecture for
    generating, updating, and removing the structured service chains that are related
    to network data flows. Service Functions (SFs) Classifiers, Services Function
    Forwarders (SFFs), and the SFC control planes are the core constituents of the
    SFC framework. The SFC management plane interacts with all of these elements and
    is in charge of a variety of tasks such as establishing service configuration
    paths, choosing Service Functions for a demanded SFC, informing classifiers about
    how to categorize traffic flows, and implementing traffic forwarding guidelines
    in Services Function Forwarders based on the service configuration path. Classifiers
    classify traffic flows by comparing them to the policy established by the management
    plane and then applying the necessary assortment of network service functions.
    The service function is in charge of specialized processing and can be deployed
    as a virtual component or as an actual network component. As appropriate, the
    Services Function Forwarder forwards traffic to the related service functionalities
    or into the classifier based on service configuration path information. The MEC
    application tier is now quite dynamic due to the continuous expansion of end-user
    activities. Furthermore, because of the large number of virtualization solution
    ideas in terms of designs, deployments, or implementations, SFC is one of the
    essential players in the MEC environment. SFC allows MEC to adjust a networking
    service function to the end user context and deliver end-to-end services (Behravesh
    et al., 2021). Incorporation of SFC within MEC is an acceptable technique for
    organizing service function implementation, realizing desired strategies, adapting
    applications when approaches or policies evolve, and rationally allocating resources
    to provide required or requested services. SFC offers a wide range of MEC applications,
    which can improve MEC functioning in terms of resource optimization, privacy,
    and accessibility (Wang et al., 2019a). The study (Chen and Liao, 2019) solves
    the inter-MEC handover issue by offering the proper option to locate and move
    SFCs to accommodate subscribers of a 5G infrastructure with MEC regarding service
    latency. When a user switches cells, the proposed method determines which Virtualized
    Network Functions of Service Functions Chains should indeed be transferred, which
    MEC services should be used, and what amount of resources ought to be assigned.
    These decisions are made to minimize service disruption. To achieve optimal QoS
    for IoT operations, the researchers (Forti et al., 2021) devised a method for
    carefully placing service functionalities of SFCs in the networks'' edge. They
    presented a probability-based logic programming technique for ranking locations
    of VNF chains depending on bandwidth, end-to-end latencies, and safety requirements.
    Leveraging the advantages of the protected service chaining framework Feng et
    al., 2021 introduced a novel SFC architecture to meet various security needs for
    Multi-Access Edge Computing. This system attempts to deliver real-time secure
    service sequencing to mobile subscribers. A fuzzy intelligence system-based technique
    is used to appropriately arrange the security features and therefore generate
    the security support chains to achieve a superior level of safety and an optimum
    sequence of the security mechanisms (Li et al., 2017). Masoumi et al. (2022) and
    Bai et al. (Bai et al.) investigated the MEC contexts with a focus on VNF failures.
    To address this issue, they deployed selected VNFs rather than the entire SFC.
    The procedure of choosing which VNFs are redundant is dependent on available statistics,
    which are determined by the average interval between problems and the average
    time necessary to fix problems. A VNF redundancy solution is built in light of
    the edge network''s restricted resources and depending on this parameter. 5.7.
    Radio Access Control The radio accessibility networks in 4G/5G systems are made
    up of two primary constituents: the Remote Radio Heads (RRHs), which handle radio
    spectrum transmission/reception, and the Baseband Units (BBUs), which handle signal
    processing. China Mobile presented the Cloud-RAN (C-RAN) network design in 2010
    (I et al., 2014). C-RAN groups BBUs from a detached central workplace, referred
    to as a BBU controller, distant from their corresponding RRHs; the combined BBUs
    are identified using private routers that reside within the BBU controller. The
    C-RAN design allows for lower power consumption, more efficient operation, and
    increased dependability. One downside of C-RAN is indeed the vast separation between
    the BBU pool and RRHs, which generates additional delay across the BBU pool and
    users. This problem spawned a new RAN design known as Fog-RAN (F-RAN), whereby
    every BBU controller is built by aggregating just a limited number of RRHs, allowing
    users more alternatives for determining the best BBU venue to operate their services.
    Similarly, the reference (Zeng et al., 2019a) compared F-RAN with C-RAN, indicating
    that the F-RAN gives a faster service, however, at a relatively higher cost. 5.7.1.
    Heterogeneous C-RAN To handle the enormous rise in network traffic, and a vast
    number of interconnected devices, infrastructure densification has emerged as
    the crucial component of 5G networks by deploying additional access points and
    base stations and utilizing spatial spectrum reusing. A heterogeneous network
    (HetNet) is a merger of higher-power macro cells with lower-power small cells,
    such as femto cells, pico cells, micro cells, and relay nodes. HetNets were designed
    because of the following advantages (Xu et al., 2021a): (i) increased coverage
    and efficiency, (ii) minimization of the macro cell dependability, and (iii) decreased
    cost and subscriber turnover. Nevertheless, there are significant problems with
    deploying dense HetNets: (i) significant interference, (ii) insufficient energy
    management, and (iii) lack of flexibility and adaptability. Heterogeneous C-RANs
    (H-CRANs) (Alhumaima et al., 2015) are offered as a promising option to deliver
    excellent energy and spectral efficiency (Kim and Chang, 2019). H-CRANs can also
    provide broad coverage and good energy efficiency, whereas MEC can provide significant
    computational facilities for low-latency applications (Makris et al., 2019). Combining
    these two critical technologies will enable 5G to accommodate more applications.
    H-CRAN may be integrated with MEC to ease the installation of the MEC system,
    taking into account the computing and storage facilities in the BBU pools (Abdelhakam
    and Elmesalawy, 2018) as well as the deployment of the RRHs (Elhattab et al.,
    2021). As a result, the integration of MEC and H-CRANs can provide the following
    advantages: By collocating MEC and H-CRAN, the expenditure on MEC implementation
    can be significantly minimized. Everyone seems to know, deploying a suitably wide
    MEC network requires a big investment. Re-architecting MEC installation to C-RAN
    infrastructure is one option to reduce investment costs. The cost of transferring
    extra task processing over the operating RRHs or BBU pool will be decreased in
    this situation. The integration of MEC with H-CRAN can give the operational versatility
    and infrastructure reconfigurability that the virtualization of H-CRAN may deliver.
    The H-CRAN can speed up radio deployment by lowering the time required in traditional
    deployments, such as typical General-Purpose Processing units. Since C-RAN virtualizes
    many RAN operations, MEC may benefit from H-CRAN''s coverage, energy efficiency,
    network simplification, and high security (Wang et al., 2016). H-CRAN MEC may
    be implemented in a variety of settings. H-CRAN, for example, may interpret task
    signals from any place, such as a cell tower co-located area. Since H-CRAN implementation
    necessitates a significant amount of computing power, it may immediately transform
    into a MEC server to compute workloads from users. In conjunction with the advantages
    listed above, various obstacles with H-CRAN MEC platforms can be emerged via H-CRAN
    and MEC collocation, such as deployment scenario planning. The key problems of
    H-CRAN MEC technologies are elaborated on below: The equilibrium of rollout and
    network effectiveness inside the H-CRAN MEC architecture should be thoroughly
    examined. Since the H-CRAN provides dynamic capacity, the functionality of MEC
    systems is affected by the separation distance between the C-RAN/MEC and cell
    site, e.g., how effectively it can support activities. For example, while putting
    the C-RAN/MEC platform in a central facility might greatly save costs, it results
    in excessive latency. In this instance, use cases must be carefully examined to
    determine which programs should execute at which locations. Most MEC capacity
    management solutions take into account the computational resource on MEC hosts
    and may therefore be used directly in H-CRAN MEC. Yet, concurrently optimizing
    computing resources and scheduling network resources in H-CRAN remains difficult.
    Inter-cell and cross-layer interference must be regarded, specifically in HetNets.
    Furthermore, the adaptive resource management method based on C-RAN NFV may require
    being updated to flexibly plan virtual computing resources under variable network
    capacities and task entry rates. Another issue that must be addressed with H-CRAN
    MEC systems is confidentiality. Since MEC service offers a wide range of applications,
    including third-party applications that are not directly regulated by mobile network
    carriers. There is a danger that these applications will deplete network resources
    or allow hackers to disrupt network operations. As a result, executing integrity
    assurance tests on programs should be considered during installation or upgrade.
    Because of the presence of inter-carrier interference (ICI) or disturbance, the
    resource provisioning issue in H-CRAN MEC systems is significantly more difficult
    than standard or typical MEC systems. To counteract this impact, the spectrum
    resource inside every cell can be separated into orthogonal sub-channels that
    should be assigned to mobile users effectively (for example, which sub-channel
    or sub-carrier user device has to use for offloading a task to the host MEC server).
    To decrease inter-cell interference or disturbance in H-CRAN MEC systems, several
    types of resources must be orchestrated efficiently, including not just traditional
    wireless resources (e.g., transmit power, sub-channel, space, and time), but also
    countervailing expenses (e.g., harvested energy, the backhaul spectrum, caching
    storage, and computing capabilities). User association, computational offloading,
    interference reduction, and allocating resources are the key problems of high-density
    H-CRAN MEC technologies. More significantly, these issues are inextricably linked
    and must be addressed together. On one side, it is expected that a large number
    of MEC systems will be extensively implemented in the coming years, varying in
    size (processing capabilities) and functionality (computation speeds). The link
    between subscribers and MEC systems, on the contrary, is highly dependent on the
    placement sites of MEC systems. Whenever a user device travels within the geographic
    region served by the unified BBUs, user motion can be disregarded. This type of
    BBU centralization affects the system performance and user experience. 5.8. Device-to-Device
    Communication (D2D)-Aided MEC The exponential expansion of mobile data transmission
    and context-aware programs necessitates novel techniques to more effectively utilize
    bandwidth and enhance coverage while reducing latency and energy usage. Since
    every communication must be transmitted through the centralized control unit,
    the star configuration of mobile networks with a central control point, such as
    a base station or access point, suffers from inadequacies. D2D transmission, on
    the other hand, is a radio technology that allows direct data transfer between
    two neighboring UDs without the participation of the wireless network''s central
    management point or core network, i.e., without crossing the base station or access
    point. This direct D2D connection has various advantages, including enhanced spectrum
    efficiency, higher data rates across devices, lower power consumption, and shorter
    end-to-end latency. D2D connectivity has been exploited in numerous research works
    to offload computations to other adjacent users (not utilizing MEC facilities)
    (Mehrabi et al., 2021), (Feng et al., 2018). Moreover, accessing caches at adjacent
    users (although not leveraging MEC caches) has been examined in previous research
    (Mahmoodi et al., 2023), notably audiovisual file caching at some other UEs (Baccour
    et al., 2020). Fig. 9 depicts D2D-aided edge computation/MEC scenarios. Download
    : Download high-res image (380KB) Download : Download full-size image Fig. 9.
    D2D-aided edge computation or MEC scenario. Unfortunately, D2D communication has
    certain implementation issues. One problem is the requirements to acquire exact
    channel information, for example, channel estimation and transmission control,
    which introduces overhead. Another significant problem in D2D communications is
    privacy. D2D transmission is inherently vulnerable to security threats since a
    user''s content goes through another user''s device (Haus et al., 2017). Selfish
    exploitation activity of user devices is another impediment to cooperative multi-device
    D2D transmission; since some devices may take other devices'' communications resources,
    i.e., multi-hop D2D connectivity via intermediate relay mobile users and preventing
    the relay user to communicate with others (Del Carpio et al., 2015). Interference
    and movement management are also significant issues. As a result, when constructing
    device-enhanced MEC platforms that incorporate D2D transmission, these D2D communication
    issues must be carefully examined. Notwithstanding these obstacles, D2D communication
    offers great potential for a variety of realistic use case circumstances in forthcoming
    communication systems. The work next goes over a few sample use-case instances
    in detail. National Security and Confidentiality: The wireless cellular communication''s
    dependency on the accessibility of the wireless communications infrastructure
    causes serious challenges in emergency and catastrophe situations such as flooding
    and earthquakes. Such calamities frequently destroy the mobile network infrastructure,
    causing cellular wireless communication to be disrupted. D2D communications, on
    the other hand, do not necessitate a permanent network infrastructure and may
    thus continue functioning even if the cellular network architecture is broken.
    Because of this benefit of direct D2D interaction, for emergency conditions, the
    United States governing body for the regulation of telecommunications for national
    and public safety and the European administrative body for Postal and Telecommunications
    prepared regulations for next-generation state-level security and confidentiality
    of communication systems (Bopape et al., 2020). Proximity and Location-Based Services:
    With a growing emphasis on online gaming, promotional campaigns, and social media
    platforms (e.g., Instagram, Facebook, and others), there is a greater need for
    effective quick connectivity to endorse interconnections between adjacent users
    with reduced latency and battery capacity while maintaining high degrees of user
    confidentiality. D2D communication can permit such connections between nearby
    user devices, for example, a cell phone communicating to a computer or other cell
    phones for storing or exchanging video and photographs (Choi and Han, 2015), (Griffith
    et al., 2017). Vehicular Connectivity: Another major use scenario of D2D transmission
    is vehicular or V2X interaction, which is classified into three types: vehicle-to-network
    (V2N), vehicle-to-infrastructure (V2I), and vehicle-to-vehicle (V2V). Recent substantial
    advancements in computing and communication infrastructures, as well as vehicular
    sensing technologies, have moved focus on V2X communications to strengthen public
    safety and smart transportation systems, accident avoidance mechanisms, and electric
    car charging (Liang et al., 2017a), (Liu et al., 2019a). Authors noticed that
    the aforementioned use cases, along with a diverse variety of other D2D connectivity
    use circumstances, have the possibility of benefiting considerably from jointly
    utilizing MEC computing and caching resources, alongside the facilities of other
    neighboring mobile end devices, i.e., device-enhanced MEC (Mehrabi et al., 2019),
    (He et al., 2019a). 5.9. Machine Learning and Artificial Intelligence Machine
    Learning (ML) has been used in a wide range of applications, including digital
    assistants, video monitoring, social media platforms, email spam and virus screening,
    search engine result refinement, and product suggestion. There are various reasons
    why machine learning algorithms are becoming more popular (Wang et al., 2022c):
    (i) ML facilitates mechanisms that can automatically adjust and configure themselves
    to legitimate users, (ii) ML can explore new information from massive database
    systems, (iii) ML can resemble human and substitute certain strenuous tasks, that
    necessitates some cognition, (iv) ML can develop programs those are challenging
    and costly to build manually because they contain specialized comprehensive abilities
    or expertise synchronized to a specific project, and at last (v) massive rise
    in computing capabilities. In general, ML is classified into three fundamental
    types: Unsupervised Learning, Supervised Learning, and Reinforcement Learning
    (RL) with Deep Learning (DL), presented as a breakthrough approach and a big step
    forward toward ML, capable of achieving higher-level abstractions based on simple
    elements. The International Telecommunication Union (ITU) Telecommunications Standardization
    Division (ITU-T, 2020) recently suggested a uniform framework for ML in forthcoming
    networks, in which MEC would play crucial roles as a producer, pre-processor,
    collector, modeler, strategist, distributor, and sink. MEC, for example, may receive
    data from users and subsequently preprocess the data by running an ML algorithm
    to extract the required information before delivering the outcome to the centralized
    cloud for any further training. Furthermore, various surveys and tutorials covering
    ML, DL, and related implementations in networking and communications have been
    published, and readers can resort to these publications for further information
    (von Rueden et al., 2023). Fig. 10 visualizes a graphic demonstration of the incorporation
    of AI/ML and analytics in a MEC server. Download : Download high-res image (384KB)
    Download : Download full-size image Fig. 10. Incorporation of the analytics and
    AI/ML in an MEC server. Because of the fast expansion of wireless communication
    systems and infrastructures, it is expected that Artificial Intelligence (AI)
    or machine intelligence generally, and ML in particular, may have crucial functions
    in 5G, 6G, and beyond (Tang et al., 2021a), (Kaur et al., 2021). In general, ML
    can offer the following benefits: The capacity to learn from massive data to enhance
    system operations and efficiency is the most obvious benefit of ML, which may
    be done without additional hand-crafting functionality. Because (i) mobile data
    traffic is extensive, (ii) mobile traffic raises at unprecedented rates, (iii)
    mobile traffic is non-stationary (i.e., the timeline for reliability and validity
    can indeed be comparatively short), (iv) mobile data effectiveness is not retained
    (i.e., data accumulated can sometimes be low-quality as well as disruptive), and
    (v) mobile data is diverse, the significance of learning emerges inherently in
    wireless systems (i.e., data traffic can be produced by various types of devices
    and services). Due to huge state and action domains, varied network devices, and
    varying QoS objectives, joint communication, caching, computing, and control (4C)
    management in 5G as well as beyond is enormously challenging (Adam et al.). In
    such cases, ML can provide instantaneous and/or completely distributed approaches.
    Furthermore, model-free wireless communications raise difficulties such as channel
    estimation, problem statements, and closed-form solutions, all of which may be
    easily tackled by ML (Fadlullah et al., 2022). Finally, since edge computing will
    serve an essential role in enabling low-latency operations and the bulk of intelligent
    services will be placed at the edge of the network, hence, edge intelligence will
    develop. Using edge intelligence to extract useable information from huge amounts
    of mobile data might increase the capabilities of tiny IoT devices and allow the
    implementation of computationally intensive and lower-latency edge services (Xue
    et al., 2020). Edge learning, on the contrary, can avoid the limitations of cloud
    intelligence and on-device intelligence by balancing learning design complexity
    and training duration (Zhou et al., 2021a). Optimizing MEC confronts various issues,
    including cache placement, communications and computing resource planning, computational
    task allocation, and combined 4C optimization. A variety of difficulties in MEC
    systems have been investigated in research (Ndikumana et al., 2020), including
    computing offloading, caching, combined 4C optimization, safety and confidentiality,
    big data analyses, and the mobile crowd. AI fosters technological innovation by
    improving data analysis insights, particularly in time-varying and complicated
    networks. In this setting, stretching AI advances to the edge of the network has
    generated an entirely novel field known as edge cognition or intelligence. It
    is not only a combination of MEC with AI (Chen et al., 2019), but a whole new
    and complementary strategy for employing AI at the edge of the networks (Zhou
    et al., 2019a). Nonetheless, edge intelligence adoption remains in the early stages.
    Edge hardware can use edge intelligence to execute model training and inferences
    locally, minimizing the need for frequent contact with cloud services. Reinforcement
    Learning and Deep Learning approaches have increasingly become the most prevalent
    AI techniques in edge intelligence. Reference (Jiang et al., 2021b) asserts that
    the convergence of Artificial Intelligence and MEC is indeed an unavoidable trend,
    and provides justification for this assertion. • Reduced latency and bandwidth
    use • Adaptation to changing environments • More diverse edge applications and
    services • MEC facilitates pervasive AI • AI features can facilitate MEC. AI has
    the potential to significantly improve the cognitive efficiency and effectiveness
    and competence of the vehicular Internet of Things (IoV) to adapt to frequently
    developing dynamic environments. Moreover, AI provides numerous task specifications
    for the allocation of resources, data processing task scheduling, and traffic
    prediction forecasting. By introducing AI technologies to edge access nodes, edge
    intelligence has demonstrated exciting potential for handling various intelligent
    vehicular scenarios. The work (Tang et al.a) developed a Double-Deep Q-network
    algorithm-based solution for minimizing the cost of transmission, storage, and
    computing in a vehicular network. Nevertheless, there are still issues that must
    be addressed before edge intelligence can be widely used, such as. • System dynamics
    and transparency • Infrastructure and network design • Models of lightweight training
    • Privacy and security. 6. Technical features of MEC 6.1. Computation offloading
    Since user devices are capacity restricted, they must offload some of the resource-intensive
    activities (e.g., video analytics or processing) to certain other resource-rich
    terminals to conserve capacity, energy, and time. It is also one of the inspirations
    behind the MCC paradigm, which enables cloud services for mobile users. Nevertheless,
    MCC imposes several constraints, i.e., escalation of the traffic load (backhaul
    and radio) and latency. To solve these restrictions, offloading computations in
    edge-cloud computing/MEC are presented, which consists of three components (Islam
    et al., 2021). The very first aspect, termed as application/task splitting, is
    in charge of selecting between three ways based on different performance metrics:
    localized processing, complete offloading, or partial offloading (Shakarami et
    al., 2020b). While making this determination in a versatile and complicated network
    infrastructure is difficult, the partitioning technique (either automated application
    assessment or programmer-specified segmentation) adds further challenges. The
    second component, which consists of three actions, is concerned with task allocation.
    Step 1 is to identify resource-rich endpoints (for example, edge servers) which
    may be utilized for offloading. Step 2 is task sequencing, which involves assigning
    partitioned tasks or workloads to found nodes for optimal performance. The final
    step or step 3 is task allocation. The last component maintains several categories
    of resources. The in-practice offloading strategies are described below. Full
    Computation: (i) Local Computation: All computing activities are executed in local
    devices (UDs) rather than transferred directly to the MEC infrastructure, resulting
    in substantially lower latency than offloading computing workloads to the servers.
    Unfortunately, this technique throws significant constraints on the device''s
    battery performance and energy usage. This will also impose special demands on
    the computing capacity of the end devices. (ii) Edge Computation: Unlike local
    computation, edge computation sends all compute-intensive operations to MEC nodes
    for processing. Afterward, there are three forms of latency to consider: a) the
    time it takes to transfer content to the MEC; b) the time it takes for the MEC
    hosts to analyze content; and c) the time it takes for the device to obtain the
    processed data. Let''s anticipate that the total of such three delays will be
    less than the local computational latency. The devices'' energy is also conserved
    in this manner. Partial Computation: Partial computation typically divides processing
    into two sections: offloaded and non-offloaded segments, which tend to be more
    adaptable than the binary choice of local computation or edge computation. The
    non-offloaded data segments will be processed locally by the user device, while
    the offloaded data segments will be processed on the edge side. Nevertheless,
    precisely distinguishing two factors and balancing the limitations is difficult.
    Moreover, offloading decisions are made by taking into consideration a variety
    of factors such as energy usage, latency, and communication bandwidth. Li et al.
    (Li and Wu, 2023) studied a novel joint optimization approach for caching and
    computation offloading in a MEC infrastructure for vehicular networks. Ke et al.
    (2020) constructed an adaptive Deep Reinforcement Learning-based strategy for
    computation offloading in a MEC-based heterogeneous or collaborative vehicular
    infrastructure. Researchers jointly examined the system''s reliability, energy
    usage, and execution delay. The offloading selection is an important aspect of
    the MEC since it decides where the computation needs to be performed. While making
    the selection, one ought to take into consideration not just the power consumption
    of the end devices as well as the energy consumed by the MEC hosts. Consider the
    scenario of a single user device transferring computation to several servers (e.g.,
    under very tight latency constraints) as well as the scenario of numerous devices
    offloading. Most studies, however, consider that the user device remains immobile
    when making offloading decisions. Users can frequently move, and in some cases,
    they may relocate from one location to another. In this scenario, the preference
    to minimize the expense relative to user mobility is a reason for concern. Cyber
    foraging provides computation offloading, strengthening mobile device capabilities
    while significantly improving the energy economy. CloneCloud partitions code automatically
    at the thread layer. Cuckoo examined component-level partitioning, and a related
    mechanism was built in MAUI. Yet, the transaction state may be extended because
    of an unbound delay in computation offloading to a faraway cloud. In certain circumstances,
    this may use more energy than local computations. MEC consumes less energy due
    to lower latency and closeness to the RAN edge. As a result, consumers benefit
    from speedier execution and improved performance. Encoding represents the most
    computationally intensive element for video services. Whenever a mobile device
    wants to initiate a video chat during teleconferencing, a negotiation process
    is undertaken to pick the kind of encoding accessible before the content is encoded
    within the mobile device and afterward uploaded. Such a procedure requires power
    and may take some time. Tusa et al. (Tusa and Clayman, 2021) suggested a communication
    mechanism to transfer the encoding function to an edge computing server as a solution.
    Offloading the encoding portion will conserve energy and reduce delays in connectivity,
    providing excellent video quality. Machine-to-machine (M2M), wearable tech, or
    other IoT devices may shift compute-intensive programs to the edge. This may be
    accomplished by dividing the program and transferring only the data-intensive
    portions to the edge. Abdelwahab et al. (2016) presented REPLISOM, a computation
    offloading paradigm for IoT services in which the edge cloud submits a demand
    to the respective IoT devices, obtaining information relative to the connected
    service, i.e., generally a memory replication of the VM. REPLISOM is built along
    the Long-Term Evolution (LTE)-evolved memory replicating mechanism, which uses
    D2D connectivity to combine numerous memory copies from the surrounding user devices
    into a unified compact form, which is subsequently fetched from the device. Compressed
    sample-building techniques are employed to administer such memory replicas, lowering
    the effort required by typical devices to push for saving energy and money by
    minimizing the number of repeated requests at once. The work (Ning et al., 2019)
    considered the idea of a hybrid cloud, which splits applications flawlessly in
    core and edge cloud services, enabling delay responsive and user-interactive features
    at the edge, thereby preserving a configurable cloud approach, and this is an
    edge-cloud cooperative scheme focused on IoT applications. The goal is to make
    two new types of applications more efficient: extremely accurate indoor localization
    which relies on reduced latency and flexible and robust video surveillance that
    preserves bandwidth. Another sector that needs a substantial amount of computational
    power is mobile games. Mobile gaming may become more interactive owing to lower
    reaction times by transferring the rendering portion from handheld devices. In
    the context of multiplayer gaming, the trade-off around offloading and cloud service
    efficiency should be evaluated to prevent overloading to a specific cloud platform.
    In this regard, Hu et al. (Hu and Wang, 2022) and Qin et al. (2019) suggested
    a distributed computational offloading model based on game theory. The model takes
    into account a multi-channel radio network and a multi-user offload strategy,
    with the equilibrium state attained by taking into account the number of users
    who can gain from edge computing. RAN-aware content optimizer, a complementing
    service that harnesses the features of MEC, can improve the effectiveness of computational
    offloading. Obtaining information on RAN performance and user context before offloading
    can help both the devices and the network. Magurawalage et al. (2014) suggested
    a network-aware and energy-efficient architecture for application offloading depending
    on network accessibility, radio signal performance, and surrogate computational
    resources that are available. It deconstructs applications into their constituent
    parts and develops an offloading online approach according to the previously mentioned
    optimization factors. Wu et al. (Wu, 2021) presented computation architecture
    from the standpoint of IoT applications. It then analyzed cutting-edge concepts
    for AI-empowered cloud-edge coordination for IoT. Lastly, a list of prospective
    research problems and open queries is presented and addressed. To solve the tradeoff
    between low computing power and substantial latency, while also ensuring data
    integrity throughout the offloading process Wu et al. (2021a) proposed a Blockchain-based
    cloud-edge-IoT computing framework that takes advantage of both MCC and MEC, with
    MEC servers providing reduced latency computing services and MCC servers providing
    more computational capability. Furthermore, the work designed an energy-efficient
    dynamic computational task offloading (EEDTO) algorithm that chooses the appropriate
    computing location live, either on the MCC server or the IoT device, or the MEC
    server to jointly optimize energy usage as well as task response time. Xue et
    al. (2022a) presented an efficient offloading system for DNN Reasoning Acceleration
    (EosDNN) within a cloud-edge-local cooperative context, where DNN reasoning acceleration
    is primarily employed in task migration delay optimization as well as real-time
    DNN query realization. Xue et al. (2022b) investigated DNN model segmentation
    and offloading and proposed a unique optimization strategy for parallel task offloading
    of substantial DNN models in a cooperative cloud-edge-local setting with restricted
    resources. To acquire the DNN offloading method, an enhanced Double Dueling Prioritized
    Deep Q-Network (DDPQN) technique is suggested in conjunction with the coupling
    cooperation level and node balancing degree. Fig. 11 depicts the notable and evolving
    algorithms for computational task offloading (Hu et al., 2022; Xu et al., 2021b;
    Guo et al., 2018, 2019; He et al., 2019b; Wang et al., 2021b; Nguyen et al., 2019;
    Shu and Li, 2023; Kang et al., 2023; Su et al., 2020; Yang et al., 2019a, 2021a;
    Chen et al., 2023a; Li et al., 2018a; Waqar et al., 2022; Zhang et al., 2023;
    Huang et al., 2021; Hossain et al., 2021). Download : Download high-res image
    (906KB) Download : Download full-size image Fig. 11. Computational task offloading
    algorithms. 6.2. Communications Communication links between mobile users and cloud
    services are generally characterized as bit-stream or bit-pipe with either fixed
    rates or arbitrary rates with specified distributions in MCC research. Such coarse
    designs are used for adaptability and may be appropriate for the architecture
    of MCC platforms where the focus is on mitigating latency in core networks and
    managing large-scale clouds but not wireless communication delay. In the case
    of MEC systems, the situation is distinct. Considering the small-scale nature
    of edge clouds and the concentration to latency-critical activities, the key design
    goal is to reduce communication latency by creating a highly improved air interface.
    As a result, the bit-pipe or bit-stream concepts discussed above are unsuitable
    since they miss several essential aspects of wireless transmission and are overly
    simplified to allow for the deployment of sophisticated communication mechanisms.
    To be more explicit, wireless networks differ from their wired equivalents in
    the following fundamental ways. (i) In wireless transmission medium multipath
    fading arises due to reflections, environmental disturbance, and refractions or
    dispersions of signals from scattering elements in the surroundings (for example,
    walls, trees, and buildings), rendering the channels extremely time-varying and
    causing significant inter-symbol interference (ISI). In trustworthy transmissions,
    effective ISI reduction methods like spread spectrum and equalization are required.
    (ii) The broadcast aspect of wireless transmissions causes a radio wave to be
    affected or disrupted by other radio waves in the same spectrum, potentially lowering
    their respective reception signal-to-interference plus noise ratio (SINR) and
    increasing the likelihood of signal loss during the time of detection (at the
    receiver end). To deal with performance deterioration, interference control has
    emerged as one of the most critical design concerns for wireless communication
    systems, attracting significant research efforts. (iii) The spectrum scarcity
    has been a major impediment against incredibly high-rate radio access, encouraging
    extensive research into exploiting unique spectrum resources, formulating novel
    transceiver configurations and network frameworks such as novel multiple access
    techniques to enhance spectrum efficiency, and constructing spectrum sharing and
    accumulation strategies to facilitate effective use of segmented and underexploited
    spectrum resources. Since wireless channels vary randomly in time, frequency,
    and space, it is vital to building efficient MEC systems that smoothly combine
    control of computational offloading with radio resource management. For example,
    if the wireless channels are under severe fade, the decrease in task execution
    delay during remote computing may not be adequate to substitute for the increment
    in transmission latency caused by the severe decrease in transmission rates. In
    these kinds of instances, it is preferable to postpone offloading until the channel
    gain is acceptable or to transfer to a different frequency/spatial communications
    channel having higher quality for offloading. Additionally, boosting transmission
    power can boost data rates while also increasing data transfer energy usage. The
    foregoing concerns imply the collaborative design of offloading or transferring
    and wireless transmissions that are adaptable to time-varying channels based on
    precise channel-state information (CSI). Communications in MEC systems are primarily
    between base stations or access points and user devices, with the potential of
    direct D2D connections (Nadeem et al., 2021). MEC systems are small server facilities
    installed by cloud computing/telecom vendors that may be co-located with radio
    access points, such as public WLAN hotspots and base stations, to save expenditure
    (for example, transmission site rents). The radio access points not only offer
    a wireless channel for the MEC workstations but also allow access to the faraway
    data center over backhaul networks, allowing the MEC server to offload some computing
    workloads to other MEC workstations or large-scale cloud computing facilities.
    D2D connections with surrounding devices give the option to transmit computing
    tasks or workloads to MEC servers for mobile devices that cannot interact directly
    with MEC servers owing to limited wireless interfaces. Moreover, D2D transmissions
    allow the peer-to-peer sharing of resources and computing load management within
    a group of user devices. There are several commercialized wireless transmission
    technologies available currently, including Bluetooth, radio frequency identification
    (RFID), near-field communications (NFC), WLAN, and mobile networks such as LTE,
    5G, beyond 5G, and 6G networks (planned to be deployed within 2030). With varied
    data rates and communication capabilities, these technologies can facilitate mobile
    offloading from user devices to access points or peer-to-peer wireless collaboration.
    In terms of operating frequencies, maximum transmission range, and data throughput,
    conventional wireless communication systems differ greatly. Since the transmission
    or coverage area and transmission rate in NFC are so limited, it is best suited
    for purposes that involve little information transmission, such as e-payment and
    physical access verification. RFID is identical to NFC in a sense that it permits
    only one-way transmission. Bluetooth represents a more powerful method for enabling
    short-distance D2D connectivity in MEC platforms. WLAN, LTE, 5G (and forthcoming
    6G) are key technologies facilitating long-distance connectivity between user
    devices and MEC servers, which may be dynamically switched based on connection
    stability. The networking and communication protocols must be modified for the
    implementation of wireless technology solutions in MEC systems to encompass both
    the communications and computing facilities and efficiently enhance computational
    efficiency, which is more complex than data transfer. Hu et al. (2018a) developed
    a MEC architecture and accompanying communication protocols for content delivery
    and processing in vehicular connectivity networks that include IEEE 802.11p, licensed
    sub-6 GHz spectrum, and millimeter wave (mmWave) communications. Liu et al. (2018a)
    presented a unified heterogeneous networking system for MEC and fiber-wireless
    accessibility networks that employ network virtualization to accomplish dynamic
    coordination of networking, storage, and computational resources to accommodate
    various application requirements. Peng et al. (2020) provided an adaptive spectrum
    management approach to improve spectrum resource usage during the communication
    with the MEC server for a self-driving vehicular network. Song et al. (2021) developed
    a novel MEC architecture for satellite-terrestrial integrated IoT systems leveraging
    Low Earth Orbit (LEO) satellites. Li et al. (2021b) presented a multi-relay aided
    computation offloading architecture for MEC with energy harvesting (MEC-EH) infrastructure.
    A computational operation may be conducted in this system by transporting it to
    the MEC host via multiple neighboring relay units. 6.3. Caching and distributed
    content delivery Since content, particularly video constitutes the most popular
    wireless network services, thousands of visual content elements are transmitted
    regularly to content providers'' systems. Such content is kept in huge quantities
    in the providers'' data warehouses before being transcoded from originating version
    to the final delivery version and disseminated to various streaming servers across
    many network locations for continued distribution. Despite attempts to distribute
    material, certain users may encounter service disruptions owing to buffering issues
    and jitter, particularly in mobile contexts. By expanding Content Delivery Network
    solutions to the cellular edge, consumers'' QoE may be improved while backbone
    and core network utilization are reduced. Many study findings support this conclusion.
    Studies (Liu et al., 2018b; Fang et al., 2021; Yang et al., 2022a; Ugwuanyi et
    al., 2019) particularly provided several architectural styles for supporting parallel
    and distributed edge servers able to execute audiovisual caching and broadcasting
    to improve QoE for content distribution. In terms of the influence on backhaul,
    caching contents can reduce bandwidth needs by up to 35% (Woo et al., 2013). Recently,
    Media Cloud, a system for effectively distributing adaptive streaming services
    for video, has been presented, which provides an elastic virtualized content delivery
    infrastructure at the edge. MNOs and OTT vendors may fully leverage the MEC system
    by pre-caching popular material at the edge based on analytics and user/service
    predicting data. A context-aware networking with edge caching features, relying
    on big data analysis is examined, whereas a content positioning and delivery technique
    based on content recommender systems and wireless environment features are detailed
    in (Monção et al., 2022) and (Wei et al., 2021). Caching may be implemented in
    a hybrid fashion for networked cloud edges, in which each edge server distributes
    the cached resource metadata in the format of a portfolio. When users request
    contents for the very initial time in a traditional cloud computing system, the
    request is forwarded to a distant cloud platform to retrieve the needed material.
    Unlike traditional cloud operations, hybrid caching allows users to request material
    from other local edge platforms rather than retrieving content from faraway clouds.
    Besides video, augmented reality is another sort of material that is extremely
    dependent on round trip length and network availability. MEC can offer optimal
    QoS, particularly for 3D picture files and other large or complex contents, by
    storing those locally rather than relying on central cloud facilities and core
    network infrastructure. Zhong et al. (2021) intended to reduce the average task
    executing time in the MEC system by taking into account the variability of task
    requirements, caching of distributed applications, and access point or base station
    cooperation. Ugwuanyi et al. (2021) presented and examined a novel system Predictive-Collaborative-Replacement
    (PCR) for managing content caching within MECs that incorporates proactive prediction,
    coordination across MECs, and a substitution algorithm. Tefera et al. (2021) investigated
    a decentralized adaptable resource-aware communication, computation, and the caching
    system based on Deep Reinforcement Learning (DRL) that can orchestrate varying
    network settings. MEC''s capacity limits its ability to cache all versions of
    popular streaming. Video transcoding alleviates this issue to some extent by transforming
    the higher attainable video bit rate to such a required lower one; nonetheless,
    transcoding a significant amount of videos simultaneously might soon exhaust the
    accessible edge processing capacity. As a result, caching adequate video bit rates
    which can serve the greatest number of network subscribers is a difficult task.
    To tackle this challenge and make better use of network edge facilities (processing
    and storage), Kumar et al. (2020a) developed a non-traditional strategy for video
    caching that makes use of network information offered by MEC''s Radio Networking
    Information (RNI) Application Program Interfaces (APIs). Ding et al. (2019) presented
    an edge content distribution and update (ECDU) mechanism. The researchers implemented
    a variety of content hosts in the ECDU architecture to store raw content gathered
    from mobile users, as well as cache pools to preserve content that is often queried
    at the network''s edge. 7. Contemporary supporting technologies for MEC 7.1. Multiple
    access technologies 7.1.1. Non-Orthogonal Multiple Access (NOMA) NOMA is a potential
    multiple access technology for forthcoming wireless communications technologies.
    NOMA enables several users to utilize the single resource block by utilizing successive
    interference canceling (SIC) (Caceres et al., 2022) and superposition coding (SC)
    (Chung, 2021) at the receiving and transmitting ends, respectively. As a result,
    the advantageous qualities of NOMA concerning the allocation of resources and
    interference reduction have been thoroughly examined in numerous situations that
    are tempting to cope with the aforementioned issues of combining connectivity,
    sensing, and computational functionalities. As a result, assessing the potential
    advantages of the NOMA scheme in multi-functional communication systems is quite
    valuable. Because of its greater spectral efficiency, NOMA has been regarded as
    a major enabling innovation in next-generation communication systems. On one aspect,
    the NOMA principle fundamentally alters the layout of potential multiple access
    systems. In particular, unlike traditional orthogonal multiple access (OMA), which
    assigns orthogonal bandwidth spectrum resources to user devices or subscribers,
    NOMA facilitates users to utilize the same spectrum, with multiple access interference
    managed by advanced transceiver configurations. As a result, as opposed to OMA,
    NOMA provides greater flexibility for effectively utilizing finite bandwidth resources
    (Dai et al., 2018). NOMA is broadly categorized in two variants (Budhiraja et
    al., 2021a): code-domain NOMA (CD-NOMA) and power-domain NOMA (PD-NOMA). CD-NOMA
    employs user-specific sequences to share the total available radio spectrum, whereas
    PD-NOMA takes use of user channel gain variances and multiplexes subscribers in
    the power domain. Another notable newly developed variant of NOMA is cognitive
    radio-NOMA (CR-NOMA) (Do et al., 2020a). Sparse code multiple access (SCMA), orthogonal
    frequency-division multiple access (OFDMA), code division multiple access (CDMA),
    and multi-user shareable access (MUSA) are instances of code-domain oriented access
    techniques (Rebhi et al., 2021). NOMA can handle more subscribers than available
    sub-channels, which contributes to a variety of benefits such as huge connectivity,
    lower latency, improved spectral efficiencies, and improved channel feedback.
    With the provision of unique possibilities, the integration of NOMA and MEC may
    considerably increase user satisfaction and system performance. While NOMA has
    several benefits in terms of improving bandwidth efficiency and cell-edge connectivity,
    soothing the channel feedback demand, and lowering transmission latency, MEC provides
    significant benefits not just for subscribers, but also vendors and third-party
    providers, and allows for improved overall network performance. NOMA and MEC together
    can offer low-latency connectivity. Since the 5G and beyond communications infrastructure
    will not be entirely constructed around one technology, it is crucial to manage
    the infrastructure from a variety of viewpoints, including air interface, network
    design, and enabling techniques. NOMA and MEC seem to be two potential options
    for dealing with latency needs. MEC relocates cloud services and processes to
    the edge of the network, where the majority of data is created and managed. As
    a result, MEC enables edge applications to better fulfill end users'' reduced
    latency requirements as opposed to cloud computing. Similarly, flexible scheduling
    as well as grant-free accessibility in NOMA offer decreased communication delays
    for 5G and beyond network users. NOMA and MEC may be coupled in a variety of ways
    with various current wireless technologies, including mmWave connectivity (Qi
    et al.), multi-input multi-output (MIMO), massive MIMO, and so on, to improve
    connection, spectrum efficiency, energy consumption, and computing capabilities
    (Yılmaz and Özbek, 2023). Massive MIMO, as an example, may dramatically boost
    the spectral performance of wireless networks by extensive spatial multiplexing;
    hence, massive MIMO-NOMA can allow vast connectivity while maintaining excellent
    spectral efficiency. MmWave frequencies can be utilized for wireless communication
    systems to provide gigabit-per-second data speeds. The substantial gains produced
    by massive MIMO can compensate the severe path loss induced by mmWave. As a consequence,
    NOMA and MEC may be used in conjunction with the mmWave-based massive MIMO scheme
    to permit numerous devices to offload computational workloads at higher data rates.
    Since communication speed is vital in a ubiquitous computation scenario, the use
    of NOMA to assist work offloading has drawn considerable interest. In particular,
    the effect of NOMA on workload offloading was assessed in (Qian et al., 2020)
    using several asymptotic performance evaluations. In (Zheng et al., 2021; Cheng
    et al., 2020; Yang et al., 2020a) authors investigated and showed the benefits
    of NOMA in diverse MEC systems and cache-aided MEC systems, correspondingly. The
    authors of (Du et al., 2020a) and (Nduwayezu and Yun, 2022) investigated communication
    and computational resource management of NOMA-MEC systems, demonstrating the benefits
    of NOMA in improving energy efficiency. The authors of (Ding et al., 2022) made
    a new contribution by devising a unique hybrid offloading strategy for MEC systems
    considering the perspective of energy efficiency. 7.1.2. Rate Splitting Multiple
    Access (RSMA) RSMA is developed as a kind of multiple access approach for forthcoming
    wireless communication systems for non-orthogonal transmissions, interference
    mitigation, and rate optimization. RSMA has been viewed as a possible key enabler
    for the 6G wireless communication due to its high spectral/energy efficiency,
    dependability, and resilience for both uplink and downlink multiple-user communications.
    Inter-user interference is being partially decoded and handled as noise in the
    downlink or downward RSMA, and this not only optimizes decoding efficiency and
    complication, but also allows flexibility to connect NOMA and space division multiple
    access (SDMA). Using channel second-order characteristics the work (Mao et al.,
    2018), suggested a more generic downlink rate-splitting scheme for the MIMO technology
    with imperfect or insufficient CSI at the transmitters. As opposed to NOMA and
    SDMA, message prioritization is advantageous for successfully controlling power-domain
    distortion and achieving a better spectral efficiency for downlink communication.
    The work (Bansal et al., 2021) deployed an IRS to support the downlink RSMA technology,
    which outperformed the IRS-assisted downlink NOMA scheme in terms of outage performance
    (Mishra et al., 2022). suggested the RSMA-aided downlink transmission architecture
    for cell-free massive machine-type connectivity. Contrary to the downlink or downstream
    RSMA, the split signal streams or radio waves from multiple devices in the uplink
    or upstream RSMA are completely decoded at the base station receiver utilizing
    SIC (Kumar et al., 2022; Yang et al., 2019b; Li et al., 2020a; Abbasi and Yanikomeroglu,
    2021). The advantage of using uplink RSMA over uplink NOMA using SIC processing
    is that it achieves the complete capacity range of multiple access channels or
    streams (Abbasi and Yanikomeroglu, 2021). Yet, RSMA implementations for uplink
    media access control (MAC) remain in their early stages. Since the split data
    streams of multiple uplink users greatly increase detection complications at the
    receivers, the optimal effectiveness of uplink RSMA is heavily dependent on rate-splitting
    settings as well as the SIC decoding sequence at the receiver end (Arora and Jaiswal,
    2022), (Yang et al., 2021b). Liu et al., 2022a presented a rate-splitting strategy
    for an upstream CR-NOMA system to decrease user scheduling complexities, in which
    the divided signal streams may be decoded sequentially even with the identical
    received power level. Reference (Liu et al., 2020a) proposed a rate-splitting
    strategy for an upstream NOMA system to increase user integrity and outage performance.
    Taking into account the partial rate limitations across subscribers (Yang et al.,
    2022b), optimized the sum rate in the case of uplink RSMA. With the assistance
    of minimum mean squared error (MMSE)-based SIC (Zeng et al., 2019b), utilized
    an upstream RSMA to assure max-min user integrity in single-input multi-output
    (SIMO) NOMA systems. The work (Santos et al., 2021) utilized uplink RSMA for physical-level
    network slicing to provide ultra-reliable and lower-latency (URLLC) and enhanced
    or improved mobile broadband connectivity (eMBB). References (Kong et al., 2022a),
    (Jaafar et al., 2020) utilized uplink RSMA in aerial networks to improve communication
    reliability, reduce interference, and improve weighted sum-rate effectiveness.
    Moreover, the research (Kong et al., 2022b) utilized the utility of RSMA for satellite
    communications by integrating beamforming and uplink RSMA. Reference (Khisa et
    al., 2022) introduced a collaborative RSMA for uplink user collaboration. The
    work (Tegos et al., 2022) examined outage behavior for a two-user upstream RSMA
    system taking into account all potential SIC decoding orders. Although NOMA-MEC
    allows several users to transfer their computing workloads to a base station at
    the same time, the implemented NOMA with SIC may not attain the entire capacity
    domain of uplink multiple access streams, limiting the system efficiency of NOMA-MEC
    systems. To reach the full capacity domain of uplink multiple access streams extensive
    research is necessary to establish the appropriate SIC decoding sequence and transmit
    power assignment. The works (Ou et al., 2023), (Abbasi and Yanikomeroglu, 2023),
    therefore, analyzed the deployment of uplink RSMA in latency-aware MEC scenarios.
    To use RSMA for uplink MACs, efficient interference control, and low-complexity
    SIC handling are on the horizon. The work (Xiao et al., 2023a) proposed a CR-inspired
    rate-splitting scheme to maximize the achievable rate of a secondary user (SU),
    meanwhile maintaining the primary users’ (PU) outage performance the same as orthogonal
    multiple access (OMA). Inspired by the capability attained by uplink RSMA as well
    as CR-inspired rate-splitting the work (Liu et al., 2022b) envisioned an RSMA-assisted
    MEC (RSMA-MEC) strategy to boost the successful computing probability (SCP) and
    minimize offloading delay in a MEC mechanism. A single MEC host and many randomly
    deployed users comprise the network. Since user or device geo-positions have a
    substantial influence on channel conditions, acceptable and realistic location
    modeling is required to assess system efficiency. As a result, the work considered
    that the positions of randomly dispersed users pursue a uniform Poisson Point
    Process (PPP) (Liu et al., 2022b). Recognizing the impact of user geo-positions
    and related inter-user interference, it separated users into a centralized and
    an edge or periphery cluster to construct a paired-user CR-based RSMA and utilized
    it to facilitate MEC. 7.2. Unmanned Aerial Vehicle (UAV)-Assisted MEC Typical
    grounded infrastructure-based MEC systems are inapplicable in dense urban or non-rural
    regions or environmental disaster zones since establishing network services in
    these hostile conditions is typically inefficient. Admittedly, aerial edge computation
    mechanism, namely, UAV-enabled airborne MEC paradigm obtained increased research
    interest from both business and academia (Li et al., 2019b). Because of UAVs''
    inherent characteristics such as on-demand implementation, relatively inexpensive,
    configurable maneuverability, line-of-sight (LoS) communication, high gliding
    altitude, etc., UAVs exerting as aerial MEC stations can be used in a variety
    of scenarios ranging from civilian to military for important tasks. When the servers
    integrated with base stations are overcrowded or inaccessible, aerial or UAV-aided
    MEC systems may often act as a supplement to terrestrial MEC networks. The LoS
    communication and mobility of UAVs, in particular, can considerably minimize task
    offloading latency as well as energy usage for MEC platforms. Fig. 12 depicts
    UAV-assisted MEC infrastructures. Download : Download high-res image (539KB) Download
    : Download full-size image Fig. 12. UAV-assisted MEC infrastructures. As a result,
    the merging of UAVs with MEC is considered to be a win-win solution for next-generation
    communication systems, playing a critical role in delivering flexible and omnipresent
    communication and computing services in a variety of circumstances (Li et al.,
    2023a), (Sharma et al., 2020). When compared to traditional ground infrastructure-based
    MEC solutions, UAV-aided MEC has demonstrated significant benefits, owing mostly
    to the unique characteristics of UAVs (Song et al., 2022a). Some of the primary
    benefits of UAV-aided MEC are stated below: Ad-Hoc and Cost-Effective Implementation:
    Due to UAVs'' better scalability and ease of deployment, UAV-aided MEC frameworks
    can be rapidly dispatched at reasonable costs in response to real-time requirements
    and offer computation offloading potentials to users with constrained local computing
    capacities, particularly in areas where connectivity facilities are infrequently
    distributed or even completely destroyed. Coverage and Computational Performance
    Optimization: Since UAVs often fly at a higher altitude; a vast region may be
    successfully served with a small subset of UAVs. More crucially, with the use
    of inter-UAV linkages, a swarm of UAVs, i.e., a flying ad hoc network (FANET)
    may collectively accomplish activities in large-scale locations. While performing
    as an auxiliary computational service vendor, MEC servers enabled by UAVs can
    also considerably increase computation capacity in hotspot locations, allowing
    more users to be supported through powerful computational services. Consistent
    LoS Offloading Connectivity: Besides increased coverage, another advantage of
    greater cruising altitudes for UAVs in airborne MEC is the increased likelihood
    of LoS connections. As compared to terrestrial fading mediums, LoS connections
    can provide more dependable wireless connections for workload offloading and computational
    outcome downloading, meeting MEC''s rigorous QoS criteria. Energy Usage and Delay
    Reduction: Controllable mobility of UAVs offers an extended technical dimension
    of freedom (DoF) for airborne MEC as compared to ground infrastructure-based MEC
    platforms. Better channel characteristics can be achieved by UAV trajectory adjustment.
    The offloading energy usage and task delay for UAV-aided MEC may thus be greatly
    decreased when combined with suitable resource allocation algorithms. UAVs are
    classed into numerous categories based on their size, weight, wing design, flight
    time, and altitude (Song et al., 2022a), such as higher altitude platforms (HAPs)
    vs. lower altitude platforms (LAPs), rotary-wing vs. fixed-wing, big vs. small
    or miniature UAVs, and so on. Depending on the specific application circumstances,
    many varieties of UAVs can indeed be operated for UAV-assisted MEC. Fixed-wing
    UAVs, including small airplanes, have faster flight speeds, longer ranges, and
    can carry heavier weights than rotary-wing UAVs; however, they must constantly
    move forward to stay aloft. Hence, fixed-wing UAVs often fly ahead indefinitely
    and are utilized to cover a broad region while providing computing facilities.
    Rotary-wing UAVs, on the other hand, like quadrotor drones, may hover over a specified
    spot. Furthermore, loads of rotary-wing-based UAVs are generally tiny because
    of their tiny shape and weight. The main advantage is that they can land and take
    off vertically without the need for an airfield or launcher, allowing for faster
    and more flexible deployment. HAPs typically fly up to 17–22 km altitude and are
    intended for long-term services (days or months) over wide geographic regions
    (Gao et al., 2021). The advantage of LAPs is their inexpensive cost and quick
    deployment or replacement, notwithstanding their limited computing capacity. As
    previously stated, UAVs can function as airborne base stations incorporating MEC
    workstations to deliver computational services to wireless users in places where
    network infrastructures are unevenly dispersed or even completely destroyed. Furthermore,
    while infrastructure-based MEC implemented within a preset territory may be unable
    to adjust with time-varying configurations of service demands, UAVs can be quickly
    deployed to designated locations to address temporary or unanticipated demands.
    UAVs, on the other side, can operate as relays to help users offload tasks to
    more capable distant MEC nodes with two or even more hops. Since UAVs may modify
    their placements to experience optimal channel conditions, UAV relays provide
    additional options for a performance increase when compared to traditional static
    relaying. Owing to a single UAV''s limited processing capacity, numerous UAVs
    can collaborate to enhance the coverage area and computational capability, where
    extensive design is needed for cooperative computing procedures. Additionally,
    using inter-UAV linkages, a swarm of UAVs may be built as a FANET. In this context,
    task bits produced by small UAVs can be transferred to a lead UAV with rich computational
    power for real-time computing. Moreover, workloads from ground users can be assigned
    among numerous UAVs within the FANET. LAPs can potentially be combined with HAPs
    and ground infrastructure to establish an information network. Since HAPs own
    a broad outlook on the entire system, they are accountable for offering omnipresent
    connectivity and computing services, whereas LAPs complement the terrestrial MEC
    systems and ensure high QoS standards (You et al., 2022), (Bashir and Mohamad
    Yusof, 2019). Inspired by the great versatility of UAVs Jiang et al. (2023) investigated
    a multi-UAVs-aided MEC platform in which UAVs perform two roles, contributing
    to facilitating computation or functioning as relays, to minimize total usage
    of time and energy. As a result, an optimization approach is devised to reduce
    the total power utilization of the MEC unit. The problem is then defined as a
    Markov process of decision-making. Afterward, two reinforcement learning approaches,
    Q-learning as well as competitive Deep Reinforcement Learning are suggested to
    find the best policies for computational offloading as well as resource allocation.
    Diao et al. (2021) investigated a UAV-assisted MEC platform with a multi-antenna
    base station. The work explored offloading optimizations and scheduling solutions
    for energy consumption reduction. Apostolopoulos et al. (2023) proposed a unique
    data-offloading decision-making paradigm in which users can partially offload
    their content to a sophisticated MEC system comprised of both terrestrial and
    UAV-mounted MEC workstations. Zhang et al. (2021a) presented and analyzed UAV-assisted
    MEC system in which a hovering UAV as well as a terrestrial base station endowed
    with computational capabilities serving a variety of ground UDs. The system intended
    to reduce the weighted expense of time delay and energy usage while keeping offloading
    selections and resource competition in mind. 7.3. Energy harvesting, Simultaneous
    Wireless Information and Power Transfer (SWIPT), and MEC The modern industrial
    environment is more conscious of its obligation to optimize energy consumption
    and management across all areas, including communications. Energy harvesting (Williams
    et al., 2021), often referred to as power harvesting or energy scavenging is a
    viable technology for 5G networks as it provides a viable alternative to standard
    energy supply sources (Wu et al., 2017). The primary idea behind energy harvesting
    is to harvest various accessible energy sources and use them to power energy-constrained
    devices to extend their lifetime. Energy harvesting, in conjunction with the conventional
    power grid, can assist in meeting the energy demands of the different layers of
    5G and beyond 5G networks, such as sensing devices in IoTs, portable devices,
    base stations in HetNets, helping relays in D2D networks, and computing servers.
    Furthermore, new advancements in sophisticated materials and equipment designs
    aid in the realization of energy-harvesting circuitry for tiny portable consumer
    electronic devices, accelerating the use of energy harvesting for IoT devices.
    Energy harvesting is simple in principle but complex in execution, which is heavily
    influenced by the sort of energy harvesting source. Harvestable energies can be
    derived from environmental or man-made sources that are either controlled or uncontrolled.
    To exploit the respective energy sources, several energy harvesting technologies
    (e.g., pyroelectrics, electrostatics, photovoltaic conversion, piezoelectrics,
    thermoelectrics, and radio frequency (RF)-based energy harvesting) can be used.
    Moreover, various devices may harvest varying amounts of energy; for instance,
    wearable devices (Liu et al., 2022c), intelligent footwear (Gupta et al., 2023),
    etc. RF transmissions are less impacted by climate or other ambient environmental
    variables than conventional natural energy resources. As a consequence, these
    signals may be easily managed and planned, and radio frequency-based energy harvesting
    has a high potential to offer steady energy to low-energy systems such as IoT
    devices, wireless sensing networks (WSNs), and remote geographical area connectivity
    scenarios in 5G and beyond networks. Radio frequency-based energy harvesting can
    scrounge wireless energies from (i) ambient resources (e.g., FM, AM, and WLAN)
    that can be predictable or sometimes uncertain, or (ii) specialized sources that
    are placed to perform as an energy supply. Radio frequency-based energy harvesting
    from ambient sources generally necessitates an intelligent mechanism that monitors
    communication frequency ranges and time frames for harvesting possibilities. Wireless
    Power Transfer (WPT) may be regarded as radio frequency-based energy harvesting
    with effective management of specialized energy sources between emitters and harvesters.
    SWIPT works in the same way as a standard wireless telecommunications network
    that consisted of a base station and several mobile user devices. Antenna arrays
    are installed upon that base station. The radio frequency signal resource provided
    by the base station may be utilized to transport both energy and information.
    Every mobile station changes communication modes randomly to capture information,
    energy, or perhaps both. In particular, under the D2D scenario, power and information
    can be delivered in both bi-directional orientations between smart devices. SWIPT
    technology combines WPT with Wireless Information Transmission (WIT). Nicola Tesla
    introduced the WPT technique in 1914 (Hui, 2016). It is currently transformed
    into two primary branches: far-field WPT-based electromagnetic (EM) emission and
    near-field WPT-based EM interaction, all of which are gaining popularity due to
    their safety, versatility, and environmental friendliness. For quite a long time,
    academics have been interested in near-field WPT transmission systems because
    of their close to 90% power transfer efficiency (PTE) and tens of kilowatt transmission
    capacity (Li et al., 2020b). The failure to fulfill mobility, on the other hand,
    is a fatal flaw. When compared to cable charging, the advantage of near-field
    WPT appears to be minor. Neither the issue of battery scarcity nor the hassle
    of recharging can be alleviated. Far-field WPT, particularly when paired with
    WIT technologies, on the other hand, presents a viable solution to the dispute
    between the high rate of transmission and extended lifespan of battery-powered
    devices in 5G and beyond communication systems while satisfying wireless charging
    standards for mobility. To date, most manufacturers have focused solely on WPT
    technology, particularly near-field WPT systems, which have seen significant commercial
    success in recent years. Some prominent handset manufacturers have announced the
    availability of smartphones that feature coupling-based WPT, while electric vehicle
    (EV) manufacturers are eager to file patents enabling coupling-based wireless
    recharging. The advancement of distant range WPT is not being considered. As a
    result, this survey intended to present the commercialization development of far-field
    WPT. Unlike the Wireless Power Consortium (WPC) which focuses solely on magnetic
    inducing or resonation technique the Air Fuel Alliance (AFA) (EPC. Taiwan Technology
    Standards Agency, 2016), the main global institution on WPT standards and technologies
    includes not only magnetic technologies but also radio frequency technologies
    to address a variety of circumstances. In the year 2019, Air Fuel Alliance Development
    Forum occurred in Shenzhen, China, on March 12th and 13th (AirFuel, 2019). On
    the forum, a few RF-based WPT devices were exhibited. These systems are developed
    based on a power emitter, a harvesting unit, and a charging station; with a maximum
    transmission power of up to 100 kW (Mahesh et al., 2021). Powercast has the greatest
    charging distance among these items or devices, reaching 24 m at a 915 MHz frequency
    (Khalifeh et al., 2021). This section introduces four common resource allocation
    schemes. Most academics concentrate on various allocation algorithms, whereas
    other antenna developers are interested in new antenna structure designs. Power
    Splitting (PS): Power splitting is the best approach to achieve simultaneous information
    and power transmission. The radio frequency signal captured by the antennas is
    segregated by the power splitting architecture with a specified power splitting
    ratio β, whereby β % of such signal goes to the decoder circuitry and (1- β) %
    passes to the battery circuitry at the same time (Hu et al., 2017). Time Switching
    (TS): Time Switching is a low-complexity design that only requires a switching
    module ahead of the receiver structure. The equipment is in the phase of information
    transfer whenever the switching circuit is switched to the decoder circuitry.
    When it changes to the battery circuitry, the state of the transfer of power is
    activated (Kang et al., 2020). Antenna Switching (AS): Antenna Switching is a
    simple construction with several antennas. It is comparable to a split antenna
    arrangement, with the exception that the receivers cannot collect information
    and power at the same time. When the energy-collecting antennas are activated,
    the entire RF signal resources are utilized to charge the batteries. While the
    information-collecting antennas are engaged, the RF resources are utilized for
    decoding (Jalali et al., 2023). Antenna Separated: Multi-antennas are used at
    the transmitters to realize the segregated transmission structure. Certain antennas
    are employed for power transfer, while others are utilized for data communication.
    Apart from that, the inside architecture of the antenna has gained recognition.
    A dual-band antenna configuration is developed in the paper (Mondal and Paramesh,
    2020) to broadcast mmWave signals at two separate frequencies. Furthermore, high
    frequency is utilized for power transfer while low frequency is employed for transmitting
    information. A comparable dual-band antenna may be installed equitably on terminal
    devices. Scientists and engineers have suggested SWIPT-based MEC systems to enable
    all devices to experience real-time and high-throughput facilities (Ponnimbaduge
    Perera et al., 2018). SWIPT technology enables networks to deliver the computation
    result and energies to the device through the downlink at the same time. Moreover,
    this method can compensate for the issue of high attenuation in typical long-distance
    wireless power transmission (Zhang et al., 2021b), (Liang et al., 2019). Typically,
    the two most common SWIPT transmission mechanisms are Power Splitting and Time
    Switching. Although the MEC server''s energy transmission is adequate for devices
    near the base station (BS), it is insufficient for devices that are perpetually
    outside of the MEC service range. Insufficient power causes the equipment to go
    out of operation abruptly, which is deleterious for human-embedded devices or
    safety detecting systems. To compensate for this weakness, several researchers
    proposed SWIPT-based D2D networking (Ghosh et al., 2022; Lim et al., 2023; Budhiraja
    et al., 2021b). This network enables devices with adequate energy to send energy
    to devices that have limited energy capacity. Among the foregoing radio frequency
    signal resource provisioning schemes, there is a typical and crucial difficulty
    that exists throughout the SWIPT framework, i.e., the radio frequency-energy harvesting
    tradeoff (Liu et al., 2016), (Kang et al., 2018). How much radio frequency signal
    resources ought to be dedicated to information processing to maintain excellent
    communication performance, and how much should be dedicated to energy harvesting
    to extend the device''s lifetime? Regardless of whether additional antennas are
    allocated for information transmission, the radio wave power passing to the decoding
    unit is increased, thereby, the amount of information harvesting is increased
    and all of these procedures result in the insufficient radio frequency signal
    remaining for battery charging. Consequently, in most circumstances, finding a
    suitable allocation mechanism to maximize the tradeoff is critical to the SWIPT
    mechanism. Cooperative task offloading or transferring and resource distribution
    in energy harvesting-enabled small cell network systems is discussed in (Do et
    al., 2020b), (Meng et al., 2021) to optimize the number of workloads or tasks
    executed by edge computing nodes while minimizing their latency and energy costs.
    The researchers presented a Deep RL-based methodology for online transferring
    to minimize computing complexities in large energy harvesting-driven systems in
    their papers (Ammar et al., 2022; Min et al., 2019a; Seid et al., 2022). The works
    (Wang et al., 2022d), (Shakarami et al., 2021) applied DL approaches which learn
    binary offloading selections based on previous offloading experiences and suggested
    techniques successfully improve workload offloading behavior. A decentralized
    implementation of suggested techniques, on the other hand, is still required to
    allow users to determine offloading selections in a distributed way through a
    learning process. Furthermore (Min et al., 2019b), presented a privacy-aware offloading
    technique based on the RL algorithm for a medical IoT device powered by energy
    harvesting. The policy for task offloading implemented by the edge device may
    be established by considering the degree of privacy, energy usage, and computational
    delay through each time slot. The authors investigated computational offloading
    and resource management for energy harvesting in (Xia et al., 2021; Zhao et al.,
    2021a; Hu et al., 2021). These mobile devices first gather energy from RF transmissions,
    which they may utilize to conduct local activities or transfer (tasks) to a MEC
    host. Several alternative offloading techniques can also attain self-sufficiency.
    State-of-the-art approaches for task transporting in MEC and radio power transmission
    to end terminals are reported in (Mustafa et al., 2021). The authors proved the
    efficacy of the WPT approach in charging high-end cell devices, which have become
    increasingly prevalent in MEC. Yet, when computational resources become limited,
    MEC''s performance may suffer. As a result, they emphasized the impact of deciding
    across task-offloading solutions and offloading positions on the energy consumption
    of MEC systems. 7.4. Intelligent Reflecting Surface (IRS)-Assisted MEC An IRS
    can adjust the wireless network intelligently to optimize signal strength obtained
    at the destination. This is in stark contrast to previous strategies that improved
    wireless communications by improvements at the transmitter or recipient (receiver).
    An IRS is made up of numerous IRS components, each of which may reflect the incoming
    signal at a different angle. The transmitted message in such IRS-assisted connections
    flows from the sender or transmitter to the IRS gets enhanced at the IRS, and
    then goes from the IRS towards the receiver or recipient. This type of communication
    technology is especially beneficial when the transmitter and receiver are separated
    by a weaker wireless channel caused by barriers or bad environmental circumstances,
    as well as when they do not share a line-of-sight link. Fig. 13 illustrates an
    IRS-assisted MEC transmission paradigm. Download : Download high-res image (253KB)
    Download : Download full-size image Fig. 13. IRS-assisted communication in an
    MEC paradigm. IRSs are expected to play a major function in 6G networks due to
    their capacity to configure wireless environments, according to numerous wireless
    communications specialists. In November 2018, a Japanese network operator named
    NTT DoCoMo and MetaWave, a startup, showed the application of IRS-like innovation
    for supporting radio transmission in 28 GHz frequency (NTT DoCoMo and Metawave
    announce, 2018; Pérez-Adán et al., 2021; Long et al., 2021). IRSs have also been
    compared to 5G communication systems'' massive MIMO technique. IRSs reflect radio
    signals and hence use less (active IRSs) or even no power (passive IRSs), but
    massive MIMO emits signals and requires significantly more power (Wang et al.,
    2021c; Basar et al., 2019; Hu et al., 2018b). In just the same sense, researchers
    of (Boulogeorgos and Alexiou, 2020) contrasted the energy consumption of IRS-assisted
    systems with amplify-and-forward (AF) relay networks, while the researchers of
    (Björnson et al., 2020) evaluated the effectiveness of IRS with decode-and-forward
    (DF) relays. IRSs have already been presented and widely researched in recent
    times as a viable technology for forthcoming wireless communication networks (Gong
    et al., 2020), (Wu et al., 2021b). An IRS, in particular, is a vast array of passive
    elements or components. By adaptively regulating the mirrored EM waves’ phase-shift
    that impact upon the surfaces, it can provide a desirable wireless propagation
    scenario between transmitting and receiving devices (Abeywickrama et al., 2020),
    (Wang et al., 2020b). By applying IRS in communication networks, channel power
    gain may be successfully increased and transmission QoS can be optimized without
    consuming substantial additional power. Recent breakthroughs in configurable meta-materials
    make it easier to build IRSs (Pan et al., 2021) for improving the spectrum and
    power consumption of wireless communications. An IRS, in particular, is made up
    of IRS supervisor (typically a microcontroller) and a huge number of passively
    reflecting components. According to the IRS controller''s directives, each IRS
    reflective element is susceptible to altering both the phase and amplitude of
    the mirrored waves, thereby, cooperatively modifying the data transmission environment.
    IRSs achieve gain by combining virtual array-generated gain and reflection-assisted
    beamforming gain. These virtual array gains are generated by integrating both
    the LoS and IRS-reflected radio waves, whereas the reflection-assisted beamforming
    gains are acquired by actively regulating the phase shift generated by IRS components.
    By integrating both of these distinct advantages, the IRS obtains the ability
    to increase the offloading likelihood of success of devices, hence enhancing the
    capacity of MEC frameworks. Minimizing execution latency and minimizing energy
    usage are two legitimate goals in the context of task or workload offloading in
    MEC platforms. Existing works (e.g., (Zhu et al., 2022), (Lu et al., 2022a)) on
    IRS-aided MEC mechanisms have demonstrated that properly implementing an IRS next
    to the offloading subscribers or devices and efficiently constructing the passive
    beamforming can assist the reconfiguration of the computation-load dispersion
    among users. Moreover, it can improve the device-to-MEC data transmission rates,
    thus significantly minimizing their computation-offloading latency, particularly
    when the device-to-MEC LoS links are blocked. If the primary LoS link connecting
    the wireless user devices and MEC stations is obstructed, the data can indeed
    be transferred via the IRS-assisted mirrored channel. Reflection-based beamforming
    may be implemented explicitly by concurrently adjusting the reflection parameters
    of IRSs. That includes both the enhancement of the transmission rate for cell
    edge user devices and strengthening physical-level security. Consequently, with
    the assistance of IRSs, MEC''s overall performance may be significantly enhanced
    (Sheng et al., 2020). Additionally, unlike traditional transceivers, IRSs require
    just low-complexity control circuitry rather than a high-power-based radio-frequency
    chain. As a result, they may be densely placed at a low expense and use of energy
    to facilitate ubiquitous MEC. Moreover, integrating IRSs into current MEC platforms
    does not need the development of unique conventions or MEC hardware (Bai et al.,
    2021). Zheng et al. (Zheng and Yan, 2022) explored the latency reduction challenge
    by developing IRS-assisted MEC networks. Chen et al. (2023b) proposed a unified
    dynamical beamforming approach to enhance the total computational rate of an IRS-assisted
    MEC infrastructure, whereby each device adopts a binary offloading strategy. Zhou
    et al. (Zhou et al.) investigated a collaborative task computing architecture
    in which the source node transfers some of its computational tasks to multiple
    user devices with the support of double IRSs. Chen et al. (2023c) designed and
    analyzed the IRS-aided wireless-powered MEC system in which each device''s computational
    workload is separated into two sections for local processing and offloading to
    MEC servers. Both Time Division Multiple Access (TDMA) and NOMA techniques are
    studied for offloading. Ha et al. (2022) studied the effectiveness of an IRS-assisted
    uplink NOMA-based MEC system considering a Nakagami-m fading channel. 7.5. Game
    theory for MEC Game Theory has already been utilized successfully to develop,
    design, and improve the functioning of numerous representative telecommunications
    and networking systems. The games in such situations, as usual, contain a diverse
    set of participants with competing objectives. Game theory (Tushar et al., 2023)
    is a mathematical application that is employed in the analysis of the tactics
    used by various participants while making decisions, resulting in an increase
    in reward among them as well as an increase in the reward for a specific player.
    O. Morgensterns and J. von Neumann were the initial researchers to employ games
    in zero-sum contests, which are games with just one participant (Chi et al., 2022).
    Cooperative game theory refers to a type of game whereby a set of players'' activities
    and formation impact the outcome of the game. Non-cooperative plays or games,
    on the opposite hand, do not need players to work together to decide the game''s
    conclusion. Each participant can perform their actions, and the ultimate consequences
    are determined by their activities (Cheng and Yu, 2019). Furthermore, in non-cooperative
    games or plays, each participant can pick from a list of rules. In simple words,
    Game Theory is a field of practical mathematics that explores how rational individuals
    may interact amongst themselves to acquire stable management of system facilities
    to satisfy the service objectives (of those players) when faced with a combination
    of common and finite network resources. Game Theory investigates the interactions
    of self-interested and autonomous actors. There has been a significant amount
    of studies in wireless network technology over the past few years, as well as
    at the present moment, there is a great interest in applying Game Theory in wireless
    communications (Dasilva et al., 2011), such as cognitive radio or spectrum sensing
    (Liang et al., 2017b), sensor networks (Mkiramweni et al., 2019), (Sun et al.,
    2021), and multimedia social networks (Song et al., 2020). Game Theory can provide
    helpful recommendations for administering wireless data connections to handle
    the rising MEC issues, such as energy consumption, storage, virtualization, and
    computation at the edge of the network. Game theory, which is augmented with the
    assistance of learning algorithms (Houda et al., 2022), contributes significantly
    to the management of parameter configuration, as well as their modeling and evaluation.
    Initially, the work will go through the benefits that non-cooperative (NC) Game
    Theory may utilize in a variety of wireless network domains. In network resource
    distribution, non-cooperative Game Theory helps to improve the strategy of using
    a shared pool amongst a group of customers that wish to use a limited number of
    resources fairly. This means that non-cooperative Game Theory enables elastic
    and scalable resource management. The fundamental issue in power control is distinguishing
    and reducing signal interference. Without non-cooperative Game Theory, energy
    usage is enormous in this case, and non-cooperative Game Theory assisted in resolving
    this issue. Non-cooperative Game Theory improves the management of a specific
    communication link that will be shared by multiple users in the context of MAC.
    Multi-rate adaptive forwarding with non-cooperative Game Theory is used to direct
    traffic across a network to maximize end-user performance. For security, non-cooperative
    Game Theory is employed to maximize network longevity while allowing low-power
    computing and IoT connectivity. Together with this, wireless nodes stay competitive
    against one another for a constrained shared resource. Previously an algorithm
    is utilized that overcomes the problem of adaptability as opposed to the centralized
    model. Moreover, MEC delivers computing capabilities within close proximity, to
    provide the optimum user experience (Xiao et al., 2020). Li et al. (2018b) proposed
    a Game Theoretic approach for power management in interference-aware multiple-user
    MEC systems. The research (Chen et al., 2021) investigated relative delay minimization
    in MEC-enabled UAV clusters. The optimization issue is described as an offloading
    game to address the challenge in dispersed UAV networks. Wang et al. (2021d) examined
    the topic of computational offloading in MEC systems in the case of vehicular
    communications and presented two algorithms: the game-based computational offloading
    (GBCO) algorithm and the optimum offloading algorithm. Li et al. (2020c) presented
    a Game Theory-based method for task/workload offloading and resource management
    in the MEC framework to minimize energy consumption and delay. In (Li et al.,
    2021c) game theory and DRL are utilized to offload computational tasks in a dynamic
    edge computing paradigm. Xia et al. (2023) proposed Game Theory-based mobility-aware
    computation offloading and resource management strategies for MEC systems. 7.6.
    Auction Theory and MEC Despite its many advantages, MEC exhibits significant resource
    management challenges. Edge servers in MEC, in particular, often have constrained
    storage, computing, and network resources, but end users or subscribers have rapidly
    increasing processing needs. As a result, one difficulty is determining how to
    optimally distribute services to users. Furthermore, the MEC invites new service
    providers or vendors into the computing market. Since both vendors and users are
    inherently selfish (Chen et al., 2020) motivating vendors and users to engage
    in the market is a different challenge. Auction Theory, a prominent economic technique,
    has been frequently employed to handle similar challenges, for example, in wireless
    communications (Jiang et al., 2014a). Auction-based systems, in particular, are
    promising because they may fairly and effectively allocate sellers'' constrained
    resources to purchasers in a transactional form at competitive pricing. Trustworthiness,
    trade balance, independent rationality, and economic productivity are all desired
    qualities of an ideal auction-based system (Wu et al., 2022b). Auction-based processes
    ensure that resources are distributed to the purchasers who place the most worth
    on them. Considering these benefits, various works have lately utilized Auction
    Theory to tackle the resource management problem in MEC. It is based on previous
    research that different sorts of auction techniques are appropriate for various
    kinds of issues in certain application contexts. 7.6.1. Terminologies of auction
    mechanisms The following are the standard terminologies and classifications of
    auction mechanisms used in auction literature (Zhang et al., 2013): Auctioneer:
    An auctioneer is often an executor who implements the auction mechanism and decides
    the champions and payments including both bidders (purchasers or in this sense
    users) and sellers (in this context MEC service providers) based on the auction
    guidelines. The auctioneer in the MEC marketplace might be a resource provider
    or a trustworthy third party. Bidder: A bidder is indeed a purchaser who wants
    to buy products or services from vendors. In the MEC marketplace, user devices
    often operate as buyers looking to acquire a variety of resources (e.g., computing,
    caching, content delivery resources, etc.) to complete compute-intensive and latency-sensitive
    computational workloads. Auction participants include both bidders as well as
    sellers. Seller: A seller is the proprietor of the auction items or services who
    wish to sell those at a specific price to maximize profit. Edge servers often
    operate as merchants (or sellers) in the MEC marketplace, owning a variety of
    commodities like computation and storage facilities, networking bandwidths, and
    so on. Price: Typically, the price was determined by competitiveness during an
    auction procedure. It might be an initial price at which a seller consents to
    sell the product or resource, a bidding price, it is the price at which a buyer
    consents to pay for the resource, or a closing price (the transaction price),
    which is the ultimate purchasing price in the event. Commodity: A commodity is
    a trade object between a supplier and a purchaser in an auction. Sellers compete
    with buyers to sell a valuable product at the best possible price. Products in
    the MEC market might be computational services, caching services, or network services.
    Auctions can be classified in a variety of ways, both in theory and in practice
    (Huang et al., 2022b), however, they are frequently classed according to the following
    characteristics (Shoham and Leyton-Brown, 2008): Single-vs. Two-Sided Auctions:
    Only one party (buyers or sellers) can put bids during a single-sided (i.e., forward
    and reverse) auction. In the context of a two-sided auction, both sides can put
    bids. Uni-vs. Multi-Attribute: Allotments in uni-attribute bidding or auction
    are defined only by one characteristic (e.g., bid price). On the other hand, under
    a multi-attribute auction, additional qualities (for example, QoS) are also taken
    into account. Open-Cry vs. Sealed-Bid: At open-cry bidding, each bidder screeches
    out his bid, but in a sealed-bid context, each bid is placed discreetly. Single-vs.
    Multi-Item (Combinative) Auctions: A single-item bid trades only one kind of product
    on the marketplace, whereas a combinative auction trades numerous sorts of things.
    Single-vs. Multi-Unit: In the context of a single-unit bid, just one unit of every
    item may be traded; however, many units can indeed be marketed or traded throughout
    a multi-unit auction. An auction process can be characterized further according
    to the periods at which the marketplace is cleared (Shoham and Leyton-Brown, 2008).
    Offline Auction or Bid: Once all bids have been received, the marketplace is cleared.
    Online Auction: Buyers come and leave without notice, and the auctioneer eliminates
    the market in real time as soon as an updated request is received or a new option
    becomes available. Sequential Auction: Buyers can place several offers, as well
    as the auctioneer eliminates the marketplace regularly within a selected time
    constraint. The primary purpose of auction technique design is to establish auction
    guidelines so that a targeted objective is attained in the convergence of selfish
    actors'' behavior. As a result, with an auction mechanism that is properly built,
    the necessary auction aims will be easily attained while selfish actors pursue
    their motivations and preferences. The following portion describes a few of the
    desired qualities in auction mechanism development. Trustworthiness: Guarantees
    that participants'' fairness is within their best interests, and hence they cannot
    gain from dishonesty (He et al., 2018). Independent Rationality: By applying the
    technique, any participant can raise his or her benefit (Kumar et al., 2021).
    Trade Balance: The market neither builds surpluses nor runs deficits (Diamanti
    and Papavassiliou, 2022). Economic Productivity: The maximization of value across
    all individuals (social welfare) entails a socially optimal distribution of resources.
    Furthermore, if the utility loss tends to be null as the highest social benefit
    approaches indefinitely, the mechanism is referred to as Asymptotically Efficient
    (Kumar et al., 2021). Computational Tractability: The auction process clears the
    marketplace by discovering commodity allocation and price within a tractable calculation
    time (Kumar et al., 2021). 7.6.2. Auction approaches for MEC In general, the MEC
    trade consists of several sellers or vendors and numerous buyers or user devices,
    with buyers requiring sellers'' resources to complete computational-intensive
    and latency-sensitive activities. Because of its many-to-many structure, multi-item
    auction or bidding is widely used to handle resource trade in the MEC trade (Diamanti
    and Papavassiliou, 2022). Furthermore, in the MEC trade, user devices often bid
    for a combination of resources (e.g., computation, storage, networking, energy,
    and so forth), and combinative bidding (Zhang et al., 2017a) is an appropriate
    method. As a result, the next sections introduce combinative (Xu et al., 2020)
    and multi-attribute auctions (Xiao et al., 2023b), (Zeng et al., 2020). Combinative
    Auction: A combinative or combinatorial auction is a type of auction whereby each
    buyer bids on a variety of commodities. In contrast to typical auction systems,
    purchasers can acquire a package of combinative commodities, including many sorts
    of commodities. Each bidder or purchaser puts a bid to the auction house or vendor
    in the combinative bidding, indicating the demand for a group of products instead
    of a single item. The auctioneer provides an efficient allocation system to the
    purchasers after accumulating bids/asks given by buyers/sellers. Contemplate a
    combinative bidding in a MEC trade for computing service provision. The market
    consists of three purchasers (bidders) and one vendor (auctioneer). Purchasers
    place a bid to vendors that describe the computational resource requirements.
    Each offer, in particular, reflects the need for computing and energy resources.
    The challenge then becomes determining who the winners are and how much each winner
    must pay. Certain optimization methods, such as greedy algorithm (Wei et al.,
    2018), dynamic programming (Zhang and Fu, 2020), and graph neural network algorithm
    (Wu et al., 2021c), can be used to tackle this problem. The combinative auction
    is ideal for trading a package of complementary products and may effectively increase
    the auction effectiveness of allocating multiple commodity combinations. Multi-Attribute
    Auction: It is commonly used to solve optimum resource allocation issues in MEC.
    Unlike traditional auctions (for example, Dutch auction (Pop et al., 2020), English
    auction (Pop et al., 2020), first-price and second-price-based sealed or enclosed-bid
    auction (Wang and Xiang, 2014)), a multi-attribute auction is a many-to-many framework,
    meaning that the number of vendors and purchasers is even more than one. For a
    multi-attribute auction, vendors and purchasers submit their inquiries and bidding
    prices to an auction house, which acts as the auctioneer''s agent. The auctioneer
    then organizes the asks (retailer''s prices) and bids (bidder''s prices) in ascending
    and decreasing order. The auctioneer then computes the transaction price. Eventually,
    the successful bidder obtains the product and pays the associated seller. Repeating
    the preceding procedure will reveal the matching association between the remaining
    purchasers and vendors, as well as related hammer prices. 7.7. Digital twin-enabled
    MEC Digital Twin is an effective method of connecting the real and digital worlds
    (Semeraro et al., 2021). Digital Twin has undergone four development cycles as
    a result of enhanced communication and computing technologies, including information
    emulating, digital simulations, value systems chain interaction, and linked operational
    and industrial activities. To that aim, there are three types of Digital Twins,
    with specific definitions as follows: Monitoring Digital Twin: In this context,
    Digital Twin is employed to monitor a physical item (Caiza and Sanz, 2022). The
    physical item has no interaction with its virtual environment, and modifications
    to the physical entity do not influence its virtual form after it has been generated
    and vice versa. Simulation Digital Twin: Digital Twin is characterized in this
    context as a simulation tool or program (Schluse et al., 2018). Physical things
    may be understood, anticipated, and enhanced by computer simulation, allowing
    their effectiveness to be enhanced. The virtual model evolves alongside the actual
    thing, but the physical item does not alter in response to the virtual model''s
    dynamism. Operational Digital Twin: The data flow between physical items and the
    respective twins is bilateral in this situation (Örs et al., 2020). Specifically,
    physical things broadcast information and updates to their twins, which are formed
    and updated depending on that knowledge, and the actual object''s circumstances
    may be anticipated. Conversely, the twin will transmit back information, including
    ideal remedies for a specific issue, to direct physical object actions. Based
    on the preceding criteria, Digital Twin is now commonly referred to as “an intelligent
    and developing system that correctly and digitally copies a physical thing across
    several granularity layers and monitors, regulates, and improves the physical
    object throughout its life cycle” (Wu et al., 2021d). Digital Twin is made up
    of three parts: (i) tangible items, such as a car, a robot, a complicated system,
    or a human; (ii) digital twins of physical items; and (iii) connections between
    physical items and their digital twins. The physical items convey their status
    and produced sensing data to the interactive twins for Digital Twin modeling and
    updating via the link, and the interactive twins offer feedback towards physical
    objects. As a result, dynamic interactions and the synchronized evolution of the
    actual item and its virtual duplicate are possible. To that purpose, Digital Twin
    is an effective method for simulating, analyzing, forecasting, and optimizing
    the physical model throughout its entire life cycle. Fig. 14 shows the evolution
    of Digital Twin. Download : Download high-res image (647KB) Download : Download
    full-size image Fig. 14. The evolution of Digital Twin. Subsequently, a new approach
    called the Digital Twin-enabled MEC network has been developed to bridge the gap
    between the physical MEC facility and digital systems (Plageras and Psannis, 2022).
    Digital Twin models are developed and managed by Digital Twin-enabled MEC network
    with the help of the networks'' edge (Sun et al., 2020a). The Digital Twin-enabled
    MEC network is made up of three technical segments: the edge layer, the device/physical
    layer, and the Digital Twin network. Local computations and connectivity are handled
    just at the physical level, as well as the physical layer''s real-time state and
    modeling parameters are transferred to the edge layer through physical-to-physical
    communication. Real-time Digital Twin models and modifications are handled at
    the edge of the network. Edge units, such as base stations, can gather physical
    elements'' functioning status information and evolve their behavioral model depending
    on the acquired data in time-varying surroundings. Furthermore, edge nodes continually
    check the status of physical components to ensure compatibility with the respective
    twins. Digital Twin may be installed on any edge computing server, and indeed
    the edge tier is invisible to users. Conversely, all users are capable of exchanging
    edge services (with each other). Using twin-to-twin connectivity, Digital Twins
    are virtually joined to form connectivity with shared infrastructure in the Digital
    Twin network. Digital Twin-enabled MEC network can capture real-time network characteristics
    and utilize them to make optimum network decisions instantly from a centralized
    standpoint. In this purpose, the desired Digital Twin-enabled MEC network may
    be used to directly develop and optimize network strategies such as workload offloading,
    allocation of resources, caching, and so forth, and the connectivity schemes''
    effectiveness and affordability can be improved. In reality, without a Digital
    Twin-enabled MEC network, continual connections among edge computing servers and
    devices under their coverage are necessary to collect real time feedback for task
    transferring and resource allocation choices. As a result, the deployment of Digital
    Twins aids in getting optimal resource provisioning solutions while reducing communication
    costs. Digital Twin-enabled MEC network can enable computationally intensive applications
    like Metaverse (Van Huynh et al., 2022a) and automated vehicles (Guo et al., 2022),
    (Zhao et al., 2020). Fig. 15 depicts the structure of a Digital Twin service-based
    MEC platform. Download : Download high-res image (667KB) Download : Download full-size
    image Fig. 15. The structure of a Digital Twin service-based MEC platform. The
    Digital Twin-enabled MEC network framework is separated into three tiers: the
    physical layer or level or tier, the virtual tier, and the application tier. In
    a MEC framework, the physical tier includes all physical equipment, such as end-user
    devices and edge computing servers, as well as the wireless connectivity infrastructure.
    End devices have limited computing and storage capabilities, therefore tasks must
    be offloaded to one or even more edge computing platforms for cooperative computation.
    Twin mappings exist in the virtual tier such as identical or twin-end devices,
    twin edge computing hosts, and twin communication environments (networked physical
    items and wireless communication environments of the physical tier). Metadata
    such as the actual entities'' real-time status are captured in the physical tier
    and then transmitted to the virtual tier twins through physical-to-twin connections
    (Khan et al., 2022b), (Liu et al., 2022d). The virtual layer includes the Digital
    Twins that completely replicate the physical items and govern and regulate the
    physical layer through simulations, predictions, and optimizations (Liu et al.,
    2022d). The Digital Twin system records original data of physical things, such
    as hardware configurations, user details such as geo-location and available resources,
    statistical facts, and real-time MEC server operating status, and it also monitors
    the system''s dynamism. This information is vital for building high-precision
    Digital Twin models of physical things and communication environments in the virtual
    tier. In reality, however, it is difficult to mimic a physical phenomenon/entity
    virtually which will be absolutely equivalent to its physical counterpart. Moreover,
    several critical aspects must be considered, such as reliability, fault tolerance,
    reduced latency, and security. To address these difficulties, sophisticated enabling
    technologies like connectivity, data processing, management approaches, Machine
    Learning, and Blockchain may be deployed to Digital Twin modeling techniques and
    making decisions while safeguarding user privacy. Relying on the various developed
    concepts and big data, Digital Twins may aid in finding intelligent approaches
    to application layer challenges including task transferring and allocation of
    resources in IoT (Lu et al., 2021a), (Lu et al., 2021b), vehicular communications
    systems (Fan et al., 2021), (Yuan et al., 2022), the space-air-ground integrated
    network (SAGIN) (Cui et al., 2022; Zhou et al., 2022; Sun et al., 2022b), health
    services (Okegbile et al.), wireless infrastructure (Van Huynh et al., 2022b),
    and so on. Additionally, effective interconnections between Digital Twin-enabled
    MEC network system levels are necessary to connect physical things, twins, and
    services (Lu et al., 2021c). The twin-to-physical entity interface, in particular,
    allows real-time interactivity between both the Digital Twin tier and the physical
    tier. The system''s broad information may be retrieved by communicating between
    twins via twin-to-twin integration. Moreover, services such as IoT, transportation
    networks, and healthcare seek a function from the Digital Twin-enabled MEC network
    platform via twin-to-application interactions, and the virtual tier feeds back
    the ideal selections to the application tier. Communications are essential for
    interconnectivity and have a direct influence on system performance, particularly
    throughput, transmission delay, security, and so on. A network''s connectivity
    resources are typically limited or constrained. Attempting different decisions
    in a real-time network is quite difficult. Admittedly, one can execute various
    communication activities in a virtualized edge network using a Digital Twin-enabled
    MEC network, and then acquire the ideal operation characteristics to report the
    actual network to achieve optimal system performance, including transmission rate,
    bandwidth utilization, security, and so on. Numerous studies are concentrating
    on employing Digital Twin to enhance the network system performance. In (Cai et
    al.), the Digital Twin-enabled MEC network is used to determine the best transmission
    resource allocation to reduce traffic congestion. MEC may be utilized to process
    some computing activities and ease the strain on network computational resources.
    However, networks’ computational resources are constrained; therefore, it is still
    challenging to handle intensive computing tasks to serve upcoming programs including
    Metaverse, Augmented Reality (AR), Virtual Reality (VR), XR, etc. Sophisticated
    edge computing has recently evolved as a result of combining MEC with AI, whereby
    multiple AI models are built using various ML approaches, including DL, RL, and
    Transfer Learning, which demand a large amount of computational resources to train
    as well as update the model. Digital Twin-enabled MEC network can be used to boost
    computing performance (Groshev et al., 2021). By building Digital Twin-enabled
    MEC networks for such a cloud infrastructure, the physical component and AI algorithms
    are integrated with the virtual realm to conduct task computations and model training
    or construction, and indeed the results are subsequently given back to the respective
    physical components. To that aim, the Digital Twin-enabled MEC network enables
    resource-constrained user devices and servers to process intense computational
    workloads and AI model construction or training (Yang et al.). Ozdogan et al.,
    2022 suggested a novel Blockchain-based Digital Twin-enabled 6G edge networking
    infrastructure recovery system. 7.8. Open-source MEC for 6G The primitive MEC
    design is based on specialized hardware, and its unique software functions are
    tightly interwoven with the hardware, making it too stiff to accommodate fast-developing
    situations accelerated by 6G. As a solution, a dynamic mechanism described by
    open-source software operating on general-purpose technology platforms combines
    open-source mobile networking with MEC. 7.8.1. The fundamentals of an open-source
    network Open-source (OS) network (Garrich et al., 2019) architecture is comprised
    of four key components: the cloud entity that includes the core network (CN);
    the edge context that includes decentralized MEC hosting; the diversified terminal
    aspect; and the network context that includes a RAN attaching the edge as well
    as terminal realms (users) and a transport interface joining the cloud as well
    as the edge realms (Massari et al., 2021). The open-source core network framework
    is widely described in the cloud sector. SBA, for example, has been defined as
    a fundamental component of the 5G core network infrastructure in the third-generation
    partnerships Project (3GPP) specifications, demonstrating a substantial degree
    of versatility and adaptability (Ghosh et al., 2019). SBA divides the initially
    connected network functions into numerous independent system components, each
    of which provides one or even more services that may be utilized by the other
    system components (Zeydan et al., 2022). As a result, all functional blocks may
    communicate with one another to provide users with customized services, just like
    they were linked to a single service interface. Nevertheless, due to the increasing
    real-time complexity requirements, the OS RAN framework (Papatheofanous et al.,
    2021; Hoeschele et al., 2022; Lee et al., 2021) has yet to be specified in the
    network realm. Several viable options, including RAN decoupling and reconfiguring,
    have been proposed to do this. To begin, traditional network''s tightly connected
    RAN parts, functions, and resources must be entirely separated. These might then
    be dynamically reconstructed to meet the needs of the user, resulting in personalized
    virtual RANs. Simply to be said, the OS idea enables the easy formation of 6G
    RANs using LEGO blocks (Kukliński et al., 2021; Liu et al., 2022e; Angui et al.,
    2022). RAN Decoupling: A strictly closed operating system underpins the tightly
    connected 4G wireless networks. As a result, at least four distinct RAN decoupling
    strategies, notably hardware/software detaching or decoupling (HSD) (Kumar et
    al., 2020b), the user plane and control plane separating (UCPS) (Yan et al., 2016),
    (Onireti et al., 2014), the central unit and decentralized unit partitioning or
    segregation (CDUS) (Gupta et al., 2020), (Matoussi et al., 2020), and downlink
    and uplink detaching (DUD) (Dai et al., 2019), (Xu et al., 2023), have been proposed
    for coordinating a beneficial level of accessibility in 6G. There is no universal
    consensus on the ideal open-source RAN design since they all offer various advantages
    and disadvantages. It should be noted that HSD and UCPS provide the foundation
    of open-source wireless networks or cellular network systems, while CDUS, as well
    as DUD, extend the versatility and accessibility of wireless networks. HSD decouples
    services and resources from physical infrastructure. Because of the competent
    variety of NFV technological advances, network resources may be generalized and
    redistributed among multiple users, and distinct services can be provided by the
    same architecture, thereby decoupling functionality from the underlying architecture.
    UCPS is inspired by SDN, which isolates control signals from data transport. The
    user plane and control plane are formed by installing database stations within
    higher frequency bands and controlling stations within the most favorable lower
    frequency spectrum, accordingly. From the standpoint of the open-source protocol
    layer, CDUS constitutes one of the fundamental technologies utilized in the next-generation
    or evolving cloud RAN (Chua et al., 2016), (Hasabelnaby and Chaaban, 2022) that
    focuses on splitting the typical evolved node base station (eNB) into the wirelessly
    distributed unit and the central unit (Mubarak et al., 2018). Tightly connected
    downlink and uplink transmissions limit a user''s connectivity versatility. A
    dynamic user association with high energy efficiency may be accomplished by using
    DUD through the dual connection, in which a user attaches to a small base station
    and a macro base station for its uplink and downlink transmission, respectively.
    RAN Reconfiguration: Traditional RANs have indeed been softwarized and separated
    into distinct virtual network functions utilizing RAN decoupling technology. Consequently,
    to serve a variety of applications, just the appropriate detached network functions
    will be selected and combined to build RANs on demand. Currently, at least two
    proposed ways are in practice for the efficient reconfiguration of 6G RAN, notably
    MANO and RAN slicing or splitting (Mubarak et al., 2018; Guan et al., 2021; Rico-Palomo
    et al., 2022). RAN slicing (Mei et al., 2020; Habibi et al., 2022; Azimi et al.,
    2022) enables the dynamic allocation of virtualized network functions and radio
    capacities in RAN to multiple services. RAN slicing explicitly picks the appropriate
    network functions by using allocated resources and afterward wraps them to construct
    a tailored virtual network whenever a specified service request arises. MANO is
    another successful method for establishing RAN reconfiguration, which is more
    adaptable to varied network perspectives than RAN splitting or segmentation. The
    management section is in charge of comprehensive RAN monitoring (Giannopoulos
    et al., 2021; Pham et al., 2023; Khodashenas et al., 2017), such as life cycle
    management (Giannopoulos et al., 2021), defect detection (Pham et al., 2023),
    auditing (Khodashenas et al., 2017), and security management (Olimid and Nencioni,
    2020), (Liyanage et al., 2023). The orchestrating component (Gramaglia et al.,
    2022) assists the network functions and leverages the underlying resources to
    reassemble the RAN, and it consists primarily of two sub-components: selecting
    the proper network functions and properly scheduling those for a particular service.
    RAN Capability Enhancement: MEC enables local computation and storage resources
    that may be connected well with open or accessible RAN to provide customers with
    low-latency applications while also decreasing the stress on the transmission
    network (Agarwal et al., 2023; Parvez et al., 2018; Hasabelnaby and Chaaban, 2021).
    Even though the open-source RAN framework is yet to be fully specified, an open-source
    MEC system (Ciobanu et al., 2021), (Slamnik-Kriještorac and Marquez-Barja, 2020)
    based on the aforementioned decoupling and reconfiguration techniques will be
    explained in this survey work. Additionally, to support its attractive applications,
    varied networking architecture, and various resources, 6G will become increasingly
    complicated. As a result, traditional network management approaches involved with
    human-controlled ideologies will become insufficient, necessitating the use of
    advanced artificial intelligence (AI)-based solutions (Song et al., 2022b) inside
    the RAN for achieving self-organizing, autonomous operation, and capital expenditure
    or operational expenditure savings (Giannopoulos et al., 2022; Li et al., 2023b;
    Arnaz et al., 2022). 7.8.2. Open-source MEC In this part, the work mentioned the
    use of decoupling and reconfiguring to incorporate the essential notions of open-source
    networks into MEC and presented the open-source MEC concept to improve the accessibility
    and flexibility of upcoming MEC systems (Haavisto et al., 2019). Specifically,
    open-source MEC splits the closely connected service functions into numerous separate
    NFs and supports them flexibly via a simpler service-based interface (SBI) by
    establishing a service-based MEC layer. After that, network functions’ MANO is
    designated for customizing MEC by presenting the handy template and instance idea,
    in which the open-source templates for MECs are specified and then instantiated
    as needed. Open-Source MEC Framework: The applications layer and MANO layer comprise
    the complete structure, in which the former (applications layer) is primarily
    accountable for data handling and delivery, involving the infrastructure tier,
    virtualization tier, service-based MEC tier, and applications or services tier
    from down to up. On the other hand, the latter (MANO layer) is made up of MANO
    and Virtualized Infrastructure Manager, and that accomplishes the open-source
    MEC platform''s coordinating and resource administration. The neighboring layers
    are connected via standard interfaces and collaborate to provide MEC services
    sought by users (Ullah et al., 2019), (Zhao et al., 2021b). Fig. 16 depicts the
    open-source MEC framework. Download : Download high-res image (816KB) Download
    : Download full-size image Fig. 16. Open-source MEC framework. The infrastructure
    tier is the lowest, encompassing the whole systems computational, caching, and
    connectivity facilities. The processing unit in particular provides high-performance
    computational capacity for such service-based MEC tier, which is located directly
    beneath the applications or services tier. The major elements of the caching services
    at the infrastructure tier are storage devices, i.e., the solid-state drive/hard
    drive. This layer''s connectivity resources comprise bandwidth along with access
    points and next-gen base stations (gNBs) (I et al., 2020). In the virtualization
    tier the underpinning three-dimensional facilities may be detached from the specialized
    hardware and aggregated into a pool of resources, relying on the NFV principle,
    and shared by many network functions at the service-oriented MEC tier. Next, as
    shown in the virtualization tier, open-source MEC builds many Virtual Machines
    or Dockers that function on the inherent resource pool and may concurrently support
    various network functions for offering customized services (Zhao et al., 2021b).
    The main aspect of open-source-MEC is the service-oriented MEC layer (Brik et
    al., 2020), which contains a uniform south-bound functionality interface and varied
    network functions, with the south-bound functionality interface connecting different
    network functions jointly based on the uniform stateless hypertext transmission
    protocol (HTTP) to ensure that they may interact directly with one another if
    needed. The work also presented the detached centralized service functionalities
    into distinct network functions. The user plane function (UPF) of the service-oriented
    MEC tier is specifically taken from the 5G core network for open-source MEC to
    offer open-source MEC-5G new radio (NR) features (Huang et al., 2017, 2022a; Lv
    et al., 2018). After that, to improve open-source MEC, the work grabbed numerous
    more network functions from the 5G core network control plane and constructed
    several new network functions. These network functions are accessible to one another
    and may be merged at any time, allowing all users to activate and deactivate them
    quickly in recognition of specific customized services. Indeed, the subscriber
    does not need to rely on open-source-specifics. Instead, customers will access
    customized services through numerous applications at the application tier. For
    the sake of convenience, only two applications are studied in this article, which
    are relative to the caching and computation queries, i.e., the application tier''s
    high throughput and intense computational situations. Furthermore, the MANO layer
    or tier is in charge of controlling these network functions alongside applications,
    as well as ordering the Virtualized Infrastructure Manager to assign resources.
    As previously stated, MANO is responsible for MEC reconfiguring, in which open-source
    MEC templates and implementations are suggested for executing flexible reconfiguration
    as needed. The MANO plane''s Virtualized Infrastructure Manager maintains virtualized
    resources following the directions of MANO and ensures that suitable computing,
    caching, and networking resources are available for the top levels. MEC Decoupling:
    To separate MEC functionalities, research works envisioned the open-source MEC
    platform''s service-based MEC tier, which developed from the classic MEC tier,
    which is mostly built of south-bound functionality interface as well as service-oriented
    network functions. Studies extracted and depicted a service-based MEC tier''s
    precise structure. The south-bound functionality interface links all of the NFs
    to facilitate interface design and deployment. The north-bound functionality interface
    is indeed an application programs interface (API) that allows the application
    tier to communicate well with the service-based MEC Tier to receive services.
    The east-bound functionality interface is an API that allows the network functions’
    MANO to communicate well with service-based MEC Tier, which is primarily used
    to control and organize network function resource management and life phase. The
    work (Alvizu et al., 2017) presented an integrated Representational State Transfer-ful
    (RESTful) API for such south-bound functionality interface, north-bound functionality
    interface, and east-bound functionality interface based on the identical HTTP,
    which itself is lightweight and readily read by both humans and computers. Hence,
    the service-based MEC tier enables direct connections between network functions,
    MANOs and network functions, and network functions and applications, minimizing
    interface complexity and protocol inconsistencies in the conventional MEC design
    (Krishna and Sharma, 2021). MEC Reconfiguration: While MEC administration is not
    directly tied to open-source MEC realignment, the ETSI-proposed combined monitoring
    of MEC and NFV may still be employed in open-source MEC; however, the idea of
    service-based MEC tiers is certain to offer issues to the orchestration approach.
    As a result, the study briefed open-source MEC templates (Zhao et al., 2021b),
    as well as the implementations of network functions, through Kubernetes, which
    enables the flexible ordering of network functions and any future MEC reconfiguration
    (Zhao et al., 2021b). Further, specifically, certain templates, such as a computation-heavy
    template, are pre-prescribed for related specific applications. Nevertheless,
    if no user requests such a specific service or application, the framework will
    not allocate the necessary resources for the template, therefore the template
    is void. If a subscriber requests (such a service or application), the framework
    will activate it by allocating computational, caching, and networking facilities
    as an instantiation (Pencheva and Atanasov, 2017). 7.9. Quantum computing-inspired
    MEC In contrast to traditional MEC employing classical information technology
    (Fan and Han, 2022), the intrinsic qubit noise in adaptable quantum computers
    will degrade qubit integrity as the number of qubits, gates, and tests rises (Bertels
    et al., 2020). Configurable quantum computers may run quantum processors with
    sophisticated cryogenic equipment and fault-tolerant techniques to enable fault-tolerant
    quantum computation (Sohn and Heo, 2018). Quantum computers operate at exceedingly
    low temperatures to reduce the entropy of quantum components. Quantum distortion,
    on the opposite side, is combated by the employment of fault-tolerant systems,
    such as those that utilize surface codes, concatenated codes, and bosonic qubits
    (De Micheli et al., 2022). In error codes, for example, information is dispersed
    among many physical qubits that comprise a logical qubit. Ultimately, fault-tolerant
    quantum computation at cryogenic temperatures may provide enormous increases in
    processing and energy efficiencies of configurable quantum computers. As scalable
    quantum computing achieves the needed capacity and quality, Multi-Access Edge-Quantum
    Computing (MEQC), in conjunction with the remote access provided by numerous edge
    servers, will strive to push the bounds of quantum advantage (Wang et al., 2021e).
    Users throughout the world may have access to and profit from cloud/edge quantum
    technology vendors, including Amazon Braket, IBM Quantum, and Azure Quantum by
    offloading computing activities to quantum systems in edge servers. This increases
    the possibility of identifying novel usage for quantum computation and addressing
    current challenges, hence accelerating the practical deployment of quantum processing.
    MEQC, by dramatically boosting the attraction of quantum computation to mobile
    consumers, can present users with a variety of potentially dangerous applications,
    including quantum ray tracing (Santos et al., 2022). MEQC differs significantly
    from standard edge cloud computation in terms of processing power and energy usage.
    To begin, quantum computing employs quanta accumulation, distortion, and entanglement
    to speed computational processes in novel ways. Secondly, quantum computers typically
    perform at extremely low temperatures. As contrasted with traditional computers,
    the majority of quantum computers'' power consumption is utilized to sustain the
    ultra-low temperature enclosure. Finally, to ensure the dependability of computing
    outcomes, quantum computers identify suitable error-correcting codes and the level
    of error correction convolution based on respective energy and latency restrictions.
    As a result, empowering devices (i.e., edge user devices) to effectively offload
    workloads to quantum edge computing nodes or servers remains a difficult topic.
    Xu et al. (2022) proposed a unique mobile edge quantum computation concept that
    extends quantum computing capabilities to mobile edge infrastructures that are
    closer to mobile consumers (i.e., edge devices). Passian et al. (2022) proposed
    a quantum-edge simulator in their research work to simulate sensing and quantum-edge
    computing. (Li et al., 2020) proposed a quantum ant colony-approach-based mobility-aware
    service deployment in the context of SDN-empowered MEC. (Masdari et al., 2020)
    proposed a quantum-based arithmetic optimizing algorithm for energy-efficient
    computation offloading in MEC. 7.10. Passive optical network (PON)-Assisted MEC
    Current MEC, 5G as well as beyond 5G research have emphasized the advantages of
    employing optical fiber networking to handle the growing number of connected devices
    and associated QoS-aware services. Optical fiber has a large capacity and minimal
    propagation latency. The cost-effective and extended-capacity passive optical
    network (PON) has emerged as a natural alternative for backhauling and fronthauling
    of 5G and beyond networking infrastructure, using the advantage of optical fiber
    which has already been installed in the majority of metropolitan or residential
    areas as well as commercial premises (Liu, 2022). The PON''s design, hardware,
    efficiency, capacity, and underlying standards and algorithms have all developed
    over time (Kantarci and Mouftah, 2012). These advancements, together with optical
    fiber implementations that have already reached residential and commercial premises,
    have facilitated the transmission of latency-sensitive as well as bandwidth-hungry
    applications to a significant number of consumers. Such services characterize
    5G and 6G features since they demand millisecond-level latencies, ultra-reliable
    networking capabilities, and a considerable quantity of computation. The fusion
    of MEC and PON systems is a promising option for delivering latency-sensitive
    but task-intensive services to meet such demanding QoS requirements (Dias et al.,
    2023). Wang et al. (2019b) examined the latency-aware network designing of a Wavelength
    Division Multiplexing (WDM)-PON-based MEC-enabled fiber optic wireless access
    network under architectural and management restrictions. Das et al. (Das and Ruffini,
    2021) proposed a hybrid analytical-iterative approach for estimating optimal virtual
    PON segment allocation in evolving MEC-based C-RAN, resulting in mesh access connectivity
    with ultra-low-level end-to-end delay. Wang et al. (2020c) proposed a joint optimization
    approach to minimize the deployment cost and latency for a Time Division Multiplexing
    (TDM)-PON network-based MEC-empowered C-RAN. Das et al. (2020) proposed a unique
    PON-based mobile fronthaul transport infrastructure focused on PON virtualization
    which permits EAST-WEST connectivity in addition to standard NORTH-SOUTH connectivity.
    Hu et al. (Hu et al.) proposed an application-aware MAC scheduling approach for
    MEC-enabled TDM-PON fronthaul. 8. MEC deployment cases The survey has categorized
    the deployment cases into two separate categories: (i) system-aware deployment
    cases and (ii) user-oriented deployment cases. System-aware deployment cases are
    indicating the system-aware/specific/sophisticated deployment and orchestration
    cases that are offered by the MEC service provider to a certain host or platform
    which provides specific service to the users and such services are video streaming,
    gaming services, edge intelligence-aware image/fingerprint processing for recognition,
    etc. In this circumstance, users will get a service through a platform or host
    and the platform or host will be the orchestrator of the MEC service or facilities.
    User or user devices here in this context are only service consumers. User-oriented
    deployment cases are indicating the end-user level services such as IoT services
    and other related ones. These types of MEC services will be readily available
    directly to the end-users. In this circumstance, users can demand the vendor or
    have some capabilities to orchestrate certain of the MEC facilities or services
    as per their needs or requirements, i.e., different IoT services have different
    requirements (enabled by the vendors under the supervision of regulatory entities
    if required). 8.1. System-aware deployment cases 8.1.1. Applications hosting An
    Application Service Provider (ASP) can deploy services at a MEC node to give a
    quick response. Such services may be useful in emergencies such as healthcare
    services, disaster tackling, and so forth. It can be beneficial for local services
    needed by the regional communities inside the MEC host''s region since it reduces
    network bandwidth and operating expenses. Yang et al. (2019c) presented an enhanced
    video streaming service hosted by a MEC server. The most popular programs on PCs,
    tablets, and cell phones are games. The devices on which they are featured are
    linked through LAN, WLAN, and/or cellular connectivity. Since games demand low
    latency, which typical cloud services cannot provide, they may be served on a
    MEC server near users (Singh et al., 2022b). While the game or program is running,
    the user may travel outside the base station''s coverage area, making it difficult
    to remain connected to the MEC server. 8.1.2. D2D communications MEC is required
    to minimize latency in real-time vehicle-to-vehicle (V2V) (Huang and Lai, 2020),
    machine-to-machine (M2M) (Li et al., 2019c), (Li et al., 2019d), or device-to-device
    (D2D) supervising. Due to significant latency, M2M, D2D, and V2V communications
    are not viable over the cloud. D2D transmissions have a wide range of applications,
    including media downloading, peer-to-peer (P2P) file transfer (Qin et al., 2020),
    online interactive gaming, streaming services, mobile social networking (MSN),
    and so on. A D2D link allows users in close proximity to interact with one another.
    Individuals who participate in MSNs share a shared interest. As opposed to the
    client-server model, they can get material from one another. Rivera et al. (2021)
    proposed a blockchain-enabled peer-to-peer secure task sharing through the assistance
    of a MEC server. MEC hosts can be useful in enabling real-time communication for
    automobiles via D2D communication, therefore avoiding the incidence of an undesirable
    road collision by the delivery of immediate warning messages (Ma and Sun, 2022).
    8.1.3. Edge intelligence Pervasive computing incorporates computation and networking
    capabilities into ordinary things, resulting in massive amounts of heterogeneous
    content. MEC server may do data analytics to limit the amount of data delivered
    to the cloud platform. A traffic tracking app, for example, only transmits the
    data of aggregate speed as opposed to the particular speed. A further example
    is a “kid missing crowd program,” in which only images with a kid in the frame
    are sent to the data center. This use case presents an application operating on
    a MEC server near the radio network which gets a huge volume of data from sensors
    and devices linked towards the MEC host. The program then evaluates the data and
    extracts vital information such as kid photos for the “kid missing crowd apps”
    (Wu et al., 2020) and aggregated speed for the “traffic monitoring platform,”
    (Anwar et al., 2022) which it then transmits to a centralized cloud service. Bellavista
    et al. (2019) presented a MEC framework for mobile crowd monitoring, with data
    analytics conducted on the MEC server. To preserve backhaul bandwidth, data undergoes
    filtering at the MEC server before being sent to the cloud server in this scenario.
    8.2. User-oriented deployment cases 8.2.1. Smart cities Smart cities are a complicated
    Internet of Things (IoT) paradigm that intends to manage public affairs by implementing
    Information and Communication Technology (ICT) viable alternatives. Smart cities
    may employ public resources more efficiently, which improves the degree of services
    offered to customers while lowering operating costs for public administration.
    For example, one realistic smart city project, Padova smart city, has been accomplished
    in the Italian city of Padova, which may pick open data with ICT technologies
    for public administration as soon as possible to make the optimal utilization
    of public resources (Cenedese et al., 2014). Neom smart city is another example
    of a smart city that is planned to be developed in Saudi Arabia (Alam et al. et
    al., 2020). As a sophisticated cyber-physical system (CPS) implementation, smart
    cities may include various sub-applications or features such as smart grid, smart
    traffic, smart buildings, waste treatment, environmental sensing, smart health,
    intelligent lighting, and so on. The aforementioned sub-applications or facilities
    should be endorsed by a unified connectivity/communications infrastructure or
    communications systems developed for such sub-applications or facilities should
    be integrated to form a large-scale interrelated network model for CPS application
    areas, to make the best utilization of public resources in city areas. The deployment
    of MEC will improve the utility or features of a smart city to make it more suitable
    for living (Zhao et al., 2021c; Wang et al., 2023; Peng et al., 2022). 8.2.2.
    Smart agriculture Smart agriculture or precision farming utilizes the most modern
    and advanced ICT technology to increase agricultural profitability and environmental
    sustainability. Smart farming focuses on operating actuators (motors, pumping,
    illumination regulators, and so forth) according to sensor data (humidity, temperature,
    brightness, etc.). Furthermore, UAV-aided computation is a sort of application-oriented
    edge computing that is now popular in agricultural advancement (Reddy Maddikunta
    et al., 2021). The UAVs, which are equipped with cameras and computation capacity,
    are dispersed in the formation of a swarm and hover over enormous agri-fields
    to monitor grain or drop conditions. Besides crop yield, MEC may be used to study
    farm animal behavioral patterns, which is important for animal health and well-being.
    Nevertheless, one of the issues that agricultural lands experience is geographic
    isolation as well as a lack of robust and consistent data network connectivity.
    To resolve this problem, private edge computing as well as communication architecture,
    provides a significant alternative for enriching agricultural regions. Another
    reason to use edge computation (MEC) is to preserve the main network from overload
    as sending all video recordings and sensor data from several farm fields to the
    centralized server will create significant latency in the network (Chen and Yang,
    2019), (Qu et al., 2021a). 8.2.3. Smart vehicular network or transportation The
    deployment of MEC improves the safety and intelligence of modern smart city roadways
    and transit systems. Smart roads help to disseminate awareness amongst road elements
    by utilizing an advanced traffic control system, in which cars assisted by the
    MEC services may interact with each other to preserve road traffic safeness and
    balance. It is also feasible to address particular road scenarios more effectively
    by using vehicle-to-thing interaction, such as in the event of a collision or
    the case of the movement of emergency automobiles (police car, ambulance), when
    cars are directed proactively to clear up specific road lines (Nallaperuma et
    al., 2019). Traffic collision detection (Lin et al., 2022a) constitutes one of
    the services that will be housed in the MEC server dependent on data acquired
    from the driving environment; underneath MEC server may instruct and adjust real-time
    vehicular speed and trajectory, in addition to lighting controls to minimize collision
    incidents. MEC is also used to augment road cameras with features, including vehicle
    tracking and recognition (Sun et al., 2018). The road surface condition seems
    to be another concern in road safety; the project (Liu et al., 2022f) envisioned
    deploying a congested surface sensor module having the capability of vehicle supervision,
    wherein data is gathered and processed utilizing edge computing nodes. 8.2.4.
    Smart grid A smart grid framework is an electrical system that includes energy
    efficiency-assuring resources, renewable energy resources, and smart appliances.
    Intelligent meters located across the network are employed to receive and send
    energy usage estimation. The intelligent meter''s data is overseen through supervisory
    control and data acquisition (SCADA) technologies that regulate and stabilize
    the electricity grid. Furthermore, SCADA systems may be supported by dispersed
    intelligent meters and miniature grids that are interconnected with MEC. For example,
    MEC can balance as well as scale, the load based on information supplied by certain
    microgrids and intelligent meters (Zhang et al., 2020a; Ma et al., 2022; Jiang
    et al., 2022a). 8.2.5. Smart industries MEC has been completely linked with the
    most prevalent industry 4.0 standards; this integration is often referred to as
    industrial edge computation. Proactive maintenance is a strategy used by emerging
    industries to decrease capital expenditure and operational expenditure. The machine
    is fitted with numerous IIoT (industrial IoT) sensors, such as heating, vibrations,
    and pressure sensors that collect data and send it to edge computing nodes and
    the data will be analyzed for forecasting machine faults and mistakes (Bebortta
    et al., 2022). Furthermore, the fourth industrial revolution wants to include
    AI in its production processes. Since industrial enterprises cannot send private
    information to a public cloud (for example, recordings from production areas),
    companies have to depend on edge computing (Deng et al., 2022a). Several instances
    are available at present stating how industrial revolution 4.0 is employing edge
    intelligence for object identification with machines, autonomously guided vehicles
    (AGV), and human position estimation. Apart from the aforementioned sectors, E-commerce
    businesses are in serious need of real-time connection with their consumers, as
    delivering promptly with MEC represents one of the finest browsing perceptions
    they can give to their clients. Similarly, by improving their recording devices
    with computer vision features, MEC may assist protect in-store payment terminals
    (Qu et al., 2021b). 8.2.6. E-healthcare Several scholars are interested in technological
    advancements in the health sector. Healthcare, like other sectors, can benefit
    from MEC (Bishoyi and Misra, 2022); for example, patients afflicted with strokes
    may fall. As per stroke stats, someone in the United States gets a stroke every
    40 s (Preventing, 2023). Falls are prevalent in stroke victims, who typically
    have hypoglycemia, hypertension, muscular weakness, and other symptoms. According
    to current studies, one-third of strokes might be avoided by preventing falls
    as soon as possible (Jain and Semwal, 2022). A substantial amount of study has
    been conducted in an attempt to detect and avoid falls, for instance, by integrating
    human-computer interface devices like smartphones, smartwatches, and Google Glass,
    however some limits persist. Recently, researchers introduced a DL-based fall
    detection mechanism utilizing MEC technologies (Fakhrulddin et al., 2020), (Al-Rakham
    et al., 2021). U-fall is built on a fall detection system that employs acceleration
    magnitude measurements and non-linear time-series analysis. U-fall detects motion
    using smart device sensors including gyroscopes and motion sensors. To provide
    real-time identification, U-fall smartly maintains the integrity between both
    the smartphone as well as the MEC server. Furthermore, the suggested infrastructure
    is capable of producing correct findings, making it more trustworthy and dependable
    (Liu et al., 2020b; Sharma and Diwakar, 2021; Mahmud et al., 2017). 8.2.7. Smart
    banking Blockchain, invented in 2008 is an electronic trading system that is independent
    of any third-party operator (banks, government, etc.). Transaction verification
    in Blockchain is performed by miners, who focus on solving a theoretically and
    computationally difficult challenge known as the verification of work. One of
    the main role players behind intelligent banking is Blockchain technology. In
    IoT devices, Blockchain typically cannot be directly implemented due to the computation-intensive
    nature of mining tasks. As a result, outsourcing mining tasks to the MEC server
    is a viable option. Furthermore, a notable Blockchain issue is pricing cooperative
    miners, since there is a requirement to boost MEC service providers’ revenue while
    simultaneously safeguarding miners'' investment benefits from transferring to
    the MEC server (Ugwuanyi et al., 2018), (Al-lawati and Al-Badi, 2016). 8.2.8.
    Infotainment Internet Protocol Television (IPTV) (Chakareski, 2015), (Gao et al.,
    2018) over Wireless to the Everything/x (WTTx) delivers wireless broadband services
    via wireless broadband communication systems (e.g., cellular networks) (Guo et
    al., 2011), (Li, 2017). At this moment, network connectivity and television (TV)
    infrastructure are distinct. IPTVoWTTx enables operators and over-the-top (OTT)
    broadcasters to replace TV infrastructure while saving money on infrastructure
    expenditure. IPTV through WTTx will enable OTT providers to deliver content swiftly
    by utilizing the existing mobile networks. IPTV has a three-tier-based design.
    The IPTV''s centralized cloud server or data center acts as a repository for Video
    on Demand material, which is the first tier. The second tier is just the IPTV
    edge server, which offers users with Electronic Program Schedule as well as Video
    on Demand features. Consumers are represented by the third layer. The IPTV central
    hub is often implemented on a regional basis, such as in a big city or significant
    metropolitan area. To deliver the finest user experience, IPTV edge servers are
    installed closer to users (Barry et al., 2020). However, high mobility creates
    issues in dealing with the frequent switchover of operations and services to offer
    end customers uninterrupted service. 8.2.9. Video analytics In the past, security
    cameras are employed to send data to a centralized server. Because of the increasing
    prevalence of security cameras, the conventional client-server design may be unable
    to transmit footage from millions of devices, putting a strain on the network.
    In this instance, MEC will be advantageous by including cognition at the device
    directly, which is configured to transfer information through the network whenever
    motion is detected. Moreover, MEC-enabled video surveillance might be useful for
    a variety of systems, such as traffic control applications that can identify road
    congestion or an accident based on traffic patterns. The program can also aid
    facial recognition; for instance, if an individual commits an offense, his image
    (captured by the sophisticated cameras) may be sent to the edge computing server
    to help track down the perpetrator (Yang et al., 2019c), (Chao et al., 2021; Jiang
    et al., 2022b; Ma and Mashayekhy, 2021; Viola et al., 2018; Zhang et al., 2020b;
    Wang et al.; Wu et al., 2022c). 8.2.10. Immersive audiovisual streaming (AR/VR)
    A live either indirect or direct representation of a tangible, real-world situation
    whose attributes are augmented (or replaced) by computer-generated sensory stimuli
    such as music, video, animations, or Global Positioning System (GPS) data is referred
    to as augmented reality (AR). AR applications can deliver extra information in
    real-time after processing such data. AR applications are extremely localized
    and need both low latency and extensive data computation. The exhibition video
    guide is a portable electronic device that gives thorough information on certain
    artifacts that cannot be readily exhibited. AR plays a crucial role in online
    gaming, like Pokémon Go, Ingress, Minecraft Earth, Jurassic World Alive, Angry
    Birds AR: Isle of Pigs, The Walking Dead: Our World, etc. By precisely evaluating
    the raw data, MEC enables AR service ought to have the capacity to differentiate
    the desired contents and subsequently transfer AR data to its intended subscriber.
    MEC-enabled AR devices have recently received a lot of interest (Elbamby et al.,
    2018). AR platforms such as Junaio, Layar, Wikitude, and Google Goggles have recently
    incorporated mobile technology (Goh et al., 2019). AR provides a real-world user
    experience by merging actual and virtual items that exist concurrently. Modern
    AR services, such as news, TV shows, sports, object detection, games, and so forth,
    have become adaptable in their audio as well as visual components. Metaverse is
    a prominent instance of such advancement of AR (Dwivedi et al., 2022). Yet, AR
    systems typically require high computational power for task or workload offloading,
    low latency for improved QoE, and high throughput to support indefinite IT services.
    MEC platforms have been identified as a potential for latency-sensitive AR technologies
    (Li et al., 2022c). They enhance AR systems, for instance, by increasing throughput
    by moving cognition to the network''s edge rather than depending on the main network.
    Transferring computation-intensive activities to the closest cloudlet is thus
    more optimal and efficient, improving user experience. Brain-computer interface
    is one instance of AR functionality that works by recognizing human brain signals
    (Santos et al., 2014; Qiu et al., 2017; Qiao et al., 2020). The work (Qiao et
    al., 2019) reviewed the escalation of mobile augmented reality (MAR), i.e., Web
    AR with the advancement of networking and computing technologies such as 5G and
    beyond networks and MEC. 9. Security aspects of MEC 9.1. Edge networking-level
    threats The edge network provides computing power, data storage, and administration
    services (Shi et al., 2016). It facilitates application and service delivery by
    sharing infrastructures, platforms, and supervisory planes (Bolettieri et al.,
    2022). Nevertheless, some components, such as isolation (Ranaweera et al., 2021b),
    (Nowak et al., 2021) may not provide an equivalent to cloud security needs. In
    a MEC integrated edge network context, applications or functions may not be created
    utilizing trusted computing techniques, which raises risk (Ma et al., 2020). Privacy
    Leak: Unapproved accessibility to MEC nodes may jeopardize data confidentiality
    (Hou et al., 2020). The MEC concept restricts the breadth of privacy breaches
    by splitting data and limiting access (Kim et al., 2020). Edge network infrastructures,
    on the other hand, may exfiltrate confidential material and rich network contextual
    information, including client status details, traffic data, and local network
    circumstances, which are used by different services to provide context-aware management
    (Abdulqadder and Zhou, 2022). The work (Liu et al., 2022g) presented a deep deterministic
    privacy-preserving mechanism. Privilege Escalation: When a malicious party takes
    advantage of a design flaw, bug, or configuration issue in an application or operating
    system, he or she gains elevated privileges on restricted elements that would
    normally be limited to that individual (Maniatakos, 2013). The malicious attacker
    can then utilize the newly obtained unauthorized rights to steal sensitive data,
    conduct administrative operations, or distribute malware, thereby causing catastrophic
    damage to server operations (Tsoutsos and Maniatakos, 2014). The works (Jaafar
    et al., 2016; Qiang et al., 2018; Zhang et al., 2019) proposed prevention mechanisms
    against this type of attack. Service Manipulation: Service manipulation assaults,
    unlike cyber-criminals who aim to steal information or grab it as hostage through
    ransomware, can be difficult to detect. Hackers can make erroneous alterations
    to data, which might have disastrous consequences (Zhang et al., 2020c). A device
    in a cluster placed around an edge network that participates in service delivery
    can operate as a decentralized computing system, and if the device is hacked,
    the entire group can be affected (Bang et al., 2023). An internal aggressor with
    proper rights can not only alter information exchange, but also launch rogue services
    that really can send misleading management information including historical data
    to third parties (Ye et al., 2020), (Yang et al., 2022c). The references (Sivanathan
    et al., 2020) and (M. V. K et al., 2022) presented prevention mechanisms against
    this kind of attack. Rogue Data Center: The edge network seems to be more difficult
    to maintain and protect than traditional cloud computing settings (Verma and Verma,
    2021). In this threat scenario, an attacker might take control of a whole edge
    network by installing malicious equipment pretending as an edge networking unit
    (i.e., access point, routers, switches, network cards) that lies between a cloud
    server situated in the core infrastructure and user devices to affect connections
    with external devices (Dulik, 2021). Several works have proposed multiple prevention
    mechanisms against this sort of attack (Lin et al., 2022b; Varshney and Sagar,
    2018; Varma and Narayanan, 2016). Physical Damage: Cyber-physical security combines
    numerous technological disciplines, such as physical, computer, and connectivity
    resources on various spatial scales, all of which are regulated by computational
    models (Liu et al., 2017). Systems are frequently linked to an IP network. Throughout
    the event of an invasion, the entire ecology might be disturbed or brought to
    a standstill by physical disruption (Humayed et al., 2017). Monitoring the computational
    capabilities for unexpected usage or load (Zhang and Li, 2019) becomes a possible
    security check to protect the internal structures of the MEC servers against this
    attack. Several prevention mechanisms are presented in these works (Yang et al.,
    2020b; Barrère et al., 2018; Keerthi et al., 2017). Resource Misuse: Malicious
    actors may attack consumers, companies, or other service vendors using MEC resources
    (Tang et al., 2021b). For instance, the malicious attackers can be massive-scale
    mechanized click fraud, for “mining” electronic currencies, or brute-force computer
    threats on credential databases (Shende et al., 2017). A third type of attack
    is resource usage, for example, a rogue VM might scan the regional network for
    susceptible IoT devices as well as host botnet nodes (Lu et al., 2016). The works
    (Idhom et al., 2020; Carpenter et al., 2021; Oba et al., 2022) proposed several
    prevention schemes against this cyber attack. VM Manipulation: The host layer
    or tier or level is a significant functional aspect in MEC. It contains the MEC
    Platform Manager, Virtualized Infrastructure Manager, and MEC hosts that deploy
    resources and deliver functions to MEC users utilizing virtualization approaches
    including VNF and VM. Nevertheless, when deployed in MEC infrastructure, virtualization
    methods create various security concerns such as VM tampering, VM escaping, Domain
    Name System (DNS) escalation, VNF placement shift, security-log inspection, and
    surveillance (Lal et al., 2017), (Rehman et al., 2013). The attack vectors influence
    the host tier activities of orchestration elements (Ranaweera et al., 2019). Trusted
    Platform Manager (TPM) and Virtual Machine Introspection (VMI) are two approaches
    recommended for dealing with virtualization-related security issues (Jin et al.,
    2019b). Injection Attacks: Injection attacks remain one of the most common and
    dangerous web application threats (Sharma and Jain, 2014), (KhariSonam and Kumar,
    2016). They can cause data loss, data breaches, data integrity failure, denial
    of service (DoS), and the compromise of a system or device. Injection attacks
    are a broad class of threat vectors that allow adversaries to inject malicious
    code into a system, which is then processed by a mediator as an element of a request
    or instructions, causing the affected program''s execution pattern to be altered
    (Chapple et al., 2021; Barabosch and Gerhards-Padilla, 2014; J. N. O.S and Bhanu,
    2018). Prevention mechanisms against this attack are mentioned in the reference
    works (Al-Shareeda et al., 2022; Chowdhury et al., 2021; Gogoi et al., 2021).
    9.2. Access network-level security threats Access network safeguarding is essential
    for MEC platforms’ efficiency which provides a framework for safe connectivity
    with user devices and cloud infrastructure (Wu et al., 2016), (Vidhani and Vidhate,
    2022). Failing to deploy adequate access network security rules exposes hostile
    parties to exploit important vectors, resulting in significant network risks (Wang
    et al., 2019c). Infrastructure for access networks includes network components
    and systems that support information flows across devices connected towards the
    access network infrastructure, MEC host, as well as core networks. An attacker
    may attempt to compromise the access network infrastructures, associated devices,
    or communication links (Hachimi et al., 2020). Vulnerabilities to MEC services
    and applications include applications or services misuse (Akman et al., 2021)
    and the distributed denial of service (DDoS) attack attempts (Deng et al., 2022b).
    Denial of Service: Access networks are subject to DoS assaults, which might take
    the form of DDoS attacks (Tan et al., 2019) or radio jamming (Khadr et al., 2022).
    The persistence of Virtual Machines distributed across numerous MEC hosts raises
    the possibility of exploited Virtual Machines coordination in a substantial assault,
    such as DDoS (Galloway et al., 2019). Whenever a service or application is hacked,
    the infected service or application uses MEC resources, i.e., network bandwidth,
    processing power, or memory. The attack causes a delay in an applications'' or
    services'' response or destroys the MEC node''s operation, resulting in service
    interruption. Security orchestrating, automating, and response (SOAR) mechanisms
    are available in the security sector to give an autonomous and preemptive security
    strategy to this sort of serious threat (Bartwal et al., 2022). Sophisticated
    DoS/DDoS prevention architectonics are described in the reference works (Li et
    al., 2018c; Gyamfi and Jurcut, 2022; Chang et al., 2021; Ali et al., 2020; Li
    and Wang, 2018; Dobrin and Dimiter, 2021). Man-in-the-Middle: The MitM attack
    is typically distinguished by the existence of a malevolent third party across
    two or more interacting parties, surreptitiously relaying or intercepting their
    conversation (Conti et al., 2016). A MitM attack is classified as an infrastructure
    assault in the MEC scenario, in which the malicious attacker attempts to seize
    a specific network section and proceeds to perform attacks, including phishing
    and eavesdropping, on linked devices (Singh et al., 2021). Since MEC services
    and applications rely heavily on virtualization, conducting a MitM attack attempt
    on many Virtual Machines may have an impact on both parties or sides or victims
    of the assault (Du et al., 2020b). Reference (Khader and Lai, 2015; Amin and Mahamud,
    2019; Chakaravarthi et al., 2017) works proposed MitM prevention mechanisms. Rogue
    Gateway: The decentralized MEC architecture enables an environment in which hostile
    attackers might design and install illegal gateways to undertake unauthorized
    actions. Unauthorized gateways can constitute a substantial hazard by enabling
    backdoor access to critical resources if they have access to networking equipment,
    programs, and edge services. Solution mechanisms against this issue are described
    in these works (Zhang et al., 2020d), (Igarashi et al., 2021). Inconsistent Execution
    of Security Policies: The synchronization and uniform compliance of security functionalities
    during the time of mobile devices'' switch from one operator to another poses
    a difficulty for the providers of mobile networks. This activity demonstrates
    the necessity for telecom operators to share security policies on an adaptive
    scale to ensure that subscriber traffic across networking devices is safely managed,
    especially when devices’ connection shifts from the MEC of one operator to the
    MEC of another operator (Zhao et al., 2021d). Communication Channels: The radio
    channels of a wireless communication network are formed through a wireless transmission
    medium, which constitutes the most vulnerable connection in a communication network.
    Man-in-the-middle attack, eavesdropping, Sybil, replay attack, smurfing attack,
    spoofing, and DoS are all mobile telecommunications threat vectors (Abbasi et
    al., 2022). There is a danger of unauthorized intrusion to offloaded information
    during the offloading procedure (Samy et al., 2022), (Apostolopoulos et al., 2020).
    Risk considerations emphasize interoperability and compatibility issues with the
    user devices’ connectivity to the base station. Advanced researches are ongoing
    to offer a secure communication channel for MEC services (Lu et al., 2022b; Xu
    and Ren, 2020; Wang et al., 2022e; Han et al., 2019; Liu et al., 2020c, 2021;
    Zhang et al., 2021c). As a threat prevention or security assuring mechanism for
    access network safeguarding and establishing secure access from anywhere Secure
    Access Service Edge (SASE) emerged as a notable security solution. SASE represents
    a cloud architectural concept that integrates network with security as a service
    capability into a single cloud-based service (Chen et al., 2023d). SASE, in theory,
    expands safety and networking features beyond what is generally provided. This
    enables individuals to make use of secure web gateway, zero-trust network access,
    firewall as a service, and a slew of threat detection services. SASE is made up
    of two components: Security Service on the Edge as well as SD-WAN. When correctly
    deployed, a SASE strategy enables enterprises to ask for secure access regardless
    of the location of their users, tasks, devices, or applications. This constitutes
    a major benefit in ensuring the security of distant workers. SaaS applications
    are fast gaining traction, and data is rapidly moving between data centers, regional
    offices, multi- and hybrid-cloud, and edge-cloud circumstances. SASE offers secure
    surfing, business application accessibility, and SaaS application accessibility
    from anywhere. 9.3. Core infrastructure-level security threats Access and edge
    network activities for MEC are supported and managed by core infrastructure (Sun
    et al.). The trustworthiness of the core architecture can have a knock-on effect
    on other systems, such as the cloud. Under this context, it is vital to examine
    the credible threats against the fundamental or core infrastructure (Suomalainen
    et al., 2021), (Lopez et al., 2022). Privacy Leak: Accessibility to core infrastructure
    raises the chance of attackers obtaining information held on edge infrastructure,
    raising worries about privacy leaks. In the case of edge network breaches the
    potential impact of a privacy violation is confined to the type of content the
    adversary has achieved access to (Go et al., 2019). “Privacy by design” represents
    a unique MEC security technique. The principles comprise privacy functionality
    protection built into the design, preemptive action instead of a reactive response
    to privacy breaches, and data confidentiality throughout the lifespan (Deng et
    al., 2022c). References (Noh et al., 2023), (Qiu and Ma, 2018) mentioned several
    prevention mechanisms against this sort of attack on core infrastructure. ICT
    Intrusions: An ICT intrusion is defined by the attacker intending to breach into
    a system or device to purposefully modify data and manage the hardware it relies
    on (Jiang et al., 2014b), (Zhang et al., 2018). Data manipulation (Singh and Borisagar,
    2022), background information (Wang, 2021), collusion (Khan et al., 2021), outside
    forging (Anda et al., 2022), eavesdropping (Li et al., 2015), likability (Porambage
    et al., 2021), Sybil (Mulla and Sambare, 2015), and identity attacks (Garzon et
    al., 2022) are examples of such attacks. Security assurance mechanisms against
    these attacks are proposed in these works (Xie et al., 2023; Deng et al., 2020;
    Kim et al., 2022; Wang et al., 2020d; Liao et al., 2018; D. N and S. J and S.
    P, 2022). Software-Based Attacks (on Virtual Infrastructure): NFV security raises
    serious issues regarding its flexibility and the protection of the underlying
    telecommunications infrastructure (Luo et al., 2020; Farris et al., 2019; Jiang
    et al., 2021c). It has the greatest influence on system resilience in addition
    to the entire quality of available services. The bulk of security risks target
    NFV infrastructure''s fundamental architectural features, such as VNF modification,
    VNF location change, and information exfiltration as well as damage (Thiruvasagam
    et al., 2021). Fawcett et al. (2018) offered an SDN-based conceptually centralized
    control solution that allows dynamism in networking security systems by collecting
    intelligence from networking devices via configurable APIs and using virtualization.
    In the network periphery, virtual security mechanisms can be used to detect possible
    threats, isolate vulnerable network devices, and prevent them from jeopardizing
    system security (Cox et al., 2017). Chawla et al., 2021 proposed an advanced prevention
    mechanism against the attack on virtual infrastructure. Rogue Infrastructure:
    This threat implies that attackers target specific components of the core infrastructures,
    and a successful assault might enable control of services and applications found
    in MEC servers. Even though the possibility of an adversary effectively launching
    this attempt is extremely low, effective security controls and procedures for
    critical MEC systems are still required. Security and privacy assurance mechanisms
    against this issue are proposed in these works (Lu et al., 2018), (Shamseddine
    et al., 2019). 9.4. Edge device-level security threats The sensitivity of user-controlled
    device material is considered while defining security and privacy needs (Zhou
    et al., 2019b). Users sometimes become active contributors who develop data and
    engage in the sharing of information in addition to obtaining services. Unfortunately,
    there will be fraudulent users who will seek to disrupt functions and negatively
    impact the performance of edge devices (Pourhabibi et al., 2020). Information
    Injection: An attacker can insert malicious data onto any hacked device to spread
    misleading information (R. V et al., 2022). Poisoning (Zhang et al., 2021d) is
    a hostile operation in which attackers insert bogus information into an electronic
    system. Outside forging (Gopinath and Latha, 2021) happens when misleading messages
    containing fabricated information are created to jeopardize the confidentiality
    of victim modules. In the intelligent manufacturing arena, for example, an attacker
    injects erroneous pressure readings to delay valve activation to cause malfunctions
    (Tsou et al., 2023). Prevention mechanisms against these sorts of attacks are
    prescribed in these works (Kaur et al., 2018; Zhang and Li, 2022; Xu, 2022). Eavesdropping:
    Adversaries intercept data transmissions across communication channels (through
    which the user devices are communicating), obtaining access to sensitive information
    (Zou and Zhu, 2016). Side-Channel Attacks: The goal is to obtain sensitive private
    information by gaining unauthorized access to user devices. Passcodes, login information,
    email, and geo-location information are the primary targets of this sort of cyber-attack
    (Kim and Hur, 2022). An efficient intrusion detection system or intrusion prevention
    system with ML mechanisms could be a viable solution for detecting malware in
    a user device (Gupta et al., 2019; Albalawi et al., 2022; Wang et al., 2020e,
    2021f; Bhasin et al., 2022). 9.5. MEC level security challenges 9.5.1. MEC system-level
    Global Defenses: The administration and orchestration of many security features
    is a difficult topic, and activating security methods separately on several entities
    does not always imply that the entire system is safe. It is necessary to strike
    a balance across local (decentralized) as well as global (centralized) defensive
    systems, as well as assuring accountability and flexibility. A central surveillance
    system should be put in place to provide insight into the MEC infrastructure,
    and all aspects ought to be verifiable. The end-to-end protection mechanism is
    required whenever possible to ensure privacy (Nguyen et al., 2021; Zhang et al.,
    2021e; Sun et al., 2020b). MEO Security: Virtualization attacks are possible against
    MEO (Mishra et al., 2020). A compromised MEC orchestrator (MEO) might have a significant
    influence on the whole MEC system''s operation. Examples include the suspension
    of MEC critical programs, the enrollment of malicious application bundles, and
    uneven resource utilization of the MEC hosts. Self-analysis methods for hypervisors
    must be used; for Linux-based systems, Security Enhanced/Extended Linux (SELinux)
    may be useful (Amith Raj et al., 2016), (Gul et al., 2019). Nevertheless, given
    the quickness with which technology and assaults develop nowadays, it is deemed
    more appropriate to approach through software-programmable solutions rather than
    inflexible hardware (to allow for updates, changing targets, reactive attacks,
    and so forth). The issues of privacy and confidentiality in softwarization as
    well as virtualization do not only pertain to MEC, since adaptable solutions are
    necessary to safeguard 5G and beyond networks in general. Within the MEC-in-NFV
    infrastructure, solutions to virtualization challenges and security frameworks
    have been examined (Schiller et al., 2018), (Fu et al., 2020). They comprise Trusted
    Platform Manager (TPM)-based attestation and validation of MEC applications and
    VNFs, in addition to the Customer Facing Service (CFS) portal queries. Auto-configurable
    security features, as well as approaches to safeguard VNF in NFV settings, are
    suggested (Huang et al., 2020), (Zhang et al., 2021f). Ideally, a security facilitator
    relying on softwarization (VNF/SDN) might eliminate the requirement for manual
    configuration, which is no longer practicable under present conditions. Yet, it
    is unclear how such a cybersecurity orchestrator should be designed and incorporated
    into the MEC architecture. Interconnection Security: OSS is subject to attacks
    outside the MEC infrastructure at the MEC system level because of its interaction
    with the CFS interface and user applications through the LCM proxy. It creates
    security vulnerabilities; for instance, the CFS interface is vulnerable to DDoS
    assaults (Eliyan and Pietro, 2021). OSS can be used to disguise adversaries that
    claim to have valid access. In the absence of suitable security controls, a large
    volume of queries from the OSS towards the MEC orchestrator may harm the performance
    of the MEC orchestrator (Vilalta et al., 2021). 9.5.2. MEC host-level Physical
    Security: MEC hosts are placed near the network''s edge, near user devices, and
    in open surroundings. As a result, the physical placement of the MEC hosts gets
    insecure with host-level devices being even more susceptible to physical intrusions
    than system-level equipment, which is often located in a more physically safe
    region. Moreover, the desire to deploy a large number of MEC hosts to encompass
    a whole region creates concerns about maintaining a high degree of physical security
    (Szefer et al., 2012). This raises the danger of unwanted physical accessibility
    and, as a result, physical degradation or manipulation of devices, with direct
    repercussions for accessibility (e.g., DoS attack attempts) and secrecy (e.g.,
    data leakage via both active as well as passive assaults) (Tiwary et al., 2018).
    The equipment may lack the hardware security of generic servers (Koteshwara, 2021),
    however, the MEC equipment should include anti-theft as well as anti-damage procedures
    as a kind of protection. Tamper protection is unquestionably a strong security
    method for preventing the reading of secret data (e.g., cryptography keys) and
    ought to be implemented in the context of MEC hardware as well. The well-known
    notion of the weakest point applies in this situation: the protection of the total
    system is guaranteed by the safety of the weakest point (the attackers usually
    target weak points.). MEC hosts with inadequate security can quickly become targets
    (Angin et al., 2016; Chae et al., 2018; Shirazi et al., 2017; Artych et al., 2022).
    Privacy of User Location: Location tracking facilitated by MEC might be viewed
    as both a benefit and a concern. Unauthorized exposure to the Location Application
    Program Interface (API) can pass sensitive data concerning user navigation and
    tracking over time (Riaz et al., 2018), (AlShalaan et al., 2022), comparable to
    unauthorized entry to radio network data in mobile communications (i.e., access
    to user recognition mechanisms, which may jeopardize the users’ confidentiality)
    (Zhang et al., 2017b), (Yu, 2022a). MEC hosts (and hence users) are thus directly
    vulnerable to location privacy problems (Wang et al., 2020f). To mitigate such
    dangers, API privacy and highly synthesized location records or processed/encrypted
    data play a significant role. Nevertheless, when GPS services are unavailable
    or in the case of emergencies, the MEC geo-location service can be useful. Reference
    literature performed research on location privacy (Guo et al., 2021a; Cui et al.,
    2023; He et al., 2017; Yu, 2022b). Local Defenses: Because of their local nature,
    host-level assaults have a geographically limited impact on the end users. This
    enables MEC to impose security procedures and minimize assaults in the local networking
    segment (Cheng et al., 2022a). MEC is appropriate for deploying a defense perimeter,
    such as against DDoS assaults where the attacker only targets a tinier traffic
    stream, and the edge may inform the core network well about the source of risk,
    resulting in overall improved reliability. MEC''s remote or localized aspect can
    further improve confidentiality by preventing data from approaching centralized
    servers and therefore eliminating a threat to centralized infrastructure. For
    example, the processing of photographs of vehicle registration plates at the edge
    server and recognition of the registration number to just pass the number to the
    central server (this prevents the possibility of location leaks) (Butt et al.,
    2019). Conversely, at the same moment, it is considered that local information
    transfer (as opposed to, say, transferring information via the internet) decreases
    data exposure (Costantino et al., 2023), but can raise security threats when the
    volume of data traversing nodes is large, due to heavy traffic and location near
    the network''s edge (Shu et al., 2015). The works (Hauer, 2015), (He et al., 2019c)
    presented several security enhancement techniques for MEC-assisted IoT context.
    Virtualization Security: Malicious Virtual Machines might try to take advantage
    of their hosting (Sato et al., 2022). Malicious insiders having adequate permission
    to access as well as harm a Virtual Machine or even a suspicious Virtual Machine
    with advanced privileges (Rocha et al., 2013), (Chaturvedi et al., 2020) might
    be used in Virtual Machine manipulation attempts. If a Virtual Machine is operating
    on many servers, a typical DoS attack or assault may cause harm to all servers
    at the same time (Sheinidashtegol and Galloway, 2017). As a defense against DoS
    assaults, Virtual Machines'' resource usage should be controlled, and resource
    usage should be balanced between servers (Aishwarya and Malliga, 2014), (Singh
    et al., 2020). In terms of data privacy, subscriber information is preserved at
    the MEC server level, which means it might be exposed. Moreover, the possibility
    of data alteration necessitates proper backup and recovery capabilities, which
    are strongly linked to dependability prevention. Virtualization attacks may disrupt
    orchestrating on the host side, and an exploited Virtualized Infrastructure Manager
    can cause MEC services to fail (Aljuhani and Alharbi, 2017). Another kind of contamination
    is service manipulation, which can have serious effects including DoS or data
    leaking attacks (Baek et al., 2014). If a system got corrupted (not only through
    virtualization assaults, but in a broad sense), the adversary may intervene at
    multiple levels (e.g., applications, services, resource usage) and launch a variety
    of attacks. Constrained Resources: The use of computationally complex security
    techniques, such as heavy encryption, can be an issue. For example, edge devices
    may have restricted connection and resources, limiting the security standards
    that may be implemented and facilitating the vulnerability of attacks. This might
    lead to limitations in the adoption of high-security systems, such as authentication.
    The employment of public-key cryptography, particularly, public-key infrastructure
    (PKI) may be problematic due to high computational expense and maintenance (Kim
    and An, 2014). The adoption of lightweight encryption can be considered in this
    sense (Zhang et al., 2021g), (Kaur et al., 2019). Data deduplication technologies
    at the edge (detecting and deleting duplicate data or even preventing recomputations)
    would improve the performance of resource or capacity-constrained devices. Nevertheless,
    doing this while retaining security is generally attainable with Fully Homomorphic
    Cryptography, which involves extremely high computation overhead (Alabdulatif
    et al., 2020). The European Authority for Cybersecurity (ENISA) recognized the
    complexity of implementing security approaches in 5G and beyond networks (because
    of the combination of technologies like cloud computing, fog computing, and edge
    computing), and the necessity of efficient cryptographic algorithms (due to resource
    constraints on nodes) as major aspects in 5G and beyond security studies and developments.
    10. Lessons learned, challenges, and future directions This section of the paper
    includes lessons learned through the survey. Moreover, this section described
    the open issues and future research scope relative to MEC technology. 10.1. Lessons
    learned The lessons learned through this survey work are enlisted below. • 5G
    Service Based/Oriented Architecture (SBA/SOA) suggested by 3GPP can enable a higher
    degree of service access efficiency and flexibility for the MEC framework. • Softwarization/SDN
    can provide enhanced scalability, accessibility, resiliency, and interoperability
    for MEC services. • Virtualization of the network can minimize the OPEX and CAPEX
    and can ensure more flexibility and rapid implementation of new services. • Network
    slicing, as one MEC enabler, can deliver the dynamic infrastructure and effective
    resource utilization. • ICN employs two conceptual designs, namely connectivity
    and caching at MEC computing servers to reduce the bandwidth congestion problem
    and enhance data delivery. • SFC can allow MEC to adjust a networking service
    function to the end user context and deliver end-to-end services. Incorporation
    of SFC within MEC is an acceptable technique for organizing service function implementation,
    realizing desired strategies, adapting applications when approaches or policies
    evolve, and rationally allocating resources to provide required or requested services.
    SFC offers a wide range of MEC applications, which can improve MEC functioning
    in terms of resource optimization, privacy, and accessibility. • H-CRAN can assure
    ease of the installation of the MEC system, taking into account the computing
    and storage facilities in the BBU pools as well as the deployment of the RRHs.
    By collocating MEC and H-CRAN, the expenditure on MEC implementation can be significantly
    minimized. The integration of MEC with H-CRAN can give the operational versatility
    and infrastructure reconfigurability that the virtualization of H-CRAN may deliver.
    • D2D-aided MEC can provide ad-hoc computation resources as per demand for scenarios
    such as IoT, V2V, V2X, etc. • AI/ML-based computation/communication resource allocation
    schemes can help to improve the efficiency of the MEC system. • NOMA and RSMA
    multiple access techniques can improve the communication resource utilization
    for MEC services. • The implementation of UAVs in MEC communication for content
    offloading can enable the possibility of ad-hoc implementation, reduces energy
    consumption and communication delay, provides better coverage and computational
    performance optimization. • The integration of SWIPT technology with MEC can enable
    networks to deliver computational services and energies to the device through
    the uplink and downlink at the same time. • The deployment of IRSs can improve
    the device-to-MEC data transmission rates, thus significantly minimizing their
    computation-offloading latency, particularly when the device-to-MEC LoS links
    are blocked. Since IRSs establish LoS links and/or introduce scattering and beamforming
    gains, thereby, improved communication channel/s can be obtained which improves
    energy efficiency. • Game Theory and Auction Theory can be utilized for improved
    computation and communication resource orchestration in the MEC framework. • Digital
    Twin-enabled MEC network can establish a bridge between the physical MEC facility
    and digital systems. It can capture real-time network characteristics and utilize
    them to make optimum network decisions instantly from a centralized standpoint.
    Therefore, it may be used to directly develop and optimize network strategies
    such as workload offloading, allocation of resources, caching, and so forth, and
    the connectivity schemes'' effectiveness and affordability can be improved. It
    can enable computationally intensive applications like Metaverse and automated
    vehicles. • Open-source MEC can improve the accessibility and flexibility of the
    evolving MEC systems. Specifically, open-source MEC splits the closely connected
    service functions into numerous separate NFs and supports them flexibly via a
    simpler service-based interface (SBI) by establishing a service-based MEC layer.
    The open-source MEC framework will be highly assistive to embrace the evolving
    network infrastructures. • Scalability, computational and energy efficiency can
    be achieved by incorporating quantum computing in MEC. However, the research on
    quantum computing for MEC is still in its early stage, therefore, further research
    is required. • The fusion of MEC and PON systems can be a promising option for
    delivering latency-sensitive but task-intensive services to meet such demanding
    QoS requirements. • Applications and services such as smart cities, smart agriculture,
    smart industries, the e-healthcare, smart vehicular network, video analytics,
    etc. can be benefited from the escalated computational facilities of MEC. • State-of-the-art
    security or privacy-preserving mechanisms should be researched and analyzed continuously
    since threats against the MEC framework are ever-evolving and maintaining security
    and privacy is a significant challenging task. Especially, the security and privacy
    of the Virtual Machines should be strictly maintained otherwise confidentiality
    of the system and user data can be threatened. 10.2. Challenges and future directions
    Standardization: Edge computing has evolved as an enticing and critical paradigm
    for scientific and industrial endeavors. Various standardization organizations
    have worked hard to develop suggestions and references for attempting to incorporate
    MEC from either the edge of the network or the MEC-5G standardized network level
    (Tang et al.b). The International Electro-technical Commission (IEC) and ISO have
    worked hard to define cloud infrastructure, software packages, the Virtual Machine
    and Container maintenance, and orchestration. ETSI is a major participant in the
    5G-MEC industry, and its various white papers have contributed to the standardization
    of MEC systems. A versatile standardization approach for the MEC paradigm is required
    to embrace beyond 5G, i.e., 6G networks. In this context, frameworks like open-source
    MEC should be appropriately analyzed and enhanced to incorporate versatile 6G
    networking infrastructure. Energy Consumption: Environmental change has become
    one of the major urgent challenges of the twenty-first century. Global warming
    has forced the world to concentrate more on clean, renewable energy. Yet, with
    today''s growing electricity usage owing to revolutionary frontier applications,
    there is a greater than ever necessity for utilizing renewable sources of energy
    as the major driver of MEC infrastructure. Whereas MEC promises to minimize energy
    consumption, there has been a growing demand for MEC servers to be powered by
    clean energy and recovered energy sources. Nonetheless, significant attempts are
    being made to reduce MEC servers'' energy use while simultaneously making offloading
    decisions that promote MEC powered by renewable energy (Li et al., 2019e). The
    MEC deployment moves storage and computing facilities that were previously in
    the data center to the network''s edge. On the other side, the network edge may
    react to user requests while also decreasing the unnecessary use of returning
    services. One of the primary areas of attention in MEC network enhancement is
    energy efficiency. Media caching, processing, and connectivity between MECs and
    subscribers or user devices altogether can result in high energy usage in the
    MEC transmission scheme. As a result, it is crucial to design an effective resource
    optimization method to minimize system energy usage to adequately plan caching,
    processing, and communication resources. Moreover, there are three types of energy
    usage for the combined optimization of MEC-based media caching and encoding/decoding:
    caching, encoding/decoding, and signal propagation energy consumption. As a result,
    determining ways to enhance total energy efficiency while simultaneously considering
    caching, encoding/decoding, and transmission is a significant problem for future
    research. Efficiency and Scalability of Mobility Management Network Functions:
    During handovers, packets or data payloads may be delayed at the originating base
    station for improved mobility control (Wang et al., 2020g). The queued packets
    are sent from the originating base station to the replacement base station when
    the user device is switched over to another base station. Any shared anchoring
    switch linked to the source continues to multicast payloads to all potential recipient
    base stations. MEC hosts connected to base stations can cache packets, and just
    a high-capacity MEC server can serve as the anchoring switch. Further study is
    needed to improve the efficiency and scalability of mobility management networking
    functions with the help of MEC hosts. In addition to the foregoing, when a device
    is going to be handed over, observing the interactions of MEC features, such as
    the functionality of the targeted MEC host and position management, is an unexplored
    field. Handover may be conducted while taking MEC factors into account: for instance,
    a handover to a MEC server must be executed only when it has adequate resources
    (Ho and Nguyen, 2022). Studies concentrating on mobility management, notably Virtual
    Machine migration, have largely assumed that each user device''s computational
    task/s is computed by a single computing station. As a result, the difficulty
    is to manage the Virtual Machine migration method effectively when the operation
    is offloaded to multiple computational nodes. Furthermore, Virtual Machine migration
    places a heavy burden on the backhaul and therefore causes significant latency,
    which renders it inappropriate for real-time operations. As a result, more improved
    methodologies for highly rapid Virtual Machine migration in millisecond range
    need to be devised. Nevertheless, because of the communication issues between
    computing servers, this approach is quite difficult. As a result, a more practical
    task is finding ways to pre-migrate the computations in advance (e.g., using certain
    prediction techniques) such that no service interruption is apparent to users.
    Although solutions above may reduce the Virtual Machine migrating period, stand-alone
    Virtual Machine migrating may indeed be inappropriate for real-time applications.
    As a result, it is crucial to direct the majority of studies on the collaboration
    of independent mobility management strategies. Dynamic management and joint integration
    of all strategies (including power management, Virtual Machine migration, compaction
    of migrated content, and/or route planning) should be explored more extensively
    in this respect to improve the user experience for user devices and improve the
    entire system performance for migrating users. MEC Service Orchestration and Programmability:
    Service orchestration and reconfigurability concerning distinct levels of the
    MEC system (i.e., application platform, infrastructure, and services offered on
    the system) remain unresolved concerns that pose substantial obstacles. Service
    orchestration must be undertaken in tandem with network resource portability,
    taking potentials of the Virtualized Network Function allocation into account,
    particularly the stretching of functions across a collection of edge-cloud systems.
    Edge-cloud systems located across separate management domains present additional
    impediments for service orchestration when considering interoperable resources,
    where further studies are needed for resource aggregation as well as service mapping
    mechanisms, in addition to the specification of the associated APIs. This article
    discusses various service attributes of edge-cloud coordination and reconfigurability,
    such as (i) service operational activities, i.e., allocating resources, service
    alignment, platform selection, and trustworthiness, (ii) continuity of service
    and flexibility within a cluster of MEC/edge computing stations, and (iii) cooperative
    optimization of Virtualized Network Functions as well as MEC services on pervasive
    edge computing systems to achieve effective utilization of resources and cross-layer
    or inter-tier optimization or improvement among edge-cloud services. Moreover,
    it provides an overview of the diverse MEC system architecture scenarios and specifies
    the prospective MEC orchestrator deployment alternatives while taking into account
    various SDN/NFV convergence opportunities that enable diverse edge-cloud resource
    management varieties. The development and standardization initiatives for providing
    effective MEC services are currently underway, with several hurdles remaining
    (Du et al., 2022). One of the main areas is the establishment of enhanced APIs
    that will allow third parties to purchase and coordinate resources on MEC systems
    simply and efficiently, including the related data models. Several APIs and data
    formats are still being discussed in ETSI-MEC in light of evolving usage cases,
    such as leveraging the mmWave scientific breakthrough to aid the MEC platform.
    Similar APIs should additionally be extended to give RAN-relevant or network-related
    information, allowing the application to observe the network. Multiple-MEC Coordinated
    Collaboration: MECs are often deployed in a dispersed fashion in edge networks,
    with MEC-based caches and computing capabilities scattered across the network.
    An individual MEC has limited storage capacity and computational power. Unnecessary
    caching and computation operations will cause the MEC server to become overloaded.
    When these activities are forwarded to the data center within the cloud, the expenses
    will rise. As a result of multi-MEC in decentralized mode, nearby MEC servers
    can work together to execute caching and computational activities. Other inactive
    MEC systems can be utilized to minimize network expenses and/or enhance the performance
    of the network when the present MEC server lacks caching or processing resources
    (Tran et al., 2017), (Shantharama et al., 2018). Also, resource sharing across
    MEC servers becomes a significant study topic. For example, how to choose a desired
    server among several other MEC servers that cache related material when the target
    multimedia content demanded by the subscriber is not cached by a native MEC host.
    The transfer of local computational activities to other MEC servers whenever the
    native MEC server''s processing demand is preoccupied is a notable research issue
    as well. The resource-sharing strategy based on decentralized multi-MEC collaboration
    has to be investigated further in the future to increase resource usage and QoE.
    Offloading Decision: The offloading approach selection is critical since it decides
    whether the task computing is executed locally, remotely, or concurrently in both
    places. All current researches on the offloading selection consider solely the
    user device''s energy usage into account. Nevertheless, to be consistent with
    forthcoming green networking, overall energy usage at the MEC (such as computation
    and associated communication) should be considered further throughout the offloading
    decision-making process. Furthermore, all studies working with the offloading
    decisions assume completely static circumstances, in which the user devices do
    not move before or during offloading. Yet, the energy required for data transmission
    might be drastically altered even during offloading when channel quality degrades
    owing to fading or uneven mobility. Therefore, offloading potentially increases
    energy usage and/or execution time as compared to local computing. As a result,
    it is required to provide more sophisticated ways for offloading decision-making.
    Such as the deployment of prediction approaches for the improvement of transmission
    quality in the context of user mobility (at the time of task offloading). Moreover,
    recent works that focus on the partly offloading decisions ignore the prospect
    of offloading particular pieces to various edge computing nodes. Many edge computing
    nodes provide greater flexibility and raise the likelihood that offloading towards
    the MEC will indeed be advantageous for the end device (in terms of energy usage
    and task execution time). A fundamental problem, in this case, is the aspect of
    backhaul connectivity between the interacting MEC systems and the capacity to
    accommodate their fluctuating load and characteristics during the offloading selection.
    User Experience and Bandwidth Tradeoffs: User experience is a metric of a user''s
    satisfaction or dissatisfaction with a service that focuses on the full service
    experience. The advancement of adjustable bitrate is a crucial motivation for
    investigating an effective way to improve user experience, consequently providing
    consumers with a distinct service to improve user satisfaction (Li et al., 2022d).
    Furthermore, research on the balance between bandwidth efficiency and user experience
    optimization in MEC-dependent caching and encoding/decoding techniques is an important
    area of study. From the standpoint of video content suppliers, two critical variables
    must be addressed for system optimization: (i) the requirement to lower the expenses
    of caching and computation; (ii) assuring an improved user experience to consumers
    (especially, meeting the requirement of computation and communication latency).
    As a result, how to optimize the expenditure of bandwidth capacity for MEC-dependent
    caching and encoding with user experience is an essential future research path.
    Security: In every network, privacy and safety are always key concerns. As a result,
    data and edge infrastructure security and privacy are critical for computational
    offloading. Privacy and confidentiality problems for security setup, threat detection,
    threat mitigation, and system verification remain unresolved. To limit the security
    vulnerabilities on the server level, software-defined segregation, and trust management
    verification are viable alternatives (Liu et al., 2019b). While running Secure
    Sockets Layer (SSL) or Transport Layer Security (TLS) guidelines on the UE is
    prohibitive in some contexts (Rango et al., 2020), identifying lightweight security
    standards for the MEC system that may be utilized for user identification, access
    regulation, password, and credential management is still challenging. In the application
    segmentation approach, the trade-off between computation for encrypting data and
    the requirement to use security certificates should be addressed. Unlike standard
    cloud computing, MEC exhibits substantial security vulnerabilities, particularly
    when placed within the ground base stations, or in places where it is exposed
    to physical assaults. As a result, MEC installations increase security measures
    against on-site threats. MEC also necessitates stricter security rules since third-party
    partners can obtain access to the service and retrieve information regarding user
    geographic location and radio statistics. Authentication based on third-party
    platform access credentials should be evaluated. One such possibility is to use
    strategies focused on public-key infrastructure (PKI). Another significant issue
    is isolation between various stakeholders, namely, between hosted programs. A
    security assault on one program should not impact other applications that are
    running, and isolation should offer privacy while ensuring bidirectional trust
    among collaborating parties. To provide safe cooperation and interoperability
    across diverse resources and multiple operating parties, fine-grained authorization
    with suitable encryption should be considered. In (Li et al., 2021d), a cutting-edge
    examination of security and privacy concerns related to edge cloud is undertaken,
    whereas (Salahdine et al., 2022) offers preliminary research on MEC security.
    With cloud computing, several intrusion detection approaches are in existence,
    but massive-scale geographically-distributed setups remain a concern for the forthcoming
    ultra-dense networks. Space-Air-Ground Integrated Network and MEC: UAVs have superior
    coverage and can deliver stable and smooth services utilizing LoS connections
    since they fly at higher altitudes. Yet, in some adverse conditions, UAVs may
    be unable to conduct vital duties on their own. As a result, help from other networks,
    such as satellite networks, is required (Peng et al., 2021). Since satellites
    often have extensive coverage areas, they may transport data and control instructions
    between UAVs and distant ground networks. Moreover, the MEC workstations can indeed
    be integrated with satellites to improve the edge computing capacity. The space-air-ground
    interactive MEC system has various obstacles. Satellites'' most prominent drawbacks
    are propagation loss as well as latency. Nevertheless, the high operational costs
    of satellite connectivity may preclude widespread use. Thankfully, recent advances
    in satellite technology have decreased the cost of LEO satellites significantly,
    and the transmission delay may be dropped to 1–4 ms because of the low orbit height
    (Liu et al., 2022h), (Bonafini et al., 2022). Yet, the rapid mobility of satellites
    as well as UAVs frequently alters the channel state, therefore, resulting in rapid
    handover, making management of space-air-ground unified MEC systems problematic.
    More significantly, operating satellites and UAVs in heterogeneous systems to
    provide reliable signaling exchanges and data transfer among many stakeholders
    is difficult. To address interoperability challenges and accomplish the promised
    advantages of space-air-ground interoperable MEC systems, more research into a
    complete mechanism for collaborative communication and computation, resource allocation,
    and cost-effective protocol design is required (Cheng et al., 2022b), (Guo et
    al., 2021b). Edge Intelligence: Edge intelligence is gaining traction, allowing
    user devices to execute the pre-trained DL algorithm from the edge of the network
    natively. Federated Learning (FL) seems to be a potential distributed Deep Neural
    Network (DNN) training approach to enhance confidentiality. The author of (Mills
    et al., 2022) recommended leaving the raw information on the user device and developing
    a common framework on the edge computing server by combining locally computed
    or processed updates. As a result, FL is a viable future option to deploy edge
    intelligence to improve MEC functioning. The Accessible Business Rules Framework
    (ABR)-based caching solution, which is based on the DRL algorithm, is an important
    research avenue for video streaming systems (Mu et al.). Every video block throughout
    the adaptable bitrate streaming system includes numerous bitrate variants. Caching
    multiple bitrates can result in a loss in caching resource usage and an escalation
    of network expenses due to the capacity constraint of the MEC-based caching system.
    With the assistance of MEC, network information, such as network link conditions
    and user activity, may be observed in real-time. Furthermore, this data may be
    examined and processed utilizing a DRL-based approach, which predicts the popularity
    of media content as well as the bitrate level for user reactions. As a result,
    for audiovisual caching with the relevant bitrate variant, the resource allocation
    technique may be determined in advance, which can increase the cache hit percentage
    and caching resource usage. When ML methods are deployed on resource-limited MEC
    workstations, there is a discrepancy between computational capacity and learning
    performance. As a result, efficiently implementing an ML algorithm on a MEC server
    with such a huge number of subscribers and a massive quantity of training data
    is difficult. Whereas an Artificial Neural Network (ANN) has several layers (e.g.,
    input/output and hidden layers), the optimizer and the stratified MEC architecture
    are intended to function together. An instantaneous layer of the complete ANN
    model can indeed be transferred to and handled by multiple MEC workstations (e.g.,
    MEC at macro and small cells), and the outcome of edge training is then relocated
    to higher-tier cloud computing for further processing. The shrinking of training
    dataset size, the utilization of pervasive computing, and the protection of user
    data confidentiality altogether provide significant benefits to cooperative learning.
    Moreover, DL techniques may be used at MEC servers to identify tainted and/or
    fabricated data, hence enhancing data quality. In (Ming et al., 2022), Ming et
    al. investigated Graph-Assisted Reinforcement Learning for video surveillance
    using IoT devices. Adaptive video functionality and edge processing have been
    developed to increase accuracy and minimize latency with constrained capacity.
    Traditional ML techniques are ineffective in protecting data confidentiality.
    Federated Learning distributes training data among individual users, allowing
    them to develop a shared model jointly while maintaining their unique data locally.
    Furthermore, Federated Learning can address serious disadvantages of Distributed
    Learning (Zhou et al., 2021b), such as (i) a scarcity of training data and time,
    (ii) poor performance because of diverse user functionality and network connectivity
    states, (iii) an inconsistent quantity of training dataset, and (iv) data that
    is not autonomous and identically dispersed among subscribers. Federated Learning
    is projected to be a powerful solution for addressing a variety of issues in MEC.
    Consider the computation offloading issue, in which large numbers of users attempt
    to offload their operations to a MEC server for external processing. Traditionally,
    to decide the offloading decision, users must send information to the MEC server
    along with channel conditions, current battery status, and computation capabilities;
    nevertheless, this information can be exposed to eavesdroppers and unlawfully
    utilized to anticipate the user location. With Federated Learning, every user
    downloads the master template from the MEC system and then determines the offloading
    preference based simply on its local features, where the MEC is only responsible
    for refreshing the master template in response to specific user variations. Federated
    Learning may ensure data privacy while also providing distributed offloading choices,
    making it suited for large-scale MEC infrastructures (Yu et al., 2021). 11. Conclusion
    The work performed a survey on the concurrent advancements of the MEC paradigm.
    Under this circumstance, it reviewed the existing survey or review works to provide
    an insight into the present state of research and development progress relative
    to MEC technologies. The work then illustrated an overview of the relative cloud
    technologies and fundamentals of MEC and hereafter discussed the architectural
    aspects of MEC. Afterward, the survey stated the contemporary state-of-the-art
    enabling technologies for MEC, including Virtual Machines, Containers, SDN, NFV,
    network slicing, ICN, SFC, radio access control, such as C-RAN, F-RAN, H-CRAN,
    D2D, machine learning/artificial intelligence, etc. Further, the advancing supporting
    technologies such as NOMA and RSMA, deployment of UAVs, energy harvesting and
    SWIPT, implementation of IRSs, Game Theory, Auction Theory, Digital Twin technologies,
    open-source framework (open-source MEC), quantum computing, PON, etc. are discussed.
    Then, system-aware and user-oriented deployment scenarios of MEC technologies
    are stated. Moreover, a brief description of security issues to MEC infrastructure
    and relative prevention mechanisms are briefed. Finally, the work provided a brief
    description of the lessons learned through the survey and mentioned challenges
    and future scope for the further improvement of MEC technologies. Hopefully, this
    survey will be highly supportive to enthusiasts from academia and industry. Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgement The authors would like to express
    their gratitude to the Department of Electrical and Computer Engineering, New
    York University (NYU) Abu Dhabi, Abu Dhabi, UAE. Appendix. Table 3 represents
    the list of acronyms. Table 3. List of Acronyms Acronyms Definitions 3GPP Third
    Generation Partnership Project 4C Joint Communication, Caching, Computing, and
    Control 5G Fifth Generation 6G Sixth Generation ABR Accessible Business Rules
    AF Application Function AFA Air Fuel Alliance AGV Autonomously Guided Vehicle
    AI Artificial Intelligence AM Amplitude Modulation ANN Artificial Neural Network
    AP Access Point API Application Program Interface AR Augmented Reality AS Antenna
    Splitting ASP Application Service Provider AUSF Authenticating/Identification
    Server Function B5G Beyond 5G BBU Baseband Unit BS Base Station BSS Base Station
    Subsystem CAPEX Capital Expenditure CDMA Code Division Multiple Access CD-NOMA
    Code-Domain NOMA CDUS Central Unit and Decentralized Unit Segregation CFS Customer
    Facing Service CN Core Network CPS Cyber-Physical System CPU Central Processing
    Unit C-RAN Cloud-RAN CR-NOMA Cognitive Radio-NOMA CSI Channel-State Information
    D2D Device-to-Device DDL Distributed Deep Learning DDoS Distributed DoS DDPQN
    Double Dueling Prioritized Deep Q-Network DDQN Double-Deep Q-Network DITEN Digital
    Twin-Enabled Edge Network DL Deep Learning DNN Data Network Name/Deep Neural Network
    DNS Domain Name System DoS Denial of Service DRL Deep Reinforcement Learning DT
    Digital Twin DUD Downlink and Uplink Detaching E2E End-to-End EB Exabytes EBI
    East-Bound functionality Interface EC Edge Computing ECDU Edge Content Distribution
    And Update EEDTO Energy-Efficient Dynamic Computational Task Offloading EH E-Healthcare/Energy
    Harvesting EM Element Maintenance/Electromagnetic eMBB enhanced Mobile Broadband
    ETSI European Telecommunications Standards Institute EV Electric Vehicle FANET
    Flying Ad Hoc Network FEC Fog-Edge-Cloud FM Frequency Modulation F-RAN Fog-RAN
    GBCO Game-Based Computational Offloading GPS Global Positioning System GT Game
    Theory HAP Higher Altitude Platform H-CRAN Heterogeneous-CRAN HetNet Heterogeneous
    Network HSD Hardware/Software Detaching Or Decoupling HTTP Hypertext Transmission
    Protocol IaaS Infrastructure-as-a-Service ICI Inter-Carrier Interference ICN Information-Centric
    Networking ICT Information and Communication Technology IEEE Institute of Electrical
    and Electronics Engineers IETF Internet Engineering Task Force IIoT Industrial
    IoT IoT Internet of Things IoV Internet of Vehicles/Vehicular Internet of Things
    IPTV Internet Protocol Television IPTVoWTTx IPTV over WTTx IRS Intelligent Reflecting
    Surface ISG Industrial Specifications Group ISI Inter-Symbol Interference ITS
    Intelligent Transportation Systems ITU International Telecommunication Union kW
    Kilo Watt LAN Local Area Network LAP Lower Altitude Platform LAPN/LADN Local Area
    Packet/Data Network LCM Lifecycle Management LEO Low Earth Orbit LoS Line-of-Sight
    LTE Long Term Evolution M2M Machine-to-Machine MAC Media Access Control MACC Mobile
    Ad Hoc-Based Cloud Computing MAR Mobile Augmented Reality MCC Mobile Cloud Computing
    MEC Mobile Edge Computing/Multi-Access Edge Computing MEH MEC Host MEO MEC Orchestrator
    MEP MEC Platform MEQC Multi-Access Edge-Quantum Computing MIMO Multi-Input Multi-Output
    MitM Man-in-the-Middle ML Machine Learning mMIMO Massive MIMO MMSE Minimum Mean
    Squared Error mmWave Millimeter Wave MNO Mobile Network Operator MSN Mobile Social
    Networking MUSA Multi-User Shareable Access NBI North-Bound functionality Interface
    NC Non-Cooperative NEF Network Exposure Function NFC Near-Field Communications
    NFV Network/Networking Functions Virtualization NFV MANO NFV Management and Orchestration
    NFVI NFV Infrastructures NFVO NFV Operator NLoS Non Line-of-Sight NOMA Non-Orthogonal
    Multiple Access NRF Network Resource Function OFDMA Orthogonal Frequency-Division
    Multiple Access OMA Orthogonal Multiple Access OPEX Operational Expenditure OS
    Operating System/Open-Source OSS Operation Support Subsystem OTT Over-The-Top
    PaaS Platform-as-a-Service PCF Policy Control Function PCR Predictive-Collaborative-Replacement
    PD-NOMA Power-Domain NOMA PKI Public-Key Infrastructure PON Passive Optical Network
    PPP Poisson Point Process PS Power Splitting PTE Power Transfer Efficiency PU
    Primary User QoE Quality of Experience QoS Quality of Service RAN Radio Access
    Network RAT Radio Access Technique/Technology RESTful Representational State Transfer-ful
    RF Radio Frequency RFID Radio Frequency Identification RIS Reconfigurable Intelligent
    Surface RL Reinforcement Learning RNI Radio Networking Information RRH Remote
    Radio Head RSMA Rate Splitting Multiple Access SaaS Software-as-a-Service SAGIN
    Space-to-Air-to-Ground Integrated Network SASE Secure Access Service Edge SBA
    Service Based Architecture SBI Service-Based Interface/South-Bound functionality
    Interface SC Superposition Coding SCADA Supervisory Control and Data Acquisition
    SCMA Sparse Code Multiple Access SDMA Space Division Multiple Access SDN Software
    Defined/Driven Networking SELinux Security Enhanced/Extended Linux SF Service
    Function SFC Service Function Chaining SFF Service Function Forwarder SIC Successive
    Interference Canceling SIMO Single-Input Multi-Output SINR Signal-to-Interference
    Plus Noise Ratio SLR Systematic Literature Review SMF Session Maintenance Function
    SOA Service Oriented Architecture SOAR Security Orchestrating, Automating, and
    Response SSC Sessions and Services Continuity SSL Secure Sockets Layer SU Secondary
    User SWIPT Simultaneous Wireless Information and Power Transfer TDM Time Division
    Multiplexing TDMA Time Division Multiple Access THz Terahertz TLS Transport Layer
    Security TPM Trusted Platform Manager TS Time Splitting UAV Unmanned Aerial Vehicle
    UCPS User Plane and Control Plane Separating UD User Device UDM Unified Data Managing
    UE User Equipment UPF User Plane Function URLLC Ultra-Reliable and Lower-Latency
    V2I Vehicle-to-Infrastructure V2N Vehicle-to-Network V2V Vehicle-to-Vehicle V2X
    Vehicle-to-Everything VIM Virtualized Infrastructure Manager VIM Virtualized/Virtual
    Infrastructure Manager VM Virtual Machines VMI Virtual Machine Introspection VNF
    Virtualized Network Function VNFM VNF Management VR Virtual Reality WAN Wide Area
    Network WDM Wavelength Division Multiplexing WIT Wireless Information Transmission
    WLAN Wireless Local Area Network WPC Wireless Power Consortium WPT Wireless Power
    Transfer WSN Wireless Sensing Network WTTx Wireless to the Everything/x XR Extended
    Reality Data availability No data was used for the research described in the article.
    References 5G Coral, 2018 5G Coral ETSI MEC Meeting in UC3M (2018) [Online]. Available:
    https://euprojects.netcom.it.uc3m.es/5g-coral/etsi-mec-meeting-in-uc3m/ Accessed:
    February 2023 Google Scholar Abbasi and Yanikomeroglu, 2021 O. Abbasi, H. Yanikomeroglu
    Rate-splitting and NOMA-enabled uplink user cooperation 2021 IEEE Wireless Communications
    and Networking Conference Workshops (WCNCW) (2021) Nanjing, China Google Scholar
    Abbasi and Yanikomeroglu, 2023 O. Abbasi, H. Yanikomeroglu Transmission scheme,
    detection and power allocation for uplink user cooperation with NOMA and RSMA
    IEEE Transactions on Wireless Communications, vol. 22 (2023), pp. 471-485 no.
    1 CrossRefView in ScopusGoogle Scholar Abbasi et al., 2022 M. Abbasi, M. Plaza-Hernández,
    J. Prieto, J.M. Corchado Security in the internet of things application layer:
    requirements, threats, and solutions IEEE Access, vol. 10 (2022), pp. 97197-97216
    CrossRefView in ScopusGoogle Scholar Abdelhakam and Elmesalawy, 2018 M.M. Abdelhakam,
    M.M. Elmesalawy Joint beamforming design and BBU computational resources allocation
    in heterogeneous C-RAN with QoS guarantee 2018 International Symposium on Networks,
    Computers and Communications (ISNCC) (2018), pp. 1-6 Rome, Italy CrossRefGoogle
    Scholar Abdelwahab et al., 2016 S. Abdelwahab, B. Hamdaoui, M. Guizani, T. Znati
    Replisom: disciplined tiny memory replication for massive IoT devices in LTE edge
    cloud IEEE Internet of Things Journal, 3 (3) (2016), pp. 327-338 View in ScopusGoogle
    Scholar Abdulqadder and Zhou, 2022 I.H. Abdulqadder, S. Zhou SliceBlock: context-aware
    authentication handover and secure network slicing using DAG-blockchain in edge-assisted
    SDN/NFV-6G environment IEEE Internet of Things Journal, vol. 9 (2022), pp. 18079-18097
    no. 18 CrossRefView in ScopusGoogle Scholar Abeywickrama et al., 2020 S. Abeywickrama,
    R. Zhang, C. Yuen Intelligent reflecting surface: practical phase shift model
    and beamforming optimization ICC 2020 - 2020 IEEE International Conference on
    Communications (ICC) (2020), pp. 1-6 Dublin, Ireland CrossRefGoogle Scholar Adam
    et al M. M. Adam, L. Zhao, K. Wang and Z. Han, \"Beyond 5G networks: integration
    of communication, computing, caching, and control,\" in China Communications.
    Google Scholar Agarwal et al., 2023 B. Agarwal, M.A. Togou, M. Ruffini, G.-M.
    Muntean QoE-driven optimization in 5G O-RAN-Enabled HetNets for enhanced video
    service quality IEEE Communications Magazine, 61 (1) (2023), pp. 56-62 CrossRefView
    in ScopusGoogle Scholar Ahammed et al., 2022 T.B. Ahammed, R. Patgiri, S. Nayak
    A Vision on the Artificial Intelligence for 6G Communication ICT Express (2022)
    Google Scholar Ahson and Mahgoub, 1998 S.A. Ahson, I. Mahgoub Research issues
    in mobile computing 1998 IEEE International Performance, Computing and Communications
    Conference. Proceedings (Cat. No.98CH36191), Tempe/Phoenix, AZ, USA (1998), pp.
    209-215 View in ScopusGoogle Scholar AirFuel, 2019 AirFuel 2nd Annual Developers
    Forum (2019) [Online]. Available: https://airfuel.org/developers-forum-2019-recap/
    Accessed: February 2023 Google Scholar Aishwarya and Malliga, 2014 R. Aishwarya,
    S. Malliga Intrusion detection system- an efficient way to thwart against Dos/DDos
    attack in the cloud environment 2014 International Conference on Recent Trends
    in Information Technology (2014), pp. 1-6 Chennai, India CrossRefGoogle Scholar
    Akhlaqi and Hanapi, 2023 M.Y. Akhlaqi, Z.B.M. Hanapi Task offloading paradigm
    in mobile edge computing-current issues, adopted approaches, and future directions
    Journal of Network and Computer Applications, 212 (2023) Google Scholar Akman
    et al., 2021 G. Akman, P. Ginzboorg, V. Niemi, “Privacy-preserving access for
    multi-access edge computing (MEC) applications,” In Computational Science and
    its Applications – ICCSA 2021. ICCSA 2021. Lecture Notes in Computer Science,
    Springer, Cham. Google Scholar Al-lawati and Al-Badi, 2016 A. Al-lawati, A.H.
    Al-Badi The impact of cloud computing IT departments: a case study of Oman''s
    financial institutions 2016 3rd MEC International Conference on Big Data and Smart
    City (ICBDSC), Muscat, Oman (2016), pp. 1-10 CrossRefGoogle Scholar Al-Rakham
    et al., 2021 M.S. Al-Rakham, et al. FallDeF5: a fall detection framework using
    5G-based deep gated recurrent unit networks IEEE Access, 9 (2021), pp. 94299-94308
    Google Scholar Al-Shareeda et al., 2022 M.A. Al-Shareeda, S. Manickam, S.A. Sari
    A survey of SQL injection attacks, their methods, and prevention techniques 2022
    International Conference on Data Science and Intelligent Computing (ICDSIC), Karbala,
    Iraq (2022), pp. 31-35 CrossRefView in ScopusGoogle Scholar Alabdulatif et al.,
    2020 A. Alabdulatif, I. Khalil, X. Yi Towards secure big data analytic for cloud-enabled
    applications with fully homomorphic encryption Journal of Parallel and Distributed
    Computing, 137 (2020), pp. 192-204 View PDFView articleView in ScopusGoogle Scholar
    Alam et al. et al., 2020 T. Alam, et al. Big data for smart cities: a case study
    of NEOM city, Saudi Arabia M.A. Khan, F. Algarni, M.T. Quasim (Eds.), Smart Cities:
    A Data Analytics Perspective. Lecture Notes in Intelligent Transportation and
    Infrastructure, Springer, Cham (2020) Google Scholar Albalawi et al., 2022 A.
    Albalawi, V. Vassilakis, R. Calinescu Side-channel attacks and countermeasures
    in cloud services and infrastructures NOMS 2022-2022 IEEE/IFIP Network Operations
    and Management Symposium (2022), pp. 1-4 Budapest, Hungary CrossRefGoogle Scholar
    Alghamdi et al., 2020 R. Alghamdi, et al. Intelligent surfaces for 6G wireless
    networks: a survey of optimization and performance analysis techniques IEEE Access,
    8 (2020), pp. 202795-202818 CrossRefView in ScopusGoogle Scholar Alhumaima et
    al., 2015 R.S. Alhumaima, M. Khan, H.S. Al-Raweshidy Modelling the energy efficiency
    of heterogeneous cloud radio access networks 2015 International Conference on
    Emerging Technologies (ICET) (2015), pp. 1-6 Peshawar, Pakistan CrossRefGoogle
    Scholar Ali et al., 2020 J. Ali, B. -h. Roh, B. Lee, J. Oh, M. Adil A machine
    learning framework for prevention of software-defined networking controller from
    DDoS attacks and dimensionality reduction of big data 2020 International Conference
    on Information and Communication Technology Convergence (ICTC) (2020), pp. 515-519
    Jeju, Korea (South) CrossRefView in ScopusGoogle Scholar Ali et al., 2021 B. Ali,
    M.A. Gregory, S. Li Multi-access edge computing architecture, data security and
    privacy: a review IEEE Access, 9 (2021), pp. 18706-18721 CrossRefView in ScopusGoogle
    Scholar Aljuhani and Alharbi, 2017 A. Aljuhani, T. Alharbi Virtualized Network
    Functions security attacks and vulnerabilities 2017 IEEE 7th Annual Computing
    and Communication Workshop and Conference (CCWC) (2017), pp. 1-4 Las Vegas, NV,
    USA CrossRefGoogle Scholar AlShalaan et al., 2022 M. AlShalaan, R. AlSubaie, R.
    Latif Location privacy issues in location-based services 2022 Fifth International
    Conference of Women in Data Science at Prince Sultan University (WiDS PSU) (2022),
    pp. 129-132 Riyadh, Saudi Arabia CrossRefView in ScopusGoogle Scholar Alvizu et
    al., 2017 R. Alvizu, et al. Comprehensive survey on T-SDN: software-defined networking
    for transport networks IEEE Communications Surveys & Tutorials, vol. 19 (2017),
    pp. 2232-2283 no. 4 View in ScopusGoogle Scholar Alwis et al., 2021 C.D. Alwis,
    et al. Survey on 6G frontiers: trends, applications, requirements, technologies
    and future research IEEE Open Journal of the Communications Society, 2 (2021),
    pp. 836-886 CrossRefView in ScopusGoogle Scholar Amin and Mahamud, 2019 A.A.M.M.
    Amin, M.S. Mahamud An alternative approach of mitigating ARP based man-in-the-middle
    attack using client site bash script 2019 6th International Conference on Electrical
    and Electronics Engineering (ICEEE) (2019), pp. 112-115 Istanbul, Turkey CrossRefGoogle
    Scholar Amith Raj et al., 2016 M.P. Amith Raj, A. Kumar, S.J. Pai, A. Gopal Enhancing
    security of Docker using Linux hardening techniques 2016 2nd International Conference
    on Applied and Theoretical Computing and Communication Technology (iCATccT) (2016),
    pp. 94-99 Bangalore, India View in ScopusGoogle Scholar Ammar et al., 2022 Ammar,
    et al. Requirements for energy-harvesting-driven edge devices using task-offloading
    approaches Electronics, 11 (3) (2022) Google Scholar Anda et al., 2022 L. Anda,
    R. Roštecký, M. Galiński Identification of malicious behaviour in content delivery
    network environment 2022 International Symposium ELMAR, Zadar, Croatia (2022),
    pp. 115-118 CrossRefView in ScopusGoogle Scholar Angin et al., 2016 P. Angin,
    B. Bhargava, R. Ranchal Tamper-resistant autonomous agents-based mobile-cloud
    computing NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium,
    Istanbul, Turkey (2016), pp. 843-847 CrossRefView in ScopusGoogle Scholar Angui
    et al., 2022 B. Angui, R. Corbel, V.Q. Rodriguez, E. Stephan Towards 6G zero touch
    networks: the case of automated Cloud-RAN deployments 2022 IEEE 19th Annual Consumer
    Communications & Networking Conference (CCNC) (2022), pp. 1-6 Las Vegas, NV, USA
    CrossRefView in ScopusGoogle Scholar Anwar et al., 2022 M.R. Anwar, S. Wang, M.F.
    Akram, S. Raza, S. Mahmood 5G-Enabled MEC: a distributed traffic steering for
    seamless service migration of internet of vehicles IEEE Internet of Things Journal,
    vol. 9 (2022), pp. 648-661 no. 1 CrossRefView in ScopusGoogle Scholar Apostolopoulos
    et al., 2020 P.A. Apostolopoulos, E.E. Tsiropoulou, S. Papavassiliou Risk-aware
    data offloading in multi-server multi-access edge computing environment IEEE/ACM
    Transactions on Networking, vol. 28 (2020), pp. 1405-1418 no. 3 CrossRefView in
    ScopusGoogle Scholar Apostolopoulos et al., 2023 P.A. Apostolopoulos, G. Fragkos,
    E.E. Tsiropoulou, S. Papavassiliou Data offloading in UAV-assisted multi-access
    edge computing systems under resource uncertainty IEEE Transactions on Mobile
    Computing, 22 (1) (2023), pp. 175-190 CrossRefView in ScopusGoogle Scholar Arnaz
    et al., 2022 A. Arnaz, J. Lipman, M. Abolhasan, M. Hiltunen Toward integrating
    intelligence and programmability in open radio access networks: a comprehensive
    survey IEEE Access, vol. 10 (2022), pp. 67747-67770 CrossRefView in ScopusGoogle
    Scholar Arora and Jaiswal, 2022 G. Arora, A. Jaiswal Zero SIC based rate splitting
    multiple access technique IEEE Communications Letters, 26 (10) (2022), pp. 2430-2434
    CrossRefView in ScopusGoogle Scholar Arthurs et al., 2022 P. Arthurs, L. Gillam,
    P. Krause, N. Wang, K. Halder, A. Mouzakitis A taxonomy and survey of edge cloud
    computing for intelligent transportation systems and connected vehicles IEEE Transactions
    on Intelligent Transportation Systems, 23 (7) (2022), pp. 6206-6221 CrossRefView
    in ScopusGoogle Scholar Artych et al., 2022 R. Artych, et al. Security constraints
    for placement of latency sensitive 5G MEC applications 2022 9th International
    Conference on Future Internet of Things and Cloud (FiCloud) (2022), pp. 40-45
    Rome, Italy CrossRefView in ScopusGoogle Scholar Azimi et al., 2022 Y. Azimi,
    S. Yousefi, H. Kalbkhani, T. Kunz Applications of machine learning in resource
    management for RAN-slicing in 5G and beyond networks: a survey IEEE Access, 10
    (2022), pp. 106581-106612 CrossRefView in ScopusGoogle Scholar Babar et al., 2021
    M. Babar, M.S. Khan, F. Ali, M. Imran, M. Shoaib Cloudlet computing: recent advances,
    taxonomy, and challenges IEEE Access, 9 (2021), pp. 29609-29622 CrossRefView in
    ScopusGoogle Scholar Baccour et al., 2020 E. Baccour, et al. Collaborative hierarchical
    caching and transcoding in edge network with CE-D2D communication Journal of Network
    and Computer Applications, 172 (2020) Google Scholar Baek et al., 2014 H.w. Baek,
    A. Srivastava, J.V.d. Merwe CloudVMI: virtual machine introspection as a cloud
    service 2014 IEEE International Conference on Cloud Engineering (2014), pp. 153-158
    Boston, MA, USA CrossRefView in ScopusGoogle Scholar Bai et al J. Bai, X. Chang,
    F. Machida, L. Jiang, Z. Han and K. S. Trivedi, \"Impact of service function aging
    on the dependability for MEC service function chain,\" in IEEE Transactions on
    Dependable and Secure Computing. Google Scholar Bai et al., 2021 T. Bai, C. Pan,
    C. Han, L. Hanzo Reconfigurable intelligent surface aided mobile edge computing
    IEEE Wireless Communications, 28 (6) (2021), pp. 80-86 CrossRefView in ScopusGoogle
    Scholar Baig et al., 2015 R. Baig, et al. Community clouds at the edge deployed
    in Guifi.net 2015 IEEE 4th International Conference on Cloud Networking (CloudNet),
    Niagara Falls, ON, Canada (2015), pp. 213-215 CrossRefView in ScopusGoogle Scholar
    Bale et al., 2021 A.S. Bale, et al. Mobile cloud computing - enabling technologies
    and applications 2021 6th International Conference on Signal Processing, Computing
    and Control (ISPCC), Solan, India (2021), pp. 491-496 CrossRefView in ScopusGoogle
    Scholar Banafaa et al., 2023 M. Banafaa, et al. 6G mobile communication technology:
    requirements, targets, applications, challenges, advantages, and opportunities
    Alexandria Engineering Journal, 64 (2023), pp. 245-274 View PDFView articleView
    in ScopusGoogle Scholar Bang et al., 2023 I. Bang, V.A.S. Manya, J. Kim, T. Kim
    On the effect of malicious user on D2D cluster: CSI forgery and countermeasures
    IEEE Access, vol. 11 (2023), pp. 5517-5527 CrossRefView in ScopusGoogle Scholar
    Bansal et al., 2021 A. Bansal, K. Singh, B. Clerckx, C.-P. Li, M.-S. Alouini Rate-splitting
    multiple access for intelligent reflecting surface aided multi-user communications
    IEEE Transactions on Vehicular Technology, 70 (9) (2021), pp. 9217-9229 CrossRefView
    in ScopusGoogle Scholar Barabosch and Gerhards-Padilla, 2014 T. Barabosch, E.
    Gerhards-Padilla Host-based code injection attacks: a popular technique used by
    malware 2014 9th International Conference on Malicious and Unwanted Software:
    the Americas (MALWARE), Fajardo, PR, USA (2014), pp. 8-17 View in ScopusGoogle
    Scholar Barakabitze and Hines, 2023 Alcardo Barakabitze, Andrew Hines Management
    of multimedia services in emerging architectures using big data analytics: MEC,
    ICN, and fog/cloud computing Multimedia Streaming in SDN/NFV and 5G Networks:
    Machine Learning for Managing Big Data Streaming, IEEE (2023), pp. 119-132 Google
    Scholar Barbara, 1999 D. Barbara Mobile computing and databases-A survey IEEE
    Transactions on Knowledge and Data Engineering, vol. 11 (1999), pp. 108-117 no.
    1 View in ScopusGoogle Scholar Barbarulo et al., 2022 F. Barbarulo, C. Puliafito,
    A. Virdis, E. Mingozzi Enabling application relocation in ETSI MEC: a container-migration
    approach 2022 IEEE 33rd Annual International Symposium on Personal, Indoor and
    Mobile Radio Communications (PIMRC) (2022), pp. 1-6 Kyoto, Japan CrossRefGoogle
    Scholar Barrère et al., 2018 M. Barrère, et al. CPS-MT: a real-time cyber-physical
    system monitoring tool for security research 2018 IEEE 24th International Conference
    on Embedded and Real-Time Computing Systems and Applications (RTCSA), Hakodate,
    Japan (2018), pp. 240-241 CrossRefView in ScopusGoogle Scholar Barry et al., 2020
    M.A. Barry, J.K. Tamgno, C. Lishou Influence of quality service in IP/MPLS network
    load with IPTV and VoD services 2020 22nd International Conference on Advanced
    Communication Technology (ICACT) (2020), pp. 378-385 Phoenix Park, Korea (South)
    CrossRefView in ScopusGoogle Scholar Bartwal et al., 2022 U. Bartwal, S. Mukhopadhyay,
    R. Negi, S. Shukla Security orchestration, automation, and response engine for
    deployment of behavioural honeypots 2022 IEEE Conference on Dependable and Secure
    Computing (DSC) (2022), pp. 1-8 Edinburgh, United Kingdom CrossRefGoogle Scholar
    Basar et al., 2019 E. Basar, M. Di Renzo, J. De Rosny, M. Debbah, M.-S. Alouini,
    R. Zhang Wireless communications through reconfigurable intelligent surfaces IEEE
    Access, 7 (2019), pp. 116753-116773 CrossRefView in ScopusGoogle Scholar Bashir
    and Mohamad Yusof, 2019 M.N. Bashir, K. Mohamad Yusof Green mesh network of UAVs:
    a survey of energy efficient protocols across physical, data link and network
    layers 2019 4th MEC International Conference on Big Data and Smart City (ICBDSC)
    (2019), pp. 1-6 Muscat, Oman View in ScopusGoogle Scholar Bebortta et al., 2022
    S. Bebortta, D. Senapati, C.R. Panigrahi, B. Pati Adaptive performance modeling
    framework for QoS-aware offloading in MEC-based IIoT systems IEEE Internet of
    Things Journal, 9 (12) (2022), pp. 10162-10171 CrossRefView in ScopusGoogle Scholar
    Behravesh et al., 2021 R. Behravesh, D. Harutyunyan, E. Coronado, R. Riggio Time-sensitive
    mobile user association and SFC placement in MEC-enabled 5G networks IEEE Transactions
    on Network and Service Management, vol. 18 (2021), pp. 3006-3020 no. 3 CrossRefView
    in ScopusGoogle Scholar Bellavista et al., 2019 P. Bellavista, D. Belli, S. Chessa,
    L. Foschini A social-driven edge computing architecture for mobile crowd sensing
    management IEEE Communications Magazine, vol. 57 (2019), pp. 68-73 no. 4 CrossRefView
    in ScopusGoogle Scholar Bertels et al., 2020 K. Bertels, et al. Quantum computer
    architecture toward full-stack quantum accelerators IEEE Transactions on Quantum
    Engineering, vol. 1 (2020), pp. 1-17 CrossRefGoogle Scholar Bhasin et al., 2022
    S. Bhasin, D. Jap, S. Picek On (in)Security of edge-based machine learning against
    electromagnetic side-channels 2022 IEEE International Symposium on Electromagnetic
    Compatibility & Signal/Power Integrity (EMCSI) (2022), pp. 262-267 Spokane, WA,
    USA CrossRefView in ScopusGoogle Scholar Bishoyi and Misra, 2022 P.K. Bishoyi,
    S. Misra Towards energy-and cost-efficient sustainable MEC-assisted healthcare
    systems IEEE Transactions on Sustainable Computing, 7 (4) (2022), pp. 958-969
    CrossRefView in ScopusGoogle Scholar Bittencourt et al., 2018 L. Bittencourt,
    et al. The internet of things, fog and cloud continuum: integration and challenges
    Internet of Things, 3–4 (2018), pp. 134-155 View PDFView articleView in ScopusGoogle
    Scholar Björnson et al., 2020 E. Björnson, Ö. Özdogan, E.G. Larsson Intelligent
    reflecting surface versus decode-and-forward: how large surfaces are needed to
    beat relaying? IEEE Wireless Communications Letters, 9 (2) (2020), pp. 244-248
    CrossRefView in ScopusGoogle Scholar Blanco et al., 2017 B. Blanco, et al. Technology
    pillars in the architecture of future 5G mobile networks: NFV, MEC and SDN Computer
    Standards & Interfaces, 54 (4) (2017), pp. 216-228 View PDFView articleView in
    ScopusGoogle Scholar Bolettieri et al., 2022 S. Bolettieri, D. Thai Bui, R. Bruno
    Towards end-to-end application slicing in Multi-access Edge Computing systems:
    architecture discussion and proof-of-concept Future Generation Computer Systems,
    136 (2022), pp. 110-127 View PDFView articleView in ScopusGoogle Scholar Bonafini
    et al., 2022 S. Bonafini, C. Sacchi, F. Granelli, R. Bassoli, F.H.P. Fitzek, K.
    Kondepu 3D cloud-RAN functional split to provide 6G connectivity on Mars 2022
    IEEE Aerospace Conference (AERO), Big Sky, MT (2022), pp. 1-13 USA CrossRefGoogle
    Scholar Bopape et al., 2020 L.P. Bopape, B. Nleya, P. Khumalo A privacy and security
    preservation framework for D2D communication based smart grid services 2020 Conference
    on Information Communications Technology and Society (ICTAS) (2020), pp. 1-6 Durban,
    South Africa CrossRefGoogle Scholar Botez et al., 2020 R. Botez, C.-M. Iurian,
    I.-A. Ivanciu, V. Dobrota Deploying a dockerized application with Kubernetes on
    Google cloud platform 2020 13th International Conference on Communications (COMM)
    (2020), pp. 471-476 Bucharest, Romania CrossRefView in ScopusGoogle Scholar Boulogeorgos
    and Alexiou, 2020 A.-A.A. Boulogeorgos, A. Alexiou Performance analysis of reconfigurable
    intelligent surface-assisted wireless systems and comparison with relaying IEEE
    Access, 8 (2020), pp. 94463-94483 CrossRefView in ScopusGoogle Scholar Bréhon–Grataloup
    et al., 2022 L. Bréhon–Grataloup, R. Kacimi, A. Beylot Mobile edge computing for
    V2X architectures and applications: a survey Computer Networks, 206 (2022) Google
    Scholar Breitgand et al., 2021 D. Breitgand, V. Eisenberg, N. Naaman, N. Rozenbaum,
    A. Weit Toward True Cloud Native NFV MANO,\" 2021 12th International Conference
    on Network of the Future (NoF) (2021), pp. 1-5 Coimbra, Portugal CrossRefGoogle
    Scholar Brik et al., 2020 B. Brik, P.A. Frangoudis, A. Ksentini Service-oriented
    MEC applications placement in a federated edge cloud architecture ICC 2020 - 2020
    IEEE International Conference on Communications (ICC) (2020), pp. 1-6 Dublin,
    Ireland CrossRefGoogle Scholar Budhiraja et al., 2021a I. Budhiraja, et al. A
    systematic review on NOMA variants for 5G and beyond IEEE Access, 9 (2021), pp.
    85573-85644 CrossRefView in ScopusGoogle Scholar Budhiraja et al., 2021b I. Budhiraja,
    N. Kumar, S. Tyagi, S. Tanwar, M. Guizani SWIPT-enabled D2D communication underlaying
    NOMA-based cellular networks in imperfect CSI IEEE Transactions on Vehicular Technology,
    70 (1) (2021), pp. 692-699 CrossRefView in ScopusGoogle Scholar Butt et al., 2019
    T.A. Butt, R. Iqbal, K. Salah, M. Aloqaily, Y. Jararweh Privacy management in
    social internet of vehicles: review, challenges and blockchain based solutions
    IEEE Access, vol. 7 (2019), pp. 79694-79713 CrossRefView in ScopusGoogle Scholar
    Caceres et al., 2022 F.M. Caceres, K. Sithamparanathan, S. Sun Theoretical analysis
    of hybrid SIC success probability under Rayleigh channel for uplink CR-NOMA IEEE
    Transactions on Vehicular Technology, 71 (10) (2022), pp. 10584-10599 CrossRefView
    in ScopusGoogle Scholar Cai et al G. Cai, B. Fan, Y. Dong, T. Li, Y. Wu and Y.
    Zhang, \"Task-efficiency oriented V2X communications: digital twin meets mobile
    edge computing,\" in IEEE Wireless Communications. Google Scholar Caiza and Sanz,
    2022 G. Caiza, R. Sanz Digital twin for monitoring an industrial process using
    augmented reality 2022 17th Iberian Conference on Information Systems and Technologies
    (CISTI) (2022), pp. 1-5 Madrid, Spain CrossRefGoogle Scholar Canto et al., 2021
    C.B. Canto, P.J. Roig, S. Filiposka, S.A. Carrasco, K. Gilly Challenges of implementing
    NFV-based multi-access edge computing environments 2021 29th Telecommunications
    Forum (TELFOR) (2021), pp. 1-4 Belgrade, Serbia CrossRefGoogle Scholar Cao et
    al., 2020 K. Cao, Y. Liu, G. Meng, Q. Sun An overview on edge computing research
    IEEE Access, 8 (2020), pp. 85714-85728 CrossRefView in ScopusGoogle Scholar Cao
    et al., 2021 K. Cao, S. Hu, Y. Shi, A.W. Colombo, S. Karnouskos, X. Li A survey
    on edge and edge-cloud computing assisted cyber-physical systems IEEE Transactions
    on Industrial Informatics, 17 (11) (2021), pp. 7806-7819 CrossRefView in ScopusGoogle
    Scholar Cárdenas and Fernández, 2020 A. Cárdenas, D. Fernández Network slice lifecycle
    management model for NFV-based 5G virtual mobile network operators 2020 IEEE Conference
    on Network Function Virtualization and Software Defined Networks (NFV-SDN) (2020),
    pp. 120-125 Leganes, Spain CrossRefView in ScopusGoogle Scholar Carpenter et al.,
    2021 J. Carpenter, J. Layne, E. Serra, A. Cuzzocrea Detecting botnet nodes via
    structural node representation learning 2021 IEEE International Conference on
    Big Data (Big Data) (2021), pp. 5357-5364 Orlando, FL, USA CrossRefView in ScopusGoogle
    Scholar Castro et al., 2016 H. Castro, M. Villamizar, O. Garcés, J. Pérez, R.
    Caliz, P.F.P. Arteaga Facilitating the execution of HPC workloads in Colombia
    through the integration of a private IaaS and a scientific PaaS/SaaS marketplace
    2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing
    (CCGrid) (2016), pp. 693-700 Cartagena, Colombia CrossRefView in ScopusGoogle
    Scholar Cenedese et al., 2014 A. Cenedese, A. Zanella, L. Vangelista, M. Zorzi
    Padova smart city: an urban internet of things experimentation Proceeding of IEEE
    International Symposium on a World of Wireless, Mobile and Multimedia Networks
    2014 (2014), pp. 1-6 Sydney, NSW, Australia CrossRefGoogle Scholar Chabbouh et
    al., 2017 O. Chabbouh, S.B. Rejeb, N. Agoulmine, Z. Choukair Cloud RAN architecture
    model based upon flexible RAN functionalities split for 5G networks 2017 31st
    International Conference on Advanced Information Networking and Applications Workshops
    (WAINA) (2017), pp. 184-188 Taipei, Taiwan CrossRefView in ScopusGoogle Scholar
    Chae et al., 2018 H. Chae, C.S. Lee, T.H. Kim The anti-tampering process and case
    study by the operating mode of various unmanned ground vehicles 2018 IEEE/ASME
    International Conference on Advanced Intelligent Mechatronics (AIM) (2018), pp.
    1432-1437 Auckland, New Zealand CrossRefView in ScopusGoogle Scholar Chakaravarthi
    et al., 2017 S. Chakaravarthi, P. Visu, B. Balu, V. Vineshwaran, M. Yakeshraj
    Web service registration and routing system and inter web proxy service model
    prevents the message alteration attacks, man-in-the middle attacks 2017 International
    Conference on Information Communication and Embedded Systems (ICICES) (2017),
    pp. 1-10 Chennai, India CrossRefGoogle Scholar Chakareski, 2015 J. Chakareski
    Joint source-channel rate allocation and client clustering for scalable multistream
    IPTV IEEE Transactions on Image Processing, 24 (8) (2015), pp. 2429-2439 View
    in ScopusGoogle Scholar Chang et al., 2021 Z.-L. Chang, C.-Y. Lee, C.-H. Lin,
    C.-Y. Wang, H.-Y. Wei Game-theoretic Intrusion Prevention System Deployment for
    Mobile Edge Computing 2021 IEEE Global Communications Conference (GLOBECOM), Madrid,
    Spain (2021), pp. 1-6 Google Scholar Chantre and Saldanha da Fonseca, 2020 H.D.
    Chantre, N.L. Saldanha da Fonseca The location problem for the provisioning of
    protected slices in NFV-based MEC infrastructure IEEE Journal on Selected Areas
    in Communications, 38 (7) (2020), pp. 1505-1514 CrossRefView in ScopusGoogle Scholar
    Chao et al., 2021 T.-H. Chao, J.-H. Wu, Y. Chiang, H.-Y. Wei 5G Edge Computing
    Experiments with Intelligent Resource Allocation for Multi-Application Video Analytics,\"
    2021 30th Wireless and Optical Communications Conference WOCC), Taipei, Taiwan
    (2021), pp. 80-84 CrossRefView in ScopusGoogle Scholar Chapple et al., 2021 Mike
    Chapple, James Michael Stewart, Darril Gibson Malicious code and application attacks
    ISC) 2 CISSP Certified Information Systems Security Professional Official Study
    Guide, Wiley (2021), pp. 993-1040 Google Scholar Chaturvedi et al., 2020 A.K.
    Chaturvedi, P. Kumar, K. Sharma Proposing innovative intruder detection system
    for host machines in cloud computing 2020 9th International Conference System
    Modeling and Advancement in Research Trends (SMART) (2020), pp. 292-296 Moradabad,
    India CrossRefView in ScopusGoogle Scholar Chawla et al., 2021 G.S. Chawla, et
    al. VMGuard: state-based proactive verification of virtual network isolation with
    application to NFV IEEE Transactions on Dependable and Secure Computing, vol.
    18 (2021), pp. 1553-1567 no. 4 View in ScopusGoogle Scholar Chen and Liao, 2019
    Y.-T. Chen, W. Liao Mobility-aware service function chaining in 5G wireless networks
    with mobile edge computing ICC 2019 - 2019 IEEE International Conference on Communications
    (ICC) (2019), pp. 1-6 Shanghai, China View PDFView articleGoogle Scholar Chen
    and Yang, 2019 J. Chen, A. Yang Intelligent agriculture and its key technologies
    based on internet of things architecture IEEE Access, 7 (2019), pp. 77134-77141
    CrossRefView in ScopusGoogle Scholar Chen et al., 2019 Z. Chen, Q. He, L. Liu,
    D. Lan, H.-M. Chung, Z. Mao An artificial intelligence perspective on mobile edge
    computing 2019 IEEE International Conference on Smart Internet of Things (SmartIoT)
    (2019), pp. 100-106 Tianjin, China CrossRefView in ScopusGoogle Scholar Chen et
    al., 2020 X. Chen, Y. Cai, L. Li, M. Zhao, B. Champagne, L. Hanzo Energy-efficient
    resource allocation for latency-sensitive mobile edge computing IEEE Transactions
    on Vehicular Technology, 69 (2) (2020), pp. 2246-2262 CrossRefView in ScopusGoogle
    Scholar Chen et al., 2021 R. Chen, et al. Joint computation offloading, channel
    access and scheduling optimization in UAV swarms: a game-theoretic learning approach
    IEEE Open Journal of the Computer Society, 2 (2021), pp. 308-320 CrossRefGoogle
    Scholar Chen et al., 2022 H.-M. Chen, Y.-F. Lu, S.-Y. Chen, C.-J. Chang, Z.-X.
    Zheng Design of an NFV MANO architecture for 5G private network with 5G CN cloud-edge
    collaborative mechanism 2022 8th International Conference on Applied System Innovation
    (ICASI) (2022), pp. 92-95 Nantou, Taiwan Google Scholar Chen et al., 2023a J.
    Chen, Q. Ding, X. Yang Non-cooperative game algorithms for computation offloading
    in mobile edge computing environments Journal of Parallel and Distributed Computing,
    172 (2023), pp. 18-31 View PDFView articleCrossRefGoogle Scholar Chen et al.,
    2023b G. Chen, Q. Wu, R. Liu, J. Wu, C. Fang IRS aided MEC systems with binary
    offloading: a unified framework for dynamic IRS beamforming IEEE Journal on Selected
    Areas in Communications, 41 (2) (2023), pp. 349-365 CrossRefView in ScopusGoogle
    Scholar Chen et al., 2023c G. Chen, Q. Wu, W. Chen, D.W.K. Ng, L. Hanzo IRS-aided
    wireless powered MEC systems: TDMA or NOMA for computation offloading? IEEE Transactions
    on Wireless Communications, vol. 22 (2023), pp. 1201-1218 no. 2 CrossRefView in
    ScopusGoogle Scholar Chen et al., 2023d R. Chen, et al. Overview of the development
    of secure access service edge International Conference on Signal and Information
    Processing, Networking and Computers, Lecture Notes in Electrical Engineering,
    vol. 996 (2023), pp. 138-145 View in ScopusGoogle Scholar Cheng and Yu, 2019 L.
    Cheng, T. Yu Game-theoretic approaches applied to transactions in the open and
    ever-growing electricity markets from the perspective of power demand response:
    an overview IEEE Access, 7 (2019), pp. 25727-25762 CrossRefView in ScopusGoogle
    Scholar Cheng et al., 2020 Q. Cheng, et al. Efficient resource allocation for
    NOMA-MEC system in ultra-dense network: a mean field game approach 2020 IEEE International
    Conference on Communications Workshops (ICC Workshops) (2020), pp. 1-6 Dublin,
    Ireland View in ScopusGoogle Scholar Cheng et al., 2022a S.-M. Cheng, B.-K. Hong,
    C.-F. Hung Attack detection and mitigation in MEC-enabled 5G networks for AIoT
    IEEE Internet of Things Magazine, vol. 5 (2022), pp. 76-81 no. 3 CrossRefGoogle
    Scholar Cheng et al., 2022b N. Cheng, et al. 6G service-oriented space-air-ground
    integrated network: a survey Chinese Journal of Aeronautics, 35 (9) (2022), pp.
    1-18 View PDFView articleView in ScopusGoogle Scholar Chi et al., 2022 C. Chi,
    Y. Wang, X. Tong, M. Siddula, Z. Cai Game theory in internet of things: a survey
    IEEE Internet of Things Journal, 9 (14) (2022), pp. 12125-12146 CrossRefView in
    ScopusGoogle Scholar Chin et al W. -L. Chin, H. -A. Ko, N. -W. Chen, P. -W. Chen
    and T. Jiang, \"Securing NFV/SDN IoT using Vnfs over a compute-intensive hardware
    resource in NFVI,\" in IEEE Network. Google Scholar Choi and Han, 2015 K.W. Choi,
    Z. Han Device-to-Device discovery for proximity-based service in LTE-advanced
    system IEEE Journal on Selected Areas in Communications, vol. 33 (2015), pp. 55-66
    no. 1 View in ScopusGoogle Scholar Chowdhury, 2021 M. Chowdhury Time and energy-efficient
    hybrid job scheduling scheme for mobile cloud computing empowered wireless sensor
    networks International Journal of Ad Hoc and Ubiquitous Computing, 37 (1) (2021),
    pp. 26-36 View in ScopusGoogle Scholar Chowdhury et al., 2021 S. Chowdhury, A.
    Nandi, M. Ahmad, A. Jain, M. Pawar A comprehensive survey for detection and prevention
    of SQL injection 2021 7th International Conference on Advanced Computing and Communication
    Systems (ICACCS) (2021), pp. 434-437 Coimbatore, India CrossRefView in ScopusGoogle
    Scholar Chua et al., 2016 M.Y.-K. Chua, F.R. Yu, S. Bu Dynamic operations of cloud
    radio access networks (C-RAN) for mobile cloud computing systems IEEE Transactions
    on Vehicular Technology, 65 (3) (2016), pp. 1536-1548 CrossRefGoogle Scholar Chung,
    2021 K. Chung Correlated superposition coding: lossless two-user NOMA implementation
    without SIC under user-fairness IEEE Wireless Communications Letters, 10 (9) (2021),
    pp. 1999-2003 CrossRefView in ScopusGoogle Scholar Ciobanu et al., 2021 A.-I.-E.
    Ciobanu, E. Borcoci, M.-C. Vochin A quality-of-service scenario awareness for
    use-cases of open-source management and control system hub in edge computing 2021
    IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)
    (2021), pp. 1-5 Bucharest, Romania CrossRefGoogle Scholar Cisco, 2018 Cisco Cisco
    Annual Internet Report (2018–2023) White Paper. March 2020 [Online]. Accessible:
    https://www.cisco.com/c/en/us/solutions/collateral/executive-perspectives/annual-internet-report/white-paper-c11-741490.html
    Accessed: February 2023 Google Scholar Conti and Giordano, 2014 M. Conti, S. Giordano
    Mobile ad hoc networking: milestones, challenges, and new research directions
    IEEE Communications Magazine, vol. 52 (2014), pp. 85-96 no. 1 View in ScopusGoogle
    Scholar Conti et al., 2016 M. Conti, N. Dragoni, V. Lesyk A survey of man in the
    middle attacks IEEE Communications Surveys & Tutorials, vol. 18 (2016), pp. 2027-2051
    no. 3 View in ScopusGoogle Scholar Costantino et al., 2023 G. Costantino, M. De
    Vincenzi, F. Martinelli, I. Matteucci A privacy-preserving solution for intelligent
    transportation systems: private driver DNA IEEE Transactions on Intelligent Transportation
    Systems, vol. 24 (2023), pp. 258-273 no. 1 CrossRefView in ScopusGoogle Scholar
    Cox et al., 2017 J.H. Cox, et al. Advancing software-defined networks: a survey
    IEEE Access, vol. 5 (2017), pp. 25487-25526 View in ScopusGoogle Scholar Cruz
    et al., 2022 P. Cruz, N. Achir, A.C. Viana On the edge of the deployment: a survey
    on multi-access edge computing ACM Computing Surveys, 55 (5) (2022), pp. 1-34
    CrossRefGoogle Scholar Cui et al., 2022 H. Cui, et al. Space-air-ground integrated
    network (SAGIN) for 6G: requirements, architecture and challenges China Communications,
    19 (2) (2022), pp. 90-108 CrossRefView in ScopusGoogle Scholar Cui et al., 2023
    G. Cui, Q. He, F. Chen, H. Jin, Y. Xiang, Y. Yang Location privacy protection
    via delocalization in 5G mobile edge computing environment IEEE Transactions on
    Services Computing, vol. 16 (2023), pp. 412-423 no. 1 View in ScopusGoogle Scholar
    D. N and S. J and S. P, 2022 D. N, S. J and S. P Intrusion detection in wireless
    sensor networks using optics algorithm 2022 International Conference on Applied
    Artificial Intelligence and Computing (ICAAIC) (2022), pp. 1265-1272 Salem, India
    Google Scholar Dai et al., 2018 L. Dai, B. Wang, Z. Ding, Z. Wang, S. Chen, L.
    Hanzo A survey of non-orthogonal multiple access for 5G IEEE Communications Surveys
    & Tutorials, 20 (3) (2018), pp. 2294-2323 CrossRefView in ScopusGoogle Scholar
    Dai et al., 2019 C. Dai, K. Zhu, R. Wang, Y. Xu Decoupled uplink-downlink user
    association in ultra-dense networks: a contract-theoretic approach 2019 IEEE Wireless
    Communications and Networking Conference (WCNC), Marrakesh, Morocco (2019), pp.
    1-6 Google Scholar Das and Ruffini, 2021 S. Das, M. Ruffini Optimal virtual PON
    slicing to support ultra-low latency mesh traffic pattern in MEC-based Cloud-RAN
    2021 International Conference on Optical Network Design and Modeling (ONDM) (2021),
    pp. 1-5 Gothenburg, Sweden View in ScopusGoogle Scholar Das et al., 2020 S. Das,
    F. Slyne, A. Kaszubowska, M. Ruffini Virtualized EAST–WEST PON architecture supporting
    low-latency communication for mobile functional split based on multiaccess edge
    computing Journal of Optical Communications and Networking, vol. 12 (2020), pp.
    109-119 no. 10 View in ScopusGoogle Scholar Dasilva et al., 2011 L.A. Dasilva,
    H. Bogucka, A.B. Mackenzie Game Theory in Wireless Networks IEEE Commun. Mag.,
    49 (8) (August 2011), pp. 110-111 View in ScopusGoogle Scholar De Micheli et al.,
    2022 G. De Micheli, J.-H.R. Jiang, R. Rand, K. Smith, M. Soeken Advances in quantum
    computation and quantum technologies: a design automation perspective IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems, vol. 12 (2022), pp. 584-601
    no. 3 CrossRefView in ScopusGoogle Scholar Del Carpio et al., 2015 L.F. Del Carpio,
    A.A. Dowhuszko, O. Tirkkonen, G. Wu Simple clustering methods for multi-hop cooperative
    device-to-device communication 2015 IEEE 81st Vehicular Technology Conference
    (VTC Spring) (2015), pp. 1-6 Glasgow, UK CrossRefGoogle Scholar Deng et al., 2020
    L. Deng, J. Luo, J. Zhou, J. Wang Identity-based secret sharing access control
    framework for information-centric networking 2020 IEEE/CIC International Conference
    on Communications in China (ICCC) (2020), pp. 507-511 Chongqing, China CrossRefView
    in ScopusGoogle Scholar Deng et al., 2022a C. Deng, X. Fang, X. Wang, K. Law Software
    orchestrated and hardware accelerated artificial intelligence: toward low latency
    edge computing IEEE Wireless Communications, 29 (4) (2022), pp. 110-117 CrossRefView
    in ScopusGoogle Scholar Deng et al., 2022b Y. Deng, et al. Resource provisioning
    for mitigating edge DDoS attacks in MEC-enabled SDVN IEEE Internet of Things Journal,
    vol. 9 (2022), pp. 24264-24280 no. 23 CrossRefView in ScopusGoogle Scholar Deng
    et al., 2022c X. Deng, T. Lv, L. Song Novel efficient block chain and rule-based
    intelligent privacy share system in future network IEEE INFOCOM 2022 - IEEE Conference
    on Computer Communications Workshops (INFOCOM WKSHPS) (2022), pp. 1-8 New York,
    NY, USA View in ScopusGoogle Scholar Diamanti and Papavassiliou, 2022 M. Diamanti,
    S. Papavassiliou Trading in collaborative mobile edge computing networks: a contract
    theory-based auction model 2022 18th International Conference on Distributed Computing
    in Sensor Systems (DCOSS), Marina del Rey (2022) Los Angeles, CA, USA Google Scholar
    Diao et al., 2021 X. Diao, W. Yang, L. Yang, Y. Cai UAV-Relaying-Assisted multi-access
    edge computing with multi-antenna base station: offloading and scheduling optimization
    IEEE Transactions on Vehicular Technology, 70 (9) (2021), pp. 9495-9509 CrossRefView
    in ScopusGoogle Scholar Dias et al., 2023 I. Dias, L. Ruan, C. Ranaweera, E. Wong
    From 5G to beyond: passive optical network and multi-access edge computing integration
    for latency-sensitive applications Optical Fiber Technology, 75 (2023) Google
    Scholar Ding et al., 2019 C. Ding, A. Zhou, Y. Liu, S. Wang ECDU: an edge content
    delivery and update framework in mobile edge computing EURASIP Journal on Wireless
    Communications and Networking, 268 (2019) Google Scholar Ding et al., 2022 Z.
    Ding, D. Xu, R. Schober, H.V. Poor Hybrid NOMA offloading in multi-user MEC networks
    IEEE Transactions on Wireless Communications, 21 (7) (2022), pp. 5377-5391 CrossRefView
    in ScopusGoogle Scholar Djigal et al., 2022 H. Djigal, J. Xu, L. Liu, Y. Zhang
    Machine and deep learning for resource allocation in multi-access edge computing:
    a survey IEEE Communications Surveys & Tutorials, 24 (4) (2022), pp. 2449-2494
    CrossRefView in ScopusGoogle Scholar Do et al., 2020a D.-T. Do, M.-S.V. Nguyen,
    F. Jameel, R. Jäntti, I.S. Ansari Performance evaluation of relay-aided CR-NOMA
    for beyond 5G communications IEEE Access, 8 (2020), pp. 134838-134855 CrossRefView
    in ScopusGoogle Scholar Do et al., 2020b D.-T. Do, C.-B. Le, F. Afghah Enabling
    full-duplex and energy harvesting in uplink and downlink of small-cell network
    relying on power domain based multiple access IEEE Access, 8 (2020), pp. 142772-142784
    CrossRefView in ScopusGoogle Scholar Dobrin and Dimiter, 2021 D. Dobrin, A. Dimiter
    DDoS Attack Identification Based on SDN,\" 2021 IEEE 20th International Symposium
    on Network Computing and Applications (NCA) (2021), pp. 1-8 Boston, MA, USA CrossRefGoogle
    Scholar Douch et al., 2022 S. Douch, M.R. Abid, K. Zine-Dine, D. Bouzidi, D. Benhaddou
    Edge computing technology enablers: a systematic lecture study IEEE Access, 10
    (2022), pp. 69264-69302 CrossRefView in ScopusGoogle Scholar Du et al., 2020a
    J. Du, N. Xue, D. Zhai, H. Cao, J. Feng, G. Lu Energy-efficient user clustering
    and resource management for NOMA based MEC systems 2020 IEEE Globecom Workshops
    (GC Wkshps (2020), pp. 1-6 Taipei, Taiwan View in ScopusGoogle Scholar Du et al.,
    2020b R. Du, W. He, J. Tian Virtual machine security migration strategy based
    on the edge computing environment 2020 16th International Conference on Mobility,
    Sensing and Networking (MSN) (2020), pp. 803-808 Tokyo, Japan CrossRefView in
    ScopusGoogle Scholar Du et al., 2022 J. Du, et al. Resource pricing and allocation
    in MEC enabled blockchain systems: an A3C deep reinforcement learning approach
    IEEE Transactions on Network Science and Engineering, vol. 9 (2022), pp. 33-44
    no. 1 CrossRefView in ScopusGoogle Scholar Dulik, 2021 M. Dulik Deploying Fake
    Network Devices to Obtain Sensitive User Data,\" 2021 Communication and Information
    Technologies (KIT) Vysoke Tatry, Slovakia (2021), pp. 1-5 CrossRefGoogle Scholar
    Dwivedi et al., 2022 Y.K. Dwivedi, et al. Metaverse beyond the hype: multidisciplinary
    perspectives on emerging challenges, opportunities, and agenda for research, practice
    and policy International Journal of Information Management, 66 (2022) Google Scholar
    Ebrahim et al., 2022 M.A. Ebrahim, G.A. Ebrahim, H.K. Mohamed, S.O. Abdellatif
    A deep learning approach for task offloading in multi-UAV aided mobile edge computing
    IEEE Access, 10 (2022), pp. 101716-101731 CrossRefView in ScopusGoogle Scholar
    Elayan et al., 2020 H. Elayan, O. Amin, B. Shihada, R.M. Shubair, M.-S. Alouini
    Terahertz band: the last piece of RF spectrum puzzle for communication systems
    IEEE Open Journal of the Communications Society, 1 (2020), pp. 1-32 CrossRefView
    in ScopusGoogle Scholar Elbamby et al., 2018 M.S. Elbamby, C. Perfecto, M. Bennis,
    K. Doppler Toward low-latency and ultra-reliable virtual reality IEEE Network,
    vol. 32 (2018), pp. 78-84 no. 2 CrossRefView in ScopusGoogle Scholar Elhattab
    et al., 2021 M. Elhattab, M. Kamel, W. Hamouda Edge-aware remote radio heads cooperation
    for interference mitigation in heterogeneous C-RAN IEEE Transactions on Vehicular
    Technology, 70 (11) (2021), pp. 12142-12157 CrossRefView in ScopusGoogle Scholar
    Eliyan and Pietro, 2021 L.F. Eliyan, R.D. Pietro DoS and DDoS attacks in Software
    Defined Networks: a survey of existing solutions and research challenges Future
    Generation Computer Systems, 122 (2021), pp. 149-171 View PDFView articleView
    in ScopusGoogle Scholar Elmeadawy and Shubair, 2019 S. Elmeadawy, R.M. Shubair
    6G wireless communications: future technologies and research challenges 2019 International
    Conference on Electrical and Computing Technologies and Applications (ICECTA),
    Ras Al Khaimah, United Arab Emirates (2019), pp. 1-5 CrossRefGoogle Scholar EPC.
    Taiwan Technology Standards Agency, 2016 EPC. Taiwan Technology Standards Agency
    Introduces AirFuel Alliance''s Resonant Wireless Charging Standard (2016) [Online].
    Available: https://epc-co.com/epc/events-and-news/news/artmid/1627/articleid/1756/taiwan-technology-standards-agency-introduces-airfuel-alliances-resonant-wireless-charging-standard--
    Accessed: February 2023 Google Scholar Erbati and Schiele, 2021 M.M. Erbati, G.
    Schiele Application- and reliability-aware service function chaining to support
    low-latency applications in an NFV-enabled network 2021 IEEE Conference on Network
    Function Virtualization and Software Defined Networks (NFV-SDN) (2021), pp. 120-123
    Heraklion, Greece CrossRefView in ScopusGoogle Scholar Ericsson, 2021 Ericsson
    Ericsson Mobility report June 2021. [Online]. Available: https://iotbusinessnews.com/download/white-papers/ERICSSON-june-2021-mobility-report.pdf
    Accessed: February 2023 Google Scholar Ericsson, 2022 Ericsson Ericsson Mobility
    Report: Global 5G Growth amid Macroeconomic Challenges (2022) [Online]. Available:
    https://www.ericsson.com/en/press-releases/2022/11/ericsson-mobility-report-global-5g-growth-amid-macroeconomic-challenges
    Accessed: February 2023 Google Scholar ETSI, 2014 ETSI New ETSI Mobile-Edge Computing
    Industry Specification Group Starts Work (2014) [Online]. Available: https://www.etsi.org/newsroom/news/852-2014-12-news-new-etsi-mobile-edge-computing-industry-specification-group-starts-work?jjj=1680708097216
    Accessed: February 2023 Google Scholar ETSI., 2018 ETSI. ETSI Multi-Access Edge
    Computing Group Publishes White Paper on Role for 5G (2018) [Online]. Available:
    https://www.etsi.org/newsroom/press-releases/1314-2018-06-press-etsi-multi-access-edge-computing-group-publishes-white-paper-on-role-for-5g
    Accessed: February 2023 Google Scholar ETSI, 2019 ETSI ETSI GS MEC 003 V2.1.1
    (2019-01) (2019) [Online]. Available: https://www.etsi.org/deliver/etsi_gs/mec/001_099/003/02.01.01_60/gs_mec003v020101p.pdf
    Accessed: February 2023 Google Scholar ETSI. Network, 2023 ETSI. Network Functions
    Virtualisation (NFV) [Online]. Available: https://www.etsi.org/technologies/nfv?highlight=ytoxontpoja7czozoijuznyio30=
    Accessed: February 2023 Google Scholar Fadlullah et al., 2022 Z.M. Fadlullah,
    B. Mao, N. Kato Balancing QoS and security in the edge: existing practices, challenges,
    and 6G opportunities with machine learning IEEE Communications Surveys & Tutorials,
    24 (4) (2022), pp. 2419-2448 CrossRefView in ScopusGoogle Scholar Fakhrulddin
    et al., 2020 S.S. Fakhrulddin, S. Kamel Gharghan, S.L. Zubaidi Accurate fall localization
    for patient based on GPS and accelerometer sensor in outside the house 2020 13th
    International Conference on Developments in eSystems Engineering (DeSE), Liverpool,
    United Kingdom (2020), pp. 432-436 CrossRefView in ScopusGoogle Scholar Fan and
    Han, 2022 L. Fan, Z. Han Hybrid quantum-classical computing for future network
    optimization IEEE Network, vol. 36 (2022), pp. 72-76 no. 5 CrossRefView in ScopusGoogle
    Scholar Fan et al., 2021 B. Fan, Y. Wu, Z. He, Y. Chen, T.Q.S. Quek, C.-Z. Xu
    Digital twin empowered mobile edge computing for intelligent vehicular lane-changing
    IEEE Network, 35 (6) (2021), pp. 194-201 CrossRefView in ScopusGoogle Scholar
    Fang et al., 2021 C. Fang, et al. Cache-assisted content delivery in wireless
    networks: a new game theoretic model IEEE Systems Journal, 15 (2) (2021), pp.
    2653-2664 CrossRefView in ScopusGoogle Scholar Farris et al., 2019 I. Farris,
    T. Taleb, Y. Khettab, J. Song A Survey on Emerging SDN and NFV Security Mechanisms
    for IoT Systems IEEE Communications Surveys & Tutorials, 21 (1) (Firstquarter
    2019), pp. 812-837 CrossRefView in ScopusGoogle Scholar Fatemi Moghaddam et al.,
    2015 F. Fatemi Moghaddam, M. Ahmadi, S. Sarvari, M. Eslami, A. Golkar Cloud Computing
    Challenges and Opportunities: A Survey,\" 2015 1st International Conference on
    Telematics and Future Generation Networks (TAFGEN) Kuala Lumpur, Malaysia (2015),
    pp. 34-38 CrossRefGoogle Scholar Fawcett et al., 2018 L. Fawcett, S. Scott-Hayward,
    M. Broadbent, A. Wright, N. Race Tennison: a distributed SDN framework for scalable
    network security IEEE Journal on Selected Areas in Communications, vol. 36 (2018),
    pp. 2805-2818 no. 12 CrossRefView in ScopusGoogle Scholar Feng et al., 2018 J.
    Feng, L. Zhao, J. Du, X. Chu, F.R. Yu Computation offloading and resource allocation
    in D2D-enabled mobile edge computing 2018 IEEE International Conference on Communications
    (ICC) (2018), pp. 1-6 Kansas City, MO, USA View PDFView articleGoogle Scholar
    Feng et al., 2021 B. Feng, H. Zhou, G. Li, Y. Zhang, K. Sood, S. Yu Enabling machine
    learning with service function chaining for security enhancement at 5G edges IEEE
    Network, 35 (5) (2021), pp. 196-201 CrossRefView in ScopusGoogle Scholar Feng
    et al., 2022 Feng, et al. Computation offloading in mobile edge computing networks:
    a survey Journal of Network and Computer Applications, 202 (2022) Google Scholar
    Filali et al., 2020 A. Filali, A. Abouaomar, S. Cherkaoui, A. Kobbane, M. Guizani
    Multi-access edge computing: a survey IEEE Access, 8 (2020), pp. 197017-197046
    CrossRefView in ScopusGoogle Scholar Filippou et al., 2020 M.C. Filippou, et al.
    Multi-access edge computing: a comparative analysis of 5G system deployments and
    service consumption locality variants IEEE Communications Standards Magazine,
    4 (2) (2020), pp. 32-39 CrossRefView in ScopusGoogle Scholar Forti et al., 2021
    S. Forti, et al. Probabilistic QoS-aware placement of VNF chains at the edge Theory
    and Practice of Logic Programming, vol. 22, Cambridge University Press (2021)
    no. 1 Google Scholar Fu et al., 2020 X. Fu, F.R. Yu, J. Wang, Q. Qi, J. Liao Performance
    Optimization for Blockchain-Enabled Distributed Network Function Virtualization
    Management and Orchestration IEEE Trans. Veh. Technol., 69 (6) (June 2020), pp.
    6670-6679 CrossRefView in ScopusGoogle Scholar Galloway et al., 2019 J. Galloway,
    A. White, P. O''Boyle, S. Wyllie Poster abstract: the impact of DDoS attack on
    docker Containers compared to virtual machines 2019 IEEE Cloud Summit (2019),
    pp. 122-123 Washington, DC, USA CrossRefView in ScopusGoogle Scholar Gao et al.,
    2018 Y. Gao, X. Wei, X. Zhang, W. Zhuang A combinational LDA-based topic model
    for user interest inference of energy efficient IPTV service in smart building
    IEEE Access, 6 (2018), pp. 48921-48933 CrossRefView in ScopusGoogle Scholar Gao
    et al., 2021 N. Gao, S. Jin, X. Li, M. Matthaiou Aerial RIS-assisted high altitude
    platform communications IEEE Wireless Communications Letters, 10 (10) (2021),
    pp. 2096-2100 CrossRefView in ScopusGoogle Scholar Garrich et al., 2019 M. Garrich,
    F.-J. Moreno-Muro, M.-V. Bueno Delgado, P. Pavón Mariño Open-source network optimization
    software in the open SDN/NFV transport ecosystem Journal of Lightwave Technology,
    37 (1) (2019), pp. 75-88 CrossRefView in ScopusGoogle Scholar Garzon et al., 2022
    S.R. Garzon, H. Yildiz, A. Küpper Towards decentralized identity management in
    multi-stakeholder 6G networks 2022 1st International Conference on 6G Networking
    (6GNet) (2022), pp. 1-8 Paris, France CrossRefGoogle Scholar Ghosh et al., 2019
    A. Ghosh, A. Maeder, M. Baker, D. Chandramouli 5G evolution: a view on 5G cellular
    technology beyond 3GPP release 15 IEEE Access, 7 (2019), pp. 127639-127651 CrossRefView
    in ScopusGoogle Scholar Ghosh et al., 2022 S. Ghosh, S.D. Roy, S. Kundu UAV assisted
    SWIPT enabled NOMA based D2D network for disaster management Wireless Personal
    Communications, 128 (2022), pp. 2341-2362 Google Scholar Giannopoulos et al.,
    2021 D. Giannopoulos, P. Papaioannou, C. Tranoris, S. Denazis Monitoring as a
    service over a 5G network slice, 2021 Joint European Conference on Networks and
    Communications & 6G Summit (EuCNC/6G Summit) (2021), pp. 329-334 Porto, Portugal
    CrossRefView in ScopusGoogle Scholar Giannopoulos et al., 2022 A. Giannopoulos,
    et al. Supporting intelligence in disaggregated open radio access networks: architectural
    principles, AI/ML workflow, and use cases IEEE Access, vol. 10 (2022), pp. 39580-39595
    CrossRefView in ScopusGoogle Scholar Gill et al., 2022 S.S. Gill, et al. AI for
    Next Generation Computing: Emerging Trends and Future Directions, vol. 19, Internet
    of Things (2022) Google Scholar Giust et al., 2017 F. Giust, X. Costa-Perez, A.
    Reznik Multi-access edge computing: an overview of ETSI MEC ISG, IEEE Future Networks.
    IEEE 5G Tech Focus, 1 (4) (2017) Google Scholar Go et al., 2019 S.J.Y. Go, R.
    Guinto, C.A.M. Feasting, I. Austria, R. Ocampo, W.M. Tan An SDN/NFV-enabled architecture
    for detecting personally identifiable information leaks on network traffic 2019
    Eleventh International Conference on Ubiquitous and Future Networks (ICUFN) (2019),
    pp. 306-311 Zagreb, Croatia CrossRefView in ScopusGoogle Scholar Goethals et al.,
    2022 T. Goethals, F. De Turck, B. Volckaert Extending Kubernetes clusters to low-resource
    edge devices using virtual Kubelets IEEE Transactions on Cloud Computing, vol.
    10 (2022), pp. 2623-2636 no. 4 CrossRefView in ScopusGoogle Scholar Gogoi et al.,
    2021 B. Gogoi, T. Ahmed, A. Dutta Defending against SQL injection attacks in web
    applications using machine learning and natural language processing 2021 IEEE
    18th India Council International Conference (INDICON) (2021), pp. 1-6 Guwahati,
    India CrossRefGoogle Scholar Goh et al., 2019 E.S. Goh, M.S. Sunar, A.W. Ismail
    3D object manipulation techniques in handheld mobile augmented reality interface:
    a review IEEE Access, vol. 7 (2019), pp. 40581-40601 CrossRefView in ScopusGoogle
    Scholar Gong et al., 2020 S. Gong, et al. Toward smart wireless communications
    via intelligent reflecting surfaces: a contemporary survey IEEE Communications
    Surveys & Tutorials, 22 (4) (2020), pp. 2283-2314 CrossRefView in ScopusGoogle
    Scholar Gonzalez et al., 2018 A.J. Gonzalez, G. Nencioni, A. Kamisiński, B.E.
    Helvik, P.E. Heegaard Dependability of the NFV orchestrator: state of the art
    and research challenges IEEE Communications Surveys & Tutorials, 20 (4) (2018),
    pp. 3307-3329 CrossRefView in ScopusGoogle Scholar Gopinath and Latha, 2021 B.
    Gopinath, B. Latha Restriction of forgery attacks using AntiForgery token in machine
    learning 2021 International Conference on Advancements in Electrical, Electronics,
    Communication, Computing and Automation (ICAECA) (2021), pp. 1-4 Coimbatore, India
    CrossRefGoogle Scholar GPP. 3GPP TS 23, 2017 3GPP. 3GPP TS 23.501 (2017) [Online].
    Available: https://www.3gpp.org/ftp/Specs/archive/23_series/23.501/ Accessed:
    February 2023 Google Scholar Gramaglia et al., 2022 M. Gramaglia, et al. Network
    intelligence for virtualized RAN orchestration: the DAEMON approach 2022 Joint
    European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit)
    (2022), pp. 482-487 Grenoble, France CrossRefView in ScopusGoogle Scholar Griffith
    et al., 2017 D. Griffith, A. Ben Mosbah, R. Rouil Group Discovery Time in Device-To-Device
    (D2D) Proximity Services (ProSe) Networks IEEE INFOCOM 2017 - IEEE Conference
    on Computer Communications, Atlanta, GA, USA (2017), pp. 1-9 CrossRefGoogle Scholar
    Groshev et al., 2021 M. Groshev, C. Guimarães, J. Martín-Pérez, A. de la Oliva
    Toward intelligent cyber-physical systems: digital twin meets artificial intelligence
    IEEE Communications Magazine, 59 (8) (2021), pp. 14-20 CrossRefView in ScopusGoogle
    Scholar Guan et al., 2021 W. Guan, H. Zhang, V.C.M. Leung Customized slicing for
    6G: enforcing artificial intelligence on resource management IEEE Network, 35
    (5) (2021), pp. 264-271 CrossRefView in ScopusGoogle Scholar Gul et al., 2019
    M.J. Gul, R. Rabia, Y. Jararweh, M.M. Rathore, A. Paul Security flaws of operating
    system against live device attacks: a case study on live Linux distribution device
    2019 Sixth International Conference on Software Defined Systems (SDS) (2019),
    pp. 154-159 Rome, Italy CrossRefView in ScopusGoogle Scholar Guo and Yu, 2022
    H. Guo, X. Yu A survey on blockchain technology and its security Blockchain: Research
    and Applications, 3 (2) (2022) Google Scholar Guo et al., 2011 T. Guo, C.H. Foh,
    J. Cai, D. Niyato, E.W.M. Wong Performance evaluation of IPTV over wireless home
    networks IEEE Transactions on Multimedia, 13 (5) (2011), pp. 1116-1126 View in
    ScopusGoogle Scholar Guo et al., 2018 F. Guo, H. Zhang, H. Ji, X. Li, V.C.M. Leung
    Energy efficient computation offloading for multi-access MEC enabled small cell
    networks 2018 IEEE International Conference on Communications Workshops (ICC Workshops)
    (2018), pp. 1-6 Kansas City, MO, USA View in ScopusGoogle Scholar Guo et al.,
    2019 F. Guo, H. Zhang, H. Ji, X. Li, V.C.M. Leung Joint trajectory and computation
    offloading optimization for UAV-assisted MEC with NOMA IEEE INFOCOM 2019 - IEEE
    Conference on Computer Communications Workshops (INFOCOM WKSHPS) (2019), pp. 1-6
    Paris, France View in ScopusGoogle Scholar Guo et al., 2021a P. Guo, B. Ye, Y.
    Chen, T. Li, Y. Yang, X. Qian A location data protection protocol based on differential
    privacy 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl
    Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data
    Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech),
    AB, Canada (2021), pp. 306-311 CrossRefView in ScopusGoogle Scholar Guo et al.,
    2021b F. Guo, F.R. Yu, H. Zhang, X. Li, H. Ji, V.C.M. Leung Enabling massive IoT
    toward 6G: a comprehensive survey IEEE Internet of Things Journal, vol. 8 (2021),
    pp. 11891-11915 no. 15 CrossRefView in ScopusGoogle Scholar Guo et al., 2022 J.
    Guo, et al. Survey on Digital Twins for Internet of Vehicles: Fundamentals, Challenges,
    and Opportunities,” Digital Communications and Networks (2022) Google Scholar
    Gupta et al., 2019 H. Gupta, et al. Impact of side channel attack in information
    security 2019 International Conference on Computational Intelligence and Knowledge
    Economy (ICCIKE), United Arab Emirates, Dubai (2019), pp. 291-295 CrossRefView
    in ScopusGoogle Scholar Gupta et al., 2020 H. Gupta, M. Sharma, A. Franklin A,
    B.R. Tamma Apt-RAN: a flexible split-based 5G RAN to minimize energy consumption
    and handovers IEEE Transactions on Network and Service Management, vol. 17 (2020),
    pp. 473-487 no. 1 CrossRefView in ScopusGoogle Scholar Gupta et al., 2023 S. Gupta,
    et al. Development of a novel footwear based power harvesting system Advances
    in Electrical Engineering, Electronics and Energy, 3 (2023) Google Scholar Gür
    et al., 2020 G. Gür, P. Porambage, M. Liyanage Convergence of ICN and MEC for
    5G: opportunities and challenges IEEE Communications Standards Magazine, 4 (4)
    (2020), pp. 64-71 CrossRefView in ScopusGoogle Scholar Gür et al., 2022 G. Gür,
    et al. Integration of ICN and MEC in 5G and beyond networks: mutual benefits,
    use cases, challenges, standardization, and future research IEEE Open Journal
    of the Communications Society, 3 (2022), pp. 1382-1412 CrossRefView in ScopusGoogle
    Scholar Gyamfi and Jurcut, 2022 E. Gyamfi, A. Jurcut M-TADS: a multi-trust DoS
    attack detection system for MEC-enabled industrial loT 2022 IEEE 27th International
    Workshop on Computer Aided Modeling and Design of Communication Links and Networks
    (CAMAD) (2022), pp. 166-172 Paris, France CrossRefView in ScopusGoogle Scholar
    Ha et al., 2022 D.B. Ha, V.T. Truong, V.N. Vo Performance analysis of intelligent
    reflecting surface-aided mobile edge computing network with uplink NOMA scheme
    N.S. Vo, Q.T. Vien, D.B. Ha (Eds.), Industrial Networks and Intelligent Systems.
    INISCOM 2022, Lecture Notes of the Institute for Computer Sciences, Social Informatics
    and Telecommunications Engineering, vol. 444, Springer, Cham (2022) Google Scholar
    Haavisto et al., 2019 J. Haavisto, M. Arif, L. Lovén, T. Leppänen, J. Riekki Open-source
    RANs in Practice: an Over-the-air Deployment for 5G MEC,\" 2019 European Conference
    on Networks and Communications EuCNC), Valencia, Spain (2019), pp. 495-500 CrossRefView
    in ScopusGoogle Scholar Habibi et al., 2020 P. Habibi, M. Farhoudi, S. Kazemian,
    S. Khorsandi, A. Leon-Garcia Fog computing: a comprehensive architectural survey
    IEEE Access, vol. 8 (2020), pp. 69105-69133 CrossRefView in ScopusGoogle Scholar
    Habibi et al., 2022 M.A. Habibi, F.Z. Yousaf, H.D. Schotten Mapping the VNFs and
    VLs of a RAN slice onto intelligent PoPs in beyond 5G mobile networks IEEE Open
    Journal of the Communications Society, 3 (2022), pp. 670-704 CrossRefView in ScopusGoogle
    Scholar Hachimi et al., 2020 M. Hachimi, G. Kaddoum, G. Gagnon, P. Illy Multi-stage
    jamming attacks detection using deep learning combined with Kernelized support
    vector machine in 5G cloud radio access networks 2020 International Symposium
    on Networks, Computers and Communications (ISNCC), Canada, Montreal, QC (2020),
    pp. 1-5 CrossRefGoogle Scholar Han et al., 2019 S. Han, et al. Energy efficient
    secure computation offloading in NOMA-based mMTC networks for IoT IEEE Internet
    of Things Journal, vol. 6 (2019), pp. 5674-5690 no. 3 CrossRefView in ScopusGoogle
    Scholar Hasabelnaby and Chaaban, 2021 M.A. Hasabelnaby, A. Chaaban End-to-End
    rate enhancement in C-RAN using multi-pair two-way computation 2021 Joint European
    Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit) (2021),
    pp. 49-54 Porto, Portugal CrossRefView in ScopusGoogle Scholar Hasabelnaby and
    Chaaban, 2022 M.A. Hasabelnaby, A. Chaaban Multi-pair computation for C-RAN with
    intra-cloud and inter-cloud communications IEEE Wireless Communications Letters,
    11 (12) (2022), pp. 2537-2541 CrossRefView in ScopusGoogle Scholar Hauer, 2015
    B. Hauer Data and information leakage prevention within the scope of information
    security IEEE Access, vol. 3 (2015), pp. 2554-2565 View in ScopusGoogle Scholar
    Haus et al., 2017 M. Haus, M. Waqas, A.Y. Ding, Y. Li, S. Tarkoma, J. Ott Security
    and privacy in device-to-device (D2D) communication: a review IEEE Communications
    Surveys & Tutorials, 19 (2) (2017), pp. 1054-1079 View in ScopusGoogle Scholar
    He et al., 2017 T. He, E.N. Ciftcioglu, S. Wang, K.S. Chan Location privacy in
    mobile edge clouds: a chaff-based approach IEEE Journal on Selected Areas in Communications,
    vol. 35 (2017), pp. 2625-2636 no. 11 View in ScopusGoogle Scholar He et al., 2018
    J. He, D. Zhang, Y. Zhou, X. Lan, Y. Zhang Towards a truthful online auction for
    cooperative mobile task execution 2018 IEEE SmartWorld, Ubiquitous Intelligence
    & Computing, Advanced & Trusted Computing, Scalable Computing & Communications,
    Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)
    (2018), pp. 546-553 Guangzhou, China CrossRefView in ScopusGoogle Scholar He et
    al., 2019a Y. He, J. Ren, G. Yu, Y. Cai D2D communications meet mobile edge computing
    for enhanced computation capacity in cellular networks IEEE Transactions on Wireless
    Communications, 18 (3) (2019), pp. 1750-1763 CrossRefView in ScopusGoogle Scholar
    He et al., 2019b Y. He, J. Ren, G. Yu, Y. Cai Joint computation offloading and
    resource allocation in D2D enabled MEC networks ICC 2019 - 2019 IEEE International
    Conference on Communications (ICC) (2019), pp. 1-6 Shanghai, China Google Scholar
    He et al., 2019c X. He, R. Jin, H. Dai Deep PDS-learning for privacy-aware offloading
    in MEC-enabled IoT IEEE Internet of Things Journal, vol. 6 (2019), pp. 4547-4555
    no. 3 CrossRefView in ScopusGoogle Scholar Ho and Nguyen, 2022 T.M. Ho, K.-K.
    Nguyen Joint server selection, cooperative offloading and handover in multi-access
    edge computing wireless network: a deep reinforcement learning approach IEEE Transactions
    on Mobile Computing, vol. 21 (2022), pp. 2421-2435 no. 7 View in ScopusGoogle
    Scholar Hoeschele et al., 2022 T. Hoeschele, F. Kaltenberger, A.I. Grohmann, E.
    Tasdemir, M. Reisslein, F.H.P. Fitzek 5G InterOPERAbility of open RAN components
    in large testbed ecosystem: towards 6G flexibility European Wireless 2022; 27th
    European Wireless Conference (2022), pp. 1-6 Dresden, Germany Google Scholar Hossain
    et al., 2021 M.D. Hossain, T. Sultana, M.A. Hossain, E.-N. Huh Edge orchestration
    based computation peer offloading in MEC-enabled networks: a fuzzy logic approach
    2021 15th International Conference on Ubiquitous Information Management and Communication
    (IMCOM), Seoul, Korea (South) (2021), pp. 1-7 Google Scholar Hou et al., 2020
    Y. Hou, S. Garg, L. Hui, D.N.K. Jayakody, R. Jin, M.S. Hossain A data security
    enhanced access control mechanism in mobile edge computing IEEE Access, vol. 8
    (2020), pp. 136119-136130 CrossRefView in ScopusGoogle Scholar Hou et al., 2022
    L. Hou, M.A. Gregory, S. Li A survey of multi-access edge computing and vehicular
    networking IEEE Access, 10 (2022), pp. 123436-123451 CrossRefView in ScopusGoogle
    Scholar Houda et al., 2022 Z.A.E. Houda, B. Brik, A. Ksentini, L. Khoukhi, M.
    Guizani When federated learning meets game theory: a cooperative framework to
    secure IIoT applications on edge computing IEEE Transactions on Industrial Informatics,
    18 (11) (2022), pp. 7988-7997 CrossRefView in ScopusGoogle Scholar Hu and Wang,
    2022 H.-C. Hu, P.-C. Wang Computation offloading game for multi-channel wireless
    sensor networks Sensors, 22 (22) (2022) Google Scholar Hu et al L. Hu, W. Wang,
    S. Zhong, H. Guo, J. Li and Y. Pan, \"APP-Aware MAC scheduling for MEC-backed
    TDM-PON mobile fronthaul,\" in IEEE Communications Letters. Google Scholar Hu
    et al., 2017 Z. Hu, C. Yuan, F. Gao Maximizing harvested energy for full-duplex
    SWIPT system with power splitting IEEE Access, vol. 5 (2017), pp. 24975-24987
    View in ScopusGoogle Scholar Hu et al., 2018a Q. Hu, C. Wu, X. Zhao, X. Chen,
    Y. Ji, T. Yoshinaga Vehicular multi-access edge computing with licensed sub-6
    GHz, IEEE 802.11p and mmWave IEEE Access, 6 (2018), pp. 1995-2004 CrossRefGoogle
    Scholar Hu et al., 2018b S. Hu, F. Rusek, O. Edfors Beyond massive MIMO: the potential
    of data transmission with large intelligent surfaces IEEE Transactions on Signal
    Processing, 66 (10) (2018), pp. 2746-2758 CrossRefView in ScopusGoogle Scholar
    Hu et al., 2021 H. Hu, Q. Wang, R.Q. Hu, H. Zhu Mobility-aware offloading and
    resource allocation in a MEC-enabled IoT network with energy harvesting IEEE Internet
    of Things Journal, 8 (24) (2021), pp. 17541-17556 CrossRefView in ScopusGoogle
    Scholar Hu et al., 2022 Z. Hu, et al. An efficient online computation offloading
    approach for large-scale mobile edge computing via deep reinforcement learning
    IEEE Transactions on Services Computing, vol. 15 (2022), pp. 669-683 no. 2 CrossRefView
    in ScopusGoogle Scholar Huang and Lai, 2020 C.-M. Huang, C.-F. Lai The delay-constrained
    and network-situation-aware V2V2I VANET data offloading based on the multi-access
    edge computing (MEC) architecture IEEE Open Journal of Vehicular Technology, vol.
    1 (2020), pp. 331-347 CrossRefGoogle Scholar Huang et al., 2017 A. Huang, N. Nikaein,
    T. Stenbock, A. Ksentini, C. Bonnet Low latency MEC framework for SDN-based LTE/LTE-A
    networks 2017 IEEE International Conference on Communications (ICC) (2017), pp.
    1-6 Paris, France Google Scholar Huang et al., 2020 M. Huang, W. Liang, X. Shen,
    Y. Ma, H. Kan Reliability-aware virtualized network function services provisioning
    in mobile edge computing IEEE Transactions on Mobile Computing, vol. 19 (2020),
    pp. 2699-2713 no. 11 CrossRefView in ScopusGoogle Scholar Huang et al., 2021 L.
    Huang, L. Zhang, S. Yang, L.P. Qian, Y. Wu Meta-learning based dynamic computation
    task offloading for mobile edge computing networks IEEE Communications Letters,
    25 (5) (2021), pp. 1568-1572 CrossRefView in ScopusGoogle Scholar Huang et al.,
    2022a P.-H. Huang, F.-C. Hsieh, W.-J. Hsieh, C.-Y. Li, Y.-D. Lin Prioritized traffic
    shaping for low-latency MEC flows in MEC-enabled cellular networks 2022 IEEE 19th
    Annual Consumer Communications & Networking Conference (CCNC) (2022), pp. 120-125
    Las Vegas, NV, USA CrossRefView in ScopusGoogle Scholar Huang et al., 2022b X.
    Huang, B. Zhang, C. Li Incentive Mechanisms for Mobile Edge Computing: Present
    and Future Directions IEEE Network, 36 (6) (November/December 2022), pp. 199-205
    CrossRefView in ScopusGoogle Scholar Huda and Moh, 2022 S.M.A. Huda, S. Moh Survey
    on computation offloading in UAV-Enabled mobile edge computing Journal of Network
    and Computer Applications, 201 (2022) Google Scholar Hui, 2016 S.Y.R. Hui Past,
    present and future trends of non-radiative wireless power transfer CPSS Transactions
    on Power Electronics and Applications, 1 (1) (2016), pp. 83-91 CrossRefGoogle
    Scholar Humayed et al., 2017 A. Humayed, J. Lin, F. Li, B. Luo Cyber-physical
    systems security—a survey IEEE Internet of Things Journal, vol. 4 (2017), pp.
    1802-1831 no. 6 View in ScopusGoogle Scholar I et al., 2014 C.-L. I, J. Huang,
    R. Duan, C. Cui, J. Jiang, L. Li Recent progress on C-RAN centralization and cloudification
    IEEE Access, vol. 2 (2014), pp. 1030-1039 Google Scholar I et al., 2020 C.-L.
    I, S. Kuklinskí, T. Chen A perspective of O-RAN integration with MEC, SON, and
    network slicing in the 5G era IEEE Network, 34 (6) (2020), pp. 3-4 CrossRefGoogle
    Scholar Idhom et al., 2020 M. Idhom, H.E. Wahanani, A. Fauzi Network security
    system on multiple servers against brute force attacks 2020 6th Information Technology
    International Seminar (ITIS), Surabaya, Indonesia (2020), pp. 258-262 CrossRefView
    in ScopusGoogle Scholar Iftikhar et al., 2023 S. Iftikhar, et al. AI-Based Fog
    and Edge Computing: A Systematic Review, Taxonomy and Future Directions, vol.
    21, Internet of Things (2023) Google Scholar Igarashi et al., 2021 K. Igarashi,
    H. Kato, I. Sasase Rogue access point detection by using ARP failure under the
    MAC address duplication 2021 IEEE 32nd Annual International Symposium on Personal,
    Indoor and Mobile Radio Communications (PIMRC) (2021), pp. 1469-1474 Helsinki,
    Finland CrossRefView in ScopusGoogle Scholar Islam et al., 2021 A. Islam, et al.
    A survey on task offloading in multi-access edge computing Journal of Systems
    Architecture, 118 (2021) Google Scholar ITU-T, 2020 ITU-T FG-NET2030 – Focus Group
    on Technologies for Network 2030 Network 2030 Architecture Framework (2020) [Online].
    Available: https://www.itu.int/en/ITU-T/focusgroups/net2030/Documents/Network_2030_Architecture-framework.pdf
    Accessed: February 2023 Google Scholar J. N. O.S and Bhanu, 2018 J. N. O.S, S.M.S.
    Bhanu A survey on code injection attacks in mobile cloud computing environment
    2018 8th International Conference on Cloud Computing, Data Science & Engineering
    (Confluence) (2018), pp. 1-6 Noida, India Google Scholar Jaafar et al., 2016 F.
    Jaafar, G. Nicolescu, C. Richard A systematic approach for privilege escalation
    prevention 2016 IEEE International Conference on Software Quality, Reliability
    and Security Companion (QRS-C) (2016), pp. 101-108 Vienna, Austria CrossRefView
    in ScopusGoogle Scholar Jaafar et al., 2020 W. Jaafar, S. Naser, S. Muhaidat,
    P.C. Sofotasios, H. Yanikomeroglu Multiple access in aerial networks: from orthogonal
    and non-orthogonal to rate-splitting IEEE Open Journal of Vehicular Technology,
    1 (2020), pp. 372-392 CrossRefView in ScopusGoogle Scholar Jain and Semwal, 2022
    R. Jain, V.B. Semwal A novel feature extraction method for preimpact fall detection
    system using deep learning and wearable sensors IEEE Sensors Journal, 22 (23)
    (2022), pp. 22943-22951 CrossRefView in ScopusGoogle Scholar Jalali et al., 2023
    J. Jalali, A. Khalili, A. Rezaei, J. Famaey, W. Saad Power-efficient antenna switching
    and beamforming design for multi-user SWIPT with non-linear energy harvesting
    2023 IEEE 20th Consumer Communications & Networking Conference (CCNC) (2023),
    pp. 746-751 Las Vegas, NV, USA CrossRefView in ScopusGoogle Scholar Jiang et al.,
    2014a J. Jiang, J.D. Li, X.Y. Liu Network selection policy based on auction theory
    in heterogeneous wireless communication systems Science China Information Sciences,
    58 (2014), pp. 1-10 Google Scholar Jiang et al., 2014b Y. Jiang, J. Huang, W.
    Jin The research progress of network intrusion tolerance International Conference
    on Cyberspace Technology (CCT 2014) (2014), pp. 1-4 Beijing Google Scholar Jiang
    et al., 2021a X. Jiang, F.R. Yu, T. Song, V.C.M. Leung A survey on multi-access
    edge computing applied to video streaming: some research issues and challenges
    IEEE Communications Surveys & Tutorials, 23 (2) (2021), pp. 871-903 CrossRefView
    in ScopusGoogle Scholar Jiang et al., 2021b K. Jiang, C. Sun, H. Zhou, X. Li,
    M. Dong, V.C.M. Leung Intelligence-empowered mobile edge computing: framework,
    issues, implementation, and outlook IEEE Network, 35 (5) (2021), pp. 74-82 CrossRefView
    in ScopusGoogle Scholar Jiang et al., 2021c P. Jiang, et al. Building in-the-cloud
    network functions: security and privacy challenges Proceedings of the IEEE, vol.
    109 (2021), pp. 1888-1919 12 CrossRefView in ScopusGoogle Scholar Jiang et al.,
    2022a J. Jiang, Y. Wang, P. Xin Blockchain technology enabled communication network
    for 5G MEC architecture of smart grids 2022 IEEE 8th International Conference
    on Computer and Communications (ICCC) (2022), pp. 253-257 Chengdu, China CrossRefView
    in ScopusGoogle Scholar Jiang et al., 2022b X. Jiang, F.R. Yu, T. Song, V.C.M.
    Leung Intelligent resource allocation for video analytics in blockchain-enabled
    internet of autonomous vehicles with edge computing IEEE Internet of Things Journal,
    vol. 9 (2022), pp. 14260-14272 no. 16 CrossRefView in ScopusGoogle Scholar Jiang
    et al., 2023 Z. Jiang, R. Cao, S. Zhang Joint optimization strategy of offloading
    in multi-UAVs-assisted edge computing networks Journal of Ambient Intelligence
    and Humanized Computing (2023) Google Scholar Jin et al., 2019a T. Jin, W. Zheng,
    X. Wen, X. Chen, L. Wang Optimization of computation resource for container-based
    multi-MEC collaboration system 2019 IEEE 30th Annual International Symposium on
    Personal, Indoor and Mobile Radio Communications (PIMRC) (2019), pp. 1-7 Istanbul,
    Turkey CrossRefView in ScopusGoogle Scholar Jin et al., 2019b X. Jin, Q. Wang,
    X. Li, X. Chen, W. Wang Cloud virtual machine lifecycle security framework based
    on trusted computing Tsinghua Science and Technology, 24 (5) (2019), pp. 520-534
    CrossRefGoogle Scholar Jin et al., 2022 H. Jin, M.A. Gregory, S. Li A review of
    intelligent computation offloading in multiaccess edge computing IEEE Access,
    10 (2022), pp. 71481-71495 CrossRefView in ScopusGoogle Scholar Kahvazadeh et
    al., 2022 S. Kahvazadeh, H. Khalili, R.N. Silab, B. Bakhshi, J. Mangues-Bafalluy
    Vertical-oriented 5G platform-as-a-service: user-generated content case study
    2022 IEEE Future Networks World Forum (FNWF) (2022), pp. 706-711 Montreal, QC,
    Canada CrossRefView in ScopusGoogle Scholar Kamboj and Ghumman, 2016 S. Kamboj,
    N.S. Ghumman A survey on cloud computing and its types 2016 3rd International
    Conference on Computing for Sustainable Global Development (INDIACom) (2016),
    pp. 2971-2974 New Delhi, India View in ScopusGoogle Scholar Kang et al., 2018
    J.-M. Kang, I.-M. Kim, D.I. Kim Wireless information and power transfer: rate-energy
    tradeoff for nonlinear energy harvesting IEEE Transactions on Wireless Communications,
    17 (3) (2018), pp. 1966-1981 CrossRefView in ScopusGoogle Scholar Kang et al.,
    2020 S. Kang, H. Lee, S. Hwang, I. Lee Time switching protocol for multi-antenna
    SWIPT systems 2020 IEEE Wireless Communications and Networking Conference (WCNC)
    (2020), pp. 1-6 Seoul, Korea (South) Google Scholar Kang et al., 2023 H. Kang,
    M. Li, S. Fan, W. Cai Combinatorial auction-enabled dependency-aware offloading
    strategy in mobile edge computing 2023 IEEE Wireless Communications and Networking
    Conference (WCNC) (2023), pp. 1-6 Glasgow, United Kingdom View in ScopusGoogle
    Scholar Kantarci and Mouftah, 2012 B. Kantarci, H.T. Mouftah Bandwidth distribution
    solutions for performance enhancement in long-reach passive optical networks IEEE
    Communications Surveys & Tutorials, vol. 14 (2012), pp. 714-733 no. 3 View in
    ScopusGoogle Scholar Kaur et al., 2018 T. Kaur, A. Girdhar, G. Gupta A robust
    algorithm for the detection of cloning forgery 2018 IEEE International Conference
    on Computational Intelligence and Computing Research (ICCIC) (2018), pp. 1-6 Madurai,
    India CrossRefGoogle Scholar Kaur et al., 2019 K. Kaur, S. Garg, G. Kaddoum, M.
    Guizani, D.N.K. Jayakody A lightweight and privacy-preserving authentication protocol
    for mobile edge computing 2019 IEEE Global Communications Conference (GLOBECOM)
    (2019), pp. 1-6 Waikoloa, HI, USA Google Scholar Kaur et al., 2021 J. Kaur, M.A.
    Khan, M. Iftikhar, M. Imran, Q. Emad Ul Haq Machine Learning Techniques for 5G
    and beyond IEEE Access, 9 (2021), pp. 23472-23488 CrossRefView in ScopusGoogle
    Scholar Ke et al., 2020 H. Ke, J. Wang, L. Deng, Y. Ge, H. Wang Deep reinforcement
    learning-based adaptive computation offloading for MEC in heterogeneous vehicular
    networks IEEE Transactions on Vehicular Technology, 69 (7) (2020), pp. 7916-7929
    CrossRefView in ScopusGoogle Scholar Keerthi et al., 2017 C.K. Keerthi, M.A. Jabbar,
    B. Seetharamulu Cyber physical systems(CPS):Security issues, challenges and solutions
    2017 IEEE International Conference on Computational Intelligence and Computing
    Research (ICCIC) (2017), pp. 1-4 Coimbatore, India CrossRefGoogle Scholar Khader
    and Lai, 2015 A.S. Khader, D. Lai Preventing man-in-the-middle attack in Diffie-Hellman
    key exchange protocol 2015 22nd International Conference on Telecommunications
    (ICT) (2015), pp. 204-208 Sydney, NSW, Australia View in ScopusGoogle Scholar
    Khadr et al., 2022 M.H. Khadr, H.B. Salameh, M. Ayyash, H. Elgala, S. Almajali
    Jamming resilient multi-channel transmission for cognitive radio IoT-based medical
    networks Journal of Communications and Networks, vol. 24 (2022), pp. 666-678 no.
    6 CrossRefView in ScopusGoogle Scholar Khalifa et al., 2014 A. Khalifa, M. Azab,
    M. Eltoweissy Towards a mobile ad-hoc cloud management platform 2014 IEEE/ACM
    7th International Conference on Utility and Cloud Computing (2014), pp. 427-434
    London, UK CrossRefView in ScopusGoogle Scholar Khalifeh et al., 2021 A. Khalifeh,
    et al. Radio frequency based wireless charging for unsupervised clustered WSN:
    system implementation and experimental evaluation Energies, 14 (7) (2021) Google
    Scholar Khan et al., 2021 B.U.I. Khan, F. Anwar, R.F. Olanrewaju, M.L.B.M. Kiah,
    R.N. Mir Game theory analysis and modeling of sophisticated multi-collusion attack
    in MANETs IEEE Access, vol. 9 (2021), pp. 61778-61792 CrossRefView in ScopusGoogle
    Scholar Khan et al., 2022a M.A. Khan, et al. A survey on mobile edge computing
    for video streaming: opportunities and challenges IEEE Access, 10 (2022), pp.
    120514-120550 CrossRefView in ScopusGoogle Scholar Khan et al., 2022b L.U. Khan,
    Z. Han, W. Saad, E. Hossain, M. Guizani, C.S. Hong Digital twin of wireless systems:
    overview, taxonomy, challenges, and opportunities IEEE Communications Surveys
    & Tutorials, 24 (4) (2022), pp. 2230-2254 CrossRefView in ScopusGoogle Scholar
    KhariSonam and Kumar, 2016 M. Khari, Vaishali Sonam, M. Kumar Comprehensive study
    of web application attacks and classification 2016 3rd International Conference
    on Computing for Sustainable Global Development (INDIACom) (2016), pp. 2159-2164
    New Delhi, India View in ScopusGoogle Scholar Khisa et al., 2022 S. Khisa, M.
    Almekhlafi, M. Elhattab, C. Assi Full duplex cooperative rate splitting multiple
    access for a MISO broadcast channel with two users IEEE Communications Letters,
    26 (8) (2022), pp. 1913-1917 CrossRefView in ScopusGoogle Scholar Khodashenas
    et al., 2017 P.S. Khodashenas, et al. Service mapping and orchestration over multi-tenant
    cloud-enabled RAN IEEE Transactions on Network and Service Management, 14 (4)
    (2017), pp. 904-919 View in ScopusGoogle Scholar Kim and An, 2014 D. Kim, S. An
    Efficient and scalable public key infrastructure for wireless sensor networks
    The 2014 International Symposium on Networks (2014), pp. 1-5 Computers and Communications,
    Hammamet, Tunisia Google Scholar Kim and Chang, 2019 T. Kim, J.M. Chang Profitable
    and energy-efficient resource optimization for heterogeneous cloud-based radio
    access networks IEEE Access, vol. 7 (2019), pp. 34719-34737 CrossRefView in ScopusGoogle
    Scholar Kim and Hur, 2022 H. Kim, J. Hur PCIe Side-Channel attack on I/O device
    via RDMA-enabled network card 2022 13th International Conference on Information
    and Communication Technology Convergence (ICTC), Jeju Island, Korea, Republic
    of (2022), pp. 1468-1470 CrossRefView in ScopusGoogle Scholar Kim et al., 2020
    Y. Kim, J.G. Park, J.-H. Lee Security threats in 5G edge computing environments
    2020 International Conference on Information and Communication Technology Convergence
    (ICTC) (2020), pp. 905-907 Jeju, Korea (South) CrossRefView in ScopusGoogle Scholar
    Kim et al., 2022 J.-D. Kim, M. Ko, J.-M. Chung Novel analytical models for Sybil
    attack detection in IPv6-based RPL wireless IoT networks 2022 IEEE International
    Conference on Consumer Electronics (ICCE), Las Vegas, NV (2022), pp. 1-3 USA Google
    Scholar Kiran et al., 2020 N. Kiran, X. Liu, S. Wang, C. Yin VNF placement and
    resource allocation in SDN/NFV-enabled MEC networks 2020 IEEE Wireless Communications
    and Networking Conference Workshops (WCNCW) (2020), pp. 1-6 Seoul, Korea (South)
    Google Scholar Kong et al., 2022a H. Kong, et al. Ergodic sum rate for uplink
    NOMA transmission in satellite-aerial-ground integrated networks Chinese Journal
    of Aeronautics, 35 (9) (2022), pp. 58-70 View PDFView articleView in ScopusGoogle
    Scholar Kong et al., 2022b H. Kong, M. Lin, Z. Wang, J.-Y. Wang, W.-P. Zhu, J.
    Wang Combined robust beamforming with uplink RSMA for multibeam satellite systems
    IEEE Transactions on Vehicular Technology, 71 (9) (2022), pp. 10167-10172 CrossRefView
    in ScopusGoogle Scholar Koteshwara, 2021 S. Koteshwara Security risk assessment
    of server hardware architectures using graph analysis 2021 Asian Hardware Oriented
    Security and Trust Symposium (AsianHOST) (2021), pp. 1-4 Shanghai, China CrossRefGoogle
    Scholar Krishna and Sharma, 2021 S.M.H. Krishna, R. Sharma Survey on application
    programming interfaces in software defined networks and network function virtualization
    Global Transitions Proceedings, 2 (2) (2021), pp. 199-204 CrossRefView in ScopusGoogle
    Scholar Ku et al., 2016 Y.-J. Ku, D.-Y. Lin, H.-Y. Wei Fog RAN over general purpose
    processor platform 2016 IEEE 84th Vehicular Technology Conference (VTC-Fall) (2016),
    pp. 1-2 Montreal, QC, Canada Google Scholar Kukliński et al., 2021 S. Kukliński,
    L. Tomaszewski, R. Kołakowski, P. Chemouil 6G-LEGO: a framework for 6G network
    slices Journal of Communications and Networks, vol. 23 (2021), pp. 442-453 no.
    6 CrossRefView in ScopusGoogle Scholar Kumar et al., 2020a S. Kumar, S.V. Doddala,
    A.A. Franklin, J. Jin RAN-aware adaptive video caching in multi-access edge computing
    networks Journal of Network and Computer Applications, 168 (2020) Google Scholar
    Kumar et al., 2020b H. Kumar, V. Sapru, S.K. Jaisawal O-RAN based proactive ANR
    optimization 2020 IEEE Globecom Workshops (GC Wkshps (2020), pp. 1-4 Taipei, Taiwan
    CrossRefGoogle Scholar Kumar et al., 2021 D. Kumar, G. Baranwal, D.P. Vidyarthi
    A survey on auction based approaches for resource allocation and pricing in emerging
    edge technologies Journal of Grid Computing, 20 (2021) Google Scholar Kumar et
    al., 2022 A. Kumar, F.Y. Li, J. Martinez-Bauset Revealing the benefits of rate-splitting
    multiple access for uplink IoT traffic GLOBECOM 2022 - 2022 IEEE Global Communications
    Conference (2022), pp. 111-116 Rio de Janeiro, Brazil CrossRefView in ScopusGoogle
    Scholar Kuo et al., 2018 P.-H. Kuo, et al. An integrated edge and fog system for
    future communication networks 2018 IEEE Wireless Communications and Networking
    Conference Workshops (WCNCW) (2018), pp. 338-343 Barcelona, Spain View in ScopusGoogle
    Scholar Lal et al., 2017 S. Lal, T. Taleb, A. Dutta NFV: security threats and
    best practices IEEE Communications Magazine, vol. 55 (2017), pp. 211-217 no. 8
    View in ScopusGoogle Scholar Lee et al., 2021 H. Lee, Y. Jang, J. Song, H. Yeon
    O-RAN AI/ML workflow implementation of personalized network optimization via reinforcement
    learning 2021 IEEE Globecom Workshops (GC Wkshps) (2021), pp. 1-6 Madrid, Spain
    Google Scholar Lewis et al., 2014 G. Lewis, S. Echeverría, S. Simanta, B. Bradshaw,
    J. Root Tactical cloudlets: moving cloud computing to the edge 2014 IEEE Military
    Communications Conference (2014), pp. 1440-1446 Baltimore, MD, USA CrossRefView
    in ScopusGoogle Scholar Li, 2017 M. Li Queueing analysis of unicast IPTV with
    adaptive modulation and coding in wireless cellular networks IEEE Transactions
    on Vehicular Technology, 66 (10) (2017), pp. 9241-9253 View in ScopusGoogle Scholar
    Li and Wan, 2018 W. Li, X. Wan An analysis and comparison for public cloud technology
    and market development trend in China 2018 5th IEEE International Conference on
    Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference
    on Edge Computing and Scalable Cloud (EdgeCom) (2018), pp. 200-205 Shanghai, China
    CrossRefView in ScopusGoogle Scholar Li and Wang, 2018 H. Li, L. Wang Online orchestration
    of cooperative defense against DDoS attacks for 5G MEC 2018 IEEE Wireless Communications
    and Networking Conference (WCNC) (2018), pp. 1-6 Barcelona, Spain View PDFView
    articleGoogle Scholar Li and Wu, 2023 B. Li, R. Wu Joint perception data caching
    and computation offloading in MEC-enabled vehicular networks Computer Communications,
    199 (2023), pp. 139-152 View PDFView articleGoogle Scholar Li et al., 2015 X.
    Li, H.-N. Dai, Y. Wang, H. Wang Eavesdropping activities in wireless networks:
    impact of channel randomness TENCON 2015 - 2015 IEEE Region 10 Conference (2015),
    pp. 1-4 Macao Google Scholar Li et al., 2017 G. Li, et al. Fuzzy theory based
    security service chaining for sustainable mobile-edge computing Mobile Information
    Systems, 2017 (2017) Google Scholar Li et al., 2018a J. Li, H. Gao, T. Lv, Y.
    Lu Deep reinforcement learning based computation offloading and resource allocation
    for MEC 2018 IEEE Wireless Communications and Networking Conference (WCNC) (2018),
    pp. 1-6 Barcelona, Spain View PDFView articleGoogle Scholar Li et al., 2018b N.
    Li, J.-F. Martinez-Ortega, V.H. Diaz Distributed power control for interference-aware
    multi-user mobile edge computing: a game theory approach IEEE Access, 6 (2018),
    pp. 36105-36114 CrossRefView in ScopusGoogle Scholar Li et al., 2018c Y. Li, R.
    Hou, K.-S. Lui, H. Li An MEC-based DoS attack detection mechanism for C-V2X networks
    2018 IEEE Global Communications Conference (GLOBECOM), United Arab Emirates, Abu
    Dhabi (2018), pp. 1-6 View PDFView articleGoogle Scholar Li et al., 2019a T. Li,
    L. Zhao, R. Duan, H. Tian SBA-based mobile edge computing 2019 IEEE Globecom Workshops
    (GC Wkshps) (2019), pp. 1-6 Waikoloa, HI, USA Google Scholar Li et al., 2019b
    B. Li, Z. Fei, Y. Zhang UAV communications for 5G and beyond: recent advances
    and future trends IEEE Internet of Things Journal, 6 (2) (2019), pp. 2241-2263
    CrossRefView in ScopusGoogle Scholar Li et al., 2019c M. Li, F.R. Yu, P. Si, Y.
    Zhang Energy-efficient machine-to-machine (M2M) communications in virtualized
    cellular networks with mobile edge computing (MEC) IEEE Transactions on Mobile
    Computing, vol. 18 (2019), pp. 1541-1555 no. 7 CrossRefView in ScopusGoogle Scholar
    Li et al., 2019d M. Li, L. Yang, F.R. Yu, W. Wu, Z. Wang, Y. Zhang Joint optimization
    of networking and computing resources for green M2M communications based on DRL
    2019 IEEE Global Communications Conference (GLOBECOM) (2019), pp. 1-6 Waikoloa,
    HI, USA View PDFView articleGoogle Scholar Li et al., 2019e H. Li, S. Ci, C. Yang,
    X. Tan, S. Hou, Z. Wang Moving to green edges: a cooperative MEC framework to
    reduce energy demand of clouds 2019 IEEE Globecom Workshops (GC Wkshps) (2019),
    pp. 1-6 Waikoloa, HI, USA View PDFView articleGoogle Scholar Li et al., 2020a
    L. Li, K. Chai, J. Li, X. Li Resource allocation for multicarrier rate-splitting
    multiple access system IEEE Access, vol. 8 (2020), pp. 174222-174232 CrossRefView
    in ScopusGoogle Scholar Li et al., 2020b Y. Li, X. Li, X. Dai A simultaneous wireless
    power and data transmission method for multi-output WPT systems: analysis, design,
    and experimental verification IEEE Access, 8 (2020), pp. 206353-206359 CrossRefView
    in ScopusGoogle Scholar Li et al., 2020c N. Li, J. Yan, Z. Zhang, J.F. Martinez,
    X. Yuan Game theory based joint task offloading and resource allocation algorithm
    for mobile edge computing 2020 16th International Conference on Mobility, Sensing
    and Networking (MSN) (2020), pp. 791-796 Tokyo, Japan CrossRefView in ScopusGoogle
    Scholar Li et al., 2021a J. Li, W. Liang, Y. Ma Robust service provisioning with
    service function chain requirements in mobile edge computing IEEE Transactions
    on Network and Service Management, 18 (2) (2021), pp. 2138-2153 CrossRefView in
    ScopusGoogle Scholar Li et al., 2021b M. Li, X. Zhou, T. Qiu, Q. Zhao, K. Li Multi-relay
    assisted computation offloading for multi-access edge computing systems with energy
    harvesting IEEE Transactions on Vehicular Technology, 70 (10) (2021), pp. 10941-10956
    CrossRefView in ScopusGoogle Scholar Li et al., 2021c S. Li, X. Hu, Y. Du Deep
    reinforcement learning and game theory for computation offloading in dynamic edge
    computing markets IEEE Access, vol. 9 (2021), pp. 121456-121466 CrossRefView in
    ScopusGoogle Scholar Li et al., 2021d Y. Li, Y. Yu, W. Susilo, Z. Hong, M. Guizani
    Security and privacy for edge intelligence in 5G and beyond networks: challenges
    and solutions IEEE Wireless Communications, vol. 28 (2021), pp. 63-69 no. 2 Google
    Scholar Li et al., 2022a T. Li, X. He, S. Jiang, J. Liu A survey of privacy-preserving
    offloading methods in mobile-edge computing Journal of Network and Computer Applications,
    203 (2022) Google Scholar Li et al., 2022b W. Li, Q. Fan, W. Cui, F. Dang, X.
    Zhang, C. Dai Dynamic virtual machine consolidation algorithm based on balancing
    energy consumption and quality of service IEEE Access, 10 (2022), pp. 80958-80975
    CrossRefView in ScopusGoogle Scholar Li et al., 2022c Y. Li, et al. Collaborative
    Content Caching and Task Offloading in Multi-Access Edge Computing IEEE Transactions
    on Vehicular Technology (2022) Google Scholar Li et al., 2022d J. Li, et al. Budget-aware
    user satisfaction maximization on service provisioning in mobile edge computing
    IEEE Transactions on Mobile Computing (2022) Google Scholar Li et al., 2023a Q.
    Li, L. Shi, Z. Zhang, G. Zheng Resource allocation in UAV-enabled wireless-powered
    MEC networks with hybrid passive and active communications IEEE Internet of Things
    Journal, 10 (3) (2023), pp. 2574-2588 CrossRefView in ScopusGoogle Scholar Li
    et al., 2023b N. Li, et al. Transforming the 5G RAN with innovation: the confluence
    of cloud native and intelligence IEEE Access, vol. 11 (2023), pp. 4443-4454 CrossRefView
    in ScopusGoogle Scholar Li et al., 2120 C. Li, Y. Zhang and Y. Luo, “Quantum ant
    colony-based mobile-aware service deployment in SDN-based multi- access edge computing
    environments,” Research Square preprint. https://doi.org/10.21203/rs.3.rs-821834/v1.
    Google Scholar Liang et al., 2017a L. Liang, G.Y. Li, W. Xu Resource allocation
    for D2D-enabled vehicular communications IEEE Transactions on Communications,
    65 (7) (2017), pp. 3186-3197 View in ScopusGoogle Scholar Liang et al., 2017b
    W. Liang, S.X. Ng, L. Hanzo Cooperative overlay spectrum access in cognitive radio
    networks IEEE Communications Surveys & Tutorials, 19 (3) (2017), pp. 1924-1944
    View in ScopusGoogle Scholar Liang et al., 2019 Y. Liang, Y. He, J. Qiao, A.P.
    Hu Simultaneous wireless information and power transfer in 5G mobile networks:
    a survey, 2019 Computing, Communications and IoT Applications (ComComAp) (2019),
    pp. 460-465 Shenzhen, China CrossRefView in ScopusGoogle Scholar Liang et al.,
    2022 B. Liang, M.A. Gregory, S. Li Multi-access Edge Computing fundamentals, services,
    enablers and challenges: a complete survey Journal of Network and Computer Applications,
    199 (2022) Google Scholar Liao et al., 2018 C.-H. Liao, H.-H. Shuai, L.-C. Wang
    Eavesdropping prevention for heterogeneous Internet of Things systems 2018 15th
    IEEE Annual Consumer Communications & Networking Conference (CCNC) (2018), pp.
    1-2 Las Vegas, NV, USA CrossRefView in ScopusGoogle Scholar Light, 2016 Reading
    Light ETSI Drops ''Mobile'' from MEC (2016) [Online]. Available: https://www.lightreading.com/mobile/mec-(mobile-edge-computing)/etsi-drops-mobile-from-mec/d/d-id/726273
    Accessed: February 2023 Google Scholar Lim et al., 2023 D.-W. Lim, C.-J. Chun,
    J.-M. Kang Transmit power adaptation for D2D communications underlaying SWIPT-based
    IoT cellular networks IEEE Internet of Things Journal, 10 (2) (2023), pp. 987-1000
    CrossRefView in ScopusGoogle Scholar Lin et al., 2020 Lin, et al. A survey on
    computation offloading modeling for edge computing Journal of Network and Computer
    Applications, 169 (2020) Google Scholar Lin et al., 2022a D.-J. Lin, M.-Y. Chen,
    H.-S. Chiang, P.K. Sharma Intelligent traffic accident prediction model for internet
    of vehicles with deep learning approach IEEE Transactions on Intelligent Transportation
    Systems, 23 (3) (2022), pp. 2340-2349 CrossRefView in ScopusGoogle Scholar Lin
    et al., 2022b Y. Lin, Y. Huang, S.-Y. Hsieh, L. Lin, F. Xia Layered malicious
    nodes detection with graph attention network in human-cyber-physical networks
    2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems (MASS)
    (2022), pp. 523-529 Denver, CO, USA CrossRefView in ScopusGoogle Scholar Lingayat
    et al., 2018 A. Lingayat, R.R. Badre, A. Kumar Gupta Performance evaluation for
    deploying docker Containers on baremetal and virtual machine 2018 3rd International
    Conference on Communication and Electronics Systems (ICCES) (2018), pp. 1019-1023
    Coimbatore, India CrossRefView in ScopusGoogle Scholar Liu, 2022 X. Liu Enabling
    optical network technologies for 5G and beyond Journal of Lightwave Technology,
    vol. 40 (2022), pp. 358-367 no. 2 CrossRefView in ScopusGoogle Scholar Liu et
    al., 2016 W. Liu, X. Zhou, S. Durrani, P. Popovski SWIPT with practical modulation
    and RF energy harvesting sensitivity 2016 IEEE International Conference on Communications
    (ICC) (2016), pp. 1-7 Kuala Lumpur, Malaysia View PDFView articleGoogle Scholar
    Liu et al., 2017 Y. Liu, Y. Peng, B. Wang, S. Yao, Z. Liu Review on cyber-physical
    systems IEEE/CAA Journal of Automatica Sinica, vol. 4 (2017), pp. 27-40 no. 1
    View in ScopusGoogle Scholar Liu et al., 2018a J. Liu, G. Shou, Y. Liu, Y. Hu,
    Z. Guo Performance evaluation of integrated multi-access edge computing and fiber-wireless
    access networks IEEE Access, 6 (2018), pp. 30269-30279 CrossRefView in ScopusGoogle
    Scholar Liu et al., 2018b M. Liu, Y. Teng, K. Cheng, Y. Man, B. Zhang Joint content
    caching and delivery policy for heterogeneous cellular networks 2018 IEEE 29th
    Annual International Symposium on Personal, Indoor and Mobile Radio Communications
    (PIMRC) (2018), pp. 1-5 Bologna, Italy Google Scholar Liu et al., 2019a Z. Liu,
    Y. Xie, K.Y. Chan, K. Ma, X. Guan Chance-constrained optimization in D2D-based
    vehicular communication network IEEE Transactions on Vehicular Technology, 68
    (5) (2019), pp. 5045-5058 CrossRefView in ScopusGoogle Scholar Liu et al., 2019b
    Y. Liu, B. Zhao, P. Zhao, P. Fan, H. Liu A Survey: Typical Security Issues of
    Software-Defined Networking China Communications, 16 (7) (July 2019), pp. 13-31
    CrossRefGoogle Scholar Liu et al., 2020a H. Liu, T.A. Tsiftsis, K.J. Kim, K.S.
    Kwak, H.V. Poor Rate splitting for uplink NOMA with enhanced fairness and outage
    performance IEEE Transactions on Wireless Communications, 19 (7) (2020), pp. 4657-4670
    CrossRefView in ScopusGoogle Scholar Liu et al., 2020b Y. Liu, M. Peng, G. Shou,
    Y. Chen, S. Chen Toward edge intelligence: multiaccess edge computing for 5G and
    internet of things IEEE Internet of Things Journal, 7 (8) (2020), pp. 6722-6747
    CrossRefView in ScopusGoogle Scholar Liu et al., 2020c B. Liu, J. Song, J. Wang,
    H. Sun, Q. Wang Robust secure wireless powered MISO cognitive mobile edge computing
    IEEE Access, vol. 8 (2020), pp. 62356-62366 CrossRefView in ScopusGoogle Scholar
    Liu et al., 2021 L. Liu, et al. Blockchain-enabled secure data sharing scheme
    in mobile-edge computing: an asynchronous advantage actor–critic learning approach
    IEEE Internet of Things Journal, vol. 8 (2021), pp. 2342-2353 no. 4 CrossRefView
    in ScopusGoogle Scholar Liu et al., 2022a H. Liu, Z. Bai, H. Lei, G. Pan, K.J.
    Kim, T.A. Tsiftsis A new rate splitting strategy for uplink CR-NOMA systems IEEE
    Transactions on Vehicular Technology, 71 (7) (2022), pp. 7947-7951 CrossRefView
    in ScopusGoogle Scholar Liu et al., 2022b H. Liu, Y. Ye, Z. Bai, K.J. Kim, T.A.
    Tsiftsis Rate splitting multiple access aided mobile edge computing in cognitive
    radio networks 2022 IEEE International Conference on Communications Workshops
    (ICC Workshops), Seoul, Korea, Republic of (2022), pp. 598-603 CrossRefView in
    ScopusGoogle Scholar Liu et al., 2022c M. Liu, et al. Biomechanical energy harvesting
    for wearable and mobile devices: state-of-the-art and future directions Applied
    Energy, 321 (2022) Google Scholar Liu et al., 2022d T. Liu, L. Tang, W. Wang,
    Q. Chen, X. Zeng Digital-twin-assisted task offloading based on edge collaboration
    in the digital twin edge network IEEE Internet of Things Journal, 9 (2) (2022),
    pp. 1427-1444 CrossRefView in ScopusGoogle Scholar Liu et al., 2022e H. Liu, J.
    Zong, Q. Wang, Y. Liu, F. Yang Cloud native based intelligent RAN architecture
    towards 6G programmable networking 2022 7th International Conference on Computer
    and Communication Systems (ICCCS) (2022), pp. 623-627 Wuhan, China CrossRefView
    in ScopusGoogle Scholar Liu et al., 2022f F. Liu, J. Chen, Q. Zhang, B. Li Online
    MEC offloading for V2V networks IEEE Transactions on Mobile Computing (2022) Google
    Scholar Liu et al., 2022g S. Liu, C. Zheng, Y. Huang, T.Q.S. Quek Distributed
    reinforcement learning for privacy-preserving dynamic edge caching IEEE Journal
    on Selected Areas in Communications, vol. 40 (2022), pp. 749-760 no. 3 CrossRefView
    in ScopusGoogle Scholar Liu et al., 2022h J. Liu, et al. A data-driven parallel
    adaptive large neighborhood search algorithm for a large-scale inter-satellite
    link scheduling problem Swarm and Evolutionary Computation, 74 (2022) Google Scholar
    Liumei et al., 2016 Z. Liumei, W. Yichuan, Z. Lei, J. Wenjiang Towards energy
    efficient cloud: an optimized ant colony model for virtual machine placement Journal
    of Communications and Information Networks, 1 (4) (2016), pp. 116-132 Google Scholar
    Liyanage et al., 2021 M. Liyanage, P. Porambage, A.Y. Ding, A. Kalla Driving forces
    for multi-access edge computing (MEC) IoT integration in 5G ICT Express, 7 (2)
    (2021), pp. 127-137 View PDFView articleView in ScopusGoogle Scholar Liyanage
    et al., 2023 M. Liyanage, A. Braeken, S. Shahabuddin, P. Ranaweera Open RAN Security:
    Challenges and Opportunities Journal of Network and Computer Applications (2023)
    Google Scholar Long et al., 2021 W. Long, R. Chen, M. Moretti, W. Zhang, J. Li
    A promising technology for 6G wireless networks: intelligent reflecting surface
    Journal of Communications and Information Networks, 6 (1) (2021), pp. 1-16 CrossRefView
    in ScopusGoogle Scholar Lopez et al., 2022 M.A. Lopez, G.N.N. Barbosa, D.M.F.
    Mattos New barriers on 6G networking: an exploratory study on the security, privacy
    and opportunities for aerial networks 2022 1st International Conference on 6G
    Networking (6GNet) (2022), pp. 1-6 Paris, France Google Scholar Lu et al., 2016
    Z. Lu, W. Wang, C. Wang On the evolution and impact of mobile botnets in wireless
    networks IEEE Transactions on Mobile Computing, 15 (9) (2016), pp. 2304-2316 View
    in ScopusGoogle Scholar Lu et al., 2018 X. Lu, X. Wan, L. Xiao, Y. Tang, W. Zhuang
    Learning-based rogue edge detection in VANETs with ambient radio signals 2018
    IEEE International Conference on Communications (ICC) (2018), pp. 1-6 Kansas City,
    MO, USA View in ScopusGoogle Scholar Lu et al., 2019 J. Lu, L. Xiao, Z. Tian,
    M. Zhao, W. Wang 5G enhanced service-based core design 2019 28th Wireless and
    Optical Communications Conference (WOCC) (2019), pp. 1-5 Beijing, China View in
    ScopusGoogle Scholar Lu et al., 2021a Y. Lu, X. Huang, K. Zhang, S. Maharjan,
    Y. Zhang Communication-efficient federated learning for digital twin edge networks
    in industrial IoT IEEE Transactions on Industrial Informatics, 17 (8) (2021),
    pp. 5709-5718 CrossRefView in ScopusGoogle Scholar Lu et al., 2021b Y. Lu, X.
    Huang, K. Zhang, S. Maharjan, Y. Zhang Communication-efficient federated learning
    and permissioned blockchain for digital twin edge networks IEEE Internet of Things
    Journal, 8 (4) (2021), pp. 2276-2288 CrossRefView in ScopusGoogle Scholar Lu et
    al., 2021c Y. Lu, S. Maharjan, Y. Zhang Adaptive Edge Association for Wireless
    Digital Twin Networks in 6G IEEE Internet Things J., 8 (22) (November 2021), pp.
    16219-16230 CrossRefView in ScopusGoogle Scholar Lu et al., 2022a J. Lu, et al.
    Performance analysis for IRS-assisted MEC networks with unit selection Physical
    Communication, 55 (2022) Google Scholar Lu et al., 2022b W. Lu, et al. Secure
    NOMA-based UAV-MEC network towards a flying eavesdropper IEEE Transactions on
    Communications, vol. 70 (2022), pp. 3364-3376 no. 5 CrossRefView in ScopusGoogle
    Scholar Luo et al., 2020 J.-L. Luo, S.-Z. Yu, S.-J. Peng SDN/NFV-Based security
    service function tree for cloud IEEE Access, vol. 8 (2020), pp. 38538-38545 CrossRefView
    in ScopusGoogle Scholar Lv and Xiu, 2020 Z. Lv, W. Xiu Interaction of edge-cloud
    computing based on SDN and NFV for next generation IoT IEEE Internet of Things
    Journal, 7 (7) (2020), pp. 5706-5712 CrossRefView in ScopusGoogle Scholar Lv et
    al., 2010 Chengtong Lv, Qing Li, Lei Zhou, Junjie Peng, Wu Zhang, Tingting Wang
    PaaS: a revolution for information technology platforms 2010 International Conference
    on Educational and Network Technology (2010), pp. 346-349 Qinhuangdao View in
    ScopusGoogle Scholar Lv et al., 2018 H. Lv, D. Chen, Y. Wang Deployment of edge-computing
    in 5G NFV environment and future service-based architecture 2018 IEEE 4th International
    Conference on Computer and Communications (ICCC) (2018), pp. 811-816 Chengdu,
    China CrossRefView in ScopusGoogle Scholar M. V. K et al., 2022 M.V.K.I. Johnraja
    Jebadurai, G.J. Leelipushpam Paulraj, J. Jebadurai Mitigating sinkhole attack
    in RPL based internet of things environment using optimized K means clustering
    technique 2022 6th International Conference on Electronics, Communication and
    Aerospace Technology (2022), pp. 502-507 Coimbatore, India Google Scholar Ma and
    Mashayekhy, 2021 W. Ma, L. Mashayekhy Poster: adaptive video offloading in mobile
    edge computing 2021 IEEE 41st International Conference on Distributed Computing
    Systems (ICDCS) (2021), pp. 1130-1131 DC, USA CrossRefView in ScopusGoogle Scholar
    Ma and Sun, 2022 Z. Ma, S. Sun Research on vehicle-to-road collaboration and end-to-end
    collaboration for multimedia services in the internet of vehicles IEEE Access,
    vol. 10 (2022), pp. 18146-18155 CrossRefView in ScopusGoogle Scholar Ma et al.,
    2020 B. Ma, Z. Ye, X. Zhang, J. Chen, Y. Zhou, Q. Xia Security of Edge Computing
    based on Trusted Computing 2020 6th International Symposium on System and Software
    Reliability (ISSSR), Chengdu, China (2020), pp. 132-137 CrossRefView in ScopusGoogle
    Scholar Ma et al., 2022 C. Ma, et al. 5G enabling streaming media architecture
    with edge intelligence gateway in smart grids 2022 IEEE 5th International Conference
    on Computer and Communication Engineering Technology (CCET) (2022), pp. 233-237
    Beijing, China CrossRefView in ScopusGoogle Scholar Magurawalage et al., 2014
    C.M.S. Magurawalage, et al. Energy-efficient and network-aware offloading algorithm
    for mobile cloud computing Computer Networks, 74 (2014), pp. 22-33 Part B Google
    Scholar Mahesh et al., 2021 A. Mahesh, B. Chokkalingam, L. Mihet-Popa Inductive
    wireless power transfer charging for electric vehicles–A review IEEE Access, 9
    (2021), pp. 137667-137713 CrossRefView in ScopusGoogle Scholar Mahmoodi et al.,
    2023 H.B. Mahmoodi, J. Kaleva, S.P. Shariatpanahi, A. Tölli D2D assisted multi-antenna
    coded caching IEEE Access, 11 (2023), pp. 16271-16287 CrossRefView in ScopusGoogle
    Scholar Mahmud et al., 2017 M.S. Mahmud, H. Wang, A.M. Esfar-E-Alam, H. Fang A
    wireless health monitoring system using mobile phone accessories IEEE Internet
    of Things Journal, 4 (6) (2017), pp. 2009-2018 View in ScopusGoogle Scholar Makris
    et al., 2019 N. Makris, V. Passas, C. Nanis, T. Korakis On minimizing service
    access latency: employing MEC on the fronthaul of heterogeneous 5G architectures
    2019 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN)
    (2019), pp. 1-6 Paris, France CrossRefGoogle Scholar Maksimović, 2018 M. Maksimović
    The role of osmotic computing in internet of things 2018 17th International Symposium
    INFOTEH-JAHORINA (INFOTEH), East Sarajevo, Bosnia and Herzegovina (2018), pp.
    1-4 CrossRefView in ScopusGoogle Scholar Maniatakos, 2013 M. Maniatakos Privilege
    escalation attack through address space identifier corruption in untrusted modern
    processors 2013 8th International Conference on Design & Technology of Integrated
    Systems in Nanoscale Era (DTIS), United Arab Emirates, Abu Dhabi (2013), pp. 161-166
    CrossRefView in ScopusGoogle Scholar Mao et al., 2018 Y. Mao, B. Clerckx, V.O.K.
    Li Rate-splitting multiple access for downlink communication systems: bridging,
    generalizing, and outperforming SDMA and NOMA EURASIP Journal on Wireless Communications
    and Networking, 133 (2018) Google Scholar Masdari et al., 2120 M. Masdari, K.
    Majidzadeh, E. Doustsadigh, A. Babazadeh and R. Asemi, “Energy-Aware Computation
    Offloading in Mobile Edge Computing Using Quantum-Based Arithmetic Optimization
    Algorithm,” Research Square preprint. https://doi.org/10.21203/rs.3.rs-2221212/v1.
    Google Scholar Masoumi et al., 2022 M. Masoumi, et al. Dynamic online VNF placement
    with different protection schemes in a MEC environment 2022 32nd International
    Telecommunication Networks and Applications Conference (ITNAC) (2022), pp. 1-6
    Wellington, New Zealand CrossRefGoogle Scholar Massari et al., 2021 S. Massari,
    N. Mirizzi, G. Piro, G. Boggia An open-source tool modeling the ETSI-MEC architecture
    in the industry 4.0 context 2021 29th Mediterranean Conference on Control and
    Automation (MED) (2021), pp. 226-231 PUGLIA, Italy CrossRefView in ScopusGoogle
    Scholar Matoussi et al., 2020 S. Matoussi, I. Fajjari, S. Costanzo, N. Aitsaadi,
    R. Langar 5G RAN: functional split orchestration optimization IEEE Journal on
    Selected Areas in Communications, vol. 38 (2020), pp. 1448-1463 no. 7 CrossRefView
    in ScopusGoogle Scholar Mehrabi et al., 2019 M. Mehrabi, D. You, V. Latzko, H.
    Salah, M. Reisslein, F.H.P. Fitzek Device-enhanced MEC: multi-access edge computing
    (MEC) aided by end device computation and caching: a survey IEEE Access, 7 (2019),
    pp. 166079-166108 CrossRefView in ScopusGoogle Scholar Mehrabi et al., 2021 M.
    Mehrabi, et al. Mobility- and energy-aware cooperative edge offloading for dependent
    computation tasks Network, 1 (2) (2021) Google Scholar Mei et al., 2019 J. Mei,
    X. Wang, K. Zheng Intelligent network slicing for V2X services toward 5G IEEE
    Network, 33 (6) (2019), pp. 196-204 CrossRefView in ScopusGoogle Scholar Mei et
    al., 2020 J. Mei, X. Wang, K. Zheng An intelligent self-sustained RAN slicing
    framework for diverse service provisioning in 5G-beyond and 6G networks Intelligent
    and Converged Networks, 1 (3) (2020), pp. 281-294 CrossRefView in ScopusGoogle
    Scholar Meng et al., 2021 Y. Meng, Z. Zhang, Y. Huang Cache- and energy harvesting-enabled
    D2D cellular network: modeling, analysis and optimization IEEE Transactions on
    Green Communications and Networking, 5 (2) (2021), pp. 703-713 CrossRefView in
    ScopusGoogle Scholar Michailidis et al., 2022 E.T. Michailidis, K. Maliatsos,
    D.N. Skoutas, D. Vouyioukas, C. Skianis Secure UAV-aided mobile edge computing
    for IoT: a review IEEE Access, 10 (2022), pp. 86353-86383 CrossRefView in ScopusGoogle
    Scholar Mills et al., 2022 J. Mills, J. Hu, G. Min Multi-task federated learning
    for personalised deep neural networks in edge computing IEEE Transactions on Parallel
    and Distributed Systems, vol. 33 (2022), pp. 630-641 no. 3 CrossRefView in ScopusGoogle
    Scholar Min et al., 2019a M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, W. Zhuang
    Learning-based computation offloading for IoT devices with energy harvesting IEEE
    Transactions on Vehicular Technology, 68 (2) (2019), pp. 1930-1941 CrossRefView
    in ScopusGoogle Scholar Min et al., 2019b M. Min, et al. Learning-based privacy-aware
    offloading for healthcare IoT with energy harvesting IEEE Internet of Things Journal,
    6 (3) (2019), pp. 4307-4316 CrossRefView in ScopusGoogle Scholar Ming et al.,
    2022 Z. Ming, et al. Edge-based video surveillance with graph-assisted reinforcement
    learning in smart construction IEEE Internet of Things Journal, vol. 9 (2022),
    pp. 9249-9265 no. 12 CrossRefView in ScopusGoogle Scholar Mishra et al., 2020
    R.A. Mishra, A. Kalla, K. Shukla, A. Nag, M. Liyanage B-VNF: blockchain-enhanced
    architecture for VNF orchestration in MEC-5G networks 2020 IEEE 3rd 5G World Forum
    (5GWF) (2020), pp. 229-234 Bangalore, India CrossRefView in ScopusGoogle Scholar
    Mishra et al., 2022 A. Mishra, Y. Mao, L. Sanguinetti, B. Clerckx Rate-splitting
    assisted massive machine-type communications in cell-free massive MIMO IEEE Communications
    Letters, 26 (6) (2022), pp. 1358-1362 CrossRefView in ScopusGoogle Scholar Mkiramweni
    et al., 2019 M.E. Mkiramweni, C. Yang, J. Li, W. Zhang A survey of game theory
    in unmanned aerial vehicles communications IEEE Communications Surveys & Tutorials,
    21 (4) (2019), pp. 3386-3416 CrossRefView in ScopusGoogle Scholar Monção et al.,
    2022 A.C.B.L. Monção, S.L. Correa, A.C. Viana, K.V. Cardoso Combining resource-aware
    recommendation and caching in the era of MEC for improving the experience of video
    streaming users IEEE Transactions on Services Computing (2022) Google Scholar
    Mondal and Paramesh, 2020 S. Mondal, J. Paramesh Power-efficient design techniques
    for mm-wave hybrid/digital FDD/Full-Duplex MIMO transceivers IEEE Journal of Solid-State
    Circuits, 55 (8) (2020), pp. 2011-2026 CrossRefView in ScopusGoogle Scholar Moura
    and Hutchison, 2019 J. Moura, D. Hutchison Game theory for multi-access edge computing:
    survey, use cases, and future trends IEEE Communications Surveys & Tutorials,
    21 (1) (2019), pp. 260-288 CrossRefView in ScopusGoogle Scholar Mu et al P. K.
    Mu, J. Zheng, T. H. Luan, L. Zhu, Z. Su and M. Dong, \"AMIS-MU: edge computing
    based adaptive video streaming for multiple mobile users,\" in IEEE Transactions
    on Mobile Computing. Google Scholar Mubarak et al., 2018 A.S. Mubarak, H. Esmaiel,
    E.M. Mohamed LTE/Wi-Fi/mmWave RAN-level interworking using 2C/U plane splitting
    for future 5G networks IEEE Access, 6 (2018), pp. 53473-53488 CrossRefView in
    ScopusGoogle Scholar Mukherjee et al., 2018 M. Mukherjee, L. Shu, D. Wang Survey
    of fog computing: fundamental, network applications, and research challenges IEEE
    Communications Surveys & Tutorials, 20 (3) (2018), pp. 1826-1857 CrossRefView
    in ScopusGoogle Scholar Mulla and Sambare, 2015 M. Mulla, S. Sambare Efficient
    analysis of lightweight Sybil attack detection scheme in Mobile Ad hoc Networks
    2015 International Conference on Pervasive Computing (ICPC) (2015), pp. 1-6 Pune,
    India CrossRefGoogle Scholar Mustafa et al., 2021 E. Mustafa, et al. Joint wireless
    power transfer and task offloading in mobile edge computing: a survey Cluster
    Computing, 25 (2021), pp. 2429-2448 Google Scholar Nadeem et al., 2021 L. Nadeem,
    et al. Integration of D2D, network slicing, and MEC in 5G cellular networks: survey
    and challenges IEEE Access, 9 (2021), pp. 37590-37612 CrossRefView in ScopusGoogle
    Scholar Nakazato et al., 2019 J. Nakazato, M. Nakamura, Y. Tao, G.K. Tran, K.
    Sakaguchi Benefits of MEC in 5G cellular networks from telecom operator''s view
    points 2019 IEEE Global Communications Conference (GLOBECOM) (2019), pp. 1-7 Waikoloa,
    HI, USA CrossRefGoogle Scholar Nallaperuma et al., 2019 D. Nallaperuma, et al.
    Online incremental machine learning platform for big data-driven smart traffic
    management IEEE Transactions on Intelligent Transportation Systems, 20 (12) (2019),
    pp. 4679-4690 CrossRefView in ScopusGoogle Scholar Ndikumana et al., 2020 A. Ndikumana,
    et al. Joint communication, computation, caching, and control in big data multi-access
    edge computing IEEE Transactions on Mobile Computing, 19 (6) (2020), pp. 1359-1374
    CrossRefView in ScopusGoogle Scholar Nduwayezu and Yun, 2022 M. Nduwayezu, J.
    Yun Latency and energy aware rate maximization in MC-NOMA-based multi-access edge
    computing: a two-stage deep reinforcement learning approach Computer Networks,
    207 (2022) Google Scholar Nguyen et al., 2019 P.-D. Nguyen, V.N. Ha, L.B. Le Computation
    offloading and resource allocation for backhaul limited cooperative MEC systems
    2019 IEEE 90th Vehicular Technology Conference (VTC2019-Fall) (2019), pp. 1-6
    Honolulu, HI, USA Google Scholar Nguyen et al., 2021 D.C. Nguyen, P.N. Pathirana,
    M. Ding, A. Seneviratne BEdgeHealth: a decentralized architecture for edge-based
    IoMT networks using blockchain IEEE Internet of Things Journal, vol. 8 (2021),
    pp. 11743-11757 no. 14 CrossRefView in ScopusGoogle Scholar Nikravan and Kashani,
    2022 M. Nikravan, M.H. Kashani A review on trust management in fog/edge computing:
    techniques, trends, and challenges Journal of Network and Computer Applications,
    204 (2022) Google Scholar Ning et al., 2019 Z. Ning, P. Dong, X. Kong, F. Xia
    A cooperative partial computation offloading scheme for mobile edge computing
    enabled internet of things IEEE Internet of Things Journal, 6 (3) (2019), pp.
    4804-4814 CrossRefView in ScopusGoogle Scholar Noh et al., 2023 J. Noh, Y. Kwon,
    J. Son, S. Cho Blockchain-based one-time authentication for secure V2X communication
    against insiders and authority compromise attacks IEEE Internet of Things Journal,
    10 (7) (2023), pp. 6235-6248 CrossRefView in ScopusGoogle Scholar Nowak et al.,
    2021 T.W. Nowak, et al. Verticals in 5G MEC-use cases and security challenges
    IEEE Access, 9 (2021), pp. 87251-87298 CrossRefView in ScopusGoogle Scholar NTT
    DoCoMo and Metawave announce, 2018 NTT DoCoMo and Metawave announce successful
    demonstration of 28GHz-band 5G using world''s first meta-structure technology
    [Online]. Available: https://www.bloomberg.com/press-releases/2018-12-04/ntt-docomo-and-metawave-announce-successful-demonstration-of-28ghz-band-5g-using-world-s-first-meta-structure-technology
    Accessed: March, 2023 Google Scholar Oba et al., 2022 S. Oba, B. Hu, K. Kamiya,
    K. Takahashi Balanced score propagation for botnet detection ICC 2022 - IEEE International
    Conference on Communications, Republic of, Seoul, Korea (2022), pp. 2357-2362
    CrossRefView in ScopusGoogle Scholar Okegbile et al S. D. Okegbile, J. Cai, C.
    Yi and D. Niyato, \"Human digital twin for personalized healthcare: vision, architecture
    and future directions,\" in IEEE Network. Google Scholar Olimid and Nencioni,
    2020 R.F. Olimid, G. Nencioni 5G network slicing: a security overview IEEE Access,
    8 (2020), pp. 99999-100009 CrossRefView in ScopusGoogle Scholar Onireti et al.,
    2014 O. Onireti, A. Imran, M.A. Imran, R. Tafazolli Cell outage detection in heterogeneous
    networks with separated control and data plane European Wireless 2014; 20th European
    Wireless Conference (2014), pp. 1-6 Barcelona, Spain Google Scholar Örs et al.,
    2020 E. Örs, R. Schmidt, M. Mighani, M. Shalaby A conceptual framework for AI-based
    operational digital twin in chemical process engineering 2020 IEEE International
    Conference on Engineering, Technology and Innovation (ICE/ITMC) (2020), pp. 1-8
    Cardiff, UK CrossRefGoogle Scholar Ou et al., 2023 X. Ou, X. Xie, H. Lu, H. Yang
    Resource allocation in MU-MISO rate-splitting multiple access with SIC errors
    for URLLC services IEEE Transactions on Communications, 71 (1) (2023), pp. 229-243
    CrossRefView in ScopusGoogle Scholar Ozdogan et al., 2022 M.O. Ozdogan, L. Carkacioglu,
    B. Canberk Digital twin driven blockchain based reliable and efficient 6G edge
    network 2022 18th International Conference on Distributed Computing in Sensor
    Systems (DCOSS), Marina del Rey, Los Angeles, CA, USA (2022), pp. 342-348 CrossRefView
    in ScopusGoogle Scholar Pahl et al., 2019 C. Pahl, A. Brogi, J. Soldani, P. Jamshidi
    Cloud container technologies: a state-of-the-art review IEEE Transactions on Cloud
    Computing, 7 (3) (2019), pp. 677-692 CrossRefView in ScopusGoogle Scholar Pan
    et al., 2021 G. Pan, J. Ye, J. An, M.-S. Alouini Full-duplex enabled intelligent
    reflecting surface systems: opportunities and challenges IEEE Wireless Communications,
    28 (3) (2021), pp. 122-129 CrossRefView in ScopusGoogle Scholar Papatheofanous
    et al., 2021 E.A. Papatheofanous, D. Reisis, K. Nikitopoulos LDPC hardware acceleration
    in 5G open radio access network platforms IEEE Access, 9 (2021), pp. 152960-152971
    CrossRefView in ScopusGoogle Scholar Parada et al., 2018 C. Parada, F. Fontes,
    C. Marques, V. Cunha, C. Leitão Multi-access edge computing: a 5G technology 2018
    European Conference on Networks and Communications (EuCNC) (2018) Ljubljana, Slovenia
    Google Scholar Parvez et al., 2018 I. Parvez, A. Rahmati, I. Guvenc, A.I. Sarwat,
    H. Dai A survey on low latency towards 5G: RAN, core network and caching solutions
    IEEE Communications Surveys & Tutorials, vol. 20 (2018), pp. 3098-3130 no. 4 CrossRefView
    in ScopusGoogle Scholar Passas et al., 2021 V. Passas, N. Makris, C. Nanis, T.
    Korakis V2MEC: low-latency MEC for vehicular networks in 5G disaggregated architectures
    2021 IEEE Wireless Communications and Networking Conference (WCNC) (2021), pp.
    1-6 Nanjing, China CrossRefGoogle Scholar Passian et al., 2022 A. Passian, et
    al. The concept of a quantum edge simulator: edge computing and sensing in the
    quantum era Sensors, 23 (1) (2022) Google Scholar Pencheva and Atanasov, 2017
    E. Pencheva, I. Atanasov Mobile edge computing services for dynamic quality of
    service control 2017 20th Conference of Open Innovations Association (FRUCT) (2017),
    pp. 365-370 St. Petersburg, Russia View in ScopusGoogle Scholar Peng et al., 2020
    H. Peng, Q. Ye, X. Shen Spectrum management for multi-access edge computing in
    autonomous vehicular networks IEEE Transactions on Intelligent Transportation
    Systems, 21 (7) (2020), pp. 3001-3012 CrossRefView in ScopusGoogle Scholar Peng
    et al., 2021 H. Peng, H. Wu, X.S. Shen Edge intelligence for multi-dimensional
    resource management in aerial-assisted vehicular networks IEEE Wireless Communications,
    vol. 28 (2021), pp. 59-65 no. 5 CrossRefView in ScopusGoogle Scholar Peng et al.,
    2022 K. Peng, H. Huang, P. Liu, X. Xu, V.C.M. Leung Joint optimization of energy
    conservation and privacy preservation for intelligent task offloading in MEC-enabled
    smart cities IEEE Transactions on Green Communications and Networking, 6 (3) (2022),
    pp. 1671-1682 CrossRefView in ScopusGoogle Scholar Pereira Ferreira and Sinnott,
    2019 A. Pereira Ferreira, R. Sinnott A performance evaluation of Containers running
    on managed Kubernetes services 2019 IEEE International Conference on Cloud Computing
    Technology and Science (CloudCom) (2019), pp. 199-208 Sydney, NSW, Australia CrossRefView
    in ScopusGoogle Scholar Pérez-Adán et al., 2021 D. Pérez-Adán, et al. Intelligent
    reflective surfaces for wireless networks: an overview of applications, approached
    issues, and open problems, Electronics, 10 (19) (2021) Google Scholar Pham et
    al., 2020 Q.-V. Pham, et al. A survey of multi-access edge computing in 5G and
    beyond: fundamentals, technology integration, and state-of-the-art IEEE Access,
    8 (2020), pp. 116974-117017 CrossRefView in ScopusGoogle Scholar Pham et al.,
    2023 V.-Q. Pham, H.-T. Thieu, A. Kak, N. Choi HexRIC: building a better near-real
    time network controller for the open RAN ecosystem Proceedings of the 24th International
    Workshop on Mobile Computing Systems and Applications (HotMobile ''23), Association
    for Computing Machinery, New York, NY, USA (2023), pp. 15-21 CrossRefView in ScopusGoogle
    Scholar Phan and Liu, 2018 L. Phan, K. Liu OpenStack network acceleration scheme
    for datacenter intelligent applications 2018 IEEE 11th International Conference
    on Cloud Computing (CLOUD) (2018), pp. 962-965 San Francisco, CA, USA CrossRefView
    in ScopusGoogle Scholar Plageras and Psannis, 2022 A.P. Plageras, K.E. Psannis
    Digital twins and multi-access edge computing for IIoT Virtual Reality & Intelligent
    Hardware, 4 (6) (2022), pp. 521-534 View PDFView articleView in ScopusGoogle Scholar
    Ponnimbaduge Perera et al., 2018 T.D. Ponnimbaduge Perera, D.N.K. Jayakody, S.K.
    Sharma, S. Chatzinotas, J. Li Simultaneous wireless information and power transfer
    (SWIPT): recent advances and future challenges IEEE Communications Surveys & Tutorials,
    20 (1) (2018), pp. 264-302 CrossRefView in ScopusGoogle Scholar Pop et al., 2020
    C. Pop, M. Prata, M. Antal, T. Cioara, I. Anghel, I. Salomie An Ethereum-based
    implementation of English, Dutch and First-price sealed-bid auctions 2020 IEEE
    16th International Conference on Intelligent Computer Communication and Processing
    (ICCP), Romania, Cluj-Napoca (2020), pp. 491-497 CrossRefView in ScopusGoogle
    Scholar Porambage et al., 2018 P. Porambage, J. Okwuibe, M. Liyanage, M. Ylianttila,
    T. Taleb Survey on multi-access edge computing for internet of things realization
    IEEE Communications Surveys & Tutorials, 20 (4) (2018), pp. 2961-2991 CrossRefView
    in ScopusGoogle Scholar Porambage et al., 2021 P. Porambage, G. Gür, D.P.M. Osorio,
    M. Liyanage, A. Gurtov, M. Ylianttila The roadmap to 6G security and privacy IEEE
    Open Journal of the Communications Society, 2 (2021), pp. 1094-1122 CrossRefView
    in ScopusGoogle Scholar Pourhabibi et al., 2020 T. Pourhabibi, K.-L. Ong, B.H.
    Kam, Y.L. Boo Fraud detection: a systematic literature review of graph-based anomaly
    detection approaches Decision Support Systems, 133 (2020) Google Scholar Preventing,
    2023 C.D.C. Preventing Stroke deaths [Online]. Available: https://www.cdc.gov/vitalsigns/stroke/index.html
    Accessed: February 2023 Google Scholar Qi et al X. Qi, M. Peng and H. Zhang, \"Joint
    mmWave beamforming and resource allocation in NOMA-MEC network for internet of
    things,\" in IEEE Transactions on Vehicular Technology. Google Scholar Qian, 2022
    Y. Qian Beyond 5G wireless communication technologies IEEE Wireless Communications,
    29 (1) (2022), pp. 2-3 Google Scholar Qian et al., 2020 L.P. Qian, B. Shi, Y.
    Wu, B. Sun, D.H.K. Tsang NOMA-enabled mobile edge computing for internet of things
    via joint communication and computation resource allocations IEEE Internet of
    Things Journal, 7 (1) (2020), pp. 718-733 CrossRefView in ScopusGoogle Scholar
    Qiang et al., 2018 W. Qiang, J. Yang, H. Jin, X. Shi PrivGuard: protecting sensitive
    Kernel data from privilege escalation attacks IEEE Access, vol. 6 (2018), pp.
    46584-46594 CrossRefView in ScopusGoogle Scholar Qiao et al., 2019 X. Qiao, P.
    Ren, S. Dustdar, L. Liu, H. Ma, J. Chen Web AR: a promising future for mobile
    augmented reality—state of the art, challenges, and insights Proceedings of the
    IEEE, vol. 107 (2019), pp. 651-666 no. 4 CrossRefView in ScopusGoogle Scholar
    Qiao et al., 2020 X. Qiao, Y. Huang, S. Dustdar, J. Chen 6G vision: an AI-driven
    decentralized network and service architecture IEEE Internet Computing, vol. 24
    (2020), pp. 33-40 no. 4 CrossRefView in ScopusGoogle Scholar Qin et al., 2019
    A. Qin, C. Cai, Q. Wang, Y. Ni, H. Zhu Game theoretical multi-user computation
    offloading for mobile-edge cloud computing 2019 IEEE Conference on Multimedia
    Information Processing and Retrieval (MIPR) (2019), pp. 328-332 San Jose, CA,
    USA CrossRefView in ScopusGoogle Scholar Qin et al., 2020 M. Qin, L. Chen, N.
    Zhao, Y. Chen, F.R. Yu, G. Wei Computing and relaying: utilizing mobile edge computing
    for P2P communications IEEE Transactions on Vehicular Technology, vol. 69 (2020),
    pp. 1582-1594 no. 2 CrossRefView in ScopusGoogle Scholar Qiu and Ma, 2018 Y. Qiu,
    M. Ma A privacy-preserving proximity testing for location-based services 2018
    IEEE Global Communications Conference (GLOBECOM), United Arab Emirates, Abu Dhabi
    (2018), pp. 1-6 Google Scholar Qiu et al., 2017 S. Qiu, Z. Li, W. He, L. Zhang,
    C. Yang, C.-Y. Su Brain–machine interface and visual compressive sensing-based
    teleoperation control of an exoskeleton robot IEEE Transactions on Fuzzy Systems,
    vol. 25 (2017), pp. 58-69 no. 1 View in ScopusGoogle Scholar Qiu et al., 2022
    H. Qiu, K. Zhu, N.C. Luong, C. Yi, D. Niyato, D.I. Kim Applications of auction
    and mechanism design in edge computing: a survey IEEE Transactions on Cognitive
    Communications and Networking, 8 (2) (2022), pp. 1034-1058 CrossRefView in ScopusGoogle
    Scholar Qu et al., 2021a C. Qu, et al. DroneCOCoNet: learning-based edge computation
    offloading and control networking for drone video analytics Future Generation
    Computer Systems, 125 (2021), pp. 247-262 View PDFView articleView in ScopusGoogle
    Scholar Qu et al., 2021b Q. Qu, C. Liu, X. Bao E-Commerce Enterprise Supply Chain
    Financing Risk Assessment Based on Linked Data Mining and Edge Computing Mobile
    Information Systems (2021) Google Scholar Qureshi et al., 2011 S.S. Qureshi, T.
    Ahmad, K. Rafique, Shuja-ul-islam Mobile cloud computing as future for mobile
    applications - implementation methods and challenging issues 2011 IEEE International
    Conference on Cloud Computing and Intelligence Systems (2011), pp. 467-471 Beijing,
    China View in ScopusGoogle Scholar R. V et al., 2022 R. V, S. K. R, M. V, S. M.
    M and M. B Fraudulent information prediction using block chain technology and
    machine learning 2022 International Conference on Applied Artificial Intelligence
    and Computing (ICAAIC) (2022), pp. 1319-1325 Salem, India Google Scholar Rahimi
    et al., 2021 H. Rahimi, Y. Picaud, K.D. Singh, G. Madhusudan, S. Costanzo, O.
    Boissier Design and simulation of a hybrid architecture for edge computing in
    5G and beyond IEEE Transactions on Computers, 70 (8) (2021), pp. 1213-1224 CrossRefView
    in ScopusGoogle Scholar Ramneek et al., 2019 Ramneek, et al. Multi-access edge
    computing in 5G network slicing: opportunities and challenges 2019 International
    Conference on Information and Communication Technology Convergence (ICTC), Jeju,
    Korea (South) (2019), pp. 30-32 CrossRefView in ScopusGoogle Scholar Ranaweera
    et al., 2019 P. Ranaweera, A.D. Jurcut, M. Liyanage Realizing multi-access edge
    computing feasibility: security perspective 2019 IEEE Conference on Standards
    for Communications and Networking (CSCN) (2019), pp. 1-7 Granada, Spain CrossRefGoogle
    Scholar Ranaweera et al., 2021a P. Ranaweera, A.D. Jurcut, M. Liyanage Survey
    on multi-access edge computing security and privacy IEEE Communications Surveys
    & Tutorials, 23 (2) (2021), pp. 1078-1124 CrossRefView in ScopusGoogle Scholar
    Ranaweera et al., 2021b P. Ranaweera, A. Jurcut, M. Liyanage MEC-enabled 5G use
    cases: a survey on security vulnerabilities and countermeasures ACM Computing
    Surveys, 54 (9) (2021) Google Scholar Rango et al., 2020 F.D. Rango, G. Potrino,
    M. Tropea, P. Fazio Energy-aware dynamic internet of things security system based
    on elliptic curve cryptography and message queue telemetry transport protocol
    for mitigating replay attacks Pervasive and Mobile Computing, 61 (2020) Google
    Scholar Ray et al., 2019 P.P. Ray, D. Dash, D. De Edge computing for Internet
    of Things: a survey, e-healthcare case study and future direction Journal of Network
    and Computer Applications, 140 (2019), pp. 1-22 View PDFView articleCrossRefView
    in ScopusGoogle Scholar Rayani et al., 2020 M. Rayani, R.H. Glitho, H. Elbiaze
    ETSI multi-access edge computing for dynamic adaptive streaming in information
    centric networks GLOBECOM 2020 - 2020 IEEE Global Communications Conference (2020),
    pp. 1-6 Taipei, Taiwan CrossRefGoogle Scholar Raza et al., 2020 M.R. Raza, A.
    Varol, N. Varol Cloud and fog computing: a survey to the concept and challenges
    2020 8th International Symposium on Digital Forensics and Security (ISDFS) (2020),
    pp. 1-6 Beirut, Lebanon Google Scholar Rebhi et al., 2021 M. Rebhi, K. Hassan,
    K. Raoof, P. Chargé Sparse code multiple access: potentials and challenges IEEE
    Open Journal of the Communications Society, 2 (2021), pp. 1205-1238 CrossRefView
    in ScopusGoogle Scholar Reddy Maddikunta et al., 2021 P.K. Reddy Maddikunta, et
    al. Unmanned aerial vehicles in smart agriculture: applications, requirements,
    and challenges IEEE Sensors Journal, 21 (16) (2021), pp. 17608-17619 CrossRefView
    in ScopusGoogle Scholar Rehman et al., 2013 A. Rehman, S. Alqahtani, A. Altameem,
    T. Saba Virtual machine security challenges: case studies International Journal
    of Machine Learning and Cybernetics, 5 (2013), pp. 729-742 Google Scholar Reis
    et al., 2022 D. Reis, B. Piedade, F.F. Correia, J.P. Dias, A. Aguiar Developing
    docker and docker-compose specifications: a developers'' survey IEEE Access, vol.
    10 (2022), pp. 2318-2329 CrossRefGoogle Scholar Riaz et al., 2018 Z. Riaz, F.
    Dürr, K. Rothermel Location Privacy and Utility in Geo-Social Networks: Survey
    and Research Challenges,\" 2018 16th Annual Conference on Privacy, Security and
    Trust (PST) (2018), pp. 1-10 Belfast, Ireland CrossRefGoogle Scholar Rico-Palomo
    et al., 2022 J.J. Rico-Palomo, J. Galeano-Brajones, D. Cortes-Polo, J.F. Valenzuela-Valdes,
    J. Carmona-Murillo Chained orchestrator algorithm for RAN-slicing resource management:
    a contribution to ultra-reliable 6G communications IEEE Access, 10 (2022), pp.
    113662-113677 CrossRefView in ScopusGoogle Scholar Rindos and Wang, 2016 A. Rindos,
    Y. Wang Dew computing: the complementary piece of cloud computing 2016 IEEE International
    Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking
    (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)
    (2016), pp. 15-20 Atlanta, GA, USA View in ScopusGoogle Scholar Rivera et al.,
    2021 A.V. Rivera, A. Refaey, E. Hossain A blockchain framework for secure task
    sharing in multi-access edge computing IEEE Network, vol. 35 (2021), pp. 176-183
    no. 3 CrossRefView in ScopusGoogle Scholar Rocha et al., 2013 F. Rocha, T. Gross,
    A.V. Moorsel Defense-in-Depth against malicious insiders in the cloud 2013 IEEE
    International Conference on Cloud Engineering (IC2E) (2013), pp. 88-97 San Francisco,
    CA, USA CrossRefView in ScopusGoogle Scholar Salahdine et al., 2022 F. Salahdine,
    T. Han, N. Zhang Security in 5G and beyond Recent Advances and Future Challenges
    Security and Privacy (2022) Google Scholar Samy et al., 2022 A. Samy, I.A. Elgendy,
    H. Yu, W. Zhang, H. Zhang Secure task offloading in blockchain-enabled mobile
    edge computing with deep reinforcement learning IEEE Transactions on Network and
    Service Management, vol. 19 (2022), pp. 4872-4887 no. 4 CrossRefView in ScopusGoogle
    Scholar Santos et al., 2014 M.E.C. Santos, A. Chen, T. Taketomi, G. Yamamoto,
    J. Miyazaki, H. Kato Augmented reality learning experiences: survey of prototype
    design and evaluation IEEE Transactions on Learning Technologies, vol. 7 (2014),
    pp. 38-56 no. 1 View in ScopusGoogle Scholar Santos et al., 2021 E.J.D. Santos,
    R.D. Souza, J.L. Rebelatto Rate-splitting multiple access for URLLC uplink in
    physical layer network slicing with eMBB IEEE Access, 9 (2021), pp. 163178-163187
    CrossRefGoogle Scholar Santos et al., 2022 L.P. Santos, T. Bashford-Rogers, J.
    Barbosa, P. Navratil Towards Quantum Ray Tracing (2022) arXiv preprint. arXiv:
    2204.12797 Google Scholar Sarrigiannis et al., 2018 I. Sarrigiannis, E. Kartsakli,
    K. Ramantas, A. Antonopoulos, C. Verikoukis Application and network VNF migration
    in a MEC-enabled 5G architecture 2018 IEEE 23rd International Workshop on Computer
    Aided Modeling and Design of Communication Links and Networks (CAMAD) (2018),
    pp. 1-6 Barcelona, Spain CrossRefGoogle Scholar Sarrigiannis et al., 2020 I. Sarrigiannis,
    K. Ramantas, E. Kartsakli, P.-V. Mekikis, A. Antonopoulos, C. Verikoukis Online
    VNF lifecycle management in an MEC-enabled 5G IoT architecture IEEE Internet of
    Things Journal, 7 (5) (2020), pp. 4183-4194 CrossRefView in ScopusGoogle Scholar
    Sato et al., 2022 M. Sato, R. Nakamura, T. Yamauchi, H. Taniguchi Improving transparency
    of hardware breakpoints with virtual machine introspection 2022 12th International
    Congress on Advanced Applied Informatics (IIAI-AAI) (2022), pp. 113-117 Kanazawa,
    Japan CrossRefView in ScopusGoogle Scholar Sattari et al., 2020 A. Sattari, R.
    Ehsani, T. Leppänen, S. Pirttikangas, J. Riekki Edge-supported microservice-based
    resource discovery for mist computing 2020 IEEE Intl Conf on Dependable, Autonomic
    and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl
    Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology
    Congress (DASC/PiCom/CBDCom/CyberSciTech) (2020), pp. 462-468 Calgary, AB, Canada
    CrossRefView in ScopusGoogle Scholar Satyanarayanan et al., 2009 M. Satyanarayanan,
    P. Bahl, R. Caceres, N. Davies The case for VM-based cloudlets in mobile computing
    IEEE Pervasive Computing, 8 (4) (2009), pp. 14-23 View in ScopusGoogle Scholar
    Schiller et al., 2018 E. Schiller, et al. CDS-MEC: NFV/SDN-based Application Management
    for MEC in 5G Systems Comput. Network., 135 (April 2018), pp. 96-107 View PDFView
    articleView in ScopusGoogle Scholar Schluse et al., 2018 M. Schluse, M. Priggemeyer,
    L. Atorf, J. Rossmann Experimentable digital twins—streamlining simulation-based
    systems engineering for industry 4.0 IEEE Transactions on Industrial Informatics,
    14 (4) (2018), pp. 1722-1731 CrossRefView in ScopusGoogle Scholar Seid et al.,
    2022 A.M. Seid, J. Lu, H.N. Abishu, T.A. Ayall Blockchain-enabled task offloading
    with energy harvesting in multi-UAV-assisted IoT networks: a multi-agent DRL approach
    IEEE Journal on Selected Areas in Communications, 40 (12) (2022), pp. 3517-3532
    CrossRefView in ScopusGoogle Scholar Semeraro et al., 2021 C. Semeraro, M. Lezoche,
    H. Panetto, M. Dassisti Digital twin paradigm: a systematic literature review
    Computers in Industry, 130 (2021) Google Scholar Seng et al., 2019 S. Seng, X.
    Li, C. Luo, H. Ji, H. Zhang A D2D-assisted MEC computation offloading in the blockchain-based
    framework for UDNs ICC 2019 - 2019 IEEE International Conference on Communications
    (ICC) (2019), pp. 1-6 Shanghai, China CrossRefGoogle Scholar Shah et al., 2020
    S.D.A. Shah, M.A. Gregory, S. Li, R.D.R. Fontes SDN enhanced multi-access edge
    computing (MEC) for E2E mobility and QoS management IEEE Access, 8 (2020), pp.
    77459-77469 CrossRefView in ScopusGoogle Scholar Shah et al., 2021 S.D.A. Shah,
    M.A. Gregory, S. Li Cloud-native network slicing using software defined networking
    based multi-access edge computing: a survey IEEE Access, 9 (2021), pp. 10903-10924
    CrossRefView in ScopusGoogle Scholar Shahzadi et al., 2017 S. Shahzadi, M. Iqbal,
    T. Dagiuklas, Z.U. Qayyum Multi-access edge computing: open issues, challenges
    and future perspectives Journal of Cloud Computing, 6 (2017) Google Scholar Shakarami
    et al., 2020a A. Shakarami, M. Ghobaei-Arani, M. Masdari, M. Hosseinzadeh A survey
    on the computation offloading approaches in mobile edge/cloud computing environment:
    a stochastic-based perspective Journal of Grid Computing, 18 (2020), pp. 639-671
    CrossRefView in ScopusGoogle Scholar Shakarami et al., 2020b A. Shakarami, M.
    Ghobaei-Arani, A. Shahidinejad A survey on the computation offloading approaches
    in mobile edge computing: a machine learning-based perspective Computer Networks,
    182 (2020) Google Scholar Shakarami et al., 2021 A. Shakarami, A. Shahidinejad,
    M. Ghobaei-Arani An autonomous computation offloading strategy in mobile edge
    computing: a deep learning-based hybrid approach Journal of Network and Computer
    Applications, 178 (2021) Google Scholar Shamseddine et al., 2019 M. Shamseddine,
    W. Itani, A. Al-Dulaimy, J. Taheri Mitigating rogue node attacks in edge computing
    2019 2nd IEEE Middle East and North Africa COMMunications Conference (MENACOMM),
    Manama, Bahrain (2019), pp. 1-6 CrossRefGoogle Scholar Shantharama et al., 2018
    P. Shantharama, A.S. Thyagaturu, N. Karakoc, L. Ferrari, M. Reisslein, A. Scaglione
    LayBack: SDN management of multi-access edge computing (MEC) for network access
    services and radio resource sharing IEEE Access, vol. 6 (2018), pp. 57545-57561
    CrossRefView in ScopusGoogle Scholar Sharghivand et al., 2021 N. Sharghivand,
    F. Derakhshan, N. Siasi A comprehensive survey on auction mechanism design for
    cloud/edge resource management and pricing IEEE Access, 9 (2021), pp. 126502-126529
    CrossRefView in ScopusGoogle Scholar Sharma and Diwakar, 2021 A. Sharma, C. Diwakar
    Future aspects on MEC (mobile edge computing): offloading mechanism 2021 6th International
    Conference on Signal Processing, Computing and Control (ISPCC), Solan, India (2021),
    pp. 34-39 CrossRefView in ScopusGoogle Scholar Sharma and Jain, 2014 C. Sharma,
    S.C. Jain Analysis and classification of SQL injection vulnerabilities and attacks
    on web applications 2014 International Conference on Advances in Engineering &
    Technology Research (ICAETR - 2014) (2014), pp. 1-6 Unnao, India CrossRefGoogle
    Scholar Sharma et al., 2020 A. Sharma, et al. Communication and networking technologies
    for UAVs: a survey Journal of Network and Computer Applications, 168 (2020) Google
    Scholar Sheinidashtegol and Galloway, 2017 P. Sheinidashtegol, M. Galloway Performance
    impact of DDoS attacks on three virtual machine hypervisors 2017 IEEE International
    Conference on Cloud Engineering (IC2E) (2017), pp. 204-214 Vancouver, BC, Canada
    View in ScopusGoogle Scholar Shende et al., 2017 V. Shende, G. Sudi, M. Kulkarni
    Fast cryptanalysis of RSA encrypted data using a combination of mathematical and
    brute force attack in distributed computing environment 2017 IEEE International
    Conference on Power, Control, Signals and Instrumentation Engineering (ICPCSI)
    (2017), pp. 2446-2449 Chennai, India View in ScopusGoogle Scholar Sheng et al.,
    2020 M. Sheng, Y. Wang, X. Wang, J. Li Energy-efficient multiuser partial computation
    offloading with collaboration of terminals, radio access network, and edge server
    IEEE Transactions on Communications, 68 (3) (2020), pp. 1524-1537 CrossRefView
    in ScopusGoogle Scholar Shi et al., 2016 W. Shi, J. Cao, Q. Zhang, Y. Li, L. Xu
    Edge Computing: Vision and Challenges IEEE Internet Things J., 3 (5) (October
    2016), pp. 637-646 View in ScopusGoogle Scholar Shirazi et al., 2017 S.N. Shirazi,
    A. Gouglidis, A. Farshad, D. Hutchison The extended cloud: review and analysis
    of mobile edge computing and fog from a security and resilience perspective IEEE
    Journal on Selected Areas in Communications, vol. 35 (2017), pp. 2586-2595 no.
    11 View in ScopusGoogle Scholar Shirin Abkenar et al., 2022 F. Shirin Abkenar,
    et al. A survey on mobility of edge computing networks in IoT: state-of-the-art,
    architectures, and challenges IEEE Communications Surveys & Tutorials, 24 (4)
    (2022), pp. 2329-2365 CrossRefView in ScopusGoogle Scholar Shoham and Leyton-Brown,
    2008 Y. Shoham, K. Leyton-Brown Multiagent Systems: Algorithmic, Game-Theoretic,
    and Logical Foundations Cambridge Univ. Press, Cambridge, U.K. (2008) Google Scholar
    Shu and Li, 2023 W. Shu, Y. Li Joint offloading strategy based on quantum particle
    swarm optimization for MEC-enabled vehicular networks Digital Communications and
    Networks, 9 (1) (2023), pp. 56-66 View PDFView articleView in ScopusGoogle Scholar
    Shu et al., 2015 X. Shu, D. Yao, E. Bertino Privacy-preserving detection of sensitive
    data exposure IEEE Transactions on Information Forensics and Security, vol. 10
    (2015), pp. 1092-1103 no. 5 View in ScopusGoogle Scholar Sindjoung et al., 2022
    M.L.F. Sindjoung, M. Velempini, A.B. Bomgni A MEC architecture for a better quality
    of service in an Autonomous Vehicular Network Computer Networks, 219 (2022) Google
    Scholar Singh and Borisagar, 2022 P.P. Singh, K. Borisagar Paging vulnerabilities
    in 5G new radio networks and mitigation to enhance security performance 2022 IEEE
    World Conference on Applied Intelligence and Computing (AIC) (2022), pp. 914-918
    Sonbhadra, India CrossRefView in ScopusGoogle Scholar Singh et al., 2020 A.K.
    Singh, R.K. Jaiswal, K. Abdukodir, A. Muthanna ARDefense: DDoS detection and prevention
    using NFV and SDN 2020 12th International Congress on Ultra Modern Telecommunications
    and Control Systems and Workshops (ICUMT) (2020), pp. 236-241 Brno, Czech Republic
    CrossRefView in ScopusGoogle Scholar Singh et al., 2021 J. Singh, Y. Bello, A.R.
    Hussein, A. Erbad, A. Mohamed Hierarchical security paradigm for IoT multiaccess
    edge computing IEEE Internet of Things Journal, vol. 8 (2021), pp. 5794-5805 no.
    7 CrossRefView in ScopusGoogle Scholar Singh et al., 2022a U. Singh, et al. Coalition
    games for performance evaluation in 5G and beyond networks: a survey IEEE Access,
    10 (2022), pp. 15393-15420 CrossRefView in ScopusGoogle Scholar Singh et al.,
    2022b R. Singh, R. Sukapuram, S. Chakraborty Mobility-aware multi-access edge
    computing for multiplayer augmented and virtual reality gaming 2022 IEEE 21st
    International Symposium on Network Computing and Applications (NCA) (2022), pp.
    191-200 Boston, MA, USA CrossRefView in ScopusGoogle Scholar Siriwardhana et al.,
    2021 Y. Siriwardhana, P. Porambage, M. Liyanage, M. Ylianttila A survey on mobile
    augmented reality with 5G mobile edge computing: architectures, applications,
    and technical aspects IEEE Communications Surveys & Tutorials, 23 (2) (2021),
    pp. 1160-1192 CrossRefView in ScopusGoogle Scholar Sivanathan et al., 2020 A.
    Sivanathan, H.H. Gharakheili, V. Sivaraman Detecting behavioral change of IoT
    devices using clustering-based network traffic modeling IEEE Internet of Things
    Journal, vol. 7 (2020), pp. 7295-7309 no. 8 CrossRefView in ScopusGoogle Scholar
    Slamnik-Kriještorac and Marquez-Barja, 2020 N. Slamnik-Kriještorac, J.M. Marquez-Barja
    Demo abstract: assessing MANO performance based on VIM platforms within MEC context
    IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM
    WKSHPS) (2020), pp. 1338-1339 Toronto, ON, Canada CrossRefView in ScopusGoogle
    Scholar Sohn and Heo, 2018 I.K. Sohn, J. Heo An introduction to fault-tolerant
    quantum computation and its overhead reduction schemes 2018 Tenth International
    Conference on Ubiquitous and Future Networks (ICUFN) (2018), pp. 44-46 Prague,
    Czech Republic CrossRefView in ScopusGoogle Scholar Sojaat and Skalaa, 2017 Z.
    Sojaat, K. Skalaa The dawn of Dew: dew Computing for advanced living environment
    2017 40th International Convention on Information and Communication Technology,
    Electronics and Microelectronics (MIPRO) (2017), pp. 347-352 Opatija, Croatia
    View in ScopusGoogle Scholar Song et al., 2020 X. Song, W. Jiang, X. Liu, H. Lu,
    Z. Tian, X. Du A survey of game theory as applied to social networks Tsinghua
    Science and Technology, 25 (6) (2020), pp. 734-742 CrossRefView in ScopusGoogle
    Scholar Song et al., 2021 Z. Song, Y. Hao, Y. Liu, X. Sun Energy-efficient multiaccess
    edge computing for terrestrial-satellite internet of things IEEE Internet of Things
    Journal, 8 (18) (2021), pp. 14202-14218 CrossRefView in ScopusGoogle Scholar Song
    et al., 2022a Z. Song, et al. A comprehensive survey on aerial mobile edge computing:
    challenges, state-of-the-art, and future directions Computer Communications, 191
    (2022), pp. 233-256 View PDFView articleView in ScopusGoogle Scholar Song et al.,
    2022b L. Song, X. Hu, G. Zhang, P. Spachos, K.N. Plataniotis, H. Wu Networking
    systems of AI: on the convergence of computing and communications IEEE Internet
    of Things Journal, vol. 9 (2022), pp. 20352-20381 no. 20 CrossRefView in ScopusGoogle
    Scholar Spinelli and Mancuso, 2021 F. Spinelli, V. Mancuso Toward enabled industrial
    verticals in 5G: a survey on MEC-based approaches to provisioning and flexibility
    IEEE Communications Surveys & Tutorials, 23 (1) (2021), pp. 596-630 CrossRefView
    in ScopusGoogle Scholar Su et al., 2020 C. Su, F. Ye, T. Liu, Y. Tian, Z. Han
    Computation offloading in hierarchical multi-access edge computing based on contract
    theory and bayesian matching game IEEE Transactions on Vehicular Technology, 69
    (11) (2020), pp. 13686-13701 CrossRefView in ScopusGoogle Scholar Sun et al C.
    Sun, X. Wu, X. Li, Q. Fan, J. Wen and V. C. M. Leung, \"Cooperative computation
    offloading for multi-access edge computing in 6G mobile networks via soft actor
    critic,\" in IEEE Transactions on Network Science and Engineering. Google Scholar
    Sun et al., 2018 J. Sun, C. Yang, T. Tanjo, K. Sage, K. Aida, “Implementation
    of self-adaptive middleware for mobile vehicle tracking applications on edge computing.”
    In: Y. Xiang, J. Sun, G. Fortino, A. Guerrieri, J. Jung (eds) Internet and Distributed
    Computing Systems. IDCS 2018. Lecture Notes in Computer Science, vol. vol. 11226.
    Springer, Cham. Google Scholar Sun et al., 2020a W. Sun, H. Zhang, R. Wang, Y.
    Zhang Reducing offloading latency for digital twin edge networks in 6G IEEE Transactions
    on Vehicular Technology, 69 (10) (2020), pp. 12240-12251 CrossRefView in ScopusGoogle
    Scholar Sun et al., 2020b P. Sun, L. Luo, S. Liu, W. Wu Adaptive rule engine for
    anomaly detection in 5G mobile edge computing 2020 IEEE 20th International Conference
    on Software Quality, Reliability and Security Companion (QRS-C) (2020), pp. 690-691
    Macau, China CrossRefView in ScopusGoogle Scholar Sun et al., 2021 Z. Sun, et
    al. Applications of game theory in vehicular networks: a survey IEEE Communications
    Surveys & Tutorials, 23 (4) (2021), pp. 2660-2710 CrossRefView in ScopusGoogle
    Scholar Sun et al., 2022a Y. Sun, H. Ochiai, H. Esaki Decentralized deep learning
    for multi-access edge computing: a survey on communication efficiency and trustworthiness
    IEEE Transactions on Artificial Intelligence, 3 (6) (2022), pp. 963-972 CrossRefView
    in ScopusGoogle Scholar Sun et al., 2022b W. Sun, S. Lian, H. Zhang, Y. Zhang
    Lightweight digital twin and federated learning with distributed incentive in
    air-ground 6G networks IEEE Transactions on Network Science and Engineering (2022)
    Google Scholar Suomalainen et al., 2021 J. Suomalainen, J. Julku, M. Vehkaperä,
    H. Posti Securing public safety communications on commercial and tactical 5G networks:
    a survey and future research directions IEEE Open Journal of the Communications
    Society, vol. 2 (2021), pp. 1590-1615 CrossRefView in ScopusGoogle Scholar Szefer
    et al., 2012 J. Szefer, P. Jamkhedkar, Yu-Yuan Chen, R.B. Lee Physical attack
    protection with human-secure virtualization in data centers IEEE/IFIP International
    Conference on Dependable Systems and Networks Workshops (DSN 2012) (2012), pp.
    1-6 Boston, MA, USA CrossRefGoogle Scholar Tabatabaee Malazi et al., 2022 H. Tabatabaee
    Malazi, et al. Dynamic service placement in multi-access edge computing: a systematic
    literature review IEEE Access, 10 (2022), pp. 32639-32688 CrossRefView in ScopusGoogle
    Scholar Taleb et al., 2017 T. Taleb, K. Samdanis, B. Mada, H. Flinck, S. Dutta,
    D. Sabella On multi-access edge computing: a survey of the emerging 5G network
    edge cloud architecture and orchestration IEEE Communications Surveys & Tutorials,
    19 (3) (2017), pp. 1657-1681 View in ScopusGoogle Scholar Tan et al., 2019 X.
    Tan, H. Li, L. Wang, Z. Xu Global orchestration of cooperative defense against
    DDoS attacks for MEC 2019 IEEE Wireless Communications and Networking Conference
    (WCNC), Marrakesh, Morocco (2019), pp. 1-6 Google Scholar Tang et al., 2021a F.
    Tang, B. Mao, Y. Kawamoto, N. Kato Survey on machine learning for intelligent
    end-to-end communication toward 6G: from network access, routing to traffic control
    and streaming adaption IEEE Communications Surveys & Tutorials, 23 (3) (2021),
    pp. 1578-1598 CrossRefView in ScopusGoogle Scholar Tang et al., 2021b F. Tang,
    B. Mao, N. Kato, G. Gui Comprehensive survey on machine learning in vehicular
    network: technology, applications and challenges IEEE Communications Surveys &
    Tutorials, vol. 23 (2021), pp. 2027-2057 no. 3 CrossRefView in ScopusGoogle Scholar
    Tang et al., 2022 F. Tang, X. Chen, T.K. Rodrigues, M. Zhao, N. Kato Survey on
    digital twin edge networks (DITEN) toward 6G IEEE Open Journal of the Communications
    Society, 3 (2022), pp. 1360-1381 CrossRefView in ScopusGoogle Scholar Tang et
    al.a H. Tang, H. Wu, G. Qu and R. Li, \"Double deep Q-network based dynamic framing
    offloading in vehicular edge computing,\" in IEEE Transactions on Network Science
    and Engineering. Google Scholar Tang et al.b F. Tang, X. Chen, M. Zhao and N.
    Kato, \"The roadmap of communication and networking in 6G for the Metaverse,\"
    in IEEE Wireless Communications. Google Scholar Tefera et al., 2021 G. Tefera,
    K. She, M. Shelke, A. Ahmed Decentralized adaptive resource-aware computation
    offloading & caching for multi-access edge computing networks Sustainable Computing:
    Informatics and Systems, 30 (2021) Google Scholar Tegos et al., 2022 S.A. Tegos,
    P.D. Diamantoulakis, G.K. Karagiannidis On the performance of uplink rate-splitting
    multiple access IEEE Communications Letters, 26 (3) (2022), pp. 523-527 CrossRefView
    in ScopusGoogle Scholar Thiruvasagam et al., 2021 P.K. Thiruvasagam, A. Chakraborty,
    A. Mathew, C.S.R. Murthy Reliable placement of service function chains and virtual
    monitoring functions with minimal cost in softwarized 5G networks IEEE Transactions
    on Network and Service Management, vol. 18 (2021), pp. 1491-1507 no. 2 CrossRefView
    in ScopusGoogle Scholar Tiwary et al., 2018 M. Tiwary, S. Sharma, P. Mishra, H.
    El-Sayed, M. Prasad, D. Puthal Building scalable mobile edge computing by enhancing
    quality of services 2018 International Conference on Innovations in Information
    Technology (IIT), Al Ain (2018), pp. 141-146 United Arab Emirates CrossRefView
    in ScopusGoogle Scholar Tran et al., 2017 T.X. Tran, A. Hajisami, P. Pandey, D.
    Pompili Collaborative mobile edge computing in 5G networks: new paradigms, scenarios,
    and challenges IEEE Communications Magazine, vol. 55 (2017), pp. 54-61 no. 4 View
    in ScopusGoogle Scholar Tsou et al., 2023 C.-W. Tsou, Y.-W. Ma, Y.-H. Tu, J.-L.
    Chen Security assist mechanisms for industrial control systems with authentication
    2023 25th International Conference on Advanced Communication Technology (ICACT),
    Pyeongchang (2023), pp. 186-188 Korea, Republic of CrossRefView in ScopusGoogle
    Scholar Tsoutsos and Maniatakos, 2014 N.G. Tsoutsos, M. Maniatakos Fabrication
    attacks: zero-overhead malicious modifications enabling modern microprocessor
    privilege escalation IEEE Transactions on Emerging Topics in Computing, vol. 2
    (2014), pp. 81-93 no. 1 View in ScopusGoogle Scholar Tusa and Clayman, 2021 F.
    Tusa, S. Clayman The impact of encoding and transport for massive real-time IoT
    data on edge resource consumption Journal of Grid Computing, 19 (2021) Google
    Scholar Tushar et al., 2023 W. Tushar, et al. A survey of cyber-physical systems
    from a game-theoretic perspective IEEE Access, 11 (2023), pp. 9799-9834 CrossRefView
    in ScopusGoogle Scholar Ugwuanyi et al., 2018 E.E. Ugwuanyi, S. Ghosh, M. Iqbal,
    T. Dagiuklas Reliable resource provisioning using bankers'' deadlock avoidance
    algorithm in MEC for industrial IoT IEEE Access, 6 (2018), pp. 43327-43335 CrossRefView
    in ScopusGoogle Scholar Ugwuanyi et al., 2019 E.E. Ugwuanyi, S. Ghosh, M. Iqbal,
    T. Dagiuklas, S. Mumtaz, A. Al-Dulaimi Co-operative and hybrid replacement caching
    for multi-access mobile edge computing 2019 European Conference on Networks and
    Communications (EuCNC) (2019), pp. 394-399 Valencia, Spain CrossRefView in ScopusGoogle
    Scholar Ugwuanyi et al., 2021 E.E. Ugwuanyi, M. Iqbal, T. Dagiuklas A novel predictive-collaborative-replacement
    (PCR) intelligent caching scheme for multi-access edge computing IEEE Access,
    9 (2021), pp. 37103-37115 CrossRefView in ScopusGoogle Scholar Ullah et al., 2019
    R. Ullah, M.A.U. Rehman, B.-S. Kim Design and implementation of an open source
    framework and prototype for named data networking-based edge cloud computing system
    IEEE Access, 7 (2019), pp. 57741-57759 CrossRefView in ScopusGoogle Scholar Vaezi
    et al., 2022 M. Vaezi, et al. Cellular, wide-area, and non-terrestrial IoT: a
    survey on 5G advances and the road toward 6G IEEE Communications Surveys & Tutorials,
    24 (2) (2022), pp. 1117-1174 CrossRefView in ScopusGoogle Scholar van der Westhuizen
    and Hancke, 2017 H.W. van der Westhuizen, G.P. Hancke Mobile cloud computing and
    application program interfaces — a review 2017 IEEE AFRICON (2017), pp. 1569-1574
    Cape Town, South Africa CrossRefView in ScopusGoogle Scholar Van Huynh et al.,
    2022a D. Van Huynh, S.R. Khosravirad, A. Masaracchia, O.A. Dobre, T.Q. Duong Edge
    intelligence-based ultra-reliable and low-latency communications for digital twin-enabled
    Metaverse IEEE Wireless Communications Letters, 11 (8) (2022), pp. 1733-1737 CrossRefView
    in ScopusGoogle Scholar Van Huynh et al., 2022b D. Van Huynh, et al. URLLC edge
    networks with joint optimal user association, task offloading and resource allocation:
    a digital twin approach IEEE Transactions on Communications, vol. 70 (2022), pp.
    7669-7682 no. 11 CrossRefView in ScopusGoogle Scholar Varma and Narayanan, 2016
    D.A. Varma, M. Narayanan Identifying malicious nodes in Mobile Ad-Hoc Networks
    using polynomial reduction algorithm 2016 International Conference on Electrical,
    Electronics, and Optimization Techniques (ICEEOT) (2016), pp. 1179-1184 Chennai,
    India CrossRefView in ScopusGoogle Scholar Varshney and Sagar, 2018 R.K. Varshney,
    A.K. Sagar An improved AODV protocol to detect malicious node in ad hoc network
    2018 International Conference on Advances in Computing, Communication Control
    and Networking (ICACCCN), Greater Noida, India (2018), pp. 222-227 CrossRefView
    in ScopusGoogle Scholar Verma and Verma, 2021 A. Verma, V. Verma Comparative study
    of cloud computing and edge computing: three level architecture models and security
    challenges International Journal of Distributed and Cloud Computing, 9 (1) (2021),
    pp. 13-17 Google Scholar Vidhani and Vidhate, 2022 S.M. Vidhani, A.V. Vidhate
    Security Challenges in 5G Network: a technical features survey and analysis 2022
    5th International Conference on Advances in Science and Technology (ICAST) (2022),
    pp. 592-597 Mumbai, India CrossRefView in ScopusGoogle Scholar Vilalta et al.,
    2021 R. Vilalta, et al. TeraFlow: secured autonomic traffic management for a tera
    of SDN flows 2021 Joint European Conference on Networks and Communications & 6G
    Summit (EuCNC/6G Summit) (2021), pp. 377-382 Porto, Portugal CrossRefView in ScopusGoogle
    Scholar Villari et al., 2016 M. Villari, M. Fazio, S. Dustdar, O. Rana, R. Ranjan
    Osmotic computing: a new paradigm for edge/cloud integration IEEE Cloud Computing,
    3 (6) (2016), pp. 76-83 View in ScopusGoogle Scholar Viola et al., 2018 R. Viola,
    A. Martin, M. Zorrilla, J. Montalbán MEC proxy for efficient cache and reliable
    multi-CDN video distribution 2018 IEEE International Symposium on Broadband Multimedia
    Systems and Broadcasting (BMSB) (2018), pp. 1-7 Valencia, Spain CrossRefGoogle
    Scholar von Rueden et al., 2023 L. von Rueden, et al. Informed machine learning
    – a taxonomy and survey of integrating prior knowledge into learning systems IEEE
    Transactions on Knowledge and Data Engineering, 35 (1) (2023), pp. 614-633 View
    in ScopusGoogle Scholar Waheed et al., 2022 A. Waheed, et al. A comprehensive
    review of computing paradigms, enabling computation offloading and task execution
    in vehicular networks IEEE Access, 10 (2022), pp. 3580-3600 CrossRefView in ScopusGoogle
    Scholar Wang, 2021 Z. Wang Information security risk and solution of computer
    network under big data background 2021 IEEE Conference on Telecommunications,
    Optics and Computer Science (TOCS) (2021), pp. 312-314 Shenyang, China CrossRefView
    in ScopusGoogle Scholar Wang and Xiang, 2014 Y. Wang, C. Xiang Which is more suitable
    for rare flowers, first-price sealed-bid auction or second-price sealed-bid auction?
    2014 IEEE International Conference on Mechatronics and Automation (2014), p. 2111
    Tianjin, China CrossRefView in ScopusGoogle Scholar Wang et al S. Wang, S. Bi
    and Y. -J. A. Zhang, \"Edge video analytics with adaptive information gathering:
    a deep reinforcement learning approach,\" in IEEE Transactions on Wireless Communications.
    Google Scholar Wang et al., 2016 L. Wang, K.-K. Wong, M. Elkashlan, A. Nallanathan,
    S. Lambotharan Secrecy and energy efficiency in massive MIMO aided heterogeneous
    C-RAN: a new look at interference IEEE Journal of Selected Topics in Signal Processing,
    10 (8) (2016), pp. 1375-1389 View in ScopusGoogle Scholar Wang et al., 2019a M.
    Wang, B. Cheng, W. Feng, J. Chen An efficient service function chain placement
    algorithm in a MEC-NFV environment 2019 IEEE Global Communications Conference
    (GLOBECOM) (2019), pp. 1-6 Waikoloa, HI, USA View in ScopusGoogle Scholar Wang
    et al., 2019b X. Wang, Y. Ji, J. Zhang, L. Bai, M. Zhang Low-latency oriented
    network planning for MEC-enabled WDM-PON based fiber-wireless access networks
    IEEE Access, vol. 7 (2019), pp. 183383-183395 CrossRefView in ScopusGoogle Scholar
    Wang et al., 2019c D. Wang, B. Bai, K. Lei, W. Zhao, Y. Yang, Z. Han Enhancing
    information security via physical layer approaches in heterogeneous IoT with multiple
    access mobile edge computing in smart city IEEE Access, vol. 7 (2019), pp. 54508-54521
    CrossRefView in ScopusGoogle Scholar Wang et al., 2020a X. Wang, M. Lu, Y. Wang
    Workload optimization and energy consumption reduction strategy of private cloud
    in manufacturing industry 2020 IEEE 11th International Conference on Software
    Engineering and Service Science (ICSESS) (2020), pp. 440-444 Beijing, China CrossRefView
    in ScopusGoogle Scholar Wang et al., 2020b J. Wang, Y.-C. Liang, S. Han, Y. Pei
    Robust beamforming and phase shift design for IRS-enhanced multi-user MISO downlink
    communication ICC 2020 - 2020 IEEE International Conference on Communications
    (ICC) (2020), pp. 1-6 Dublin, Ireland Google Scholar Wang et al., 2020c X. Wang,
    Y. Ji, J. Zhang, L. Bai, M. Zhang Joint optimization of latency and deployment
    cost over TDM-PON based MEC-enabled cloud radio access networks IEEE Access, vol.
    8 (2020), pp. 681-696 CrossRefView in ScopusGoogle Scholar Wang et al., 2020d
    H. Wang, L. Ma, H. -y. Bai A three-tier scheme for Sybil attack detection in wireless
    sensor networks 2020 5th International Conference on Computer and Communication
    Systems (ICCCS) (2020), pp. 752-756 Shanghai, China CrossRefView in ScopusGoogle
    Scholar Wang et al., 2020e H. Wang, H. Sayadi, G. Kolhe, A. Sasan, S. Rafatirad,
    H. Homayoun Phased-guard: multi-phase machine learning framework for detection
    and identification of zero-day microarchitectural Side-Channel Attacks 2020 IEEE
    38th International Conference on Computer Design (ICCD) (2020), pp. 648-655 Hartford,
    CT, USA Google Scholar Wang et al., 2020f W. Wang, S. Ge, X. Zhou Location-privacy-aware
    service migration in mobile edge computing 2020 IEEE Wireless Communications and
    Networking Conference (WCNC) (2020), pp. 1-6 Seoul, Korea (South) Google Scholar
    Wang et al., 2020g D. Wang, X. Tian, H. Cui, Z. Liu Reinforcement learning-based
    joint task offloading and migration schemes optimization in mobility-aware MEC
    network China Communications, vol. 17 (2020), pp. 31-44 no. 8 CrossRefView in
    ScopusGoogle Scholar Wang et al., 2021a Y. Wang, H. Wang, S. Chen, Y. Xia A Survey
    on Mainstream Dimensions of Edge Computing,” 2021 the 5th International Conference
    on Information System and Data Mining ICISDM 2021), New York, NY, USA (2021),
    pp. 1-9 View PDFView articleGoogle Scholar Wang et al., 2021b J. Wang, et al.
    Joint computation offloading and resource allocation for MEC-enabled IoT systems
    with imperfect CSI IEEE Internet of Things Journal, 8 (5) (2021), pp. 3462-3475
    CrossRefView in ScopusGoogle Scholar Wang et al., 2021c Z. Wang, D. Shi, H. Wu
    The role of massive MIMO and intelligent reflecting surface in 5G/6G networks
    2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)
    (2021) Hangzhou, China Google Scholar Wang et al., 2021d H. Wang, Z. Lin, K. Guo,
    T. Lv Energy and delay minimization based on game theory in MEC-assisted vehicular
    networks 2021 IEEE International Conference on Communications Workshops (ICC Workshops)
    (2021), pp. 1-6 Montreal, QC, Canada Google Scholar Wang et al., 2021e S. Wang,
    Z. Pei, C. Wang, J. Wu Shaping the future of the application of quantum computing
    in intelligent transportation system Intelligent and Converged Networks, vol.
    2 (2021), pp. 259-276 no. 4 CrossRefView in ScopusGoogle Scholar Wang et al.,
    2021f H. Wang, et al. Evaluation of machine learning-based detection against Side-Channel
    Attacks on autonomous vehicle 2021 IEEE 3rd International Conference on Artificial
    Intelligence Circuits and Systems (AICAS) (2021), pp. 1-4 Washington DC, DC, USA
    Google Scholar Wang et al., 2022a X. Wang, et al. Holistic service-based architecture
    for space-air-ground integrated network for 5G-advanced and beyond China Communications,
    19 (1) (2022), pp. 14-28 View in ScopusGoogle Scholar Wang et al., 2022b Y. Wang,
    L. Zhao, H. Tu, G. Zhou, Q. Yin Design and implementation of endogenous intelligence-based
    multi-access edge computing 2022 IEEE 33rd Annual International Symposium on Personal,
    Indoor and Mobile Radio Communications (PIMRC) (2022), pp. 515-521 Kyoto, Japan
    CrossRefView in ScopusGoogle Scholar Wang et al., 2022c M. Wang, W. Fu, X. He,
    S. Hao, X. Wu A survey on large-scale machine learning IEEE Transactions on Knowledge
    and Data Engineering, 34 (6) (2022), pp. 2574-2594 View in ScopusGoogle Scholar
    Wang et al., 2022d Z. Wang, T. Lv, Z. Chang Computation offloading and resource
    allocation based on distributed deep learning and software defined mobile edge
    computing Computer Networks, 205 (2022) Google Scholar Wang et al., 2022e L. Wang,
    J. Wang, X. Jiang, J. Cui, B. Zheng Intelligent reflective surface-assisted MEC
    securely calculates offloading policy 2022 14th International Conference on Wireless
    Communications and Signal Processing (WCSP) (2022), pp. 60-65 Nanjing, China Google
    Scholar Wang et al., 2023 H. Wang, X. Wang, X. Lan, T. Su, L. Wan BSBL-based auxiliary
    vehicle position analysis in smart city using distributed MEC and UAV-deployed
    IoT IEEE Internet of Things Journal, 10 (2) (2023), pp. 975-986 CrossRefView in
    ScopusGoogle Scholar Waqar et al., 2022 N. Waqar, S.A. Hassan, A. Mahmood, K.
    Dev, D.-T. Do, M. Gidlund Computation offloading and resource allocation in MEC-enabled
    integrated aerial-terrestrial vehicular networks: a reinforcement learning approach
    IEEE Transactions on Intelligent Transportation Systems, 23 (11) (2022), pp. 21478-21491
    CrossRefView in ScopusGoogle Scholar Wei et al., 2018 F. Wei, S. Chen, W. Zou
    A greedy algorithm for task offloading in mobile edge computing system China Communications,
    vol. 15 (2018), pp. 149-157 no. 11 CrossRefGoogle Scholar Wei et al., 2021 X.
    Wei, et al. Wireless edge caching based on content similarity in dynamic environments
    Journal of Systems Architecture, 115 (2021) Google Scholar Wei et al., 2022 P.
    Wei, et al. Reinforcement learning-empowered mobile edge computing for 6G edge
    intelligence IEEE Access, 10 (2022), pp. 65156-65192 CrossRefView in ScopusGoogle
    Scholar Wijethilaka and Liyanage, 2021 S. Wijethilaka, M. Liyanage Survey on network
    slicing for internet of things realization in 5G networks IEEE Communications
    Surveys & Tutorials, 23 (2) (2021), pp. 957-994 CrossRefView in ScopusGoogle Scholar
    Williams et al., 2021 A.J. Williams, M.F. Torquato, I.M. Cameron, A.A. Fahmy,
    J. Sienz Survey of energy harvesting technologies for wireless sensor networks
    IEEE Access, 9 (2021), pp. 77493-77510 CrossRefView in ScopusGoogle Scholar Woo
    et al., 2013 S. Woo, et al. Comparison of caching strategies in modern cellular
    backhaul networks 11th Annual International Conference on Mobile Systems, Applications,
    and Services (2013), pp. 319-332 CrossRefView in ScopusGoogle Scholar Wu, 2021
    Y. Wu Cloud-edge orchestration for the internet of things: architecture and AI-powered
    data processing IEEE Internet of Things Journal, 8 (16) (2021), pp. 12792-12805
    CrossRefView in ScopusGoogle Scholar Wu et al., 2016 J. Wu, K. Ota, M. Dong, C.
    Li A hierarchical security framework for defending against sophisticated attacks
    on wireless sensor networks in smart cities IEEE Access, vol. 4 (2016), pp. 416-424
    View in ScopusGoogle Scholar Wu et al., 2017 Q. Wu, G.Y. Li, W. Chen, D.W.K. Ng,
    R. Schober An overview of sustainable green 5G networks IEEE Wireless Communications,
    24 (4) (2017), pp. 72-80 View in ScopusGoogle Scholar Wu et al., 2020 D. Wu, J.
    Liu, Z. Yang Bilateral satisfaction aware participant selection with MEC for mobile
    crowd sensing IEEE Access, vol. 8 (2020), pp. 48110-48122 CrossRefView in ScopusGoogle
    Scholar Wu et al., 2021a H. Wu, K. Wolter, P. Jiao, Y. Deng, Y. Zhao, M. Xu EEDTO:
    an energy-efficient dynamic task offloading algorithm for blockchain-enabled IoT-edge-cloud
    orchestrated computing IEEE Internet of Things Journal, 8 (4) (2021), pp. 2163-2176
    CrossRefView in ScopusGoogle Scholar Wu et al., 2021b Q. Wu, S. Zhang, B. Zheng,
    C. You, R. Zhang Intelligent reflecting surface-aided wireless communications:
    a tutorial IEEE Transactions on Communications, 69 (5) (2021), pp. 3313-3351 CrossRefView
    in ScopusGoogle Scholar Wu et al., 2021c T. Wu, W. Jing, X. Wen, Z. Lu, S. Zhao
    A scalable computation offloading scheme for MEC based on graph neural networks
    2021 IEEE Globecom Workshops (GC Wkshps) (2021), pp. 1-6 Madrid, Spain Google
    Scholar Wu et al., 2021d Y. Wu, K. Zhang, Y. Zhang Digital twin networks: a survey
    IEEE Internet of Things Journal, vol. 8 (2021), pp. 13789-13804 no. 18 CrossRefView
    in ScopusGoogle Scholar Wu et al., 2022a Y. Wu, H.-N. Dai, H. Wang, Z. Xiong,
    S. Guo A survey of intelligent network slicing management for industrial IoT:
    integrated approaches for smart transportation, smart energy, and smart factory
    IEEE Communications Surveys & Tutorials, 24 (2) (2022), pp. 1175-1211 CrossRefView
    in ScopusGoogle Scholar Wu et al., 2022b B. Wu, X. Chen, L. Jiao Collaborative
    computing based on truthful online auction mechanism in internet of things H.
    Gao, X. Wang (Eds.), Collaborative Computing: Networking, Applications and Worksharing.
    CollaborateCom 2021, Lecture Notes of the Institute for Computer Sciences, Social
    Informatics and Telecommunications Engineering, vol. 407, Springer, Cham (2022)
    Google Scholar Wu et al., 2022c C.-W. Wu, et al. Enhancing fan engagement in a
    5G stadium with AI-based technologies and live streaming IEEE Systems Journal,
    vol. 16 (2022), pp. 6590-6601 no. 4 CrossRefView in ScopusGoogle Scholar Xia et
    al., 2019 L. Xia, M. Zhao, Z. Tian 5G service based core network design 2019 IEEE
    Wireless Communications and Networking Conference Workshop (WCNCW), Marrakech,
    Morocco (2019), pp. 1-6 Google Scholar Xia et al., 2021 S. Xia, Z. Yao, Y. Li,
    S. Mao Online distributed offloading and computing resource management with energy
    harvesting for heterogeneous MEC-enabled IoT IEEE Transactions on Wireless Communications,
    20 (10) (2021), pp. 6743-6757 CrossRefView in ScopusGoogle Scholar Xia et al.,
    2023 C. Xia, Z. Jin, J. Su, B. Li Mobility-aware offloading and resource allocation
    strategies in MEC network based on game theory Wireless Communications & Mobile
    Computing, 2023 (2023) Google Scholar Xiao et al., 2020 Z. Xiao, et al. Vehicular
    Task Offloading via Heat-Aware MEC Cooperation Using Game-Theoretic method IEEE
    Internet Things J., 7 (3) (March 2020), pp. 2038-2052 CrossRefView in ScopusGoogle
    Scholar Xiao et al., 2023a Y. Xiao, S.A. Tegos, P.D. Diamantoulakis, Z. Ma, G.K.
    Karagiannidis On the ergodic rate of cognitive radio inspired uplink multiple
    access IEEE Communications Letters, vol. 27 (2023), pp. 95-99 no. 1 View in ScopusGoogle
    Scholar Xiao et al., 2023b Xiao, et al. Multi-round auction-based resource allocation
    for edge computing: maximizing social welfare Future Generation Computer Systems,
    140 (2023), pp. 365-375 View PDFView articleCrossRefGoogle Scholar Xie et al.,
    2023 D. Xie, J. Yang, W. Bian, F. Chen, T. Wang An improved identity-based anonymous
    authentication scheme resistant to semi-trusted server attacks IEEE Internet of
    Things Journal, vol. 10 (2023), pp. 734-746 no. 1 Google Scholar Xu, 2022 M. Xu
    FedDBG: privacy-preserving dynamic benchmark gradient in federated learning against
    poisoning attacks 2022 International Conference on Networking and Network Applications
    (NaNA) (2022), pp. 483-488 Urumqi, China CrossRefView in ScopusGoogle Scholar
    Xu and Ren, 2020 Q. Xu, P. Ren Delay-aware secure transmission in MEC-enabled
    multicast network 2020 IEEE/CIC International Conference on Communications in
    China (ICCC) (2020), pp. 1262-1267 Chongqing, China CrossRefView in ScopusGoogle
    Scholar Xu et al., 2018 P. Xu, J. Su, Z. Zhang Distributed hybrid cloud management
    platform based on rule engine 2018 IEEE 11th International Conference on Cloud
    Computing (CLOUD) (2018), pp. 836-839 San Francisco, CA, USA CrossRefView in ScopusGoogle
    Scholar Xu et al., 2020 Y. Xu, K. Zhu, S. Li Hierarchical combinatorial auction
    in computing resource allocation for mobile blockchain Wireless Communications
    and Mobile Computing, 2020 (2020) Google Scholar Xu et al., 2021a Y. Xu, G. Gui,
    H. Gacanin, F. Adachi A survey on resource allocation for 5G heterogeneous networks:
    current research, future trends, and challenges IEEE Communications Surveys &
    Tutorials, 23 (2) (2021), pp. 668-695 CrossRefView in ScopusGoogle Scholar Xu
    et al., 2021b Y. Xu, B. Gu, R.Q. Hu, D. Li, H. Zhang Joint computation offloading
    and radio resource allocation in MEC-based wireless-powered backscatter communication
    networks IEEE Transactions on Vehicular Technology, 70 (6) (2021), pp. 6200-6205
    CrossRefView in ScopusGoogle Scholar Xu et al., 2022 M. Xu, et al. Learning-based
    Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing
    (2022) arXiv preprint. arXiv: 2211.06681 Google Scholar Xu et al., 2023 Y. Xu,
    B. Qian, K. Yu, T. Ma, L. Zhao, H. Zhou Federated learning over fully-decoupled
    RAN architecture for two-tier computing acceleration IEEE Journal on Selected
    Areas in Communications, vol. 41 (2023), pp. 789-801 no. 3 CrossRefView in ScopusGoogle
    Scholar Xue et al., 2017 N. Xue, H. Haugerud, A. Yazidi On automated cloud bursting
    and hybrid cloud setups using Apache Mesos 2017 3rd International Conference of
    Cloud Computing Technologies and Applications (CloudTech), Rabat, Morocco (2017),
    pp. 1-8 CrossRefView in ScopusGoogle Scholar Xue et al., 2020 H. Xue, B. Huang,
    M. Qin, H. Zhou, H. Yang Edge computing for internet of things: a survey, 2020
    International Conferences on Internet of Things (iThings) and IEEE Green Computing
    and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)
    and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)
    (2020), pp. 755-760 Rhodes, Greece CrossRefView in ScopusGoogle Scholar Xue et
    al., 2022a M. Xue, H. Wu, R. Li, M. Xu, P. Jiao EosDNN: an efficient offloading
    scheme for DNN inference acceleration in local-edge-cloud collaborative environments
    IEEE Transactions on Green Communications and Networking, 6 (1) (2022), pp. 248-264
    CrossRefView in ScopusGoogle Scholar Xue et al., 2022b M. Xue, H. Wu, G. Peng,
    K. Wolter DDPQN: an efficient DNN offloading strategy in local-edge-cloud collaborative
    environments IEEE Transactions on Services Computing, vol. 15 (2022), pp. 640-655
    no. 2 CrossRefView in ScopusGoogle Scholar Yan et al., 2016 L. Yan, X. Fang, G.
    Min, Y. Fang A low-latency collaborative HARQ scheme for control/user-plane decoupled
    railway wireless networks IEEE Transactions on Intelligent Transportation Systems,
    vol. 17 (2016), pp. 2282-2295 no. 8 View in ScopusGoogle Scholar Yang et al J.
    Yang et al., \"A parallel intelligence-driven resource scheduling scheme for digital
    twins-based intelligent vehicular systems,\" in IEEE Transactions on Intelligent
    Vehicles. Google Scholar Yang et al., 2019a Q. Yang, et al. Computation offloading
    for fast CNN inference in edge computing, RACS ''19: Proceedings of the Conference
    on Research in Adaptive and Convergent Systems (2019), pp. 101-106 CrossRefGoogle
    Scholar Yang et al., 2019b Z. Yang, M. Chen, W. Saad, W. Xu, M. Shikh-Bahaei Sum-rate
    maximization of uplink rate splitting multiple access (RSMA) communication 2019
    IEEE Global Communications Conference (GLOBECOM) (2019), pp. 1-6 Waikoloa, HI,
    USA View in ScopusGoogle Scholar Yang et al., 2019c S.-R. Yang, Y.-J. Tseng, C.-C.
    Huang, W.-C. Lin Multi-access edge computing enhanced video streaming: proof-of-concept
    implementation and prediction/QoE models IEEE Transactions on Vehicular Technology,
    vol. 68 (2019), pp. 1888-1902 no. 2 CrossRefView in ScopusGoogle Scholar Yang
    et al., 2020a Z. Yang, Y. Liu, Y. Chen, N. Al-Dhahir Cache-aided NOMA mobile edge
    computing: a reinforcement learning approach IEEE Transactions on Wireless Communications,
    19 (10) (2020), pp. 6899-6915 CrossRefView in ScopusGoogle Scholar Yang et al.,
    2020b C. Yang, Z. Shi, H. Zhang, J. Wu, X. Shi Multiple attacks detection in cyber-physical
    systems using random finite set theory IEEE Transactions on Cybernetics, vol.
    50 (2020), pp. 4066-4075 no. 9 CrossRefView in ScopusGoogle Scholar Yang et al.,
    2021a X. Yang, H. Luo, Y. Sun, J. Zou, M. Guizani Coalitional game-based cooperative
    computation offloading in MEC for reusable tasks IEEE Internet of Things Journal,
    8 (16) (2021), pp. 12968-12982 CrossRefView in ScopusGoogle Scholar Yang et al.,
    2021b Z. Yang, M. Chen, W. Saad, M. Shikh-Bahaei Optimization of rate allocation
    and power control for rate splitting multiple access (RSMA) IEEE Transactions
    on Communications, 69 (9) (2021), pp. 5988-6002 CrossRefView in ScopusGoogle Scholar
    Yang et al., 2022a S. Yang, L. Jiao, R. Yahyapour, J. Cao Online orchestration
    of collaborative caching for multi-bitrate videos in edge computing IEEE Transactions
    on Parallel and Distributed Systems, 33 (12) (2022), pp. 4207-4220 CrossRefView
    in ScopusGoogle Scholar Yang et al., 2022b Z. Yang, M. Chen, W. Saad, W. Xu, M.
    Shikh-Bahaei Sum-rate maximization of uplink rate splitting multiple access (RSMA)
    communication IEEE Transactions on Mobile Computing, 21 (7) (2022), pp. 2596-2609
    View in ScopusGoogle Scholar Yang et al., 2022c X. Yang, W. Luo, L. Zhang, Z.
    Chen, J. Wang Data leakage attack via backdoor misclassification triggers of deep
    learning models 2022 4th International Conference on Data Intelligence and Security
    (ICDIS) (2022), pp. 61-66 Shenzhen, China View in ScopusGoogle Scholar Yannuzzi
    et al., 2017 M. Yannuzzi, et al. Toward a converged OpenFog and ETSI MANO architecture
    2017 IEEE Fog World Congress (FWC) (2017), pp. 1-6 Santa Clara, CA, USA View in
    ScopusGoogle Scholar Yaqoob et al., 2016 I. Yaqoob, et al. Mobile Ad Hoc Cloud:
    A Survey,” Wireless Communications & Mobile Computing (2016) Google Scholar Ye
    et al., 2020 H.-B. Ye, H.-Y. Weng, H.C. Jiau Exploiting third-party SDK sensitive
    data leakage 2020 International Computer Symposium (ICS) (2020), pp. 485-490 Tainan,
    Taiwan CrossRefView in ScopusGoogle Scholar Yılmaz and Özbek, 2023 S.S. Yılmaz,
    B. Özbek Massive MIMO-NOMA based MEC in task offloading for delay minimization
    IEEE Access, 11 (2023), pp. 162-170 CrossRefView in ScopusGoogle Scholar You et
    al., 2022 W. You, C. Dong, Q. Wu, Y. Qu, Y. Wu, R. He Joint task scheduling, resource
    allocation, and UAV trajectory under clustering for FANETs China Communications,
    vol. 19 (2022), pp. 104-118 no. 1 CrossRefView in ScopusGoogle Scholar Yu, 2022a
    N.Y. Yu Performance analysis of signature-based grant-free random access under
    impersonation attacks IEEE Access, vol. 10 (2022), pp. 72925-72935 CrossRefView
    in ScopusGoogle Scholar Yu, 2022b J. Yu Design of location security protection
    system based on internet of things 2022 4th International Conference on Applied
    Machine Learning (ICAML) (2022), pp. 273-276 Changsha, China CrossRefView in ScopusGoogle
    Scholar Yu et al., 2016 H.-F. Yu, C.-J. Hsieh, H. Yun, S.V.N. Vishwanathan, I.
    Dhillon Nomadic computing for big data analytics Computer, 49 (4) (2016), pp.
    52-60 View in ScopusGoogle Scholar Yu et al., 2021 S. Yu, X. Chen, Z. Zhou, X.
    Gong, D. Wu When deep reinforcement learning meets federated learning: intelligent
    multitimescale resource management for multiaccess edge computing in 5G ultradense
    network IEEE Internet of Things Journal, vol. 8 (2021), pp. 2238-2251 no. 4 CrossRefView
    in ScopusGoogle Scholar Yuan et al., 2022 X. Yuan, J. Chen, N. Zhang, J. Ni, F.R.
    Yu, V.C.M. Leung Digital twin-driven vehicular task offloading and IRS configuration
    in the internet of vehicles IEEE Transactions on Intelligent Transportation Systems,
    23 (12) (2022), pp. 24290-24304 CrossRefView in ScopusGoogle Scholar Zeng et al.,
    2019a Y. Zeng, A. Al-Quzweeni, T.E.H. El-Gorashi, J.M.H. Elmirghani Energy efficient
    virtualization framework for 5G F-RAN 2019 21st International Conference on Transparent
    Optical Networks (ICTON) (2019), pp. 1-4 Angers, France Google Scholar Zeng et
    al., 2019b J. Zeng, T. Lv, W. Ni, R.P. Liu, N.C. Beaulieu, Y.J. Guo Ensuring max–min
    fairness of UL SIMO-NOMA: a rate splitting approach IEEE Transactions on Vehicular
    Technology, 68 (11) (2019), pp. 11080-11093 CrossRefView in ScopusGoogle Scholar
    Zeng et al., 2020 R. Zeng, S. Zhang, J. Wang, X. Chu FMore: an incentive scheme
    of multi-dimensional auction for federated learning in MEC 2020 IEEE 40th International
    Conference on Distributed Computing Systems (ICDCS) (2020), pp. 278-288 Singapore,
    Singapore CrossRefView in ScopusGoogle Scholar Zeydan et al., 2022 E. Zeydan,
    J. Mangues-Bafalluy, J. Baranda, M. Requena, Y. Turk Service based Virtual RAN
    Architecture for Next Generation Cellular Systems IEEE Access, 10 (2022), pp.
    9455-9470 CrossRefView in ScopusGoogle Scholar Zhang and Fu, 2020 Y. Zhang, J.
    Fu Efficient computation offloading in mobile edge computing based on dynamic
    programming 2020 Chinese Automation Congress (CAC) (2020), pp. 1381-1385 Shanghai,
    China CrossRefView in ScopusGoogle Scholar Zhang and Li, 2019 H. Zhang, Z. Li
    Anomaly detection approach for urban sensing based on credibility and time-series
    analysis optimization model IEEE Access, vol. 7 (2019), pp. 49102-49110 CrossRefView
    in ScopusGoogle Scholar Zhang and Li, 2022 H. Zhang, M. Li Multi-round data poisoning
    attack and defense against truth discovery in crowdsensing systems 2022 23rd IEEE
    International Conference on Mobile Data Management (MDM) (2022), pp. 109-118 Paphos,
    Cyprus CrossRefView in ScopusGoogle Scholar Zhang et al., 2013 Y. Zhang, C. Lee,
    D. Niyato, P. Wang Auction Approaches for Resource Allocation in Wireless Systems:
    a Survey IEEE Communications Surveys & Tutorials, 15 (3) (Third Quarter 2013),
    pp. 1020-1041 View in ScopusGoogle Scholar Zhang et al., 2017a H. Zhang, F. Guo,
    H. Ji, C. Zhu Combinational auction-based service provider selection in mobile
    edge computing networks IEEE Access, vol. 5 (2017), pp. 13455-13464 View in ScopusGoogle
    Scholar Zhang et al., 2017b X. Zhang, Q. Jia, L. Guo Secure and optimized unauthorized
    secondary user detection in dynamic spectrum access 2017 IEEE Conference on Communications
    and Network Security (CNS) (2017), pp. 1-9 Las Vegas, NV, USA View PDFView articleGoogle
    Scholar Zhang et al., 2018 B.-Y. Zhang, C.-Z. Wei, X.-H. Yang, B.-B. Song Design
    and implementation of a network based intrusion detection systems 2018 International
    Conference on Security, Pattern Analysis, and Cybernetics (SPAC) (2018), pp. 451-454
    Jinan, China CrossRefView in ScopusGoogle Scholar Zhang et al., 2019 Z. Zhang,
    S. Yoon, M. Shin The design of graph-based privacy protection mechanisms for mobile
    systems 2019 International Conference on Platform Technology and Service (PlatCon)
    (2019), pp. 1-6 Jeju, Korea (South) Google Scholar Zhang et al., 2020a L. Zhang,
    J. Hao, G. Zhao, M. Wen, T. Hai, K. Cao Research and application of AI services
    based on 5G MEC in smart grid 2020 IEEE Computing, Communications and IoT Applications
    (ComComAp) (2020), pp. 1-6 Beijing, China View PDFView articleGoogle Scholar Zhang
    et al., 2020b Y. Zhang, J.-H. Liu, C.-Y. Wang, H.-Y. Wei Decomposable intelligence
    on cloud-edge IoT framework for live video analytics IEEE Internet of Things Journal,
    vol. 7 (2020), pp. 8860-8873 no. 9 CrossRefView in ScopusGoogle Scholar Zhang
    et al., 2020c Y. Zhang, J. Xiao, S. Hao, H. Wang, S. Zhu, S. Jajodia Understanding
    the manipulation on recommender systems through web injection IEEE Transactions
    on Information Forensics and Security, vol. 15 (2020), pp. 3807-3818 CrossRefView
    in ScopusGoogle Scholar Zhang et al., 2020d Z. Zhang, H. Hasegawa, Y. Yamaguchi,
    H. Shimada Rogue AP detection using similarity of backbone delay fluctuation histogram
    2020 International Conference on Information Networking (ICOIN) (2020), pp. 239-244
    Barcelona, Spain CrossRefView in ScopusGoogle Scholar Zhang et al., 2021a K. Zhang,
    X. Gui, D. Ren, D. Li Energy–latency tradeoff for computation offloading in UAV-assisted
    multiaccess edge computing system IEEE Internet of Things Journal, 8 (8) (2021),
    pp. 6709-6719 CrossRefView in ScopusGoogle Scholar Zhang et al., 2021b Y. Zhang,
    S. Chen, X. Li, Y. Tang Design of high-power static wireless power transfer via
    magnetic induction: an overview CPSS Transactions on Power Electronics and Applications,
    6 (4) (2021), pp. 281-297 CrossRefGoogle Scholar Zhang et al., 2021c Z. Zhang,
    Y. Fu, G. Cheng, X. Lan, Q. Chen Secure offloading design in multi-user mobile-edge
    computing systems 2021 IEEE 6th International Conference on Computer and Communication
    Systems (ICCCS) (2021), pp. 695-703 Chengdu, China CrossRefView in ScopusGoogle
    Scholar Zhang et al., 2021d J. Zhang, B. Chen, X. Cheng, H.T.T. Binh, S. Yu PoisonGAN:
    generative poisoning attacks against federated learning in edge computing systems
    IEEE Internet of Things Journal, 8 (5) (2021), pp. 3310-3322 CrossRefView in ScopusGoogle
    Scholar Zhang et al., 2021e Z. Zhang, G. Huang, S. Hu, W. Zhang, Y. Wu, Z. Qin
    FDO-ABE: a fully decentralized lightweight access control architecture for mobile
    edge computing 2021 IEEE 6th International Conference on Computer and Communication
    Systems (ICCCS) (2021), pp. 193-198 Chengdu, China View in ScopusGoogle Scholar
    Zhang et al., 2021f Z. Zhang, Z. Xu, G. Wu, J. Wang, H. Ren, G. Yuan Trust-aware
    service chaining in mobile edge clouds with VNF sharing 2021 IEEE 24th International
    Conference on Computer Supported Cooperative Work in Design (CSCWD) (2021), pp.
    410-415 Dalian, China CrossRefGoogle Scholar Zhang et al., 2021g J. Zhang, M.
    Wu, Q. Zhang, C. Peng A lightweight data sharing scheme with resisting key abuse
    in mobile edge computing IEEE INFOCOM 2021 - IEEE Conference on Computer Communications
    Workshops (INFOCOM WKSHPS) (2021), pp. 1-6 Vancouver, BC, Canada Google Scholar
    Zhang et al., 2023 X. Zhang, et al. BiLSTM-based federated learning computation
    offloading and resource allocation algorithm in MEC ACM Transactions on Sensor
    Networks, 19 (3) (2023), pp. 1-20 Google Scholar Zhao et al., 2020 L. Zhao, G.
    Han, Z. Li, L. Shu Intelligent digital twin-based software-defined vehicular networks
    IEEE Network, 34 (5) (2020), pp. 178-184 CrossRefView in ScopusGoogle Scholar
    Zhao et al., 2021a F. Zhao, Y. Chen, Y. Zhang, Z. Liu, X. Chen Dynamic offloading
    and resource scheduling for mobile-edge computing with energy harvesting devices
    IEEE Transactions on Network and Service Management, 18 (2) (2021), pp. 2154-2165
    CrossRefView in ScopusGoogle Scholar Zhao et al., 2021b L. Zhao, G. Zhou, G. Zheng,
    C.-L. I, X. You, L. Hanzo Open-source multi-access edge computing for 6G: opportunities
    and challenges IEEE Access, 9 (2021), pp. 158426-158439 CrossRefView in ScopusGoogle
    Scholar Zhao et al., 2021c Y. Zhao, K. Xu, H. Wang, B. Li, M. Qiao, H. Shi MEC-enabled
    hierarchical emotion recognition and perturbation-aware defense in smart cities
    IEEE Internet of Things Journal, 8 (23) (2021), pp. 16933-16945 CrossRefView in
    ScopusGoogle Scholar Zhao et al., 2021d G. Zhao, F. Zhang, L. Yu, H. Zhang, Q.
    Qiu, S. Xu Collaborative 5G multiaccess computing security: threats, protection
    requirements and scenarios 2021 ITU Kaleidoscope: Connecting Physical and Virtual
    Worlds (ITU K) (2021), pp. 1-8 Geneva, Switzerland View in ScopusGoogle Scholar
    Zheng and Yan, 2022 W. Zheng, L. Yan Latency minimization for IRS-assisted mobile
    edge computing networks Physical Communication, 53 (2022) Google Scholar Zheng
    et al., 2021 G. Zheng, C. Xu, H. Long, X. Zhao MEC in NOMA-HetNets: a joint task
    offloading and resource allocation approach 2021 IEEE Wireless Communications
    and Networking Conference (WCNC) (2021), pp. 1-6 Nanjing, China Google Scholar
    Zhong et al., 2021 S. Zhong, S. Guo, H. Yu, Q. Wang Cooperative service caching
    and computation offloading in multi-access edge computing Computer Networks, 189
    (2021) Google Scholar Zhou et al Y. Zhou et al., \"Joint optimization for cooperative
    computing framework in double-IRS-aided MEC systems,\" in IEEE Wireless Communications
    Letters. Google Scholar Zhou et al., 2019a Z. Zhou, X. Chen, E. Li, L. Zeng, K.
    Luo, J. Zhang Edge Intelligence: Paving the Last Mile of Artificial Intelligence
    with Edge Computing Proc. IEEE, 107 (8) (August 2019), pp. 1738-1762 CrossRefView
    in ScopusGoogle Scholar Zhou et al., 2019b W. Zhou, Y. Jia, A. Peng, Y. Zhang,
    P. Liu The Effect of IoT New Features on Security and Privacy: New Threats, Existing
    Solutions, and Challenges yet to Be Solved IEEE Internet Things J., 6 (2) (April
    2019), pp. 1606-1616 CrossRefView in ScopusGoogle Scholar Zhou et al., 2021a Q.
    Zhou, et al. On-device learning systems for edge intelligence: a software and
    hardware synergy perspective IEEE Internet of Things Journal, 8 (15) (2021), pp.
    11916-11934 CrossRefView in ScopusGoogle Scholar Zhou et al., 2021b X. Zhou, W.
    Liang, J. She, Z. Yan, K.I.-K. Wang Two-layer federated learning with heterogeneous
    model aggregation for 6G supported internet of vehicles IEEE Transactions on Vehicular
    Technology, vol. 70 (2021), pp. 5308-5317 no. 6 CrossRefView in ScopusGoogle Scholar
    Zhou et al., 2022 C. Zhou, J. Gao, M. Li, X. Sherman Shen, W. Zhuang Digital twin-empowered
    network planning for multi-tier computing Journal of Communications and Information
    Networks, 7 (3) (2022), pp. 221-238 CrossRefView in ScopusGoogle Scholar Zhu et
    al., 2015 C. Zhu, V.C.M. Leung, L.T. Yang, L. Shu Collaborative location-based
    sleep scheduling for wireless sensor networks integrated with mobile cloud computing
    IEEE Transactions on Computers, 64 (7) (2015), pp. 1844-1856 View in ScopusGoogle
    Scholar Zhu et al., 2022 Y. Zhu, B. Mao, N. Kato A dynamic task scheduling strategy
    for multi-access edge computing in IRS-aided vehicular networks IEEE Transactions
    on Emerging Topics in Computing, 10 (4) (2022), pp. 1761-1771 CrossRefView in
    ScopusGoogle Scholar Zou and Zhu, 2016 Y. Zou, J. Zhu Intercept probability analysis
    of joint user-Jammer selection against eavesdropping 2016 IEEE Global Communications
    Conference (GLOBECOM) (2016), pp. 1-6 Washington, DC, USA View in ScopusGoogle
    Scholar Cited by (2) RCFS: rate and cost fair CPU scheduling strategy in edge
    nodes 2024, Journal of Supercomputing Personalized medicine through quantum computing:
    Tailoring treatments in healthcare 2023, Quantum Innovations at the Nexus of Biomedical
    Intelligence Mobasshir Mahbub received the MSc Engineering degree in Electrical
    and Electronic Engineering under the Department of Electrical and Electronic Engineering,
    Ahsanullah University of Science and Technology, Dhaka, Bangladesh. He graduated
    (September 2018) in Electronic and Telecommunication Engineering under the Department
    of Electronics and Communications Engineering, East West University, Dhaka, Bangladesh
    with an outstanding academic performance (Merit Scholarship and Dean''s List Award).
    His fields of interest are IoT, 5G/B5G/6G Wireless Communications, UAV-Aided Communication,
    Multi-Tier Heterogeneous Network, Multi-Access Edge Computing, NOMA, etc. He is
    currently working as a Graduate Research Assistant at the Department of Electrical
    and Computer Engineering, New York University (NYU) Abu Dhabi, UAE on a research
    project relative to the implementation of IRS technology for 6G. Moreover, He
    is also working as a Research Assistant at the Department of Electrical and Electronic
    Engineering, Ahsanullah University of Science and Technology, Dhaka, Bangladesh
    on a research project relative to the LiFi technology. He collaborated with the
    Department of Electrical and Computer Engineering, New York University (NYU) Abu
    Dhabi, UAE; Department of Electrical Engineering, Jouf University, Saudi Arabia;
    Department of Computer Science and Engineering, Independent University Bangladesh
    (IUB), Dhaka, Bangladesh; Department of Electrical and Electronic Engineering,
    University of Liberal Arts Bangladesh (ULAB), Dhaka, Bangladesh. He served as
    a Project Engineer, in Transport Network Rollout Department under the Technology
    Division of Robi Axiata Ltd., a renowned telecom operator of Bangladesh. He authored
    four book chapters (in Springer, IGI Global), eighteen (first author in seventeen
    papers) research papers published in reputed international journals (in Elsevier,
    Springer, EAI), and twenty two (first author in twenty papers) conference papers
    published in reputed international conferences (IEEE Sponsored) including IEEE
    SECON Workshops. He received ‘Best Presenter Award’ in ‘Mobile Communication’
    track of IEMTRONICS 2022, Toronto, Canada and ‘Best Paper Award’ in ‘Network Architecture’
    track of IEEE UEMCON 2021, New York, USA for his research papers. He is a Graduate
    Student member of IEEE. He is a member of IEEE Computer Society, Bangladesh Chapter.
    He is serving as a voluntary editor for IEEE Communications Society (ComSoc) since
    2019. He is also serving as a member of several technical committees of several
    IEEE communities and societies. He served as a member of technical program committee
    and reviewer for 5th and 6th IEEE FMEC Conference, Paris, France, 2020 and 2021.
    He served as a keynote speaker in several workshops, seminars, and webinars on
    Microcontroller Interfacing, Embedded System, IoT, and Robotics (EWU, ULAB, and
    AUST). He is serving as a reviewer for journals of reputed publishers such as
    Springer-Nature, IEEE, Elsevier, IET, ACM, Hindawi, Wiley, Taylor and Francis,
    De Gruyter, IOP, MDPI, Tech Science Press, EAI, and Emerald and served as reviewer
    for several international conferences. He is currently engaged in research on
    6G transmission analysis. Raed M. Shubair (Senior Member, IEEE) received the B.Sc.
    degree (Hons.) in electrical engineering from Kuwait University, Kuwait, in June
    1989, and the Ph.D. degree (Hons.) in electrical engineering from the University
    of Waterloo, Canada, in February 1993, for which he received the University of
    Waterloo Distinguished Doctorate Dissertation Award. He is currently a Full Professor
    of electrical engineering affiliated with New York University (NYU), Abu Dhabi.
    His current and past academic and research appointments also include Massachusetts
    Institute of Technology (MIT), Harvard University, and the University of Waterloo.
    He was a Full Professor of electrical engineering with Khalifa University (formerly,
    Etisalat University College), United Arab Emirates, from 1993 to 2017, during
    which he received several times the Excellence in Teaching Award and Distinguished
    Service Award. He has over 400 publications in the form of articles in peer-reviewed
    journals, papers in referred conference proceedings, book chapters, and U.S. patents.
    His publication span several research areas, including 6G and terahertz communications,
    modern antennas and applied electromagnetics, signal and array processing, machine
    learning, the IoT and sensor localization, medical sensing, and nano-biomedicine.
    He is a fellow of MIT Electromagnetics Academy and a Founding Member of MIT Scholars
    of the Emirates. He is a standing member of the editorial boards of several international
    journals and serves regularly for the steering, organizing, and technical committees
    of IEEE flagship conferences in Antennas, Communications, and Signal Processing,
    including several editions of IEEE AP-S/URSI, EuCAP, IEEE GlobalSIP, IEEE WCNC,
    and IEEE ICASSP. He is the General Chair of IEEE WCNC 2024. He is also a Board
    Member of European School of Antennas and the Regional Director of the IEEE Signal
    Processing Society in IEEE Region 8 Middle East. He is a Founding Member of five
    IEEE society chapters in United Arab Emirates, which are the IEEE Communication
    Society Chapter, the IEEE Signal Processing Society Chapter, the IEEE Antennas
    and Propagation Society Chapter, the IEEE Microwave Theory and Techniques Society
    Chapter, and the IEEE Engineering in Medicine and Biology Society Chapter. He
    was a recipient of several international awards, including the Distinguished Service
    Award from ACES Society, USA, and MIT Electromagnetics Academy, USA. He organized
    and chaired numerous technical special sessions and tutorials in IEEE flagship
    conferences. He delivered more than 60 invited speaker seminars and technical
    talks in world-class universities and flagship conferences. He has served as the
    TPC Chair for IEEE MMS2016 and IEEE GlobalSIP 2018 Symposium on 5G satellite networks.
    He served as the Founding Chair for the IEEE Antennas and Propagation Society
    Educational Initiatives Program. He is the Founder and the Chair of IEEE at New
    York University at Abu Dhabi. He is an officer of IEEE ComSoc Emerging Technical
    Initiative (ETI) on Machine Learning for Communications. He is the Founding Director
    of the IEEE UAE Distinguished Seminar Series Program for which he was selected
    to receive, along with Mohamed AlHajri of MIT, the 2020 IEEE UAE Award of the
    Year. He served as an Invited Speaker for U.S. National Academies of Sciences,
    Engineering, and Medicine Frontiers Symposium. He holds several leading roles
    in the international professional engineering community. He is also an Editor
    of IEEE Journal of Electromagnetics, RF and Microwaves in Medicine and Biology,
    and IEEE Open Journal of Antennas and Propagation. View Abstract © 2023 Elsevier
    Ltd. All rights reserved. Recommended articles BAC-CRL: Blockchain-Assisted Coded
    Caching Certificate Revocation List for Authentication in VANETs Journal of Network
    and Computer Applications, Volume 218, 2023, Article 103716 Junwei Liang, …, Dongsheng
    Cheng View PDF TTAF: A two-tier task assignment framework for cooperative unit-based
    crowdsourcing systems Journal of Network and Computer Applications, Volume 218,
    2023, Article 103719 Bo Yin, …, Sai Tang View PDF Scalable inter-domain network
    virtualization Journal of Network and Computer Applications, Volume 218, 2023,
    Article 103701 Jie Sun, …, Jianwei Niu View PDF Show 3 more articles Article Metrics
    Citations Citation Indexes: 1 Captures Readers: 45 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Journal of Network and Computer Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Contemporary advances in multi-access edge computing: A survey of fundamentals,
    architecture, technologies, deployment cases, security, challenges, and directions'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sun D.
  - Hu J.
  - Wu H.
  - Wu J.
  - Yang J.
  - Sheng Q.Z.
  - Dustdar S.
  citation_count: '1'
  description: The scope of the Industrial Internet of Things (IIoT) has stretched
    beyond manufacturing to include energy, healthcare, transportation, and all that
    tomorrow's smart cities will entail. The realm of IIoT includes smart sensors,
    actuators, programmable logic controllers, distributed control systems (DCS),
    embedded devices, supervisory control, and data acquisition systems - all produced
    by manufacturers for different purposes and with different data structures and
    formats; designed according to different standards and made to follow different
    protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous
    data, and how can we uniformly structure them to suit thousands of different applications?
    In this article, we survey the four pillars of information science that enable
    collaborative data access in an IIoT - standardization, data acquisition, data
    fusion, and scalable architecture - to provide an up-to-date audit of current
    research in the field. Here, standardization in IIoT relies on standards and technologies
    to make things communicative; data acquisition attempts to transparently collect
    data through plug-and-play architectures, reconfigurable schemes, or hardware
    expansion; data fusion refers to the techniques and strategies for overcoming
    heterogeneity in data formats and sources; and scalable architecture provides
    basic techniques to support heterogeneous requirements. The article also concludes
    with an overview of the frontier researches and emerging technologies for supporting
    or challenging data access from the aspects of 5G, machine learning, blockchain,
    and semantic web.
  doi: 10.1145/3612918
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Journal Home Just Accepted Latest
    Issue Archive Authors Editors Reviewers About Contact Us HomeACM JournalsACM Computing
    SurveysVol. 56, No. 2A Comprehensive Survey on Collaborative Data-access Enablers
    in the IIoT SURVEY SHARE ON A Comprehensive Survey on Collaborative Data-access
    Enablers in the IIoT Authors: Danfeng Sun , Junjie Hu , Huifeng Wu , Jia Wu ,
    + 3 Authors Info & Claims ACM Computing SurveysVolume 56Issue 2Article No.: 50pp
    1–37https://doi.org/10.1145/3612918 Published:15 September 2023Publication History
    0 citation 682 Downloads eReaderPDF ACM Computing Surveys Volume 56, Issue 2 Previous
    Next Abstract 1 INTRODUCTION 2 STANDARDIZATION IN THE IIOT 3 DATA ACQUISITION
    4 DATA FUSION 5 SCALABLE ARCHITECTURE 6 EMERGING FIELDS AND THEIR CHALLENGES IN
    CDA 7 CONCLUSION REFERENCES Index Terms Recommendations Comments Skip Abstract
    Section Abstract The scope of the Industrial Internet of Things (IIoT) has stretched
    beyond manufacturing to include energy, healthcare, transportation, and all that
    tomorrow’s smart cities will entail. The realm of IIoT includes smart sensors,
    actuators, programmable logic controllers, distributed control systems (DCS),
    embedded devices, supervisory control, and data acquisition systems—all produced
    by manufacturers for different purposes and with different data structures and
    formats; designed according to different standards and made to follow different
    protocols. In this sea of incompatibility, how can we flexibly acquire these heterogeneous
    data, and how can we uniformly structure them to suit thousands of different applications?
    In this article, we survey the four pillars of information science that enable
    collaborative data access in an IIoT—standardization, data acquisition, data fusion,
    and scalable architecture—to provide an up-to-date audit of current research in
    the field. Here, standardization in IIoT relies on standards and technologies
    to make things communicative; data acquisition attempts to transparently collect
    data through plug-and-play architectures, reconfigurable schemes, or hardware
    expansion; data fusion refers to the techniques and strategies for overcoming
    heterogeneity in data formats and sources; and scalable architecture provides
    basic techniques to support heterogeneous requirements. The article also concludes
    with an overview of the frontier researches and emerging technologies for supporting
    or challenging data access from the aspects of 5G, machine learning, blockchain,
    and semantic web. Skip 1INTRODUCTION Section 1 INTRODUCTION The Industrial Internet
    of Things (IIoT) is a significant and powerful driving force behind the fourth
    industrial revolution. Through a massive number of heterogeneous sensing devices,
    each with the ability to calculate, communicate, and collaborate, IIoT is bringing
    interconnectivity and intelligence to industrial systems. For engineers and data
    scientists, IIoT is an exciting, challenging, and inescapable aspect of research.
    Figure 1 illustrates the breadth of sectors IIoT is benefiting, ranging from smart
    factories and logistics to smart cities and transportation. Table 1 summarizes
    the many surveys that have been undertaken on different aspects of IIoT. Table
    1. Application Field Reference Contributions Description O D F C T K A Intelligent
    manufacturing Zheng et al. [267] ✓ – – ✓ ✓ – ✓ Intelligent manufacturing, or smart
    factory, is viewed as the fourth revolution in the manufacturing industry. Lu
    et al. [146] – ✓ – ✓ ✓ – ✓ Mabkhot et al. [152] ✓ ✓ ✓ ✓ – ✓ ✓ Show More Table
    1. Surveys of IIoT Application Fields O = Origin, D = Definition, F = Fundamentals,
    C = Challenges, T = Trends, K = Key Technologies, A = Applications. Fig. 1. Fig.
    1. Three-tier IIoT system architecture: (1) edge tier: monitoring, control; (2)
    platform tier: data collection and conversion; (3) enterprise tier: business and
    application domains (analytics, management). Viewed as the fourth revolution in
    manufacturing, intelligent manufacturing is at the forefront of IIoT. Through
    integration and interoperability, factories are becoming smart. Devices such as
    programmable logic controllers (PLC) and embedded controllers are being combined
    with sensor networks and cloud-based control systems [267]. This enables data
    exchanging and knowledge sharing between two systems [146], as well as seamless
    operations across organizational boundaries. IIoT is also making energy production,
    delivery, and usage more efficient. With the benefit of smart sensors, network
    devices, and data analytics, energy companies are able to monitor asset performance
    and investigate accidents remotely and safely [11, 58, 61, 148]. In healthcare,
    IIoT is showing enormous promise as a way for medical professionals to monitor,
    aggregate, and act on patients’ medical data in real-time [86, 123]. IIoT is also
    enabling demand-based production lines for factory and laboratory processes like
    drug packing [228]. In logistics and supply chains, IIoT-enabled technologies
    mean products and materials can be tracked in real-time. This tracking data help
    enterprises to manage their warehouses and transport fleets, as well as optimize
    their processes [24, 237]. In terms of transportation, IIoT is improving vehicle-to-vehicle
    communications and remote monitoring for smart cars [125], smart trains [65],
    unmanned aerial vehicles (UAVs) [10], and so on. In addition to networking, these
    applications are helping to reduce maintenance costs, fuel consumption, and accident
    response times. However, the shining jewel of IIoT is the smart city, where physical
    infrastructure, services, and networking all integrate to make almost every element
    of a city more intelligent and more efficient [255]. As just a few examples, IIoT
    is making possible: structural health monitoring, pipeline network monitoring,
    camera surveillance networks, urban traffic monitoring, smart grid monitoring,
    and streetlight monitoring [54]. Importantly, it is not just academic research
    into IIoT that is thriving. Many commercial enterprises are also building IIoT
    platforms to capitalize on this new revolution, both horizontal and vertical.
    Vertical platforms focus on providing a suite of services to support an entire
    field from the ground up. The success of these types of platforms often depends
    heavily on the host’s competitive advantages and experience in the field. Examples
    include MindSphere [206] from Siemens, EcoStructure from Schneider Electric [197],
    and ABB Ability [5]. Horizontal platforms are a more ambitious endeavor, providing
    more generalized applications to suit the requirements across a range of industries.
    At this stage, these platforms are mainly driven by huge cloud service providers,
    such as the AWS IoT [13] by Amazon and Azure IoT [161] by Microsoft. The drawback
    of horizontal platforms is that many small and medium-sized enterprises (SMEs)
    are required to do outsourcing work due to their lack of specific industry experience.
    GE’s Predix is a typical case of where a horizontal transformed into a vertical
    platform. Indeed, the road ahead for IIoT is bright. However, as with all maturing
    technologies, there are still challenges to overcome. At present, IIoT is on the
    precipice of explosive growth. The four main issues slowing take-off are: (1)
    The complexity of industrial plants and equipment. The sheer range of equipment
    and countless combinations between them turns digitizing any factory into a huge
    project. (2) The heterogeneity of data, data sources, and data collection protocols.
    Every machine, equipment controller, and sensor manages its data in a different
    way, and although there are many conversion tools, exchange frameworks, and international
    standards for compatibility, the conventions are far from universal. Getting all
    the machines and their manufacturers to a point where implementation is transparently
    “plug and play” has long been a challenge facing the industry. (3) The collecting
    data needs to be pre-processed to a required structure before it can be used for
    analytics. However, the edge devices cannot know the actual data source mapping
    to many dynamic applications, so it is impossible to pre-determine which data
    preprocessing program the edge device should apply. A method of quickly constructing
    a data pre-processing logic according to the needs of the scenario at hand is
    urgently needed, as is a strategy for deploying diverse and personalized intelligent
    services for different edge devices. (4) Traditional computing architectures are
    straining. A few years ago, the expectation was that all IoT would move toward
    cloud-based data centers, and those cloud centers would be sufficiently powerful
    to handle all IoT needs [187]. However, while many consumer-level IoT systems
    can benefit from this scheme, most end-point devices in an IIoT application demand
    much lower latency than a central server can provide. In industrial applications,
    computing generally needs to happen much closer to the sensor or device producing
    the data. Edge computing is an emerging computing architecture that allows for
    this [33, 203, 238], but more solutions that integrate cloud computing and edge
    computing need to be designed and applied [68]. Compared to cloud-only solutions,
    hybrid approaches can provide more efficient and flexible computing with lower
    latency and high resource utilization to support the fast decision and response
    times required for industrial applications. In addition, scalability is critical
    to IIoT architectures, as the exponential growth of data generated by connected
    devices and industry requires scalable resource management and deployment technologies
    that can scale horizontally to meet customer needs. Around these issues, we investigated
    many past surveys and research articles. We list the high-quality surveys from
    2018 in Table 2 and separated them into several research directions as their authors
    claimed, namely, edge computing, identity resolution system, interoperability,
    network slicing, security, privacy, blockchain, digital twin, deep learning, 5G,
    fault diagnosis, and additive manufacturing. Except for a few earlier review articles
    [122, 208] using a comprehensive perspective, other surveys provide quality introductions
    to their specific directions and throughout the state-of-the-art literature as
    well as the existing commercial platforms. We have an insight that the key to
    the above-mentioned issues is data. A more precise explanation is to focus on
    the enablers for the common stages of the data flow while optimizing the usage
    of edge-cloud resources and integrating emerging techniques. We conclude it as
    collaborative data access (CDA) that focuses on existing promising research of
    data awareness, data acquisition, data fusion, data security, and scalable architecture
    to alleviate the above issues, respectively. The state-of-the-art research lacks
    a comprehensive survey on the collaboration required across the five areas to
    address the issue of accessing massive and heterogeneous industrial data. Table
    2 compares the difference between the enabled CDA and other recent surveys. Table
    2. Directions Surveys and their relevance to the collaborative data access Points
    of concern Comprehensiveness Sisinni et al. [208] compared the IoT, IIoT, Industry
    4.0, and CPS in detail around interoperability, security and privacy, highlighting
    the opportunities and challenges. While exhibiting some overlap with the CDA,
    there is no explicit concentration on a specific topic. • Energy efficiency •
    Real-time performance • Coexistence • Interoperability • Security and privacy
    Khan et al. [122] added the discussion of enabling technologies for IIoT compared
    with Reference [208], no explicit focus on a specific topic, but pointed out the
    challenge of the collaborative IIoT and involved blockchain and data fusion, which
    have some hints for the CDA. • Cloud computing • Artificial Intelligence • Blockchain
    • Big Data • Augmented and virtual reality Edge computing Qiu et al. [183] discussed
    concepts, architectures, applications, challenges, and opportunities of edge computing
    under the context of the IIoT, which is different from the CDA, which emphasizes
    the process of data flow and leverages the cloud and edge resources. • Routing
    • Task scheduling • Data storage and analytics • security • Standardization Show
    More Table 2. Comments on High-quality Surveys from the Perspective of IIoT Research
    Directions Hence, our focus is the current research into tools, methods, and technologies
    that enable CDA for massive industrial applications, especially large-scale edge
    networks. Through our investigation, we also aim to provide research directions
    for future studies in this field. Based on our investigation into CDA-related
    fields, we described the current research status using progressive statements
    of problems and corresponding solutions. We also identified unresolved issues,
    providing research directions for future studies in this field. We have divided
    the review into six sections, each representing a key scientific issue. These
    are standardization in the IIoT, data acquisition, data fusion, scalable architecture,
    and emerging fields, as shown in Figure 2. Before a thing in an IIoT system can
    send or receive data, it must be aware of what and where other things in the system
    are. The servers and edge devices must also be aware of how to process all the
    data produced by the end units. The process of awareness is standardization, which
    is the first step in enabling CDA. These issues are detailed in Section 2. Section
    3 concerns data acquisition, which includes the methods used to connect to and
    communicate with entities in the network. Data fusion refers to the techniques
    and strategies for overcoming heterogeneity in data formats and sources [119,
    262]. Recent developments in this quarter are reviewed in Section 4. Scalable
    architecture, discussed in Section 5, is the scaffolding of CDA and customized
    applications in a scalable way. In addition to the pressing issues facing researchers,
    we have included a survey of emerging technologies in the field in Section 6.
    As potential accelerators of CDA, the topics include 5G, machine learning, semantic
    web, and blockchain. The article concludes in Section 7 with a brief summary of
    the topics covered. Fig. 2. Fig. 2. The structure of this article. Skip 2STANDARDIZATION
    IN THE IIOT Section 2 STANDARDIZATION IN THE IIOT Today, communication protocols,
    interfaces, and standards developed by many different equipment manufacturers
    are the cause of high complexity, and solving these problems is the main goal
    of different international initiatives. As just a few examples of these consortiums,
    in the United States, the Industrial Internet Consortium (IIC) is committed to
    achieving universal interconnection methods and intelligent analysis [44]. In
    China, the “Made in China 2025” plans to make a manufacturing revolution upon
    information and communication technology (ICT) [243]. In Germany, the “Industry
    4.0” (I4.0) initiatives focus on the future of manufacturing through industrial
    ICT [198]. The main standardization result of I4.0 is its reference architecture
    model, known as RAMI 4.0 [234]. To support the goals of different international
    initiatives, standardization is essential [176]. The technical specifications
    defined by the consensus are recognized standards that ensure uniformity and encourage
    interoperability, simplifying the work of stakeholders. Due to the wide variety
    of standards in IIoT, this article focuses on standards related to CDA. International
    Electrotechnical Commission (IEC) is the earliest organization participating in
    standardization work. IEC/TC65 is one of their groups, which works on measurement
    and process automation and is the core of the I4.0 International Standardization
    Technical Committee [147]. While Technical Committee 184 from International Organization
    for Standardization (ISO) makes efforts on automation systems and integration
    [1]. Virtualizing physical assets, as shown in Figure 3, is the basis for data
    access. This section introduces virtual objects and their common terminology standards.
    In addition, this article divides the application fields of IIoT general standards
    and technical standards into common terminology, perception layer, transmission
    layer, and application layer, which are mapped to the traditional IIoT structure
    [28]. Fig. 3. Fig. 3. Standardization in the IIoT. 2.1 Virtual Object Describing
    highly heterogeneous physical assets is essential for better access to them, i.e.,
    virtual representations of industrial assets. Standard data formats for describing
    virtual objects include eCl@ss or IEC Common Data Dictionary (CDD), which can
    be specified by the standard IEC 61360 [259]. A typical example of a virtual interface
    application for physical assets is the Asset Management Shell (AAS) in I4.0. The
    following is an example of how to virtualize objects in I4.0: Figure 3 shows the
    form of the AAS, which can be divided into digital factories (DF) Header and Body,
    and DF corresponds to the concept of DF framework in IEC 62832 [223]. DF Header
    includes properties that identify the physical asset and AAS itself as one uniquely
    I4.0 component [166]. This properties list is called a manifest, which is a set
    of defined meta-information that can be accessed from the outside [220]. DF Body
    inherits many basic models considering specific domain knowledge, such as communication,
    configuration, energy efficiency, engineering, identification, life cycle status,
    status monitoring, and safety and security [254]. These models in different fields
    need to be formulated and maintained independently by corresponding experts. Each
    model contains well-structured attributes linked to related data or functions.
    Such attributes are regarded as a list in the DF body that complies with a standard
    data format. DF body also has a component manager used to manage and access services
    in each sub-model. In addition, virtual interfaces generally provide communication
    interfaces that allow external components to access data and functions, ensuring
    continuous access to operational data generated by the physical asset, such as
    bearing weights and linear axis speeds [256]. 2.2 Common Terminology The basic
    requirement for representing an object (industrial asset) in the information world
    is to describe the attributes [267], such as “height,” “length,” or “width.” In
    the IIoT environment, the attributes of these industrial assets are stored in
    an ontology and described in general terms. IEC 61360, 61987, and 62683 are the
    main standards used to describe the attributes of industrial assets. Among them,
    the IEC61360 issued by IEC SC3D and the ISO 13584 issued by ISO TC184/SC4 jointly
    developed the PLIB standard, which provides a common method for describing the
    asset ontology [95]. The description method defines different types of information
    about the supplier library, such as the resource structure and information model
    of the asset, the characteristic level and attribute principle of the asset. These
    regulations represent assets independently of any particular vendor-defined identifier.
    As shown in Figure 3, the shared dictionary defined by PLIB has been shared as
    the basis of many asset ontologies, such as measuring instruments, mechanical
    fasteners, optics and photonics, electronic and electronic components, factory
    equipment, and environmental declarations [50, 117]. Some industry associations
    in the private sector also maintain common data dictionaries in a PLIB-compatible
    form, such as eCl@ss in Europe, JEITA/ECALS in Japan, and EdF/Renault/PSA trio
    in France. IEC61360-1 details the structure and use of the asset ontology, also
    known as the “reference data dictionary.” IEC61360-2 specifies the dictionary
    data in more detail, and IEC61360-6 specifies the quality standards for the dictionary’s
    content [224]. Note that the data models defined in IEC61360-2 are also published
    in ISO13584-42. There are currently many noteworthy studies related to the PLIB
    standard. Pethig et al. [175] proposed an information model with an AAS for monitoring
    the condition of servo motors based on IEC 61360 attributes. The system was implemented
    with PLCs. By using IEC 61360-compliant attributes, the system could automatically
    identify the type of servo and configure the appropriate safety thresholds [231].
    From an analysis of the potential of ISO 13584, Leukel et al. [136] concluded
    that PLIB standards were a good choice for product data management. 2.3 Perception
    Layer The perception layer involves the definitions and explanations of data sources
    (sensors, field devices, etc.) for collecting data and knowledge from our physical
    world [106]. Moreover, the perception layer depends on data acquisition and location-sensing
    technologies such as sensors, quick response codes, and RFID, namely, automatic
    identification and data capture (AIDC). AIDC technology solves the issue of real-time
    access to information and is widely used in the field of IIoT. Therefore, there
    are many standards for AIDC technology. For example, ISO/IEC 19762:2016 defines
    and explains the terms in automatic identification technology. TransducerML, SensorML,
    and TEDS provide definitions for sensors and RFID in the AIDC method and are common
    technical standards for sensor data exchange. As shown in Figure 3, ISO and IEC
    provide a series of standards to enable factories to build more reliable and smarter
    sensor networks [208]. These standards include definitions and requirements for
    functional layer interfaces and entity-oriented interfaces of smart sensors, as
    well as methods for cooperative exchange of messages (such as ISO/IEC 29182, ISO/IEC
    20005, and ISO/IEC 30101) [96, 100, 101]. In addition, RFID, as a method of automatic
    identification and data collection, can recognize multiple objects moving at high-speed
    even in harsh environments without manual intervention [192]. RFID technology
    has also been used to locate and track humans, even when they are not carrying
    an electronic device [190, 252]. ISO/IEC 20248 specifies the digital signatures
    RFID needs to identify and authenticate some data, its source, and the proper
    method of reading them [97]. ISO/IEC 15459 and ISO/IEC 18000 series provide registration
    procedures and numbering systems for RFID. In addition, standards such as ISO/IEC
    29143 and ISO/IEC 29175 are related to mobile AIDC and RFID and support real-time
    data capture at the perception layer [98, 99]. 2.4 Transmission Layer The transmission
    layer is the messenger connecting layers of perception and application through
    communication networks that are divided into wired and wireless communication
    networks. IEC 61158 and IEC 61784 series standards in the wired communication
    network support fieldbus configuration and configuration files. For example, ISO
    15745-3:2003, based on IEC 61158, describes the communication network configuration
    file and the equipment configuration file of the control system based on IEC 61158;
    ISO 15745-1:2003 specifies the general elements and rules used to describe the
    integration model and application interoperability profile and its component profile
    (process profile, information exchange profile, and resource profile) [230]. The
    technical standards of wired communication include IP, VPN, Controller Area Network
    bus (CANBUS), ControlNet, DeviceNet, ModBus, ProfiBus, WorldFIP, Industrial Ethernet,
    RS232, RS485, and so on. Among them, standards such as ProfiNet, EtherCat, and
    Powerlink are designed for real-time distributed connections. IEC 62591 and 62601
    standards are used for industrial wireless transmission, which mainly standardizes
    wireless communications in process automation. The ISO/IEC 8802 series are telecommunications
    and exchange between information systems, that is, wired or wireless transmission
    of local and metropolitan area networks. Technical standards for wireless communication
    include spatially limited networks (e.g., WiFi, Zigbee, Bluetooth, 6LoWPAN, HomeRF,
    Near Field Communication, and Z-Wave) and 3GPP 2—5th generation mobile communication
    technology. 2.5 Application Layer The application layer related to CDA undertakes
    tasks of calculation, data processing, and mining to realize adaptive control,
    lean management, and intelligent decision-making. The standards in this layer
    can be classified as data, software framework, and communication configuration
    standards. Data standards describe device information, interface information,
    and configuration information, such as describing which functional blocks exist
    in the device type, available parameters, the data types of these parameters,
    and allowable ranges of these parameters. Data technical standards include electronic
    device description language (EDDL), Field Device Tool/Device Type Manager, M2MXML,
    and so on. The EDDL technical standard attempts to build a uniform engineering
    environment that supports various devices produced by any supplier. As an outside-the-square
    example of EDDL’s usefulness, Banerjee et al. [21] used it to describe databases
    and files, modeling them as “devices” with their own power to communicate. Software
    framework standards describe software component specifications. Technical standards
    include Master Data Management (MDM), service-oriented architecture, Open Service
    Gateway Initiative, software as a service (SaaS), and so on. IEC 62453 series
    are used to standardize the configuration of the communication interfaces between
    field devices and control systems, which can configure the communication configuration
    files of ControlNet, EtherNet/IP, and DeviceNet. IEC 61804 series are responsible
    for standardizing communication characteristics of smart field instruments and
    device parameters. 2.6 Summary and Lessons With the development of IIoT, many
    industry standards have been developed and iterated to improve uniformity and
    interoperability for CDA. Equipment, suppliers, factories, and production lines
    are developed under common and technical standards. This article maps the application
    areas of IIoT general standards and technical standards to the four-layer structure
    of terminology-perception-transmission-application. The virtual and digital representation
    of physical assets with AAS is the basis for data access. Its structure is divided
    into DF Header and DF Body. Among them, the DF Body contains attributes linked
    to various data or functions, and such attributes must follow the standard data
    format. The standard data format is described by common terminology defined by
    PLIB. The common terminology defined by PLIB has been used in many fields, such
    as measuring instruments, mechanical fasteners, optics and photonics, electronic
    and electronic components, factory equipment, and environmental declarations.
    The standards in the perception layer are related to the construction of data
    sources (sensors, Radio Frequency Identification (RFID) systems, etc.), which
    are also the underlying standards for IIoT data access. The standards in the transmission
    layer are used to standardize communication networks, e.g., WirelessHART and WIA-PA,
    which establish a bridge for its upper and lower layers. The application layer
    standard is the key to encouraging device interoperability, such as IEC 61804
    (EDDL). All the above standards ensure that CDA is performed in a uniform and
    standardized manner across different layers. Skip 3DATA ACQUISITION Section 3
    DATA ACQUISITION IIoT captures data from industrial assets such as machines and
    industrial equipment in assembly lines. In the real world, these industrial assets
    will have different types, makes, and models of equipment [34]. In addition, the
    production time and operating environment will be different. Data acquisition
    refers to obtaining data from the entities (industrial assets) in the network
    that are used for interconnection, data exchange, and communication through different
    interfaces and protocols. In the IIoT environment, data comes from different types
    of devices and represents billions of objects [9]. Newer industrial assets typically
    have easy-to-obtain mechanisms for data acquisition, while older assets do not.
    Older industrial assets lack networking and data management capabilities, so obtaining
    data from them poses even greater challenges. To obtain data from these industrial
    assets, we must first understand the functions of the assets and determine the
    correct parameters that need to be measured according to business needs. After
    the parameters are determined, the data can be obtained from different types of
    data sources. Some common data types and sources are: – Simulate sensor data,
    such as pressure, temperature, flow, liquid level, and other data. – Binary sensor
    data (on/off) from proximity, level, limit, pressure switch, and so on. – Sensor
    data through field protocols such as Modbus and CANBUS. Modbus is a commonly used
    serial communications protocol in industrial applications. CANBUS is a standard
    protocol that is widely used in the automotive and heavy equipment industry. –
    Wireless sensor data from sensors through Bluetooth and wireless HART. – Data
    from PLC, SCADA systems. – Data from equipment controllers such as engine control
    units and dedicated controllers used for operating equipment. – Data from 3rd
    party cloud platforms through application programming interfaces. – Data from
    the web, such as weather and air quality. – Data from enterprise systems (asset
    master data, maintenance data, spares data). In an industrial environment, enterprise
    systems include enterprise resource planning systems, maintenance management systems,
    spare parts management systems, and so on. 3.1 Data Types in IIoT The acquired
    data can be roughly divided into the following types: – Direct Sensor Data Acquisition:
    It is about acquiring data from standard sensors such as pressure, temperature,
    flow, proximity, level, and so on. In particular, sensor data and wireless sensor
    data acquired through the field protocols also belong to it. Besides, derived
    measurements are done if direct measurements are not possible due to physical,
    technological, or cost constraints. This is done by measuring a suitable proxy
    parameter and deriving the target parameter from this proxy parameter. – Data
    from Industrial Control Systems: PLC, DCS, programmable automation controllers,
    computerized numerical controllers, motion controllers, and SCADA systems are
    some of the most commonly used Industrial Control Systems (ICSs). OPC UA, EthernetIP,
    Profinet, Modbus, and serial interfaces are some common interfaces for obtaining
    data from ICSs. Although interfaces such as OPC UA, EthernetIP, and Profinet are
    used in the newer versions of ICSs, older legacy systems still use serial interfaces
    and Modbus. – Environment data: It is mainly produced by enterprise systems and
    includes natural and human environment data. Natural environment data is about
    the description of the environment and their combination and processing data,
    such as machine ID and machine type. Human environment data involves people in
    managing production, such as order information. The following will introduce three
    methods for solving the above-mentioned data collection problems: plug-and-play
    architecture, reconfigurable scheme, and hardware expansion. 3.2 Plug-and-play
    Architecture Plug-and-play architecture is an important solution to the problem
    of direct sensor data acquisition. Especially the problem of wireless sensor data
    collection. To support plug-and-play architecture, the industry has proposed a
    unified family of standards—the Institute of Electrical and Electronics Engineers
    (IEEE) 1451 [211]. IEEE 1451 is a set of smart sensor interface standards for
    connecting sensors, actuators, microprocessors, and/or instrument systems in a
    control or field network through a standard set of communication interfaces [180,
    211]. Many researchers have used this system in their designs for applications
    such as environment monitoring systems [128, 130, 235], precision agriculture
    [62, 232], and intelligent vehicles [127, 135]. Q. Chi et al. [42] adopted a complex
    programmable logic device (CPLD) as the main controller of an IoT environment
    and proposed a new method for designing reconfigurable smart sensor interfaces
    for industrial wireless sensor networks (WSNs). The device combines the latest
    CPLD programmable technology with IEEE 1451.2 smart sensor specification standards.
    Bordel et al. [26] realized a certain degree of multi-source sensor data collection
    based on the IEEE 1451 standards by designing a configurable multi-sensor acquisition
    interface. IEEE 1451 has been applied as a standard interface to an environmental
    monitoring system [129], a green building [154], and digital twins [210]. Both
    Cherian et al. [41] and Guevara et al. [76] used standardized transducer electronic
    data sheets (TEDS) specified in terms of IEEE 1451.2 in their applications to
    do with data perception. The definition of various transducers through these TEDS
    is a key element of the IEEE 1451 standards. TEDS contains the information required
    to act as an interface between a measuring instrument, like a sensor, and the
    control system. Sometimes, they are embedded on a storage device connected to
    the sensor, but they can also reside in embedded memory on the sensor itself,
    such as on an EEPROM [41]. The Open Geospatial Consortium (OGC) develops the Sensor
    Web Enablement standards for building a sensor web to discover and use sensors
    flexibly [27]. Based on these standards, Bröring et al. [29] developed a sensor
    plug-and-play infrastructure to integrate sensors on-the-fly with minimal human
    intervention. Therefore, although some researchers draw on IEEE 1451.4 [140],
    others have turned to the OGC [27] and still others have turned to TEDS [41, 42,
    76], especially analog transducer users in this latter case. These plug-and-play
    standard solutions can help a lot to solve the direct sensor data acquisition
    problem. However, the plug-and-play architecture has a limited scope of application.
    3.3 Reconfigurable Scheme The reconfigurable scheme is mainly used to solve the
    collection problem of data from equipment and data from ICSs. It also supports
    sensor data from field protocols. The reconfigurable scheme utilizes the system
    characteristics of programmable to realize a universal data acquisition system.
    For a universal data acquisition system, in the face of many data items that are
    very different from each other and tightly coupled to a specific system, a manually
    coded protocol parser is difficult to reuse. Liu et al. [141] provided a framework
    for automatic online application protocol field extraction that can be parsed
    from context-sensitive application protocols and can also generate optimizations
    based on simple extraction specifications. Adesina et al. [7] and Graham et al.
    [73] selected a deterministic finite-state automaton as the general protocol analysis
    engine. Some single-chip computers are also used in universal acquisition systems
    due to their programmability, such as field programmable gate array (FPGA), Raspberry
    Pi, and PLC. Different ways of using FPGA hardware to collect data have been explored
    in many studies [22, 67, 82, 126, 179]. One of the most difficult problems to
    address has been overcoming the fact that every device manufacturer designs and
    uses its own bespoke protocol with no concern for compatibility or communication
    with out-of-house units. Bao et al.’s [22] solution to this issue is a reconfigurable
    data acquisition system for industrial sensors that uses an FPGA as the core controller.
    Wu et al. [239] also proposed a dynamic data access system but with embedded PLCs
    that have flexible hardware and software structures [240, 241]. The universal
    data acquisition system makes use of configurability to realize the parsing of
    various field protocols. However, programmable devices still have resource and
    performance limitations. 3.4 Hardware Expansion The last of the three approaches
    is to extend a device’s physical interface with a specific fieldbus. In fact,
    most manufacturers offer specialized hardware extension schemes. Schneider’s TM3
    Bus Coupler series serves as an instance of hardware extension, aiming to improve
    the performance of the Modicon M221, M241, and M251 logic controllers. Alternatively,
    the Schneider STBXCA100X series is a Modicon STB distributed I/O solution, divided
    into different products according to length. Siemens provides proprietary modules
    SIMATIC IoT2000 series to extend their product lines to the IIoT environments.
    Omron’s CP1W units can expand I/O units, LCs, and any other devices via Ethernet
    [170, 205]. Omron introduces EtherCAT as the machine control network. EtherCAT
    can adopt linear, star, and ring topologies, and the possible expansion on each
    node helps to achieve the maximum network flexibility of machine design [170].
    However, hardware expansion cannot be used as a general solution for IIoT because
    of its high price and physical inflexibility. 3.5 Summary and Lessons As shown
    in Table 3, the methods of collecting data are mainly divided into plug-and-play
    architecture, reconfigurable scheme, and hardware expansion. Among them, plug-and-play
    architecture is a way to address the issue of direct sensor data acquisition,
    especially the wireless sensor data collection problem. Its advantage is that
    IEEE 1451 standardizes the plug-and-play interfaces for data acquisition. However,
    the disadvantage is that it is not applicable to data sources such as ICSs. Reconfigurable
    schemes are mainly used to solve the collection problem of data from equipment
    and data from ICSs. In particular, it also supports sensor data from field protocols
    in direct sensor data acquisition. The programmability of some single-board computers
    or microcontrollers (e.g., PLC, FPGA, and Raspberry Pi) is used to implement a
    reconfigurable system, which can be used as a general data acquisition system.
    However, programmable devices still have resource and performance limitations.
    Hardware extension is a physical extension method proposed by many manufacturers,
    which can be applied to collect data from various data sources. However, hardware
    expansion has the disadvantages of high price and insufficient flexibility. Table
    3. Solution Author Application Types Description Limitation S E I Plug-and-play
    Architecture Kumar et al. [129] Environment monitoring system ✓ ✓ - IEEE 1451
    standardizes the plug-and-play interfaces for data acquisition. Configurable interfaces
    that comply with IEEE 1451 only work with some types of sensors. Wei et al. [235]
    Precision agriculture ✓ – – Lee et al. [135] Intelligent vehicles ✓ – ✓ Show More
    Table 3. Data Acquisition S = Direct Sensor Data Acquisition, E = Data from Equipment,
    I = Data from ICSs. Skip 4DATA FUSION Section 4 DATA FUSION Data fusion refers
    to the techniques and strategies for overcoming heterogeneity in data formats
    and sources. The heterogeneity in data can take two forms: the data format, such
    as text data, relational data, image data, and audio and video data [262]; or
    its structure, namely, structured, semi-structured, or unstructured data [119].
    In industrial environments, and in big data generally, most data are unstructured.
    They have vagueness, which means the information suggests little about what it
    conveys [262]. To illustrate, the word “wind” on its own is vague. It could be
    a noun or a verb; it could relate to air, a path, a piece of string, a stomach
    pain. Without context or further information, we cannot know what this word is
    intended to convey. For our purposes, we define heterogeneous data as data from
    different sensor sources, with different collection periods or types but that
    relate to each other. Data fusion technology structures and integrates heterogeneous
    data from different sources to improve its comprehensibility, accessibility, and
    extensibility. This is an active area of research for both academia and industry,
    with the main spheres of inquiry being compressed storage, fault prediction, location,
    and status assessment. Descriptions and some representative examples of these
    research themes follow. Storage compression refers to reducing the size of the
    data and tries not to lose useful information. For example, in pursuit of more
    efficient storage solutions, Park et al. [174] applied machine learning to process
    data into a vector representation of only a single formula. Cheng et al. [39]
    proposed a lossy fast lifting wavelet transform to compress distributed sensing
    data for large-scale WSNs. This strategy largely reduced the amount of data that
    needed to be transmitted. Fault prediction is used to detect faults by specific
    sensors. Here, Zhou et al. [269] devised a multi-sensor global feature extraction
    method for monitoring the condition of various tools in a milling process based
    on kernel extreme learning machine and a genetic algorithm, while Zhou et al.
    [268] fused a multimodal feature data to extract abnormal frequency features for
    rotating machinery diagnosis. Location studies focus on identifying the precise
    location of objects. Zhang et al. [261] developed a localized fingerprint technology
    based on heterogeneous feature fusion and the time-of-arrival feature to enhance
    the precision and robustness of indoor positioning. Alatis et al. [12] proposed
    a method of estimating the position and orientation of mobile robots based on
    the fusion of inertial sensor data and vision. Status assessment is to give a
    comprehensive evaluation from many indicators. In this stream, Ha et al. [78]
    proposed an air quality assessment system that merges many indoor air quality
    index, such as CO, P M 2.5 , and S O 2 . Sun et al. [214] proposed an intelligent
    data fusion framework to compute the health status of a bridge according to an
    index. The framework comprises a hybrid neural network built on adaptive resonant
    theory and an adaptive fuzzy inference system. In real-world scenarios, deploying
    an efficient data fusion framework not only requires an appropriate algorithm
    but also a systematic framework to support data collection, analyses, and decision
    intelligence. Table 4 lists several surveys on data fusion applications and frameworks.
    In this section, we provide examples of contemporary data fusion in terms of three
    frameworks: edge-based, cloud-based, and collaborative frameworks. Note that collaborative
    frameworks focus more on cooperatively processing data compared with edge-based
    and cloud-based ones. Table 4. C1 Task/Type Author and one-sentence summary Advantages
    P E S Y L M C Application Compression Park et al. [174]: A machine learning-based
    method to process industrial data into a representative vector represented only
    by one formula for reducing storage consumption – – – ✓ – ✓ – Bardwaj et al. [23]:
    A method of compressing data at each local sensor to improve the accuracy of fusion
    estimation – – – ✓ – ✓ – Cheng et al. [39]: A distributed fast lifting wavelet
    based on the lossy data compression algorithm – – – ✓ – ✓ – Show More Table 4.
    Surveys in Data Fusion Application and Framework C1 = Category, P = Precision,
    E = Energy, S = Safety, Y = Yield (efficiency), L = Latency, M = Memory (Storage),
    C = Computing. 4.1 Edge-based Frameworks Izadi et al. [102] proposed a fusion
    approach based on fuzzy logic, which is applied in a WSN attempting to improve
    the QoS and reduce the energy consumption of the edge WSN. The strategy is to
    reduce the amount of data transmitted through fuzzy rules that distribute and
    collect only true values. This eliminates redundant data, which consequently reduces
    energy consumption and increases the lifespan of the WSN. Yang et al. [251] argue
    that transferring data from a sensor node to the cloud server without prior processing
    raises several concerns, including high latency, privacy leaks, and excessive
    bandwidth consumption. To address these issues, they devised a method of fusing
    temporal data based on a Gaussian process. Essentially, the methods make predictions
    from time-series data streams at the edge device so only the temporal features
    of the stream need to be transferred between the cloud and edge. Larras et al.
    [132] suggest shifting the small-scale classification and feature extraction parts
    of the processing to the sensors using clique-based neural networks (CBNNs) as
    a generic classifier. CBNNs allow the method to be divided into clusters and dispatched
    to different nodes of a WSN, which would help to reduce communications overheads
    as well as ensure privacy. Qian et al.’s [181] solution for a system of rotating
    machines involves real-time fault diagnosis and dynamic controllers deployed on
    edge computing nodes. Most of the data are computed on electronic communications
    networks (ECNs), where the controllers can modify the operational parameters if
    needed. Only important data are sent to the server for further analysis, which
    reduces storage and computing requirements on the cloud. The features are extracted
    via a fast Fourier transform and fused with a classical backpropagation neural
    network. 4.2 Cloud-based Frameworks Due to the constrained resources of edge devices,
    data fusion methods performed at the edge tend to be fixed, uncomplex, and non-interactive.
    Hence, most complex data fusion tasks with changing demands are either performed
    by a central server or designed for centralized computing architectures. Mu et
    al. [202] designed an intelligent transportation cyber-physical cloud control
    system, where dynamic information from heterogeneous sensors is sent to a central
    control platform. The data processing and analysis are fused within an intelligent
    transportation network, where the prediction results and control schemes are generated
    simultaneously and sent to the terminal systems to support unified monitoring,
    management, decision-making, and control services. Hao et al. [244] claim that
    data throughput and computing power in traditional information fusion systems
    cannot meet the demands of future competition. By traditional, they are referring
    to systems based on a single platform with a small number of sensors. As such,
    this article puts forward a design and a highly detailed workflow plan for an
    information fusion system based on cloud computing with more resilience and greater
    capacity for data processing. Liu et al. [142] present an architecture for teaching
    cloud-based robotic systems how to navigate. The learning scheme follows lifelong
    federated reinforcement learning (LFRL) to support reducing training time without
    sacrificing accuracy. With LFRL, robots in different environments can fuse and
    share models to help each other learn more quickly. To overcome issues with high
    latency, many researchers have turned to cluster frameworks, such as Spark [258]
    and Hadoop [204]. These packages offer as near to real-time cloud computing as
    is currently possible and are, therefore, very popular. Sirisakdiwan et al. [207]
    claim that, at present, there are many problems with multi-stream applications,
    including difficulties with deployment and monitoring, redundant coding, and inefficient
    job queueing. To overcome these issues, they propose a Spark streaming framework
    that supports the deployment of multiple heterogeneous streams in a single Spark
    application. They use the case of anomaly detection in a network to evaluate the
    effects of fusing all these streams. Stojanovic et al. [212] developed a software
    platform, RemoteHealth, to provide support for the collection, fusion, processing,
    and analysis of large-scale data. Their application also involved human activity
    recognition on a dedicated server within the RemoteHealth platform, all based
    on Apache Spark. 4.3 Collaborative Frameworks Although the Spark framework allows
    data fusion with real-time streams, many researchers implement these tasks on
    a distributed system for better performance. For example, Asif et al. [16] presented
    a five-layered Internet of Health Things (IoHT) framework, consisting of mist,
    fog, and cloud layers. The mist computing layer at the extreme edge of the network
    performs rapid rule-based preprocessing of the sensor data with basic tasks, such
    as data aggregation, fusion, or filtering, which is key to ensuring time-critical
    data processing. Wang et al. [233] claim that real-time monitoring is barely possible
    with a centralized cloud solution, because the data collected by a single edge
    node is insufficient to accurately describe the state of an environment. Their
    solution is a fog-based environmental monitoring framework, which can fuse multi-source
    heterogeneous data locally. A cloud data training system is responsible for updating
    the fusing models in different edge nodes. However, rather than sending an entire
    model, the edge nodes only transmit the model parameters as an update, not the
    raw data. Arooj et al. [15] proposed a five-layer collaborative multimodal architecture.
    In particular, the third layer is dedicated to knowledge discovery with the collected
    data. They consider that an increased number of nodes results in higher communication
    overhead, so having one central server process all the data is neither feasible
    nor efficient. Their data fusion solutions are based on context descriptors, tensor
    decomposition, and semantic approaches. Valente et al. [226] implemented a container-based
    microservice to fuse multi-sensor data, which is configured as fog architecture
    and built from open-source IoT middleware. With this strategy, the scale of the
    system can adjust automatically when excessive data is input from the edge nodes.
    4.4 Summary and Lessons Collected data from the IIoT environment is massive and
    heterogeneous, which is challenging the data fusion for further processing. In
    terms of the applications, the fusion can be performed in three frameworks: edge-based,
    cloud-based, and collaborative. The edge-based framework is suitable for applications
    with a small amount of data but real-time requirements. The cloud-based framework
    can process complex and high-computational data fusion but loss the real-time
    capability. Collaborative frameworks leverage both advantages of cloud and edge
    features to fuse data. As the last stage of data access, data fusion prepares
    well-structured data for further applications, and as a whole process, data acquisition
    and fusion or even preprocessing such as data cleaning can be done in one framework
    in a flexible way. For instance, the data stream cleaning system proposed by Sun
    et al. [215] executes the data collection, fusion, and cleaning sequentially and
    scalably, where the programs of the above three stages can be updated according
    to requirements. However, their scalable mechanism is not general. This is also
    the reason we investigate virtualization technology and provide a discussion in
    the following section. Skip 5SCALABLE ARCHITECTURE Section 5 SCALABLE ARCHITECTURE
    Factories add new production lines, and transportation networks add new hubs and
    vehicles to their fleets. So, too, IIoT architectures need to be able to scale
    to cater to this growth. Moreover, every new entity in the system is likely to
    come with its own data processing logic, and this growth in heterogeneity also
    needs to be accommodated. In this section, we review progress toward scalable
    architectures to support these requirements. Beginning at the dawn of networking
    with the first central server, we review the many and varied rules, methods, functions,
    and implementations of computer systems up to and including the virtual technology
    of today. 5.1 Evolution of Computing Architectures In the past few decades, various
    types of computing paradigms and architectures have emerged to suit almost every
    purpose under the sun. Supercomputing is the original purpose that originated
    from the invention of the first computer to the first supercomputer UNIVAC LARC
    in 1960. However, as impressive as these large proprietary mainframes with high-performance
    computing capabilities were, they could still only perform one task at a time.
    To perform multiple tasks, multiple systems were needed, each running in parallel.
    Hence, the 1970s saw the advent of cluster computing, open massively parallel
    processing, and symmetric multiprocessing. In the 1980s, grid computing emerged
    [80], and this concept dominated academia right through until the early 1990s
    [64]. It was in this decade that architectures truly began to diverge, especially
    in terms of data storage. Some frameworks focused on centralization, others on
    decentralization, but, over the next decade, we would witness the evolution of
    commodity clusters, peer-to-peer, web services, virtual clusters, HPC systems,
    laaS, PaaS, and SaaS. All these ideas would culminate in 2006 with the popular
    term “cloud computing” because of Amazon’s Elastic Compute Cloud product. In its
    initial conception, cloud computing was typically implemented as a joint hardware
    and software platform, designed to let businesses share resources and make high-priced
    computing services affordable to SMEs [113]. The next few years would see a tumult
    of innovation. The Internet of Things was born during this time as an interconnected
    system of edge devices with high data processing requirements [167], as were fog
    computing, edge computing, and edge-cloud collaborations. Fog computing, which
    appeared in 2011, uses terminal devices, i.e., fog nodes, to bear most of a system’s
    storage and calculation requirements and the internet as the communications medium
    [17]. Fog platforms include fog nodes and cloud servers. The fog node has a certain
    amount of calculation ability, which provides real-time and quality of service
    (QoS) services near the IIoT environment, while the cloud servers support the
    distributed fog nodes with elastic and huge computing power [25]. Edge computing
    surfaced next. Edge computing can be traced back to the 1990s, but it has been
    widely used recently due to the dramatic performance improvement of edge hardware.
    As a distributed computing paradigm, the edge devices collect the data and perform
    some or all of the calculations required before transmitting the results through
    the network [260]. Because much of the computing is done at the edge, these systems
    typically have lower latency, lower transmission costs, better response times,
    and better QoS than conventional cloud systems. For this reason, they are a popular
    choice for applications with large-scale mobile data [37, 193]. Time-sensitive
    applications can also benefit from edge computing, particularly with a highly
    reliable and available internet connection [209]. The major shortcoming of edge
    computing is that the edge devices are generally resource-constrained, which means
    their computing power is limited to simple, non-intensive tasks [203]. The current
    trend for integrating time-sensitive applications within the resource limitations
    of edge devices is an edge-cloud collaboration. These frameworks combine the real-time
    performance of edge computing with the powerful functions of cloud computing,
    giving play to the advantages of both paradigms. Currently, research interest
    in edge-cloud collaboration is high, which is resulting in very rapid developments
    in this area. However, in general, edge-cloud computing frameworks comprise the
    edge layer, the communications layer, and the cloud layer [35, 45, 52, 159, 182,
    208, 219, 229, 248]. Some architectures include an additional fog layer between
    the edge and the cloud. Gateways aggregate and transfer data from the physical
    devices in the edge layer, through the communications layer, to the cloud layer
    at the top. The connections between layers can be wired or wireless. The edge
    layer might include devices, such as sensors, controllers, and actuators; systems,
    such as supervisory control and data acquisition systems (SCADAs), DCS, and manufacturing
    execution systems (MES) [59, 159]. Example protocols used for the communication
    layer include Internet protocol version 6 over a low-power wireless personal area
    network (6LoWPAN), message queue telemetry transport (MQTT), extensible messaging
    and presence protocol (XMPP), advanced message queuing protocol (AMQP), data distribution
    service (DDS), constrained application protocol (CoAP), and open platform communications
    unified architecture (OPC UA) [59, 196]. Also, emerging technologies such as network
    virtualization, software-defined networking (SDN), and fifth generation mobile
    networks (5G) are playing important roles in this layer. 5.2 Virtualization Technology
    Virtualization is the key to achieving dynamic and scalable services. Virtualization
    technology is a kind of resource management technology that abstracts the physical
    resources of a computer (e.g., networks, memory, CPUs, storage, I/O equipment)
    in software manner, overcoming the limitation caused by the indivisibility of
    physical resources. Virtualization technology enables a single physical computer
    to be transformed into several logical computers, realizing dynamic scheduling
    and multi-scale sharing of physical resources, which in turn improves resource
    utilization. In its early form, virtualization simply meant dividing a system’s
    resources and purposing them to different applications. Today, virtualization
    can mean anything using software to mimic hardware to support larger and more
    complex applications. A list of studies on virtualization is provided in Table
    5. Table 5. C1 Methodologies Advantages Disadvantages Contributions Para-Virtualization
    Full J. Hwang et al. [94]: An ARN CPU architecture for virtualizing full systems
    and a prototype implementation of a hypervisor, called Xen. • High reliability
    • High performance Show More Table 5. Surveys in Virtualization Technology Virtualization
    technology mainly includes virtual machines, containers, and container orchestration
    engines. The types fall into two broad categories: full virtualization and para-virtualization
    [14]. Full virtualization manages virtual machines through an additional intermediate
    software layer between the virtual machine and the hardware called a hypervisor
    [188]. With full virtualization, the underlying hardware is fully simulated, which
    means native applications and software can usually run transparently and seamlessly.
    However, because the hypervisor uses resources, which adds overhead to the system,
    performance is not as good as if one were using the actual machine. Para-virtualization
    essentially adds some interfaces so users can run software and apps designed for
    another system on their own host environment. It gives the appearance of operating
    in a particular environment, but the artifice is superficial. However, because
    para-virtualization modifies the operating system kernel to use the application
    programming interface provided by the virtualization layer hypervisor instead
    of nonvirtualizable instructions, it eliminates the need for a virtual machine
    monitor (VMM) translation process. Therefore, para-virtualization generally performs
    better than full virtualization. 5.2.1 Virtual Machines. The most widely used
    virtualization technology is the virtual machine. Virtual machines come in two
    main flavors: one runs on bare machines, e.g., Xen, VMware vSphere, and KVM; the
    other runs on an operating system, e.g., VMware Workstation. Both are essentially
    a copy of an application running in parallel on the same server, which is an efficient
    way of using hardware resources. It also makes many application services more
    feasible and/or more robust. For example, virtual environments are less prone
    to interruptions of service, and, when services are interrupted, they tend to
    recover more quickly. Upgrading from old to new software versions is easier—one
    simply installs the new version on a new virtual machine and flips over to the
    new machine when the time is right. Also, the isolation inherent to virtual machines
    means different applications are less likely to interfere with each other. Further,
    virtual machines decouple an application from an actual physical server, which
    means the server running the service can be changed dynamically to suit prevailing
    circumstances. The process of changing servers is called fast migration [145].
    However, perhaps the greatest benefit of virtual machines is that users can use
    as many or as few resources as needed to suit their current workload requirements
    [178]. Scaling programs by adding and deleting resources is part of the application
    execution process. Today, there is even virtualization technology for deploying
    and managing the cloud platforms themselves. OpenStack is a pre-eminent example
    [32]. 5.2.2 Containers. In addition to virtual machines, containers are another
    virtualization technology that has emerged in recent years [53]. They are like
    virtual machines but structurally different. As shown in Figure 4, virtual machines
    and containers both comprise basic hardware facilities, i.e., a physical host
    machine running an operating system (HostOS), binary files, development dependent
    libraries, and an application service layer. But a virtual machine includes a
    virtual machine operating system (GuestOS) and operates on top of the VMM/hypervisor,
    while a container runs upon the container engine, which in turn runs on the operating
    system. In other words, a virtual machine must simulate a complete operating system,
    while a container is just an instance share of the native operating system running
    on the physical machine. Fig. 4. Fig. 4. Emerging fields with relevance to CDA.
    Up to now, many container technologies have emerged, which can be mainly divided
    into four categories according to their functions: container runtime (such as
    CRI-O and Containerd) [60], container engine (e.g., Rkt, OpenVZ, LXC) [115], application
    container engine (like Docker) [246], and container orchestration engine (such
    as Kubernetes and Apache Mesos) [107]. (1) The container runtime implements the
    Container Runtime Interface (CRI) specification, which is used to manage the deployment
    and execution of containers. (2) The container engine provides a lightweight virtual
    runtime environment at the operating system (OS) level, using the same kernel
    mechanism as the OS. This technology allows for the virtualization of a physical
    machine into multiple OS instances. (3) The application container engine is an
    application encapsulation technology that does not have its own kernel or virtual
    hardware. It only encapsulates the application and its dependent environment as
    an independent object, achieving isolation of application processes. The application
    in it runs directly on the host kernel. (4) The container orchestration engine
    is used for the automatic deployment, scaling, and load balancing of container-based
    applications. Containers have three main advantages: (1) They are lightweight.
    Because the containers themselves do not have a kernel, they consume fewer resources
    than a virtual machine. Also, this means container startup is usually a second-level
    process, which is more suitable for the rapid startup requirements of most industrial
    environments. Plus, it means application services can be deployed and migrated
    quickly. Salonidis et al. [153] introduced dynamic service migration based on
    containers in a mobile edge-cloud framework. Li et al. [150] introduced a rapid
    migration architecture for edge services based on containers. Kakakhel et al.
    [116] defined the different properties of containers and discussed how to dynamically
    migrate stateful applications through containers to extend fog and mobile edge
    computing services. (2) They have good scalability. Different types of devices
    can automatically pull updated versions of an image file to update a container
    through a container engine, destroying the old version of the container to release
    resources. Darrous et al. [46] proposed two container image placement algorithms,
    k-Center-Based Placement (KCBP) and KCBP-Without-Conflict (KCBP-WC), to reduce
    the maximum retrieval time of a container image across edge servers. KCBP and
    KCBP-WC are optimized based on their K-centers. (3) They have good isolation.
    Isolating applications with containers, both physically and in terms of system
    resources, is largely a function of Cgroups and namespace technology. This is
    a highly cohesive system that can rapidly separate services, which perfectly suits
    microservices architecture. Ren et al. [139] provide an excellent explanation
    of the isolation performance of containers. Unlike cloud computing, the edge nodes
    in edge computing have less CPU power, memory, storage, and networking capabilities,
    so any technology installed on an edge device to make it more scalable must be
    lightweight. For this reason, container technology is suited to edge applications
    and, as such, an abundance of solutions have already been developed. For instance,
    Ahmed et al. [8] proposed three different Docker optimization methods to improve
    the efficiency of hardware resource use in a fog architecture that involves very
    small computing nodes, such as Raspberry Pi. The three optimizations, sequential
    image downloading, multi-threaded decompression, and input/output pipelining,
    each combine to reduce a server’s deployment time by approximately 30%. Huang
    et al. [91] introduced a technology that combines containers and edge computing
    around parked vehicles to improve resource allocation in transport systems. Tang
    et al. [221] proposed a container-based edge unloading framework for a self-driving
    application. The framework includes an offload decision module, an offload scheduler
    module, and edge offload middleware based on lightweight virtualization. Jaiswal
    et al. [103] provided a collection framework for patient data in an intelligent
    medical system based on Raspberry Pi-configured containers. Mendki et al. [158]
    applied Docker to the analysis of IoT video at the edge in a study on the feasibility
    of analyzing surveillance video in real-time with a deep learning framework, again,
    based on Raspberry Pi. Their performance metric was the cost of containerization.
    Hsieh et al. [88] demonstrate various application scenarios in an edge computing
    platform with the support of containers, including service deployment for smart
    cities such as air quality monitoring and voice and image recognition/classification.
    They tested deployment speeds, QoS levels, and the response times of some event-driven
    mechanisms. Navarro et al. [19] introduced the idea of using containers to deploy
    services from a local server as a micro edge-cloud collaboration. They show how
    a container approach can help third parties create and share customized services
    at the edge of a network to better meet specific local needs and constraints.
    They also illustrate that containers in edge-cloud environments are particularly
    suited to PaaS systems due to their lightweight size and flexibility. Lee et al.
    [173] demonstrate that container and clustering technology can promote the applicability
    of applications on the distributed multi-cloud platform collaborating with a series
    of network nodes. In fact, today, containers have become so interwoven with distributed
    computing that many commercial edge devices are either containerized or container-ready,
    e.g., Azure IoT Edge [161], Balena [20], and Amazon’s AWS IoT Greengrass [13].
    Azure IoT Edge was announced as an open-source product by Microsoft in 2018. It
    comprises certified Edge hardware, modules, a runtime, and a cloud interface.
    An edge module is an execution unit, implemented in a Docker-compatible container,
    that runs business logic at the edge. The runtime allows for the use of custom
    and cloud logic on edge devices. It sits on the edge device and performs management
    and communication operations. The cloud interface remotely monitors and manages
    the edge devices. Balena consists of BalenaCloud, BalenaOS, and BalenaEngine.
    BalenaCloud is Balena’s fully managed cloud service. In addition to a server that
    can automatically package code into containers, it includes device and client
    software. BalenaOS is a simple Linux operating system suitable for edge devices
    that comes with the BalenaEngine pre-installed. The whole suite forms a lightweight,
    Docker-compatible container engine. AWS IoT Greengrass enables customers to build
    edge devices and applications. With this kit, devices can securely connect to
    a local network and interactive information without requiring to communicate with
    the cloud. 5.2.3 Container Orchestration. Container orchestration allows containers
    to run as a cluster in distributed environments. An orchestration engine usually
    includes a container management mechanism, a scheduling mechanism, cluster definitions,
    and a service discovery module. Through these components, the engine organically
    combines containers into microservice applications to fulfill business needs.
    Containers can even be expanded horizontally across multiple edge nodes. The mainstream
    engines currently include Docker Swarm, Mesos, and Kubernetes. Gao et al. [66]
    introduced Docker and Docker Swarm into the software definition function of the
    original Bluetooth, transforming BT-SDF into an extensible and flexible IoT function
    redefinition framework. Mesos is a general cluster resource scheduling platform
    that, together with Marathon, provides all the functionality of a container orchestration
    engine. Rattihalli et al. [186] presented an Apache-Mesos container migration
    system based on a two-stage elastic cluster architecture, showing how this framework
    could be used to manage a scientific workload in three different cloud/cluster
    environments. Developed by Google, Kubernetes is an open-source container orchestration
    engine and supports both Docker and CoreOS containers. Malawski et al. [171] designed
    an end-to-end containerization solution supporting either Kubernetes or Amazon
    electrical control system for scientific workflows. The solution can execute mixed
    workflows, over multiple computing infrastructures if needed, to significantly
    reduce the burden of infrastructure management. Kubernetes is associated with
    some of the more lightweight edge computing platforms, such as KubeEdge and K3s.
    KubeEdge is an open-source system developed by Huawei for extending container
    orchestration to edge devices [247]. K3s is a lightweight distributed system launched
    by Rancher Labs [103]. This product is designed to run Kubernetes in environments
    with limited resources. K3s supports the x86_64, ARM64, and ARMv7 architectures,
    allowing K3s to work more flexibly across edge infrastructure. The implementation
    is a lightweight version of Kubernetes that deletes old, non-essential code, integrates
    running packaging processes, and uses containers instead of Docker as the runtime
    container engine. SQLite can also be included as an optional data store. To solve
    the problem of data confidentiality, Dupont et al. [55] proposed a migration platform
    of IoT functions that relies on Kubernetes clusters to perform horizontal roaming
    and vertical offloading. In a practical demonstration of a healthcare scenario,
    Dupont and colleagues showed how to use the IoT roaming function with medical
    data. In a second case, they highlighted the offload capabilities of the platform
    to optimize the remote diagnosis of mechanical engines. However, Kaur et al. [120]
    find that existing Kubernetes solutions lack sufficient robustness to deal with
    interference and minimize energy consumption in IIoT settings. Therefore, they
    provided a solution to use an additional competent controller that can manage
    containers on edge cloud nodes, called KEIDS, which considers a system’s carbon
    footprint and energy consumption and is robust to interference. KEIDS is based
    on the integer linear programming method of multi-objective optimization problems.
    Chen et al. [36] proposed a scalable IoT/M2M architecture based on an OpenStack
    cloud comprising Kubernetes-enabled Network Function Virtualization (NFV) plus
    a management and orchestration engine. Its extended functions are based on Kubernetes
    with the addition of container-based network functions. Their experiments show
    that their NFV platform with Kubernetes has better scalability than systems without.
    The performance of these different orchestration engines has been compared in
    multiple studies. Modak et al. [163], for example, compared Docker Swarm and Kubernetes
    with a cloud-based data security application. Magedanz et al. [84] evaluated how
    containers on fog nodes influence the performance of various applications, comparing
    Kubernetes, Mesos-Marathon [160], and Docker Swarm, and how they met the needs
    of running applications. These researchers also proposed a container orchestration
    framework for a fog computing infrastructure. Gromann et al. [75] proposed a multi-architecture
    framework composed of multiple Docker images called PyMon—a Django application
    that can be used to monitor and collect various data from a host. Before choosing
    Docker, they compared the resource use of Kubernetes and Docker Swarm. Skip 6EMERGING
    FIELDS AND THEIR CHALLENGES IN CDA Section 6 EMERGING FIELDS AND THEIR CHALLENGES
    IN CDA While innovation in many ancillary fields will help to accelerate advancements
    in IIoT, recently emerging technologies in some of these fields show particular
    promise for helping to promote collaborative data access. Figure 4 provides an
    overview of these four fields and the potential benefits they may bring. The four
    fields are 5G, blockchain, semantic web, and machine learning. 6.1 5G As the communication
    technology of the next decade, 5G mobile networks provide a significant improvement
    in speed, latency, capacity, reliability, security, and energy consumption compared
    to the previous generation of communication technology [38, 111, 184]. 5G defines
    three key communication scenarios in enhanced Mobile Broadband (eMBB), massive
    Machine Type Communication (mMTC), and Ultra-Reliable and Low Latency Communication
    (URLLC) [227]. Among them, mMTC has huge benefits for CDA, which can provide access
    connections around 1k–1M devices/ k m 2 . It is poised to act as an enabler for
    the connection of sensors, devices, computational platforms, software applications,
    and operators. Connectivity implies seamless vertical and horizontal integration
    across all layers of the IIoT architecture. Further, the mobility, flexibility,
    and cost-effectiveness of 5G can well support CDA, especially in industrial scenarios.
    However, the challenges are also in front of CDA to use 5G [4]: – Gap between
    5G and industrial applications. At the current stage, 5G still focuses on eMBB
    serving for domains like mobile communication, instead of mMTC. Although use cases
    have been released by 3GPP [2, 3], it is still confusing for manufacturing researchers
    and engineers to develop 5G applications for CDA. – Spectrum and operator models.
    5G provides the possibility for CDA everywhere through wireless channels, however,
    to meet the extreme requirements such as low latency and high reliability, a specific
    spectrum is highly preferred. This can be alternatively replaced by solutions
    such as regional licenses with well-designed operator models, which is a huge
    challenge. – Lack of industrial components and testbeds. Unlike smartphones, industrial
    devices are significantly different from each other, and integration issues with
    other existing communication technologies also cannot be ignored. However, the
    importance of CDA is universality and easy usage, which conflicts with existing
    issues. 6.2 Machine Learning There is hope that edge-cloud collaborations will
    solve many of tomorrow’s problems with CAD in the IIoT—especially the resource
    constraints of the edge and the delays caused by central processing in the cloud
    [9, 168]. But from real-time task scheduling to computational offloading to security
    issues to communication delays, the true panacea to all ills are the advances
    being made every day in machine learning [200]. For task-scheduling problems,
    machine learning algorithms combined with a good partition strategy can use the
    operating cycles in edge-cloud architectures to achieve low latency, low energy
    consumption, and high data throughput [118]. Further, deep learning techniques
    [143, 151, 213] excel at scheduling predictions, and a well-design neural network
    can make them quickly [108]. In terms of security issues, machine learning algorithms
    at the edge can fully protect the access to private data stored on end devices.
    They can also reduce computing overheads and authentication delays [264]. Communication
    delays can be overcome with algorithms that reduce the cost of obtaining data
    from edge devices, decrease bandwidth requirements, and/or reduce waiting times.
    For example, one of many approaches to delivering these outcomes is to select
    the most valuable data samples for analysis at the preprocessing stage on the
    edge node. Cai et al.’s strategy for doing this has an anomaly detection and sample
    selection model working on each edge node with a machine learning classification
    scheme that was originally trained on the cloud [31]. Computation offloading is
    usually designed as a task delegation strategy that distributes jobs to edge devices.
    Each device is regarded as an agent, and the overall system costs are minimized
    through machine learning algorithms [110, 138, 144]. Accordingly, edge platforms
    have blurred into a part of the distributed computing environment to some extent.
    What sets the two environments apart is the heterogeneity of edge devices and
    their limited computing and storage resources, which poses a huge challenge to
    machine learning at the edge [194]. However, edge-cloud architecture, which combines
    real-time computing on edge devices and global aggregated computing on the cloud,
    promises a balance between computing power and real-time [49]. Since a typical
    edge device does not have enough processing power to train a machine learning
    model in real-time, the model can be generated in the cloud for use at the edge
    [48, 134, 164, 165, 172]. Machine learning has shown its bright future but still
    meets the following issues: – Black box. Neural networks as the main techniques
    of machine learning are like black boxes, which are almost unexplainable. This
    issue becomes extremely critical for CDA, since industrial applications prioritize
    performance factors such as reliability, robustness, and high precision. A black
    box cannot guarantee these. – Distributed scenarios. Industrial devices are commonly
    distributed and collaborated, which are also resource-constrained. This is challenging
    the mode of edge intelligence. Distributed learning or federated learning [87,
    124] is promising, however, they should be more focused on embedded devices for
    CDA. 6.3 Semantic Web Semantic web technology is an ever-expanding field that
    has found its way into more industries over the years. It is a discipline that
    basically focuses on ways of describing information in a formal and machine-interpretable
    way [83]. As a facilitator of semantic interoperability, ontologies provide definitions
    and interpretations of concepts associated with metadata in a particular domain.
    To name some examples, the Web Ontology Language (OWL) is a computational logic-based
    W3C standard language [156], while the Semantic Web Rule Language (SWRL) is designed
    to express horn-like rules and logic in both the OWL DL and OWL Lite sub-languages
    of OWL [85]. SPARQL is a resource description framework query language for semantic
    web databases. These are some of the building blocks at the top levels of semantic
    technology that are providing users with the necessary structures to link data.
    In the near future, IIoT is expected to connect sensors, actuators, devices, gateways,
    and software services in such massive amounts that the communication loads will
    be unprecedented and massively challenging. One of the main issues will be handling
    device heterogeneity [191] and the different standards and formats for storing
    and communicating data. Semantic technology promises to deal with M2M communications
    and integrations through its ability to describe objects, share and integrate
    information, and infer knowledge [72]. Currently, there is a considerable amount
    of literature on the combination of the semantic web and IoT [131, 218], which
    is quickly becoming known as the Semantic Web of Things (SWoT). A review of this
    literature follows. To facilitate semantic interoperability, IIC publishes the
    Industrial Internet Connectivity Framework (IICF) [112], which redefines the traditional
    Open System Interconnection (OSI) model and implements semantic interoperability
    on the information layer (the application layer in OSI). In this layer, distributed
    data management and heterogeneous data interoperability rely on designated ontologies
    to automatically process and accurately interpret the exchanged data. Mayer et
    al. [155] introduced a semantic framework for IIoT called Open Semantic Framework
    (OSF), in which domain-specific knowledge is grouped in ontologies to enable knowledge
    integration. The domain-specific ontologies work as a semantic footstone that
    allows machines to collaborate with each other without human effort. Aimed at
    the level of interoperability required of Industry 4.0, González et al. proposed
    the Standards Ontology (STO) describing more than 60 I4.0 standards and their
    relations [74]. This is one of the ontologies that could facilitate the integration
    of standards on both conceptual and implementational levels. Zhu et al. [270]
    proposed an ontology-based architecture for integrating semantic information from
    distributed data nodes into an IIoT system. Similarly, Willner et al. [236] leveraged
    semantic web technologies to formally describe interactions and exchange information
    based on the oneM2M standard [217]. Their goal is to ensure that all components
    in an IIoT system have the ability to exchange information. Giustozzi et al. [71]
    developed an ontology of industrial processes to promote knowledge exchange and
    re-usability from the perspective of context representation and context reasoning
    to achieve context awareness. Kaed et al. put forward a semantic query engine
    called SQenIoT [56] and a semantic rule engine named SRE [57] for IIoT gateways,
    with the aim of retrieving information about the environment to implement dynamic
    but flexible rule-based control strategies. Ontology techniques have been an indispensable
    part of IIoT, while it still meets challenges in practice: – Domain ontology.
    Ontology is an important role in every stage of CDA. However, industrial domain
    knowledge differs much from one to another, so building them is time-consuming.
    Other techniques such as machine learning should be involved to simplify this
    work. – Sharing of ontology. There are some industrial ontologies in the literature,
    but merging them and sharing the knowledge across the ontology is a challenge.
    This is important for widely promoting existing ontology to new domains. 6.4 Blockchain
    CDA is based on edge-cloud collaboration frameworks, where some thorny issues
    are rising, including privacy, access threats, and authorization hacks such as
    modifying key metadata [6, 105]. To ensure the security and privacy of IIoT systems,
    some are looking to blockchain [199]. Its decentralized nature, resistance to
    tampering, and strong consistency and traceability make it an efficient and trustworthy
    method of providing trust between distributed devices [90, 249]. Blockchain consolidates
    information with a timestamp, then passes itself wholesale to other nodes on the
    network who verify that the information has been added to the chain through an
    encrypted hash [18]. This same verifiable technology can be used to build a secure
    and interoperable ecosystem for edge-cloud collaborative architecture [227]. A
    universal and decentralized blockchain-enhanced Pub/Sub communication model can
    circumvent the vulnerabilities publish/subscribe stream models have to malicious
    attacks and single points of failure [89]. Additionally, the edge-cloud collaborative
    architecture combined with blockchain technology can be used to share sensitive
    patient data and perform their calculations [137]. Besides, Zhang et al. proposed
    the “Internet of Things Blockchain” (IBoT) framework [265]. They introduced data
    encryption in the framework to avoid malicious tampering of credential data and
    smart contracts to protect data exchange. In contrast, the blockchain-based trusted
    data management scheme proposed by Ma et al. also incorporates matrix-based multi-channel
    data segmentation and isolation [266], while Zhang et al. focus more on the importance
    of the security of shared data in the blockchain industrial IoT domain [263].
    The studies on combined blockchain/SDN solutions include Luo et al. [149], who
    proposed a distributed SDIIoT-enabled blockchain to control different SDNs to
    synchronize local views between servers and reach a final consensus on the global
    view. Medhane [157] proposed a security framework that combines blockchain technology
    with an edge-cloud collaboration architecture and SDNs to face data confidentiality
    challenges. Guo et al. [77] introduced alliance blockchain and deep reinforcement
    learning to solve problems of trust and adaptation with resource allocation in
    an edge-cloud collaboration. Xiao et al. [245] use SDN and blockchain for rapid
    fake news detection, while Yazdinejad et al. [253] developed a secure and efficient
    SDN controller architecture supported by blockchain that is provably superior
    to the classic blockchain. Blockchain is a hot topic not only for CDA but also
    for all applications about security, while issues are equally serious in industrial
    domains: – High resource consumption. Blockchain takes a lot of computational
    power, storage, and energy. This is unacceptable for resource-constrained devices
    in CDA. IoTA is a promising direction to address this issue, which uses a directed
    acyclic graph (DAG) structure. Each IoTA node does not need to store the entire
    transaction chain. Moreover, creating and validating transactions on the network
    does not necessitate validation with every other transaction, leading to a significant
    reduction in computing and storage requirements. However, industrial applications
    need long-term tests. – Delay. The feature of the blockchain is distribution,
    meanwhile, it produces delay, and this is insufficient for real-time scenarios
    of CDA, such as an access request of a fast-moving robot. Skip 7CONCLUSION Section
    7 CONCLUSION Over the past few decades, IIoT has proven to be a highly successful
    technology in a range of fields. However, as the datasphere approaches the zettabyte
    scale, the M2M connectivity between the soon-to-be 15 billion heterogeneous devices
    is challenging conventional data communications and processing systems. This survey
    focuses on data access as the base of IIoT systems, providing a review of both
    the existing and emerging technologies enabling collaborative data access. We
    reviewed the current research on data access from the perspective of standardization,
    data acquisition, and fusion. To support these aspects, we investigated the history
    of computing architectures and discussed the techniques for scalable architectures
    with a focus on edge-cloud collaborations as the trend for future IIoT systems.
    We also discussed how emerging technologies such as 5G, machine learning, blockchain,
    and semantic web will advance data access going forward. In summary, the development
    of IIoT has begun to take shape with numerous achievements in both academic and
    commercial fields. However, IIoT still faces the challenge of device access—the
    existing IIoT platforms cannot effectively address the issue of accessing massive
    and heterogeneous data sources at the edge side. Further research is necessary.
    REFERENCES [1] 184 ISO T. C.. 2022. Automation systems and integration. Retrieved
    from https://www.iso.org/committee/54110.html Reference [2] 22.804 3GPP TR. 2020.
    Study on Communication for Automation in Vertical domains. Retrieved from https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3187
    Reference [3] 22.821 3GPP TR. 2018. Feasibility Study on LAN Support in 5G. Retrieved
    from https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3281
    Reference [4] ACIA 5G. 2019. 5G for Connected Industries and Automation. Retrieved
    from https://5g-acia.org/wp-content/uploads/2021/04/WP_5G_for_Connected_Industries_and_Automation_Download_19.03.19.pdf
    Reference [5] ABB. 2022. Introducing ABB Ability. Retrieved from https://global.abb/topic/ability/en/about
    Reference [6] Aceto Giuseppe, Persico Valerio, and Pescapé Antonio. 2019. A survey
    on information and communication technologies for industry 4.0: State-of-the-art,
    taxonomies, perspectives, and challenges. IEEE Commun. Surv. Tutor. 21, 4 (2019),
    3467–3501. Reference [7] Adesina Tolulope and Osasona Oladiipo. 2019. A novel
    cognitive IoT gateway framework: Towards a holistic approach to IoT interoperability.
    In IEEE 5th World Forum on Internet of Things (WF-IoT’19). IEEE, 53–58. Reference
    [8] Ahmed A. and Pierre G.. 2018. Docker container deployment in fog computing
    infrastructures. In IEEE International Conference on Edge Computing (EDGE’18).
    1–8. Reference 1Reference 2 [9] Al-Gumaei Khaled, Schuba Kornelia, Friesen Andrej,
    Heymann Sascha, Pieper Carsten, Pethig Florian, and Schriegel Sebastian. 2018.
    A survey of internet of things and big data integrated solutions for industrie
    4.0. In A Survey of Internet of Things and Big Data Integrated Solutions for Industrie
    4.0, Vol. 1. IEEE, 1417–1424. Reference 1Reference 2 [10] Al-Turjman Fadi and
    Alturjman Sinem. 2020. 5G/IoT-enabled UAVs for multimedia delivery in industry-oriented
    applications. Multim. Tools Applic. 79 (2020), 8627–8648. Reference [11] Alahakoon
    Damminda and Yu Xinghuo. 2015. Smart electricity meter data intelligence for future
    energy systems: A survey. IEEE Trans. Industr. Inform. 12, 1 (2015), 425–436.
    Reference 1Reference 2 [12] Alatise Mary B. and Hancke Gerhard P.. 2017. Pose
    estimation of a mobile robot based on fusion of IMU data and vision data using
    an extended Kalman filter. Sensors 17, 10 (2017), 2164. Reference 1Reference 2
    [13] Amazon. 2023. AWS IoT overview. Retrieved from https://aws.amazon.com/iot/?nc1=h_ls
    Reference 1Reference 2 [14] Anish Babu S., J. Hareesh M., Martin John Paul, Cherian
    Sijo, and Sastri Yedhu. 2014. System performance evaluation of para virtualization,
    container virtualization, and full virtualization using Xen, OpenVZ, and XenServer.
    In 4th International Conference on Advances in Computing and Communications. 247–250.
    Reference [15] Arooj Ansif, Farooq Muhammad Shoaib, Umer Tariq, Rasool Ghulam,
    and Wang Bo. 2020. Cyber physical and social networks in IoV (CPSN-IoV): A multimodal
    architecture in edge-based networks for optimal route selection using 5G technologies.
    IEEE Access 8 (2020), 33609–33630. Reference 1Reference 2 [16] Asif-Ur-Rahman
    Md., Afsana Fariha, Mahmud Mufti, Kaiser M. Shamim, Ahmed Muhammad R., Kaiwartya
    Omprakash, and James-Taylor Anne. 2018. Toward a heterogeneous mist, fog, and
    cloud-based framework for the internet of healthcare things. IEEE Internet Things
    J. 6, 3 (2018), 4049–4062. Reference 1Reference 2 [17] Atlam Hany F., Walters
    Robert J., and Wills Gary B.. 2018. Fog computing and the internet of things:
    A review. Big Data Cogn. Comput. 2, 2 (2018), 10. Reference [18] Bahga Arshdeep
    and Madisetti Vijay K.. 2016. Blockchain platform for industrial internet of things.
    J. Softw. Eng. Applic. 9, 10 (2016), 533–546. Reference [19] Baig Roger, Centelles
    Roger Pueyo, Freitag Felix, and Navarro Leandro. 2017. On edge microclouds to
    provide local container-based services. In Global Information Infrastructure and
    Networking Symposium (GIIS’17). 31–36. Reference [20] Balena. Balena. Retrieved
    from https://www.balena.io/ Reference [21] Banerjee Suprateek and Großmann Daniel.
    2016. An electronic device description language based approach for communication
    with DBMS and file system in an industrial automation scenario. In IEEE 21st International
    Conference on Emerging Technologies and Factory Automation (ETFA’16). IEEE, 1–4.
    Reference [22] Bao Shuang, Yan Hairong, Chi Qingping, Pang Zhibo, and Sun Yuying.
    2017. FPGA-based reconfigurable data acquisition system for industrial sensors.
    IEEE Trans. Industr. Inform. 13, 4 (2017), 1503–1512. Reference 1Reference 2Reference
    3 [23] Bardwaj A. Anand, Anandaraj M, Kapil K., Vasuhi S., and Vaidehi V.. 2008.
    Multi sensor data fusion methods using sensor data compression and estimated weights.
    In International Conference on Signal Processing, Communications and Networking.
    IEEE, 250–254. Reference [24] Barreto Luis, Amaral Antonio, and Pereira Teresa.
    2017. Industry 4.0 implications in logistics: An overview. Procedia Manuf. 13
    (2017), 1245–1252. Reference 1Reference 2 [25] Bonomi Flavio, Milito Rodolfo,
    Zhu Jiang, and Addepalli Sateesh. 2012. Fog computing and its role in the internet
    of things. In 1st Edition of the MCC Workshop on Mobile Cloud Computing. 13–16.
    Reference [26] Bordel Borja, Rivera Diego Sánchez De, and Alcarria Ramón. 2016.
    Plug-and-play transducers in cyber-physical systems for device-driven applications.
    In 10th International Conference on Innovative Mobile and Internet Services in
    Ubiquitous Computing (IMIS’16). IEEE, 316–321. Reference [27] Botts Mike, Percivall
    George, Reed Carl, and Davidson John. 2006. OGC® sensor web enablement: Overview
    and high level architecture. In 2nd International Conference on GeoSensor Networks.
    Springer, 175–190. Reference 1Reference 2 [28] Boyes Hugh, Hallaq Bil, Cunningham
    Joe, and Watson Tim. 2018. The Industrial Internet of Things (IIoT): An analysis
    framework. Comput. Industr. 101 (2018), 1–12. Reference [29] Bröring Arne, Maué
    Patrick, Janowicz Krzysztof, Nüst Daniel, and Malewski Christian. 2011. Semantically-enabled
    sensor plug & play for the sensor web. Sensors 11, 8 (2011), 7568–7605. Reference
    [30] Bugnion Edouard, Devine Scott, Rosenblum Mendel, Sugerman Jeremy, and Wang
    Edward Y.. 2012. Bringing virtualization to the X86 architecture with the original
    VMware workstation. ACM Trans. Comput. Syst. 30, 4 (2012), 1–51. Reference [31]
    Cai He, Hua Cunqing, and Xu Wenchao. 2019. Design of active learning framework
    for collaborative anomaly detection. In 11th International Conference on Wireless
    Communications and Signal Processing (WCSP’19). IEEE, 1–7. Reference [32] Cao
    Ronghui, Tang Zhuo, Liu Chubo, and Veeravalli Bharadwaj. 2020. A scalable multicloud
    storage architecture for cloud-supported medical internet of things. IEEE Internet
    Things J. 7, 3 (2020), 1641–1654. Reference [33] Chang Hyunseok, Hari Adiseshu,
    Mukherjee Sarit, and Lakshman T. V.. 2014. Bringing the cloud to the edge. In
    IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS’14). IEEE,
    346–351. Reference [34] Chang Wo L., David Boyd, and Orit Levin. 2019. NIST Big
    Data Interoperability Framework: Volume 6, Reference Architecture. (October 2019).
    Reference [35] Chekired Djabir Abdeldjalil, Khoukhi Lyes, and Mouftah Hussein
    T.. 2018. Industrial IoT data scheduling based on hierarchical fog computing:
    A key for enabling smart factory. IEEE Trans. Industr. Inform. 14, 10 (2018),
    4590–4602. Reference [36] Chen Hung-Li and Lin Fuchun Joseph. 2019. Scalable IoT/M2M
    platforms based on Kubernetes-enabled NFV MANO architecture. In International
    Conference on Internet of Things (iThings) and IEEE Green Computing and Communications
    (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart
    Data (SmartData). 1106–1111. Reference 1Reference 2 [37] Chen Jiasi and Ran Xukan.
    2019. Deep learning with edge computing: A review. Proc. IEEE 107, 8 (2019), 1655–1674.
    Reference [38] Cheng Jiangfeng, Chen Weihai, Tao Fei, and Lin Chun-Liang. 2018.
    Industrial IoT in 5G environment towards smart manufacturing. J. Industr. Inf.
    Integ. 10 (2018), 10–19. Reference [39] Cheng Ledan, Guo Songtao, Wang Ying, and
    Yang Yuanyuan. 2016. Lifting wavelet compression based data aggregation in big
    data wireless sensor networks. In IEEE 22nd International Conference on Parallel
    and Distributed Systems (ICPADS’16). IEEE, 561–568. Reference 1Reference 2 [40]
    Cheng Ying, Zhang Yongping, Ji Ping, Xu Wenjun, Zhou Zude, and Tao Fei. 2018.
    Cyber-physical integration for moving digital factories forward towards smart
    manufacturing: A survey. Int. J. Advan. Manuf. Technol. 97, 1–4 (2018), 1209–1221.
    Reference [41] Cherian Avarachan, Wobschall Darold, and Sheikholeslami Mehrdad.
    2017. An IoT interface for industrial analog sensor with IEEE 21451 protocol.
    In IEEE Sensors Applications Symposium (SAS’17). IEEE, 1–5. Reference 1Reference
    2Reference 3 [42] Chi Qingping, Yan Hairong, Zhang Chuan, Pang Zhibo, and Xu Li
    Da. 2014. A reconfigurable smart sensor interface for industrial WSN in IoT environment.
    IEEE Trans. Industr. Inform. 10, 2 (2014), 1417–1425. Reference 1Reference 2Reference
    3 [43] Chi Yuanfang, Dong Yanjie, Wang Jane, Yu F. Richard, and Leung Victor C.
    M.. 2022. Knowledge-based fault diagnosis in industrial internet of things: A
    survey. IEEE Internet Things J. 9, 15 (2022), 12886–12900. Reference [44] Consortium.
    Industrial Internet2022. Industrial internet reference architecture. Retrieved
    from https://www.iiconsortium.org/IIRA/ Reference [45] Xu Li Da, He Wu, and Li
    Shancang. 2014. Internet of things in industries: A survey. IEEE Trans. Industr.
    Inform. 10, 4 (2014), 2233–2243. Reference [46] Darrous Jad, Lambert Thomas, and
    Ibrahim Shadi. 2019. On the importance of container image placement for service
    provisioning in the edge. In 28th International Conference on Computer Communication
    and Networks (ICCCN’19). 1–9. Reference [47] De Suparna, Bermudez-Edo Maria, Xu
    Honghui, and Cai Zhipeng. 2022. Deep generative models in the industrial internet
    of things: A survey. IEEE Trans. Industr. Inform. 18, 9 (2022), 5728–5737. Reference
    [48] Deng Shuiguang, Xiang Zhengzhe, Zhao Peng, Taheri Javid, Gao Honghao, Yin
    Jianwei, and Zomaya Albert Y.. 2020. Dynamical resource allocation in edge for
    trustable internet-of-things systems: A reinforcement learning method. IEEE Trans.
    Industr. Inform. 16, 9 (2020), 6103–6113. Reference [49] Deng Shuiguang, Zhao
    Hailiang, Fang Weijia, Yin Jianwei, Dustdar Schahram, and Zomaya Albert Y.. 2020.
    Edge intelligence: The confluence of edge computing and artificial intelligence.
    IEEE Internet Things J. 7, 8 (2020), 7457–7469. Reference [50] Diedrich Christian,
    Belyaev Alexander, Schröder Tizian, Vialkowitsch Jens, Willmann Alexander, Usländer
    Thomas, Koziolek Heiko, Wende Jörg, Pethig Florian, and Niggemann Oliver. 2017.
    Semantic interoperability for asset communication within smart factories. In 22nd
    IEEE International Conference on Emerging Technologies and Factory Automation
    (ETFA’17). IEEE, 1–8. Reference [51] Ding Zhiming, Yang Bin, Chi Yuanying, and
    Guo Limin. 2015. Enabling smart transportation systems: A parallel spatio-temporal
    database approach. IEEE Trans. Comput. 65, 5 (2015), 1377–1391. Reference [52]
    Dizdarević Jasenka, Carpio Francisco, Jukan Admela, and Masip-Bruin Xavi. 2019.
    A survey of communication protocols for internet of things and related challenges
    of fog and cloud computing integration. ACM Computing Surveys (CSUR) 51, 6 (2019),
    1–29. Reference [53] Docker. What is a Container? Retrieved from https://www.docker.com/resources/what-container.
    (n.d.). Reference [54] Du Rong, Santi Paolo, Xiao Ming, Vasilakos Athanasios V.,
    and Fischione Carlo. 2018. The sensable city: A survey on the deployment and management
    for smart city monitoring. IEEE Commun. Surv. Tutor. 21, 2 (2018), 1533–1560.
    Reference 1Reference 2 [55] Dupont Corentin, Giaffreda Raffaele, and Capra Luca.
    2017. Edge computing in IoT context: Horizontal and vertical Linux container migration.
    In 2017 Global Internet of Things Summit (GIoTS). IEEE, 1–4. Reference 1Reference
    2 [56] Kaed Charbel El, Khan Imran, Hossayni Hicham, and Nappey Philippe. 2016.
    SQenloT: Semantic query engine for industrial Internet-of-Things gateways. In
    2016 IEEE 3rd World Forum on Internet of Things (WF-IoT). IEEE, 204–209. Reference
    [57] Kaed Charbel El, Khan Imran, Berg Andre Van Den, Hossayni Hicham, and Saint-Marcel
    Christophe. 2017. SRE: Semantic rules engine for the industrial Internet-of-Things
    gateways. IEEE Trans. Industr. Inform. 14, 2 (2017), 715–724. Reference [58] Erol-Kantarci
    Melike and Mouftah Hussein T.. 2014. Energy-efficient information and communication
    infrastructures in the smart grid: A survey on interactions and open issues. IEEE
    Commun. Surv. Tutor. 17, 1 (2014), 179–197. Reference 1Reference 2 [59] Esfahani
    Alireza, Mantas Georgios, Matischek Rainer, Saghezchi Firooz B, Rodriguez Jonathan,
    Bicaku Ani, Maksuti Silia, Tauber Markus G., Schmittner Christoph, and Bastos
    Joaquim. 2017. A lightweight authentication mechanism for M2M communications in
    industrial IoT environment. IEEE Internet Things J. 6, 1 (2017), 288–296. Reference
    1Reference 2 [60] Espe Lennart, Anshul Jindal, Vladimir Podolskiy, and Michael
    Gerndt. 2020. Performance Evaluation of Container Runtimes. In Proceedings of
    the 10th International Conference on Cloud Computing and Services Science (CLOSER’20).
    273–281. Reference [61] Fang Xi, Misra Satyajayant, Xue Guoliang, and Yang Dejun.
    2011. Smart grid—The new and improved power grid: A survey. IEEE Commun. Surv.
    Tutor. 14, 4 (2011), 944–980. Reference 1Reference 2 [62] Fernandes Miguel A.,
    Matos Samuel G., Peres Emanuel, Cunha Carlos R., López Juan A., Ferreira P. J.
    S. G., Reis M. J. C. S., and Morais Raul. 2013. A framework for wireless sensor
    networks management for precision viticulture and agriculture based on IEEE 1451
    standard. Comput. Electron. Agric. 95 (2013), 19–30. Reference [63] Figueroa-Lorenzo
    Santiago, Añorga Javier, and Arrizabalaga Saioa. 2020. A survey of IIoT protocols:
    A measure of vulnerability risk analysis based on CVSS. ACM Comput. Surv. 53,
    2 (2020), 1–53. Reference [64] Foster Ian. 2003. THE GRID: Computing without bounds.
    Scient. Am. 288, 4 (2003), 78–85. Reference [65] Fraga-Lamas Paula, Fernández-Caramés
    Tiago M., and Castedo Luis. 2017. Towards the internet of smart trains: A review
    on industrial IoT-connected railways. Sensors 17, 6 (2017), 1457. Reference 1Reference
    2 [66] Gao Yuan, Wang Haoxuan, and Huang Xin. 2016. Applying Docker swarm cluster
    into software defined internet of things. In 8th International Conference on Information
    Technology in Medicine and Education (ITME’16). IEEE, 445–449. Reference [67]
    Garcia Rafael, Gordon-Ross Ann, and George Alan D.. 2009. exploiting partially
    reconfigurable FPGAs for situation-based reconfiguration in wireless sensor networks.
    In 17th IEEE Symposium on Field Programmable Custom Computing Machines. IEEE,
    243–246. Reference 1Reference 2 [68] Georgakopoulos Dimitrios, Jayaraman Prem
    Prakash, Fazia Maria, Villari Massimo, and Ranjan Rajiv. 2016. Internet of things
    and edge cloud computing roadmap for manufacturing. IEEE Cloud Comput. 3, 4 (2016),
    66–73. Reference [69] Gharaibeh Ammar, Salahuddin Mohammad A., Hussini Sayed Jahed,
    Khreishah Abdallah, Khalil Issa, Guizani Mohsen, and Al-Fuqaha Ala. 2017. Smart
    cities: A survey on data management, security, and enabling technologies. IEEE
    Commun. Surv. Tutor. 19, 4 (2017), 2456–2501. Reference [70] Giang Nam Ky, Leung
    Victor C. M., and Lea Rodger. 2016. On developing smart transportation applications
    in fog computing paradigm. In 6th ACM Symposium on Development and Analysis of
    Intelligent Vehicular Networks and Applications. 91–98. Reference [71] Giustozzi
    Franco, Saunier Julien, and Zanni-Merk Cecilia. 2018. Context modeling for Industry
    4.0: An ontology-based proposal. Procedia Comput. Sci. 126 (2018), 675–684. Reference
    [72] Goudos Sotirios K., Dallas Panagiotis I., Chatziefthymiou Stella, and Kyriazakos
    Sofoklis. 2017. A survey of IoT key enabling and future technologies: 5G, mobile
    IoT, sematic web and applications. Wirel. Person. Commun. 97, 2 (2017), 1645–1675.
    Reference [73] Graham Robert David and Johnson Peter C.. 2014. Finite state machine
    parsing for internet protocols: Faster than you think. In IEEE Security and Privacy
    Workshops. IEEE, 185–190. Reference [74] Grangel-González Irlán, Baptista Paul,
    Halilaj Lavdim, Lohmann Steffen, Vidal Maria-Esther, Mader Christian, and Auer
    Sören. 2017. The Industry 4.0 standards landscape from a semantic integration
    perspective. In 22nd IEEE International Conference on Emerging Technologies and
    Factory Automation (ETFA’17). IEEE, 1–8. Reference [75] Großmann Marcel and Klug
    Clemens. 2017. Monitoring container services at the network edge. In 29th International
    Teletraffic Congress (ITC’17).. 130–133. Reference [76] Guevara Jean A., Vargas
    Enrique A., Fatecha Arturo F., and Barrero Federico. 2015. Dynamically reconfigurable
    WSN node based on ISO/IEC/IEEE 21451 TEDS. IEEE Sensors J. 15, 5 (2015), 2567–2576.
    Reference 1Reference 2 [77] Guo Shaoyong, Yao Dai, Siya Xu, Xuesong Qiu, and Feng
    Qi. 2019. Trusted cloud-edge network resource management: Drl-driven service function
    chain orchestration for IoT. IEEE Internet of Things Journal 7, 7 (2019). Reference
    [78] Ha Quang Phuc, Metia Santanu, and Phung Manh Duong. 2020. Sensing data fusion
    for enhanced indoor air quality monitoring. IEEE Sensors J. 20, 8 (2020), 4430–4441.
    Reference 1Reference 2 [79] Haghnegahdar Lida, Joshi Sameehan S., and Dahotre
    Narendra B.. 2022. From IoT-based cloud manufacturing approach to intelligent
    additive manufacturing: Industrial Internet of Things—An overview. Int. J. Advan.
    Manuf. Technol. 79 (2022), 1–18. Reference [80] Hashem Ibrahim Abaker Targio,
    Yaqoob Ibrar, Anuar Nor Badrul, Mokhtar Salimah, Gani Abdullah, and Khan Samee
    Ullah. 2015. The rise of “big data” on cloud computing: Review and open research
    issues. Inf. Syst. 47 (2015), 98–115. Reference [81] Hazra Abhishek, Adhikari
    Mainak, Amgoth Tarachand, and Srirama Satish Narayana. 2021. A comprehensive survey
    on interoperability for IIoT: Taxonomy, standards, and future directions. ACM
    Comput. Surv. 55, 1 (2021), 1–35. Reference [82] Hinkelmann Heiko, Reinhardt Andreas,
    Varyani Sameer, and Glesner Manfred. 2008. A reconfigurable prototyping platform
    for smart sensor networks. In 4th Southern Conference on Programmable Logic. 125–130.
    Reference 1Reference 2 [83] Hitzler Pascal, Krotzsch Markus, and Rudolph Sebastian.
    2009. Foundations of Semantic Web Technologies. CRC Press. Reference [84] Hoque
    Saiful, Brito Mathias Santos De, Willner Alexander, Keil Oliver, and Magedanz
    Thomas. 2017. Towards container orchestration in fog computing infrastructures.
    In IEEE 41st Annual Computer Software and Applications Conference (COMPSAC’17).
    294–299. Reference [85] Horrocks Ian, Patel-Schneider Peter F., Boley Harold,
    Tabet Said, Grosof Benjamin, and Dean Mike. 2004. SWRL: A semantic web rule language
    combining OWL and RuleML. W3C Memb. Submiss. 21, 79 (2004), 1–31. Reference [86]
    Hossain M. Shamim and Muhammad Ghulam. 2016. Cloud-assisted Industrial Internet
    of Things (IIoT)–enabled framework for health monitoring. Comput. Netw. 101 (2016),
    192–202. Reference 1Reference 2 [87] Hsieh Kevin, Harlap Aaron, Vijaykumar Nandita,
    Konomis Dimitris, Ganger Gregory R., Gibbons Phillip B., and Mutlu Onur. 2017.
    Gaia: Geo-distributed machine learning approaching LAN speeds. In USENIX Symposium
    on Networked Systems Design and Implementation (NSDI’17). 629–647. Reference [88]
    Hsieh Yu-Chen, Hong Hua-Jun, Tsai Pei-Hsuan, Wang Yu-Rong, Zhu Qiuxi, Uddin M.d.
    Yusuf Sarwar, Venkatasubramanian Nalini, and Hsu Cheng-Hsin. 2018. Managed edge
    computing on Internet-of-Things devices for smart city applications. In IEEE/IFIP
    Network Operations and Management Symposium. 1–2. Reference [89] Huang Bobo, Rui
    Zhang, Zhihui Lu, Yiming Zhang, Jie Wu, Lu Zhan, and Patrick C. K. Hung. 2020.
    BPS: A reliable and efficient pub/sub communication model with blockchain-enhanced
    paradigm in multi-tenant edge cloud. Journal of Parallel and Distributed Computing
    143, (2020). Reference [90] Huang Junqin, Kong Linghe, Chen Guihai, Wu Min-You,
    Liu Xue, and Zeng Peng. 2019. Towards secure industrial IoT: Blockchain system
    with credit-based consensus mechanism. IEEE Trans. Industr. Inform. 15, 6 (2019),
    3680–3689. Reference [91] Huang Xumin, Li Peichun, and Yu Rong. 2019. Social welfare
    maximization in container-based task scheduling for parked vehicle edge computing.
    IEEE Commun. Lett. 23, 8 (2019), 1347–1351. Reference [92] Huang Yuzhou, Cai Kaiyu,
    Zong Ran, and Mao Yugang. 2019. Design and implementation of an edge computing
    platform architecture using Docker and Kubernetes for machine learning. In 3rd
    International Conference on High Performance Compilation, Computing and Communications.
    29–32. Reference [93] Huo Ru, Zeng Shiqin, Wang Zhihao, Shang Jiajia, Chen Wei,
    Huang Tao, Wang Shuo, Yu F. Richard, and Liu Yunjie. 2022. A comprehensive survey
    on blockchain in industrial internet of things: Motivations, research progresses,
    and future challenges. IEEE Commun. Surv. Tutor. 24, 1 (2022), 88–122. Reference
    [94] Hwang Joo-Young, Suh Sang-Bum, Heo Sung-Kwan, Park Chan-Ju, Ryu Jae-Min,
    Park Seong-Yeol, and Kim Chul-Ryun. 2008. Xen on ARM: System virtualization using
    Xen hypervisor for ARM-based secure mobile phones. In 5th IEEE Consumer Communications
    and Networking Conference. 257–261. Reference [95] IEC-International Electrotechnical
    Commission and others. 2016. IEC 61360-6. Retrieved from https://webstore.iec.ch/publication/25984
    Reference [96] ISO. ISO/IEC 20005:2013. Retrieved from https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/05/09/50952.html
    Reference [97] ISO. ISO/IEC 20248:2018. Retrieved from https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/06/74/67412.html
    Reference [98] ISO. ISO/IEC 29143:2011. Retrieved from https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/04/51/45166.html
    Reference [99] ISO. ISO/IEC 29175:2012. Retrieved from https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/04/52/45254.html
    Reference [100] ISO. ISO/IEC 29182-1:2013. Retrieved from https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/04/52/45261.html
    Reference [101] ISO. ISO/IEC 30101:2014. Retrieved from https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/05/32/53221.html
    Reference [102] Izadi Davood, Abawajy Jemal H., Ghanavati Sara, and Herawan Tutut.
    2015. A data fusion method in wireless sensor networks. Sensors 15, 2 (2015),
    2964–2979. Reference 1Reference 2 [103] Jaiswal Kavita, Sobhanayak Srichandan,
    Mohanta Bhabendu Kumar, and Jena Debasish. 2017. IoT-cloud based framework for
    patient’s data collection in smart healthcare system using Raspberry-Pi. In International
    Conference on Electrical and Computing Technologies and Applications (ICECTA’17).
    IEEE, 1–4. Reference 1Reference 2 [104] Jan Bilal, Farman Haleem, Khan Murad,
    Talha Muhammad, and Din Ikram Ud. 2019. Designing a smart transportation system:
    An internet of things and big data approach. IEEE Wirel. Commun. 26, 4 (2019),
    73–79. Reference [105] Janjua Kanwal, Shah Munam Ali, Almogren Ahmad, Khattak
    Hasan Ali, Maple Carsten, and Din Ikram Ud. 2020. Proactive forensics in IoT:
    Privacy-aware log-preservation architecture in fog-enabled-cloud using holochain
    and containerization technologies. Electronics 9, 7 (2020), 1172. Reference [106]
    Jasperneite Juergen, Sauter Thilo, and Wollschlaeger Martin. 2020. Why we need
    automation models: Handling complexity in Industry 4.0 and the internet of things.
    IEEE Industr. Electron. Mag. 14, 1 (Mar.2020), 29–40. Reference [107] Jawarneh
    Isam Mashhour Al, Bellavista Paolo, Bosi Filippo, Foschini Luca, Martuscelli Giuseppe,
    Montanari Rebecca, and Palopoli Amedeo. 2019. Container orchestration engines:
    A thorough functional and performance comparison. In IEEE International Conference
    on Communications (ICC’19). 1–6. Reference [108] Jian Chengfeng, Ping Jing, and
    Zhang Meiyu. 2021. A cloud edge-based two-level hybrid scheduling learning model
    in cloud manufacturing. Int. J. Product. Res. 59, 16 (2021), 4836–4850. Reference
    [109] Jiang Bin, Li Jianqiang, Yue Guanghui, and Song Houbing. 2021. Differential
    privacy for industrial internet of things: Opportunities, applications, and challenges.
    IEEE Internet Things J. 8, 13 (2021), 10430–10451. Reference [110] Jiang Feibo,
    Wang Kezhi, Dong Li, Pan Cunhua, Xu Wei, and Yang Kun. 2020. Deep-learning-based
    joint resource scheduling algorithms for hybrid MEC networks. IEEE Internet Things
    J. 7, 7 (2020), 6252–6265. Reference [111] Jiang Tao, Zhang Jianhua, Tang Pan,
    Tian Lei, Zheng Yi, Dou Jianwu, Asplund Henrik, Raschkowski Leszek, D’Errico Raffaele,
    and Jämsä Tommi. 2021. 3GPP standardized 5G channel model for IIoT scenarios:
    A survey. IEEE Internet Things J. 8, 11 (2021), 8799–8815. Reference 1Reference
    2 [112] Joshi Rajive, Didier Paul, Jimenez Jaime, and Carey Timothy. 2017. The
    industrial internet of things volume G5: Connectivity framework. Industr. Internet
    Consort. Rep. (2017). https://www.iiconsortium.org/pdf/IIC_PUB_G5_V1.0_PB_20170228.pdf
    Reference [113] Jula Amin, Sundararajan Elankovan, and Othman Zalinda. 2014. Cloud
    computing service composition: A systematic literature review. Expert Syst. Applic.
    41, 8 (2014), 3809–3824. Reference [114] Zhang Jun, Chen Kai, Zuo Baojing, Ma
    Ruhui, Dong Yaozu, and Guan Haibing. 2010. Performance analysis towards a KVM-based
    embedded real-time virtualization architecture. In 5th International Conference
    on Computer Sciences and Convergence Information Technology. 421–426. Reference
    [115] Kaiser Shahidullah, Haq M.d. Sadun, Tosun Ali Şaman, and Korkmaz Turgay.
    2022. Container technologies for ARM architecture: A comprehensive survey of the
    state-of-the-art. IEEE Access 10 (2022), 84853–84881. Reference [116] Kakakhel
    Syed Rameez Ullah, Mukkala Lauri, Westerlund Tomi, and Plosila Juha. 2018. Virtualization
    at the network edge: A technology perspective. In 3rd International Conference
    on Fog and Mobile Edge Computing (FMEC’18). 87–92. Reference [117] Kampert David
    and Epple Ulrich. 2012. Modeling asset information for interoperable software
    systems. In IEEE 10th International Conference on Industrial Informatics. IEEE,
    947–952. Reference [118] Kang Yiping, Hauswald Johann, Gao Cao, Rovinski Austin,
    Mudge Trevor, Mars Jason, and Tang Lingjia. 2017. Neurosurgeon: Collaborative
    intelligence between the cloud and mobile edge. ACM SIGARCH Comput. Archit. News
    45, 1 (2017), 615–629. Reference [119] Kapil Gayatri, Agrawal Alka, and Khan R.
    A.. 2016. A study of big data characteristics. In International Conference on
    Communication and Electronics Systems (ICCES’16). 1–4. Reference 1Reference 2
    [120] Kaur Kuljeet, Garg Sahil, Kaddoum Georges, Ahmed Syed Hassan, and Atiquzzaman
    Mohammed. 2020. KEIDS: Kubernetes based Energy and Interference Driven Scheduler
    for industrial IoT in edge-cloud ecosystem. IEEE Internet Things J. 7, 5 (2020),
    4228–4237. Reference 1Reference 2 [121] Khalil Ruhul Amin, Saeed Nasir, Masood
    Mudassir, Fard Yasaman Moradi, Alouini Mohamed-Slim, and Al-Naffouri Tareq Y..
    2021. Deep learning in the industrial internet of things: Potentials, challenges,
    and emerging applications. IEEE Internet Things J. 8, 14 (2021), 11016–11040.
    Reference [122] Khan Wazir Zada, Rehman M. H., Zangoti Hussein Mohammed, Afzal
    Muhammad Khalil, Armi Nasrullah, and Salah Khaled. 2020. Industrial internet of
    things: Recent advances, enabling technologies and open challenges. Comput. Electric.
    Eng. 81 (2020), 106522. Reference 1Reference 2 [123] Kim Jayoung, Campbell Alan
    S., Ávila Berta Esteban-Fernández de, and Wang Joseph. 2019. Wearable biosensors
    for healthcare monitoring. Nat. Biotechnol. 37, 4 (2019), 389–406. Reference 1Reference
    2 [124] Kraska Tim, Ameet Talwalkar, John C. Duchi, Rean Griffith, Michael J.
    Franklin, and Michael I. Jordan. 2013. MLbase: A Distributed Machine-learning
    System. In 6th Biennial Conference on Innovative Data Systems Research (CIDR’13).
    2–1. Reference [125] Krasniqi X. and Hajrizi E.. 2016. Use of IoT technology to
    drive the automotive industry from connected to full autonomous vehicles. IFAC-PapersOnLine
    49, 29 (2016), 269–274. Reference [126] Krasteva Yana Esteves, Portilla Jorge,
    Torre Eduardo de la, and Riesgo Teresa. 2011. Embedded runtime reconfigurable
    nodes for wireless sensor networks applications. IEEE Sensors J. 11, 9 (2011),
    1800–1810. Reference 1Reference 2 [127] Kularatna Nihal and Sudantha B. H.. 2008.
    An environmental air pollution monitoring system based on the IEEE 1451 standard
    for low cost requirements. IEEE Sensors J. 8, 4 (2008), 415–422. Reference [128]
    Kumar Anuj and Hancke Gerhard P.. 2014. An energy-efficient smart comfort sensing
    system based on the IEEE 1451 standard for green buildings. IEEE Sensors J. 14,
    12 (2014), 4245–4252. Reference [129] Kumar Anuj, Singh Abhishek, Kumar Ashok,
    Singh Manoj Kumar, Mahanta Pinakeswar, and Mukhopadhyay Subhas Chandra. 2018.
    Sensing technologies for monitoring intelligent buildings: A review. IEEE Sensors
    J. 18, 12 (2018), 4847–4860. Reference 1Reference 2 [130] Kumar Anuj, Singh I.
    P., and Sud S. K.. 2011. Energy efficient and low-cost indoor environment monitoring
    system based on the IEEE 1451 standard. IEEE Sensors J. 11, 10 (2011), 2598–2610.
    Reference [131] Kumar K. N. Prashanth, Kumar V. Ravi, and Raghuveer K.. 2017.
    A survey on semantic web technologies for the internet of things. In International
    Conference on Current Trends in Computer, Electrical, Electronics and Communication
    (CTCEEC’17). IEEE, 316–322. Reference [132] Larras Benoit and Frappé Antoine.
    2020. Distributed clique-based neural networks for data fusion at the edge. In
    IEEE International Conference on Artificial Intelligence Circuits and Systems
    (AICAS’20). 55–58. Reference 1Reference 2 [133] Lau Billy Pik Lik, Marakkalage
    Sumudu Hasala, Zhou Yuren, Hassan Naveed Ul, Yuen Chau, Zhang Meng, and Tan U-Xuan.
    2019. A survey of data fusion in smart city applications. Inf. Fusion 52 (2019),
    357–374. Reference [134] Lee Juyong and Lee Jihoon. 2020. Juice recipe recommendation
    system using machine learning in MEC environment. IEEE Consum. Electron. Mag.
    9, 5 (2020), 79–84. Reference [135] Lee Kyung Chang, Kim Man Ho, Lee Suk, and
    Lee Hong Hee. 2004. IEEE-1451-based smart module for in-vehicle networking systems
    of intelligent vehicles. IEEE Trans. Industr. Electron. 51, 6 (2004), 1150–1158.
    Reference 1Reference 2 [136] Leukel Joerg. 2004. Standardization of product ontologies
    in B2B relationships-on the role of ISO 13584. In Americas Conference on Information
    Systems (AMCIS’04). 510. https://aisel.aisnet.org/cgi/viewcontent.cgi?article=2086&context=amcis2004
    Reference [137] Li Peilong, Xu Chen, Jin Hao, Hu Chunyang, Luo Yan, Cao Yu, Mathew
    Jomol, and Ma Yunsheng. 2020. ChainSDI: A software-defined infrastructure for
    regulation-compliant home-based healthcare services secured by blockchains. IEEE
    Syst. J. 14, 2 (2020), 2042–2053. Reference [138] Li Quanyi, Yao Haipeng, Mai
    Tianle, Jiang Chunxiao, and Zhang Yan. 2020. Reinforcement-learning- and belief-learning-based
    double auction mechanism for edge computing resource allocation. IEEE Internet
    Things J. 7, 7 (2020), 5976–5985. Reference [139] Li Youhuizi, Zhang Jiancheng,
    Jiang Congfeng, Wan Jian, and Ren Zujie. 2019. PINE: Optimizing performance isolation
    in container environments. IEEE Access 7 (2019), 30410–30422. Reference [140]
    Licht Torben R.. 2001. The IEEE 1451.4 proposed standard. IEEE Instrum. Measur.
    Mag. 4, 1 (2001), 12–18. Reference [141] Liu Alex X., Meiners Chad R., Norige
    Eric, and Torng Eric. 2014. High-speed application protocol parsing and extraction
    for deep flow inspection. IEEE J. Select. Areas Commun. 32, 10 (2014), 1864–1880.
    Reference [142] Liu Boyi, Lujia Wang, and Ming Liu. 2019. Lifelong Federated Reinforcement
    Learning: A learning architecture for navigation in cloud robotic systems. IEEE
    Robotics and Automation Letters 4, 4 (2019), 4555–4562. Reference 1Reference 2
    [143] Liu Fanzhen, Xue Shan, Wu Jia, Zhou Chuan, Hu Wenbin, Paris Cecile, Nepal
    Surya, Yang Jian, and Yu Philip S.. 2020. Deep learning for community detection:
    Progress, challenges and opportunities. In 29th International Joint Conference
    on Artificial Intelligence (IJCAI’20). 4981–4987. Reference [144] Liu Xiaolan,
    Yu Jiadong, Wang Jian, and Gao Yue. 2020. Resource allocation with edge computing
    in IoT networks via machine learning. IEEE Internet Things J. 7, 4 (2020), 3415–3426.
    Reference [145] Lu Wei, Meng Xianyu, and Guo Guanfei. 2019. Fast service migration
    method based on virtual machine technology for MEC. IEEE Internet Things J. 6,
    3 (2019), 4344–4354. Reference [146] Lu Yang. 2017. Industry 4.0: A survey on
    technologies, applications and open research issues. J. Industr. Inf. Integ. 6
    (2017), 1–10. Reference 1Reference 2 [147] Lu Yan, Witherell Paul, and Jones Albert.
    2020. Standard connections for IIoT empowered smart manufacturing. Manuf. Lett.
    26 (2020), 17–20. Reference [148] Lund Henrik, Østergaard Poul Alberg, Connolly
    David, and Mathiesen Brian Vad. 2017. Smart energy and smart energy systems. Energy
    137 (2017), 556–565. Reference 1Reference 2 [149] Luo Jia, Chen Qianbin, Yu F.
    Richard, and Tang Lun. 2020. Blockchain-enabled software-defined industrial internet
    of things with deep reinforcement learning. IEEE Internet Things J. 7, 6 (2020),
    5466–5480. Reference [150] Ma Lele, Yi Shanhe, Carter Nancy, and Li Qun. 2019.
    Efficient live migration of edge services leveraging container layered storage.
    IEEE Trans. Mob. Comput. 18, 9 (2019), 2020–2033. Reference [151] Ma Xiaoxiao,
    Wu Jia, Xue Shan, Yang Jian, Sheng Quan Z., and Xiong Hui. 2021. A comprehensive
    survey on graph anomaly detection with deep learning. arXiv preprint arXiv:2106.07178
    (2021). Reference [152] Mabkhot Mohammed M., Al-Ahmari Abdulrahman M., Salah Bashir,
    and Alkhalefah Hisham. 2018. Requirements of the smart factory system: A survey
    and perspective. Machines 6, 2 (2018), 23. Reference [153] Machen Andrew, Wang
    Shiqiang, Leung Kin K., Ko Bong Jun, and Salonidis Theodoros. 2017. Live service
    migration in mobile edge clouds. IEEE Wirel. Commun. 25, 1 (2017), 140–147. Reference
    [154] Maiti Ananda, Kist Alexander A., and Maxwell Andrew D.. 2018. Automata-based
    generic model for interoperating context-aware ad-hoc devices in internet of things.
    IEEE Internet Things J. 5, 5 (2018), 3837–3852. Reference 1Reference 2 [155] Mayer
    Simon, Hodges Jack, Yu Dan, Kritzler Mareike, and Michahelles Florian. 2017. An
    open semantic framework for the industrial internet of things. IEEE Intell. Syst.
    32, 1 (2017), 96–101. Reference [156] McGuinness Deborah L. and Harmelen Frank
    Van. 2004. OWL web ontology language overview. W3C Recomm. 10, 10 (2004), 2004.
    Reference [157] Medhane Darshan Vishwasrao, Sangaiah Arun Kumar, Hossain M. Shamim,
    Muhammad Ghulam, and Wang Jin. 2020. Blockchain-enabled distributed security framework
    for next generation IoT: An edge-cloud and software defined network integrated
    approach. IEEE Internet Things J. 7, 7 (2020), 6143–6149. Reference [158] Mendki
    Pankaj. 2018. Docker container based analytics at IoT edge video analytics usecase.
    In 3rd International Conference on Internet of Things: Smart Innovation and Usages
    (IoT-SIU’18). 1–4. Reference [159] Meng Zhaozong, Wu Zhipeng, Muvianto Cahyo,
    and Gray John. 2016. A data-oriented M2M messaging mechanism for industrial IoT
    applications. IEEE Internet Things J. 4, 1 (2016), 236–246. Reference 1Reference
    2 [160] Inc. Mesosphere. 2023. Marathon: A container orchestration platform for
    Mesos and DC/OS. Retrieved from https://mesosphere.github.io/marathon/ Reference
    [161] Microsoft. 2023. Azure IoT overview. Retrieved from https://azure.microsoft.com/en-au/overview/iot/#overview
    Reference 1Reference 2 [162] Minerva Roberto, Lee Gyu Myoung, and Crespi Noel.
    2020. Digital twin in the IoT context: A survey on technical features, scenarios,
    and architectural models. Proc. IEEE 108, 10 (2020), 1785–1824. Reference [163]
    Modak Arsh, Chaudhary S. D., Paygude P. S., and Ldate S. R.. 2018. Techniques
    to secure data on cloud: Docker swarm or Kubernetes? In 2nd International Conference
    on Inventive Communication and Computational Technologies (ICICCT’18). 7–12. Reference
    [164] Morariu Cristina, Morariu Octavian, Răileanu Silviu, and Borangiu Theodor.
    2020. Machine learning for predictive scheduling and resource allocation in large
    scale manufacturing systems. Comput. Industr. 120 (2020), 103244. Reference [165]
    Mrozek Dariusz, Koczur Anna, and Małysiak-Mrozek Bożena. 2020. Fall detection
    in older adults with mobile IoT devices and machine learning in the cloud and
    on the edge. Inf. Sci. 537 (2020), 132–147. Reference [166] Neto Luís, Gonçalves
    Gil, Torres Pedro, Dionísio Rogério, and Malhão Sérgio. 2019. An industry 4.0
    self description information model for software components contained in the administration
    shell. In 8th International Conference on Intelligent Systems and Applications.
    Reference [167] Ng Irene C. L. and Wakenshaw Susan Y. L.. 2017. The internet-of-things:
    Review and research directions. Int. J. Res. Market. 34, 1 (2017), 3–21. Reference
    [168] Ning Zhaolong, Kong Xiangjie, Xia Feng, Hou Weigang, and Wang Xiaojie. 2019.
    Green and sustainable cloud of things: Enabling collaborative edge computing.
    IEEE Commun. Mag. 57, 1 (2019), 72–78. Reference [169] Nishikiori Masaaki. 2011.
    Server virtualization with VMware vSphere 4. Fujitsu Scient. Technic. J. 47, 3
    (2011), 356–361. Reference [170] OMRON. 2023. Omron CP1W-20EDR1 datasheet. Retrieved
    from https://datasheet.octopart.com/CP1W-20EDR1-Omron-datasheet-12510914.pdf Reference
    1Reference 2Reference 3 [171] Orzechowski Michal, Balis Bartosz, Pawlik Krystian,
    Pawlik Maciej, and Malawski Maciej. 2018. Transparent deployment of scientific
    workflows across clouds—Kubernetes approach. In IEEE/ACM International Conference
    on Utility and Cloud Computing Companion (UCC Companion’18). 9–10. Reference [172]
    Oyekanlu Emmanuel. 2018. Osmotic collaborative computing for machine learning
    and cybersecurity applications in industrial IoT networks and cyber physical systems
    with Gaussian mixture models. In IEEE 4th International Conference on Collaboration
    and Internet Computing (CIC’18). IEEE, 326–335. Reference [173] Pahl Claus and
    Lee Brian. 2015. Containers and clusters for edge cloud architectures—A technology
    review. In 3rd International Conference on Future Internet of Things and Cloud.
    379–386. Reference [174] Park Junmin, Park Hyunjae, and Choi Young-June. 2018.
    Data compression and prediction using machine learning for industrial IoT. In
    International Conference on Information Networking (ICOIN’18). IEEE, 818–820.
    Reference 1Reference 2 [175] Pethig Florian, Niggemann Oliver, and Walter Armin.
    2017. Towards Industrie 4.0 compliant configuration of condition monitoring services.
    In IEEE 15th International Conference on Industrial Informatics (INDIN’17). IEEE,
    271–276. Reference [176] Petrenko Alexey S., Petrenko Sergei A., Makoveichuk Krystina
    A., and Chetyrbok Petr V.. 2018. The IIoT/IoT device control model based on narrow-band
    IoT (NB-IoT). In IEEE Conference of Russian Young Researchers in Electrical and
    Electronic Engineering (EIConRus’18). IEEE, 950–953. Reference [177] Petrolo Riccardo,
    Loscri Valeria, and Mitton Nathalie. 2017. Towards a smart city based on cloud
    of things, a survey on the smart city vision and paradigms. Trans. Emerg. Telecommun.
    Technol. 28, 1 (2017), e2931. Reference [178] Pietri Ilia and Sakellariou Rizos.
    2016. Mapping virtual machines onto physical machines in cloud computing: A survey.
    Comput. Surv. 49, 3 (2016). Reference [179] Portilla Jorge, Riesgo Teresa, and
    Castro Angel de. 2007. A reconfigurable FPGA-based architecture for modular nodes
    in wireless sensor networks. In 3rd Southern Conference on Programmable Logic.
    203–206. Reference 1Reference 2 [180] Potter D.. 2002. Smart plug and play sensors.
    IEEE Instrum. Measur. Mag. 5, 1 (2002), 28–30. Reference [181] Qian Gang, Lu Siliang,
    Pan Donghui, Tang Huasong, Liu Yongbin, and Wang Qunjing. 2019. Edge computing:
    A promising framework for real-time fault diagnosis and dynamic control of rotating
    machines using multi-sensor data. IEEE Sensors J. 19, 11 (2019), 4211–4220. Reference
    1Reference 2 [182] Qiu Tie, Chen Ning, Li Keqiu, Atiquzzaman Mohammed, and Zhao
    Wenbing. 2018. How can heterogeneous internet of things build our future: A survey.
    IEEE Commun. Surv. Tutor. 20, 3 (2018), 2011–2027. Reference [183] Qiu Tie, Chi
    Jiancheng, Zhou Xiaobo, Ning Zhaolong, Atiquzzaman Mohammed, and Wu Dapeng Oliver.
    2020. Edge computing in industrial internet of things: Architecture, advances
    and challenges. IEEE Commun. Surv. Tutor. 22, 4 (2020), 2462–2488. Reference [184]
    Rao Sriganesh K. and Prasad Ramjee. 2018. Impact of 5G technologies on Industry
    4.0. Wirel. Person. Commun. 100, 1 (2018), 145–159. Reference [185] Rathore M.
    Mazhar, Ahmad Awais, Paul Anand, and Jeon Gwanggil. 2015. Efficient graph-oriented
    smart transportation using internet of things generated big data. In 11th International
    Conference on Signal-Image Technology & Internet-based Systems (SITIS’15). IEEE,
    512–519. Reference [186] Rattihalli Gourav, Govindaraju Madhusudhan, and Tiwari
    Devesh. 2019. Towards enabling dynamic resource estimation and correction for
    improving utilization in an Apache Mesos cloud environment. In 19th IEEE/ACM International
    Symposium on Cluster, Cloud and Grid Computing (CCGRID’19). 188–197. Reference
    [187] Ray Partha Pratim. 2016. A survey of IoT cloud platforms. Fut. Comput. Inform.
    J. 1, 1–2 (2016), 35–46. Reference [188] Ren Yi, Liu Ling, Zhang Qi, Wu Qingbo,
    Guan Jianbo, Kong Jinzhu, Dai Huadong, and Shao Lisong. 2016. Shared-memory optimizations
    for inter-virtual-machine communication. Comput. Surv. 48, 4 (2016), 1–42. Reference
    [189] Ren Yuzheng, Xie Renchao, Yu F. Richard, Huang Tao, and Liu Yunjie. 2020.
    Potential identity resolution systems for the industrial internet of things: A
    survey. IEEE Commun. Surv. Tutor. 23, 1 (2020), 391–430. Reference [190] Ruan
    Wenjie, Sheng Quan Z., Yao Lina, Li Xue, Falkner Nickolas J. G., and Yang Lei.
    2018. Device-free human localization and tracking with UHF passive RFID tags:
    A data-driven approach. J. Netw. Comput. Applic. 104 (2018), 78–96. Reference
    [191] Sadeghi Ahmad-Reza, Wachsmann Christian, and Waidner Michael. 2015. Security
    and privacy challenges in industrial internet of things. In 52nd Annual Design
    Automation Conference. IEEE, 1–6. Reference [192] Sanpechuda T. and Kovavisaruch
    L.. 2008. A review of RFID localization: Applications and techniques. In 5th International
    Conference on Electrical Engineering/Electronics, Computer, Telecommunications
    and Information Technology. IEEE, 769–772. Reference [193] Satyanarayanan Mahadev.
    2017. The emergence of edge computing. Computer 50, 1 (2017), 30–39. Reference
    [194] Schmitt Jacqueline, Bönig Jochen, Borggräfe Thorbjörn, Beitinger Gunter,
    and Deuse Jochen. 2020. Predictive model-based quality inspection using machine
    learning and edge cloud computing. Adv. Eng. Inform. 45 (2020), 101101. Reference
    [195] Schneider. 2023. TM3XTRA1 remote transmitter module TM3 bus Schneider Electric.
    Retrieved June 23, 2023 from https://www.schneider-electric.com/en/product/TM3XTRA1/remote-transmitter-module-tm3---bus/
    Reference [196] Schneider Stan. 2017. The Industrial Internet of Things (IIoT)
    applications and taxonomy. Internet Things Data Analyt. Handb. (2017), 41–81.
    Reference [197] Electric Schneider. 2022. What Are EcoStruxure IT Software & Digital
    Services? Retrieved from https://ecostruxureit.com/what-is-ecostruxure-it/ Reference
    [198] Schweichhart Karsten. 2016. Reference Architectural Model Industrie 4.0
    (RAMI 4.0). Retrieved from https://www.plattform-i40.de Reference [199] Sengupta
    Jayasree, Ruj Sushmita, and Bit Sipra Das. 2020. A comprehensive survey on attacks,
    security issues and blockchain solutions for IoT and IIoT. J. Netw. Comput. Applic.
    149 (2020), 102481. Reference [200] Sharma Shree Krishna and Wang Xianbin. 2017.
    Live data analytics with collaborative edge and cloud processing in wireless IoT
    networks. IEEE Access 5 (2017), 4621–4635. Reference [201] Shashikant Kadam Rekha
    and Kulkarni Anju. 2020. Reconfigurable patch antenna design using pin diodes
    and Raspberry PI for portable device application. Wirel. Person. Commun. 112,
    3 (2020), 1809–1828. Reference [202] Shengdong Mu, Zhengxian Xiong, and Yixiang
    Tian. 2019. Intelligent traffic control system based on cloud computing and big
    data mining. IEEE Trans. Industr. Inform. 15, 12 (2019), 6583–6592. Reference
    1Reference 2 [203] Shi Weisong, Cao Jie, Zhang Quan, Li Youhuizi, and Xu Lanyu.
    2016. Edge computing: Vision and challenges. IEEE Internet Things J. 3, 5 (2016),
    637–646. Reference 1Reference 2 [204] Shvachko Konstantin, Kuang Hairong, Radia
    Sanjay, and Chansler Robert. 2010. The Hadoop distributed file system. In IEEE
    26th Symposium on Mass Storage Systems and Technologies (MSST’10). IEEE, 1–10.
    Reference [205] SIEMENS. 2023. Siemens 6ES76470AA001YA2. https://www.dacelsolutions.com/upload/dokumanlar/Siemens%206ES76470AA001YA2.pdf
    Reference 1Reference 2Reference 3 [206] Siemens. 2022. MindSphere documentation
    overview. Retrieved from https://siemens.mindsphere.io/en/docs/documentation-overview
    Reference [207] Sirisakdiwan Tanwa and Nupairoj Natawut. 2019. Spark framework
    for real-time analytic of multiple heterogeneous data streams. In 2nd International
    Conference on Communication Engineering and Technology (ICCET’19). IEEE, 1–5.
    Reference 1Reference 2 [208] Sisinni Emiliano, Saifullah Abusayeed, Han Song,
    Jennehag Ulf, and Gidlund Mikael. 2018. Industrial internet of things: Challenges,
    opportunities, and directions. IEEE Trans. Industr. Inform. 14, 11 (2018), 4724–4734.
    Navigate to [209] Sittón-Candanedo Inés, Alonso Ricardo S., Corchado Juan M.,
    Rodríguez-González Sara, and Casado-Vara Roberto. 2019. A review of edge computing
    reference architectures and a new global edge proposal. Fut. Gen. Comput. Syst.
    99 (2019), 278–294. Reference [210] Song Eugene Y., Burns Martin, Pandey Abhinav,
    and Roth Thomas. 2019. IEEE 1451 smart sensor digital twin federation for IoT/CPS
    Research. In IEEE Sensors Applications Symposium (SAS’19). IEEE, 1–6. Reference
    1Reference 2 [211] Song Eugene Y. and Lee Kang. 2008. Understanding IEEE 1451-networked
    smart transducer interface standard—What is a smart transducer? IEEE Instrum.
    Measur. Mag. 11, 2 (2008), 11–17. Reference 1Reference 2 [212] Stojanović Dragan
    H., Stojanović Natalija M., Đorđević Igor, and Ilić Aleksandra I. Stojnev. 2019.
    Sensor data fusion and big mobility data analytics for activity recognition. In
    14th International Conference on Advanced Technologies, Systems and Services in
    Telecommunications (TELSIKS’19). IEEE, 66–69. Reference 1Reference 2 [213] Su
    Xing, Xue Shan, Liu Fanzhen, Wu Jia, Yang Jian, Zhou Chuan, Hu Wenbin, Paris Cecile,
    Nepal Surya, Jin Di, Sheng Quan Z., and Yu Philip S.. 2022. A comprehensive survey
    on community detection with deep learning. IEEE Trans. Neural Netw. Learn. Syst.
    (2022), 1–21. https://ieeexplore.ieee.org/document/9732192 Reference [214] Sun
    Dawei, Lee Vincent C. S., and Lu Ye. 2016. An intelligent data fusion framework
    for structural health monitoring. In IEEE 11th Conference on Industrial Electronics
    and Applications (ICIEA’16). 49–54. Reference 1Reference 2 [215] Sun Danfeng,
    Xue Shan, Wu Huifeng, and Wu Jia. 2021. A data stream cleaning system using edge
    intelligence for smart city industrial environments. IEEE Trans. Industr. Inform.
    18, 2 (2021), 1165–1174. Reference [216] Sundaramurthy R. and Nagarajan V.. 2016.
    Design and implementation of reconfigurable virtual instruments using Raspberry
    Pi core. In International Conference on Communication and Signal Processing (ICCSP’16).
    2309–2313. Reference [217] Swetina Jorg, Lu Guang, Jacobs Philip, Ennesser Francois,
    and Song JaeSeung. 2014. Toward a standardized common M2M service layer platform:
    Introduction to oneM2M. IEEE Wirel. Commun. 21, 3 (2014), 20–26. Reference [218]
    Szilagyi Ioan and Wira Patrice. 2016. Ontologies and semantic web for the internet
    of things—A survey. In 42nd Annual Conference of the IEEE Industrial Electronics
    Society. IEEE, 6949–6954. Reference [219] Tajalli Seyede Zahra, Mardaneh Mohammad,
    Taherian-Fard Elaheh, Izadian Afshin, Kavousi-Fard Abdollah, Dabbaghjamanesh Morteza,
    and Niknam Taher. 2020. DoS-resilient distributed optimal scheduling in a fog
    supporting IIoT-based smart microgrid. IEEE Trans. Industr. Applic. 56, 3 (2020),
    2968–2977. Reference [220] Takahashi Kiyotaka, Ogata Yuji, and Nonaka Youichi.
    2017. A proposal of unified reference model for smart manufacturing. In 13th IEEE
    Conference on Automation Science and Engineering (CASE’17). 964–969. Reference
    [221] Tang Jie, Yu Rao, Liu Shaoshan, and Gaudiot Jean-Luc. 2020. A container
    based edge offloading framework for autonomous driving. IEEE Access 8 (2020),
    33713–33726. Reference 1Reference 2 [222] Tange Koen, Donno Michele De, Fafoutis
    Xenofon, and Dragoni Nicola. 2020. A systematic survey of industrial internet
    of things security: Requirements and fog computing opportunities. IEEE Commun.
    Surv. Tutor. 22, 4 (2020), 2489–2520. Reference [223] Tantik Erdal and Anderl
    Reiner. 2017. Integrated data model and structure for the asset administration
    shell in industrie 4.0. Procedia CIRP 60 (2017), 86–91. Reference [224] Trappey
    Amy J. C., Trappey Charles V., Govindarajan Usharani Hareesh, Chuang Allen C.,
    and Sun John J.. 2017. A review of essential standards and patent landscapes for
    the internet of things: A key enabler for industry 4.0. Adv. Eng. Inform. 33 (Aug.2017),
    208–229. Reference [225] Ursutiu D., Samoila C., Jinga V., and Altoe F.. 2016.
    The future of “hardware–software reconfigurable.” In International Conference
    on Interactive Collaborative Learning. Springer, 269–275. Reference [226] Valente
    Fredy João, Morijo João Paulo, Vivaldini Kelen Cristiane T., and Trevelin Luis
    Carlos. 2019. Fog-based data fusion for heterogeneous IoT sensor networks: A real
    implementation. In 15th International Conference on Network and Service Management
    (CNSM’19). IEEE, 1–5. Reference 1Reference 2 [227] Varga Pal, Peto Jozsef, Franko
    Attila, Balla David, Haja David, Janky Ferenc, Soos Gabor, Ficzere Daniel, Maliosz
    Markosz, and Toka Laszlo. 2020. 5G support for industrial IoT applications—Challenges,
    solutions, and research gaps. Sensors 20, 3 (2020), 828. Reference 1Reference
    2 [228] Wan Jiafu, Tang Shenglong, Li Di, Imran Muhammad, Zhang Chunhua, Liu Chengliang,
    and Pang Zhibo. 2018. Reconfigurable smart factory for drug packing in healthcare
    Industry 4.0. IEEE Trans. Industr. Inform. 15, 1 (2018), 507–516. Reference 1Reference
    2Reference 3 [229] Wan Jiafu, Tang Shenglong, Shu Zhaogang, Li Di, Wang Shiyong,
    Imran Muhammad, and Vasilakos Athanasios V.. 2016. Software-defined industrial
    internet of things in the context of industry 4.0. IEEE Sensors J. 16, 20 (2016),
    7373–7380. Reference [230] Wang Lan, Hayashi Shinpei, and Saeki Motoshi. 2021.
    Applying class distance to decide similarity on information models for automated
    data interoperability. Int. J. Softw. Eng. Knowl. Eng. 31, 03 (Mar.2021), 405–434.
    Reference [231] Wang Pan, Ye Feng, and Chen Xuejiao. 2018. A smart home gateway
    platform for data collection and awareness. IEEE Commun. Mag. 56, 9 (2018), 87–93.
    Reference [232] Wang Shulong, Hou Yibin, Gao Fang, and Ji Xinrong. 2016. A novel
    IoT access architecture for vehicle monitoring system. In IEEE 3rd World Forum
    on Internet of Things (WF-IoT’16). 639–642. Reference [233] Wang Wendong, Feng
    Cheng, Zhang Bo, and Gao Hui. 2019. Environmental monitoring based on fog computing
    paradigm and internet of things. IEEE Access 7 (2019), 127154–127165. Reference
    1Reference 2 [234] Wegener P. D.. 2018. German standardization roadmap industrie
    4.0 version 3. DIN e 2018 (2018). Reference [235] Wei Jiantao, Zhang Naiqian,
    Wang Ning, Lenhert Donald, Neilsen Mitchell, and Mizuno Masaaki. 2005. Use of
    the “smart transducer” concept and IEEE 1451 standards in system integration for
    precision agriculture. Comput. Electron. Agric. 48, 3 (2005), 245–255. Reference
    1Reference 2 [236] Willner Alexander, Diedrich Christian, Younes Raéd Ben, Hohmann
    Stephan, and Kraft Andreas. 2017. Semantic communication between components for
    smart factories based on oneM2M. In 22nd IEEE International Conference on Emerging
    Technologies and Factory Automation (ETFA’17). IEEE, 1–8. Reference [237] Witkowski
    Krzysztof. 2017. Internet of things, big data, industry 4.0—Innovative solutions
    in logistics and supply chains management. Procedia Eng. 182 (2017), 763–769.
    Reference 1Reference 2 [238] Wu Huifeng, Hu Junjie, Sun Jiexiang, and Sun Danfeng.
    2019. Edge computing in an IoT base station system: Reprogramming and real-time
    tasks. Complexity 2019 (2019). https://www.hindawi.com/journals/complexity/2019/4027638/
    Reference [239] Wu Huifeng, Sun Danfeng, Peng Lan, Yao Yuan, Wu Jia, Sheng Quan
    Z., and Yan Yi. 2019. Dynamic edge access system in IoT environment. IEEE Internet
    Things J. 7, 4 (2019), 2509–2520. Reference [240] Wu Huifeng, Yan Yi, Sun Danfeng,
    and Rene Simon. 2019. A customized real-time compilation for motion control in
    embedded PLCs. IEEE Trans. Industr. Inform. 15, 2 (2019), 812–821. Reference 1Reference
    2 [241] Wu Huifeng, Yan Yi, Sun Danfeng, and Simon Rene. 2019. VCA protocol-based
    multilevel flexible architecture on embedded PLCs for visual servo control. IEEE
    Trans. Industr. Electron. 67, 3 (2019), 2450–2459. Reference [242] Wu Yulei, Dai
    Hong-Ning, Wang Haozhe, Xiong Zehui, and Guo Song. 2022. A survey of intelligent
    network slicing management for industrial IoT: Integrated approaches for smart
    transportation, smart energy, and smart factory. IEEE Commun. Surv. Tutor. 24,
    2 (2022), 1175–1211. Reference [243] Wübbeke Jost, Meissner Mirjam, Zenglein Max
    J., Ives Jaqueline, and Conrad Björn. 2016. Made in China 2025. Mercator Instit.
    China Studies. Pap. China 2 (2016), 74. Reference [244] Hao Xiangdong, Li Fei,
    and Gao Xiaoguang. 2015. Construction of information fusion system based on cloud
    computing. In 4th International Conference on Computer Science and Network Technology
    (ICCSNT’15). 1461–1465. Reference 1Reference 2 [245] Xiao Yonggang, Liu Yanbing,
    and Li Tun. 2020. Edge computing and blockchain for quick fake news detection
    in IoV. Sensors 20, 16 (2020), 4360. Reference [246] Xin Xu, Yan Zhang, Yueying
    Hao, Yulei Jiang, and Mingzhi Geng. 2022. Research of container security reinforcement
    multi-service APP deployment for new power system on substation. In 4th Asia Energy
    and Electrical Engineering Symposium (AEEES’22). 945–949. Reference [247] Xiong
    Ying, Sun Yulin, Xing Li, and Huang Ying. 2018. Extend cloud to edge with KubeEdge.
    In IEEE/ACM Symposium on Edge Computing (SEC’18). 373–377. Reference [248] Xu
    Hansong, Yu Wei, Griffith David, and Golmie Nada. 2018. A survey on industrial
    internet of things: A cyber-physical systems perspective. IEEE Access 6 (2018),
    78238–78259. Reference [249] Xu Yang, Ren Ju, Wang Guojun, Zhang Cheng, Yang Jidian,
    and Zhang Yaoxue. 2019. A blockchain-based nonrepudiation network computing service
    scheme for industrial IoT. IEEE Trans. Industr. Inform. 15, 6 (2019), 3632–3641.
    Reference [250] Yan Jianghui, Liu Jinping, and Tseng Fang-Mei. 2020. An evaluation
    system based on the self-organizing system framework of smart cities: A case study
    of smart transportation systems in China. Technol. Forecast. Social Change 153
    (2020), 119371. Reference [251] Yang Linfu and Liu Bin. 2019. Temporal data fusion
    at the edge. In IEEE International Conferences on Ubiquitous Computing & Communications
    (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing,
    Networking and Services (SmartCNS). IEEE, 9–14. Reference 1Reference 2 [252] Yao
    Lina, Sheng Quan Z., Li Xue, Gu Tao, Tan Mingkui, Wang Xianzhi, Wang Sen, and
    Ruan Wenjie. 2017. Compressive representation for device-free activity recognition
    with passive RFID signal strength. IEEE Trans. Mob. Comput. 17, 2 (2017), 293–306.
    Reference [253] Yazdinejad Abbas, Parizi Reza M., Dehghantanha Ali, Zhang Qi,
    and Choo Kim-Kwang Raymond. 2020. An energy-efficient SDN controller architecture
    for IoT networks with blockchain-based security. IEEE Trans. Serv. Comput. 13,
    4 (2020), 625–638. Reference [254] Ye Xun and Hong Seung Ho. 2019. Toward industry
    4.0 components: Insights into and implementation of asset administration shells.
    IEEE Industr. Electron. Mag. 13, 1 (Mar.2019), 13–25. Reference [255] Yin ChuanTao,
    Xiong Zhang, Chen Hui, Wang JingYuan, Cooper Daven, and David Bertrand. 2015.
    A literature survey on smart cities. Sci. China Inf. Sci. 58, 10 (2015), 1–18.
    Reference 1Reference 2 [256] Yli-Ojanperä Matti, Sierla Seppo, Papakonstantinou
    Nikolaos, and Vyatkin Valeriy. 2019. Adapting an agile manufacturing concept to
    the reference architecture model industry 4.0: A survey and case study. J. Industr.
    Inf. Integ. 15 (Sept.2019), 147–160. Reference [257] Yue Xiao, Wang Huiju, Jin
    Dawei, Li Mingqiang, and Jiang Wei. 2016. Healthcare data gateways: Found healthcare
    intelligence on blockchain with novel privacy risk control. J. Med. Syst. 40,
    10 (2016), 218. Reference [258] Zaharia Matei, Chowdhury Mosharaf, Das Tathagata,
    Dave Ankur, Ma Justin, McCauly Murphy, Franklin Michael J., Shenker Scott, and
    Stoica Ion. 2012. Resilient distributed datasets: A fault-tolerant abstraction
    for in-memory cluster computing. In 9th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI’12). 15–28. Reference [259] Zezulka František,
    Marcon P., Bradac Zdenek, Arm Jakub, Benesl T., and Vesely Ivo. 2018. Communication
    systems for industry 4.0 and the IIoT. IFAC-PapersOnLine 51, 6 (2018), 150–155.
    Reference [260] Zhang Daniel, Vance Nathan, and Wang Dong. 2019. When social sensing
    meets edge computing: Vision and challenges. In 28th International Conference
    on Computer Communication and Networks (ICCCN’19). IEEE, 1–9. Reference [261]
    Zhang Lingwen, Xiao Ning, Yang Wenkao, and Li Jun. 2019. Advanced heterogeneous
    feature fusion machine learning models and algorithms for improving indoor localization.
    Sensors 19, 1 (2019), 125. Reference 1Reference 2 [262] Zhang Lili, Xie Yuxiang,
    Xidao Luan, and Zhang Xin. 2018. Multi-source heterogeneous data fusion. In International
    Conference on Artificial Intelligence and Big Data (ICAIBD’18). 47–51. Reference
    1Reference 2Reference 3 [263] Zhang Qikun, Li Yongjiao, Wang Ruifang, Liu Lu,
    Tan Yu-an, and Hu Jingjing. 2021. Data security sharing model based on privacy
    protection for blockchain-enabled industrial Internet of Things. Int. J. Intell.
    Syst. 36, 1 (2021), 94–111. Reference [264] Zhang Yin, Qian Yongfeng, Wu Di, Hossain
    M. Shamim, Ghoneim Ahmed, and Chen Min. 2019. Emotion-aware multimedia systems
    security. IEEE Trans. Multim. 21, 3 (2019), 617–624. Reference [265] Zhang Zheng,
    Huang Liang, Tang Renzhong, Peng Tao, Guo Lihang, and Xiang Xingwei. 2020. Industrial
    blockchain of things: A solution for trustless industrial data sharing and beyond.
    In IEEE 16th International Conference on Automation Science and Engineering (CASE’20).
    1187–1192. Reference [266] Zhaofeng Ma, Xiaochang Wang, Jain Deepak Kumar, Khan
    Haneef, Hongmin Gao, and Zhen Wang. 2020. A blockchain-based trusted data management
    scheme in edge computing. IEEE Trans. Industr. Inform. 16, 3 (2020), 2013–2021.
    Reference [267] Zheng Pai, Sang Zhiqian, Zhong Ray Y., Liu Yongkui, Liu Chao,
    Mubarok Khamdi, Yu Shiqiang, and Xu Xun. 2018. Smart manufacturing systems for
    industry 4.0: Conceptual framework, scenarios, and future perspectives. Front.
    Mechan. Eng. 13, 2 (2018), 137–150. Reference 1Reference 2Reference 3 [268] Zhou
    Funa, Hu Po, Yang Shuai, and Wen Chenglin. 2018. A multimodal feature fusion-based
    deep learning method for online fault diagnosis of rotating machinery. Sensors
    18, 10 (2018), 3521. Reference 1Reference 2 [269] Zhou Yuqing and Xue Wei. 2018.
    A multisensor fusion method for tool condition monitoring in milling. Sensors
    18, 11 (2018), 3866. Reference 1Reference 2 [270] Zhu Tao, Dhelim Sahraoui, Zhou
    Zhihao, Yang Shunkun, and Ning Huansheng. 2017. An architecture for aggregating
    information from distributed data nodes for industrial internet of things. Comput.
    Electric. Eng. 58 (2017), 337–349. Reference Index Terms A Comprehensive Survey
    on Collaborative Data-access Enablers in the IIoT Computer systems organization
    Architectures Distributed architectures Cloud computing Other architectures Data
    flow architectures Information systems Data management systems Recommendations
    A Comprehensive Survey on Interoperability for IIoT: Taxonomy, Standards, and
    Future Directions In the era of Industry 4.0, the Internet-of-Things (IoT) performs
    the driving position analogous to the initial industrial metamorphosis. IoT affords
    the potential to couple machine-to-machine intercommunication and real-time information-gathering
    within ... Read More A Comprehensive Survey on Attacks, Security Issues and Blockchain
    Solutions for IoT and IIoT Abstract In recent years, the growing popularity of
    Internet of Things (IoT) is providing a promising opportunity not only for the
    development of various home automation systems but also for different industrial
    applications. By leveraging ... Read More Blockchain-envisioned access control
    for internet of things applications: a comprehensive survey and future directions
    Abstract With rapid advancements in the technology, almost all the devices around
    are becoming smart and contribute to the Internet of Things (IoT) network. When
    a new IoT device is added to the network, it is important to verify the authenticity
    of the ... Read More Comments 150+ References View Issue’s Table of Contents Footer
    Categories Journals Magazines Books Proceedings SIGs Conferences Collections People
    About About ACM Digital Library ACM Digital Library Board Subscription Information
    Author Guidelines Using ACM Digital Library All Holdings within the ACM Digital
    Library ACM Computing Classification System Digital Library Accessibility Join
    Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: ACM Computing Surveys
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Comprehensive Survey on Collaborative Data-access Enablers in the IIoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kale A.
  - Sun Z.
  - Fan C.
  - Ma X.
  citation_count: '1'
  description: Recently, workflow management platforms are gaining more attention
    in the artificial intelligence (AI) community. Traditionally, researchers self-managed
    their workflows in a manual and tedious way that heavily relies on their memory.
    Due to the complexity and unpredictability of AI models, they often struggled
    to track and manage all the data, steps, and history of the workflow. AI workflows
    are time-consuming, redundant, and error-prone, especially when big data is involved.
    A common strategy to make these workflows more manageable is to use a workflow
    management system, and we recommend Geoweaver, an open-source workflow management
    system that enables users to create, modify and reuse AI workflows all in one
    place. To make our work in Geoweaver reusable by the other workflow management
    systems, we created an add-on functionality geoweaver_cwl, a Python package that
    automatically converts Geoweaver AI workflows into the Common Workflow Language
    (CWL) format. It will allow researchers to easily share, exchange, modify, reuse,
    and build a new workflow from existing ones in other CWL-compliant software. A
    user study was conducted with the existing workflows created by Geoweaver to collect
    suggestions and fill in the gaps between our package and Geoweaver. The evaluation
    confirms that geoweaver_cwl can lead to a well-versed AI process while disclosing
    opportunities for further extensions. The geoweaver_cwl package is publicly released
    online at https://pypi.org/project/geoweaver-cwl/0.0.1/.
  doi: 10.1016/j.acags.2023.100126
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Technical framework of
    the geoweaver_cwl package 3. Use case implementation, result, and evaluation 4.
    Discussion 5. Conclusions Code availability CRediT authorship contribution statement
    Declaration of competing interest Acknowledgment Data availability References
    Show full outline Cited by (1) Figures (9) Show 3 more figures Applied Computing
    and Geosciences Volume 19, September 2023, 100126 Geoweaver_cwl: Transforming
    geoweaver AI workflows to common workflow language to extend interoperability
    Author links open overlay panel Amruta Kale a, Ziheng Sun b c, Chao Fan a, Xiaogang
    Ma a d Show more Share Cite https://doi.org/10.1016/j.acags.2023.100126 Get rights
    and content Under a Creative Commons license open access Highlights • Built a
    python package geoweaver_cwl based on the Common Workflow Language (CWL). • Geoweaver_cwl
    can automatically transform Geoweaver workflows into CWL. • Utility of geoweaver_cwl
    was verified with several existing Geoweaver workflows. • Geoweaver_cwl has wide
    applicability in geosciences to facilitate open science. Abstract Recently, workflow
    management platforms are gaining more attention in the artificial intelligence
    (AI) community. Traditionally, researchers self-managed their workflows in a manual
    and tedious way that heavily relies on their memory. Due to the complexity and
    unpredictability of AI models, they often struggled to track and manage all the
    data, steps, and history of the workflow. AI workflows are time-consuming, redundant,
    and error-prone, especially when big data is involved. A common strategy to make
    these workflows more manageable is to use a workflow management system, and we
    recommend Geoweaver, an open-source workflow management system that enables users
    to create, modify and reuse AI workflows all in one place. To make our work in
    Geoweaver reusable by the other workflow management systems, we created an add-on
    functionality geoweaver_cwl, a Python package that automatically converts Geoweaver
    AI workflows into the Common Workflow Language (CWL) format. It will allow researchers
    to easily share, exchange, modify, reuse, and build a new workflow from existing
    ones in other CWL-compliant software. A user study was conducted with the existing
    workflows created by Geoweaver to collect suggestions and fill in the gaps between
    our package and Geoweaver. The evaluation confirms that geoweaver_cwl can lead
    to a well-versed AI process while disclosing opportunities for further extensions.
    The geoweaver_cwl package is publicly released online at https://pypi.org/project/geoweaver-cwl/0.0.1/.
    Previous article in issue Next article in issue Keywords AI workflowsExplainabilityTransparencyProvenanceCommon
    workflow language 1. Introduction We are witnessing a widespread adoption of artificial
    intelligence (AI) and machine learning (ML) in our everyday life. The recent success
    of deep learning (DL) has largely contributed to the huge success of AI/ML models.
    DL algorithms are widely used in mission-critical applications like healthcare,
    autonomous robots and vehicles, image classification, and detection. Despite the
    significant improvement in performance and predictions, the black-box nature of
    DL algorithms can raise social and ethical questions about their operations and
    results. Even the programmer designing the complex AI/ML model finds it difficult
    to gain insight into an internal system that is often opaque. This issue has extended
    the research focus from improving accuracy to explainable and interpretable ML
    models (Doshi-Velez et al., 2017; Gilpin et al., 2018; Adadi and Berrada, 2018;
    Wing, 2020; Sun et al., 2022). Recent interests in explainable artificial intelligence
    (XAI) and trustworthy artificial intelligence (TAI) have achieved great momentum
    in making AI/ML models more explainable, interpretable, and transparent (Adadi
    and Berrada, 2018; Rudin, 2018, 2019; Wing, 2020). XAI proposes a shift toward
    more transparent AI. It aims to develop a set of strategies to make ML models
    more explainable while maintaining their high predictive accuracy (Ribeiro et
    al., 2016; Gunning and Aha, 2019). As the field of XAI continues to expand, it
    is important to develop new research strategies that include the provenance of
    upstream steps and history model runs. The diverse nature of AI/ML models in the
    field of XAI requires a multi-disciplinary approach, and in our previous papers,
    we highlighted the importance of provenance documentation and its benefit for
    AI/ML models (Kale et al., 2023; Kale and Ma, 2023). We suggest that adopting
    approaches and methods from the field of provenance will help to generate resourceful
    explanations and improve reproducibility (Ma et al., 2017; Zeng et al., 2019;
    Kale et al., 2023). Provenance provides transparency into the data processing
    steps, allowing researchers to understand how the data was created and processed.
    This enables researchers to reproduce the results by repeating the same steps,
    ensuring that the results are reliable and can be trusted. Additionally, provenance
    can be utilized for quality control purposes by allowing researchers to detect
    errors or inconsistencies in the data processing steps. This helps ensure that
    the data is of superior quality and that the outcomes are trustworthy and reproducible.
    Scientific workflow management systems like Kepler (Altintas et al., 2004), DataRobot
    (DataRobotCloud, 2012), Datatron (Datatron, 2016), Metaclip (Bedia et al., 2019),
    Amazon SageMaker (Das et al., 2020), and Geoweaver (Sun et al., 2020) are widely
    utilized for data analysis, providing a means for informed decision-making, and
    promoting innovation. These tools provide several ways to explore the provenance
    repository by tracking model activity, recording changes in the data and model,
    and outlining best practices for data processing. However, their growing popularity
    has led to concerns regarding collaboration and the possibility of hindering workflow
    reusability and portability. To address this concern, we support a standardized
    approach to computational workflows that fosters collaboration and mitigates these
    risks. This paper highlights the significance of Common Workflow Language (CWL),
    a practical set of standards that enables the description and sharing of computational
    workflows among a diverse community of users in various fields of science and
    engineering. This paper provides an overview of the CWL standards and our new
    Python package, geoweaver_cwl that transform Geoweaver AI/ML workflows into CWL
    scripts. We emphasize the importance of standardization in promoting collaboration
    and workflow portability and highlight how CWL can provide a practical solution
    to these challenges. By promoting the adoption of CWL standards and our geoweaver_cwl
    package, we aim to advance the field of computational workflows and promote their
    effective use in scientific research and engineering. This paper will describe
    how the tool is developed and implemented in our use cases and is organized as
    follows. In section 2, we first describe an overview of CWL, followed by the conceptual
    framework of Geoweaver, and then describe the architecture of geoweaver_cwl. In
    section 3, we demonstrate our Python package by applying a use case from the Geoweaver
    platform and assess the quality of the package and its influence in Geoweaver.
    In section 4, we discuss the importance of adopting CWL standards and highlight
    the future direction of our work. Finally, we conclude with a few additional remarks.
    2. Technical framework of the geoweaver_cwl package 2.1. An overview of the common
    workflow language CWL is a community standard to describe command-line-based workflows
    (Amstutz et al., 2016). It offers a typical but simplified set of generalizations
    that are commonly implemented in many popularworkflow management systems. The
    language''s declarative format enables users to describe the process of executing
    diverse software tools and workflows through their command-line interface. Previously,
    to link the command-line tools researchers need to write shell scripts. Although
    these scripts offer an efficient approach to accessing the tools, writing, and
    maintaining them requires specialized knowledge. As a result, researchers spend
    more time maintaining the scripts than conducting their research (Sun et al.,
    2022). However, with the increase in workflow popularity, the number of workflow
    management tools has increased, and each of them has its standards for specifying
    the tools and workflows. This has reduced the portability and interoperability
    of these workflows. CWL aims to reduce the barrier to researchers using these
    technologies by providing a standard to unify them. CWL standards explicitly support
    the usage of container technologies like Docker, Singularity, Shifter. These container
    technologies allow encapsulation of software dependencies and system configuration
    ensuring that the workflow can be executed in a consistent and reproducible environment,
    regardless of the underlying system (Pahl et al., 2017). In addition to providing
    a consistent execution environment, container technologies also facilitate the
    sharing and reuse of workflows. By packaging the workflow and its dependencies
    in a container image, it can be easily shared and executed on other systems without
    the need to install additional software or configure the environment. Overall,
    container technologies play a critical role in ensuring the reproducibility and
    portability of CWL workflows. 2.2. Conceptual framework of the geoweaver workflow
    management system Geoweaver is a unique platform designed for NASA''s Earth Observing
    System Data and Information System (EOSDIS), which provides earth scientists with
    the ability to manage, share, replicate, and reuse artificial intelligence/machine
    learning (AI/ML) workflows. The platform is equipped with a user-friendly graphical
    interface that enables individuals with limited programming experience to create
    and execute workflows with ease. Geoweaver offers a comprehensive range of AI
    workflows that include data preprocessing, training, testing of AI algorithms,
    and post-processing of results into an ad hoc automated workflow, which is particularly
    useful for AI practitioners (Sun et al., 2020). One of the unique features of
    Geoweaver is its integration with open-source software tools that are commonly
    used in geospatial data analysis. This integration enables users to incorporate
    different software packages into their workflows seamlessly without requiring
    extensive knowledge of each tool individually. Furthermore, Geoweaver''s versatility
    enables it to support various data formats and processing capabilities, making
    it a valuable tool for individuals working in different fields such as environmental
    science, agriculture, and urban planning. Geoweaver''s scalability is another
    notable advantage. The platform is built on a distributed computing architecture
    that can manage large geospatial datasets and perform computationally intensive
    analyses. Additionally, Geoweaver supports high-performance computing resources
    such as multi-core CPUs, clusters, and cloud computing, which enhance its computational
    power. Geoweaver is a unique and valuable tool for managing geospatial data workflows.
    Its flexible workflow composition, integration with open-source software tools,
    scalability, and support for a wide range of data formats and processing capabilities
    make it an ideal platform for AI practitioners and earth scientists. The fundamental
    design of Geoweaver is organized into three modules (Host, Process, and Workflow),
    which enable AI practitioners to sort and reuse their AI/ML experiments. • Host:
    This module serves as the cornerstone for the framework, differentiating it from
    other workflow management system. It enables users to connect to several resources
    such as virtual machines, Jupyter server instances, Secure shell (SSH), and third-party
    computing platforms like Google Earth Engine, Jupyter Notebook Server, and Google
    Colab. Additionally, the file transfer services (file uploading from local computers
    to remote servers, and file downloading from remote servers to local computers)
    provided by the host module allow users to transfer their workflow from one platform
    to another. • Process: This module includes five submodules and one database.
    As most of the current AI/ML experiments employ Python programming, the process
    module supports Python, Jupyter Notebook, Shell scripting (bash), and SSH for
    running system-level programs. All the dependent libraries like Deep Learning,
    Keras, PyTorch, and TensorFlow are easily accessible in the process. The process
    editor/creator interface allows users to create new processes and edit existing
    ones. Whenever a new process is created, it gets stored in a MySQL database. The
    process monitor is responsible for all the execution events in the process module
    and reports the real-time status. Once the execution is complete the input, output,
    and code that has been executed will be recorded and stored in a database. The
    provenance manager is responsible for evaluating the recorded history of each
    process in order to assess data quality and recover from failure. • Workflow:
    The term “Workflow” is a wide-ranging phrase that can be interpreted in a variety
    of ways (Jablonski and Bussler, 1996; Van der Aalst, 1998; Kiepuszewski et al.,
    2003). For instance, many geoscientists often refer to Jupyter Notebook or bash
    script as a workflow. In Geoweaver, workflow denotes a pipeline linking multiple
    processes together. The workflow module consists of two functions (1) Building
    workflows from the existing process and (2) Managing the query, edits, and execution
    of the workflows. Geoweaver supports not only DAG (Directed Acyclic Graph) workflow,
    but also other types of workflows such as cyclic, linear, and branching workflows.
    DAG workflows are commonly used in Geoweaver as they are well-suited for managing
    complex workflows with many interdependent tasks. However, Geoweaver''s support
    for different workflow types allows users to choose the most appropriate workflow
    pattern for their specific needs. The workflow module displays a color-coded real-time
    status of each process in the execution mode. Different colors represent the status
    of each process: blue means the process is waiting; yellow means the process is
    running; green means the process is finished running; and red means the process
    failed. A more detailed demonstration of Geoweaver is described in a previous
    paper (Kale and Ma, 2023). Exporting and importing the existing workflows in Geoweaver
    is simple and easy. The downloaded workflow can be automatically loaded into the
    workspace and ready for execution and reuse. Fig. 1 describes the framework of
    Geoweaver with the three core modules. Download : Download high-res image (797KB)
    Download : Download full-size image Fig. 1. Workflow management framework of Geoweaver
    and its core modules (Host, Process, and Workflow), adapted from (Sun et al.,
    2020). 2.3. Architecture of the geoweaver_cwl wrapper tool There are several workflow
    management system and languages which are used for expressing workflows into CWL.
    The Galaxy platform, widely used for managing and analyzing genomic data is a
    popular system that can translate existing workflows into CWL in order to share
    and reproduce the existing workflows (Gu et al., 2021). Snakemake is another popular
    workflow management system in the bioinformatics community where workflows can
    be automatically exported to CWL (Köster and Rahmann, 2012). Nextflow is a workflow
    management system designed for big data processing is also used for expressing
    workflows into CWL (Di et al., 2017). In this paper we designed a wrapper tool
    named geoweaver_cwl, we regard “wrapper tool” as a process of using CWL to describe
    command-line tools so that they can be run as an application or a tool in part
    of a larger workflow. Using the wrapper tool with CWL will make all the documents
    portable, sharable, and executable. The preliminary step for creating a workflow
    in Geoweaver is through the workflow module. The workspace allows users to compose
    a workflow using existing processes. Once the workflow is created it can be downloaded
    with two options “workflow with process code” or “workflow with process code and
    history”. The first will simply download the workflow and source code. The latter
    will download all the history of the prior workflow executions in addition to
    the source code and workflow. The downloaded workflow comes with a Zip file that
    includes a code folder, a history folder, and a workflow file. The code folder
    contains the code files (processes) used to form the workflow, the history folder
    contains the historical details of each process like the begin_time, end_time,
    input, and output. The workflow file contains the information on the nodes and
    edges that link together to form the workflow. To further extend the portability
    and interoperability of workflows built in the Geoweaver framework, we designed
    geoweaver_cwl, a Python package that captures inputs (source and target processes)
    from a Geoweaver workflow file and transforms them into CWL scripts. A key contribution
    to our work is an add-on functionality that dynamically generates corresponding
    CWL code without the user having to know the CWL syntax. The CWL file features
    text fields that comprehensively describe workflow commands and parameters. Fig.
    2 illustrates a brief architecture of geoweaver_cwl. The package contains two
    main functions “generate_cwl” and “generate_yml”. The generate_cwl function takes
    workflow. json from Geoweaver as the input, captures the nodes, and the edges
    from the workflow, and writes the steps that form the data flow into CWL scripts.
    To capture the source and target from the workflow file, we iteratively visit
    each node in the workflow, and each visited node that has not been previously
    processed becomes a source node. Then, for each source node, we compile the child
    nodes, and each child node serves as a target for the source node. Each source-target
    pair is processed by writing a CWL script that provides explicit inputs and outputs
    for each phase. Carrying out this procedure eventually enables us to generate
    the CWL scripts for the whole workflow. Equations (1), (2)) below describes the
    process of translating the workflow file into CWL. (1) (2) where  = process_list
    Download : Download high-res image (395KB) Download : Download full-size image
    Fig. 2. Architecture of geoweaver_cwl package with key functions.  = Graph edge
    extraction function W,  = file writing functions Additionally, the function also
    generates a new subdirectory called “elementary_cwl_files” which stores new CWL
    files (the processes used in the workflow) translated from the code folder. Below
    is the pseudo-code of the generate_cwl and generate_yml functions. Graph edge
    extraction function. Download : Download high-res image (212KB) Download : Download
    full-size image File writing function for workflow. cwl. Download : Download high-res
    image (107KB) Download : Download full-size image File writing function for elementary
    CWL files. Download : Download high-res image (51KB) Download : Download full-size
    image The generate_yml function produces a Yet Another Markup Language (YAML)
    file, which writes the input to run the workflow. cwl file. The YAML file describes
    which input to run for the cwl files. Download : Download high-res image (40KB)
    Download : Download full-size image The geoweaver_cwl package is fully open access
    and the installation is simple. The package can be downloaded from: https://pypi.org/project/geoweaver-cwl/0.0.1/.
    Fig. 3 demonstrates the installation steps for the geoweaver_cwl package along
    with the use of some functions. To facilitate reuse and adaptation, we have made
    the source code, a detailed user guide, and concrete self-contained examples file
    available on GitHub under an open-source license: https://github.com/amrutakale08/geoweaver_cwl
    and self-contained example on https://github.com/amrutakale08/geoweaver_cwl-usecases.
    Download : Download high-res image (335KB) Download : Download full-size image
    Fig. 3. Installation and usage of the geoweaver_cwl package. Once the workflow
    files are described in CWL scripts, they can be executed using any other software
    that supports CWL, like cwltool, Arvados, Toil, CWL-Airflow, and more. In this
    paper, we are going to use the traditional cwltool. To run the newly generated
    CWL files from Geoweaver, we will use the below command. We invoke cwl_runner
    with workflow. cwl and input object input. yml on the command line. Download :
    Download high-res image (35KB) Download : Download full-size image The command
    will trigger all the functions inside the CWL and YAML files in the same order
    as Geoweaver and is supposed to get the same results. As mentioned above, the
    advantage of CWL is that it provides a solution for describing portable and reusable
    workflows. The transformation from Geoweaver to CWL through the geoweaver_cwl
    package allows geoscientists to easily share, exchange, modify, and reuse workflows.
    Additionally, CWL-compliant applications are highly portable and can be run in
    a variety of environments, including local or cloud infrastructures. 3. Use case
    implementation, result, and evaluation Based on the geoweaver_cwl package, we
    tested a list of workflows from simple to complicated ones. Here we use a Geoweaver
    workflow available on GitHub (https://github.com/earth-artificial-intelligence/kenya-crop-mask-geoweaver)
    to demonstrate and verify the usability of our package. The scientific topic of
    that workflow is the annual and in-season mapping of cropland in Kenya (Tseng
    et al., 2020). The GitHub repository contains the code folder, history folder,
    and workflow. json file. We installed the geoweaver_cwl package and followed the
    above-mentioned procedures to describe the workflow in the CWL text document.
    After using the functions generate_cwl and generate_yml, we obtained the files
    “input.yml”, “workflow.cwl”, and “elementary cwl files folder”, which included
    the cwl files used in creating the workflow. The workflow translation process
    was fast and easy, and we also noticed that using cwltool speeds up workflow execution
    compared to the original procedure in Geoweaver. Yet, we still need to run more
    use cases to see whether CWL and cwltools always have shorter execution time comparing
    with Geoweaver. We successfully transformed the Geoweaver workflow of Kenya cropland
    mapping into CWL format using the geoweaver_cwl package. The left part of Fig.
    4 shows the described workflow in CWL from the workflow. json in Geoweaver. The
    CWL file contains a cwlVersion section which indicates the version of the CWL
    document. The class section with a value of Workflow indicates that this document
    describes the workflow. The inputs and outputs sections describe the inputs and
    outputs of the workflow, respectively. The steps section describes the actual
    steps of the workflow. In this example, the first step is to run the “scripts_exports.cwl”
    present in the folder elementary_cwl_files. The code of “scripts_exports.cwl”
    is illustrated in the right part of Fig. 4. The workflow steps in CWL do not always
    run in the written sequence. Instead, the order is determined by the dependencies
    across steps. To evaluate the result of the transformation we ran the CWL text
    document using cwltool, and we observed that it executed smoothly and generated
    the same result as in Geoweaver. The CWL result of this example is accessible
    on GitHub: https://github.com/amrutakale08/geoweaver_cwl-usecases. We are now
    transforming more Geoweaver AI workflows into CWL with this package and sharing
    the results on GitHub. Interested readers can go to that GitHub repository through
    the above link to test and adapt those use cases. Download : Download high-res
    image (528KB) Download : Download full-size image Fig. 4. Exemplar scripts of
    workflow steps in the workflow. cwl from workflow. json file (left) and the CWL
    text document scripts_export.cwl describing computational steps present in the
    elementary_cwl_files folder (right). Geoweaver provides a unique combination of
    features, such as a user-friendly interface, full-stack code, a history of previous
    versions, and sharable AI/ML workflows. It is a user-friendly entry point to solve
    AI-related workflow issues for a variety of disciplines in geosciences as well
    as beyond. The geoweaver_cwl package developed in this work further extends the
    portability and interoperability of workflows created in Geoweaver. The package
    can quickly transform Geoweaver workflows into CWL format, and the result can
    be run on many CWL-compliant software applications. Moreover, the CWL result can
    be also executed on diverse computing platforms including local computers, cloud
    environments, or high-performance clusters. The transformation process is intuitive
    and new users will spend less time getting familiar with the package. 4. Discussion
    We encourage geoscientists as well as other AI practitioners to use Geoweaver
    and the geoweaver_cwl package to increase the reproducibility and interoperability
    of their work. The developed package helps automatically transform Geoweaver AI/ML
    workflows to a community standard CWL. As an extension to Geoweaver, the CWL result
    can be executed on diverse computing platforms which gives users more opportunities
    to run the workflow without compromising provenance or having to recreate the
    workflow if they want to use another workflow management system. CWL can formally
    describe inputs, outputs, and other execution details of the workflow in a text-based
    document. It supports workflows that specify dependencies among tools and use
    one device output as input to another. CWL documents are text-based so that they
    can be created manually, without or with less computer programming. However, ensuring
    that these documents adhere to the CWL syntax specification may restrict some
    users from adopting it. The developed geoweaver_cwl addresses this gap. It can
    automatically describe workflows into CWL to make it effortless for geoscientists
    to share data analysis workflows in varied formats without learning the technical
    details of the CWL syntax. There are a wide variety of workflow management system
    software tools available all over the research community, that are constantly
    being developed, revised, and improved every day. While the availability of such
    tools benefits the community, it also presents a great challenge: as more and
    more tools are created, a set of standards needs to be adopted in order to ensure
    the portability and reproducibility of the resulting workflows. CWL, as reflected
    in its name, aims to be such a community standard to harmonize the workflow formats
    proposed by various workflow management system software tools. Reproducibility
    enables researchers to track and debug potential errors and validate the authenticity
    of the results, and as such it plays a vital role to make scientific research
    accurate, efficient, and cost-effective. Because CWL tracks code versions, inputs,
    outputs, and more, researchers can use it to pinpoint where the analysis went
    wrong, or where in the analysis the particular piece of data leads to new insights.
    Therefore, the transformation from Geoweaver workflows to CWL format is a necessary
    extension with regards to broad portability and reproducibility. Portability is
    crucial when it comes to scientific research and analysis. When one workflow is
    designed for a type of computational environment such as a personal computer it
    may not function in a similar way as in the cloud. Therefore, researchers may
    spend more time and effort in debugging the tool to make it work in the desired
    environment. This could result in inconsistent outcomes or errors. In contrast,
    CWL enables portability by being explicit about inputs, outputs, data location,
    and execution models that can be executed on any of the CWL-compliant environments.
    CWL-based documents can be downloaded, edited, and executed on local infrastructure
    or uploaded and executed in the cloud. The scientific provenance research community
    has evolved significantly in recent years to provide several strategic capabilities,
    to make AI/ML workflows more explainable and reproducible. The declarative approach
    to describe workflow in CWL scripts facilitates and encourages users to explicitly
    declare every single step, improving the white box view of reviewing process and
    potential provenance. Such workflows will eliminate the “black box” nature by
    offering insights into the entire process used to build artifacts. This will support
    the research community in carrying out thorough studies that will enable them
    to satisfy those essential requirements for building a transparent and explainable
    AI/ML application. Documenting provenance to support published research should
    be considered a best practice rather than an afterthought. The community should
    be encouraged to follow well-established and consensus best practices for workflow
    design and software environment deployment. The aim of Geoweaver and the geoweaver_cwl
    package is to promote the efforts in that direction. In order to improve the efficiency
    of the developed geoweaver_cwl package, our plan is to continue using Geoweaver
    and the package with more AI research projects. So far, we have only tested our
    package on definite workflows created by Geoweaver, and we believe further analyses
    are necessary to validate the broad utility of the package. For instance, with
    the small number of use cases of geoweaver_cwl application we noticed that the
    CWL and cwltools have shorter execution time comparing with Geoweaver. Nevertheless,
    the diverse datasets, algorithms, and workflow may lead to varied performance,
    so we need to do more tests to see if that shorter execution time is always true.
    For our future work, we would like to collaborate with a diverse research team
    from different domains and collect complex use cases from them. Testing different
    use cases will confirm additional details and novel functions and also ensure
    that our package satisfies the end-user requirement. Geoweaver is developed and
    implemented using Java. A plan under discussion among our team is to have a new
    version of Geoweaver in Python, called “pygeoweaver”. In that way the Python-based
    geoweaver_cwl package can be naturally included as part of the new “pygeoweaver”
    platform, to address the needs from both Geoweaver and the CWL communities. 5.
    Conclusions In this paper, we first introduced Geoweaver and presented a wrapper
    tool, called geoweaver_cwl, that overcomes current challenges of achieving repeatability,
    reproducibility, and reusability of workflows. To assess the outcome, we tested
    geoweaver_cwl with multiple use cases provided by Geoweaver and illustrated one
    of them in this paper. The study demonstrates that the geoweaver_cwl package can
    bring great benefits to the geoscience community. The code is publicly available
    on GitHub (https://github.com/amrutakale08/geoweaver_cwl) and open to anyone who
    wants to import Geoweaver AI/ML workflows into CWL-compliant workflow management
    system software applications. We encourage the research community to participate
    in the adoption of Geoweaver by integrating the geoweaver_cwl package into their
    projects. We would like to hear comments and suggestions from the community to
    facilitate the development of new functionality in future versions. Code availability
    The geoweaver_cwl Python package is made open access at: https://pypi.org/project/geoweaver-cwl/0.0.1/.
    The source code of the package is accessible at: https://github.com/amrutakale08/geoweaver_cwl
    and exemplar results are accessible at: https://github.com/amrutakale08/geoweaver_cwl-usecases.
    The source code of the Geoweaver platform is accessible at: https://github.com/ESIPFed/Geoweaver.
    CRediT authorship contribution statement Amruta Kale: Writing – review & editing,
    Writing – original draft, Software, Methodology, Conceptualization. Ziheng Sun:
    Writing – review & editing, Resources, Methodology. Chao Fan: Writing – review
    & editing, Resources, Methodology. Xiaogang Ma: Writing – review & editing, Validation,
    Methodology, Conceptualization. Declaration of competing interest The authors
    declare that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgment
    The work was supported by the National Science Foundation under Grants No. 2019609
    and No. 2126315 and the National Aeronautics and Space Administration under Grant
    No. 80NSSC21M0028. The authors thank two anonymous reviewers for their reviews
    and comments on an early version of the paper. Data availability Data and code
    are shared on GitHub and links were provided in the manuscript. References Adadi
    and Berrada, 2018 A. Adadi, M. Berrada Peeking inside the black box: a survey
    on explainable artificial intelligence (XAI) IEEE Access, 6 (2018), pp. 52138-52160
    CrossRefView in ScopusGoogle Scholar Altintas et al., 2004, I. Altintas, C. Berkley,
    E. Jaeger, M. Jones, B. Ludascher, S. Mock Kepler: an extensible system for design
    and execution of scientific workflows June Proceedings of the 16th International
    Conference on Scientific and Statistical Database Management, Santorini, Greece
    (2004), pp. 423-424 CrossRefView in ScopusGoogle Scholar Amstutz et al., 2016
    P. Amstutz, M.R. Crusoe, N. Tijanić, B. Chapman, J. Chilton, M. Heuer, A. Kartashov,
    D. Leehr, H. Ménager, M. Nedeljkovich, M. Scales Common Workflow Language, v1.0
    Figshare (2016), 10.6084/m9.figshare.3115156.v2 Google Scholar Bedia et al., 2019
    J. Bedia, D. San-Martín, M. Iturbide, S. Herrera, R. Manzanas, J.M. Gutiérrez
    The METACLIP semantic provenance framework for climate products Environ. Model.
    Software, 119 (2019), pp. 445-457 View PDFView articleView in ScopusGoogle Scholar
    Das et al., 2020 P. Das, N. Ivkin, T. Bansal, L. Rouesnel, P. Gautier, Z. Karnin,
    L. Dirac, L. Ramakrishnan, A. Perunicic, I. Shcherbatyi, W. Wu Amazon SageMaker
    Autopilot: a white box AutoML solution at scale Proceedings of the Fourth International
    Workshop on Data Management for End-To-End Machine Learning (2020), pp. 1-7 Portland,
    OR,USA CrossRefGoogle Scholar DataRobotCloud, 2012 AI Cloud DataRobot https://www.datarobot.com/
    (2012), Accessed 18th Jan 2022 Google Scholar Di Tommaso et al., 2017 P. Di Tommaso,
    M. Chatzou, E.W. Floden, P.P. Barja, E. Palumbo, C. Notredame Nextflow enables
    reproducible computational workflows Nat. Biotechnol., 35 (4) (2017), pp. 316-319
    CrossRefView in ScopusGoogle Scholar Doshi-Velez et al., 2017 F. Doshi-Velez,
    M. Kortz, R. Budish, C. Bavitz, S. Gershman, D. O''Brien, K. Scott, S. Schieber,
    J. Waldo, D. Weinberger, A. Weller Accountability of AI under the law: the role
    of explanation arXiv preprint (2017) arXiv:1711.01134 Google Scholar Gilpin et
    al., 2018 L.H. Gilpin, D. Bau, B.Z. Yuan, A. Bajwa, M. Specter, L. Kagal Explaining
    explanations: an overview of interpretability of machine learning Proceedings
    of the 2018 IEEE 5th International Conference on Data Science and Advanced Analytics
    (DSAA), Turin, Italy (2018), pp. 80-89 CrossRefView in ScopusGoogle Scholar Gu
    et al., 2021 Q. Gu, A. Kumar, S. Bray, A. Creason, A. Khanteymoori, V. Jalili,
    B. Grüning, J. Goecks Galaxy-ML: an accessible, reproducible, and scalable machine
    learning toolkit for biomedicine PLoS Comput. Biol., 17 (6) (2021), Article e1009014
    CrossRefGoogle Scholar Gunning and Aha, 2019 D. Gunning, D. Aha DARPA''s explainable
    artificial intelligence (XAI) program AI Mag., 40 (2) (2019), pp. 44-58 CrossRefView
    in ScopusGoogle Scholar Jablonski and Bussler, 1996 S. Jablonski, C. Bussler Workflow
    Management: Modeling, Concepts, Architecture and Implementation Cengage Learning
    (1996) Google Scholar Kale and Ma, 2023 A. Kale, X. Ma Provenance in earth AI
    Z. Sun, N. Cristea, P. Rivas (Eds.), Artificial Intelligence in Earth Science,
    Elsevier (2023), pp. 357-378 Amsterdam View PDFView articleView in ScopusGoogle
    Scholar Kale et al., 2023 A. Kale, T. Nguyen, F. Harris Jr., C. Li, J. Zhang,
    X. Ma Provenance documentation to enable explainable and trustworthy AI: a literature
    review Data Intelligence, 5 (1) (2023), pp. 139-162, 10.1162/dint_a_00119 View
    in ScopusGoogle Scholar Kiepuszewski et al., 2003 B. Kiepuszewski, A.P. Barros,
    W. Van Der Aalst, A. Ter Hofstede Workflow patterns Distributed Parallel Databases,
    14 (1) (2003), pp. 5-51 Google Scholar Köster and Rahmann, 2012 J. Köster, S.
    Rahmann Snakemake—a scalable bioinformatics workflow engine Bioinformatics, 28
    (19) (2012), pp. 2520-2522 CrossRefView in ScopusGoogle Scholar Ma et al., 2017
    X. Ma, S.E. Beaulieu, L. Fu, P. Fox, M. Di Stefano, P. West Documenting provenance
    for reproducible marine ecosystem assessment in open science P. Diviacco, H.M.
    Glaves, A. Leadbetter (Eds.), Oceanographic and Marine Cross-Domain Data Management
    for Sustainable Development, IGI Global, Hershey, PA, USA (2017), pp. 100-126
    Google Scholar Datatron MLOps, 2016 Datatron MLOps Machine learning operations
    https://datatron.com/ (2016), Accessed 18th Jan 2022 Google Scholar Pahl et al.,
    2017 C. Pahl, A. Brogi, J. Soldani, P. Jamshidi Cloud container technologies:
    a state-of-the-art review IEEE Transactions on Cloud Computing, 7 (3) (2017),
    pp. 677-692 Google Scholar Ribeiro et al., 2016 M.T. Ribeiro, S. Singh, C. Guestrin
    Why should I trust you? Explaining the predictions of any classifier Proceedings
    of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining, San Francisco, CA, USA (2016), pp. 1135-1144 CrossRefView in ScopusGoogle
    Scholar Rudin, 2018 C. Rudin Please stop explaining black box models for high
    stakes decisions Proceedings of the 32nd Conference of Neural Information Processing
    Systems (NIPS), Workshop on Critiquing and Correcting Trends Machine Learning
    (2018) Montreal, Canada, 20pp. Available: https://arxiv.org/abs/1811.10154 Google
    Scholar Rudin, 2019 C. Rudin Stop explaining black box machine learning models
    for high stakes decisions and use interpretable models instead Nat. Mach. Intell.,
    1 (5) (2019), pp. 206-215 CrossRefView in ScopusGoogle Scholar Sun et al., 2020
    Z. Sun, L. Di, A. Burgess, J.A. Tullis, A.B. Magill Geoweaver: advanced cyberinfrastructure
    for managing hybrid geoscientific AI workflows ISPRS Int. J. Geo-Inf., 9 (2) (2020),
    p. 119, 10.3390/ijgi9020119 View in ScopusGoogle Scholar Sun et al., 2022 Z. Sun,
    L. Sandoval, R. Crystal-Ornelas, S.M. Mousavi, J. Wang, C. Lin, N. Cristea, D.
    Tong, W.H. Carande, X. Ma, Y. Rao, J.A. Bednar, A. Tan, J. Wang, S. Purushotham,
    T.E. Gill, J. Chastang, D. Howard, B. Holt, C. Gangodagamage, P. Zhao, P. Rivas,
    Z. Chester, J. Orduz, A. John A review of earth artificial intelligence Comput.
    Geosci., 159 (2022), Article 105034, 10.1016/j.cageo.2022.105034 View PDFView
    articleView in ScopusGoogle Scholar Tseng et al., 2020 G. Tseng, H. Kerner, C.
    Nakalembe, I. Becker-Reshef Annual and in-season mapping of cropland at field
    scale with sparse labels. Tackling climate change with machine learning workshop
    at NeurIPS ’20, virtual conference https://www.climatechange.ai/papers/neurips2020/29
    (2020), Accessed 17th Nov 2022 Google Scholar Van der Aalst, 1998 W.M. Van der
    Aalst The application of Petri nets to workflow management J. Circ. Syst. Comput.,
    8 (1) (1998), pp. 21-66 View in ScopusGoogle Scholar Wing, 2020 J.M. Wing Ten
    research challenge areas in data science Harvard Data Science Review, 2 (3) (2020),
    10.1162/99608f92.c6577b1f Google Scholar Zeng et al., 2019 Y. Zeng, Z. Su, I.
    Barmpadimos, A. Perrels, P. Poli, K.f. Boersma, A. Frey, X. Ma, K. de Bruin, H.
    Gossen, W. Timmermans Towards a traceable climate service: assessment of quality
    and usability of essential climate variables Rem. Sens., 11 (10) (2019), p. 1186,
    10.3390/rs11101186 View in ScopusGoogle Scholar Cited by (1) Utility of the Python
    package Geoweaver_cwl for improving workflow reusability: an illustration with
    multidisciplinary use cases 2023, Earth Science Informatics © 2023 The Authors.
    Published by Elsevier Ltd. Recommended articles The cultural-social nucleus of
    an open community: A multi-level community knowledge graph and NASA application
    Applied Computing and Geosciences, Volume 20, 2023, Article 100142 Ryan M. McGranaghan,
    …, Edlira Vakaj View PDF AnnRG - An artificial neural network solute geothermometer
    Applied Computing and Geosciences, Volume 20, 2023, Article 100144 Lars H. Ystroem,
    …, Fabian Nitschke View PDF GeoSim: An R-package for plurigaussian simulation
    and Co-simulation between categorical and continuous variables Applied Computing
    and Geosciences, Volume 19, 2023, Article 100130 George Valakas, Konstantinos
    Modis View PDF Show 3 more articles Article Metrics Citations Citation Indexes:
    1 Captures Readers: 9 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Applied Computing and Geosciences
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Geoweaver_cwl: Transforming geoweaver AI workflows to common workflow language
    to extend interoperability'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Alwabel A.
  citation_count: '0'
  description: 'Advancements in container technology can improve the efficiency of
    cloud systems by reducing the initiation time of virtual machines (VMs) and improving
    portability. Therefore, many cloud service providers offer cloud services based
    on the container as a service (CaaS) model. Container placement (CP) is a mechanism
    that allocates containers to a pool of VMs by mapping new containers to VMs and
    simultaneously considering VM placements on physical machines. The CP mechanism
    can serve several purposes, such as reducing power consumption and optimizing
    resource availability. This study presents directed container placement (DCP),
    a novel policy for placing containers in CaaS cloud systems. DCP extends the whale
    optimization algorithm, an optimization technique aimed at reducing the power
    consumption in cloud systems with a minimum effect on the overall performance.
    The proposed mechanism is evaluated against established methods, namely, improved
    genetic algorithm and discrete whale optimization using two criteria: energy savings
    and search time. The experiments demonstrate that DCP consumes approximately 78%
    less power and reduces the search time by approximately 50% in homogeneous clouds.
    In addition, DCP saves power by approximately 85% and reduces the search time
    by approximately 30% in heterogeneous clouds.'
  doi: 10.3390/electronics12153369
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Electronics All Article Types Advanced   Journals
    Electronics Volume 12 Issue 15 10.3390/electronics12153369 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editor Mehdi
    Sookhak Subscribe SciFeed Recommended Articles Related Info Link More by Author
    Links Article Views 755 Table of Contents Abstract Introduction Literature Review
    Problem Formulation Proposed Mechanism Performance Evaluation Conclusions Funding
    Data Availability Statement Conflicts of Interest References share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    A Novel Container Placement Mechanism Based on Whale Optimization Algorithm for
    CaaS Clouds by Abdulelah Alwabel Department of Computer Sciences, Prince Sattam
    Bin Abdulaziz University, AlKharj 1194, Saudi Arabia Electronics 2023, 12(15),
    3369; https://doi.org/10.3390/electronics12153369 Submission received: 5 July
    2023 / Revised: 31 July 2023 / Accepted: 5 August 2023 / Published: 7 August 2023
    Download keyboard_arrow_down     Browse Figures Versions Notes Abstract Advancements
    in container technology can improve the efficiency of cloud systems by reducing
    the initiation time of virtual machines (VMs) and improving portability. Therefore,
    many cloud service providers offer cloud services based on the container as a
    service (CaaS) model. Container placement (CP) is a mechanism that allocates containers
    to a pool of VMs by mapping new containers to VMs and simultaneously considering
    VM placements on physical machines. The CP mechanism can serve several purposes,
    such as reducing power consumption and optimizing resource availability. This
    study presents directed container placement (DCP), a novel policy for placing
    containers in CaaS cloud systems. DCP extends the whale optimization algorithm,
    an optimization technique aimed at reducing the power consumption in cloud systems
    with a minimum effect on the overall performance. The proposed mechanism is evaluated
    against established methods, namely, improved genetic algorithm and discrete whale
    optimization using two criteria: energy savings and search time. The experiments
    demonstrate that DCP consumes approximately 78% less power and reduces the search
    time by approximately 50% in homogeneous clouds. In addition, DCP saves power
    by approximately 85% and reduces the search time by approximately 30% in heterogeneous
    clouds. Keywords: cloud computing; CaaS; container placement; CP; energy efficiency
    1. Introduction Containerization is a relatively new technology for virtualizing
    applications in a lightweight manner and has led to significant utilization in
    cloud application management [1]. Cloud computing has become an efficient paradigm
    that offers computational abilities on a pay-per-usage basis [2]. Advancements
    in container technology can improve the efficiency of cloud systems by reducing
    the initiation time of virtual machines (VMs) and improving portability [3]. Consequently,
    popular cloud service providers, such as Amazon and Google, offer cloud services
    based on the container as a service (CaaS) model. The term container refers to
    multi-tenant deployment techniques involving process isolation on a shared kernel
    to package an application and run it with isolated dependencies [4]. A container
    wraps a piece of software together with objects needed for the execution (i.e.,
    runtime, libraries, and code) and permits easy deployment on any type of machine,
    whether physical machines (PMs) or VMs. Container technology exploits virtualization
    at the operating system (OS) level to take advantage of the flexibility and portability
    of software. A traditional VM requires all OS resources to be occupied [5], whereas
    a container can share the same OS kernel [6]. Therefore, a container consumes
    fewer resources and has a shorter deployment time because it is lightweight. However,
    it requires designing a feasible placement policy with optimal energy consumption
    under the adopted container-based cloud computing architecture [7]. Using containers
    with cloud computing promises to improve the portability between different cloud
    providers, which can help avert the risk of vendor locks [8]. Docker [9] and Kubernetes
    [10] are some of the most popular container solutions. They are built around container
    engines in which a container acts as a portable means to package applications,
    resulting in the need to manage dependencies between containers in multi-tier
    applications. Figure 1 illustrates a cloud-based container architecture used to
    employ both VMs and container technology to provide specific requirements for
    each application. However, the VM layer can be eliminated while still providing
    a container for applications. Figure 1. Container with VM architecture. Container
    placement (CP) is a mechanism that allocates a list of containers to a pool of
    VMs by mapping new containers to VMs and simultaneously considering VM placements
    on the PMs [11]. The CP mechanism can help reduce power consumption and optimize
    resource availability. This paper presents directed container placement (DCP),
    a novel policy for CP in CaaS cloud systems. DCP employs an optimization technique
    with the aim of reducing power consumption in cloud systems with minimal effect
    on the overall performance. The mechanism is evaluated using two criteria: energy
    savings and search time. The remainder of this paper is organized as follows.
    Section 2 reviews the related work in the literature. Section 3 formulates the
    CP problem and Section 4 details the proposed mechanism. The experimental results
    are presented and discussed in Section 5. Section 6 summarizes the study and outlines
    future research directions. 2. Literature Review Cloud computing is a demanding
    technique. Over time, people are opting for digitization with the increasing use
    of smart applications and cloud services for everything. Therefore, clouds are
    moving to micro-clouding services to increase energy efficiency and minimize the
    burden on cloud data centers, owing to the increase in utilized resources. A container-based
    VM solution is a lightweight version that has recently been used to improve the
    performance of cloud services. This section highlights the contributions of previous
    studies with regards to the energy consumption, performance-aware, resource utilization
    and quality-aware mechanisms of container-based cloud systems as summarized in
    Table 1. Table 1. Summary of reviewed papers. 2.1. Energy-Aware Mechanisms The
    authors in [12] studied the CaaS environment model as a power optimization problem.
    Their proposed model also outperformed the energy efficiency issue in terms of
    CaaS via container association and reduced the number of active servers. They
    proposed an algorithm with correlation-aware placement, which showed that overload
    and underload threshold algorithms performed better than other algorithms when
    the selected container was larger with regards to migration. However, this work
    lacks studying and analyzing the proposed mechanism on different cloud environments,
    such as overloaded and underloaded data centers. The authors in [13] proposed
    a container-based edge service management system (CoESMS), a multi-objective function
    for reducing energy consumption by studying different limitations, such as the
    CPU, memory, and worker budget. The results showed that the performance of the
    projected method was significantly improved, with a reduction in energy utilization
    of 21.75% and a decrease of 11.42% in service level agreement (SLA) violations.
    General simulations were executed on the capacity traces obtained from Planet
    Lab. This work, however, can be further improved by considering resource utilization
    and throughput metrics in the mechanism. An energy-alert model for merging optimized
    cloud containers was proposed in [14]. The algorithm was based on two phases and
    called multi-type particle swarm optimization (TMPSO). The results showed that
    the proposed algorithm could provide results in a more effective and efficient
    method, specifically for handling massive requests. The algorithm also showed
    additional energy savings compared with current approaches. This work can be extended
    by optimizing container consolidation during runtime. The researchers in [15]
    presented the improved genetic algorithm (IGA), an enhanced genetic algorithm
    that works more efficiently in terms of energy savings for the CP problem. It
    operates more efficiently than the existing conventional GA, First-Fit, and particle
    swarm optimization (PSO). In conventional GA, a defect occurs, in which the variety
    of the population cannot be a guarantee as the utilization of resource VMs becomes
    complex. This problem was solved using IGA by optimizing the CP using a nonlinear
    energy consumption model. The results demonstrated that this technique can reduce
    the overall severe energy consumption. The proposed work, however, does not provide
    nor optimize the search time to find the best solution. As it is becoming popular
    for data centers to use container services, deploying this service through cloud
    computing raises the hurdle of power consumption and resource utilization. In
    [16], the researchers proposed discrete whale optimization (DWO), which is a method
    to report the issue of the container and location of VM in CaaS situations, along
    with the idea of enhancing resource utilization and reducing power consumption
    based on the whale optimization algorithm (WOA) proposed by [31]. The authors
    planned a procedure built on WOA to solve the double phases of positioning as
    a single optimization issue. The results showed the superiority of the proposed
    technique over the evaluation approaches for a group of test situations. Therefore,
    the algorithm could be utilized to hold a greater number of containers or VMs
    because it decreased the number of PMs utilized. However, the proposed work can
    be extended to reduce the search time to find the best possible solution to reduce
    power consumption in CaaS cloud systems. The authors in [17] developed a method
    for balancing the optimal performance service for the initial container placement
    in CaaS and the optimal power consumption. They designed a metric for the isolation
    application of UTS4 (a container-placing solution) based on anti-affinity limitations.
    This positioning optimization problem was then transformed to obtain the best
    solution for placement, along with low energy consumption and low UTS4. Additionally,
    a nonlinear power-consumption model was proposed. To find the placement solution,
    an algorithm was proposed and named the first-fit-based IGA algorithm (FF-IGA),
    which quickly places containers as an evolution of IGA to enhance the container
    management cycle. The results illustrated the effectiveness of the proposed algorithm
    and metric model by minimizing power consumption. However, mangy genetic-based
    approaches, including this work, endure a search time overhead, which negatively
    affects the overall performance of the systems. 2.2. Performance-Aware Mechanisms
    The authors in [18] proposed a scheduling framework to leverage the online primal-dual
    framework with a learning-based scheme for obtaining dual solutions. It permits
    a job to state its job limit, inter-container, and selected cloud containers that
    influence the new scheduling algorithm strategy. They implemented a primal-dual
    model that shows the primal answer based on its double constraints in online and
    offline algorithms. The offline scheduling algorithm contains a new parting oracle
    to distinguish violated dual constraints. The online scheduling algorithm influences
    the online model based on a learning structure to obtain dual results. The results
    showed that the proposed scheduling frameworks were more efficient and attained
    a close-to-ideal combined job estimation. However, the focus of this paper is
    on a different problem, which makes it out of the scope of this paper. The authors
    in [19] targeted the problem of complexity in the operations of micro service
    storage. As cloud storage is increasing day by day, clouds are using these micro
    service containers to implement their applications. The aforementioned authors
    examined the initialization and runtime containerized performance applications
    together and reported that the default placement approach delivered by orchestrators
    is usually incomplete. Their performance-aware technique outperformed the default
    placement approach as shown in various experiments on multiple services. The performance
    placement strategy was varied up to 2x and 2.21x for the 50th and 99th percentiles,
    respectively. The authors in [20] studied containers on a multi-cloud to multi-fog
    architecture and its related applications, attempting to address the task of personal
    cloud building. In addition, a task-planning algorithm built on energy balance
    was suggested to improve the lifetime of wireless sensor networks (WSNs) without
    expanding the postponement of tasks. They estimated the execution of virtual systems
    and containers under high-level concurrence, and the end effect proved that containers
    were improved compared with virtual devices. In addition, they created power expenses
    and task planning for the fog nodes and terminal devices (TDs). The proposed algorithm
    could efficiently stabilize the power of the TDs in a system while minimizing
    the service latency, which resulted in improving the performance of the system.
    Although this work managed to moderately consume power, it was not the main focus
    of the work to reduce power consumption. In [21], to overcome the migration cost
    and consume less power, the authors suggested a combination of migration (CPER)
    and planned energy–performance-aware allocation (EPC-FU) methods to examine energy-saving
    capacities and the presentation of several workload types in data centers with
    containers. For a million containers, the overall top method bounded migrations
    to approximately 1.9% of the containers, of which the migrating cost recovered
    was 61.89%. The results showed that if more performance- and energy-effective
    hosts are migrated compared to containers that run for a long time, the economic
    feasibility of data centers is increased. However, the execution time of the proposed
    approach can negatively affect the QoS of CaaS cloud systems. 2.3. Resource Utilization
    Mechanisms The authors in [22] proposed a learning approach called deep reinforcement
    to combine each of the functioning containers along with the various resource
    needs of at least the quantity of physical machinery. They presented and executed
    a multi-resource bin-filler algorithm (called RACC) that influences a deep learning
    method, called fit-for-packing, for attributing the near-optimal number of containers
    on physical machinery. The results showed that RACC attained an improved job slowdown
    compared with baseline algorithms. In addition, RACC expressed a considerable
    improvement in resource utilization. This work, however, does not consider power
    consumption as an evaluation metric of the proposed mechanism. The authors in
    [23] proposed availability-aware container-scheduling techniques to improve the
    accessibility of application services at the end of a cloud data center. They
    planned to use physical computing and VMs for scheduling purposes with limitations
    because they have high availability values. The UML model was used for the computation.
    The CloudSim simulator was used to execute and integrate the strategies. The Docker
    container and the proposed strategy were compared in this study. Based on the
    results, other strategies had lower service availability than the proposed availability-aware
    strategy. In addition, this strategy obtained suitable host CPU utilization compared
    to other strategies. In [24], to address the high computation cost, the authors
    combined container assignment and task provisioning in an active fog computing
    situation. Both the movement and irregular differences in the core function load
    were considered. The expansion problem of the number of appeals aided by the fog
    computing platform was expressed using integer linear programming. The results
    showed that the PSO-based algorithm achieved the best results; however, the execution
    time was much longer than that of a greedy algorithm, which achieved up to 30%
    worse outcomes with no execution duration. In fact, this work focuses on the fog
    environment rather than cloud systems. According to [25], the micro-cloud problem
    discussed in cloud computing is primarily moving towards micro storage. Thus,
    resources for devices are limited, with a lack of financial incentives for both
    the owner of the applications and the provider. By introducing a small overhead
    in arrangement times, their proposed solution accomplished the assignment of more
    requests, with an effective allocation of 83% in contradiction to 57% of prevalent
    answers, as measured on an arrangement of memory-concentrated workloads and a
    real CPU. However, this approach suffers from a drawback of causing an overhead
    of scheduling time. The authors in [26] presented a heterogeneous task allocation
    strategy (HATS) for convenient and low-cost container orchestration via resource
    utilization optimization. HATS provided three features: (a) HATS initializes CP
    for optimal use by task packing; (b) using multiple auto-scaling algorithms, the
    authors proposed a pricing model of an elastic instance to adapt the cluster size
    with regards to the workload variation in runtime; and (c) they presented a rescheduling
    approach that uses the check pointing method of the container for permitting the
    cleaning of the VM of underused instances to save costs while maintaining task
    growth. In [27], an OpenStack-built middleware policy was presented, in which
    containers can be positioned/achieved at the fog stages. The authors presented
    an S4T platform to provide an extension of the cloud for app designers and infrastructure
    executives. They presented a system proficient in handling distant containers
    that can collaborate and travel between various nodes without seeing the real
    outline of the structure. S4T offers a facility completely in agreement with OpenStack
    and cooperates with many of its facilities (e.g., Neutron and Keystone). The conventional
    fog processing architecture is for a single data hub and several fog joints. It
    is incapable of adjusting to existing advancements in personal clouds. In [28],
    heuristic algorithms and answers were developed for different situations and many
    problems emerged as NP-hard problems. The planned method was applied and used
    for estimation on the de facto PaaS platform, namely, CloudFoundary. The results
    showed that the network load could be minimized by up to 60% with no harmful effect
    on its balance. On a large scale, they experimented with 2400 applications, achieving
    savings of approximately 90% in network traffic by removing 50% of the load balance.
    However, these two studied are not applicable for cloud systems because they were
    developed for fog systems. 2.4. Quality-Aware Mechanisms The authors in [29] proposed
    a method called the elastic provisioning of VMs for container deployment (EVCD),
    which is an overall design for the placement of containers for cloud systems.
    EVCD regulates container placement on computer-generated machines that can be
    attained free on demand while enhancing the quality of service (QoS) measurements.
    In addition, it offers a standard next to which another container distribution
    heuristic can be matched. Accordingly, they estimated and emphasized the disadvantages
    of two well-known heuristics used to describe container placement. The authors
    in [30] proposed an autonomic containerized service scaler (ACTS) system that
    vertically and horizontally measured a group of mixed containerized services exposed
    to various workloads to obtain results for several high-level QoE metrics. They
    employed ACTS in a few digitized facilities of the Shared Services of the Ministry
    of Health (SPMS) community corporation. The results showed that the proposed method
    could sufficiently adjust the configuration of all services as a direct response
    to modifications in its workload. The results also reduced cost and proved the
    capability of ACTS to retain the QoE system of measurement under restrictive standards
    pre-settled during SLAs. However, both EVCD and ACTS do not provide a means to
    reduce power consumption on CaaS cloud systems. 3. Problem Formulation Server
    consolidation harnesses virtualization by sharing hardware to place multiple VMs
    on the same PM; therefore, fewer running PMs lead to greater power savings [32].
    This section describes the objective model and corresponding constraints for the
    problem of placing containers efficiently to reduce power consumption. The power
    consumption of various PMs in data centers is calculated mainly based on the CPU,
    memory, disk storage, and networking, with the CPU consuming the most power [33].
    CP refers to the containers hosted by VMs when VMs are placed on PMs in CaaS cloud
    systems [16]. This paper proposes a novel CP mechanism that improves the utilization
    of running PMs in cloud systems, leading to power savings. Assuming that C is
    the set of containers to be assigned to V, V is the set of VMs to be run on P.
    Moreover, P is the set of available PMs in the CaaS data center. Let the resource
    specification of 𝑝 𝑚 𝑖 be defined as (𝑝 𝑚 𝑐𝑝𝑢 𝑖 ,𝑝 𝑚 𝑚𝑒𝑚 𝑖 ) , which represent
    the total processing and memory capacities, respectively, of 𝑝 𝑚 𝑖 . Let the tuple
    (𝑣 𝑚 𝑐𝑝𝑢 𝑗 ,𝑣 𝑚 𝑚𝑒𝑚 𝑗 ) denote the processing and memory capacities, respectively,
    of 𝑣 𝑚 𝑗 . The resource requirements for the container 𝑐 𝑘 are denoted by ( 𝑐
    𝑐𝑝𝑢 𝑘 , 𝑐 𝑚𝑒𝑚 𝑘 ) , which represent the processing and memory capacities, respectively,
    of 𝑐 𝑘 for 𝑐 𝑘 ∈𝐶 . 𝑐 𝑘 can be assigned to 𝑣 𝑚 𝑗 as follows: 𝑐 𝑐𝑝𝑢 𝑘 ≤𝑣 𝑚 𝑎𝑣𝑙.𝑐𝑝𝑢
    𝑗 ,∀ 𝑐 𝑘 ∈𝐶,𝑣 𝑚 𝑗 ∈𝑉 (1) and 𝑐 𝑚𝑒𝑚 𝑘 ≤𝑣 𝑚 𝑎𝑣𝑙.𝑚𝑒𝑚 𝑗 ,∀ 𝑐 𝑘 ∈𝐶,𝑣 𝑚 𝑗 ∈𝑉 (2) where
    𝑣 𝑚 𝑎𝑣𝑙.𝑐𝑝𝑢 𝑗 and 𝑣 𝑚 𝑎𝑣𝑙.𝑚𝑒𝑚 𝑗 denote the available processing and memory powers,
    respectively, of 𝑣 𝑚 𝑗 . This implies that containers can be placed on the same
    VM as long as the total processing or memory power of the containers does not
    exceed that of the VM. This can be formulated as 𝑎𝑠𝑠𝑖𝑔𝑛( 𝑐 𝑘 ,𝑣 𝑚 𝑗 ) , a function
    that returns true if 𝑣 𝑚 𝑗 hosts 𝑐 𝑘 or false otherwise, and is expressed as follows:
    𝑎𝑠𝑠𝑖𝑔𝑛( 𝑐 𝑘 ,𝑣 𝑚 𝑗 )= ⎧ ⎩ ⎨     𝑡𝑟𝑢𝑒,𝑖𝑓𝑒𝑞𝑢𝑎𝑡𝑖𝑜𝑛𝑠 (1)𝑎𝑛𝑑(2)𝑎𝑟𝑒𝑡𝑟𝑢𝑒 𝑓𝑎𝑙𝑠𝑒,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒
    (3) Moreover, 𝑣 𝑚 𝑗 is hosted on 𝑝 𝑚 𝑖 as follows: 𝑣 𝑚 𝑐𝑝𝑢 𝑗 ≤𝑝 𝑚 𝑎𝑣𝑙.𝑐𝑝𝑢 𝑖 ,∀𝑣
    𝑚 𝑗 ∈𝑉,𝑝 𝑚 𝑖 ∈𝑃 (4) and 𝑣 𝑚 𝑚𝑒𝑚 𝑗 ≤𝑝 𝑚 𝑎𝑣𝑙.𝑚𝑒𝑚 𝑖 ,∀𝑣 𝑚 𝑗 ∈𝑉,𝑝 𝑚 𝑖 ∈𝑃 (5) where
    𝑝 𝑚 𝑎𝑣𝑙.𝑐𝑝𝑢 𝑖 and 𝑝 𝑚 𝑎𝑣𝑙.𝑚𝑒𝑚 𝑖 denote the available processing and memory, respectively,
    of 𝑝 𝑚 𝑖 . Let 𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑒(𝑣 𝑚 𝑗 ,𝑝 𝑚 𝑖 ) be a function that returns true if 𝑣 𝑚
    𝑗 can be allocated to 𝑝 𝑚 𝑖 or false otherwise. It can be expressed as follows:
    𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑒(𝑣 𝑚 𝑗 ,𝑝 𝑚 𝑖 )= ⎧ ⎩ ⎨     𝑡𝑟𝑢𝑒,𝑖𝑓𝑒𝑞𝑢𝑎𝑡𝑖𝑜𝑛𝑠 (4)𝑎𝑛𝑑(5)𝑎𝑟𝑒𝑡𝑟𝑢𝑒 𝑓𝑎𝑙𝑠𝑒,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒
    (6) This study adopts a linear server energy consumption model for containerized-based
    cloud systems [21]. The power consumption of 𝑝 𝑚 𝑖 is denoted by 𝑝𝑤𝑟(𝑝 𝑚 𝑖 ) and
    is calculated as follows: 𝑝𝑤𝑟(𝑝 𝑚 𝑖 )=𝑝 𝑚 𝑖𝑑𝑙𝑒 𝑖 +(𝑝 𝑚 𝑚𝑎𝑥 𝑖 −𝑝 𝑚 𝑖𝑑𝑙𝑒 𝑖 )×𝑢𝑡𝑙(𝑝
    𝑚 𝑖 ) (7) where 𝑝 𝑚 𝑚𝑎𝑥 𝑖 is the power consumed by the PM when it runs at full
    utilization level and 𝑝 𝑚 𝑖𝑑𝑙𝑒 𝑖 is the power consumed by the PM when it is idle.
    An idle PM can consume an average of 70% of the power consumed by a machine at
    full utilization [34]. 𝑢𝑡𝑙(𝑝 𝑚 𝑖 ) denotes the current utilization level of 𝑝
    𝑚 𝑖 , which is calculated as follows: 𝑢𝑡𝑙(𝑝 𝑚 𝑖 )= 𝑝 𝑚 𝑐𝑝𝑢.𝑢𝑠𝑒𝑑 𝑖 𝑝 𝑚 𝑐𝑝𝑢 𝑖 (8)
    where 𝑝 𝑚 𝑐𝑝𝑢.𝑢𝑠𝑒𝑑 𝑖 denotes the total CPU power used for 𝑝 𝑚 𝑖 . In other words,
    this refers to the total processing power of all VMs hosted on 𝑝 𝑚 𝑖 . Let 𝐸 𝑐𝑝
    denote the total energy consumed by PMs using a particular CP in the CaaS cloud
    system, which is given as follows: 𝐸 𝑐𝑝 = ∑ 𝑖=1 𝑝 𝑝𝑤𝑟(𝑝 𝑚 𝑖 ) (9) Therefore, a
    power-saving CP mechanism should aim at minimizing 𝐸 𝑐𝑝 . 4. Proposed Mechanism
    System Model Figure 2 illustrates two mechanisms, namely, A and B, that can be
    adopted in CaaS cloud systems. Mechanism A does not focus on power consumption;
    therefore, it distributes the VMs around two different PMs, whereas Mechanism
    B places the three VMs on one PM if the PM can host all the VMs. Mechanism B can
    help reduce the overall power consumption provided that the second PM is in a
    power-saving mode. However, CP mechanisms can affect the overall performance of
    CaaS clouds because the overhead of finding an appropriate solution for CP can
    delay the processing of jobs in CaaS systems [35]. Therefore, the optimal mechanism
    should involve a trade-off between reducing the energy consumption and improving
    the system performance. Figure 2. Different Placement Mechanisms. This section
    presents the proposed DCP mechanism that extends the DWO mechanism [16], which
    is a container placement mechanism designed to improve resource utilization and
    reduce power consumption in CaaS-based cloud systems. DWO adopts the WOA [31],
    which is a meta-heuristic optimization algorithm that imitates the social behavior
    of humpback whales. A group of Humpback whales (i.e., search agents) make a spiral
    shape around the prey and then swim up to hunt the prey (i.e., a solution) inside
    the circles. Three phases are used by the humpback whale: (i) encircling, (ii)
    spiral bubble-net feeding, and (iii) searching for prey. The position of the prey
    is unknown; therefore, the current best search agent can be any position near
    the prey in the search space. Other search agents then update their positions
    according to the current search agent [16]. The algorithm employs a search process
    that commences with a randomly generated population of solutions (a matrix of
    solutions), which then evolves over successive generations. The solutions are
    always combined to form the next generation of solutions, allowing the matrix
    to be enhanced over the course of generations. However, the main difference between
    the current work and recently published papers in the literature (particularly
    the DWO mechanism [16]) is that the proposed DCP mechanism utilizes a heuristic
    approach to direct (hence, the name of the mechanism) the search process in the
    mechanism. This approach plays a vital role in reducing the overall power consumption
    of CaaS cloud systems with an acceptable overhead time. The workflow of the DCP
    policy is depicted in Figure 3, which contains two phases: initialization and
    search. The initialization phase uses the number of PMs (p), VMs (v), and containers
    (c) as the input parameters. The mechanism also obtains the number of possible
    solutions ( 𝑁𝑊 ) and total number of iterations ( 𝑖𝑡𝑒𝑟𝑁 ) to search for the best
    solution. The mechanism sets a as a double random number ∈[2,0] that is linearly
    decreased from 2 to 0 [31]. The number of iterations t is set as zero. t is an
    iteration counter from one to 𝑖𝑡𝑒𝑟𝑁 . Moreover, 𝑐 𝑓 1 and 𝑐 𝑓 2 are the coefficient
    vectors that are employed to determine how to update current solutions [31]. Those
    vectors are calculated as follows: 𝑐 𝑓 1 =2𝑎·𝑟𝑎𝑛𝑑−𝑎 (10) 𝑐 𝑓 2 =𝑎·𝑟𝑎𝑛𝑑 (11) Figure
    3. DCP flowchart. Next, the policy initializes the solution matrix ( 𝑁𝑊𝑚𝑎𝑡𝑟𝑖𝑥
    ), which is a matrix of NW possible solutions, and each solution can be implemented
    using the CP mechanism in a cloud system. It randomly assigns containers to the
    VMs and allocates VMs to the PMs. Furthermore, the policy in the next step employs
    a repair mechanism that repairs the solutions in the 𝑁𝑊𝑚𝑎𝑡𝑟𝑖𝑥 because the random
    allocation of containers to VMs can overload some VMs with containers, i.e., the
    CPU or RAM requirements of the containers are greater than those of a VM that
    hosts those containers. Similarly, some PMs can be overloaded with VMs. Overloaded
    VMs or PMs must be repaired by migrating containers and VMs from overloaded VMs
    and PMs, respectively. The repair mechanism employs a two-factor approach to identify
    the most suitable containers (or VMs) for migration. Next, an appropriate candidate
    VM (or PM) is selected as the new host for these containers (or VMs) [16]. This
    approach aims to decrease the total power consumption by improving resource utilization.
    The first factor is the overloaded factor 𝑂𝐹(ℎ𝑒,𝑔𝑒) , which is a function employed
    to select a guest element 𝑔𝑒 . The guest element can be a container or VM to be
    migrated from an overloaded hosting element ℎ𝑒 , which can be a VM or PM. The
    overloaded factor can be calculated as follows: 𝑂𝐹(ℎ𝑒,𝑔𝑒)=          ℎ
    𝑒 𝑐𝑝𝑢 −𝑔 𝑒 𝑐𝑝𝑢 ℎ 𝑒 𝑐𝑝𝑢 − ℎ 𝑒 𝑟𝑎𝑚 −𝑔 𝑒 𝑟𝑎𝑚 ℎ 𝑒 𝑟𝑎𝑚          (12) 𝑔𝑒 is
    a generic term that refers to either containers or VMs, and ℎ𝑒 is a generic term
    that refers to either VMs or PMs. The second factor is the selection factor 𝑆𝐹(ℎ𝑒,𝑔𝑒)
    , which is a function employed to select the VM (or PM) that is the most suitable
    candidate among all available choices. It can be calculated as follows: 𝑆𝐹(ℎ𝑒,𝑔𝑒)=
             ℎ 𝑒 𝑐𝑝𝑢 +𝑔 𝑒 𝑐𝑝𝑢 ℎ 𝑒 𝑐𝑝𝑢 − ℎ 𝑒 𝑟𝑎𝑚 +𝑔 𝑒 𝑟𝑎𝑚 ℎ 𝑒 𝑟𝑎𝑚    
    (13) Both Equations (12) and (13) are utilized to minimize the idle fragmentation
    of resources and help to efficiently harness the available resources to the maximum
    extent possible with an aim to improve resource utilization. The repair mechanism
    fixes one solution at a time as shown in Algorithm 1. The mechanism input is a
    list of ℎ𝑒 . If ℎ𝑒 is overloaded, the mechanism retrieves all 𝑔𝑒 elements to select
    the elements to be migrated. The mechanism selects the 𝑔𝑒 element with the minimum
    𝑂𝐹 . Subsequently, it removes it from the current ℎ𝑒 and adds it to the migration
    list as illustrated in lines 3–14 in the algorithm. Furthermore, the mechanism
    selects an appropriate destined host ( 𝑑𝑒𝑠𝐻𝑒 ) for each 𝑔𝑒 in the migration list,
    𝑔𝑒𝑀𝑖𝑔𝑟𝑎𝑡𝑒𝐿𝑖𝑠𝑡 . The 𝑑𝑒𝑠𝐻𝑒 with the least 𝑆𝐹 is selected to host 𝑔𝑒 provided that
    𝑑𝑒𝑠𝐻𝑒 can host it as given in Equation (3) or Equation (6) if it is a VM or PM,
    respectively. In the next step, DCP initializes the best solution ( 𝑏𝑠 ) by creating
    a solution that allocates VMs to the PMs based on the first-fit (FF) heuristic.
    The best solution initialization mechanism is presented in Algorithm 2. DCP employs
    the FF approach with the aim of reducing power consumption by stacking more VMs,
    which improves resource utilization. In contrast to other mechanisms based on
    the WOA, such as [36] and [16], DCP does not choose the best solution among the
    NW solutions in the solution matrix. Rather, it constructs a solution that can
    direct the search process, resulting in a reduced search time. The next step is
    to select the first solution 𝑠 𝑟 in the 𝑁𝑊𝑚𝑎𝑡𝑟𝑖𝑥 and compare it with 𝑏𝑠 in terms
    of energy consumption, according to Equation (9). If 𝑠 𝑟 consumes less energy,
    it is selected as the new 𝑏𝑠 . The search phase begins by updating the parameters
    a, 𝑐 𝑓 1 , and 𝑐 𝑓 2 as mentioned in the initialization phase. Subsequently, if
    the current solution is not the same as the best solution, 𝑠 𝑟 is updated using
    the aforementioned parameters and 𝑏𝑠 , as explained in the WOA. Otherwise, DCP
    chooses the next solution in the matrix and continues the search process. The
    feasibility of the updated solution of 𝑠 𝑟 is checked to determine if it needs
    repair by sending it to the repair mechanism. Next, 𝑠 𝑟 is evaluated if it consumes
    less energy than 𝑏𝑠 ; subsequently, 𝑏𝑠 becomes 𝑠 𝑟 as explained in the previous
    stage. The search phase then selects the next solution 𝑠 𝑟 in the solution matrix
    to repeat the same steps in this stage until the last solution in the matrix.
    The entire search phase undergoes 𝑖𝑡𝑒𝑟𝑁 iterations, and each iteration is supposed
    to generate better solutions with 𝑏𝑠 being updated. Finally, DCP generates the
    optimal solution 𝑏𝑠 , which preserves the optimal energy in the CaaS cloud system
    under consideration. Algorithm 1 Repair mechanism. 1: input:  ℎ𝑒𝐿𝑖𝑠𝑡 2: foreach  𝑜ℎ𝑒
    in ℎ𝑒𝐿𝑖𝑠𝑡  do 3:  while  𝑜ℎ𝑒.𝑖𝑠𝑂𝑣𝑒𝑟𝑙𝑜𝑎𝑑𝑒𝑑==𝑡𝑟𝑢𝑒  do 4:    𝑔𝑒𝐿𝑖𝑠𝑡←𝑜ℎ𝑒.𝑔𝑒𝑡𝐺𝑒𝐿𝑖𝑠𝑡()
    5:    𝑂𝐹=𝑚𝑎𝑥𝑖𝑚𝑢𝑚𝑉𝑎𝑙𝑢𝑒 6:    𝑚𝑖𝑔𝐺𝑒←𝑛𝑢𝑙𝑙 7:    foreach  𝑔𝑒 in 𝑔𝑒𝐿𝑖𝑠𝑡  do 8:     𝑂
    𝐹 𝑡𝑚𝑝 =𝑂𝐹(ℎ𝑒,𝑔𝑒)             //Equation (12) 9:     if  𝑂 𝐹 𝑡𝑚𝑝 <𝑂𝐹  then 10:      𝑂𝐹=𝑂
    𝐹 𝑡𝑚𝑝 11:      𝑚𝑖𝑔𝐺𝑒←𝑔𝑒 12:     end if 13:     𝑜ℎ𝑒.𝑟𝑒𝑚𝑜𝑣𝑒𝐸𝑙𝑒𝑚𝑒𝑛𝑡(𝑚𝑖𝑔𝐺𝑒) 14:     𝑔𝑒𝑀𝑖𝑔𝑟𝑎𝑡𝑒𝐿𝑖𝑠𝑡.𝑎𝑑𝑑(𝑚𝑖𝑔𝐺𝑒)
    15:    end for 16:  end while 17: end for 18: foreach  𝑔𝑒 in 𝑔𝑒𝑀𝑖𝑔𝑟𝑎𝑡𝑒𝐿𝑖𝑠𝑡  do
    19:  𝑆𝐹=𝑚𝑎𝑥𝑖𝑚𝑢𝑚𝑉𝑎𝑙𝑢𝑒 20:  𝑑𝑒𝑠𝐻𝑒←𝑛𝑢𝑙𝑙 21:  foreach  ℎ𝑒 in ℎ𝑒𝐿𝑖𝑠𝑡  do 22:    if  ℎ𝑒.𝑐𝑎𝑛𝐻𝑜𝑠𝑡(𝑔𝑒)==𝑡𝑟𝑢𝑒  then
    23:     𝑆𝐹𝑡𝑚𝑝=𝑆𝐹(ℎ𝑒,𝑔𝑒)             //Equation (13) 24:     if  𝑆𝐹𝑡𝑚𝑝<𝑆𝐹  then
    25:      𝑆𝐹=𝑆𝐹𝑡𝑚𝑝 26:      𝑑𝑒𝑠𝐻𝑒=ℎ𝑒 27:     end if 28:    end if 29:  end for
    30:  𝑑𝑒𝑠𝐻𝑒.𝑎𝑑𝑑𝐸𝑙𝑒𝑚𝑒𝑛𝑡(𝑔𝑒) 31:  𝑔𝑒𝑀𝑖𝑔𝑟𝑎𝑡𝑒𝐿𝑖𝑠𝑡.𝑟𝑒𝑚𝑜𝑣𝑒(𝑔𝑒) 32:  𝑢𝑝𝑑𝑎𝑡𝑒(ℎ𝑒𝐿𝑖𝑠𝑡,𝑑𝑒𝑠𝐻𝑒)
    33: end for Algorithm 2 Best solution initialization. 1: input:  𝑝𝑚𝐿𝑖𝑠𝑡 , 𝑣𝑚𝐿𝑖𝑠𝑡
    , 𝑐𝑜𝑛𝑡𝐿𝑖𝑠𝑡 2: output:  𝑏𝑠 3: foreach  𝑐𝑜𝑛𝑡𝑖𝑛𝑒𝑟 in 𝑐𝑜𝑛𝑡𝐿𝑖𝑠𝑡  do 4:  foreach  𝑣𝑚
    in 𝑝𝑣𝑚𝐿𝑖𝑠𝑡  do 5:    if  𝑎𝑠𝑠𝑖𝑔𝑛(𝑐𝑜𝑛𝑡,𝑣𝑚)==𝑡𝑟𝑢𝑒  then 6:     𝑣𝑚.𝑎𝑠𝑠𝑖𝑔𝑛(𝑐𝑜𝑛𝑡) 7:    end
    if 8:  end for 9: end for 10: foreach  𝑣𝑚 in 𝑣𝑚𝐿𝑖𝑠𝑡  do 11:  foreach  𝑝𝑚 in 𝑝𝑚𝐿𝑖𝑠𝑡  do
    12:    if  𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑒(𝑣𝑚,𝑝𝑚)==𝑡𝑟𝑢𝑒  then 13:     𝑝𝑚.𝑎𝑙𝑙𝑜𝑐𝑎𝑡𝑒(𝑣𝑚) 14:    end if 15:  end
    for 16: end for 17: 𝑏𝑠.𝑠𝑒𝑡𝑆𝑜𝑙𝑢𝑡𝑖𝑜𝑛(𝑣𝑚𝐿𝑖𝑠𝑡,𝑝𝑚𝐿𝑖𝑠𝑡) 18: return  𝑏𝑠 5. Performance
    Evaluation 5.1. Experimental Setup The experiments in this study are performed
    on a computer with a 2.8 GHz Intel Core i7 processor and 16 GB of RAM with a MacOS
    Mojave OS. The experiments are conducted in the Java programming language. Two
    sets of experiments are conducted to evaluate the DCP policy. The first and second
    sets test the DCP policy in homogeneous and heterogeneous cloud systems, respectively.
    Each experiment tests the mechanism for different container numbers: 100, 200,
    300, 400, 500, 600, 700, 800, 900, and 1000. The DCP policy is based on the WOA,
    which produces solutions randomly; therefore, each experiment is run ten independent
    times for each number of containers, i.e., for number of containers = 100, the
    experiment is run ten times, then another ten times when the number of containers
    = 200 and so on. The total number of runs in each experimental set is 100. The
    reported results are the averages of the corresponding run results for each number
    of containers. Two state-of-the-art container placement mechanisms are implemented
    for comparison to comprehensively evaluate the performance of DCP. The first is
    the IGA mechanism [15], which is a gene-based mechanism that utilizes different
    exchange mutation operations to determine the optimal solution. The second mechanism
    is the DWO mechanism [16], which is based on the WOA for solving the optimization
    problem of container-to-VM and VM-to-PM placement strategies. Both mechanisms
    are discussed in Section 2. They were selected because they both focus on energy
    consumption aspects in CaaS cloud systems. In addition, they both are evolution-based
    approaches, which make them suitable mechanisms to compare DCP with. The search
    space for DCP, IGA, and WOA is set as 100. The number of solutions generated for
    IGA and WOA is set as 120 in each run. However, the number of solutions for the
    DCP policy is reduced to 30 because it decreases both the power consumption and
    execution time as demonstrated later in this section through the experiments.
    The cloud system infrastructure in this simulation comprises PMs, VMs, and containers.
    The hardware specifications and power consumption values of the PMs are collected
    from the SPECpower benchmarks [37] as listed in Table 2. 𝑝 𝑖𝑑𝑙𝑒 refers to the
    power consumed by a PM when it is idle, whereas 𝑝 𝑚𝑎𝑥 refers to the power consumed
    under full utilization. The number of available PMs is set as 1000. Various types
    of VMs are listed in Table 3, which represent several VM instances from Amazon
    [21]. Eight different types of containers are simulated in these experiments,
    each of which contains different CPU and RAM capabilities as summarized in Table
    4. The cloud infrastructure values remain constant throughout the experiments.
    Table 2. PM types. Table 3. VM types. Table 4. Container types. 5.2. Experiment
    I The first experiment investigates the effectiveness of the DCP mechanism in
    a homogeneous cloud system. The simulated cloud system comprises PMs with the
    same hardware and power specifications. The hardware and power consumption values
    used in this experiment are those of the “Fujitsu Primergy TX1310 M5” model as
    listed in Table 2. Furthermore, one VM type is employed for this experiment, which
    is “m3.mediu” as listed in Table 3. Eight different types of containers are randomly
    employed as listed in Table 4. The DCP mechanism outperforms both the IGA and
    DWO mechanisms in terms of power consumption. The average power consumption of
    the system is approximately 3100 W, 14,200 W, and 15,500 W for DCP, IGA, and DWO,
    respectively, as illustrated in Figure 4. Overall, the power consumed by the PMs
    for DCP decreases by approximately 78% compared with IGA; note that IGA yields
    better results on average than DWO. The effectiveness of the DCP mechanism improves
    as the number of containers increases as shown in Figure 5. Figure 4. Power consumption
    in homogeneous environment. Figure 5. Power consumption per container number Set
    in homogeneous. The DCP mechanism employs a resource utilization approach that
    aims to reduce the number of active PMs for placing VMs on PMs, leading to more
    power preservation. Therefore, more containers to run leads to a higher number
    of PMs. Table 5 lists the number of running VMs and PMs for each mechanism for
    each set of number of containers. For example, the DCP mechanism consumes approximately
    800 W when the number of containers is 100, achieving approximately 53% and 70%
    power savings compared to IGA (1700 W) and DWO (2660) mechanisms, respectively.
    This is because the number of active PMs for DCP is six compared with 8 and 12
    for the IGA and DWO mechanisms, respectively. The power savings increase sharply
    to more than ten-fold when the number of containers is 1000; to elaborate, the
    IGA and DWO mechanisms consume 30,000 W and 28,000 W, respectively, whereas DCP
    consumes approximately 5600 W. Table 5. Number of VMs and PMs in Homogeneous Environment.
    The search times for each mechanism are shown in Figure 6. The DCP mechanism outperforms
    its counterparts in terms of performance. The results show that the DWO mechanism
    behaves poorly in terms of searching for the optimal solution as the number of
    containers increases. Although the IGA mechanism yields far better outcomes in
    terms of search time than DWO, it does not meet the performance of the DCP mechanism
    because DCP requires approximately 17 s on average to find the optimal solution,
    whereas this figure doubles to approximately 36 s for IGA. The search time for
    DWO is approximately 213 s on average. Overall, DCP demonstrably improves the
    performance of the container placement mechanisms by approximately 50%. Figure
    6. Search time in homogeneous environment. 5.3. Experiment II The second experiment
    investigates the behavior of the DCP mechanism in heterogeneous cloud systems.
    In such an environment, PMs have various hardware and power consumption specifications,
    which is a more realistic assumption than that of homogeneous cloud systems. The
    power consumption results are depicted in Figure 7. The average power consumption
    for the DCP mechanism is approximately 1500 W, whereas the power consumption for
    the IGA and DWO mechanisms is approximately 33,000 W and 10,000 W, respectively.
    DCP saves power by approximately 85% compared to the other mechanisms. The effectiveness
    of the DCP mechanism improves as the number of containers increases as shown in
    Figure 8. Figure 7. Power consumption in heterogeneous environment. Figure 8.
    Power consumption per container number set in heterogeneous environment. Table
    6 lists the numbers of active VMs and PMs for each mechanism according to each
    container number set. Note that both DCP and DWO achieve better power consumption
    figures comparable to those for the same mechanisms from the first experiment
    because these mechanisms can find better solutions to reduce power consumption
    when different PM and VM specifications are employed. The greater the number of
    hosted containers, the greater the effectiveness of DCP. When the number of containers
    is 1000, the power consumed by DCP is 2700 W, whereas DWO consumes approximately
    14,300 W. Table 6. Number of VMs and PMs in heterogeneous environment. The search
    time for DCP is approximately 21 s on average, whereas the average time required
    to find the optimal solution for IGA is approximately 69 s and that for IGA is
    approximately 212 s. DWO preserves power better than IGA but at the expense of
    performance. DCP manages to reduce power consumption and improve performance in
    comparison with other mechanisms. Figure 9 presents the average search time for
    each mechanism. More containers lead to more searching, resulting in more time
    being required to find the best solution. For example, DCP requires, on average,
    approximately 1 s to find the best solution when the number of containers is 100;
    however, the time required increases to just more than 50 s when the number of
    containers is 1000. Figure 9. Search time in heterogeneous environment. This section
    demonstrates that the proposed mechanism can save energy in both homogeneous and
    heterogeneous cloud systems. DCP can perform better in heterogeneous systems because
    the mechanism employs a resource utilization approach that focuses on reducing
    the number of active PMs. However, the mechanism does not focus on reducing the
    number of active VMs compared with other related mechanisms. Furthermore, the
    performance of DCP is better than that of the alternatives because it reduces
    the time required to find the optimal solution in both homogeneous and heterogeneous
    environments. 6. Conclusions CP mechanisms are critical in CaaS cloud systems
    with regards to energy savings. This study presents the DCP mechanism, a novel
    CP policy that significantly reduces power consumption. DCP extends the WOA technique
    to find the best solution in a relatively short time. DCP is evaluated and compared
    with the IGA and DWO mechanisms in two different cloud systems: homogeneous and
    heterogeneous. The experiments demonstrate that DCP consumes approximately 78%
    less power while reducing the search time by approximately 50% in homogeneous
    clouds. In addition, DCP saves power by approximately 85% while reducing the search
    time by approximately 30% in heterogeneous clouds. Considering the scope for future
    research, the DCP mechanism can be improved as follows. First, it can consider
    more optimization objectives, such as high availability and low resource wastage.
    Furthermore, DCP can be extended to reduce the number of VMs assigned to containers.
    Second, the evaluation conducted in this study uses simulations with randomly
    generated containers, VMs, and PMs. Therefore, the proposed mechanism can be further
    tested on real systems. Third, the DCP policy is a static placement mechanism.
    Thus, it can be extended to be a dynamic mechanism with the aim of rescheduling
    containers during runtime. Finally, the DCP mechanism can be extended and evaluated
    for cloud systems integrated with fog systems. Funding This project was supported
    by the Deanship of Scientific Research at Prince Sattam Bin Abdulaziz University
    under research project No. 2018/01/9371. Data Availability Statement No data were
    used to support this study. Conflicts of Interest The author declares no conflict
    of interest. References Pahl, C.; Brogi, A.; Soldani, J.; Jamshidi, P. Cloud container
    technologies: A state-of-the-art review. IEEE Trans. Cloud Comput. 2019, 7, 677–692.
    [Google Scholar] [CrossRef] Buyya, R.; Ranjan, R.; Calheiros, R.N. Modeling and
    simulation of scalable Cloud computing environments and the CloudSim toolkit:
    Challenges and opportunities. In Proceedings of the 2009 International Conference
    on High Performance Computing & Simulation, Leipzig, Germany, 21–24 June 2009;
    pp. 1–11. [Google Scholar] [CrossRef] Morabito, R. Virtualization on Internet
    of Things Edge Devices with Container Technologies: A Performance Evaluation.
    IEEE Access 2017, 5, 8835–8850. [Google Scholar] [CrossRef] Randal, A. The Ideal
    versus the Real: Revisiting the History of Virtual Machines and Containers. ACM
    Comput. Surv. 2020, 53, 1–31. [Google Scholar] [CrossRef] [Green Version] Barham,
    P.; Dragovic, B.; Fraser, K.; Hand, S.; Harris, T.; Ho, A.; Neugebauer, R.; Pratt,
    I.; Warfield, A. Xen and the art of virtualization. In Proceedings of the Nineteenth
    ACM Symposium on Operating Systems Principles-SOSP ’03, New York, NY, USA, 19–22
    October 2003; p. 164. [Google Scholar] [CrossRef] Bernstein, D. Containers and
    Cloud: From LXC to Docker to Kubernetes. IEEE Cloud Comput. 2014, 1, 81–84. [Google
    Scholar] [CrossRef] Piraghaj, S.F.; Dastjerdi, A.V.; Calheiros, R.N.; Buyya, R.
    Efficient Virtual Machine Sizing for Hosting Containers as a Service (SERVICES
    2015). In Proceedings of the 2015 IEEE World Congress on Services, New York, NY,
    USA, 27 June–2 July 2015; IEEE: Piscataway, NJ, USA, 2015; pp. 31–38. [Google
    Scholar] [CrossRef] Linthicum, D.S. Moving to Autonomous and Self-Migrating Containers
    for Cloud Applications. IEEE Cloud Comput. 2016, 3, 6–9. [Google Scholar] [CrossRef]
    Merkel, D. Docker Lightweight Linux Containers for Consistent Development and
    Deployment. J. Linux 2014, 239, 2. [Google Scholar] Hightower, K.; Burns, B.;
    Beda, J. Kubernetes: Up and Running: Dive into the Future of Infrastructure, 1st
    ed.; O’Reilly Media Inc.: Sebastopol, CA, USA, 2017; p. 272. [Google Scholar]
    Hussein, M.K.; Mousa, M.H.; Alqarni, M.A. A placement architecture for a container
    as a service (CaaS) in a cloud environment. J. Cloud Comput. 2019, 8, 7. [Google
    Scholar] [CrossRef] [Green Version] Piraghaj, S.F.; Dastjerdi, A.V.; Calheiros,
    R.N.; Buyya, R. A Framework and Algorithm for Energy Efficient Container Consolidation
    in Cloud Data Centers. In Proceedings of the 2015 IEEE International Conference
    on Data Science and Data Intensive Systems, Sydney, NSW, Australia, 11–13 December
    2015; IEEE: Piscataway, NJ, USA, 2015; pp. 368–375. [Google Scholar] [CrossRef]
    Kaur, K.; Dhand, T.; Kumar, N.; Zeadally, S. Container-as-a-Service at the Edge:
    Trade-off between Energy Efficiency and Service Availability at Fog Nano Data
    Centers. IEEE Wirel. Commun. 2017, 24, 48–56. [Google Scholar] [CrossRef] Shi,
    T.; Ma, H.; Chen, G. Energy-Aware Container Consolidation Based on PSO in Cloud
    Data Centers. In Proceedings of the 2018 IEEE Congress on Evolutionary Computation
    (CEC), Rio de Janeiro, Brazil, 8–13 July 2018; IEEE: Piscataway, NJ, USA, 2018;
    pp. 1–8. [Google Scholar] [CrossRef] Zhang, R.; Chen, Y.; Dong, B.; Tian, F.;
    Zheng, Q. A Genetic Algorithm-Based Energy-Efficient Container Placement Strategy
    in CaaS. IEEE Access 2019, 7, 121360–121373. [Google Scholar] [CrossRef] Al-Moalmi,
    A.; Luo, J.; Salah, A.; Li, K.; Yin, L. A whale optimization system for energy-efficient
    container placement in data centers. Expert Syst. Appl. 2021, 164, 113719. [Google
    Scholar] [CrossRef] Zhang, R.; Chen, Y.; Zhang, F.; Tian, F.; Dong, B. Be Good
    Neighbors: A Novel Application Isolation Metric Used to Optimize the Initial Container
    Placement in CaaS. IEEE Access 2020, 8, 178195–178207. [Google Scholar] [CrossRef]
    Zhou, R.; Li, Z.; Wu, C. Scheduling Frameworks for Cloud Container Services. IEEE/ACM
    Trans. Netw. 2018, 26, 436–450. [Google Scholar] [CrossRef] Boza, E.F.; Abad,
    C.L.; Narayanan, S.P.; Balasubramanian, B.; Jang, M. A case for performance-aware
    deployment of containers. In Proceedings of the WOC 2019—Proceedings of the 2019
    5th International Workshop on Container Technologies and Container Clouds, Part
    of Middleware 2019, Davis, CA, USA, 9–13 December 2019; pp. 25–30. [Google Scholar]
    [CrossRef] Luo, J.; Yin, L.; Hu, J.; Wang, C.; Liu, X.; Fan, X.; Luo, H. Container-based
    fog computing architecture and energy-balancing scheduling algorithm for energy
    IoT. Future Gener. Comput. Syst. 2019, 97, 50–60. [Google Scholar] [CrossRef]
    Khan, A.A.; Zakarya, M.; Buyya, R.; Khan, R.; Khan, M.; Rana, O. An energy and
    performance aware consolidation technique for containerized datacenters. IEEE
    Trans. Cloud Comput. 2019, 9, 1305–1322. [Google Scholar] [CrossRef] [Green Version]
    Nanda, S.; Hacker, T.J. RACC: Resource-Aware Container Consolidation using a Deep
    Learning Approach. In Proceedings of the First Workshop on Machine Learning for
    Computing Systems, New York, NY, USA, 12 June 2018; pp. 1–5. [Google Scholar]
    [CrossRef] Alahmad, Y.; Daradkeh, T.; Agarwal, A. Availability-Aware Container
    Scheduler for Application Services in Cloud. In Proceedings of the 2018 IEEE 37th
    International Performance Computing and Communications Conference, IPCCC 2018,
    Orlando, FL, USA, 17–19 November 2018. [Google Scholar] [CrossRef] Mseddi, A.;
    Jaafar, W.; Elbiaze, H.; Ajib, W. Joint Container Placement and Task Provisioning
    in Dynamic Fog Computing. IEEE Internet Things J. 2019, 6, 10028–10040. [Google
    Scholar] [CrossRef] Mendes, S.; Simão, J.; Veiga, L. Oversubscribing micro-clouds
    with energy-aware containers scheduling. In Proceedings of the ACM Symposium on
    Applied Computing, Limassol, Cyprus, 8–12 April 2019; Part F1477. pp. 130–137.
    [Google Scholar] [CrossRef] [Green Version] Zhong, Z.; Buyya, R. A Cost-Efficient
    Container Orchestration Strategy in Kubernetes-Based Cloud Computing Infrastructures
    with Heterogeneous Resources. ACM Trans. Internet Technol. 2020, 20, 1–24. [Google
    Scholar] [CrossRef] Benomar, Z.; Longo, F.; Merlino, G.; Puliafito, A. Cloud-based
    Enabling Mechanisms for Container Deployment and Migration at the Network Edge.
    ACM Trans. Internet Technol. 2020, 20, 1–28. [Google Scholar] [CrossRef] Zhao,
    D.; Mohamed, M.; Ludwig, H. Locality-Aware Scheduling for Containers in Cloud
    Computing. IEEE Trans. Cloud Comput. 2020, 8, 635–646. [Google Scholar] [CrossRef]
    Nardelli, M.; Hochreiner, C.; Schulte, S. Elastic Provisioning of Virtual Machines
    for Container Deployment. In Proceedings of the 8th ACM/SPEC on International
    Conference on Performance Engineering Companion, New York, NY, USA, 22–26 April
    2017; pp. 5–10. [Google Scholar] [CrossRef] Santos, G.; Paulino, H.; Vardasca,
    T. QoE-aware auto-scaling of heterogeneous containerized services (and its application
    to health services). In Proceedings of the 35th Annual ACM Symposium on Applied
    Computing, New York, NY, USA, 30 March–3 April 2020; pp. 242–249. [Google Scholar]
    [CrossRef] [Green Version] Mirjalili, S.; Lewis, A. The Whale Optimization Algorithm.
    Adv. Eng. Softw. 2016, 95, 51–67. [Google Scholar] [CrossRef] Berl, a.; Gelenbe,
    E.; Di Girolamo, M.; Giuliani, G.; De Meer, H.; Dang, M.Q.; Pentikousis, K. Energy-Efficient
    Cloud Computing. Comput. J. 2010, 53, 1045–1051. [Google Scholar] [CrossRef] [Green
    Version] Beloglazov, A.; Buyya, R. Optimal online deterministic algorithms and
    adaptive heuristics for energy and performance efficient dynamic consolidation
    of virtual machines in Cloud data centers. Concurr. Comput. Pract. Exp. 2012,
    24, 1397–1420. [Google Scholar] [CrossRef] Kusic, D.; Kephart, J.O.; Hanson, J.E.;
    Kandasamy, N.; Jiang, G. Power and performance management of virtualized computing
    environments via lookahead control. Clust. Comput. 2008, 12, 1–15. [Google Scholar]
    [CrossRef] [Green Version] Ahmad, I.; AlFailakawi, M.G.; AlMutawa, A.; Alsalman,
    L. Container scheduling techniques: A Survey and assessment. J. King Saud Univ.-Comput.
    Inf. Sci. 2022, 34, 3934–3947. [Google Scholar] [CrossRef] Vhatkar, K.N.; Bhole,
    G.P. Optimal container resource allocation in cloud architecture: A new hybrid
    model. J. King Saud Univ.-Comput. Inf. Sci. 2022, 34, 1906–1918. [Google Scholar]
    [CrossRef] Lange, K.D. Identifying Shades of Green: The SPECpower Benchmarks.
    Computer 2009, 42, 95–97. [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note:
    The statements, opinions and data contained in all publications are solely those
    of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s).
    MDPI and/or the editor(s) disclaim responsibility for any injury to people or
    property resulting from any ideas, methods, instructions or products referred
    to in the content.  © 2023 by the author. Licensee MDPI, Basel, Switzerland. This
    article is an open access article distributed under the terms and conditions of
    the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Alwabel, A. A Novel Container Placement Mechanism
    Based on Whale Optimization Algorithm for CaaS Clouds. Electronics 2023, 12, 3369.
    https://doi.org/10.3390/electronics12153369 AMA Style Alwabel A. A Novel Container
    Placement Mechanism Based on Whale Optimization Algorithm for CaaS Clouds. Electronics.
    2023; 12(15):3369. https://doi.org/10.3390/electronics12153369 Chicago/Turabian
    Style Alwabel, Abdulelah. 2023. \"A Novel Container Placement Mechanism Based
    on Whale Optimization Algorithm for CaaS Clouds\" Electronics 12, no. 15: 3369.
    https://doi.org/10.3390/electronics12153369 Note that from the first issue of
    2016, this journal uses article numbers instead of page numbers. See further details
    here. Article Metrics Citations No citations were found for this article, but
    you may check on Google Scholar Article Access Statistics Article access statistics
    Article Views 7. Jan 17. Jan 27. Jan 6. Feb 16. Feb 26. Feb 7. Mar 17. Mar 27.
    Mar 0 1000 250 500 750 For more information on the journal statistics, click here.
    Multiple requests from the same IP address are counted as one view.   Electronics,
    EISSN 2079-9292, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Electronics (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Novel Container Placement Mechanism Based on Whale Optimization Algorithm
    for CaaS Clouds
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Dusane A.V.
  - Adhiya K.P.
  citation_count: '0'
  description: The distributed systems are very effective when it deals with massive
    data processing. Nowadays, entire world generates high-dimensional data such as
    audio, video, image etc. To process such extensive data at a minimum is hard for
    a stand-alone machine, and this is a big challenge for the computer system to
    evaluate such data. The distributed framework is the solution for the process
    of such extensive data. Still, during the execution, some faulty or straggler
    nodes can increase the overall computation time to process data. However, to detect
    such straggler nodes, from large distributed systems are mandatory before assigning
    jobs to VM. Early identification of such faulty node can future save the overall
    computation time. In this paper, we proposed a hybrid machine learning model for
    detecting faulty nodes in large distributed machines using collaboration of reinforcement
    and supervised machine learning. The large Virtual Machine (VM’s) log data has
    been collected from the distributed environment and proceeded with reinforcement
    learning algorithm for module training and supervised machine learning for module
    testing. According to extracted features, reinforcement learning encompasses an
    activation function that generates the label for the respective node, whether
    healthy or faulty. In the testing phase, the natural world VM’s log data has been
    collected and evaluated with supervised machine learning classifiers. Several
    machine learning classification algorithms have evaluated and acquired the results.
    The SVM provides higher accuracy over the other machine learning classifiers with
    our reinforcement learning model.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Journal of Theoretical and Applied Information Technology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'HYBRIDML: FAULTY NODE DETECTION USING HYBRID LEARNING MODEL FOR DISTRIBUTED
    DATA CENTRES'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Berkaoui A.
  - Gahi Y.
  citation_count: '0'
  description: In an era where businesses continuously try to conquer new markets
    and at-tract new customers, Big Data usage is becoming a strategic enabler to
    sup-port such ambitions. The upward trend of enterprises offering omnichannel
    experiences to customers and prospects gives rise to vast amounts of non-traditional
    data sources, which need to be leveraged to turn them into action-able insights.
    Enterprises are becoming aware of these challenges and are trying to integrate
    their existing corporate data with non-traditional acquired Big Data to unlock
    meaningful use cases. The IT landscape has coincided with these trends, offering
    modern technology stacks in infrastructure, Big Data platforms, and artificial
    intelligence tools and frameworks. However, the path toward data analytics in
    the enterprise concerning business requirements, time to market, and industrialization
    constraints remain a real challenge. Implementing a data analytics use case efficiently
    requires heterogeneous steps ranging from data preparation, model training, validation,
    serving, and managing the product lifecycle to guarantee sustainability and ultimately
    reach the expected business outcomes. Therefore, the enterprises' data ecosystem
    should support these steps by providing a suitable platform enabling data scientists'
    collaboration with tools to unleash data analytics at scale. This paper addresses
    these needs and suggests a scalable cloud-ready architecture that fits into data
    science pipelines with all their steps, specificities, and requirements. The propositions
    made through this contribution are driven by business ambitions and confronted
    with a benchmark that presents the available solutions in the IT marketplace with
    careful consideration of their strengths and weaknesses. Finally, the suggested
    architecture is exposed and discussed with perspectives and future research areas.
  doi: 10.1145/3607720.3607744
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesNISSProceedingsNISS ''23A
    private cloud-based Datalab for scalable DSML pipelines RESEARCH-ARTICLE SHARE
    ON A private cloud-based Datalab for scalable DSML pipelines Authors: Abdellah
    Berkaoui , Youssef Gahi Authors Info & Claims NISS ''23: Proceedings of the 6th
    International Conference on Networking, Intelligent Systems & SecurityMay 2023Article
    No.: 22Pages 1–7https://doi.org/10.1145/3607720.3607744 Published:13 November
    2023Publication History 0 citation 16 Downloads View all FormatsPDF NISS ''23:
    Proceedings of the 6th International Conference on Networking, Intelligent Systems
    & Security A private cloud-based Datalab for scalable DSML pipelines Pages 1–7
    Previous Next ABSTRACT References Index Terms Recommendations Comments ABSTRACT
    In an era where businesses continuously try to conquer new markets and at-tract
    new customers, Big Data usage is becoming a strategic enabler to sup-port such
    ambitions. The upward trend of enterprises offering omnichannel experiences to
    customers and prospects gives rise to vast amounts of non-traditional data sources,
    which need to be leveraged to turn them into action-able insights. Enterprises
    are becoming aware of these challenges and are trying to integrate their existing
    corporate data with non-traditional acquired Big Data to unlock meaningful use
    cases. The IT landscape has coincided with these trends, offering modern technology
    stacks in infrastructure, Big Data platforms, and artificial intelligence tools
    and frameworks. However, the path toward data analytics in the enterprise concerning
    business requirements, time to market, and industrialization constraints remain
    a real challenge. Implementing a data analytics use case efficiently requires
    heterogeneous steps ranging from data preparation, model training, validation,
    serving, and managing the product lifecycle to guarantee sustainability and ultimately
    reach the expected business outcomes. Therefore, the enterprises’ data ecosystem
    should support these steps by providing a suitable platform enabling data scientists’
    collaboration with tools to unleash data analytics at scale. This paper addresses
    these needs and suggests a scalable cloud-ready architecture that fits into data
    science pipelines with all their steps, specificities, and requirements. The propositions
    made through this contribution are driven by business ambitions and confronted
    with a benchmark that presents the available solutions in the IT marketplace with
    careful consideration of their strengths and weaknesses. Finally, the suggested
    architecture is exposed and discussed with perspectives and future research areas.
    References Ravi Bhalla, 2014. The omnichannel customer experience: Driving engagement
    through digitization : Journal of Digital & Social Media Marketing https://www.forbes.com/sites/louiscolumbus/2018/12/23/big-data-analytics-adoption-soared-in-the-enterprise-in-2018,
    accessed on 02/05/2023 https://online.hbs.edu/blog/post/types-of-data-analysis,
    accessed on 02/05/2023 Show All References Index Terms A private cloud-based Datalab
    for scalable DSML pipelines Computer systems organization Architectures Distributed
    architectures Information systems Software and its engineering Software organization
    and properties Software system structures Distributed systems organizing principles
    Index terms have been assigned to the content through auto-classification. Recommendations
    Big data analytics in Cloud computing: an overview Abstract Big Data and Cloud
    Computing as two mainstream technologies, are at the center of concern in the
    IT field. Every day a huge amount of data is produced from different sources.
    This data is so big in size that traditional processing tools are unable ... Read
    More DataLab: a version data management and analytics system BIGDSE ''16: Proceedings
    of the 2nd International Workshop on BIG Data Software Engineering One challenge
    in big data analytics is the lack of tools to manage the complex interactions
    among code, data and parameters, especially in the common situation where all
    these factors can change a lot. We present our preliminary experience with DataLab,
    ... Read More Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data
    Analytics in the Cloud BIGDATACONGRESS ''13: Proceedings of the 2013 IEEE International
    Congress on Big Data Data Analytics has proven its importance in knowledge discovery
    and decision support in different data and application domains. Big data analytics
    poses a serious challenge in terms of the necessary hardware and software resources.
    The cloud technology ... Read More Comments 24 References View Table Of Contents
    Footer Categories Journals Magazines Books Proceedings SIGs Conferences Collections
    People About About ACM Digital Library ACM Digital Library Board Subscription
    Information Author Guidelines Using ACM Digital Library All Holdings within the
    ACM Digital Library ACM Computing Classification System Digital Library Accessibility
    Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: ACM International Conference Proceeding Series
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A private cloud-based Datalab for scalable DSML pipelines
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhu L.
  - Wu F.
  - Hu Y.
  - Huang K.
  - Tian X.
  citation_count: '2'
  description: Container-based cloud technology has changed the delivery mode of traditional
    applications and brought a breakthrough development to the field of cloud computing.
    However, the uncertainty of cloud environment and variability of application requirements
    increase the scheduling cost of tasks in container cloud. In particular, how to
    balance the business performance and utilization efficiency of cloud resources
    in the peak stage of application access is the focus of future for container cluster
    technology. In this paper, we propose a heuristics multi-objective task scheduling
    framework based on reinforcement learning (AC-CCTS). The proposed framework not
    only solves the problems of single objective and local convergence in traditional
    task scheduling methods, but also reduces the cost of experiential learning with
    reinforcement learning methods. Firstly, we define container cloud environment,
    scheduling agent, scheduling actions and scheduling evaluation methods to establish
    a deep reinforcement learning-based dynamic scheduling model. Then, based on Actor-Critic
    algorithm, we design heuristic rules and prioritized experience replay method
    to speed up convergence of task scheduling and decrease learning costs. At the
    same time, we provide compensation mechanism for dynamic task scheduling to improve
    the robustness of the approach. Finally, we implement comparative experiments
    to simulate various scheduling scenarios and verify the effectiveness of AC-CCTS
    from different perspectives such as resource balance, resource utilization and
    QoS. Compared with traditional meta-heuristic scheduling methods such as FIMPSO,
    HWOA-MBA and other reinforcement learning algorithms such as DeepRM-Plus and RLSched,
    AC-CCTS shows better resource utilization efficiency and convergence stability
    in container-based cloud task scheduling.
  doi: 10.1007/s00521-023-08208-6
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Neural Computing and Applications
    Article A heuristic multi-objective task scheduling framework for container-based
    clouds via actor-critic reinforcement learning Original Article Published: 17
    March 2023 Volume 35, pages 9687–9710, (2023) Cite this article Download PDF Access
    provided by University of Nebraska-Lincoln Neural Computing and Applications Aims
    and scope Submit manuscript Lilu Zhu , Feng Wu, Yanfeng Hu, Kai Huang & Xinmei
    Tian  472 Accesses 2 Citations 1 Altmetric Explore all metrics Abstract Container-based
    cloud technology has changed the delivery mode of traditional applications and
    brought a breakthrough development to the field of cloud computing. However, the
    uncertainty of cloud environment and variability of application requirements increase
    the scheduling cost of tasks in container cloud. In particular, how to balance
    the business performance and utilization efficiency of cloud resources in the
    peak stage of application access is the focus of future for container cluster
    technology. In this paper, we propose a heuristics multi-objective task scheduling
    framework based on reinforcement learning (AC-CCTS). The proposed framework not
    only solves the problems of single objective and local convergence in traditional
    task scheduling methods, but also reduces the cost of experiential learning with
    reinforcement learning methods. Firstly, we define container cloud environment,
    scheduling agent, scheduling actions and scheduling evaluation methods to establish
    a deep reinforcement learning-based dynamic scheduling model. Then, based on Actor-Critic
    algorithm, we design heuristic rules and prioritized experience replay method
    to speed up convergence of task scheduling and decrease learning costs. At the
    same time, we provide compensation mechanism for dynamic task scheduling to improve
    the robustness of the approach. Finally, we implement comparative experiments
    to simulate various scheduling scenarios and verify the effectiveness of AC-CCTS
    from different perspectives such as resource balance, resource utilization and
    QoS. Compared with traditional meta-heuristic scheduling methods such as FIMPSO,
    HWOA-MBA and other reinforcement learning algorithms such as DeepRM-Plus and RLSched,
    AC-CCTS shows better resource utilization efficiency and convergence stability
    in container-based cloud task scheduling. Similar content being viewed by others
    A novel strategy for deterministic workflow scheduling with load balancing using
    modified min-min heuristic in cloud computing environment Article 15 March 2024
    Bias Estimation Correction in Multi-Agent Reinforcement Learning for Mixed Cooperative-Competitive
    Environments Article 16 November 2023 An artificial intelligence strategy for
    the deployment of future microservice-based applications in 6G networks Article
    28 March 2024 1 Introduction In recent years, container technology has been developed
    rapidly, and widely concerned by academia and industry. The ecosystem of container
    technology is gradually formed [1]. Container promotes the rapid development of
    container cloud because of its light weight, fast startup, and small resource
    occupation [2]. More and more applications migrated to the cloud platform, leading
    to the expansion of datacenter. With the container cloud, users can make more
    efficient use of the massive computing resources, expand business scale flexibly,
    and fully meet business needs in normal and explosive periods, respectively [3].
    However, the characteristics of container cloud with high elasticity and scalability
    also bring difficulties in task scheduling and resource management. Different
    from traditional virtual machine, highly dynamic and various feature of container-based
    cloud workloads and environments considerably increases the complexity of shared
    resource management. Consequently, this requires current container scheduling
    systems to be further enhanced in response to the rising resource heterogeneity,
    application distribution and workload diversity, such as (1) with the increase
    and decline of traffic, containerized task can be flexibly and quickly scaled
    to achieve dynamic and reasonable adjustment of resources, (2) containerized task
    can be migrated to the appropriate node in time which is transparent to users,
    ensuring service availability in the case of node failure or network congestion,
    (3) container-based cloud scheduler often use multi-replicas operation mode to
    realize load balancing to ensure the high available of business. Due to the short
    development time and the low technology maturity of container cloud [4], application
    containers face more complex bottlenecks in the large-scale clusters. Therefore,
    how to establish an effective task optimization scheduling and feedback evaluation
    mechanism to ensure quality of service and improve resource utilization is one
    of the hotspots in current research. Faced with the traditional task scheduling
    and resource management problems, researchers have designed many sophisticated
    models or algorithms, such as some dynamic task scheduling framework [5,6,7],
    heuristic scheduling algorithms [15,16,17,18,19,20], meta-heuristic scheduling
    algorithms [21,22,23,24,25,26,27,28], etc. On the one hand, these methods generally
    complete static scheduling actions from the perspective of inherent resource usage,
    lack of feedback adjustment of the running environment, and easily fall into local
    optimal, unable to achieve optimal convergence. On the other hand, because the
    parameters of these models are difficult to adjust adaptively, they are challenged
    by the uncertainty and complexity of the environment in the data center. In recent
    years, with the success of Alpha Go [8], Reinforcement learning (RL) [9] has become
    a hot research branch in machine learning. RL is used to describe and solve problems
    in which agents learn policies to maximize rewards or achieve specific goals in
    the process of interacting with the environment. RL, especially deep reinforcement
    learning (DRL) [10], has strong environmental adaptive ability and can adapt to
    container-based cloud environment with high randomness. Although the existing
    reinforcement learning based scheduling algorithms [30,31,32,33,34,35] to some
    extent alleviate the traditional task scheduling algorithms are easy to fall into
    local optimum, weak environmental adaptability, and other problems. However, due
    to the need for a lot of empirical knowledge to explore the optimal strategy,
    reinforcement learning based algorithms still have the problems of long convergence
    time, low convergence performance and high learning cost, which are difficult
    to meet the changing task scheduling requirements in container cloud environment.
    In this paper, a heuristic multi-objective task 1 scheduling framework based on
    Actor-Critic reinforcement learning is proposed for the optimization of task scheduling
    and resource allocation in the container-based cloud. The main contributions are
    as follows: Based on the theory of deep reinforcement learning, we define cloud
    environment, scheduling agent, scheduling actions and scheduling evaluation methods,
    and establish a learning model for multi-objective task scheduling in the container-based
    cloud. Based on Actor-Critic algorithm, we design and implement a heuristic multi-objective
    task scheduling framework and its algorithm (AC-CCTS). Firstly, we add rule-based
    heuristic probability function to evaluate scheduling ability of nodes and help
    agent to make better decisions when the scheduling experience is less, so as to
    reduce the cost of trial-and-error learning in the early stage of the agent. Then,
    we use prioritized experience replay of samples to update the high-priority states
    first to accelerate scheduling convergence. Finally, we use a method of scheduling
    compensation to conduct repetitive scheduling for high-load tasks to balance node
    resource. From an engineering perspective, we combined with the Kubernetes which
    a container-oriented resource orchestration tool to build an experimental platform.
    Compared with traditional meta-heuristic scheduling and current state-of-the-art
    reinforcement learning scheduling methods, AC-CCTS shows better resource utilization
    efficiency and convergence stability in container-based cloud task scheduling.
    The rest of the current paper is organized as follows: Sect. 2 summarizes some
    previous studies closely related to our work. Our proposed methodologies named
    AC-CCTS are presented and described detailed in Sect. 3 including the problem
    formulation, building framework, and algorithm design. Experimental evaluation
    and result analysis are carried out in Sect. 4. Finally, conclusions and some
    prospects are given in Sect. 5. 2 Related work Task scheduling optimization for
    container-based cloud plays an important role in resource optimization of cloud
    environment. Reasonable allocation of various resources such as storage, computing,
    and network bandwidth is of great significance to the improvement of resource
    utilization and quality of service. Because of its strong academic value and economic
    benefits, more and more research achievements have been made on the related technologies
    of container cloud scheduling. In this paper, we classify these researches into
    model-based, meta-heuristic based, and reinforcement learning based task scheduling.
    The relevant overview is given below. Model-based task scheduling, using mathematical
    modeling methods to solve optimization problems of task scheduling. Ref. [11]
    designed a linear scheduling model based on dynamic scheduling and batch processing
    for tasks with different resource requirements, constraints, and life cycles in
    private cloud environments, which improves resource allocation efficiency. In
    order to improve the flexibility of resource expansion for container cloud applications,
    two-step scheduling model was proposed in Ref. [12]. In the first step, reinforcement
    learning method was used to control the horizontal and vertical elasticity of
    container. In the second step, integer linear programming model was solved or
    network aware heuristic algorithm was used to solve the container placement problem.
    Ref. [13] established a multi-criteria node selection model for the singleness
    problem of Kubernetes scheduling criteria, considering multiple criteria including
    node CPU utilization, node memory utilization, number of node containers, and
    node image transmission time. Using TOPSIS method determines the priority of nodes
    and realizes the optimal node scheduling of the container. In addition, Ref. [14]
    uses the Analytic Hierarchy Process (AHP) to solve the prioritization problem
    of user requests in workflow task scheduling. Model-based task scheduling has
    no complicated optimization process and is simple and easy to understand. However,
    a pure model is often difficult to describe all the characteristics of a container-based
    cloud environment with high randomness, so there are problems of low scheduling
    accuracy and environmental adaptability. Heuristic-based task scheduling mainly
    focuses on finding the optimal or near optimal policy of the problem. These methods
    rely on the scheduling problem itself and use the characteristics of problem to
    find the optimal solution in search space, such as greedy scheduling [15] [16],
    Round Robin (RR) scheduling [17], Min–Max/Min-Min scheduling [18][18], Heterogeneous
    Earliest Finish Time (HEFT) scheduling [20], etc. However, meta-heuristic based
    task scheduling mainly uses random search methods that simulate natural biological
    evolution or group social behavior (such as genetic algorithm, artificial bee
    algorithm, etc.) to solve large-scale task scheduling problems. Aiming at the
    task scheduling problem in heterogeneous distributed systems. Ref. [21] proposed
    a multi-objective weighted genetic algorithm, which is based on multiple metrics
    including waiting time, utility, and throughput to achieve integrated optimization.
    Ref. [22] used artificial bee colony algorithm to study the scheduling problem
    of container cloud micro services, and proposed a component-dependent scheduling
    model. The mutation operator is introduced to enhance the global optimization
    ability, and the mutation probability is set to a dynamic value varying with the
    number of iterations to accelerate convergence. To solve problem of multi-objective
    balanced scheduling, Ref. [23] proposed a hybrid firefly with improved multi-objective
    PSO algorithm, which offers effective load balancing, resource usage and response
    time. Based on the particle swarm optimization algorithm and the multi-objective
    optimization model including resource utilization, resource imbalance and service
    access delay, Ref. [24] achieved the comprehensive optimization of cluster’s resources
    and service’s performance. Ref. [25] proposed a novel hybrid whale optimization-based
    algorithm to solve the multi-objective task scheduling problem in cloud computing
    environment. Cloudsim experiments show that the algorithm has a great improvement
    in many metrics of task scheduling, such as resource utilization, load balance,
    execution time, etc. In addition, based on grey wolf optimization and other meta-heuristic
    algorithms, Ref. [26,27,28] realizes the optimization of task completion time,
    cluster resource utilization and other goals. Heuristic and meta-heuristic intelligent
    algorithm can solve the NP-complete problems [29] by making effective use of the
    characteristics of the problem or imitating some group behaviors in nature. It
    is particularly outstanding in exploring the optimal solution and adapting ability
    in the process of algorithm running, but its disadvantage is that it takes too
    long and the optimization process is cumbersome. Reinforcement learning based
    task scheduling, by establishing a scheduling process in line with the Markov
    decision process (MDP), which accumulates scheduling experience and repeatedly
    strengthens scheduling strategy by means of scheduling execution and evaluation
    feedback, realizes positive feedback scheduling for optimizing various objectives.
    Through simulating the task unloading process in heterogeneous edge scenarios,
    in Ref. [30], an asynchronous Actor-Critic algorithm (A3C) is used as an unloading
    method to optimize edge nodes’ workload and reduce energy consumption and time
    overhead. By modeling the resource environmental state, task scheduling action
    and reward, DeepRM proposed in Ref. [31] transforms the task scheduling problem
    with multiple dependencies into a reinforcement learning problem for solving.
    It has faster resource convergence speed and can adapt to different task scheduling
    environments. Ref. [32] introduced imitation learning on the basis of Ref. [31],
    which improves the efficiency of reinforcement learning and the performance of
    multi resource scheduling. Ref. [33] established a new job scheduling framework
    based on dual Actor-Critic deep reinforcement learning, which combines job scheduling
    with resource utilization optimization, and controls the execution timing of task
    scheduling through two time-slots. For the container adaptive scheduling problem
    of cloud data center, Ref. [34] proposed RLSched algorithm. RLSched relies on
    an enhanced network model with action shaping, which filters invalid actions and
    prevents the agent to fall into a sub-optimal policy. Experiments show that RLSched
    can adapt to large-scale container task scheduling scenarios. So as to optimize
    resource utilization and task completion time. In addition, Ref. [35] combined
    SASA reinforcement learning and genetic algorithm to manage cloud resources. The
    method explores scheduling workflow tasks through SASA reinforcement learning,
    and optimizes the deadline of tasks and resource utilization of nodes through
    fitness function of the genetic algorithm. Alternatively, task scheduling based
    on reinforcement learning strengthens strategy or value through continuous scheduling
    and feedback, uses experience knowledge to guide scheduling, and can adapt to
    environmental changes. In particular, the scheduling method based on deep reinforcement
    learning is suitable for the optimization of large-scale task scheduling under
    the uncertain environment through its ability of the state identification and
    representation. However, the early scheduling schemes based on the reinforcement
    learning mostly adopted direct learning method and evaluated scheduling results
    through environmental sampling. As this pure reinforcement learning requires lots
    of empirical knowledge to explore the optimal strategy, it has the problem of
    long learning convergence time. In summary, the mathematical model-based scheduling
    method has better convergence rate and resource efficiency, but poor environmental
    adaptability. Heuristic and meta-heuristic based scheduling method has better
    accuracy, but time-consuming. Reinforcement learning based scheduling method has
    better environmental adaptability, but large learning cost. To facilitate comparison,
    we summarize the main work related to task scheduling in Table 1. Aiming at the
    characteristics of randomness and complexity of container-based cloud, we innovatively
    propose a dynamic multi-objective task scheduling framework. Compared with the
    existing multi-objective optimization methods, the proposed framework has some
    advantages in terms of timeliness, environmental adaptability, and service performance.
    The proposed method makes full use of the diverse workload characterization with
    deep network learning, the environmental interaction with reinforcement learning
    and the directional exploration ability with heuristic method, effectively accelerate
    the convergence of task scheduling, meanwhile, improve the resource utilization
    efficiency and quality of service of the container cloud. Table 1 Comparison of
    main related works about task scheduling Full size table 3 Proposed methodology
    This section provides a detailed introduction to the proposed dynamic multi-objective
    task scheduling framework. First, we model nodes and tasks to describe the dynamic
    scheduling problem. Then, we build the deep reinforcement learning based dynamic
    scheduling framework by modeling the container-based cloud environment, scheduling
    agent, scheduling actions and scheduling evaluation methods. Finally, we construct
    system optimization objective, design heuristic probability function, state priority
    function and method of scheduling compensation, and put forward the heuristic
    multi-objective task scheduling algorithm. 3.1 Problem formulation As aforementioned,
    the uncertainty of cloud environment and variability of application requirements
    increase the scheduling cost of tasks in container cloud. Due to its flexible
    scalability, multi-replicas operation, and fast migration, containerized tasks
    are prone to resource imbalance and performance degradation dramatically during
    service running. Most of the existing related methods tend to devise the cluster
    scheduling from the perspective of the balance between task and resource, such
    as cluster energy consumption [30], resource allocation [11, 12] and utilization
    [13, 31, 32, 34]. Differ from the existing algorithms, the environmental feedback
    and domain experience are taken into consideration to address the scheduling timeliness
    and environmental adaptability, which are neglected in most of the previous scheduling
    algorithms for container-based cloud architectures. In the container, cloud task
    scheduling scenario with J nodes and I tasks, there are JI scheduling schemes
    in total, and selecting the optimal scheme is a NP-hard problem recognized by
    the industry. To simplify, we transform the container cloud task scheduling problem
    into a multi-objective combinatorial optimization problem, establish a multi-objective
    formula shown in (1), and carry out combinatorial optimization from resource utilization
    rate (RUR), resource balance degree (RBD) and quality of service (QoS) to achieve
    optimal scheduling, as in (1). $${\\rm Max}\\sum\\limits_{j = 1}^{J} {{\\rm RUR}\\left(
    {j,t} \\right)} \\& {\\rm Max}\\sum\\limits_{j = 1}^{J} {{\\rm RBD}\\left( {j,t}
    \\right)} \\& {\\rm Max}\\sum\\limits_{i = 1}^{I} {{\\rm QoS}\\left( {i,t} \\right)}$$
    (1) $${\\rm RUR}\\left( {j,t} \\right) = \\frac{1}{K}\\sum\\limits_{j = 1}^{J}
    {U_{j} \\left( {k,t} \\right)}$$ (2) $${\\rm RBD}\\left( {j,t} \\right) = 1 -
    \\sqrt {{{\\sum\\limits_{k = 1}^{K} {\\left( {U_{j} \\left( {k,t} \\right) - {\\rm
    RUR}\\left( {j,t} \\right)} \\right)^{2} } } \\mathord{\\left/ {\\vphantom {{\\sum\\limits_{k
    = 1}^{K} {\\left( {U_{j} \\left( {k,t} \\right) - {\\rm RUR}\\left( {j,t} \\right)}
    \\right)^{2} } } K}} \\right. \\kern-0pt} K}}$$ (3) $${\\rm QoS}\\left( {i,t}
    \\right) = {{{\\rm RT}_{C} \\left( {i,t} \\right)} \\mathord{\\left/ {\\vphantom
    {{{\\rm RT}_{C} \\left( {i,t} \\right)} {{\\rm RT}\\left( {i,t} \\right)}}} \\right.
    \\kern-0pt} {RT\\left( {i,t} \\right)}}$$ (4) $$U_{j} \\left( {k,t} \\right) =
    {{\\left( {Q_{j} \\left( k \\right) - R_{j} \\left( {k,t} \\right)} \\right)}
    \\mathord{\\left/ {\\vphantom {{\\left( {Q_{j} \\left( k \\right) - R_{j} \\left(
    {k,t} \\right)} \\right)} {Q_{j} \\left( k \\right)}}} \\right. \\kern-0pt} {Q_{j}
    \\left( k \\right)}}$$ (5) $${\\rm RT}_{C} \\left( {i,t} \\right) = \\left\\{
    \\begin{gathered} {\\rm RT}_{0} ,\\begin{array}{*{20}c} {} \\\\ \\end{array} \\begin{array}{*{20}c}
    {} \\\\ \\end{array} {\\rm RT}\\left( {i,t} \\right) \\ge {\\rm RT}_{0} \\hfill
    \\\\ {\\rm RT}\\left( {i,t} \\right),\\begin{array}{*{20}c} {} \\\\ \\end{array}
    \\begin{array}{*{20}c} {} \\\\ \\end{array} {\\rm RT}\\left( {i,t} \\right) <
    {\\rm RT}_{0} \\hfill \\\\ \\end{gathered} \\right.$$ (6) $$O_{i} \\left( {k,t}
    \\right) \\le R_{j} \\left( {k,t} \\right),\\forall k \\in \\left\\{ {1,2,...,K}
    \\right\\}$$ (7) $$\\delta \\le QoS\\left( {i,t} \\right) \\le 1,\\begin{array}{*{20}c}
    {} \\\\ \\end{array} 0 < \\delta < 1$$ (8) In Formula (1), RUR (j, t) and RBD
    (j, t) are the resource utilization rate and resource balance degree of node j(1 ≤ j ≤ J)
    at time t respectively, and QoS(i, t) is the quality of service for service i(1 ≤ i ≤ I)
    at time t. In the expanded Formulas (2) to (4), Uj(k, t) is the utilization of
    resource k(1 ≤ k ≤ K) on node j at time t, which is defined as the ratio of resource
    usage to resource quota, as in (5). Where Qj(k) and Rj(k, t) represent resource
    quota and remaining amount, respectively. In addition, RTC(i, t) and RT(i, t)
    are compliance response time and 95-line response time of service i under concurrent
    requests at time t, respectively. We define that the 95-line response time does
    not exceed the threshold RT0 as compliance, as in (6). In addition, Formulas (7)
    and (8) are scheduling constraints, which restrict the remaining amount Rj(k,
    t) of resource k on scheduling node j to be no less than the demand Oi(k, t) for
    resource k of scheduling service i, and the quality of service QoS(i, t) is no
    less than the threshold δ. In order to obtain the non-dominated solution for multi-objective
    task scheduling in container-based cloud environment with large randomness to
    achieve optimal scheduling. Firstly, the proposed framework makes full use of
    the workload characterization with deep learning and the reward feedback of scheduling
    action with reinforcement learning to improve the adaptability of scheduling agent
    to the random cloud environment. Secondly, the proposed framework integrates the
    domain knowledge based heuristic strategy and the priority-aware experience replay
    strategy to optimize the learning efficiency. Specifically, we added heuristics
    to the action selection which help the agent make decision. This can alleviate
    inefficient learning caused by the less experience of large-scale load scheduling
    at the beginning of cluster service. On the other hand, we redesigned the priority
    experience replay of the sample to accelerate the global convergence of task scheduling
    by updating the high-priority state first. Finally, we present scheduling compensation
    for the dynamic characteristics of container cloud, including resource monitoring,
    analysis and task migration. The method can balance the current state of cluster
    by task migration scheduling according to the dynamic changes of the environment
    in time. 3.2 Framework of heuristic multi-objective task scheduling The task dynamic
    scheduling framework is shown in Fig. 1. It is composed of five parts: task queue
    (TQ), agent, scheduling actuator (SA), container-based cloud environment (CCE)
    and environment monitor (EM). Among them, the task queue is used to cache tasks
    to be scheduled according to the priority of task submission, and supports the
    scheduling agent to batch pull tasks for scheduling. The tasks in the task queue
    come from two aspects. One is the initial scheduling tasks submitted by the user,
    and the other is the repetitive scheduling tasks submitted by the environment
    monitor. The scheduling agent is the core of the whole scheduling framework, which
    is used to construct the optimal scheduling scheme and control the execution of
    scheduling actions. The scheduling agent uses a deep reinforcement learning model
    based on Actor-Critic, adds a heuristic probability function to guide scheduling,
    and uses a sample based prioritized experience replay method to accelerate learning.
    The function of scheduling executor is to execute scheduling actions. According
    to the scheduling scheme submitted by the scheduling agent, the corresponding
    scheduling actions are selected and executed from the action space. The execution
    result of scheduling action is to pull target task’s container to the optimal
    scheduling node. The container-based cloud environment is composed of multiple
    physical nodes under multi-center, and basic components required by container
    runtime are configured, such as docker, image registry, etc. environment monitor
    includes two parts: scheduling evaluation and environment state monitor. Among
    them, scheduling evaluation realizes the evaluation of scheduling action in the
    form of reward. Environment state monitoring realizes collection of environmental
    characteristic metrics and analysis of repetitive scheduling tasks. Fig. 1 Overview
    of heuristic multi-objective task scheduling framework Full size image The running
    process of the scheduling framework AC-CCTS consists of the following six steps.
    (a) Build or update task queue and cache tasks to be scheduled according to the
    priority order of task submission. (b) The scheduling agent pulls scheduling tasks
    from task queue in batches and constructs scheduling scheme (scheduling actions)
    according to current environmental state. (c) The scheduling actuator executes
    scheduling scheme. (d) The environmental monitor collects and evaluates scheduling
    actions according to the reward rule and feeds it back to scheduling agent. (e)
    The scheduling agent performs iterative optimization of Actor-Critic network parameters
    based on evaluation feedback. (f) The environment state monitor monitors the changes
    in the environment state, performs analysis of repetitive scheduling tasks, and
    feeds back to the task queue, and then proceeds to step (a). As aforementioned,
    through the mutual assistance of forward scheduling and compensation mechanism,
    the proposed framework completes the closed-loop scheduling, and ensures the accuracy
    of task scheduling and resource utilization to some extent. 3.2.1 State space
    Since the environment state at the current scheduling time in the container-based
    cloud is only related to the previous ones, the transitions of states conform
    to the Markov property. We usually describe the task scheduling process of container-based
    cloud as a Markov Decision Process (MDP). Suppose the cloud environment cluster
    is composed of J nodes, and each node has K different types of resources. We model
    the state of cloud environment as a feature vector composed of cluster state,
    node state and task state, as shown in Formula (9), and related feature items
    are shown in Table 2. $${\\rm ST}\\left( t \\right) = \\left( {{\\varvec{Q}}^{c}
    \\left( t \\right),{\\varvec{Q}}^{d} \\left( t \\right),{\\varvec{Q}}^{s} \\left(
    t \\right)} \\right)^{T}$$ (9) Table 2 Characteristics of container-based cloud
    environment Full size table Among them, Qc(t), Qd(t), Qs(t), respectively represent
    the feature vector of cluster state, node state and task state. As shown in Table
    2, Qc(t) is composed of three types of characteristics: cluster resource utilization,
    mean of cluster resource utilization and standard deviation of cluster resource
    utilization, as shown in Formula (10). Qd(t) is composed of four types of characteristics:
    node resource utilization, mean and standard deviation of node resource utilization,
    and task occupancy rate of node, as shown in Formula (11). Qs(t) is composed of
    two types of characteristics: resource request rate and dependent occupancy rate
    of task, as shown in Formula (12). $${\\varvec{Q}}^{c} \\left( t \\right) = \\left(
    {{\\varvec{U}}^{c} \\left( t \\right),\\overline{{U^{c} }} \\left( t \\right),V^{c}
    \\left( t \\right)} \\right)^{T}$$ (10) $${\\varvec{Q}}^{d} \\left( t \\right)
    = \\left( {{\\varvec{U}}^{d} \\left( t \\right),\\overline{{{\\varvec{U}}^{d}
    }} \\left( t \\right),{\\varvec{V}}^{d} \\left( t \\right),{\\varvec{S}}^{d} \\left(
    t \\right)} \\right)^{T}$$ (11) $${\\varvec{Q}}^{s} \\left( t \\right) = \\left(
    {{\\varvec{R}}^{s} \\left( t \\right),D^{s} \\left( t \\right)} \\right)$$ (12)
    In Formula (10), the vector of cluster resource utilization is composed of the
    utilization rate of all kinds of resources in the cluster, with a total of K characteristics,
    as shown in Formula (13). Where Ukc(t) is the utilization rate of the k-th resource
    of cluster at time t, \\(\\overline{{U^{c} }} \\left( t \\right)\\) is the average
    utilization rate of all kinds of cluster resources with one feature, as shown
    in Formula (14), and V c(t) is the standard deviation of the utilization rate
    of all kinds of cluster resources with one feature, as shown in Formula (15).
    $${\\varvec{U}}^{c} \\left( t \\right) = \\left( {U_{1}^{c} \\left( t \\right),U_{2}^{c}
    \\left( t \\right), \\cdot \\cdot \\cdot ,U_{k}^{c} \\left( t \\right) \\cdot
    \\cdot \\cdot ,U_{K}^{c} \\left( t \\right)} \\right)^{T}$$ (13) $$\\overline{{U^{c}
    }} \\left( t \\right) = {{\\sum\\limits_{j = 1}^{J} {U_{k}^{c} \\left( t \\right)}
    } \\mathord{\\left/ {\\vphantom {{\\sum\\limits_{j = 1}^{J} {U_{k}^{c} \\left(
    t \\right)} } J}} \\right. \\kern-0pt} J}$$ (14) $$V^{c} \\left( t \\right) =
    \\sqrt {{{\\sum\\limits_{k = 1}^{K} {\\left( {U_{k}^{c} \\left( t \\right) - \\overline{{U^{c}
    }} \\left( t \\right)} \\right)^{2} } } \\mathord{\\left/ {\\vphantom {{\\sum\\limits_{k
    = 1}^{K} {\\left( {U_{k}^{c} \\left( t \\right) - \\overline{{U^{c} }} \\left(
    t \\right)} \\right)^{2} } } {\\left( {K - 1} \\right)}}} \\right. \\kern-0pt}
    {\\left( {K - 1} \\right)}}}$$ (15) In Formula (11), the vector of node resource
    utilization is composed of all kinds of resource utilization on the node, with
    a total of J × K features, as shown in Formula (16). Where, \\(U_{j,k}^{d} \\left(
    t \\right)\\) represents the utilization rate of resource k on node nj at time
    t. \\(\\overline{{{\\varvec{U}}^{d} }} \\left( t \\right)\\) is a mean vector
    composed of the utilization rate of various resources of nodes, with a total of
    J features, as shown in Formula (17). Where, \\(\\overline{{U_{j}^{d} }} \\left(
    t \\right)\\) represents the average utilization rate of all kinds of resources
    on node nj at time t. \\(V^{d} \\left( t \\right)\\) is a standard deviation vector
    of utilization rate of various node resources, with a total of J features, as
    shown in Formula (18). Here, \\(V_{j}^{d} \\left( t \\right)\\) represents the
    standard deviation of the utilization rate of various resources on the node nj
    at time t. \\({\\varvec{S}}^{d} \\left( t \\right)\\) is a vector composed of
    task occupancy rate of nodes, with a total of J features, as shown in Formula
    (19). Where \\(S_{j}^{d} \\left( t \\right)\\) represents the occupancy rate of
    tasks on node nj at time t, that is, the rate of tasks’ amounts on node nj to
    the total amounts of cluster tasks. $${\\varvec{U}}^{d} \\left( t \\right) = \\left(
    {U_{1,1}^{d} \\left( t \\right),U_{1,2}^{d} \\left( t \\right), \\cdot \\cdot
    \\cdot ,U_{j,k}^{d} \\left( t \\right) \\cdot \\cdot \\cdot ,U_{J,K}^{d} \\left(
    t \\right)} \\right)^{T}$$ (16) $$\\overline{{{\\varvec{U}}^{d} }} \\left( t \\right)
    = \\left( {\\overline{{U_{1}^{d} }} \\left( t \\right),\\overline{{U_{2}^{d} }}
    \\left( t \\right), \\cdot \\cdot \\cdot ,\\overline{{U_{j}^{d} }} \\left( t \\right),
    \\cdot \\cdot \\cdot ,\\overline{{U_{J}^{d} }} \\left( t \\right)} \\right)^{T}$$
    (17) $${\\varvec{V}}^{d} \\left( t \\right) = \\left( {V_{1}^{d} \\left( t \\right),V_{2}^{d}
    \\left( t \\right), \\cdot \\cdot \\cdot ,V_{j}^{d} \\left( t \\right), \\cdot
    \\cdot \\cdot ,V_{J}^{d} \\left( t \\right)} \\right)^{T}$$ (18) $${\\varvec{S}}^{d}
    \\left( t \\right) = \\left( {S_{1}^{d} \\left( t \\right),S_{2}^{d} \\left( t
    \\right), \\cdot \\cdot \\cdot ,S_{j}^{d} \\left( t \\right) \\cdot \\cdot \\cdot
    ,S_{J}^{d} \\left( t \\right)} \\right)^{T}$$ (19) In Formula (12), the vector
    of task resource demand rate \\({\\varvec{R}}^{s} \\left( t \\right)\\) is composed
    of the demand rate of tasks for various resources, with K features in total, as
    shown in Formula (20). Where \\(R_{i,k}^{s} \\left( t \\right)\\) is the rate
    between the requirement of task si for resource k and the remaining amounts of
    corresponding resources in the cluster. The dependent occupancy rate \\(D^{s}
    \\left( t \\right)\\) represents the rate of dependent tasks of si to total corresponding
    tasks in the cluster, as shown in Formula (21). Where, \\(N_{i}^{s} \\left( t
    \\right)\\) is the number of dependent tasks required by si, and \\(N_{i,j}^{s}
    \\left( t \\right)\\) is the number of existing dependent task instances on node
    nj at time t, J is the number of cluster nodes. $${\\varvec{R}}^{s} \\left( t
    \\right) = \\left( {R_{i,1}^{s} \\left( t \\right),R_{i,2}^{s} \\left( t \\right),
    \\cdot \\cdot \\cdot ,R_{i,k}^{s} \\left( t \\right) \\cdot \\cdot \\cdot ,R_{i,K}^{s}
    \\left( t \\right)} \\right)^{T}$$ (20) $$D^{s} \\left( t \\right) = {{N_{i}^{s}
    \\left( t \\right)} \\mathord{\\left/ {\\vphantom {{N_{i}^{s} \\left( t \\right)}
    {\\sum\\limits_{j = 1}^{J} {N_{i,j}^{s} \\left( t \\right)} }}} \\right. \\kern-0pt}
    {\\sum\\limits_{j = 1}^{J} {N_{i,j}^{s} \\left( t \\right)} }}$$ (21) 3.2.2 Action
    space Scheduling actions are the mapping relationship between tasks and nodes.
    Reinforcement learning emphasizes the continuous interaction between agent and
    environment. In the process of interaction, the agent chooses actions according
    to current environmental state, and enhances the probability of action through
    environmental reward. For a container-based cloud environment with J nodes, the
    task scheduling action can be represented by Formula (22), indicating that the
    tasks can be scheduled to any node in the cluster. $$A = \\left\\{ {\\left. a
    \\right|a \\in \\left\\{ {1,2, \\cdot \\cdot \\cdot ,J} \\right\\}} \\right\\}$$
    (22) Different from traditional task scheduling, container-based cloud tasks can
    be scheduled to multiple nodes in the form of muti-replicas at the same time,
    so the action matrix of container-based cloud scheduling problem can usually be
    expressed as a dense I × J matrix, as shown in Formula (23). Where the matrix
    element value Ii,j is 1 or 0,1 shows that task si is bound to node nj, and vice
    versa. $$AC = \\left[ {\\begin{array}{*{20}c} {I_{0,0} } & {I_{0,1} } & { \\cdot
    \\cdot \\cdot } & {I_{0,J - 1} } \\\\ {I_{1,0} } & {I_{1,1} } & { \\cdot \\cdot
    \\cdot } & {I_{1,J - 1} } \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ {I_{I
    - 1,0} } & {I_{I - 1,1} } & { \\cdot \\cdot \\cdot } & {I_{I - 1,J - 1} } \\\\
    \\end{array} } \\right]$$ (23) 3.2.3 Reward The objective of scheduling optimization
    for container-based cloud task in this paper is to improve the balance and utilization
    rate of resources, meanwhile, avoid the stability problem of running nodes and
    tasks caused by resource usage tilt, and improving quality of service. Therefore,
    we established an immediate reward for scheduling action as shown in Formula (24)
    from the perspective of multi-objective optimization. $$R\\left( t \\right) =
    \\alpha \\cdot {\\rm ERBD}\\left( t \\right) + \\left( {1 - \\alpha } \\right)
    \\cdot {\\rm ERUR}\\left( t \\right)$$ (24) $${\\rm ERBD}\\left( t \\right) =
    \\frac{1}{J}\\left( {\\sum\\limits_{j = 1}^{J} {{\\rm RBD}\\left( {j,t} \\right)}
    - \\sum\\limits_{j = 1}^{J} {{\\rm RBD}\\left( {j,t - 1} \\right)} } \\right)$$
    (25) $${\\rm ERUD}\\left( t \\right) = \\frac{1}{J}\\left( {\\sum\\limits_{j =
    1}^{J} {{\\rm RUR}\\left( {j,t} \\right)} - \\sum\\limits_{j = 1}^{J} {{\\rm RUR}\\left(
    {j,t - 1} \\right)} } \\right)$$ (26) where ERBD (t) and ERUR (t) represent the
    increment of resource balance degree and resource utilization rate of the cluster
    caused by the previous and subsequent scheduling, respectively, \\(\\alpha\\)
    is coordination coefficient. \\(RUR\\left( {j,t} \\right)\\) and \\(RBD\\left(
    {j,t} \\right)\\) are defined as in Formulas (2) and (3). We hope to achieve multi-objective
    optimization of resource utilization, resource balance and QoS through reasonable
    task scheduling, but there may be some contradictions between them. For example,
    an increase in resource utilization may lead to a decrease in QoS. It is worth
    noting that we did not directly add the objective of QoS to the reward formula,
    but established a scheduling compensation mechanism to monitor service state and
    reschedule tasks with non-compliant QoS. The advantage of this is that we can
    maximize the resource balance and resource utilization while ensuring the quality
    of service. Experiments show that our method is effective and can find non-dominated
    solutions to the problem in finite time. Furthermore, we not only consider the
    immediate reward under the current scheduling, but also consider the impact of
    subsequent scheduling on the entire resource environment. Therefore, we use a
    cumulative reward shown in Formula (27) to evaluate scheduling actions. Where
    \\(\\gamma\\) is discount rate, which is used to control the contribution rate
    of subsequent rewards. \\(\\xi\\) is the reward threshold, which is used to control
    the scope of rewards. $$G\\left( t \\right) = R\\left( t \\right) + \\gamma R\\left(
    {t + 1} \\right) + \\gamma^{2} R\\left( {t + 2} \\right) + \\cdot \\cdot \\cdot$$
    (27) $$s.t.\\begin{array}{*{20}c} {} \\\\ \\end{array} 0 < \\gamma < 1,\\begin{array}{*{20}c}
    {} \\\\ \\end{array} - \\xi \\le G\\left( t \\right) \\le \\xi$$ (28) 3.3 Dynamic
    task scheduling algorithm As mentioned above, traditional scheduling algorithms
    such as greedy algorithm and heuristic algorithm have the problems of scheduling
    performance, which is difficult to meet the requirements of resource efficiency
    and QoS in large-scale task scheduling scenarios. However, the existing reinforcement
    learning algorithms need large amount of experience to explore the optimal strategy,
    which leads to the bottleneck of learning efficiency. Therefore, aiming at the
    problems of traditional reinforcement learning in task scheduling, we improve
    the method of container cloud scheduling from three aspects: firstly, we add heuristic
    method based on container cloud scheduling rules, which evaluates the scheduling
    ability of each node in advance by scheduling rules to assist agent to make scheduling
    decisions, thus reducing the cost of trial-and-error of agent. Then, the method
    of prioritized experience replay is added to speed up agent learning, and the
    method is used to focus on learning important experience to avoid meaningless
    and irrelevant state updates and improve scheduling performance. Finally, we integrate
    the scheduling compensation mechanism to realize the closed-loop scheduling of
    tasks and reduce the impact of uncertain factors in the cloud environment on the
    container-based cluster. 3.3.1 Heuristic probability functions for container cloud
    Traditional reinforcement learning algorithms need large amount of experience
    to explore the optimal scheduling strategy, but when faced a new environment,
    agent can only accumulate experience through continuous trial-and-error learning.
    This cost of trial-and-error is unacceptable in some task scheduling scenarios.
    Therefore, we design heuristic strategy, aiming at using scheduling rules to inspire
    agent to conduct directional exploratory learning, rather than blind trial-and-error
    learning. In Actor-Critic framework, action sample at is obtained by sampling
    through action probability distribution of Actor network. As shown in Formular
    (29). $$a_{t} \\sim \\pi \\left( {\\left. a \\right|s_{t} ;\\theta_{t} } \\right)$$
    (29) Among them, st is environmental state at time t, and θt is parameter of neural
    network. Obviously, at the beginning of the scheduling, the inaccuracy of network
    parameters leads to large randomness of scheduling. We use scheduling rules to
    design a heuristic probability function H(a|st), which aims to evaluate the scheduling
    ability of nodes, and assist agent in making scheduling decisions. As shown in
    Formular (30). $$a_{t} \\sim \\left( {1 - \\delta_{t} } \\right)\\pi \\left( {\\left.
    a \\right|s_{t} ;\\theta_{t} } \\right) + \\delta_{t} \\cdot H\\left( {\\left.
    a \\right|s_{t} } \\right)$$ (30) Among them, \\(\\delta_{t} \\in \\left[ {0,1}
    \\right]\\) is the impact coefficient, which is used to balance scheduling decisions
    of heuristic method and reinforcement learning. H(a|st) is based on “resource
    remaining priority principle” and “load balance priority principle”. As shown
    in Formula (31). $$H\\left( {\\left. a \\right|s_{t} } \\right) = \\mu \\cdot
    {\\rm LBD}\\left( {n_{j} ,t} \\right) + \\left( {1 - \\mu } \\right) \\cdot {\\rm
    RRD}\\left( {n_{j} ,t} \\right)$$ (31) $${\\rm LBD}\\left( {n_{j} ,t} \\right)
    = 1 - {{\\overline{{V_{j}^{d} }} \\left( t \\right)} \\mathord{\\left/ {\\vphantom
    {{\\overline{{V_{j}^{d} }} \\left( t \\right)} {\\sum\\limits_{j = 1}^{J} {\\overline{{V_{j}^{d}
    }} \\left( t \\right)} }}} \\right. \\kern-0pt} {\\sum\\limits_{j = 1}^{J} {\\overline{{V_{j}^{d}
    }} \\left( t \\right)} }}$$ (32) $${\\rm RRD}\\left( {n_{j} ,t} \\right) = 1 -
    {{\\overline{{U_{j}^{d} }} \\left( t \\right)} \\mathord{\\left/ {\\vphantom {{\\overline{{U_{j}^{d}
    }} \\left( t \\right)} {\\sum\\limits_{j = 1}^{J} {\\overline{{U_{j}^{d} }} \\left(
    t \\right)} }}} \\right. \\kern-0pt} {\\sum\\limits_{j = 1}^{J} {\\overline{{U_{j}^{d}
    }} \\left( t \\right)} }}$$ (33) Among them, LBD and RRD are load balance degree
    and resource remaining degree of nodes, respectively. μ is regulation coefficient,
    which is used to balance LBD and RRD. In addition, in order to realize the heuristics
    learning only perform scheduling coordination when the number of experiences is
    small, and does not affect the scheduling decision of reinforcement learning when
    the number of experiences is large, we have added an impact coefficient \\(\\delta_{t}\\)
    in Formula (30), the definition of \\(\\delta_{t}\\) is shown in Formula (34).
    $$\\delta_{t} = \\left\\{ \\begin{gathered} {{\\left( {N^{th} - N_{t}^{s} } \\right)}
    \\mathord{\\left/ {\\vphantom {{\\left( {N^{th} - N_{t}^{s} } \\right)} {N^{th}
    }}} \\right. \\kern-0pt} {N^{th} }},0 \\le N_{t}^{s} < N^{th} \\hfill \\\\ 0,\\begin{array}{*{20}l}
    {} \\\\ \\end{array} N_{t}^{s} \\ge N^{th} \\begin{array}{*{20}c} {} \\\\ \\end{array}
    {||}\\begin{array}{*{20}l} {} \\\\ \\end{array} N^{th} { = }0 \\hfill \\\\ \\end{gathered}
    \\right.$$ (34) Among them, \\(N_{t}^{s}\\) is the rate of the amount of scheduling
    experience at time t to the total amount of cluster tasks, and \\(0 \\le N^{th}
    \\le 1\\) is heuristic threshold. When \\(0 \\le N_{t}^{s} < N^{th}\\), the value
    of \\(\\delta_{t}\\) is inversely proportional to the number of scheduling experience,
    so as to gradually reduce the intensity of the heuristic method. When \\(N_{t}^{s}
    \\ge N^{th}\\), \\(\\delta_{t} = 0\\) the scheduling decision is completely determined
    by reinforcement learning, and the heuristic method is no longer involved. Since
    heuristics are used for action selection, the Actor-Critic algorithm itself has
    not been modified, so it does not affect the correctness of the algorithm. The
    action sampling with heuristic is shown in Algorithm 1. The algorithm’s inputs
    are environmental state st, the heuristic threshold Nth, the regulation coefficient
    μ, the number of node resource types K, and the node set D. The algorithm’s output
    is scheduling action. In the pseudo code, lines 1 ~ 5 are used to calculate the
    impact coefficient δt. Lines 6 ~ 11 are used to calculate the comprehensive scheduling
    probability of nodes. Line 12 ~ 25 sample scheduling action, and verify whether
    the residual number of resources on pre-scheduling node can meet the resource
    requirements of the scheduling task, if not, re-sample scheduling action. Line
    26 returns the optimal scheduling action. In summary, according to the scheduling
    rules, the heuristic probability function H(a|st) guides agent to make scheduling
    decisions when scheduling experience is small, which can effectively reduce the
    cost of trial-and-error learning and accelerate scheduling convergence. When the
    scheduling experience is accumulated to a certain level, the independent decision-making
    is completely made by reinforcement learning, and its environmental adaptability
    is effectively used to achieve more accurate scheduling. 3.3.2 Prioritized experience
    replay method for container cloud The method of experience replay allows agent
    to recall and reuse previous experience, accelerating learning convergence. However,
    traditional experience replay uses uniform sampling to replay experience from
    the experience pool, ignoring the difference in importance between experiences.
    For this reason, we use a method of prioritized experience replay, which gives
    priority to update the state with higher priority, and eliminate the irrelevant
    or meaningless state updates. In terms of the design of experience priority, on
    the one hand, we think that the greater reward of the action, the greater the
    importance of relevant experience, on the other hand, the larger the TD-error,
    the larger the learning space, and the more helpful it is to make up for the learning
    shortcomings. Therefore, the priority of experience is defined as Formula (35).
    $$P\\left( i \\right) = \\lambda \\cdot G\\left( i \\right) + \\left( {1 - \\lambda
    } \\right) \\cdot \\left| {\\delta \\left( i \\right)} \\right|$$ (35) Among them,
    i represents experience (s, a, r, s''), G(i) is the accumulate reward as shown
    in Formula (26), and δ(i) = Q(i)-G(i) is the TD-error. \\(\\lambda \\left( {0
    \\le \\lambda \\le 1} \\right)\\) is a real variable, used to balance G(i) and
    δ(i). In terms of implementation, we use strategy ε-greedy to balance the exploration
    and utilization of experience. That is, the experience is sampled randomly with
    probability \\(\\varepsilon\\), or the experience with the highest priority is
    selected with probability \\(1 - \\varepsilon\\). The pseudocode of prioritized
    experience replay is shown in Algorithm2. The algorithm’s inputs are memory size
    N, real variable \\(\\lambda\\), etc. And the algorithm’s outputs are experience
    samples. Among the pseudocode, line 1 initializes the experience pool \\(H\\).
    Line 2 stores the latest experience with the highest priority, ensuring that it
    is sampled with a high probability. Line 3 uses strategy \\(\\varepsilon - greedy\\)
    to sample experience. Line 4 updates the experience priority. The proposed prioritized
    experience replay evaluates importance of experience from the perspective of reward
    and TD-error, and samples experience with prioritization to accelerate scheduling
    convergence. 3.3.3 Scheduling compensation mechanism Due to the randomness of
    cloud environment, with the continuous scheduling and access of massive tasks,
    the scheduling algorithm is difficult to ensure the one-time scheduling of tasks
    to meet the diversified needs of users for resource utilization and QoS. For example,
    due to the access of peak traffic, some tasks may experience sharp performance
    degradation or even crash after running for a period. Therefore, it is necessary
    to schedule tasks dynamically to ensure quality of service meet user requirements
    as far as possible. We provide two scheduling compensation mechanisms to support
    dynamic task scheduling, namely “timed environment scanning” and “special event
    response”. Among them, the “timed environment scanning” supports sampling and
    analyzing cluster environmental state at a certain time interval, and dynamically
    migrating some tasks with unsatisfactory quality of service on the high-load nodes
    (we assume that the tasks are stateless), to balance node resources and improve
    task performance. “Special event response” supports timely response to special
    events such as “node online”, “node offline” and “node collapse”. Through dynamic
    migration and adjustment of all or part of tasks on related nodes, the disaster
    tolerance and destruction resistance of the cluster can be improved. “Timed environment
    scanning” analyzes load of nodes and quality of service according to the preset
    period, and improves service performance through dynamic migration of high-load
    tasks. However, this strategy lacks the flexibility to deal with emergencies and
    the response speed of dispatching is slow. “Special event response” refers to
    the timely migration of tasks after the occurrence of emergencies (such as node
    collapse), to ensure the stability of the platform, but this strategy lacks predictability
    and integrity. We adopt a dynamic scheduling strategy based on the dual drive
    of “timed environment scanning” and “special event response”, which ensures the
    emergency response to special events on the one hand, and ensures the platform
    stability and service quality through periodic exploration of the environment
    on the other hand. The task analysis algorithm for dynamic scheduling is shown
    in Algorithm 3. The algorithm’s inputs are node set D, timed scan interval it,
    load threshold th, etc. the algorithm’s output is a set of dynamic scheduling
    tasks. In the pseudocode, line 1 initializes dynamic scheduling task queue DTQ,
    and line 2 initializes and starts the environment scan timer. lines 3–11 analyze
    the dynamic scheduling tasks under the mode of “timed environment scanning”. Among
    them, lines 4 and 5 scan and analyze the load of node resource, and lines 6 to
    8 mark the tasks with poor QoS on nodes whose load exceeds threshold as repetitive
    scheduling tasks and push them into DTQ. Lines 12 to 23 analyze the dynamic tasks
    under the mode of “special event response”. Among them, lines 13 and 14 deal with
    both “node crash” (NCS) and “node offline” (NFL) events. Namely, all tasks on
    the crashed or offline nodes are marked for rescheduling. Lines 15 to 22 are the
    processing of event “node online” (NOL), in which lines 16 to 20 analyze the load
    of node resource, identify high-load tasks on nodes whose load exceeds 0.8th as
    rescheduling tasks, and push them into DTQ. Line 23 supports extended handling
    of other special events. 3.3.4 Implementation of scheduling algorithm The overview
    of the proposed heuristic multi-objective task scheduling algorithm based on Actor-Critic
    (AC) is shown in Fig. 2, which is divided into three stages: 1. State sampling:
    Use Formula (8) to sample the environmental state from the state space to provide
    inputs for the Actor-Critic neural network. 2. Reinforcement learning interaction
    (RL interac-tion): Agent makes scheduling decisions (optimal scheduling action)
    through Algorithm 1 and evaluates scheduling actions through Formula (26). The
    interaction results of reinforcement learning form transition data (s, a, r, s'').
    We use sum-tree structure to construct experience memory to store these transitions,
    and use prioritized experience replay method to support algorithm iteration. 3.
    Model learning: Through scheduling execution and evaluation feedback, the parameters
    of Actor-Critic network were repeatedly adjusted. Where the Critic network is
    trained using Stochastic Gradient Descent (SGD) method, and its parameters are
    updated as in Formula (36). The Actor network is trained using Stochastic Gradient
    Ascent (SGA) method [36], and its parameters are updated as shown in Formula (37).
    $$w_{t + 1} = w_{t} - \\eta_{1} \\cdot \\delta_{j} \\cdot \\nabla_{w} Q\\left(
    {s_{t} ,a_{t} } \\right)$$ (36) $$\\theta_{t + 1} = \\theta_{t} + \\eta_{2} \\cdot
    \\sigma_{j} \\cdot \\nabla_{\\theta } \\pi \\left( {s_{t} ,a_{t} } \\right)$$
    (37) Fig. 2 Overview heuristic multi-objective task scheduling algorithm Full
    size image Among them, \\(w_{t}\\) and \\(\\theta_{t}\\) are parameters of Actor
    and Critic neural network. \\(\\eta_{1}\\), \\(\\eta_{2}\\) are learning rate.
    \\(\\delta_{t} = q_{t} - (r_{t} + \\gamma \\cdot q_{t + 1} )\\),\\(\\sigma_{t}
    = q\\left( {s_{t} ,a_{t} } \\right) - v\\left( {s_{t} } \\right)\\) are TD-error
    and advantage function. Algorithm 4 is the pseudo code of the proposed AC-CCTS,
    the algorithm’s inputs are node set D, task set S, minibatch k, learning rate
    \\(\\eta_{1}\\) and \\(\\eta_{2}\\), replay interval T, capacity of experience
    pool N etc. The algorithm’s output is the optimal scheduling. Figure 3 shows the
    running process of the proposed AC-CCTS, which mainly includes the following 7
    steps: Fig. 3 The flow of the proposed AC-CCTS Full size image Step 1: Initialize
    the neural network parameter w and θ as random weights. Corresponds to lines 1 ~ 2
    of pseudocode. Step 2: Use Algorithm 3 to start the analysis of dynamic scheduled
    tasks. construct buffer queue to cache the initial scheduling and rescheduling
    tasks submitted by the user and environment monitor. Corresponding to lines 3
    to 5 of pseudocode. Step 3: Batch pulls tasks from the task queue for scheduling
    until the task queue is empty and in the waiting state. Corresponding to lines
    6 to 8 of pseudocode. Step 4: Use Algorithm 1 to sample scheduling actions and
    construct the optimal scheduling scheme. Here, the optimal scheduling scheme is
    the binding relationship between tasks and nodes. Corresponding to line 9 of pseudocode.
    Step 5: Scheduling Executor executes the optimal scheduling scheme and pulls up
    the target task container on the optimal scheduling node. Environment Monitor
    evaluates scheduling actions and monitors state transition of environment. Corresponding
    to line 10 to 12 of pseudocode. Step 6: Use Algorithm 2 to store the prioritized
    scheduling experiences by using a sum-tree data structure, and sample scheduling
    experiences by using \\(\\varepsilon - greedy\\) method. Compute TD-Error \\(\\delta_{t}\\)
    and advantage function value \\(\\sigma_{t}\\) for parameters updating of Actor-Critic
    network. Corresponding to pseudocode lines 13 to 21. Step 7: Update parameters
    w and \\(\\theta\\) of Actor-Critic neural network using SGD and SGA methods.
    Corresponding to line 22 and 23. The time consumption of AC-CCTS algorithm mainly
    lies in the construction of the optimal scheduling scheme, the experience storage
    and sampling, and the update of network parameters, which correspond to steps
    9, 13, and 22–23 respectively. Suppose that the cluster has J nodes and K basic
    resources, the number of layers of Actor network and Critic network are all L,
    and each layer has M parameters on average. Then, the time complexity of steps
    4, 6 and 22–23 is \\(O\\left( {J \\cdot K} \\right)\\), \\(O\\left( {k \\cdot
    log \\, N} \\right)\\) and \\(O\\left( {J \\cdot K + L \\cdot M \\cdot \\tau }
    \\right)\\), respectively. So, the time complexity of AC-CCTS is \\(O\\left( {J
    \\times K + L \\times M \\times \\tau + k \\cdot log \\, N} \\right)\\). Here,
    \\(\\tau\\) is the average time for the neural network to derive its parameters,
    N is the capacity of experience memory and k is the number of experiences per
    replay. Because environmental state in Step 4 and experience sampling in Step
    6 can be processed in parallel, and the number of layer and parameter dimension
    of neural network can be effectively controlled according to actual scheduling
    scenarios, there is still room for further optimization of the time complexity
    of the algorithm. 4 Evaluation In this section, we first design four comparative
    experiments in different scenarios to evaluate the convergence, static and dynamic
    scheduling performance of AC-CCTS, and make a comprehensive comparison with other
    excellent scheduling algorithms, including reinforcement learning based scheduling
    algorithms DeepRM-Plus [32] and RLSched [34], meta-heuristic based scheduling
    algorithms FIMPSO [23] and HWOA-MBA [25]. In addition, design ablation experiments
    to verify the effectiveness of heuristic strategy, prioritized experience replay,
    and scheduling compensation mechanisms. Finally, design parameter optimization
    experiments to optimize the performance of AC-CCTS and determine the value range
    of model parameters in engineering applications. 4.1 Experiment setting In terms
    of experimental environment, we use Cloudsce, a self-developed simulation environment,
    and Kubernetes, an online container cloud environment, as the experimental platform.
    Cloudsce is used to simulate large-scale task scheduling scenarios and evaluate
    the convergence of algorithms through Alibaba production cluster tracking data.
    Kubernetes is used to simulate real task scheduling scenarios and evaluate the
    static and dynamic scheduling performance of algorithms through centralized scheduling
    of local tasks. The resource configuration of Kubernetes (V1.16.2) is shown in
    Table 3. In addition, we use tensorflow 2.0.4 to build the neural network model,
    JMeter (v5.4.0) tool for load injection and QoS evaluation. Table 3 Cluster resources
    Full size table In terms of experimental dataset, the Alibaba cluster tracking
    dataset clusterdata-2011–2 [37] and the local real business dataset are used.
    clusterdata-2011–2 contains metadata and runtime information for 4 K machines,
    71 K online services, and 4 M batch jobs. The local dataset contains 100 real
    template tasks, which are business applications on geographic data processing,
    message communication, etc. We expanded the 100 template tasks by multi-replicas
    method, and the final number of task instances was 5000. According to the type
    of resource request, tasks are divided into five types: CPU-intensive, memory-intensive,
    disk-intensive, bandwidth-intensive, and non-intensive. Among them, non-intensive
    tasks have a more balanced use of resources and have no tendency. In the experiment,
    the Alibaba dataset tasks are arrived according to the original Poisson distribution.
    For the local dataset, we assume that the number of tasks arriving at the Kubernetes
    cloud platform in a time slot (1 min) conforms to the Poisson distribution, and
    simulates the task arrival time at a frequency of λ = 50. In terms of algorithm
    parameter setting, Actor and Critic neural network are both 4-layer structures,
    the number of nodes of the two hidden layers are 256 and 128 respectively, and
    the activation function is relu. Adam optimizer was used to train the network.
    The settings of other parameters are shown in Table 4. Table 4 Parameters setting
    of PA-CCTS Full size table In terms of experimental evaluation, we selected four
    metrics from the perspective of cluster resource and service quality evaluation.
    They are resource utilization rate (RUR), resource imbalance degree (RID), service
    response time (RT) and throughput (TH). Among them, RUR and RID can reflect the
    comprehendsive utilization efficiency of cluster resources as shown in Formulas
    (38) and (39). $${\\text{RUR}}\\left( t \\right) = \\sum\\limits_{j = 1}^{J} {{\\rm
    RUR}\\left( {j,t} \\right)/} J$$ (38) $${\\rm RID}\\left( t \\right) = {{\\sum\\limits_{j
    = 1}^{J} {\\left( {1 - {\\rm RBD}\\left( {j,t} \\right)} \\right)} } \\mathord{\\left/
    {\\vphantom {{\\sum\\limits_{j = 1}^{J} {\\left( {1 - {\\rm RBD}\\left( {j,t}
    \\right)} \\right)} } J}} \\right. \\kern-0pt} J}$$ (39) RT and TH are commonly
    used evaluation metrics of quality of service, as shown in Formulas (40) and (41).
    $${\\rm RT} = \\frac{1}{M}\\sum\\limits_{i = 1}^{M} {{\\rm RT}\\left( {s_{i} }
    \\right)}$$ (40) where RT (si) is the average time consumed between application
    si submitting a certain number of concurrent requests and returning responses,
    and M is the amounts of HTTP applications for QoS evaluation. $${\\rm TH} = \\frac{1}{M}\\sum\\limits_{i
    = 1}^{M} {{\\rm TH}\\left( {s_{i} } \\right)}$$ (41) $${\\rm TH}\\left( {s_{i}
    } \\right) = \\frac{{N_{req} \\left( {s_{i} } \\right)}}{{T_{end} \\left( {s_{i}
    } \\right) - T_{start} \\left( {s_{i} } \\right)}}$$ (42) where \\(N_{req} \\left(
    {s_{i} } \\right)\\) is the number of successful requests of the application under
    a certain concurrent access, \\(T_{{\\rm start}} \\left( {s_{i} } \\right)\\)
    and \\(T_{{\\rm end}} \\left( {s_{i} } \\right)\\) are the start-time and end-time
    of the QoS testing, respectively. \\({\\rm TH}\\left( {s_{i} } \\right)\\) is
    the throughput of the application. 4.2 Experiment result Experiment A evaluates
    the scheduling convergence of the algorithm. We use Cloudsce to simulate a large-scale
    task scheduling scenario to evaluate the convergence of the algorithm. Specifically,
    a 1000-node container cloud cluster is simulated, and a hybrid scheduling experiment
    is performed using 20 K online services and 2 M batch jobs of the Alibaba tracking
    dataset. All tasks are arrived in their original timestamps with a Poisson distribution.
    The end condition of episode is that all tasks are scheduled or the scheduling
    node cannot meet the resource demand of the task. The algorithm is iterated 500
    times, and the experiment is repeated 10 times to take the average result to reduce
    random error. Figure 4a–d shows the comparison results of algorithms AC-CCTS,
    DeepRM-Plus, RLSched, FIMPSO and HWOA-MBA under the four metrics of cumulative
    reward, proportion of successfully scheduled tasks, resource utilization rate
    (RUR) and resource imbalance degree (RID). Overall, the cumulative reward curve
    of PA-CCTS is significantly higher than that of the comparison algorithms, and
    the proportion of successful task scheduling and the utilization of cluster resources
    are also better. From Fig. 4a, when the number of iterations is small, the cumulative
    reward curve of PA-CCTS is between DeepRM-Plus and RLSched, which is close to
    the meta heuristic algorithm FIMPSO, but inferior to HWOA-MBA. As the number of
    iterations increases, the cumulative reward curve of PA-CCTS rises rapidly and
    converges stably, surpassing the comparison algorithm after about 100 iterations.
    Compared with DeepRM-Plus and RLSched, the scheduling convergence performance
    of PA-CCWS is improved by an average of 14.7% and 16.2%, respectively. In terms
    of meta-heuristicbased comparison algorithms, HWOA-MBA uses the mutation operator
    of the bees algorithm to enhance the search ability of the random double adaptive
    whale optimization algorithm, thus improving the probability of obtaining the
    optimal solution of the multi-objective scheduling problem. However, FIMPSO is
    difficult to adapt to the cloud environment with high randomness due to its poor
    noise robustness. Compared with HWOA-MBA and FIMPSO, the scheduling convergence
    of PA-CCWS is improved by an average of 7.8% and 27.1%, and the resource utilization
    is improved by about 7.7% and 14.2% on average, respectively. Furthermore, Fig.
    4c, d show that PA-CCWS has higher task scheduling efficiency and more balanced
    resource load than the comparison algorithms. Benefiting from the global dynamic
    optimization capability, PA-CCTS effectively avoids falling into local optimal
    state. Firstly, heuristic strategy is used to assist the Agent in scheduling decision-making,
    which reduces the cost of trial-and-error learning in the early stage of reinforcement
    learning, and obtains scheduling capabilities close to meta-heuristics. Meanwhile,
    priority experience replay is used to avoid irrelevant state updates to accelerate
    model learning and improve the effect of scheduling convergence. Finally, through
    performance aware scheduling compensation, enhance the adaptability of the environment,
    balance the usage of resources, and improve the quality of service. Fig. 4 Comparison
    of scheduling convergence of algorithms in simulation environment Full size image
    Experiment B evaluates the static scheduling performance of the algorithm (Tasks
    are scheduled only once and will not be migrated). Five types of local applications
    are uniformly selected for scheduling in the online environment, and the number
    of tasks in a time slot (1 min) conforms to the Poisson distribution of parameter
    λ = 50. Each group of experiments are repeated for 10 times to take the average
    of results (The order of task arrival remained the same in each experiment). Experimental
    results are shown in Fig. 5a, b, and we can see the performance of AC-CCTS is
    better than comparison algorithms in terms of RID and RUR. Overall performance
    ranking is AC-CCTS > HWOA-MBA > DeepRM-Plus > RLSched > FIMPSO. From the RID curve,
    when the number of tasks is less than 1.0 × 103, AC-CCTS is close to HWOA-MBA
    and DeepRM-Plus, and is superior to FIMPSO and RLSched. However, with the increase
    of the number of tasks, HWOA-MBA and DeepRM-Plus converges premat-urely, while
    AC-CCTS decreases rapidly and converges stably. It shows that in the real scheduling
    scenario, AC-CCTS also has better resource balancing scheduling performance. From
    the RUR curve, the resource utilization rate of AC-CCTS is obviously better than
    that of the comparison algorithms when the number of tasks is greater than 3 × 103.
    As can be seen from the RID curve that the resource scheduling of AC-CCTS is more
    balanced at this time, and the number of successfully scheduled tasks is also
    more. Compared with traditional meta-heuristic scheduling algorithms HWOA-MBA
    and FIMPSO, the resource utilization is increased by about 8.9% and 13.6% on average,
    respectively. Experiments show that in static scheduling scenarios, AC-CCTS can
    effectively use heuristic strategy to guide scheduling, the heuristic strategy
    can be used to make more correct scheduling decisions in the case of lack of scheduling
    experience. With the increase of the number of tasks, AC-CCTS accumulated more
    real scheduling experience. Through the replay of prioritized experience, the
    scheduling scheme was repeatedly optimized, and the learning efficiency of Actor-Critic
    algorithm was significantly improved. However, although the reinforcement learning
    algorithms DeepRM-Plus and RLSched use imitation learning or action-shaping to
    accelerate model learning, they lack the guidance of domain knowledge and do not
    consider the importance difference between empirical samples, resulting in premature
    convergence of the model. Meta heuristic algorithms FIMPSO and HWOA-MBA are difficult
    to handle highly random container cloud scheduling scenarios due to their own
    shortcomings. Fig. 5 Comparison of task static scheduling performance Full size
    image Experiment C we fix the number of tasks and evaluate the impact of time
    on dynamic scheduling performance of the algorithm. We design three dynamic scheduling
    scenarios with tasks of 1 × 103, 3 × 103 and 5 × 103, and each scenario has the
    same proportion of five types of applications. The scanning period of the environment
    is set to 1 min, and the state of cluster resources is recorded every 30 s. Each
    group of experiments is repeated 10 times to get the mean value of the results.
    In the three dynamic scheduling scenarios, the node resource thresholds are set
    to 0.4, 0.6 and 0.9, respectively. When the load of node resources exceeds the
    corresponding threshold, dynamic task scheduling is triggered. The experimental
    results are shown in Fig. 6a–i. It can be seen that dynamic scheduling improves
    the resource scheduling performance of each algorithm, but AC-CCTS is better.
    Fig. 6 Comparison of dynamic task scheduling performance. where a, b, c, d, e,
    f, g, h, i are the RID, RUR and proportion of repetitive scheduling tasks curves
    corresponding to the number of tasks 1 × 103, 3 × 103, and 5 × 103, respectively
    Full size image In the dynamic scheduling scenario where the number of tasks is
    1 × 103, the RID curve of AC-CCTS (Fig. 6a) is slightly better than HWOA-MBA,
    but HWOA-MBA is relatively smooth. In the dynamic scheduling scenario with the
    number of tasks of 3 × 103, The RID curves of AC-CCTS and HWOA-MBA (Fig. 6d) have
    comparable convergence performance, but AC-CCTS converges faster. In the dynamic
    scheduling scenario with the number of tasks of 5 × 103, the RID of AC-CCTS is
    obviously better than the comparison algorithms. Compared with reinforcement learning
    algorithm DeepRM-Plus and meta-heuristic algorithm HWOA-MBA, the resource utilization
    rate is improved by about 11.9% and 10.7% on average, respectively. Meanwhile,
    it can be seen from Fig. 6c, f, i that the number of AC-CCTS rescheduling tasks
    is less, indicating that the task scheduling of AC-CCTS is more reasonable. Thanks
    to the performance aware scheduling compensation mechanism, AC-CCTS monitors the
    running state of the container-based cloud environment in real time, migrates
    and adjusts the unbalanced tasks in a timely manner, and enhances the adaptability
    of the scheduling agent to the environment. Experiment D evaluates the impact
    of resource state on QoS. In the dynamic scheduling scenario where the number
    of tasks is 3 × 103, HTTP applications (the type and number of applications selected
    are consistent with the comparison algorithms) are selected from the cluster for
    QoS testing. Every 10 min, use JMeter automatic script to make concurrent interface
    call requests to these HTTP applications (the number of concurrent requests is
    1000), and record the RT and TH values. Figure 7a, b shows that RT metric of all
    algorithms decrease with time, while TH metric is opposite, which indicates that
    dynamic scheduling can improve the QoS of applications. Compared with the initial
    state of AC-CCTS itself (time = 15 min), the (RT, TH) performance is improved
    by approximately (31.3%, 42.3%), which is obviously better than the comparison
    algorithms. The experimental results show that AC-CCTS can balance resources and
    improve the quality of service through dynamic scheduling. Fig. 7 Comparison of
    QoS performance of applications Full size image Experiment E we conducted ablation
    experiments to verify impact of heuristic rules (H), prioritized experience replay
    (E) and method of scheduling compensation (D) on the scheduling performance of
    AC-CCTS. Five types of applications were uniformly selected for scheduling, and
    each group of experiments was repeated for 10 times to take the average of the
    results. In the experiment of scheduling compensation, the scanning cycle of the
    environment is set to 1 min, and the node resource threshold is set to 0.9. Under
    various combinations of “H”, “E” and “D”, we compared the convergence performance
    of RID and RUR of the algorithm in the scenario with the number of tasks being
    5 × 103. Figure 8a, b shows the experimental results. Among them, “ + H” means
    to add the heuristic rules alone, “ + HD” means to add the heuristic rules and
    scheduling compensation at the same time, and others are similar. We use A2C as
    the benchmark for comparison. Data analysis shows that the performance of (RID,
    RUR) is improved by (21%, 20%), (16%, 17%) compared with A2C after adding “H”
    and “E” separately. And improved by (33%, 26%) after adding both \"HE\". Therefore,
    the introduction of heuristics learning and prioritized experience replay can
    improve task scheduling performance. In addition, we continue to add method of
    scheduling compensation “D” on the basis of “H” and “E”, the performance of (RID,
    RUR) was improved (26%, 24%) and (21%, 22%), respectively. Finally, we added “H”,
    “E” and “D” at the same time, namely, the proposed AC-CCTS algorithm. Compared
    with A2C, metrics RID and RUR achieved performance improvement of 40% and 31%,
    which proved the effectiveness of the proposed heuristic multi-objective dynamic
    scheduling. Fig. 8 Ablation experiment of heuristic rules (H), prioritized experience
    replay (E) and scheduling compensation (D) Full size image Experiment F evaluates
    the impact of parameters on the scheduling performance of AC-CCTS. Five types
    of applications were uniformly selected for scheduling, and each group of experiments
    is repeated for 10 times to take the average of the results. The scanning period
    of the environment is set to 1 min, and the node resource threshold is set to
    0.9. The state of cluster resources is recorded every 30 s. In Fig. 9a, b, we
    evaluate the impact of heuristic threshold Nth on task scheduling performance.
    We take the curve of Nth = 0 as the benchmark for comparison, because the heuristic
    strategy has not been added. Overall, with the increase of Nth, the performance
    improvement of RID and RUR tends to increase first and then decrease, and the
    best performance is achieved when Nth = 0.6. When the number of tasks is less
    than 1 × 103, the increase of Nth brings about the improvement of RID and RUR
    to varying degrees, and this advantage is amplified when the number of tasks is
    2 × 103 ~ 4 × 104. However, with the progress of scheduling and the accumulation
    of scheduling experience, this advantage is gradually weakened, because at this
    time, the scheduling strategy and scheduling evaluation of agent become more reasonable.
    It shows that heuristic method can improve the scheduling performance of Actor-Critic
    algorithm in the absence of experience. But, too much heuristic involvement in
    scheduling decisions will lead to a decline in scheduling performance. Generally,
    it is reasonable to set Nth as 0.2 ~ 0.6. Fig. 9 Impact of heuristic threshold
    Nth on convergence performance of resource Full size image In Fig. 10a, b, we
    evaluate the impact of real variable \\(\\lambda\\) in prioritized experience
    replay on task scheduling performance. Set the capacity of experience pool N = 10,000,
    mini-batch k = 50. We take the curve of \\(\\lambda\\) = 0 as the benchmark for
    comparison, because the priority function only contains TD-error. It can be seen
    that when \\(\\lambda\\) = 0.2 and 0.4, the RID curve is below the \\(\\lambda\\) = 0
    curve, they show better performance of resource balancing scheduling. Compared
    with \\(\\lambda\\) = 0 curve, (RID, RUR) were improved (10.7%, 3.8%), (12.5%,
    4.6%) in \\(\\lambda\\) = 0.2, 0.4, respectively. It shows that the priority function
    considering both action reward G(i) and TD-error is more conducive to the improvement
    of scheduling performance. However, according to the curves of \\(\\lambda\\) = 0.6,
    0.8 and 1.0, the proportion of G(i) should not be too large, which would lead
    to over-fitting of neural network parameters and a decline in scheduling performance.
    Generally, it is reasonable to set \\(\\lambda\\) as 0.2 ~ 0.4. In addition, we
    further verify the incremental prioritized experience replay, that is, verify
    scheduling performance when the experience pool has a certain amount of initial
    experience, and find that the addition of initial experience can further improve
    the convergence performance of the model. Fig. 10 Impact of parameter \\(\\lambda\\)
    of prioritized experience replay on convergence performance of resource Full size
    image In Fig. 11a, b, we evaluate the impact of real variable \\(\\alpha\\) in
    rewards function on task scheduling performance. We take the curve of \\(\\alpha\\) = 0
    as the benchmark for comparison, because only the rewards for resource utilization
    are considered at this time. Overall, with the increase of \\(\\alpha\\), the
    performance improvement of RID and RUR of AC-CCTS tends to increase first and
    then decrease, and the best performance is achieved when \\(\\alpha\\) = 0.4,
    Compared with \\(\\alpha\\) = 0, (RID, RUR) performance is improved by about (11.62%,
    8.5%). In addition, \\(\\alpha\\) = 0.6 has the same performance as \\(\\alpha\\) = 0,
    while \\(\\alpha\\) = 0.8 and 1.0 have poor performance. It shows that the scheduling
    performance can be improved when considering both the increment of resource balance
    degree and the increment of resource utilization rate. Generally, it is reasonable
    to set \\(\\alpha\\) as 0.2 ~ 0.6. Fig. 11 Impact of parameter \\(\\alpha\\) of
    instant rewards on convergence performance of resource Full size image Figure
    12a, b shows the effect of learning rates η1, η2 on RID and RUR. We can see that
    the final convergence results of RID and RUR are different under different combinations
    of η1 and η2. With the increase of learning rate, RID tends to be small first
    and then large, while RUR is just the opposite. It shows that too small or too
    large learning rate is not conducive to the convergence of reinforcement learning
    scheduling. The learning rate is too small, the parameters of neural network are
    adjusted too fine, and the scheduling takes a long time to converge. The learning
    rate is too large, the parameters of neural network are adjusted too rough, and
    the scheduling is easy to jump out of the global optimal and occur the phenomenon
    of overfitting. Learning rates η1 and η2 are usually set to 0.02 ~ 0.06 can achieve
    better scheduling convergence performance. Fig. 12 Impact of learning rate on
    RID and RUR Full size image In summary, AC-CCTS algorithm effectively integrates
    the heuristic method, prioritized experience replay and scheduling compensation
    method, continuously exploring and exploiting scheduling actions with scheduling
    policies and experience for a better decision. 5 Conclusion Aiming at the optimization
    problems of task scheduling in the container cloud, we propose a heuristic multi-objective
    task scheduling framework and algorithms based on Actor-Critic reinforcement learning.
    First, by defining the container cloud environment, scheduling agent, scheduling
    actions and scheduling evaluation models, we established a deep reinforcement
    learning model for the problem of task scheduling. Then, we propose a dynamic
    scheduling algorithm using Actor-Critic reinforcement learning based on the proposed
    scheduling architecture. That is, the heuristic probability function is designed
    to help agent to make better scheduling decisions and improve the scheduling accuracy.
    Prioritized experience replay is used to conduct targeted learning according to
    the importance of experience, avoid irrelevant or meaningless status updates,
    and effectively accelerate learning convergence. The method of scheduling compensation
    is used to dynamically schedule tasks to ensure the stability of environment and
    the quality of service. Finally, through the design of comparative experiments,
    the proposed algorithm is compared with other reinforcement learning algorithms
    such as DeepRM-Plus and RLSched, meta-heuristic algorithms such as FIMPSO and
    HWOA-MBA. The results show that the proposed algorithm has better environmental
    adaptability and resource optimization efficiency. Due to the differences of resource
    types in the cloud environment, some tasks with relevance to specific hardware
    should consider not only the characteristics of the current cluster, tasks and
    nodes, but also the application scenarios of specific tasks to realize scheduling
    optimization. In addition, due to the limitation of scheduling decisions in the
    efficiency and performance with the single agent, we will explore the multi-agent
    scheduling learning. Notes We regard the task in our work as the represents of
    all types of cloud computing workloads about operation mode (e.g., long-running
    services, batch jobs, periodic jobs, etc.) References Xie XL, Wang Q (2020) A
    scheduling algorithm based on multi-objective container cloud task. J Shandong
    Univ (Eng Sci) 50(04):14–21 Google Scholar   Bian JF (2017) Docker-based application
    container cluster management system. Shandong University, Jinan Google Scholar   He
    Z (2020) Novel container cloud elastic scaling strategy based on Kubernetes. In:
    2020 IEEE 5th information technology and mechatronics engineering conference (ITOEC),
    pp 1400–1404. https://doi.org/10.1109/ITOEC49072.2020.9141552 Pahl C, Brogi A,
    Soldani J (2019) Cloud container technologies: a state-of-the-art review. IEEE
    Trans Cloud Comput 7(3):677–692. https://doi.org/10.1109/TCC.2017.2702586 Article   Google
    Scholar   Avinab M, Wang Y, Zhang F et al (2018) Energy-aware fault-tolerant dynamic
    task scheduling scheme for virtualized cloud data centers. Mobile Netw Appl. https://doi.org/10.1007/s11036-018-1062-7
    Article   Google Scholar   Alsadie D (2021) A metaheuristic framework for dynamic
    virtual machine allocation with optimized task scheduling in cloud data centers.
    IEEE Access 9:74218–74233. https://doi.org/10.1109/ACCESS.2021.3077901 Article   Google
    Scholar   Ebadifard F, Babamir SM (2021) Autonomic task scheduling algorithm for
    dynamic workloads through a load balancing technique for the cloud-computing environment.
    Clust Comput 24(2):1075–1101. https://doi.org/10.1007/s10586-020-03177-0 Article   Google
    Scholar   Silver D, Huang A, Maddison CJ et al (2016) Mastering the game of go
    with deep neural networks and tree search. Nature 529(7587):484–489. https://doi.org/10.1038/nature16961
    Article   Google Scholar   Sutton RS, Barto AG (1998) Reinforcement learning.
    Bradford Book 15(7):665–685. https://doi.org/10.1007/978-3-642-27645-3 Article   Google
    Scholar   Li H, Kumar N, Chen R, et al (2018) Deep reinforcement learning. In:
    ICASSP 2018 - 2018 IEEE international conference on acoustics, speech and signal
    processing (ICASSP) Wang Z, Liu H, Han L et al (2021) Research and implementation
    of scheduling strategy in kubernetes for computer science laboratory in universities.
    Information (Switzerland) 12(1):16–26. https://doi.org/10.3390/info12010016 Article   Google
    Scholar   Rossi F, Cardellini V, Presti F L (2019) Elastic deployment of software
    containers in geo-distributed computing environments. In: 2019 IEEE symposium
    on computers and communications (ISCC). https://doi.org/10.1109/ISCC47284.2019.8969607
    Menouer T (2021) KCSS: Kubernetes container scheduling strategy. J Supercomput
    77(5):4267–4293. https://doi.org/10.1007/s11227-020-03427-3 Article   Google Scholar   Jaybhaye
    S M, Attar V Z (2021) Heterogeneous resource provisioning for workflow-based applications
    using AHP in cloud computing. In: Proceedings of international conference on communication
    and computational technologies. Springer, Singapore, pp 453–465. https://doi.org/10.1007/978-981-15-5077
    5_41 Zhou Z, Xie H, Li F (2019) A novel task scheduling algorithm integrated with
    priority and greedy strategy in cloud computing. J Intell Fuzzy Syst 37(4):1–9.
    https://doi.org/10.3233/JIFS-179299 Article   Google Scholar   Zhou Z, Wang H,
    Shao H et al (2020) A high-performance scheduling algorithm using greedy strategy
    toward quality of service in the cloud environments. Peer-to-Peer Netw Appl 13(6):2214–2223.
    https://doi.org/10.1007/s12083020-00888-4 Article   Google Scholar   Kumar S,
    Dumka A (2021) Load balancing with the help of round robin and shortest job first
    scheduling algorithm in cloud computing. In: Proceedings of international conference
    on machine intelligence and data science applications. Springer, Singapore, pp
    213–223. https://doi.org/10.1007/978-981-33-4087-9_19 Gao Y (2021) Min-max scheduling
    of batch or drop-line jobs under agreeable release and processing times. Asia-Pacific
    J Oper Res. https://doi.org/10.1142/S0217595921500238 Article   MATH   Google
    Scholar   Singh S, Singh V (2016) A genetic based improved load balanced Min-Min
    task scheduling algorithm for load balancing in cloud computing. In: 2016 8th
    international conference on computational intelligence and communication networks.
    IEEE, pp 677–681. https://doi.org/10.1109/CICN.2016.139 Samadi Y, Zbakh M., Tadonki
    C (2018) E-HEFT: Enhancement heterogeneous earliest finish time algorithm for
    task scheduling based on load balancing in cloud computing. In: 2018 international
    conference on high performance computing and simulation (HPCS), pp 601–609. https://doi.org/10.1109/HPCS.2018.00100
    Soltani N, Barekatain B, Neysiani BS (2021) MTC: Minimizing time and cost of cloud
    task scheduling based on customers and providers needs using genetic algorithm.
    Int J Intell Syst Appl 13(2):38–51. https://doi.org/10.5815/ijisa.2021.02.03 Article   Google
    Scholar   Peng L, Song J, He X et al (2018) Resource scheduling optimisation algorithm
    for containerised microservice architecture in cloud computing. Int J High Perform
    Syst Archit 8(1–2):51–58. https://doi.org/10.1504/IJHPSA.2018.094144 Article   Google
    Scholar   Devaraj AFS, Elhoseny M, Dhanasekaran S et al (2020) Hybridization of
    firefly and improved multi-objective particle swarm optimization algorithm for
    energy efficient load balancing in cloud computing environments. J Parallel Distrib
    Comput 142:36–45. https://doi.org/10.1016/j.jpdc.2020.03.022 Article   Google
    Scholar   Li H, Wang X, Gao S, et al (2020) A service performance aware scheduling
    approach in containerized cloud. In: 2020 IEEE 3rd international conference on
    computer and communication engineering technology (CCET). IEEE, pp 194–198. https://doi.org/10.1109/CCET50901.2020.9213084
    Manikandan N, Gobalakrishnan N, Pradeep K (2022) Bee optimization based random
    double adaptive whale optimization model for task scheduling in cloud computing
    environment. Comput Commun 187:35–44. https://doi.org/10.1016/j.comcom.2022.01.016
    Article   Google Scholar   Jian CF, Chen JW, Zhang MY (2019) Improved chaotic
    bat swarm cooperative scheduling algorithm for edge computing. J Chin Comput Syst
    40(11):2424–2430 Google Scholar   Chang J, Hu Z, Tao Y (2018) Task scheduling
    based on dynamic non-linear PSO in cloud environment. In: 2018 IEEE 9th international
    conference on software engineering and service science (ICSESS). Beijing, China,
    pp 877–880, https://doi.org/10.1109/ICSESS.2018.8663825 Ziyath SPM, Senthilkumar
    S (2020) MHO: meta heuristic optimization applied task scheduling with load balancing
    technique for cloud infrastructure services. J Ambient Intell Humaniz Comput.
    https://doi.org/10.1007/s12652-020-02282-7 Article   Google Scholar   Ding S,
    Wu J, Xie G and Zeng G (2017) A hybrid heuristic-genetic algorithm with adaptive
    parameters for static task scheduling in heterogeneous computing system. In: 2017
    IEEE Trustcom/BigDataSE/ICESS, pp 761–766. https://doi.org/10.1109/Trustcom/BigDataSE/ICESS.2017.310
    Zou J, Hao T, Yu C et al (2021) A3C-DO: A regional resource scheduling framework
    based on deep reinforcement learning in edge scenario. IEEE Trans Comput 70(2):228–239.
    https://doi.org/10.1109/TC.2020.2987567 Article   MATH   Google Scholar   Mao
    H, Alizadeh M, Menache I, et al (2016) Resource management with deep reinforcement
    learning. In Proceedings of the 15th ACM workshop on tot topics in networks, pp
    50–56. https://doi.org/10.1145/3005745.3005750 Guo W, Tian W, Ye Y et al (2020)
    Cloud resource scheduling with deep reinforcement learning and imitation learning.
    IEEE Internet Things J 8(5):3576–3586. https://doi.org/10.1109/JIOT.2020.3025015
    Article   Google Scholar   Che H, Bai Z, Zuo R et al (2020) (2020) A deep reinforcement
    learning approach to the optimization of data center task scheduling. Complexity
    4:1–12. https://doi.org/10.1155/2020/3046769 Article   Google Scholar   Lorido-Botran
    T, Bhatti M K (2021) Adaptive container scheduling in cloud data centers: a deep
    reinforcement learning approach. In: International conference on advanced information
    networking and applications. Springer, Cham, pp 572–581. https://doi.org/10.1007/978-3-030-75078-7_57
    Asghari A, Sohrabi MK, Yaghmaee F (2020) Task scheduling, resource provisioning,
    and load balancing on scientific workflows using parallel SARSA reinforcement
    learning agents and genetic algorithm. J Supercomput 77(3):2800–2828. https://doi.org/10.1007/s11227-020-03364-1
    Article   Google Scholar   Pham N, Nguyen L, Phan D, et al (2020) A hybrid stochastic
    policy gradient algorithm for reinforcement learning. In: International conference
    on artificial intelligence and statistics. PMLR, pp 374–385 Alibaba Inc (2018)
    Alibaba production cluster data v2018. Website. https://github.com/alibaba/-clusterdata/tree/v2018
    Download references Author information Authors and Affiliations School of Information
    Science and Technology, University of Science and Technology of China, Hefei,
    230026, China Lilu Zhu, Feng Wu & Xinmei Tian Aerospace Information Research Institute,
    Chinese Academy of Sciences, Suzhou, 215123, China Lilu Zhu & Kai Huang Aerospace
    Information Research Institute, Chinese Academy of Sciences, Beijing, 100094,
    China Yanfeng Hu Corresponding author Correspondence to Lilu Zhu. Ethics declarations
    Conflict of interest No potential conflict of interest was reported by the authors.
    Data availability The datasets analysed during this study are available in the
    github repository: https://github.com/alibaba/-clusterdata/tree/v2018. Additional
    information Publisher''s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Springer Nature or its licensor (e.g. a society or other partner) holds exclusive
    rights to this article under a publishing agreement with the author(s) or other
    rightsholder(s); author self-archiving of the accepted manuscript version of this
    article is solely governed by the terms of such publishing agreement and applicable
    law. Reprints and permissions About this article Cite this article Zhu, L., Wu,
    F., Hu, Y. et al. A heuristic multi-objective task scheduling framework for container-based
    clouds via actor-critic reinforcement learning. Neural Comput & Applic 35, 9687–9710
    (2023). https://doi.org/10.1007/s00521-023-08208-6 Download citation Received
    23 August 2021 Accepted 06 January 2023 Published 17 March 2023 Issue Date May
    2023 DOI https://doi.org/10.1007/s00521-023-08208-6 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Container-based cloud Dynamic multi-objective Reinforcement learning Heuristic
    rules Use our pre-submission checklist Avoid common mistakes on your manuscript.
    Sections Figures References Abstract Introduction Related work Proposed methodology
    Evaluation Conclusion Notes References Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Neural Computing and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A heuristic multi-objective task scheduling framework for container-based
    clouds via actor-critic reinforcement learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhou N.
  - Zhou H.
  - Hoppe D.
  citation_count: '6'
  description: Containers improve the efficiency in application deployment and thus
    have been widely utilised on Cloud and lately in High Performance Computing (HPC)
    environments. Containers encapsulate complex programs with their dependencies
    in isolated environments making applications more compatible and portable. Often
    HPC systems have higher security levels compared to Cloud systems, which restrict
    users' ability to customise environments. Therefore, containers on HPC need to
    include a heavy package of libraries making their size relatively large. These
    libraries usually are specifically optimised for the hardware, which compromises
    portability of containers. Per contra, a Cloud container has smaller volume and
    is more portable. Furthermore, containers would benefit from orchestrators that
    facilitate deployment and management of containers at a large scale. Cloud systems
    in practice usually incorporate sophisticated container orchestration mechanisms
    as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration
    on HPC systems have been proposed in state of the art. This paper gives a survey
    and taxonomy of efforts in both containerisation and its orchestration strategies
    on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly,
    challenges are discussed and the potentials for research and engineering are envisioned.
  doi: 10.1109/TSE.2022.3229221
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Software...
    >Volume: 49 Issue: 4 Containerization for High Performance Computing Systems:
    Survey and Prospects Publisher: IEEE Cite This PDF Naweiluo Zhou; Huan Zhou; Dennis
    Hoppe All Authors 4 Cites in Papers 928 Full Text Views Abstract Document Sections
    I. Introduction II. Concepts and Technologies for Containerisation III. Container
    Engines and Runtimes for HPC Systems IV. Container Orchestration V. Research Challenges
    and Vision Show Full Outline Authors Figures References Citations Keywords Metrics
    Footnotes Abstract: Containers improve the efficiency in application deployment
    and thus have been widely utilised on Cloud and lately in High Performance Computing
    (HPC) environments. Containers encapsulate complex programs with their dependencies
    in isolated environments making applications more compatible and portable. Often
    HPC systems have higher security levels compared to Cloud systems, which restrict
    users’ ability to customise environments. Therefore, containers on HPC need to
    include a heavy package of libraries making their size relatively large. These
    libraries usually are specifically optimised for the hardware, which compromises
    portability of containers. Per contra , a Cloud container has smaller volume and
    is more portable. Furthermore, containers would benefit from orchestrators that
    facilitate deployment and management of containers at a large scale. Cloud systems
    in practice usually incorporate sophisticated container orchestration mechanisms
    as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration
    on HPC systems have been proposed in state of the art. This paper gives a survey
    and taxonomy of efforts in both containerisation and its orchestration strategies
    on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly,
    challenges are discussed and the potentials for research and engineering are envisioned.
    Published in: IEEE Transactions on Software Engineering ( Volume: 49, Issue: 4,
    01 April 2023) Page(s): 2722 - 2740 Date of Publication: 14 December 2022 ISSN
    Information: DOI: 10.1109/TSE.2022.3229221 Publisher: IEEE Funding Agency: SECTION
    I. Introduction Containers have been widely adopted on Cloud systems. Applications
    together with their dependencies are encapsulated into containers [1], which can
    ensure environment compatibility and enable users to move and deploy programs
    easily among clusters. Containerisation is a virtualisation technology [2]. Rather
    than creating an entire operating system (called guest OS) on top of a host OS
    as in a Virtual Machine (VM), containers only share the host kernel, which makes
    containers more lightweight than VMs. Containers on Cloud are often dedicated
    to run micro-services [3] and one container mostly hosts one application or a
    part of it. High Performance Computing (HPC) systems are traditionally employed
    to perform large-scale financial, engineering and scientific simulations [4] that
    demand low latency (e.g., interconnect) and high throughput (e.g., the number
    of jobs completed over a specific time). To satisfy different user requirements,
    HPC systems normally provide predefined modules with specific software versions
    that users can switch by loading or unloading the modules with the desired packages
    [5]. This approach requires assistance of system administrators and therefore
    limits increasing user demands for environment customisation. On a multi-tenant
    environment as on HPC systems, especially HPC production systems, installation
    of new software packages on-demand by users is restricted, as it may alter the
    working environments of existing users and even raise security risks. Module-enabled
    software environments are also inconvenient for dynamic Artificial Intelligence
    (AI) software stacks [6]. Big Data Analytics hosted on Cloud are compute-intensive
    or data-intensive, mainly due to deployments of AI or Machine Learning (ML) applications,
    which demand extremely fast knowledge extraction in order to make rapid and accurate
    decisions. HPC-enabled AI can offer optimisation of supply chains, complex logics,
    manufacturing, simulation and underpin modelling to solve complex problems [7].
    Typically, AI applications have sophisticated requirements of software stacks
    and configurations. Containerisation not only enables customised environments
    on HPC systems, but also brings research reproducibility into practice. Containerised
    applications can become complex, e.g., thousands of separate containers may be
    required in production, and containers may require network isolation among each
    other for security reasons. Sophisticated strategies for container orchestration
    [8] have been developed on Cloud or big-data clusters to meet such requirements.
    HPC systems, per contra, lack features of efficiency in container scheduling and
    management (e.g. load balancing and auto container scaling), and often provide
    no integrated support for environment provisioning (i.e., infrastructure, configurations
    and dependencies). There have been numerous studies on containerisation and container
    orchestration on Cloud [2], [9], [10], [11], [12], [13], [14], [15], however,
    there is no comprehensive survey on these technologies and techniques for HPC
    systems existing as of yet. This article: Investigates state-of-the-art works
    in containerisation on HPC systems and underscores their differences with respect
    to the Cloud; Introduces the representative orchestration frameworks on both HPC
    and Cloud environments, and highlights their feature differences; Gathers the
    related studies in the integration of container orchestration strategies on Cloud
    into HPC environments; Discusses the challenges and envisions the potential directions
    for research and engineering. The rest of the paper is organised as follows. First,
    Section II introduces the background on containerisation technologies and techniques.
    Key technologies of state-of-the-art container engines (Section III) and orchestration
    strategies (Section IV) are presented, and the feature differences thereof between
    HPC and Cloud systems are discussed. Next, Section V describes research challenges
    and the vision. Lastly, Section VI concludes this paper. SECTION II. Concepts
    and Technologies for Containerisation The main differences between containerisation
    technologies on Cloud and HPC systems are in terms of security and the types of
    workloads. The HPC applications tend to require more resources as to not only
    CPUs, but also the amount of memory and network speed. HPC communities have, therefore,
    developed sophisticated workload managers to leverage hardware resources and optimise
    application scheduling. Since the typical applications on Cloud differ significantly
    from those in HPC centres with respect to the sizes, execution time and requirements
    of the availability of hardware resources [16], the management systems on Cloud
    are evolved to include architectures different from those on HPC systems. Research
    and engineering on containerisation technologies and techniques for HPC systems
    can be classified into two broad categories: Container engines/runtimes; Container
    orchestration. In the first category, various architectures of container engines
    have been developed which vary in usage of namespaces (see Section II-A), image
    formats and programming languages. The research in the latter category is still
    in its primitive stage, which will be discussed in Section IV. A. Containerisation
    Concepts Containerisation is an OS-level virtualisation technology [17] that provides
    separation of application execution environments. A container is a runnable instance
    of an image that encapsulates a program together with its libraries, data, configuration
    files, etc. [1] in an isolated environment, hence it can ensure library compatibility
    and enables users to move and deploy programs easily among clusters. A container
    utilises the dependencies in its host kernel. The host merely needs to start a
    new process that is isolated from the host itself to boot a new container [18],
    thus making container start-up time comparable to that of a native application.
    In contrary, a traditional VM loads an entire guest kernel (simulated OS) into
    memory, which can occupy gigabytes of storage space on the host and requires a
    significant fraction of system resources to run. VMs are managed by hypervisor
    which is also known as Virtual Machine Monitor (VMM) that partitions and provisions
    VMs with hardware resources (e.g., CPU and memory). The hypervisor gives the hardware-level
    virtualisation [19], [20]. Fig. 1 highlights the architecture distinction of VMs
    and containers. It is worth noting that containers can also run inside VMs [21].
    Besides portability, containers also enable reproducibility, i.e. once a program
    has been defined inside the container, its included working environment remains
    unchanged regardless of its running occurrences. Nevertheless, the shared kernel
    strategy presents an obvious pitfall: a Windows containerised application cannot
    execute on Unix kernels. Obviously, this should not become an impediment to its
    usage as Unix-like OS are often the preference for HPC systems. Fig. 1. Structure
    comparison of VMs and containers. On the VM side, the virtualisation layer often
    appears to be hypervisor while on the container side it is the container runtimes.
    Show All HPC applications are often highly optimised for processor architectures,
    interconnects, accelerators and other hardware aspects. Containerised applications,
    therefore, need to compromise between performance and portability. The studies
    have shown that containers can often achieve near-native performance [18], [22],
    [23], [24], [25], [26], [27] (see Section III-B). Linux has several namespaces
    [28] that isolate various kernel resources: mount (file system tree and mounts),
    PID (process ID), UTS (hostname and domain name), network (e.g., network devices,
    ports, routing tables and firewall rules), IPC (inter-process communication resources)
    and user. The last namespace is an unprivileged namespace that grants the unprivileged
    process access to traditional privileged functionalities under a safe context.
    More specifically, the user namespace allows to map user ID (UID) and group ID
    (GID) from hosts to containers, meaning that a user having UID 0 (root) inside
    a container can be mapped to a non-root ID (e.g., 100000) outside the container.
    Cgroups (Control Groups) is another namespace that is targeted to limit, isolate
    and measure resource consumption of processes. Cgroups is useful for a multi-tenant
    setting as excess resource consumption of certain users will be only adverse to
    themselves. One application of Linux namespaces is the implementation of containers,
    e.g., Docker, the most widely-used container engine, uses namespaces to provide
    the isolated workspace that is called container. When a container executes, Docker
    creates a set of namespaces for that container. B. Docker There are multiple techniques
    that realise the concept of containers. Docker is among the most popular ones
    [27]. After its appearance in 2013, various container solutions aimed for HPC
    have emerged [22]. Docker, initially based on LXC [29], is a container engine
    that supports multiple platforms, i.e., Linux, OSX and Windows. A Docker container
    image is composed of a readable/writable layer above a series of read-only layers.
    A new writable layer is added to the underlying layers when a new Docker container
    is created. All changes that are made to the running container, such as writing
    new files, modifying or deleting existing files, are written to this thin writable
    container layer. Docker adopts namespaces including Cgroups to provide resource
    isolation and resource limitation, respectively. Table I highlights the usage
    of namespaces with respect to Docker and a list of container engines targeted
    for HPC environments. TABLE I Linux Namespace Supports for HPC-Targeted Container
    Engines (Section III) and Docker in the Year of 2022 Docker provides network isolation
    and communication by creating three types of networks: host, bridge and none.
    The bridge network is the default Docker network. The Docker engine creates a
    subset or gateway to the bridged network. This software bridge allows Docker containers
    to communicate within the same bridged network; meanwhile, isolates the containers
    from a different bridged network. Containers in the same host can communicate
    via the default network by the host IP address. To communicate with the containers
    located on a different host, the host needs to allocate ports on its IP address.
    Managing ports brings overhead which can intensify at scale. Dynamically managing
    ports can solve this issue which is better handled by orchestration platforms
    as introduced in Section IV-B. Docker is widely adopted in Cloud where users often
    have root privileges. The root privilege is required to execute the Docker application
    and its Daemon process that provides the essential services. Originally running
    Docker with root permission brings some advantages to Cloud users. For instance,
    users can run their applications and alternative security modules to provide separation
    among different allocations [30]; users can also mount host filesystems to their
    containers. Root privilege can cause security issues. Therefore, the latest updates
    of Docker engine start to support rootless daemon and enable users to execute
    containers without root. Nevertheless, other security concerns still persist.
    For instance, usage of Unix socket can be changed to TCP socket which will grant
    an attacker a remote control to execute any containers in the privileged mode.
    Additionally, rootless Docker does not run out of box, system administrators need
    to carefully set the namespaces of hosts to separate resources and user groups
    in order to guarantee security. Hence HPC centres that typically have high security
    requirements are still reluctant to enable the Docker support on their systems.
    SECTION III. Container Engines and Runtimes for HPC Systems This section first
    reviews the state-of-the-art container engines/runtimes designed for HPC systems
    and compares the major differences with the mainstream Cloud container engine,
    i.e., Docker. Next, Section III-B shows the performance evaluation of the reviewed
    HPC container engines. A. State-of-The-Art Container Engines and Runtimes A list
    of representative container engines and runtimes for HPC systems is given in this
    section. They differ in functional extent and implementation, however, also hold
    some similarities. Tables I and II summarise the feature differences and similarities
    between Docker and a list of main HPC container engines. TABLE II Comparison of
    Docker With the List of Container Engines for HPC Systems 1) Shifter Shifter [31]
    is a prototypical implementation of container engine for HPC developed by NERSC.
    It utilises Docker for image building workflow. Once an image is built, users
    can submit it to an unprivileged gateway which injects configurations and binaries,
    flattens it to an ext4 file system image, and then compresses to squashfs images
    that are copied to a parallel filesystem on the nodes. In this way, Shifter insulates
    the network filesystem from image metadata traffic. Root permission of Docker
    is naturally deprived from Shifter that only grant user-level privileges. Existing
    directories can be also mounted inside Shifter image by passing certain flags.
    As an HPC container engine, Shifter supports MPICH that is an implementation of
    the Message Passing Interface (MPI) [32], [33] standard. To enable accelerator
    supports such as GPU without compromising container portability, Shifter runtime
    swaps the built-in GPU driver of a Shifter container with an ABI (Application
    Binary Interface) compatible version at the container start-up time. 2) Charliecloud
    Charliecloud [28] runs containers without privileged operations or daemons. Charlicloud
    can convert a Docker image into a tar file and unpacks it on the HPC nodes. Installation
    of Charliecloud does not require root permission. Such non-intrusive mechanisms
    are ideal for HPC systems. Charliecloud is considered to be secure against shenanigans,
    such as chroot escape, bypass of file and directory permission, privileged ports
    bound to all host IP addresses or UID set to an unmapped UID [15]. MPI is supported
    by Charliecloud. Injecting host files into images is used by Charliecloud to solve
    library compatibility issues, such as GPU libraries that may be tied to specific
    kernel versions. 3) Singularity Singularity is the most-widely used HPC container
    engine in academia and industry. Singularity [34] was specifically designed from
    the outset for HPC systems. Contrasting with Docker, it gives the following merits
    [23]: Running with user privileges and no daemon process. Only user privileges
    are required to execute Singularity applications. Acquisition of root permission
    is only necessary when users want to build or rebuild images, which can be performed
    on their own working computers. Unprivileged users can also build an image from
    a definition file with a few restrictions by ”fake root” in Singularity, however,
    some methods requiring to create block devices (e.g., /dev/null) may not always
    work correctly in this way; Seamless integration with HPC systems. Singularity
    natively supports GPU, MPI and InfiniBand [16]. No additional network configurations
    are expected in contrast with Docker containers; Portable via a single image file
    (SIF format). On the contrary, Docker is built up on top of layers of files. Two
    approaches are often used to execute MPI applications using Singularity, i.e.,
    hybrid model and bind model. The former compiles MPI binaries, libraries and the
    MPI application into a Singularity image. The latter binds the container on a
    host location where the container utilises the MPI libraries and binaries on the
    host. The latter model has a smaller image size since it does not include compiled
    MPI libraries and binaries in the image. Utilising the host libraries is also
    beneficial to application performance, however, the version of MPI implementation
    that is used to compile the application inside the container must be compatible
    with the version available on the host. The hybrid model is recommended, as mounting
    storage volumes on the host often require privileged operations. Most Docker images
    can be converted to singularity images directly via simple command lines (e.g.
    docker save, singularity build). Singularity has quickly become the ipso facto
    standard container engine for HPC systems. 4) SARUS SARUS [35] is another container
    engine targeted for HPC systems. SARUS relies on runc1 to instantiate containers.
    runc is a CLI (Command-Line Interface) tool for spawning and running containers
    according to the OCI (Open Container Initiative) specification. Different from
    the aforementioned engines, the internal structure of SARUS is based on the OCI
    standard (see Section V-A2). As shown in Fig. 2, the CLI component takes the command
    lines which either invoke the image manager component or the runtime component.
    The latter instantiates and executes containers by creating a bundle that comprises
    a root filesystem directory and a JSON configuration file. The runtime component
    then calls runc that will spawn the container processes. It is worth noting that
    functionalities of SARUS can be extended by calling customised OCI hooks, e.g.,
    MPI hook. Fig. 2. The internal structure of SARUS. OCI hooks include MPI hook.
    Show All 5) UDocker UDocker2 is a Python wrapper for the Docker container, which
    executes only simple Docker containers in user space without the acquisition of
    root privileges. UDocker provides a Docker-like CLI and only supports a subset
    of Docker commands, i.e., search, pull, import, export, load, save, create and
    run. It is worth noting that UDocker neither makes use of Docker nor requires
    its presence on the host. It executes containers by simply providing a chroot-like
    environment over the extracted container. 6) Other HPC Container Engines More
    and more HPC container engines are being developed, this section gives an overview
    of some that are targeted for special use cases. Podman [36] makes use of the
    user namespace to execute containers without privilege escalation. A Podman container
    image comprises layers of read-write files as Docker. It adopts the same runtime
    runc as in SARUS and Docker. The runtime crun, which is faster than runc, is also
    supported. A notable feature of Podman is as its name denotes: the concept of
    pod. A pod groups a set of containers that collectively implements a complex application
    to share namespaces and simplify communication. This feature enables the convergence
    with the Kubernetes [37] environment (Section IV-B1), however, requires advanced
    kernel features (e.g., version 2 Cgroups and user-space FUSE). These kernel features
    are not yet compatible with network filesystems to make full use of the rootless
    capabilities of Podman and consequently restrain its usage from HPC production
    systems [38]. Similar to UDocker, Socker [39] is a simple secure wrapper to run
    Docker in HPC environments, more specifically SLURM (Section IV-A2). It does not
    support the user namespace, however, it takes the resource limits imposed by SLURM.
    Enroot3 from NVIDIA can be considered as an enhanced unprivileged chroot. It removes
    much of the isolation that the other container engines normally provide but preserves
    filesystem separation. Enroot makes use of user and mount namespaces. B. Performance
    Evaluation for HPC Container Engines This section only selects the representative
    works as given in Table III, rather than exhausting the literature, to show the
    performance of containers that are specifically targeted for HPC systems in terms
    of CPU, memory, disk (I/O), network and GPU. Table VI lists the benchmarks utilised
    in these work. Overall, the container startup latency can be high on the Cloud.
    This startup overhead is caused by building containers from multiple image layers,
    setting read-write layers and monitoring containers [27]. An HPC container is
    composed of a single image or directory (with exception to Podman) and monitoring
    is performed by HPC systems. TABLE III Overview of the Related Work on Container
    Performance Evaluation in Terms of CPU, Memory, Disk, Network and GPU on HPC Systems
    TABLE IV The List of HPC Benchmarks Mentioned in Section III-B The work in [24],
    utilising the IMB [42] benchmark suite and HPCG [43] benchmarks, proved that little
    overhead of network bandwidth and CPU computing overhead is caused by Singularity
    when dynamically linking vendor MPI libraries in order to efficiently leverage
    advanced hardware resources. With the Cray MPI library, Singularity container
    achieved 99.4% efficiency of native bandwidth on a Cray XC [44] HPC testbed when
    running the IMB benchmark suite. However, the efficacy drastically drops to 39.5%
    with Intel MPI. Execution time evaluated with the HPCG benchmarks, indicated that
    the performance penalty caused by Singularity is negligible with Cray MPI, though
    the overhead can reach 18.1% with Intel MPI. The performance degradation with
    Intel MPI is mostly because of the vendor-tuned MPI library which does not leverage
    hardware resources from a different vendor, e.g., interconnect. Hu et al. [23]
    evaluated the Singularity performance in terms of CPU capacity, memory, network
    bandwidth and GPU with Linpack benchmarks [45] and four typical HPC applications
    (i.e., NAMD [46], VASP [47], WRF [48] and AMBER [49]). Singularity provides close
    to native performance on CPU, memory and network bandwidth. A slight overhead
    (4.135%) is shown on NVIDIA GPU. Muscianisi et al. [41] illustrated the performance
    impact of Singularity with the increasing number of GPU nodes. The evaluation
    was carried out on CINECA''s GALILEO systems with TensorFlow [50] applications.
    The results again demonstrated that the container environments caused negligible
    performance overhead. The work by Hale et al. [18] presented the CPU performance
    of Shifter with HPGMG-FE (MPI implementation) benchmarks [51] on Cray XC30 (192
    cores, 24 cores per compute node) where the performance margin between Shifter
    container and bare metal is unnoticeable. Comparison is also given for MPI with
    implementation in C++ and Python using a custom benchmark. The authors observed
    that it could take over 30 minutes to import the Python modules when running natively
    with 1,000 processes. Each process of a Python application imports modules from
    the filesystem on each node. Accesses to many small files on an HPC filesystem
    using many processes can be extremely slow comparing with the accesses to a few
    large files. The containerised benchmark has already included all the modules
    in its image that is mounted as a single file on each node, therefore, Shifter
    container outperforms the native execution in this case. Bahls [40] also evaluated
    the execution time of Shifter on Cray XC and Cray XE/XK systems exploiting Cray
    HSN (High Speed Network). Their results showed that Shifter gave comparable performance
    to bare metal. The study in [22] compared the performance of Shifter and Singularity
    against bare metal in terms of computation time using two biological use cases
    on three types of supercomputer CPU architectures: Intel Skylake, IBM Power9 and
    Arm-v8. Containerised applications can scale at the same rate as the bare-metal
    counterparts. However, the authors also observed that with a small number of MPI
    ranks, containers should be built as generic as possible, per contra, when it
    comes to a large number of cores, containers need to be tuned for the hosts. Without
    performance comparison with bare-metal applications, the work in [27] studied
    the CPU, memory, network and I/O performance of Charliecloud, Podman and Singularity.
    All the containers behave similarly with respect to the CPU and memory usage.
    Charliecloud and Singularity have comparable I/O performance. Charliecloud incurs
    large overhead on Lustre''s MDS (Metadata Server) and OSS (Object Storage Server)
    due to its bare tree structure. Comparing with the structures of shared layers
    (as in Docker), this structure needs to access a large number of individual files
    from the image tree from Lustre. Consequently, it causes network overhead when
    data is transmitted from the client node over the network at container start-up
    time. Similarly, as Singularity is stored as a single file on Lustre, a large
    amount of data needs to be loaded at starting point resulting in a data transmission
    spike on network. SARUS has shown strong scaling capability on Cray XC systems
    with hybrid GPU and CPU nodes [35]. The performance difference between SARUS and
    bare metal is less than 0.5% up to 8 nodes and 6.2% up to 256 nodes. No specific
    metrics are given in terms of GPU, though GPU has been used as accelerators. C.
    Section Highlights Containers are introduced to HPC systems, as they enable environment
    customisation for users, which offers the solutions to application compatibility
    issues. This is particularly important on HPC systems that are typically inflexible
    for environment modifications. Notably, HPC container engines are designed to
    meet the high-security requirements on HPC systems. Multiple prevailing engines
    have been described in this section, they share some common features: Non-root
    privileges; Often can convert Docker images to their own image formats; Supports
    of MPI that are typical HPC applications; Use host network rather than pluggable
    network drivers. Yet differences exist in their image formats. Layered image format
    is seen in Docker (UDocker wraps Docker image layers to a local directory), which
    is executed by pulling the image layers that have not been previously downloaded
    on the host. HPC container images are stored in a single directory or file which
    can be transferred to the compute nodes easily avoiding the pulling operations
    that require network access. HPC container engines show various ways to incorporate
    well-tuned libraries targeting for the hosts in order to achieve optimised performance,
    e.g., OCI hooks (SARUS), injecting host files into images (Charliecloud). Section
    III-B aims to give examples that can provide general advices on how to build the
    container images to maximise performance. Clearly, performance loss can occur
    in certain cases which are summarised in the second column of Table III. SECTION
    IV. Container Orchestration Orchestration under the context herein means automated
    configuration, coordination and management of Cloud or HPC systems. In theory,
    HPC workload manager can be also addressed as orchestrator, however, this article
    takes the former term as it is the custom terminology that has been long-used
    and widely understood in the HPC area. The driving factors that push HPC workload
    managers and Cloud orchestrators to be developed in different directions can be
    multiple. This will be discussed at the end of this section (Section IV-D). However,
    first it is important to understand the mechanisms of HPC workload managers (Section
    IV-A) and Cloud orchestrators (Section IV-B). Mostly, container orchestration
    for HPC systems either relies on the orchestration strategies of the existing
    Cloud orchestrators or exploits the mechanisms of current HPC workload managers
    or software tools. This point will be depicted in Section IV-C. A. Workload Managers
    for HPC Systems Cloud aims to exploit economy of scale by consolidating applications
    into the same hardware [16] and the hardware resources can be easily extended
    based on user demands. In contrast, HPC centres have large-scale hardware resources
    available and reserve computing resources exclusively for users. Table V underscores
    the main differences between HPC workload managers and Cloud orchestrators. A
    typical HPC system is managed by a workload manager. A workload manager comprises
    a resource manager and a job scheduler. A resource manager [52] allocates resources
    (e.g., CPU and memory), schedules jobs and guarantees no interference from other
    user processes. A job scheduler determines the job priorities, enforces resource
    limits and dispatches jobs to available nodes [53]. TABLE V Comparison of HPC
    Workload Managers (Section IV-A) and Cloud Orchestrators (Section IV-2) HPC workload
    managers incorporate a big family, such as PBS [54], Spectrum LSF [55], Grid Engine
    [56], OAR [57] and Slurm [58]. Slurm and PBS are two main-stream workload managers.
    The workload managers shares some common features: a centralised scheduling system,
    a queuing system and static resource management mechanisms, which will be detailed
    in this section. 1) PBS PBS stands for Portable Batch System which includes three
    versions: OpenPBS, PBS Pro and TORQUE. OpenPBS is open-source and TORQUE is a
    fork of OpenPBS. PBS Pro is dual-licensed under an open-source and commercial
    license. The structure of a TORQUE-managed cluster consists of a head node and
    many compute nodes as illustrated in Fig. 3 where only three compute nodes are
    shown. The head node (coloured in blue in Fig. 3) controls the entire TORQUE system.
    A pbs_server daemon and a job scheduler daemon are located on the head node. The
    batch job is submitted to the head node (in some cases, the job is first submitted
    to a login node and then transferred to the head node). A node list that records
    the configured compute nodes is maintained on the head node. The architecture
    of this kind as shown in Fig 3 represents the fundamental cluster structure of
    main-stream HPC workload managers. The procedure of job submission on TORQUE is
    briefly described as follows: The job is submitted to the head node by the command
    qsub. A job is normally written in the format of a PBS script. A job ID is returned
    to the user as the standard output of qsub. Fig. 3. TORQUE structure. pbs_server
    , scheduler and pbs_mom are the daemons running on the nodes. Mother superior
    is the first node on the node list (on step4). Show All The job record, which
    incorporates a job ID and the job attributes, is generated and passed to pbs_server
    . pbs_server transfers the job record to the job scheduler daemon. The job scheduler
    daemon adds the job into a job queue and applies a scheduling algorithm to it
    (e.g., FIFO: First In First Out) which determines the job priority and its resource
    assignment. When the scheduler finds the list of nodes for the job, it returns
    the job information to pbs_server . The first node on this list becomes the mother
    superior and the rest are called sister MOMs or sister nodes. pbs_server allocates
    the resources and passes the job control as well as execution information to the
    pbs_mom daemon installed on the mom superior node instructing to launch the job
    on the assigned compute nodes. The pbs_mom daemons on the compute nodes manage
    the execution of jobs and monitor resource usage. pbs_mom will capture all the
    outputs and direct them to stdout and stderr which are written into the output
    and error files and are copied to the designated location when the job completes
    successfully. The job status (completed or terminated) will be passed to pbs_server
    by pbs_mom . The job information will be updated. In TORQUE, nodes are partitioned
    into different groups called queues . In each queue, the administrator sets limits
    for resources such as walltime and job size. This feature can be useful for job
    scheduling in a large HPC cluster where nodes are heterogeneous or certain nodes
    are reserved for special users. This feature is commonly seen in HPC workload
    managers. TORQUE has a default scheduler FIFO, and is often integrated with a
    more sophisticated job scheduler, such as Maui [59]. Maui is an open source job
    scheduler that provides advanced features such as dynamic job prioritisation,
    configurable parameters, extensive fair share capabilities and backfill scheduling.
    Maui functions in an iterative manner like most job schedulers. It starts a new
    iteration when one of the following conditions is met: (1) a job or resource state
    alters; (2) a reservation boundary event occurs; (3) an external command to resume
    scheduling is issued; (4) a configuration timer expires. In each iteration, Maui
    follows the below steps [60]: Obtain resource records from TORQUE; Fetch workload
    information from TORQUE; Update statistics; Refresh reservations; Select jobs
    that are eligible for priority scheduling; Prioritise eligible jobs; Schedule
    jobs by priority and create reservations; Backfill jobs. Despite an abundance
    of algorithms, only a few scheduling strategies are practically in use by job
    schedulers. Backfilling scheduling [61] allows jobs to take the reserved job slots
    if this action does not delay the start of other jobs having reserved the resources,
    thus allowing large parallel jobs to execute and avoiding resource underutilisation.
    Differently, Gang scheduling [62] attempts to take care of the situations when
    the runtime of a job is unknown, allowing smaller jobs to get fairer access to
    the resources. Both scheduling strategies are also seen in SLURM and backfilling
    can be also found in LSF. 2) SLURM The structure of a SLURM (Simple Linux Utility
    for Resource Management) [58] managed cluster is composed of one or two SLURM
    servers and many compute nodes. Its procedure of job submission is similar to
    that of TORQUE. Fig. 4 illustrates the structure of SLURM. Its server hosts the
    slurmctld daemon which is responsible for cluster resource and job management.
    SLURM servers and the corresponding slurmctld daemons can be deployed in an active/passive
    mode in order to provide services of high reliability for computing clusters.
    Each compute node hosts one instance of the slurmd daemon, which is responsible
    for job staging and execution. There are additional daemons, e.g., slurmdbd which
    allows to collect and record accounting information for multiple SLURM-managed
    clusters and slurmrestd that can be used to interact with SLURM through a REST
    API (RESTful Application Programming Interface). The SLURM resource list is held
    as a part of the slurm.conf file located on SLURM server nodes, which contains
    a list of nodes including features (e.g., CPU speed and model, amount of memory)
    and configured partitions (named queue in PBS) including partition names, list
    of associated nodes and job priority. Fig. 4. SLURM structure. Show All Both PBS
    and SLURM have little (if at all) dedicated supports for container workloads.
    Containers are only scheduled as conventional HPC workloads, e.g lacking of load-balancing
    supports. 3) Spectrum LSF IBM platform Load Sharing Facility (LSF), targeted for
    enterprises, is designed for distributed HPC deployments. LSF is based on the
    Utopia job scheduler [55] developed at the University of Toronto. Its Session
    Scheduler runs and manages short-duration batch jobs, which enables users to submit
    multiple tasks as a single LSF job, consequently reduces the number of job scheduling
    decisions. Session Scheduler can efficiently share resources regardless of job
    execution time and can make thousands of scheduling decisions per second. These
    capabilities create a focus on throughput which is often critical for HPC workloads.
    Fig. 5 illustrates the structure of LSF. Its license scheduler allows to make
    policies that control the way software licenses are shared among users within
    an organisation. Jobs are submitted via the command line interface, API or IBM
    platform application centre. Job submission carries similar procedure as in TORQUE.
    Fig. 5. Spectrum LSF structure. Show All LSF supports container workloads: Docker,
    Singularity and Shifter. LSF configures container runtime control in the application
    profile4 that is managed by the system administrator. Users do not need to consider
    which containers are used for their jobs, instead only need to submit their jobs
    to the application profile and LSF automatically manages the container runtime
    control. Section IV-C3 elaborates this feature in more details. B. Orchestration
    Frameworks on Cloud Cloud clusters often include orchestration mechanisms to coordinate
    tasks and hardware resources. Cloud has evolved mature orchestrators to manage
    containers efficiently. Container orchestrators can offer [11], [15], [37]: Resource
    limit control. Reserve a specific amount of CPUs and memory for a container, which
    restrains interference from other containers and provides information for scheduling
    decisions; Scheduling. It determines the policies that optimise the placement
    of containers on nodes; Load balancing. It distributes workloads among container
    instances; Health check. It verifies if a faulty container needs to be destroyed
    or replaced; Fault tolerance. It allows to maintain a desired number of containers;
    Auto-scaling. It automatically adds and removes containers. Additionally, a container
    orchestrator should also simplify networking, enable service discovery and support
    continuous deployment [63]. 1) Kubernetes Kubernetes originally developed by Google
    is among the most popular open-source container orchestrators, which has a rapidly
    growing community and ecosystem with numerous platforms being developed upon it.
    The architecture of Kubernetes comprises a master node and a set of worker nodes.
    Kubernetes runs containers inside pods that are scheduled to run either on master
    or worker nodes. A pod can include one or multiple containers. Kubernetes provides
    its services via deployments that are created by submission of yaml files. Inside
    a yaml file, users can specify services and computation to perform on the cluster.
    A user deployment can be performed either on the master node or the worker nodes.
    Kubernetes is based on a highly modular architecture which abstracts the underlying
    infrastructure and allows internal customisation, such as the deployment of software-defined
    networks or storage solutions. It also supports various big-data frameworks, such
    as Hadoop MapReduce [64], Spark [65] and Kafka [66]. Kubernetes incorporates a
    powerful set of tools to control the life cycle of applications, e.g., parameterised
    redeployment in case of failures and state management. Furthermore, it supports
    software-defined infrastructures5 [67] and resource disaggregation [68] by leveraging
    container-based deployments and particular drivers (e.g., Container Runtime Interface
    driver, Container Storage Interface driver and Container Network Interface driver)
    based on standardised interfaces. These interfaces enable the definition of abstractions
    for fine-grain control of computation, states and communication in multi-tenant
    Cloud environments along with optimal usage of the underlying hardware resources.
    Kubernetes incorporates a scheduling system that permits users to specify different
    schedulers for each job. The scheduling system makes the decisions based on two
    steps before the actual scheduling operations: Node filtering. The scheduler locates
    the node(s) that fit(s) the workload, e.g., a pod is specified with node affinity,
    therefore, only certain nodes can meet the affinity requirements or some nodes
    may not include enough CPU resources to serve the request. Normally the scheduler
    does not traverse the entire node list, instead it selects the one/ones detected
    first. Node priority calculation. The scheduler calculates a score for each node,
    and the highest scoring node will run that pod. Kubernetes has started being utilised
    to assist HPC systems in container orchestration (Section IV-C). 2) Docker Swarm
    Docker Swarm [69] is built for the Docker engine. It is a much simpler orchestrator
    comparing with Kubernetes, e.g., it offers less rich functionalities, limited
    customisations and extensions. Docker Swarm is hence lightweight and suitable
    for small workloads. In contrast, Kubernetes is heavyweight for individual developers
    who may only want to set up an orchestrator for simplistic applications and perform
    infrequent deployments. Nevertheless, Docker Swarm still has its own API, and
    provides filtering, scheduling and load-balancing. API is a strong feature commonly
    used in Cloud orchestrators, as it enables applications or services to talk to
    each other and provides connections with other orchestrators. The functionalities
    of Docker Swarm may be applied to perform container orchestration on HPC systems
    as detailed in Section IV-C3. 3) Apache Mesos and YARN Apache Mesos [70] is a
    cluster manager that provides efficient resource isolation and sharing across
    distributed applications or frameworks. Mesos removes the centralised scheduling
    model that would otherwise require to compute global schedules for all the tasks
    running on the different frameworks connected to Mesos. Instead, each framework
    on a Mesos cluster can define its own scheduling strategies. For instance, Mesos
    can be connected with MPI or Hadoop [71]. Mesos utilises a master process to manage
    slave daemons running on each node. A typical Mesos cluster includes 3 ∼ 5 masters
    with one acting as the leader and the rest on standby. The master controls scheduling
    across frameworks through resource offers that provide resource availability of
    the cluster to slaves. However, the master process only suggests the amount of
    resources that can be given to each framework according to the policies of organisations,
    e.g fair sharing. Each framework rules which resources or tasks to accept. Once
    a resource offer is accepted by a framework, the framework passes Mesos a description
    of the tasks. The slave comprises two components, i.e., a scheduler registered
    to the master to receive resources and an executor process to run tasks from the
    frameworks. Mesos is a non-monolithic scheduler which acts as an arbiter that
    allocates resources across multiple schedulers, resolves conflicts, and ensures
    fair distribution of resources. Apache YARN (Yet Another Resource Negotiator)
    [72] is a monolithic scheduler which was developed in the first place to schedule
    Hadoop jobs. YARN is designed for long-running batch jobs and is unsuitable for
    long-running services and short-lived interactive queries. Mesosphere Marathon6
    is a container orchestration framework for Apache Mesos. Literature has seen the
    usage of Mesos together with Marathon in container orchestration on HPC systems
    as detailed in Section IV-C3. 4) Ansible Ansible [73] is a popular software orchestration
    tool. More specifically, it handles configuration management, application deployment,
    cloud provisioning, ad-hoc task execution, network automation and multi-node orchestration.
    The architecture of Ansible is simple and flexible, i.e., it does not require
    a special server or daemons running on the nodes. Configurations are set by playbooks
    that utilise yaml to describe the automation jobs, and connections to other nodes
    are via ssh. Nodes managed by Ansible are grouped into inventories that can be
    defined by users or drawn from different Cloud environments. Ansible is adopted
    by the SODALITE framework (Section IV-C4) as a key component to automatically
    build container images. 5) OpenStack OpenStack [74] is mostly deployed as infrastructure-as-a-service
    (IaaS)7 [75] on Cloud. It can be utilised to deploy and manage cloud-based infrastructures
    that support various use cases, such as web hosting, Big Data projects, software
    as a service (SaaS) [76] delivery and deployment of containers, VMs or bare-metal.
    It presents a scalable and highly adaptive open source architecture for Cloud
    solutions and helps to leverage hardware resources [77]. It also manages heterogeneous
    compute, storage and network resources. Together with its support of containers,
    container orchestrators such as Docker Swarm, Kubernetes and Mesos, Openstack
    enables the possibilities to quickly deploy, maintain, and upgrade complex and
    highly available infrastructures. OpenStack is also used in HPC communities to
    provide IaaS to end-users, enabling them to dynamically create isolated HPC environments.
    Academia and industry have developed a plethora of Cloud orchestrators. This article
    only reviews the ones that are mostly relevant to the HPC communities and the
    ones that have seen their usage in container orchestration for HPC systems, and
    the rest is out of the scope herein. C. Bridge Orchestration Strategies Between
    HPC and Cloud There are numerous works in literature [11], [78], [79], [80] on
    container orchestration for Cloud clusters, however, they are herein out of the
    scope. This section reviews the works that have been performed on the general
    issues of bridging the gap between conventional HPC and service-oriented infrastructures
    (Cloud). Overall, the state-of-the-art works on container orchestration for HPC
    systems fall into four categories as illustrated in Fig. 6. Added functionalities
    to HPC workload managers. It relies on workload managers for resource management
    and scheduling; meanwhile adopts additional software such as MPI for container
    orchestration. Fig. 6. The four types of container orchestration on HPC systems.
    Show All Connector between Cloud and HPC. Containers are scheduled from Cloud
    clusters to HPC clusters. This architecture isolates the HPC resources from Cloud
    so as to ensure HPC environment security; meanwhile offers application developments
    with flexible environments and powerful computing resources. Cohabitation. Workload
    managers and Cloud orchestrators co-exist on an HPC cluster, such as IBM LSF-Kubernetes.
    This gives a direction for the provision of HPC resources as services. In practice,
    the HPC workload managers and Cloud orchestrators do not coexist in one cluster.
    Meta-orchestration. An additional orchestrator is implemented on top of the Cloud
    orchestrator and HPC workload manager. There are pros and cons of the above four
    categories, which are outlined in Table VI. In addition, a research and engineering
    trend [12], [30], [81], [82], [83] is to move HPC applications to Cloud, as Cloud
    provides flexible and cost-effective services which are favoured by small-sized
    or middle-sized business. Beltre et al. [84] proposed to manage HPC applications
    by Kubernetes on a Cloud cluster with powerful computing resources and InfiniBand,
    which demonstrated comparable performance in containerised and bare-metal environments.
    The approach of this kind may be extended to HPC systems, however, remains unpractical
    for HPC centres to completely substitute their existing workload managers. TABLE
    VI A List of the Related Work on Container Orchestration for HPC Systems 1) Added
    Functionalities to WLM A potential research direction is to complement workload
    managers with container orchestration or make use of the existing HPC software
    stacks. Wofford et al. [85] simply adopt Open Runtime Environment (orted) reference
    implementation from Open MPI to orchestrate container launch suitable for arbitrary
    batch schedulers. Julian et al. [86] proposed their prototype for container orchestration
    in an HPC environment. A PBS-based HPC cluster can automatically scale up and
    down as load demands by launching Docker containers using the job scheduler Moab
    [98]. Three containers serve as the front-end system, scheduler (it runs PBS and
    Moab inside) and compute node (launches pbs_mom daemon, see Section IV-A1). More
    compute node containers are scheduled when there is no sufficient number of physical
    nodes. Unused containers are destroyed via external Python scripts when jobs complete.
    This approach may offer a solution for resource elasticity on HPC systems (Section
    V-B6). Similarly, an early study [87] described two models that can orchestrate
    Docker containers using an HPC workload manager. The former model launches a container
    to behave as one compute node which holds all assigned processes, whilst the latter
    boots a container per process by MPI launchers. The latter work seems to be outdated
    as to MPI applications which can be now automatically scaled with Singularity
    support. 2) Connector Between Cloud and HPC Cloud technologies are evolving to
    be able to support complex applications of HPC, Big Data and AI. Nevertheless,
    the applications with intensive computation and high inter-processor communication
    could not scale well, particularly due to the lack of low latency networks (e.g.,
    InfiniBand) and the usage of network virtualisation for network isolation. A research
    and development trend is to converge HPC and Cloud in order to take advantage
    of the resource management and scheduling of both HPC and Cloud infrastructures
    with minimal intrusion to HPC environments. Furthermore, the software stack and
    workflows in Cloud and HPC are usually developed and maintained by different organisations
    and users with various goals and methodologies, hence a connector between HPC
    and Cloud systems would bridge the gap and solve compatibility problems. Zhou
    et al. [88], [89], [90], [91] described the design of a plugin named Torque-Operator
    that serves as the key component to its proposed hybrid architecture. The containerised
    AI applications are scheduled from the Kubernetes-managed Cloud cluster to the
    TORQUE-managed HPC cluster where the performance of the compute-intensive or data-demanding
    applications can be significantly enhanced. This approach is less intrusive to
    HPC systems, however, its architecture shows one drawback: the latency of the
    network bridging the Cloud and HPC clusters can be high, when a large amount of
    data needs to be transferred in-between. DKube8 is a commercial software that
    is able to execute a wide range of AI/ML components scheduled from Kubernetes
    to SLURM. The software comprises a Kubernetes plugin and a SLURM Plugin. The former
    is represented as a hub that runs MLOps (Machine Learning Operations) management
    and associated Kubernetes workloads, while the latter connects to SLURM. 3) Cohabitation
    Liu et al. [92] showed how to dynamically migrate computing resources between
    HPC and OpenStack clusters based on demands. At a higher level, IBM has demonstrated
    the ability to run Kubernetes pods on Spectrum LSF where LSF acts as a scheduler
    for Kubernetes. An additional Kubernetes scheduler daemon needs to be installed
    into the LSF cluster, which acts as a bridge between LSF and the Kuberentes server.
    Kubelet will execute and manage pod lifecycle on target nodes in the normal fashion.
    IBM released LSF connector to Kubernetes, which makes use of the core LSF scheduling
    technologies and Kubernetes API functionalities. Kubernetes needs to be installed
    in a subset of the LSF managed HPC cluster. This architecture allows users to
    run Kubernetes and HPC batch jobs on the same infrastructure. The LSF scheduler
    is packed into containers and users submit jobs via kubectl. The LSF scheduler
    listens to the Kubernetes API server and translates pod requests into jobs for
    the LSF scheduler. This approach can add additional heavy workloads to HPC systems,
    as Kubernetes relies deployments of services across clusters to perform load balancing,
    scheduling, auto scheduling, etc. Piras et al. [93] implemented a method that
    expanded Kubernetes clusters with HPC clusters through Grid Engine. Submission
    is performed by PBS jobs to launch Kubernetes jobs. Therefore, HPC nodes are added
    to Kubernetes clusters by installing Kubernetes core components (i.e., kubeadm
    and Kubelet) and Docker container engine. On HPC, especially HPC production systems
    in HPC centres, adding new software packages that require using root privileges
    can cause security risks and alter the working environments of current users.
    The security issues will be further elaborated in Section V-A4. Khan et al. [1]
    proposed to containerise HPC workloads and install Mesos and Marathon (Section
    IV-B3) on HPC clusters for resource management and container orchestration. Its
    orchestration system can obtain the appropriate resources satisfying the needs
    of requested services within defined Quality-of-Service (QoS) parameters, which
    is considered to be self-organised and self-managed meaning that users do not
    need to specifically request resource reservation. Nevertheless, this study has
    not shown insight into novel strategies of container orchestration for HPC systems.
    Wrede et al. [94] performed their experiments on HPC clusters using Docker Swarm
    as the container orchestrator for automatic node scaling and using C++ algorithmic
    skeleton library Muesli [99] for load balance. Its proposed working environment
    is targeted for Cloud clusters. Usage of Docker cannot be easily extended to HPC
    infrastructures especially to HPC production systems due to the security risks.
    4) Meta-Orchestration Croupier [95] is a plugin implemented on Cloudify9 server
    that is located at a separate node in addition to the nodes that are managed by
    an HPC workload manager and a Cloud orchestrator. Croupier establishes a monitor
    to collect the status of every infrastructure and the operations (e.g., status
    of the HPC batch queue). Croupier together with Cloudify, can orchestrate batch
    applications in both HPC and Cloud environments. Similarly, Di Nitto et al. [96]
    presented the SODALITE10 framework by utilising XOpera11 to manage the application
    deployment in heterogeneous infrastructures. Colonnelli et al. [97] presented
    a proof-of-concept framework (i.e., Streamflow) to execute workflows on top of
    the hybrid architecture consisting of Kubernetes-managed Cloud and OCCAM [100]
    HPC cluster. D. Section Highlights HPC workload managers and Cloud orchestrators
    have distinct ways to manage clusters mainly because of their types of workloads
    and hardware resource availabilities. Table V summaries the differences of key
    features between HPC workload managers and Cloud Orchestrators. Typical HPC jobs
    are large workloads with long but ascertainable execution time and large throughput.
    HPC jobs are often submitted to a batch queue within a workload manager where
    jobs wait to be scheduled from minutes to days. Per contra, job requests can be
    granted immediately on Cloud as resources are available on demand. Batch-queuing
    is insufficient to satisfy the needs of Cloud communities: most of jobs are short
    in duration and the Cloud services are persistently long-running programs. Most
    of the HPC workload managers support Checkpointing that allows applications to
    save the execution states of a running job and restart the job from the checkpointing
    when a crash happens. This feature is critical for an HPC application with execution
    time typically from hours to months. Because it enables the application to recover
    from error states or resume from the state when it was previously terminated by
    the workload manager when its walltime limit had been reached or resource allocation
    had been exceeded. In contrary, jobs on Cloud, which are often micro-service programs,
    are usually relaunched in case of failures [101]. A container orchestrator offers
    an important property, i.e., container status monitoring. This is practical for
    long-running Cloud services, as it can monitor and replace unhealthy containers
    per desired configuration. HPC systems do not offer the equivalence of container
    pod which bundle performance monitoring services with the application itself as
    in Cloud systems [13]. Additionally, HPC workload managers often do not provide
    capabilities of application elasticity or necessary API at execution time, however,
    these capabilities are important for task migration and resource allocation changes
    at runtime on Cloud [102]. Section IV-C has reviewed the approaches to address
    the issues of container orchestration on HPC systems, which are summarised in
    Table VI. Overall, a container orchestrator on its own does not address all the
    requirements of HPC systems [3], as a result cannot replace existing workload
    managers in HPC centres. An HPC workload manager lacks micro-service support and
    deeply-integrated container management capabilities in which container orchestrators
    manifest their efficiency. SECTION V. Research Challenges and Vision The distinctions
    between Cloud and HPC clusters are diminishing, especially with the trend of HPC
    Clouds in industry [103]. HPC Cloud is becoming an alternative to on-premise HPC
    clusters for executing scientific applications and business analytics models [16].
    Containerisation technologies help to ease the efforts of moving applications
    between Cloud and HPC. Nevertheless, not all applications are suitable for containerisation.
    For instance, in the typical HPC applications such as weather forecast or modelling
    of computational fluid dynamics, any virtualisation or high-latency networks can
    become the bottlenecks for performance. Containerisation in HPC still faces challenges
    of different folds (Section V-A). Interest in using containers on HPC systems
    is mainly due to the encapsulation and portability that yet may trade off with
    performance. In practice, containers deployed on HPC clusters often have large
    image size and as a result each HPC node can only host a few containers that are
    CPU-intensive and memory-demanding. In addition, implementation of AI frameworks
    such as TensorFlow and PyTorch [104] typically also have large container image
    size. Architecture of HPC containers should be able to easily integrate seamlessly
    with HPC workload managers. The research directions (Section V-B) which can be
    envisioned are not only to adapt the existing functionalities from Cloud to HPC,
    but to also explore the potentials of containerisation so as to improve the current
    HPC systems and applications. A. Challenges and Open Issues Although containerisation
    enables compatibility, portability and reproducibility, containerised environments
    still need to match the host architecture and exploit the underlying hardware.
    The challenges that containerisation faces on HPC systems are in three-fold: compatibility,
    security and performance. Some issues still remain as open questions. Table VII
    summarises the potential solutions to the research challenges and the open questions
    that will be discussed in this section. TABLE VII Overview of Research Challenges
    and Potential Solutions 1) Library Compatibility Issues Mapping container libraries
    and their dependencies to the host libraries can cause incompatibility. Glibc
    [105], which is an implementation of C standard library that provides core supports
    and interfaces to kernel features, can be a common library dependency. The version
    of Glibc on the host may be older or newer than the one in the container image,
    consequently introducing symbol mismatches. Additionally, when the container OS
    (e.g., Ubuntu 18.04) and the host OS are different (e.g., CentOS 7), it is likely
    that some kernel ABI are incompatible, which may lead to container crashes or
    abnormal behaviours. This issue can also occur to MPI applications. As a result
    users must either build an exact version of the host MPI or have the privilege
    to mount the host MPI dependency path into the container. A research direction
    to handle library mismatches between container images and hosts is to implement
    a container runtime library at a lower level. For instance, Nvidia implemented
    the library libnvidia-container12 that manages driver or library matching at container
    runtime, i.e., using a hook interface to inject and/or activate the correct library
    versions. However, the libnvidia-container library can be only applied to Nividia
    GPUs. A significant modification of this library code is likely to be needed in
    order to be adapted for other GPU suppliers. In practice, such a compatibility
    layer would also require supports from different HPC interconnect and accelerator
    vendors. 2) Compatibility Issues of Container Engines and Images Not all Docker
    images can be converted by HPC container engines to their own formats. Moreover,
    to reuse HPC container implementations between container engines, users need to
    learn different container command lines to build the corresponding images, which
    further complicates adoption of containers for HPC applications. This issue calls
    for container standardisation. OCI is a Linux foundation project that designs
    open standards for container image formats (a filesystem bundle or rootfs) and
    multiple data volume [106]. Some guidelines were proposed in [63], i.e., a container
    should be: Not bound to higher-level frameworks, e.g., an orchestration stack;
    Not tightly associated with any particular vendor or project; Portable across
    a wide variety of OSs, hardware, CPUs, clusters, etc. Unfortunately, this standard
    cannot guarantee that the runtime hooks built for one runtime can be used by another.
    For example, container privileges (e.g., mount host filesystems) assumed by one
    container runtime may not be translated to unprivileged runtimes (e.g., not all
    HPC centres have mount namespace enabled) [107]. 3) Kernel Optimisation In general,
    containers are forbidden by the host to install their own kernel modules for the
    purpose of application isolation [108]. This is a limitation for the applications
    requiring kernel customisation, because the kernels of their HPC hosts cannot
    be tuned and optimised. Shen et al. [108] proposed an Xcontainer to address this
    issue by tuning the Linux kernel into library OS that supports binary compatibility.
    This functionality is yet to be explored in HPC containers. 4) Security Issues
    Containers face three major threats [109]: Privilege Escalation. Attackers gain
    access to hosts and other containers by breaking out of their current containers.
    Denial-of-Service (DoS). An attack causes services to become inaccessible to users
    by disruption of a machine or network resources. Information Leak. Confidential
    details of other containers are leaked and utilised for further attacks. Multiple
    or many containers share a host kernel, therefore, one container may infect other
    containers. In this case, a container does not reduce attack surfaces, but rather
    brings multiple instances of attack surfaces. For example, starting from version
    V3.0, Singularity has added Cgroups support that allows users to limit the resources
    consumed by containers without the help from a batch scheduling system (e.g.,
    TORQUE). This feature helps to prevent DoS attacks when a container seizes control
    of all available system resources which prohibits other containers from operating
    properly. Execution of HPC containers (including the Docker Engine starting from
    v19.03) does not require root privileges on the host. Containers in general adopt
    namespaces to isolate resources among users and map a root user inside a container
    to a non-root user on the host. The User namespace nevertheless is not a panacea
    to resolve all problems of resource isolation. User exposes code in the kernel
    to non-privileged users, which was previously limited to root users. A container
    environment is generated by users, and it is likely that some software inside
    a container may be embedded with security vulnerabilities. Root users inside a
    container may escalate their privileges via application level vulnerability. This
    can bring security issues to the kernel that does not account for mapped PIDs/GIDs.
    This issue can be addressed in two ways: (1) avoiding root processes inside HPC
    containers; (2) installing container engines with user permission instead of sudo
    installation. Security issues of the user namespace continue to be discovered
    even in the latest version of Linux kernels. Therefore, many HPC production centres
    have disabled the configuration of this namespace, which prevents usage of almost
    any state-of-the-art HPC containers. How to address the risks of using namespaces
    still remains an open question. 5) Performance Degradation GPU and accelerators
    often require customised or proprietary libraries that need to be bound to container
    images so as to leverage performance. This operation is at the cost of portability
    [107]. It is de facto standard to utilise the optimised MPI libraries for HPC
    interconnects, such as InfiniBand and Slingshot [110], and it is likely that the
    container performance degrades in a different HPC infrastructure [22] (see Section
    III-B). There is no simple solution to address this issue. Another example presented
    in [111] identified the performance loss due to increasing communication cost
    of MPI processes. This occurs when the number of containers (MPI processes running
    inside containers) rises on a single node, e.g., point to point communication
    (MPI_Irecv, MPI_Isend), polling of pending asynchronous messages (MPI_Test) and
    collective communication (MPI_Allreduce). B. Research and Engineering Opportunities
    Research studies should continue working on solutions to the open question identified
    in Section V-A. This section discusses current research and engineering directions
    that are interesting, yet still need further development. This section also identifies
    new research opportunities that yet need to be explored. The presentation of this
    section is arranged from short-term vision to long-term efforts. Table VIII summarises
    the potentials discovered in literature and the prospects given by the authors.
    TABLE VIII Future Directions of Research and Engineering 1) Containerisation of
    AI in HPC Model training of AI/DL applications can immensely benefit from the
    compute power (GPU or CPU), storage and security [112] of HPC clusters in addition
    to the superior GPU-aware scheduling and features of workflow automation provided
    by workload managers. The trained models are subsequently deployed on Cloud for
    scalability at low cost and on HPC for computation speed. Exploiting HPC infrastructures
    for ML/DL training is becoming a topic of increasing importance [113]. For example,
    Fraunhofer13 has developed the software framework Carme14 that combines established
    open source ML and Data Science tools with HPC backends. The execution environments
    of the tools are provided by predefined Singularity containers. AI applications
    are usually developed with high-level scripting languages or frameworks, e.g.,
    TensorFlow and PyTorch, which often require connections to external systems to
    download a list of open-source software packages during execution. For instance,
    an AI application written in Python cannot be compiled into an executable that
    has included all the dependencies ready for execution as in C/C++. Therefore,
    the developers need flexibility to customise the execution environments. Since
    HPC environments, especially on HPC production systems, are often based on closed-source
    applications and their users have restricted account privileges and security restrictions
    [6], deployment of AI applications on HPC infrastructures is challenging. Besides
    the predefined module environments or virtual environments (such as Anaconda),
    containerisation can be an alternative candidate, which enables easy transition
    of AI workloads to HPC while fully taking advantage of HPC hardware and the optimised
    libraries of AI applications without compromising security. Huerta et al. [114]
    recommend three guidelines for containerisation of AI applications for HPC centres:
    Provide up-to-date documentation and tutorials to set up or launch containers.
    Maintain versatile and up-to-date base container images that users can clone and
    adapt, such as a container registry (see Section V-B2). Give instructions on installation
    or updates of software packages into containers. The AI program depends on distributed
    training software, such as Horovod [115], which then depends on system architecture
    and specific versions of software packages such as MPI. Increasing amount of new
    software frameworks are being developed using containerisation technologies to
    facilitate deployment of AI applications on HPC systems. Further research is still
    needed to improve scalability and enable out-of-box usage. 2) HPC Container Registry
    Container registry is a useful repository to provide pre-built container images
    that can be accessed easily either by public or private users by pulling images
    to the host directly. It is portable to deploy applications in this way on Cloud
    clusters. Accesses to external networks are often blocked in HPC centres, so users
    need to upload images onto the clusters manually. One solution is to set up a
    private registry within the HPC centres that offer pre-built images suitable for
    the targeted systems and architectures. A container registry is also a way to
    ensure container security. It is a good security practice to ensure that images
    executed on the HPC systems are signed and pulled from a trusted registry. Scanning
    vulnerabilities on the registry should be regularly performed. To simplify usage,
    the future work can enable HPC workload managers to boot the default containers
    on the compute nodes (by pulling images from the private registry) which match
    the environments with all the required libraries and configuration files of user
    login nodes where users implement their own workflows and submit their jobs. The
    jobs should be started without user awareness of the presence of containers and
    without additional user intervention. 3) Linux Namespace Guidelines The set of
    Linux namespaces used within an implementation depends on the policies of HPC
    centres [116]. HPC centres should provide clear instructions on the availabilities
    of namespaces. For example, different user groups may have different namespaces
    enabled or disabled. A minimal set of namespaces should be enabled for a general
    user group: mount and user, which are suitable for node-exclusive scheduling.
    PID and Cgroups should be provided to restrict resource usage and enforce process
    privacy, which are useful for shared-node scheduling. Advanced use cases may require
    additional sets of namespaces. When users submit the container jobs, workload
    managers can start the containers with appropriate namespaces enabled. 4) DevOps
    DevOps aims at integrating efforts of development (Dev) and operations (Ops) to
    automate fast software delivery while ensuring correctness and reliability [117],
    [118]. This concept is influential in Cloud Computing and has been widely adopted
    in industry, as DevOps tools minimise the overhead of managing a large amount
    of micro-services. In HPC environments, typical applications have large workloads,
    hence the usage of DevOps should concentrate on research reproducibility. Nevertheless,
    the off-the-shelf DevOps tools are not well fitted for HPC environments, e.g.,
    the dependencies of MPI applications are too foreign for the state-of-the-art
    DevOps tools. A potential solution is to develop HPC-specific DevOps tools for
    the applications that are built and executed on on-premise clusters [16]. Unfortunately,
    HPC environments are known to be inflexible and typical HPC applications are optimised
    to leverage resources, thereby generation of DevOps workflows can be restricted
    and slow. Such obstacles can be overcome by containerisation, which may provision
    DevOps environments. For instance, Sampedro et al. [119] integrate Singularity
    with Jenkins [120] that brings CICD15 practices into HPC workflows. Jenkins is
    an open-source automation platform for building and deploying software, which
    has been applied at some HPC sites as a general-purpose automation tool. 5) Middleware
    System A middleware system, which bridges container building environments with
    HPC resource managers and schedulers, can be flexible. A middleware system can
    be either located on an HPC cluster or connect to it with secured authentication.
    The main task of the middleware is to perform job deployment, job management,
    data staging and generating non-root container environments [121]. Different container
    engines can be swiftly switched, optimisation mechanisms can be adapted to the
    targeted HPC systems and workflow engines [122] can be easily plugged in. Middleware
    systems can be a future research direction that provides a portable way to enable
    DevOps in HPC centres. 6) Resource Elasticity One major difference between resource
    management on HPC and Cloud is the elasticity [123], i.e., an HPC workload manager
    runs on a fixed set of hardware resources and the workloads of its jobs at any
    point can not exceed the resource capacity, while Cloud orchestrators can scale
    up automatically the hardware resources to satisfy user needs (e.g., AWS spot
    instances). Static reservation is a limitation for efficient resource usages on
    HPC systems [124]. One future direction of containerisation for HPC systems can
    work towards improvement of the elasticity of HPC infrastructure, which can be
    introduced to its workload manager. In [123], the authors presented a novel architecture
    that utilises Kubernetes to instantiate the containerised HPC workload manager.
    In this way, the HPC infrastructure is dynamically instantiated on demand and
    can be served as a single-tenant or multi-tenant environment. A complete containerised
    environments on HPC system may be impractical and much more exploration is still
    needed. 7) Moving Towards Minimal OS Containers may be utilised to partially substitute
    the current HPC software stack. Typical compute nodes on HPC clusters do not contain
    local storage (e.g., hardware disk), therefore lose states after reboots. The
    compute node boots via a staged approach [116]: (1) a kernel and initial RAM disk
    are loaded via a network device; (2) a root filesystem is mounted via the network.
    In a monolithic stateless system, modification of the software components often
    requires system rebooting to completely activate the functions of updates. Using
    containerised software packages on top of a minimal OS (base image) on the compute
    nodes, reduces the number of components in the kernel image, hence decreasing
    the frequency of node reboots. Furthermore, the base image of reduced size also
    simplifies the post-boot configurations that need to run in the OS image itself,
    consequently the node rebooting time is minimised. Additionally, when a failure
    occurs, a containerised service can be quickly replaced without affecting the
    entire system. Long-term research is required on HPC workload managers to control
    the software stack and workloads that are partially native and partially containerised.
    Moreover, it needs to explored whether containerisation of the entire OS on HPC
    systems is feasible. SECTION VI. Concluding Remarks This paper presents a survey
    and taxonomy for the state-of-the-art container engines and container orchestration
    strategies specifically for HPC systems. It underlines differences of containerisation
    on Cloud and HPC systems. The research and engineering challenges are also discussed
    and the opportunities are envisioned. HPC systems start to utilise containers
    as thereof reduce environment complexity. Efforts have been also made to ameliorate
    container security on HPC systems. This article identified three points to increase
    the security level: (1) set on-site container registry, (2) give Linux namespaces
    guidelines (3) and remove root privilege meanwhile avoid permission escalation.
    Ideally, HPC containers should require no pre-installation of container engines
    or installation can be performed without root privileges, which not only meets
    the HPC security requirements but also simplifies the container usability. Containers
    will continue to play a role in reducing the performance gap and deployment complexity
    between on-premise HPC clusters and public Clouds. Together with the advancement
    of low-latency networks and accelerators (e.g., GPUs, TPUs [125]), it may eventually
    reshape the two fields. Containerised workloads can be moved from HPC to Cloud
    so as to temporarily relieve the peak demands and can be also scheduled from Cloud
    to HPC in order to exploit the powerful hardware resources. The research and engineering
    trend are working towards implementation of the present container orchestrators
    within HPC clusters, which however still remains experimental. Many studies have
    been devoted to container orchestration on Cloud, however, it can be foreseen
    that the strategies will be eventually introduced to HPC workload managers. In
    the future, it can be presumed that containerisation will play an essential role
    in application development, improve resource elasticity and reduce complexity
    of HPC software stacks. ACKNOWLEDGMENTS The authors would like to thank Dr. Joseph
    Schuchart for proof-reading the contents. Authors Figures References Citations
    Keywords Metrics Footnotes More Like This Implementing a Hybrid Virtual Machine
    Monitor for Flexible and Efficient Security Mechanisms 2010 IEEE 16th Pacific
    Rim International Symposium on Dependable Computing Published: 2010 I/O for Virtual
    Machine Monitors: Security and Performance Issues IEEE Security & Privacy Published:
    2008 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Software Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Containerization for High Performance Computing Systems: Survey and Prospects'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Keller Tesser R.
  - Borin E.
  citation_count: '2'
  description: OS-level virtualization (containers) has become a popular alternative
    to hypervisor-based virtualization. From a system-administration point-of-view,
    containers enable support for user-defined software stacks, thus freeing users
    of restrictions imposed by the host’s pre-configured software environment. In
    high performance computing (HPC), containers inspire special interest due to their
    potentially low overheads on performance. Moreover, they also bring benefits in
    portability and scientific reproducibility. Despite the potential advantages,
    the adoption of containers in HPC has been relatively slow, mainly due to specific
    requirements of the field. These requirements gave rise to various HPC-focused
    container implementations. Besides unprivileged container execution, they offer
    different degrees of automation of system-specific optimizations, which are necessary
    for optimal performance. When we looked into the scientific literature on containers
    applied to HPC, we were unable to find an up-to-date overview of the state-of-the-art.
    For this reason, we developed this extensive survey, including 93 carefully selected
    works. Overall, based on our survey, we argue that issues related to performance
    overhead are mostly solved. There is, however, a clear trade-off between performance
    and portability, since optimal performance often depends on host-specific optimizations.
    A few works propose solutions to mitigate this issue, but there is still room
    for improvement. Besides, we found surprisingly few works that deal with portability
    between dedicated HPC systems and public cloud platforms.
  doi: 10.1007/s11227-022-04848-y
  full_citation: '>'
  full_text: ">\n\"Your privacy, your choice We use essential cookies to make sure\
    \ the site can function. We also use optional cookies for advertising, personalisation\
    \ of content, usage analysis, and social media. By accepting optional cookies,\
    \ you consent to the processing of your personal data - including transfers to\
    \ third parties. Some third parties are outside of the European Economic Area,\
    \ with varying standards of data protection. See our privacy policy for more information\
    \ on the use of your personal data. Manage preferences for further information\
    \ and to change your choices. Accept all cookies Skip to main content Log in Find\
    \ a journal Publish with us Track your research Search Cart Home The Journal of\
    \ Supercomputing Article Containers in HPC: a survey Published: 27 October 2022\
    \ Volume 79, pages 5759–5827, (2023) Cite this article Download PDF Access provided\
    \ by University of Nebraska-Lincoln The Journal of Supercomputing Aims and scope\
    \ Submit manuscript Rafael Keller Tesser & Edson Borin  1134 Accesses 4 Citations\
    \ 1 Altmetric Explore all metrics Abstract OS-level virtualization (containers)\
    \ has become a popular alternative to hypervisor-based virtualization. From a\
    \ system-administration point-of-view, containers enable support for user-defined\
    \ software stacks, thus freeing users of restrictions imposed by the host’s pre-configured\
    \ software environment. In high performance computing (HPC), containers inspire\
    \ special interest due to their potentially low overheads on performance. Moreover,\
    \ they also bring benefits in portability and scientific reproducibility. Despite\
    \ the potential advantages, the adoption of containers in HPC has been relatively\
    \ slow, mainly due to specific requirements of the field. These requirements gave\
    \ rise to various HPC-focused container implementations. Besides unprivileged\
    \ container execution, they offer different degrees of automation of system-specific\
    \ optimizations, which are necessary for optimal performance. When we looked into\
    \ the scientific literature on containers applied to HPC, we were unable to find\
    \ an up-to-date overview of the state-of-the-art. For this reason, we developed\
    \ this extensive survey, including 93 carefully selected works. Overall, based\
    \ on our survey, we argue that issues related to performance overhead are mostly\
    \ solved. There is, however, a clear trade-off between performance and portability,\
    \ since optimal performance often depends on host-specific optimizations. A few\
    \ works propose solutions to mitigate this issue, but there is still room for\
    \ improvement. Besides, we found surprisingly few works that deal with portability\
    \ between dedicated HPC systems and public cloud platforms. Similar content being\
    \ viewed by others A survey of Kubernetes scheduling algorithms Article Open access\
    \ 13 June 2023 Containerization technologies: taxonomies, applications and challenges\
    \ Article 08 June 2021 The New Hardware Development Trend and the Challenges in\
    \ Data Management and Analysis Article Open access 24 September 2018 1 Introduction\
    \ A real challenge in high performance computing (HPC) environments is to support\
    \ the wide variety of use-cases and software required by users, without overburdening\
    \ them or the system administrators. This is even harder because HPC applications\
    \ commonly have complex dependencies. Moreover, there may be conflicts with other\
    \ applications, and their own dependencies. This issue is aggravated even more\
    \ by the growing computational demands from other fields, such as artificial intelligence\
    \ and big-data, which leads their users to seek the utilization of HPC systems.\
    \ We could amend this problem by veering away from the traditional administrator\
    \ centered software management, and implementing strategies to allow users to\
    \ configure their own customized software environments. On the other hand, this\
    \ places most of the burden on the users, and there are also concerns related\
    \ to system security and stability. Moreover, software portability can still be\
    \ a challenge, but a worth one, since it reduces user effort, by potentially eliminating\
    \ the repeated building and configuration effort associated with moving an application\
    \ to a new system. Other fields have widely employed hypervisor-based virtualization\
    \ to provide customizable software environments, in the form of virtual machines\
    \ (VMs), while also providing portability and security. HPC, however, has shied\
    \ away from this approach, due to performance overheads added by the extra layers\
    \ between applications and hardware. It is worth mentioning that cloud computing\
    \ instances are most often provided in the form of a VM, especially in public,\
    \ commercial, clouds. Therefore, this is arguably the most accessible way to run\
    \ HPC applications in the cloud. In this scenario, the Cloud is a means to provide\
    \ HPC-capable computing resources for users or institutions that do not have access\
    \ to, and cannot afford owning, a dedicated HPC system. A better alternative to\
    \ hypervisor-based virtualization in HPC is operating-system level virtualization\
    \ (containers). In this approach, the application runs inside a container environment,\
    \ which is isolated from the host via a combination of OS-kernel level features,\
    \ such as namespaces and cgroups. Figure 1 illustrates the main differences between\
    \ containers and hypervisor-based VMs. As illustrated, an application in a VM\
    \ has a complete guest OS and a hypervisor between itself and the host OS. Containers,\
    \ on the other hand, execute directly on top of the host OS, sharing the same\
    \ OS kernel, and some of the host’s software. Thus, containers should provide\
    \ reduced execution overheads compared to VMs, potentially reaching near native-OS\
    \ performance. Another advantage is that containers may contain only the software\
    \ that is actually needed to run the application, thus reducing their size. Compared\
    \ to VMs, this should make containers faster to deploy, and to transfer between\
    \ different systems, as well as requiring less storage space. Fig. 1 Hypervisor-based\
    \ virtualization (VMs, left) and OS-level virtualization (containers, right) Full\
    \ size image Containers bring a series of potential benefits to HPC, such as improved\
    \ software environment flexibility and portability, as well as enhanced research\
    \ reproducibility. Regarding the first, they should allow users to create their\
    \ own user-defined software stack (UDSS) [69], containing all the necessary software\
    \ and dependencies. In terms of portability, containers should allow this UDSS\
    \ to be transferred between different systems, thus reducing the need for software\
    \ re-installation and reconfiguration, or even eliminating it. In addition, users\
    \ may employ a base container image as a starting point for their custom environment.\
    \ To reduce the burden placed on users, developers and sysadmins can provide pre-configured\
    \ base containers, with commonly used tools, frameworks, and libraries. These\
    \ images may be either generic and publicly available, or tailored to a particular\
    \ system. Reproducibility, in the context of scientific software, refers to the\
    \ ability of obtaining equal, or sufficiently similar, results when repeating\
    \ a previous experiment. In computational science, the stability of results may\
    \ depend on the software itself, on its dependencies, as well as on the whole\
    \ software and hardware environment. As a direct consequence of their portability,\
    \ containers should also provide enhanced reproducibility. Building actually reproducible\
    \ containers, however, entails a careful selection of their contents. Reproducibility\
    \ may depend on specific versions of software components, such as dependencies\
    \ (e.g., libraries), and build tools (e.g., compilers). As the technology advanced,\
    \ several implementations of containers have been developed, the most prominent\
    \ being Docker. It was, arguably, the first one to provide a high-level management\
    \ interface for users, thus being responsible for a huge growth in the popularity\
    \ of container technology. Earlier container implementations include Linux VServer,\
    \ and LXC. The latter was the original base layer of Docker. Despite being considered\
    \ a de-facto standard, Docker never really gained traction in HPC. One reason\
    \ being that it was designed to support microservices, thus placing a heavy focus\
    \ on environment isolation. This contrasts with the sharing approach that is most\
    \ common in HPC. In addition, its containers are launched by a daemon that has\
    \ administrative privileges, which are transferred to any application executed\
    \ inside a container. This raises serious concerns regarding the possibility of\
    \ privilege escalation, especially in HPC systems, which are usually multi-tenant.\
    \ Due to these concerns, and as interest in the use of containers in HPC increased,\
    \ a series of HPC-focused implementations were proposed. Among them, Shifter [25],\
    \ Singularity [46], and Charliecloud [69] are, arguably, the most popular ones.\
    \ In order to actually be useful for HPC, a container implementation has to deal\
    \ with a series of HPC specific concerns. Among the requirements to be satisfied\
    \ are: (a) Secure implementation, without the risk of privilege escalation. Meaning\
    \ that containers should not have administrative privileges. (b) Low performance\
    \ overhead compared to running applications natively on the host. (c) Access to\
    \ specialized HPC hardware, such as performance accelerators (e.g., GPU, FPGA)\
    \ and high performance network interconnects (e.g., InfiniBand). (d) Access to\
    \ specialized software, such as vendor-optimized libraries and tools. Section\
    \ 3 discusses these concerns in more detail. In order to advance the state-of-the-art\
    \ on the use of containers for HPC, it is essential to understand the advancements\
    \ that were already made, as well as the current state of the research in this\
    \ subject. With this goal, this survey presents a carefully selected collection\
    \ of 93 works on this subject, which were published between 2016 and mid-August\
    \ 2022. To the best of our knowledge, the most recent survey to touch on this\
    \ topic [15] only includes publications up to 2018. We are also aware of a work\
    \ published in 2022 by Bentaleb et al. [10], in the form of an overview of container\
    \ technologies, instead of a survey, but it touches very briefly on HPC, in a\
    \ short subsection that references only seven other works. In this survey, we\
    \ provide some statistics about the collection of selected works and, most importantly,\
    \ summarize each work’s contribution towards overcoming the major challenges related\
    \ to using containers in HPC. Based on this survey, it seems that the challenges\
    \ related to performance overheads have already been overcome. The same can be\
    \ said for the challenge of running containers without administrative privileges.\
    \ Moreover, their benefits to portability and reproducibility are also confirmed.\
    \ Conversely, the current strategies to provide access to specialized hardware\
    \ and vendor-optimized software can hurt container portability. So, users may\
    \ be faced with a trade-off between performance and portability. 1.1 Organization\
    \ of this text The remainder of this text is organized as follows. In Sect. 2\
    \ we provide an historical overview of container technology, and present Docker,\
    \ the de-facto standard container implementation in the industry. Next, in Sect.\
    \ 3 we describe HPC-specific requirements for containers, together with a few\
    \ desirable characteristics. Following in Sect. 4, we present the main container\
    \ implementations that were specifically designed for HPC. Then, in Sect. 5, we\
    \ describe the methodology we employed to search and select works for this survey,\
    \ and to classify them according to the major challenges they address. In Sect.\
    \ 6, we present some statistics about the works in this survey, including an overview\
    \ of the main challenges they address, and of the container implementations they\
    \ employ. Then we enter into a detailed literature review, starting with Sect.\
    \ 7, where we describe its organization. This is followed by Sects. 8 to 13, where\
    \ we summarize the contributions of each work towards surpassing the main challenges\
    \ we identified in this survey. These are related to performance overheads (resource-specific\
    \ and overall), portability, reproducibility, and to running containers without\
    \ administrative privileges. To close the review, in Sect. 14 we summarize a few\
    \ related works, which are previous survey or overview papers that are included\
    \ in our selected publications. Finally, in Sect. 15 we present the insights we\
    \ derived from the information we compiled, delineate research opportunities,\
    \ and discuss the future directions of our work. 2 Overview of container technologies\
    \ As the technology advanced, diverse container implementations were released\
    \ to the public. In HPC, however, a series of specific requirements has slowed\
    \ down the adoption of containers. Moreover, these often clash with the design\
    \ philosophy of traditional container technologies, especially with their focus\
    \ in isolation. Nevertheless, there have been many attempts at using containers\
    \ in this field, which led to the development of HPC-specific container implementations.\
    \ Next, we have a historical overview of container technologies. After that, we\
    \ end this section by presenting Docker [59], the industry’s de-facto container\
    \ implementation (Sect. 2.2). 2.1 Historical overview Early Linux container implementations\
    \ provided a lower level of abstraction, which restricted their public to users\
    \ with a deeper system administration knowledge. Launched circa 2005, Linux VServer\
    \ 1 is one of the oldest implementations of Linux OS-level virtualization. It\
    \ uses its own capabilities to achieve isolation, via a kernel patch. Isolation\
    \ of the container’s file system is done using chroot 2. Moreover, it employs\
    \ a global PID namespace 3 to hide processes that are outside the container, and\
    \ to prevent inter-container communication between processes. One interesting\
    \ aspect, especially for distributed applications, is that VServer does not implement\
    \ network isolation. Therefore, all containers share the host’s network subsystems.\
    \ To avoid containers sniffing communication from or to another container, it\
    \ adds a filtering mechanism to the network stack. CPU isolation is achieved by\
    \ overlapping the standard Linux scheduler with a filtering mechanism, to provide\
    \ fair-sharing and work-conservation of the CPU. Resource limitation is performed\
    \ using system calls provided by the Linux kernel, and through capabilities added\
    \ to it by VServer. Moreover, there is support for using cgroups 4 to limit the\
    \ resource usage of each container. This is helpful in maintaining performance\
    \ stability when simultaneously executing multiple containers in the same host.\
    \ Another early Linux container implementation is OpenVZ 5, which provides similar\
    \ functionality to Linux VServer but is built on top of kernel namespaces. The\
    \ PID namespace is used to guarantee process isolation between containers. The\
    \ IPC namespace ensures that every container has its own shared memory segments,\
    \ semaphores, and messages. Finally, it employs the network namespace, so that\
    \ every container has its own network stack (devices, routing tables, firewall\
    \ rules, etc.). Moreover, a real network device can be assigned to a specific\
    \ container. OpenVZ also includes a two-level CPU scheduler, which acts at both\
    \ intercontainer and intra-container levels. Their respective purposes are to\
    \ ensure fair-sharing of CPU resources between containers, and between processes\
    \ in the same container. Linux Containers 6 or simply LXC is one of the most well-known\
    \ container implementations. As OpenVZ, it employs kernel namespaces to accomplish\
    \ process and resource isolation. Moreover, the PID, IPC and network namespaces\
    \ are also employed. Besides, it uses cgroups to define the configuration of the\
    \ network namespaces, to limit CPU usage, and to isolate containers and processes.\
    \ Finally, IO operations are controlled by a Completely Fair Queuing (CFQ) 7 scheduler.\
    \ Instead of using LXC directly, it is also possible to use LXD 8, which is a\
    \ high-level container manager built on top of LXC. The surge in popularity of\
    \ containers has really come with the launch of Docker in 2013 [59]. It was initially\
    \ built on top of LXC, but newer versions employ its own low-level container library.\
    \ Its user interface provides a higher level of abstraction than LXC, thus being\
    \ one of its main innovations, and greatly simplifying the creation, building,\
    \ sharing, deploying, and execution of containers. Docker is considered the de-facto\
    \ container implementation in the industry, being widely employed for the deployment\
    \ of microservices in the cloud. On June 2015, the Open Container Initiative (OCI)\
    \ 9 was launched by Docker, CoreOS and other leaders in the container industry.\
    \ According to their website, the OCI is ”an open governance structure for the\
    \ express purpose of creating open industry standards around container formats\
    \ and runtime.” The OCI standards currently include two specifications: the Runtime\
    \ Specification and the Image Specification. The Runtime Specification defines\
    \ how to execute a file system bundle. According to it, an OCI implementation\
    \ would download an OCI image, which would be unpacked into an OCI runtime file\
    \ system bundle. This bundle would then be run by an OCI runtime. The Image Specification\
    \ defines how to create an OCI Image, which would generally be done by a build\
    \ system. The build output would be an image manifest, a file system (layer) serialization,\
    \ and an image configuration. An OCI image should contain the information required\
    \ to launch the application on the target platform. To help jump-start the initiative,\
    \ Docker donated its container format and runtime, runC, to the OCI. 2.2 Docker\
    \ As mentioned above, Docker[59] is the current de-facto standard container technology\
    \ in the industry. It was designed having in mind the deployment of microservices,\
    \ with an emphasis on isolation. Besides the runtime system, it provides Docker\
    \ Hub, a public repository where container images can be stored and shared by\
    \ common users. Initially, Docker was a high-level user interface for LXC, which\
    \ later was replaced by its own low-level container library. Despite its popularity,\
    \ some problems hamper the adoption of Docker in HPC. First, its focus on isolation\
    \ goes against the parallel application paradigm of sharing data between processes.\
    \ One of the main difficulties is to have direct access to specialized HPC hardware,\
    \ and to the host’s vendor-optimized libraries and tools. Both are of high importance\
    \ in order to achieve the best possible performance from the system. The second\
    \ and most important issue is that Docker, by default, requires its daemon to\
    \ run with administrative privileges. This is usually not an issue in cloud platforms,\
    \ which employ a security model based on a trusted user running trusted containers.\
    \ Moreover, in the cloud, the common approach is for the user to have complete\
    \ control over their instances. Which is possible due to each instance, be it\
    \ VM or bare-metal, having only one user, and being “reinstalled” at every reservation.\
    \ This way, it should not be possible for one user to modify another user’s environment.\
    \ Multi-tenant HPC systems, on the other hand, typically have an administrator\
    \ that manages the system, and users are not allowed to modify it. This way, it\
    \ is easier to maintain the system security and stability in an environment that\
    \ is shared by different users. Hence, running a user-defined container launched\
    \ by a daemon with administrative privileges may introduce security issues. Despite\
    \ these difficulties, there were some attempts to use Docker for HPC [4, 8, 57,\
    \ 64, 68, 71, 75, 81]. Based on these, we gather that the problems of accessing\
    \ specialized HPC hardware and host optimized libraries are relatively easy to\
    \ solve. The secure execution of Docker, however, presents a harder challenge.\
    \ Most of the works employing Docker in HPC simply ignore this issue, with very\
    \ few works really attempting to solve it. Among these, Sparks [81] implemented\
    \ Docker plugins to enhance its security, and Azab [4] developed Socker, a wrapper\
    \ to securely run Docker on HPC systems. In the next two \uFEFFsections, we respectively\
    \ discuss the HPC-specific requirements for containers, and container implementations\
    \ that were specifically designed for HPC. 3 HPC-specific requirements for containers\
    \ In order to be useful for high performance computing applications, container\
    \ implementations should satisfy the requirements enumerated in the next four\
    \ paragraphs. Secure implementation: Container technologies must protect the OS\
    \ against malicious or faulty user software, which is especially important on\
    \ multi-tenant systems. Low performance overhead: The performance of HPC workloads\
    \ inside a container should be close to the one achieved when running the same\
    \ workload directly on the host. Access to specialized hardware: HPC systems are\
    \ usually equipped with specialized hardware, such as computation accelerators\
    \ and high performance interconnects. Accessing them is essential for getting\
    \ the best possible performance out of the system. Therefore, containerized HPC\
    \ applications should be able to access this hardware, with negligible or non-existent\
    \ performance penalties. Access to specialized software: In order to get the most\
    \ performance out of the system, HPC hardware vendors often provide libraries\
    \ and tools that are optimized to run on their hardware. Therefore, it is essential\
    \ for containerized HPC applications to have access to this optimized software,\
    \ which is installed in the host. 3.1 Desirable features for HPC containers Besides\
    \ the above requirements, the following features are also desirable for HPC containers.\
    \ The two main ones are presented below. Enabling the deployment of User-defined\
    \ Software Stacks (UDSS) [69]: That is, to enable users to create customized environments\
    \ containing applications and their dependencies. This would enable running their\
    \ applications independent of the software that is provided in the host. Such\
    \ independence greatly enhances the portability of the environment, which is important\
    \ when migrating the application between systems, or when aiming at reproducibility.\
    \ Supporting standard container images (OCI/Docker): The use of standard container\
    \ image formats enhances flexibility and usability. For instance, supporting Docker\
    \ images would allow the container to be created on the user’s personal computer,\
    \ using Docker, and to be shared via Docker Hub. Moreover, it would be executable\
    \ both in HPC systems, using an HPC-specific runtime, and in the cloud, using\
    \ Docker. In addition, this would enable the execution of a wide array of container\
    \ images already available in public repositories, as well as using these as base\
    \ images for new containers. 3.2 Possibly unnecessary features In addition to\
    \ the above requirements and features, it is important to remark that a few common\
    \ characteristics of containers are often unnecessary for HPC. For instance, for\
    \ most HPC applications it may not make sense to run more than one container per\
    \ host. Therefore, performance isolation between containers may not be required.\
    \ Nevertheless, some HPC sites allow multiple jobs to simultaneously share a computer\
    \ node. In these cases, containers should respect the resource usage limits imposed\
    \ by the job scheduler. Moreover, in cases where different jobs do not share any\
    \ computer node, which is common in HPC, there is usually no need for network\
    \ isolation. 4 HPC container implementations Based on our survey, the three most\
    \ popular HPC container implementations are Singularity, Shifter, and Charliecloud.\
    \ Below, we present a short description of each of these solutions. Shifter [25,\
    \ 38] is one of the first HPC container implementations, initially aimed at Cray\
    \ systems. It enables the unprivileged execution of containers, and supports Docker\
    \ images via conversion to its own image format. So, users can create the images\
    \ using Docker, thus taking advantage of publicly available Docker images. Shifter\
    \ integrates an image gateway that serves as a container database and can integrate\
    \ with Docker Hub. Moreover, it integrates with the Slurm job scheduler, to facilitate\
    \ the submission of container jobs. In addition, Shifter relies on MPI ABI compatibility\
    \ to link the container’s MPI with the host’s, to take advantage of host-specific\
    \ optimizations and high performance interconnects. Access to GPUs, however, needs\
    \ to be manually configured. Besides being known to provide near native performance\
    \ and currently being installable in non-Cray systems, its popularity is hampered\
    \ by a complex installation process. Singularity [46] is another HPC-focused container\
    \ runtime that allows the unprivileged execution of containers. Its installation,\
    \ however, still requires administrative access. It has its own container image\
    \ format, but enables pulling and converting Docker images. Singularity provides\
    \ native support for specialized network interconnects, and for GPU accelerators.\
    \ Access to vendor optimized software can be achieved by mount-binding into the\
    \ container a directory from the host, containing the required software. This\
    \ can be done by passing a specific argument when calling the runtime. Charliecloud\
    \ [69] is a runtime that aims to be a lightweight User Defined Software Stack\
    \ (UDSS) implementation. It differentiates itself from the others by having a\
    \ small code footprint and not requiring administrative privileges, even for installation.\
    \ Different from Shifter and Singularity, it relies on User namespaces, which\
    \ is an unprivileged namespace that allows users without administrative privileges\
    \ to create any of the other namespaces. For this reason, Charliecloud requires\
    \ a new enough Linux kernel to support this feature, which has to be enabled in\
    \ the kernel configuration. User namespaces enable Charliecloud to run Docker\
    \ containers without administrative privileges. Locally, Charliecloud employs\
    \ Docker tools but, before execution, the Docker image must be flattened to a\
    \ single archive file. Since it employs Docker tools that require administrative\
    \ privileges, this must be done before transferring the image to the HPC system.\
    \ Charliecloud does not employ any network related namespace, thus it allows direct\
    \ access to the host’s network. Access to devices and file systems is available\
    \ inside the container by default. User-space drivers, needed for hardware such\
    \ as GPUs and high performance interconnects, can be used by making the required\
    \ library files available inside the container. Two ways to accomplish this are\
    \ installing compatible versions of these inside the container, or bind-mounting\
    \ a host directory, containing the necessary files, to a directory inside the\
    \ container. The same approach can be employed to access vendor optimized software.\
    \ Finally, besides the above, a few other container implementations that appear\
    \ in the works we surveyed are capable of unprivileged execution. Among them are\
    \ Podman 10, rootless Docker 11, and udocker [26]. 5 Survey methodology Our survey\
    \ methodology is divided in four stages. The first two were the search for papers\
    \ on the chosen subject, and the refinement of our scope. They are detailed, respectively,\
    \ in Sects. 5.1 and 5.2. The search process consisted of several steps in which\
    \ we expanded our list of candidate works based on the results of the previous\
    \ step. The scope refinement consisted in trimming our list of pre-selected works,\
    \ by removing those deemed irrelevant or out of the scope of this survey. In the\
    \ last two stages, described in Sect. 5.3, we read and classified the selected\
    \ works. The former consisted of reading the full text of these publications,\
    \ and registering some data about them. In the latter, we used this data to classify\
    \ the works according to the main challenges they discuss, and/or attempt to solve.\
    \ 5.1 Search for relevant works We started our search by looking for papers on\
    \ the use of containers for HPC which were published from the year 2016 to 2022.\
    \ First, we searched for articles on relevant HPC conferences and journals, which\
    \ are listed in Table 1, using the search terms “container” OR “containers” OR\
    \ “os-level virtualization”. Since these are HPC conferences and journals, we\
    \ did not include terms such as HPC or high performance computing. In our next\
    \ step, we expanded this search by using Google Scholar to look for papers from\
    \ the same period, but without limiting ourselves to specific venues. We used\
    \ the search terms (“container” OR “containers” OR “os-level virtualization”)\
    \ AND (HPC OR “high performance computing”). We limited our search to the first\
    \ 10 pages (100 results), and the results were sorted by relevance (as ranked\
    \ by the search service). In these two initial stages, we narrowed down our results\
    \ by scanning the titles of the papers and, if needed, their abstracts. In this\
    \ process, we were able to eliminate those works that were clearly out of our\
    \ scope. Table 1 Relevant publication venues considered in the first step of our\
    \ search Full size table The third step in our search was to identify which conferences\
    \ and journals had most papers listed in the results of the previous stages. We\
    \ then expanded our results by inspecting all the papers in these venues that\
    \ matched our search terms and were published in the last considered years (2016–2022).\
    \ Originally, our survey only included works published until early 2020. For these,\
    \ since we had a large amount of results, we did an extra fourth step, where we\
    \ considered the citations and citation counts, in order to trim-down our results.\
    \ First we looked into the related works cited by the pre-selected papers to make\
    \ sure we did not miss any important work. Based on this, we selected to go to\
    \ the next phase both the works that were frequently cited and those cited in\
    \ a context that indicated they could be relevant. Second, among the works that\
    \ were found either via search or via citations, but were not yet selected for\
    \ the next phase, we selected those that had 100 or more citations on Google Scholar.\
    \ Later, we updated our survey to include more recent works (up to mid-August\
    \ 2022), but decided not to do this citation-based step, since the number of pre-selected\
    \ works was already considerably smaller than in the original 2016–2020 search.\
    \ At the end of this first stage, we arrived at 185 pre-selected works. 5.2 Scope\
    \ refinement The next stage of our selection process was refining the scope of\
    \ our search, by selecting only those papers that actually deal with the use of\
    \ containers in the scope of high performance computing. This filtering was done\
    \ by reading the titles, abstracts, and conclusions of the papers that were pre-selected\
    \ in our search stage. In some cases, we also looked into their evaluation methodology,\
    \ to determine if it included any HPC-related metric. During this process, we\
    \ also gathered statistics for use in a later stage, such as which container technologies\
    \ were employed, and what aspects of the use of containers were explored by each\
    \ work. The first filtering criteria was if the work’s stated proposal was actually\
    \ related to the use of containers. We selected papers that proposed using containers\
    \ as part of the solution for a problem, those which evaluated some aspect of\
    \ their usage (e.g., performance overheads), and those which consisted of literature\
    \ reviews on the subject. For instance, some papers mentioned containers in their\
    \ related work, but only evaluated the performance of hypervisor-based virtual\
    \ machines. Therefore, these were considered out of the scope of this survey.\
    \ Next, we created three filtering criteria to define which of these works on\
    \ containers are in the scope of HPC. The first criterion was if the authors actually\
    \ stated their work was aimed at solving a problem from the HPC field. The second\
    \ was if the work evaluated the performance of HPC workloads (benchmarks or applications)\
    \ when running inside containers. According to this criterion, the evaluation\
    \ of performance metrics not specifically related to HPC was not considered enough\
    \ to qualify the paper as in our scope. The third criterion was if the work was\
    \ a survey of existing literature that explicitly included works on containers\
    \ applied to the HPC field. The papers which met any of these criteria were considered\
    \ to be in the scope of our survey, thus leaving us with a total of 98 selected\
    \ works. 5.3 Reading and classification After the search and selection stage,\
    \ we read the 98 selected works. Upon reading the full text, it became clear that\
    \ 5 of these papers did not fit our scope. Therefore, they were removed from the\
    \ survey, arriving at a total count of 93 papers included in the survey. In this\
    \ stage, we registered the container implementations being used by each work,\
    \ and classified them according to the challenges they address. A preliminary\
    \ classification had already been done during the scope refinement stage. The\
    \ full reading of the paper allowed us to refine, correct, and expand this classification.\
    \ We specifically registered if the work utilized Docker, Singularity, Shifter,\
    \ Linux VServer, OpenVZ, LXC, or Charliecloud. We were already aware that these\
    \ were the most popular container implementation, based on the preliminary reading\
    \ performed at earlier stages. Works that employed other container runtimes or\
    \ implemented their own, were classified as “Other”. We also classified the challenges\
    \ addressed by each paper into eleven classes: Communication overhead, Storage\
    \ overhead, CPU overhead, GPU overhead, Memory Overhead, Overall application overhead,\
    \ Other Overhead, Portability/Migration (of code or binaries), Reproducibility,\
    \ Unprivileged container execution, and Survey or summary. The classes for Memory\
    \ and Overall overheads were late additions, done during the full-reading stage.\
    \ In the case of research surveys, we registered the implementations that were\
    \ discussed in the text, and these works were classified as belonging, exclusively,\
    \ to the Survey class. In Sects. 8 to 13, we summarize the contributions of the\
    \ selected works towards understanding and surmounting the challenges above. 6\
    \ Statistics In this section, we present and analyze a few statistics about the\
    \ papers included in this survey. This should help to understand aspects like\
    \ the challenges researchers are trying to solve by using containers, and the\
    \ popularity of different container implementations in the scope of HPC. First,\
    \ we present the distribution of the number of papers by year of publication.\
    \ Next, we take a look at the total number of papers that employ each container\
    \ technology, to which we add an analysis of how their usage evolved over the\
    \ years comprehended in our survey. Finally, we perform the same analysis for\
    \ the number of papers that address each of the challenges included in our classification.\
    \ 6.1 Year of publication The shaded area in Fig. 2 represents the number of selected\
    \ works per year of publication. We can observe a small increase in publications\
    \ from 2016 to 2017. This coincides with the appearance of HPC-focused container\
    \ runtimes, like Shifter and Singularity. In 2018, however, it seems the research\
    \ interest in the subject has decreased, but not by much. Nevertheless, going\
    \ from 2018 to 2019, the number of publications more than doubled. This coincides\
    \ with a large increase in popularity for Singularity, and also a small increase\
    \ in the interest on Charliecloud and Shifter. For 2020, the number of works remained\
    \ almost the same, thus indicating a continued interest in the subject. In 2021\
    \ and 2022, however, we see a considerable decrease in the number of publications.\
    \ One reason could be that that most issues regarding containers in HPC, especially\
    \ the ones related to performance, already had well known solutions by then. Moreover,\
    \ we were only able to include works published until mid-August 2022, which was\
    \ when we finished our search. Fig. 2 Total number of works by year of publication\
    \ (gray area, red points), and number of works that employed each container implementation\
    \ on a specific year (color figure online) Full size image 6.2 Employed container\
    \ technologies The colored bars in Fig. 2 represent the number of selected works\
    \ that analyzed each container implementation, by year of publication. Initially,\
    \ in 2016, the only higher level implementation to show up is Docker. We also\
    \ have the first appearance of an HPC container implementation, Shifter. In addition,\
    \ we have a few studies employing lower level implementations, such as OpenVZ,\
    \ Linux VServer and LXC. Along with a significant increase in the number of works,\
    \ 2017 marks the first appearance of Singularity, already close to Docker in number\
    \ of works. Charliecloud is another implementation that appears for the first\
    \ time in 2017. Moreover, Shifter shows some growth but is not close to the number\
    \ of papers that deal with Singularity containers. The “other” category also shows\
    \ some growth, due to attempts by individual research teams to craft their own\
    \ container solution for HPC. In 2018, all implementations, except Charliecloud,\
    \ show a decrease in the number of papers, following the previously mentioned\
    \ decrease in the total number of published works on containers for HPC. Docker\
    \ remains in the lead, with Singularity in the second place. The significant increase\
    \ in the number of publications in 2019, even surpassing 2017, seems to be a result\
    \ of the growing interest in Singularity, which took the lead from Docker, by\
    \ a wide margin. The number of works on Docker containers, on the other hand,\
    \ remained stable from 2018 to 2019. In 2020, Singularity remained in the lead,\
    \ and there was a small increase in the number of publications dealing with Docker\
    \ containers. Charliecloud maintained an almost stable publication count and,\
    \ for the first time, there were no publications employing lower level container\
    \ implementations. Interestingly, for this year we could not identify any paper\
    \ using Shifter inside our scope. Singularity remained in the lead in 2021, while\
    \ Docker and “Other” showed up less frequently, and Charliecloud took second place,\
    \ but remained at the same level as in the two previous years. Interestingly,\
    \ in 2022 Docker returned to the first place, even if it surpassed Singularity\
    \ by only one work. 6.3 Addressed challenges Figure 3a shows the total number\
    \ of selected papers that address each challenge from our classification (Sect.\
    \ 5.3). The first thing we notice is that the main challenge being attacked is\
    \ portability. Which is not surprising, since that is one of the main advantages\
    \ containerization brings to HPC. Following it closely, we have the concern with\
    \ the overall application execution overhead. That is, the potential decrease\
    \ in performance of the application as a whole, without taking into account specific\
    \ components of this overhead (such as, computation, memory, and communication\
    \ overheads). Other works evaluate resource-specific overheads. Among them, the\
    \ communication overhead, which takes the third place in our chart, and represents\
    \ one of the main concerns when containerizing parallel applications. Below, but\
    \ still standing out, we have the storage overhead. Continuing with the resource-specific\
    \ performance, we have a few works that measured pure CPU overhead, the overhead\
    \ for GPU accelerated computation, and others that measured the overhead for memory\
    \ operations. The “other overheads” category includes different types of overhead\
    \ that did not fit into the other categories (e.g., memory consumption, energy\
    \ consumption, deployment time, and startup time). Therefore, the fact that it\
    \ includes more works than some resource-specific overheads is not a concern.\
    \ Surprisingly, it seems that few works focus on reproducibility of research.\
    \ Only the survey or summary category has fewer works, four being presented as\
    \ literature surveys, and one as general overview, or summary, about container\
    \ technology. Fig. 3 Challenges that were addressed by the selected works Full\
    \ size image Figure 3b shows the evolution of the interest on each challenge along\
    \ the years. For 2016, most works analyzed the communication overhead of containers,\
    \ as well as their overall overhead on HPC applications. This trend continues\
    \ in 2017, for which we can also observe a significant increase in works related\
    \ to portability and to the overall overhead. While the interest in portability\
    \ remained in 2018, the number of works analyzing communication overheads decreased.\
    \ This could indicate a loss of interest in this subject, due to the appearance\
    \ of technologies that solve this issue, such as HPC-specific container implementations,\
    \ and of mechanisms that enable Docker to directly access HPC network hardware.\
    \ In addition, the interest in the overall performance overhead for applications\
    \ did not decrease as much as in the other overheads. There was also a small increase\
    \ in works analyzing overheads we did not include in our classification. In 2019,\
    \ we see a large increase in works dealing with most challenges, except storage\
    \ overhead. Notice the large increase in the number of works concerned with portability.\
    \ Looking back at Fig. 2, we could attribute most of this growth to the increased\
    \ popularity of Singularity. 2020 presented a similar distribution of concerns\
    \ as the previous year, but there were fewer works dealing with portability, memory\
    \ and CPU computation overheads, and no works addressing GPU overheads. In 2021,\
    \ most works were still concerned with portability, while interest in the other\
    \ challenges decreased. This may be due to most of them already having well known\
    \ solutions by then, especially the performance-related ones. Moreover, based\
    \ on our survey, many solutions for preserving performance inside containers tend\
    \ to compromise portability. Finally, in 2022 we see a decrease in papers on portability,\
    \ but this data is still incomplete as we finished our search in mid-August 2022.\
    \ 7 Organization of this survey In Sects. 8 to 13, based on our classification,\
    \ we summarize how the selected works deal with each of our pre-defined challenges.\
    \ Many of them evaluate some type of performance overhead that may be caused by\
    \ containerization. Such works compare the performance of different container\
    \ technologies to the performance of bare-metal, of other container implementations,\
    \ or of hypervisor-based virtualization. Some experiments measure the performance\
    \ of specific operations or system resources (hardware or software), to evaluate\
    \ resource specific overheads. These are related to communication, permanent storage,\
    \ CPU computation, GPU computation, and volatile memory. A last subclass, called\
    \ other overhead, includes experiments that do not fit in the previous ones. For\
    \ example, those that evaluate the startup overhead caused by container deployment.\
    \ Other experiments evaluate the performance overhead of applications as a whole,\
    \ without paying attention to specific operations or system resources. Such applications\
    \ can be full-fledged real-world applications, benchmark applications, or even\
    \ benchmark suites that include several benchmark programs (e.g., NPB, HPCC).\
    \ According to our classification, these experiments evaluate overall application\
    \ overheads. The experiments on resource-specific and overall application overheads\
    \ are, respectively, in Sects. 8 and 9. In addition, Sect. 10 includes overhead-related\
    \ experiments that do not fit into our pre-defined categories. Next, in Sect.\
    \ 11, we move to the challenge of application portability, and in Sect. 12 we\
    \ deal with reproducibility. Finally, in Sect. 13, we present works that deal\
    \ with the execution of containers without administrative privileges. We feel\
    \ necessary to reinforce the fact that we classified each experiment according\
    \ its goals. That is, the purpose the researchers expected it to fulfill. By focusing\
    \ on goals, instead of tools (software and hardware), it becomes easier to match\
    \ the experiments to our pre-defined challenges. For this reason, a specific application\
    \ or benchmark may appear in different sections, since different works may have\
    \ employed it with different objectives, For example, one work may have used it\
    \ to compare containerized and bare-metal performances, while another may have\
    \ evaluated how different network configurations impact its performance. We would\
    \ consider these, respectively, an overall application overhead experiment, and\
    \ a communication overhead experiment. 8 Resource specific overheads In this section,\
    \ we present experiments aimed at evaluating different kinds of resource specific\
    \ overhead, which are overheads in operations that utilize specific hardware or\
    \ software resources. This kind of exploration normally employs benchmarks that\
    \ stress that resource, to discover potential performance limitations, or to compare\
    \ different environment configurations. We classified these experiments according\
    \ to types of overhead, each related to a specific system resource. These include\
    \ overheads in communication (shared memory and network interconnection), computation\
    \ (CPU and GPU), permanent storage (local, remote/networked, and parallel file\
    \ systems), and volatile-memory operations. Next, we dedicate a subsection to\
    \ each of these classes. 8.1 Communication overhead Chung et al. [20] evaluated\
    \ the communication performance of Docker over InfiniBand, using tests from the\
    \ IB driver and the OSU Micro-benchmarks. In both, Docker’s point-to-point performance\
    \ was equivalent to the physical machines. Moreover, Docker performed better than\
    \ VM in the OSU collective communication benchmarks. Bahls [6] employed IMB to\
    \ evaluate the network performance of Shifter on 128 nodes of a Cray system. The\
    \ containers were configured to rely on MPICH-ABI compatibility with the host’s\
    \ Cray MPI to access the high performance interconnection. They compared the performance\
    \ of containers with two different versions of IMB, one compiled with an open-source\
    \ compiler, and another built with the Cray Compiling Environment (CCE). Many\
    \ MPI routines achieved near-native performance inside the containers, but several\
    \ others, including Gather, Reduce_scatter, and Scatter, were up to 3 times slower\
    \ than the native CCE-compiled version. Herbein et al. [30] proposed a series\
    \ of techniques to improve resource allocation for HPC application containers.\
    \ Among these, they implemented a mechanism that enables setting bandwidth limits\
    \ for containers (throttling) and also provides a preferential delivery service\
    \ to critical application containers (prioritization). Moreover, it can ”control\
    \ latency and delay while providing a way to reduce data losses.” Tests demonstrated\
    \ that their prioritization implementation works, and that containers with equal\
    \ priority receive an equal share of the available bandwidth. The throttling functionality\
    \ was evaluated by executing three containers with different bandwidth limits\
    \ in the same node. The results show that their implementation of container network\
    \ throttling efficiently enforces bandwidth limits. Traditional MPI implementations\
    \ employ their network channel for communication between co-resident containers,\
    \ and the much faster Shared Memory (SHM) channel or Cross Memory Attach (CMA)\
    \ channels for communication inside a container. Zhang et al. [96] proposed a\
    \ locality-aware MPI library for container-based HPC clouds, which dynamically\
    \ detects co-resident containers, enabling them to communicate through SHM or\
    \ CMA. They demonstrated that, even with a constant process count, the performance\
    \ of inter-container communication through the network channel decreases as the\
    \ number of co-resident containers increases. To evaluate their container-locality\
    \ aware MPI, they used Docker containers to run a series of benchmarks and applications\
    \ on bare-metal hosts of the Chameleon Cloud. They used the OSU Micro-benchmarks\
    \ to measure MPI communication performance between two co-resident containers.\
    \ Compared to default MPI, two-side point-to-point communication performance improved\
    \ by 79% in latency, and 191% in bandwidth, and bidirectional bandwidth improved\
    \ by 407%. One-side point-to-point communication showed improvements up to 95%\
    \ in latency and increased the bandwidth by a factor of 9. The performance of\
    \ collective MPI communication for 64 containers across 16 nodes (256 processes\
    \ total) improved from 28%, for MPI_Alltoall, up to 86% for MPI_Allgather. All\
    \ these experiments achieved significantly low overheads (up to 9%) compared to\
    \ the native environment. The authors also evaluated the performance of Graph’500\
    \ and NPB (Class D). Their MPI implementation also improved the performances of\
    \ both Graph’500 (up to 16%) and NPB class D (up to 11% for CG), with small overheads\
    \ (respectively 5% and 9%). Thus, it was able to remove the bottleneck in the\
    \ intra-node inter-container communication. Zhang et al. [95] evaluated the performance\
    \ of Docker containers using PCI Passthrough (Container-PT) and KVM VMs using\
    \ either PCI Passthrough (VM-PT) or SR-IOV (VM-SR-IOV) to directly access the\
    \ InfiniBand hardware. They tested IB verbs, MPI communications (OSU Micro-benchmarks),\
    \ and applications (Graph’500, SPEC, NPB, and LAMMPS) in bare-metal nodes of the\
    \ Chameleon Cloud, equipped with a 56 Gb/s InfiniBand adapter. For both the IB-level\
    \ and the MPI-level point-to-point communication experiments, VM-PT displayed\
    \ lower latencies than VM-SR-IOV, while Container-PT presented larger latency\
    \ than VM-PT. The point-to-point throughput of all three configurations was similar\
    \ to native. In the MPI-level collective communication benchmarks, Container-PT\
    \ achieved the smallest overheads compared to native executions (\\\\(\\\\approx\
    \ {{10}{\\\\%}}\\\\)). Finally, application performance with Container-PT was\
    \ near-native, and better than with both VM configurations. The authors argue\
    \ that, even if SR-IOV resulted in worse performances for VMs, its ability to\
    \ share IO devices between multiple virtual environments provides the necessary\
    \ flexibility for building large-scale HPC clouds. In a work that proposed a container-based\
    \ HPC ecosystem for OpenPOWER systems, Kuity and Peddoju [43] ran the HPCC benchmarks\
    \ on a single-socket IBM Turismo SCM POWER8 machine (3.857GHz, \\\\(8\\\\,\\\\\
    text {cores} \\\\times 8\\\\,\\\\text {threads}\\\\), 128 GB RAM, 10 Gb/s Ethernet).\
    \ They observed the median of 20 results of each benchmark, for Docker, KVM, and\
    \ bare-metal. Network performance was measured with the HPCC-included PTRANS (parallel\
    \ matrix transpose) and b_eff (MPI point-to-point latency and bandwidth measurements)\
    \ benchmarks. Their results from PTRANS (\\\\(50{,}000\\\\times 50{,}000\\\\)\
    \ processes, with \\\\(2\\\\times 16\\\\), \\\\(3\\\\times 16\\\\) and \\\\(4\\\
    \\times 13\\\\) process grid sizes), show no bandwidth overhead for containers\
    \ when running 48 or 56 threads. With 8 and 16 threads the containers presented\
    \ significantly higher median bandwidths than bare-metal, and with 32 threads\
    \ it was slightly higher than bare-metal. VM was the slowest configuration, except\
    \ for 16 threads, for which it presented the highest bandwidth, being slightly\
    \ higher than the one for containers. No explanation was provided for these differences\
    \ in behavior between different thread counts. In addition, there are a few issues\
    \ with the presentation of the results. First, it is not clear, however, what\
    \ these thread counts represent. Second, the authors say that performance degrades\
    \ when they increase the number of threads, but their graphics show the reverse.\
    \ For b_eff  (8byte messages), the latency results displayed the same behavior\
    \ for the container and the bare-metal ecosystem, and higher latency for the VM.\
    \ Similarly, the measured bandwidths of container and bare-metal were nearly identical.\
    \ As part of the evaluation of their Virtual Container Cluster (VCC) model, Higgins\
    \ et al. [31] employed HPCC to evaluate its MPI communication performance. Their\
    \ Docker-based solution uses Weave to create a Software Defined Network (SDN).\
    \ On two 4-node testbeds equipped with Gigabit Ethernet, it achieved 5-7% overhead\
    \ in the random ring bandwidth benchmark, compared to bare-metal. On a 16-node\
    \ testbed equipped with InfiniBand, however, the results were much worse, with\
    \ bandwidths 42-45% slower than bare-metal. In the random latency benchmark, they\
    \ observed 20-30% overhead over bare-metal. Despite these results, the authors\
    \ considered that ”the Weave VXLAN overlay network has good scalability attributes\
    \ and a predictable performance response, in the context of a parallel communication\
    \ network”. Zhang et al. [97] evaluated the MPI communication overhead of Singularity,\
    \ by running the OSU Micro-benchmark son 32 bare-metal nodes of the Chameleon\
    \ Cloud (2 \\\\(\\\\times\\\\) 12-core Intel Xeon E5-2670 v3, 128 GB RAM, 56 Gb/s\
    \ InfiniBand) and on four Intel Knights Landing (KNL) nodes (Intel Xeon Phi 7250\
    \ @ 1.4GHz, 96 GB RAM, 16 GB MCDRAM, Intel OmniPath interconnection). They detected\
    \ a minor performance overhead for internode point-to-point communication in the\
    \ Chameleon Cloud nodes (<7%). In the KNL platform using the in-cache memory mode,\
    \ they detected higher intra-node latencies, and internode latency overheads remained\
    \ below 8%. For KNL in flat memory mode, Singularity was able to deliver near-native\
    \ performance. The MPI collective communication performance was evaluated with\
    \ 52 processes across the 32 Xeon-E5 nodes, and 128 processes across 2 KNL nodes.\
    \ Overall, singularity achieved collective communications overheads smaller than\
    \ 8%. Moreover, the authors highlighted the substantial benefits of KNL’s flat\
    \ memory mode for collective operations with messages larger than 256 kB (16–67%).\
    \ Singularity was able to consistently reflect this behavior. Kovacs [42] used\
    \ iPerf to compare the network throughput of LXC, Docker, and Singularity between\
    \ two hosts, in comparison with KVM and native execution. With their default configuration\
    \ (bridged networking), LXC and Docker fell behind both native and KVM execution.\
    \ They also detected high retransmission rates, for both LXC and Docker, when\
    \ using bridged networking. With host networking, LXC, Docker, and Singularity\
    \ (host networking by default), achieved similar performances to the native environment.\
    \ Le and Paz [48] evaluated the network performance of Singularity on the San\
    \ Diego Supercomputer Center (SDSC) Comet supercomputer. The IMB Sendrecv benchmark\
    \ demonstrated slightly lower than native bandwidth for Singularity. Network latency\
    \ measurements by the IMB Ping-pong benchmark showed interleaved latency results\
    \ between containerized and native executions. The OSU benchmarks displayed slightly\
    \ different results, with the non-containerized run presenting a slight increase\
    \ in latency for message sizes above 400,000 bytes. Overall, this evaluation shows\
    \ that Singularity incurs very small overheads for MPI communication. Also using\
    \ the OSU Micro-benchmarks, Arango et al. [3] evaluated the network performance\
    \ of LXC, Docker and Singularity containers. All three of them showed better latencies\
    \ than bare-metal. The smallest latency was achieved with LXC (\\\\(>{32}{\\\\\
    %}\\\\) of bare-metal), while Singularity’s was slightly better than Docker’s,\
    \ which was slightly better than bare-metal’s. Their measurements also show a\
    \ significant advantage for LXC in terms of bandwidth, compared to bare-metal,\
    \ while Docker and Singularity presented significant overheads. While we cannot\
    \ be sure, since this information is not clearly presented in the paper, these\
    \ results may be explained by the lack of a high performance interconnection.\
    \ The work by Younge et al. [93] includes the evaluation of the communication\
    \ overhead of containers by executing the same IMB container image on a Cray XC30\
    \ system and on Amazon Web Services (AWS) EC2. On the Cray system, Singularity\
    \ with Cray MPI achieved near native bandwidth, while with Intel MPI the performance\
    \ dropped to 39.5% of the native bandwidth. On Amazon EC2, the measured bandwidth\
    \ was only 6.9% of the peak bandwidth achieved through the Cray’s Ares network.\
    \ In the native Cray environment, the ping-pong benchmark displayed an 11.3% latency\
    \ difference between running IMB with static or with dynamically linked libraries.\
    \ This is an important result, because the containers employ dynamic linking of\
    \ the host’s optimized libraries. The ping-pong latency in the Singularity container\
    \ with Cray MPI and of the dynamically linked native executions presented no statistically\
    \ significant difference. On the other hand, using Intel MPI resulted in a large\
    \ negative impact, with latencies over 6000 times the ones obtained with Cray\
    \ MPI. On EC2, the latencies were even longer than for Intel MPI on the Cray system.\
    \ The MPI_Allreduce benchmark measured the latency across 768 MPI ranks (32 nodes\
    \ on the HPC system, 48 nodes on EC2). It showed a small overhead when using dynamically\
    \ linked Cray MPI natively, and no difference in performance compared to the containers.\
    \ Chen et al. [19] measured the network performance of different back ends of\
    \ their Build and Execution Environment (BEE), when transferring data between\
    \ two processes. BEE-VM (Docker over KVM) and BEE-Openstack (Docker over bare-metal\
    \ Openstack cloud nodes) used InfiniBand via SR-IOV. BEE-Charliecloud (Charliecloud\
    \ directly on top of the native environment) and BEE-AWS (Docker inside a Xen\
    \ VMs, i.e., Amazon EC2 instances) used Ethernet. Hpcbench, with message sizes\
    \ from 1 to \\\\(2^{22}\\\\) bytes, showed that all four back ends can provide\
    \ similar network bandwidth and latency to the baseline, for both point-to-point\
    \ and all-to-all communication. In a case study on the use of containers in the\
    \ Blue Waters supercomputer, Belkin et al. [7] evaluated the communication overhead\
    \ of MPI applications inside Shifter containers. They ran the MPI_Bcast, MPI_Reduce,\
    \ MPI_Alltoall, and MPI_Alltoallv benchmarks from the OSU Micro-benchmarks on\
    \ 4 to 64 nodes (64 to 1024 ranks). Their results suggest that the performances\
    \ in Shifter and in the native Cray Linux Environment (CLE) are statistically\
    \ the same. Yet another work to employ the OSU Micro-benchmarks was published\
    \ by Saha et al. [75]. They evaluated the latency of different networking configurations\
    \ for running containers. The benchmarks ran on six 48-core nodes of Chameleon\
    \ Cloud, interconnected via InfiniBand. RDMA communication with the InfiniBand\
    \ hardware was enabled in the containers by mapping the host’s drivers into them.\
    \ They tested three Docker container setups, and one Singularity setup. The Docker\
    \ setups consisted of one container per node with host networking, one container\
    \ per node with overlay network (via Docker Swarm), and multiple containers per\
    \ node using an overlay network (varying the number of MPI ranks per container).\
    \ The OSU Alltoallv collective communication benchmark with 65,536-byte messages\
    \ did not display any significant change in latency for the approaches with a\
    \ single container per node. Moreover, executing multiple MPI processes per container\
    \ resulted in a maximum increase of 0.82% in latency, when running one MPI process\
    \ per container, and 0.72% with 4 processes per container. In another set of experiments,\
    \ Saha et al. [75] measured the container networking overhead of Ethernet compared\
    \ to InfiniBand. Once again, they employed six 48-core bare-metal nodes of the\
    \ Chameleon Cloud. The OSU benchmark measured an increase of \\\\(\\\\approx {{245}{\\\
    \\%}}\\\\) in network latency when using Ethernet, compared to InfiniBand. For\
    \ MiniFE, a CPU-intensive benchmark, the Ethernet performance overhead was 1%\
    \ for bare-metal, and 1.4% with Docker. Meaning, the CPU performance hit from\
    \ using a slower network was minimal. For KMI Hash, a memory-intensive benchmark,\
    \ the performance degradation of using Ethernet was 84% on bare-metal and 85%\
    \ on Docker, when compared to InfiniBand on bare-metal. This demonstrates the\
    \ importance of having access to specialized high-bandwidth and low-latency interconnects,\
    \ especially for memory- or network-intensive applications. Sande Veiga et al.\
    \ [77] also employed the OSU benchmark. This time, to measure the containerized\
    \ bandwidth of InfiniBand in the FinisTerrae II cluster, with three different\
    \ MPI versions. They only executed point-to-point tests, which resulted in latency\
    \ values which were ”correct values for InfiniBand networks.” Moreover, the bandwidth\
    \ reached \\\\(\\\\approx {{6}\\\\,{\\\\hbox {GB/s}}}\\\\) which is ”a correct\
    \ value range for InfiniBand.” Hu et al. [35] employed the OSU Micro-benchmarks\
    \ to test the MPI-level network bandwidth overhead of Singularity over 1 Gb/s\
    \ Ethernet. For message size 1 GB, Singularity’s bandwidth was 1.87% smaller than\
    \ native, while the average bandwidth for other sizes was 3.72% larger than native.\
    \ Continuing the work presented on Saha et al. [75], Beltre et al. [8] evaluated\
    \ the usage of container orchestrators to run HPC workloads in the cloud. They\
    \ measured the TCP/IP and InfiniBand network overheads of employing Docker Swarm\
    \ and Kubernetes, compared with bare-metal. For this purpose, they ran the Alltoallv\
    \ (latency) and OSU Bi-directional (bandwidth) benchmarks over six 48-core nodes\
    \ of Chameleon Cloud. The TCP/IP results showed substantial latency and bandwidth\
    \ overheads for both orchestrators. Kubernetes, whose bandwidth stagnated for\
    \ message sizes larger than 4 kB, presented the worst results. With Docker Swarm,\
    \ however, network performance gets closer to bare-metal as message sizes increase.\
    \ The InfiniBand experiments showed much better results, with both solutions attaining\
    \ only \\\\(\\\\approx {{2}{\\\\%}}\\\\) average overheads relative to bare-metal.\
    \ Maliszewski et al. [54] proposed using network interface (NIC) aggregation to\
    \ improve the performance of HPC applications in container-based cloud platforms.\
    \ To evaluate this approach, they aggregated 1 to 4 NICs as slaves of a virtual\
    \ NIC, using network bonding (balance round-robin mode). They used network bridging\
    \ to provide access to the aggregated interface from inside LXD containers, on\
    \ an OpenNebula managed cloud. Network evaluation with NetPIPE showed significant\
    \ latency improvements for both LXD and native executions. Moreover, LXD using\
    \ 4 aggregated NICs overcame the native throughput at the 10 kB message size.\
    \ The authors also evaluated the performance of the NAS Parallel Benchmarks on\
    \ two nodes, each running 16 MPI processes. Overall, LXD achieved higher performance,\
    \ with up to 38.95% improvement for a network intensive application (IS). The\
    \ native version suffered small performance losses when aggregating 3 and 4 NICs.\
    \ The authors assign this to an excessive retransmission of packets by the networking\
    \ implementation of the underlying OS. Continuing the previous work, Maliszewski\
    \ et al. [55], performed new experiments, now using bonding mode 4 (IEEE 802.3ad),\
    \ with three hash policies (layer 2, layer 2+3, and layer 3+4), aggregating either\
    \ 2 or 4 NICs. Their results demonstrated \\\\(\\\\approx {{40}{\\\\%}}\\\\) improvement\
    \ for IS, with 4 aggregated NICs. BT and SP presented only small performance improvements,\
    \ which the authors attribute to the use of blocking communication. Except for\
    \ FT, the layer 2 policy resulted in performance improvements, while the other\
    \ two caused performance losses. For FT, the higher layer hash policies actually\
    \ improved performance, which the authors say is due to its intensive network\
    \ usage. On experiments with Alya, a computational fluid-dynamics (CFD) code,\
    \ Rudyy et al. [72] evaluated the performance of container images executed on\
    \ an HPC cluster and on the MareNostrum supercomputer, using Singularity, Shifter,\
    \ and Docker. On the cluster, they identified a slowdown when running on Docker,\
    \ in the section of the code that performs MPI communication. Shifter and Singularity\
    \ did not present such slowdown, and achieved similar performance to bare-metal.\
    \ Alya’s FSI code (fluid structure artery simulation) was tested on MareNostrum,\
    \ using both a Singularity image with access to system resources (system-specific),\
    \ and another without it (self-contained). While the system-specific container\
    \ performed similar to bare-metal, the self-contained one suffered from performance\
    \ degradation in the same part of the code as in the previous experiments with\
    \ Docker. This reinforces the importance of containerized HPC applications having\
    \ access to host resources, such as high performance fabrics. Lim et al. [50]\
    \ evaluated five Docker network configurations, being Docker Linux Overlay, Weave\
    \ Net, Flannel, Calico with IP-in-IP, and Calico without IP-in-IP. Based on the\
    \ results of small-scale experiments (2 to 4 nodes with 10 Gb/s Ethernet) using\
    \ iPerf3, Hpcbench, OSU Micro-benchmarks, and HPL, they proposed Calico as the\
    \ best performing solution, being comparable with bare-metal. For the OSU and\
    \ HPL benchmarks, they also tested a Singularity container. Docker with Calico,\
    \ and Singularity achieved similar levels of performance to bare-metal, for both\
    \ benchmarks. Peiro Conde [67] used the OSU Micro-benchmarks to evaluate Singularity\
    \ containers on an ARM-based cluster with an InfiniBand interconnection. Overall,\
    \ Singularity achieved higher bandwidth than bare-metal for small messages, albeit\
    \ by a small margin. With larger message sizes (from \\\\(2^{10}\\\\) to \\\\\
    (2^{20}\\\\) bytes), the bandwidth of Singularity and bare-metal behaved similarly.\
    \ Moreover, for message sizes between \\\\(2^{10}\\\\) and \\\\(2^{15}\\\\) bytes,\
    \ Singularity performed significantly better than bare-metal. The authors attributed\
    \ this behavior to differences between container and host libraries. Simchev and\
    \ Atanassov [78] evaluated the performance of two Kubernetes-based MPI clusters:\
    \ one with all containers mapped to different physical hosts, and another with\
    \ every 2 containers mapped to the same physical host. These were deployed on\
    \ four cloud instances with 2 virtual CPUs and 4 GB of RAM each. Through experiments\
    \ employing the Intel MPI Benchmarks they found that the configuration with shared\
    \ physical hosts obtained better MPI-communication performance. They also concluded\
    \ that ”the overall performance of MPI operations is mainly determined by the\
    \ networking implementation.” In a work aimed at analyzing the portability and\
    \ usability of user-defined containerized stacks, Ruhela et al. [73] evaluated\
    \ the performance of Singularity, Charliecloud, and Podman, at various problem\
    \ scales. To evaluate their communication overhead, they ran the Intel MPI Benchmarks\
    \ on nodes of the Frontera supercomputer. Their MPI_Allreduce experiments on 4000\
    \ nodes show nearly the same latency for the native execution and for the two\
    \ container solutions. The authors remark that, in order to achieve performance\
    \ at scale for containerized HPC, one must afford the cost of building support\
    \ for high performance interconnects. Liu and Guitart [51] analyzed the performance\
    \ of different container deployment schemes for HPC workloads. They evaluated\
    \ different container granularities, as well as the benefits of setting memory\
    \ and CPU affinity for the containers. Their experiments compared Docker using\
    \ bridge networking and three Singularity configurations, (1) with host-networking\
    \ and no cgroups (default), (2) with host-networking and using cgroups for resource\
    \ isolation, and (3) with bridge networking and cgroups. They executed applications\
    \ from the HPCC benchmark suite (DGEMM, FFT, STREAM, PTRANS, RandomAccess, and\
    \ b_eff) on a single dual-socket host with 18 CPU cores, of which they only used\
    \ 16. Results for the two configurations that use bridge-networking, show the\
    \ expected performance degradation for MPI communication on multi-container workloads.\
    \ In this configuration, the authors explain, processes on different containers\
    \ can not detect if they reside in the same host. Thus, they will not attempt\
    \ shared memory communication. By default, Singularity is not affected by this\
    \ issue, since it employs host-networking. In addition, over-subscription (32\
    \ processes on 8 CPU cores) resulted in some imbalance, which was remedied by\
    \ setting-up CPU affinity to pin processes to specific cores. Moreover, setting\
    \ memory affinity improved the performance of distributed memory benchmarks in\
    \ NUMA (Non-Uniform Memory Access) platforms. Finally, the authors did not observe\
    \ any performance variation related to a specific container technology or granularity.\
    \ To evaluate the communication overhead of containers at large scale, Ruhela\
    \ et al. [74] ran the IMB MPI_Bcast and MPI_Alltoall benchmarks. MPI_Bcast was\
    \ run in a cluster equipped with 20 Gb/s InfiniBand, with dual-socket Intel Xeon\
    \ Platinum 8280 (56 cores per node). Results with both 1 and 56 processes per\
    \ node (one process per container) on a total 6144 nodes showed on par communication\
    \ latency by Charliecloud and bare-metal. The MPI_Alltoall benchmarks ran on an\
    \ InfiniBand-equipped cluster with 3.2 Tbps peak bandwidth, and 256-core nodes\
    \ (dual-socket AMD EPYC 7742). The results showed similar communication behavior\
    \ for Charliecloud and bare-metal, with negligible differences in latency. Liu\
    \ and Guitart [52] evaluated the performance implications of running multiple\
    \ containers per node instead of a single one. They evaluated host-networking\
    \ and overlay networking configurations, but multi-container per host scenarios\
    \ were only tested with overlay networking. The overlay configurations included\
    \ TCP/IP (TCP/IP over Ethernet), IPoIB (TCP/IP over InfiniBand), and RDMA (direct\
    \ access to the InfiniBand hardware via RDMA). Single container tests used Docker\
    \ and default Singularity, while multi-container used Docker and Singularity-instance\
    \ (containers running in the background as isolated instances). With a single\
    \ container per host, they ran the OSU MPI_Alltoallv benchmark with 128 processes\
    \ on 4 hosts and the OSU Bidirectional Bandwidth benchmark. Their results showed\
    \ that RDMA is faster than IPoIB, which in turn is faster than TCP/IP. Moreover,\
    \ all configurations that did not rely on overlay networking had the same performance\
    \ as bare-metal, while overlay networking caused large latency and bandwidth degradation.\
    \ On a second set of tests, they ran the MPI_Alltoall benchmark while continually\
    \ increasing the number of containers until each had a single process. RDMA presented\
    \ the best latencies, followed by IPoIB, and then by TCP/IP. Moreover, all configurations\
    \ presented substantial increase in latency as message sizes increased, especially\
    \ for medium (256 B–16 kB) and large messages (32 kB–1 MB). In addition, there\
    \ were large differences in latency between different network configurations.\
    \ For the largest message size (1 MB), RDMA achieved \\\\(\\\\approx 400 \\\\\
    times {10}^{3} \\\\,\\\\upmu {\\\\hbox {s}}\\\\), IPoIB \\\\(\\\\approx {2.5\\\
    \\times 10^{6}}\\\\,{\\\\upmu {\\\\hbox {s}}}\\\\), and TCP/IP\\\\(\\\\approx\
    \ {30\\\\times 10^{6}\\\\,{\\\\upmu {\\\\hbox {s}}}}\\\\). On a third experiment,\
    \ to evaluate MPI communication intensive workloads, they ran the HPCC’s RandomRing,\
    \ G-PTRANS, G-FFT, and G-RandomAccess benchmarks. When increasing the number of\
    \ containers, the first three presented similar performance on TCP/IP and IPoIB\
    \ and degraded performance for RDMA, while G-FFT presented degrading performance\
    \ on all overlay configurations, but more pronounced on RDMA. This was explained\
    \ by network communication being the bottleneck for TCP/IP and IPoIB, while for\
    \ RDMA the bottleneck was memory performance. In a fourth battery of tests, they\
    \ ran two MPI throughput benchmarks, namely EP-STREAM (memory bandwidth) and EP-DGEMM\
    \ (computation). EP-STREAM performed similarly with all container counts, while\
    \ EP-DGEMM showed improvements with one container per process when using cgroups\
    \ (to restrict them to specific cores). Both benchmarks demonstrated penalties\
    \ due to overlay networking. The last experiment by Liu and Guitart [52] evaluated\
    \ the impact of CPU and memory affinity on multi-container deployments. Overall,\
    \ enabling affinity did not solve any of the performance degradation in the communication-intensive\
    \ benchmarks. Nevertheless, EP-DGEMM on all network configurations and G-FFT on\
    \ RDMA had significant performance improvements. Moreover, the multi- and single-container\
    \ performances of EP-DGEMM became on par. Finally, the performance of G-RandomAccess\
    \ on IPoIB degraded, due to load imbalance between processes, but had a small\
    \ improvement on RDMA, which is more sensitive to memory latency. Overall, based\
    \ on the experiments summarized in this section, we can conclude that containers\
    \ are capable of providing near-native network communication performance. For\
    \ this purpose, however, they must be configured to provide direct access to the\
    \ network hardware from inside the container. The works discussed above demonstrate\
    \ and evaluate various solutions to achieve this goal. In most cases, network\
    \ isolation is sacrificed in exchange for performance. This should not be a problem,\
    \ since that is not a requirement for HPC. In fact, dedicated HPC systems usually\
    \ allow applications to run without any network isolation. One aspect that needs\
    \ some attention, however, is that some solutions require having a copy of system\
    \ specific libraries inside the container. Therefore, one must consider the consequences\
    \ for portability when choosing whether to employ such approach (more details\
    \ in Sect. 11). 8.2 CPU computation overhead In their work on improving resource\
    \ management for HPC applications running in containers, Herbein et al. [30] introduced\
    \ a CPU allocation mechanism that allows Docker to define a dynamic time-slice\
    \ for each container. Containers are allocated CPU cores according to a round-robin\
    \ police, and each container is allowed exclusive usage of the core until their\
    \ time-slice ends. This mitigates performance losses due to frequent context switches\
    \ and improves cache efficiency. In experiments where an independent instance\
    \ of LINPACK was executed in each container, the time to run 5 containers decreased\
    \ from \\\\(\\\\approx {20}\\\\) times the time to run a single container to \\\
    \\(\\\\approx {5}\\\\) times. It is worth mentioning that the length of the time-slice\
    \ had to be carefully tuned in order to find its optimal value for the target\
    \ application and system. Kuity and Peddoju [43] evaluated the computational performance\
    \ of a system composed of single-socket OpenPOWER machines, equipped with an IBM\
    \ Turismo SCM POWER8 CPU (3.857GHz, \\\\({8}\\\\,{\\\\mathrm{cores}}\\\\times\
    \ {8}\\\\,{\\\\mathrm{threads}}\\\\), 64 kB L1 data cache, 32 kB L1 instruction\
    \ cache, 512 kB L2, 8 MB L3), 128 GB RAM, and 10 Gb/s Ethernet. They ran the HPL,\
    \ DGEMM, and FFT benchmarks (part of HPCC) in CHPCE containers (their own HPC\
    \ container runtime), KVM, and on bare-metal. Based on 20 executions, HPL was\
    \ slightly faster in the container, and 2% slower in the VM, compared to bare-metal.\
    \ For DGEMM, they ran both SingleDGEMM and StarDGEMM. The former initially presented\
    \ slightly better than native performance, but it proportionally dropped as the\
    \ number of threads increased. Nevertheless, it decreased at a slower rate than\
    \ the VM executions. StarDGEMM also displayed this performance degradation, with\
    \ containers eventually becoming even slower than the VM. For FFT, StarFFT performed\
    \ better than G-FFT, but still shows performance degradation as thread count is\
    \ increased. The authors attribute the displayed performance degradation to an\
    \ increase in memory contention. Aiming to evaluate their CPU computation overheads,\
    \ Kovacs [42] ran sysbench inside LXC, Docker, Singularity, and KVM virtual environments.\
    \ On a single 16-core server, all three container solutions achieved native performance,\
    \ while the VM-based executions were slightly slower. Arango et al. [3] employed\
    \ HPL-LAPACK to measure the CPU overhead of LXC, Docker, and Singularity. Singularity\
    \ achieved the best result, with an average performance 5.42% better than native.\
    \ Compared to native CPU throughput, Docker was 2.89% slower, and LXC was 7.76%\
    \ slower. Hu et al. [35] employed HPL to evaluate the CPU overhead of Singularity.\
    \ It achieved near native performance (only 0.4% average overhead) on a 5-node\
    \ parallel execution. As part of an evaluation of Charliecloud, Shifter, and Singularity,\
    \ Torrez et al. [84] measured their CPU overheads compared to bare-metal. All\
    \ four environments achieved nearly identical performances for sysbench, using\
    \ 36 threads in a single node (3 CPU sockets \\\\(\\\\times\\\\) 12 cores). Gantikow\
    \ et al. [24] analyzed the suitability for HPC of the Podman runtime, which enables\
    \ running unprivileged containers. Among others, their CPU performance measurements\
    \ with sysbench did not show any significant computation overhead from containerization\
    \ (\\\\(\\\\le {0.4}{\\\\%}\\\\)). Based on this, and on application overheads\
    \ (see Sect. 9), they concluded that, besides having a different focus, Podman\
    \ is suitable for HPC. Liu and Guitart [52] measured the impact of running multiple\
    \ containers per host on the performance of HPCC’s EP-DGEMM benchmark. Its performance\
    \ improved (7-16%) when running one container per process combined with cgroups\
    \ to restrict containers to specific cores, similar to explicitly pinning these\
    \ processes. Moreover, setting CPU and memory affinity improved its performance\
    \ for all tested configurations, and also made it independent on container count.\
    \ Based on the results above, it is clear that containers are capable of executing\
    \ CPU-based applications with similar or equal computation performance to the\
    \ native environment. Consequently, their performance impact on computation-intensive\
    \ applications should be negligible. Moreover, no special configuration is necessary\
    \ to achieve optimal results. Therefore, from a CPU computation standpoint, containers\
    \ are a good match for HPC. 8.3 Permanent storage overhead Bahls [6] used the\
    \ IOR benchmark to evaluate the storage performance of Shifter containers on Cray\
    \ systems. On 1 to 200 nodes, its reading and writing performances were comparable\
    \ to native. In order to determine in which conditions OS-level virtualization\
    \ is more suitable for IO-bound HPC applications than hypervisor-based virtualization,\
    \ Beserra et al. [11] evaluated the IO performance of LXC containers against VMs,\
    \ in comparison with native performance. They utilized the fs_test benchmark to\
    \ evaluate the MPI-IO performance of the file system on two quad-core nodes (8\
    \ GB RAM, and 1 Gb/s Ethernet). First, they executed individual instances (processes)\
    \ of the benchmark on each CPU core. With a small file size (1 kB), KVM presented\
    \ more stable performance for writing operations than LXC. For reading operations,\
    \ LXC performed close to native execution, but its performance deteriorated with\
    \ the increase in the number of containers (2 or 3 per host). KVM, on the other\
    \ hand, maintained very stable reading performance, but was still slower than\
    \ LXC. With a larger file size (128 MB), LXC achieved writing speeds close to\
    \ native, and performance did not deteriorate when the containers per host ratio\
    \ was increased. In this case, it was actually KVM that presented such performance\
    \ degradation. LXC presented some oscillation in writing speed with the larger\
    \ files, but its behavior was similar to native. In a second battery of tests,\
    \ the authors executed a single cooperative parallel instance of fs_test, using\
    \ all available cores. With the 1 kB file size, the performance with one container\
    \ or VM per host was similar to native. With multiple virtual environments per\
    \ node, however, both reading and writing performances significantly decreased,\
    \ for both container and VM. For the 128 MB file size, writing performance was\
    \ more stable, and with 2 instances per node LXC performed close to the native\
    \ environment. For reading operations, the authors observed the same performance\
    \ degradation as with the 1 kB file size. Overall, both technologies achieved\
    \ near-native IO performance, which degraded as the number of virtual environments\
    \ in the same host increased. Moreover, this issue was more noticeable with KVM,\
    \ which presented worse IO performance than LXC in all test cases. One of the\
    \ resource allocation improvements proposed by Herbein et al. [30] deals with\
    \ disk IO contention and load imbalance. They propose a two-tiered mechanism,\
    \ at node and cluster level, which extends Docker and Docker Swarm to monitor\
    \ the IO activity of containers. This enables considering IO load-balance when\
    \ mapping containers to data center nodes, and avoiding allocating more containers\
    \ on nodes whose IO bandwidth is saturated. They also implemented a mechanism\
    \ to enable the Docker daemon to set IO bandwidth limits for containers. The expected\
    \ result is improved IO load balance and the avoidance of IO hotspots. This solution\
    \ was evaluated by running an IO-intensive application on 90 containers mapped\
    \ to 14 VMs (each with a dedicated CPU core and hard-drive). Their results show\
    \ their solution was able to provide well-balanced IO bandwidth between the VMs.\
    \ Without their mechanism, the same test cases presented large differences in\
    \ IO bandwidth across containers. To evaluate the storage performance of containers\
    \ on an OpenPOWER system (IBM Turismo SCM POWER8 @ 3.857GHz, 8 cores \\\\(\\\\\
    times\\\\) 8 threads, 128 GB RAM, and 10 Gb/s Ethernet.), Kuity and Peddoju [43]\
    \ ran the IOzone benchmark (20 times). They observed very little overhead related\
    \ to IO operations due to containerization. On the other hand, executions of IOzone\
    \ inside a VM displayed significant performance penalties, especially for larger\
    \ files, which were attributed to inefficiencies in the VM’s IO driver. Arango\
    \ et al. [3] employed the IOzone benchmark to measure the storage overhead of\
    \ LXC, Docker, and Singularity. All three presented some overhead for write operations,\
    \ with LXC and Singularity achieving similar performance, while Docker was the\
    \ worst (37.28% overhead). For read operations, Singularity achieved almost native\
    \ performance, LXC had an overhead of 16.39%, and Docker was the worst, with an\
    \ overhead of 65.25%. The IO performance of random reads and random writes was\
    \ also better with LXC and Singularity than with Docker. Moreover, Singularity\
    \ was better than bare-metal both when operating on a contained file system and\
    \ when using a bind-mounted shared NFS (Network File System). For random reads\
    \ and writes to a shared NFS, LXC and Docker were worse than bare-metal, with\
    \ LXC being slightly worse than Docker. For random reads and writes from a contained\
    \ file system, LXC was faster than bare-metal, while Docker presented significant\
    \ overheads. The authors say these bad results happened because they used Docker’s\
    \ default multilayered file system (AUFS). Chen et al. [19] evaluated the storage\
    \ overhead of their Build and Execution Environment (BEE). For this, they measured\
    \ the reading and writing performance of Linux’s dd command with file sizes from\
    \ 1 MB to 1 GB. The overhead of BEE-Charliecloud was only 0.08% for both read\
    \ and write operations. BEE-VM was slower, with averages overheads of 15.2% for\
    \ reads and 17.1% for writes. For BEE-AWS the averages were 0.4% for reads and\
    \ 0.3% for writes. Finally, the overheads for BEE-Openstack were, on average,\
    \ 5% for reads (1.7-7.6%), and 4.1% for writes (1.7-6.4%). Belkin et al. [7] evaluated\
    \ the IO performance of Shifter jobs on the Blue Waters supercomputer. They ran\
    \ the IOR MPI-IO benchmark on 16 nodes (7 cores each). Their results suggest that\
    \ there is no substantial difference in reading and writing performance between\
    \ the Shifter and the native Cray Linux Environment (CLE). The performance overhead\
    \ of Docker graph storage (used to store and manage images, containers, and other\
    \ metadata), was explored by Sparks [81]. More specifically, they dealt with the\
    \ case of diskless HPC hosts, which use a network-accessible parallel file system\
    \ for storage. They discussed three solutions, OverlayFS mounted over tmpfs (in\
    \ memory), a new OverlayFS implementation using a loopback XFS to mount files\
    \ from a shared parallel storage, and a hybrid loopback XFS and tmpfs approach\
    \ where directories with high performance requirements were mounted on tmpfs.\
    \ The hybrid approach presented performance close to the native OverlayFS on top\
    \ of a local Btrfs. The loopback XFS device presented slight better than native\
    \ performance for image download, but its writing performance decreased. Abraham\
    \ et al. [1] studied the impact of HPC container solutions in the IO throughput\
    \ of HPC parallel file systems (PFS). They executed a series of benchmarks on\
    \ top of the Lustre PFS. Container images with the configured benchmarks were\
    \ executed using Docker, Podman, Singularity, and Charliecloud. They detected\
    \ some startup overhead for Docker and Podman, and network overhead at startup\
    \ time for Singularity and Charliecloud. Despite this, they concluded that ”the\
    \ observed throughput of containers on Lustre is at par with containers running\
    \ from local storage.” Other insights included that Singularity had the best start-up\
    \ times, and that Singularity and Charliecloud have equivalent IO throughput.\
    \ Charliecloud, however, incurred on more PFS operations (metadata and IO). To\
    \ evaluate the storage overheads of Singularity and Charliecloud, Ruhela et al.\
    \ [74] employed the NAS BTIO (BTIO.mpi_io_full) and IOR benchmarks. The BTIO tests\
    \ evaluated Charliecloud with three different problem sizes (classes C, D and\
    \ E) and up to 14K processes on two different clusters. Results showed negligible\
    \ or non-existent overhead in most cases, except for an outlier at 224 processes,\
    \ whose IO-time was \\\\(\\\\approx\\\\) 8.95% longer than native, even if the\
    \ total time was only \\\\(\\\\approx\\\\) 1.72% longer. In the IOR experiments,\
    \ Charliecloud was evaluated with up to 114K processes, and Singularity with up\
    \ to 4098 processes. Both presented similar to native read and write throughputs.\
    \ Huang et al. [36] analyzed the feasibility of employing IO workload management\
    \ tools to handle issues caused by IO-intensive workloads running inside containers.\
    \ They claim that while the existing tools can address issues for conventional\
    \ HPC applications, ”none of them were designed and tested to accommodate the\
    \ cases of such isolated environments.” For their feasibility study, they used\
    \ the Optimal Overload IO Protection System (OOOPS), and the chosen container\
    \ runtime was Singularity. Making the OOOPS library work with the containers was\
    \ relatively easy when the OS distribution inside the container was the same as\
    \ on the host. For three other containers with two different versions of a different\
    \ distribution, however, they had to employ more advanced strategies. Moreover,\
    \ a different strategy was required for each container. Their evaluation process\
    \ employed OOOPS’s test_stat command for small IO tests, and an ad-hoc MPI-IO\
    \ test designed to mimic the behavior of a real containerized HPC application.\
    \ For the latter, they ran 128 MPI tasks on 4 compute nodes of the Frontera supercomputer,\
    \ and employed a Lustre parallel file system. Results of the small IO tests using\
    \ 9 different container images, demonstrated that they successfully enabled OOOPS\
    \ for a range of container environments. The MPI-IO tests with unlimited IO for\
    \ two different containers presented comparable execution times to the native\
    \ environment (\\\\(\\\\approx {120\\\\,{\\\\hbox {s}}}\\\\)). Moreover, tests\
    \ with two different levels of IO throttling also presented similar containerized\
    \ and native execution times. According to the authors, this indicates that OOOPS\
    \ worked properly for containerized MPI-IO running across multiple nodes. Besides\
    \ this evaluation, this work presented a practical exampled where they used a\
    \ containerized version of LAMMPS to perform IO-intensive molecular dynamics simulations\
    \ on Frontera. The results of this test demonstrated that the throttling mechanism\
    \ in OOOPS works effectively in both native and containerized environments. Overall,\
    \ the results we summarized above show that container technology can achieve storage\
    \ (IO) performance comparable to native. Moreover, they represent a remarkable\
    \ improvement compared to hypervisor-based virtual machines. To achieve the best\
    \ performance, however, some aspects need attention. One of them is to avoid IO\
    \ intensive operations over Docker’s overlay file system (AUFS). It is probably\
    \ better to mount a host directory inside the container, either as a volume or\
    \ via a bind mount. Another aspect to consider is the number of containers mapped\
    \ to each physical host. As discovered by multiple works, IO performance tends\
    \ to degrade as this number increases, especially with small files. 8.4 GPU computation\
    \ overhead Arango et al. [3] chose NAMD as a benchmark to evaluate the GPU computation\
    \ overhead of LXC, Docker, and Singularity. The experiments took place in an 8-core\
    \ machine equipped with an NVIDIA Tesla K200m GPU. LXC got very close to native\
    \ performance, while Docker (nvidia-docker) and Singularity achieved better performance\
    \ than native execution (by a small margin). Khan et al. [41] presented the experiences\
    \ of the EU H2020 CloudLightning project on using containers to provide HPC applications\
    \ as a service. Their focus was on applications that exploit computation accelerators\
    \ of various types. Regarding GPUs, they measured the performance of the widely\
    \ used BLAS linear algebra library. They experimented with two different builds\
    \ of GEMM, both on bare-metal and inside a Docker container (using the nvidia-docker\
    \ wrapper). One version of GEMM was linked with OpenBLAS, a CPU-based BLAS implementation.\
    \ The second used cuBLAS, which employs CUDA to execute in NVIDIA GPUs. The testbed\
    \ was a 48-core computer node, with 64 GB of RAM and one NVIDIA Tesla P100 GPU.\
    \ The bare-metal executions performed significantly better than the containerized\
    \ ones, while the cuBLAS version presented the largest containerization overheads\
    \ (1.1–1.27\\\\(\\\\times\\\\)). Gomes et al. [26] evaluated the containerized\
    \ performance of GPU-accelerated applications using a CentOS 7.3 and an Ubuntu\
    \ 16.04 container image. The first application was DisVis, a software package\
    \ for modeling biomolecular complexes. Executions on Docker and udocker were faster\
    \ than native, with gains of 1-2% for CentOS, and \\\\(\\\\approx {{5}{\\\\%}}\\\
    \\) for Ubuntu. For the second application, GROMACS, the CentOS containers were\
    \ 3-5% slower than native execution for both Docker and udocker in F3 mode (based\
    \ on LD_PRELOAD, employing fakechroot and modifications to ELF headers). With\
    \ Ubuntu, the containerized performance was the same as native. Tests with udocker\
    \ in P1 mode resulted in up to 22% performance penalty. The authors say the large\
    \ performance loss in this mode is due to the elevated number of system calls\
    \ caused by context switches (hybrid OpenMP and GPU execution). Moreover, they\
    \ explain that newer versions of udocker mitigate this issue by filtering in only\
    \ the relevant system calls. Hu et al. [35] evaluated the GPU overhead of Singularity\
    \ for three real applications. Their experiments took place in a single 56-core\
    \ node, with 128 GB of RAM, and two NVIDIA Tesla P100 GPUs. The first application\
    \ was NAMD (NAnoscale Molecular Dynamics), with timestep sizes 100 ps and 10 ps,\
    \ executing on both GPUs, with 4, 8, 16, 32, and 48 CPU threads. With the 100\
    \ ps timestep, the overhead was from − 0.812% (4 cores) to 0.807% (24 cores).\
    \ For the 10 ps timestep, the overhead was from \\\\(-\\\\,1.675\\\\%\\\\) (32\
    \ cores) to 0.843% (48 cores). The second application was the GPU version of VASP,\
    \ whose containerized executions achieved negligible performance differences from\
    \ native execution (\\\\(<{0.05}{\\\\%}\\\\) with one GPU,  0.313% with two GPUs).\
    \ The third GPU application was AMBER, a package of programs for molecular dynamics\
    \ simulations of proteins and acids. For the tested input data, the execution\
    \ on one GPU lasted 57,072 s in Singularity and 57,775 s natively. With two GPUs,\
    \ Singularity presented an execution time overhead of \\\\(\\\\approx {{4.135}{\\\
    \\%}}\\\\). For the NAMD and VASP experiments, the authors concluded that the\
    \ performance of GPU applications in Singularity containers is very close, or\
    \ basically similar, to native application. For AMBER, they concluded that the\
    \ overhead is acceptable considering the total execution time. Grupp et al. [27]\
    \ evaluated the usage of optimized containers built from official TensorFlow (TF)\
    \ images, to test and deploy neural-network applications. They trained two neural\
    \ network architectures, Resnet-50 and AlexNet, with synthetic data and with the\
    \ ImageNet data set. Their experiments employed two multi-GPU systems, one equipped\
    \ with four NVIDIA Tesla K80 GPUs (12 GB VRAM), and another with four NVIDIA GTX\
    \ 980 Ti (6 GB VRAM). First, the benchmarks were executed inside a container instantiated\
    \ with udocker [26] (briefly described in Sect. 13), using 1, 2, and 4 K80 GPUs.\
    \ The results using synthetic data closely matched the official TF benchmark results\
    \ for the same GPU. Compared to the executions with synthetic data, no performance\
    \ penalty was observed for Resnet-50 using the ImageNet data set. For AlexNet,\
    \ however, there was clear performance degradation with two and four GPUs. The\
    \ authors explain that this degradation also shows up in the official TF results,\
    \ but at a smaller scale. Nonetheless, they attributed this difference to the\
    \ low throughput of their network storage, instead of GPU overhead. Following\
    \ these experiments, they compared the udocker and Singularity container performances\
    \ of Resnet-50 and AlexNet using the ImageNet data set. Their results did not\
    \ indicate any significant performance difference between the two runtimes. Next,\
    \ they evaluated the two neural networks with ImageNet data on 1, 2, and 4 NVIDIA\
    \ GTX 680 Ti GPUs, achieving slightly more than double the performance obtained\
    \ in the K80-based setup. In another experiment, to detect differences in accuracy\
    \ between different GPU counts, they performed long-term training tests of Resnet-50\
    \ (ImageNet data set). Executions with udocker, on both K80 (5 epochs) and GTX\
    \ 980 Ti (10 epochs) setups, did not show any differences between executions with\
    \ 1, 2, and 4 GPUs. Moreover, the training time in the K80 setup showed very good\
    \ linear scaling, while the 980 Ti setup had very similar scaling to the shorter\
    \ runs. Finally, in an evaluation of the training loss in the 980 Ti setup, they\
    \ found very similar behaviors in the learning curves for all three GPU counts.\
    \ Muscianisi et al. [62] evaluated the performance overhead of containerized TF\
    \ using GPU accelerators. For this purpose, they built a Singularity image from\
    \ the official TensorFlow Docker image and used it to train different neural networks,\
    \ using the ImageNet data set. To compare the containerized and the officially\
    \ reported TF performance, they ran Resnet-50 with batch size 32, using 1 to 3\
    \ NVIDIA K80 GPUs. Then, to compare containerized and bare-metal executions, they\
    \ trained six different neural networks, using 1, 2, 3, or 4 GPUs, with batch\
    \ sizes 32, 64, and 128 per GPU. In both experiments, the Singularity container\
    \ did not introduce any relevant performance overhead. Newlin et al. [63] analyzed\
    \ the performance trade-offs of executing AI workloads in a containerized environment.\
    \ They compared the performance of benchmarks from MLPerf running natively and\
    \ in a Singularity container. The experiments employed two single-GPU nodes, equipped\
    \ with NVIDIA Tesla P100 GPUs, and three multi-GPU nodes, one with 8 \\\\(\\\\\
    times\\\\) NVIDIA Titan V, another with 4 \\\\(\\\\times\\\\) NVIDIA Titan X,\
    \ and a third one with 4 \\\\(\\\\times\\\\) NVIDIA GTX 1080 Ti. The evaluation\
    \ metric was time-to-accuracy (TTA), which is the time the benchmark takes to\
    \ reach a target level of accuracy. The Sentiment Analysis and Translation benchmarks\
    \ showed no statistically significant difference between container and native\
    \ execution. On the other hand, RNN Translator showed statistically significant\
    \ overhead for Singularity on multi-GPU executions. Moreover, Recommendation showed\
    \ slightly statistically significant overhead for native single-GPU execution\
    \ (i.e., native performance was worse than in the container). This last benchmark\
    \ was not evaluated over multiple GPUs. Ruhela et al. [74] evaluated GPU-based\
    \ executions of MILC and VPIC on Singularity and bare-metal. Both of them were\
    \ evaluated on a cluster with have four NVIDIA V100 GPUs per node, while VPIC\
    \ was also evaluate on a cluster with four NVIDIA Quadro RTX 5000 GPUs per node.\
    \ Both clusters are equipped with InfiniBand interconnects, and all executions\
    \ utilized 256 GPU. MILC on the V100 cluster presented less than 4% overhead for\
    \ singularity, while VPIC presented a slight overhead on the V100 cluster (\\\\\
    (\\\\approx\\\\) 1.96%) and a negligible overhead on the RTX 5000 cluster (\\\\\
    (\\\\approx\\\\) 0.65%). Lee et al. [49] evaluated the GPU-based performance of\
    \ three deep-learning workloads and three HPC workloads when using gShare, their\
    \ proposed framework for managing GPU memory allocation when multiple containers\
    \ share the same GPU. Their results displayed negligible performance overhead\
    \ when using gShare compared to nvidia-docker without gShare. As seen above, container\
    \ technology is capable of providing similar GPU computation performance to the\
    \ native environment. Moreover, among the few exceptions, at least one was due\
    \ to issues unrelated to containers (network file system). Therefore, we can conclude\
    \ that containers are a suitable environment to execute applications that rely\
    \ on GPU accelerators. 8.5 Memory overhead As part of their evaluation of containers\
    \ in an OpenPOWER platform, Kuity and Peddoju [43] ran 20 executions of the STREAM\
    \ (array size 23283 GB) and RandomAcces benchmarks (from HPCC) on a system equipped\
    \ with an IBM Turismo SCM POWER8 CPU (3.857GHz, 8 cores \\\\(\\\\times\\\\) 8\
    \ threads, 64 kB L1 data cache, 32 kB L1 instruction cache, 512 kB L2, 8 MB L3),\
    \ 128 GB of RAM, and 10 Gb/s Ethernet. Among the three tested variants, EP-STREAM\
    \ and SingleSTREAM (single thread) obtained similar performances in both container\
    \ and bare-metal environments, with VM slight behind these two. StarSTREAM, however,\
    \ presented performance degradation as the thread count was increased, due to\
    \ contention in the shared L3 cache. In addition, both the regular RandomAcess\
    \ and the StarRandomAccess benchmarks scaled well in all three environments. For\
    \ small thread counts, MPIRandomAccess achieved slightly higher performance on\
    \ bare-metal. As the number of threads increased, however, its bare-metal and\
    \ container performances converged. The VM environment, on the other hand, presented\
    \ performance overheads up to 52.6% (32 threads) when running MPIRandomAccess.\
    \ Arango et al. [3], analyzed the memory performance of LXC, Docker, and Singularity.\
    \ The latter achieved slightly higher memory bandwidth than bare-metal on STREAM,\
    \ while LXC had a significant overhead and Docker was the worst (\\\\(\\\\approx\
    \ {{36}{\\\\%}}\\\\) average overhead). Brayford et al. [12] detected negligible\
    \ memory overhead when using Charliecloud to run neural networks such as Resnet-50\
    \ and AlexNet. Sande Veiga et al. [77] evaluated the memory bandwidth of Singularity\
    \ containers through experiments executed on 48 cores of the FinisTerrae II cluster.\
    \ The results of STREAM, with a non-cacheable array size, showed negligible bandwidth\
    \ differences between containerized and native executions. Besides pure CPU performance,\
    \ Torrez et al. [84] evaluated the impact of Charliecloud, Shifter, and Singularity\
    \ on memory performance. In addition to running STREAM, they separately tracked\
    \ the memory consumption by the container runtimes and by HPCG. They presented\
    \ similar performance results in all four environments, but memory consumption\
    \ between container runtimes differed by hundreds of megabytes. Nevertheless,\
    \ the authors considered the differences irrelevant, representing at most 1-2%\
    \ of the total available RAM. Hu et al. [35] also employed STREAM, in their evaluation\
    \ of Singularity. Their results show very close containerized and native performances,\
    \ with a maximum overhead of 0.506%, for the copy operation. Besides their CPU\
    \ overhead evaluation, Gantikow et al. [24] employed the STREAM benchmark to analyze\
    \ the memory performance of Podman containers. Their results show less than 1%\
    \ memory bandwidth difference between the native, Singularity, and Podman environments.\
    \ Thus, indicating no significant memory overhead from containerization. As part\
    \ of their performance evaluation of medium and large scale containerized workloads,\
    \ Ruhela et al. [74] evaluated the memory consumption when running MILC with up\
    \ to 14K processes on a cluster composed of 56-core nodes. Their results for Charliecloud\
    \ containers were nearly identical to native. Liu and Guitart [52] measured the\
    \ effects on memory bandwidth from running multiple containers per nodes, with\
    \ Docker and with Singularity instances. They ran the HPCC’s EP-STREAM benchmark\
    \ on a five node HPC cluster with an InfiniBand interconnection, while increasing\
    \ the number of containers until there was one process per container. Their results\
    \ showed that EP-STREAM performed similarly in all test cases. The experiments\
    \ summarized above show that containers can provide near-native memory (RAM) performance\
    \ to the encapsulated applications. Even though there is not a clear reason for\
    \ containers to cause this type of overhead, there were a few experiments that\
    \ measured significant overheads. One of these results [43] was due to contention\
    \ in the L3 cache when increasing the number of threads of STREAM. It is not clear,\
    \ however, what would cause this issue to be more exacerbated when inside the\
    \ container. Another work [3] encountered significant overheads for LXC and Docker\
    \ containers, but not for Singularity. In this case, Singularity’s advantage maybe\
    \ due to its more permissive default configuration. If so, it may be possible\
    \ to improve the performance of LXC and Docker by configuring them to behave more\
    \ similar to Singularity. 9 Overall application overhead In the previous section,\
    \ we discussed experiments whose goal was to evaluate the impact of containers\
    \ on the performance of operations related to specific system resources. Now,\
    \ we present experiments whose goal, as stated in the respective work, was to\
    \ evaluate the performance overhead on the application as a whole. In this context,\
    \ we use the term “application” to refer either to a synthetic benchmark or to\
    \ a real, full-fledged, application. It is important to note that some applications\
    \ may also appear in other sections, since they may have been used in experiments\
    \ whose goal was resource-specific (e.g., to compare different networking configurations).\
    \ As we have seen in Fig. 3, 35 of our selected works tackle this challenge. Thus,\
    \ it represents the largest portion of the overhead-related works. Moreover, each\
    \ work may include many of such experiments, which leads to a total of 72 experiments\
    \ (42 applications and 30 benchmarks). Looking at the complete set, however, we\
    \ began to see similarities among most results. Therefore, to avoid having a disproportionately\
    \ long section, we decided to summarize these works based on the benchmarks or\
    \ applications they employed, and on the measured overheads. The total number\
    \ of application and benchmark containerization overhead experiments is displayed\
    \ in Fig. 4. Additionally, in Tables 2 and 3, we list the experiments that employ\
    \ each application or benchmark. They are identified by a citation to the work\
    \ where they were performed, and classified into four levels of overhead, corresponding\
    \ to the best result found in that work. Considering an overhead \\\\(o\\\\) the\
    \ levels are (1) High (\\\\(o>{15}{\\\\%}\\\\)), Significant/Considerable (\\\\\
    ({5}{\\\\%}<o\\\\le {15}{\\\\%}\\\\)), Small (\\\\({2}{\\\\%}<o\\\\le {5}{\\\\\
    %}\\\\)), and Negligible/None (\\\\(o\\\\le {2}{\\\\%}\\\\)). As a remark, we\
    \ note that 9 experiments, from 4 different works, did not compare the performances\
    \ of container-based and bare-metal executions. Fig. 4 Application and benchmark\
    \ experiment counts by level of overhead. The shaded area represents the total\
    \ counts Full size image Table 2 Application-based experiment counts by level\
    \ of overhead Full size table Table 3 Benchmark-based experiment counts by level\
    \ of overhead Full size table Looking at this data, it becomes clear is that in\
    \ the majority of experiments, \\\\(\\\\approx {{78}{\\\\%}}\\\\) (56 of 72),\
    \ container-based executions presented negligible or small performance overheads\
    \ over bare-metal. Moreover, if we disregard the 9 experiments that did not actually\
    \ evaluate the overhead over bare-metal, this proportion grows up to \\\\(\\\\\
    approx {{89}{\\\\%}}\\\\) (56 of 63), with \\\\(\\\\approx {{74.60}{\\\\%}}\\\\\
    ) measuring negligible or absent overheads, and \\\\(\\\\approx {{14.29}{\\\\\
    %}}\\\\) measuring small overheads. Only 7 of the 72 experiments (\\\\(\\\\approx\
    \ {9.72\\\\%}\\\\)) resulted in significant or high overheads, representing only\
    \ \\\\(\\\\approx {11.11\\\\%}\\\\) of the ones that actually compared containers\
    \ with bare-metal. Below, we discuss these experiments and try to explain these\
    \ overheads, when possible. Azab [4] evaluated the containerization overhead of\
    \ Bowtie, a short DNA sequence aligner, running 2, 4, 8, 16, and 32 processes\
    \ on two 16-core nodes. Its container-based executions incurred high overheads\
    \ when executing more than 2 processes, with both Docker (\\\\(\\\\approx {26.9{-}70\\\
    \\%}\\\\)) and Socker (\\\\(\\\\approx {42.3{-}70\\\\%}\\\\)). Interestingly,\
    \ Socker performed similar to Docker when executing 1, 2, 16, and 32 processes.\
    \ At first glance, these overheads may seem really high, but the presented execution\
    \ times include the container deployment, start-up and removal times. Therefore,\
    \ these results do not represent the actual performance of the application. Moreover,\
    \ the container images were removed at the end of each job and, consequently,\
    \ redeployed for every new container job. In another experiment, Steffenel et\
    \ al. [82] measured significant overheads (up to 7.8% with 6 processes) running\
    \ WRF (Weather Research and Forecasting model) inside containers, even though\
    \ other works [35, 89] managed to achieve negligible overheads for this application.\
    \ By analyzing execution traces, Steffenel et al. [82] noticed that a long time\
    \ was spent on MPI_Wait events. Therefore, these overheads are likely related\
    \ to communication. They attributed this issue to the Docker Overlay Network they\
    \ employed in their Docker Swarm container cluster. Among the experiments with\
    \ benchmarks, Chung et al. [21] found high overheads when running HPL inside Docker\
    \ containers. This is unexpected, since containerization should not significantly\
    \ slow down to such a computation-intensive benchmark. They compared bare-metal,\
    \ Docker, and VM-based executions with different input sizes, on two nodes equipped\
    \ with an Intel Xeon 2670 CPU (2.6GHz, 8 cores, 16 threads), 64 GB RAM, and 1\
    \ Gb/s Ethernet. Since all configurations were tested in the same nodes, and HPL\
    \ is not communication intensive, the slow interconnection should not be a major\
    \ source of overhead. The real issue seems to be that HPL was executed with 32\
    \ processes, which is double the number of CPU cores in their testbed. Even though\
    \ the two CPUs can simultaneously run 32 threads, via simultaneous multi-threading\
    \ (SMT), such an optimized compute-intensive application should keep the CPU’s\
    \ functional units busy most of the time. Therefore, leaving little opportunity\
    \ for multi-threading. This work also evaluated Docker-based executions of HPL,\
    \ with 32 processes, and increasing container counts. This resulted in increasing\
    \ performances up to 16 instances (one per physical core). With 32 instances,\
    \ however, performance went down by \\\\(\\\\approx {{61.5}{\\\\%}}\\\\). Another\
    \ interesting result from this work is that Docker was able to run HPL with input\
    \ sizes larger than 85% of the available RAM, which was not possible inside a\
    \ VM. In another experiment, Chung et al. [21] measured high overheads for the\
    \ execution of Graph’500 inside a container. They evaluated its execution with\
    \ graph scales from 20 to 26 (controlled by the variable SCALE). Overall, both\
    \ Docker and bare-metal performances slowly increased for scales 20-22, remained\
    \ mostly stable for scales 20-22, and significantly increased for scales 25 and\
    \ 26. The Docker execution overhead, however, significantly changed between different\
    \ scales. The largest variability was found in the 20-24 interval, where it goes\
    \ up and down inside the \\\\(\\\\approx {3.8-13.3\\\\%}\\\\) range. For larger\
    \ scales, the performances for both Docker and bare-metal significantly improve,\
    \ but bare-metal presents higher, seemingly linear, speedup up. Docker’s performance,\
    \ on the other hand, does not increase as much, and further decelerates when going\
    \ from SCALE 25 to 26, with respective \\\\(\\\\approx {{15.9}{\\\\%}}\\\\) and\
    \ \\\\(\\\\approx {{34.24}{\\\\%}}\\\\) overheads. The process count for these\
    \ experiments is not specified by its authors. Therefore, we assume it is the\
    \ same as in their previous experiments (HPL). So, they may have, once again,\
    \ relied on SMT to run double the processes as the available CPU core count, which\
    \ could be causing the high overheads. Nevertheless, different from HPL, increasing\
    \ the number of Docker instances had no negative effect in the performance of\
    \ Graph’500. In fact, the best performance was achieved with 32 instances (one\
    \ process per container). A final interesting result from this experiment is that\
    \ Docker performed much closer to the VM than to bare-metal, only distancing itself\
    \ for scales 25 and 26. Besides experiments with Graph’500, Zhang et al. [97]\
    \ observed significant overheads in Singularity-based executions of NPB (class\
    \ D) (512 processes) on their Haswell platform. Among the programs included in\
    \ NPB, the overheads were \\\\(\\\\approx {{5.66}{\\\\%}}\\\\) for CG, \\\\(\\\
    \\approx {{5.88}{\\\\%}}\\\\) for MG, and \\\\(\\\\approx {{6.14}{\\\\%}}\\\\\
    ) for LU. BT and SP were not executed, and the plot in the paper does not display\
    \ any visible performance difference between bare-metal and container executions\
    \ of EP, FT, and IS (the actual values were not included in the text). Unfortunately,\
    \ no explanation was provided for the higher overheads for CG, MG, and LU. Muhtaroglu\
    \ et al. [61] observed significant overheads when running the Terasort benchmark\
    \ (random number sorting using Hadoop), on Scaleway C2S (4 cores) and C2M (8 cores).\
    \ Docker presented a significant overhead of \\\\(\\\\approx {6.9}{\\\\%}\\\\\
    ) (65 s) for the 10 GB input size. In contrast, with a 20 GB input, it presented\
    \ a small overhead (\\\\(\\\\approx 2.6\\\\%\\\\)). Moreover, on the same platform,\
    \ they achieved small overheads for both Teragen (<5%) and OpenFOAM DepthCharge2D\
    \ (3–4%). In another experiment, they ran two concurrent 10 GB Terasort jobs led\
    \ to \\\\(\\\\approx {{6.6}{\\\\%}}\\\\) shorter average execution times than\
    \ the 20 GB bare-metal case. On the other hand, the same strategy led the IO-intensive\
    \ Teragen to run 32% slower than the 20 GB configuration. Thus, indicating the\
    \ need to consider IO-intensiveness when mapping containers to hosts. Unfortunately,\
    \ the authors do not explain the significant overhead for the 10 GB Terasort.\
    \ Nevertheless, based on the fact that the 20 GB case obtained smaller overheads,\
    \ we formulate the hypothesis that the larger the problem size led to better overlapping\
    \ of computation and IO operations. If true, this would indicate Docker is causing\
    \ some kind of IO-related overhead (e.g., communication, storage). It is also\
    \ unfortunate that the sample sizes for these experiments is not provided. Therefore,\
    \ we cannot adequately infer the accuracy of the provided averages. Ruhela et\
    \ al. [74] found significant overheads for Charliecloud on tests with CPU-based\
    \ MILC running up to 140K processes at a cluster (28 cores \\\\(\\\\times\\\\\
    ) 2 sockets per node @ 2.7GHz, 100 Gb/s InfiniBand). This overhead was less than\
    \ 10% at various system scales, but this includes setup and teardown costs. According\
    \ to the authors, these would be amortized in real-world executions, ”where MILC\
    \ is allowed to run for multiple trajectories and several steps per trajectory.”\
    \ Based on the results in this section, the overall conclusion is that containers\
    \ are capable to execute HPC applications with negligible or small performance\
    \ overheads compared to bare-metal execution. This is demonstrated by the fact\
    \ that only 9 of the 63 experiments (from 32 works) that evaluated this aspect\
    \ resulted in significant or high overheads. In most cases where results indicated\
    \ higher overheads, their causes were not directly related to containerization.\
    \ Among these, in one work [4] the experiments were designed to aggregate the\
    \ execution time of the application with the container deployment and startup\
    \ times. Moreover, previously deployed container images were purposefully made\
    \ non-reusable. Similarly, Ruhela et al. [74] included the container setup and\
    \ teardown times in their measurements. In another case [82], the technology or\
    \ configuration choices were suboptimal for the test scenario. In addition, at\
    \ least one work [21] relied on hardware characteristics that may have caused\
    \ performance degradation. Finally, one work [61] presented an incomplete statistical\
    \ analysis, and another [97] did not provide enough information to explain the\
    \ significant overheads they encountered. 10 Other overhead Bahls [6] evaluated\
    \ the startup times of containerized versions of PISM, Quantum Espresso, and POP2\
    \ on Cray systems. They tested two techniques to access the host’s Cray MPI. In\
    \ general, the versions that rely on MPICH-ABI compatibility presented longer\
    \ startup times than the one that included Cray MPI libraries inside the container.\
    \ The authors mention, however, that the lower performance of the MPICH-ABI approach\
    \ may be due to a workaround that requires copying some libraries to a Lustre\
    \ file system. Higgins et al. [31] evaluated the deployment overheads of their\
    \ virtual container cluster (VCC) model, which is based on Docker and uses Weave\
    \ to create an overlay network. Their results show that VCC introduces overhead\
    \ at deployment, with the longest portion of time being spent on pulling the container\
    \ image. Moreover, their virtual node discovery process also added some overhead.\
    \ Sparks [80] evaluated the performance of Shifter, Singularity, runC and rkt\
    \ containers on Cray systems. Among other aspects, they analyzed the startup time\
    \ compared to the native Cray Linux Environment (CLE). Shifter and Singularity\
    \ performed well as the number of nodes increased, while runC had the largest\
    \ startup cost, because a complete image has to be copied to the host prior to\
    \ execution. rkt also took longer than the first two, due to having to create\
    \ a per-node instance of the container, in tmpfs. The authors remark on the possibility\
    \ to improve the startup time of runC by pre-fetching image data. Arango et al.\
    \ [3] measured the time to execute a simple operation (echo “Hello world!”), on\
    \ LXC, Docker. This includes starting, executing, and stopping the container.\
    \ According to the authors, the shutdown operation of the container is the slowest.\
    \ Based on their graphics, it seems that Docker is the slowest to run the full\
    \ set of operations (using the docker-run command), while LXC is slightly faster.\
    \ It also seems that the time to run the command on an already running container\
    \ is negligible for both runtimes. Belkin et al. [7] evaluated the startup time\
    \ of Shifter container jobs launched via a specially configured PBS resource manager.\
    \ This can be either automated by PBS, or done by passing the arguments directly\
    \ to Shifter. They tested a user-defined image (UDI) with 26 MB and another with\
    \ 1.7 GB. First, to evaluate how job startup time depends on the number of nodes,\
    \ they launched a Shifter job that exploits only one processor per node. The results\
    \ suggest that the startup time of a Shifter job is practically independent of\
    \ UDI size. For less than 256 nodes, they found a sublinear dependence of the\
    \ job startup time on the node count, which became linear beyond 256 nodes, and\
    \ superlinear beyond 2048 nodes. Their second experiment used 80 nodes to evaluate\
    \ the dependence of Shifter job startup time on the number of MPI ranks per node.\
    \ Their results, once again, displayed startup times practically independent of\
    \ UDI size. When UDI arguments were specified to PBS at job submission time, the\
    \ startup times were the same, regardless of the number of processes per node.\
    \ When passing the arguments directly to Shifter, however, the startup time became\
    \ dependent on the number of MPI ranks per node. Ramon-Cortes et al. [71] evaluated\
    \ the deployment of MatMul (a matrix multiplication code), and of an embarrassingly\
    \ parallel application in a KVM cluster managed by Openstack, a Docker cluster\
    \ built with Docker Swarm, a third one built with Mesos, and on an execution with\
    \ Singularity. KVM’s creation phase takes 109 s, and its deployment takes \\\\\
    (\\\\approx {{30}\\\\,{\\\\hbox {s}}}\\\\) in the best case, or \\\\(\\\\approx\
    \ {{98}\\\\,{\\\\hbox {s}}}\\\\) if the image needs to be downloaded. For Docker\
    \ with Openstack the creation phase took 24 s with the base image available, and\
    \ up to 82 s otherwise. The deployment phase for the Docker setup took from \\\
    \\(\\\\approx {{5}\\\\,{\\\\hbox {s}}}\\\\), when all layers were available in\
    \ the Docker engines, and up to \\\\(\\\\approx {{98}\\\\,{\\\\hbox {s}}}\\\\\
    ) when they were not. The Mesos setup used its default Docker execution, thus\
    \ the creation and deployment phases lasted the same as in the previous result.\
    \ The creation phase for Singularityis the same as for Docker, but, the image\
    \ must be imported and converted prior to execution. With the image ready, the\
    \ deployment is considerably faster than Docker’s. The compilation overhead of\
    \ MasterCode in a containerized environment was evaluated by Gomes et al. [26].\
    \ This complex application interfaces several scientific codes, and is directly\
    \ dependent on a wide range of tools and libraries. Moreover, it includes several\
    \ other applications and their dependencies. The average compilation times for\
    \ udocker in P1 mode (based on ptrace) were \\\\(\\\\approx {{30}{\\\\%}}\\\\\
    ) longer than on the native environment. By using a newer version of udocker,\
    \ which implements SECCOMP filtering to reduce the number of traced system calls,\
    \ this performance loss was reduced to \\\\(\\\\approx {{5}{\\\\%}}\\\\). Muhtaroglu\
    \ et al. [61] evaluated the deployment overhead of Docker containers employed\
    \ to run a CPU-intensive workload (OpenFOAM CFD). They observed deployment times\
    \ 10–15\\\\(\\\\times\\\\) shorter than with VMs. Thus, concluding that “dockerized”\
    \ HPC can be set-up and deployed much faster than VM-based alternatives. They\
    \ also remark that any remaining potential performance gains in containerized\
    \ application execution would be negligible compared to this huge improvement\
    \ in deployment times. Besides, they add, this enables a quick setup-test-reconfigure-retest\
    \ cycle, thus removing IT bottlenecks and, consequently, allowing engineers and\
    \ scientists to solve real scientific and sectoral problems. Rudyy et al. [72]\
    \ evaluated the deployment overhead of Shifter, Singularity, and Docker. The first\
    \ two presented negligible overheads, but Docker’s was much higher (103 s, for\
    \ 112 MPI processes). In an evaluation of the execution of real-world scientific\
    \ applications on Singularity, Wang et al. [89] also took time measurements outside\
    \ the container (Singularity startup, image loading, application startup, IO,\
    \ and so on). They detected a big overhead for loading the largest image, which\
    \ includes a built-in copy of the Intel compiler. For other two images, which\
    \ mount-bind the host’s compiler and MPI, this overhead was negligible. Additionally,\
    \ two of the applications presented much slower start-ups on the image without\
    \ binds than on the other two. Therefore, the authors recommend keeping container\
    \ images small. Finally, they observed that increasing the node count did not\
    \ affect the overall overheads. As part of their evaluation of containerized HPC\
    \ on an ARM-based cluster, Peiro Conde [67] compared the energy consumption of\
    \ container-based and bare-metal executions of GROMACS. They concluded that there\
    \ is almost no difference in energy consumption between the two. Besides that,\
    \ a very basic test with a single container executing only the sleep command,\
    \ demonstrated an average startup time of 6 seconds for Singularity. Another experiment,\
    \ creating several MPI ranks that, once again, only execute the sleep command,\
    \ has shown increasing startup times for Singularity as the number of ranks increases.\
    \ It is worth noticing, however, that this increase is sublinear (from \\\\(\\\
    \\approx {{6}\\\\,{\\\\hbox {s}}}\\\\) to \\\\(\\\\approx {{17}\\\\,{\\\\hbox\
    \ {s}}}\\\\) when going from 1 to 256 cores), and that the startup time for bare-metal\
    \ also increases, albeit at a smaller rate. Besides evaluating communication and\
    \ overall execution overhead, Ruhela et al. [73] measured the total memory consumption\
    \ when running the MILC benchmark natively, and in a Singularity or Charliecloud\
    \ container. Those experiments launched 69, 984 processes on 1296 nodes of the\
    \ Frontera supercomputer. The authors observed similar memory utilization for\
    \ all three cases, at various problem sizes. As part of an evaluation of containers\
    \ applied to medium and large-scale HPC workloads, Ruhela et al. [74] measured\
    \ the container setup and teardown times, and the containerization overheads on\
    \ a supercomputer workload. By running the IMB MPI_Init benchmark with 3584 processes\
    \ (64 nodes \\\\(\\\\times\\\\) 56 processes) to 229,376 processes (4096 nodes\
    \ \\\\(\\\\times\\\\) 64 processes), they measure setup and teardown overheads\
    \ between 6% and 14% for Charliecloud. To evaluate a supercomputer workload, they\
    \ simulated a typical job arrival pattern in supercomputer centers, based on data\
    \ from previous works, and employing an exponential distribution to the job durations.\
    \ In this experiment, Charliecloud presented up to 4% accumulated overhead for\
    \ all containerized against all bare-metal runs. About this result, Ruhela et\
    \ al. [74] remark that higher problem sizes and job lengths are more favorable\
    \ to containers, since this results in less jobs per unit of time. Another interesting\
    \ overhead presented by Ruhela et al. [74] occurred in their IMB MPI_Alltoall\
    \ tests where, despite both Singularity and Charliecloud achieving on par communication\
    \ performance with bare-metal, and similar CPU usage and memory occupation, the\
    \ former incurred 10% more overheads. They attributed this difference to a higher\
    \ number of page faults, context switches and file IO operations. Okuno et al.\
    \ [65] evaluated the potential increase in total throughput when running an ensemble\
    \ o multiple containerized molecular dynamics (MD) simulation in parallel, measured\
    \ in simulated ns per day. This means running multiple simulations in parallel,\
    \ each one inside a different container. For this purpose, they ran two different\
    \ simulations on OpenMM and GROMACS, on a single computer node (Intel Xeon Gold\
    \ 6148, 20 cores \\\\(\\\\times\\\\) 2 sockets @ 2.40GHz, 2 threads per core,\
    \ 96 GB DDR4 per CPU socket). First, they evaluated single container speedup with\
    \ an increasing number of threads (relative to one thread). GROMACS scaled much\
    \ better than OpenMM, with one of the simulations on OpenMM only scaling up to\
    \ 20 threads. In a second experiment, they evaluated the effect of increasing\
    \ the number of containers (up to 20) while proportionally decreasing the number\
    \ of threads per container. Both OpenMM and GROMACS achieved better ensemble throughput\
    \ when increasing the number of containers. Moreover, OpenMM presented a larger\
    \ increase in total throughput when increasing the number of containers, due to\
    \ its worse single-container scalability. On the other hand, GROMACS presented\
    \ a much more pronounced reduction in speedup in each individual simulation. Last,\
    \ they analyzed the effects of simultaneous multi-threading (SMT) on ensemble\
    \ throughput, by mapping containers to different logical cores in the same physical\
    \ CPU core. GROMACS presented no improvement, while OpenMM presented up to 2.22\
    \ times larger ensemble throughput with two containers per physical core on a\
    \ total 10 cores. LMPX [91], a container implementation aimed at composability,\
    \ presented the lowest overhead for starting and immediately destroying a container\
    \ compared to Singularity, Docker, udocker, and Podman. The second place was taken\
    \ by Singularity, which was more than 6 times slower. Moreover, LPMX was also\
    \ shown to incur in considerable overhead for software installation and compilation.\
    \ Its authors argue this is not a big issue since it should be a one time cost\
    \ that would be diluted along multiple executions. Most of the experiments summarized\
    \ in this section analyzed overheads on container deployment and startup. In this\
    \ aspect, overall, we can see that containers perform better than VMs. Moreover,\
    \ in most cases, HPC-specific implementations performed better. Therefore, we\
    \ can say HPC-specific container implementations provide the best deployment times.\
    \ Among the other results, by integrating Shifter with PBS, one work [7] demonstrated\
    \ application startup times that are independent of container count. Another [43],\
    \ has shown inconclusive results (due to different execution environments) for\
    \ Shifter containers with a copy of vendor-optimized MPI compared to relying on\
    \ MPICH-ABI compatibility. A third one [26] found significant compilation overhead\
    \ for a physics application in an udocker container in P1 mode, but this seems\
    \ to be resolved in newer versions of udocker. One specific work [89] has shown\
    \ big overheads when including a large piece of software in the container, which\
    \ reinforces the importance of keeping container images small. One work [73] measured\
    \ similar to native memory utilization by an application running on Singularity\
    \ and Charliecloud containers. In experiments with Charliecloud, one work [74]\
    \ showed a small aggregated overhead for a simulated typical job arrival pattern,\
    \ and considerable overhead in an MPI_Alltoall benchmark. Finally, a work related\
    \ to genomic analysis workloads [65] demonstrated that running multiple containerized\
    \ application instances can increase ensemble throughput, but the benefit is larger\
    \ for applications with bad parallel scalability. 11 Application portability Portability\
    \ is arguably one of the main advantages containers bring to HPC. They allow the\
    \ application a high degree of independence from the underlying platform. In other\
    \ words, they enable the creation of User Defined Software Stacks (UDSS) [69],\
    \ including all the dependencies needed to run their application. Thus, eliminating\
    \ the need to install and configure these dependencies, as well as the application\
    \ itself, on every system where it needs to be executed. Besides advantages in\
    \ terms of system management, this feature reduces or eliminates the need for\
    \ rebuilding and reconfiguration when attempting to run the application on a new\
    \ system. Weidner et al. [90] discussed several challenges faced by traditional\
    \ HPC platforms in order to provide support for application portability, and for\
    \ the efficient execution of what they call ”second generation applications”.\
    \ These have characteristics such as (1) typically non-monolithic designs; (2)\
    \ dynamic runtime behavior and resource requirements; or (3) relying on high-level\
    \ tools and frameworks to manage computation, data and communication. In addition,\
    \ they may explore new computation and data handling paradigms, or operate on\
    \ a federated context, spanning multiple geographically distributed HPC systems.\
    \ According to the authors, the barriers to running these applications in HPC\
    \ systems stem from their static resource allocation policies, and the difficulty\
    \ of (1) handling high volumes of data, (2) efficiently handling intermediate\
    \ data, (3) inspecting the progress of executions of coupling parts of application\
    \ concurrently running on different platforms, and (4) porting the applications\
    \ to new platforms. After analyzing several details of these challenges and recommending\
    \ approaches to mitigate them, the authors presented cHPC (container HPC), which\
    \ consists of OS-level services and APIs that run alongside and can integrate\
    \ with existing job schedulers, via LXC containers. This way, it can provide isolated\
    \ user-deployed application environment containers, with support for introspection,\
    \ and for using of cgroups for resource throttling. Applications, either containerized\
    \ or native, continue being submitted via the HPC job scheduler. Container and\
    \ node metrics are collected in real-time, to provide platform and application\
    \ introspection. This data can be streamed to different types of consumers (e.g.,\
    \ users, monitoring systems, and the application itself), and is also stored in\
    \ a database. The latter is used to compare platform data with user-set thresholds,\
    \ and to send alarms whenever these are violated. According to the authors, a\
    \ version of cHPC has been deployed in a virtual 128-core Slurm cluster deployed\
    \ on 8 dedicated Amazon EC2 instances. In addition, they claimed to be in the\
    \ process of deploying it in a 324-node, 1536-core cluster. Moreover, they mention\
    \ the non-invasiveness of their implementation as being a key asset, and being\
    \ unable to optimize the HPC jobs that run alongside the containerized applications\
    \ as being a suboptimal aspect of their system. Hale et al. [28] reported on how\
    \ the FEniCs project uses containers to distribute its software, packaged together\
    \ with its entire execution environment. This way, they facilitate its installation\
    \ and execution, resulting in increased portability, and making it accessible\
    \ to more users. Moreover, packaging the entire environment in a container also\
    \ reduces the probability of crashes due to software incompatibility. de Bayser\
    \ and Cerqueira [22] developed an approach to facilitate the integration of Docker\
    \ with MPI, which they implemented via a wrapper for the mpiexec command of MPICH.\
    \ This wrapper identifies the number of requested hosts and uses Docker Swarm\
    \ to create a container cluster, where the MPI application will be executed. Container\
    \ placement and load balancing is left entirely to Docker Swarm, with no user\
    \ input. Unfortunately, this work does not provide any evaluation of this mechanism.\
    \ Nguyen and Bein [64] used Docker Swarm mode to automate MPI cluster setup and\
    \ deployment, thus providing ”a ready-to-use solution for developing and deploying\
    \ MPI programs in a cluster of Docker containers running on multiple machines.”\
    \ Their goals were to provide a consistent development environment for MPI projects,\
    \ and a simple workflow to deploy MPI applications in a real cluster. Their solution\
    \ revolves around providing a base container image that can be used to create\
    \ a consistent development environment, by running it in interactive mode and\
    \ mounting the project directory inside the container. For deployment, the developer\
    \ would create a new layer on top of a second base image, which is pre-configured\
    \ to facilitate its deployment and execution of the MPI application in the cluster,\
    \ using Docker Swarm. A few provided shell scripts are used to facilitate this\
    \ process. Younge et al. [93] worked on deploying the same Docker image on a laptop,\
    \ on two clusters at Sandia National Laboratory, and on a cloud environment (AWS).\
    \ The idea is to configure the container on the laptop, and then make it available\
    \ for other platforms to execute it using Singularity or Docker. They used a single\
    \ Docker image to execute HPCG and IMB, with Singularity in the clusters, and\
    \ Docker on AWS. According to its authors, this was the first work to evaluate\
    \ Singularity on a Cray XC-series testbed. This required a few modifications to\
    \ the Cray Native Linux environment, as well as making Cray’s custom software\
    \ stack accessible inside the container. Khan et al. [41] presented four use cases\
    \ to demonstrate containerization efforts of the CloudLightning project. The chosen\
    \ applications are either CPU-based, or employ hardware accelerators such as FPGA-based\
    \ DFEs, Intel Xeon Phi, or NVIDIA GPUs. Their goal is to facilitate access to\
    \ these, by employing an application-as-a-service model coupled with automated\
    \ resource allocation based on QoS parameters. This approach is expected to lower\
    \ the technical barriers and to greatly reduce the required time investment to\
    \ start using these applications, leading to more efficient resource usage. Performance\
    \ results were presented in Sects. 8.4 and 9. Chen et al. [19] created the Build\
    \ and Execution Environment (BEE), which aims to build similar HPC-friendly environments\
    \ across different platforms. It uses standard docker images, but hides launch\
    \ details from users. Additionally, it exposes access to host-level networks and\
    \ can also configure shared directories. Its structure is composed of four back\
    \ ends, which run dockerized applications. On HPC clusters, we would employ BEE-Charliecloud,\
    \ while BEE-VM allows for hardware architecture independence, by running Docker\
    \ on top of a virtual-machine. BEE-AWS enables execution on AWS, by using Docker\
    \ on top of a standard AWS virtual machine. Last, we have BEE-Openstack, which\
    \ uses Docker to execute the application on top of the Openstack cloud OS. The\
    \ four back ends were evaluated in terms of communication bandwidth and latency,\
    \ storage read and write performance, the execution times of a CPU-intensive and\
    \ a memory-intensive OpenMP benchmarks, and of a memory-bound plasma physics code\
    \ (MPI and pthreads). Based on the results, the authors concluded that BEE can\
    \ achieve comparable performance to native execution. Sampedro et al. [76] presented\
    \ a containerization-based continuous integration and continuous delivery (CI/CD)\
    \ architecture to ”support the development and management of scientific codes\
    \ and software environments for HPC end users.” The paper presents a use case\
    \ of a CI/CD pipeline to automate the benchmarking of the MFIX-Exa application.\
    \ Jenkins is used to automatically pull new commits to build the application in\
    \ a Singularity container, and to test the new versions. If all tests are passed,\
    \ Jenkins pushes the new container into the registry. Jenkins can also automate\
    \ performance tests of the new container, run scripts to plot the results, and\
    \ also check correctness of the results. It may also automatically generate historical\
    \ plots, to compare different versions, or to associate performance changes to\
    \ specific commits. This combination of Singularity and Jenkins enables automatic\
    \ code testing, facilitating the sharing of containers through the Singularity\
    \ register. Moreover, Jenkins’s automation eliminates most of the manual steps\
    \ in the process. Belkin et al. [7] presented the efforts on supporting Shifter\
    \ containers on the Blue Waters supercomputer (a Cray system). They modified the\
    \ PBS resource manager to execute special prologue and epilogue scripts. These\
    \ perform the automatic setup and teardown of container environments on the computation\
    \ nodes. Collected data on the usage of Shifter on Blue Waters show that the majority\
    \ of the node hours were spent by the LIGO project, for the execution of High-Throughput\
    \ Computing (HTC) codes. This project uses the Open Science Grid (OSG), and Shifter\
    \ was employed to enable them to use an OSG-ready Docker image on Blue Waters,\
    \ ”eliminating the need to adapt the image for each resource provider.” Other\
    \ projects using Shifter on Blue Waters include ATLAS and NanoGrav, which are\
    \ also based on OSG. Besides, Shifter was also used to execute the QWalk and PysCF\
    \ electron structure computational physics and chemistry codes, and PowerGRID,\
    \ an MPI application for medical resonance image construction which can leverage\
    \ GPU accelerators. Ramon-Cortes et al. [71] proposed a methodology for integrating\
    \ container engines with parallel programming models and runtimes. Their prototype\
    \ integrates the COMPSs (COMP Superscalar), a task based programming model, with\
    \ containerization. This was achieved by extending its integration with cluster\
    \ resource and queue managers to support the deployment and execution of containerized\
    \ applications. COMPSs is responsible for the scheduling and execution of the\
    \ tasks in the computing nodes. The produced prototype is able to employ different\
    \ container engines to build a Docker or Mesos cluster, or to employ Singularity,\
    \ and to transparently deploy and execute the containers in an HPC cluster. Additionally,\
    \ the COMPSs prototype is able to execute either with a static or with a dynamic\
    \ pool of containers. The latter enables adaptation to resource usage and availability.\
    \ Based on the experience of PRACE (Partnership for Advanced Computer in Europe)\
    \ in supporting Singularity containers in HPC clusters, Sande Veiga et al. [77]\
    \ presented a paper on using containers to deploy and integrate complex software\
    \ and their dependencies. They specifically focus on maximum portability without\
    \ wasting computational capacity. This work presents different approaches and\
    \ configurations for running Singularity containers in HPC environments, each\
    \ with its advantages and drawbacks, in terms of portability and performance.\
    \ These include using the same MPI version in container and host, or relying on\
    \ ABI compatibility to support mixed MPI versions, or using MPI binding (mount\
    \ binding the host’s MPI libraries). They also compared using the vendor’s MPI\
    \ implementation versus OpenMPI (same version on host and container). On their\
    \ tests, the container with Intel MPI achieved \\\\({\\\\approx {5}}\\\\) times\
    \ faster execution of Quantum Espresso than the one with OpenMPI. This reinforces\
    \ the importance of hardware specific customization (e.g., vendor drivers and\
    \ libraries) and optimization of containers, even if it hurts portability. In\
    \ their conclusions, the authors state that the minimal performance loss from\
    \ containerization is more than justified by ”the time that will be saved using\
    \ this novel form of software deployment.” Hu et al. [35] evaluated the portability\
    \ of Singularity images, by testing a container image for the VASP application\
    \ on a different server than the one it was built for. They were able to attain\
    \ stable executions on the new server, with good performance results. Moreover,\
    \ they tested Singularity’s ability to use Docker images, by building a Singularity\
    \ container from a WRF image pulled from Docker Hub, and evaluating the performance\
    \ overhead of executing this application inside this container (see Sect. 9).\
    \ Piras et al. [68] presented an approach to accommodate service-oriented and\
    \ container-oriented workloads into existing HPC infrastructure in a minimally\
    \ disruptive way. They proposed a method to expand Kubernetes clusters running\
    \ in a cloud infrastructure onto HPC resources managed by a batch-style scheduler.\
    \ This was achieved by implementing a hybrid scheduling technique that enables\
    \ the setup and tear-down of Kubernetes worker nodes onto HPC nodes allocated\
    \ through the Grid Engine (GE) batch job scheduler. The authors claim this approach\
    \ is already in use to run services and a WRF workflow on an always-on cluster,\
    \ that is deployed on a private Openstack cloud. This allows the Kubernetes cluster\
    \ to be expanded on-demand, when the resources on the Openstack cluster are insufficient\
    \ to execute a workload. According to the authors, this expansion capability was\
    \ used to run the WRF workflow on hundreds of MPI-connected cores. Grupp et al.\
    \ [27] proposed packaging performance-optimized versions of commonly utilized\
    \ machine-learning software into containers, to serve as building-blocks for complex\
    \ neural network applications. They started from an optimized container image\
    \ provided by the TensorFlow project. These containers were tested by executing\
    \ two different neural network models on two different multi-GPU systems, in which\
    \ they displayed good speedup, and no overhead due to containerization. Michel\
    \ and Burnett [60] presented their effort at creating a base-container to provide\
    \ GPU-optimized common Compute Vision and Machine Learning libraries, which can\
    \ be used as the building-block for the interactive design and test of complex\
    \ analytics. Their starting point was the cuda_tensorflow_opencv container, which\
    \ contains many often used tools for ML research. They compiled various tagged\
    \ versions of the container, and made these builds available in Docker Hub. The\
    \ authors demonstrate how to run the container for use as a repeatable test framework,\
    \ besides using it as a base for a new container, once the repeatable tests are\
    \ done. They also demonstrated the integration of the Darknet “You Only Look Once”\
    \ algorithm (object detection and classification) into a new container. The base\
    \ image, cudnn_tensorflow_opencv, was created from the cuda_tensorflow_opencv\
    \ image, by installing the required cudnn packages from NVIDIA. They used this\
    \ Darknet container to perform object detection on a video file and display, frame-by-frame,\
    \ the bounding box and category name. According to the authors, this demonstrates\
    \ the easy abstraction of the GPU-optimized Docker image it is built from. They\
    \ also remark on the building-blocks approach as being ”the right one to design\
    \ a reproducible and reusable video streaming ML & CV HPC solution.” Moreover,\
    \ they note that it can be used as the foundation for novel analytics, reducing\
    \ the technical barrier of entry, and allowing researchers to focus on the analytics\
    \ instead of system deployment. Youn et al. [92] used Singularity containers to\
    \ distribute their DARE-MetaQA framework to nodes of two different HPC clusters\
    \ belonging to two institutions. This framework employs meta-learning neural network\
    \ models, and aims to enable large-scale question-answering. It uses PyTorch and\
    \ Dask to enable the concurrent execution of multiple models on distributed resources,\
    \ and to take advantage of multi-GPU environments. They chose to use containers\
    \ as a means to lower development costs, by not having to repeatedly deal with\
    \ dependency management and compatibility issues that arise from using multiple\
    \ HPC systems. Community Collections (CC) [56] combines Singularity with Lmod\
    \ to enable the sharing of software stacks across different HPC sites. Its goal\
    \ is to reduce the redundancy in building software required by researchers at\
    \ different sites. It handles the deployment of CC, of its dependencies, and of\
    \ Lua extensions to Lmod. The latter enables the creation of versionless modules,\
    \ whose software packages can be automatically updated as soon as a new version\
    \ becomes available (using nvchecker). CC is designed to work side-by-side with\
    \ the Spack and Easybuild package managers. It relies on community maintained\
    \ sources to retrieve and deploy containers in order to fulfill user requests.\
    \ More specific software installation is handled by Lmod. In addition, pre-built\
    \ containers can be shared between HPC sites. According to the authors, besides\
    \ HPC systems, CC is effectively deployed on cloud resources, having been tested\
    \ on Google Cloud Platform resources. An important limitation of CC is that it\
    \ does not provide an easy way to handle hardware-specific optimizations. In this\
    \ case, CC trades-off performance for portability. Canon and Younge [14] discussed\
    \ the obstacles to achieving the portability promised by containers in HPC, as\
    \ well as potential solutions. They argue that the mechanisms to achieve flexibility,\
    \ portability, and reproducibility, through containerization, come at the cost\
    \ of performance. Moreover, that the current solutions, such as bind-mounting\
    \ host libraries, have result in a trade-off between portability and performance.\
    \ They also highlight the lack of interoperability between container runtimes,\
    \ especially in the way they handle the mapping of host libraries into containers,\
    \ and the incorporation of specialized hardware. In relation to the bind-mount\
    \ approach, they point-out that differences in implementation and in user interfaces\
    \ make it difficult, or even impossible, to use the same container image with\
    \ different runtimes. In addition, they discuss issues due to the reliance on\
    \ ABI compatibility between host and container libraries. Moreover, they highlight\
    \ a series of circumstances where these two approaches may fail. As a solution,\
    \ Canon and Younge [14] proposed using the label capabilities of the OCI image\
    \ format to communicate container requirements to the runtime, in a standard way.\
    \ With this in place, if the runtime can meet the requirements, it would automatically\
    \ perform the necessary configuration at initialization. To demonstrate this approach,\
    \ they employed a Python script to retrieve metadata from an image and extract\
    \ labels that were used to select the appropriate Shifter modules for the container.\
    \ In addition, they discussed potential solutions to the reliance on ABI and version\
    \ compatibility between injected libraries and the container images. The most\
    \ promising one, according to them, would be a container compatibility layer,\
    \ in the form of a lower-level runtime library that would provide access to specialized\
    \ hardware. They highlight, however, some difficulties in making this solution\
    \ gain traction, like the need for all HPC container runtimes to agree to leverage\
    \ this tool, and for support from various vendors. They also mention a dual virtualization\
    \ approach, with containers on top of a lightweight hypervisor-based VM. It is\
    \ not clear, however, if the added performance overhead would not make this solution\
    \ unsuitable for HPC. To facilitate the execution of applications built with different\
    \ versions of the Cray Processing Environment (CPE) on the same system, Martinasso\
    \ et al. [57] proposed to encapsulate application, dependencies, and the required\
    \ version of the Cray Development Toolkit (CDT) in a container image. This image\
    \ would be used to build the application in a local machine. Due to the large\
    \ size of CDT, however, they created a Python script to extract the application’s\
    \ binaries and their dependencies. These can be placed in a new lightweight container,\
    \ which can be executed in a Cray HPC system, independent of which version of\
    \ CPE is in the hosts. They proposed two use cases for this approach. The first\
    \ is for research reproducibility, by enabling to keep a version of the application\
    \ compiled with a specific software environment. The other is for testing the\
    \ performance of an application with different CPE versions (e.g., to test performance\
    \ regressions, or to run it with an older version of CPE that provides better\
    \ performance). Another use-case they mentioned is testing new versions of the\
    \ CPE before updating the system. Researchers from LRZ (Leibniz Research Center)\
    \ and Intel proposed using containers to deploy AI frameworks in a secure HPC\
    \ environment [12], and presented their successful Charliecloud-based implementation\
    \ on the SuperMUC-NG supercomputer. AI researchers and developers are used to\
    \ configuring their environments by using tools that download the necessary software\
    \ from the Internet. For security reasons, however, many HPC systems do not allow\
    \ external network access. In addition, pre-configuring the nodes is often impossible,\
    \ due to incompatible dependencies between different software, or even between\
    \ different versions of the same software. Their proposal is to employ containers\
    \ to create user-defined software-stacks (UDSS) [69]. Thus, allowing the user\
    \ to pre-configure the environment, before transferring it to the HPC system.\
    \ Using Charliecloud, they were able to set up a software stack, and to implement\
    \ and test the full experimental pipeline in less than one day. Moreover, the\
    \ authors highlight the importance of being able to use the Intel-optimized version\
    \ of TensorFlow, and Intel MPI in order to achieve optimal performance. Rudyy\
    \ et al. [72] presented a study on performance and portability of containers on\
    \ four different systems. They were able to deploy and execute a containerized\
    \ CFD application on the Lenox cluster (Intel Xeon based, 1 Gb/s Ethernet) using\
    \ Docker, Shifter, and Singularity. All three used the same Docker image, the\
    \ only difference being that for Singularity the image must first be converted\
    \ using the singularity-build tool. On a second set of experiments, to analyze\
    \ the portability of containers for HPC, they evaluated Singularity-based executions\
    \ of the same CFD code on three systems with different architectures. These were:\
    \ (a) the MareNostrum4 supercomputer, a large-scale Intel Xeon based system with\
    \ an 100 Gb/s Intel OmniPath interconnection network; (b) CTE Power, which is\
    \ based on the IBM POWER9 processor and an InfiniBand network; and (c) ThunderX,\
    \ an ARMv8 system whose nodes are interconnected via 40 Gb/s Ethernet. The containerized\
    \ application was able to achieve near bare-metal performance by using Singularity’s\
    \ ability to access the host’s MPI, thus being able to take advantage of the available\
    \ high performance interconnects. When creating three different Singularity images\
    \ to evaluate the execution of real-world scientific applications on Stampede\
    \ 2, Wang et al. [89] observed that the two images that used the same Linux distribution\
    \ as the host were able to execute the applications right out of the box. On the\
    \ other hand, the third image, which was from another distribution, needed minor\
    \ modifications. Their overall conclusion, based also on their overhead evaluation,\
    \ was that it is viable to provide customizable container images for HPC users\
    \ while still providing optimal performance for their applications. The Argonne\
    \ Leadership Computing Facility (ALCF) has employed Singularity to provide portability\
    \ of software stacks between systems, including development systems and large-scale\
    \ supercomputers [29]. One of their stated goals is to make HPC more approachable,\
    \ especially for new users and users from research fields which may not have a\
    \ strong computing culture. Singularity was used to run ATLAS (from the Large\
    \ Hadron Collider’s ATLAS experiment) on an ALCF supercomputer, and to share CANDLE\
    \ (CANcer Distributed Learning Environment) framework workflows among users, as\
    \ well to enable these users to tailor its software stack to their needs. Yuan\
    \ et al. [94] presented a system for scientific collaboration on the Coastal Resilience\
    \ Collaboratory (CRC), and to reduce the necessary effort to run scientific applications\
    \ on different systems. They use Docker to deploy Jupyter notebooks to user machines.\
    \ These are used by researchers and engineers as an interactive tool to customize\
    \ their simulations. Data sharing and access to HPC resources is handled by the\
    \ Agave framework, and Singularity is used to deploy the simulation images in\
    \ cloud-enabled resources. Besides that, they created a Coastal Model Repository,\
    \ which provides a collection of container images targeting cloud-like infrastructures.\
    \ To analyze the use of containers for the portability of HPC applications, Peiro\
    \ Conde [67] built several images for different applications (including the OSU\
    \ Micro-benchmarks, OpenFOAM, and GROMACS), and evaluated these in an ARM-based\
    \ cluster. Based on their experience, they suggest that system administrators\
    \ provide Docker and Singularity usage guidelines to all users. And, moreover,\
    \ that they provide base Docker images for their systems, which can be used by\
    \ experienced users to create custom images. These images would then be shared\
    \ in a repository, to be used by all users of the system. Among the potential\
    \ flaws to this workflow, they cite the large amount of work and time required\
    \ to create the container images, and the potentially large storage space requirement\
    \ for the custom images. As a final remark, Peiro Conde [67] state that ”there\
    \ is still a lot of room for improvement in usage” and that ”HPC-oriented containers\
    \ such as Singularity require users with good expertise on the matter.” With the\
    \ intention of providing maintainers an information base to make informed choices\
    \ when containerizing their MPI application, Hursey [37] analyzed several challenges\
    \ faced during build and execution time. They presented common solutions to these,\
    \ discussing the advantages and disadvantages of each. Among the explored challenges\
    \ are library compatibility issues at build time, and cross-version incompatibilities\
    \ at run time (between container and host binaries). They also discussed the choice\
    \ between mapping one container per process or mapping one container per node.\
    \ In addition, they took into account how container isolation mechanisms (e.g.,\
    \ cgroups, namespaces, capabilities) can hinder the efficiency of MPI implementations.\
    \ The challenge of providing system-optimized MPI inside a container was also\
    \ discussed, including three alternatives to overcome it. Finally, the authors\
    \ included a discussion on how the experience of running containers on traditional\
    \ HPC environments can be translated to a container orchestration environment,\
    \ especially Kubernetes. Aoyama et al. [2] integrated HPCCM (HPC Container Maker)\
    \ into MEGADOCK, a large scale protein docking application. They aimed at integrating\
    \ the container deployment workflow across HPC systems with different specifications.\
    \ The HPCCM framework provides easy configuration of containers for different\
    \ systems, handling issues such as host-dependent libraries and MPI-ABI compatibility\
    \ between host and container. This integration was tested in the ABCI and Tsubame\
    \ 3.0 HPC systems, with good performance results (see Sect. 9). Prominence [47]\
    \ is a platform that allows users to run jobs across different resources while\
    \ enabling these jobs to access both software and data. It uses Docker container\
    \ images that are executed with either Singularity or udocker. In this platform,\
    \ jobs preferably run locally, but have the ability to burst onto non-local clouds\
    \ including, by order of preference, national research clouds, EGI resources,\
    \ and public clouds. According to the authors, Prominence was successfully tested\
    \ with Openstack, Microsoft Azure, AWS and Google Cloud, in addition to Openstack\
    \ and OpenNebula clouds in EGI FedCloud. The authors remarked the loss in portability\
    \ when a container was optimized to get the best possible performance in a specific\
    \ platform, especially in a case where compiler optimizations specific to a CPU\
    \ architecture made it incompatible with some previously supported systems. Cerin\
    \ et al. [17] designed an approach for containerizing an HPC resource scheduler.\
    \ More specifically, the scheduler daemons hosted in the managed HPC nodes are\
    \ executed inside containers. Moreover, they use cgroups to limit the amount of\
    \ resources assigned to each of these containers. This way, it becomes possible\
    \ to employ these nodes to run other applications or services simultaneously with\
    \ HPC applications. The rationale behind this is to take advantage of the fact\
    \ that many HPC applications are not able to utilize the whole capacity of the\
    \ computation nodes at all times. By isolating the resources available to the\
    \ container manager, it becomes possible for more general purpose jobs to co-inhabit\
    \ with the HPC jobs, allowing them to execute concurrently while isolated inside\
    \ their own containers. These non-HPC jobs would use the potentially underutilized\
    \ resources in the node, therefore, increasing resource utilization. The implementation\
    \ presented by Cerin et al. [17] is based on Kubernetes, to manage and orchestrate\
    \ both the job manager (Slurm) containers and the general purpose ones. The container\
    \ runtime is CRI-O, which is meant to be a lightweight alternative to Docker and\
    \ is designed with privilege separation in mind. This work, however, does not\
    \ present any evaluation of the proposed approach. To get a deeper sense of the\
    \ feasibility of using containers in HPC, Ruhela et al. [73] employed a different\
    \ system and runtime to run the same container image from previous experiments\
    \ (see Sects. 8.1, 9 and 8.5). Instead of the Frontera supercomputer and Singularity\
    \ or Charliecloud, they employed a cluster that resembles the Stampede2 HPC system,\
    \ and used Podman. All workloads ran correctly, but there was a minor performance\
    \ degradation (5–10%). The authors hypothesize this is because Podman uses fuse-overlayfs,\
    \ and employs additional inter-process isolation compared to Singularity and Charliecloud.\
    \ Vaillancourt et al. [87] developed a system that automates tasks related to\
    \ deploying scientific workloads in cloud environments. Its goal is to reduce\
    \ the time needed to adapt existing HPC applications for running in cloud environments.\
    \ It automatically manages the different deployment options of the available computing\
    \ clouds, and configures networking support for parallel application execution.\
    \ In addition, it can translate legacy application build and deployment mechanisms\
    \ to their cloud equivalents. The authors highlight two compelling cases for moving\
    \ to the cloud: (1) when computational resources are not available anywhere else,\
    \ and (2) when cloud resources are more readily available than HPC-style resources.\
    \ They also mention that ”conversely, the public cloud might be cost-prohibitive\
    \ for very large-scale or large-output applications [...].” Their tool combines\
    \ Docker, to provide portability and reproducibility, with Terraform and Ansible\
    \ for automatic deployment, management and provisioning of cloud resources. They\
    \ used it to encapsulate, deploy, and run three representative scientific application\
    \ workflows (Lake_Problem_DPS, WRF, and FRB_pipeline) on AWS and on the Aristotle\
    \ Cloud Federation. The EASEY framework [34] aims to facilitate the deployment\
    \ of applications on ”very-large scale” systems, with minimal interaction. Its\
    \ goal is to reduce the time scientists spend on deploying and tuning their application\
    \ on new systems, as well as on job submission. They discuss some initial development,\
    \ which enables the transformation of Docker-based applications into Charliecloud\
    \ containers. Moreover, to optimize the performance on the target system, they\
    \ have a mechanism that enables the automatic addition of system-specific libraries\
    \ to the container. EASEY was evaluated using a Docker container image for the\
    \ LULESH benchmark, which was ported to DASH. Charliecloud-based executions on\
    \ up to 32,768 cores of the SuperMUC-NG supercomputer presented negligible overhead\
    \ compared to native performance. In addition, EASEY reproduced its scaling behavior,\
    \ and kept its performance very close to the original manually compiled and naively\
    \ executed application. Jung et al. [39] developed an infrastructure based on\
    \ Docker and Kubernetes to facilitate the execution of the ROMS model (Regional\
    \ Ocean Modeling System) in different public (AWS, Google Cloud, and Microsoft\
    \ Azure) and private (a laptop, and two different clusters) computational environments.\
    \ They created a ROMS container image, which they executed in cloud clusters with\
    \ different core counts. The authors claim that, in a variety of public and private\
    \ environments, the container-based architecture makes the numerical ocean modeling\
    \ much easier than the VM-based one. Ruhela et al. [74] employed five different\
    \ HPC systems to perform an evaluation of containers applied to medium and large-scale\
    \ HPC workloads. Regarding container portability, they remark on the difficulty\
    \ brought by working with different MPI implementations and CUDA versions. They\
    \ mention situations where different containers had to be used for different experiments,\
    \ and also for different clusters. The latter was due to each system requiring\
    \ different MPI implementations, to achieve optimal performance. They also reported\
    \ some difficulty on installing CUDA libraries in Charliecloud’s unprivileged\
    \ containers. In most cases, this was solved by the runtime’s capability of injecting\
    \ the host’s CUDA libraries into the container, but they were still unable to\
    \ run one of the applications in one system. They also ran into difficulties caused\
    \ by the misunderstanding of requirements for containerized workloads or by misconfigured\
    \ installations, which required help from system administrators to solve. Such\
    \ issues result from cluster having different software ecosystems and from differences\
    \ between how containers and traditional HPC applications work. Finally, they\
    \ conclude that ”while in some cases containers provide portability, in large-scale\
    \ multicluster situations, containerization does not simplify the work needed\
    \ to complete the tasks and adds an additional level of complexity to the existing\
    \ scientific applications.” Chang et al. [18] analyzed the feasibility of running\
    \ MPI-based applications encapsulated in Singularity containers on NASA’s High-End\
    \ Computing Capability (HECC) resources. HECC provides access to both in-house\
    \ resources and commercial cloud resources from Amazon AWS. Their experiments\
    \ comprised one HPC workload and two machine learning workloads. For the HPC tests,\
    \ they employed JEDI Singularity containers with a Finite-Volume Cubed-Sphere\
    \ Dynamical Core (FV3)-JEDI atmospheric simulation. This workload was tested on\
    \ both in-house and AWS resources. The machine learning workloads were a recommender\
    \ system called Neural Collaborative Filtering, and Resnet-50 trained on the ImageNet\
    \ data set. These were run inside a TensorFlow container from the NVIDIA GPU Cloud\
    \ (NGC), and were tested only on in-house resources. The authors provide a detailed\
    \ presentation of all the hurdles they faced when trying to run these workloads\
    \ on different on-site HECC resources and on AWS resources. Running them on a\
    \ single node, with a single container, did not present any significant difficulties,\
    \ since access to the host’s MPI is not necessary. Running on multiple nodes,\
    \ however, presented a series of issues, especially due to requiring access the\
    \ host’s MPI. There were also difficulties due to differences in network configuration\
    \ on different resource types, and on the AWS resources they had to replace their\
    \ default shared file system for Lustre. The final conclusion by Chang et al.\
    \ [18] was that while Singularity-based containers with hybrid container-side\
    \ and host-side MPI is feasible, users are likely to run into issues whose solution\
    \ require knowledge of Singularity, MPI libraries, and of the network infrastructures.\
    \ Therefore, most users would need some help to run their workloads, as such issues\
    \ are usually handled by HPC-site support staff. While this work includes some\
    \ container performance measurements, they were not compared with bare-metal performance.\
    \ Brayford et al. [13] demonstrated the building, configuration, and deployment\
    \ of a quantum gate simulation application developed in Julia and containerized\
    \ with Charliecloud on multiple HPC systems with different CPU architectures (Arm\
    \ ThunderX2, Arm Fujitsu A64fx, AMD Rome and Intel Skylake) and instruction sets.\
    \ Moreover, they were able to combine the software environment inside the container\
    \ with host-side software, to be able to use a tool that was installed in the\
    \ hosts to profile the application installed inside the container. They considered\
    \ the results extremely promising and that they justify the effort of developing\
    \ the HPC-specific containers. Zhou et al. [98] proposed a dual-level scheduling\
    \ strategy for container jobs in HPC systems, by creating a bridge between an\
    \ HPC workload manager and a container orchestrator. It is aimed at being used\
    \ in a hybrid architecture composed of an HPC cluster and a Cloud cluster. To\
    \ accomplish this they implemented a tool named TORQUE Operator which bridges\
    \ the TORQUE HPC workload manager and Kubernetes. Kubernetes schedules jobs on\
    \ the first level, and on the second level they are scheduled by TORQUE. In addition,\
    \ their chosen container was Singularity. Its key architectural aspect is that\
    \ the login node for TORQUE, which submits the jobs on the HPC cluster, is a VM\
    \ that serves as a Kubernetes worker node. Moreover, TORQUE Operator also includes\
    \ user management logic, to ensure that HPC cluster jobs belong to the correct\
    \ users, thus assuring confidentiality and the right user privileges. Besides\
    \ presenting their architecture and tool, Zhou et al. [98] present two agriculture-related\
    \ machine learning use cases, and a performance evaluation using the BPMF MPI\
    \ benchmark. All three applications show performance gains by running in the HPC\
    \ cluster compared to cloud VM, and two of them scale up to 20 nodes (one of the\
    \ ML codes is sequential). Finally, this work does not discuss or evaluate containerization\
    \ overheads. BeeSwarm [85] is a system that can be integrated with existing continuous\
    \ integration (CI) systems to perform performance and parallel scaling tests for\
    \ HPC applications. It is based on a modified version of BEE [19], which was presented\
    \ earlier in this section. During CI testing, the tool builds a container for\
    \ the tested application and triggers its deployment in the platform where the\
    \ scalability tests will be executed. BeeSwarm is responsible for allocating the\
    \ computational resources for the scalability tests via backends that support\
    \ different cloud APIs, as well as HPC batch schedulers. The test output is sent\
    \ back to the CI environment, where it is parsed to generate a report for the\
    \ user. They present use cases employing Travis CI and GitLab CI using Docker\
    \ containers to deploy the tests into resources of the Chameleon Cloud, as well\
    \ as employing Github Actions using Charliecloud containers to deploy the tests\
    \ on the Google Compute Engine (GCE). Tippit et al. [83] report on their team's\
    \ use of Singularity to distribute their GPU-based quantum simulation software\
    \ developed in the Julia programming language. They remark on how this allows\
    \ them to share their work with others while ensuring that software versions and\
    \ libraries will exactly match their own. Moreover, they mention the advantage\
    \ brought by the reduced performance overhead of containers compared with hypervisor-based\
    \ VMs. The Physikalisches Institut University of Bohn has employed containerization\
    \ on a hybrid HTC and HPC cluster, to support the specific requirements of CERN’s\
    \ ATLAS experiment software stack, while also allowing individual users to choose\
    \ the OS where their jobs should run [23]. Moreover, they say that implementing\
    \ this setup was made straightforward by the container awareness of the HTCondor\
    \ resource management system. In this scheme, HTCondor is responsible for instantiating\
    \ the container corresponding to the user specified OS and for making sure that\
    \ users never access the bare-metal machine. One difficulty they had was that\
    \ supporting interactive jobs on containers instantiated by HTCondor required\
    \ some “tricky” SSH configuration. In their system, users have access to container\
    \ images based on official Linux distribution images, with some adaptations. These\
    \ are automatically rebuilt daily or when their recipes are updated. They chose\
    \ Singularity as their container runtime, due to off-the-shelf support by HTCondor,\
    \ but mention plans to switch to a fully unprivileged solution. Among other benefits,\
    \ Kadri et al. [40] discuss containers as a solution to improve the portability\
    \ of bioinformatics software. The 41 works we analyzed in this section focus on\
    \ different levels of portability enhancement that can be provided by containers.\
    \ The most basic level consists of facilitating the setup of user-defined software\
    \ stacks. On the next level, we have works aimed at easing the portability of\
    \ applications between different dedicated HPC systems, or different cloud-computing\
    \ platforms. Finally, we have works on providing portability between different\
    \ types of systems, which may include development systems, dedicated HPC systems,\
    \ or cloud-computing environments. The former may be useful for setting up a CI/CD\
    \ pipeline, or to implement an automated application (or experiment) configuration-execution\
    \ workflow. Besides these, there are also a few works aimed at facilitating the\
    \ distribution of a single application, or at providing base container images\
    \ to provide pre-configured environments aimed at a specific field (such as AI),\
    \ or at providing optimized environments for specific systems. Finally, we have\
    \ a few works that analyze the portability of HPC application containers. Among\
    \ the analyzed works, we are most interested on the ones that propose solutions\
    \ to provided portability between different systems, especially those aimed at\
    \ HPC and cloud-computing platforms. Overall, including those focused on a single\
    \ system, more than three quarters of the works in this section deal with dedicated\
    \ HPC systems, while those that deal with cloud environments amount to only slightly\
    \ over one fifth of them. Moreover, if we restrict ourselves to works that allow\
    \ portability between different systems, close to 36% of the 41 works deal with\
    \ portability between different HPC systems, about 7% between cloud platforms,\
    \ near 21% between dedicated HPC systems and cloud, and about 12% between personal\
    \ (or development) computers and the cloud. This could indicate some potential\
    \ for research, especially on improving the portability of applications between\
    \ dedicated HPC systems and cloud resources. 12 Research reproducibility The portability\
    \ of containers allows the same software environment to be used to repeat experiments\
    \ on different systems, by different researchers. Therefore, containers can provide\
    \ enhanced reproducibility of execution for scientific application. This is an\
    \ important aspect in attaining reproducible of research results. Especially in\
    \ view of the large array of scientific and engineering fields that rely on computational\
    \ data processing and simulation. In this subsection, we present the contributions\
    \ of the surveyed works to research reproducibility. Singularity Hub is a container\
    \ repository and a framework to build and deploy Singularity containers [79].\
    \ It aims to make containers movable and transparent, while enabling customizable\
    \ builds and scalable deployment. It also includes tools to facilitate the comparison\
    \ and assessment of the reproducibility of containers. According to Sochat et\
    \ al. [79], ”this open standard and these functions support the larger underlying\
    \ goal of encouraging data sharing and reproducible science.” Besides Singularity\
    \ Hub, this work presents novel reproducibility metrics to compare containers\
    \ based on a desired level of reproducibility. The authors present a set of pre-specified\
    \ reproducibility levels, including “identical”, “replicate” (built in different\
    \ machines or at different times), “recipe” (different environments, labels, and\
    \ runscripts), and “base” (same base container). Hisle et al. [33] worked on the\
    \ containerization of Autodock Vina, a molecular docking program used in the search\
    \ for potential drugs among a large set of molecules. The authors explain that\
    \ this program is known to run significantly faster when compiled for the target\
    \ hardware, compared to the available pre-compiled binary. Moreover, they remarked\
    \ that speeding-up Autodock Vina should result in important savings in HPC resource\
    \ usage, since the docking process often requires hundreds of thousands of CPU-hours.\
    \ One problem, however, is that its setup and compilation is time-consuming and\
    \ difficult, due to dependencies on outdated compilers and libraries. By using\
    \ Singularity to containerize the application and its dependencies, the authors\
    \ aim to enable users to easily compile it in their target systems. Moreover,\
    \ they observe that most of its users lack the knowledge and skills to manually\
    \ set up such an environment, thus elevating the importance of the pre-configured\
    \ container. Results from this work show that, compared to the pre-compiled binaries,\
    \ the execution time of the source-compiled binaries is 33% shorter. Martinasso\
    \ et al. [57] demonstrated how they can use containers to ensure experiment reproducibility\
    \ even after the HPC system’s execution environment is updated. They specifically\
    \ focused on supporting applications built with different versions of the Cray\
    \ Processing Environment (CPE). As a use case, first they compiled the COSMO weather\
    \ forecast model inside a container containing a specific version of CPE. Then,\
    \ they used a script to extract the generated binaries and dependencies, which\
    \ they placed in a new lightweight container image that enables executing the\
    \ application in the HPC system, independent of the version of CPE installed in\
    \ the host. Therefore, this eliminates the need for the weather researchers to\
    \ rebuild COSMO at every CPE update, which also entails repeating previous experiments\
    \ to ensure numerical reproducibility. Heinonen [29] reports on the benefits of\
    \ using Singularity for research at the Argonne Leadership Computing Facility\
    \ (ALCF). The author states that reproducibility ”can be seen as one of the primary\
    \ assets of container technology”, due to its crucial role in scientific research.\
    \ The Cloud Resilience Collaboratoty (CRC) developed a container-based system\
    \ to enable and facilitate the collaboration between its members [94]. Simulations\
    \ are configured through Jupyter notebooks encapsulated in Docker containers,\
    \ and deployed in the cloud as Singularity containers. The authors argue that\
    \ both beginner and computer-literate users can benefit from such a system, which\
    \ not only makes all machines look the same for the user, but also encapsulates\
    \ and tracks the provenance information of the experiments, thus leading to better\
    \ reproducibility. Vaillancourt et al. [86] developed a method to create infrastructure-independent\
    \ reproducible containers for scientific software. Their solution is based on\
    \ providing container templates based on best practices for each environment.\
    \ It was implemented using Singularity and the Nix package manager. Nix allows\
    \ to specify the specific versions of packages and dependencies to be installed.\
    \ The container templates, which are stored in a Container Template Library (CTL),\
    \ consist of a Docker file along with necessary scripts, including a Nix configuration\
    \ file. This helps to ensure the reproducibility of the container, as long as\
    \ Nix is supported in the target environment. This work, in specific, includes\
    \ the provision of container templates that can run in various cloud environments.\
    \ Moreover, the authors demonstrate a ”flexible HPC-style infrastructure that\
    \ is deployable across multiple cloud providers.” They employed this infrastructure\
    \ to execute HPL on virtual clusters in two different cloud infrastructures (JetStream\
    \ and Red Cloud). Cavet et al. [16] reported on the progress of the ComputeOps\
    \ project, which aims to ”explore the benefits of containerization for the IN2P3\
    \ (Institut national de physique nucléaire et de physique des particules) scientific\
    \ communities.” Among other topics, the authors highlight the usability of containers\
    \ in continuous integration pipelines. Through empirical evaluation, they defined\
    \ that this is the best solution to fulfill their objectives. At the time this\
    \ work was published, they were working on a Comprehensive Software Archive Network\
    \ (CSAN), aimed at enabling the full reproducibility of containers. CSAN builds\
    \ upon Singularity and the Guix package-manager. Besides portability, the container\
    \ image and infrastructure created by Jung et al. [39] to run ROMS also aims at\
    \ improving reproducibility. The results of this containerized model, executed\
    \ in various container clusters with different node counts, were the same from\
    \ their control model, regardless of OS or hardware environment. Citing the authors,\
    \ ”This suggests that a container-based cluster is appropriate for use in the\
    \ computational reproducibility of the numerical ocean model.” Kadri et al. [40]\
    \ remark on sharing bioinformatics software as a container as a means to achieve\
    \ reproducible behavior, since ”the contents, including the version of the software\
    \ and its dependencies, are locked down.” While it does not seem as popular a\
    \ research topic as the portability that supports it, several works have investigated\
    \ the contribution of container environments to reproducibility. This being one\
    \ of the main contributions of this technology to scientific research and, specifically,\
    \ to scientific computing [29]. The works above touch into providing general support\
    \ for (a) building, storing, and sharing reproducible containers [40, 79]; (b)\
    \ quantifying such reproducibility [79]; (c) improving the reproducibility of\
    \ specific applications [33, 39, 57]; (d) keeping results reproducible by allowing\
    \ to execute different versions of a software environment on the same system [57];\
    \ (e) facilitating the sharing, configuration, and execution of a scientific application\
    \ environment [94]; (f) providing a pre-configured and reproducible application\
    \ environment that can be built in different systems [16, 86]; as well as (g)\
    \ implementing infrastructure-independent reproducibility [86]. 13 Unprivileged\
    \ user containers Unprivileged container execution, i.e., without administrative\
    \ privileges, is of paramount importance in order to allow their usage in multi-tenant\
    \ HPC systems. This requirement is pointed out by several authors [6, 8, 9, 12,\
    \ 22, 26, 27, 31, 32, 35, 40, 48, 58, 62, 68, 72, 75,76,77, 88, 89], and is one\
    \ of the main reasons why Docker did not gain as much traction in HPC as in other\
    \ fields. Below, we summarize a few works that proposed solutions to this problem.\
    \ Higgins et al. [32] developed a Docker API proxy that acts as an intermediary\
    \ between the Docker clients and the actual Docker daemon. To enforce security\
    \ rules, it manipulates the content of client requests and of the responses to\
    \ the caller. These rules consist of filters specifying keys that should be removed\
    \ from client requests, and key-value pairs that should be merged into them. This\
    \ proxy is able to identify the user that is communicating with it, and associates\
    \ user information with their stored containers. Such information is used to ensure\
    \ that the target container of received API calls is owned by the issuing user.\
    \ Besides that, it is also used to filter the content of container listings, leaving\
    \ only those that are actually owned by the requesting user. The authors say these\
    \ strategies were already in place in the Eridani cluster at the University of\
    \ Huddersfield. According to them, ”they are effective to mitigate the privilege\
    \ escalation and resource exhaustion attacks [...]”, and ”this solution required\
    \ no additional setup on the user’s behalf and operates transparently [...]”,\
    \ thus allowing ”[...] easy integration into the HPC environment without any significant\
    \ configuration changes.” Socker [4] is a Docker wrapper that aims to securely\
    \ execute Docker containers on HPC systems, while respecting resource usage restrictions\
    \ imposed by the job scheduler. To achieve secure execution, it runs the container\
    \ as the user who submitted the job. To respect resource usage constraints, the\
    \ wrapper enforces the membership of the container process in the cgroups the\
    \ resource scheduler created for the job. One important limitation is that this\
    \ approach only works if the container spawns only one process. Tests of Socker\
    \ using MPI jobs submitted through Slurm demonstrated no performance overhead\
    \ compared to Docker. Gomes et al. [26] proposed udocker, an alternative Linux\
    \ container implementation that enables the pulling, extraction, and execution\
    \ of Docker containers in user mode, with no need for administrative privileges.\
    \ In addition, udocker is a self-installing tool, whose installation can be done\
    \ by unprivileged users. It assumes that container images will be prepared using\
    \ Docker, on the user’s personal computer. udocker implements four different container\
    \ execution modes, which allows it to adapt to different host capabilities. These\
    \ modes are denominated P, F, R, and S, with a few variations for modes P and\
    \ F: (1) P modes are based on ptrace and implemented using PRoot; (2) F modes\
    \ are based on LD_PRELOAD and implemented using fakechroot; (3) the R1 mode uses\
    \ unprivileged user namespaces and is implemented using runC; and (4) the S1 mode\
    \ is based on Singularity. Sparks [81] proposed extending Docker’s authorization\
    \ framework to control access to various commands and the usage of runtime options.\
    \ Their solution uses a Docker authentication plugin to allow configuring the\
    \ Docker daemon for granular access policies. Besides that, the daemon can also\
    \ be configured to leverage extended authentication mechanisms. The system administrator\
    \ would be the one responsible for registering plugins as part of the configuration\
    \ and startup of the Docker daemon. Unfortunately, the authors did not present\
    \ any evaluation or real-world use case of this scheme. Sarus [9] is an OCI-compliant\
    \ container implementation that extends runC to address unique requirements of\
    \ HPC environments. These include obtaining native performance from dedicated\
    \ hardware, improved security on multi-tenant systems, support for network parallel\
    \ file systems and diskless nodes, and compatibility with workload managers. Sarus\
    \ is composed of a CLI (command-line interface) component, an image manager, and\
    \ a runtime component. Container processes are owned by the user who executed\
    \ the container launching command, thus avoiding privilege escalation. Additionally,\
    \ Sarus is capable of swapping the MPI runtime inside the container with an MPICH-ABI\
    \ compatible implementation from the host. Besides, it is able to use the NVIDIA\
    \ Container Runtime hook, to enable accessing GPUs inside the containers. Priedhorsky\
    \ et al. [70] argue that the difficulty of building container images with unprivileged\
    \ tools is slowing down the adoption of containers by supercomputer users, even\
    \ when the supercomputers themselves support containers. With this in mind, they\
    \ explore the issue of container build privilege in HPC, including the different\
    \ container privilege models, and what makes building containers in an unprivileged\
    \ environment harder than running them (e.g., package managers assuming privileged\
    \ access, with packages requiring multiple UIDs and GIDs, and the execution of\
    \ privileged system calls on installation). Moreover, they analyze the strategies\
    \ adopted by Rootless Podman and Charliecloud to solve this problem. Rootless\
    \ Podman utilizes a mount namespace with a privileged user namespace, employing\
    \ multiple container GIDs and UIDs which are independent of the host’s. The root\
    \ user inside the container is mapped to an unprivileged host user. This model\
    \ requires privileged host tools to assist building, and computer sites must maintain\
    \ ID mapping files to ensure proper isolation between different users. In addition,\
    \ according to Priedhorsky et al. [70], Podman’s namespace implementation was\
    \ incompatible with shared file systems. Charliecloud, on the other hand, uses\
    \ a mount namespace and an unprivileged user namespace, with only one UID and\
    \ GID mapped into the container (aliases to the user’s IDs in the host). This\
    \ model enables fully unprivileged container building and execution. The main\
    \ advantage is that it does not rely on the correct configuration of the helper\
    \ tools, like maintaining proper the user ID maps. This is accomplished by automatically\
    \ injecting the command line tool fakeroot into the building process. This tool\
    \ acts as a wrapper that fools the wrapped process into thinking it is running\
    \ with root privileges. LPMX [91] is an unprivileged container implementation\
    \ aimed at providing mutual composability to containers, especially for use in\
    \ scientific pipelines, such as the ones commonly employed in genome analysis.\
    \ More specifically, it allows encapsulating the different tools that compose\
    \ the pipeline into separate containers while retaining their ability of calling\
    \ other tools that are outside the container (i.e.,in the host or in another container).\
    \ Moreover, they employ a layered file system to reduce the size of the container.\
    \ Different from other implementations, however, this file system runs completely\
    \ in user space, thus not requiring administrative privileges. Finally, just like\
    \ udocker, it does not require administrative privileges neither to build nor\
    \ to run containers, thus being fully rootless. In tests, it displayed considerable\
    \ software installation and compilation overheads, but minimal overheads for genomic\
    \ analysis application execution. As we have seen, several works tried to solve\
    \ the problem imposed by privileged container execution. Some of them, by modifying\
    \ Docker or implementing extra measures to make it more secure [4, 32, 81]. A\
    \ few others, by proposing novel container technologies to allow the unprivileged\
    \ execution of containers [9, 26, 91]. Besides these, we have the HPC container\
    \ implementations seen in Sect. 4, which also implement unprivileged execution.\
    \ Some of these still require administrative privileges for building the containers,\
    \ but there are already solutions that managed to eliminate this requirement.\
    \ 14 Related work Pahl et al. [66] published a state-of-the-art review about cloud\
    \ container technologies, in the form of a systematic mapping of 46 papers published\
    \ between 2007 and 2016. Interesting observations from this review include that\
    \ (1) ”there is more work on deployment and management services than design and\
    \ architecture concerns”; (2) there is an equal spread between IaaS and PaaS focus;\
    \ and (3) evaluation is mainly done through experiments and case studies. The\
    \ authors conclude that ”container technology is still in its formative stages”,\
    \ and that there is a lack of tools to automate and facilitate container management\
    \ and orchestration. It is worth noticing that, despite being published in 2019,\
    \ this paper was actually submitted in 2017. The authors also detected a growing\
    \ interest and usage of containers as lightweight virtualization solutions at\
    \ the IaaS level, and as application management solutions at the PaaS level. Finally,\
    \ they indicate the possibility of using containers to support continuous development\
    \ in the cloud. Among other analyses, they looked at the distribution of studies\
    \ by community, by motivation, and by employed container technologies. Their results\
    \ show that almost half of the reviewed works were developed by the Cloud and\
    \ Big Data community. At second place was the distributed-systems community (23%\
    \ of the studies). Noticeable is the absence of works by the HPC community. Pahl\
    \ et al. [66] also showed that the main motivation for using containers, in the\
    \ analyzed sample, was lightweight resource management (33% of the works). The\
    \ second most frequent was ease of deployment (26%), followed by automatic deployment\
    \ (15%), and large-scale deployment (9%). Additionally, the main container technology\
    \ employed by the works was Docker (40% of the studies), followed by LXC (21%).\
    \ Notable is the absence of HPC targeted container technologies, such as Singularity,\
    \ Charliecloud and Shifter. Possibly, because this survey only includes works\
    \ published before these technologies became available. Medrano-Jaimes et al.\
    \ [58] published a technology review of container platforms designed for HPC,\
    \ based on a collection of 10 works published from 2015 to 2018. They discuss\
    \ containers as a solution to allow user environments in HPC systems, in contrast\
    \ to traditional approaches, such as Linux modules, package managers, and Python\
    \ virtual environments. According to them, these approaches do not fix the issue\
    \ of portability, making it difficult for users to transfer their work to a new\
    \ system. Moreover, they remark on the use of containers in science, to provide\
    \ mobility and reproducibility, in addition to portability. This work includes\
    \ an analysis of three HPC container technologies, namely Charliecloud, Shifter,\
    \ and Singularity. The authors considered Charliecloud the easiest to use, due\
    \ to demanding little effort from users and sysadmins, and providing simple and\
    \ straightforward commands. Shifter was remarked for being a proven solution for\
    \ working with Docker containers, but with the drawback of its installation and\
    \ maintenance processes requiring more commitment from the system administrators.\
    \ Regarding Singularity, they identify the native InfiniBand support as its main\
    \ differential. They also mention the capability of configuring its software according\
    \ to site policies, and the existence of a cloud service for storing and sharing\
    \ Singularity containers. The potential performance improvement when using Shifter\
    \ or Singularity to execute applications that are not optimized for distributed\
    \ file systems is also remarked upon. Among their conclusions, they say that the\
    \ container solutions for HPC environments present, in general, satisfactory results.\
    \ Nevertheless, they argue that ”there is still a lot of work to be done”. Finally,\
    \ they highlight the continuous growth of the use of containers for HPC, attributed\
    \ to advantages like the improved scientific reproducibility. Bachiega et al.\
    \ [5] published a survey on containers, with focus on performance evaluation.\
    \ It comprehends 25 works published from 2010 to 2017, and is based on a systematic\
    \ mapping considering main databases like Springer, IEEE, ACM, and Scopus. The\
    \ selected papers were classified according to the performance evaluation method,\
    \ among analytic performance modeling, simulation performance modeling, or performance\
    \ measurement. The authors found out that the performance measurement technique\
    \ was extensively used, in different scenarios. The 22 works that employed this\
    \ technique, including several HPC-related ones, utilized benchmarks scientific\
    \ applications, and other virtualization tools. The authors remark, however, on\
    \ ”the lack of comparison of containers with the use of public networks, data\
    \ migration, disaster recovery, and parallel programs.” Moreover, they found only\
    \ one work that employed analytic modeling, and it was restricted to a specific\
    \ container environment. Besides, they found only two papers using the simulation\
    \ technique, both describing ContainerCloudSim. To analyze the state-of-the-art\
    \ in container technologies, Casalicchio and Iannucci [15] surveyed 97 works published\
    \ until 2018. Interestingly, 90% of these were published between 2015 and 2018,\
    \ and 50% of them in 2017 and 2018. Their goal was to identify the most demanding\
    \ application fields employing containers, the related challenges, the proposed\
    \ solutions, and the remaining open issues. Among the application fields, the\
    \ majority of the analyzed works is focused on orchestration (38%), followed by\
    \ applications (26%), security (23%), and performance comparison (13%). In high\
    \ performance computing, which they consider a subcategory of applications, they\
    \ identified the demand for ease of deployment of user-defined software stacks\
    \ or virtual HPC clusters, and for leveraging the flexibility of containers in\
    \ real life computation and data intensive jobs. Additionally, they identified\
    \ four challenges related to the use of containers applied to HPC: (1) container\
    \ support for MPI and CUDA and for execution on GPU and FPGA; (2) the scheduling\
    \ of containerized HPC jobs; (3) mechanisms to guarantee native performance in\
    \ containers; and (4) solutions to guarantee the proper level of security in terms\
    \ of isolation, data security, and network security. They also remark on the HPC-specific\
    \ challenges of efficiently scheduling containerized IO-intensive jobs, and mitigating\
    \ or solving the IO performance limitation of containers. The authors conclude\
    \ that more effort should be applied to solve challenges in IO performance, multilayer\
    \ adaptation, energy efficiency, and security. Moreover, they remark that security\
    \ is probably the challenge with most urgent needs. The only HPC focused container\
    \ technology to appear in this survey is Charliecloud [69], even though technologies\
    \ like Shifter and Singularity already existed in the period it comprehends. Bentaleb\
    \ et al. [10] published an extensive overview of virtualization focused on containers.\
    \ They summarize important aspects of container technology, including container\
    \ implementations (Docker, Singularity and udocker are discussed in more detail),\
    \ container architecture, and container lifecycle, and container orchestration.\
    \ Then, they discuss taxonomies of container technologies found in the scientific\
    \ literature and propose ”a new one that covers and completes those proposed in\
    \ the literature.” Following that, they discuss applications of containerization\
    \ in different application domains, including big data, high-performance computing,\
    \ grid computing, high-throughput computing, cloud computing, internet of things,\
    \ fog and edge computing, and DevOps. There is a short subsection for each domain,\
    \ and the one about HPC only provides a superficial view of the application of\
    \ containers in this domain, comprising only 7 related works. Finally, this work\
    \ also provides a quick overview of the metrics that are commonly used to evaluate\
    \ containers. Only two of the five related works above are specific to HPC [5,\
    \ 58]. The most up-to-date survey [15], to the best of our knowledge, only includes\
    \ works published up to 2018. In addition, while the overview of container technology\
    \ published by Bentaleb et al. [10] is more recent, it only includes a short subsection\
    \ on HPC, where they only cite seven references. In comparison, our survey is\
    \ specific to the HPC domain, is more up-to-date than the surveys above, and comprehends\
    \ a substantially larger selection of works related to containers in HPC than\
    \ the overview paper. 15 Conclusion This work is a research literature survey\
    \ on the use of containers for high performance computing (HPC). We carefully\
    \ selected 93 papers published between 2016 and mid-August 2022. The last survey\
    \ we could find on the subject only analyzed papers published up until 2018, with\
    \ the vast majority of them being published before or in 2007. There is an overview\
    \ of container technology that is more recent than these surveys [10], but it\
    \ provides a very superficial view of their application in HPC, in a short section\
    \ that only discusses seven related works. So, we believe this survey provides\
    \ an important, and necessary, updated overview of the progress of research on\
    \ the subject. We can clearly see that most works are focused on analyzing or\
    \ mitigating potential performance overheads due to containerization. These studies\
    \ are normally empirical, and compare the performance of benchmarks and applications\
    \ running inside containers, with their performance in other environments, such\
    \ as virtual machines, or directly on the host’s native environment (bare-metal).\
    \ Their results demonstrate that, in most cases, containers do not incur significant\
    \ computational performance overhead compared to native execution. Moreover, in\
    \ this aspect, containers are generally better than VMs. In terms of network performance,\
    \ however, containers may be significantly slower than native execution. But this\
    \ depends on the capabilities and configuration of the container runtime and image.\
    \ There are, however, a series of techniques that allow containers to overcome\
    \ this limitation, mostly dealing with giving the container access to the host\
    \ network. In the case of HPC, there is an extra difficulty related to giving\
    \ software inside the container access to specialized network hardware, which\
    \ may need specific drivers or libraries to be operated. One way to work around\
    \ this is to mount-bind the required software, including host-optimized MPI implementations,\
    \ into a directory inside the container. It is possible to link the user application\
    \ to these libraries, thus getting access to the host’s specialized hardware.\
    \ A similar approach would be to simply copy these files inside the container.\
    \ This technique can also be used to give containerized applications access to\
    \ GPU accelerators. One main drawback of these approaches is that they break portability,\
    \ since they are very system-specific. Another approach is to rely on Application\
    \ Binary Interface  (ABI) compatibility between the host's MPI implementation\
    \ and the one inside the container. For example, if both the host and the container\
    \ MPI have compatible versions of the MPICH-ABI, then it is possible for the containerized\
    \ application to use the host’s MPI, which allows access to the host dedicated\
    \ HPC interconnect. A secondary concern for all the techniques presented here\
    \ is that they reduce container isolation. But this is not a major concern for\
    \ HPC, since it is not usual for more than one user to be allocated on the same\
    \ host, and security concerns are managed at host-level. Moreover, we could argue\
    \ that it makes no sense, in most cases, for a user to instantiate more than one\
    \ container per host. In terms of storage performance (IO), there seems to be\
    \ some degradation, especially when reading and writing data to Docker’s layered\
    \ file system, which should be avoided. In other cases, unless there are multiple\
    \ containers per host, the storage performance should be close to native. Considering\
    \ the above, we can conclude that, in terms of performance overhead, containers\
    \ are a good solution for HPC, especially compared to hypervisor-based VMs. Moreover,\
    \ getting near-native computing performance with containers is straightforward,\
    \ and the other kinds of overhead, such as communication, have solutions that\
    \ are already supported by HPC-specific container implementations. In what concerns\
    \ software portability, containers are clearly a very useful tool. They provide\
    \ users the benefits of virtualization, without the need to configure and deploy\
    \ a whole new system, as required by traditional VMs. This, and the reliance on\
    \ publicly available base images, makes for a considerable reduction in the size\
    \ of the customized environments, making them easier to share and faster to deploy\
    \ on new systems. Another clear benefit of building upon a base container is the\
    \ possibility to use pre-configured containers which already include commonly\
    \ used software environments. These can be found on public container repositories\
    \ or provided by system administrators. Finally, an important consequence of container\
    \ portability, is making it easier to provide reproducible applications, which\
    \ is of utmost importance for science. One surprising aspect is that, while there\
    \ are plenty of works dealing with portability, few of them [18, 19, 39, 47, 72,\
    \ 87, 93, 98] deal with the portability of HPC applications from dedicated HPC\
    \ systems to cloud environments. Most works deal with portability between different\
    \ users, different HPC systems, and from personal computer to HPC-systems. Works\
    \ that deal with cloud environments seem to mostly focus on the portability of\
    \ traditional cloud workloads from the cloud to HPC systems (e.g., machine-learning\
    \ and big-data applications). There also seems to be a prevalence of the usage\
    \ of private clouds in these works. This points to an unexplored research opportunity\
    \ on facilitating the portability of traditional HPC workloads (usually executed\
    \ on dedicated HPC systems) to the cloud. This could be an interesting topic to\
    \ explore, specifically focusing on the portability between the user’s personal\
    \ computer, HPC systems, and public cloud environments. In terms of container\
    \ implementations, our data demonstrates a clear growth of interest in HPC-specific\
    \ runtimes, which seems to follow the general growth of interest in using of containers\
    \ for HPC. Runtimes such as Shifter, Singularity, and Charliecloud are already\
    \ able to deal with most of the specific challenges and desired aspects we presented\
    \ in Sect. 3. Besides, this seems to be the direction the HPC research community\
    \ has favored, rather than relying on more standard solutions, such as Docker.\
    \ As future work, we are interested in developing techniques and tools to ease\
    \ the migration of traditional HPC workloads to public cloud environments. This\
    \ move to the cloud has the potential to reduce the barrier to adoption of HPC\
    \ by projects and institutions that do not have access to, and cannot afford to\
    \ own, a dedicated HPC system. Data availability Data sharing is not applicable\
    \ to this article as no datasets were generated or analyzed during the current\
    \ study. Notes http://linux-vserver.org/ (Acc. in Sept. 2022). https://www.gnu.org/software/coreutils/manual/html_node/chroot-invocation.html\
    \ (Acc. in Sept. 2022). https://man7.org/linux/man-pages/man7/namespaces.7.html\
    \ (Acc. in Sept. 2022). https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/cgroups.html\
    \ (Acc. in Sept. 2022). https://openvz.org (Acc. in Sept. 2022). https://linuxcontainers.org/\
    \ (Acc. in Sept. 2022). https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt\
    \ (Acc. in Sept. 2022). https://linuxcontainers.org/lxd/ (Acc. in Sept. 2022).\
    \ https://opencontainers.org/ (Acc. in Sept. 2022). https://podman.io/ (Acc. in\
    \ Sept. 2022). https://docs.docker.com/engine/security/rootless/ (Acc. in Sept.\
    \ 2022). References Abraham S, Paul AK, Khan RIS, Butt AR (2020) On the use of\
    \ containers in high performance computing environments. In: IEEE 13th International\
    \ Conference on Cloud Computing (CLOUD). IEEE, Beijing, China, pp 284–293. https://doi.org/10.1109/cloud49709.2020.00048\
    \ Aoyama K, Watanabe H, Ohue M, Akiyama Y (2020) Multiple HPC environments-aware\
    \ container image configuration workflow for large-scale all-to-all protein–protein\
    \ docking calculations. In: Supercomputing Frontiers. Springer, Cham, pp 23–39.\
    \ https://doi.org/10.1007/978-3-030-48842-0_2 Arango C, Dernat R, Sanabria J (2017)\
    \ Performance evaluation of container-based virtualization for high performance\
    \ computing environments. arXiv:1709.10140. Accessed 13 Sept 2022 Azab A (2017)\
    \ Enabling Docker containers for high-performance and many-task computing. In:\
    \ IEEE International Conference on Cloud Engineering, pp 279–285. IEEE, Vancouver,\
    \ Canada. https://doi.org/10.1109/ic2e.2017.52 Bachiega NG, Souza PSL, Bruschi\
    \ SM, de Souza SRS (2018) Container-based performance evaluation: a survey and\
    \ challenges. In: IEEE International Conference on Cloud Engineering (IC2E). IEEE,\
    \ Vancouver, Canada, pp 398–403. https://doi.org/10.1109/ic2e.2018.00075 Bahls\
    \ D (2016) Evaluating shifter for HPC applications. In: Cray User Group. CUG,\
    \ London, UK. https://cug.org/proceedings/cug2016_proceedings/includes/files/pap135s2-file1.pdf.\
    \ Accessed 13 Sept 2022 Belkin M, Haas R, Arnold GW, Leong HW, Huerta EA, Lesny\
    \ D, Neubauer M (2018) Container solutions for HPC systems. In: Proceedings of\
    \ the Practice and Experience on Advanced Research Computing. ACM, New York, NY,\
    \ USA. https://doi.org/10.1145/3219104.3219145 Beltre AM, Saha P, Govindaraju\
    \ M, Younge A, Grant RE (2019) Enabling HPC workloads on cloud infrastructure\
    \ using kubernetes container orchestration mechanisms. In: IEEE/ACM International\
    \ Workshop on Containers and New Orchestration Paradigms for Isolated Environments\
    \ in HPC (CANOPIE-HPC). IEEE, Denver, CO, USA, pp 11–20. https://doi.org/10.1109/canopie-hpc49598.2019.00007\
    \ Benedicic L, Cruz FA, Madonna A, Mariotti K (2019) Sarus: highly scalable Docker\
    \ containers for HPC systems. In: International Conference on High Performance\
    \ Computing. LNCS—ISC-HPC. Springer, Cham, pp 46–60. https://doi.org/10.1007/978-3-030-34356-9_5\
    \ Bentaleb O, Belloum AS, Sebaa A, El-Maouhab A (2022) Containerization technologies:\
    \ taxonomies, applications and challenges. J Supercomput 78(1):1144–1181. https://doi.org/10.1007/s11227-021-03914-1\
    \ Article   Google Scholar   Beserra D, Moreno ED, Endo PT, Barreto J (2016) Performance\
    \ evaluation of a lightweight virtualization solution for HPC I/O scenarios. In:\
    \ IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE,\
    \ Melbourne, Australia, pp 004681–004686. https://doi.org/10.1109/smc.2016.7844970\
    \ Brayford D, Vallecorsa S, Atanasov A, Baruffa F, Riviera W (2019) Deploying\
    \ AI frameworks on secure HPC systems with containers. In: IEEE High Performance\
    \ Extreme Computing Conference (HPEC). IEEE, Waltham, MA, USA, pp 1–6. https://doi.org/10.1109/hpec.2019.8916576\
    \ Brayford D, Allalen M, Iapichino L, Brennan J, Moran N, Q’Riordan LJ, Hanley\
    \ K (2021) Deploying containerized QuanEX quantum simulation software on HPC systems.\
    \ In: 3rd International Workshop on Containers and New Orchestration Paradigms\
    \ for Isolated Environments in HPC (CANOPIE-HPC), pp 1–9. https://doi.org/10.1109/CANOPIEHPC54579.2021.00005\
    \ Canon RS, Younge A (2019) A case for portability and reproducibility of HPC\
    \ containers. In: IEEE/ACM International Workshop on Containers and New Orchestration\
    \ Paradigms for Isolated Environments in HPC (CANOPIE-HPC). IEEE, Denver, CO,\
    \ USA, pp 49–54. https://doi.org/10.1109/canopie-hpc49598.2019.00012 Casalicchio\
    \ E, Iannucci S (2020) The state-of-the-art in container technologies: application,\
    \ orchestration and security. Concurr Comput 32(17):5668. https://doi.org/10.1002/cpe.5668.e5668cpe.5668\
    \ Article   Google Scholar   Cavet C, Souchal M, Gadrat S, Grasseau G, Satirana\
    \ A, Bailly-Reyre A, Dadoun O, Mendoza V, Chamont D, Marchal-Duval G, Medernach\
    \ E, Pansanel J (2020) ComputeOps: container for high performance computing. EPJ\
    \ Web Conf 245:07006. https://doi.org/10.1051/epjconf/202024507006 Article   Google\
    \ Scholar   Cerin C, Greneche N, Menouer T (2020) Towards pervasive containerization\
    \ of HPC job schedulers. In: International Symposium on Computer Architecture\
    \ and High Performance Computing (SBAC-PAD). IEEE, Porto, Portugal, pp 281–288.\
    \ https://doi.org/10.1109/sbac-pad49847.2020.00046 Chang Y-TS, Heistand S, Hood\
    \ R, Jin H (2021) Feasibility of running singularity containers with hybrid MPI\
    \ on NASA high-end computing resources. In: 3rd International Workshop on Containers\
    \ and New Orchestration Paradigms for Isolated Environments in HPC (CANOPIE-HPC),\
    \ pp 17–28. https://doi.org/10.1109/CANOPIEHPC54579.2021.00007 Chen J, Guan Q,\
    \ Liang X, Bryant P, Grubel P, McPherson A, Lo L-T, Randles T, Chen Z, Ahrens\
    \ JP (2018) Build and execution environment (BEE): an encapsulated environment\
    \ enabling HPC applications running everywhere. In: IEEE International Conference\
    \ on Big Data (Big Data). IEEE, Seattle, WA, USA, pp 1737–1746. https://doi.org/10.1109/bigdata.2018.8622572\
    \ Chung MT, Le A, Quang-Hung N, Nguyen D-D, Thoai N (2016) Provision of Docker\
    \ and InfiniBand in high performance computing. In: International Conference on\
    \ Advanced Computing and Applications (ACOMP). IEEE, Can Tho City, Vietnam, pp\
    \ 127–134. https://doi.org/10.1109/acomp.2016.027 Chung MT, Quang-Hung N, Nguyen\
    \ M-T, Thoai N (2016) Using Docker in high performance computing applications.\
    \ In: IEEE Sixth International Conference on Communications and Electronics (ICCE).\
    \ IEEE, Ha Long Bay, Vietnan, pp 52–57. https://doi.org/10.1109/cce.2016.7562612\
    \ de Bayser M, Cerqueira R (2017) Integrating MPI with Docker for HPC. In: IEEE\
    \ International Conference on Cloud Engineering (IC2E). IEEE, Vancouver, BC, Canada,\
    \ pp 259–265. https://doi.org/10.1109/ic2e.2017.40 Freyermuth O, Wienemann P,\
    \ Bechtle P, Desch K (2021) Operating an HPC/HTC cluster with fully containerized\
    \ jobs using HTCondor, Singularity, CephFS and CVMFS. Comput Softw Big Sci 5(1):1–20.\
    \ https://doi.org/10.1007/s41781-020-00050-y Article   Google Scholar   Gantikow\
    \ H, Walter S, Reich C (2020) Rootless containers with Podman for HPC. In: International\
    \ Conference on High Performance Computing. LNCS—ISC-HPC. Springer, Cham, pp 343–354.\
    \ https://doi.org/10.1007/978-3-030-59851-8_23 Gerhardt L, Bhimji W, Canon S,\
    \ Fasel M, Jacobsen D, Mustafa M, Porter J, Tsulaia V (2017) Shifter: containers\
    \ for HPC. J Phys Conf Ser 898:082021. https://doi.org/10.1088/1742-6596/898/8/082021\
    \ Article   Google Scholar   Gomes J, Bagnaschi E, Campos I, David M, Alves L,\
    \ Martins JA, Pina JA, López-García A, Orviz P (2018) Enabling rootless Linux\
    \ containers in multi-user environments. The udocker tool. Comput Phys Commun\
    \ 232:84–97. https://doi.org/10.1016/j.cpc.2018.05.021 Article   Google Scholar\
    \   Grupp A, Kozlov V, Campos I, David M, Gomes J, López García Á (2019) Benchmarking\
    \ deep learning infrastructures by means of TensorFlow and containers. In: International\
    \ Conference on High Performance Computing. LNCS—ISC-HPC, pp. 478–489. Springer,\
    \ Cham. https://doi.org/10.1007/978-3-030-34356-9_36 Hale JS, Li L, Richardson\
    \ CN, Wells GN (2017) Containers for portable, productive, and performant scientific\
    \ computing. Comput Sci Eng 19(6):40–50. https://doi.org/10.1109/mcse.2017.2421459\
    \ Article   Google Scholar   Heinonen N (2019) ALCF research benefits from singularity.\
    \ https://www.hpcwire.com/off-the-wire/alcf-research-benefits-from-singularity/.\
    \ Accessed 13 Feb 2022 Herbein S, Dusia A, Landwehr A, McDaniel S, Monsalve J,\
    \ Yang Y, Seelam SR, Taufer M (2016) Resource management for running HPC applications\
    \ in container clouds. In: International Conference on High Performance Computing.\
    \ LNCS—ISC-HPC, vol. 9697, pp. 261–278. Springer, Frankfurt, Germany. https://doi.org/10.1007/978-3-319-41321-1_14\
    \ Higgins J, Holmes V, Venters C (2016) Autonomous discovery and management in\
    \ virtual container clusters. Comput J 60(2):240–252. https://doi.org/10.1093/comjnl/bxw102\
    \ Article   Google Scholar   Higgins J, Holmes V, Venters C (2016) Securing user\
    \ defined containers for scientific computing. In: International Conference on\
    \ High Performance Computing and Simulation (HPCS). IEEE, Innsbruck, Austria,\
    \ pp 449–453. https://doi.org/10.1109/hpcsim.2016.7568369 Hisle MS, Meier MS,\
    \ Toth DM (2018) Accelerating AutoDock vina with containerization. In: Proceedings\
    \ of the Practice and Experience on Advanced Research Computing. ACM, New York,\
    \ NY, USA. https://doi.org/10.1145/3219104.3219154 Höb M, Kranzlmüller D (2020)\
    \ Enabling EASEY deployment of containerized applications for future HPC systems.\
    \ In: 20th International Conference on Computational Science (ICCS). Springer,\
    \ Cham, pp 206–219. https://doi.org/10.1007/978-3-030-50371-0_15 Hu G, Zhang Y,\
    \ Chen W (2019) Exploring the performance of singularity for high performance\
    \ computing scenarios. In: IEEE 5th International Conference on Data Science and\
    \ Systems (DSS). IEEE, Zhangjiajie, China, pp 2587–2593. https://doi.org/10.1109/hpcc/smartcity/dss.2019.00362\
    \ Huang L, Wang Y, Lu C-Y, Liu S (2021) Best practice of IO workload management\
    \ in containerized environments on supercomputers. In: Proceedings of Practice\
    \ and Experience in Advanced Research Computing (PEARC). ACM, New York, NY, USA.\
    \ https://doi.org/10.1145/3437359.3465561 Hursey J (2020) Design considerations\
    \ for building and running containerized MPI applications. In: IEEE/ACM International\
    \ Workshop on Containers and New Orchestration Paradigms for Isolated Environments\
    \ in HPC (CANOPIE-HPC). IEEE, Atlanta, GA, USA, pp 35–44. https://doi.org/10.1109/canopiehpc51917.2020.00010\
    \ Jacobsen DM, Canon RS (2015) Contain this: unleashing Docker for HPC. https://cug.org/proceedings/cug2015_proceedings/includes/files/pap157-file2.pdf.\
    \ Accessed 13 Sept 2022 Jung K, Cho Y-K, Tak Y-J (2021) Containers and orchestration\
    \ of numerical ocean model for computational reproducibility and portability in\
    \ public and private clouds: application of ROMS 3.6. Simul Model Pract Theory\
    \ 109:102305. https://doi.org/10.1016/j.simpat.2021.102305 Article   Google Scholar\
    \   Kadri S, Sboner A, Sigaras A, Roy S (2022) Containers in bioinformatics: applications,\
    \ practical considerations, and best practices in molecular pathology. J Molec\
    \ Diag 24(5):442–454. https://doi.org/10.1016/j.jmoldx.2022.01.006 Article   Google\
    \ Scholar   Khan M, Becker T, Kuppuudaiyar P, Elster AC (2018) Container-based\
    \ virtualization for heterogeneous HPC clouds: insights from the EU h2020 CloudLightning\
    \ project. In: 2018 IEEE International Conference on Cloud Engineering (IC2E).\
    \ IEEE, Orlando, FL, USA, pp 392–397. https://doi.org/10.1109/ic2e.2018.00074\
    \ Kovacs A (2017) Comparison of different Linux containers. In: 40th International\
    \ Conference on Telecommunications and Signal Processing (TSP). IEEE, Barcelona,\
    \ Spain, pp 47–51. https://doi.org/10.1109/tsp.2017.8075934 Kuity A, Peddoju SK\
    \ (2017) Performance evaluation of container-based high performance computing\
    \ ecosystem using OpenPOWER. In: International Conference on High Performance\
    \ Computing. LNCS—ISC-HPC. Springer, Cham, pp 290–308. https://doi.org/10.1007/978-3-319-67630-2_22\
    \ Kumar M, Kaur G (2022) Containerized MPI application on Infiniband based HPC:\
    \ an empirical study. In: 3rd International Conference for Emerging Technology\
    \ (INCET), pp 1–6. https://doi.org/10.1109/INCET54531.2022.9824366 Kumar Abhishek\
    \ M (2020) High performance computing using containers in cloud. Int J Adv Trends\
    \ Comput Sci Eng 9(4):5686–5690. https://doi.org/10.30534/ijatcse/2020/220942020\
    \ Article   Google Scholar   Kurtzer GM, Sochat V, Bauer MW (2017) Singularity:\
    \ scientific containers for mobility of compute. PLoS One 12(5):1–20. https://doi.org/10.1371/journal.pone.0177459\
    \ Article   Google Scholar   Lahiff A, de Witt S, Caballer M, La Rocca G, Pamela\
    \ S, Coster D (2020) Running HTC and HPC applications opportunistically across\
    \ private, academic and public clouds. EPJ Web Conf 245:07032. https://doi.org/10.1051/epjconf/202024507032\
    \ Article   Google Scholar   Le E, Paz D (2017) Performance analysis of applications\
    \ using singularity container on SDSC Comet. In: Proceedings of the Practice and\
    \ Experience in Advanced Research Computing 2017 on Sustainability, Success and\
    \ Impact (PEARC). ACM, New York, NY, USA. https://doi.org/10.1145/3093338.3106737\
    \ Lee M, Ahn H, Hong C-H, Nikolopoulos DS (2022) gShare: a centralized GPU memory\
    \ management framework to enable GPU memory sharing for containers. Future Gener\
    \ Comput Sys 130:181–192. https://doi.org/10.1016/j.future.2021.12.016 Article\
    \   Google Scholar   Lim SB, Woo J, Li G (2020) Performance analysis of container-based\
    \ networking solutions for high-performance computing cloud. Int J Electr Comput\
    \ Eng 10(2):1507. https://doi.org/10.11591/ijece.v10i2.pp1507-1514 Article   Google\
    \ Scholar   Liu P, Guitart J (2020) Performance comparison of multi-container\
    \ deployment schemes for HPC workloads: an empirical study. J Supercomput 77(6):6273–6312.\
    \ https://doi.org/10.1007/s11227-020-03518-1 Article   Google Scholar   Liu P,\
    \ Guitart J (2022) Performance characterization of containerization for HPC workloads\
    \ on Infiniband clusters: an empirical study. Clust Comput 25(2):847–868. https://doi.org/10.1007/s10586-021-03460-8\
    \ Article   Google Scholar   Ma H, Wang L, Tak BC, Wang L, Tang C (2016) Auto-tuning\
    \ performance of MPI parallel programs using resource management in container-based\
    \ virtual cloud. In: IEEE 9th International Conference on Cloud Computing (CLOUD).\
    \ IEEE, San Francisco, CA, USA, pp 545–552. https://doi.org/10.1109/cloud.2016.0078\
    \ Maliszewski AM, Vogel A, Griebler D, Roloff E, Fernandes LG, Navaux Philippe\
    \ OA (2019) Minimizing communication overheads in container-based clouds for HPC\
    \ applications. In: IEEE Symposium on Computers and Communications (ISCC). IEEE,\
    \ Barcelona, Spain, pp 1–6. https://doi.org/10.1109/iscc47284.2019.8969716 Maliszewski\
    \ AM, Roloff E, Griebler D, Gaspary LP, Navaux POA (2020) Performance impact of\
    \ IEEE 802.3ad in container-based clouds for HPC applications. In: International\
    \ Conference on Computational Science and Its Applications (ICCSA). Springer,\
    \ Cham, pp 158–167. https://doi.org/10.1007/978-3-030-58817-5_13 Manalo K, Baber\
    \ L, Bradley R, You Z-Q, Zhang N (2019) Community collections. In: Proceedings\
    \ of Practice and Experience in Advanced Research Computing (PEARC) Rise of the\
    \ Machines (Learning). ACM, New York, NY, USA. https://doi.org/10.1145/3332186.3332199\
    \ Martinasso M, Gila M, Sawyer W, Sarmiento R, Peretti-Pezzi G, Karakasis V (2019)\
    \ Cray programming environments within containers on cray XC systems. Concurr\
    \ Comput 32(20):5543. https://doi.org/10.1002/cpe.5543.e5543cpe.5543 Article \
    \  Google Scholar   Medrano-Jaimes F, Lozano-Rizk JE, Castañeda-Avila S, Rivera-Rodriguez\
    \ R (2019) Use of containers for high-performance computing. In: International\
    \ Conference on Supercomputing in Mexico. CCIS—ISUM. Springer, Cham, pp 24–32.\
    \ https://doi.org/10.1007/978-3-030-10448-1_3 Merkel D (2014) Docker: lightweight\
    \ Linux containers for consistent development and deployment. https://www.linuxjournal.com/content/docker-lightweight-linux-containers-consistent-development-and-deployment.\
    \ Accessed 13 Sept 2022 Michel M, Burnett N (2019) Enabling GPU-enhanced computer\
    \ vision and machine learning research using containers. In: International Conference\
    \ on High Performance Computing. LNCS—ISC-HPC. Springer, Cham, pp 80–87. https://doi.org/10.1007/978-3-030-34356-9_8\
    \ Muhtaroglu N, Ari I, Kolcu B (2018) Democratization of HPC cloud services with\
    \ automated parallel solvers and application containers. Concurr Comput 30(21):4782.\
    \ https://doi.org/10.1002/cpe.4782 Article   Google Scholar   Muscianisi G, Fiameni\
    \ G, Azab A (2019) Singularity GPU containers execution on HPC cluster. In: Weiland\
    \ M, Juckeland G, Alam S, Jagode H (eds) International conference on high performance\
    \ computing. LNCS—ISC-HPC. Springer, Cham, pp 61–68. https://doi.org/10.1007/978-3-030-34356-9_6\
    \ Newlin M, Smathers K, DeYoung ME (2019) ARC containers for AI workloads. In:\
    \ Proceedings of Practice and Experience in Advanced Research Computing (PEARC)\
    \ Rise of the Machines (Learning). ACM, New York, NY, USA. https://doi.org/10.1145/3332186.3333048\
    \ Nguyen N, Bein D (2017) Distributed MPI cluster with docker swarm mode. In:\
    \ IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC).\
    \ IEEE, Las Vegas, NV, USA, pp 1–7. https://doi.org/10.1109/ccwc.2017.7868429\
    \ Okuno S, Hirai A, Fukumoto N (2022) Performance analysis of multi-containerized\
    \ MD simulations for low-level resource allocation. In: IEEE International Parallel\
    \ and Distributed Processing Symposium Workshops (IPDPSW), pp 1014–1017. https://doi.org/10.1109/IPDPSW55747.2022.00162\
    \ Pahl C, Brogi A, Soldani J, Jamshidi P (2019) Cloud container technologies:\
    \ a state-of-the-art review. IEEE Trans Cloud Comput 7(3):677–692. https://doi.org/10.1109/tcc.2017.2702586\
    \ Article   Google Scholar   Peiro Conde K (2020) Containers in HPC: is it worth\
    \ it? Master’s thesis. http://hdl.handle.net/2117/335295. Accessed 13 Sept 2022\
    \ Piras ME, Pireddu L, Moro M, Zanetti G (2019) Container orchestration on HPC\
    \ clusters. In: International Conference on High Performance Computing. LNCS—ISC-HPC.\
    \ Springer, Cham, pp 25–35. https://doi.org/10.1007/978-3-030-34356-9_3 Priedhorsky\
    \ R, Randles T (2017) Charliecloud. In: Proceedings of International Conference\
    \ for High Performance Computing, Networking, Storage and Analysis, SC ’17. ACM,\
    \ New York, NY, USA. https://doi.org/10.1145/3126908.3126925 Priedhorsky R, Canon\
    \ RS, Randles T, Younge AJ (2021) Minimizing privilege for building HPC containers.\
    \ In: Proceedings of International Conference for High Performance Computing,\
    \ Networking, Storage and Analysis, SC, pp 1–14 https://doi.org/10.1145/3458817.3476187\
    \ Ramon-Cortes C, Serven A, Ejarque J, Lezzi D, Badia RM (2018) Transparent orchestration\
    \ of task-based parallel applications in containers platforms. J Grid Comput 16(1):137–160.\
    \ https://doi.org/10.1007/s10723-017-9425-z Article   Google Scholar   Rudyy O,\
    \ Garcia-Gasulla M, Mantovani F, Santiago A, Sirvent R, Vazquez M (2019) Containers\
    \ in HPC: a scalability and portability study in production biological simulations.\
    \ In: IEEE International Parallel and Distributed Processing Symposium (IPDPS).\
    \ IEEE, Rio de Janeiro, RJ, Brazil, pp. 567–577. https://doi.org/10.1109/ipdps.2019.00066\
    \ Ruhela A, Vaughn M, Harrell SL, Zynda GJ, Fonner J, Evans RT, Minyard T (2020)\
    \ Containerization on petascale HPC clusters. In: Supercomp State Pract Arch.\
    \ SC—SOTP. https://sc20.supercomputing.org/proceedings/sotp/sotp_files/sotp120s2-file1.pdf\
    \ Ruhela A, Harrell SL, Evans RT, Zynda GJ, Fonner J, Vaughn M, Minyard T, Cazes\
    \ J (2021) Characterizing containerized HPC applications performance at petascale\
    \ on CPU and GPU architectures. In: International Conference on High Performance\
    \ Computing. LNCS—ISC-HPC. Springer, Cham, pp 411–430. https://doi.org/10.1007/978-3-030-78713-4_22\
    \ Saha P, Beltre A, Uminski P, Govindaraju M (2018) Evaluation of docker containers\
    \ for scientific workloads in the Cloud. In: Proceedings of the Practice and Experience\
    \ on Advanced Research Computing. ACM, New York, NY, USA. https://doi.org/10.1145/3219104.3229280\
    \ Sampedro Z, Holt A, Hauser T (2018) Continuous integration and delivery for\
    \ HPC. In: Proceedings of the Practice and Experience on Advanced Research Computing\
    \ (PEARC). ACM, New York, NY, USA. https://doi.org/10.1145/3219104.3219147 Sande\
    \ Veiga V, Simon M, Azab A, Fernandez C, Muscianisi G, Fiameni G, Marocchi S (2019)\
    \ Evaluation and benchmarking of Singularity MPI containers on EU research e-infrastructure.\
    \ In: IEEE/ACM International Workshop on Containers and New Orchestration Paradigms\
    \ for Isolated Environments in HPC (CANOPIE-HPC). IEEE, Denver, CO, USA, pp 1–10.\
    \ https://doi.org/10.1109/canopie-hpc49598.2019.00006 Simchev T, Atanassov E (2020)\
    \ Performance effects of running container-based Open-MPI cluster in public cloud.\
    \ In: International Conference on Large-Scale Scientific Computing (LSSC). Springer,\
    \ Cham, pp 254–262. https://doi.org/10.1007/978-3-030-41032-2_29 Sochat VV, Prybol\
    \ CJ, Kurtzer GM (2017) Enhancing reproducibility in scientific computing: metrics\
    \ and registry for Singularity containers. PLoS One 12(11):0188511. https://doi.org/10.1371/journal.pone.0188511\
    \ Article   Google Scholar   Sparks J (2017) HPC containers in use. In: Proceedings\
    \ of the Cray User Group (CUG). https://cug.org/proceedings/cug2017_proceedings/includes/files/pap164s2-file1.pdf.\
    \ Accessed 13 Sept 2022 Sparks J (2018) Enabling docker for HPC. Concurr Comput\
    \ 31(16):5018. https://doi.org/10.1002/cpe.5018 Article   Google Scholar   Steffenel\
    \ LA, Charão AS, Alves B, de Araujo LR, da Silva LF (2020) MPI to Go: container\
    \ clusters for MPI applications. In: International Conference on Cloud Computing\
    \ and Services Science. Springer, Cham, pp 199–222. https://doi.org/10.1007/978-3-030-49432-2_10\
    \ Tippit J, Hodson DD, Grimaila MR (2021) Julia and singularity for high performance\
    \ computing. In: Advances in Parallel and Distributed Processing, and Applications,\
    \ pp 3–15. Springer, Cham. https://doi.org/10.1007/978-3-030-69984-0_1 Torrez\
    \ A, Randles T, Priedhorsky R (2019) HPC container runtimes have minimal or no\
    \ performance impact. In: IEEE/ACM International Workshop on Containers and New\
    \ Orchestration Paradigms for Isolated Environments in HPC (CANOPIE-HPC). IEEE,\
    \ Denver, CO, USA, pp 37–42. https://doi.org/10.1109/canopie-hpc49598.2019.00010\
    \ Tronge J, Chen J, Grubel P, Randles T, Davis R, Wofford Q, Anaya S, Guan Q (2021)\
    \ BeeSwarm: enabling parallel scaling performance measurement in continuous integration\
    \ for HPC applications. In: 36th IEEE/ACM International Conference on Automated\
    \ Software Engineering (ASE), pp 1136–1140. https://doi.org/10.1109/ASE51524.2021.9678805\
    \ Vaillancourt PZ, Coulter JE, Knepper R, Barker B (2020) Self-scaling clusters\
    \ and reproducible containers to enable scientific computing. In: IEEE High Performance\
    \ Extreme Computing Conference (HPEC). IEEE, Boston, MA, USA, pp 1–8. https://doi.org/10.1109/hpec43674.2020.9286208\
    \ Vaillancourt P, Wineholt B, Barker B, Deliyannis P, Zheng J, Suresh A, Brazier\
    \ A, Knepper R, Wolski R (2020) Reproducible and portable workflows for scientific\
    \ computing and HPC in the cloud. In: Practice and Experience in Advanced Research\
    \ Computing (PEARC). ACM, Portland, OR, USA, pp 311–320. https://doi.org/10.1145/3311790.3396659\
    \ Vallee G, Gutierrez CEA, Clerget C (2019) On-node resource manager for containerized\
    \ HPC workloads. In: IEEE/ACM International Workshop on Containers and New Orchestration\
    \ Paradigms for Isolated Environments in HPC (CANOPIE-HPC). IEEE, Denver, CO,\
    \ USA, pp 43–48. https://doi.org/10.1109/canopie-hpc49598.2019.00011 Wang Y, Evans\
    \ RT, Huang L (2019) Performant container support for HPC applications. In: Proceedings\
    \ of the Practice and Experience in Advanced Research Computing on Rise of the\
    \ Machines (learning) (PEARC). ACM, Chicago, IL, USA. https://doi.org/10.1145/3332186.3332226\
    \ Weidner O, Atkinson M, Barker A, Filgueira Vicente R (2016) Rethinking high\
    \ performance computing platforms. In: ACM International Workshop on Data-Intensive\
    \ Distributed Computing. ACM, Kyoto, Japan, pp 19–26. https://doi.org/10.1145/2912152.2912155\
    \ Yang X, Kasahara M (2022) LPMX: a pure rootless composable container system.\
    \ BMC Bioinf 23(1):1–13. https://doi.org/10.1186/s12859-022-04649-3 Article  \
    \ Google Scholar   Youn C, Das AK, Yang S, Kim J (2019) Developing a meta framework\
    \ for key-value memory networks on HPC clusters. In: Proceedings of the Practice\
    \ and Experience in Advanced Research Computing on Rise of the Machines (Learning)\
    \ (PEARC). ACM, New York, NY, USA. https://doi.org/10.1145/3332186.3332216 Younge\
    \ AJ, Pedretti K, Grant RE, Brightwell R (2017) A tale of two systems: Using containers\
    \ to deploy HPC applications on supercomputers and clouds. In: IEEE International\
    \ Conference on Cloud Computing Technology and Science (CloudCom). IEEE, Hong\
    \ Kong, pp 74–81. https://doi.org/10.1109/cloudcom.2017.40 Yuan S, Brandt SR,\
    \ Chen Q, Zhu L, Salatin R, Dooley R (2020) A sustainable collaboratory for coastal\
    \ resilience research. Future Gener Comput Syst 111:786–792. https://doi.org/10.1016/j.future.2019.11.002\
    \ Article   Google Scholar   Zhang J, Lu X, Panda DK (2016) High performance MPI\
    \ library for container-based HPC cloud on InfiniBand clusters. In: 45th international\
    \ conference on parallel processing (ICPP). IEEE, Philadelphia, PA, USA, pp 268–277.\
    \ https://doi.org/10.1109/icpp.2016.38 Zhang J, Lu X, Panda DK (2016) Performance\
    \ characterization of hypervisor-and container-based virtualization for HPC on\
    \ SR-IOV enabled InfiniBand clusters. In: IEEE International Parallel and Distributed\
    \ Processing Symposium Workshops (IPDPSW). IEEE, Chicago, IL, USA, pp 1777–1784.\
    \ https://doi.org/10.1109/ipdpsw.2016.178 Zhang J, Lu X, Panda DK (2017) Is singularity-based\
    \ container technology ready for running MPI applications on HPC clouds? In: Proceedings\
    \ of the10th International Conference on Utility and Cloud Computing (UCC ’17).\
    \ ACM, New York, NY, USA, pp 151–160. https://doi.org/10.1145/3147213.3147231\
    \ Zhou N, Georgiou Y, Pospieszny M, Zhong L, Zhou H, Niethammer C, Pejak B, Marko\
    \ O, Hoppe D (2021) Container orchestration on HPC systems through Kubernetes.\
    \ J Cloud Comput 10(1):1–14. https://doi.org/10.1186/s13677-021-00231-z Article\
    \   Google Scholar   Download references Funding This work received partial financial\
    \ support from Petrobras, FAPESP (Grants 2013/08293-7 and 2019/12792-5), and CNPq\
    \ (Grant 314645/2020-9). Author information Author notes Edson Borin contributed\
    \ equally to this work. Authors and Affiliations Institute of Computing, University\
    \ of Campinas, Av. Albert Einstein, 1251, Campinas, SP, 13083-852, Brazil Rafael\
    \ Keller Tesser & Edson Borin Center for Computing in Engineering and Sciences,\
    \ University of Campinas, R. Josué Castro, s/n, Campinas, SP, 13083-861, Brazil\
    \ Rafael Keller Tesser Corresponding author Correspondence to Rafael Keller Tesser.\
    \ Ethics declarations Conflict of interest The authors have no relevant financial\
    \ or non-financial conflicts of interest to disclose. Additional information Publisher's\
    \ Note Springer Nature remains neutral with regard to jurisdictional claims in\
    \ published maps and institutional affiliations. Appendix Appendix Useful and\
    \ commonly repeated acronyms and abbreviations: ABI: Application Binary Interface;\
    \ AI: Artificial Intelligence; AMG: Algebraic Multigrid method; API: Application\
    \ Programming Interface; AUFS: Docker’s Union File System; AWS: Amazon Web Services;\
    \ BEE: Build and Execution Environment; CCE: Cray Computing Environment; CDT:\
    \ Cray Development Toolkit; CFD: Computational Fluid-Dynamics; CFQ: Completely\
    \ Fair Queuing; CI: Continuous Integration; CI/CD: Continuous Integration and\
    \ Continuous Delivery; CLE: Cray Linux Environment; CLI: Command-Line Interface;\
    \ CMA: Cross Memory Attach; CPE: Cray Processing Environment; CPU: Central Processing\
    \ Unit; CV: Computer Vision; DASH: a C++ template library for distributed data\
    \ structures; DFE: Data-Flow Engine; EC2: [Amazon] Elastic Compute Cloud; ELF:\
    \ Executable and Linkable Format; FPGA: Field-Programmable Gate Array; GID: Group\
    \ Identification; GPU: Graphics Processing Unit; HPC: High Performance Computing;\
    \ HPCC: High Performance Computing Challenge; HPL: High Performance LINPACK; HTC:\
    \ High Throughput Computing; IB: InfiniBand; IMB: Intel MPI Benchmarks; IO: Input\
    \ and Output; IOR: a parallel IO benchmark; IP: Internet Protocol; IPC: Inter-Process\
    \ Communication; IPoIB: IP over InfiniBand; IT: Information Technology; KMI: K-mer\
    \ Matching Interface; KNL: [Intel] Knights Landing; KVM: Kernel Virtual Machine;\
    \ LXC: Linux Containers; LXD: An interface for LXC; MCDRAM: Multi-Channel Dynamic\
    \ Random Access Memory; MD: Molecular Dynamics; ML: Machine Learning; MPI: Message\
    \ Passing Interface; NAS: NASA Advanced Supercomputing Division; NFS: Network\
    \ File System; NIC: Network Interface Controller; NPB: NAS Parallel Benchmark;\
    \ NUMA: Non-Uniform Memory Access; OCI: Open Container Initiative; OS: Operating\
    \ System; OSU: Ohio State University; PCI: Peripheral Component Interconnect;\
    \ PFS: Parallel File System; PID: Process Identification; RAM: Random-Access Memory;\
    \ RDMA: Remote Direct Memory Access; RNN: Recursive Neural Network; ROMS: Regional\
    \ Ocean Modeling System; SDN: Software-Defined Network; SHM: Shared Memory; SMT:\
    \ Simultaneous Multi-Threading; SR-IOV: Single Root IO Virtualization; SSH: Secure\
    \ Shell; TCP: Transmission Control Protocol; TF: TensorFlow; UDI: User-Defined\
    \ Image; UDSS: User-Defined Software Stack; UID: User Identification; VCC: Virtual\
    \ Container Cluster; VM: Virtual Machine; VXLAN: Virtual Extensible Local Area\
    \ Network; WRF: Weather Research and Forecasting [model]. Rights and permissions\
    \ Springer Nature or its licensor holds exclusive rights to this article under\
    \ a publishing agreement with the author(s) or other rightsholder(s); author self-archiving\
    \ of the accepted manuscript version of this article is solely governed by the\
    \ terms of such publishing agreement and applicable law. Reprints and permissions\
    \ About this article Cite this article Keller Tesser, R., Borin, E. Containers\
    \ in HPC: a survey. J Supercomput 79, 5759–5827 (2023). https://doi.org/10.1007/s11227-022-04848-y\
    \ Download citation Accepted 21 September 2022 Published 27 October 2022 Issue\
    \ Date March 2023 DOI https://doi.org/10.1007/s11227-022-04848-y Share this article\
    \ Anyone you share the following link with will be able to read this content:\
    \ Get shareable link Provided by the Springer Nature SharedIt content-sharing\
    \ initiative Keywords Containers HPC High performance computing Parallel processing\
    \ OS-level virtualization Survey Use our pre-submission checklist Avoid common\
    \ mistakes on your manuscript. Sections Figures References Abstract Introduction\
    \ Overview of container technologies HPC-specific requirements for containers\
    \ HPC container implementations Survey methodology Statistics Organization of\
    \ this survey Resource specific overheads Overall application overhead Other overhead\
    \ Application portability Research reproducibility Unprivileged user containers\
    \ Related work Conclusion Data availability Notes References Funding Author information\
    \ Ethics declarations Additional information Appendix Rights and permissions About\
    \ this article Advertisement Discover content Journals A-Z Books A-Z Publish with\
    \ us Publish your research Open access publishing Products and services Our products\
    \ Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio\
    \ BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state\
    \ privacy rights Accessibility statement Terms and conditions Privacy policy Help\
    \ and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University\
    \ of Nebraska-Lincoln (3000134173) © 2024 Springer Nature\""
  inline_citation: '>'
  journal: Journal of Supercomputing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Containers in HPC: a survey'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Olaya P.
  - Kennedy D.
  - Llamas R.
  - Valera L.
  - Vargas R.
  - Lofstead J.
  - Taufer M.
  citation_count: '5'
  description: To trust findings in computational science, scientists need workflows
    that trace the data provenance and support results explainability. As workflows
    become more complex, tracing data provenance and explaining results become harder
    to achieve. In this paper, we propose a computational environment that automatically
    creates a workflow execution's record trail and invisibly attaches it to the workflow's
    output, enabling data traceability and results explainability. Our solution transforms
    existing container technology, includes tools for automatically annotating provenance
    metadata, and allows effective movement of data and metadata across the workflow
    execution. We demonstrate the capabilities of our environment with the study of
    SOMOSPIE, an earth science workflow. Through a suite of machine learning modeling
    techniques, this workflow predicts soil moisture values from the 27 km resolution
    satellite data down to higher resolutions necessary for policy making and precision
    agriculture. By running the workflow in our environment, we can identify the causes
    of different accuracy measurements for predicted soil moisture values in different
    resolutions of the input data and link different results to different machine
    learning methods used during the soil moisture downscaling, all without requiring
    scientists to know aspects of workflow design and implementation.
  doi: 10.1109/TPDS.2022.3220539
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Parallel...
    >Volume: 34 Issue: 2 Building Trust in Earth Science Findings through Data Traceability
    and Results Explainability Publisher: IEEE Cite This PDF Paula Olaya; Dominic
    Kennedy; Ricardo Llamas; Leobardo Valera; Rodrigo Vargas; Jay Lofstead; Michela
    Taufer All Authors 6 Cites in Papers 796 Full Text Views Open Access Under a Creative
    Commons License Abstract Document Sections 1 Introduction 2 Modeling Containerized
    Workflows 3 Singularity/Apptainer Implementation 4 Traceability and Explainability
    in a Real Use Case in Earth Science 5 Measuring Overheads and Performance Show
    Full Outline Authors Figures References Citations Keywords Metrics Abstract: To
    trust findings in computational science, scientists need workflows that trace
    the data provenance and support results explainability. As workflows become more
    complex, tracing data provenance and explaining results become harder to achieve.
    In this paper, we propose a computational environment that automatically creates
    a workflow execution''s record trail and invisibly attaches it to the workflow''s
    output, enabling data traceability and results explainability. Our solution transforms
    existing container technology, includes tools for automatically annotating provenance
    metadata, and allows effective movement of data and metadata across the workflow
    execution. We demonstrate the capabilities of our environment with the study of
    SOMOSPIE, an earth science workflow. Through a suite of machine learning modeling
    techniques, this workflow predicts soil moisture values from the 27 km resolution
    satellite data down to higher resolutions necessary for policy making and precision
    agriculture. By running the workflow in our environment, we can identify the causes
    of different accuracy measurements for predicted soil moisture values in different
    resolutions of the input data and link different results to different machine
    learning methods used during the soil moisture downscaling, all without requiring
    scientists to know aspects of workflow design and implementation. Published in:
    IEEE Transactions on Parallel and Distributed Systems ( Volume: 34, Issue: 2,
    01 February 2023) Page(s): 704 - 717 Date of Publication: 08 November 2022 ISSN
    Information: DOI: 10.1109/TPDS.2022.3220539 Publisher: IEEE Funding Agency: SECTION
    1 Introduction Computational workflows play a key role in scientific discovery.
    These workflows are growing more complex: they consist of different modeling,
    analysis, and visualization modules; they run on increasingly heterogeneous systems;
    and they use machine learning (ML) methods with limited transparency. For scientists
    using these workflows to study scientific phenomena, trusting data, methods, software,
    and hardware becomes more necessary than ever. Scientists can trust their findings
    only through in-depth data lineage and the complete record trail of the methods
    generating the results. The data lineage and record trail combined enables scientists
    to trace data back to its sources and explain computational methods and their
    output. Provenance collection techniques [1], [2], [3] and container technologies
    [4], [5], [6], [7] are promising approaches to achieve data traceability through
    data lineage and result explainability through record trails. Provenance provides
    data lineage with a thorough description of the history of the data evolution,
    allowing the scientist to trace the data back to its origin and observe interactions
    between data and applications. However, there are two main limitations of current
    provenance solutions. First, the provenance metadata is separate from the dataflow,
    so any effort to match metadata to workflow components (i.e., data and applications)
    requires manual work. For example, solutions such as PASS [8], Pachyderm [9],
    and REANA [10] use separate metadata databases. Because the metadata is in a different
    location than the workflow components, it is harder for the scientists to query
    metadata and match with the components to do any provenance analysis. Second,
    scientists track provenance with custom systems that do not offer portability
    across heterogeneous platforms. For example, work in [8], [11], [12] builds on
    custom file systems, and work in [13], [14], [15], [16], [17] builds on custom
    software packaging. Containers offer a lightweight solution to track provenance
    across platforms by encapsulating applications and dependencies into an isolated
    environment [18], [19], [20], [21]. However, there are three main limitations
    of current container solutions. First, containers are used for services that do
    not save long-term states; state storage is relegated to existing, shared storage
    infrastructure. Second, container solutions do not automatically create data lineage
    and record trails. Third, current container technologies lack an effective way
    to move data through a containerized workflow. To overcome these limitations,
    we propose a computational environment that is seamlessly integrated with container
    technology, automatically creates a workflow execution''s record trail, and invisibly
    attaches the trail to the workflow''s intermediate and output data. To this end,
    we decouple data and applications of traditionally tightly coupled workflows and
    encapsulate data and applications into individual fine-grained containers. We
    augment both data and application containers to expose provenance metadata and
    to move data across the containerized workflow effectively. Additionally, we create
    an interface for visualizing and studying the metadata that scientists can use
    to understand data lineage and computational methods. We demonstrate how our environment
    enables data traceability and results explainability for the SOMOSPIE (Soil Moisture
    Spatial Inference Engine) workflow [22]. SOMOSPIE uses a suite of ML modeling
    techniques to downscale the 27 km resolution satellite data from the ESA-CCI soil
    moisture database [23] to higher resolutions necessary for practical use in earth
    sciences including precision forestry and agriculture, hydrology for landscape
    ecology, and regeneration dynamics [24], [25]. Our environment enables scientists
    to link differences in scientific results back to different input data sources
    (data lineage for traceability); and link different results to specific methods
    used, without necessarily mastering all the aspects of implementation and execution
    of the workflow (record trail for explainability). For example, with SOMOSPIE,
    our environment enables scientists to identify different predicted soil moisture
    values caused by different input data sources and their resolutions; or to connect
    different results to specific ML methods used during the soil moisture downscaling.
    We measure the performance of our environment in terms of execution time, storage
    space, and IO bandwidth. We demonstrate how our environment has limited overhead
    and is effective for establishing trustworthiness in the SOMOSPIE workflow. While
    we demonstrate the benefits of our environment with soil moisture prediction,
    our solution is application-agnostic: our environment can be easily adapted to
    general workflows consisting of self-contained applications. This work makes the
    following contributions: An environment based on fine-grained containerization
    of both data and applications which automatically creates data lineage and record
    trail of workflow executions, enabling traceability of data and explainability
    of results. A Singularity/Apptainer based implementation of our fine-grained containerized
    environment which automatically annotates each container with its data lineage
    and record trail and effectively supports zero-copy movement of data across containers.
    A demonstration of the environment capabilities to trace data sources and explain
    ML results for an exemplary earth science workflow, SOMOSPIE [22]. The paper is
    organized as follows. Section 2 describes the methodology to model a workflow
    into a fine-grained containerized environment. The implementation of the environment
    using Singularity/Apptainer as the selected container technology is explained
    in Section 3. Section 4 demonstrates the use of our containerized environment
    for an earth science workflow for traceability and explainability. We quantify
    the impact on overhead and performance in Section 5. We discuss the interoperability
    and adaptation of our environment in distributed systems in Section 6. Section
    7 discusses the related work. Finally, Section 8 concludes with a summary of the
    findings and directions for future work. SECTION 2 Modeling Containerized Workflows
    We describe how a scientific workflow can be modeled using our fine-grained containerized
    environment. We present our solution in terms of the decoupling of workflows into
    application and data components, the communication between these components, the
    automatic annotation of each component, and the visualization of the associated
    metadata. 2.1 From Native to Fine-grained Workflow Modeling A workflow is composed
    of one or multiple interoperable, self-contained applications, each with its own
    software stack and input and output data. These applications can range from data
    generation, data collection and merging, data pre-processing and feature extraction
    to data analysis, modeling, and visualization. Such a workflow can be executed
    on native HPC and cloud platforms (Fig. 1a). When container technologies are used
    in HPC or cloud platforms, the whole workflow is usually deployed in a single
    coarse-grained container (Fig. 1b). This coarse-grained containerization enables
    easy deployment and management. However, the coarse-grained approach makes it
    difficult to exactly track executions or identify all the workflow components
    and their interactions for building an in-depth data lineage and record trail.
    In addition, such an approach does not enable reusability and composability of
    individual workflow components. Instead, we decouple the workflow into its components
    (i.e., applications and data) and use a fine-grained containerized workflow approach
    that encapsulates each workflow component into its own container (Fig. 1c). Each
    container serves as an immutable object with a unique hash code for permanent
    identification, enabling easy data lineage and record trails creation. Our approach
    is applicable to workflows that can be modeled as directed acyclic graphs (DAGs)
    whose nodes (applications) and vertices (data in movement from one application
    to another) can be both containerized. In theory any workflow with such features
    can be abstracted into a fine-grained set of interconnected containers. Fig. 1.
    Composition of scientific workflows on (a) native environment (i.e., original
    workflow execution on backbone HPC or cloud platforms), (b) using a coarse-grained
    containerized environment with a single monolithic container and (c) using our
    proposed fine-grained containerized environment (i.e., decoupled workflow components
    in data and application independent containers). Show All 2.2 Designing Application
    and Data Containers Given an application, we containerize it by encapsulating
    the executable or script with the respective software stack (i.e., OS, libraries,
    and software packages). By doing so, scientists can reuse the application container
    across different workflows and reduce the overall storage requirement. Furthermore,
    in our environment all application containers are annotated with provenance information
    including their unique identifier and creation time. The containerization of data
    is unique to our approach. Because containers are isolated systems, they do not
    support persistence of data; consequently, only applications are normally containerized,
    while the data is hosted in local databases, storage volumes, or images [26],
    [27], [28]. We move away from this implementation by leveraging the work of Lofstead
    and co-authors called Data Pallets [29] that defines data as a separate and immutable
    object once created. To create this object, we define a data container that follows
    a file-system-in-a-file model. Given an individual dataset (i.e., input, intermediary,
    or output data), we containerize it into a single and independent data container.
    Similar to the application container, data containers are augmented to expose
    provenance information such as the unique hash code, creation time, execution
    task and record trail. By doing so, data containers provide trustworthiness, portability,
    and shareability of their content to users and across workflows. Two important
    principles inform our fine-grained containerized workflow approach. First, we
    separate applications and data into their own containers allowing specificity
    and unique identification of the components in a workflow for traceability and
    explainability purposes. Second, our approach values intermediate data; rather
    than discharging intermediate data, it encapsulates this data in order to provide
    a complete preservation of the data lineage for traceability and reusability.
    2.3 Designing Communication Between Containers The fine-grained containerization
    of the workflow components introduces a new challenge in the execution: data has
    to be moved from one container to the next in an efficient way. When the movement
    is done through standard container technology, it requires the usage of the host
    node''s storage to serve as a buffer in a two-copy data transfer as shown in Fig.
    2a. We propose a new communication approach that enables a zero-copy data transfer:
    we bind mount direct paths inside the data and application containers through
    their namespaces, thus avoiding data sharing via the host. This allows containers
    to directly exchange data without creating extra copies or using external storage.
    Ultimately, our approach reduces the time and space needed to transfer data between
    containers. Fig. 2b shows the same example of data transfer for the two-copy approach
    in Fig. 2a but with our zero-copy data transfer implementation. In essence, an
    application container can read and write directly from one or multiple data containers.
    Fig. 2. Communication between the workflow components in two-copy (a) and zero-copy
    (b) data transfer for our fine-grained containerized environment. Show All 2.4
    Designing Annotated Containers The fine-grained containerization of a workflow
    allows us to annotate its executions, capturing metadata at a fine-grained level.
    To deploy the annotation for different workflow executions, the metadata collection
    has to be automatic. There are three open questions when automatically annotating
    workflow executions: Where should the metadata be allocated? We augment each container
    with an extra partition, within which we allocate the metadata. Our approach enables
    the tight integration of each workflow component and its metadata, facilitating
    the traceability of both the individual components and the workflow as a whole.
    While tightly coupling the metadata partition with the workflow components, our
    environment effectively stashes the metadata information from regular workflow
    executions. This means that the data or application itself in the container can
    never be contaminated by managing the metadata partition. The metadata information
    can still be accessed at any time through standard container partition access
    commands. What metadata should be captured? We capture the container''s identification,
    creation time, execution task, and record trail. The container identification
    is a tuple (i.e., [UUID, name]) composed of a universally unique identifier (i.e.,
    UUID) and a name assigned when the container is created. While the UUID is unique,
    the name can be modified. The creation time captures the point in time when the
    container is written to disk. The execution task is the set of instructions to
    run the application. This information includes parameters such as initial conditions,
    random seeds, and other setting values. The container''s record trail is a set
    of ordered tuples (i.e., [UUID, name]) that defines the pipeline of containers
    preceding the current one (i.e., data and application containers used before the
    current container and triggering its use). How and when should the metadata be
    collected? For each container, we expose information embedded in the container
    environment, which is otherwise hidden to the workflow execution, to collect our
    metadata. For both data and application containers, the structure of the metadata
    partition (i.e., [container''s identification, creation time, execution execution
    task, and record trail]) is identical. The content of the partitions depends on
    the type of container and is collected both statically and dynamically. The static
    metadata is collected when a container is created and includes the container identification
    and creation time. The pool of dynamic metadata includes the execution task and
    record trail. For all containers, (a) the execution task is initialized with noop
    meaning that there is no operation; and (b) the record trail is initialized with
    NULL meaning that there are not predecessor containers because an execution has
    not happened yet. The dynamic metadata is populated only at the time a workflow
    is executed. For purposes of reusability across workflows, the static metadata
    of application containers remains as initialized. On the other hand, the execution
    task and record trail of the data containers are updated at the execution time
    to capture the data generation. Fig. 3 shows an example of automatic metadata
    collection for a workflow with three data containers serving as input ( Input
    ), intermediate ( Inter ), and output ( Out ) respectively, and two application
    containers ( Ap p 1 and Ap p 2 ). The static metadata is initialized with the
    container identification tuple and container creation time. During the execution
    of the workflow, the execution task and record trail are updated for the data
    containers. The figure presents a snapshot of when the whole workflow has finished
    its execution, meaning that Ap p 1 and Ap p 2 have written the results into the
    intermediate and output containers respectively. The record trail of the output
    container includes the output, Ap p 2 , and intermediate containers’ identifications.
    With the identification of the intermediate, we can retrieve its record trail
    which includes the intermediate, application1, and input containers’ identifications.
    We merge the two containers’ record trails and define all the containers used
    in the workflow pipeline. The combination of the static and dynamic metadata enables
    building the in-depth data lineage and the complete record trail of the applications
    generating the results. Fig. 3. Example of the metadata partitions for a workflow
    in our fine-grained containerized environment, composed of once executed three
    data containers serving as input ( Input ), intermediate ( Inter ), and output
    ( Out ) respectively, and two application containers ( Ap p 1 and Ap p 2 ). The
    partitions are populated with both static and dynamic information. Show All 2.5
    User Interface We provide a user interface to facilitate the study of collected
    metadata. This interface reads the metadata of one or multiple containers. From
    this metadata, the interface constructs a workflow visual representation as a
    directed graph. Scientists can interact and adapt the graph to their needs for
    further analysis. The nodes of the graph represent each container in the workflow
    which are labeled by the universally unique identifiers (i.e., UUID) and include
    their respective metadata as their description. Additionally, nodes are distinguished
    between data and application containers. For each container, the interface backtraces
    its record trail and connects the nodes in the order of the lineage of that execution.
    Subsequently, the interface constructs individual subgraphs of each execution.
    These subgraphs are merged to find common patterns and containers shared across
    multiple executions, providing a graph with unique nodes (data and application
    containers) connected based on the data lineage constructed from the descriptions
    of the metadata. SECTION 3 Singularity/Apptainer Implementation We build our computational
    environment by augmenting Singularity/Apptainer [5] container technology. We develop
    a Jupyter Notebook [30] interface for the metadata visualization and analysis.
    3.1 Selecting the Container Technology While there are many different container
    technologies (e.g., Docker [4], Singularity/Apptainer [5], Charliecloud [6], Podman
    [7]), in our work, we use Singularity/Apptainer for three main reasons. First,
    it does not require administrative privileges that are challenging to obtain in
    tightly controlled environments such as the US National Laboratories. Second,
    the use of the SIF (Singularity Image File) format container allows us to customize
    the content of each container with different types of partitions (i.e., metadata
    partition, application partition, data partition), where each container can have
    one or more partitions. Last, Singularity/Apptainer supports user-defined add-on
    functionalities through plugins. These plugins are packages that can be dynamically
    loaded by the Singularity/Apptainer at runtime, augmenting Singularity/Apptainer
    with experimental, non-standard, and vendor-specific functionalities. Some of
    these functionalities allow users to add commands and flags for the container''s
    creation and execution; the functionalities can serve as an interface with more
    complex subsystems (i.e, compute and storage devices) at runtime. We extend Singularity/Apptainer
    to feature three functionalities needed to implement the designed fine-grained
    containerized environment in Sections 2.2, 2.3, and 2.4. First, we use the SIF
    format container to automatically create the individual application and data containers.
    We initialize each container with a metadata partition, providing users with access
    to information otherwise hidden to them. Second, we design a zero-copy data transfer
    mechanism for the Singularity/Apptainer''s technology which facilitates data movement
    across containers. This zero-copy functionality is now part of the Singularity/Apptainer
    code [31]. Last, we automatically annotate the workflow with provenance information.
    Both automatic creation and annotation functionalities are integrated in a plugin
    that is part of our software release [32], [33]. Furthermore, we implement a Jupyter
    notebook that serves as the user interface designed in Section 2.5. 3.2 Creating
    Application and Data Containers We augment the Singularity/Apptainer runtime with
    a plugin that supports the automatic creation and execution of fine-grained containerized
    workflows. The plugin can work with any workflow that can be modeled as a DAG.
    Given a workflow, our plugin has the ability to automatically create a fine-grained
    sequence of data and application containers using apptainer workflow –create.
    We use a web service on the local machine at port 5000 to facilitate the user
    with the generation of the fine-grained workflow. Through the web service, the
    user provides information about the workflow, such as the definition file of an
    application (i.e., application executable and software stack); the number, location,
    and size of each input data; and the expected size of the output data. The plugin
    uses this information to create the workflow with its individual application and
    data containers. Specifically, the plugin encapsulates individual datasets and
    application executables or scripts into independent file system partitions. For
    application containers, the plugin encapsulates the application executable or
    script together with the software system stack in a squashFS partition. For data
    containers, the plugin compresses the data in an Ext3 file system partition. Both
    types of containers include metadata in a JSON generic file system partition.
    3.3 Zero-copy Communication Between Containers We extend the Singularity/Apptainer
    technology to support direct transferring data between containers without going
    through host or external storage (i.e., zero-copy data transfer). We use the bind
    mount functionality to define a bind path that directly links a directory from
    the source container to a directory in the destination container. During an execution,
    the bind path is parsed, capturing which containers and directories to use. The
    source and destination containers are loaded in the environment. The source directory
    is replicated in the destination container; any change on the directory inside
    the destination container is also reflected in the source container and vice versa.
    3.4 Implementing Annotated Containers We define a second functionality in our
    plugin apptainer workflow –run [workflow_description].json that grants the user
    the ability to execute a fine-grained containerized workflow while also annotating
    containers with metadata. A user executes the fine-grained containerized workflows
    by running the command apptainer workflow –run [workflow_description].json. This
    command triggers the Singularity/Apptainer API callback clicallback that activates
    our plugin. Once the plugin is active, it starts the automatic collection of metadata
    in the workflow. To this end, for each application container''s execution, the
    plugin collects two pieces of information. First, it collects the application''s
    execution task settings (e.g., initial conditions, random seeds, and other setting
    values). Second, it collects the bind path that lists all input and output data
    containers for that application container. Following the data containers listed
    in a bind path, the plugin uses the SIF API to extract each container''s identification
    and creation time. The plugin appends that information to the record trail of
    output data containers for that bind path. Given an output container, its execution
    task and record trail are transformed into JSON format and added as a new file
    descriptor in its metadata partition. 3.5 Jupyter Notebook User Interface We develop
    a Jupyter notebook [30] that serves as a user interface for inspecting and gaining
    insights from the collected metadata. It allows the user to select the metadata
    of one or more containers and to backtrace the execution dataflow, which is represented
    through a directed graph. We use the package NetworkX expanding the open-source
    function NetworkX Viewer [34] with a customized node token class to tailor the
    interactive visualization of the workflow graph. Specifically, our interface enables
    users to (i) create the nodes based on the list of containers in the record trail
    where the attributes of each node follow the structure of the metadata; (ii) assign
    different colors to distinguish between data and application containers; (iii)
    build independent subgraphs based on the dataflow stated in the metadata of each
    container; (iv) merge the subgraphs to build larger graphs by using common patterns
    and unique components; (v) visualize the graphs in an interactive session where
    the user can reorganize it; and (vi) obtain detailed provenance information about
    any container. SECTION 4 Traceability and Explainability in a Real Use Case in
    Earth Science We demonstrate how our fine-grained containerized approach is used
    to study two cases on an earth science workflow, SOMOSPIE [22]. In the first case,
    the workflow has to be traced back to the input data to explain different levels
    of predictions’ resolutions (a case of missing data traceability). In the second
    case, the annotations are used to discover which different ML methods are the
    reason for different soil moisture predictions when using the same data (a case
    of missing result explainability). 4.1 A Data-driven Workflow for Soil Moisture
    Prediction Soil moisture, the percentage of water by weight or volume in soil,
    is critical for linking climate dynamics to ecosystem functioning, playing a key
    role in the Earth''s water and carbon cycles. The current availability of spatial
    soil moisture information across large areas (e.g., continents) comes from satellite-based
    remote sensing sources (e.g., ESA CCI [23], NASA SMAP [35]). There are however
    two major limitations of satellite-based soil moisture information: (i) large
    areas have spatial information gaps (e.g., where there is high canopy density,
    frozen soil, or extremely dry conditions); and (ii) they have coarse granularity
    (around 27×27 km grids). There is a pressing need to improve the spatial representation
    of soil moisture for applications in earth sciences (e.g., ecological niche modeling,
    carbon monitoring systems, and other Earth system models). Predictions can be
    used in policy making and precision agriculture (e.g., optimizing irrigation practices
    and other land management decisions). We use the open-source workflow for fine-grained
    soil moisture predictions called SOMOPIE [22]. The workflow fills missing spatial
    information and increases spatial resolution of the satellite information. The
    workflow consists of three steps: (i) satellite data and terrain parameters are
    input to a sequence of execution steps; (ii) ML methods transform the satellite
    data into higher resolution and gap-free predictions using K-Nearest Neighbors
    (KNN), Random Forest (RF), and Surrogate Based Modeling (SBM); and (iii) visualization
    methods rebuild predictions into formats suitable for further study. Fig. 4 shows
    an abstraction of the workflow. For scientists, the entire workflow execution
    can be opaque, preventing easy data traceability and results explainability. For
    example, different resolutions of the terrain parameters data (different input
    data) used for generating fine-grained predictions are not easy to trace from
    the output. Results obtained using different ML methods (different executables)
    are not easy to link to the specific method used. We show how our fine-grained
    containerized approach can be applied to the SOMOSPIE workflow to enable traceability
    and explainability in two cases. In the first case, the different input data fed
    into the workflow has to be traced back in order to explain the different levels
    of details in the predictions (a case of missing traceability). In the second
    case, different ML methods are the reason for different soil moisture predictions
    when using the same data (a case of missing explainability). Fig. 4. SOMOPIE''s
    modular workflow for predicting soil moisture composed of four modules (i.e.,
    data collation, preprocessing, modeling, and analysis). Show All 4.2 Integrating
    Traceability Fig. 5 shows an example of missing data traceability. SOMOSPIE is
    used with different input data resulting in different levels of details for the
    prediction of a region centered around Oklahoma (a rich agricultural area). The
    longitude is on the x -axis, the latitude is on the y -axis, and each of the pixels/coordinates
    represents a soil moisture (SM) value. Each one of the three figures represents
    the same area of Oklahoma where the longitude runs from -101.5 to -94.0 and the
    latitude runs from 33.5 to 37.0. The soil moisture ranges from 0.175 to 0.35 and
    is mapped into a color gradient where the lowest and driest SM value is red and
    the highest and most moisturized SM value is blue. Fig. 5. Example of soil moisture
    output visualization for Oklahoma predicted on three different resolutions (i.e.,
    1 km, 250 m, and 90 m) generated with SOMOPIE. Show All The terrain parameters
    are selected from different datasets with different resolutions: 1 km, 250 m,
    and 90 m. The ML method is fed with the input datasets and generates predictions
    from the satellite resolution (27 km) down to the terrain parameters resolution.
    However, the figures do not reveal the different resolutions of the input data
    to the scientists using the same workflow. Our approach annotates the workflow
    to capture the data provenance in the metadata, providing scientists with full
    transparency in the data transformations from input to output in the workflow.
    Fig. 6 shows the output of our interface for the predicted soil moisture values
    in Fig. 5. The interface is fed with the metadata of the containers. Based on
    the metadata, the interface builds and represents the workflow as a graph. The
    graph shows four data containers (nodes in blue), symbolizing the input data containers
    connected to an application container (first orange node). This application container
    corresponds to the ML method that generates the three data containers with the
    soil moisture predictions at a higher resolution. These predictions are then visualized
    in the second application container (second orange node), which are encapsulated
    in three independent output data containers (shown in Fig. 5). Based on the graph
    representation, the scientist can interact with any container and obtain its lineage.
    Fig. 6 presents the metadata partition of each of the containers executed in the
    workflow. This metadata partition includes the UUID (simplified in the figure),
    container name, creation time, execution task, and the record trail. The record
    trail shows the lineage of containers that were used to generate the current component.
    Starting from the three output data containers (09,10,11) the record trail shows
    in bold that the same visualization application (08) was executed on three independent
    predictions’ datasets (05, 06, 07). Based on the metadata of the intermediate
    data containers (05, 06, 07), the record trail reveals (in bold) that each of
    these predictions were the result of executing the KNN application (04) with the
    same training data (00) and three independent evaluation datasets with different
    resolutions (01,02,03). Finally, scientists can trace the three different outputs
    back to the data sources and explain how the observed differences come from the
    three input datasets, each with a different resolution. Furthermore, because our
    interface directly maps any difference to specific containers used during the
    execution, it makes it easier for the scientists to retrieve those containers
    and use them to reproduce the results or generate new studies. Fig. 6. Graph representation
    of the SOMOSPIE workflow executions for Oklahoma on three resolutions (i.e., 1
    km, 250 m, and 90 m) generated by the Jupyter notebook interface. It includes
    the metadata visualization for each containerized component. Show All 4.3 Integrating
    Explainability Fig. 7 shows an example of lack of result explainability for the
    same region. The figure shows three visualizations of the fine-grained soil moisture
    predictions with the same resolution of 250 m. The three predictions are generated
    using the same input data but different prediction methods (i.e., KNN, RF, SBM).
    The figures and associated fine-grained predictions do not reveal any information
    about the reasoning beyond the sharpness and high mixture of dry and moist values
    in Fig. 7a, the tile-like values in Fig. 7b, or the smoother transition between
    dry to moisturized values in Fig. 7c. Once again, our automatic annotation of
    the workflow generates metadata revealing the differences in the output data,
    providing the scientists with an explanation of the results in terms of the methodology
    used. Fig. 8 shows the output of our interface for the predicted soil moisture
    in Fig. 7. As in the previous case, the interface is fed with the metadata of
    the containers, and based on the metadata it builds the workflow as a graph. The
    output graph of the interface shows that there are three possible paths that start
    from two input data containers (both blue nodes). A fork in the execution occurs
    at the first stage of the workflow where there are three different application
    containers corresponding to the three different ML methods (first three orange
    nodes). Each of the ML methods generates an intermediate data container with the
    soil moisture predictions. Finally, the three predictions are visualized, generating
    three independent output data containers including the three visualizations (last
    three blue nodes). We also present the metadata partition for each of the containerized
    components. By backtracking the record trail, it is possible to reveal that the
    output containers (10,16,17) were generated by using the soil moisture predictions
    (06,14,15) from three different ML methods (04,12,13). By providing scientists
    with the workflow annotations automatically generated by our environment, we enable
    scientists to explain the different results by linking them to the ML methods
    used during downscaling. Furthermore, the application containers can be reused
    to generate new workflows without re-writing the application or re-installing
    the software stack. Fig. 7. Example of soil moisture output visualization for
    Oklahoma predicted with three different ML methods (i.e., KNN, RF, and SBM) generated
    with SOMOSPIE. Show All Fig. 8. Graph representation of the SOMOSPIE workflow
    executions for Oklahoma with three ML methods (KNN, RF, and SBM) generated by
    the Jupyter notebook interface. It includes the metadata visualization for each
    containerized component. Show All SECTION 5 Measuring Overheads and Performance
    We collect the performance measurements of our fine-grained containerized environment
    and compare them with both a native environment (without containerization) and
    a coarse-grained containerized environment (for which we containerize all the
    applications inside a single container and store the data on the native memory).
    The three environments are part of a broader set of environment configurations
    with our and the native settings at the two ends of the testing spectrum and the
    coarse-grained environment as a trade-off in between the other two. In other words,
    the coarse-grained environment (single container) serves as the link between the
    native (no containers) and the fine-grained (multiple independent containers)
    environments. 5.1 Experimentation Platform Settings We run a diverse set of tests
    for SOMOSPIE and collect execution time in seconds and storage space in MBs for
    the different environments. Furthermore, we measure the bandwidth in MB/s for
    different data read and write workloads. We run the tests on XSEDE Jetstream2
    [36] configured as a virtual machine with 16 cores with AMD Milan 7713 CPU type,
    60 GB of RAM (DIMM), and 60 GB local disk (HDD) space with an attached volume
    of 100 GB. In terms of software, the virtual machine has Ubuntu 20.04 as the OS,
    Apptainer 1.1, Go 1.19, and Python 3.8.10 using the next packages: numpy-1.23.2,
    pandas-1.4.3, scipy-1.9.0, and scikit-learn-1.1.2. 5.2 Execution Times We measure
    the execution times of all three environments (i.e., the native, the coarse-grained,
    and the fine-grained). Specifically, we measure the execution times of SOMOSPIE
    on Oklahoma with three resolutions: (a) 1 km, (b) 250 m, and (c) 90 m. We run
    each resolution 5 times using the three ML methods (i.e., KNN, RF, and SBM) for
    the different environments on top of Jetstream2. Fig. 9 shows the results. In
    the figure, we observe that the smaller the data and the more inexpensive the
    application is, the more time overhead is seen when deploying our fine-grained
    containerized environment compared to the native (77%) and the coarse-grained
    (53%) environments. As the data increases and the application becomes more complex
    and time consuming, the overhead drops to 10% compared to native and 1.5% compared
    to coarse-grained. Overhead is always expected given the extra layers of virtualization
    and the metadata management. Still, as applications are more complex and are deploying
    larger data, the observed overhead is an acceptable trade-off for the gained traceability
    of data and explainability of results. Fig. 9. Execution time of SOMOSPIE, running
    on Oklahoma with three resolutions: (a) 1 km, (b) 250 m, and (c) 90m, with three
    ML methods comparing the native (no containers), coarse (single container), and
    fine-grained containerized (multiple containers) environments. Show All 5.3 Storage
    Space We measure the storage space usage for the two environments at each end
    of our testing spectrum (i.e., native and our fine-grained). We do not present
    results for the coarse-grained containerized environment because we expect its
    storage space to match the storage of the native for data, and to match the fine-grained
    for applications. As we encapsulate the workflow components (data and applications)
    in containers, the containerization adds extra space for the encapsulation format.
    Depending on the type of containers, this extra space is allocated for different
    purposes. We distinguish between data and application containers and compare the
    storage space used in the native environment versus our fine-grained containerized
    environment. Data containers: In the native environment we have data, and in our
    fine-grained containerized environment we have data containers. A data container
    includes the data encapsulated in an Ext3 file system, metadata partition, and
    the container dependencies. We measure the size of all data containers in SOMOSPIE
    when executing the three ML methods for the different input data resolutions (1
    km, 250 m, 90 m). Fig. 10 presents the size comparison between the native environment
    and our environment for Oklahoma with two resolutions: (a) 1 km and (b) 250 m.
    On the x -axis we have the different data (data) and its containerization (c_data):
    input, c_input, intermediate (inter), c_inter, output, and c_output. Each is represented
    by a bar indicating their size in MBs, as stated on the y -axis. First, we observe
    that the container dependencies’ space (red bar) is constant. As the data increases
    in size, the container dependencies’ space becomes imperceptible. Second, for
    all containers, we observe that the space of the metadata partition (green bar)
    is negligible and always in the order of KBs. Last, we see that the extra space
    in the data container is mostly part of the Ext3 file system space (orange bar)
    and is used to encapsulate the data (blue bar) that has the same size in both
    native and containerized environments. The main cause of the large space used
    by the Ext3 file system is that it reserves space for the journaling information
    and 5% space for root processes. Ways of reducing the reserved space have been
    explored, and Ext4 [37] incorporates scalability and performance enhancements.
    However, Singularity/Apptainer chose Ext3 for their data containers as it is the
    most efficient and modifiable file system currently available on a wide variety
    of systems. Fig. 10. Comparison between the native and the fine-grained containerized
    environment for the data storage including the input, intermediate (inter), and
    output data of the earth science workflow running in Oklahoma with two resolutions:
    (a) 1 km, and (b) 250 m. Show All Application containers: In the native environment
    there are applications in the form of scripts or executables which require libraries
    and dependencies to run. In our fine-grained containerized environment we have
    application containers that include a script or executable, libraries, a metadata
    partition, and the container space which corresponds to the OS and software dependencies.
    Fig. 11 presents the comparison between native and fine-grained containerized
    applications in SOMOSPIE. On the x -axis, we have the three ML methods (application)
    and their containerization (c_application): KNN, c_KNN, RF, c_RF, SBM, c_SBM,
    and the visualization (visual) with its containerization (c_visual), all written
    in Python. On the y -axis we have the size in MBs. We observe these key properties:
    first, the scripts and libraries occupy the same space for the native and our
    containerized environment; second, as in the data containers, the metadata partition
    is negligible; and last, the extra space in the application container comes from
    the container space which includes the OS and software dependencies. The identification
    of libraries, software dependencies, and OS by the application container ensures
    replicability and transparency of the software system by guaranteeing that the
    user will always have the same versions of OS, libraries, and software dependencies,
    regardless of the platform on which the workflow is executed. Fig. 11. Comparison
    between the native and the fine-grained containerized environment for the applications
    storage in SOMOSPIE. Show All 5.4 IO Bandwidth We benchmark the IO bandwidth for
    the two environments at each end of our testing spectrum (i.e., native and fine-grained)
    using FIO, a Flexible IO tester. We do not present results for the coarse-grained
    because its bandwidth is similar to the native, as shown in [38]. In the native
    environment, we measure the bandwidth of the benchmark to read from and write
    to the virtual volume on Jetstream2. In the fine-grained containerized environment,
    we measure the bandwidth of the benchmark encapsulated in an application container
    to read from and write to a data container stored in the virtual volume. The raw
    size of the virtual volume is 100 GB. Because of the virtual volume file system
    overhead (Ext4), the virtual volume is 90 GB. The storage for the container adds
    additional overhead, thus the data container has 80 GB of usable capacity. FIO
    has the flexibility to select different IO settings including number of files,
    IO size, and sequential or random reads and writes. FIO spawns a number of files
    on a particular location doing a type of IO action. The location and the type
    of IO are specified by the user. We select four numbers of files (i.e., 1, 10,
    100, and 1000) and three IO sizes (i.e., 1 GB, 10 GB, and 80 GB) for each IO size.
    Depending on the IO size and number of files, each test has a different configuration.
    They are as follows: 1 file of size 1 GB, 10 GB, and 80 GB 10 files of size 0.1
    GB (100 MB), 1 GB, and 8 GB each 100 files of size 0.01 GB (10 MB), 0.1 GB (100
    MB), and 1 GB each 1000 files of size 0.001 GB (1 MB), 0.01 GB (10 MB), and 0.08
    GB (80 MB) each For all the listed tests, we set the block size equal to 4KB (the
    default block size for the Ext3 and Ext4 file systems). We select a sequential
    mix of read and write IO patterns, mimicking the IO pattern of SOMOSPIE. Fig.
    12 shows the results of the bandwidth for the three IO sizes: (a) 1 GB, (b) 10
    GB, and (c) 80 GB. The x -axis shows the number of files [1, 10, 100, 1000] and
    the y -axis shows the measured bandwidth in MB/s. For each number of files we
    measure the read and write IO 5 times for both environments. The measurements
    are represented in boxplots where the native measurements are in pink and the
    fine-grained containerized ones are in blue. We can extract two main observations
    from this figure. First, we observe that, as the number of files increases from
    1 file to 100 files, the bandwidth of the native environment also increases, ranging
    from 45 MB/s to 140 MB/s. This trend is observed for all IO sizes. For the larger
    number of files (i.e., 1000 files), the bandwidth drops for the native environment.
    The bandwidth is measured from the start of the metadata operation to the end
    of the data transfer. Thus, while the metadata operation time per file is fixed,
    the IO time grows proportionally with the data size. In other words, when the
    files are many and small, the metadata operation time significantly impacts the
    IO time. Second, we compare IO bandwidth when the data fits in the DRAM (in Figs.
    12a and 12b) versus when it does not (in Fig. 12c). In the figures, we observe
    that for 1 GB and 10 GB IO sizes our fine-grained containerized environment has
    higher IO performance than native. As we reach 80 GB in IO size, the performance
    for both environments are comparable. The higher performance is due to the fact
    that the data container fits in the DRAM, allowing faster data access. This is
    not the case for the larger IO size for which the container must interact with
    the virtual volume regularly, thus degrading performance back to the native implementation.
    When running this test we deal with two main constraints. First, the size of the
    DRAM (60 GB) is fixed and defined by the Jetstream2 virtual machine. Second, users
    cannot control how the DRAM is managed. It is the kernel that manages the loading
    of the containers’ content to the DRAM based on the availability of the resources.
    A modification of the kernel goes beyond the scope of our work. Fig. 12. Bandwidth
    comparison for the native (no containers) and the fine-grained containerized (multiple
    containers) environments for different file counts and sizes. Show All SECTION
    6 Interoperability, Heterogeneity, and Multiple Instances We discuss our environment''s
    interoperability with other workflows and with resource managers, as well as the
    adaptation of our environment in distributed heterogeneous systems and with multiple
    container instances. 6.1 Environment Interoperability Technology transfer, in
    which we envision the use of our environment for other applications, has been
    a key driving factor of our design. Our containerization approach requires that
    workflows are composed of one or multiple self-contained applications. Furthermore,
    users should be able to model the workflows as DAGs whose nodes (applications/tasks)
    and vertices (data in movement from one task to another) can be containerized.
    Any workflow with such features can be abstracted into a fine-grained set of interconnected
    containers, making our approach application-agnostic. We extend the Singularity/Apptainer
    runtime to support the concept of automatic creation and execution of fine-grained
    workflows that can be described as DAGs. We implement the Singularity/Apptainer
    plugin described in Section 3 which, given a workflow, has the ability to automatically
    create a fine-grained sequence of data and application containers, as well as
    the ability to execute these workflows while also annotating containers with execution
    metadata. The integration of our environment into resource managers and orchestrators
    is possible as long as they manage containerized executions. Such containerized
    executions are supported by HPC and cloud solutions such as Pegasus [39], REANA
    [10], Pachyderm [9], and Kubernetes [40]. Because data is containerized, the workflow
    manager does not have to deal with data transfer from-to local storage, adding
    additional portability for our containerized workflow across platforms. 6.2 Distributed
    Heterogeneous Resources Decomposing the workflow into fine-grained containers
    enables deploying different components on different nodes in a distributed system
    as long as the system shares storage (e.g., GPFS, object or block storage solutions).
    The bind mount across nodes can work through the shared storage connecting the
    nodes. For example, when using a cloud platform with object storage solution,
    an application container on a node can write to a data container on a different
    node; the content of the data container can be read by a second application container
    on the same node or a different node, using Kubernetes as the orchestrator of
    the containers’ execution. With the increase of GPU usage for ML-based scientific
    workflows, container technologies such as Docker, Singularity/Apptainer, and Podman
    support the execution of applications in the scientific workflows on GPUs. Specifically,
    for our containerized environment, Singularity/Apptainer supports running application
    containers that use NVIDIA''s CUDA GPU compute framework or AMD''s ROCm solution.
    Regardless of the operative system on the host machine, users can run GPU-enabled
    ML frameworks (e.g., Tensorflow, MXNet, PyTorch). Regarding the data generated
    by the GPU, we assume that this data is copied back to the CPU, given that GPU-direct
    is not a widely available technology in most HPC and cloud platforms. When data
    is copied back to the CPU, our fine-grained containerized environment encapsulates
    it in a data container and automatically collects the data lineage. 6.3 Metadata
    from Multiple Container Instances Our design supports multiple container instances,
    in which given an application container, we execute n instances of its image as
    a service. The application container image has a universally unique identifier
    (UUID). The n application instances inherit the same UUID, and through the UUID
    they link to the metadata of the image. When running multiple instances of the
    same application image, the metadata of the application image is not impacted
    but the metadata of the output data containers is. Such an impact changes based
    on where the multiple application instances write to. The n instances can write
    to their own output data container(s) or to a single, shared container(s). In
    the first case, the n application instances are writing to their own output data
    container(s) (i.e., each application instance generates one output data container).
    For each output data container, the container''s identification, creation time,
    and execution task are unique to the output data container. The record trail is
    the same across output data containers and links back to the UUID of the application
    image. In the second case, the n application instances are writing to a shared
    output data container(s) (i.e., the application instances generate one single
    output data container). For the shared output data container, the container''s
    identification and creation time are unique to the output data container. The
    execution task is a list that contains n execution tasks including initial conditions,
    random seeds, and other setting values of the n instances. The single record trail
    links back to the UUID of the application image. SECTION 7 Related Work Annotating
    a workflow execution with the provenance of its components has been previously
    used for traceability, reproducibility, and explaining results. Related work for
    collecting and preserving the provenance of a workflow at the system level include
    developing custom file systems tracking provenance such as the Lineage File System
    (LinFS) [11], PASTA in PASS (Provenance-Aware Storage Systems) [8], and Parrot[12];
    encapsulating workflows through ad hoc packages such as CDE [13], ReproZip [14],
    Umbrella [15], and Occam [16][17]; and encapsulating workflows through existing
    container technologies such as Pachyderm [9], REANA [10], and Science Capsule
    [41]. The use of custom file systems and custom ad hoc packages limits the portability
    and usability of their solutions across systems. The use of existing container
    technology to encapsulate the workflows overcomes this challenge, offering portable
    solutions. Contrary to existing containerization solutions, our approach decouples
    workflows into components (data and applications) at a finer level and maps the
    components to one-to-one single and independent containers. Containerizing data
    facilitates transportation, interpretation, and use. We adapt the premise of the
    data containerization from Data Pallets [29]. It defines storage as a new container
    type when running workflows, where the containers include the data and links to
    the application and the input deck. Even when data is containerized, intermediate
    data is treated as disposable for solutions like Prune [42], CDE [13], ReproZip
    [14], Umbrella [15], and Occam [16][17] where they focus on sharing final results
    of the workflow executions. Only Pachyderm provides a complete audit trail for
    all data across pipeline stages, including intermediate results. As with Pachyderm,
    our solution grants first class citizen access to the intermediate data and its
    metadata. Moreover, we are the first to permanently and portably attach the provenance
    invisibly to the data and the applications. We achieve this through the use of
    a second partition in the container structure. Finally, there has been a significant
    amount of work in workflow management that targets provenance. Workflow management
    systems like Pegasus [39], Kepler [43] and DAGMan [44] provide a way to orchestrate
    workflows while capturing the data provenance. Only Pegasus provides application
    containers as a solution to package software with complex dependencies. Pegasus
    currently supports Docker, Singularity/Apptainer, and Shifter. However, the workflow
    managers use the scientific workflow system to track and store the computational
    steps and their data dependencies, but information about the environment is rarely
    gathered. Furthermore, integrating a workflow to the management systems can be
    complex. It requires the translation of the workflow into the right format: DAX
    (Directed Acyclic Graph in XML) for Pegasus, XML or KAR files for Kepler, and
    DAG input file for DAGMan, for example. Finally, not all workflows are in the
    stage of managing their pipelines with these workflow tools, which makes a case
    for alternative solutions for hosting workflows and capturing provenance to allow
    traceability of data and explainability of results. Our work is intended to work
    together with these systems, using their support for containerized workflows to
    handle the workflow management tasks while we manage the automatic provenance
    collection, enabling traceability and explainability and annotation of data containers.
    SECTION 8 Conclusion In this paper, we present a fine-grained containerized environment
    using Singularity/Apptainer technology that enables scientists to achieve trust
    in findings from their workflows by seamlessly providing data traceability and
    results explainability. We demonstrate the benefits of our environment for SOMOSPIE,
    an earth science workflow that uses ML-methods to predict satellite soil moisture
    data to a resolution necessary for policy making and precision agriculture. Specifically,
    we use our environment for two use cases in which we trace back differences in
    predictions due to input data and ML-methods. When compared with native and coarse-grained
    containerized environments, we observe that our environment has limited overhead
    in terms of time (10%), storage space (5% for data containers and 30% application
    container), and it has significantly higher IO bandwidth, with a peak of 400 MB/s
    versus 50 MB/s for native. Our solution is effective for establishing trustworthiness
    in scientific findings. Future work includes the automatic orchestration of the
    workflow in our containerized environment and the creation of a catalogue of containers
    that scientists can extend, share, and use to build new scientific workflows in
    multiple domains. Code Availability The code implementing the augmented Singularity/Apptainer
    can be found at: https://github.com/TauferLab/ContainerizedEnv. ACKNOWLEDGMENTS
    The authors acknowledge the Singularity/Apptainer team, specially Cedric Clerget
    and Ian Kaneshiro, for their support. Authors Figures References Citations Keywords
    Metrics More Like This Cryptomining Detection in Container Clouds Using System
    Calls and Explainable Machine Learning IEEE Transactions on Parallel and Distributed
    Systems Published: 2021 Estimating Soil Moisture Over Winter Wheat Fields During
    Growing Season Using Machine-Learning Methods IEEE Journal of Selected Topics
    in Applied Earth Observations and Remote Sensing Published: 2021 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Parallel and Distributed Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Building Trust in Earth Science Findings through Data Traceability and Results
    Explainability
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Verma P.
  - Maurya A.K.
  - Yadav R.S.
  citation_count: '1'
  description: The advancements in computing and storage capabilities of machines
    and their fusion with new technologies like the Internet of Thing (IoT), 5G networks,
    and artificial intelligence, to name a few, has resulted in a paradigm shift in
    the way computing is done in a cloud environment. In addition, the ever-increasing
    user demand for cloud services and resources has resulted in cloud service providers
    (CSPs) expanding the scale of their data center facilities. This has increased
    energy consumption leading to more carbon dioxide emission levels. Hence, it becomes
    all the more important to design scheduling algorithms that optimize the use of
    cloud resources with minimum energy consumption. This paper surveys state-of-the-art
    algorithms for scheduling workflow tasks to cloud resources with a focus on reducing
    energy consumption. For this, we categorize different workflow scheduling algorithms
    based on the scheduling approaches used and provide an analytical discussion of
    the algorithms covered in the paper. Further, we provide a detailed classification
    of different energy-efficient strategies used by CSPs for energy saving in data
    centers. Finally, we describe some of the popular real-world workflow applications
    as well as highlight important emerging trends and open issues in cloud computing
    for future research directions.
  doi: 10.1002/spe.3292
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Software: Practice and Experience SURVEY ARTICLE Full Access A survey
    on energy-efficient workflow scheduling algorithms in cloud computing Prateek
    Verma,  Ashish Kumar Maurya,  Rama Shankar Yadav First published: 03 December
    2023 https://doi.org/10.1002/spe.3292Citations: 2 SECTIONS PDF TOOLS SHARE Abstract
    The advancements in computing and storage capabilities of machines and their fusion
    with new technologies like the Internet of Thing (IoT), 5G networks, and artificial
    intelligence, to name a few, has resulted in a paradigm shift in the way computing
    is done in a cloud environment. In addition, the ever-increasing user demand for
    cloud services and resources has resulted in cloud service providers (CSPs) expanding
    the scale of their data center facilities. This has increased energy consumption
    leading to more carbon dioxide emission levels. Hence, it becomes all the more
    important to design scheduling algorithms that optimize the use of cloud resources
    with minimum energy consumption. This paper surveys state-of-the-art algorithms
    for scheduling workflow tasks to cloud resources with a focus on reducing energy
    consumption. For this, we categorize different workflow scheduling algorithms
    based on the scheduling approaches used and provide an analytical discussion of
    the algorithms covered in the paper. Further, we provide a detailed classification
    of different energy-efficient strategies used by CSPs for energy saving in data
    centers. Finally, we describe some of the popular real-world workflow applications
    as well as highlight important emerging trends and open issues in cloud computing
    for future research directions. Abbreviations CSP cloud service provider DAG directed
    acyclic graph DC data center DNA deoxyribonucleic acid DVFS dynamic voltage frequency
    scaling EFT estimated finish time IaaS infrastructure-as-a-service IoT internet
    of things IRSA infrared science archive PaaS platform-as-a-service PMs physical
    machines QoS quality of service SaaS software-as-service SLA service level agreement
    USC University of Southern California VMs virtual machines VSL voltage supply
    level 1 INTRODUCTION 1.1 Cloud computing and its evolution The term “cloud computing”
    has been popularly defined by the U.S. National Institute of Standards and Technology
    (NIST)1 as “a model that enables ubiquitous, convenient, on-demand network access
    to a shared pool of configurable computing resources, for example, networks, servers,
    storage, applications, and services.” To put it in simple words, cloud computing
    provides remote users an online platform for accessing computing services like
    databases, servers, software services, and analytics through the Internet (the
    cloud) on a “pay-as-you-go pricing” scheme without the need for actual buying
    and installing these high-end IT infrastructure resources at their workplaces.
    Thus, cloud computing provides users with a cost-effective, efficient, reliable,
    and secure platform for business activities. Cloud computing has been in the Information
    and Communication Technology domain for the past few decades, and it has not just
    revolutionized the way in which the Information Technology industry works but
    also the functioning of different industrial organizations, government institutes,
    and academia. Its emergence and popularity can be determined from the fact that
    as of 2022, “worldwide public cloud spending will grow 20.7% to total $591.8 billion,
    up from $490.3 billion in 2022”.2 Cloud computing has helped shape the modern
    economy by providing a novel, simplified, cost-effective way to access on-demand
    subscription-based IT resources to geographically distributed customers. This
    has resulted in the accelerated growth of new businesses along with older ones
    and to quickly adapt their business model according to the dynamic and competitive
    environment. Some of the major players providing cloud computing services are
    Amazon web services (AWS),3 Microsoft Azure,4 Google Cloud platform5 and Alibaba
    Cloud.6 1.2 Workflow scheduling in cloud computing In the past few decades, workflow
    scheduling in cloud computing has gained widespread interest in industry and academia.
    But before discussing workflow scheduling in cloud computing, two questions need
    to be answered first. 1.2.1 What is scheduling in cloud computing? In general,
    the term ‘scheduling’ in computing means strategies or approaches used by schedulers
    to allocate resources to tasks to optimize one or more objectives of the participants.
    Figure 1 shows the relationship between the user, scheduler, resources, and CSPs.
    FIGURE 1 Open in figure viewer PowerPoint Interaction between different entities
    in cloud computing environment. In cloud computing, key terms used in the above
    scheduling definition can be detailed as follows. Resource: A resource in cloud
    computing can be categorized into two types: (i) Hardware resources consisting
    of processors, network links, and storage devices and (ii) Software resources:
    virtual machines (VMs)7 and containers.8 Tasks: Tasks generally refers to the
    incoming user requests and data to the cloud service providers (CSPs) based upon
    the type of cloud services (Software-as-Service, Platform-as-a-Service and Infrastructure-as-a-Service)9
    subscribed by the user. Scheduler: A scheduler in the cloud computing environment
    is a software process that allocates user requests to the available pool of cloud
    resources using scheduling approaches in a manner such that service level agreement
    (SLA)10 or policy between user and CSP is not violated, and QoS parameters (availability,
    reliability, scalability, and security) are fulfilled. Participants: Generally,
    two participants play a major role in the cloud computing environment: user or
    customer and CSPs. Users consist of individuals or organizations that use cloud
    services provided by CSPs through cloud interfaces over the Internet. Objective:
    Defining objective is a subjective issue concerning the participants in the cloud
    computing environment. For instance, from the user''s point of view, its objectives
    with respect to cloud services are lower costs, ease of access to services, scalability,
    flexibility, data security, and privacy. Whereas, from the CSPs point of view,
    its objectives are increased throughput, low latency in service delivery, optimal
    resource utilization, and pricing model. Strategies or approaches: Strategies
    refer to methods, techniques, and algorithms used to achieve the objectives of
    the participants. At the highest level, scheduling approaches can be classified
    into local and global scheduling.11 In local scheduling, processes are assigned
    to the time slices of a single processor, whereas in global scheduling, processes
    are assigned to multi-processors in a distributed environment. In general, the
    problem of scheduling tasks in cloud computing is considered an NP-hard problem,12
    so finding an optimal scheduling approach or algorithm is an uphill task for the
    researchers. Based on the type of user requests, task scheduling is classified
    into two broad categories: Bag-of-tasks scheduling and workflow scheduling.13
    Bag-of-tasks scheduling: In bag-of-tasks scheduling, user requests or applications
    are submitted to the cloud interface in the form of ‘n’ independent tasks is to
    be scheduled on ‘m’ VMs: running on physical machines in such a way that optimal
    utilization of resources is possible. Workflow scheduling: In workflow scheduling,
    user requests or applications are modeled as directed acyclic graphs (DAGs) represented
    as G(T, E), where T is a set of ‘n’ inter-dependent tasks with task dependencies
    represented by a set E consisting of e= directed edges shown as . 1.2.2 Need of
    energy efficient workflow scheduling in cloud computing In the past few decades,
    many workflow scheduling algorithms in cloud computing have been proposed in the
    literature. Some primary objectives are minimizing makespan, reducing monetary
    cost and response time, and increasing resource utilization. However recently,
    researchers have focused on designing workflow scheduling algorithms that incorporate
    techniques intending to optimize energy consumption. This shift in focus can be
    attributed to two major factors discussed below. Energy demands: With the ever-increasing
    demand for computing resources and the current enhancement in network technologies
    using 5G, it is inevitable that more and more user businesses and organizations
    will shift their business activities to the cloud. Further, emerging services
    and technologies like cloud gaming, digital currencies, virtual reality, blockchain,
    and artificial intelligence are poised to increase the demand for data services
    thereby increasing the requirements for cloud computing resources for data processing
    online. This increased computation demand is bound to increase the energy consumption
    demands at the data center facilities spread worldwide. This can be understood
    from the fact that in 2021,14 the global data center power consumption was around
    0.9%–1.3% of the electricity demand worldwide and will continue to grow at a rate
    of 10%–30% annually. An average data center facility can consume enough electricity,
    which can be used to power thousands of domestic households. From Figure 2, it
    is clearly visible that cloud computing data centers consume more electricity
    than the electricity demands of the entire country.15 In addition to this, growing
    energy demands have a direct implication for global carbon dioxide emissions as
    “data centers and data transmission networks are responsible for nearly 1% of
    energy-related greenhouse gas emissions (GHG)”.14 Monetary cost: As the energy
    demands and energy costs increase, the operational cost of running data centers
    by CSPs has also increased many folds. This can be visible from the fact that
    while in the year 2022, the total expenditure on cloud infrastructure services
    and data centers was roughly 90 billion USD, it is expected to reach 130 billion
    USD in the year 2026,16 clearly show an increase by 44.44% in total expenditure.
    While this increase in total expenditure may not be solely due to an increase
    in data center power consumption and electricity costs, these factors play a major
    part in contributing to total expenditures and monetary costs incurred by the
    CSPs. FIGURE 2 Open in figure viewer PowerPoint Annual domestic energy consumption
    (TwH)-2021. From the above discussion, it is clear that with the growing energy
    demands and increased monetary cost of running data centers, it is imperative
    to design energy-efficient approaches for scheduling workflows in a cloud environment.
    1.3 Related surveys A lot of research related to scheduling in cloud environments
    is in a growing phase, and some of the research challenges, like VM migration,26
    VM load balancing,27 resource provisioning,28 energy efficiency, system reliability,
    and so forth, still need to be explored. While most survey papers cover different
    state-of-the-art task scheduling algorithms in cloud computing, very few survey
    papers cover specifically energy-efficient workflow scheduling algorithms. Nevertheless,
    here, we discuss some survey papers related to task scheduling algorithms with,
    later at the end, some relevant papers on workflow scheduling. The authors in
    Reference 17 have surveyed different workflow scheduling algorithms by classifying
    them based on three major categories: application-model-based, scheduling-model-based,
    and resource-model-based. Moreover, various characteristics of existing state-of-the-art
    workflow scheduling algorithms are analyzed by studying their resource provisioning
    methods, scheduling objectives, and optimization strategies. In Reference 18,
    the authors highlight the importance of energy-efficient workflow scheduling in
    cloud computing and discuss different dynamic power management methods used in
    scheduling algorithms. The paper further classifies and discusses different energy-efficient
    scheduling algorithms under two categories: heuristic and meta-heuristic algorithms
    as well as highlights the research trends based on regions, journals, and conferences
    through graphical representation. The authors in Reference 19 have discussed different
    workflow scheduling techniques and algorithms used in the cloud environment. The
    research paper discusses the merits and demerits of various static, dynamic, and
    meta-heuristic-based workflow scheduling algorithms while considering three cloud
    models: traditional, serverless, and Fog-cloud.29 The authors have presented a
    workflow management system architecture that manages and schedules workflows on
    the servers and provides mathematically formulated objective functions associated
    with a scheduling algorithm. In Reference 20, the authors have provided a taxonomy
    of various resource provisioning and scheduling algorithms available in cloud
    computing. Resource provisioning is defined as a technique to provision virtualized
    resources like VMs for allocation to users whereas resource scheduling is defined
    as a way to select optimal VMs among the available VMs using different scheduling
    algorithms for the execution of user tasks. In addition, resource scheduling ensures
    that user-client SLA violations are in check and QoS parameters are met. The research
    paper provides a classification along with the merits and demerits of various
    resource provisioning algorithms based upon their static or dynamic resource provisioning
    strategy, whether offline or online, preemptive or non-preemptive. In addition,
    it classifies and reviews the resource scheduling algorithms based on heuristic,
    meta-heuristic, and hybrid techniques. The authors in Reference 21 have surveyed
    and categorized existing state-of-the-art task scheduling algorithms based on
    scheduling techniques used in heuristic-based, meta-heuristic based and machine
    learning-based scheduling algorithms. The paper highlights the merits and demerits
    along with the objective functions of the different task scheduling algorithms.
    In Reference 22, the authors have focused on surveying and comparing only multi-objective
    meta-heuristic task and workflow scheduling algorithms in a cloud computing environment.
    The paper categorizes the scheduling algorithms based on different meta-heuristic
    techniques like swarm intelligence algorithms (ant colony optimization,30 Artificial
    bee colony optimization,31 cat swarm optimization,32 etc.), genetic algorithm33
    and NSGA-II34 scheduling algorithm. In Reference 23, various existing traditional,
    heuristic, meta-heuristic, and hybrid task scheduling algorithms are surveyed
    and analyzed with special emphasis on meta-heuristic algorithms. The paper provides
    a detailed categorization of various meta-heuristic algorithms covering different
    dimensions of scheduling like (i) the nature of scheduling (single or multi-objective),
    (ii) the primary objective of scheduling (makespan, throughput, monetary cost,
    energy consumption, etc.), (iii) task-resource mapping scheme (static, dynamic,
    prediction based, etc.), (iv) scheduling constraints (task deadline, priority,
    budget, etc.). Further, the pros and cons of each state-of-the-art meta-heuristic
    algorithm were outlined, along with the comparison of various popular simulation
    tools used by scheduling algorithms to simulate and analyze the results. The authors
    in Reference 24 have reviewed different heuristic-based, meta-heuristic based,
    and other task scheduling algorithms that consider minimization of energy consumption
    as one of their objectives while scheduling tasks. The paper discusses the merits
    and demerits of different task scheduling algorithms and provides a comparative
    analysis based on QoS parameters and constraints. In Reference 25, the authors
    have provided the architecture of an energy-aware cloud workflow management system
    that enables users to model workflows and execute them in the cloud environment.
    It provides a framework whereby first, the users submit their applications through
    a graphical user interface which provides user support for modifying, submitting,
    and tracking these applications. Secondly, the workflow engine with its components
    like scheduler, workflow parser, task dispatcher, resource scheduler, and data
    control are responsible for breaking down the application into workflow consisting
    of interdependent tasks and finally scheduling these workflow tasks to be completed
    as per agreed SLA and QoS parameters. Although the paper highlights different
    workflow scheduling techniques used in algorithms, their advantages and disadvantages,
    it falls short in providing any detailed classification of workflow scheduling
    algorithms based upon well-known approaches. In addition, the paper doesn''t provide
    any information about the primary objective(s), constraints, and simulation tools
    used by the scheduling algorithms. From the above literature review, which is
    summarized in Table 1, it is clear that most of the existing survey papers provide
    basic concepts of different traditional, heuristic-based, meta-heuristic based,
    and hybrid algorithms for task scheduling. Although, some survey papers discuss
    and provide a classification of workflow scheduling algorithms; very few papers
    discuss workflow scheduling algorithms with a focus on energy consumption as one
    of their objective(s). Figure 3 shows the distribution of papers surveyed by year.
    FIGURE 3 Open in figure viewer PowerPoint The distribution of papers surveyed
    by year. TABLE 1. Comparison with existing surveys. Work Year Nature of tasks
    Key contribution Energy-aware approaches Emerging trends Research challenges Future
    directions Workflow applications Taxonomy 17 2016 Workflow The paper surveys different
    workflow scheduling algorithms under three proposed classifications namely, Application
    model-based (types of workflow used), Scheduling model-based, and Resource model-based
    classification 18 2019 Workflow and independent tasks This paper discusses different
    dynamic power management methods for reducing energy consumption as well as classifies
    different scheduling algorithms under heuristic and meta-heuristic algorithms
    19 2019 Workflow Provides a workflow management system design determining the
    execution flow of workflow applications along with a discussion of various static,
    dynamic, and meta-heuristic-based workflow scheduling strategies 20 2019 Workflow
    and independent tasks The paper studies and analyzes different scheduling algorithms
    under three categories: heuristic, meta-heuristic and hybrid scheduling approaches,
    along with their strengths and weaknesses 21 2019 Workflow and independent tasks
    The paper categorizes and discusses scheduling algorithms based on different scheduling
    techniques like swarm intelligence, evolutionary genetics, fuzzy-based scheduling,
    etc. 22 2020 Workflow and independent tasks The paper describes various multi-objective
    optimization algorithms and further discusses different multi-objective scheduling
    algorithms under different meta-heuristic approaches. 23 2021 Workflow and independent
    tasks Provides detailed classification of different meta-heuristic approaches
    used in scheduling algorithms along with their weaknesses and strengths; different
    simulation tools are discussed, and their comparative analysis is done. 24 2021
    Workflow and independent tasks The paper discusses different scheduling algorithms
    with energy optimization as one of their objectives and categorizes them into
    heuristic, meta-heuristic, and other scheduling approaches 25 2022 Workflow Surveys
    existing energy-efficient techniques used in scheduling algorithms along with
    their strengths and weaknesses Our work 2023 Workflow The paper discusses different
    scheduling algorithms with energy optimization as one of their objectives and
    classifies them into heuristic, meta-heuristic, artificial intelligence-based,
    and hybrid scheduling approaches; strengths and weaknesses of the algorithms are
    discussed, followed by an analytical discussion of the algorithms covered 1.4
    Objective of the survey In order to develop an effective scheduling algorithm,
    we need to clearly understand the different scheduling approaches and methods
    used by the state-of-the-art algorithms along with their strengths and weaknesses.
    Thus, the objective of this survey is to provide a detailed survey and analysis
    of state-of-the-art research focusing on energy-efficient workflow scheduling
    in cloud computing. In particular, the objective of this article is four-fold.
    To provide the readers with a clear view and understanding of various existing
    energy-efficient workflow scheduling algorithms by classifying them into four
    scheduling approaches such as heuristic, meta-heuristic, hybrid, and AI/ML-based
    approaches. In addition, we highlight the pros and cons of each scheduling algorithm
    as well as the simulation tools and standard input graphs used by the algorithms.
    To asses various scheduling algorithms by examining the diverse techniques employed
    in different stages of the algorithm. For instance, heuristic-based algorithms
    are analyzed based on the task priority and task-VM mapping phase of the algorithm.
    Similarly, meta-heuristic algorithms are analyzed based on the search space of
    the solution, exploitation, and exploration phase of the algorithm. To identify
    and discuss major workflow applications used for evaluating the performance of
    the scheduling algorithms in cloud computing. These include bio-informatics, astronomy,
    seismology, weather forecasting, and numerical analysis. To investigate various
    emerging technologies in the realm of cloud computing where scheduling problem
    is a primary concern. Further, we highlight some of the open challenges for scheduling
    of tasks in cloud computing that need attention and can be explored for future
    research directions. The rest of the article is organized as follows. In Section
    8, we present the basic models and provide a brief classification of energy-efficient
    strategies used by CSPs at the data centers. Further, we classify the energy-efficient
    workflow scheduling algorithms into four different categories and provide an analytical
    perspective on the scheduling algorithms in Sections 14 and 19. Next, Section
    26 describes different workflow graphs that are frequently used by the researchers
    as standard input graphs in their scheduling algorithms. The readers are further
    acquainted with the latest emerging trends, research challenges, and future directions
    for energy-efficient workflow scheduling in Section 40. Finally, we wrap our work
    with a conclusion in Section 52. In addition, to help the readers get a clear
    view of the structure of this survey, Figure 4 outlines the organization of the
    survey. FIGURE 4 Open in figure viewer PowerPoint Structure of the survey. 2 BASIC
    MODELS AND ENERGY-EFFICIENT STRATEGIES Before, going into details of different
    workflow scheduling approaches used in cloud computing, here we discuss some of
    the basic fundamental models that are generally considered while designing energy-efficient
    workflow scheduling algorithms. For this, initially, we define the basic system
    model for cloud services followed by describing the workflow model consisting
    of inter-dependent tasks. Next, we define the energy model that uses the Dynamic
    voltage and frequency scaling (DVFS) technique for efficient power consumption.
    This is followed by, highlighting different energy-efficient strategies that are
    used by CSPs at the data centers. 2.1 Basic models In this section, we describe
    some of the basic models like the system model, workflow model, and energy model
    that are generally used by different workflow scheduling algorithms for scheduling
    tasks in a workflow. 2.1.1 System model Generally, a system model for cloud services
    consists of various cloud service providers having multiple data centers (DC)
    spread across different geographical locations. This helps to reduce data latency,
    and smooth data sharing, and execution of services between users and cloud servers.
    As discussed earlier, the major components constituting a DC are racks, HVAC,
    switching devices, VMs, and PMs. A rack constitutes several PMs interconnected
    by networking devices and cooling system devices. In addition, using the virtualization
    technique, each physical machine hosts multiple VMs. 2.1.2 Workflow model Traditionally,
    a workflow is represented in the form of a directed acyclic graph (DAG): G (T,
    E), as shown in Figure 5. Here T is the set of tasks represented as such that
    vertices and E is the set of directed edges and shown as representing inter-task
    dependencies among tasks. In the sample workflow graph, each vertex is a task
    representing an individual application task with a certain workload (set of instructions
    to be executed on the same processor) and calculated in million instructions (MI),
    as a unit of measurement. In addition, each task is associated with its estimated
    computation cost on a processor denoted by . FIGURE 5 Open in figure viewer PowerPoint
    Sample workflow. Further, each edge shows a precedence constraint between task
    , which means task has higher precedence over task and hence should be scheduled
    earlier than . For the edge , task is called the parent task, and task is called
    the child task. If task needs to transmit data to task , then can start execution
    only after receiving all the data from . Each edge is also associated with a weight
    denoting communication cost, which is the time taken to transfer the data from
    task to . From the graph, it can be seen that if a task has no parent task, then
    it is called an entry task (here task ), whereas a task with no child task is
    called an exit task (here task ). Further, if a DAG has more than one entry and
    exit task, then a dummy entry task is added at the start and similarly, a dummy
    exit task at the end is added to the DAG to convert it into a DAG with only one
    entry and exit task.35, 36 2.1.3 Energy model Modern DCs use a combination of
    hardware and software techniques to manage the power consumption of the system.
    Dynamic voltage and frequency scaling (DVFS) is a technique whereby the clock
    frequency and voltage of the DVFS-enabled processor in servers are varied to conserve
    power. This can be verified from the fact that switching power consumption in
    a processor is proportional to the clock frequency and the square of the voltage
    as shown in Equation (2). Thus, lowering the clock frequency of the processor
    results in a proportionate reduction in power consumption. Further, the power
    consumption in Complementary Metal Oxide Semiconductor (CMOS) based logic circuits
    used in DCs is based upon two factors: static power and active power.37 Static
    power ( ) is the power used to keep the basic circuit running including the system
    clock and the memory in sleep modes which can only be removed by shutting down
    the whole system.38 Static power is generally constant and is given by Equation
    (1): (1) Where, V is the voltage supplied and I is the current supplied to the
    system. The active power is further divided into two parts: frequency-independent
    active power ( ) and frequency-dependent active power ( ). consists of components
    of memory and processor power that are independent of the supplied voltages and
    frequencies which can effectively be eliminated by putting them into sleep mode.
    On the other hand, denotes the frequency-dependent active power or dynamic power
    which is mainly consumed by the processor and is the dominant factor in optimizing
    the power consumption of the system. It is quadratically related to the voltage
    supplied and linearly related to the frequency37 and is given by Equation (2):
    (2) Where, is the effective capacitance, V is the voltage supplied and f is the
    clock frequency. Thus, the overall power consumption of the system39 is given
    by Equation (3): (3) Where h denotes the state of the system. When h = 1, the
    system is in the active state, and when h = 0, the system is in an inactive state.
    Considering the huge overhead of turning the system off/on after the computation
    task, the system is generally kept ‘on’ resulting in constant consumption of static
    power thus leaving only the active power of the system to be managed. The operating
    system software at the DCs monitors the workload and optimizes the dynamic power
    component, as shown in Equation (3), by determining the proper voltage and clock
    speed settings of the DVFS-enabled processors to minimize the energy consumed
    by the system. 2.2 Energy-efficient strategies used in data centers According
    to Reference 2, the worldwide end-user spending on public cloud services is poised
    to grow at a fast rate and is estimated to reach 591.8 billion USD in the year
    2023 with SaaS market share contributing 195.2 billion USD, PaaS market share
    contributing 136.4 billion USD and IaaS market share contributing 150.2 billion
    USD. To fulfill this, cloud providers need to either optimize or expand their
    cloud infrastructure (specifically, cloud data centers) consisting of hardware
    and software components to cater to the ever-increasing demand for cloud services.
    A typical cloud data center facility is spread over 300–4500 square meters of
    area consisting of core hardware components like servers, storage devices, routers,
    switches and firewalls, and software components like virtualization of software
    and applications and resource management controllers. Moreover, these core components
    require supporting sub-systems for smooth and efficient functioning. These sub-systems
    include ventilation, cooling systems, power generation, power backup systems,
    and fire and security control systems. With such huge data center facilities,
    it''s natural that their energy consumption demands are enormous. Therefore, it
    becomes imperative to design and develop management strategies to minimize energy
    consumption usage in data centers without compromising their QoS. Modern DCs use
    a combination of hardware and software techniques to manage the system''s power
    consumption. In a typical data center,40 the main components contributing to power
    consumption are servers, storage and network devices, cooling systems, and power
    conversion units which can be seen in Figure 6. Among these components, energy
    consumed by servers in processing the data contributes to a major portion of the
    overall energy consumption of DCs. In Figure 7, we have classified data center
    energy efficient strategies into four categories based on their geographical location,
    physical infrastructure, hardware, and software strategies. Data center geographical
    location and design: One of the crucial decisions before installing a data center
    is the choice of its location by the cloud provider. Proper consultancy and research
    need to be done by the cloud provider to determine the optimal location based
    on the temperature and climate conditions of the region, electricity costs, and
    user workloads in the region before installing a DC, as the mentioned factors
    have an important role to play in overall electricity consumption of the DC. In
    addition to this, the use of a server consolidation strategy to consolidate workloads
    onto fewer servers and employing energy-efficient data center design layout along
    with proper airflow management prevents further wastage of energy. Data center
    physical infrastructure: With increasing user service requests, huge amounts of
    data need to be processed and stored in the DC. This results in excessive heat
    generation at the DC premises. Hence, it becomes crucial to employ efficient heat
    dissipation strategies and optimize the use of physical resources so as to run
    the DC at its optimal level. For this, steps need to be taken to install energy-efficient
    Heating, ventilation, and air-conditioning (HVAC) and optimize power distribution
    units (PDUs) to reduce energy consumption and carbon footprints. Hardware strategies:
    DC hardware includes servers, processors, networking devices, and storage devices.
    One of the ways to minimize energy consumption at DC is by incorporating high-performance
    multi-core processor architecture for fast and efficient processing. Further,
    by using an optimal combination of solid-state drive (SSD) and hard disk drive
    (HDD) and by the efficient placement of networking devices (like switches, routers,
    and firewalls), energy bills can be reduced. Software strategies: It consists
    of software strategies consisting of software technologies and scheduling algorithms
    for optimal and efficient utilization of DC resources. It uses the virtualization
    technique to create several virtual representations or virtual machines (VMs)
    on a single physical machine, thus replicating its processing power, storage,
    and networking which in turn provides CSPs the flexibility to divide the user
    workload among the VMs. This can be done by using optimal resource management
    techniques and task scheduling algorithms for optimal and efficient scheduling
    of user workload and further mapping to VMs. Among all the above energy-efficient
    strategies, this paper focuses on software-centric approaches, particularly scheduling
    approaches to minimize energy consumption at the data center, which will be discussed
    in detail in the next section. FIGURE 6 Open in figure viewer PowerPoint Energy
    consumption of different components in a DC. FIGURE 7 Open in figure viewer PowerPoint
    Taxonomy of energy-efficient strategies used in data centers. 3 TAXONOMY OF ENERGY-EFFICIENT
    WORKFLOW SCHEDULING APPROACHES With the growing number of users opting for cloud
    services, cloud service providers need to optimize their cloud resources to cater
    to the growing demands of the users. This is where cloud scheduling techniques
    play a crucial role in the scheduling of cloud resources. This section covers
    different workflow scheduling algorithms with the primary objective of minimizing
    energy consumption. The workflow scheduling follows different approaches/techniques
    in scheduling workflow tasks and hence scheduling algorithms are classified as
    shown in Figure 8. FIGURE 8 Open in figure viewer PowerPoint Classification of
    workflow scheduling approaches. 3.1 Heuristic-based scheduling algorithms Heuristic-based
    algorithms are a class of algorithms that use problem-dependent information to
    find a near-optimal solution to a specific problem. Near-optimal solution means
    that these algorithms find an approximate solution to a given problem, that is,
    getting a good guess, without knowing how good the guess is. Heuristic-based algorithms
    have some useful characteristics like (i) they are problem-dependent, (ii) they
    have a deterministic approach, (iii) solution search space is small compared to
    other scheduling approaches, (iv) the chances of getting near-optimal results
    depend upon the input parameters provided at the time of execution of the algorithm
    for a given problem. Many heuristic-based algorithms exist for scheduling workflow
    applications in the cloud computing environment. In Reference 41, the authors
    have proposed two scheduling algorithms, ECS and ''ECS+idle,'' for scheduling
    workflows on distributed systems using the dynamic voltage scaling technique.
    Both algorithms use an objective function called ‘relative superiority metric
    (RS)’ and a ‘makespan-conservative energy reduction (MCER)’ technique to determine
    the best combination of task, host, and VSL (voltage supply level) so as to reduce
    energy consumption and makespan. In the ECS algorithm, for each task in a schedule,
    its RS value (degree of energy efficiency relative to task execution time) with
    each pair of processors and VSL is calculated. The highest RS value is selected
    using the current best combination of processor and VSL. Further, the ECS algorithm
    may tend to be confined to a local optimum resulting in the use of the MCER technique.
    The second algorithm, ‘ECS+idle,’ modifies the existing mathematical equation
    used in the ECS algorithm to calculate the RS value. Changes are made in the MCER
    technique by selecting actual energy consumption metrics to reduce energy consumption
    further while scheduling tasks. In Reference 42, the authors have proposed a power-aware
    solution to precedence-constrained tasks (PASTA) for scheduling workflows in cloud
    environments. The main idea behind the algorithm is to optimally select a subset
    of computing resources (processors) from the available pool of resources to be
    allocated to the tasks such that the remaining unused resources are either put
    in standby mode or switched off to minimize energy consumption. The PASTA algorithm
    initially determines a possible subset of computing resources from the available
    computing resources using a greedy approach that estimates the maximum parallelism
    (maximum workflow tasks that can run parallel on the computing resources). Next,
    the tasks are ordered according to the upward ranking method used in the Heterogeneous
    Earliest Finish Time (HEFT) algorithm43,44 and assigned to previously generated
    computing resource subsets such that minimum energy is consumed while executing
    these tasks. The authors in Reference 45 have proposed a Minimized Energy Consumption
    in Deployment and Scheduling (MECDS) algorithm for scheduling data-intensive workflows
    in a cloud environment. In the paper, the authors argue that in a data-intensive
    workflow, a considerable amount of time and energy is wasted during the retrieval/storage
    of input/output data between computing nodes (VM instances) and storage nodes
    (storage devices where input/output data is stored). Hence, the proposed algorithm
    provisions efficient allocation of an independent storage node to a VM instance
    by selecting a storage node (from available storage nodes) for allocation to a
    VM instance such that minimum energy is used for data transfer between the VM
    instance and storage node. Next, in the workflow scheduling phase, all tasks in
    a workflow are ranked using the ‘classical b-level rank’ method and are sorted
    in increasing order of their ranks. Finally, all tasks are mapped to VM instances
    until all tasks are scheduled. In Reference 46, the authors have proposed a DVFS-enabled
    Energy-efficient Workflow Task Scheduling algorithm (DEWTS) to schedule workflow
    tasks on DVFS-enabled processors. The proposed algorithm tends to minimize energy
    consumption by reducing the number of inefficient processors running in the system.
    This is done by decreasing the relatively inefficient processors and reclaiming
    their slack time to schedule other remaining tasks. For this, initially, all the
    tasks in the workflow are ordered based on their calculated ''rank'' value using
    the HEFT algorithm.43 Next, the algorithm sorts the processors in descending order
    based on the number of tasks assigned to each turn-on processor such that processor(s)
    with low energy utilization will be placed later in the order. Further, the tasks
    are scheduled onto processors for each iteration until the makespan of the workflow
    is less than the deadline. Finally, processor(s) which has not been assigned any
    jobs in the above iteration are shut down, thus reducing energy consumption. The
    authors in Reference 47 proposed a Minimum Dependencies Energy-efficient DAG (MinD+ED)
    scheduling algorithm to schedule workflows in a cloud environment while considering
    the utilization level of network links and servers. The proposed algorithm runs
    in two phases. In the first phase, each task of the received workflow is assigned
    a virtual deadline using the Minimum Dependencies (MinD) method, prioritizing
    tasks less dependent on others. In the second phase, each task is dynamically
    mapped to underlying servers to satisfy the virtual deadline assigned to each
    task. Mapping of tasks to servers is done using the Energy-Efficient DAG Assignment
    (ED) approach, which considers four attributes: (1) Maximum Performance of a server
    per Watt, (2) Location of a server with respect to other servers that are executing
    other tasks of the workflow, (3) Utilization of a server and, (4) Communication
    potential of a server, such that servers meet all virtual deadline requirements
    of the tasks. In Reference 48, the authors proposed a cost and energy-aware scheduling
    (CEAS) algorithm for optimizing workflow scheduling in data centers without violating
    deadline constraints. The proposed algorithm initially maps each task to an optimal
    VM based upon each task''s ‘sub-makespan’ value (task''s execution time) and ‘cost
    utility’ value (workload finished per unit cost). Next, two task merging approaches
    are used. In the first approach, two ‘sequential tasks’ (task has exactly one
    parent and one child) are merged as one whole task only when their precedence
    constraints are not violated. In the second approach, two ‘parallel tasks’ (tasks
    having the same start execution time but different finish times) are merged as
    one whole task such that the execution time of two parallel merged tasks is within
    the maximum sub-makespan value of the other already merged parallel tasks. The
    idea is to allocate these merged tasks to the same VM instance(s) so that data
    transfer cost (communication cost) between VMs is reduced, resulting in reduced
    execution cost and energy consumption. The authors in Reference 49 proposed a
    scheduling algorithm named Earliest Deadline First with DVFS and Approximate Computations
    (EDF_DVFS_AC) to schedule real-time workflow tasks in the cloud environment. The
    proposed algorithm utilizes possible schedule gaps created when the VM is in an
    idle state and the task is waiting to receive data from other hosts) and using
    DVFS and approximate computations (based on the Approximate computations model)
    to calculate the time at which a VM will be available to execute the task. The
    algorithm works in two phases: the task selection phase and the VM selection phase.
    In the task selection phase, each task in a workflow is prioritized. The task
    with the least deadline is given the highest priority, and the task with the highest
    deadline is given the lowest priority. If tasks have the same deadline, then the
    task with the maximum average computational cost is given higher priority than
    other tasks. Next, in the VM selection phase, tasks are selected based on their
    priority (highest to lowest), and each task is mapped to a VM instance, providing
    the earliest estimated finish time (EFT) value. In Reference 50, the authors have
    proposed an energy-aware scheduling algorithm that schedules workflow tasks to
    the underlying VMs in an efficient manner. The proposed algorithm runs in two
    phases: deadline partition and planning phase. In the deadline partition phase,
    a workflow graph with a deadline is divided into sub-workflows (workflow tasks),
    and each sub-workflow is assigned a sub-deadline based on the earliest start time
    (EST) and expected execution time (EET) of each task. In the planning phase, all
    the sub-workflow tasks are sorted in the increasing order of their sub-deadlines.
    Next, from the sorted list, tasks are selected in increasing order of their deadline
    and those satisfying the service level agreement (SLA) are allocated to a resource
    (VM) such that minimum energy is consumed and the task is able to finish within
    its specified deadline. Finally, those tasks that violate SLA are assigned to
    other resources. In Reference 51, the authors proposed a new scheduling algorithm
    Reliability and Energy Efficient Workflow Scheduling algorithm (REEWS), that schedules
    mission-critical applications in cloud data centers while ensuring two primary
    objectives such as high reliability and minimum energy consumption. The authors
    in their paper have addressed the problem of communication overhead whereby a
    large amount of energy is consumed in the inter-processor communication when communicating
    tasks (having a parent-child relationship) are allocated to different processors.
    The proposed algorithm groups tasks that need to communicate data (i.e., they
    have a parent-child relationship) into clusters. Each cluster is then allocated
    to a single processor resulting in the removal of communication overhead, thereby
    minimizing communication energy consumption. The authors in Reference 52 proposed
    a novel heuristic-based energy and resource-efficient workflow scheduling (ERES)
    algorithm for cloud environments. The authors have proposed a ‘workflow scheduling
    model’ used by the ERES algorithm, which analyzes the incoming workflow tasks
    based on the type of task (entry, intermediate, or exit task), number of tasks,
    dependency constraints between tasks, and so forth. The tasks are then placed
    into two queues: task queue (stores tasks that have dependency constraints and
    hence are not ready for execution) and ready queue (stores tasks from task queue
    that have fulfilled their dependency constraints and are ready for execution).
    The tasks from the ready queue are then allocated to suitable VMs by considering
    the type of parent-child relationship (one-to-one, one-to-many, etc.) of the task
    and the capacity and energy efficiency of the VMs while adhering to the dependency
    and deadline constraints of the tasks. Further, the proposed algorithm continuously
    monitors the status of hosts (as under-loaded, over-loaded, or normal) based upon
    the threshold value and accordingly allocates/de-allocates the VMs on a host or
    migrates the VMs to a different host. In Reference 53, the authors have proposed
    a new, improved NCM (Not Changing Makespan) heuristic algorithm, which is based
    upon an existing EASLA (Energy-Aware Service Level Agreement) algorithm,62 that
    exploits the unused slack time of workflow tasks created due to their parallel
    execution and task dependencies. The proposed algorithm uses the PEFT (Predict
    Earliest Finish Time) algorithm63 to determine the workflow''s schedule length
    and then tune the CPU frequencies up/down when the length of the schedule is unchanged.
    In this way, the proposed algorithm reallocates the slack time of non-critical
    tasks of the workflow, thus, reducing energy consumed by the processor. In Reference
    54, the author uses one of the multi-criteria decision-making (MCDM) methods,
    “a technique for order of preference by similarity to ideal solution” (TOPSIS)64
    to propose a workflow scheduling algorithm. The proposed algorithm initially assigns
    priority to the tasks using ‘A b-level score’. Then it uses the entropy-weighted
    method (EWM) as a weight estimation method for assigning weights to scheduling
    parameters which are provided as input to TOPSIS for ranking the available virtual
    machines (VMs). Finally, VMs are allocated to the tasks based on these VM ranks.
    Simulation results of the proposed algorithm against other MCDM-based approaches65
    were done on Matlab R2013b, which showed considerable improvement in cost, utilization
    and energy consumption. The author in Reference 55 proposed a novel energy-aware
    cloud workflow scheduling algorithm named ECWSD for geographically distributed
    data across various data centers. The authors have proposed a system architecture
    in which the algorithm firstly sorts the submitted workflow applications based
    upon either of the proposed criteria: “Earliest Deadline First (EDF)”, or “Smallest
    Slack time First (SSF)”. Secondly, the sub-deadline for each task in a workflow
    is assigned using the slack time, which is determined from the earliest finish
    time of the last task in the application. Thirdly, each task is ranked (while
    considering data transmission time) using the HEFT algorithm,43 and based on the
    assigned ranks, each task is sequenced based on three proposed sequencing rules.
    Finally, in the resource allocation phase, data centers are arranged in increasing
    order of their electricity prices and each task from the generated task sequence
    is assigned to the VM with the least electricity price and current best cost.
    In Reference 56, the authors proposed the “ -fuzzy dominance-based reliable green
    workflow scheduling (FDRGS)” algorithm with the multi-objective aim of optimizing
    the workflow schedule''s reliability and energy consumption simultaneously. FDRGS
    consists of two phases: (1) Task priority phase assigns priority to tasks based
    on their calculated rank value. Tasks with the highest rank value are assigned
    the highest priority, (2) The processor allocation phase determines the mapping
    of tasks to processors keeping in mind reliability and energy consumption objectives.
    For this, the -fuzzy dominance metric is used to quantify the relative fitness
    of solutions among the solution set and determine the best solution for scheduling
    the tasks. The authors in Reference 57 proposed the EERS algorithm, which extends
    REEWS51 algorithm. EERS algorithm is different from REEWS algorithm51 in the clustering
    phase whereby if a parent task has more than one child, then while clustering,
    EERS algorithm gives priority to parent-child pair having higher communication
    cost, thus optimizing the clustering approach resulting in reduced energy consumption.
    EERS algorithm initially recursively calculates the rank for each task using the
    average computation time of each task on different VMs such that the precedence
    constraint of the tasks is satisfied. Next, grouping parent and child tasks into
    clusters is done based on their communication cost overhead to schedule them on
    the same VMs. The algorithm then defines the sub-makespan for each task which
    is the new deadline to complete the execution of a task. Further, the algorithm
    ensures that cluster tasks are mapped to suitable VMs keeping in mind energy consumption
    and system reliability. In Reference 58, the authors proposed two workflow scheduling
    algorithms, namely, “Budget-Deadline Constrained energy-aware (BDCE)” and “Budget
    Deadline DVFS-enabled energy-aware (BDD)” algorithms for scheduling workflow applications
    in the cloud environment. The proposed algorithms categorize workflow tasks into
    levels based upon the DBL algorithm66 such that tasks at each level do not have
    any precedent relationship. Each level is then assigned an ‘id’ value which is
    the maximum distance for each task in the level to the exit task. Next, the tasks
    having the same ‘id’ values are grouped. Further, the deadline and budget are
    calculated for each level (each having an ‘id’ value) from the total deadline
    and user-defined budget. Next, tasks at each level are ordered in ascending order
    of their earliest start time and are then selected one by one to be allocated
    to the resources. The most affordable (minimum cost) and energy efficient resource
    (based upon their consumption) is selected for a task considering the budget and
    the deadline constraints. In Reference 59, the authors proposed a “Deadline-constrained
    Energy-aware Workflow Scheduling (DEWS)” algorithm to minimize data center electricity
    costs while considering the data transmission time and different electricity pricing
    schemes of different geographically distributed cloud data centers. DEWS algorithm
    has two main stages. In the first stage, three task sequencing rules (Earliest
    Deadline First, Smallest Slack Time First, and Smallest Workload First) are proposed
    to generate orderly task sequence(s). In the second stage of the algorithm, further
    optimization of the task sequence(s) and optimal selection of data center is done
    by using the ‘variable neighborhood descent (VND) method’. The VND method first
    generates alternate task sequences from initial sequences by swapping two tasks
    in the same layer. Secondly, it selects the optimal data center with the lowest
    electricity price for all tasks. Finally, after selecting the optimal data center,
    the best virtual machine (VM) is searched within the selected data center so that
    minimum energy cost and localization of data are possible. The authors in Reference
    60 have proposed a list-based energy-aware scheduling algorithm (LESA) for scheduling
    tasks in a heterogeneous multiprocessor computing environment. LESA consists of
    three sequential phases: task prioritization phase, energy distribution phase,
    and task allocation phase. In the first phase, each task is assigned a priority
    based upon the modified rank formula derived from upward and downward rank methods
    such that a task with the longest path to the exit node and the shortest path
    to the entry node is assigned a higher priority. Further, in the second phase
    of the algorithm, tasks in decreasing order of their ranks are selected and an
    energy distribution strategy is used to assign weights to each task. The weights
    are assigned such that the total available energy is distributed among each task,
    and an energy limitation value is set on each task. Finally, in the task allocation
    phase, the best processor (considering different speed levels of the processors)
    is allocated to the task such that the earliest finish time for the task is generated.
    In Reference 61, the authors have proposed a heuristic-based approach for scheduling
    workflow applications in a cloud computing environment. The authors have designed
    a scheduling framework consisting of three layers, namely: the user layer, the
    scheduling layer, and the resource layer. The user layer has three responsibilities
    in hand: (1) converting an abstract workflow into an executable workflow, (2)
    clustering of fine-grained tasks (tasks whose runtime is generally shorter than
    system overhead time) into a set of ‘jobs’ using runtime balanced clustering and,
    (3) assigning sub-deadline to each task using the critical path of the workflow
    and thus satisfying task dependencies. Next, the scheduling layer is responsible
    for scheduling the above jobs from the user layer onto virtual machines (VMs)
    using the proposed power-aware MAX-MIN scheduling algorithm, which schedules the
    jobs onto VMs on which the power difference (increment in VM power consumed on
    which task is allocated against ideal VM power), is minimum. Finally, the resource
    layer decides the optimal count of VMs on a particular host considering the current
    job workload. Also, it performs scaling up and down of physical machines based
    on the resource demand of user application(s). Summary: Heuristic-based energy-efficient
    workflow scheduling algorithms tend to solve the NP-hard scheduling problem through
    different optimization methods in an efficient and effective manner. They provide
    a balance between algorithm computational complexity and affordability, thus making
    them a preferred choice in solving scheduling problems. However, certain limitations
    exist which need to be acknowledged: (i) Local-optima problem: Heuristic algorithms
    are efficient but easily fall into local optima thereby providing sub-optimal
    solution; (ii) Solution quality: The performance of heuristic algorithms is greatly
    influenced by the heuristic function and the parameter settings used the algorithm;
    (iii) Generalization: Heuristic algorithms provide problem-specific solution and
    thus fail to provide a universally applicable solution for the scheduling problem.
    In addition, heuristic-based scheduling algorithms have been overshadowed by other
    scheduling approaches like the meta-heuristic approach, hybrid approach, and,
    more recently, machine learning-based approaches, they are still under consideration
    by many researchers. As shown in Table 2, different heuristic scheduling approaches
    are used to solve a particular scheduling problem, each having its own strengths
    and weaknesses. TABLE 2. Summary of heuristic-based energy-efficient workflow
    scheduling algorithms in cloud computing. Reference Algorithm name Objective(s)
    Strength(s) Weaknesses Standard input graph Simulation tool 41 ECS and ECS+idle
    Minimize energy consumption and makespan Low time complexity Better objective
    function Task deadlines not considered Does not avoid local optima situation Laplace
    equation LU-decomposition Fast Fourier Transformation N/A 42 PASTA Minimize energy
    consumption and makespan DVFS technique not used hence special hardware not required
    thus reducing monetary cost Higher makespan Simulation is not done on well-known
    simulator Task set taken for simulation is small Gaussian Elimination (GE) Fast
    Fourier Transform (FFT) DAGSimul 45 MECDS Minimize energy consumption and makespan
    Minimizes data transfer time resulting in reduced makespan DVFS technique not
    used hence special hardware not required thus reducing monetary cost Only data-intensive
    workflows are considered Assumption that all storage nodes have identical power
    model is unrealistic Synthetical DAG workflows INVMOD workflow CloudSim 46 DEWTS
    Minimize average execution Time, energy saving ratio Both computation-intensive
    and communication-intensive applications are handled Higher resource utilization
    by assigning tasks to idle time slots between tasks on a processor Performance
    of the algorithm is not verified using real-world applications Random DAG CloudSim
    47 MinD+ED Minimize energy consumption Utilization level of network links and
    the server is also considered while mapping tasks to servers MinD approach experiences
    the slack time problem Montage Epigenomics GreenCloud 48 CEAS Minimize cost and
    energy consumption Merged sequential and parallel tasks are allocated to the same
    VM instance; leading to reduced data transfer cost Worst case time complexity
    of CEAS is high Montage Ligo Sipht CyberShake CloudSim 49 EDF_DVFS _AC Minimize
    energy consumption and execution cost Utilizes VM idle time to schedule other
    tasks Takes into account the effects of input error on the processing time of
    tasks Algorithm performance not verified using real-world applications Algorithm
    considers only real-time tasks for scheduling Random DAG Using C++ language 50
    N/A Minimize energy consumption Increased resource utilization Algorithm performance
    not verified using real-world applications Random DAG CloudSim 51 REEWS High reliability
    and minimum energy consumption Tasks are clustered and allocated to the same processor,
    thus reducing communication overhead The task set taken for simulation is small
    Sub-target time distribution to each task results in increased makespan of workflow
    Random DAG Gaussian elimination (GE) CloudSim 52 ERES Minimize energy consumption
    and makespan, maximize resource utilization Efficient mapping of tasks onto VMs
    Host-level monitoring ensures the VMs are not overloaded or underloaded For smaller-sized
    workflows, ERES gives a higher makespan of the workflow Determination of threshold
    values is not specified Montage Ligo Sipht CyberShake Epigenomics CloudSim 53
    N/A Minimize energy consumption Uses lookahead feature to compute the makespan
    of a workflow Unused slack time is reclaimed to be assigned to other running tasks
    High time complexity Performance of algorithm not compared with real-world applications
    N/A Using C++ language 54 N/A Minimize makespan, cost, and energy consumption
    and maximize reliability Improvement in cost utilization and energy consumption
    User preferences like deadline and budget constraints are not considered Ligo
    Montage CyberShake Epigenomics Matlab R2013b 55 ECWSD Minimize total electricity
    cost Slack time of task is calculated by considering its EFT and data transmission
    time between tasks High time complexity Electricity pricing at data centers is
    variable Genome Ligo Sipht Montage CyberShake Epigenomics WorkflowSim 56 FDRGS
    Minimize makespan and energy consumption Less computation time Use of -fuzzy dominance
    metric leads to fast convergence towards the optimal set of solutions Task and
    processor set taken for the simulation are small Random DAG Fast Fourier Transform
    (FFT) CloudSim 57 EERS High reliability and minimum energy consumption Uses a
    clustering approach to reduce the communication cost between tasks Polynomial
    time complexity Sub-target time distribution to each task results in increased
    makespan of workflow CyberShake Montage WorkflowSim 58 BDCE Minimize cost, energy
    consumption and execution time Algorithm considers both DVFS and non-DVFS enabled
    resources Suitable for workflow applications with deadline and budget constraints
    Task set taken for the simulation is medium Could not outperform other algorithms
    in case of success rate metric Montage Ligo Sipht CyberShake Epigenomics WorkflowSim
    59 DEWS Minimize energy consumption Considers different pricing schemes of data
    centers Utilizes slack time of tasks while scheduling them to VMs High time complexity
    Performance of the algorithm decreases when the number of tasks increases The
    Assumption that network bandwidth speed within the data center is the same as
    between data centers is unrealistic Inspiral CyberShake Epigenomics N/A 60 LESA
    Minimize schedule length Considers speed levels of the processors as one of the
    criteria in task allocation to processors Algorithm does not fully search the
    solution space Average execution time of algorithm increases with an increase
    in the number of tasks Random DAG Montage N/A 61 N/A Minimize cost and energy
    consumption Minimizes system overhead and communication overhead between tasks
    by horizontal clustering fine-grained tasks into clusters Assumed cluster size
    of ''20'' may not be optimal on other workflow applications Algorithm not compared
    with other existing algorithms High time complexity Cybershake Montage Ligo Sipht
    WorkflowSim 3.2 Meta-heuristic based scheduling algorithms Over the past few decades,
    meta-heuristic based algorithms have gained popularity among researchers due to
    their effectiveness in solving complex and generic problems. In other words, meta-heuristic
    algorithms are generic algorithmic ideas that are not problem-specific and can
    be applied to solve a broad spectrum of issues like complex scheduling, space
    allocation, clustering, etc. In addition to this, meta-heuristic algorithms have
    some useful characteristics like (i) they are problem-independent, (ii) they have
    a non-deterministic approach, (iii) nature-inspired, (iv) solution search space
    is large compared to heuristic scheduling, (v) provide a near-optimal solution.
    So, we can define a meta-heuristic algorithm as follows: In the cloud computing
    environment, meta-heuristic based algorithms are extensively used in solving complex
    scheduling problems, which is considered an NP-Hard problem. Many scheduling algorithms
    using meta-heuristic approaches are developed, and in this subsection, we briefly
    discuss some of these algorithms. In Reference 67, the authors proposed a multi-model
    estimation of distribution algorithm (mEDA) for scheduling precedence-constrained
    parallel applications. The proposed algorithm is based upon the estimation of
    distribution algorithm (EDA),68 which is a population-based evolutionary algorithm
    providing efficient solutions in the search space with the help of a probability
    model. The mEDA algorithm initially divides the population (tasks) into three
    sub-population sets: fast sub-population, energy-efficient sub-population, and
    learning sub-population using the ‘Optimal Pareto’ approach. Next, voltage scaling
    level (VSL) assignment probability models are used upon the above sub-populations
    to firstly generate the VSL assignment arrays and, secondly, to learn and update
    the VSL assignment arrays. Next, sampling is done using the task permutation probability
    model to generate task processing permutation. For each iteration, sub-population
    permutations are adjusted using two adjustment operators (makespan and total energy
    consumption adjustment operator). Further, an improvement operator increases the
    diversity of non-dominated solutions generated in each iteration. In Reference
    69, the authors proposed a new Multi-Objective Genetic Algorithm (MOGA) for scheduling
    workflow applications in a cloud environment. The algorithm takes as input both
    dependent tasks (workflow) and independent tasks for scheduling. Next, MOGA uses
    genetic algorithm70 to generate an optimal workflow schedule satisfying both deadline
    and budget constraints. For this, MOGA starts by randomly creating the initial
    population where each chromosome (encoding string with three fields: set of tasks,
    set of VMs and set of voltage levels) in the population is accessed by calculating
    the fitness function. In each iteration of the algorithm, the fitness function
    considers user preferences and multiple conflicting objectives (that are optimized
    using the Pareto dominance concept) along with genetic operators (like crossover,
    mutation, and selection) to generate the final population (schedule). Finally,
    the final schedule is allocated to underlying resources (VMs) using the ‘gap search
    algorithm’, which initially identifies the gaps in between the consecutive busy
    periods of a resource due to the dependencies between workflow tasks and then
    fills these gaps with independent tasks to increase resource utilization. The
    authors in Reference 71 proposed a hybrid heuristic-based algorithm that optimizes
    three cloud computing processes, namely, VM allocation, task scheduling, and VM
    migration. For this, HH-ECO uses chaotic particle swarm optimization (C-PSO)80
    approach which applies a chaotic mapping function (with high randomness and regularity)
    to assist the particles in breaking away from the local optima when it reaches
    the premature convergence resulting in improved algorithm accuracy. For VM allocation,
    according to their capacity like CPU processing power, size of memory, and so
    forth, HH-ECO categorizes both the VMs and physical machines (PMs) into small,
    medium, and large types and then allocates the VMs to its appropriate PMs using
    inertia weight of the particles calculated during C-PSO algorithm iterations.
    In task scheduling, HH-ECO selects the best scheduling plan based on the chaotic
    sequence and inertia weight factors. Further, in VM migration, HH-ECO monitors
    each PM after the tasks are scheduled and decides the state of PM (overloaded,
    under-loaded, or balanced) and accordingly migrates VMs from overloaded PMs to
    under-loaded PMs using fitness source function measured using C-PSO. In Reference
    72, the authors proposed a novel Multi-objective Optimization for Makespan, Cost
    and Energy (MOMCE) algorithm for workflow scheduling in the cloud environment.
    The algorithm combines the strength of two existing meta-heuristic algorithms,
    namely, particle swarm optimization (PSO)81, 82 and Genetic algorithm (GA) to
    provide more diversity in search space during initialization of particle positions
    and to avoid getting stuck in local optima. To improve the quality of the initial
    population, MOMCE uses greedy and HEFT algorithm43 to change some of the mappings
    of particle positions to provide a diverse search space and faster convergence
    to an optimal solution. Further, the top-half best particles are chosen as ‘elite’
    parents based on their fitness values. Among the elite parents, two particles
    are chosen for passing them as input to the crossover function. Then a mutation
    operator is used over the swarm population to avoid getting stuck in local optima
    and randomly selecting a task to assign it to a random resource. In Reference
    77, the authors have proposed a multi-objective co-evolutionary algorithm, called
    ‘chaotic-preference-inspired co-evolutionary algorithm’- goal (ch-PICEA-g), for
    scheduling workflow tasks in cloud computing. The co-evolutionary algorithm, unlike
    traditional evolutionary algorithms where the initial population consists of a
    single species, consists of different interacting populations, each representing
    a different species. The ch-PICEA-g algorithm uses two chaotic mapping schemes,
    namely tent and logistic mapping (polynomial mapping), to provide uniform distribution
    and variation in chromosomes, resulting in more variety and diversity in the initial
    population compared to a random population. In addition, an improved fitness function
    is used to determine the fitness value of candidate solutions. The authors in
    Reference 79 have proposed a reliable, cost-effective, and efficient meta-heuristic
    task scheduling algorithm for workflow applications. Unlike existing scheduling
    algorithms based on GA that take random initial population, the proposed algorithm
    takes into consideration three factors: (i) current task to VM mapping, (ii) VM
    to a specific configuration of the VM, and (iii) task to CPU frequency mapping
    for the initial population of schedules. The initial population of schedules,
    represented as chromosomes, is generated using a random method and HEFT algorithm,43
    and then each chromosome is encoded using a ‘three-segment integer encoding’ scheme.
    Next, to preserve the task mapping information in a chromosome, the algorithm
    introduces a new genetic ‘crossover operator’ that uses a ‘three-segment crossover’
    operation to exchange their associated VM configuration information between the
    chromosomes. Further, the algorithm uses a ‘mutation operator’ in which chromosomes
    are mutated based on the idle time of VMs, where VMs with higher idle time have
    a high possibility of selection. The authors in Reference 73 have proposed a meta-heuristic
    bat algorithm83 for scheduling computation-intensive workflows in a cloud environment.
    For this, the workflow tasks are initially prioritized according to their computation
    cost on processors and their precedence constraints. Next, every possible task
    to the VM mapping scheme is represented as a lattice point such that each point
    is an ordered tuple of values corresponding to each task. This possible task-VM
    mapping is considered the bat algorithm''s initial population search space. This
    initial bat population is initialized with several parameters (like frequency,
    velocity, loudness, etc.) and evaluated using an objective function to determine
    the current best solution in the current population. Further, iterations are done
    to generate new populations using ‘random walk’, which are then evaluated to find
    the nearest best solution to the global best solution. In Reference 74, the authors
    have proposed a meta-heuristic algorithm named (I-MaOPSO) that provides four improvements
    in the existing MaOPSO algorithm84 so as to get non-dominated solutions with respect
    to conflicting nature of algorithm objectives. In the first improvement, a better
    initial population mix is generated by mixing the initial random population (mapping
    between workflow tasks and VMs) with the population generated by using four greedy
    methods based on four objectives of the algorithm. Secondly, the proposed algorithm
    controls the particle''s velocity by modifying the acceleration coefficients used
    in the MaOPSO algorithm. Thirdly, I-MaOPSO uses a ‘roulette wheel’ selection process
    instead of a ‘tournament selection process’ to reduce the possibility of selecting
    a weak solution as a social leader by giving a fair chance to all candidate solutions
    in the population. And, finally, in order to discourage premature convergence
    of the algorithm to a solution, I-MaOPSO calculates the lowest distance between
    the current solution and the reference points in the search space to find a cognitive
    leader in the population. The authors in Reference 75 proposed an energy minimization
    whale optimization85 algorithm ( ) which focuses on reducing energy consumption
    for workflow scheduling under budget constraints. For this firstly, the budget
    for each task is calculated using the slack cost (SC) of a workflow which is the
    difference between the budget constraint cost of an application and its minimum
    execution cost. Using this slack cost, the budget level cost and available minimum
    frequency of the task on all VMs are calculated. Afterward, an appropriate VM
    instance from available VM instances is selected such that the monetary cost of
    executing the task on the VM instance does not exceed the budget-level cost of
    the task. Finally, the proposed algorithm assigns tasks to the selected VM instances
    using a reverse learning approach to refine the initial random population size
    and update the position of the search agent to find the best agent in the entire
    whale group. In Reference 76, the authors have proposed a Non-dominated Sorting
    Particle Swarm Optimizer using fuzzy rules (F-NSPSO) algorithm for workflow scheduling
    in the cloud environment. The proposed algorithm uses a ‘Pareto-based’ non-dominated
    sorting particle swarm optimizer (NSPSO)86 to generate all possible combinations
    of the task to resource mapping pair keeping in mind different cloud resource
    configurations like resource type, billing models, and so forth. The diversity
    of the solutions (task to resource mapping pair) in NSPSO which results in providing
    a better solution is maintained using the method of ‘niche count’ or ‘crowding
    distance calculation’ (closeness of the particle to its neighborhood). Moreover,
    the large solution set generated through NSPSO is further refined to get the optimal
    solution set (task to resource mapping pair) by applying fuzzy rules along with
    the fitness function used in the proposed algorithm. In Reference 78, the authors
    have proposed a multi-objective genetic algorithm named Genetic algorithm combined
    with LCS (GALCS) for scheduling of workflows in the cloud computing environment.
    The GALCS algorithm takes into account the data volume of a task before scheduling
    the task on a virtual machine (VM) instance. A task with a higher data volume
    compared to the average data volume of all unallocated tasks is allocated to a
    VM instance with higher processing capabilities. Also, the proposed algorithm
    determines the longest common subsequences (LCS) among ‘k’ top-ranked individuals
    or superior individuals in the population during the selection process of the
    genetic algorithm. The idea behind this approach is to use these LCS to dynamically
    adjust the crossover and mutation operations for faster convergence of individuals
    in the generation of a new population. Summary: Meta-heuristic based energy efficient
    workflow scheduling algorithms such as nature-inspired algorithms (particle swarm
    optimization, whale optimization, bat algorithm, etc.) and evolutionary-inspired
    algorithms (such as genetic algorithms) provide a powerful tool in optimizing
    workflow tasks scheduling in computationally complex and parallel and distributed
    challenging environment. These algorithms are particularly suitable for multi-objective
    scheduling scenarios and provide a near-optimal solution by exploring the search
    space intelligently using their fitness and crossover functions. However, meta-heuristic
    based scheduling algorithms also suffer from certain disadvantages like (i) Computation
    intensive: Meta-heuristic algorithms are computationally intensive and time-consuming
    compared to heuristic-based algorithms as they have to search for an optimal solution
    in large and resource-constrained scheduling scenarios; (ii) Parameter tuning:
    Finding the correct parameter settings for the fitness and crossover functions
    in the exploration and exploitation phases of meta-heuristic algorithms is a difficult
    and time-consuming process; (iii) Conflicting objectives: While meta-heuristic
    based algorithms are suitable in multi-objective workflow scheduling problem,
    finding a good balance among conflicting objectives is a challenging and complex
    task and requires additional problem-specific changes to be done in the algorithm.
    Table 3, lists different meta-heuristic algorithms discussed above along with
    their strengths and weaknesses. TABLE 3. Summary of meta-heuristic based energy-efficient
    workflow scheduling algorithms in cloud computing. Reference Algorithm name Objective(s)
    Strength(s) Weaknesses Standard input graph Simulation tool 67 mEDA Minimize makespan
    and energy consumption Uses adjustment and improvement operators to increase the
    diversity of non-dominated solutions Only precedence constraint parallel application
    considered Heterogenous systems not considered Random DAG CyberShake Using C-language
    69 MOGA Minimize makespan and energy consumption Algorithm schedules both dependent
    and independent tasks High time complexity In the absence of independent tasks,
    gaps cannot be filled, resulting in low utilization of resources HC-test bench
    Discrete event-based simulator in Java 71 HH-ECO Minimize makespan and energy
    consumption Rapid convergence rate without local convergence when compared to
    PSO Low time complexity Task set taken for simulation is small Cybershake WorkflowSim
    72 MOMCE Minimize makespan, cost and energy consumption Boot time of node is considered
    for a more accurate and realistic scenario Better population mix using both PSO
    and GA Generates the least value of the fitness function High time complexity
    Real-world scientific applications not taken for simulation N/A Using C++ language
    73 EATTO Minimize energy consumption, execution time and maximize throughput Uses
    objective function to achieve a balanced trade-off between contradictory algorithm
    objectives Communication cost between tasks is not considered Local convergence
    problem Algorithm performance not verified using real-world applications N/A Using
    Python language 74 I-MaOPSO Minimize cost, makespan and energy consumption and
    maximize reliability Uses roulette wheel selection process to give equal chance
    to all solutions Reliability model not defined clearly Roulette wheel selection
    method for leader selection does not give correct results when the population
    contains individuals with large and identical values. Montage Cybershake Epigenomics
    WorkflowSim 75 EM_WOA Minimize energy consumption Better scheduling success rate
    of workflows compared to WOA and PSO Uses a reverse learning approach to provide
    refined population size Reliability of communication link is not considered Ligo
    Montage Epigenomics Gaussian elimination(GE) Cloud computing system 76 F-NSPSO
    Minimize energy consumption and makespan Uses fuzzy membership function to refine
    the solution space Energy consumption during data transfer is not addressed Montage
    Cybershake Epigenomics N/A 77 ch-PICEA-g Minimize makespan, cost, and energy consumption
    Use of chaos theory results in diversification of population and reduces premature
    convergence problem High time complexity Roulette wheel selection for best candidate
    solutions does not give correct results when the population contains chromosomes
    with identical fitness values Montage Cybershake Epigenomics WorkflowSim 78 GALCS
    Minimize makespan and energy consumption Obtains better Pareto optimal solution
    with fast convergence rate and uniformity The ideal value of ''k'' used for finding
    the longest common sub-sequence is not derived Premature convergence and insufficient
    universality of results Inspiral Montage Epigenomics CloudSim 79 N/A Minimize
    energy consumption, cost and maximize reliability Frequency scaling scheme enables
    tuning of CPU frequency during the execution of the task Chromosomes are modified
    keeping in mind the memory requirements of a task on a VM Only linear pricing
    model used The time complexity of the algorithm is not known Sipht LIGO Montage
    Cybershake CloudSim 3.3 Artificial Intelligence /machine learning based scheduling
    algorithms In the past few decades, major advancements and innovations in hardware
    and software have collectively paved the way for the growth of artificial intelligence
    (AI) in today''s world. Some of the factors that have fuelled the growth of AI
    are: (i) enhanced computing power (new generation graphical processing units (GPUs)
    and processors), (ii) expansion of the Internet of Things (IoT)87 resulting in
    (iii) social media network. All these factors have collectively resulted in a
    huge amount of data. Machine learning (ML) algorithms are trained using supervised,88
    unsupervised,89 and reinforcement learning (RL)90 techniques on these big data
    and large related data (data-sets) to analyze them and make smart decisions and
    recommendations. In addition, ML algorithms have been used to schedule workflows
    in cloud computing environments. These algorithms predict the optimal number of
    resources allocated to workflow tasks for faster workflow execution. In this sub-section,
    we discuss workflow scheduling algorithms based on machine learning approaches
    to schedule workflow tasks. The authors in Reference 91 have proposed a deep reinforcement
    learning-based cloud framework (DRL-cloud) for resource provisioning and task
    scheduling in cloud computing. The framework has two main objectives in hand:
    (i) identifying workflow tasks that do not depend on other parents or whose dependency
    constraints have been satisfied as ready tasks so that these tasks are forwarded
    to the next stage, and (ii) ready tasks are then allocated to correct server by
    using deep reinforcement learning method that trains the agents using parameters
    like ‘experience repay’ and ‘Target Network’. In Reference 92, the authors proposed
    an energy-aware multi-objective reinforcement learning (EnMORL) algorithm for
    workflow scheduling in a cloud environment while ensuring the budget constraints
    are satisfied. The proposed algorithm has two phases: the ranking phase and the
    mapping phase. In the ranking phase, the algorithm assigns a priority to each
    task in the workflow based upon each task''s average execution time on all VMs
    and the priority of successor task(s) of . Then all the tasks are arranged in
    descending order of their priorities. In the task-VM mapping phase, the reinforcement
    learning method uses a learning agent to construct a feasible solution (assigning
    a task to VM) according to the scalarized -greedy strategy. This greedy strategy
    uses Q-values (value of an action in a particular state) that are scalarized by
    assigning weights using Chebyshev scalarization function.96 The authors in Reference
    93 proposed a deadline-constrained scientific workflow scheduling algorithm (DCMORL)
    based on multi-objective reinforcement learning with the objective of minimizing
    cost and energy consumption. At first, the algorithm determines the weights using
    the ‘Chebyshev scalarization’96 function which scalarizes its Q-values and helps
    in choosing proper weights for each objective. Further, the authors argue that
    in the existing scheduling algorithms, a new task will be scheduled at each time
    step and once a new task completes its scheduling, the Earliest start time (EST)
    and Latest finish time (LFT) of the tasks connected to it can no longer be calculated
    based on its minimum execution time. Hence, the author proposed a new improved
    partial critical path strategy (MPCP) where the sub-deadlines of each task are
    calculated so that the workflow finishes executing before the deadline. The sub-deadline
    is regularly updated during the scheduling phase (at the beginning of each step)
    so that it can accurately reflect the situation of each time step. The authors
    in Reference 94 have proposed a novel resource management framework for scientific
    workflows (RMFW) that uses machine learning techniques for scheduling workflow
    tasks in a cloud environment. The proposed framework uses the reinforcement learning
    method rather than the greedy approach in Markov game theory for optimal utilization
    of cloud resources and task scheduling. The use of the reinforcement learning
    method over the greedy approach allows the selection of optimal resources for
    task allocation by not only considering the current task-to-resource allocation
    options but also future options resulting in better resource management. This
    is achieved by training multiple learning agents rather than a single agent so
    as to reduce the task scheduling time. The authors in Reference 95 have proposed
    a workflow scheduling framework for scheduling workflow tasks in an edge-cloud
    environment. The proposed framework utilizes Deep Reinforcement Learning (DRL)
    and proximal optimization techniques for scheduling tasks on the nodes. The framework
    also uses a novel hierarchical action space mechanism that uses multi-agent reinforcement
    learning to distinguish between edge and cloud nodes and allocate to the nodes
    to separate agents, thus speeding up their learning process resulting in reduced
    energy consumption and task execution time. Summary: Most of the AI/ML-based energy-efficient
    workflow scheduling algorithms discussed above are dominated by two machine learning
    approaches: reinforcement learning (RL) and deep reinforcement learning (DRL).
    Both these approaches perform adaptive decision-making for scheduling workflow
    tasks by learning from historical data and previous scheduling results in a dynamic
    and uncertain environment. RL and DRL models use advanced machine learning techniques
    to capture relationships and patterns from the input data, thus allowing better
    scheduling decisions. However, just as other scheduling approaches, AI/ML based
    scheduling approaches also have to face certain challenges like (i) Computational
    complexity of the algorithms; (ii) Lengthy training times in training RL and DRL
    models; (iii) Extensive data requirements for typically RL and DRL models; (iv)
    Data privacy and security issues in collecting and using data for training the
    models. Table 4 shows different AI/ML based algorithms discussed above along with
    their strengths and weaknesses. TABLE 4. Summary of AI/ML based energy-efficient
    workflow scheduling algorithms in cloud computing. Reference Algorithm name Objective(s)
    Strength(s) Weaknesses Standard input graph Simulation tool 91 DRL-cloud Minimize
    energy consumption and cost Use of deep reinforcement learning method provides
    optimal workflow schedules High computational complexity Lack of coordination
    among agents may lead to a local optima Google cluster-usage traces TensorFlow
    using python language 92 EnMORL Minimize the makespan and energy consumption Efficient
    assignment of weights for each objective using Chebyshev scalarization function
    Assumption of an infinite number of VMs to execute all tasks in a workflow is
    unrealistic Sipht Montage Cyber-Shake Epigenomics CloudSim 93 DCMORL Minimize
    cost and energy consumption Improved partial critical path strategy Efficient
    weight assignment for each objective The task set taken for the simulation is
    small Sipht Montage Cyber-Shake Epigenomics CloudSim 94 RMFW Minimize energy consumption,
    makespan, cost and maximize resource utilization Use of multiple learning agents
    rather than a single agent for training High time and space complexity Complexity
    of Markov game model increases with increase in workflow workload Sipht Montage
    Inspiral Cybershake WorkflowSim 95 N/A Minimize energy consumption and execution
    time Uses a novel action space mechanism to distinguish between cloud and edge
    nodes for agent allocation High computational complexity in the training process
    Centralized approach whereby the gateway node in case of failure may create a
    network bottleneck N/A CloudSim 3.4 Hybrid scheduling algorithms Hybrid scheduling
    algorithms are the class of scheduling algorithms that combine two or more scheduling
    approaches into one algorithm to solve a scheduling problem. Generally, these
    approaches consist of a combination of (i) heuristic-based algorithm with meta-heuristic
    based algorithm, and (ii) meta-heuristic algorithm with another meta-heuristic
    based algorithm. The basic idea behind combining different scheduling approaches
    is to combine the strengths of different algorithmic approaches to get a better
    initial population mix and fast convergence rate. This sub-section briefly discusses
    different hybrid scheduling algorithms with the primary objective of minimizing
    energy consumption. In Reference 97, the authors have proposed a hybrid energy-efficient
    workflow scheduling algorithm (EEWS) based upon existing hybrid chemical reaction
    optimization (HCRO),98 which is a combination of two other existing algorithms,
    namely, chemical reaction optimization (CRO)99 and heterogeneous earliest finish
    time (HEFT).43 EEWS algorithm modifies the HCRO algorithm by introducing a new
    ‘On-Wall Pseudo-Effective Collision’ operator and a new ''molecule inclusion plan''
    for including new molecules in the current population. Here, a molecule is represented
    as a priority queue that contains the given tasks in topologically sorted order.
    In addition, the algorithm also considers the performance issue of the CPU (the
    efficiency of the processor varies with time) along with other real-world issues
    like VM boot time and VM shutdown while scheduling workflow tasks. The On-Wall
    Pseudo-Effective Collision operator uses an intelligent swap mutation approach
    to get a proper mix of atoms (tasks) in a molecule. And, a new ‘molecule inclusion
    plan’ is used for the inclusion of new molecules into the current molecule population
    using operators and analyzing the potential energy of the molecule. Finally, the
    EEWS algorithm uses the HEFT algorithm for mapping the generated molecule (having
    tasks in topological order) to suitable VMs. The authors in Reference 100 have
    proposed a hybrid meta-heuristic algorithm called a harmony-inspired genetic algorithm
    (HIGA) for scheduling both independent and dependent (workflow) tasks in a cloud
    environment. Initially, the proposed algorithm generates a random population (generation)
    using a genetic algorithm (GA). In the next step, the algorithm saves the best
    individual from the evolved generation and then checks whether the algorithm (genetic
    algorithm) is stuck in the local optimal point or not. If yes, then the harmony
    search algorithm is used on the final evolved generation to find the better optimal
    region and hence find the optimal schedule for task mapping on VMs. The proposed
    algorithm carefully senses local as well as global optimal regions by combining
    the exploration capability of the genetic algorithm and exploitation capability
    of the harmony search algorithm105 resulting in fewer iterations and a quick convergence
    rate. HIGA reduces the number of iterations wasted in the local or global optimal
    region by introducing the ‘leader’ (a leader is the best candidate of a particular
    generation) concept in GA along with other parameters (traces and trials). In
    Reference 101, the proposed Hybrid Approach for Energy-aware scheduling of Deadline
    constrained workflows (HAED) algorithm uses Intelligent Water Drops (IWD)106 algorithm
    and Genetic Algorithm (GA) for scheduling of workflows. Initially, in the IWD
    algorithm, a sub-deadline is calculated for each task based on the deadline of
    the workflow and for each iteration, several workflow schedules are generated
    as possible solutions. Among these schedules, the schedule satisfying deadline
    constraints and the minimum energy consumption is chosen as the best schedule
    (final solution) which is seeded with the initial random population size of the
    GA for enhanced results. The authors in Reference 102 have proposed a hybrid workflow
    scheduling algorithm called hybrid multi-objective Greedy Ant-lion optimization
    (ALO) algorithm with the Sine-cosine algorithm (SCA) algorithm (HGALO-SCA) for
    scheduling workflow tasks in a cloud environment. The proposed algorithm uses
    the ALO algorithm for initial ant and ant-lion population generation and further
    selects the best ant-lion roulette wheel selection process. The location update
    of an ant is done using a random walk and to avoid the early convergence rate
    and to reduce the local optimization problem, the SCA algorithm107 is used to
    update the positions of the ant. The author in Reference 103 proposes a ‘Pareto’
    based multi-objective discrete ant lion optimization metahueristic algorithm (PBMO-DALO)
    for workflow scheduling in cloud data centers. The proposed algorithm ensures
    minimizing the makespan and energy consumption and improved solutions convergence
    while scheduling the workflows. For this, three mathematical models, namely the
    cloud computing model, task model, and computational energy model have been defined
    in their work. Further, the algorithm uses two approaches: (1) discrete ant lion
    optimization approach108 using a new vector-based encoding scheme for a random
    walk of ants and ant lions and a tournament selection method for choosing the
    fittest ant lion for trap building, (2) Pareto optimal solution approach uses
    a crowding distance concept where ant lions are selected from less populated regions
    of the search space, thereby providing an improvement in the diversity of obtained
    solutions. In Reference 104, the author has proposed a hybrid algorithm named
    multi-objective workflow scheduling using a hybrid bat algorithm (MOHBA) for scheduling
    workflow tasks in cloud computing. MOHBA algorithm exploits two scheduling approaches,
    namely, heuristic-based approach (using HEFT algorithm) and meta-heuristic approach
    (bat algorithm83), for scheduling of tasks on VMs. Initially, a random bat population
    is generated representing an encoding scheme of task to VM mapping which is then
    seeded with a schedule generated using the HEFT algorithm43 on workflow tasks.
    Next, the bat population is updated by comparing an individual''s frequency, loudness,
    and pulse rate values with the current local and global best solutions. Further,
    for each iteration, the fitness of an individual in a population is determined
    using an objective function that evaluates an individual based upon four parameters:
    makespan, cost, energy consumption and resource utilization, and the population
    with the best fitness values is passed to the next generation. Summary: Hybrid
    energy-efficient workflow scheduling algorithms provide flexibility in adjusting
    to different scheduling scenarios in response to dynamic workloads, resource usage
    and availability by incorporating different scheduling approaches including heuristic,
    meta-heuristic, and machine learning. The combination of different scheduling
    techniques in hybrid algorithms results in better and more efficient scheduling
    decisions compared to individual scheduling approaches. However, a few challenges
    in the use of hybrid scheduling need to be addressed such as (i) Increased computational
    complexity: Combination of different scheduling approaches results in additional
    complexity in scheduling process; (ii) Conflicting objectives: Hybrid algorithms
    need to often satisfy conflicting algorithm objectives and constraints resulting
    in careful parameter tuning of functions used in the algorithm; (iii) Interoperability:
    An effective hybrid scheduling algorithm should ensure proper integration and
    coordination between different scheduling components for smooth flow of data and
    optimal scheduling decisions. Table 5 shows different hybrid algorithms discussed
    above along with their strengths and weaknesses. TABLE 5. Summary of hybrid energy-efficient
    workflow scheduling algorithms in cloud computing. Reference Algorithm name Objective(s)
    Strength(s) Weaknesses Standard input graph Simulation tool 97 EEWS Minimize makespan
    and energy consumption CPU performance with respect to time is taken into consideration
    On-wall pseudo-effective collision operator uses intelligent swap mutation in
    HCRO technique Communication cost between tasks not considered User constraints
    not considered Sipht Ligo Inspiral Montage Cyber-Shake Epigenomics Eclipse version
    4.4.0 100 HIGA Minimize makespan and energy consumption Iterations in finding
    local or global optimal region reduced by introducing leader concept in GA Fast
    convergence rate High time complexity Montage Cybershake Epigenomics CloudSim
    101 HAED Minimize makespan, cost and energy consumption Use of Intelligent Water
    Drops technique along with GA ensures better population mix resulting in better
    results High time complexity Ligo Inspiral Montage Cyber-Shake Epigenomics WorkflowSim
    102 HGALO-SCA Minimize makespan, cost, energy consumption and maximize throughput
    Uses sine-cosine algorithm for ant position updates and to avoid local optima
    problem Algorithm performance compared with only one existing algorithm Roulette
    wheel selection for best ant-lion does not give correct results when the population
    contains individuals with identical values Ligo Montage Cyber-Shake Epigenomics
    WorkflowSim 103 PBMO-DALO Minimize makespan and energy consumption Better convergence
    rate at a lower number of tasks Reduced computation time Low convergence speed
    with an increased number of tasks Random DAG Fast Fourier Transform (FFT) CloudSim
    104 MOHBA Minimize energy consumption, cost, makespan and maximize resource utilization
    Fast convergence rate Simplistic approach Problem of local-optima Low exploitation
    ability Ligo Montage Cyber-Shake Epigenomics CloudSim 4 ANALYTICAL DISCUSSION
    In this section, we analyze different scheduling algorithms presented in Section
    19 by carefully examining the phases of the algorithm to get a greater insight
    into the functioning of the algorithm. 4.1 Analysis of heuristic-based scheduling
    algorithms As described earlier, heuristic-based scheduling algorithms are classified
    into three categories: list-based scheduling, clustering-based scheduling, and
    duplication-based scheduling. In general, each of these scheduling approaches
    has a common way of approaching the scheduling problem at hand. For instance,
    a list-based scheduling algorithm generally consists of two main phases: (i) task
    priority phase and (ii) task to resource mapping phase. Similarly, a cluster-based
    scheduling algorithm generally has two main phases: (i) task clustering phase
    and (ii) task to resource mapping phase. Further, the duplication-based scheduling
    algorithm has mainly two phases: (i) identifying the tasks for duplication and
    (ii) locating the free time slots on a resource to duplicate the tasks. Hence,
    in this section, we analyze heuristic-based algorithms discussed in Section 19
    based on their phases and the methods used in these phases. For instance, list-based
    scheduling algorithms are analyzed by identifying the type of task prioritization
    and task-to-resource allocation methods used during scheduling workflow tasks.
    Similarly, cluster-based scheduling algorithms were analyzed by identifying the
    clustering approach used and task-to-resource mapping methods used for scheduling
    tasks onto cloud resources. 4.1.1 List-based scheduling In the list-based scheduling
    algorithms, while different methods like b-level (upward ranking), t-level (downward
    ranking), or a combination of both can be used for assigning priority to a task,
    b-level has been used more frequently by authors41, 42, 45, 46, 54-56 in their
    research work. The reason behind this is that b-level ranking method is easy to
    understand and implement, and it also preserves the precedence constraints between
    the tasks. In addition, some algorithms have used other task priority methods
    like in Reference 47, the authors have factored in the dependency relationship
    between tasks, that is, tasks that are less dependent on other tasks have a higher
    priority in assigning priority to a task. Similarly, in References 49, 50, priority
    is assigned to a task based upon their deadlines, that is, the task with the least
    deadline is given the highest priority. A queuing mechanism is used in Reference
    52, where tasks are picked from a ‘ready queue’ (which stores the tasks in an
    orderly manner such that task dependencies are not violated) in assigning priorities
    to tasks. The algorithm in Reference 59, prioritizes workflow tasks by ordering
    them using three sequencing methods based upon the task''s deadline, remaining
    slack time, and its size, whereas in Reference 60, the algorithm uses a modified
    rank mechanism that uses both ‘b-level’ and ‘t-level’ to assign a rank to each
    task and based upon their rank value assigns priority to each task. 4.1.2 Clustering-based
    scheduling The analysis of clustering-based scheduling algorithms discussed in
    Section 19 is done to get answers to two questions: (i) which task clustering
    approach is used and (ii) which task-to-resource mapping method is used in the
    algorithms. The answer to the first question is discussed in the coming paragraph,
    while the answer to the second question is discussed in the next paragraph. While
    different techniques are used to cluster tasks in the algorithms, the core idea
    behind all the clustering-based techniques is to optimize the communication time
    between dependent tasks. In recent times, different task clustering techniques
    have been proposed by the researchers. For instance, in Reference 48, clustering
    of tasks is based upon identifying two types of tasks: (i) tasks with exactly
    one parent and one child and, (ii) tasks having the same starting execution time
    but with different finish times. The algorithms in References 51, 57 use the same
    technique to group the tasks that have a parent-child relationship but with a
    slight difference in Reference 57, where the algorithm while grouping of tasks
    gives priority to the parent-child pair having higher communication cost than
    its peer child thus resulting in further energy saving. The authors in Reference
    61 have used a different technique and named it ‘Runtime balanced clustering’
    for the clustering of tasks. The idea behind this approach is to balance the execution
    time of cluster(s) formed in this process such that a load imbalance problem does
    not arise. This is achieved through a greedy approach in which tasks are first
    sorted in decreasing order of their runtime, and then starting from the highest
    to lowest execution time, tasks are added to the cluster with the shortest execution
    time. In addition, different task-to-resource mapping methods have been proposed
    in several clustering-based algorithms like in Reference 48, optimal VM is selected
    by calculating the minimum utility cost of each task on all VMs such that it does
    not violate the task sub-deadline. Similarly, the algorithm in Reference 51 identifies
    clusters of the same type of tasks to be allocated to a single processor such
    that the task deadlines are not violated, whereas in Reference 57, the algorithm
    firstly calculates the optimal frequency for energy conservation and reliability
    for each task on all available VMs and then secondly, mapping of a task from each
    cluster is done to the highest reliable VM. Another technique61 in which, based
    upon the sub-deadline assigned to each task, each task is mapped to a VM on which
    the power consumption of running a task is minimal. 4.2 Analysis of meta-heuristic
    based scheduling algorithms A typical meta-heuristic based scheduling algorithm
    consists of three main components: (i) ‘search space’, which is the finite space
    of all possible solutions to the given problem at hand, (ii) ‘intensification
    or exploitation’ phase, which means to focus the search in a local region knowing
    that a current good solution is found in this region and, (iii) ‘diversification
    or exploration’ phase, which means to diversify the solutions to explore the search
    space on a global scale. We analyze different meta-heuristic based scheduling
    algorithms as discussed in Section 19 by identifying the above three components
    of the algorithms. In recent times, many meta-heuristic algorithms have been proposed
    by the researchers. For instance, using the optimal Pareto approach, the algorithm
    in Reference 67 has divided the initial search space into three sub-populations.
    In its exploitation phase, the algorithm uses VSL assignment probability models
    upon each sub-population to generate the best VSL assignment arrays, and further
    adjustment operators are used to diversify the solutions. And, in the exploration
    phase, Adjustment operators are used to increase the diversity of non-dominated
    solutions. Another algorithm,69 uses a randomly generated chromosome population
    as its search space. Further, in its exploitation phase, the algorithm uses a
    fitness function that uses the ‘weighted sum method’109 and ‘tournament-based
    selection’110 approach for best chromosome selection, and in its exploration phase,
    genetic operators like crossover and mutation are used to diversify the chromosomes
    in the population. Similarly, in Reference 71, the search space of the algorithm
    consists of tasks represented as particles with initial position and velocity
    values whereas the algorithm in its exploitation phase uses particle swarm optimization
    (C-PSO) approach to find the best solution by estimating the adaptive inertia
    weight factor and velocity of the particle. In its exploration phase, it applies
    a chaotic mapping function (with high randomness and regularity) to assist the
    particles in avoiding local convergence. The algorithm in Reference 72 has a search
    space consisting of a mixed set of particles: one set generated by a greedy approach,
    another considering minimum execution time on VM, and the last set of randomly
    generated particles. In its exploitation phase, a fitness function uses a ‘cost
    function’ to find the best particle. In its exploration phase, the algorithm uses
    a ‘crossover’ method where a single-point crossover generates a new offspring
    and a ‘mutation’ method to select a random task and assign it to a random resource.
    The algorithm in Reference 73 has an initial search space consisting of all permutations
    of task-to-VM mappings. In its exploitation phase, a ‘random walk’ is done to
    find the best solution. And in its exploration phase, it uses fitness value to
    determine the fitness of the solution. As for the algorithm in Reference 74, the
    initial search space consists of a mix of random population and a population generated
    through proposed greedy methods. In its exploitation phase, a social leader and
    a cognitive leader are selected from the population by calculating the density
    (using the density operator) and convergence measure (using the convergence operator)
    of a particle. Further in its exploration phase, the algorithm applies the mutation
    operator to 15% of the particles in the population to improve the diversity of
    the population. In Reference 77, the algorithm uses chaos theory to generate a
    mix of populations from different species. Further, in the exploitation phase,
    it uses an improved fitness function to calculate the fitness value of candidate
    solutions with respect to algorithm objectives. And, in its exploration phase,
    the algorithm uses logistic mapping and a roulette wheel selection scheme to generate
    new chromosome offspring resulting in better population diversification. Another
    algorithm75 uses a randomly generated population in its search space and uses
    a fitness function in its exploitation phase that uses two fitness parameters
    while adjusting their weight value to find the largest fitness value as a feasible
    solution. Further in its exploration phase, the algorithm uses a reverse learning
    method whereby an elite population is selected from the initial population and
    the inverse of its population. In Reference 76, the algorithm uses a search space
    of particles (task-VM mapping) generated by ordering the tasks based on their
    ranks using an upward ranking method. Whereas, in its exploitation phase, the
    algorithm uses a fitness function that uses fuzzy-based decision selection to
    find the best task-resource mapping, and further in its exploration phase, diversity
    of the population is maintained by using the concept of ‘crowding distance’ between
    particles in the population. The algorithm78 has a search space consisting of
    a population with chromosomes comprising an encoding scheme. Further, its exploitation
    phase uses a non-dominated and crowding distance sorting to find the best solution,
    and its exploration phase uses a single-point crossover method to generate new
    chromosomes and two mutation operators to replace some gene values with new values
    to get a proper mix of possible solutions. Finally, in Reference 79, an initial
    population of the algorithm consists of chromosomes generated using a random method
    and HEFT algorithm.43 Further, in its exploitation phase, the algorithm uses a
    fitness function that calculates the fitness of a chromosome by evaluating its
    overall cost. Minimum the overall cost of the chromosome, the higher the likelihood
    of its selection. And, in the exploration phase, the algorithm uses a ‘three-segment
    crossover’ operation to diversify the chromosome population, and also modified
    mutation operator is used to reduce the possibility of the algorithm getting stuck
    in local optima. 4.3 Analysis of AI/ML based scheduling algorithms Artificial
    intelligence based scheduling algorithm is analyzed here based on the type of
    machine learning approach(s) used by the algorithm. Machine learning approaches
    are classified into four major categories: (i) supervised learning, (ii) unsupervised,
    (iii) semi-supervised learning, and (iv) reinforcement learning. For instance,
    the authors in Reference 91 have proposed a cloud scheduling framework that uses
    a Deep Reinforcement learning method for optimal resource provisioning and task
    scheduling. The proposed framework uses a deep learning method to train multiple
    agents for scheduling workflow tasks in large data centers. The training technique
    in the algorithm has a fast convergence rate in finding the solution by randomly
    saving the previous action state of the agent during learning updates and using
    a separate neural network with different parameters. The authors in Reference
    92 have used a reinforcement-based machine learning approach for allocating a
    task to a VM. The authors argue that determining appropriate weights using linear
    scalarization function in the multi-objective environment is a difficult problem.
    Moreover, it can only discover solutions in convex regions of the Pareto front.111
    And, hence the authors in the proposed algorithm have used a ‘Chebyshev scalarization
    function’,96 a nonlinear scalarization method that overcomes the above drawbacks
    by selecting optimal weights in the multi-objective environment. Further, using
    the reinforcement learning approach, the algorithm constructs a feasible solution
    (task to VM mapping) based on a scalarized greedy strategy. The greedy strategy
    finds the possible list of states, action pairs, and reward mechanisms which defines
    two reward metrics each for the defined algorithm objectives. Similarly, in Reference
    93, the algorithm also uses the ‘Chebyshev scalarization function’ to determine
    an objective''s weight in a multi-objective environment. Using the reinforcement
    learning approach, the algorithm constructs a feasible solution (task to VM mapping)
    using a scalarized greedy strategy that finds the possible list of states, action
    pairs, and reward mechanisms which defines two reward metrics each for two algorithm
    objectives: deadline and energy consumption. The authors propose an upgraded strategy
    of PCP called MPCP. In Reference 94, the authors use a reinforcement machine learning
    method for optimal resource allocation and scheduling. They have designed a scheduling
    framework that uses the reinforcement learning methods in the Markov game model
    to train multiple agents instead of normally single agents, to find an optimal
    solution to task-to-resource mapping problems with reduced energy consumption.
    Similarly, in Reference 95, the authors use the Deep reinforcement learning method
    for efficient resource allocation and scheduling. The authors proposed a multi-actor
    framework that uses a centralized approach whereby, a gateway node is considered
    a master node and all other edge nodes which are directly connected to the master
    node are considered slave nodes. The master node is responsible for scheduling
    resources for slave nodes based on the real-time status of the resource availability.
    In addition, to increase the versatility of the framework with different application
    scenarios, multiple agents are used to distinguish between edge and cloud nodes.
    Further, these agents are trained using deep reinforcement learning methods to
    provide optimal allocation of resources to workflow tasks. 4.4 Analysis of hybrid
    scheduling algorithms As discussed earlier, a hybrid scheduling approach combines
    two or more approaches to generate an optimized solution for a scheduling problem.
    The analysis of the scheduling algorithm is based on the type of approaches (like
    heuristic-based algorithm with meta-heuristic based algorithm or meta-heuristic
    algorithm with another meta-heuristic based algorithm) and subsequent methods
    used in these approaches. The algorithm in Reference 97 uses a combination of
    two approaches: a list-based scheduling approach for assigning a rank to the workflow
    tasks and a meta-heuristic technique called hybrid chemical reaction optimization
    for getting optimal scheduling solutions. The algorithm''s search space comprises
    an initial molecule consisting of tasks that are ordered based on their upward
    rank. Further, in its exploitation phase, the algorithm determines the fitness
    of a molecule based upon its potential energy (the lower the potential energy,
    the better the corresponding schedule), and in its exploration phase, it uses
    the ‘On-Wall Pseudo-Effective Collision’ operator which does an intelligent swap
    mutation to get a proper mix of tasks to get a better solution. Another algorithm,100
    uses a combination of two meta-heuristic algorithms: harmony search and genetic
    algorithm, in its scheduling problem. The algorithm starts with a randomly generated
    population as its search space. Further, it uses a ‘weighted sum’ method to define
    the fitness of individuals in the population and to find the best individual in
    its exploitation phase. Next, harmony search is applied to the population generated
    by the genetic algorithm if it gets stuck in either its local or global optima.
    Similarly, the algorithm in Reference 101 also uses two meta-heuristic algorithms:
    intelligent water drops and genetic algorithm in its scheduling problem. Its initial
    search space consists of a population generated using intelligent water drops
    and a genetic algorithm. Further, in its exploitation phase, chromosomes with
    high fitness scores are selected from the current population. In its exploration
    phase, the non-dominating sorting technique is used to diversify further the solution
    set. In Reference 102, the algorithm uses a combination of meta-heuristic: ALO
    and Sine-cosine wave (SCA) optimization algorithms. Its initial population consists
    of random ant and ant-lion population. While in its exploitation phase, the ALO
    algorithm uses the roulette wheel method for selecting the best ant-lion from
    the population. And, in the exploration phase, SCA algorithm is used to update
    the ant positions, and a random chaos function with random numbers ranging between
    0 and 1 is used to diversify the solution search space and also avoid the algorithm
    being trapped in local optima. The algorithm in Reference 103 uses a Pareto approach
    and a meta-heuristic algorithm called discrete ant-lion optimization in its scheduling
    problem. The algorithm takes the initial population of randomly generated positions
    of ants and ant-lions using the Pareto approach as its search space. Further,
    a ‘Tournament selection method’ is used to select the best ant-lion by comparing
    their fitness value for each iteration in its exploitation phase, and to diversify
    the random movement of ants, a random sub-string of ant movement is reversed to
    generate a new ant movement string along with adaptive mutation is done in its
    exploration phase. Finally, in Reference 104, the algorithm uses heuristic: HEFT43
    and nature-inspired meta-heuristic bat algorithm83 for scheduling tasks in a workflow.
    In its exploitation phase, a fitness function is used to select the best solution
    from the population that satisfies the algorithm objectives. And, in the exploration
    phase, attributes like frequency, loudness, and pulse rate values of an individual
    are changed using a linear distribution equation to diversify the population.
    Figure 9, shows the percentage distribution of workflow scheduling approaches
    used by different scheduling algorithms with the objective of minimizing energy
    consumption. FIGURE 9 Open in figure viewer PowerPoint Percentage distribution
    of workflow scheduling approaches used by scheduling algorithms surveyed in this
    paper. 5 WORKFLOW APPLICATIONS The advances in computing technology have enabled
    researchers to simulate and analyze their complex, data and computation-intensive
    experiments using heterogeneous and disturbed resources. The increasing use of
    digital technologies and sensors in performing scientific experiments like high-resolution
    image capturing in astronomy or capturing seismic vibrations in seismology has
    resulted in the generation of large volumes of data (Terabytes or Petabytes),
    which are stored in databases and files. Managing and processing these large volumes
    of data is an uphill task and has further increased the complexity of analyzing
    this data. Hence, workflows represent and automate various operations involved
    in these complex scientific experiments and derive new information by systematically
    processing large volumes of data captured in experiments or generated by simulations.
    Moreover, to the best of our knowledge, previous works in this domain have not
    defined workflow applications used practically in the real world in such detail.
    Therefore, in view of this, we discuss some of the workflow applications used
    in areas like bio-informatics, astronomy, seismology, and agro-ecosystems. 5.1
    Bio-informatics The word ‘bio-informatics’ is made of two words, bio and informatics,
    ‘bio’ is derived from biology, and ‘informatics’ is derived from information technology.
    Bio-informatics112, 113 is an interdisciplinary field that uses software tools
    and techniques like pattern recognition, data mining, and machine learning to
    process and analyze large and complex biological data. Bio-informatics aims to
    utilize the computational power of computing systems to understand biological
    processes, which can further help in genome assembly, gene finding, drug design,
    protein structure prediction, and so forth. Workflows are used in the bio-informatics
    field to provide an abstract view of the computational steps in the form of a
    directed acyclic graph consisting of nodes representing tasks and edges representing
    execution dependencies between different tasks. Some of the workflows used in
    the bio-informatics field are discussed below. 5.1.1 1000 genome The 1000 genomes
    project114 was launched as one of the largest distributed data collection and
    analysis projects to study the mutational overlaps in human populations. The primary
    goal of the project was to provide potential disease-related mutations using statistical
    evaluation methods over reconstructed genomes of 2504 individuals across 26 different
    human populations. The workflow for the 1000 Genome project is shown in Figure
    10A. FIGURE 10 Open in figure viewer PowerPoint Sample workflow applications used
    in bio-informatics. 5.1.2 Epigenomics workflow The epigenomics workflow was created
    by the USC Epigenome Center and the Pegasus Team to automate the execution of
    the various genome sequencing operations. The DNA sequence data generated by the
    Genetic Analyzer system is split into several parts that can be executed in parallel.115
    The data in each part is converted into a file format that is used by the mapping
    system, as shown in Figure 10B. 5.2 Astronomy Astronomy116 is the study of celestial
    objects using knowledge of mathematics, chemistry, and physics domain to discover
    their origin and evolution over a period of time. The advancements in observational
    instruments like telescopes and spectrometers in astronomy have resulted in the
    generation of large-scale and high-resolution images as raw astronomical data
    that require complex computation for their processing. Workflows represent these
    astronomical data and are then mapped onto physical resources (processors and
    storage devices) for processing and analyzing the data. Some of the workflows
    used in the astronomy field are discussed here. 5.2.1 Montage workflow The creation
    of the montage application workflow was funded by NASA/IPAC118 and is maintained
    by IRSA. It enables astronomers to create images of regions of the sky that are
    too large to be produced by astronomical cameras. This is done by combining astronomical
    images taken from different cameras and stored in image archives such as the Two
    Micron All Sky Survey (2MASS)119 and Sloan Digital Sky Survey (SDSS)120 into composite
    images, called mosaics. The output mosaics are large and hence it is desirable
    to execute the montage application in an environment where the availability of
    storage resources can be assured, like the cloud environment. The workflow for
    montage is shown in Figure 11. FIGURE 11 Open in figure viewer PowerPoint Sample
    workflow of montage. 5.3 Seismology Seismology is the study of earthquakes due
    to the violent movement of the earth''s layers and the study of the propagation
    of seismic waves121 through the earth.122 In recent times, seismology has been
    experiencing data-intensive research driven by the huge availability of data due
    to several installed seismographs and advancements in the data science field.
    This has resulted in the design of new workflows for large-scale seismic analysis
    for future earthquake predictions. Some of the workflows used in the seismology
    field are discussed here. 5.3.1 Cybershake workflow Cybershake workflow was developed
    by the Southern California Earthquake Center (SCEC) using USC High-Performance
    Computing and Communications systems and open-science NSF resources. It determines
    the wave propagation effects due to an earthquake using physics-based 3D ground
    motion simulations.124 The workflow for cybershake is shown in Figure 12. FIGURE
    12 Open in figure viewer PowerPoint Sample workflow of cybershake. 5.4 Weather
    forecasting Weather forecasting predicts the earth''s atmosphere using physics
    principles and statistical techniques for a given location and time. The researchers
    working in weather forecasting study the atmosphere using real-time observational
    data (spread across different geographical locations) and atmospheric parameters
    like temperature, humidity, and pressure to predict the weather conditions of
    a region. This requires the creation of large-scale modeling of weather forecasting
    models and timely information dissemination and analysis for accurate weather
    predictions. Workflows can complement weather forecasting by providing a framework
    for data integration and automating the steps for researchers, thus ensuring timely
    weather prediction. Some of the workflows used in the weather forecasting field
    are discussed here. 5.4.1 CASA nowcast workflow The Center for Collaborative Adaptive
    Sensing of the Atmosphere (CASA)125 designed a workflow intending to study the
    earth''s lower atmosphere (up to 3 km of the atmosphere) using a network of high-resolution
    Doppler weather radars for accurate weather predictions. CASA Nowcast Workflow
    combines and analyzes the captured data from different Doppler radars for a certain
    period of time to predict the behavior of weather over a region. The workflow
    for CASA Nowcast is shown in Figure 13A. FIGURE 13 Open in figure viewer PowerPoint
    Sample workflow applications used in weather forecasting. 5.4.2 LEAD Linked Environments
    for Atmospheric Discovery (LEAD)126 is a project that processes real-time landscape
    observational data for weather forecasting. The LEAD weather forecast workflow127
    as shown in Figure 13B takes input data from various sensors to be further pre-processed
    and analyzed using data mining techniques for predicting weather conditions of
    a region. 5.5 Numerical analysis Numerical analysis is the study that uses numerical
    methods to find approximate but accurate solutions to hard problems. Some of the
    numerical methods used in scheduling problems are discussed below. 5.5.1 Gaussian
    elimination The Gaussian elimination (GE) method is an algorithm to solve systems
    of linear equations.128 In the context of the scheduling problem, the Gaussian
    Elimination graph represents a DAG generated from a matrix size m with DAG size
    defined as .44 Figure 14A shows a GE graph for matrix size five. It is visible
    from the graph that its odd level contains only a single task, thus reducing its
    parallelism. FIGURE 14 Open in figure viewer PowerPoint Sample workflow applications
    used in numerical analysis. 5.5.2 Fast fourier transform Fast fourier transform
    (FFT) graph represents a DAG of inter-dependent tasks with the size of the DAG
    determined as , where m is the input points.44 The first part ( ) determines the
    number of recursive task calls while the second part determines the number of
    butterfly operation tasks for some integer k. An FFT task graph for four input
    points is shown in Figure 14B. Table 6 shows workflow graphs used by different
    energy-efficient workflow scheduling algorithms for examining an algorithm''s
    performance in different workload scenarios. TABLE 6. Workflow graphs corresponding
    to real-world applications. S. No. Real-world applications Workflow graph(s) Research
    papers 1. Bio-informatics 1000 Genome, Epigenomics 47, 52, 54, 55, 58, 59, 74-78,
    92, 93, 97, 100-102, 104 2. Astronomy Montage, LIGO 47, 48, 52, 54, 55, 57, 58,
    60, 61, 74-79, 92-94, 97, 100-102, 104 3. Seismology Cybershake 48, 52, 54, 55,
    57, 59, 61, 67, 71, 74, 76, 77, 79, 92, 93 4. Numerical analysis Gaussian Elimination,
    Fast Fourier Transform 41, 42, 51, 56, 75, 103 6 EMERGING TRENDS, RESEARCH CHALLENGES
    AND FUTURE DIRECTIONS The ever-increasing growth of business services and applications
    using cloud computing as a platform for their services has propelled the urgent
    need to develop and design new computing paradigms and techniques. These computing
    models and techniques ensure they incorporate the latest advancements in software
    and hardware technologies and the user''s behavioral changes. In this section,
    we discuss some of the emerging computing paradigms like fog computing, edge computing,
    and containers, as well as techniques like blockchain and artificial intelligence,
    along with future research directions with respect to scheduling in cloud computing.
    6.1 Emerging trends With the passage of time and the changing needs of the users,
    many technologies become obsolete and less common and give way to new technologies
    that can cater to the needs of the users. In this section, we discuss some of
    the emerging trends in the field of cloud computing that have felt their presence
    in the market. 6.1.1 Fog computing Fog computing is a distributed computing standard
    that resides between cloud data centers and IoT devices/sensors.129 Typically,
    a fog computing environment collectively called as fog devices consists of several
    networking devices like routers, switches, proxy servers, base stations, and so
    forth, that are placed near the IoT devices/sensors. This proximity of fog devices
    with IoT devices/sensors ensures that a computing, storage, and networking platform
    is provided to the cloud-based services, particularly real-time and latency-sensitive
    services so that they can be executed with low latency and low network traffic.130
    Hence, fog computing ensures a heterogeneous, geographically distributed environment
    that provides better scalability, low service latency, low energy consumption,
    and operational costs for cloud-based IoT applications. These IoT applications
    generate a lot of processing requests that are decomposed into tasks and submitted
    to fog devices for processing. The order in which these tasks are processed and
    allocated to different fog devices is decided by the task scheduling algorithms
    that prioritize these tasks to optimize resource usage with minimum delay and
    cost. Several scheduling algorithms exist in the literature that use different
    scheduling approaches like traditional, heuristic-based, meta-heuristic based,
    and artificial intelligence in fog computing;131, 132 still, many important issues
    (discussed in the later half of this section) need to be explored for future research
    work while designing a scheduling algorithm in a fog computing environment. 6.1.2
    Edge computing Edge computing is a distributed computing standard that pushes
    the data processing capabilities, applications, and services from the cloud data
    centers to the edge of the network.133 The reason for doing so is the rapid increase
    of IoT and mobile devices, which has overloaded the conventional cloud computing-based
    service model resulting in poor QoS for user applications, particularly delay-sensitive
    applications. Moreover, with the assimilation of 5G technology135 in the existing
    communication network, the network providers can utilize the full potential of
    edge computing by using the radio access network (RAN)136 for providing new applications
    and services like augmented reality, video analytics, and so forth, with improved
    QoS to the end users. Although edge computing offers advantages like location
    awareness, mobility support, and low latency for IoT applications, it also has
    limitations like limited computational power, storage capacity, and energy. And,
    hence, scheduling tasks to resources and resource management in edge computing
    is an important aspect for optimal utilization of edge resources. Several resource
    scheduling approaches,137, 138 like resource provisioning, resource allocation,
    and computation offloading, are defined in the existing literature, each with
    benefits and drawbacks. 6.1.3 Serverless computing Serverless computing is an
    emerging paradigm driven largely by the industrial needs for more cost-effective
    solutions and the development of new application architectures for micro-services
    and containers.139 It provides a software development environment where the developers
    can focus on writing code for lightweight, stateless functions or cloud services
    without worrying much about the cloud resources'' operational concerns (like scaling,
    availability, managing bandwidth, etc.). Some of the major platforms providing
    serverless computing are AWS Lamda,141 Google Cloud Functions,142 and Microsoft
    Azure Functions.143 While the above characteristics of serverless computing may
    sound similar to the Platform-as-a-Service (PaaS) model, they differ in certain
    aspects described in Table 7. TABLE 7. Summary of differences between PaaS and
    serverless computing. S. No. Aspects PaaS Serverless computing 1. Scalability
    Developers have to forecast the user demands beforehand for cloud application
    services and accordingly configure the cloud application to scale up or down automatically
    Cloud applications are highly scalable and scale automatically No manual application
    configuration is required 2. Flexibility and control More flexibility and control
    over application development and deployment environment through tools provided
    by the CSPs The developer has more control over where the application is going
    to run Fewer tools are provided by CSPs for the development of functions and services
    resulting in a loss of flexibility and control No control over where the application
    is going to run 3. Pricing model Pay-as-you-go pricing model Drawback is that
    developers are charged for idle periods of the allocated server Usage-based pricing
    model whereby users only pay for the usage of the services 4. Application latency
    Applications take a longer time to be up and running Since applications are built
    as lightweight stateless functions hence take less time to be up and running As
    the request from users for cloud services or application functions arrives, the
    CSPs are required to schedule these application functions on cloud resources for
    execution such that QoS parameters are fulfilled. Various scheduling approaches
    like resource-aware, dataflow-based, and hybrid approaches are defined in the
    literature for optimal resource utilization in serverless computing.140 6.1.4
    Software-defined cloud computing The traditional data center network (DCN) consists
    of thousands of switches connecting ten thousand of servers. Using the virtualization
    technology, a server''s physical computing resources (processors) and storage
    resources are virtualized in the form of multiple VMs rented to users based on
    their needs. The DCN manages the communication between these VMs and the outside
    network through a network of switches, routers and gateways. The increased demand
    for cloud computing services has resulted in the increased traffic flow between
    VMs thus increasing the complexity of managing the traffic efficiently. Hence,
    to overwhelm the limitations of existing networks, CSPs have started incorporating
    software-defined networking (SDN)144, 145 notion into DCN. The core idea behind
    SDN is to offer a centralized global view of the whole network to manage the traffic
    flow efficiently. For this, SDN uses a programmable SDN controller, which centralizes
    the control plane of all the network resources. With centralized network information
    at hand, the SDN controller can send control packets (containing forwarding decisions)
    to the switches using a new networking protocol called OpenFlow.146, 147 SDC has
    several benefits over traditional cloud computing, like reduced energy consumption
    using network optimization techniques and improved DCN performance.148 6.2 Research
    challenges Energy-efficient workflow scheduling is a challenging research area
    whereby we need to optimize the workflow scheduling methods while minimizing energy
    consumption. Some of the research challenges that exist in this domain are listed
    below: Heterogeneous resources: Modern-day data centers constitute various heterogeneous
    components like networking devices, storage devices, CPUs and GPUs. Designing
    a scheduling algorithm for mapping workflow tasks to these heterogeneous resources
    while keeping energy consumption low and at the same time satisfying QoS parameters
    is a challenging task. Dynamic workloads: User demands in the form of workflow
    applications often have varying computing demands and nature which results in
    dynamic workloads. These workloads have rapidly changing resource demands thus
    requiring adaptive scheduling solutions that take into account these dynamic workload
    scenarios and optimize resource utilization. Multi-Objective optimization: Most
    scheduling algorithms take a multi-objective approach where more than one objective
    (like makepan, cost, deadline, energy efficiency, response time, etc.) is taken
    during workflow scheduling. Energy-efficiency objective often conflicts with other
    objectives. For instance, in order to reduce energy consumption, workflow tasks
    when run on processors with reduced frequency result in an increase in workflow
    makespan and response time. This complicates the scheduling problem at hand and
    thus requires a proper trade-off between different objectives for energy-efficient
    workflow scheduling. Reliability: Workflow applications during their execution
    are susceptible to failures. Moreover, decreasing the supply voltage for DVFS-enabled
    processors results in increased fault rates thus reducing the reliability of the
    system. Hence, balancing energy efficiency and at the same time maintaining system
    reliability presents a unique challenge for the researchers. 6.3 Future directions
    In this section, we discuss some of the research issues with respect to workflow
    scheduling in cloud computing that can be explored further by the researchers
    for their future research works. 6.3.1 Interoperability Interoperability in cloud
    computing refers to the ability of the system to communicate or move data and
    workload from one CSP or vendor having a particular cloud platform (computing
    and storage architecture) to another CSP having a different cloud platform. It
    is an important factor from the perspective of a cloud user as in the absence
    of interoperability features, a cloud user using PaaS services of a CSP has limited
    or no option to migrate his data and applications to a different CSP. This situation
    is called vendor lock-in, wherein the user is restricted from using services of
    a particular CSP, thereby under-utilizing the true potential of cloud computing
    power. One solution to this problem is using serverless computing in PaaS services
    where stateless functions are not dependent on vendor-specific execution environments
    and can move between different CSPs. Other solutions to this problem lie in designing
    new architectures, standards, and open API for the interoperability of cloud computing.
    Although many standards and open APIs addressing interoperability in cloud computing
    exist in the literature, the real inter-operable unified architecture and standards
    still need to be investigated. Moreover, both solutions require researchers to
    design and develop new scheduling algorithms that consider the changes in the
    evolving computing paradigm. 6.3.2 Green computing As discussed earlier, with
    cloud computing providing a cost-effective, scalable, and flexible computing platform
    to its users, more and more organizations are shifting their business activities
    onto cloud platforms for providing services to their clients. Moreover, in recent
    times the popularity of content delivery web services and applications providing
    rich multimedia content have forced CSPs to install servers at the network''s
    edge to deliver services with minimum latency. This has resulted in the large
    deployment of servers to meet this increasing demand, thus increasing the total
    energy consumption by the data centers. Therefore, energy-efficient scheduling
    algorithms must be developed to optimize resource utilization and decrease energy
    consumption at the data center. Recently, software-defined cloud computing (SDC)
    has opened a new scope for optimal utilization of network resources. It uses software-defined
    networking (SDN) to offer a centralized logical view of the entire network of
    a data center. This helps to efficiently monitor and manage the network traffic
    using the SDN controller. And, since the SDN controller has a central view of
    the network, it can analyze the network traffic and provide useful insights to
    the CSPs for optimal placement of switches and routers, thus reducing energy consumption.
    While SDC can provide a better alternative to traditional cloud computing, challenges
    like the scalability of SDN controller, reliability, interoperability, and increased
    implementation complexity of SDN still need to be addressed. Therefore, researchers
    need to address the above challenges while developing scheduling algorithms in
    cloud computing. 6.3.3 Machine learning Technological advances in the processing
    and storage power of computing systems have accelerated the use of machine learning
    methods in cloud computing. Machine learning methods like reinforcement learning
    (RL) and deep reinforcement learning (DRL) can be used for classifying and discovering
    new insights from large volumes of cloud data that can help to formulate better
    decision-making policies for CSPs. These learning methods use different models
    like Q-Learning, Double Q-Learning, and Markov Decision Process for the learning
    process of the agents. Although RL and DRL methods can perform better than other
    scheduling approaches like heuristic, meta-heuristic, and hybrid, specific challenges
    still need to be addressed. For instance, the high computational complexity of
    implementing DRL algorithms and the vast computing power required for processing
    huge amounts of data increased energy consumption. Hence, a balanced approach
    in developing scheduling algorithms using machine learning methods must be done
    to address the above challenges properly. 6.3.4 Security and privacy The increased
    adoption of cloud computing has provided great flexibility to its users in storing
    and accessing their data and services from remote servers without bothering much
    about their scalability and maintenance issues. While this provides a great opportunity
    to expand user businesses and services flexibly and economically, it leads to
    several security and privacy challenges like data confidentiality, data integrity,
    and data availability that need to be addressed by the CSPs. Although work has
    been done to overcome these issues with the evolution of cloud computing into
    new computing dimensions like fog computing, edge computing, and serverless computing,
    new vulnerabilities and threats need to be taken care of actively. For instance,
    edge and fog computing make data decentralized and move it from the network''s
    edge (IoT sensors) to the cloud servers. This leads to an increased risk of data
    authenticity and privacy issues and hackers exploiting the network vulnerabilities
    for distributed denial of service attacks (DDoS), resulting in degradation in
    QoS of the cloud. Another research area that needs attention is the development
    of lightweight encryption-decryption techniques that require less computing resources
    and hence more suitable in edge and fog computing environments. Moreover, in recent
    times, blockchain has emerged as a promising technology that can be explored for
    providing data security in cloud computing. What makes blockchain technology a
    popular choice are its features like a decentralized blockchain network, persistence,
    and anonymity that CSPs can utilize for providing secure data storage and services
    through the cloud. 7 CONCLUSION This paper provided a detailed survey on energy-efficient
    workflow scheduling algorithms in cloud computing. Initially, we highlighted the
    popularity and evolution of cloud computing in the modern world, followed by the
    importance of energy-efficient scheduling methods in the cloud environment. Next,
    we defined, in general, the workflow and energy model used in workflow scheduling,
    along with elaborating on different measures taken at the data center level by
    the CSPs to reduce energy consumption. Then, a detailed classification of different
    energy-aware/efficient workflow scheduling algorithms is provided whereby the
    surveyed algorithms are classified based on heuristic, meta-heuristic, AI/ML,
    and hybrid approaches. Heuristic-based workflow scheduling algorithms have less
    computational complexity than other scheduling approaches and hence have a reduced
    computation time resulting in reduced energy consumption of the host machine but
    at the cost of a non-optimal workflow schedule for the workflow scheduling problem.
    On the other hand, meta-heuristic scheduling algorithms provide a near-optimal
    solution but are hard to implement due to their high computational complexity
    and hence may consume more energy. Using AI/ML approaches in energy-efficient
    workflow scheduling has recently found more traction among researchers owing to
    their better handling of workflow tasks under dynamic workload conditions resulting
    in minimized energy consumption at the host. Hybrid scheduling algorithms combine
    the advantages of these scheduling approaches to find an optimal solution to scheduling
    workflow tasks but at the cost of increased time complexity of the algorithm.
    Further, the scheduling algorithms are analyzed based on the different methods
    used in determining their task priority, task-to-VM allocation, exploitation,
    and exploration operators so as to provide a better understanding of these algorithms,
    followed by illustrating some of the popular scientific workflow applications
    used in cloud computing. Finally, the readers are acquainted with the latest trends
    and open challenges in the cloud computing domain which can provide them with
    research directions to design better and more efficient scheduling algorithms.
    AUTHOR CONTRIBUTIONS Prateek Verma designed the research, writing-original draft
    preparation and was responsible for the final content; Ashish Kumar Maurya designed
    the paper layout and supervised the work including reviewing and editing, quality
    checking and correction; Rama Shankar Yadav supervised the work including reviewing,
    quality checking and correction. All authors read and approved the final version
    of the manuscript. FUNDING INFORMATION No funding was received for conducting
    this study. CONFLICT OF INTEREST STATEMENT The authors declare that they have
    no conflict of interest. Open Research Bibliography Citing Literature Volume54,
    Issue5 May 2024 Pages 637-682 Figures References Related Information Recommended
    Task scheduling mechanisms in cloud computing: A systematic review Aida Amini
    Motlagh,  Ali Movaghar,  Amir Masoud Rahmani International Journal of Communication
    Systems An agent‐based workflow scheduling mechanism with deadline constraint
    on hybrid cloud environment Yue-Shan Chang,  Chih-Tien Fan,  Ruey-Kai Sheu,  Syuan-Ru
    Jhu,  Shyan-Ming Yuan International Journal of Communication Systems Cost‐effective
    deadline‐aware stochastic scheduling strategy for workflow applications on virtual
    machines in cloud computing R.A. Haidri,  C.P. Katti,  P.C. Saxena Concurrency
    and Computation: Practice and Experience gSched: a resource aware Hadoop scheduler
    for heterogeneous cloud computing environments Godwin Caruana,  Maozhen Li,  Man
    Qi,  Mukhtaj Khan,  Omer Rana Concurrency and Computation: Practice and Experience
    Energy‐efficiency and sustainability in new generation cloud computing: A vision
    and directions for integrated management of data centre resources and workloads
    Rajkumar Buyya,  Shashikant Ilager,  Patricia Arroba Software: Practice and Experience
    Download PDF Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms
    of Use About Cookies Manage Cookies Accessibility Wiley Research DE&I Statement
    and Publishing Policies Developing World Access HELP & SUPPORT Contact Us Training
    and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies."'
  inline_citation: '>'
  journal: Software - Practice and Experience
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A survey on energy-efficient workflow scheduling algorithms in cloud computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kuity A.
  - Peddoju S.K.
  citation_count: '0'
  description: Reducing power consumption with tolerable performance degradation is
    a fundamental challenge in today’s containerized High Performance Computing (HPC).
    Most Dynamic Power Management (DPM) approaches proposed for the HPC environment
    are based on profile-guided power-performance prediction techniques. However,
    the complexity of DPM approaches in a multi-tenant containerized HPC environment
    (cHPCe) increases significantly due to the varying demands of users and the contention
    of shared resources. Moreover, there is limited research into software-level monitoring
    of power consumption in the popular Docker container environment since it is not
    designed keeping HPC in mind. The proposed research in this paper aims to present
    a real-time hybrid power-performance prediction approach using Long Short-Term
    Memory (LSTM) machine learning model. Unlike state-of-the-art techniques, LSTM
    with the rolling update mechanism accurately predicts the relationships between
    the tail latencies in time-series power performance data. It updates the training
    sample sequences according to the currently predicted power consumption and duration
    to predict sporadic power surges. It also proposes a power-cap determination framework
    with resource contention awareness to fine-tune power consumption in real time
    at the thread level. The proposed containerized environment is designed from scratch
    keeping HPC requirements in mind. Hence, we name the proposed approach as power-aware
    cHPCe (pcHPCe) and is evaluated and compared with native BareMetal execution using
    the NAS Parallel Benchmark (NPB) and HPC Challenge benchmark (HPCC) applications.
    Our experimental results show that the power-performance prediction model achieves
    an accuracy of 91.39% on average in real-time with an overhead of 1.6% of the
    total computing power per node. Our resource contention-aware power-cap selection
    framework attains significant power saving up to 13.1%.
  doi: 10.1007/s10586-023-04105-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Cluster Computing Article pHPCe:
    a hybrid power conservation approach for containerized HPC environment Published:
    22 July 2023 (2023) Cite this article Download PDF Access provided by University
    of Nebraska-Lincoln Cluster Computing Aims and scope Submit manuscript Animesh
    Kuity & Sateesh K. Peddoju  109 Accesses Explore all metrics Abstract Reducing
    power consumption with tolerable performance degradation is a fundamental challenge
    in today’s containerized High Performance Computing (HPC). Most Dynamic Power
    Management (DPM) approaches proposed for the HPC environment are based on profile-guided
    power-performance prediction techniques. However, the complexity of DPM approaches
    in a multi-tenant containerized HPC environment (cHPCe) increases significantly
    due to the varying demands of users and the contention of shared resources. Moreover,
    there is limited research into software-level monitoring of power consumption
    in the popular Docker container environment since it is not designed keeping HPC
    in mind. The proposed research in this paper aims to present a real-time hybrid
    power-performance prediction approach using Long Short-Term Memory (LSTM) machine
    learning model. Unlike state-of-the-art techniques, LSTM with the rolling update
    mechanism accurately predicts the relationships between the tail latencies in
    time-series power performance data. It updates the training sample sequences according
    to the currently predicted power consumption and duration to predict sporadic
    power surges. It also proposes a power-cap determination framework with resource
    contention awareness to fine-tune power consumption in real time at the thread
    level. The proposed containerized environment is designed from scratch keeping
    HPC requirements in mind. Hence, we name the proposed approach as power-aware
    cHPCe (pcHPCe) and is evaluated and compared with native BareMetal execution using
    the NAS Parallel Benchmark (NPB) and HPC Challenge benchmark (HPCC) applications.
    Our experimental results show that the power-performance prediction model achieves
    an accuracy of 91.39% on average in real-time with an overhead of 1.6% of the
    total computing power per node. Our resource contention-aware power-cap selection
    framework attains significant power saving up to 13.1%. Similar content being
    viewed by others Docker platform aging: a systematic performance evaluation and
    prediction of resource consumption Article 13 March 2022 Power Consumption Modeling
    and Prediction in a Hybrid CPU-GPU-MIC Supercomputer Chapter © 2016 Operation-Aware
    Power Capping Chapter © 2020 1 Introduction Often enormous demands flood the data
    centres with HPC facilities for flexible computing environments. To meet the user’s
    requirements, the HPC system administrators adopt a lightweight process virtualization
    technique called container to deploy customizable computing environments while
    maximizing throughput. Widely available container-based HPC solutions, like, Shifter,
    UberCloud, Singularity, and cHPCe, fulfill the demands for cloud-like flexible
    HPC without sacrificing much on performance. On the other hand, the ever-increasing
    demand for HPC infrastructure results in massive power consumption. Environmental
    Protection Agency (EPA) [1] reported that the worldwide data centers consume 220-320
    Tera Watt hours (TWh) of electricity in 2021, amounting to 0.9\\(-\\)1.3% of the
    world electricity demand. High power demand in data centers increases the overall
    operational costs and impacts the system’s performance in terms of availability
    and reliability. Therefore, the HPC community started paying attention to reducing
    power consumption in every aspect of the environment [2]. The entire HPC stack
    starting from hardware to operating system to middleware to application needs
    to be explored again from scratch, keeping power in mind, apart from other aspects
    of the environment. Moreover, optimizing application performance in a power-constrained
    environment is gaining popularity increasingly. In this respect, a fixed power-cap
    may not work well due to job-specific varying requirements. To alleviate this
    problem, research [3,4,5,6,7,8] has been carried out using either static code
    analysis or profile-guided power performance prediction approaches. The studied
    prediction models primarily use an additional system or application-level monitoring
    agent, which introduces significant overhead in a multi-tenant containerized environment.
    Performance predictability is an essential requirement for most applications in
    a power-constrained HPC environment. Most HPC applications are not designed and
    implemented, keeping power in mind. For this reason, the system architects must
    evaluate the power and performance in the early realization process based on the
    application requirements to ensure the efficient use of both resources and power.
    With the wide variety of possible system configurations and multi-dimensional
    resources, it is not easy to rapidly analyze application performance and power.
    The challenges increase significantly due to the unavailability of hardware-assisted
    methods to monitor container power consumption directly in the HPC environment.
    Substantial research [9,10,11,12] has been conducted to envisage the relationship
    between power using the performance monitoring counter and the regressive model
    to manage power consumption. However, more research is yet to be undertaken in
    containerized HPC space, especially for hardware-assisted real-time fine-grained
    software-based power monitoring and control methods without performance degradation.
    Previous research [13, 14] in this direction addressed the problem using either
    static workload characteristics or raw power/performance data from the hypervisor
    level to userspace, which incurs extra overhead to schedule the workload. They
    solved the problem either in standalone applications or Virtual Machine (VMs).
    To simplify power consumption measurement at the application thread level in a
    containerized environment, the DeepMon [15], proposed a power monitoring tool
    using unhalted_core_cycles performance counter correlation with power. It utilizes
    the ability to perform Just In Time (JIT) compilation in the recent advancement
    of the Berkeley Packet Filter (BPF) feature in Linux kernel 4.13 to develop a
    monitoring tool, which may not work in the legacy kernel used in the HPC environment.
    Similarly, WattsApp [16] also presented a software-based power prediction approach
    using WhatsUp.Net hardware to measure system-level power consumption and use resource
    usage statistics. However, using resource utilization statistics, the strategy
    provides limited insight into achieving fine-grain power estimation at the container
    level. 1.1 Motivation Most previous works used Docker as a container manager,
    which is not designed keeping HPC in mind and has several flaws to address [17].
    Also, none of the previous works addressed the contention of shared resources
    like L3 cache, a memory bus, and memory controller for co-running threads present
    inherently in a multi-tenant containerized power-constrained HPC environment in
    real-time due to unpredicted resource usage pattern [3]. Although the previous
    contention aware studies [18, 19] for shared resources used the learning phase
    for co-located applications to conclude the interference factor, which is not
    feasible in the real-time job submission perspective. Herefore, it needs an accurate
    estimation of the power-performance behavior of an application container in real-time.
    Hence, our research motivation is two-fold. Firstly, to introduce cloud-like flexibility
    in a power-constrained environment. Secondly, to add a rapid hybrid power performance
    estimation approach across various hardware resources in a containerized power-constrained
    environment. Job scheduling middleware must also incorporate a power optimization
    strategy to utilize power resources optimally, along with resource contention
    awareness, without affecting the overall performance to bridge this gap. The proposed
    work in this paper addresses all these critical concerns. 1.2 Contribution The
    significant contributions of this work are as follows: A real-time hybrid power-performance
    estimation technique of an application executing inside a multi-tenant containerized
    environment is presented using a Machine Learning (ML) methodology. A novel power-cap
    determination framework based on the application’s computation phase and system
    resources’ usage is proposed to diminish resource contention by co-scheduling
    threads without degrading much on performance in terms of overall execution time.
    The power-performance tradeoff using the NPB and HPCC benchmarks is evaluated
    in our proposed pcHPCe framework with BareMetal environment and compared with
    the state-of-the-art solution. In this paper, we construct a weighted performance
    variation table to overcome manufacturing variability. After that, our power-performance
    estimation approach evaluates the performance of an application executing inside
    a container environment using an LSTM [20] model under the power budget constraints.
    The LSTM is a Recurrent Neural Network (RNN) and is proven helpful in predicting
    power-performance relationships at the software level based on a long history
    of multivariate feature sequences and accurately capturing future trends. Previous
    work [21] reported a model called cHPCe for analytical-based data locality and
    memory bandwidth contention-aware container placement and its performance analysis.
    This paper proposed a unique pcHPCe by extending cHPCe on power monitoring and
    fine-grained power management technique at the application container level. In
    summary, the proposed work in this paper extends the previous contribution [21]
    to conserve power efficiently in a multi-tenant containerized HPC environment
    and provides an opportunity to enhance the job scheduler to achieve enhanced power
    savings. 1.3 Organization The rest of this paper is organized as follows: Sect.
    2 presents a detailed background of existing power-aware techniques in the field
    of HPC. Section 3 discusses the state-of-the-works in HPC. Section 4 describes
    the framework of the proposed pcHPCe model. Section 5 presents the preliminary
    results of memory-intensive benchmark applications. We present the details of
    the experimental setup in Sect. 5.1. Section 6 reports a summary and comparison
    of work. Section 7 concludes the work and outlines the future work. 2 Background
    Over the past few years, most researchers have focused on efficiently managing
    power without sacrificing much on performance. This section presents power conservation
    requirements in a cHPCe. It also covers the ML models studied so far for power-performance
    prediction. Our background study categorizes the discussion into two parts: Power-Constrained
    Container-based HPC and ML Models for Power-Performance Prediction. 2.1 Power-constrained
    container-based HPC Nowadays, HPC facilities are getting overwhelmed due to the
    dynamic requirements of users. The inherent overhead of hypervisor-based solutions
    to provide a flexible user environment puts a question mark on its use. The Linux
    container, a lightweight Operating System-level virtualization technology, provides
    an isolated userspace to run an application and embarks its presence to fulfill
    all the users’ needs giving comparable performance similar to the BareMetal system.
    In a virtualized/containerized environment, researchers track power and performance
    counter metrics by incorporating monitoring agents at the system level or application
    level. It may introduce significant overhead in a multi-tenant containerized environment.
    The multi-tenant environment has additional challenges due to its dynamic power
    consumption, contributing to resource sharing and workload heterogeneity. The
    challenges increase significantly due to the unavailability of hardware-assisted
    methods to monitor container power consumption directly in the HPC environment.
    In such a situation, contention for shared resources is also a limiting factor
    for overall performance due to the unpredictable use of resources among applications.
    A key concern in current-generation containerized HPC is the inherent performance
    degradation due to its resource interference and co-hosted applications [21].
    2.2 ML model for power-performance prediction Substantial research has been conducted
    to manage power consumption at a different level of the modern computing environment.
    In this regard, the performance monitoring counter and the regression models prove
    to be effective in envisaging the relationship between power performance. Earlier
    studies [9,10,11,12] on the ML algorithm already showed their effectiveness in
    predicting the trends in power and performance during application execution based
    on its characteristics. We envisage LSTM [20]—an alternative prediction model
    to the AutoRegressive Moving Average (ARMA) model—to present as a hybrid power
    consumption prediction model at the software level. Such RNNs can predict output
    based on a long multivariate feature sequence history and accurately capture future
    trends. The ML model is proven helpful in predicting power-performance relationships
    in our experiment. LSTM, with the rolling update mechanism, accurately predicts
    the relationships between the tail latencies in time-series power performance
    data. It updates the training sample sequences according to the currently predicted
    power consumption and duration. It considers job characteristics and memory usage
    to predict sporadic power surges due to a sudden large number of scientific computations
    across modules simultaneously. Section 4 provides the details of its working principle.
    Hardware performance counters monitor the occurrences of the hardware events without
    any performance penalty in modern systems. These counters can effectively estimate
    the power consumption of the different subsystems. Our experiment executes several
    micro-benchmarks to find the correlation between various performance metrics and
    power consumption. We examine the correlation of various performance metrics such
    as Instructions Per Cycle (IPC) excluding mis-speculated instructions, micro-ops
    retired per cycle, L1 cache misses, L2 hit rate, L3 total cache misses, loads
    and stores retired, branch mispredictions, average memory wait cycle to the measured
    power across few micro benchmarks. The correlation coefficient of IPC, uPC, L1
    cache misses, L2 hit rate, L3 cache misses, loads and stores retired, and memory
    wait cycle is 0.6982, 0.5546, 0.5936, 0.5428, 0.6157, 0.1964, and 0.6256, respectively.
    We also find that mispredicted branch is strongly related to the derived parameter
    memory wait cycle. We select input features for the LSTM model based on the indicative
    correlation factor. We see a strong correlation between the performance counter
    consisting of IPC, L1 and L3 cache misses, memory wait cycle, and power consumption.
    These parameters are selected for the LSTM model. 3 Related work Initially, researchers
    in this field stepped into the power conservation of the HPC environment either
    by shutting down idle resources or finding idle time to put resources in a conservative
    power state. However, the significant contributions [2, 12] in this field are
    based on application power-performance profile characterization, which mainly
    falls into these categories: static code analysis-based solution, a prior knowledge-based
    solution, profile-guided variation lookup table-based solution, an analytical
    model and empirical analysis based solution, and workload dynamics using a derived
    benchmark methodology-based solution. Static code analysis-based solutions focus
    on power-performance tradeoffs using static code analysis, which fails to provide
    actual insight into power consumption during application execution in real time.
    Based on preset voltage-frequency knowledge settings, the prior knowledge-based
    solutions put a power-cap for a specific time interval during program execution,
    which may not fit well for a new upcoming application. The approaches in profile-guided
    variation and lookup table-based solutions address power consumption using variations
    in lookup tables using static profiles without considering feedback. It might
    happen that the new application in execution does not resemble the feature of
    the applications used for generating the power-performance profile. Our proposed
    approach reassembles with workload dynamics using a derived benchmark methodology-based
    solution. 3.1 Workload dynamics using derived benchmark methodology-based solution
    Karpowicz et al. [22] proposed a customized energy-aware controller to accurately
    estimate CPU power consumption from the model-specific register. It built CPU
    workload dynamics using a derived benchmark methodology to adjust CPU frequency
    for the application-specific workload patterns dynamically. HaPPy [13] and XeMPower
    [14] models addressed the problem using either by knowing static workload characteristics
    or raw power/performance data from hypervisor level to userspace, which incurred
    extra overhead to schedule the workload. They solved the problem in standalone
    applications or VMs. Whereas Brondolin et al. [15] also proposed a monitoring
    tool to measure power consumption at the application and thread-level considering
    power-performance tradeoff. The model evaluated the power-performance tradeoff
    of a container-based distributed environment managed by Kubernetes [23]. It measured
    power consumption at the application thread level using weighted unhalted clock
    cycles correlated with power during application execution. They leveraged JIT
    compilation in the recent advancement of the BPF feature in Linux kernel 4.13
    to develop a monitoring tool, which may not work in the legacy kernel used in
    the HPC environment. WattsApp [16] also presented a software-based power prediction
    approach using WhatsUp.Net hardware to measure system-level power consumption
    and resource usage statistics. It maintained power-cap using container migration
    or deallocating resources to the container. Nevertheless, using resource utilization
    statistics, the strategy provides limited insight into achieving fine-grained
    power estimation at the container level. Holding the same standpoint, Fieni et
    al. [24] proposed a software-defined power meter, SmartWatts, to estimate power
    consumption at the granularity of the software container. They used unhalted clock
    cycle and last-level cache misses for package and Dynamic random-access memory
    (DRAM) power consumption estimation, respectively, along with hardware power monitoring
    sensors and Intel’s Running Average Power Limit (RAPL). Enes et al. [25] also
    proposed a similar power consumption prediction technique for cluster environment
    using Docker container runtime and cgroup power-capping technique to throttle
    CPU share, thereby limiting energy consumption. Table 1 depicts the summary of
    the state-of-the-art contributions in the power-aware containerization for the
    HPC environment. Table 1 Summary of Related Work on power-aware containerized
    HPC Full size table However, most previous works used Docker as a container manager,
    which needs to be designed keeping HPC in mind. In the broader perspective, the
    Docker containerized environment has several flaws that must be addressed before
    being used in HPC [17]. Very few works have been reported for power conservation
    in a containerized multi-tenant environment designed from the ground up for HPC
    purposes. Also, like most previous work on the BareMetal, we find a strong correlation
    between performance counter and power, consisting of IPC, L3 data cache misses,
    and the number of references to the last level cache. Most of the above approaches
    rely on a few system parameters and lack an accurate real-time estimation of an
    application’s power-performance behavior. Moreover, none of the previous works
    addressed the contention of shared resources like an L3 cache, a memory bus, and
    a controller for co-running threads present inherently in a multi-tenant containerized
    power-constraint HPC environment. Previous literature [21] presented a resource
    contention-aware container placement strategy for containerized multi-tenant HPC
    environment. However, it lacks an energy awareness perspective. Our work addresses
    all such concerns with a novel power-cap determination framework with resource
    contention awareness based on the application’s computation phase and system resources’
    usage at the thread level without sacrificing much on performance. 4 Containerized
    power-aware HPC The dynamic sharing of hardware resources in a multi-tenant HPC
    environment leads to complex per-task power-performance relations due to concurrent
    task execution with varying power consumption among resources. Users demand a
    flexible HPC environment like a Cloud to fulfill their execution environment needs
    similar to the home/native environment. The cHPCe: a container-based HPC environment
    using x86 and OpenPOWER systems [17] meets the users’ requirements without sacrificing
    much on performance compared to the BareMetal environment. In this environment,
    a group of processes is isolated from another group of processes that constitute
    an individual application that executes on the available hardware thread densely.
    Hence, predicting the fine-grained power consumption in such an environment introduces
    an extra layer of complexity. 4.1 Calibration to mitigate manufacturing variability
    Earlier, researchers in the HPC field felt that processors on a single die from
    a particular manufacturer behave similarly to other processors. However, recent
    research [27] shows that some processors are less power efficient due to semiconductor
    process variations, even with the exact architectural specification. This power
    variation among processors leads to performance variation even in perfectly balanced
    HPC applications. In a power-constraint environment, this manufacturing uncertainty
    will incur performance invariability where we enforce the hardware to power-cap
    to conserve the power. To alleviate this uncertainty, we calibrate the power-cap
    value for each CPU like the previous study [7, 27]. However, we use a few widely
    accepted representative benchmarks that stress different subsystems, such as CPU,
    DRAM, and IO of an environment than the earlier study with a single micro-benchmark.
    This hybrid benchmark strategy describes the power-performance behavior of the
    system more accurately. The proposed pseudo-code to overcome manufacturing variability
    is given in Algorithm 1. The calibration for manufacturing variability model selects
    each representative micro benchmark to execute on different sockets individually
    with different power-cap settings. It collects the power and performance of each
    execution against each power-cap and store it in the database. The average performance
    of all benchmarks is calculated using 20 times repeated execution samples. In
    our experiment, we use a collection of representative micro benchmarks that exhibit
    compute, memory, and IO boundness to capture the power characteristics of CPU
    and DRAM with minimal prediction error. We maintain the power-performance variation
    table against the CPU socket and power-cap for each benchmark. The job scheduling
    module utilizes the suitable power-performance variation table entry while putting
    power-cap constraints. The list of notations used throughout the paper is given
    in Table  2. Table 2 List of notations Full size table The power-performance variability
    impacts CPU power consumption significantly, making it extremely difficult to
    predict application-level power performance across different computing elements.
    We also spot that performance variation worsens with a strictly enforced power-cap.
    Our proposed algorithm helps to mitigate performance inhomogeneity in a power
    constraint containerized HPC environment. Our solution uses representative benchmarks
    to estimate performance variability, which may not have online application characteristics
    in a remote scenario. An approach to deal with such a scenario is to refine the
    Performance Variation Table (PVT) as and when a new application enters dynamically.
    4.2 Predicting power-performance in a cHPCe Very few research has been reported
    to date for a power-aware containerized HPC environment. Considering the imperfection
    and shortcomings of the existing implementations, we propose a prototype of a
    dynamic pcHPCe in Fig. 1. We inherit the cHPCe model [17] with power-aware capability.
    Several modules of the proposed model are discussed here. 4.2.1 User environment
    In the proposed pcHPCe model, users prepare the runtime image for container instantiation
    describing applications’ requirements. Users push the image(s) to the private/public
    repository to be used later for creating a containerized environment for their
    application execution. 4.2.2 User interface Users log into the job submission
    site using their credentials and submit the job describing all the QoS requirements,
    mainly the deadlines to complete the job and runtime images to be used for container
    instantiation. 4.2.3 Orchestration The job scheduler maintains a queue of jobs
    and the total allotted power budget for the environment. We modify the widely
    accepted Simple Linux Utility for Resource Management (SLURM) [28] job scheduler
    to determine the CPU topology accurately. It pins a particular migrated application
    thread to the hardware thread in case of colocated applications’ threads interference
    beyond an acceptable threshold by considering predicted performance interference
    using the average memory wait cycle. The job scheduler invokes the container manager
    to create a containerized environment for an application and maintains the currently
    available power budget for the environment. It also gives the predicted power-cap
    to the container manager to throttle a particular package and DRAM power corresponding
    to the specific job. 4.2.4 Decision In the proposed pcHPCe model, the decision
    module mainly comprises two components: the container manager and the power-performance
    prediction model. It accurately predicts the power-performance relationship of
    an application container running on a system and makes decisions for the imposed
    power-cap on the CPU and DRAM. 4.2.5 Container manager The primary function of
    the container manager is to facilitate the container runtime environment by invoking
    the cHPCe daemon. It receives a list of selected compute nodes and a power-cap
    at runtime from the job scheduler. It calls the performance and power daemon in
    the compute node to capture instantaneous performance and power consumption in
    real-time during application execution. Fig. 1 High-level prototype for pcHPCe
    Full size image 4.3 Power-performance prediction agent A power-performance prediction
    model envisages the power-performance relationship using a multi-step, multi-lag,
    multi-variate LSTM with fine-tuning in real-time. It receives the accumulated
    performance metrics and instantaneous consumed power from a shared location to
    be input to the model. We train the LSTM model with performance metrics and the
    consumed power of several representative benchmarks to stress different subcomponents
    of the computing system. Our model uses hardware events and consumed power to
    predict power-performance relation at thread level mapped to a container to provide
    a fine-grain software-level solution. Figure  2 presents the detailed working
    principle of the real-time power-performance prediction model backed by LSTM of
    the proposed pcHPCe, as shown in Figure  1. The traditional LSTM model is unable
    to predict large power swings. Therefore, a standard decomposition-based prediction
    approach can not apply. The Rolling update optimization mechanism is utilized
    with LSTM to update the training sample sequences according to the current predicted
    power consumption and duration. It considers job characteristics and memory usage
    patterns to predict sporadic power surges due to a sudden large number of scientific
    computations across modules simultaneously. The pseudo-code to predict the real-time
    power performance of an application is given in Algorithm 2. The real-time power-performance
    prediction algorithm consists of a hybrid offline and online approach to improve
    the convergence speed of the learning phase with optimal prediction accuracy.
    It first evaluates the execution time of all the representative micro-benchmarks
    stressing different system sub-components with the optimal number of CPUs using
    no power cap and stored in the database. We calculate the average performance
    of all benchmarks using 20 times repeated execution samples. Various power-cap
    starting with minimum power-cap value or the training samples are created using
    different applied power-caps, corresponding consumed power, and Performance Monitoring
    Counters (PMCs) applied power-caps, along with corresponding consumed power and
    PMCs, aid in creating the training samples. The algorithm converts time series
    profile data into supervised learning model data and partitions the data into
    train and test sets. The model further takes feedback measurements when a new
    job comes to improve long-term contextual time series information accuracy. The
    training process stops when Root Mean Square Error (RMSE) is below a certain threshold.
    This trained model predicts performance for a given power-cap value in real-time
    during application execution. Fig. 2 Working principle of LSTM Full size image
    Using a stochastic approach, our proposed multi-variate LSTM-based real-time power-performance
    prediction model accurately estimates input and output relationships. It utilizes
    monitored present and past hardware event matrices and consumed power at the application
    container thread level. The proposed method is application agnostic and estimates
    the power consumption much faster than the measurement sample without tapping
    the internal system power line. The model’s convergence and prediction accuracy
    can be further improved by investigating different model topologies, dropout layers,
    and optimizers associated with varying hidden and unit layers. 4.4 Compute node
    The compute node mainly covers performance and power daemons, and cHPCe, where
    the job executes inside a containerized environment. A bridge network connects
    all the compute nodes using a dedicated network channel. 4.4.1 Performance daemon
    It is a very lightweight daemon to capture performance-related metrics in each
    compute node to obtain the runtime performance metrics related to all jobs executing
    on the node. We use the Performance Application Programming Interface (PAPI) [29],
    a cross-platform interface due to the support of Graphics Processing Unit (GPU)
    and other component plug-ins for future use, to get available PAPI preset and
    user-defined events of interest. To monitor the hardware performance metrics of
    a CPU Socket housing multiple threads, we maintain a thread for accumulating performance
    counter values of all threads using a shared memory concept for performance reasons.
    These real-time consolidated performance metrics with instantaneous power consumption
    corresponding to each package are given to the power-performance prediction agent.
    4.4.2 Power Daemon Power daemon resides in each compute node to capture the instantaneous
    power consumption at the package and DRAM level in millisecond intervals. We utilize
    RAPL [30] interface by reading values from the sysfs power-cap interface. It also
    puts the power-cap value received from the container manager by writing power-limiting
    interface files of the Kernel at package and DRAM level for the predicted time
    interval. The dedicated threads driven by signal carry out all such activities
    for non-interrupted service by pinning to a particular hardware thread. 4.4.3
    cHPCe The cHPCe daemon activates whenever a new job request comes from the container
    manager. It creates a container environment by using the image from the shared
    storage and forms a bridge-based overlay network among the containers for a specific
    application. 4.5 Power-cap selection in job execution To conserve power in a power-constraint
    multi-tenant cHPCe, we need an efficient fine-tune power-cap selection algorithm
    during application execution. The cHPCe needs to deal with various user applications
    with unique characteristics. A universal power-cap selection strategy may not
    work well for all applications in such a situation. To date, most researchers
    emphasize the power conservation of a single node executing one similar job at
    a given time interval. However, we propose a power-cap determination methodology
    based on the applications’ computation phase and system resource usage without
    degrading much performance in a multi-tenant containerized HPC environment. The
    pseudo-code of selecting the power-cap of an application in real-time is given
    in Algorithm 2. The power-cap selection module calculates the colocated thread
    interference factor using Eq. (4) and (5) supported by the analytical interference
    model [21]. In our proposed pcHPCe framework, the job scheduler migrates a thread
    when the colocated threads’ interference exceeds the empirically determined threshold.
    It employs the Checkpoint/Restore In Userspace (CRIU) thread migration strategy,
    an approach to save thread context, transfer, and restore thread context in the
    migrated host without violating power budget constraints. Otherwise, the predicted
    socket power is adjusted to accommodate the impact on energy consumption due to
    the change in memory usage pattern, determined from the regression model. The
    job scheduler calculates the final predicted power taking manufacturing variability
    into account. It also determines the power-cap duration from applications’ phase
    behavior and time interval. The power-cap daemon receives the final predicted
    power-cap and duration for the application container from the job scheduler maintaining
    power budget constraint. Our model presents the impact on energy consumption due
    to colocated application container threads. In this context, the job scheduler
    tries to assign all the threads belonging to a particular application container
    to a single socket/die as densely as possible to achieve acceptable application
    performance by minimizing contention of the shared resource and suitable power-cap
    enforcement. The individual benchmark application runs on demanded resources to
    collect individual resource pressure in a contention-free environment. Then it
    is executed with different co-hosted application benchmarks to observe performance
    degradation. Our empirical analysis shows a strong correlation between the memory
    wait cycles and energy consumption of the processor socket due to the wait state
    causing a slowdown of the clock. The penalty of the last level cache misses, which
    causes the memory wait cycle to introduce in the execution path, can reach up
    to the typical 100 cycles in the modern processor. More memory access delays indicate
    less processor power consumption. Idling the processor during memory access time
    results in lower dynamic processor power use, thus leading to a lower average
    system power usage. The memory wait cycle depends on phase behavior and the instruction
    executed for all the co-hosted application containers. We utilize an analytical
    model [21] backed by the empirical study to determine the resource interference
    characteristics from the average memory wait cycle by co-hosted applications.
    However, the contributing factor to the average memory wait cycle due to different
    memory types is left unexplored. The future work also targets to address the contention
    due to communication. Using a regression model, the mentioned numbers in Algorithm
    3 are obtained from an empirical study like based on the change in memory usage
    impact on energy consumption. We perceive that creating the checkpoint for thread
    migration, saving it into faster storage, and restoring it to the destination
    node/socket takes an order of tens of milliseconds, as studied in previous literature
    [16, 26]. Our empirical study using a regression model for colocated threads interference
    follows a similar trend as derived below: The total power consumption of the processing
    core of a socket, \\(P_{s}\\) is calculated using $$\\begin{aligned} \\begin{aligned}
    P_{s} = P_{c} + P_{m} \\end{aligned} \\end{aligned}$$ (1) where \\(P_{c}\\) is
    the computational power consumption with data residing in register/cache, \\(P_{m}\\)
    represents the computation power requirement for retrieving/storing data from/to
    memory. Each socket has a private L1 and L2 cache and access to a shared L3 cache
    and memory. The computational power consumption due to memory access can be calculated
    by adopting the equation $$\\begin{aligned} \\begin{aligned} P_{m} = \\sum _{i=1}^{T}
    \\sum _{j=1}^{d} \\sum _{k=1}^{m} S^{i}_{j} * u^{j}_{ik} * p_{ik} \\end{aligned}
    \\end{aligned}$$ (2) where T, d, and m are threads, data access from memory, and
    memory partition. I, j, and k are used for the corresponding indices. \\(p_{ik}\\)
    denotes the computational power consumed when a unit of data is accessed from
    \\(m_{ik}\\), \\(S^{i}_{j}\\) represents jth data size retrieved/ stored by an
    ith thread, \\(u^{j}_{ik}\\) maintains unity when data is available in the memory,
    \\(m_{k}\\) otherwise 0. The \\(p_{ik}\\) can be formulated by applying the equation
    $$\\begin{aligned} \\begin{aligned} p_{ik} = intrfnc^{sckt}_{CoLocThr_{i}} * PowToAcessMem
    * T_{i} \\end{aligned} \\end{aligned}$$ (3) where \\(intrfnc^{sckt}_{CoLocThr_{i}}\\)
    indicates the colocated thread interference induced by an \\(i_{th}\\) thread
    in the socket. PowToAcessMem denotes power consumption to access memory. \\(T_{i}\\)
    is the total execution time of the ith thread. Consider a set of n threads in
    a particular socket. \\(\\Psi (TchngMemWaitCycl)\\) calculates the total change
    in memory wait cycles for n threads in a specified time frame, t due to the change
    in memory wait cycles of ith thread. $$\\begin{aligned} \\begin{aligned}&\\left.
    \\Psi (TchngMemWaitCycl) = \\right. \\\\&\\quad \\sum _{i=1}^{n} \\frac{\\delta
    (ChngMemWaitCycl_{i}) (LLCMissRate^{*}_{i})}{\\delta (t)} \\end{aligned} \\end{aligned}$$
    (4) Where \\(LLCMissRate^{*}_{i}\\) represents the last level cache miss rate
    of ith thread over t time interval of n threads. Finally, \\(intrfnc^{sckt}_{CoLocThr_{i}}\\)
    can be calculated in a specified time interval, t using equation $$\\begin{aligned}
    \\begin{aligned} intrfnc^{sckt}_{CoLocThr_{i}} = \\frac{(\\Psi (TchngMemWaitCycl))}{(\\delta
    (t))} \\end{aligned} \\end{aligned}$$ (5) In the final power-cap selection given
    n modules and their associated module power, the job scheduler maintains the following
    invariability (ignoring the static part of power consumption, which remains constant
    throughout application execution) $$\\begin{aligned} \\begin{aligned}&Pow_{cap,
    module} = Pow_{cap, CPU} + Pow_{cap, DRAM}\\\\&Pow_{budget} \\le \\sum _{j=1}^{n}
    Pow_{cap, module}^{j}, and PowCap_{module}^{min} \\end{aligned} \\end{aligned}$$
    (6) Algorithm 4 describes the detailed procedure of selecting power-cap by the
    job scheduler. The selected power-cap scheduling module first calculates available
    power for the ith module by subtracting the total consumed power of all modules
    from the power budget at the time of the application container scheduling instance.
    If the available power for the ith module is greater than the final predicted
    CPU and DRAM power-cap for the application container, then allocate the selected
    power-cap for CPU and DRAM for the predicted duration. Otherwise, calculate the
    available power-cap for the CPU by subtracting the final predicted DRAM power
    from the available power for the ith module. If the performance variation due
    to the selected power-cap for CPU and DRAM is within the constraint of QoS requirement,
    then allocate the selected power-cap for CPU and DRAM for the predicted duration.
    The database also stores information regarding the module running with a throttle
    power-cap, which shall be appropriately adjusted in the next scheduling interval.
    If the performance variation cannot fulfill the required QoS requirement at that
    time instance, allocate the application container with minimum CPU and DRAM, and
    wait for the next power-cap determination slot. Our proposed power-cap selection
    scheme enforces the desired power-cap based on the applications’ computation phase
    and system resources usage while maintaining power budget constraints. This process
    utilizes colocated thread interference with applications’ computation phase to
    enforce a power-capping limit, unlike other methods in literature mainly based
    on power budget constraint violation. Our proposed pcHPCe solution applies a reduced
    power-cap to maintain the total power consumption by all running containers within
    the power budget constraint with tolerable performance degradation. It is a lightweight
    process compared to the presently available methods, such as deallocating resources
    from containers or the restore-checkpoint technique used in practice. In the power-cap
    selection scheme, contention due to communication interference is left unexplored.
    5 Performance evaluations This section presents the results obtained from experiments
    conducted on the proposed pcHPCe. We evaluate the performance of our proposed
    environment on a cluster of x86 nodes allotted for our analysis in the HPC Lab.
    Since power usage by system components other than the CPU package and DRAM, such
    as Disk, motherboard, and NIC, is almost static, we calculate the total available
    power budget for distribution by subtracting this offset power from absolute power.
    Our investigation does not consider power consumption by the cooling system and
    power source because they are machine-dependent, typically constant, and outside
    of the scope of this paper. 5.1 Experimental testbed In this section, we summarize
    the experimental environment, as shown in Table 3. The smart manage 10-Gigabit
    network switch interconnects all the nodes in the environment. We deploy a cHPCe
    comprising x86 systems. The cHPCe uses the Linux kernel feature, namely namespace,
    as in previous work [17]. In this work, we enhance the widely popular workload
    manager, SLURM, to detect the used x86 processor architecture and pin a particular
    application thread to a hardware thread. SLURM is an open-source, highly configurable
    workload manager and allows modifying the scheduler to introduce the power awareness
    concept. The power-performance monitoring ability of the environment is also included
    using a plug-in to it. The programming interface PAPI helps us track our interest’s
    application performance metrics using preset and user-defined events. We implement
    a performance daemon, which actively monitors the performance of each thread on
    a socket specific to a job. We also build a power daemon that monitors and controls
    the power-cap for a compute node. The RAPL assists us in monitoring instantaneously
    consumed package and DRAM power in real-time. Table 3 System Characteristics Full
    size table 5.2 Overhead analysis Our pcHPCe introduces some overhead in compute
    nodes due to monitoring and controlling power using a dedicated daemon process.
    The power monitoring daemon thread executes on a dedicated hardware thread to
    capture instantaneous power consumption at package and DRAM levels in a one-millisecond
    interval. The captured power consumption information is written to fast shared
    storage, which the power-performance prediction agent utilizes. A pre-assigned
    thread is executed after a predicted time interval by a signaling mechanism to
    control the power-cap by writing the power interface file of the Kernel. The communication
    latency of reading and controlling power by the daemon process is nearly negligible
    due to fast shared storage among nodes and time-driven signaling mechanisms. To
    assess overhead due to the dedicated daemon process, we collected the execution
    time of several benchmarks with and without the daemon process. Furthermore, the
    experiments present how much additional load the proposed framework introduces
    to the BareMetal environment. The performance of all the benchmarks executing
    20 times repeatedly with a \\(95\\%\\) confidence level is reported. The computing
    overhead of power monitoring and controlling daemon is about \\(1.6\\%\\) of the
    total computing power per node, as shown in Fig. 3, which is insignificant, considering
    power-saving and overall negligible performance degradation, as discussed in Section  5.5.
    For the FT benchmark, due to the identical iteration of spikes and valleys with
    regular patterns, which are interleaved with different computation phases, monitoring
    and controlling daemon takes \\(0.6\\%\\) more computing power than other benchmarks.
    It consumes \\(0.5\\%\\) less computing power due to the compute-intensive nature
    of LU with uniform patterns. Fig. 3 Execution time overhead Full size image 5.3
    Power-performance variation and calibration In this section, we present CPU power
    and performance measurement across all the nodes at the thread to socket level
    in detail. We measure the module-level power variation and its impact on the performance
    of different benchmark applications such as MT-DGEMM, STREAM, GRAPH500, b_eff,
    and NPB3.3-MPI in different power-cap scenarios. Each socket of computes node
    executes every benchmark application separately with varying power-cap conditions,
    from 40 watts up to 130 watts. The y-axis represents the normalized execution
    time by considering no cap power condition, whereas the x-axis reports the module-level
    power consumption in watts. The power-performance (P-P) variation result has been
    reported for individual benchmark applications with performance normalized to
    no power-cap condition. In our experiments, median values of the probability density
    distribution of the obtained results over 20 repetitions are considered to plot
    the graphs. To reduce space usage, we only show the representative benchmark result
    plot (the detailed results are not demonstrated in this paper). 5.3.1 Stream benchmark
    A simple synthetic benchmark, Stream measures the sustainable memory bandwidth
    of the computing system. It mainly comprises four computation kernels, such as
    ‘COPY’ to assess transfer rate without arithmetic operation, ‘SCALE’ to perform
    a simple arithmetic operation, ‘SUM’ to check multiple load and store operations,
    and ‘TRIAD’ to allow overlapped/ chained/fused add/ multiply operations. It calculates
    the effective memory bandwidth using Equation (7). $$\\begin{aligned} \\begin{aligned}&\\left.
    COPY: c = a, \\right. \\\\&SCALE: b = \\alpha c, \\\\&ADD: c = a + b, \\\\&TRIAD:
    a = b + \\alpha c.\\\\&where \\ a, \\ b, \\ c \\ are \\ vectors \\ and \\ \\alpha
    \\ is \\ a \\ scalar. \\end{aligned} \\end{aligned}$$ (7) Fig. 4 P-P variation
    in stream benchmark Full size image We see from the Stream benchmark experiment
    in Fig. 4 that the benchmark performance degrades if we apply a power-cap less
    than 70 watts to any package. We do not observe any additional benefits by putting
    a power-cap of more than 80 watts. Therefore, power-cap variations indicate performance
    invariability for a value less than 60 watts with a worst-case performance variation
    of 0.05 to 0.21 relative to the no power-cap scenario. We also witness that in
    socket0, all computing systems give the same power-performance invariability,
    whereas in socket1, all systems show similar behavior. 5.3.2 Graph500 This benchmark
    stresses the communication subsystem. It mainly consists of three computation
    kernels. The first kernel forms an undirected graph, the second performs a Breadth-First
    Search (BFS), and the third kernel constructs all single-source shortest path
    computations on the chart. Figure 5a shows that reducing the CPU power-cap value
    from 70 watts to 50 watts increases the worst-case performance variation of 0.14
    to 0.16 relative to the no power-cap scenario, indicating a 16% difference in
    CPU frequencies across packages at 50 watts. We also notice almost no performance
    variation across the same CPU packages with higher power-cap values. A worst-case
    performance variation of 0.03 relative to the no power-cap scenario is seen. Fig.
    5 P-P variation in a Graph500. b mt-DGEMM Full size image 5.3.3 mt-DGEMM A simple
    multithreaded dense matrix multiplication benchmark, mt-DGEMM, captures the sustainable
    floating-point computational rate of double precision matrix-matrix multiplication
    of a single node. It maintains the following computational kernel $$\\begin{aligned}
    \\begin{aligned} C = \\alpha AB + \\beta C; \\end{aligned} \\end{aligned}$$ (8)
    where A, B, and C are matrices of the dimensions \\(M \\times K, K \\times N\\),
    and \\(M \\times N\\), respectively. Figure 5b shows that reducing the CPU power-cap
    value from 70 watts to 50 watts increases the worst-case performance variation
    of 0.08 to 0.11 relative to the no power-cap scenario, indicating an 11% difference
    in CPU frequencies across packages at 50 watts. We notice almost no performance
    variation across the same CPU packages. 5.3.4 NAS parallel benchmark We evaluate
    the performance of a highly parallel supercomputer using the NAS parallel benchmark
    called NPB3.3-MPI. It comprises five kernels and three pseudo applications to
    assess the computation and data movement characteristics of the computational
    fluid dynamics (CFD) applications. The computation kernel, Embarrassingly Parallel
    (EP), generates independent Gaussian Random variates using Marsalia polar methods.
    Conjugate Gradient (CG) focuses on random memory access and communication. It
    also contains a 3D discrete fast Furious Transform (FT) for all to all communications.
    Block Tri-diagonal solver (BT), Scalar Penta-diagonal solver (SP), and Lower-Upper
    Gauss-Seidel solver (LU) are three pseudo applications in the benchmark. We note
    from the result that reducing the CPU power-cap value from 70 watts to 50 watts
    increases the worst-case performance variation of 0.047 to 0.048 for the SP benchmark,
    relative to the no power-cap scenario, indicating a maximum of 6.4% difference
    in CPU frequencies across packages at 50 watts. We also perceive almost no performance
    variation across the same CPU packages with higher power-cap values, whereas the
    worst-case performance variation of 0.028 when compared to the no power-cap scenario.
    We notice the power variation across and within the package in the experiments,
    leading to performance variation. We also notice that sometimes these power variations
    do not necessarily correlate with performance variation. This performance variation
    becomes more prominent when we put the power-cap. Hence, the proposed pcHPCe framework
    considers performance variation while putting power-cap to conserve power. Our
    experimental results conclude that a single power-performance variation relation
    may not fit all the application characteristics equally. Hence to deal with different
    features of applications, we generate different PVT, as shown in Table 4, by considering
    different categories of benchmarks and using corresponding PVT. Table 4 Performance
    variation table (PVT) Full size table 5.4 Power-performance prediction model evaluation
    In this sub-section, we evaluate the performance of the implemented power-performance
    prediction model. We utilize a multi-step, multi-lag, multi-variate LSTM model
    with fine-tuning to predict instantaneous power-performance relations in real
    time. We run Stream, mt-DGEMM, b_eff, Graph500, and HPL benchmarks inside a cHPCe
    twenty times each to collect repeat power-performance profiles. The power- and
    performance-daemons capture all the performance-related metrics such as TOT_CYC,
    L1_DCM, L3_TCM, MEM_WCY, instantaneous package, and DRAM power consumption during
    execution. The sampling interval of all the selected parameters is 1 ms, captured
    by power and performance daemon. These daemons consolidate the power-performance
    metrics and write them into a shared location for input to the power-performance
    prediction model. Our LSTM model converts time series profile data into supervised
    learning model data and partitions the data into train and test sets. In this
    step, we evaluate the model accuracy starting from one-to-twenty millisecond past
    profile data to predict one-to-two millisecond future data. The LSTM network is
    built after transforming the data into the LSTM format. In multiple and dropout
    core layers, we use neurons of sizes 75, 50, and 30 in the model building. The
    proposed model utilizes the Adam optimizer and Mean Absolute Loss Function to
    measure the magnitude of errors. First, the repeated profile data of each benchmark
    train the model, and further training applies to the saved model with another
    set of benchmark profile data. The model’s accuracy is evaluated with 10-fold
    cross-validation. Our final power-performance model achieves a validation accuracy
    of approximately 91.39%. When using the LSTM with a window size of 10 and offset
    size of 2, the Mean Absolute Percentage Error (MAPE) and the RMSE are approximately
    2%\\(-\\)3.29% and 8.06%, respectively. Figure 6 shows the training data and the
    corresponding validation accuracy. Our model takes past performance and power
    data for twenty milliseconds and predicts two-milliseconds power performance with
    an approximate accuracy of 91.39%. Fig. 6 Typical LSTM a input runtime performance
    metrics. b Power-performance prediction Full size image 5.5 Power-cap selection
    algorithm evaluation This subsection presents the power-cap evaluation results
    for the NAS parallel and HPCC benchmarks. The CPU’s Thermal Design Power (TDP)
    is 95 watts and 20 watts for the DRAM in each system. We find these from the PACKAGE_ENERGY_PACKAGE
    and DRAM_ENERGY_PACKAGE model-specific Registers (MSR). Each node with two CPU
    sockets consumes 230 watts. The experimental environment is allotted a power budget
    of 1.65kW, including all system subcomponents. Assume that other components of
    the system draw constant power for our experiment. Initially, the job scheduler
    allocates a uniform power budget across all the sockets. The job scheduler dynamically
    divides the total power budget to all the containers following the proposed strategy
    according to their aggregated energy requirements. We calculate the total power
    budget allocated to each job by multiplying the number of CPUs with the power-cap
    assigned to each CPU which can satisfy the performance requirement of the job.
    5.5.1 NAS parallel benchmark We select BT and LU of the NAS parallel benchmark
    to evaluate the performance of the proposed Power-cap selection algorithm. Due
    to space constraints, we show the representative result plot only. We also analyze
    the performance of these two pseudo applications relative to BareMetal performance.
    The implemented power-performance prediction model receives the performance metrics
    and power consumption from the performance and power daemons, respectively. It
    can accurately predict power performance with high accuracy. However, we train
    the prediction model in a stacked manner with all the representative benchmarks
    to stress different system subcomponents. The expected power does not reflect
    the interference caused by the colocated applications. Hence, we utilize colocated
    thread interference using the average memory wait cycle with a threshold from
    empirical study, phase behavior of the application, predicted power to determine
    the power-cap, and duration for the next time interval of applications execution.
    Figure 7a presents the behavior of Memory Wait Cycles (MWC) to socket power consumption
    in a typical LU application benchmark execution using a different package power-cap.
    Figure 7b represents typical colocated threads interference in clusterhost1. Figures  8,  9,
    and  10 demonstrate the evaluated power-cap for the next time interval of applications;
    LU and BT using Power-cap selection algorithm. The execution duration of all the
    benchmarks is in the order of minutes. However, we zoom all the result plots to
    better visualize the effectiveness of our proposed pcHPCe framework due to the
    millisecond scale power-cap imposed. Fig. 7 a MCW vs. socket power consumption.
    b Colocated threads interference in clusterhost1 Full size image Fig. 8 Power-cap
    selection a Host1 Socket1. b Host2 Socket0 for BT Full size image Fig. 9 Phase
    behavior of LU benchmark Full size image Fig. 10 Power-cap selection a Host1 Socket0.
    b Host2 Socket1 for LU Full size image In the case of the bt application benchmark,
    for example, \\(k_{th}\\) execution time instance, the power-performance prediction
    model predicts the package power to be 50.537 watts. We study empirically that
    colocated application interference during execution can be estimated by average
    global memory wait cycles. We also observe that an increase of 1500 average global
    memory wait cycles decreases package power consumption by approximately 1.528
    watts at the beginning of the subsequent application phase. Hence, the total power-cap
    decided by the container manager is (50.537—4.584)/1.065 = 43.15 watts after incorporating
    manufacturing invariability. The job scheduler evaluates the power-cap for the
    socket according to the current power budget availability and QoS requirements
    of maintaining applications. The application phase behavior decides the power-cap
    duration based on IPC and L3 average Total Cache Miss (TCM) because giant cache
    misses usually indicate CPU is in a non-compute intensive phase. We retain the
    DRAM power-cap in a coarse-grained manner due to its frequent change that may
    introduce unpredictable performance behavior. To maintain power consumption constraints,
    the container manager finally sends these power-cap values and time duration to
    the power daemon in the compute node. Figure  8 presents the power-cap selection
    for BT benchmark, and Fig. 10 for LU benchmark using proposed power-cap determination
    framework based on the application’s computation phase and system’s resources
    usage to diminish resource contention by co-scheduling container threads. Before
    running the benchmark, the power-cap selection agent enforces the minimum possible
    power-cap in an idle state due to the running power and performance daemons. Once
    the application is submitted to execute, the orchestrator imposes a strict power-cap
    with target performance and power budget constraints. Figures  8 and 10 depict
    the achieved significant power-cap selection accuracy. We analyze behavior of
    colocated threads and power consumption on clusterhost1, socket0 and socket1,
    respectively, for LU and BT benchmarks. We notice that CPU and DRAM utilization
    increases despite the decrease in power consumption. It is due to the strict enforcement
    of power-cap with performance degradation limit using the MSR register. Of course,
    the strict enforcement of the power-cap does not change the application’s overall
    performance with a slight increase in execution time. Figures  8 and  10 show
    that the orchestrator starts with the minimum possible power-cap since the system
    is in idle state. Then, it captures the application’s behavior and allocates the
    power budget to run its maximum. We note that BT consumes more DRAM power than
    others, which leaves the orchestrator less opportunity to save power since strict
    power-cap in DRAM may have an adverse effect. Moreover, the power-cap selection
    agent exploits core power for LU applications opportunistically, resulting in
    power saving with minimal performance loss. However, there are instances where
    our model shows apparent deviation from real power consumption-for example, 2
    s and 17 s of BT benchmark and 7 s in LU benchmark in clusterhost2. Our model
    takes average power during this time range while actual power consumption oscillates
    in nature. By analyzing the IPC pattern, we conclude that relative IPC was high
    around this time frame, contributing to this deviation. 5.5.2 High performance
    computing challenge benchmark HPC Challenge benchmark [31] examines different
    subsystems of the participating nodes, such as processor, memory, and interconnect,
    to evaluate the performance of the environments made of BareMetals and containers.
    It consists of seven benchmarks: HPL, DGEMM, FFT, PTRANS, STREAM, RandomAccess,
    and b_eff Latency/Bandwidth. To fulfill the benchmark evaluation criteria, we
    use the largest problem size that occupies 70%, which is around 90GB, of the total
    memory for all the experiments. The block size and problem size are of the order
    of 100 and 100000, respectively. The process grid ratios of 1: 4, 1: 8, 1: 16,
    1: 24, 1: 32, 1: 48, 1: 56, and 1: 64 are input to assign to the hardware thread.
    We use the HPCC benchmark to provide performance bound for real applications with
    its unique power consumption profile and power-cap selection exercise by our proposed
    scheme. All the plots present the different parts of the benchmark execution on
    nodes. 5.5.3 HPL and StarDGEMM Figures  11 and  12 show the performance, power
    consumption, and power-cap selection by the proposed scheme for HPL and Star DGEMM
    benchmarks, respectively. HPL and Star DGEMM show a high spatial and temporal
    locality. They spend most of the time exercising the processor’s floating-point
    unit and little time on memory. The processor consumes more power compared to
    other tests. HPL solves a dense linear system of equations using LU factorization
    with a partial row pivoting method interleaved with short communication. We discover
    very high variation in processors’ power consumption, reaching up to 150 watts,
    and it can drop as low as 70 watts during short communication. We do not notice
    much fluctuation in memory power consumption due to high spatial and temporal
    memory locality, and it does not incur cache misses frequently. Hence, our proposed
    scheme correctly identifies the application computation phase and optimized power
    consumption by selecting the power-cap accordingly. Fig. 11 a Performance. b Power-cap
    selection for HPL Full size image Fig. 12 a Performance. b Power-cap selection
    for StarDGEMM Full size image 5.5.4 PTRANS PTRANS investigate the system interconnects
    total communication capacity by exchanging messages simultaneously between each
    pair of processors. Figure  13 presents the performance, power profile, and power-cap
    selection by the proposed scheme. A matrix of size 50000 × 50000 processes is
    taken as an input. PTRANS power profile consists of spikes in the computation
    phase and valleys in the systemwide significant data movement phase. Power consumption
    varies within the iteration also. We observe stable memory power consumption due
    to high spatial memory locality and relatively low cache misses. Our scheme correctly
    identifies (i) sufficiently long communication phases that reduce power consumption
    and (ii) data movement-dependent computation phases that perform matrix transposition
    resulting in a lowering of power consumption. We conduct experiments to estimate
    the performance based on injection rate in the number of messages/second. The
    injection bandwidth capability into the network fabric is 3.2GBs for our experiments.
    We conduct the experiments by varying levels of injection bandwidth degradation
    like half, quarter, and eighth in terms of the complete compute node’s injection
    bandwidth when the message size is 16 MB. The effect of injection bandwidth degradation,
    while applications send many messages simultaneously, is noticeable. Figure  14
    plots the achieved network injection bandwidth to compute the balance ratio v/s
    injected bandwidth degradation. The performance penalties do not grow with scale.
    Fig. 13 a Performance. b Power-cap selection for PTRANS Full size image Fig. 14
    PTRANS: Injection bandwidth to compute balance ratio against injected bandwidth
    degradation Full size image 5.5.5 FFT This benchmark measures the floating-point
    rate of execution of double-precision complex one-dimensional Discrete Fourier
    Transform (DFT) of size m. It captures inter-process communication using large
    messages. Figure  15 shows the performance, power consumption, and power-cap selection
    by the proposed scheme. MPIFFT power profile shows the trend to decrease power
    consumption at function boundaries. The spikes in power consumption represent
    computation-intensive phases, and during this time, memory power consumption goes
    up due to low spatial locality leading to a higher miss rate. This benchmark spends
    most of its time on global transposition for complex vectors using asynchronous
    inter-process communications. Valleys represent the power consumption drop during
    these communication phases. Our proposed scheme accurately captures the entire
    power profile behavior and selects the power-cap. Fig. 15 a Performance. b Power-cap
    selection for MPIFFT Full size image 5.5.6 RandomAccess This benchmark assesses
    the memory subsystem’s peak capacity by updating the system memory’s random remote
    location. Figure  16 shows the performance, power consumption, and power-cap selection
    by the proposed scheme. The benchmark shows a steady step function of relatively
    low processor power during random index generation and decreases during communication
    time with other processes. The benchmark works on a large distributed table of
    size \\(2^{p}\\), occupying approximately half of the system memory and profiling
    the system’s memory architecture. It possesses a low spatial and temporal locality
    memory access profile. It has relatively higher memory power consumption than
    other benchmarks. The memory power consumption increases on the local node once
    all the indices are received. Based on different phases, the proposed scheme accurately
    selects the power-cap. Fig. 16 a Performance. b power-cap selection for MPIRandomAccess
    Full size image In our proposed pcHPCe, to maintain the total power consumption
    by all running containers within the power budget constraint, reduced power-cap
    applies with tolerable performance degradation on the container that causes the
    power-cap violation. Figure  17 presents the peak power consumption against the
    power-cap imposed by the proposed model and RAPL power-cap (Default). Fig. 17
    Peak power consumption of jobs Full size image 6 Summary and comparison Here,
    we summarize and compare the proposed work with the BareMetal solution. Figure  18a
    presents the relative execution time of the different benchmark applications in
    the pcHPCe against BareMetal environment. We calculate the relative execution
    time of the benchmarks to the full-scale execution time. Our pcHPCe offers an
    almost near-native execution time against BareMetal environment. It introduces
    a maximum execution time overhead of 1.35% against BareMetal. Figure 18b depicts
    the power consumption distribution in pcHPCe against no power-cap BareMetal scenario
    for LU and BT benchmark. Our resource contention-aware novel power-cap selection
    framework attains considerable power savings of 10.25% and 13.1% compared to the
    BareMetal environment for BT and LU benchmark applications, respectively. Following
    the proposed scheme, the job scheduler always sets the optimal power-cap for the
    executing jobs while saving much available power budget for the waiting jobs,
    thereby achieving higher throughput. The proposed scheme does not allow power
    budget violations. The job scheduler always tries to maintain the total available
    power budget constraint. However, due to strict power budget constraints and performance
    requirements, the turnaround time of a few jobs extends, as shown in Fig. 18.
    Table 5 compares the obtained results using the proposed scheme with the state-of-the-art
    works to the maximal overhead and power saving. The comparison result between
    state-of-the-art works is representative only and might depend on the kind of
    workload in the experiments. We determine the required performance empirically
    based on the job profile. Figure 19 shows the performance degradation of all jobs
    against the performance of the job applying average power-cap to maintain the
    required performance (Nominal). Fig. 18 a Relative execution time. b Power saving
    in pcHPCe Full size image Table 5 Comparison of the experimental results with
    the state-of-the-art work Full size table Fig. 19 Performance degradation of jobs
    Full size image 7 Conclusion In the proposed pcHPCe, we implement the multi-tenant
    containerized power-aware HPC environment using a real-time power-performance
    prediction approach and a power-cap determination framework with resource contention
    awareness. In detail, our experiments present the performance of a pcHPCe to the
    BareMetal environment using NAS parallel and HPCC benchmarks. The experiment shows
    the need for an application’s real-time power-performance estimation approach
    using a state-of-the-art ML approach, LSTM, and a weighted power-performance variation
    table. We also present the necessity of the application’s computation phase behavior
    and resource contention awareness by co-scheduling threads to determine power-cap
    for Package and DRAM wisely. Our experimentation results show that the power-performance
    prediction model can achieve 91.39% accuracy on average in real-time with a negligible
    overhead of 1.6% of the total computing power per node due to power and performance
    daemons. Moreover, our resource contention-aware novel power-cap selection framework
    attains considerable power savings of up to 13.1% compared to the BareMetal environment.
    Our evaluation results demonstrate that the overall performance of the proposed
    method is better than the state-of-the-art methods, even for colocated applications
    with contention. 7.1 Future work In the future, our focus would be to explore
    the scalability test with communication interference awareness among colocated
    application threads. We intend to further investigate P-states and Turbo Boost
    in the modern processor in the context of the proposed methodology to boost the
    performance in the future. Data availability The data used to support the finding
    of this study are available from the corresponding author upon request. References
    Masanet, E., Shehabi, A., Lei, N., et al.: Recalibrating global data center energy-use
    estimates. Science 367(6481), 984–986 (2020) Article   Google Scholar   Murana,
    J., Nesmachnow, S., Armenta, F., et al.: Characterization, modeling and scheduling
    of power consumption of scientific computing applications in multicores. Clust.
    Comput. 22, 839–859 (2019) Article   Google Scholar   Soltesz, S., Potzl, ea:
    Container-based operating system virtualization: a scalable, high-performance
    alternative to hypervisors. In: Proceedings of the 2nd ACM SIGOPS/EuroSys European
    Conference on Computer Systems, pp 275–287 (2007) Young, B.D., Pasricha, S., et
    al.: Heterogeneous makespan and energy-constrained dag scheduling. In: Proceedings
    of the Workshop on Energy Efficient High Performance Parallel and Distributed
    Computing, pp 3–12 (2013) Wang, R., Tiwari, eaDevesh: Low power job scheduler
    for supercomputers: a rule-based power-aware scheduler. In: IEEE International
    Conference on Data Science and Data Intensive Systems, pp 732–733 (2015) Lai,
    Z., Lam, K.T., et al.: Powerock: power modeling and flexible dynamic power management
    for many-core architectures. IEEE Syst. J. 11(2), 600–612 (2016) Article   Google
    Scholar   Cao, T., He, eaYuan: Demand-aware power management for power-constrained
    hpc systems. In: 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid
    Computing, pp 21–31 (2016) Leon, E.A., Karlin ea: Program optimizations: the interplay
    between power, performance, and energy. Parallel Comput. 58, 56–75 (2016) Article   MathSciNet   Google
    Scholar   Kumar, A.S., Mazumdar, S.: Forecasting hpc workload using arma models
    and ssa. In: International Conference on Information Technology, IEEE, pp 294–297
    (2016) Otoom, M., Trancoso, P., et al.: Machine learning-based energy optimization
    for parallel program execution on multicore chips. Arab. J. Sci. Eng. 43(12),
    7343–7358 (2018) Article   Google Scholar   Rovnyagin, M.M., Hrapov, A.S., et
    al.: Ml-based heterogeneous container orchestration architecture. In: IEEE Conference
    of Russian Young Researchers in Electrical and Electronic Engineering, pp 477–481
    (2020) Xiao, P., Ni, Z., Liu, D., et al.: A power and thermal-aware virtual machine
    management framework based on machine learning. Clust. Comput. 24, 2231–2248 (2021)
    Article   Google Scholar   Arnaboldi, M., Brondolin, eaRolando: Hyppo: Hybrid
    performance-aware power-capping orchestrator. In: IEEE International Conference
    on Autonomic Computing, pp 71–80 (2018) Ferroni, M., Colmenares, J.A., et al.:
    Enabling power-awareness for the xen hypervisor. ACM SIGBED Rev. 15(1), 36–42
    (2018) Article   Google Scholar   Brondolin, R., Sardelli, eaTommaso: Deep-mon:
    Dynamic and energy efficient power monitoring for container-based infrastructures.
    In: IEEE International Parallel and Distributed Processing Symposium Workshops,
    pp 676–684 (2018) Mehta, H.K., Harvey, P., et al.: Wattsapp: Power-aware container
    scheduling. In: IEEE/ACM 13th International Conference on Utility and Cloud Computing,
    pp 79–90 (2020) Kuity, A., Peddoju, S.K.: Performance evaluation of container-based
    high performance computing ecosystem using openpower. In: High performance computing
    ISC high performance, pp. 290–308. Springer, Berlin (2017) Chapter   Google Scholar   Chen,
    W., Ye, K., Xu, C.Z.: Co-locating online workload and offline workload in the
    cloud: An interference analysis. In: IEEE 21st International Conference on High
    Performance Computing and Communications, pp 2278–2283 (2019) Chen, W.Y., Ye,
    eaKe-Jiang.: Interference analysis of co-located container workloads: a perspective
    from hardware performance counters. J. Comput. Sci. Technol. 35, 412–417 (2020)
    Article   Google Scholar   Sak, H., Senior, A.W., et al.: Long short-term memory
    recurrent neural network architectures for large scale acoustic modeling (2014)
    Kuity, A., Peddoju, S.K.: chpce: Data locality and memory bandwidth contention-aware
    containerized hpc. In: 24th International Conference on Distributed Computing
    and Networking, pp 160–166 (2023) Karpowicz, M.P., Arabas ea: Design and implementation
    of energy-aware application-specific cpu frequency governors for the heterogeneous
    distributed computing systems. Future Gener. Comput. Syst. 78, 302–315 (2018)
    Article   Google Scholar   Beltre, A.M., Saha, P., et al.: Enabling hpc workloads
    on cloud infrastructure using kubernetes container orchestration mechanisms. In:
    IEEE/ACM International Workshop on Containers and New Orchestration Paradigms
    for Isolated Environments in HPC, pp 11–20 (2019) Fieni, G., Rouvoy, eaRomain:
    Smartwatts: Self-calibrating software-defined power meter for containers. In:
    20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing,
    pp 479–488 (2020) Enes, J., Fieni, G., et al.: Power budgeting of big data applications
    in container-based clusters. In: IEEE International Conference on Cluster Computing,
    pp 281–287 (2020) Rocha, I., Christian, Göttel, ea: Heats: Heterogeneity-and energy-aware
    task-based scheduling. In: IEEE 27th Euromicro International Conference on Parallel,
    Distributed and Network-Based Processing (PDP), pp 400–405 (2019) Inadomi, Y.,
    Patki, T., et al.: Analyzing and mitigating the impact of manufacturing variability
    in power-constrained supercomputing. In: SC’15: Proceedings of the International
    Conference for High Performance Computing, Networking, Storage and Analysis, IEEE,
    pp 1–12 (2015) Ellsworth, D., Patki, T., et al.: A unified platform for exploring
    power management strategies. In: 4th International Workshop on Energy Efficient
    Supercomputing, IEEE, pp 24–30 (2016) McCraw, H., Ralph, J., Danalis, A., et al.:
    Power monitoring with papi for extreme scale architectures and dataflow-based
    programming models. In: IEEE International Conference on Cluster Computing, pp
    385–391 (2014) David, H., Gorbatov, E., et al.: Rapl: Memory power estimation
    and capping. In: ACM/IEEE International Symposium on Low-Power Electronics and
    Design, pp 189–194 (2010) Dongarra, J., Luszczek, P.: Overview of the hpc challenge
    benchmark suite. In: Proceeding of SPEC Benchmark Workshop (2006) Download references
    Funding No funds have been received from any agency for this research. Author
    information Authors and Affiliations Department of Computer Science and Engineering,
    Indian Institute of Technology Roorkee, Haridwar, Uttarakhand, 247667, India Animesh
    Kuity & Sateesh K. Peddoju Contributions AK and SKP designed the study. AK performed
    the simulations and wrote the paper. All authors reviewed and edited the manuscript.
    All authors read and approved the final manuscript. Corresponding authors Correspondence
    to Animesh Kuity or Sateesh K. Peddoju. Ethics declarations Conflict of interest
    The authors declare that they have no competing interests. Additional information
    Publisher''s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Springer Nature or its licensor (e.g. a society or other partner) holds exclusive
    rights to this article under a publishing agreement with the author(s) or other
    rightsholder(s); author self-archiving of the accepted manuscript version of this
    article is solely governed by the terms of such publishing agreement and applicable
    law. Reprints and permissions About this article Cite this article Kuity, A.,
    Peddoju, S.K. pHPCe: a hybrid power conservation approach for containerized HPC
    environment. Cluster Comput (2023). https://doi.org/10.1007/s10586-023-04105-8
    Download citation Received 27 April 2023 Revised 30 June 2023 Accepted 05 July
    2023 Published 22 July 2023 DOI https://doi.org/10.1007/s10586-023-04105-8 Share
    this article Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords High performance computing (HPC) Cloud computing Container
    technology Power-aware HPC Containerized HPC Use our pre-submission checklist
    Avoid common mistakes on your manuscript. Sections Figures References Abstract
    Introduction Background Related work Containerized power-aware HPC Performance
    evaluations Summary and comparison Conclusion Data availability References Funding
    Author information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Cluster Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'pHPCe: a hybrid power conservation approach for containerized HPC environment'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Carrión C.
  citation_count: '54'
  description: Continuous integration enables the development of microservices-based
    applications using container virtualization technology. Container orchestration
    systems such as Kubernetes, which has become the de facto standard, simplify the
    deployment of container-based applications. However, developing efficient and
    well-defined orchestration systems is a challenge. This article focuses specifically
    on the scheduler, a key orchestrator task that assigns physical resources to containers.
    Scheduling approaches are designed based on different Quality of Service (QoS)
    parameters to provide limited response time, efficient energy consumption, better
    resource utilization, and other things. This article aims to establish insight
    knowledge into Kubernetes scheduling, find the main gaps, and thus guide future
    research in the area. Therefore, we conduct a study of empirical research on Kubernetes
    scheduling techniques and present a new taxonomy for Kubernetes scheduling. The
    challenges, future direction, and research opportunities are also discussed.
  doi: 10.1145/3539606
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Journal Home Just Accepted Latest
    Issue Archive Authors Editors Reviewers About Contact Us HomeACM JournalsACM Computing
    SurveysVol. 55, No. 7Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges
    SURVEY SHARE ON Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges
    Author: Carmen Carrión Authors Info & Claims ACM Computing SurveysVolume 55Issue
    7Article No.: 138pp 1–37https://doi.org/10.1145/3539606 Published:15 December
    2022Publication History 15 citation 5,651 Downloads View all FormatsPDF ACM Computing
    Surveys Volume 55, Issue 7 Previous Next Abstract 1 INTRODUCTION 2 BACKGROUND,
    TERMINOLOGY AND TECHNOLOGY AVAILABLE 3 RESOURCE MANAGEMENT IN KUBERNETES 4 OTHER
    CONTAINER ORCHESTRATION ENGINES 5 LITERATURE REVIEW: TAXONOMY AND ONGOING ISSUES
    6 FUTURE DIRECTION AND OPEN ISSUES 7 CONCLUSIONS Footnotes REFERENCES Cited By
    Index Terms Recommendations Comments Skip Abstract Section Abstract Continuous
    integration enables the development of microservices-based applications using
    container virtualization technology. Container orchestration systems such as Kubernetes,
    which has become the de facto standard, simplify the deployment of container-based
    applications. However, developing efficient and well-defined orchestration systems
    is a challenge. This article focuses specifically on the scheduler, a key orchestrator
    task that assigns physical resources to containers. Scheduling approaches are
    designed based on different Quality of Service (QoS) parameters to provide limited
    response time, efficient energy consumption, better resource utilization, and
    other things. This article aims to establish insight knowledge into Kubernetes
    scheduling, find the main gaps, and thus guide future research in the area. Therefore,
    we conduct a study of empirical research on Kubernetes scheduling techniques and
    present a new taxonomy for Kubernetes scheduling. The challenges, future direction,
    and research opportunities are also discussed. Skip 1INTRODUCTION Section 1 INTRODUCTION
    Cloud-native architectures enable users and developers the flexibility to build,
    deploy, and maintain applications independently of the underlying infrastructure.
    Thus, the developer can focus on the development and delivery of the application.
    The Cloud Native Computing Foundation (CNCF) [27] defines cloud-native as a new
    computing paradigm in which applications are built based on a microservices architecture,
    packaged as containers, and dynamically scheduled and managed by an orchestrator.
    In a microservice-based architecture, an application is designed into multiple
    microservices that are deployed and managed independently and communicate with
    each other over a network [79, 136, 139]. Nowadays, containers are the de facto
    standard for implementing these microservices [2, 12, 22, 98]. The growth of containers
    has changed how users conceive the development, deployment, and maintenance of
    software applications. Containers make use of the native isolation capabilities
    of modern operating systems with a low overhead in resource consumption and obtaining
    great flexibility in their deployment. Lightweight and flexible containers have
    given rise to microservice-based application architectures. Then, this new approach
    consists of packaging the different services that make up an application in separate,
    intercommunicating containers, which are deployed in a cluster of physical or
    virtual machines. The complexity of these applications has led to the need for
    container orchestration, that is, to provide tools that help us automate the deployment,
    management, scaling, interconnection, and availability of our container-based
    applications. Many cloud service providers offer Containers as a service (CaaS)
    as a cloud service model that simplifies the deployment of containerized applications
    in the cloud [3]. CaaS use is increasing in companies that take advantage of the
    portability of containers between environments, avoiding vendor lock-in. In general,
    a CaaS platform undertakes authentication, logging, security, monitoring, networking,
    load balancing, auto-scaling, and continuous integration/continuous delivery (CI/CD)
    functions. From the cloud service provider’s perspective, a CaaS platform creates
    an abstraction layer that includes a container orchestration engine, typically
    based on the de facto standard Kubernetes [7]. Other well-known toolkits for container
    orchestration are Docker Swarm [88] and Mesos [48]. Usually, these orchestrators
    handle a cluster of physical or virtual machines to host the containerized application.
    In particular, the task of assigning physical resources to containers is performed
    by the scheduler, and the percentage of total available resources in the system
    and the Quality of Service (QoS) perceived by the user can be strongly affected
    by its implementation. Note that scheduling can focus on improving resource utilization,
    reducing energy consumption, or satisfying users’ or applications’ time requirements.
    But, as a rule, designing efficient container scheduling techniques is still an
    open issue. Even though multiple platforms exist, the wide adoption of Kubernetes
    [7, 27] across different branches of the industry [28] and public clouds providers
    (i.e., Google Kubernetes Engine (GKE), Microsoft Azure Kubernetes Service (AKS),
    Amazon Elastic Kubernetes Service (Amazon EKS)) became it as a de facto standard
    for container orchestration. So, Kubernetes is a leading open-source container
    orchestration platform, and scheduling in Kubernetes is an active field of research.
    Hence, the Scheduling Special Interest Group (SIG) [53] is a Kubernetes contributor
    community interested in various scheduling questions on Kubernetes. More precisely,
    the Kubernetes Scheduling Interest Group (k8sched) is responsible for the components
    that make pod placement decisions (the less unit scheduled by Kubernetes). Researching
    in Kubernetes scheduling for containerized applications is an area of growing
    interest and this article focuses on this topic. It is crucial to study the landscape
    of existing Kubernetes scheduling techniques and understand their strengths and
    limitations to advance in this important and growing research area. 1.1 Related
    Surveys and Our Contribution There exist many surveys discussing virtual machine
    resource management [1, 4, 39, 60, 69, 71, 77, 82, 91] and there are also surveys
    dealing with container scheduling [2, 21, 49, 57, 79, 92, 103, 135, 136]. Other
    works have been published in recent literature, many fewer in number, related
    to containers’ orchestration in fog/edge architectures [15], that is, architectures
    that provide storage and computing resources close to the place where data are
    generated [2, 49, 57, 71]. Table 1 summarizes the previous review and survey papers
    supporting the relevance of container orchestration. But, they suffer from some
    weaknesses: (a) All the papers are not updated to the new current scheduling proposals.
    (b) Some papers focus on specific architecture environments, such as fog or edge
    computing. (c) The papers do not study the topic of scheduling problems in Kubernetes
    implementations, as the de facto standard cluster scheduling and, to the best
    of our knowledge, no survey covers Kubernetes scheduling techniques to identify
    active researching area. Table 1. Ref. Survey [1] The authors analyze the characteristics
    of various workflow scheduling techniques and classify them based on their objectives
    (makespan, availability, reliability, energy, communication, utilization, security)
    and execution model (best effort, QoS constraint). [2] The survey classified the
    scheduling techniques based on the type of optimization algorithm employed to
    generate the schedule. [4] The authors present a comprehensive survey of task
    scheduling strategies published from January 2005 to March 2018 for cloud computing
    environments. Show More Table 1. Related Works in Cluster Orchestration The mentioned
    reasons above motivate us to review the primary literature on Kubernetes scheduling.
    In particular, this article analyzes the proposals for the de facto Kubernetes
    orchestrator and presents an updated review from the 2016 to 2022 period. This
    article describes the state-of-the-art of the Kubernetes orchestrator which complements
    previous surveys on container scheduling. In contrast with previous surveys, this
    article reviews the works done so far in the field from a general point of view
    and defines a set of top-down domains; from the infrastructure domain to the application
    domain including a transversal performance domain. The taxonomy is accompanied
    by the classification of a broad range of aspects studied for Kubernetes resource
    scheduling, not being limited to scheduling algorithms or specific physical devices.
    The existing Kubernetes scheduling techniques are covered focusing on research
    trends and gaps. So, this article is helpful for future research to overcome the
    actual limitations and challenges related to Kubernetes scheduling. To sum up,
    the main contributions of this article are as follows: — A study of the state-of-the-art
    of Kubernetes orchestrator for scheduling containerized applications is presented.
    — A new layered taxonomy to classify Kubernetes resource scheduling techniques
    is presented. — An updated review of recent works in Kubernetes scheduling is
    provided. — A list of current issues and challenges is compiled, as well as research
    directions in Kubernetes scheduling to promote further improvements. 1.2 Paper
    Organization Figure 1 outlines the structure of the survey. This article is organized
    as follows: Section 2 introduces the background information regarding virtualization,
    orchestration of containerized applications, and Kubernetes architecture. Then,
    Section 3 details how resource management works in Kubernetes, with a focus on
    scheduling. Besides, Section 4 reviews others container orchestration engines.
    After that, in Section 5, the new taxonomy is presented, and Sections 5.1–5.5
    detail the ongoing working topic related to the Kubernetes scheduling and the
    taxonomy described. The challenges in the area along with some future research
    are described in Section 6. Finally, Section 7 draws some conclusions. Fig. 1.
    Fig. 1. Structure of this article. Skip 2BACKGROUND, TERMINOLOGY AND TECHNOLOGY
    AVAILABLE Section 2 BACKGROUND, TERMINOLOGY AND TECHNOLOGY AVAILABLE 2.1 Operating
    System-Level Virtualization Virtualization can be defined as a technology that
    enables the creation of logical services by means of resources running on hardware
    [100]. In other words, virtualization distributes the capabilities of a resource,
    such as a network, storage, server, or application, among several users or environments
    [89]. In particular, the operating system-level virtualization technology provides
    isolated computing environments called containers within a common operating system.
    In Linux, different kernel tools such as cgroups, namespaces, or chroot are used
    for this purpose. The operating system-level virtualization technology provides
    container-based Cloud services, also known as CaaS [3]. All main cloud providers
    support container deployments in their infrastructure, such as Google Container
    Engine, Amazon Elastic Container Service (ECS), or Microsoft’s Azure Container
    Service. It is likewise possible to set up a container cluster on private premises,
    leveraging popular container orchestrators such as Docker Swarm [88] or Kubernetes
    [7]. A container is a group of one or more processes that are isolated from the
    rest of the system. It comprises an application and all its library dependencies
    and configuration files. Moreover, containers offer reproducible execution environments,
    light lifecycle management, and closer-to-metal performance than classical virtual
    machine deployments [80, 113]. Nowadays, the most well-known container manager
    is Docker [5], a layered container platform that comprises several software components
    for developing, transporting, and running containerized applications, i.e., Docker
    Daemon, containerd, or Docker registry [26]. Figure 2(a) shows the relationship
    between different container technologies to manage the lifecycle of containers
    on a node. At the lowest level of container technology rest container runtimes,
    such as LXC, RunC, CRun, or Kata, that create and run the container processes
    [115]. A high-level container runtime manages the complete container lifecycle
    of its host system being able to pull container images from registries, manage
    images, and hand them over to the lower-level runtimes. Containerd and CRI-O are
    the most popular high-level container runtimes that implement the Open Container
    Initiative (OCI), a standard specification for image formats and runtimes requirements
    providing container portability [41]. Usually, the container runtimes are wrapped
    in software components called container managers or engines that increase the
    level of abstraction. In Figure 2(a), Docker Daemon acts as a container manager
    with an API that simplifies the management of the lifecycle of the containers
    and communicates with containerd. When we scale up to a cluster, it can be extremely
    difficult to manage the lifecycle and management of containers, especially if
    the number increase with demand. Hence, container orchestration solves the problem
    by automating the scheduling, deployment, availability, load balancing, and networking
    of containers. In Figure 2(a), kubelet is the daemon component of the Kubernetes
    orchestrator that communicates with the high-level container runtime. Note that
    Kubernetes works with all container runtimes that implement the standard Container
    Runtime Interface (CRI).1 More precisely, CRI defines the runtime specification
    required for integration with Kubernetes. Fig. 2. Fig. 2. Terminology and technology
    available. 2.2 Container Orchestration As we have mentioned above, a container
    orchestrator manages and organizes microservice architectures at scale, dealing
    with the automation and lifecycle management of containers and services in a cluster.
    From a general point of view, end-users submit their jobs to the cluster manager
    master, that is, a core entity of the orchestration system. The cluster manager
    master is in charge of assigning the submitted tasks to the worker nodes of the
    compute cluster, where they are executed. It must be noted that usually jobs or
    applications are composed of one or more services, and services are composed of
    one or more heterogeneous tasks that are executed on containers. The compute cluster
    is an abstraction of interconnected nodes that can be physical or virtual machines
    in different infrastructures, such as clouds or private clusters. Container orchestrators
    can be classified as on-premise or managed solutions. Borg, Mesos [48], or Kubernetes
    [7] are examples of on-premise orchestrators that need to be installed, configured,
    and managed on physical or virtual infrastructures. Managed solutions are instead
    offered by cloud providers as a service and need only to be partially configured.
    Examples of managed solutions are GKE, Microsoft AKS, and Amazon EKS. Figure 2(b)
    shows the most characteristic components of both on-premise and managed cluster
    orchestrators. Next, we summarize the main functionality of these modules. More
    details can be found in [103] where the authors present a reference architecture
    for container orchestration systems. A key element of container orchestrators
    is scheduling, which is the focus of this article. The scheduling module is responsible
    for identifying the best location to complete incoming tasks. In other words,
    the scheduling describes a schema to place a container on a compute node of the
    cluster, at a given time instant. Most scheduling policies map containers based
    on the state of the system (resource constraints, node affinity, data location)
    as well as metrics such as power consumption or response time, or makespan. Moreover,
    the rescheduler component implements a task relocation policy that determines
    a new location for a task. This relocation can be triggered for preemption reasons
    or by the system state to consolidate the load or improve resource utilization.
    Note that it is not necessary to implement all the components to have a fully
    functional container orchestration system. The resource allocation module reserves
    the cluster resources following a request-based approach that can be static or
    dynamic over time. The load balancing module is in charge of distributing tasks
    across container instances based on criteria such as fairness, cost-energy, or
    priory. The default policy for load balancing is round-robin, but other strategies
    can be used. The autoscaling module is in charge of providing horizontal and vertical
    scaling depending on the workload demand. In the horizontal scaling, nodes are
    added or deleted while in the vertical autoscaling the node resources associated
    with a task are increased or reduced. A simple autoscaling mechanism can be based
    on a threshold value on CPU or memory usage. For example, Kubernetes uses Horizontal
    Pod Autoscaler to perform autoscaling based on CPU and memory metrics [7]. Finally,
    the admission control module is responsible for checking that there are sufficient
    resources in the cluster to run the user’s jobs and never exceed the quota assigned
    to it. And, the accounting module monitors the available resource for a user while
    the monitoring module keeps track of real-time resource consumption metrics for
    each node and collects metrics related to the health of the resources to support
    fault tolerance systems. 2.3 Kubernetes Architecture Kubernetes, also called as
    K8s, is an open-source system aimed at automating the deployment, management,
    and scaling of containerized applications in distributed clusters [7]. It is the
    evolution of Google’s Borg [126] project, which had more than a decade of experience
    running scalable applications in production. It was released to the open-source
    community in 2014 and since that time it has not stopped growing. Production systems
    that use K8s are complemented with tools from the K8s ecosystem to improve and
    facilitate their management. So, the ever-growing K8s ecosystem is composed, to
    name a few, of complementary tools for managing deployments (Helm, https://helm.sh/),
    service mesh management (Istio, http://istio.io/), monitoring (Prometheus, https://prometheus.io
    , Grafana, https://grafana.com), or logging the system (Kibana [25]). An updated
    description of the K8s ecosystem can be found in https://landscape.cncf.io. Although
    describing their features is beyond the scope of this article, it is worth noting
    that many scheduling proposals interact with some of these tools to, for example,
    extract real-time information about the state of the system or predict the use
    of a resource [20, 86, 107, 129, 130]. From an architectural point of view, a
    K8s cluster consists of a set of nodes (physical or virtual machines) integrated
    to function as a single entity (see Figure 3). These nodes have different functions,
    distinguishing between master nodes and worker nodes. Using software-defined overlay
    networks, such as Flannel or Calico, allows K8s to assign a unique IP address
    to each pod and service [97]. Note that a pod is the basic deployment unit that
    can be operated and managed in the cluster. The master node coordinates the cluster,
    and the worker nodes provide a source of resources for the cluster. A single master
    node is sufficient to run a cluster, although three master nodes are typically
    found in high availability topologies (HA). K8s uses an event-based declarative
    engine and the principle of loosely coupled components. In short, the components
    of the K8s master node are as follows: (a) etcd is a key-value database used to
    synchronize the desired state of the system. (b) scheduler maps each pod on a
    worker node. (c) API server receives commands and manipulates the data for K8s
    objects, which are persistent entities representing the state of the cluster.
    The API server exposes a RESTful HTTP API to describe an object with JSON or YAML.
    Moreover, users can send commands to the API server by using the K8s command line
    interface (CLI), kubectl. (d) controller manager monitors etcd and forces the
    system into the desired state. ReplicaSet, Deployment, Job, or DaemonSet are some
    of the most well-known K8s controllers that provide different functionalities
    such as availability, roll-back, task execution, or a pod running on each node,
    respectively. Independent controllers communicate through the object changes in
    the API server and the events that these changes trigger through informers. Controllers
    monitor the status of deployments and perform the necessary actions to ensure
    their successful execution. If the controller requires to deploy a new pod, the
    scheduler will perform the logic to find the most suitable worker node. Fig. 3.
    Fig. 3. Kubernetes orchestrator: Internal architecture. On the other hand, worker
    nodes are in charge of running the pods of an application. In particular, kubelet
    is the node agent responsible for the lifecycle of the deployed pods and monitoring
    pods and node status. To conclude this section, it is important to mention that
    K8s is supported by an active research community. For example, contributions on
    autoscaling, authentication, architecture, security, or scheduling are supported
    by SIGs, which are open source communities. In particular, the sig-scheduling2
    group focuses on contributions to scheduling pods. Skip 3RESOURCE MANAGEMENT IN
    KUBERNETES Section 3 RESOURCE MANAGEMENT IN KUBERNETES Kubernetes automates the
    most efficient distribution and scheduling of containerized applications across
    the nodes of the cluster (physical or virtual) balancing the use of resources
    on each node. Previously, Section 2.2 presented the most characteristic components
    of cluster orchestrators from a general point of view. These components can also
    be embedded in K8s (note that not all of them need to be implemented in an orchestrator).
    Table 2 summarizes the resource management features implemented in K8s along with
    their key characteristics. Note that K8s administrators and users can configure
    a wide range of components to better control resource usage and final application
    performance. When defining an application in a cluster, the master node receives
    the information via the Kubernetes API and deploys the application to the worker
    node it deems appropriate. In particular, the scheduler component looks for pods
    that are in pending state, because they have just been created and do not yet
    have a worker node assigned and finds the best worker node to run the pod. The
    placement decision in a worker node is based both on the scheduling policy and
    the user specification. Sections 3.1 and 3.2 discuss the user specifications and
    internal functionality of K8s scheduling in more detail. Table 2. Feature Technique
    Function Scheduling kube-scheduler, custom Assign containers to worker nodes based
    on different policies and user specifications. Admission control mutating and
    validating controllers, custom Intercept requests to the K8s API server before
    the object is persisted, but after the request is authenticated and authorized.
    This increases the security of the cluster. Load balancing round-robin, custom
    Distribute the load among multiple container instances. Complex policies can be
    provided by external load balancing. Show More Table 2. Features Implemented by
    K8 Orchestrator 3.1 Scheduling in Kubernetes: User Specifications Users can configure
    a wide range of options to specify the conditions that the scheduler should satisfy.
    To this end, user specifications indicate different types of constraints that
    play the role of a control admission. The constraints can be node-level, namespace-level,
    or pod-level. Next, we will provide a brief explanation. At the node level, there
    is the ability to assert some control over what workloads can be run on a given
    set of nodes via affinity and taint. The affinity property attracts pods to a
    set of nodes, while taint does the opposite. The taint property allows a node
    to repel a set of nodes. In addition, there is a complementary property called
    tolerance. Each node has a tolerance and if the taint is above the tolerance,
    it will schedule it to another node with a higher tolerance. At the pod level,
    we can optionally specify how much of each resource a container needs. Using the
    request and limit properties, we can control the minimum and maximum amount of
    resources requested for the pod to run. The most commonly specified resources
    are CPU and memory (RAM). The request quota is strict, while the limit quota is
    not. This means that the addition of the allocated resources of previous containers
    plus the new request resource cannot exceed the resource set of the node. But
    a container can exceed its limit quota for some time. It must be emphasized that
    in some cases, it may be necessary to kill the execution of a pod due to a lack
    of resources. For example, if a node has 2 cores and 1.5 has already been allocated,
    the node capacity will exceed 100% every time a new pod requests more than 0.5
    cores. In this case, K8s takes different actions depending on the QoS class of
    the pods. In the best-effort class, where the pod contains neither the request
    nor the limit property, the pod can be eliminated if necessary. In the guaranteed
    class, the pods specify a request value equal to the limit. Thus, their survival
    is guaranteed. Finally, there is a third QoS class called burstable, in which
    the pods specify the requested property. Burstable pods are guaranteed at least
    a minimum amount of resources. It is important to define these values correctly
    because pods can be eliminated in an undesirable way at a critical moment. Finally,
    at the namespace level, we can also specify the request and limit capacity of
    pods within a K8s namespace. These parameters can be fixed thanks to the LimitRange
    and ResourceQuota properties of a namespace. 3.2 Scheduling in Kubernetes: Internal
    Workflow The overall resource management process in K8s can be summarized as follows:
    When a user requests the creation of a pod with certain compute resources, the
    master node of K8s receives the request. The request is forwarded to the API server
    and the scheduler (kube-scheduler) acknowledges the request. Kube-scheduler determines
    which worker nodes (i.e., host servers) should run the pod and notifies the kubelet
    agent, running on the worker node, to create a pod. If the pod has specified computing
    resources in the description file, kubelet utilizes cgroups and tc to reserve
    computing resources for the pod. When the pod creation is finished, kubelet informs
    API server that the pod is running. At runtime, users can modify the resource
    configuration by submitting an update request of the YAML description file to
    the master node. Note that the K8s API server uses optimistic concurrency (when
    the API server detects concurrent write attempts, it rejects the latter of the
    two write operations.) In short, the scheduler runs as a pod on the master node,
    being part of the K8s control plane and is responsible for matching the best node
    on which to run new application pods. Multiple different schedulers may be used
    in a cluster but kube-scheduler is a reference implementation provided as the
    Default Scheduler in K8s. 3.2.1 Default Scheduler. It is a core component of K8s
    that acts as a typical single dispatcher. It binds a pod to a specific working
    node. The lifecycle of the default scheduler is shown in Figure 4 and works as
    follows: (a) The scheduler maintains a queue of pods called podQueue that keeps
    listening to the API Server. (b) When a pod is created, the pod metadata is first
    written to etcd through the API Server. (c) The default scheduler, as a controller,
    follows the watch state, takes action, and updates the state pattern. So, it watches
    the unbound pods from the etcd and each time an unbound pod is read from the etcd,
    the pod is added to the podQueue. (d) The main process continuously extracts pods
    from the podQueue and assigns them to the most suitable nodes for running those
    pods. (e) The scheduler updates the pod-node binding in the etcd in order to be
    notified to the kubelet on the worker nodes. (f) The kubelet component running
    in the selected worker node, which monitors the object store for assigned pods,
    is notified that a new pod is in “pending execution” and it executes the pod.
    So, the pod starts running on the node. The logic of the main process iterates
    over the nodes in a round-robin fashion and performs per each unbinding pod the
    filtering and ranking substeps. Fig. 4. Fig. 4. Life cycle of the default scheduler
    in K8. The filtering step: pre-selects workers nodes that satisfy certain requirements
    of the pod based on a list of policies. In particular, node filtering is based
    on predicates, Boolean functions that indicate whether a pod fits a worker node.
    The constraints of a pod are established by using node labels in the pod definition.
    For example, the disktype:ssd parameter in the YAML file of a K8s container specifies
    that only nodes with a solid-state hard disk are eligible for selection. Some
    policies supported in this step in K8s are the PodFitsHostPorts, which checks
    if a port requested by the pod is free in the node, the PodFitsResources which
    checks if the node has the free resources needed by the pod (e.g., CPU or memory),
    the PodFitsHost that specifies the name of the node on which the pod must be deployed,
    or the CheckNodeCondition policy that checks if a node is healthy (e.g., the network
    is available or the kubelet is ready). The ranking stage: assigns a score to the
    remaining candidates depending on certain configurations and metrics. The better
    the pod matches a node, the higher the score assigned to the node. Finally, the
    pod is bound to the node with the highest score. In this stage, worker nodes are
    prioritized by considering various factors such as the state of the node, the
    availability of resources, and the current workload. In particular, the rank can
    be set statically based on predefined different policies or a user-defined policy
    [7, 92, 107]. Selecting the node with the highest percentage of free CPU and memory
    is a commonly used ranking criterion (LeastRequestPriority policy) (default option).
    Other criteria target balanced resource usage (BalanceResourceAllocation policy),
    distribution of pods of the same service among different nodes (SelectorSpreadPriority
    policy), or random distribution among hosts (SelectorSpreadPriority policy). In
    addition, predefined preferences can be set based on node affinity or anti-affinity
    rules (e.g., the NodeAffinityPriority policy). The filtering and ranking stages
    can be configured using scheduling policies (before version v1.23) or scheduling
    profiles. In the first case, the predicates and priorities selected in the scheduler
    configuration file define the scheduler, and the default scheduler policy can
    be overridden. In the second case, a single instance of Kube-scheduler can run
    multiple profiles and the scheduler profiles are specified using the KubeSchedulerConfiguration.
    Listing 1 shows an example of specifying two different profiles and how plugins
    provide configurable behaviors at different stages of scheduling in kube-scheduler.
    By default, a pod is scheduled according to the default-scheduler profile. Specific
    profiles can be used to schedule a pod by specifying their name in .spec.schedulerName,
    as shown in Listing 2 for mynginx. 3.2.2 Extending K8 Scheduler. Kubernetes scheduler
    supports several ways to extend its functionality [107]. First, you can add new
    predicates and/or priorities to the default scheduler and recompile it. Second,
    you can implement an extender process that the default scheduler invokes as the
    final step. The scheduler extender is a configurable webhook that contains filter
    and priority endpoints that correspond to the two main phases of the scheduling
    cycle (filtering and ranging). In this case, the state of the entire cluster stored
    in the cache of the default scheduler is not shared with the scheduler extender
    and data communication is done through serial HTTP communication with the associated
    communication costs. To address these limitations, the Scheduling framework3 come
    up as an officially recommended extension to the K8s scheduler (introduced in
    Kubernetes v1.15). It defines new extension points (queue sort, pre-filter, filter,
    score, reserve, etc.) that are integrated as plugins into the existing scheduler
    at compile time. This way, users can write new scheduling functions in the form
    of plugins. Finally, you can implement a custom scheduler process that can run
    instead of or alongside the default scheduler. A detailed description of how to
    implement a custom K8s scheduler is beyond the scope of this article, but the
    Random (Marton Sereg https://banzaicloud.com/blog/k8s-custom-scheduler/) and Toy
    (Kelsey Hightower https://github.com/kelseyhightower/scheduler) schedulers are
    simple examples that show you how to write and deploy custom K8s schedulers. Detailed
    information on developing the code required to extend the functionality of the
    K8s scheduler can also be found at [134]. Note that the K8s scheduler functionality
    can be extended in different languages (e.g., Go, Python, Java). Listing 1. Defining
    profiles for kube-scheduler. Listing 2. Assigning a scheduler to a pod. Skip 4OTHER
    CONTAINER ORCHESTRATION ENGINES Section 4 OTHER CONTAINER ORCHESTRATION ENGINES
    Nowadays, Kubernetes [7] represents the first project of the CNCF [27], an organization
    that has more than 36 innovative projects focused on the Cloud Native Computing
    ecosystem. Moreover, as has been mentioned before, Kubernetes is widely adopted
    across industries and has become a de facto standard. Nevertheless, multiple frameworks
    are being used in today’s production environments for container orchestration,
    i.e., Borg [126], Mesos [48], or Docker Swarm [6]. In the following, and for the
    sake of comparison, the architectural characteristics, as well as the scheduling
    techniques of the main container-based orchestrators are briefly presented. Borg
    is the first unified container-management system developed at Google. It manages
    both long-running services and batch jobs, which had previously been handled by
    two separate systems [18, 126]. The scheduler uses several techniques to scale
    up to tens of thousands of machines. Borg uses a master-slave architecture with
    a logically centralized controller and agent processes that run on each machine.
    Borgmaster scheduler uses a monolithic architecture [110, 126]. This means that
    the scheduler is composed of a single instance that processes all the task requests
    and has a global state view of the system. When a job is submitted, the Borgmaster
    records it persistently in the Paxos store and adds the job’s tasks to a pending
    queue. The scheduler asynchronously scans this queue and assigns tasks to machines.
    The scan proceeds from high to low priority, modulated by a round-robin scheme
    within a priority to ensure fairness across users and avoid head-of-line blocking.
    The scheduling algorithm first finds machines on which run the task (machines
    that both meet the task’s constraints and have enough resources). Then, the algorithm
    selects one candidate machine attending to some score criteria such as the worst
    or best fit. Apache Mesos is an open-source cluster manager that implements a
    two-level architecture (see Figure 5(a)). This means that a centralized master
    resource manager dynamically controls which resources each framework scheduler
    owns [62]. The frameworks (i.e., hadoop, MPI, Mesos Marathon) are in charge of
    scheduling at the application level. Fig. 5. Fig. 5. Architecture of Mesos and
    Docker Swarm orchestrators. First, users interact with the frameworks that use
    ZooKeeper to locate the current master [48]. Then, the frameworks coordinate with
    the master to schedule tasks onto agent nodes. More precisely, the master manages
    resource allocations using Dominant Resource Fairness (DRF) [44] as the first-level
    scheduler and transmits the assigned workload to slave nodes. Agent nodes execute
    containers and must periodically inform the master about their available resources.
    In this two-level architecture, applications use their own scheduling policies
    based on priority preservation and fairness [110]. Hadoop YARN is a cluster manager
    designed to orchestrate Hadoop tasks, although it also supports other frameworks
    such as Spark and Storm [40]. Each application framework running on top of YARN
    coordinates its execution flows. In YARN, there is one resource manager per cluster
    and one application master per framework. The application master requests resources
    from the resource manager and generates a physical plane from the resources it
    receives. The resource manager allocates containers to applications to run on
    specific nodes. In this context, a container is a bundle of resources bounded
    to run on a specific machine and for each job, there is an application manager
    responsible for managing the lifecycle (i.e., resource consumption, the flow of
    executions). The application manager needs to harness the resources available
    on multiple nodes to complete a job. To obtain them, the application manager requests
    resources from the resource manager that contain the location preferences and
    properties of the containers. Docker Swarm is the native clustering Docker’s solution
    for the management and orchestration of a cluster where you can deploy application
    services as Docker containers. In fact, cluster management and orchestration are
    embedded as Docker Swarm mode into Docker Engine since version 1.122 [6]. Docker
    Swarm orchestration engine includes cluster management, scaling, and automatic
    fail-over but is not an enterprise scalable product solution [88]. Figure 5(b)
    shows the main components of a cluster of Docker Swarm, such as the discovery
    and scheduler module of the Swarm manager. In particular, the scheduler module
    orchestrates the swarm nodes and executes a scheduler algorithm that chooses the
    machine that will execute a container based on two phases: the filtering and the
    scheduling strategy [29]. Filtering allows selecting nodes based on specific host
    properties or specific configurations, such as affinity. In the standalone Swarm,
    the scheduler supports three different scheduling strategies: spread, binpack,
    and random. Swarm manager chooses the least loaded node to place the task under
    the spread strategy, in order to distribute the service across the swarm. In contrast,
    the binpack strategy makes Swarm choose the most loaded node to use fewer machines
    and to pack as many containers as it could. Under the random strategy, Swarm chooses
    nodes at random. To sum up, Table 3 details several features of the orchestration
    platforms. Mesos allows multiple frameworks to share one cluster. It takes a distributed
    approach to combine multiple clusters (up to 100.000 nodes) to manage both containerized
    and non-containerized applications. Comparing Mesos with K8s, Mesos lacks some
    K8s features such as persistent volumes on external storage and assigning default
    IPs to containers [78]. Regarding Docker Swarm, it offers fast deployments and
    is pretty easy to use than K8s [81]. Nevertheless, Swarm lacks automatic scale,
    native logging and monitoring components, and native dashboards. Moreover, Docker
    Swarm uses raft consensus to manage cluster state which limits its scalability
    [12]. Table 3. Framework Containertechnology Schedulerarchitecture Scheduleralgorithm
    Applications Kubernetes CRI API, OCI-compliant cluster centralized round-robin
    dispatch, filtering and ranking all (multiple co-located tasks) Borg cgroup-based
    monolithic centralized priorizated round-robin all (indepen. tasks) Apache Mesos
    mesos contain docker, singularity two-levels DRF, framework-based all (single-task)
    Show More Table 3. Comparison of Container Orchestration Engines To conclude this
    section, it is worth pointing out that there are Kubernetes-based distributions
    such as K3 [93] and KubeEdge [94] that extend the use of K8s in other environments
    reinforcing it as the de facto standard orchestrator. So, KubeEdge is a CNCF incubating
    project built on top of K8s for extending containerized application orchestration
    capabilities to hosts at edge [94]. A controller plugin for K8s, called EdgeController,
    manages edge nodes and cloud virtual machines as one logical cluster, which enables
    KubeEdge to schedule, deploy, and manage container applications across edge and
    cloud with the same API [131]. Regarding K3, it is a stripped-down version of
    K8s for IoT and Edge application that was first released in March 2019. Scheduling,
    network, and cluster logic are kept the same as in K8s [93]. Total size requirements
    are reduced through a reorganized plugin structure and a less resource-intensive
    database (sqlite3). Note that from the point of view of scheduling, the scheduler
    component is identical in both engines. Skip 5LITERATURE REVIEW: TAXONOMY AND
    ONGOING ISSUES Section 5 LITERATURE REVIEW: TAXONOMY AND ONGOING ISSUES The main
    aim of this section is to identify and summarize the existing literature on scheduling
    strategies developed for implementation in K8s. To this end, we conduct a systematic
    review of the literature, using Kitchenham and Charters’ manual as a Reference
    [59]. We also optimized the definition of the inclusion and exclusion criteria
    to identify as many relevant articles as possible. The inclusion criteria in the
    search for relevant studies were, on one hand, that the article was published
    in an international peer-reviewed journal or conference and was written in English.
    On the other hand, the exclusion criteria were that the article was not accessible
    through university services or memberships and that K8s scheduling was not the
    focus of the article, but only mentioned as an example or used by default without
    any improvement. It should be highlighted that the University provides free access
    to the browser IEEE Explore, as well as to the regular publications of Web of
    Science and Scopus. This article summarizes the results of an extensive, though
    not omni-comprehensive literature review that analyzes and summarizes the contribution
    of 96 verified sources. It also classifies the literature using a taxonomy based
    on five domains and several subdomains. Figure 6 schematically shows the structure
    of this survey based on the five domains. Note that the domains refer to the following
    questions: Fig. 6. Fig. 6. Literature classification for k8 scheduling: A domain
    approach. — Infrastructure: What infrastructures are managed by the scheduler
    in K8 deployments? — Cluster: What cluster characteristics affect the scheduling
    in K8? — Scheduling: How does the K8 scheduler work? — Application: What kind
    of applications does the K8 scheduler manage? — Performance: How mature are the
    K8 scheduling implementations in terms of their evaluation/performance? Finally,
    this literature review includes a discussion section that identifies the open
    issues and challenges, as well as the future research directions in the context
    of K8s scheduling. The key idea is to answer the following question: What are
    important open issues and future research in Kubernetes scheduling? Even though
    future challenges are not considered a classification domain in this taxonomy,
    it has been included in Figure 6 because it is an important contribution. 5.1
    Infrastructure Domain This domain is classified based on the characteristics of
    each physical or virtual component of the cluster comprising the computation,
    network, and storage resources. The subdomains are directly related to the following
    questions: — What are the physical components considered in the development of
    the K8 infrastructure? — What software system is designed to operate on physical
    devices and manage the resources of those devices? 5.1.1 Physical Layer. First,
    we will describe the physical layer issues that affect the K8s scheduler, focusing
    on the hardware architecture of the worker nodes. K8s can be used as a container
    orchestration system in data centers whose core processing hardware components
    are servers (composed of CPU, disk, memory, and network interfaces). In these
    systems, the CPU-based compute nodes are powerful, and the network connections
    have consistently high throughput. However, the K8s scheduler must manage worker
    nodes with low resource capacities as well as different processor architectures.
    For example, a microcluster consists of SBCs and can be integrated with sensor
    devices to deploy IoT applications. SBCs, such as ARM-based Raspberry Pis ( https://www.raspberrypi.com/)
    or Odroids ( https://magazine.odroid.com/), are small computers designed as a
    single-board circuit with low processing power, reduced memory capacity, and low
    prices. These hardware-constrained nodes impose difficult limitations on the scheduling
    system and the K8s scheduler must assign resources efficiently, for example, to
    prevent SBCs from running out of memory when an application is running. It should
    also be noted that an application can become very computationally intensive during
    processing, such as real-time video processing or machine learning. So certain
    types of advanced applications can significantly speed up their processing time
    by using accelerators such as GPUs, TPUs, or FPGAs. Hardware-aware scheduling
    of worker nodes is a key aspect of K8s, as shown by some of the papers reviewed
    in the literature. Table 4 shows a brief overview of some papers related to the
    physical layer, classified as hardware-aware approaches and scheduling techniques
    developed specifically for GPA devices (where Fe. refers to Feature). Table 4.
    Ref. Year Fe. Proposal [55] hardware The authors tackle heterogeneous nodes with
    different CPU architectures, memory, and network connectivity by extending K8s
    with a monitoring tool. The proposed dynamic scheduler can reschedule stateless
    applications and provide failure recovery in case of a Raspberry Pi node failure.
    [88] The authors propose to take the physical, operational, and network parameters
    into consideration, along with the software states to orchestrate edge-native
    applications dynamically on 5G. The approach extends the limits of the nodes’
    capabilities. [122] The authors present an intelligent K8s scheduler that takes
    into account the virtual and physical infrastructure of complex data centers.
    More precisely, a server hardware behavior model is generated using a wind tunnel.
    Results carried out in real data centers show that introducing hardware modeling
    into the scheduler maximizes effectiveness (around 10–20%). Show More Table 4.
    Kubernetes Scheduling Proposals Related to the Physical Layer Finally, we discuss
    and summarize a comparative analysis of the approaches in this subsection. In
    a nutshell, our review of K8s schedulers over the physical layer shows that node
    hardware properties are a key aspect of the K8s scheduler. K8s faces the challenge
    of scheduling limited computing hardware devices, such as SBCs or specialized
    hardware accelerators. Overall, CPU hardware models enhance the K8s scheduler
    by including both static (RAM, vCPU) and dynamic (power consumption) information
    about the hardware devices. In addition, static profile-based models require hardware
    analysis in advance, while dynamic hardware models rely on real-time monitoring.
    We found that contributions to scheduling GPU devices in K8s are still limited
    and at an early stage. Currently, state-of-the-art GPU scheduling techniques focus
    on a single application type and ignore GPU utilization. Typically, the application
    is pre-profiled to meet the application requirements. 5.1.2 Virtual Layer. First,
    we will describe the virtual layer issues that impact the K8s scheduler focusing
    on container interference and the network layer. On one hand, virtualization allows
    multiple software-based entities to run on a single physical device that supports
    both multi-tenancy and multi-instance, as it provides resource isolation and fault-tolerance
    between different client organizations or tenants. In a multi-tenancy architecture,
    a single instance of the software runs on serving multiple tenants, while in a
    multi-instance model each tenant has their own virtualized instance of the application.
    As a rule, a publicly accessible cluster needs to be multi-tenant [49]. On the
    other hand, K8s nodes connect to a virtual network to provide connectivity for
    pods. The network model allows K8s to assign each pod and service its IP address
    [97]. Software-defined overlay networks, such as Flannel or Calico, are software
    components that decouple the physical infrastructure from networking services.
    Moreover, K8s network virtualization provides a software-based administrative
    network entity for a tenant. Software-defined networking (SDN) and network function
    virtualization (NFV) are options adopted for managing the network through software.
    While SDN leverages the separation of the control and the data plane from underlying
    routers and switches, NFV decouples the networking functions from the underlying
    proprietary hardware covering both containerized network function (CNF) and conventional
    Virtualized Network Function (VNF) [49]. Next, a brief overview of the related
    contributions will be presented. Focusing on container interference, some guidelines
    are derived in [3] for container schedulers to consider when container orchestration
    platforms, such as K8s, are deployed on top of a virtualized IaaS layer. The evaluation
    results show that a suitable combination of virtual machines and containers provides
    maximum flexibility. However, attention must be paid to the interaction between
    these two virtualization layers, as inappropriate container scheduling in the
    virtualized infrastructure can easily lead to unacceptable performance degradation.
    Table 5 summarizes the contributions of some relevant papers on the topic. Table
    5. Ref.Year Fe. Proposal [72] interference The authors develop a reference net-based
    model (a kind of Petri net) for resource management within K8s, to better characterize
    performance issues and mitigate the effects of interference. [11] KubeSphere presents
    a multi-tenant policy-driven meta-scheduler to provide fairness among competing
    users. [56] The authors design an interference minimization model coupled with
    minimum energy utilization and maximal green energy consumption for scheduling
    industrial IoT applications. The proposed scheduler leads to improved energy utilization
    and minimal interference around 14% and 31%, respectively, in contrast to a standard
    first-come first-served scheduler. Show More Table 5. Kubernetes Scheduling Proposals
    Related to Virtual Layer On the other hand, networking is a central part of K8s
    and the K8s networking model sets predefined rules, i.e., each pod must have its
    own IP address. Several open-source Container Networking Interface (CNI) plugins
    are implemented to deploy a K8s production cluster, including Flannel, Weave,
    Calico, Cilium, or kube-router. They support overlay tunnel options in the form
    of virtual networks of nodes and logical connections built on the physical network
    of the cluster. In [98], the authors compare different CNI plugins depending on
    the need for intra-host or inter-host pod-to-pod communication. The evaluation
    results show that there is no single, universal “best” CNI plugin. Typically,
    the network topology in K8s clusters is very uniform and the K8s scheduler overlooks
    this network feature. Nevertheless, the cloud-edge infrastructure is not flat
    and it is very common that the infrastructure is hierarchically organized and
    contains many gateway edge devices managing hundreds of IoT devices. In this case,
    network-aware scheduling in K8s is particularly important, as described by the
    Polaris scheduler proposal in [83]. Network topology and traffic are also taken
    into account by the scheduler approaches proposed in [19, 85, 107]. Table 5 presents
    a brief description of their contributions in the context of network characteristics.
    In general, the results show that distributed IoT applications significantly improve
    the service delivery of the default K8s scheduler. In addition, we found several
    recent works on NVF and SDN in the reviewed literature that relate to the virtualization
    network layer [54, 99, 108, 109]. In general, these K8s scheduling proposals overcome
    the many limitations of NFVs by managing network functions with containers that
    enable fast and scalable deployment. Note that in an overloaded NF, packet loss
    is inevitable and the compute resources used by these packets are wasted, making
    efficient scheduling a challenge. Table 5 sets out the proposals of the related
    works in the reviewed literature classified under the NVF feature. Finally, a
    comparative analysis of the approaches of this subsection is discussed and summarized.
    The default K8s scheduler is not aware of hardware resource fragmentation and
    container interference leads to poor isolation of multi-tenant applications. Currently,
    it is difficult to achieve performance isolation for multi-tenancy on K8s clusters
    because this requires providing resource isolation and improving the abstraction
    of applications. Some scheduling mechanisms were introduced with their isolation
    capabilities in mind, and all of these mechanisms can provide some degree of isolation.
    Some scheduling strategies in K8s assign tenants to a defined set of cluster nodes
    so that their workload profiles do not interfere with each other and thus, avoid
    mutual influences on performance. However, this is only possible if the workload
    of the tenants is predictable. The K8s scheduler has also been extended with the
    development of control feedback loops that dynamically adjust the workload in
    the cluster and ensure the isolation of resources without requiring prior per-workload
    parameterization. Several of the proposed scheduling approaches reduce inference
    by integrating rescheduling and load balancing techniques. With respect to the
    virtualization network layer, based on the evaluated studies, we find that network
    topology plays an important role in fog/edge computing environments for IoT applications
    to minimize latency and maximize the throughput of client requests. In addition,
    some papers addressed the scheduling of flows in an NFV environment to handle
    the diversity of use cases envisaged by 5G technology. The proposed K8s scheduling
    techniques address the performance of the deployed NFs by taking into account
    the workload and resources of the worker nodes. 5.2 Cluster Domain In this taxonomy,
    the cluster domain is classified into two categories: (a) the characteristics
    of the components and (b) the environment. These subdomains are directly related
    to the following questions: — What key cluster-related components impact the K8s
    scheduling? — What characteristics of the computing paradigm affect the K8s scheduling?
    A K8s cluster is composed of a set of nodes that run different software components
    and different design options may impact the final performance of the K8s scheduler.
    So, the component subdomain comprises design parameters related to the scheduler
    architecture and multi-cluster or federation scheduler, as well as resource management
    components closely linked to the scheduler in K8s. The environment in the cluster
    domain refers to the computational technologies where K8s can be deployed. They
    differ in their design and purpose such as cloud or fog computing. Next, more
    details are provided. 5.2.1 Components. First, we will describe component issues
    that affect the K8s scheduler focusing on the scheduler architecture, the coordination
    of federated clusters, and resource management controllers that impact the scheduling.
    The scheduler architecture feature has been analyzed in works such as [110] or
    [18] for other orchestrators. In K8s, the default scheduler runs as a centralized
    controller on the master node that schedules only one task at a time [7]. It has
    a global view of the system which enables it to make better optimization decisions,
    but it is a single point of failure and suffers from scalability issues. So, custom
    schedulers extend the K8s scheduler architecture providing schedulers with modular,
    two-level, or distributed architectures [11, 23, 67, 137]. In [11], the authors
    improve the K8s monolithic scheduler to a two-level scheduler called KubeSphere.
    It receives tasks from users and dispatches them to a connected K8s cluster based
    on custom-defined policy and fairness goals. Results show that KubeSphere improves
    the fairness among users over the default monolithic K8s scheduling and reduces
    the overall average waiting time for all users in a cluster. In [137], the authors
    propose a two-level asynchronous scheduling model within and between the clouds.
    Results show that the fair allocation mechanism proposed guarantees users’ QoS.
    In [67], the authors propose a two-level scheduling algorithm for network slicing
    toward 5G. The high-level Global Scheduler deals with the assignment of the infrastructure
    resources, such as finding appropriate nodes. At the low-level, the Slice Scheduler
    performs the optimal placement for service instances considering the Best Fit
    or the First Fit heuristics. The evaluation results performed show that this approach
    is very promising for 5G slicing with large-scale orchestration systems. In [23],
    the authors describe a custom distributed scheduler for K8s based on Multi-Agent
    System (MAS) platform that delegates the scheduling decision among agents in the
    processing nodes. The distribution of the scheduling task is achieved by transferring
    node filtering and node ranking jobs into an MAS that waits for the notification
    of the winning candidate. In [46], the authors proposed a multi-agent decentralized
    dispatch based on deep reinforcement learning for edge-cloud systems. They observed
    that the scheduling delay of centralized service orchestration is almost 9 × than
    that of decentralized request dispatch. These delays are not trivial for some
    delay-sensitive service requests. Several works in the literature focused on the
    design of a multi-cluster scheduler [14, 38, 50, 105, 117]. In [38], KubeFed is
    extended with a two-phase placement scheme compatible with the cluster federation.
    KubeFed is a framework generally used to split workloads on different cloud providers
    to avoid lock-in. The proposed scheme implements a decentralized control plane
    that distributes microservices among the federated infrastructure and avoids disruption
    of application services by providing fault tolerance. Foggy is presented in [105]
    as an architectural framework for workload orchestration in a multi-tier, highly
    distributed, heterogeneous, and decentralized cloud computing system. Foggy acts
    as a matchmaker between infrastructure owner and tenants. In [50], the authors
    present a deep reinforcement learning-based job scheduler for scheduling independent
    batch jobs among multiple federated cloud computing clusters adaptively. By directly
    specifying high-level scheduling targets, the scheduler interacts with the multi-cluster
    environment and automatically learns scheduling strategies from experience without
    any prior knowledge. In [117], the authors propose an orchestration platform for
    multi-cluster applications on multiple geo-distributed K8s clusters. The platform
    allocates the requested resources to all incoming applications while making efficient
    use of resources. K8s controllers are designed to automatically place, scale,
    and burst multi-cluster applications across multiple geo-distributed K8s clusters.
    In [14], the authors extend the K8s control plane for provisioning compute resources
    across different public cloud providers and regions. The scheduler processes end-user
    requests for on-demand video streaming and optimizes the use of computing resources
    provisioned by green energy resources. Regarding the control plane, K8s provides
    controllers that work in coordination with the scheduler, such as load balancers
    or deployments and replica sets to heal container failures. In [34], the authors
    present an algorithm for scheduling heterogeneous tasks in clustered systems to
    ensure that all devices or processors perform the same amount of work in an equal
    amount of time. The approach configures a dedicated cluster to a particular task
    and introduces load balancing techniques using task migration. In [84], K8s is
    used to coordinate parked vehicles running sufficient numbers of task replicas,
    providing high service availability against possible failure caused by the mobility
    of the vehicles. In [139], the authors present a redundant placement policy for
    the deployment of microservice-based applications at the distributed edge based
    on a stochastic optimization problem. In [140], the authors develop an optimized
    autoscaler for HTC workloads that schedules the workload and resizes the container
    cluster based on the resource utilization of complete jobs, the real-time status
    of the job queue, and the resource initialization time of the cluster manager.
    In [116], the authors design a dynamic resource scheduler for TensorFlow-based
    training jobs which runs on top of K8s cluster. A dynamic allocation of resources
    is performed periodically by checking whether resource utilization in the cluster
    has achieved a certain threshold. In [101], the authors design a dynamic based-utilization
    autoscaling system in K8s. More precisely, if an application’s usage is beyond
    10% of the allocated resources, the container which is running the applications
    is re-scheduled. The migration is based on checkpoints to avoid killing the application.
    In [86], K8s resource metrics and Prometheus custom metrics are collected, calculated,
    and fetched to the horizontal pod autoscaling. The ongoing experiments show how
    these metrics affect the final performance. To sum up, a final discussion based
    on reviewed research is presented. Scheduler architecture is an important issue
    in K8s scheduler that some works have paid attention to. We observed that most
    of the schedulers with a two-level architecture decouples the allocation of resource
    and placement decision and that this feature has a high impact on cluster scalability
    [110]. Moreover, multi-cluster schedulers coordinate the configuration of multiple
    K8s clusters tackling scalability and geo-distributed environments, especially
    for cloud-fog-edge ecosystems. Most of the papers focus on providing a fair resource
    allocation while satisfying the requirements of the applications. Availability
    and energy-aware are also issues related to the cluster federation scheduler that
    some research studies have considered. Finally, it must be highlighted that many
    papers in the literature reviewed describe scheduling proposals together with
    load balancing, migration, or replication techniques to improve resource fairness,
    resilience, and availability. 5.2.2 Environment. First, we will describe the considered
    computational architectures considered in this domain, that is, the cloud, fog,
    and edge paradigms. Then, we will provide a brief overview and comparative analysis
    of the different K8s scheduling approaches categorized by the environment. The
    centralized cloud computing paradigm provides on-demand resource allocation through
    the Internet based on a pay-as-you-go model [69]. Edge/Fog computing is assumed
    as a distributed system that makes storage and computing resources closer to the
    place where data are generated, as compared to the cloud computing-based approach.
    This is mainly driven by the data explosion caused by the wide adoption of the
    Internet of Things (IoT) [16, 82]. In particular, Fog computing [15] makes use
    of IoT gateways and other computing devices within the edge network, such as smart
    routers, SBCs, personal computers, and even micro-data centers to process IoT
    data. Edge computing also supports the execution of applications near the data
    sources but using the IoT devices and the gateways of IoT devices to process data.
    A related term that also introduces storage and processing power at or near the
    edge of the network is MEC. MEC servers are owned by mobile operators and are
    located near the base stations so that they can be accessed over the Radio Access
    Network (RAN). They are mainly targeted at the support of mobile services. Edge,
    fog, and MEC are relatively new computing paradigms, and their evolution processes
    are still ongoing. So, many researchers and industries adopt different approaches.
    A more detailed explanation, which is out of the scope of this article, is provided
    in [68]. Some K8s scheduling proposals, classified by the environment domain (Fe.),
    are described in Table 6. Table 6. Ref.Year Fe. Proposal [24] cloud The paper
    presents a platform that includes a comprehensive monitoring mechanism and a dynamic
    resource-provisioning operation [10] OASIS is an online algorithm for admitting
    and scheduling asynchronous training jobs in a machine learning cluster. The number
    of concurrent workers and the server parameters for each job are dynamically adjusted
    to maximize the overall utility of all jobs based on their completion times [67]
    The paper describes a multi-level scheduling solution for mobile network slicing
    Show More Table 6. Cluster Domain Architecture of Some Kubernetes Scheduling Proposals
    According to reviewed studies, we observed that K8s scheduler has to be aware
    of the characteristics of the computational paradigm where is deployed to efficiently
    orchestrate containerized applications and provide efficient resource usage and
    low response time. Some reviewed papers proposed an efficient distribution of
    load in cloud-fog environments using the computational power of edge devices by
    offloading. For example, in [84], the authors provide a flexible and extended
    container-based architecture for online task offloading problems by leveraging
    the powerful onboard computing resources of parked vehicles. K8s coordinates parked
    vehicles to run sufficient numbers of task replicas, providing high service availability
    against possible failure caused by the mobility of parked vehicles. Moreover,
    the fair allocation mechanisms and scheduling approaches proposed for cloud computing
    are not always efficiently applicable in edge/fog architectures that have to deal
    with the new challenges of the environment, such as low computational capacity
    resources spread across different geographical locations [43]. In general, it
    can be claimed that scheduling in edge/fog computing architectures requires fully
    dynamic, network-aware, and fault tolerance solutions. But, having the computational
    resources close to the data generated by IoT devices has several benefits, such
    as low round-trip latency, minimizing the use of public network bandwidth, and
    most importantly, providing accelerated analysis and better user experience. In
    the reviewed literature, numerous approaches enhance the functionality of the
    K8s scheduler to overcome the mentioned challenges. Also, some recent proposals
    built K8s schedulers for edge-native applications on 5G. 5.3 Scheduling Domain
    The scheduling domain is classified based on the mathematical technique used by
    the scheduler and on the metrics considered by the algorithm as input parameters
    to be optimized. The subcategories match the following questions. — What type
    of mathematical model is used to select the worker node? — What metrics are used
    in the design of the K8s scheduling? Table 7 summarizes the main characteristics
    of the proposed solutions for K8s related to the scheduling domain. Table 7. Ref.
    Alg. Metrics System Resource App. perf. Other metrics CPU RAM Bw GPU Res. time
    Com. time [24] heuristic ✓ ✓ ✗ ✗ ✓ ✗ [10] ✓ ✓ ✓ ✓ ✗ ✓ job’s utility and a price
    function [67] ✓ ✓ ✗ ✗ ✗ ✗ (best-fit, first-fit) Show More Table 7. Classifying
    Kubernetes Scheduling Proposals in the Scheduling Domain 5.3.1 Scheduling Algorithms.
    They refer to the mathematical computation performed by the scheduler module that
    matches a task into a node. The scheduling problem is an NP-hard problem meaning
    that there is no polynomial complexity algorithm to find an optimal schedule for
    large-size problems. This work categorized scheduling algorithms into mathematical
    modeling, heuristic, meta-heuristic, machine learning, and gang scheduling. These
    categories differ in computational cost and performance. An exhaustive survey
    on container-based scheduling techniques is presented in [2]. The heuristic approaches,
    that are widely used in traditional scheduling, are generally of low complexity,
    and generate a satisfactory schedule in a reasonable amount of time. Some of the
    most well-known heuristic strategies are the first-fit [67], First-In-First-Out
    (FIFO) [105], and DRF [11, 44]. Besides, some mathematical modeling techniques
    analyze the scheduling problem as a set of constrained equations, such as the
    integer linear programming (ILP) problem, and then use standard optimization techniques
    to find the best solution to the problem. ILP methods present a high computational
    cost and they can only be used for small size problems [56, 84]. In the container-based
    resource scheduling problems, there are also some sophisticated proposals based
    on meta-heuristics including, for example, genetic, particle swarm, or ant colony
    algorithms [65]. Meta-heuristics are a popular class of population-based optimization
    algorithms inspired by the intelligent processes and behaviors arising in nature
    [52]. Two important characteristics of such algorithms are a selection of the
    fittest and adaptation to the environment. In general, metaheuristic algorithms
    can be divided into evolutionary algorithms (EAs), such as Genetic Algorithm (GA),
    or swarm intelligence algorithms, such as Ant Colony Optimization (ACO), Particle
    Swarm Optimization (PSO), and Whale Optimization among others [77]. Machine learning
    algorithms are considered in this classification. These techniques have the availability
    of learning and training from big data to make intelligent scheduling decisions
    based on what they have learned. Deep machine learning is a subfield of machine
    learning that uses a layered structure of artificial neural networks to learn
    and make intelligent decisions on its own and is also applied in K8s scheduling
    [9, 50, 133]. For example, a deep learning-driven scheduler for deep learning
    clusters is presented in [90]. The proposed scheduling algorithm streamlines overall
    job completion with efficient resource utilization. The proposal includes both
    offline supervised learning and live feedback through reinforcement learning.
    Related to K8s, many scheduling algorithms have been proposed that fit in the
    categories mentioned above. Table 7 categorizes some proposals attending to this
    classification. At this point, it must be highlighted that the K8s default scheduler
    and all the previous proposals manage unbound pods following a sequential process.
    However, some applications will get better performance using gang scheduling techniques
    that operate considering bundled tasks to be scheduled. Note that several tasks
    can be part of the same application. Hence, the orchestrator logic can obtain
    a more optimal deployment of the application if it is aware of the communication
    and relationship that exists between the tasks. For example, deep learning training
    applications can potentially profit from gang scheduling, because they are resource-intensive
    applications with strong dependency and high inter-task communication. Designing
    a dependency-aware task gang scheduling will focus on reducing communication overhead
    and avoiding wasted resources because a task cannot start computing before all
    its peer tasks are launched. In the reviewed literature some proposals focus on
    gang scheduling techniques, such as in [13, 64]. In particular, DRAGON is presented
    in [13] as an extended controller for a gang scheduling in which all the tasks
    of a training job are scheduled as a single unit in K8s. In [13], the author combines
    DRAGON’s and OASIS’ approaches to make a scheduler with weighted autoscaling capabilities
    and schedule its jobs with gang scheduling. Note that OASIS is a custom scheduler
    on K8s that calculates the payoff of each job in the queue using the job’s utility
    and a price function [10]. 5.3.2 Metrics Considered in Scheduling. The scheduling
    algorithms can also be classified attending to the optimization metrics. In the
    literature reviewed the scheduling algorithms are aware of different contextual
    metrics related to the infrastructure, the cluster, and the application domains.
    Metrics such as resource utilization (computation, storage, network), failure
    rate, interference, or energy are related to the infrastructure context. Then,
    location, mobility, stability, reliability, and availability are metrics related
    to the cluster domain. Finally, application requirements are usually measured
    in terms of response time, completion time, makespan, or resource demand (see
    Section 5.4 for more details). These metrics allow the scheduler to be application
    aware. Table 7 shows a sample of the more relevant metrics included in some of
    the papers reviewed in this work. The parameters most widely used by the reviewed
    scheduling algorithms are related to CPU and RAM utilization [9, 11, 13, 20, 121].
    For example, the authors propose in [13] a dynamic resource scheduler that prioritizes
    jobs with higher resource requirements and tries to schedule higher priority jobs
    first to maximize resource utilization and reduce the need to call scale function.
    In [11], the authors develop a scheduling layer on top of K8s’ monolithic scheduler
    to improve resource fairness based on DRF, user CPU and memory demand. Moreover,
    some efforts considered the network traffic as an important metric [19, 85, 107].
    In [106, 107], the authors address the orchestration of applications in a smart
    city by developing a network-aware scheduling approach. The default scheduler
    of K8s is extended to consider network infrastructure status for taking resource
    provisioning decisions. On top of the K8s platform, ElasticFog proposed in [85]
    collects network traffic information in real-time and enables real-time elastic
    resource provisioning for containerized applications in Fog computing. Furthermore,
    we observe that several research studies on this topic have considered application
    requirements or user demand (see Res. time (Response time) and Com. time (Completion
    time) metrics in Table 7) but are less common than system context-aware metrics.
    Scheduling proposals that, for example, explicitly consider the completion time
    of the task are presented in [42, 88, 137]. Likewise, storage is a metric related
    to the infrastructure context, but few reviewed papers focus on it. For example,
    in [128], the authors present a storage service orchestration with container elasticity.
    Security and carbon footprint are parameters also considered in recent works.
    Overall, the metrics used by the scheduling algorithms are intimately related
    to the monitoring module that reports the state of the system. For example, in
    [42], the authors propose a container placement scheme that balances the resource
    contention on the worker nodes combining resource usage with expected completion
    time. A suite of algorithms monitors the progress of running containers and the
    collected data are utilized to calculate their expected completion time. In [133],
    the authors propose a mechanism of re-scheduling the pod on the worker nodes by
    predicting the utilization rate of application resources. A combination model
    of grey system theory with long short-term memory neural network prediction is
    proposed. In [37], the authors optimize the prediction of the number of pods for
    Knative. Knative is a popular Kubernetes-based platform for managing serverless
    workloads. Note that serverless is an event-driven computing model that abstracts
    the operational logic from the user providing fine granularity [63]. The predictive
    model is based on double exponential smoothing. In short, in the literature there
    are scheduling algorithms that rely on the information obtained at runtime of
    the system, and other algorithms that extend this set of metrics to include the
    expected near-future behavior of the system. That is, the scheduler system selects
    a worker node based not only on the current resource demand, but also on the estimate
    or prediction of the future resource demand. To this end, scheduling algorithms
    usually include predictive or profiling techniques for application behavior or
    resource utilization. 5.4 Application Domain In general, the characteristics of
    the applications that will run in Kubernetes-controlled clusters affect scheduling,
    as each application has different QoS requirements that must be met [73]. In the
    literature reviewed, many of the scheduling proposals take into account the characteristics
    of the applications to be developed [19, 55, 84, 107], but other techniques are
    presented in a generic way [23, 34]. The application domain is classified into
    three subdomains according to (a) the type of task or application, (b) the communication
    architecture of the task, and (c) the workload generated by a set of tasks. The
    subdomains match with the following questions: — What type of applications are
    managed by the K8s scheduler? — What application architectures are managed by
    K8s scheduler? — How does the workload applied to the system affect the scheduling
    of K8s? 5.4.1 Type. The K8s default scheduling features are not designed specifically
    for a particular type of application. The scheduler handles a wide range of applications
    from High-Performance Computing (HPC), machine learning, batch, web server, IoT,
    or serverless applications. In the literature reviewed, we found some generic
    scheduling proposals, for all types of applications, but most proposals optimize
    their design for a specific type of application. For example, IoT applications
    impose specific challenges to be considered by the scheduler, such as stringent
    latency, capacity constraints, or uninterrupted services with intermittent connectivity.
    Below we will briefly mention some proposals, but Table 8 shows more examples.
    For instance, a scheduler for big data and training neural network models is proposed
    in [42], A K8s compatible scheduler for 5G edge computing applications is developed
    in [88], SpeCon [70] is proposed as a novel container scheduler optimized for
    deep learning applications, also a green energy scheduler for on-demand video
    streaming workloads is presented in [14], and the problem of running HPC workloads
    efficiently on K8s clusters is addressed in [75]. Moreover, several proposals
    support the Function as a Service (FaaS) paradigm in which the user executes a
    short-lived code without caring about the server used. In fact, Kubeless [95],
    OpenFaas [45], OpenWhisk [33], or Fission [76] are open-source serverless computing
    frameworks designed to run on top of K8s [95]. They have different architectures
    but all of them take responsibility for managing, scaling, and providing different
    resources to ensure the correct execution of functions triggered by events. The
    detailed serverless architecture, which is out of the scope of this article, is
    provided [112]. Scheduling proposals for Kubernetes-based serverless are proposed
    in [37, 66]. So, Knative’s open-source framework for Kubernetes-based serverless
    is enhanced in [37] by calculating the optimal number of pods to deploy. In [66],
    the authors present Pigeon, a FaaS framework for private cloud. Pigeon uses function-level
    resource scheduling so that the FaaS function can be directly dispatched to pre-warmed
    containers, reducing limitations imposed by K8s and increasing system performance.
    Another novelty of this framework is the introduction of an oversubscription-based
    static pre-warmed container pool. This approach eliminates the necessity of dynamically
    creating containers during burst function trigger situations and improves system
    dynamics. Table 8. Ref. System Design & Evaluation App. Framework Metrics Results
    [11] cluster synthetic testbed (4 nodes) number of tasks, waiting time Resource
    fairness is improved over default scheduling using DRF, user resource demand and
    their combination [34] simulation (3 clusters) makespan, throughput and response
    time metrics Processing time is reduced compared to the round-robin algorithm
    [50] multi-cluster simulation makespan, resource utilization The average utilization
    of each cluster is quite stable, the makespan sometimes increases Show More Table
    8. Performance Domain Characteristics of Some Kubernetes Scheduling Proposals
    Based on reviewed research studies, we found that some application-aware scheduling
    algorithms abstract application characteristics by including metrics such as duration
    (completion time) or resource requirements, see Table 7. So, applications can
    be classified as long-lived or short-lived applications depending on their duration.
    As a rule, user-facing applications and web servers are long-lived applications,
    while batch and serverless applications are usually short-lived applications.
    For example, in serverless, a function or service normally has a maximum time
    limit of up to 15 minutes. On one hand, demand refers to the type of resource
    an application or a task consume. In general, orchestrators must be aware of the
    hardware requirements of the applications to ensure their operation and also provide
    efficient resource management. For example, an application will crash and finally
    will be killed if it does not get the minimum memory capacity it needs. Note that
    memory is a non-compressible hardware requirement while the CPU requirement is
    a compressible resource and oversubscriptions are supported. Some reviewed papers
    are provided with application specification parameters in advance and the scheduler
    can use them to improve the scheduling. Other papers tackle the problem as soon
    as they are detected using re-scheduling and autoscaling. 5.4.2 Architecture.
    The architecture in the application domain is divided into independent and dependent
    tasks. A container can just run a single task, which is the simplest application
    case, or more than one task as a monolithic application. Note that K8s provides
    affinity and anti-affinity properties with pod metadata such as nodeAffinity and
    podAffinity. A user can define these parameters to run a pod on a node (or not)
    depending on where other pods are running. Today most containerized applications
    make use of the microservice approach, in which multiple dependent services must
    be allocated in the worker nodes having into account the communication pattern.
    So, in [139], the authors design a redundant placement policy for the deployment
    of microservice-based applications with sequential combinatorial structure at
    the distributed edge. In [129], the authors propose NetMARKS as a K8s scheduler
    that is aware of inter-pod dependencies and uses dynamic network metrics collected
    with Istio Service Mesh. They automatically discover inter-application relations
    to ensure efficient placement of SFCs. The authors’ analysis shows that the proposal
    reduces application response time by up to 37 percent and saves up to 50 percent
    of bandwidth in a fully automated manner. The dependent function embedding problem
    at the serverless edge is studied in [30]. The authors design an algorithm to
    get the optimal edge server for each function. The algorithm represents a function
    as a directed acyclic graph (DAG) and tries to minimize its completion time. Nevertheless,
    current state-of-the-art research does not sufficiently consider the composite
    property of services. 5.4.3 Workload. The workload determined by the set of different
    applications to be run on the system poses extra challenges to the K8s orchestrator.
    For example, the end-user experience of user-facing service might be degraded
    due to sharing resources with batch jobs. Production K8s clusters typically run
    each type of workload on different nodes to avoid this performance interference,
    which leads to under-utilization. Low resource utilization of K8s cluster results
    in high operational cost and substantial resource wastage. Running different types
    of applications on the same resource can effectively improve resource efficiency.
    For example, the authors in [138] extend K8s mechanisms to schedule best-effort
    jobs based on the real server utilization and adaptively allocate resources between
    best-effort batch jobs and latency-sensitive services. The evaluation results
    demonstrate that the average CPU utilization increases from 15% to 60% without
    SLOs violations. K8s may suffer from low resource utilization due to services
    being over-provisioned for the peak load, and the isolation between different
    K8s containers is limited [58, 72, 123, 138]. The impact of isolation and interference
    applications has been analyzed in the literature reviewed, such as we have detailed
    in Section 5.1. 5.5 Performance Domain The performance domain analyses the maturity
    of K8s scheduling by considering the frameworks designed to evaluate the proposals
    and also the metrics to evaluate the performance. So, this domain is related to
    the following questions: — What are the frameworks designed to evaluate the K8s
    scheduling? — What are the performance metrics utilized to evaluate K8s scheduling
    approaches? 5.5.1 Framework. The framework developed to evaluate the K8s scheduler
    proposal is an important aspect of the performance domain. More precisely, the
    level of abstraction of the framework is a critical aspect of the evaluation results.
    Simulation tools for K8s schedulers tend to abstract aspects that are not relevant
    for their evaluation, obtaining high scalability in terms of the number of worker
    nodes or providing multi-clusters environments [50]. However, special attention
    must be paid to the degree of abstraction so that the results obtained reflect
    the behavior of the real system. Testbed deployments solve this problem but are
    usually small-scale clusters [24, 88, 133]. Table 8 summarizes the frameworks
    used in some of the reviewed papers along with some other relevant properties
    of the performance domain, such as the platforms assessed (labeled as System)
    or the applications deployed in the framework (labeled as App.). While the system
    is related to the cluster domain described in Section 5.2, the application is
    connected to the application domain detailed in Section 5.4. Note that in the
    performance evaluation domain, we find out real applications such as IoT, web
    servers, or deep learning applications but also synthetic loads or traces that
    extract the main characteristics of real applications. From the literature reviewed,
    we note that many evaluation analyses make use of simulation techniques to devise
    how efficient the scheduling approaches are. In all the papers reviewed, the simulator
    is a custom design without standard rules. Besides, we observe that most frameworks
    design a small testbed consisting of several physical servers (see Table 8). In
    addition, some proposals are developed in virtual servers. For example, the authors
    make use of GKE in [13] to deploy the testbed. These frameworks address the problem
    of scalability but lack full control of the system. 5.5.2 Metrics. In the literature,
    all the proposals utilize some metrics to quantify the performance of the proposed
    solution. In general, the evaluation metrics are related to the objective to be
    optimized by the scheduling algorithms. In some papers, performance evaluation
    focuses on a single metric, but most examine several metrics. The more metrics
    used in the performance evaluation, the more complex the decision-making process
    will be. Therefore, different tradeoffs are usually established to balance the
    efficiency of the proposed scheduling algorithm and the final quality of the generated
    solution. As a representative example, Table 8 gathers the metrics used in some
    of the works reviewed in the literature. From the literature reviewed, we observed
    that completion time is the most commonly used metric for evaluating the effectiveness
    of scheduling algorithms. Similarly, makespan and cost are the second and third
    most frequently used parameters Performance analyses that include energy consumption
    are less frequent. To sum up, the metrics that evaluate the scheduler performance
    are related to the QoS provided to the end-user and the cost from the resource
    provider’s point of view. System QoS is measured by parameters such as total processing
    time or makespan and completion time, while CPU and memory resource utilization
    is related to system cost. Skip 6FUTURE DIRECTION AND OPEN ISSUES Section 6 FUTURE
    DIRECTION AND OPEN ISSUES Having presented a literature review and a taxonomy
    on K8s scheduling, we find that there are still some open key issues worth for
    further discussion to improve resource management in K8s. In this section, we
    discuss some future research directions to address the remaining challenges. 6.1
    Challenges Related to Infrastructure Domain 6.1.1 New Types of Physical Devices.
    Artificial Intelligence (AI) at the edge is now common in Fog/edge computing.
    AI accelerators such as Graphics Processing Unit (GPU), Field-Programmable Gate
    Arrays (FPGA), or Tensor Processing Units (TPU) are powerful machine learning
    hardware chips specifically designed to run AI and ML applications. In reviewing
    the literature, we found some recent work that integrates GPUs into the K8s scheduler,
    but no work that focuses on other accelerators. Thus, efficient resource management
    of AI accelerators in K8s has not yet been sufficiently explored. 6.1.2 Enhance
    Resource Virtualization Techniques. Several papers analyze the impact of the so-called
    noisy neighbor on job scheduling. The obtained results show that the interference
    between virtualized resources has a great impact on the final performance. Therefore,
    there is a need to improve the isolation of the virtualization layer and model
    its behavior in order to improve container scheduling in K8s. For example, a container
    should never cause the node’s operating system to fail. Otherwise, all the tenants
    running containers in the node will see their performance degraded, as well as
    future scheduled containers. Further research in this area is highly recommended.
    With respect to network virtualization techniques, VNFs must be scheduled to optimize
    the deployment of services. Therefore, to provide performance guarantees, it is
    necessary to avoid nodes with very low availability or over-loaded, and to identify
    bottleneck links. Algorithms and machine learning techniques can be a good solution
    to achieve better results. 6.2 Challenges Related to Cluster Domain 6.2.1 Distributed
    Control Plane. The K8s scheduling architecture is a centralized system based on
    the master-slave philosophy. Thus, an error in the master node could result in
    a system failure. To overcome this problem, K8s uses multiple master nodes and
    implements robust protocols [32]. Therefore, K8s scheduling should take this fact
    into account to support a fault-tolerant scheduler system and improve availability
    and resilience. Focusing on this issue, the use of distributed and coordinated
    scheduling techniques in K8s requires further study. Moreover, the centralized
    nature of the K8s orchestration system does not align well with the needs of the
    distributed fog/Edge computing environments. Therefore, modifying K8s to remove
    centralized control is an area in need of exploration. In this context, Blockchain
    technology [31] has overcome its initial application in cryptocurrencies to reach
    over other application domains, which harness its immutable, transparent, and
    available information storage capability. When combined with smart contracts it
    can provide a distributed computer where all the nodes independently and equally
    contribute to a common global system state, which must be agreed upon by consensus.
    The integration of blockchain in task scheduling is a recent area to explore.
    A few steps are already taken [61, 87], however, it is still in its infancy stage.
    6.2.2 Collaboration Among Clusters. Seamless integration and cooperation among
    cluster orchestrators is a core requirement for creating a federated and scalable
    framework. Federated frameworks extend the resilience of the system and achieve
    smart services. For example, if the service that controls the path of a vehicle
    cannot migrate to the nearest cluster orchestrator, it may be difficult to get
    a smart route in time. K8s scheduling should provide this collaborative feature
    to improve both high availability and resiliency. K8s programming must also address
    the pricing model for leasing resources, as well as for deciding when to outsource
    or in-source containers. Hence, the use of distributed and coordinated scheduling
    techniques in K8s requires further study. 6.2.3 Inter-Operability. K8s is always
    framed in a layered architecture. The lower layers make use of container virtualization
    techniques where there are some standard efforts, but this is not the case for
    the upper layers. K8s schedulers need to operate with cloud providers and the
    communication APIs are not well defined. Hence, standard APIs are required. 6.3
    Challenges Related to Scheduling Domain 6.3.1 Scalability. K8s orchestration must
    consider aspect related to scalability and adaptation of the system. Most scheduling
    proposals analyze the QoS parameters, such as completion time or makespan and
    extend K8s’ functionality to work in For/Edge architectures. An important aspect
    of successfully running applications in these environments is supporting resource
    management on dynamically scalable hardware architectures. The demand for this
    new approach to successfully support scalable scheduling is reinforced by the
    requirements of mobile vehicle applications. Sophisticated and efficient re-scheduling
    and autoscaling approaches are required to support cluster scalability. 6.3.2
    Advance Context-Aware Scheduling Algorithm. Scheduling algorithms are usually
    based on input metrics related to resource utilization, such as CPU, RAM, disk,
    and network. Considering other important input parameters is a must for scheduling
    algorithms. For example, sustainable scheduling techniques should be explored
    in depth to reduce energy consumption and ensure a lower carbon footprint. New
    K8s scheduling algorithms should help prevent environmental degradation without
    compromising the user experience. In this context, smart monitoring systems are
    becoming a challenge to enhance scheduling algorithms. Moreover, scheduling should
    not only be considered from the static infrastructure point of view. Thus, algorithms
    should consider availability and reliability as first-order metrics. These metrics
    can be very relevant in edge environments where devices are not always on. Consumer-centric
    IoT applications and services, such as connected vehicles or the smart grid, are
    examples of trending applications that should benefit from these scheduling approaches.
    Enriching K8s to allow replication and automatic service redirection to the healthy
    entities has been explored in [125]. However, there are very few contributions
    in the literature on scheduling techniques aware of the dynamic nature of the
    system. The dynamic nature of the resources is a critical feature to consider
    in new scheduling algorithms. To sum up, to overcome the above-mentioned challenges,
    new filtering and affinity ranking models are required. In addition, it can be
    useful to model the mobility pattern of end devices, cluster resource utilization,
    and application requirements. Scheduling algorithms face the challenge of enhancing
    their efficiency by integrating these models. In this regard, a challenging task
    to explore is the development of prediction techniques and runtime estimation
    techniques to obtain the required models. 6.4 Challenges Related to Application
    Domain 6.4.1 Workflows and Microservices. By default, the K8s scheduler considers
    one pod at a time, without considering the relationship between pods belonging
    to the same application. Now, with the challenge of deploying microservices-based
    applications, getting a global optimal scheduling solution for containerized workflows
    is a major issue. The development of novel Gang scheduling techniques can bring
    significant improvements. 6.4.2 Model Workloads. Understanding the characteristics
    and patterns of workloads running on K8s clusters is a critical task for improving
    K8s scheduling. The dynamic nature of the workload suggests that predictive techniques
    can improve Kubernetes’ scheduling techniques. Nowadays, workload prediction is
    still an open research topic. Moreover, in the existing literature, very narrow
    discussion toward providing a well-established benchmark of representative applications
    has been provided. It is also an open research topic. 6.5 Challenges Related to
    Performance Domain 6.5.1 Comparability and Usability. There are many proposals
    related to job scheduling, but they are isolated proposals with little interaction
    between them, in the sense that there is no continuous improvement. For example,
    at the level of scheduling algorithms in K8s, new proposals are often compared
    to simple scheduling algorithms such as round-robin or the default-scheduler.
    Providing CI of research would foster further progress in resource scheduling.
    On one hand, simulators specifically designed to evaluate scheduling proposals
    are used. In the works studied that evaluate proposals using simulation tools,
    there is a lack of an open-source tool that acts as a de facto standard. The fact
    that the evaluation environments are developed in an ad-hoc manner does not allow
    for efficient and effective analysis of the different proposals. As for evaluations
    in non-simulated environments, small test environments with 3–5 physical servers
    are usually used. It would be desirable to have a real-world platform where new
    ideas can be developed and tested incrementally (i.e., adding more features or
    selecting the appropriate ones with the ability to compare them to other proposals).
    6.5.2 Security Issues. Security is one of the main concerns of distributed systems.
    None of the reviewed scheduling techniques developed for K8s are aware of this
    fact. Scheduling should take into account the behavior of system devices to reduce
    vulnerabilities in the system. Smart scheduling should refuse to deploy containers
    on nodes with suspicious behavior that pose a threat to the system, such as when
    DDoS attacks are detected. Skip 7CONCLUSIONS Section 7 CONCLUSIONS Container orchestrations
    automate the deployment, management, scaling, interconnection, and availability
    of container-based applications. K8s is the open-source standard de facto orchestrator.
    In this survey article, we investigated the state-of-the-art of K8s functionality
    and reviewed related orchestration systems. The literature reviewed on K8s scheduling
    was studied to identify and classify the research efforts and present a taxonomy.
    Finally, possible future research directions to improve the orchestration in K8s
    clusters are identified. Footnotes 1 https://kubernetes.io/docs/concepts/architecture/cri/.
    Footnote 2 https://github.com/kubernetes/community/tree/master/contributors/devel/sig-scheduling.
    Footnote 3 https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/624-scheduling-framework.
    Footnote REFERENCES [1] Adhikari M., Amgoth T., and Srirama S. N.. 2019. A survey
    on scheduling strategies for workflows in cloud environment and emerging trends.
    Computing Surveys 52, 4, Article 68 (Aug.2019), 36 pages. Reference 1Reference
    2 [2] Ahmad I., AlFailakawi M. Gh., AlMutawa A., and Alsalman L.. 2021. Container
    scheduling techniques: A Survey and assessment. Journal of King Saud University
    - Computer and Information Sciences 34, 7 (2021), 3934–3947. Navigate to [3] Ambrosino
    G., Fioccola G. B., Canonico R., and Ventre G.. 2020. Container mapping and its
    impact on performance in containerized cloud environments. In Proceedings of the
    2020 IEEE International Conference on Service Oriented Systems Engineering. Navigate
    to [4] Arunarani A. R., Manjula D., and Sugumaran Vijayan. 2019. Task scheduling
    techniques in cloud computing: A literature survey. Future Generation Computer
    Systems 91 (2019), 407–415. Reference 1Reference 2 [5] authors The Docker. 2022.
    Empowering app development for developer. Docker Homepage. Retrieved from https://www.docker.com/.
    Reference [6] authors The Docker. 2022. Swarm mode overview. Docker Swarm Homepage.
    Retrieved from https://docs.docker.com/engine/swarm/. Reference 1Reference 2 [7]
    authors The Kubernetes. 2022. Kubernetes: Production-Grade Container Orchestration.
    Kubernetes Homepage. Retrieved from http://kubernetes.io/. Navigate to [8] Bader
    J., Thamsen L., Kulagina S., Will J., Meyerhenke H., and Kao O.. 2021. Tarema:
    Adaptive resource allocation for scalable scientific workflows in heterogeneous
    clusters. In Proceedings of the 2021 IEEE International Conference on Big Data.
    IEEE, 65–75. Reference [9] Bao Yixin, Peng Yanghua, and Wu Chuan. 2019. Deep learning-based
    job placement in distributed machine learning clusters. In Proceedings of the
    IEEE Conference on Computer Communications. 505–513. Reference 1Reference 2Reference
    3 [10] Bao Yixin, Peng Yanghua, Wu Chuan, and Li Zongpeng. 2018. Online job scheduling
    in distributed machine learning clusters. In Proceedings of the IEEE Conference
    on Computer Communications. Navigate to [11] Beltre Angel, Saha Pankaj, and Govindaraju
    Madhusudhan. 2019. KubeSphere: An approach to multi-tenant fair scheduling for
    kubernetes clusters. In Proceedings of the 2019 IEEE Cloud Summit. IEEE, 14–20.
    Navigate to [12] Bentaleb Ouafa, Belloum Adam S. Z., Sebaa Abderrazak, and El-Maouhab
    Aouaouche. 2022. Containerization technologies: Taxonomies, applications and challenges.
    The Journal of Supercomputing 78, 1 (2022), 1144–1181. Reference 1Reference 2
    [13] Bestari M., Kistijantoro A., and Sasmita A.. 2020. Dynamic resource scheduler
    for distributed deep learning training in kubernetes. In Proceedings of the 2020
    7th International Conference on Advance Informatics: Concepts, Theory and Applications.
    1–6. Navigate to [14] Biran Yahav, Pasricha Sudeep, Collins George, and Dubow
    Joel. 2016. Enabling green content distribution network by cloud orchestration.
    In Proceedings of the 2016 3rd Smart Cloud Networks Systems. 1–8. Reference 1Reference
    2Reference 3 [15] Bonomi F., Milito R., Zhu J., and Addepalli S.. 2012. Fog computing
    and its role in the internet of things. In Proceedings of the 1st Edition of the
    MCC Workshop on Mobile Cloud Computing. ACM, New York, NY. Reference 1Reference
    2 [16] Borgia E.. 2014. The Internet of Things vision: Key features, applications
    and open issues. Computer Communications 54 (2014), 1–31. Reference [17] Bulej
    L., Bureš T., Hnětynka P., and Khalyeyev D.. 2021. Self-adaptive K8S cloud controller
    for time-sensitive applications. In Proceedings of the 2021 47th Euromicro Conference
    on Software Engineering and Advanced Applications. 166–169. Reference [18] Burns
    B., Grant B., Oppenheimer D., Brewer E., and Wilkes J.. 2016. Borg, omega, and
    kubernetes. Communications of the ACM 59, 5 (42016), 50–57. Reference 1Reference
    2 [19] Caminero A. C. and Muñoz-Mansilla R.. 2021. Quality of service provision
    in fog computing: Network-aware scheduling of containers. Sensors 21, 12 (2021),
    3978. Navigate to [20] Carvalho M. and Macedo D. F.. 2021. QoE-aware container
    scheduler for co-located cloud environments. In Proceedings of the 2021 IFIP/IEEE
    International Symposium on Integrated Network Management. 286–294. Navigate to
    [21] Casalicchio E.. 2019. Container Orchestration: A Survey. Springer Int. Publishing,
    Cham, 221–235. Reference 1Reference 2 [22] Casalicchio E. and Iannucci S.. 2020.
    The state-of-the-art in container technologies: Application, orchestration and
    security. Concurrency and Computation: Practice and Experience 32, 17 (2020),
    e5668. Reference 1Reference 2 [23] Casquero O., Armentia A., Sarachaga I., F.
    Perez, D. Orive, and M. Marcos. 2019. Distributed scheduling in Kubernetes based
    on MAS for fog-in-the-loop applications. In Proceedings of the 24th IEEE International
    Conference on Emerging Technologies and Factory Automation. 1213–1217. Navigate
    to [24] Chang C., Yang S., Yeh E., Lin P., and Jeng J.. 2017. A kubernetes-based
    monitoring platform for dynamic cloud resource provisioning. In Proceedings of
    the 2017 IEEE Global Communications Conference1–6. Navigate to [25] Chhajed Saurabh.
    2015. Learning ELK Stack. Packt. Reference [26] authors The Google cloud. 2021.
    Container Registry. Container Registry Homepage. Retrieved from https://cloud.google.com/container-registry/.
    Reference [27] CNCF. 2022. Cloud Native Computing Foundation Charter. Retrieved
    from https://www.cncf.io/about/charter/. Reference 1Reference 2Reference 3 [28]
    Coté M.. 2020. Why Large Organizations Trust Kubernetes. Retrieved from https://tanzu.vmware.com/content/blog/why-large-organizations-trust-kubernetes.
    Reference [29] Cérin C., Menouer T., Saad W., and Abdallah W.. 2017. A new docker
    swarm scheduling strategy. In Proceedings of the 2017 IEEE 7th International Symposium
    on Cloud and Service Computing. 112–117. Reference [30] Deng S., Zhao H., Xiang
    Z., C. Zhang, R. Jiang, Y. Li, J. Yin, S. Dustdar, and A. Y. Zomaya. 2022. Dependent
    function embedding for distributed serverless edge computing. IEEE Transactions
    on Parallel and Distributed Systems 33, 10 (2022), 2346–2357. Reference [31] Pierro
    M. Di. 2017. What is the blockchain? Computing in Science Engineering 19, 5 (2017),
    92–95. Reference [32] Diouf G. M., Elbiaze H., and Jaafar W.. 2020. On byzantine
    fault tolerance in multi-master kubernetes clusters. Future Generation Computer
    Systems 109 (2020), 407–419. Reference [33] Djemame K., Parker M., and Datsev
    D.. 2020. Open-source serverless architectures: An evaluation of apache openwhisk.
    In Proceedings of the 2020 IEEE/ACM 13th International Conference on Utility and
    Cloud Computing. 329–335. Reference [34] Dua A., Randive S., Agarwal A., and Kumar
    N.. 2020. Efficient load balancing to serve heterogeneous requests in clustered
    systems using kubernetes. In Proceedings of the 2020 IEEE 17th Annual Consumer
    Communications & Networking Conference. 1–2. Reference 1Reference 2Reference 3
    [35] Ahmed G. El Haj, Gil-Castiñeira F., and Costa-Montenegro E.. 2021. KubCG:
    A dynamic Kubernetes scheduler for heterogeneous clusters. Software: Practice
    and Experience 51, 2 (2021), 213–234. Reference 1Reference 2 [36] Ermolenko D.,
    Kilicheva C., Muthanna A., and Khakimov A.. 2021. Internet of Things services
    orchestration framework based on kubernetes and edge computing. In Proceedings
    of the 2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic
    Engineering. 12–17. Reference [37] Fan D. and He D.. 2020. Knative autoscaler
    optimize based on double exponential smoothing. In Proceedings of the 2020 IEEE
    5th Information Technology and Mechatronics Engineering Conference. 614–617. Navigate
    to [38] Faticanti F., Santoro D., Cretti S., and Siracusa D.. 2021. An application
    of kubernetes cluster federation in fog computing. In Proceedings of the 2021
    24th Conference on Innovation in Clouds, Internet and Networks and Workshops.
    89–91. Reference 1Reference 2 [39] Fayos-Jordan R., Felici-Castell S., Segura-Garcia
    J., Lopez-Ballester J., and Cobos M.. 2020. Performance comparison of container
    orchestration platforms with low cost devices in the fog, assisting Internet of
    Things applications. Journal of Network and Computer Applications 169 (2020),
    102788. Reference 1Reference 2 [40] Foundation The Apache Software. 2022. Apache
    Hadoop YARN. In Apache Hadoop 3.3.1; Apache Hadoop YARN. Retrieved from https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html.
    Reference [41] Foundation The Linux. 2022. Open Container Initiative - OCI. Retrieved
    from https://opencontainers.org/. Reference [42] Fu Y., Zhang S., Terrero J.,
    Y. Mao, G. Liu, S. Li, and D. Tao. 2019. Progress-based container scheduling for
    short-lived applications in a kubernetes cluster. In Proceedings of the 2019 IEEE
    International Conference on Big Data. 278–287. Navigate to [43] Ghobaei-Arani
    M., Souri A., and Rahmanian A.. 2020. Resource management approaches in fog computing:
    A comprehensive review. Journal of Grid Computing 18, 1 (2020), 1–42. Reference
    [44] Ghodsi A., Zaharia M., Hindman B., Konwinski A., Shenker S., and Stoica I..
    2011. Dominant resource fairness: Fair allocation of multiple resource types.
    In Proceedings of the 8th USENIX Symposium on Networked Systems Design and Implementation.
    Reference 1Reference 2 [45] Govind H. and González-Vélez H.. 2021. Benchmarking
    serverless workloads on kubernetes. In Proceedings of the 2021 IEEE/ACM 21st International
    Symposium on Cluster, Cloud and Internet Computing. 704–712. Reference [46] Han
    Y., Shen S., Wang X., Wang S., and Leung V.. 2021. Tailored learning-based scheduling
    for kubernetes-oriented edge-cloud system. In Proceedings of the IEEE Conference
    on Computer Communications. 1–10. Reference 1Reference 2 [47] Harichane I., Makhlouf
    A., and Belalem G.. 2020. A proposal of kubernetes scheduler using machine-learning
    on CPU/GPU cluster. In Intelligent Algorithms in Software Engineering. Silhavy
    Radek (Ed.), Springer Int. Publishing, 567–580. Reference [48] Hindman B., Konwinski
    A., Zaharia M., A. Ghodsi, A. D. Joseph, R. Katz, S. Shenker, and I. Stoica. 2011.
    Mesos: A platform for fine-grained resource sharing in the data center. In Proceedings
    of the 8th USENIX Conference on Networked Systems Design and Implementation. 295–308.
    Navigate to [49] Hong Cheol-Ho and Varghese Blesson. 2019. Resource management
    in fog/edge computing: A survey on architectures, infrastructure, and algorithms.
    ACM Computing Surveys 52, 5 (2019), 1–37. DOI: Navigate to [50] Huang J., Xiao
    C., and Wu W.. 2020. RLSK: A job scheduler for federated kubernetes clusters based
    on reinforcement learning. In Proceedings of the 2020 IEEE International Conference
    on Cloud Engineering. 116–123. Navigate to [51] Huaxin S., Gu X., Ping K., and
    Hongyu H.. 2020. An improved kubernetes scheduling algorithm for deep learning
    platform. In Proceedings of the 2020 17th International Computer Conference onWavelet
    Active Media Technology and Information Processing. 113–116. Reference 1Reference
    2 [52] Hussain K., Salleh M. N. Mohd, Cheng S., and Shi Y.. 2019. Metaheuristic
    research: A comprehensive survey. Artificial Intelligence Review 52, 4 (2019),
    2191–2233. Reference [53] K8. 2022. sig-scheduling blob. Github Repository. 53
    pages. Retrieved from https://bit.ly/3jbwx5O. Reference [54] Kang R., Zhu M.,
    He F., Sato T., and Oki E.. 2021. Design of scheduler plugins for reliable function
    allocation in kubernetes. In Proceedings of the 2021 17th International Conference
    on the Design of Reliable Communication Networks. 1–3. Reference 1Reference 2
    [55] Katenbrink F., Seitz A., Mittermeier L., Müller H., and Bruegge B.. 2018.
    Dynamic scheduling for seamless computing. In Proceedings of the 2018 IEEE 8th
    International Symposium on Cloud and Service Computing. 41–48. Navigate to [56]
    Kaur K., Garg S., Kaddoum G., S. H. Ahmed, and M. Atiquzzaman. 2020. KEIDS: Kubernetes-based
    energy and interference driven scheduler for industrial IoT in edge-cloud ecosystem.
    IEEE Internet of Things Journal 7, 5 (May2020), 4228–4237. Navigate to [57] Kayal
    Paridhika. 2020. Kubernetes in fog computing: Feasibility demonstration, limitations
    and improvement scope: Invited paper. In Proceedings of the 2020 IEEE 6th World
    Forum on Internet of Things. 1–6. Reference 1Reference 2Reference 3 [58] Kim E.,
    Lee K., and Yoo C.. 2021. On the resource management of kubernetes. In Proceedings
    of the 2021 International Conference on Information Networking. 154–158. Reference
    1Reference 2 [59] Kitchenham. 2007. Guidelines for performing Systematic Literature
    Reviews in Software Engineering. EBSE Technical Report EBSE-2007-01. 53 pages.
    Retrieved from https://bit.ly/3t40kAY. Reference [60] Kumar M., Sharma S. C.,
    Goel A., and Singh S. P.. 2019. A comprehensive survey for scheduling techniques
    in cloud computing. Journal of Network and Computer Applications 143 (2019), 1–33.
    Reference 1Reference 2 [61] Li W., Cao S., Hu K., Cao J., and Buyya R.. 2021.
    Blockchain-enhanced fair task scheduling for cloud-fog-edge coordination environments:
    Model and algorithm. Security and Communication Networks 2021 (2021), 1–18. Reference
    [62] Li X., Jiang Y., Ding Y., Wei D., Ma X., and Li W.. 2020. Application research
    of docker based on mesos application container cluster. In Proceedings of the
    2020 International Conference on Computer Vision, Image and Deep Learning. 476–479.
    Reference [63] Li Zijun, Guo L., Cheng J., Chen Q., He B., and Guo M.. 2021. The
    serverless computing survey: A technical primer for design architecture. ACM Computing
    Survey (Dec. 2021). Reference [64] Lin Chan-Yi, Yeh Ting-An, and Chou Jerry. 2019.
    DRAGON: A dynamic scheduling and scaling controller for managing distributed deep
    learning jobs in kubernetes cluster. In 9th International Conference on Cloud
    Computing and Services Science. 569–577. Reference [65] Lin Miao, Xi Jianqing,
    Bai Weihua, and Wu Jiayin. 2019. Ant colony algorithm for multi-objective optimization
    of container-based microservice scheduling in cloud. IEEE Access 7 (62019), 83088–83100.
    Reference [66] Ling W., Ma L., Tian C., and Hu Z.. 2019. Pigeon: A dynamic and
    efficient serverless and faas framework for private cloud. In Proceedings of the
    2019 International Conference on Computational Science and Computational Intelligence.
    1416–1421. Reference 1Reference 2Reference 3 [67] Luong D., Outtagarts A., and
    Ghamri-Doudane Y.. 2019. Multi-level resource scheduling for network slicing toward
    5G. In Proceedings of the 2019 10th International Conference on Networks of the
    Future. 25–31. Navigate to [68] Mahmud R., Ramamohanarao K., and Buyya R.. 2020.
    Application management in fog computing environments: A taxonomy, review and future
    directions. Computing Surveys 53, 4, Article 88 (July2020), 43 pages. Reference
    [69] Manvi S. and Shyam G. Krishna. 2014. Resource management for infrastructure
    as a service (IaaS) in cloud computing: A survey. Journal of Network and Computer
    Applications 41 (2014), 424–440. Reference 1Reference 2 [70] Mao Y., Fu Y., Zheng
    W., Cheng L., Liu Q., and Tao D.. 2021. Speculative container scheduling for deep
    learning applications in a kubernetes cluster. IEEE Systems Journal (2021), 1–12.
    Reference [71] Matrouk Khaled and Alatoun Kholoud. 2021. Scheduling algorithms
    in fog computing: A survey. International Journal of Networked and Distributed
    Computing 9, 1 (2021), 59–74. Reference 1Reference 2 [72] Medel Víctor, Rana Omer,
    Bañares José Ãngel, and Arronategui Unai. 2016. Adaptive application scheduling
    under interference in kubernetes. In Proceedings of the 2016 IEEE/ACM 9th International
    Conference on Utility and Cloud Computing. 426–427. Reference 1Reference 2 [73]
    Medel V., Tolón C., Arronategui U., Tolosana-Calasanz R., Bañares J. A., and Rana
    O. F.. 2017. Client-side scheduling based on application characterization on kubernetes.
    In Proceedings of the International Conference on the Economics of Grids, Clouds,
    Systems, and Services. 162–176. Reference [74] Menouer Tarek. 2021. KCSS: Kubernetes
    container scheduling strategy. The Journal of Supercomputing 77, 5 (2021), 4267–4293.
    Reference [75] Misale C., Drocco M., Milroy D. J., Gutierrez C., S. Herbein, D.
    H. Ahn, and Y. Park. 2021. It’s a scheduling affair: PolarisACS in the cloud with
    the kubeflux scheduler. In Proceedings of the 3rd International WS on Containers
    and New Orchestration Paradigms for Isolated Env. in HPC. 10–16. Reference [76]
    Mohanty S., Premsankar G., and Francesco M. di. 2018. An evaluation of open source
    serverless computing frameworks. In Proceedings of the 2018 IEEE International
    Conference on Cloud Computing Technology and Science. 115–120. Reference [77]
    Mon Myat Myat and Khine May Aye. 2019. Scheduling and load balancing in cloud-fog
    computing using swarm optimization techniques: A survey. In Proceedings of the
    17th International Conference on Computer Applications. 8–14. Reference 1Reference
    2Reference 3 [78] Mondal S., Pan R., Kabir H. M., Tian T., and Dai H.. 2022. Kubernetes
    in IT administration and serverless computing: An empirical study and research
    challenges. The Journal of Supercomputing 78, 2 (2022), 2937–2987. Reference [79]
    Monteiro L., Almeida W. H., Hazin R., A. C. de Lima, S. K. G. e Silva, and F.
    S. Ferraz. 2018. A survey on microservice security-trends in architecture, privacy
    and standardization on cloud computing environments. International Journal on
    Advances in Security 11, 3–4 (2018), 201–213. Reference 1Reference 2 [80] Morabito
    R.. 2016. A performance evaluation of container technologies on Internet of Things
    devices. In Proceedings of the 2016 IEEE Conference on Computer Communications
    Workshops. 999–1000. Reference [81] Moravcik M. and Kontsek M.. 2020. Overview
    of docker container orchestration tools. In Proceedings of the 2020 18th International
    Conference on Emerging eLearning Technologies and Applications. 475–480. Reference
    [82] Musaddiq A., Zikria Y., Hahm O., Yu H., Bashir A., and Kim S.. 2018. A survey
    on resource management in IoT operating systems. IEEE Access 6 (2018), 8459–8482.
    Reference 1Reference 2 [83] Nastic S., Pusztai T., Morichetta A., Pujol V., Dustdar
    S., Vii D., and Xiong Y.. 2021. Polaris scheduler: Edge sensitive and SLO aware
    workload scheduling in cloud-edge-IoT clusters. In Proceedings of the IEEE 14th
    International Conference on Cloud Computing. 206–216. Reference 1Reference 2Reference
    3 [84] Nguyen K., Drew S., Huang C., and Zhou J.. 2020. Collaborative container-based
    parked vehicle edge computing framework for online task offloading. In Proceedings
    of the 2020 IEEE 9th International Conference on Cloud Networking. 1–6. Navigate
    to [85] Nguyen N. D., Phan L., Park D., Kim S., and Kim T.. 2020. ElasticFog:
    Elastic resource provisioning in container-based fog computing. IEEE Access 8
    (2020), 183879–183890. Navigate to [86] Nguyen T., Yeom Y., Kim T., Park D., and
    Kim S.. 2020. Horizontal pod autoscaling in kubernetes for elastic container orchestration.
    Sensors 20, 16 (2020), 4621. Reference 1Reference 2 [87] Núñez-Gómez C., Caminero
    B., and Carrión C.. 2021. HIDRA: A distributed blockchain-based architecture for
    fog/edge computing environments. IEEE Access 9 (2021), 75231–75251. Reference
    [88] Ogbuachi M. C., Gore C., Reale A., P. Suskovics, and B. Kovacs. 2019. Context-aware
    K8S scheduler for real time distributed 5G edge computing applications. In Proceedings
    of the 2019 International Conference on Software, Telecommunications and Computer
    Networks. 1–6. Navigate to [89] Pahl C., Brogi A., Soldani J., and Jamshidi P..
    2019. Cloud container technologies: A state-of-the-art review. IEEE Transactions
    on Cloud Computing 7, 3 (2019), 677–692. Reference [90] Peng Y., Bao Y., Chen
    Y., Wu C., Meng C., and Lin W.. 2021. DL2: A deep learning-driven scheduler for
    deep learning clusters. IEEE Transactions on Parallel and Distributed Systems
    32, 8 (2021), 1947–1960. Reference 1Reference 2 [91] Perera C., Qin Y., Estrella
    J., Reiff-Marganiec S., and Vasilakos A.. 2017. Fog computing for sustainable
    smart cities: A survey. Computing Surveys 50, 3, Article 32 (June2017), 43 pages.
    Reference [92] Prado R. P., Galán S. G., Expósito J., Marchewka A., and Ruiz-Reyes
    N.. 2020. Smart containers schedulers for microservices provision in cloud-fog-IoT
    networks. challenges and opportunities. Sensors 20, 6 (2020), 1714. Reference
    1Reference 2 [93] authors 2022 K3s project. 2022. Lightweight Kubernetes: The
    Certified Kubernetes Distribution Built for IoT and Edge Computing. Retrieved
    from https://k3s.io/. Reference 1Reference 2 [94] authors KubeEdge project. 2021.
    A Kubernetes Native Edge Computing Framework. Retrieved from https://kubeedge.io/en/.
    Reference 1Reference 2 [95] authors © Kubeless 2022 project. 2022. The Kubernetes
    Native Serverless Framework. Retrieved from https://kubeless.io/. Reference 1Reference
    2 [96] Pusztai T., Rossi F., and Dustdar S.. 2021. Pogonip: Scheduling asynchronous
    applications on the edge. In Proceedings of the 2021 IEEE 14th International Conference
    on Cloud Computing. 660–670. Reference [97] Qi S., Kulkarni S., and Ramakrishnan
    K.. 2021. Assessing container network interface plugins: Functionality, performance,
    and scalability. IEEE Transactions on Network and Service Management 18, 1 (2021),
    656–671. Reference 1Reference 2 [98] Qi S., Kulkarni S. G., and Ramakrishnan K.
    K.. 2020. Understanding container network interface plugins: Design considerations
    and performance. In Proceedings of the 2020 IEEE International Symposium on Local
    and Metropolitan Area Networks. 1–6. Reference 1Reference 2 [99] Rahali M., Phan
    C., and Rubino G.. 2021. KRS: Kubernetes resource scheduler for resilient NFV
    networks. In Proceedings of the IEEE Global Communications Conference.IEEE, Madrid,
    1–6. Reference 1Reference 2 [100] Rashid Aaqib and Chaturvedi Amit. 2019. Virtualization
    and its role in cloud computing environment. International Journal of Computer
    Sciences and Engineering 7, 4 (2019), 1131–1136. Reference [101] Rattihalli G.,
    Govindaraju M., Lu H., and Tiwari D.. 2019. Exploring potential for non-disruptive
    vertical auto scaling and resource estimation in kubernetes. In Proceedings of
    the 2019 IEEE 12th International Conference on Cloud Computing. 33–40. Reference
    [102] Rocha I., Göttel C., Felber P., M. Pasin, R. Rouvoy, and V. Schiavoni. 2019.
    Heats: Heterogeneity-and energy-aware task-based scheduling. In Proceedings of
    the 2019 27th Euromicro International Conference on Parallel, Distributed and
    Network-Based Processing. 400–405. Reference [103] Rodriguez M. A. and Buyya R..
    2019. Container-based cluster orchestration systems: A taxonomy and future directions.
    Software: Practice and Experience 49, 5 (2019), 698–719. Reference 1Reference
    2Reference 3 [104] Sami H., Mourad A., Otrok H., and Bentahar J.. 2020. FScaler:
    Automatic resource scaling of containers in fog clusters using reinforcement learning.
    In Proceedings of the 2020 International Wireless Communications and Mobile Computing.
    1824–1829. Reference 1Reference 2 [105] Santoro D., Zozin D., Pizzolli D., Pellegrini
    F. De, and Cretti S.. 2017. Foggy: A platform for workload orchestration in a
    fog computing environment. In Proceedings of the 2017 IEEE International Conference
    on Cloud Computing Technology and Science. 231–234. Navigate to [106] Santos J.,
    Wauters T., Volckaert B., and Turck F. De. 2019. Resource provisioning in fog
    computing: From theory to practice. Sensors 19, 10 (2019), 2238. Reference 1Reference
    2 [107] Santos J., Wauters T., Volckaert B., and Turck F. De. 2019. Towards network-aware
    resource provisioning in kubernetes for fog computing applications. In Proceedings
    of the 2019 IEEE Conference on Network Softwarization. 351–359. Navigate to [108]
    Santos J., Wauters T., Volckaert B., and Turck F. De. 2020. Towards delay-aware
    container-based service function chaining in fog computing. In Proceedings of
    the 2020 IEEE/IFIP Network Operations and Management Symposium. 1–9. Reference
    1Reference 2Reference 3 [109] Scazzariello M., Ariemma L., Battista G., and Patrignani
    M.. 2020. Megalos: A scalable architecture for the virtualization of network scenarios.
    In Proceedings of the 2020 IEEE/IFIP Network Operations and Management Symposium.1–7.
    Reference 1Reference 2 [110] Schwarzkopf M., Konwinski A., Abd-El-Malek M., and
    Wilkes J.. 2013. Omega: Flexible, scalable schedulers for large compute clusters.
    In Proceedings of the 8th ACM European Conference on Computer Systems. 351–364.
    Navigate to [111] Sfakianakis Y., Marazakis M., and Bilas A.. 2021. Skynet: Performance-driven
    resource management for dynamic workloads. In Proceedings of the 2021 IEEE 14th
    International Conference on Cloud Computing. 527–539. Reference [112] Shahrad
    M., Balkind J., and Wentzlaff D.. 2019. Architectural implications of function-as-a-service
    computing. In Proceedings of the 52nd Annual IEEE/ACM International Symposium
    on Microarchitecture. 1063–1075. Reference [113] Sharma P., Chaufournier L., Shenoy
    P., and Tay Y. C.. 2016. Containers and virtual machines at scale: A comparative
    study. In Proceedings of the 17th international Middleware Conference. New York,
    NY, Article 1, 13 pages. Reference [114] Song S., Deng L., Gong J., and Luo H..
    2018. Gaia scheduler: A kubernetes-based scheduler framework. In Proceedings of
    the 2018 IEEE Intl Conf on Parallel Distributed Processing with Applications,
    Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing
    & Networking, Sustainable Computing & Communications. 252–259. Reference 1Reference
    2Reference 3 [115] Stanojevic P., Usorac S., and Stanojev N.. 2021. Container
    manager for multiple container runtimes. In Proceedings of the 2021 44th International
    Convention on Information, Communication and Electronic Technology. 991–994. Reference
    [116] Surya R. Y. and Kistijantoro A. Imam. 2019. Dynamic resource allocation
    for distributed tensorflow training in kubernetes cluster. In Proceedings of the
    2019 International Conference on Data and Software Engineering. 1–6. Reference
    [117] Tamiru M. A., Pierre G., Tordsson J., and Elmroth E.. 2021. mck8s: An orchestration
    platform for geo-distributed multi-cluster environments. In Proceedings of the
    2021 International Conference on Computer Communications and Networks. 1–10. Reference
    1Reference 2 [118] Tang J., Jalalzai M. M., Feng C., Xiong Z., and Zhang Y.. 2022.
    Latency-aware task scheduling in software-defined edge and cloud computing with
    erasure-coded storage systems. IEEE Transactions on Cloud Computing (2022), 1–1.
    Reference [119] Tantawi Asser N. and Steinder Malgorzata. 2019. Autonomic cloud
    placement of mixed workload: An adaptive bin packing algorithm. In Proceedings
    of the 2019 IEEE International Conference on Autonomic Computing. 187–193. Reference
    1Reference 2 [120] Thinakaran P., Gunasekaran J. R., Sharma B., M. T. Kandemir,
    and C. R. Das. 2019. Kube-Knots: Resource harvesting through dynamic container
    orchestration in GPU-based datacenters. In Proceedings of the 2019 IEEE International
    Conference on Cluster Comp.1–13. Reference [121] Toka L.. 2021. Ultra-reliable
    and low-latency computing in the edge with kubernetes. Journal of Grid Computing
    19, 3 (2021), 1–23. Reference 1Reference 2 [122] Townend P., Clement S., Burdett
    D., R. Yang, J. Shaw, B. Slater, and J. Xu. 2019. Invited paper: Improving data
    center efficiency through holistic scheduling in kubernetes. In Proceedings of
    the 2019 IEEE International Conference on Service-Oriented System Engineering.
    156–15610. Reference 1Reference 2 [123] Tzenetopoulos A., Masouros D., Xydis S.,
    and Soudris D.. 2020. Interference-aware orchestration in kubernetes. In Proceedings
    of the International Conference on High Performance Computing. 12321. Reference
    1Reference 2 [124] Vaucher S., Pires R., Felber P., Pasin M., Schiavoni V., and
    Fetzer C.. 2018. SGX-aware container orchestration for heterogeneous clusters.
    In Proceedings of the 2018 IEEE 38th International Conference on Distributed Computing
    Systems. 730–741. Reference [125] Vayghan L., Saied M., Toeroe M., and Khendek
    F.. 2019. Microservice based architecture: Towards high-availability for stateful
    applications with kubernetes. In Proceedings of the 2019 IEEE 19th International
    Conference on Software Quality, Reliability and Security. 176–185. Reference [126]
    Verma A., Pedrosa L., Korupolu M., Oppenheimer D., Tune E., and Wilkes J.. 2015.
    Large-scale cluster management at Google with borg. In Proceedings of the 10th
    European Conference on Computer Systems. Article 18, 17 pages. Navigate to [127]
    Wang S., Gonzalez O. J., Zhou X., T. Williams, B. D. Friedman, M. Havemann, and
    T. Woo. 2020. An efficient and non-intrusive GPU scheduling framework for deep
    learning training systems. In Proceedings of the International Conference for
    High Performance Computing, Networking, Storage and Analysis. 1–13. Reference
    1Reference 2Reference 3 [128] Warke A., Mohamed M., Engel R., Ludwig H., Sawdon
    W., and Liu L.. 2018. Storage service orchestration with container elasticity.
    In Proceedings of the 2018 IEEE 4th International Conference on Collaboration
    and Internet Computing. 283–292. Reference [129] Wojciechowski Ł., Opasiak K.,
    Latusek J., M. Wereski, V. Morales, T. Kim, and M. Hong. 2021. NetMARKS: Network
    metrics-aware kubernetes scheduler powered by service mesh. In Proceedings of
    the IEEE Conference on Computer Communications. 1–9. Reference 1Reference 2 [130]
    XIE X. and Govardhan S. S.. 2020. A service mesh-based load balancing and task
    scheduling system for deep learning applications. In Proceedings of the 2020 20th
    IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing. 843–849.
    Reference [131] Xiong Ying, Sun Yulin, Xing Li, and Huang Ying. 2018. Extend cloud
    to edge with kubeedge. In Proceedings of the 2018 IEEE/ACM Symposium on Edge Computing.
    373–377. Reference 1Reference 2 [132] Yang S., Ren Y., Zhang J., Guan J., and
    Li B.. 2021. KubeHICE: Performance-aware container orchestration on heterogeneous-ISA
    architectures in cloud-edge platforms. In Proceedings of the 2021 IEEE International
    Conference on Parallel, Distributed Processing with Apps, Big Data, Cloud Computing,
    Sustainable Computing, Communications, Social Computing, Net.81–91. Reference
    [133] Yang Y. and Chen L.. 2019. Design of kubernetes scheduling strategy based
    on LSTM and grey model. In Proceedings of the 2019 IEEE 14th International Conference
    on Intelligent Systems and Knowledge Engineering. 701–707. Navigate to [134] Yilmaz
    Onur. 2021. Extending the kubernetes API. In Extending Kubernetes. Springer, 99–141.
    Reference [135] Yousefpour A., Fung C., Nguyen T., K. Kadiyala, F. Jalali, A.
    Niakanlahiji, J. Kong, and J. P. Jue. 2019. All one needs to know about fog computing
    and related edge computing paradigms: A complete survey. Journal Systems Architecture
    98 (2019), 289—330. Reference [136] Yu Dongjin, Jin Yike, Zhang Yuqun, and Zheng
    Xi. 2018. A survey on security issues in services communication of microservices-enabled
    fog applications. Concurrency and Computation: Practice and Experience 0, 0 (2018),
    e4436. Reference 1Reference 2 [137] Zhang G., Lu R., and Wu W.. 2019. Multi-resource
    fair allocation for cloud federation. In Proceedings of the 2019 IEEE 21st International
    Conference on HP Computing and Communications; 17th International Conference on
    Smart City; 5th International Conference on Data Science and Systems. 2189–2194.
    Navigate to [138] Zhang X., Li L., Wang Y., Chen E., and Shou L.. 2021. Zeus:
    Improving resource efficiency via workload colocation for massive kubernetes clusters.
    IEEE Access 9 (2021), 105192–105204. Reference 1Reference 2 [139] Zhao H., Deng
    S., Liu Z., Yin J., and Dustdar S.. 2022. Distributed redundancy scheduling for
    microservice-based applications at the edge. IEEE Transactions on Services Computing
    15, 3 (2022), 1732–1745. Reference 1Reference 2Reference 3 [140] Zheng C., Kremer-Herman
    N., Shaffer T., and Thain D.. 2020. Autoscaling high-throughput workloads on container
    orchestrators. In Proceedings of the 2020 IEEE International Conference on Cluster
    Computing. 142–152. Reference Cited By View all Huo J, Huo Z, Xiao L and He Z.
    (2024). Research on performance optimization of virtual data space across WAN.
    Frontiers of Computer Science: Selected Publications from Chinese Universities.
    18:6. Online publication date: 1-Dec-2024. https://doi.org/10.1007/s11704-023-3087-8
    Huang Y, Wang K and Ke B. Accelerating 5G Service-Based Architecture with Ebpf.
    Proceedings of the 2023 12th International Conference on Networks, Communication
    and Computing. (200-209). https://doi.org/10.1145/3638837.3638883 Bartolomeo G,
    Cao J, Su X and Mohan N. Characterizing Distributed Mobile Augmented Reality Applications
    at the Edge. Companion of the 19th International Conference on emerging Networking
    EXperiments and Technologies. (9-18). https://doi.org/10.1145/3624354.3630584
    Show All Cited By Index Terms Kubernetes Scheduling: Taxonomy, Ongoing Issues
    and Challenges Computer systems organization Architectures Distributed architectures
    Client-server architectures General and reference Document types Surveys and overviews
    Software and its engineering Software organization and properties Contextual software
    domains Operating systems Process management Scheduling Recommendations Cost-efficient
    scheduling algorithms based on beetle antennae search for containerized applications
    in Kubernetes clouds Abstract With the development of cloud-native technologies,
    Kubernetes becomes the standard of fact for container scheduling. Kubernetes provides
    service discovery and scheduling of containers, load balancing, service self-healing,
    elastic scaling, storage ... Read More A Systematic Literature Review on Maintenance
    of Software Containers Nowadays, cloud computing is gaining tremendous attention
    to deliver information via the internet. Virtualization plays a major role in
    cloud computing as it deploys multiple virtual machines on the same physical machine
    and thus results in improving ... Read More Custom Scheduling in Kubernetes: A
    Survey on Common Problems and Solution Approaches Since its release in 2014, Kubernetes
    has become a popular choice for orchestrating containerized workloads at scale.
    To determine the most appropriate node to host a given workload, Kubernetes makes
    use of a scheduler that takes into account a set of ... Read More Comments 140
    References View Issue’s Table of Contents Footer Categories Journals Magazines
    Books Proceedings SIGs Conferences Collections People About About ACM Digital
    Library ACM Digital Library Board Subscription Information Author Guidelines Using
    ACM Digital Library All Holdings within the ACM Digital Library ACM Computing
    Classification System Digital Library Accessibility Join Join ACM Join SIGs Subscribe
    to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin
    Feedback Bug Report The ACM Digital Library is published by the Association for
    Computing Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy
    Code of Ethics Feedback"'
  inline_citation: '>'
  journal: ACM Computing Surveys
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Kubernetes Scheduling: Taxonomy, Ongoing Issues and Challenges'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kaur K.
  - Mangat V.
  - Kumar K.
  citation_count: '15'
  description: Nowadays, Network Function Virtualization (NFV) is a growing and powerful
    technology in the research community and IT world. Traditional computer networks
    consist of hardware appliances such as firewalls and load balancers, called middleboxes.
    The implementation of these hardware devices is a difficult task due to their
    proprietary nature. NFV proposes an alternative way to design and deploy network
    functions called Virtual Network Functions (VNFs) on top of the commercial hardware
    by leveraging virtualization technology. NFV offers many advantages such as flexibility,
    agility, reduced capital and operational expenditure over the traditional network
    architecture. With the emergence of VNF, NFV needs to add new features regarding
    life-cycle management and end-to-end orchestration of VNFs. To fulfill this demand,
    NFV introduced the NFV-MANO framework for the management and orchestration of
    VNFs and provide network services to users. The NFV-MANO consists of NFV Orchestrator
    (NFVO), VNF Manager (VNFM), and Virtualized Infrastructure Manager (VIM). This
    paper provides a comprehensive overview of Virtualized Infrastructure Managers
    with NFV orchestration and VNF Management for implementing Service Function Chain
    (SFC) in NFV architecture. Further, this study critically analyzes relevant research
    articles and proposes a taxonomy to select an appropriate VIM based on Emulation,
    Virtualization, Containerization, and Hybrid environment for reliable SFC provisioning.
    Finally, various use cases have been identified for selecting particular VIM according
    to the requirements of the application.
  doi: 10.1016/j.comnet.2022.109281
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract BetaPowered by GenAIQuestions answered in this article
    Keywords 1. Introduction 2. Journey to Network Function Virtualization 3. Classification
    of Virtualized Infrastructure Managers (VIMs) 4. Review of service function chain
    provisioning techniques based on the selection of Virtualized Infrastructure Managers
    (VIMs) 5. Application of SDN/NFV in future state-of-the-art applications 6. Open
    research challenges 7. Conclusion and future directions Declaration of Competing
    Interest References Vitae Show full outline Cited by (19) Figures (19) Show 13
    more figures Tables (15) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show
    all tables Computer Networks Volume 217, 9 November 2022, 109281 Review article
    A review on Virtualized Infrastructure Managers with management and orchestration
    features in NFV architecture Author links open overlay panel Karamjeet Kaur, Veenu
    Mangat, Krishan Kumar Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.comnet.2022.109281
    Get rights and content Highlights • Analyzed and highlighted existing Virtualized
    Infrastructure Managers (VIMs) with NFV Orchestration (NFVO) and VNF Management
    (VNFM) operations. • Characterized existing VIMs based on Emulation, Virtualization,
    and Containerization technology to select an appropriate VIM for reliable SFC
    provisioning. Moreover, Identified different open-source MANO solutions. • Proposed
    a layered taxonomy to characterize the SFC provisioning solutions and highlighted
    use-cases for the selection of particular VIM. • Identified different future state-of-the-art
    applications based on SDN/NFV implementation. • Identified open research challenges
    that provide ample space for improvement. Abstract Nowadays, Network Function
    Virtualization (NFV) is a growing and powerful technology in the research community
    and IT world. Traditional computer networks consist of hardware appliances such
    as firewalls and load balancers, called middleboxes. The implementation of these
    hardware devices is a difficult task due to their proprietary nature. NFV proposes
    an alternative way to design and deploy network functions called Virtual Network
    Functions (VNFs) on top of the commercial hardware by leveraging virtualization
    technology. NFV offers many advantages such as flexibility, agility, reduced capital
    and operational expenditure over the traditional network architecture. With the
    emergence of VNF, NFV needs to add new features regarding life-cycle management
    and end-to-end orchestration of VNFs. To fulfill this demand, NFV introduced the
    NFV-MANO framework for the management and orchestration of VNFs and provide network
    services to users. The NFV-MANO consists of NFV Orchestrator (NFVO), VNF Manager
    (VNFM), and Virtualized Infrastructure Manager (VIM). This paper provides a comprehensive
    overview of Virtualized Infrastructure Managers with NFV orchestration and VNF
    Management for implementing Service Function Chain (SFC) in NFV architecture.
    Further, this study critically analyzes relevant research articles and proposes
    a taxonomy to select an appropriate VIM based on Emulation, Virtualization, Containerization,
    and Hybrid environment for reliable SFC provisioning. Finally, various use cases
    have been identified for selecting particular VIM according to the requirements
    of the application. Previous article in issue Next article in issue Questions
    answered in this article BetaPowered by GenAI This is generative AI content and
    the quality may vary. Learn more. What is the role of NFVO in NFV? What network
    functions are run on virtual servers in NFV? What is the challenge in the distribution
    of NFVO and VNFM layers in the NFV framework? What is the role of VNFM in NFV?
    What are the issues with virtualization based on container technology in NFV scenarios?
    Keywords Virtualized Infrastructure ManagerNetwork Function VirtualizationService
    Function ChainVNF managerMininetOpenStackKubernetesVirtual machineContainerDockerTackerMini-NFV
    1. Introduction The advantages of software-based technologies such as Network
    Function Virtualization (NFV) and Software-defined networks (SDN) are well known
    in transforming traditional networks into software-based networks [1], [2]. Both
    technologies offer benefits to enterprise IT, telecommunication service providers,
    and the data center world. NFV aims to alleviate the traditional telecommunication
    network problems, and limitations such as (1) high capital and operational expenditure,
    (2) lack of flexibility and dynamicity, (3) a large number of hardware devices
    dedicated to a particular function, (4) more energy consumption, and (5) installation
    space issues [3]. NFV paradigm is embracing SDN and cloud technologies to become
    more successful. Adoption of SDN and Cloud technologies leads to operating a dynamic
    and flexible network, VNF elasticity, Open API access, automated orchestration
    with the opportunity of cheaper, faster, and automated deployment of the network
    services [4], [5]. In order to get these benefits, telecommunication operators
    need to manage their network through the NFV Management and Orchestration (MANO)
    function. The MANO function is a critical function of the cloud infrastructure
    to get advantages such as automatic orchestration, easy deployment of network
    services and VNFs, simplification of infrastructure operations, and scalability
    [6]. ETSI NFV industry specification group defines NFV framework that consists
    of three functions. First, Network Function Virtualization Infrastructure (NFVI)
    consists of hardware resources, virtualization layer, and virtual resources to
    deploy VNFs. Second, Virtual Network Functions (VNFs) block includes Network Functions
    (NFs) running in a virtual form on a shared virtual infrastructure. Third, MANO
    contains a group of functions responsible for managing infrastructure resources,
    functions, and services [7]. MANO has gained critical importance in cloud infrastructure.
    MANO manages both infrastructure resources and network functions at the same time,
    enabling operators to optimally use their resources and offering the best quality
    services [8]. MANO function contains three elements: (1) Virtualized Infrastructure
    Manager (VIM) controls physical and virtual resources of cloud infrastructure
    e.g., Mininet, OpenStack, and Kubernetes. Hypervisor acts as a middle layer between
    hardware and software in the cloud world [9]. OpenStack is the most widely used
    hypervisor in telecommunication networks [10]. (2) Virtual Network Function Manager
    (VNFM) manages the life-cycle such as instantiation, termination, scaling, and
    up-gradation of VNFs. (3) NFV Orchestrator (NFVO) manages and orchestrates the
    NFVI resources across different VIMs and provides end-to-end network services
    between providers and consumers [11]. Telecommunication operators not only need
    to get a new VIM to manage the infrastructure, they also need to address the end-to-end
    orchestration and management of VNFs. In the NFV framework standardized by ETSI,
    there is a lack of clarity in the distribution of NFVO and VNFM layers of MANO
    function [12]. The choice of the right VNF Manager can be made using two approaches:
    dedicated VNF Manager for a specific VNF, or a general VNF Manager for all VNFs.
    The general VNF Manager manages all the VNFs from a single location and reduces
    the complexity of the MANO function. Therefore, there is a need to configure the
    VNFs using a standardized VNF model to achieve this universal configuration management
    [13]. Topology and Orchestration Specification for Cloud Applications (TOSCA)
    [14] model-based language defined by OASIS is used for configuration management.
    TOSCA is model-driven language for the creation of templates that consist of VNF
    topologies, life-cycle events, and infrastructure requirements [15]. 1.1. Prior
    reviews There are a number of survey papers that provide exhaustive coverage in
    the field of SDN architecture [16], [17], [18], [19], NFV framework [20], [21],
    [22], combined SDN/NFV architecture [23], [24], [25], [26], [27], and SFC provisioning
    approaches [6], [28]. It is observed that most of the NFV research work focuses
    on VNFs load-balancing [29], [30], [31], [32], [33], [34], VNFs placement [35],
    [36], [37], [38], [39], resilience aware SFC [40], [41], [42], energy-saving [43],
    [44], [45], whereas there is less focus on available VIM tools that provide infrastructure
    orchestration and VNF management to the NFV network. A few survey papers exist
    that introduce VIMs. Peuster et al. [46] provided MeDICINE, a prototyping and
    testing platform for the distributed cloud. The emulation framework uses a docker
    container to run network functions. In another related study [47], the authors
    provided an emulation environment for NFV with the combination of Mininet network
    emulator and click-based VNFs. Castillo-Lema et al. [48] introduced new framework
    named Mini-NFV to deploy network service (NSs) on top of Mininet with the help
    of NFV orchestration and VNF Manager modules. It also used TOSCA templates to
    deploy VNFs and NSs. The recent survey paper [49] proposed a SONATA framework
    for managing and orchestrating network services and compared it with an open-source
    MANO framework such as Open-Source MANO (OSM) and Cloudify. Table 11 compares
    our survey with the other articles existing in literature (see Table 1). Table
    1. Summary of review articles. Ref NFV architecture VIM NFVO & VNFM SFC solutions
    based on deployment environment Taxonomy of VIMs Use-Cases Empty Cell Empty Cell
    Empty Cell Empty Cell Emulation Environment as VIM (EE-VIM) Virtualization Environment
    as VIM (VE-VIM) Containerization Environment as VIM (CE-VIM) Hybrid Environment
    as VIM (HE-VIM) Empty Cell Empty Cell [46] 2016 × ✓ × × × × × × × [47] 2018 ✓
    × × × × × ✓ [48] 2019 × ✓ × × × × × × [49] 2020 ✓ ✓ × × × × × Proposed Survey
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓: Full. : Partial. ×: None. 1.2. Challenges It is necessary
    to critically review all the research studies that used different Virtualized
    Infrastructure Managers for implementing SFC in NFV architecture owing to its
    potential demand in the telecommunication industry [50]. The main aim of our paper
    is to describe the MANO component of ETSI NFV that contains VIM, VNFM, and NFVO.
    The VIM manages the life-cycle of virtual resources in the NFVI domain. That is,
    it creates, maintains, and destroys virtual machines (VMs) from physical resources
    in the NFVI domain. The need of VNFM is in charge of VNF’s Life Cycle Management
    (LCM) (s). It manages the creation, maintenance, and deletion of VNFs. It is responsible
    for the FCAPs (i.e. Fault, Configuration, Accounting, Performance and Security
    Management) of VNFs. On the other side, NFVO is responsible for managing the network
    service (NS) life cycle, together with the VNF life cycle (supported by VNFM),
    and orchestrating NFVI resources (supported by VIM) to ensure the optimal allocation
    of necessary resources and connections. To the best of our knowledge, our paper
    is the first survey paper that provided an in-depth overview of available VIMs
    along with its NFV orchestration (NFVO) and VNF management (VNFM) operations.
    This paper also proposes a taxonomy to select an appropriate VIM based on Emulation,
    Virtualization, Containerization, and Hybrid Environment for reliable SFC provisioning.
    1.3. Contributions The significant contributions of this article are as follows:
    • Analyzed and highlighted existing Virtualized Infrastructure Managers (VIMs)
    with NFV Orchestration (NFVO) and VNF Management (VNFM) operations. • Characterized
    existing VIMs based on Emulation, Virtualization, and Containerization technology
    to select an appropriate VIM for reliable SFC provisioning. Moreover, Identified
    different open-source MANO solutions. • Proposed a layered taxonomy to characterize
    the SFC provisioning solutions and highlighted use-cases for the selection of
    particular VIM. • Identified different future state-of-the-art applications based
    on SDN/NFV implementation. • Identified open research challenges that provide
    ample space for improvement. 1.4. Organization of paper The rest of the paper
    is organized as follows: The background detail to understand the value of our
    contribution is presented in Section 2. Section 3 presents the characterization
    of the available VIMs based on virtualization technique to select an appropriate
    VIM for a reliable SFC. Section 4 presents taxonomy to classify the SFC provisioning
    approaches according to the selection of VIMs such as Emulation Environment as
    VIM, Virtualization Environment as VIM, Containerization Environment as VIM, and
    Hybrid Environment as VIM. Moreover, it also highlights some of the use-cases
    according to the requirement of the application. Section 5 provides applications
    of SDN/NFV in future state-of-the-art applications. Section 6 provides open research
    challenges that provides ample space for improvement. The conclusion and future
    research directions are listed in Section 7. 2. Journey to Network Function Virtualization
    This section walks you through the journey of fast emerging NFV technology, which
    is heavily affecting the networking domain. NFV offers a new way to deploy, design,
    manage and transform the network. The networking industry is moving from a dedicated
    hardware approach to virtualization and a software-based paradigm. The main motivation
    behind this work is a faster adaption of NFV in order to meet the demand for cloud-based
    services. We are taking a look at the history of networking and how it faces challenges
    in today’s network. Although networking devices and networks have become faster
    and more resilient with maximum capacity, having evolved architecture, they still
    face a number of issues to cope with market-changing demands. In the next section,
    we identified the factors in traditional networks that have been unable to cope
    with the changing demands. Then, how NFV offers a new networking perspective to
    solve these market-changing needs. 2.1. Traditional network architecture We are
    living in a data-driven world that relies on computers, servers, networks to run
    their businesses and even their daily lives. Moreover, the absence of these resources
    even for a short time can have severe financial implications for an organization.
    Organizations need to run their applications on the server. Moreover, a single
    application requires its own server to execute. The server has to be continuously
    running throughout the day even though the server’s capacity is not fully utilized.
    To run a second application, another server is used rather than installing it
    on the same server [51], [52] as shown in Fig. 1. Hence, traditional networking
    devices are able to perform some specific tasks, and software code running on
    top of hardware devices is tightly coupled to them and limited to vendor-specific
    tasks. This problem is known as server proliferation. With the emergence of mobile
    and IoT-based network applications, network traffic with high bandwidth requirements
    is also increasing. The service providers require a way to scale and expand the
    network without a significant increase in cost. Traditional network devices create
    a number of obstacles that limit the scalability and efficiency of the network.
    The following are some of the limitations of a traditional network. (i) Inflexibility
    due to hardware and software tight coupling (ii) Scalability issue due to a limited
    set of options (iii) Long time to offer new service (iv) Management issues due
    to vendor-specific monitoring tools (v) High CAPEX and OPEX (vi) Interoperability
    issue due to different vendor implementation (vii) Underutilized and over-provisioned
    network. 2.2. Server and Network Virtualization (NV) Server virtualization is
    a proven technology in data centers, where dedicated hardware-based devices are
    replaced with virtual servers running on top of commercial-off-the-shelf servers.
    The virtualization layer is used to create multiple virtual instances on top of
    physical hardware. Server virtualization offers a number of advantages. (i) Proper
    utilization of underused servers (ii) cost reduction (iii) space and power-saving
    (iv) scalable and easy maintenance (v) OS isolation. The success of server virtualization
    moves toward network virtualization (NV). It provides an abstraction to network
    devices by combining a small number of networks into a single virtual network
    or dividing a large network into smaller ones. NV virtualize the network at layer
    2 to layer 3 of the OSI model. The VLAN and VPN are examples of NV. A VLAN logically
    divides the networks into segments. VPNs also create a secure logical tunnel from
    the source to a destination host over an insecure public network. Download : Download
    high-res image (299KB) Download : Download full-size image Fig. 1. Transition
    to NFV [53]. 2.3. Network Function Virtualization NFV was developed by the European
    Telecommunication Standard Institute (ETSI) in 2012 [53], [54]. NFV uses the concept
    of server virtualization to virtualize the hardware resources among multiple users.
    NFV virtualizes the network from layer 4 to layer 7 of the OSI model. In a traditional
    network, middleboxes such as firewalls, IDS, and IPS are used to perform network
    functionality. In NFV, these network functions are run on virtual servers. The
    software version of these network functions is called “virtual network Function”
    (VNF) as shown in Fig. 1. The NFV technology offers a number of advantages to
    the network operators as compared to hardware appliances. Some of the advantages
    of NFV are listed below: • Proper utilization of underused servers is achieved
    by means of VNFs. • NFV reduces CAPEX and OPEX by virtualizing the network functions.
    To deploy new network functions, there is no need to buy new hardware. Moreover,
    it also reduces the power consumption and space required to deploy any new network
    function. • The connection management between hardware middleboxes using wires
    and switches is very time-consuming as compared to virtual network functions.
    • The deployment, termination, and configuration of VNFs are easy as compared
    to hardware appliances. In traditional networks, achieving scalability requires
    resources, time, and proper planning. NFV enables us to dynamically shrink and
    expand resources based on demand. Furthermore, network operators can migrate the
    VNF easily from one server to another to distribute the load among servers. The
    time to launch the new network function into the market using VNF is also less
    as compared to deploying hardware middleboxes. 2.4. Network Function Virtualization
    architecture NFV uses virtualization technology to replace traditional hardware
    network devices with a software version that runs on commercial off-the-shelf
    server (COTS). The architecture that defines the hardware-based network devices
    is simple due to the vendor-locked nature of both software and hardware [55].
    In NFV, due to the software nature of network functions, an architectural framework
    or standardized method is needed to freely select virtual network functions (VNFs)
    from any vendor in order to deploy the service [56], [57]. We broadly classify
    the NFV architecture into two groups according to their development and standardized
    efforts as shown in Fig. 2: High-Level ETSI NFV Framework, and Low-Level ETSI
    NFV Framework. Download : Download high-res image (326KB) Download : Download
    full-size image Fig. 2. Classification of ETSI NFV framework. 2.4.1. High-level
    view of architecture The High-level view of ETSI NFV architecture is composed
    of three different functional blocks [4]: Network Function Virtualization Infrastructure
    (NFVI) block, VNFs block, and Management block as shown in Fig. 3. 1. NFV Infrastructure
    (NFVI) Block: This block consists of hardware resources (compute, resource, network),
    virtualization layer (hypervisor), and virtual resources on top of the virtualization
    layer. 2. Virtual Network Functions (VNFs) Block: The VNF block on top of the
    NFVI layer uses a virtual machine to implement virtualized network functions.
    For example, when a firewall is in its virtual form, it is called VNF firewall.
    3. Management and Orchestration (MANO) Block: MA-NO is the brain of the NFV architecture
    framework that communicates with both NFVI and VNF blocks. MANO manages the resources
    (creation, deletion) in the NFVI block and then allocates resources to the VNFs
    present in the VNFs block. Download : Download high-res image (289KB) Download
    : Download full-size image Fig. 3. High-level view of the NFV architecture [4].
    2.4.2. Low-level view of architecture The three basic building blocks discussed
    in Section 2.4.1 are further classified into different blocks in which each block
    has some specific role and responsibility [58]. The NFV Orchestrator (NFVO), Virtualized
    Network Function Manager (VNFM), and Virtualized Infrastructure Manager (VIM)
    blocks are present in MANO [59]. The low-level architectural view of ETSI NFV
    consists of three different layers: Infrastructure Layer, Virtual Network Function
    Layer, and Operation and Orchestration Layer as shown in Fig. 4 [60]. 1. Infrastructure
    Layer: The layer consists of NFVI and VIM blocks with each having some specific
    role and responsibility as shown in Fig. 4 [1]. NFVI: The deployment of VNFs depends
    upon the NFVI block that provides hardware resources, virtualization layer, and
    software resources to implement a particular VNF. The virtualization layer decouples
    the software resources from the underlying hardware and ensures isolation from
    other VNFs while acting as an interface to hardware resources [61]. VIM: The ETSI
    defines a management block named VIM to manage NFVI. The single VIM can manage
    the resources of a single NFVI domain. It may also be the case that there are
    multiple VIMs and each manages their respective NFVI. VIM performs the following
    tasks: • The responsibility of VIM is to manage all the compute, storage, networking
    hardware resources. • VIM is responsible for power management, health checking,
    and monitoring statistics of resource utilization. • It also collects fault information
    related to infrastructure. • Resource management such as increasing VMs and tearing
    down VMs. • It offers monitoring, optimization, and capacity planning functionality.
    2. Virtual Network Functions Layer: This layer consists of VNFs and VNFM functional
    blocks that contain some specific functions as shown in Fig. 4 [62]. VNFs: This
    block is responsible for implementing network functions in a virtual form called
    VNFs. The VNFs block is a combination of VNFs and Element Management (EM). EM
    helps VNFs block in implementing the management functions. VNFM: The functional
    block that manages all VNFs are called VNFM. The single VNFM can manage multiple
    VNFs or can serve a single VNF. The responsibilities of VNFM includes [63]: •
    Life-cycle management such as creation, updation, installation, scale-up/down,
    and deletion of VNFs. • Send requests to VIM to modify the resources as per user
    requirements. • VNFM is also responsible for FCAPS (Fault, Configuration, Accounting,
    performance, and Security) of VNFs. 3. Operation and Orchestration layer: This
    layer consists of OSS/BSS (Operational/Business Support System), NFVO, and Repositories,
    each having some specific functions as shown in Fig. 4 [64]. OSS/BSS: This framework
    allows us to move from the physical to the NFV environment without any change
    in existing tools and applications. NFVO: NFVO is responsible for the management
    and orchestration of NFV infrastructure and all software resources. NFVO performs
    the following tasks [65]: • It coordinates, authorizes the NFVI resources among
    VIMs located in the same or different PoPs. • It also manages the end-to-end service
    creation that contains the number of VNFs from different VNFMs. Services catalogue:
    There are four types of files or lists available that contain different information.
    • VNF catalogue consists of a set of usable VNFDs (VNF Descriptor) templates that
    describe the deployment and operational behavior requirements of VNFs and are
    used by VNFM block [66]. • Network Services (NS) catalogue consists of a usable
    network services template that describes the VNFs and their connectivity through
    virtual links. • NFV Instance list contains information about NS instances and
    VNF instances. • NFVI resources list is a repository that contains information
    regarding NFVI resources used to create NFV services. Download : Download high-res
    image (555KB) Download : Download full-size image Fig. 4. Low-level view of the
    NFV architecture [58]. 2.4.3. ETSI NFV MANO API specifications NFV specifies the
    requirements such as the NFV architecture and its components, as well as the protocols
    and APIs that provide the interface between the components. REST-based APIs have
    specified that cover the functionalities of the interfaces that are used at the
    reference points as shown in the Table 2. Table 2. ETSI NFV MANO API specifications.
    NFV solutions APIs Reference point API producer API consumer NFV-SOL 002 [67]
    VNF Lifecycle Management-API VNF Performance Management-API VNF Fault Management-API
    VNF Configuration-API VNF LCM Coordination-API VNF Indicator-API Ve-Vnfm VNFM
    VNFM VNFM VNF VNF/EM EM/VNF EM/VNF EM EM VNFM – VNFM NFV-SOL 003 [68] VNF Lifecycle
    Operation Granting VNF Package Management Virtualized Resource Quota Available
    Notification VNF Lifecycle Management VNF Performance Management VNF Fault Management
    VNF Indicator VNF Snapshot Package Management Or-Vnfm VNFM NFVO NFVO VNFM VNFM
    VNFM VNFM VNFM NFVO VNFM VNFM NFVO NFVO NFVO NFVO – NFV-SOL 005 [69] NSD Management
    interface NS Lifecycle Management interface NS Performance Management interface
    NS Fault Management interface VNF Package Management interface NFVI Capacity Information
    Interface VNF Snapshot Package Management interface LCM Coordination interface
    Os-Ma-Nfvo NFVO NFVO NFVO NFVO NFVO NFVO NFVO OSS/BSS OSS/BSS OSS/BSS OSS/BSS
    OSS/BSS OSS/BSS – – – NFV-IFA 006 [70] Software Image Management Resource Reservation
    Management Virtualized Resource Management Resource Performance Management Resource
    Fault Management Resource Quota Management Vi-Vnfm VIM VIM VIM VIM VIM VIM VNFM
    VNFM VNFM VNFM VNFM VNFM NFV-IFA 005 [71] Software Image Management Resource Management
    Resource Change Notification NFP Management Virtualized Resource Quota Management
    Virtualized Resource Fault Management Vi-Or VIM VIM VIM VIM VIM VIM NFVO NFVO
    NFVO NFVO NFVO NFVO NFV-SOL 009 [72] Configuration and Information Management
    interface Performance Management interface Fault Management interface Log Management
    interface NFV-MANO MANO Entity “” “” “” – – – – NFV-SOL 012 [73] Policy Management-API
    – API Producer – 2.5. NFV, SDN and cloud relationship NFV, SDN, and Cloud computing
    combine various attributes to create a flexible environment for virtual network
    functions [7]. Fig. 5 shows relationship of NFV with SDN and cloud, and how NFV
    is embracing these technologies to improve network performance and to build a
    scalable and flexible network. SDN is a networking paradigm that separates the
    control plane (brain) from the data plane (forwarding). The architecture of SDN
    is elaborated in [74] that consists of a data plane, control plane, and management
    plane. SDN is related to NFV but not dependent on it. They can transform future
    generation networks [75], if combined together. SDN serves NFV by managing the
    NFV network devices with the help of a centralized controller while NFV serves
    SDN by providing a virtualized infrastructure. The performance of the NFV framework
    increases using the SDN mechanism due to the separation of both planes in SDN.
    Hence, together SDN and NFV can make the network more agile, automated, flexible,
    and dynamic [76]. The concept of virtualization and resource sharing is not new
    in cloud computing. NFV is the foundation of cloud computing models such as IaaS,
    SaaS, and PaaS [77]. There are some similarities between NFV and cloud computing
    which are listed below: • Infrastructure-as-a-Service (IaaS) cloud is similar
    to the physical and hardware infrastructure in the NFVI block, such as (IaaS NFVI).
    • Software-as-a-Service (SaaS) cloud is same as virtual network functions in the
    VNFs block, such as (SaaS VNF). NFV is embracing SDN and cloud computing to become
    more successful. The migration from the physical server to a VM is not sufficient
    to gain maximum advantages. Adopting SDN and cloud is required as it offers benefits
    such as reduction in CAPEX and OPEX, scalability, automated orchestration, elasticity
    in VNFs, open API support, etc. [78]. In many data centers, providers re-architect
    the network to gain the benefits from these technologies. Table 3 shows the relationship
    of NFV with SDN and cloud computing. Download : Download high-res image (426KB)
    Download : Download full-size image Fig. 5. NFV relationship with SDN and Cloud.
    Table 3. Comparison of NFV with SDN and Cloud technology. Empty Cell NFV SDN Name
    Network Function Virtualization Software Defined Networking Basic Idea Transform
    NFs from dedicated to generic servers Separate control plane and Data Plane Optimizing
    Network Functions Network Infrastructure OSI Model Layer 4–7 Layer 2–3 Targets
    Service providers or operators DC, and cloud network Protocol None OpenFlow Supporting
    Organizations ETSI NFV working group ONF Initiative Supporters Network vendors
    Service providers (ISPs, ASPs, CSPs) Examples Load Balancers, Firewalls, DPI Switches,
    Routers, wireless access points Empty Cell NFV Cloud Computing Approach Service
    Abstraction Computing Abstraction Protocol NETCONF, SNMP OpenFlow Supporting Organizations
    ETSI NFV working group DMTF cloud working group Reliability Requires 5 nines (99.999%)
    Low demand Infrastructure NFVI IaaS Software VNFs SaaS 3. Classification of Virtualized
    Infrastructure Managers (VIMs) In this section, we present different VIMs which
    are chosen by researchers to implement their NFV experiments. VIMs manage the
    NFV infrastructure. VIM is the most critical component of the NFV-MANO block [65].
    We present a taxonomy to classify the available VIMs based on virtualization such
    as Process-based Virtualization/OS virtualization, Hypervisor-based Virtualization,
    and Container-based Virtualization as shown in Fig. 6. In this section, we elaborate
    on Mininet, Openstack, and Kubernetes because they are highly adopted by academia
    and the research community for validating their NFV experiments. Download : Download
    high-res image (301KB) Download : Download full-size image Fig. 6. Classification
    of VIMs based on virtualization. 3.1. Process-based virtualization The process-based
    virtualization means we can run multiple virtual hosts and switches on a single
    kernel OS. Here, we are representing VIM tools that are based on process-level
    virtualization [79] and used for NFV experiments. We also highlight its strengths
    and weaknesses for the applicability in SDN/NFV environment. 3.1.1. Mininet Mininet
    network emulator tool used to create a network consists of thousands of nodes
    such as hosts, links, and switches on a single machine. Mininet is a lightweight
    SDN testing platform [80]. We can emulate the entire hardware network into a single
    PC using Mininet. Many researchers use Mininet as an excellent tool for their
    SDN experiments [81]. Some of the features of using Mininet are listed below:
    • It runs the entire network environment on a single PC. • Mininet is an inexpensive
    testing platform to run SDN and NFV experiments. • There is no need to modify
    the code to run it on real hardware. • It is an open-source, fast, easy, scalable,
    and sharable platform. • It provides CLI, UI, and Python interfaces. Despite all
    the advantages of the Mininet, there are some limitations of Mininet which include:
    • The performance of the emulated network highly depends upon the capacity of
    a single PC. • Mininet cannot run on non-Linux compatible OpenFlow switches. •
    Mininet does not have support for the OASIS TOSCA [82] templates for NFV orchestration
    using VNFDs, NSDs to launch new VNFs and NSs. 3.1.2. Mininet extensions to support
    NFV frameworks Mininet does not support VNF functionality because its main focus
    is on SDN. The following frameworks have been developed that integrate with Mininet
    to deploy VNFs having NFV orchestration with general purpose VNF Manager. There
    are various open-source frameworks available based on process virtualization such
    as Mininet [83], EsCAPE [84], MeDICINE [46], NIEP [47], and Mini-NFV [48]. We
    summarized these frameworks by considering the parameters listed in Table 4. 1.
    Extensible Service ChAin Prototyping Environment (EsCAPE): The paper [84] discussed
    the Extensible Service ChAin Prototyping Environment (ESCAPE) based on UNIFY EU-funded
    FP7 project [85]. The framework uses Mininet to deploy and test the different
    modules of service chaining architecture. Click [86], NETCONF [87], and POX [88]
    are combined into this framework. Click, NETCONF, POX are used for the implementation
    of VNFs, VNFs management, and traffic steering through VNFs, respectively. They
    also added an Orchestrator module which is responsible for dynamically setting
    up SFCs and steering traffic through chains. 2. Multi Datacenter ServIce ChaIN
    Emulator (MeDICINE): In [46], the authors deployed Multi Datacenter ServIce ChaIN
    Emulator (MeDICINE), NFV based platform across the multi-PoP environment, and
    network functions are running as software containers. The third-party orchestration
    system is used to orchestrate and manage network functions through a standard
    interface. Both EsCAPE and MeDICINE use containers to deploy VNFs and are adequate
    for several NFV use-cases, but virtualization based on container technology also
    presents several issues in NFV scenarios [89]. First, containers are not compatible
    with cross-platform, and container management is problematic. Additionally, containers
    suffer from security problems due to kernel sharing and make them unsuitable for
    scenarios such as Virtual Customer Premises Equipment (vCPE) and Fog Computing.
    3. NFV Infrastructure Emulation Platform (NIEP): In [47], the authors implemented
    NFV infrastructure Emulation Platform (NIEP) that integrates the Mininet and Click-based
    VNFs for the evaluation of VNFs in the heterogeneous networks. NIEP addresses
    the security issues of EsCAPE and MeDICINE using hypervisor-based KVM virtualization
    [90]. The proposed solution is also portable as a result of its full virtualization.
    This framework makes it possible for network operators to test NFV experiments
    in a controlled environment before migrating to a production environment. Therefore,
    NIEP is a feasible solution for a case study such as Fog Computing and vCPE [91].
    Researchers used Mininet-based frameworks for the core evaluation of their NFV
    experiment, but none of them used the TOSCA model for the life-cycle management
    of VNFs. Existing frameworks use their own method to define VNFs as the EsCAPE
    framework uses the Python-based data model named NF-FG. As a result, Mininet does
    not support the standard TOSCA data model for VNF life-cycle management, implementation,
    and reproducibility of NFV test scenarios. 4. Mini-NFV: In [48], the authors proposed
    a mini-NFV framework, a general purpose VNF Manager and NFV orchestrator to deploy
    VNFs and network services on top of the Mininet. This framework uses the OASIS
    TOSCA templates defined by ETSI’s MANO architectural framework for defining VNF
    metadata [92]. Mini-NFV provides life-cycle management for VNFs such as VNF deployment,
    monitoring, and scaling. It loads the OASIS TOSCA templates in Mininet, and the
    workflow is similar to the OpenStack Tacker [93] workflow as discussed in Section
    4.2. According to the ETSI MANO architectural framework, the NFV-MANO has three
    blocks: NFV Orchestrator, VNF Manager, and VIM Manager. VIM Manager is responsible
    for managing the virtual infrastructure of network functions (NFVI) like computing,
    storage, and network resources [94]. In the world of Mininet, Mini-NFV is the
    project that implements both VNFM and NFVO as shown in Fig. 7. Mini-NFV is an
    excellent tool for managing virtual resources for agile SDN/NFV experiments. The
    mini-NFV framework offers some of the functionalities over the Mininet: • Mini-NFV
    maintains a catalogue of virtual resource allocation. • It is responsible for
    allocating and expanding resources. • It is also responsible to create VNFFGs
    (VNF forwarding graphs) with the help of virtual links, sub-networks, and ports.
    • It supports VNFDs (VNF descriptors) and VNFFGDs (VNF forwarding graph descriptors)
    to define VNF metadata definitions and the creation of VNF chain to form a path.
    • It also supports the Network Service Descriptor (NSD) which consists of several
    NFs having VNFs and VNFFG to create a connection between the NFs. • It provides
    start-up configuration and life-cycle management of VNFs. • Easy-to-use jinja2
    template engine is supported to automate large-scale VNF deployments [95]. • It
    supports network definitions for manually setting networks within Virtual Link
    (VL). • IP/MAC definition of a VNF at the connection point (CP). • The CPULimitedHost
    class specifies the number of processors and flavor properties. • Mini-NFV is
    lighter than vim-emu that uses the OpenStack API with complex software such as
    Docker, Mininet, Ansible, Juju, etc. Mini-NFV ignores the following properties:
    • Support to floating IP on VNFs. • Disk and RAM characteristics. At this point,
    the following properties are not implemented in Mini-NFV: • Autoscaling of VNFs.
    • Monitoring of VNFs. • Supports for container/click-based VNFs. • Support for
    NETCONF protocol Table 4. Summary of Virtualized Infrastructure Managers (VIMs)
    based on process virtualization. Empty Cell Mininet [83] EsCAPE [84] MeDICINE
    [46] NIEP [47] Mini-NFV [48] Source type Open-Source Open-Source Open-Source NA
    Open-Source Supporters Mininet community, ONF EU-funded FP7 UNIFY 5G-PPP project
    SONATA GT-FENDE project 5GinFIRE project Scalability Low Low Moderate High High
    Virtualization Para Para Para Full Para Language Python Python Python Python Python
    Security Low Low Low High Low TOSCA Support No No No NA Yes GUI Support Yes Yes
    Yes No No Controller Support Ryu, ODL POX NA NA POX, Ryu, ODL, third party controller
    VNF Support No Container running clickOS Container Virtual Machine Hosts having
    different network namespace VIM name Mininet Mininet MeDICINE Mininet Mininet
    VNFM/NFVO No EsCAPE Third-party s/w Virsh tool Mini-NFV Download : Download high-res
    image (262KB) Download : Download full-size image Fig. 7. Mini-NFV acts as VNFM
    and NFVO [48]. 3.2. Hypervisor-based virtualization The Hypervisor or Virtual
    Machine Manager (VMM) makes it possible to launch multiple VMs on a single host
    by virtualizing the physical resources among them. Type-I (bare metal) and Type-II
    (hosted) hypervisors are available [96]. The Type-I hypervisor is called a bare-metal
    hypervisor which runs directly on hardware like XEN, VMware ESXI, Hyper-V as shown
    in Fig. 8(a). On the other side, the Type-II hypervisor runs above the host operating
    system layer, such as VMware workstation, VirtualBox, and kernel virtual machine
    (KVM) [97] as shown in Fig. 8(b). In this section, we discuss OpenStack that uses
    hypervisor-based KVM virtualization to provide VMs for NFV experimentation. Download
    : Download high-res image (288KB) Download : Download full-size image Fig. 8.
    (a) Type-I (b) Type-II hypervisor. 3.2.1. OpenStack NFV The Linux Foundation project
    named open project for NFV (OPNFV) [98] uses OpenStack as the infrastructure running
    behind it. The main objective of OPNFV is to define the Open-Source technologies
    such as OpenStack and Opendaylight and how they fit together for NFV deployment.
    In ETSI NFV, OpenStack is a fundamental component to the Virtualized Infrastructure
    Managers (VIM) and has become a de-facto standard for NFV deployment [99]. The
    Heavy Reading Survey highlighted that 85.8% of telecoms regard OpenStack as a
    fundamental component to their NFV success [100]. The telecom operators and service
    providers such as China Mobile, AT&T, NTT group, Verizon, and SK Telecom already
    implemented OpenStack NFV. Many of these companies use OpenStack because it offers:
    multi-tenancy management, scalability, resource sharing, rich API for DevOps development,
    modular architecture, and resiliency feature for physical and virtual resources.
    In fact, telecom operators use OpenStack to control and manage the compute, storage,
    and network devices. In OpenStack, nine projects have been maintained officially
    such as Nova, Cinder, Swift, Keystone, Neutron, Horizon, and Glance. OpenStack
    is an open-source platform and anyone can add additional components as per their
    requirements [101]. The OpenStack community has added many projects that offer
    NFV-enhancing features, and each project has a ubiquitous code name. Table 5 shows
    summary of OpenStack projects deployed by OpenStack community to provide different
    services on OpenStack. Some of these projects include: Table 5. OpenStack services.
    Project name OpenStack service Purpose First appeared in OpenStack release Depends
    on Tacker NFV NFV orchestration Mitaka – Magnum Workload provisioning Container
    Orchestration Engine (CoE) provisioning Mitaka Keystone, Heat, Nova, Neutron,
    Glance Sahara ” Big Data Processing Framework provisioning Juno Cinder, Keystone,
    Glance, Heat, Neutron, Nova Trove ” Database as a Service Icehouse Cinder, Keystone,
    Glance, Nova, Swift Heat Orchestration Orchestration Havana Keystone Senlin ”
    Clustering service Mitaka Keystone Mistral ” Workflow service Liberty Keystone
    Zaqar ” Messaging service Liberty Keystone Blazar ” Resource reservation service
    Queens Keystone, Nova Aodh ” Alarming service Liberty Keystone Keystone Shared
    services Identity service Essex – Placement ” Placement service Stein – Glance
    ” Image service Bexar Keystone Barbican ” Key management Liberty Keystone Neutron
    Networking Networking Folsom Keystone Octavia ” Load balancer Liberty Glance,
    Keystone, Neutron, Nova Designate ” DNS service Liberty Keystone Swift Storage
    Object store Austin – Cinder ” Block storage Folsom Keystone Manila ” Shared filesystems
    Liberty – Nova Compute Compute service Austin Keystone, Neutron, Glance Zun ”
    Container service Pike Keystone, Neutron, Kuryr Horizon Web frontend Dashboard
    Essex Keystone Ceilometer Monitoring Metering & data collection service Havana
    – Panko ” Event, metadata indexing service Newton – Monasca ” Monitoring Mitaka
    – Kuryr Containers OpenStack networking integration for containers – Neutron Download
    : Download high-res image (266KB) Download : Download full-size image Fig. 9.
    Tacker acts as VNFM and NFVO. 1. TACKER (NFV Orchestration) project provides NFV
    orchestration (NFVO) and VNF Management to deploy NSs, VNFs, and end-to-end life-cycle
    management of VNFs on NFV platform such as OpenStack. 2. MAGNUM (Container Orchestration
    Engine Provisioning) is responsible for life-cycle management of Container Orchestration
    Engine (COE) such as Docker Swarm, Kubernetes, and Apache Mesos. 3. SAHARA (Big
    Data Processing Framework Provisioning) project provides Big Data Processing Framework
    such as Spark and Hadoop on top of the OpenStack. 4. TROVE (Database as a Service)
    provides relational and non-relational database features on OpenStack without
    performing any complex tasks. 5. HEAT (Orchestration) uses Heat Orchestration
    Template (HOT) in the form of text file to orchestrate the infrastructure resources
    of the cloud applications. It also provides auto-scaling service. 6. SENLIN (Clustering
    Service) provides a clustering service to OpenStack clouds. The aim of this project
    is to make orchestration easier for similar objects. 7. MISTRAL (Workflow Service)
    project manages the correct execution order of process that consists of series
    of task and their task relationship. It provides correct execution flow, synchronization,
    and parallelism. 8. ZAQAR (Messaging Service) project sends messages between different
    components of multi-tenant cloud. 9. BLAZAR (Resource Reservation Service) allows
    users to reserve resources for a particular amount of time. 10. AODH (Alarming
    Service) triggers actions according to the defined rules against data collected
    by ceilometer. 3.2.2. Tacker OpenStack Tacker is an open-source project based
    on ETSI’s MANO architecture framework. The project deploys and operates VNFs with
    the help of the NFV Orchestrator and VNF Manager on NFV platforms [102]. In the
    world of OpenStack, Tacker is the project that implements both VNFM and NFVO as
    shown in Fig. 9. Tacker is suitable for use with or without an SDN controller.
    In the first step, VNFM functionality with NFV orchestration can be tested without
    running an SDN controller. Additionally, Tacker also supports integrating the
    SDN controller as an NFV Orchestrator for testing NFV experiments [103]. 1. Tacker
    Architecture OpenStack Tacker architecture provides different blocks for orchestrating
    and managing end-to-end NS having different VNFs based on ETSI’s MANO framework
    [104]. Tacker’s high-level architecture includes NFV catalogue, APIs for the VNF
    life-cycle management, VNF monitoring, and automatic healing features as shown
    in Fig. 10(a). Tacker uses OpenStack or Kubernetes [105] as VIMs with nova, neutron,
    heat, keystone services. • The NFV catalogue consists of TOSCA templates such
    as NSD, VNFD, VNFFGD, and TOSCA template validation. To define NSD and VNFD, the
    standard declarative language TOSCA OASIS is used to describe all components.
    The TOSCA templates are embedded into the NFV catalogue to create VNF/VNFFG. we
    can also launch VNF/VNFFG directly from the templates. • The VNFM consists of
    managing the life-cycle of VNF with the initial configuration, health status,
    auto-scaling, and auto-healing features based on the VNFD policy. • The NFVO consists
    of multi-site VIM support for effective VNF placement, SFC support used to connect
    several VNF using VNFFGD policy, VIM resource checking, and allocation [106].
    The interaction between the various components of the Tacker implementation is
    shown in Fig. 10(b). In the Tacker workflow, tacker-client sends REST API calls
    to tacker-server [107]. Tacker-server performs a few basic operations with tacker-DB.
    The remaining operations sent to tacker-conductor using RPC, and the VNF life-cycle
    driver calls a suitable infra-driver such as OpenStack or Kubernetes to perform
    a real action. Tacker-conductor calls Mgmt-driver and monitor-driver for configuration
    and monitoring of VNFs. The Kubernetes driver provides the container as VNF, and
    the OpenStack driver provides the VM as VNF. Table 6 presents core packages and
    components used in Tacker architecture. Download : Download high-res image (771KB)
    Download : Download full-size image Fig. 10. (a) High-level view of Openstack
    Tacker project [104] (b) Tacker workflow. The Tacker project modified the OpenStack
    horizon service to provide a graphical interface to manage VNFs. We have installed
    OpenStack Tacker project into a VM having IP address “192.168.197.238”. In the
    OpenStack dashboard, “VNF Management” and “NFV Orchestration” tabs are installed
    as shown in Fig. 11 to facilitate interaction with the VNFs through GUI. Table
    6. Various packages and components present in Tacker architecture. Package name
    Operation python-tackerclient Package used for CLI and REST API calls tacker Main
    package of Tacker project Component name Operation tacker-client Communicate with
    tacker using REST API tacker-server Calls conductor using RPC tacker-db Performs
    some simple database operations tacker-conductor Responsible for all operation
    of VNFs and calls required drivers to provide interface to NFV infrastructure
    infra-driver Manage exact actions for OpenStack or Kubernetes resources vim-driver
    Used to register a particular VIM Mgmt-driver Implements exact actions to configure
    VNFs monitor-driver Implements exact actions to monitor VNFs policy-driver Responsible
    for Policy based VNF operations Download : Download high-res image (223KB) Download
    : Download full-size image Fig. 11. VNF Management and NFV Orchestration in OpenStack
    dashboard. Download : Download high-res image (414KB) Download : Download full-size
    image Fig. 12. (a) TOSCA-parser, HEAT-Translator, and Tacker interaction (b) Workflow
    for VNF deployment. Table 7. Fields used in VNFD and VNFFGD templates. Field name
    Template name Description tosca_definitions_version: VNFD, VNFFGD Tosca version
    on which template is based tosca_default_namespace: VNFD, VNFFGD Optional field
    that mentions default namespace description: VNFD, VNFFGD Description about the
    template metadata: template_name VNFD, VNFFGD Name of the template topology_template
    VNFD, VNFFGD Describes the topology of VNF or VNFFG topology_template: node_template:
    VDU, CP, VL VNFD Describes node type of a VNF such as properties of Virtual Deployment
    Unit, Connection Points, Virtual Links topology_template: node_template: FP VNFFGD
    Describes node type of a VNFFG such as properties and path of Forwarding Path
    topology_template: groups: VNFFG VNFFGD Describes grouping of nodes properties
    of VNF forwarding graph 2. OASIS TOSCA NFV Template Tacker uses the standard declarative
    language OASIS TOSCA to define the topology of a network that consists of network
    components. The TOSCA language uses the TOSCA template to specify VNF Descriptor
    (VNFD), VNF Forwarding Graph Descriptor (VNFFGD), and Network Service Descriptor
    (NSD) [108]. The TOSCA parser parses these templates and converts the simple TOSCA
    profile into an NFV YAML file. Heat Translator translates the non-HEAT template
    (TOSCA) into the Heat Orchestration Template (HOT) as shown in Fig. 12(a). OpenStack
    Heat project uses both TOSCA-Parser and Heat-Translator for NFVO and VNFM operations.
    The VNF deployment workflow is composed of VNFD for the creation of VNF, and VNFFGD
    for the creation of VNFFG as shown in Fig. 12(b). The VNFD and VNFFGD are optional
    because we can also create VNFs and VNFFGs directly bypassing the parameters without
    having to go through steps 2 and 4. VNFD and VNFFGD Template: The TOSCA standard
    defines multiple fields for the structure of the VNFD and VNFFGD templates [109].
    The VNF Descriptor template contains VNF behavior and deployment information and
    is written in the YAML language. On the other side, the VNFFG Descriptor template
    contains behavioral and deployment information of VNFFG. The list of fields used
    in VNFD and VNFFGD templates are presented in Table 7. A. Node Types (VNFD): The
    valid VNFD consists of three components Virtual Deployment Units (VDUs), Connection
    Points (CPs), and Virtual Links (VLs) under node_template. node_template is a
    child of topology_template. – Virtual Deployment Units (VDUs): Virtual Deployment
    Unit is a basic unit of VNF and it specifies type, properties and capabilities
    of the virtual machine that hosts the network function. * type: tosca.nodes.nfv.VDU.Tacker
    * properties: Describes the VDU properties such as image to be used, the availability
    zone, the floating Ips, the flavor of the VDU configuration, the custom user data,
    the monitoring policies and the management driver. * capabilities: Describes the
    computing properties of VDU such as RAM size, disk size, number of CPUs and cores
    per CPU. – Connection Points (CPs): A Connection point is a virtual NIC that connecting
    both internal and external virtual links. A CP always binds with VDU and virtual
    Link. * type: tosca.nodes.nfv.CP.Tacker * properties: Describes CP properties
    such as anti_spoofing_protection, CP management, CP order on VDU, security groups
    associated with CP, Mac and IP address definition using CP. – Virtual Links (VLs):
    Virtual Link is a logical entity that provides connectivity between VDUs. * type:
    tosca.nodes.nfv.VL * properties: Describes VL properties such as the name of the
    network VL connected to and the provider that generated VL. B. Node Types (VNFFGD):
    Tacker VNFFGD contains information about the single Forwarding Path. VNFFGD describes
    the VNFFG topology and is written in YAML language. – Forwarding Path (FP): VNFFGD
    contains entry about FP that consists of a chain of VNFs used to create a path.
    * type: tosca.nodes.nfv.FP.Tacker * properties: Describes FP properties, such
    as path ID for path identification, traffic matching policy, VNF chain (path).
    C. Groups (VNFFGD): In VNFFGD there may be a only single VNFFG or it consists
    of FP and a VNFFG. – VNF Forwarding Graph (VNFFG): VNFFG maps the FP to other
    types of nodes. * type: tosca.groups.nfv.VNFFG * properties: Describes the properties
    of VNF-FG such as vendor, version, virtual link, connection points, and constituent
    VNFs. * members: list of forwarding paths. 3. VNF Management (VNFM) OpenStack
    Tacker CLI is used for the creating, updating, deleting, and listing of VNFs and
    VNFDs. The syntax and workflow of OpenStack Tacker are similar to mini-NFV to
    implement VNFs and VNFDs [103], [110]. The following steps are carried out for
    the management of VNFs. Table 8 shows list of commands used for implementing these
    steps. (a) Register OpenStack VIM: In the first step, register OpenStack as VIM
    by creating vim_config.yaml file for VNF deployment. The config file consists
    of auth_url, username, password, project_name, project_domain_name, user_domain_name
    parameters. (b) Initiation VNF with VNF descriptor: Create sample-vnfd.yaml template
    file for the creation of VNF. It contains information about the VDU, CP, and VL.
    Next, create VNF by specifying the name of the vnfd file. (c) Direct VNF initiation:
    We can also create VNF directly from the template file without loading it into
    the VNFD catalogue. (d) Delete VNFD and VNF: We can also delete VNFD and VNF by
    assigning VNFD, VNF names to the delete command. (e) Check VNFM status: The list
    command is used to display VIM, VNFD, and VNF created in earlier steps. We can
    also check the attributes of instances using the show command. 4. NFV Orchestration
    (NFVO) The VNF forwarding graph (VNFFG) component present in OpenStack Tacker
    performs orchestration and traffic management across VNFs [93]. In a nutshell,
    VNFFG is also called Service Function Chain(SFC). SFC is an ordered list of VNFs
    to traverse the traffic, and the classifier decides which traffic to pass through
    them. VNFFGD describes the VNFFG, and VNFD describes the VNF. Moreover, VNFFGs
    can be initiated from VNFFGD or directly from the VNFFGD template. The following
    steps are performed for NFV orchestration. Table 9 shows list of commands used
    for implementing these steps. (a) Create VNFFGD: The OpenStack CLI as shown in
    the Table 9 is used for creating VNFFGD. The parameter –vnffgd-file defines the
    name of the YAML file. The field name of vnffgd denotes the name selected for
    VNFFGD. (b) Create VNFFG from VNFFGD: The second command listed in Table 9 is
    used to create VNF Forwarding Graph from VNFFGD. The parameter named “–vnffgd-name”
    specifies the name of VNFFGD. The –vnf-mapping specifies a VNFD to VNF logic instance
    mapping list, for example, VNFD1:VNF1, VNFD2:VNF2. The symmetrical argument is
    set to true if reverse traffic should pass through the path, otherwise, it is
    set to false. (c) Direct initiation of VNFFG: We can directly build VNFFG from
    the VNFFGD template without initiating VNFFGD. (d) Parametrized VNFFG: The value
    of the VNFFGD template can be parametrized using the –param-file argument that
    specifies the list of parameters. Different values can be passed when creating
    a VNFFG using the same VNFFGD template. (e) Check NFVO status: The sixth command
    shown in Table 9 verifies the status of NFVO resources. The seventh and eighth
    command are used to check the sub-components of VNFFG. Table 8. List of commands
    for VNF Management (VNFM). S.No Operation Command 1. Register OpenStack VIM “openstack
    vim register –config-file vim_config.yaml –description ‘first vim’ –is-default
    openstackvim” 2. Create VNF descriptor “openstack vnf descriptor create –vnfd-file
    ¡sample-vnfd.yaml¿ ¡vnfd name¿” 3. Create VNF using VNFD “openstack vnf create
    –vnfd-name ¡vnfd name¿ ¡vnf name¿” 4. Direct VNF Initiation “openstack vnf create
    –vnfd-template ¡file name of vnfd¿ ¡vnf name¿” 5. List VIMs, VNFDs, VNFs “openstack
    vim list”, “openstack vnf descriptor list”, “openstack vnf list” 6. Delete VNF
    Descriptor “openstack vnf descriptor delete ¡vnfd name¿” 7. Delete VNF “openstack
    vnf delete ¡name of vnf¿” 8. Inspect VNF “openstack vnf show ¡name of vnf¿” Table
    9. List of commands for NFV Orchestration (NFVO). S.No Operation Command 1. Create
    VNF forwarding graph descriptor “openstack vnf graph descriptor create –vnffgd-file
    ¡name of vnffgd file¿ ¡vnffgd name¿” 2. Create VNFFG using VNFFGD “openstack vnf
    graph create –vnffgd-name ¡vnffgd name¿ –vnf-mapping ¡vnf mapping¿ –symmetrical
    ¡boolean¿ ¡vnffg name¿” 3. Create VNFFG directly from VNFFGD template “openstack
    vnf graph create –vnffgd-template ¡name of vnffgd template¿ –vnf-mapping ¡vnf
    mapping¿ –symmetrical ¡boolean¿ ¡vnffg name¿ 4. Parametrized VNFFG template “openstack
    vnf graph create –vnffgd-name ¡vnffgd name¿ –param-file ¡param file¿ –vnf-mapping
    ¡vnf mapping¿ –symmetrical ¡boolean¿ ¡vnffg name¿ 5. Viewing VNFFG “openstack
    vnf graph list”, “openstack vnf graph show ¡vnffg name¿” 6. Check NFP status “openstack
    vnf network forwarding path list”, “openstack vnf network forwarding path show
    ¡nfp-id¿” 7. Viewing Chain “openstack vnf chain list”, “openstack vnf chain show
    ¡chain-id¿” 8. Viewing Classifier “openstack vnf classifier list”, “openstack
    vnf classifier show ¡classifier-id¿” 3.3. Container-based virtualization The VM
    deployment in hypervisor-based virtualization increases scalability and agility
    by turning more VMs on a physical host. But nevertheless, each VM requires RAM,
    Storage, CPU, Guest OS, and thus consumes more resources. To solve this issue,
    a lightweight tool called container-based virtualization is required [111]. The
    kernel of the host OS is used to run the multiple guest OS in container-based
    virtualization. We can run multiple isolated containers on one instance of the
    OS using Docker. 3.3.1. Docker Docker is one of the most popular container technologies
    since it was first launched in 2013. It is an open-source platform that allows
    developers to group applications into executable components known as containers.
    Docker toolkit provides simple commands for deploying, building, executing, upgrading,
    and stopping containers [112]. A container is a software module that collects
    the code and all its dependencies. Multiple containers share the kernel of the
    OS, and each container runs as an isolated process in the user space. Docker containers
    are portable, lightweight, and have less start-up time than VMs [113], [114].
    In the Table 10, we compare containers to the virtual machines. However, Docker
    technology is insufficient to manage containerized applications because thousands
    of docker containers are used in complex software projects [115]. To address this
    challenge, a container orchestration tool such as Kubernetes, Docker Swarm [116]
    is used by most companies to orchestrate and manage the life-cycle of containers
    [117]. Table 10. Comparison between container and virtual machine [113]. Empty
    Cell Virtual machine Container Definition It runs on top of the hypervisor. It
    runs on top of the host operating system. Isolation Completely isolated from the
    host OS and other VM. Lightweight isolate from host OS and other containers. Size
    VM size is very large. Container size is very small. Boot time Takes a minute
    to run. Take a few seconds to run. Security More secure. Less secure. OS Each
    application running in a VM can have different OS. Each application running in
    a container shares same OS. Memory VM uses more memory. Container requires less
    memory. Environment It virtualizes the computer system. It virtualizes the OS.
    Load Balancing Move running VMs to another server in a failover cluster for load
    balancing. Containers cannot move themselves. Container uses orchestrator to automatically
    start or stop containers. Portability VMs are less portable as it is harder to
    move. Containers are easy to move. Deployment Single VM using Windows Admin Center.
    Multiple VMs by Virtual Machine Manager (VMM). Single Container using Docker.
    Multiple VMs using orchestrator tool such as Kubernetes. Applications When we
    want to run different applications using different OS. Container is useful when
    we want to run maximum applications using minimum servers. Examples KVM, VMWare,
    Xen Docker 3.3.2. Kubernetes (k8s) Kubernetes (k8s) allows the orchestration of
    thousands of containers and is known as Container Orchestration Engine (CoE).
    It is an open-source platform and originally developed by Google. It is responsible
    for managing cluster of containerized applications. It manages the deployment,
    networking of containers, automatic scaling, and launch of a new container in
    the event of a failure of an existing one [118], [119]. Kubernetes supports many
    microservices that constitute a useful application. It automatically orchestrates
    the cluster and makes the required modifications to its components. On the other
    hand, in the absence of the Kubernetes manager, we need to manually update thousands
    of containers every time new features are added to the application. This manual
    process takes considerable time and can lead to errors. In the ETSI NFV architecture,
    Kubernetes fulfills both VIM and VNFM functions [120]. Some of the reasons for
    using Kubernetes include : 1. Availability: The multi-master and worker nodes
    function of K8s allows you to connect multiple clusters. This feature provides
    high availability by automatically moving one container to another cluster if
    it fails or is down [121]. 2. Deployment: Users describe which containers and
    how many to deploy. Kubernetes keeps the container running and also handles deployment
    changes such as pause, resume when required. 3. Self-healing: Kubernetes can reboot
    or replace the container that fails, kills the container that is not responding
    to the health check operation [122]. 4. Discovery and load balancing: To disclose
    a container, Kubernetes uses a DNS name or IP address. Kubernetes also balances
    the incoming load if the traffic to a single container is large and makes the
    deployment stable [123]. 5. Storage Orchestration System: Kubernetes can use local
    storage, public cloud storage to secure the storage system. A. Kubernetes architecture
    and components: Kubernetes is a client–server architecture that consists of one
    or more master nodes that act as a control plane, and worker nodes are responsible
    for the execution of the containers [123]. Download : Download high-res image
    (547KB) Download : Download full-size image Fig. 13. K8 architecture [123]. 1.
    Master Node: The master node consists of Kube-API-Server, ETCD, Kube-Scheduler,
    Kube-Controller-Manager, and Cloud- Controller-Manager as shown in Fig. 13 [121].
    Kube-APIServer: The API server is a frontend for the Kubernetes master component
    which receives all REST requests from the “kubectl” component. The API server
    also communicates with the ETCD to ensure that the data is stored. The Kube-API
    server can scale horizontally by deploying more instances to balance the traffic.
    ETCD: It is a distributed key–value store that stores information about the cluster
    data such as the number of pods, pod status to manage the cluster. It is only
    accessed by the API server and stores configuration information on all the nodes
    in the cluster in a distributed way. Kube-Scheduler: It is responsible for monitoring
    the working node and assigning the least loaded node to the newly created container.
    It continues to track available resources as well as resources assigned to the
    existing workload. Kube-Controller-Manager: This component runs multiple controllers
    such as node controller (responds when nodes go down), replication controller
    (maintains an exact number of containers, and endpoints controller (joins service
    and pods) on the master node. Cloud-Controller-Manager: Cloud controller manager
    is used for executing cloud-specific controllers. It connects the cluster to the
    cloud provider’s APIs. It also executes a controller that interacts with the cloud
    provider. 2. Worker Node: The worker node consists of kubelet, container runtime,
    docker, and Kube-proxy as shown in Fig. 13. Kubelet: The Kubelet agent runs on
    each worker node within the cluster and sends the health report from the worker
    node to the master node. It ensures that pods and their containers are healthy
    and working in the desired state. Container Runtime: Kubernetes uses Docker as
    the container runtime engine for starting and stopping containers. Kube-proxy:
    Kube-proxy runs on each worker node to send a request to correct pods/containers
    running on different nodes of a cluster. Pod: The pod act as a wrapper for a container
    and the smallest unit of K8 architecture. The master node schedules the pod on
    a specific node according to the availability of the resources. 3.4. Discussion
    Although Kubernetes and OpenStack are competitors, both are open-source and mutually
    complementary technologies. To improve scalability and automation, we can combine
    Kubernetes and OpenStack [124]. We can deploy Kubernetes on an OpenStack infrastructure
    to launch and maintain applications [125], [126]. The Magnum project delivers
    Kubernetes in an OpenStack environment [127]. The Magnum API is used for the management
    and operation of Kubernetes clusters, objects, and services. The OpenStack cloud
    orchestration tool allows containers to be aligned within that infrastructure
    and share resources among them. The benefits of the combination of OpenStack and
    Kubernetes [128], [129], [130] include: • Kubernetes with a large-scale distributed
    system runs smoothly. At the same time, OpenStack lacks the stability to operate
    smoothly. Combining these two technologies enhances stability since OpenStack
    uses a more modernized architecture. • OpenStack has a steep learning curve because
    of its complexity and sometimes being a hindrance to users. Deploying OpenStack
    is much easier to manage with the Kubernetes orchestration features. • Security
    is the major concern in Kubernetes technology due to kernel sharing among containers.
    OpenStack provides a high level of security when it works with Kubernetes to resolve
    this issue. • Kubernetes has a self-healing infrastructure feature and makes OpenStack
    easier to manage and resilient to the failure of compute nodes. • The combination
    of OpenStack with Kubernetes makes the open standard for container technology
    and becomes universally applicable. 3.5. Open-source NFV-MANO solutions The NFV
    MANO is responsible for monitoring workloads deployed on a cloud infrastructure.
    The point of this section is to offer an overview of open-source MANO solutions
    that are currently available. We divided open-source MANO solutions into two categories,
    as shown in the Table 11. (i) MANO stack subset solutions provide a mechanism
    for a subset of the MANO stack. (ii) MANO stack integration solutions provide
    a complete MA-NO framework comprising VIM, VNF Manager, and NFVO. Table 11. Other
    Open-Source NFV-MANO Solutions. MANO stack subset solutions MANO stack integration
    solutions Management and orchestration solutions Infrastructure solutions Empty
    Cell Empty Cell Project ONAP [131] OSM [132] Cloudify [133] Tacker [108] OpenBaton
    [134] OpenStack [135] OpenVIM [136] OpenMANO [137] OpenNFV [137] Leader Linux
    Foundation TeleFonica Gigaspace OpenStack Foundation Fraunhofer Open Infrastructure
    Foundation OSM project Telefonica HP NFV MANO Framework VNFM, VIM, NFVO, OSS/BSS
    VNFM, VIM, NFVO VNFM, NFVO VNFM, NFVO VNFM, VIM, NFVO VIM VIM VNFM, VIM, NFVO
    VNFM, VIM, NFVO Resource Orchestration Cloud, SDN, NFV Cloud, SDN, NFV Cloud,
    NFV Cloud, NFV Cloud, NFV Cloud, NFV Cloud, NFV SDN Cloud, NFV Supported VIM OpenStack
    OpenStack, VMware, AWS, GCP, OpenVIM Openstack, Azure, GCP, CloudStack OpenStack
    AWS, OpenStack, Docker container – – OpenVIM, OpenStack HP Helion OpenStack Supported
    VNFM Generic VNFM Juju, OpenMANO Generic and Specific VNFM Generic and Specific
    VNFM Generic & Specific VNFM, and Juju Tacker, OpenBaton, OSM OpenMANO, OSM OpenMANO
    HP NFV Director Data Model Support HOT, TOSCA, YANG YANG TOSCA HOT, TOSCA TOSCA
    – OASIS TOSCA YANG – Learning Curve Complex because not well documented Easy due
    to active community support – Moderate due to regular updates Easy due to GUI
    Easy due to community support Easy as compared to OpenStack Complex due to MANO
    integration Complex due to MANO integration Kubernetes Support Not Supported Can
    run multiple instances with a different namespace Support cloud services through
    the Kubernetes plugin Can manage Kubernetes cluster Enable using OpenBaton K8s
    driver Using Magnum project Not supported Supported – Language Java, Python, Javascript
    Python – Python Python, Go, Java Python Python Python – CLI Support Yes Yes Yes
    Yes Yes Yes Yes Yes Yes Dashboard No Yes No No Yes Yes No No No Maturity Medium
    High High Medium Medium High Low High Low Compute Virtualization – – – – – KVM
    KVM/libvirtd KVM Xen (libvirtd) Adopted By Service providers such as AT&T, China
    Mobile, and vendors such as Nokia, Cisco, Ericsson, and Huawei Network Operators
    such as British Telecom and Telefonica, vendors such as Intel, RIFT.io, Dell,
    Brocade Belgium Telecommunication service provider and operator Research community
    Mobile Core Network Telecom service providers such as SK Telecom, China Mobile,
    AT&T Adopted by the OSM community as VIM Not suitable for commercial deployment
    AT&T, Vodafone, China Mobile, and IT vendors such as Cisco, Dell, HP, Huawei,
    Ericsson, and Redhat 3.5.1. MANO stack subset solutions We classified it into
    two categories according to provided MANO capabilities. (a) Management & Orchestration
    Solutions (VNFM & NFVO) provide orchestration and lifecycle management of VNFs.
    (b) Infrastructure Solutions (VIM) is responsible for building the MANO infrastructure.
    • Management and Orchestration Solutions (VNFM & NFVO): – Open Network Automation
    Platform (ONAP) [131], [138]: AT&T’s ECOMP and OPEN-O projects merged to create
    ONAP, an Open-Source MANO solution. ONAP is used by Cloud service providers and
    operators to orchestrate, manage and automate the network. On top of that, ONAP
    offers different graphic design and monitoring tools allowing users to run these
    services in a dynamic and real-time cloud environment. The global mobile market
    wants to use ONAP to design and deliver the future connected world due to the
    thriving and emerging community dedicated to ONAP’s success. – Open-Source MANO
    (OSM) [132], [139]: Open-Source MANO is an ETSI-hosted initiative to build an
    open standards NFV MANO software stack consistent with its proposed architecture.
    At Mobile World Congress (MWC) 2016, the idea was initially exhibited as an operator
    use case. OSM uses OpenMANO and RIFT.io as well as OpenStack and Ubuntu Juju projects.
    Additionally, OSM offers support for multi-site management so service delivery
    can be automated across locations. From the SDN perspective, OSM’s second release
    supports Opendaylight (ODL), Open Network Operating System (ONOS), and Floodlight,
    as well as VMWare vCloud, OpenStack, and AWS (Amazon Web Services) as VIMs. Telcos
    such as British Telecom, Korea Telecom, Telekom Austria Group, Telefónica, and
    Telenor, as well as suppliers such as Mirantis, RIFT.io, Intel, Dell, Brocade,
    RADware, and others, support OSM. – Cloudify [133]: The Cloudify platform is an
    Open-Source cloud orchestration solution based on TOSCA. Firstly, cloudify began
    as an orchestration platform, similar to OpenStack HEAT. Then, an enhanced cloudify
    Telecom Edition was born to support NFV-related use cases. This solution is built
    around ETSI MANO architecture and used as NFVO and VNFM. Due to APIs that it supports,
    it interacts with multiple VIMs, containers, and even external and non-virtualized
    infrastructure and devices, OSS and BSS. As VIMs, Cloudify supports OpenStack,
    VMWare vcloud, Azure Cloud Stack, SoftLayer, and AWS. Moreover, it supports ODL,
    OpenContrail, and ONOS as SDN controllers. – Tacker [108]: In OpenStack, Tacker
    aims to create an Open NFV Orchestrator (NFVO) and an Open Virtual Network Functions
    Manager (VNFM) for deploying and managing VNFs in an OpenStack-managed virtual
    infrastructure. It promises to comply with the ETSI MANO Architectural Framework,
    offering a fully functional stack to orchestrate VNFs from end to end. The VNFM
    component of Tacker can manage the entire lifecycle of a virtual network function,
    including start and stop operation, monitoring, configuration, and auto-healing.
    NFVO component, on the other hand, enables end-to-end deployment of Network Services,
    VNF placement and service function chaining, management resource allocation using
    VIM, and orchestration of VNFs across multiple VIMs. – OpenBaton [134]: The Technical
    University of Berlin and Fraunhofer FOKUS have launched an Open-Source project
    called Open Baton that provides a network orchestration framework for multiple
    points of presence through the use of NFV MANO. This current release of Open Baton
    provides a significant increase in functionality. The Generic EMS and Generic
    VNFM enable it to orchestrate a diversity of Virtual Network Functions (VNFs)
    compiled as part of any network service. The plug-and-play model makes it easy
    to integrate AMQP and REST APIs into existing VNFMs and SDKs in different programming
    languages (Java, Python, Go). NFVIs are managed across multiple sites with heterogeneous
    virtualization and cloud computing. Although OpenStack is the most widely used,
    it provides drivers that enable additional VIM types. At the infrastructure level,
    Multi-Tenancy is supported through network slicing, making optimum use of SDN
    technology to guarantee network service isolation across physical resources shared
    by multiple services. • Infrastructure Solutions (VIM): – OpenStack [135]: Various
    technologies can be employed to build an NFV data center. NFV OpenStack is one
    of the powerful open-source technologies used to construct a virtualized infrastructure.
    Many of the world’s major service providers, cloud providers, and large corporations
    use OpenStack to develop an NFV infrastructure compliant with the European Telecommunications
    Specifications Institute (ETSI) MANO NFV standards. The Linux Foundation and ETSI
    have implemented OPNFV by using OpenStack as the Virtualized Infrastructure Manager
    (VIM), a part of the MANO framework. – OpenVIM [136]: Compared with other VIMs,
    OpenVIM is a lightweight VIM based on NFV principles and relies on the OpenFlow
    controller (OFC). OpenVIM controls infrastructure and provides EPA features such
    as CPU, memory, and NUMA pinning through OFC. Firstly, OpenMANO contributed to
    ETSI Open-Source MANO (OSM) as seed code. With OSM Release 1, OpenVIM is also
    contributing to OSM. OpenVIM is now a part of the OSM project, administered by
    ETSI, after being developed initially as part of the OpenMANO suite. With OpenVIM,
    the entire lifecycle of the virtual infrastructure manages through a single piece
    of software. 3.5.2. MANO stack integration solutions These projects provide a
    complete MANO solution comprising VIM, VNFM, and NFVO as an integration project.
    • OpenMANO [137]: Telefonica’s OpenMANO project combines VIM (OpenVIM), a VNF
    manager, and an orchestrator. As part of the Open-MANO architecture, there are
    three components: OpenMANO, OpenVIM, and a graphic user interface. Command-line
    interfaces (CLIs) use to interact with OpenMANO and OpenVIM. OpenMANO can also
    interact with OpenStack as VIM as part of its architecture. In this project, OpenVIM
    acts like an NFV VIM. It is similar to OpenStack and provides compute and networking
    capabilities for NFV infrastructure nodes and an OpenFlow controller for deploying
    virtual machines. OpenMANO also communicates with OpenVIM through a REST-based
    Northbound interface. It includes CPU and NUMA pinning, PCI passthrough, and other
    EPA-friendly features. The OpenMANO project is a Network Functions Virtualization
    Orchestrator (NFVO) implementation. OpenMANO communicates with OpenVIM through
    a REST-based northbound interface and is responsible for creating and deleting
    VNF and NS templates and instances. Furthermore, the orchestrator is enhanced
    with platform-specific fields in addition to VNF and NS descriptors, whereas the
    VNFM is quite generic and DSL-capable. OpenMANO is an actual basic implementation
    that cannot be deployed commercially. • OpenNFV [137]: An OpenNFV platform is
    HP’s NFV platform uses open-source technology to provide a secure end-to-end NFV
    and SDN infrastructure. OpenNFV aims to provide solutions for every functional
    block defined in the ETSI NFV reference architecture. OpenNFV MANO contains three
    functional blocks: NFV Director as NFVO, NFV Manager as VNFM, and Helion Openstack
    as VIM. NFV Director of MANO architecture is responsible for deploying and monitoring
    VNFs. A vital goal of this effort is to ensure that VNFs run efficiently using
    both hardware platforms and virtual environments. VNF managers manage the life
    cycle of VNFs, such as the creation, deletion, and scaling of VNFs. Helion OpenStack
    provides NFV infrastructure to run VNFs. 4. Review of service function chain provisioning
    techniques based on the selection of Virtualized Infrastructure Managers (VIMs)
    4.1. Systematic search methodology Systematic scrutiny offers a precise method
    to collect and identify the research articles existing in a literature related
    to a particular research problem. We followed Systematic Literature Review (SLR)
    protocol to conduct our work, helps to identify research challenges that provide
    further direction for researchers. The SLR is used by number of researchers to
    present their surveys in the field of Cloud Computing [140], Software Engineering
    [141], and Security [142], etc. The following are the series of steps that we
    performed to conduct systematic research: • The first step is to formulate the
    search string to select an appropriate research articles related to our research
    problem. The standard search string such as “NFV Management and Orchestration”,
    “Service Function Chaining Implementation Platform”, and “Virtualized Infrastructure
    Managers for NFV” are used to conduct a search. • We used general and specific
    phase to collect research articles based on these search strings. In a general
    phase, the digital library (Google Scholar) is used. The four digital libraries
    such as ACM [143], Springer [144], IEEE Xplore [145], and ScienceDirect [146]
    is used in specific phase. The time period 2015–2021 is used to collect the literature
    related to research problem. • The research articles, collected from all the digital
    libraries, have been further narrowed down based on inclusion and exclusion criteria
    for critical analysis. In the filtering process, we removed duplicate entries
    and excluded entries based on title, abstract, and full text. In the end, we obtained
    36 high quality research papers. • Finally, we critically reviewed all the research
    papers to collect the relevant data from each one and filled in the pre-designed
    excel form. The form consists of fields such as Deployment Environment, VIM, VNFM
    & NFVO, Experiment Setup, SDN Controller and Language, and Performance Metrics.
    • We classified these research articles into four categories by providing taxonomy
    according to the selection of Virtualized Infrastructure Managers (VIMs) including
    Management and Orchestration features to perform NFV experiments as described
    in Section 4.2. 4.2. Proposed taxonomy In this section, we conducted an in-depth
    analysis of literature research articles that selected Emulation, Virtualization,
    Containerization, and Hybrid Environment to implement its NFV experiments. Next,
    we classified these research articles into four categories according to the selection
    of Virtualized Infrastructure Managers (VIMs). 1. Emulation Environment as VIM
    (EE-VIM) 2. Virtualization Environment as VIM (VE-VIM) 3. Containerization Environment
    as VIM (CE-VIM) 4. Hybrid Environment as VIM (HE-VIM) In the end, out of the total
    research articles, seven papers belong to EE-VIM, eighteen studies related to
    VE-VIM, seven studies related to CE-VIM, and only four studies provide information
    regarding HE-VIM. The state-of-the-art taxonomy as shown in Fig. 14 classifies
    these research articles into eight categories based on deployment environment,
    types of network functions, number of SFCs, SFP strategy, VIM, VNFM & NFVO feature,
    network scenario, SDN controller and language support. Download : Download high-res
    image (642KB) Download : Download full-size image Fig. 14. Taxonomy of Virtualized
    Infrastructure Managers. Download : Download high-res image (477KB) Download :
    Download full-size image Fig. 15. Classification of Virtualized Infrastructure
    Managers according to Deployment Environment. 4.3. Emulation Environment as VIM
    (EE-VIM) The proposed taxonomy as shown in Fig. 15 considers VIM and VNFM & NFVO
    parameters to divide the Service Function Chain Provisioning solutions based on
    Emulation Environment. Table 12 shows summary of some research articles that selected
    Emulation Environment as VIM to implement NFV experiments. We reviewed a number
    of research articles that come under this category and which are described as
    follows: 1. Hong et al. [30] implemented I2NSF interface to monitor the NSFs and
    divide the load among them using Mininet framework. They failed to test the experiment
    in OpenStack. The overhead incurred due to additional step by monitoring and load
    balancing is also not considered in this paper. 2. Jiefei et al. [147] presented
    a design that balances the load among multiple chains using a connection-aware
    load balancer. They failed to evaluate the experiment with NF chain having multiple
    network functions of different types and on top of a real network such as OpenStack.
    3. Cheng et al. [148] implemented a placement algorithm where virtual switches
    equipped with network functions instead of implementing network functions on separate
    middleboxes. This framework reduces 2/3 traffic of the current network. The network
    services implemented on SDN switches is less powerful than implemented on middleboxes
    but it reduces unnecessary forwarding. 4. Thai et al. [29] implemented algorithms
    to solve network and server load balancing simultaneously and they test their
    experiment on Mininet emulator. It improves bandwidth utilization but they did
    not used OpenStack for their experiment evaluation. 5. Ma et al. [149] developed
    an algorithm for selection of efficient service function path that improves resource
    utilization by eliminating the overloaded SFs, and SF having price greater than
    the acceptable price. They did not evaluate their experiment in real network and
    with different type of VNFs. 6. Thai et al. [150] implemented Hash based Traffic
    Steering on soft switches (HATS) for chaining virtualized network functions without
    control and data plane overhead using ODL controller and OpenvSwitch. They conducted
    their experiment in a fat tree topology using Mininet emulator. 7. Baldoni et
    al. [151] proposed an emulation framework that consists of two PCs named “MiniNet”
    and “B”, running Mininet emulator and Open Daylight controller respectively. This
    framework enables fast deployment and testing of SDN/NFV experiments without the
    need for complex hardware. Table 12. SFC provisioning approaches classified based
    on Emulation Environment as VIM . Ref. Deployment environment VIM VNFM & NFVO
    Experiment setup SDN controller Type of NF Language used Performance metrics Hong
    et al. [30] Emulation Mininet Mini-NFV, Controller SDN network (3 clients), External
    network (Facebook and google server), 3 VNFs, 4 SFF, 1 security controller Open
    Daylight Firewall Java, YANG, XML Throughput Jiefei et al. [147] Emulation Mininet
    – Dell Server (I2 cores, 32 GB memory, 1 GB Ethernet cards) represents Mininet
    having 5 switches and 6 servers (hosts and network functions). 2 PCs (dual core
    6320 Intel CPU, 4 GB memory) represents LB1 and LB2. – IDS (snort), Firewall,
    Proxy C++ Average throughput Cheng et al. [148] Emulation Mininet Controller 6
    OpenFlow switches, 3 network functions, SDN Controller – Firewall, IDS, Proxy
    – Number of hops Thai et al. [29] Emulation Mininet – 4-ary FatTree topology,
    Open vSwitch, SDN controller Open Daylight Helium-SR3 – Java Bandwidth utilization
    Ma et al. [149] Emulation Mininet Mini-NFV, Controller Ingress node, service node
    (SN) 4, number of service function instance (SFI) with different SF type 3, egress
    node, SFC controller Open Daylight – Java Packet loss rate, SFC rejection rate,
    Resource utilization, Standard Deviation Thai et al. [150] Emulation Mininet Controller
    4pod fat tree topology using Mininet emulator, click elements to implement 4 types
    of VNFs, 16 instances per VNF, Open vSwitch 2.5.0, SDN controller Open Daylight
    Beryllium-SR2 L2 packet forwarding (LF), bandwidth shaper (BS), IP packet filter
    (IF), NAT Java VNF and network path load balancing, total number of flow entries,
    service chaining time. Baldoni et al. [151] Emulation Mininet Controller Mininet
    and Open Daylight installed on two Intel NUC DC53427HYE (Ubuntu 14.04 OS) Pcs,
    Mininet PC equipped with 3 USB-Ethernet adapters, 3 WiFi-Access points act as
    an CPE, 4G FemtoCell provides access to LTE smartphones Open Daylight Hydrozen
    Acquirer, Streamer Java – 4.3.1. Discussion In Emulation Environment as VIM (EE-VIM),
    our proposed taxonomy considers Virtualized Infrastructure Managers (VIMs) along
    with available Orchestration & Management (NFVO & VNFM) parameters to divide the
    Service Function Chain Provisioning solutions based on Emulation Environment.
    The seven research articles that belong to EE-VIM out of the total articles. After
    reviewing all the research articles, we propose different use cases for the appropriate
    selection of emulation environment as VIM. The choice of a particular VIM depends
    on the requirements of the particular application. Firstly, Mininet alone does
    not support NFV functionality. The open-source platform such as EsCAPE, MeDICINE,
    NIEP, and Mini-NFV integrate with Mininet having NFV Orchestrator and VNF Manager
    to deploy VNFs. In various NFV experiments, Mini-NFV makes use of Mininet for
    agile SDN/NFV experiments [152]. Emulation environment suitable for the following
    scenarios. • When we require an easy transition from emulated to real-world NFV
    experimentation scenarios without any changes in the code, Mini-NFV is used. •
    The Mini-NFV makes use of the parametrized OASIS TOSCA template defined by ETSI
    to gain reproducibility, standardization, and agility in NFV experiments. • Mini-NFV
    is best choice when only generic VNFs are needed from a source host to a destination
    host, and isolation of VNFs is not required. • The Emulation capabilities of Mini-NFV
    might be good to use for small proofs-of-concept in research, but it is not well
    adapted to large-scale emulations that are needed in an industry environment.
    • EsCAPE, MeDICINE use containers to deploy VNFs and adequate for NFV use-cases.
    They suffer from security problems due to kernel sharing and make them unsuitable
    for scenarios such as Virtual Customer Premises Equipment (vCPE) and Fog Computing.
    • NIEP is a best solution for a case study such as Fog Computing and vCPE, as
    it integrates the Mininet and Click-based VNFs for the life-cycle management of
    VNFs. It provides complete emulation environment to test NFV experiments in a
    controlled scenario before deployment in production environment. 4.4. Virtualization
    Environment as VIM (VE-VIM) The proposed taxonomy as shown in Fig. 15 considers
    VIM and VNFM & NFVO parameters to divide the Service Function Chain Provisioning
    Solutions based on Virtualization Environment. Table 13 shows summary of research
    articles that selected Virtualization Environment as VIM to implement NFV experiments.
    We reviewed a number of research articles that come under this category, and which
    are described as follows. 1. Chou et al. [153] implemented VLAN and OpenFlow (VOFSFC)
    based SFC using real time testbed including Pica 8 OpenFlow Switch, RYU controller,
    and All-in-one hypervisor to implement network functions. 2. Kim et al. [154]
    calculated the physical and virtual resource usage using reinforcement learning
    to develop dynamic service function chaining. They tested their experiment on
    top of FNCP architecture (NFV service providing platform) consisting of OpenStack
    and ODL controller. 3. Medhat et al. [155] developed a QoS-aware SFC orchestrator
    that is capable of rerouting the traffic to a new path if the existing path contains
    overloaded network functions. They used opensource MANO Open-Baton toolkit with
    OpenStack and ODL controller for their evaluation. However, the proposed work
    can be validated with multiple chains and scaling-out and -in multiple SF instances.
    4. Yang et al. [156] proposed an alarm-based monitoring driver using OpenStack
    API (ceilometer, Heat) and ODL driver in Tacker for high availability of SFC.
    5. Akhtar et al. [157] proposed control-theoretic load balancing approach for
    VNFs using RINA distributed monitoring application on top of GENI testbed. 6.
    Sun et al. [158] proposed SFC Deployment Optimization (SFCDO) algorithm based
    on BFS to optimize both computing resources and end-to-end delay. They also designed
    optimal selection factor (OSF) to select a node with least load. 7. Ziri et al.
    [159] implemented the SFC on OpenFlow environment with OpenDayLight controller.
    They designed, implemented, and tested SFC in real environment and showed that
    solution is functional and sound. 8. Yi et al. [160] solved scalable SFC provisioning
    problem (S2FCP2) by developing reactive and proactive strategies. They validated
    their work using small-scale and large-scale network topologies and showed that
    the reactive approach is better in a large network and proactive is best for a
    small network. 9. Shen et al. [161] presented adaptive scheduling mechanism to
    fulfill dynamic flow requirements and achieve network performance while minimizing
    the cost. To validate the work, they used BCube and Barabasi–Albert real-world
    network topologies with OpenFlow enabled switches and a SDN controller. 10. Soualah
    et al. [162] proposed an algorithm named energy-efficient tree-based chain algorithm
    (EE-TCA) that reduces the energy consumption at the hardware/software level by
    minimizing the number of servers and sharing VNFs among multiple tenants. 11.
    Davoli et al. [163] proposed separate control plane solution to handle SFC entities
    while keeping it independent from the underlying infrastructure. They deployed
    VMs on top of single machine through libvirt/KVM virtualization. To define the
    capacity of elements in the SFC, 12. Heideker et al. [164] used queueing modeling
    approach. They evaluated the performance on a testbed having paravirtualization
    (XEN) for VNF implementation and Pica 8 OpenFlow enabled SDN switch. 13. Ozdem
    et al. [165] developed a subscriber aware dynamic SFC framework and tested in
    a live telecom lab. They showed that the proposed framework able to assign a SFC
    in a 2.9 s for 10,000 simultaneous SFC requests and scale well for 2 million subscribers.
    14. Gharbaoui et al. [166] validated their orchestration system using FedFIRE+
    EU H2020 [167] experimentation platform that provides realistic setup for SDN
    & NFV deployment. 15. Huff et al. [86] proposed “holistic composer” approach for
    the composition of SFC having click-on-OSv based VNFs running on the Openstack
    Tacker NFV orchestrator. They showed that proposed framework is 4.145 times faster
    to compose 128 SFC concurrently rather than doing it sequentially. 16. Lee et
    al. [168] proposed auto-scaling framework that determines the scaling in/out of
    VNFs using deep Q-network (DQN) algorithm. They validated the proposed approach
    in an OpenStack based testbed. They showed that the proposed approach can minimizes
    SLO violation while adds or remove VNF instances. 17. Cheng et al. [169] proposed
    service function chain instantiation framework based on NFV and SDN known as “Matchmaker”.
    They evaluated their work in a testbed having NetFPGA opensflow switches, 2 servers
    running clickOS, and floodlight controller. 18. Tu et al. [170] presented PPTMon
    (Packet processing Time Monitoring) approach that provides per-hop (single VNF)
    and end-to-end monitoring (multiple VNFs in SFC). They used real OpenStack environment
    to evaluate PPTMon framework and results showed that PPTMon works well in real
    world SFCs (see Table 13). Table 13. SFC provisioning approaches classified based
    on Virtualization Environment as VIM. Ref. Deployment environment VIM VNFM & NFVO
    Experiment setup SDN controller Type of NF Language used Performance metrics Chou
    et al. [153] Virtualization – – Pica 8 OpenFlow switch, 2 service function servers
    using All-in-one hypervisor, OpenFlow Controller, client, and destination RYU
    Firewall, IDS, Web and Video Optimizer, DPI, NAT Python Traffic statistics Kim
    et al. [154] Virtualization OpenStack Playnet-MANO FNCP infrastructure consists
    of 3 physical nodes (Intel ® Xeon ® CPU E5-2697 v2 @ 2.70 GHz), ubuntu 14.04LST,
    3 service function instance on each node, virtual network using OpenStack, ODL
    controller Open Daylight – Java CPU utilization Medhat et al. [155] Virtualization
    OpenStack Fraunhofer FOKUS Open Baton toolkit 2 hosts: one including NFVI functionality
    running OpenStack (Liberty), ODL (Beryllium), OVS. Second host contains MANO functionality
    using Open Baton Open Daylight Beryllium Firewall, HTTP Header Enrichment (HHE),
    Parental Control (PC) Java on top of Spring framework Traffic load, packet loss
    rate Yang et al. [156] Virtualization OpenStack Tacker, Ceilometer, Heat VMware
    workstation 12 pro, OpenStack Liberty, processor: Intel Core TM i7 3770 CPU @
    3.4 GHz 3.4. RAM: 16 GB, Disk Space 500 GB, Tacker 0.2.1, OS: Ubuntu 14.04 LTS
    Open Daylight – Java, yaml – Akhtar et al. [157] Virtualization GENI testbed RINA
    framework Source, destination, 2 VNFs, OpenvSwitch, OpenFlow controller, Proportional
    Integral controller, RINA framework – IDS(Snort) – CPU load Sun et al. [158] Virtualization
    OpenStack – Chinese network topology with 55 nodes, each node having 2000 computing
    resources, each link bandwidth resources: 200 Mbps, end-to-end delay of each link:
    U(30,130), SFC length: U(4,6) – – – Bandwidth consumption, Load rate, delay Ziri
    et al. [159] Virtualization OpenStack – 3 Dell M610 blades in a Dell M1000 chassis,
    3 node OpenStack, OS: Redhat 7, OpenvSwitch, SDN controller, traffic generation
    tool: Ixia Open Daylight Firewall, DPI, Router, Policy traffic switch (PTS) Java
    – Yi et al. [160] Virtualization OpenStack – Simulation Deployment: BT Europe
    (24 nodes, 37 links), Interoute (110 nodes, 148 links), synthetic (100 nodes,
    570 links), GT-ITM, x64 PC with Intel Core i5, 8 GB RAM, OS: ubuntu 14.0.4, Realistic
    Deployment: topology consists of 4 VM instances: 2 hosts, 2 VNFs – Transcoder,
    Compressor C++, Python, JSON Execution Time, Acceptance ratio, Average Delay,
    Cost Shen et al. [161] Virtualization – – Intel Xeon E5-2630 v4 (10 cores @2.20
    GHz) server, BCube topology: 80 switch node, 32 server node, 20 VNF instance,
    link capacity 1Gbps, Barabasi–Albert network: 80 switch node, 30 server node,
    25 VNF instance, link capacity 1Gbps – – – Throughput, resource utilization, scaling
    frequency Soualah et al. [162] Virtualization OpenStack – 19 servers from Network
    and Cloud Federation, VNFs in each SFC request:3, CPU of each VNF: Firewall (4
    CPUs), NAT (1 CPUs), Proxy (2 CPUs), IDS (8 CPUs), total requests:40. Open Daylight
    Firewall, Proxy, NAT, IDS Java, Tosca Resource usage, Rejection rate, Power consumption,
    execution time Davoli et al. [163] Virtualization KVM SDN controller 1 physical
    server having 5 virtual machines through libvirt/KVM interconnecting using virtual
    networks, VM OS: ubuntu 14.04 LTS, SDN controller ONOS DPI, Traffic Shaper, Integrity
    Checker Java Throughput Heideker et al. [164] Virtualization XEN – Two Intel Xeon
    E5220 @ 2.27 GHz with 4 physical cores and 8 threads, 24 GB RAM, 2 Gb Ethernet
    interface, Pica8 P-3295 OpenFlow switch, XEN hypervisor for VMs with 1 vCPU and
    1 GB RAM – Firewall, NAT – Latency, Throughput, Service Rate Ozdem et al. [165]
    Virtualization OpenStack SDN Controller OpenStack virtualization framework running
    over leaf-spine architecture, 2 servers (1 CPU, Xeon D-1518, 4 cores 2*16 GB DDR4,
    240 GB SSD), 2 mobile phones, Traffic generator tool: Cisco TREX, SDN Controller
    ONOS Firewall, Traffic Optimization – SFC setup time, throughput Gharbaoui et
    al. [166] Virtualization QEMU-KVM Chain Optimizer, VIM and WIM orchestrator VirtualWall
    infrastructure consists of three SDN DC slices connected with SDN WAN slice (having
    5 physical nodes running OVS and ONOS) through VXLAN tunnel, SDN Controller, VIM
    Orchestrator, and WIM orchestrator ONOS – – Response time, Latency Huff et al.
    [86] Virtualization OpenStack, Click-on-oSv OpenStack Tacker Intel Core i7-6700HQ
    CPU with 4 cores up to 3.5 GHz, 12 GB DDR4 RAM, OS: Ubuntu 16.04.3, Apache HTTP
    server 2.4.18, MongoDB database – – Python, JSON, and TOSCA Time to compose SFC
    of different length Lee et al. [168] Virtualization OpenStack – Dell server operating
    ubuntu 18.04 consists of compute & controller node, AI node, and monitoring node.
    VNF instance operating ubuntu 16.04 and vCPU 1 core, RAM 1024 MB, disk 10 GB.
    InfluxDB, Apache Bench (AB), traffic generation tool: RUBiS – Firewall (iptables),
    Flow monitor (ntopng), DPI (nDPI), IDS (Suricata), and proxy (HAProxy) – Response
    time, Standard Deviation, Service Level Objective violation Cheng et al. [169]
    Virtualization Xen SDN Controller Mesh Network consists of 3 OpenFlow switches
    (NetFPGA), 2 servers running ClickOS virtualization, 1 server running Floodlight
    controller. Servers configuration: 3.4 GHz Intel Core i7 processor, 4 GB DDR4
    RAM FloodLight Firewall, DPI, NAT, IDS – Chain setup time, throughput Tu et al.
    [170] Virtualization OpenStack – One controller node, Dell R610server with 2 intel
    Xeon X5650,24 GB RAM for compute node, 3 VMs on compute node (1 client, 1 VNF,
    1 server), Open vSwitch for interconnection, VMs OS: ubuntu 19.10, tcpdump, Apache
    Bench – Firewall & NAT (iptables), DPI (nDPI), IPS (Suricata), LB (IPVS) – Average
    processing time, throughput 4.4.1. Discussion In Virtualization Environment as
    VIM (VE-VIM), our proposed taxonomy considers Virtualized Infrastructure Managers
    (VIMs) along with available Orchestration & Management (NFVO & VNFM) parameters
    to divide the Service Function Chain Provisioning solutions based on Virtualization
    Environment. We reviewed eighteen research articles that come under this category.
    After critically reviewing all the research articles, we analyzed virtual machines
    are still a reliable way for the development of virtual network functions and
    are far from obsolete. In ETSI NFV, OpenStack is a fundamental component of the
    Virtualized Infrastructure Managers (VIM) and has become a de-facto standard for
    NFV deployment. The OpenStack Tacker project deploys and operates VNFs in a virtual
    machine with the help of the NFV Orchestrator and VNF Manager. VMs securely store
    your application and have longer life-cycle than containers. Virtualization environment
    suitable for the following scenarios. • A monolithic architecture application.
    • It requires multiple applications to have different operating systems. • We
    need a separate security system and a platform with persistent storage. • We need
    an operating system with full functionality. • Telecom service providers such
    as China Mobile, AT & T, and NTT Group use OpenStack NFV because it offers: multi-tenancy
    management, scalability, resource sharing, a rich API for DevOps development,
    modular architecture, and resiliency to physical and virtual resources. 4.5. Containerization
    Environment as VIM (CE-VIM) The proposed taxonomy considers VIM support tools
    and VNFM & NFVO support tools parameter to divide the Service Function Chain Provisioning
    Solutions based on Containerization Environment as shown in Fig. 15. Table 14
    presents summary of some research articles that selected containerization Environment
    as VIM to implement NFV experiments. We reviewed number of research articles that
    comes under this category includes: 1. Livi et al. [171] developed service function
    chaining of container-based network functions to reduce resource cost due to sharing
    of kernel among number of processes. But they failed how to assign a part of chain
    to a single tenant and how we can add multiple distinct VNFs instead of homogeneous
    VNFs in a real-world scenario. 2. Dab et al. [172] used microservice based container
    architecture to solve a traffic steering problem in 5G network. They evaluate
    their proposed scheme in Kubernetes based platform including one master and two
    worker nodes. 3. Jeong et al. [173] developed machine learning approach to quickly
    find out the best path from source to a destination from multiple paths. They
    validated their algorithm using Containernet (fork of Mininet) that supports docker
    container. As a future work, we want to apply this method in a more realistic
    NFV environment such as OpenStack. 4. Santos et al. [174] implemented SFC controller
    as an extension to the default scheduling features in Kubernetes for container-based
    service function chaining, while minimizing network latency. They also highlighted
    that SFC controller is used to optimal place the service function chain in various
    smart cities use cases such as waste management, surveillance camera, etc. 5.
    Sekine et al. [175] used Docker and Kubernetes framework to develop an IoT-centric
    orchestration framework. They developed a real network consists of 5 desktop PCs
    to implement Kubernetes worker nodes and connected with a master node at 100 Mbps
    using ethernet cable. 6. Hong et al. [176] evaluated their container based SFC
    using Berkeley Extensible Software Switch (BESS), Click for the implementation
    of network functions, and DPDK for high performance I/O. They showed that click
    based containerized VNFs preserves high performance. 7. Pattaranantakul et al.
    [177] implemented “chainSign”, a self-checking mechanism that ensures that SFC
    to be implemented is secure. According to the ChainSign algorithm, every VNF in
    SFC has to sequentially sign the received packet, then the final VNF verifies
    the signature and validates it. Table 14. SFC provisioning approaches classified
    based on Containerization Environment as VIM . Ref. Deployment environment VIM
    VNFM & NFVO Experiment setup SDN controller Type of NF Language used Performance
    metrics Livi et al. [171] Containerization Docker Pair of virtual ethernet (veth)
    Dell PowerEdge R410 with two quad core Intel Xeon E5606 processors, 12 GB memory,
    OS: Debian Sid, kernel version: 4.4.0, iperf3, OpenvSwitch: 2.3.0, Docker: 1.10.2
    – Firewall (iptables), Traffic Shaping Python Throughput Dab et al. [172] Containerization
    Kubernetes (K8) Docker, Kind VMware ubuntu virtual machine 16.04.6 LTS, Kubernetes
    cluster including 1 master node, 2 worker node, Docker version: 19.03.4, Go language
    version: go1.12.10, kubectl API version: v1.16.0 – Firewall, VPN Gateway Go Deployment
    time, network latency Jeong et al. [173] Containerization Containernet (fork of
    Mininet to support docker container) Docker Ubuntu 16.04 server of 2.67 GHz Intel
    Xeon CPU, 24 GB RAM, SDN network using Containernet, SDN controller, OpenFlow
    version: 1.4, Topology generator ONOS – – Successful SFC requests Santos et al.
    [174] Containerization Kubernetes SFC Controller (extension of kubernetes scheduler)
    Kubernetes cluster on imec Virtual Wall infrastructure at IDLab, Belgium, Fog
    cloud infrastructure using Kubeadm, Kubeadm: v1.13.4, Kubectl: v1.13.4, Go: v1.11.5,
    Docker: 18.09.2, Linux kernel: 4.4.0-34-generic, OS: ubuntu 16.04.1 LTS SFC controller
    (extension of default Kubernetes scheduler) – Go Execution time, network latency
    Sekine et al. [175] Containerization Kubernetes SFC controller 5 desktop PCs for
    Kubernetes worker nodes, Pub/Sub model (Apache Kafka) and k8 master node, worker
    node connected to a Kafka broker at 100 Mbps, 6 service functions – Face detection
    functions, image crop function, image mosaic function – End-to-end-delay Hong
    et al. [176] Containerization Docker 17.03-1-ce EtherMirror App Two servers having
    specification Intel 12 core Xeon E5-2670 2.30 GHz CPU and 64 GB & 32 GB DDR4 memory
    connected through Mellanox SX1036 switch, OS: ubuntu 16.04.2, linux kernel: 4.4.0,
    Docker 17.03.1-ce – – – Throughput, End-to-end latency Pattaranantakul et al.
    [177] Containerization Docker – Physical node (Intel core i7, 2 CPU cores, 4 GB
    RAM), VirtualBox virtualization and Vagrant tool, network nodes using docker containers,
    10 SFFs, 7 VNF instances Open-Daylight – Python CPU utilization, end-to-end latency,
    responsiveness 4.5.1. Discussion In Containerization Environment as VIM (CE-VIM),
    our proposed taxonomy considers Virtualized Infrastructure Managers (VIMs) along
    with available Orchestration & Management (NFVO & VNFM) parameters to divide the
    Service Function Chain Provisioning solutions based on Containerization Environment.
    After reviewing seven research articles related to this category, we noted that
    the container technology [178] is gaining a lot of attention from industry due
    to its scalability, resource optimization, and agile software development. The
    Docker toolkit provides simple commands for deploying, building, executing, upgrading,
    and stopping containers. Docker technology, on the other hand, is insufficient
    for managing containerized applications with thousands of Docker containers. To
    address this challenge, a container orchestration tool such as Kubernetes, Docker
    Swarm, etc. is used by most companies to orchestrate and manage the life-cycle
    of containers. Containerization environment would be suitable for the following
    scenarios to deploy virtual network functions in a container. • Application have
    a multi-service architecture. • To minimize the number of servers. • To build
    a cloud-native application. • To run multiple instances of a single application.
    Containers are a good choice when you have just one operating system. • There
    is no need for security since containers lack security due to the same OS kernel
    sharing. • No need for fault resilience because isolating faults is a tedious
    task as they replicate in all containers. • Network requirements such as network
    isolation and fixed CN IP are not required. 4.6. Hybrid Environment as VIM (HE-VIM)
    The proposed taxonomy considers VIM support and Architecture parameter to divide
    the Service Function Chain Provisioning Solutions based on Hybrid Environment
    as shown in Fig. 15. Table 15 presents summary of some research articles that
    selected Hybrid Environment as VIM to implement NFV experiments. We reviewed number
    of research articles that comes under this category includes: 1. Hadi et al. [128]
    implemented Pishahang framework consisting of hybrid VM/CN based VNFs on OpenStack
    and Kubernetes (K8) infrastructure. The dynamic allocation of IP address to container
    by K8 makes it difficult to provide service chaining. 2. Callegati et al. [179]
    highlighted some practical problems while providing dynamic service function chaining
    of VNFs implemented using OpenStack Cloud Platform. They implemented Business
    User (BU), Residential User (RU), DPI, WAN Accelerator (WANA) as VMs running inside
    OpenStack, and virtual router (VR) running as container-based VM. 3. Ledjiar et
    al. [180] implemented Network Function Virtualization as a service (NFVaaS) framework
    that allows orchestration of VNFs in multi-tenant SDN network with the help of
    Open-Source technologies such as ClickOS and OpenVirteX hypervisor. 4. Borylo
    et al. [181] implemented standalone and reliable monitoring tool that is separated
    from the control plane to improve scalability. They performed a series of experiments
    to validate the proposed mechanism on their internal testbed and public cloud
    infrastructure. They showed that proposed mechanism offers significant advantages
    as compared to the existing solutions that are integrated with control plane.
    Table 15. SFC provisioning approaches classified based on Hybrid Environment as
    VIM . Ref. Deployment environment VIM VNFM & NFVO Experiment detup SDN controller
    Type of NF Language used Performance metrics Hadi et al. [128] Containerization
    & virtualization OpenStack, Kubernetes (k8) Pishahang, SONATA Source, CN based
    forwarder, VM based forwarder, destination – Forwarder – ICMP ping traffic Callegati
    et al. [179] Containerization & virtualization OpenStack Service Chaining Orchestrator
    Multi-tenant slice consists of source, destination, 7 VNFs, 2 virtual routers,
    2 Openflow switches POX BU, RU, DPI, WANA, Traffic Conditioner (TC), Virtual Router
    (VR) – Throughput Ledjiar et al. [180] Emulation & Virtualization Mininet, Xen
    NFVaaS Layer Physical host (quad core 2.60 GHz CPU Intel Core i5-4300M, 8 GB RAM).
    VirtualBOX having 1 VM (twp 2.60 GHz CPUs. 4 GB RAM, 16 GB Disk, OS:ubuntu 12.04.4.LTS)
    having Mininet 2.2.1 for SDN network, SDN Controller, OpenVirtX network hypervisor,
    clickOS virtual machine Flood-Light Firewall, IDS – Migration time, throughput,
    maximum no of active VNFs Borylo et al. [181] Containerization & Virtualization
    Docker, OpenStack (KVM) NFVaaS Layer Testbed based on private cloud: three VMS
    deployed on AWS cloud, A controller node to run Openstack controller, two compute
    node where VNFs are running. Testbed based on container technology: Four physical
    machine (4 CPU intel i5-4460 up to 3.2 GHz, 8 GB RAM), first machine runs Ryu
    controller and source, second and third machine run docker, fourth machine acts
    as destination. Ryu – Python Service provisioning time, response time, accuracy
    4.6.1. Discussion In Hybrid Environment as VIM (HE-VIM), our proposed taxonomy
    considers Infrastructure Vendor and Architecture parameters to divide the Service
    Function Chain Provisioning solutions based on Hybrid Environment. We reviewed
    four articles that used a hybrid environment for service function chain provisioning.
    We observed three ways to deploy VNFs for service providers: virtual machine-based
    VNF, container-based VNF, a mix of VM and containers. At this point, deploying
    VNFs in virtual machines is a good choice. However, many challenges restrict the
    use of container-based VNFs because containers are still a growing technology
    and in a nascent stage. Hence, VNFs deployed in the mixture of VNFs and containers
    can improve scalability and automation. The deployment of VNFs in a hybrid environment
    is an open research challenge where some VNFs may be deployed in virtual machines
    and some in containers. The central controller at the MANO layer is required to
    manage and orchestrate both VMs (e.g OpenStack) and containers (e.g Kubernetes)
    of two different virtualized infrastructure managers (VIMs). 5. Application of
    SDN/NFV in future state-of-the-art applications Cloud service providers (CSPs)
    use different networking devices to provide services to their customers. Network
    Function Virtualization helps service providers to provide services more efficiently
    using a virtual version of hardware equipment. In NFV, the virtual version of
    any network function is called Virtual Network Function (VNF), which runs on top
    of the COT server in the form of virtual machines. NFV offers benefits such as
    reduced hardware and maintenance cost, easy network update, reduced space to install
    a new device, and reduced power consumption. SDN is the best solution to efficiently
    control VNFs from the central place with the help of the most popular OpenFlow
    API, which separates the data plane from the control plane. Therefore, combining
    these novel networking technologies makes the network more robust in security,
    power consumption, and performance optimization. The following section describes
    how SDN and NFV apply in 5G, Metaverse, IoT Digital Twins (DT) network, Customer
    Premises Equipment (CPE), Evolved Packet Core (EPC), and Blockchain technologies
    to make them more efficient. 5.1. 5G network slicing and metaverse NFV is a core
    element of the 5G network, enabling it to virtualize all of the network’s hardware
    devices. NFV also provides cloud-based systems with an agile network architecture
    that enables them to develop flexible and programmable networks that satisfy the
    requirements of an application, client, or operator [182]. A key component of
    5G networks is network slicing since 5G has many new use cases and services. Comparing
    the network slicing implementation with the traditional one is shown in Fig. 16(a)
    and (b). Network slicing is a network architectural feature that supports creating
    multiple virtual networks on shared physical equipment. By dividing a physical
    network into various virtual networks, 5G virtualization can support different
    radio access networks (RANs) or different types of services for different segments
    of customers. The control plane and user plane of the network slices are entirely
    separate, so the user is presented with the same experience as if the network
    were physically separated. Mobile broadband with high throughput and low latency
    is a significant use case for network slicing. 5G network slicing offers benefits
    such as greater bandwidth, mobility, security, and availability [183]. With Software-Defined
    Networking (SDN) and Network Function Virtualization (NFV) in 5G networks, we
    can programmatically manage and control network resources [184]. Metaverse enabled
    by 5G, requiring high data throughput, reduced latency, higher system capacity,
    and multiple device connections that only 5G can offer [185]. Metaverse is a persistent,
    digital world made up of large amounts of data and activity that remains anchored
    where you put it even when you are not there. As a result, before engaging in
    an experience, we must first download the majority of it locally. To connect to
    the Metaverse directly, you will need to expect 100 s of Mbps, which only 5G can
    deliver [186]. Download : Download high-res image (786KB) Download : Download
    full-size image Fig. 16. (a) Traditional network (b) Network Slicing implementation.
    5.2. Virtual Customer Premises Equipment (CPE) We are taking an example of CPE
    in Fig. 17 to illustrate the cost reduction with NFV implementation. Traditionally,
    a single CPE consists of Network Address Translation (NAT), Firewall, Intrusion
    Detection System (IDS), Intrusion Prevention System (IPS), Routing, and Dynamic
    Host Configuration Protocol (DHCP) [187]. These functions reside in the physical
    device and are individually located at each customer site. In this implementation,
    separate technicians require each customer to add, remove, and update a function.
    In case of additions, a complete change of the device is required. Therefore,
    it is expensive for both the customers and the ISPs. Fig. 18 illustrates the NFV
    implementation in which some functions of the traditional CPE model are transferred
    from the customer site to the service provider site. Now, it is easier to make
    changes to any function for all customers since that would require changes at
    the service provider site only. In addition to it, adding a new function, say
    Deep Packet inspection (DPI) for all or subset of customers, can be done quickly
    at the service provider site. The deployment of a large-scale network offers cheaper
    CPE by saving its operational cost [188]. vCPE is the most critical driver of
    NFV deployment because of the benefits such as simplified operations, reduced
    CAPEX, and speed of service delivery. Download : Download high-res image (302KB)
    Download : Download full-size image Fig. 17. Traditional CPE network. Download
    : Download high-res image (350KB) Download : Download full-size image Fig. 18.
    NFV based CPE network. 5.3. IoT based Digital Twin Network (DTN) A significant
    benefit of IoT technology is connecting millions of devices. The estimated 125
    billion connected objects in the world by 2030 reflect the rapid growth of the
    Internet of Things (IoT) in various fields [5], [189]. These devices require a
    network infrastructure that provides adequate connectivity and delivers applications.
    It requires increased investment in network infrastructure to meet growing needs.
    To mitigate these issues, service providers use the concept of infrastructure
    sharing. Due to this, virtualization is being used to provide network sharing,
    handle big data explosions from connected devices, and simplify management tasks
    based on network virtualization. A combination of SDN and NFV, two complementary
    architectures, is emerging to address several networking challenges comprehensively.
    As a result, the SDN/NFV structure for IoT enables secure connections and easier
    access to IoT platforms. The network function virtualization (NFV) provides the
    scale and flexibility needed for IoT services by automating the control, management,
    and orchestration of network resources. Digital Twins and IoT are changing how
    we interact with the virtual and physical worlds. The virtual representation of
    its physical counterpart is called the digital twin and provides the connection
    and access to intelligence in the physical world through IoT. To send information,
    the physical and virtual worlds connect using an SDN controller [190]. Both We
    can SDN and NFV paradigms can combine to provide a flexible network. A controller
    is responsible for creating the topology model and providing the required intelligence
    to the Digital Twin Network [191]. 5.4. Blockchain based IoT applications with
    SDN/NFV implementations Blockchain and IoT have emerged as critical technologies
    of the digital transformation revolution and apply in a wide range of applications
    [192]. IoT technology indeed has many applications in various sectors, but security
    and protection issues hinder the development of its applications. Therefore, when
    combined with blockchain technology, its applications can be developed significantly.
    Internet of Things networks connect millions of devices, and at the same time,
    availability and performance should optimize. Firstly, SDN has used a centralized
    controller to handle the whole network. SDN networks can easily and quickly be
    controlled from a single location, and as a result, third parties can attack the
    network. However, distributed SDN controllers have improved network performance
    in recent years. Using multiple controllers allows balancing device and controller
    load, minimizing packet loss, and minimizing network traffic. Then, SDN-based
    IoT applications combined with another leading blockchain technology to enhance
    security and privacy. Network Function Virtualization (NFV) is also a recent technology
    [193]. It optimizes network scalability, saves energy, and distributes load among
    users. 5.5. Virtual Evolved Packet Core (vEPC) vEPC is another example of NFV
    gaining much attention from industry and Telecommunication Service providers (TSPs).
    The EPC is a core network for LTE that consists of PGW (Packet Data Gateway),
    SGW (Serving Gateway), MME (Mobility Management Entity), and HSS (Home Subscriber
    Server), as shown in Fig. 19. Different proprietary devices must implement these
    network functions in the traditional EPC model. Therefore, to change the functionality
    and capacity of a given network function requires a replacement of that network
    function. The right side of Fig. 19 shows virtualized EPC. In vEPC, all or a subset
    of network functions virtualize on shared cloud infrastructure [194]. The TSPs
    can handle changing market conditions easily and quickly due to their flexibility
    and dynamic scaling features. For example, we can increase (scale) user plan resources
    or VNFs such as virtual MME independent of the control plane. Therefore, an easy
    software upgrade on the vEPC network functions allows us for faster launch innovative
    services. Download : Download high-res image (205KB) Download : Download full-size
    image Fig. 19. vEPC model. 6. Open research challenges 6.1. Orchestrator scalability
    According to a report by Statistica [195], the total number of connected devices
    around the world will be more than 75.44 billion in 2025. This growth has been
    influenced by the emergence of vertical industries such as the Internet of Things,
    Smart Cities, and Sensor Networks. To support a large number of connected nodes
    in this scenario, the orchestration mechanism must be able to accommodate the
    growth of networks and services. Furthermore, network services can be deployed
    across a wide range of third-party-managed domains, infrastructure that spans
    a large geographic area, and a variety of resources such as access, transport,
    and core networks. The components involved, such as orchestrators, controllers,
    and managers, must be scalable in this environment. The majority of current Orchestrator
    use cases revolve around deploying a network service in a controlled environment.
    The orchestrator in a production environment is in charge of orchestrating millions
    of customers and services at the same time. As a result, scalability is a critical
    feature for NFV MANO’s success. The centralized approach used by some orchestration
    solutions can be problematic in terms of scalability. The orchestrators mentioned
    by the authors [196], [197] imply that the orchestration process should not be
    limited to a single orchestrator to process end-to-end network services. As a
    result, developing a massively scalable orchestration process is a major challenge.
    One or more orchestrators may be involved in this process, which will become open
    and flexible enough to address future applications. 6.2. Security and privacy
    Software-based networks replace hardware-based provider components with software-based
    solutions, leading to changes in how services are delivered [198]. Such a network
    can enable automation, programmability, and flexibility owing to technologies
    like SDN and NFV. In general, it is controlled by centralized control, which poses
    security and resiliency vulnerabilities [199]. As a result, additional protection
    capabilities, such as enhanced management capabilities like authentication, access
    control, and fault management, must be implemented. Typically, the services are
    deployed first, followed by any security development activities. However, in a
    highly linked and virtualized world, security must be a top priority. Service
    instantiation refers to automated processes that add and delete network elements
    and functions without requiring human intervention. The inclusion of a malicious
    node, which can launch attacks, capture vital information, and even cause the
    entire service to be disrupted, is a serious problem. The ability to hide individual
    characteristics of each domain is a critical requirement for a multi-domain orchestration
    platform. Therefore, it protects the domains’ privacy and confidentiality while
    also preserving the capabilities and resources of a third-party component [200].
    Resilience in essential components like orchestrators, controllers, and managers
    is also a major issue since it has a direct influence on overall service operations.
    Furthermore, open interfaces that provide network programmability and component
    communication with other external parts like OSS and other orchestrators are a
    hot topic in research [199], [201], [202]. 6.3. Multi-domain orchestration One
    domain is a complete area of functionality that one service from one service provider
    offers. Extending the idea of the domain to multi-domain is that many domains
    that offer specific functionality are taken and composed together to deliver their
    functionality as a whole. In terms of multi-domain orchestration, there are many
    issues to consider as we move closer to true 5G system integration. There are
    no defined standard interfaces for inter-and intra-orchestrator communication.
    There is no provisioning for this type of design in the MANO architecture. Furthermore,
    the main focus of ETSI ISG NFV is on the data center, therefore there are numerous
    challenges and unanswered questions regarding how to implement the proposed design
    in the wireless domain. The authors in [137] list a number of open questions and
    challenges for NFV orchestration, including (a) resource management, (b) placement
    of network functions, (c) dynamic scaling, (d) automation, and (e) self-allocation.
    Other aspects of the problem include how distributed management can be implemented
    and communication overhead and delay issues. Security considerations, as well
    as challenges related to interoperability between different vendors, are also
    important to consider. In some ways, the feasibility of multi-domain orchestration
    is subjective and hazy, because different providers and vendors have completely
    different incentives and business models. The paper [203] demonstrated a multi-domain
    NS deployment based on OSM, OpenStack, and OpenDayLight denoted as Open . They
    developed Open ++, which extends Open networking functionality by allowing the
    interconnection of VNFs belonging to the same NS within a single domain or spread
    across multiple domains. 6.4. Resource and service management Physical resources
    should be utilized efficiently to deliver the economies of scale that NFV promises.
    This requires efficient methods for determining which physical resources (servers)
    network services should be placed on, as well as the ability to shift functions
    from one server to another for purposes like load balancing, energy efficiency,
    and failure recovery. Because the task of locating functions is closely related
    to virtual network embedding [204], it can be expressed as an optimization problem
    with a and has been followed by [205], [206], [207]. The responsibility of the
    network service is to efficiently organize the resource requirements, configuration
    settings, administration policies, and performance indicators. The service model
    will enable underlying layers’ resources and capabilities to be abstracted. It
    makes functions easier to understand and gives a generic approach to representing
    resources and services. However, translating higher-level rules, which are generated
    by resource allocation and optimization algorithms, into a lower-level configuration
    is a huge difficulty. To provide automated and consistent translation, templates
    and standards should be constructed [14]. Furthermore, standardization can facilitate
    interoperability and integration of network service templates, as well as address
    limits in service deployment in a heterogeneous environment. TOSCA [48], [208],
    YANG, and HOT are examples of templates and data modeling languages for NFV and
    Network Service (NS). Furthermore, several groups, such as Open Baton and Gohan,
    propose their own approaches to the notion of Network Services. Further work needs
    to be done on resource and service modeling in softwarized networks, including
    multi-domain scenarios. This innovation will enable network service interoperability
    and also effective mapping between high-level configuration and underlying infrastructure.
    Currently, there is no interoperability between the different orchestration platforms.
    6.5. Advanced intelligence in NFV-MANO NFV MANO has been the subject of a lot
    of research in recent years. Initially, this research concentrated on complex
    optimization problems and near-optimal heuristic solutions. However, NSPs must
    include an increasing level of operational efficiency into their NFV-enabled networks
    as network complexity rises and futuristic networks take shape. Machine Learning
    (ML) is one such technology that has been used by numerous entities in NFV-enabled
    networks, most notably the NFV Orchestrator. Traditional machine learning (ML)
    offers enormous operational efficiencies, such as real-time and high-volume data
    processing [209], but issues like privacy, security, scalability, transferability
    make it difficult to deploy. NSPs can harness the benefits of standard machine
    learning while also solving the major issues associated with it by implementing
    Advanced Intelligence techniques such as Reinforcement Learning and Federated
    Learning. These two Advanced Intelligence techniques, however, should not be considered
    separately. The use of Federated Reinforcement Learning (FRL) and the coupling
    of RL and FL is a critical enabler for this approach and the level of intelligence
    necessary for future networks. FRL has already been implemented in some projects
    [210], but its application to NFV Orchestration is still a work in progress. Moving
    forward, the bottom-up micro-functionality approach in combination with FL, RL,
    and FRL presents significant contributions for valuable contributions. 7. Conclusion
    and future directions In the era of 5G and beyond, the NFV-MANO framework provides
    agile, flexible orchestration and management of Virtual Network Functions (VNFs).
    This paper provides a comprehensive overview of Virtualized Infrastructure Managers
    (VIMs) with its strengths for implementing Service Function Chains (SFCs) in NFV
    architecture. We also discussed various technical aspects related to the VIMs
    i.e. architecture, VNF Manager and NFV Orchestrator component, OASIS TOSCA templates.
    Moreover, we critically reviewed the prominent research articles that indicated
    VIM is a critical element of the MANO system for NFV experiments. We also proposed
    a taxonomy to select an appropriate VIM based on deployment environments such
    as Emulation, Virtualization, Containerization, and Hybrid environment as VIM
    to implement a reliable service function chain. After an in-depth analysis of
    research articles that selected “Emulation Environment as VIM” such as Mininet,
    we have observed that most of the researchers failed to test NF chain having multiple
    network functions of different types. As far as auto-scaling and monitoring of
    VNF are concerned, the proposed system fails to support it. Many researchers used
    “Virtualization Environment as VIM” for their NFV experiments. We analyzed that
    virtual machines are the de facto standard in Telco NFV deployment. After examining
    the studies that use “Containerization Environment as VIM” such as Kubernetes,
    we concluded that it offers significant advantages as compared to VM in terms
    of efficiency. But the deployment of VNFs in container-only platforms is not advisable
    as containers are still a growing technology and in an immature stage as compared
    to VMs. Security risks are also involved with containers due to sharing of OS
    kernel. Moreover, isolating fault is also cumbersome task with containers. In
    the end, we concluded that both containers and VMs have their own strengths and
    weaknesses and the choice is dependent upon application needs. VM is a better
    choice when you want to run multiple applications on a server with a wide variety
    of dedicated OS to manage them. Containers are the best choice when you want to
    run multiple instances of the same application on minimum number of servers. As
    for future research directions, deploying VNFs in a Hybrid environment is an open
    research challenge in which some VNF components are deployed in virtual machines
    and some in containers. The central controller at the MANO layer is required to
    manage and orchestrate both VMs (OpenStack for VM) and containers (Kubernetes
    for a container) of two different virtualized infrastructure managers (VIMs).
    Declaration of Competing Interest The authors declare that they have no known
    competing financial interests or personal relationships that could have appeared
    to influence the work reported in this paper. References [1] Matias Jon, Garay
    Jokin, Toledo Nerea, Unzilla Juanjo, Jacob Eduardo Toward an SDN-enabled NFV architecture
    IEEE Commun. Mag., 53 (4) (2015), pp. 187-193 View in ScopusGoogle Scholar [2]
    Zhang Tianzhu, Linguaglossa Leonardo, Giaccone Paolo, Iannone Luigi, Roberts James
    Performance benchmarking of state-of-the-art software switches for NFV Comput.
    Netw., 188 (2021), Article 107861 View PDFView articleView in ScopusGoogle Scholar
    [3] Mirjalily Ghasem, Luo Zhiquan Optimal network function virtualization and
    service function chaining: A survey Chin. J. Electron., 27 (4) (2018), pp. 704-717
    CrossRefView in ScopusGoogle Scholar [4] Zhang Tianzhu, Qiu Han, Linguaglossa
    Leonardo, Cerroni Walter, Giaccone Paolo NFV platforms: Taxonomy, design choices
    and future challenges IEEE Trans. Netw. Serv. Manag., 18 (1) (2020), pp. 30-48
    CrossRefGoogle Scholar [5] Ray Partha Pratim, Kumar Neeraj SDN/NFV architectures
    for edge-cloud oriented IoT: A systematic review Comput. Commun., 169 (2021),
    pp. 129-153 View PDFView articleView in ScopusGoogle Scholar [6] Kaur Karamjeet,
    Mangat Veenu, Kumar Krishan A comprehensive survey of service function chain provisioning
    approaches in SDN and NFV architecture Comp. Sci. Rev., 38 (2020), Article 100298
    View PDFView articleView in ScopusGoogle Scholar [7] Barakabitze Alcardo Alex,
    Ahmad Arslan, Mijumbi Rashid, Hines Andrew 5G network slicing using SDN and NFV:
    A survey of taxonomy, architectures and future challenges Comput. Netw., 167 (2020),
    Article 106984 View PDFView articleView in ScopusGoogle Scholar [8] Fei Xincai,
    Liu Fangming, Zhang Qixia, Jin Hai, Hu Hongxin Paving the way for NFV acceleration:
    A taxonomy, survey and future directions ACM Comput. Surv., 53 (4) (2020), pp.
    1-42 CrossRefView in ScopusGoogle Scholar [9] Desai Ankita, Oza Rachana, Sharma
    Pratik, Patel Bhautik Hypervisor: A survey on concepts and taxonomy Int. J. Innov.
    Technol. Explor. Eng., 2 (3) (2013), pp. 222-225 CrossRefGoogle Scholar [10] Zhang
    Yuxia, Zhou Minghui, Stol Klaas-Jan, Wu Jianyu, Jin Zhi How do companies collaborate
    in open source ecosystems? an empirical study of openstack IEEE/ACM 42nd International
    Conference on Software Engineering (ICSE), IEEE (2020), pp. 1196-1208 Google Scholar
    [11] Sridharan Srinivasan A literature review of network function virtualization
    (NFV) in 5G networks Int. J. Comput. Trends Technol., 68 (2020), pp. 49-55 CrossRefGoogle
    Scholar [12] Ahmed M. Alwakeel, Abdulrahman K. Alnaim, Eduardo B. Fernandez, A
    pattern for NFV management and orchestration (MANO), in: Proceedings of the 8th
    Asian Conference on Pattern Languages of Programs, 2019. Google Scholar [13] Mamushiane
    Lusani, Lysko Albert A., Mukute Tariro, Mwangama Joyce, Du Toit Zaaid Overview
    of 9 open-source resource orchestrating ETSI MANO compliant implementations: A
    brief survey 2019 IEEE 2nd Wireless Africa Conference (WAC), IEEE (2019), pp.
    1-7 CrossRefGoogle Scholar [14] Fulber-Garcia Vinicius, Duarte Elias P. Jr., Huff
    Alexandre, dos Santos Carlos R.P. Network service topology: Formalization, taxonomy
    and the CUSTOM specification model Comput. Netw., 178 (2020), Article 107337 View
    PDFView articleView in ScopusGoogle Scholar [15] el houda Nouar Nour, Yangui Sami,
    Faci Noura, Drira Khalil, Tazi Saïd A semantic virtualized network functions description
    and discovery model Comput. Netw., 195 (2021), Article 108152 Google Scholar [16]
    Lu Jie, Zhang Zhen, Hu Tao, Yi Peng, Lan Julong A survey of controller placement
    problem in software-defined networking IEEE Access, 7 (2019), pp. 24290-24307
    CrossRefView in ScopusGoogle Scholar [17] Saraswat Surbhi, Agarwal Vishal, Gupta
    Hari Prabhat, Mishra Rahul, Gupta Ashish, Dutta Tanima Challenges and solutions
    in software defined networking: A survey J. Netw. Comput. Appl., 141 (2019), pp.
    23-58 View PDFView articleView in ScopusGoogle Scholar [18] Chica Juan Camilo
    Correa, Imbachi Jenny Cuatindioy, Vega Juan Felipe Botero Security in SDN: A comprehensive
    survey J. Netw. Comput. Appl., 159 (2020), Article 102595 Google Scholar [19]
    Ono Daichi, Guillen Luis, Izumi Satoru, Abe Toru, Suganuma Takuo A proposal of
    port scan detection method based on packet-in messages in OpenFlow networks and
    its evaluation Int. J. Netw. Manage. (2021), Article e2174 View in ScopusGoogle
    Scholar [20] Herrera Juliver Gil, Botero Juan Felipe Resource allocation in NFV:
    A comprehensive survey IEEE Trans. Netw. Serv. Manag., 13 (3) (2016), pp. 518-532
    Google Scholar [21] Pattaranantakul Montida, He Ruan, Song Qipeng, Zhang Zonghua,
    Meddahi Ahmed NFV security survey: From use case driven threat analysis to state-of-the-art
    countermeasures IEEE Commun. Surv. Tutor., 20 (4) (2018), pp. 3330-3368 CrossRefView
    in ScopusGoogle Scholar [22] Medhat Ahmed M., Carella Giuseppe, Lück Christian,
    Corici Marius-Iulian, Magedanz Thomas Near optimal service function path instantiation
    in a multi-datacenter environment 2015 11th International Conference on Network
    and Service Management (CNSM), IEEE (2015), pp. 336-341 View in ScopusGoogle Scholar
    [23] Alam Iqbal, Sharif Kashif, Li Fan, Latif Zohaib, Karim Md. Monjurul, Biswas
    Sujit, Nour Boubakr, Wang Yu A survey of network virtualization techniques for
    Internet of Things using SDN and NFV ACM Comput. Surv., 53 (2) (2020), pp. 1-40
    Google Scholar [24] Nguyen Van-Giang, Brunstrom Anna, Grinnemo Karl-Johan, Taheri
    Javid SDN/NFV-based mobile packet core network architectures: A survey IEEE Commun.
    Surv. Tutor., 19 (3) (2017), pp. 1567-1602 View in ScopusGoogle Scholar [25] Farris
    Ivan, Taleb Tarik, Khettab Yacine, Song Jaeseung A survey on emerging SDN and
    NFV security mechanisms for IoT systems IEEE Commun. Surv. Tutor., 21 (1) (2018),
    pp. 812-837 Google Scholar [26] Bonfim Michel S., Dias Kelvin L., Fernandes Stenio
    F.L. Integrated NFV/SDN architectures: A systematic literature review ACM Comput.
    Surv., 51 (6) (2019), pp. 1-39 CrossRefGoogle Scholar [27] Hong Peilin, Xue Kaiping,
    Li Defang, et al. Resource aware routing for service function chains in SDN and
    NFV-enabled network IEEE Trans. Serv. Comput. (2018), pp. 1-13 CrossRefGoogle
    Scholar [28] Bhamare Deval, Jain Raj, Samaka Mohammed, Erbad Aiman A survey on
    service function chaining J. Netw. Comput. Appl., 75 (2016), pp. 138-155 View
    PDFView articleView in ScopusGoogle Scholar [29] Thai Minh-Tuan, Lin Ying-Dar,
    Lai Yuan-Cheng A joint network and server load balancing algorithm for chaining
    virtualized network functions 2016 IEEE International Conference on Communications
    (ICC), IEEE (2016), pp. 1-6 Google Scholar [30] Hong Dongjin, Kim Jinyong, Hyun
    Daeyoung, Jeong Jaehoon Paul A monitoring-based load balancing scheme for network
    security functions 2017 International Conference on Information and Communication
    Technology Convergence (ICTC), IEEE (2017), pp. 668-672 CrossRefView in ScopusGoogle
    Scholar [31] Zamani Ali, Bakhshi Bahador, Sharifian Saeed An efficient load balancing
    approach for service function chain mapping Comput. Electr. Eng., 90 (2021), Article
    106890 View PDFView articleView in ScopusGoogle Scholar [32] Lee Giwon, Kim Myeongsu,
    Choo Sukjin, Pack Sangheon, Kim Younghwa Optimal flow distribution in service
    function chaining The 10th International Conference on Future Internet (2015),
    pp. 17-20 CrossRefView in ScopusGoogle Scholar [33] Lin Po-Ching, Lin Ying-Dar,
    Wu Cheng-Ying, Lai Yuan-Cheng, Kao Yi-Chih Balanced service chaining in software-defined
    networks with network function virtualization Computer, 49 (11) (2016), pp. 68-76
    View in ScopusGoogle Scholar [34] Gu Yunjie, Hu Yuxiang, Ding Yuehang, Lu Jie,
    Xie Jichao Elastic virtual network function orchestration policy based on workload
    prediction IEEE Access, 7 (2019), pp. 96868-96878 CrossRefView in ScopusGoogle
    Scholar [35] Thai Minh-Tuan, Lin Ying-Dar, Lin Po-Ching, Lai Yuan-Cheng Hash-based
    load balanced traffic steering on softswitches for chaining virtualized network
    functions 2017 IEEE International Conference on Communications (ICC), IEEE (2017),
    pp. 1-6 CrossRefGoogle Scholar [36] Cao Haotong, Zhu Hongbo, Yang Longxiang Dynamic
    embedding and scheduling of service function chains for future SDN/NFV-enabled
    networks IEEE Access, 7 (2019), pp. 39721-39730 CrossRefView in ScopusGoogle Scholar
    [37] Soualah Oussama, Mechtri Marouen, Ghribi Chaima, Zeghlache Djamal Online
    and batch algorithms for VNFs placement and chaining Comput. Netw., 158 (2019),
    pp. 98-113 View PDFView articleView in ScopusGoogle Scholar [38] Kim Hee-Gon,
    Park Suhyun, Lange Stanislav, Lee Doyoung, Heo Dongnyeong, Choi Heeyoul, Yoo Jae-Hyoung,
    Hong James Won-Ki Graph neural network-based virtual network function deployment
    optimization Int. J. Netw. Manage. (2021), Article e2164 View in ScopusGoogle
    Scholar [39] Bouet Mathieu, Leguay Jérémie, Combe Théo, Conan Vania Cost-based
    placement of vDPI functions in NFV infrastructures Int. J. Netw. Manage., 25 (6)
    (2015), pp. 490-506 CrossRefView in ScopusGoogle Scholar [40] Medhat Ahmed M.,
    Carella Giuseppe A., Pauls Michael, Monachesi Marcello, Corici Marius, Magedanz
    Thomas Resilient orchestration of Service Functions Chains in a NFV environment
    2016 IEEE Conference on Network Function Virtualization and Software Defined Networks
    (NFV-SDN), IEEE (2016), pp. 7-12 View in ScopusGoogle Scholar [41] Li Defang,
    Hong Peilin, Xue Kaiping, Pei Jianing Availability aware VNF deployment in datacenter
    through shared redundancy and multi-tenancy IEEE Trans. Netw. Serv. Manag., 16
    (4) (2019), pp. 1651-1664 CrossRefView in ScopusGoogle Scholar [42] Zhang Shilei,
    Wang Ying, Li Wenjing, Qiu Xuesong Service failure diagnosis in service function
    chain 2017 19th Asia-Pacific Network Operations and Management Symposium (APNOMS),
    IEEE (2017), pp. 70-75 CrossRefView in ScopusGoogle Scholar [43] Alenezi Mamdouh,
    Almustafa Khaled, Meerja Khalim Amjad Cloud based SDN and NFV architectures for
    IoT infrastructure Egypt. Inform. J., 20 (1) (2019), pp. 1-10 View PDFView articleView
    in ScopusGoogle Scholar [44] Kim Siri, Han Yunjung, Park Sungyong An energy-aware
    service function chaining and reconfiguration algorithm in NFV 2016 IEEE 1st International
    Workshops on Foundations and Applications of Self* Systems (FAS* W), IEEE (2016),
    pp. 54-59 Google Scholar [45] Sun Gang, Li Yayu, Yu Hongfang, Vasilakos Athanasios
    V., Du Xiaojiang, Guizani Mohsen Energy-efficient and traffic-aware service function
    chaining orchestration in multi-domain networks Future Gener. Comput. Syst., 91
    (2019), pp. 347-360 View PDFView articleView in ScopusGoogle Scholar [46] Peuster
    Manuel, Karl Holger, Van Rossem Steven MeDICINE: Rapid prototyping of production-ready
    network services in multi-PoP environments 2016 IEEE Conference on Network Function
    Virtualization and Software Defined Networks (NFV-SDN), IEEE (2016), pp. 148-153
    CrossRefView in ScopusGoogle Scholar [47] Tavares Thales Nicolai, da Cruz Marcuzzo
    Leonardo, Garcia Vinícius Fulber, de Souza Giovanni Venâncio, Franco Muriel Figueredo,
    Bondan Lucas, De Turck Filip, Granville Lisandro Zambenedetti, Junior Elias Procópio
    Duarte, dos Santos Carlos Raniery Paula, et al. NIEP: NFV infrastructure emulation
    platform 2018 IEEE 32nd International Conference on Advanced Information Networking
    and Applications (AINA), IEEE (2018), pp. 173-180 CrossRefView in ScopusGoogle
    Scholar [48] Castillo-Lema José, Neto Augusto Venâncio, de Oliveira Flávio, Kofuji
    Sergio Takeo Mininet-NFV: Evolving mininet with OASIS TOSCA NVF profiles towards
    reproducible NFV prototyping 2019 IEEE Conference on Network Softwarization (NetSoft),
    IEEE (2019), pp. 506-512 CrossRefView in ScopusGoogle Scholar [49] Trakadas Panagiotis,
    Karkazis Panagiotis, Leligou Helen C., Zahariadis Theodore, Vicens Felipe, Zurita
    Arturo, Alemany Pol, Soenen Thomas, Parada Carlos, Bonnet Jose, et al. Comparison
    of management and orchestration solutions for the 5G era J. Sensor Actuator Netw.,
    9 (1) (2020), p. 4 CrossRefView in ScopusGoogle Scholar [50] Trivisonno Riccardo
    Applicability of network function virtualization in 5 G network Wiley 5G Ref:
    The Essential 5G Reference Online, Wiley Online Library (2019), pp. 1-26 CrossRefGoogle
    Scholar [51] Ma Wenrui, Medina Carlos, Pan Deng Traffic-aware placement of NFV
    middleboxes 2015 IEEE Global Communications Conference (GLOBECOM), IEEE (2015),
    pp. 1-6 View in ScopusGoogle Scholar [52] Sherry Justine, Hasan Shaddi, Scott
    Colin, Krishnamurthy Arvind, Ratnasamy Sylvia, Sekar Vyas Making middleboxes someone
    else’s problem: network processing as a cloud service ACM SIGCOMM Comput. Commun.
    Rev., 42 (4) (2012), pp. 13-24 Google Scholar [53] Alwakeel Ahmed M., Alnaim Abdulrahman
    K., Fernandez Eduardo B. Toward a reference architecture for NFV 2019 2nd International
    Conference on Computer Applications & Information Security (ICCAIS), IEEE (2019),
    pp. 1-6 Google Scholar [54] Sun Gang, Zhu Gungyang, Liao Dan, Yu Hongfang, Du
    Xiaojiang, Guizani Mohsen Cost-efficient service function chain orchestration
    for low-latency applications in NFV networks IEEE Syst. J., 13 (4) (2018), pp.
    3877-3888 Google Scholar [55] Bilal Anwer, Theophilus Benson, Nick Feamster, Dave
    Levin, Programming slick network functions, in: Proceedings of the 1st Acm Sigcomm
    Symposium on Software Defined Networking Research, 2015, pp. 1–13. Google Scholar
    [56] Trajkovska Irena, Kourtis Michail-Alexandros, Sakkas Christos, Baudinot Denis,
    Silva João, Harsh Piyush, Xylouris George, Bohnert Thomas Michael, Koumaras Harilaos
    SDN-based service function chaining mechanism and service prototype implementation
    in NFV scenario Comput. Stand. Interfaces, 54 (2017), pp. 247-265 View PDFView
    articleView in ScopusGoogle Scholar [57] Atoui Wassim Sellil, Assy Nour, Gaaloul
    Walid, Ben Yahia Imen Grida A model-driven approach for deployment descriptor
    design in network function virtualization Int. J. Netw. Manage. (2021), Article
    e2165 Google Scholar [58] De Benedictis Marco, Lioy Antonio On the establishment
    of trust in the cloud-based ETSI NFV framework 2017 IEEE Conference on Network
    Function Virtualization and Software Defined Networks (NFV-SDN), IEEE (2017),
    pp. 280-285 CrossRefView in ScopusGoogle Scholar [59] Network functions virtualisation
    (NFV); Management and orchestration (2021) https://www.etsi.org/deliver/etsi_gs/NFV-MAN/001_099/001/01.01.01_60/gs_NFV-MAN001v010101p.pdf.
    Accessed: 2021-07-10 Google Scholar [60] Joshi Kaustubh, Benson Theophilus Network
    function virtualization IEEE Internet Comput., 20 (6) (2016), pp. 7-9 View in
    ScopusGoogle Scholar [61] Veeraraghavan Malathi, Sato Takehiro, Buchanan Molly,
    Rahimi Reza, Okamoto Satoru, Yamanaka Naoaki Network function virtualization:
    A survey IEICE Trans. Commun. (2017) 2016NNI0001 Google Scholar [62] Szabo Robert,
    Kind Mario, Westphal Fritz-Joachim, Woesner Hagen, Jocha David, Csaszar Andras
    Elastic network functions: opportunities and challenges IEEE Netw., 29 (3) (2015),
    pp. 15-21 View in ScopusGoogle Scholar [63] López Lorena Isabel Barona, Caraguay
    Ángel Leonardo Valdivieso, Villalba Luis Javier García, López Diego Trends on
    virtualisation with software defined networking and network function virtualisation
    IET Netw., 4 (5) (2015), pp. 255-263 View in ScopusGoogle Scholar [64] Han Bo,
    Gopalakrishnan Vijay, Ji Lusheng, Lee Seungjoon Network function virtualization:
    Challenges and opportunities for innovations IEEE Commun. Mag., 53 (2) (2015),
    pp. 90-97 View in ScopusGoogle Scholar [65] Yi Bo, Wang Xingwei, Li Keqin, Huang
    Min, et al. A comprehensive survey of network function virtualization Comput.
    Netw., 133 (2018), pp. 212-262 View PDFView articleView in ScopusGoogle Scholar
    [66] Tranoris Christos, Denazis Spyros A workflow for onboarding verticals on
    5G/NFV experimental network facility 2020 IEEE International Conference on Communications
    Workshops (ICC Workshops), IEEE (2020), pp. 1-5 CrossRefGoogle Scholar [67] ETSI
    GS NFV-SOL 002 V3.3.1 (2021) https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/002/03.03.01_60/gs_NFV-SOL002v030301p.pdf.
    Accessed: 2021-10-20 Google Scholar [68] ETSI GS NFV-SOL 003 V3.5.1 (2021) https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/003/03.05.01_60/gs_NFV-SOL003v030501p.pdf.
    Accessed: 2021-10-20 Google Scholar [69] ETSI GS NFV-SOL 005 V3.3.1 (2021) https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/005/03.03.01_60/gs_nfv-sol005v030301p.pdf.
    Accessed: 2021-10-21 Google Scholar [70] ETSI GS NFV-IFA 006 V3.5.1 (2021) https://www.etsi.org/deliver/etsi_gs/NFV-IFA/001_099/006/03.05.01_60/gs_NFV-IFA006v030501p.pdf.
    Accessed: 2021-10-21 Google Scholar [71] ETSI GS NFV-IFA 005 V3.1.1 (2021) https://www.etsi.org/deliver/etsi_gs/nfv-ifa/001_099/005/03.01.01_60/gs_nfv-ifa005v030101p.pdf.
    Accessed: 2021-10-21 Google Scholar [72] ETSI GS NFV-SOL 009 V3.3.1 (2021) https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/009/03.03.01_60/gs_NFV-SOL009v030301p.pdf.
    Accessed: 2021-10-21 Google Scholar [73] ETSI GS NFV-SOL 012 V3.3.1 (2021) https://www.etsi.org/deliver/etsi_gs/NFV-SOL/001_099/012/03.04.01_60/gs_NFV-SOL012v030401p.pdf.
    Accessed: 2021-10-21 Google Scholar [74] Janz Christopher, Ong Lyndon, Sethuraman
    Karthik, Shukla Vishnu Emerging transport SDN architecture and use cases IEEE
    Commun. Mag., 54 (10) (2016), pp. 116-121 View in ScopusGoogle Scholar [75] Duan
    Qiang, Ansari Nirwan, Toy Mehmet Software-defined network virtualization: An architectural
    framework for integrating SDN and NFV for service provisioning in future networks
    IEEE Netw., 30 (5) (2016), pp. 10-16 View in ScopusGoogle Scholar [76] Yousaf
    Faqir Zarrar, Bredel Michael, Schaller Sibylle, Schneider Fabian NFV and SDN—Key
    technology enablers for 5G networks IEEE J. Sel. Areas Commun., 35 (11) (2017),
    pp. 2468-2478 View in ScopusGoogle Scholar [77] Akyıldız Hasan Anıl, Saygun Ece
    SDN-NFV-cloud introduction in the context of service chaining 23rd Signal Processing
    and Communications Applications Conference (SIU), IEEE (2015), pp. 2605-2608 View
    in ScopusGoogle Scholar [78] Hoang Doan B., Farahmandian Sarah Security of software-defined
    infrastructures with SDN, NFV, and cloud computing technologies Guide to Security
    in SDN and NFV, Springer (2017), pp. 3-32 CrossRefGoogle Scholar [79] Arya Kapil,
    Garg Rohan, Polyakov Artem Y., Cooperman Gene Design and implementation for checkpointing
    of distributed resources using process-level virtualization 2016 Ieee International
    Conference on Cluster Computing (Cluster), IEEE (2016), pp. 402-412 View in ScopusGoogle
    Scholar [80] Kaur Karamjeet, Singh Japinder, Ghumman Navtej Singh Mininet as software
    defined networking testing platform International Conference on Communication,
    Computing & Systems (ICCCS) (2014), pp. 139-142 CrossRefGoogle Scholar [81] Hasan
    Muhamad, Dahshan Hisham, Abdelwanees Essam, Elmoghazy Aly SDN mininet emulator
    benchmarking and result analysis 2020 2nd Novel Intelligent and Leading Emerging
    Sciences Conference (NILES), IEEE (2020), pp. 355-360 CrossRefView in ScopusGoogle
    Scholar [82] Luzar Anže, Stanovnik Sašo, Cankar Matija Examination and comparison
    of TOSCA orchestration tools European Conference on Software Architecture, Springer
    (2020), pp. 247-259 CrossRefView in ScopusGoogle Scholar [83] De Oliveira Rogério
    Leão Santos, Schweitzer Christiane Marie, Shinoda Ailton Akira, Prete Ligia Rodrigues
    Using mininet for emulation and prototyping software-defined networks 2014 IEEE
    Colombian Conference on Communications and Computing (COLCOM), IEEE (2014), pp.
    1-6 CrossRefGoogle Scholar [84] Csoma Attila, Sonkoly Balázs, Csikor Levente,
    Németh Felicián, Gulyás András, Tavernier Wouter, Sahhaf Sahel ESCAPE: Extensible
    service chain prototyping environment using mininet, click, netconf and pox ACM
    SIGCOMM Comput. Commun. Rev., 44 (4) (2014), pp. 125-126 CrossRefView in ScopusGoogle
    Scholar [85] UNIFY: Unifying cloud and carrier networks (2021) https://www.eict.de/projekte/#project-23.
    Accessed: 2021-07-05 Google Scholar [86] Huff Alexandre, Venancio Giovanni, Marcuzzo
    Leonardo da C., Garcia Vinicius F., dos Santos Carlos R.P., Duarte Elias P. A
    holistic approach to define service chains using Click-on-OSv on different NFV
    platforms 2018 IEEE Global Communications Conference (GLOBECOM), IEEE (2018),
    pp. 1-6 CrossRefGoogle Scholar [87] Valenčić D., Mateljan V. Implementation of
    NETCONF protocol 2019 42nd International Convention on Information and Communication
    Technology, Electronics and Microelectronics (MIPRO), IEEE (2019), pp. 421-430
    CrossRefView in ScopusGoogle Scholar [88] Jehad Ali, Seungwoon Lee, Byeong-hee
    Roh, Performance analysis of POX and Ryu with different SDN topologies, in: Proceedings
    of the 2018 International Conference on Information Science and System, 2018,
    pp. 244–249. Google Scholar [89] Suo Kun, Zhao Yong, Chen Wei, Rao Jia An analysis
    and empirical study of container networks IEEE INFOCOM 2018-IEEE Conference on
    Computer Communications, IEEE (2018), pp. 189-197 CrossRefView in ScopusGoogle
    Scholar [90] Abeni Luca, Faggioli Dario Using Xen and KVM as real-time hypervisors
    J. Syst. Archit., 106 (2020), Article 101709 View PDFView articleView in ScopusGoogle
    Scholar [91] Garcia Vinícius Fülber, Souza Giovanni Venâncio De, Jr. Elias Procopio
    Duarte, Tavares Thales Nicolai, Marcuzzo Leonardo Da Cruz, Santos Carlos RP Dos,
    Franco Muriel Figueredo, Bondan Lucas, Granville Lisandro Zambenedetti, Schaeffer-Filho
    Alberto Egon, et al. On the design and development of emulation platforms for
    NFV-based infrastructures Int. J. Grid Util. Comput., 11 (2) (2020), pp. 230-242
    CrossRefView in ScopusGoogle Scholar [92] Kim MinSik, Do Truong-Xuan, Kim YoungHan
    TOSCA-based clustering service for network function virtualization 2016 International
    Conference on Information and Communication Technology Convergence (ICTC), IEEE
    (2016), pp. 1176-1178 View in ScopusGoogle Scholar [93] Mandal Priyatosh, Jain
    Rekha Comparison of openstack orchestration procedures 2019 International Conference
    on Smart Applications, Communications and Networking (SmartNets), IEEE (2019),
    pp. 1-4 Google Scholar [94] Mechtri Marouen, Ghribi Chaima, Soualah Oussama, Zeghlache
    Djamal NFV orchestration framework addressing SFC challenges IEEE Commun. Mag.,
    55 (6) (2017), pp. 16-23 View in ScopusGoogle Scholar [95] Jinja2 templates (2021)
    https://www.ibm.com/docs/en/qradar-common?topic=1-jinja2-templates. Accessed:
    2021-06-20 Google Scholar [96] Abeni Luca, Faggioli Dario An experimental analysis
    of the xen and kvm latencies 2019 IEEE 22nd International Symposium on Real-Time
    Distributed Computing (ISORC), IEEE (2019), pp. 18-26 CrossRefView in ScopusGoogle
    Scholar [97] Chae MinSu, Lee HwaMin, Lee Kiyeol A performance comparison of linux
    containers and virtual machines using Docker and KVM Cluster Comput., 22 (1) (2019),
    pp. 1765-1775 CrossRefView in ScopusGoogle Scholar [98] OPNFV (2021) https://www.opnfv.org/.
    Accessed: 2021-04-15 Google Scholar [99] Chen Jinlin, Chen Yiren, Tsai Shi-Chun,
    Lin Yi-Bing Implementing NFV system with OpenStack 2017 IEEE Conference on Dependable
    and Secure Computing, IEEE (2017), pp. 188-194 Google Scholar [100] Heavy reading
    survey (2021) https://www.comparethecloud.net/news/heavy-reading-survey-86-percent-of-global-telecoms-report-openstack-important-or-essential-to-their-success/.
    Accessed: 2021-06-25 Google Scholar [101] Shrivastwa Alok, Sarat Sunil Learning
    OpenStack Packt Publishing Ltd (2015) Google Scholar [102] Pedone Ignazio, Lioy
    Antonio, Valenza Fulvio Towards an efficient management and orchestration framework
    for virtual network security functions Secur. Commun. Netw., 2019 (2019) Google
    Scholar [103] Vu Xuan Tuong, Lee Jangwon, Nguyen Quang Huy, Sun Kyoungjae, Kim
    Younghan An architecture for enabling VNF auto-scaling with flow migration 2020
    International Conference on Information and Communication Technology Convergence
    (ICTC), IEEE (2020), pp. 624-627 CrossRefView in ScopusGoogle Scholar [104] Tacker-OpenStack
    NFV orchestration (2021) https://wiki.openstack.org/wiki/Tacker. Accessed: 2021-06-15
    Google Scholar [105] Chen Hung-Li, Lin Fuchun Joseph Scalable IoT/M2M platforms
    based on kubernetes-enabled NFV MANO architecture 2019 International Conference
    on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom)
    and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData),
    IEEE (2019), pp. 1106-1111 CrossRefView in ScopusGoogle Scholar [106] Bolivar
    Luis Tomas, Tselios Christos, Area Daniel Mellado, Tsolis George On the deployment
    of an open-source, 5G-aware evaluation testbed 2018 6th IEEE International Conference
    on Mobile Cloud Computing, Services, and Engineering (MobileCloud), IEEE (2018),
    pp. 51-58 CrossRefView in ScopusGoogle Scholar [107] Bringhenti Daniele, Marchetto
    Guido, Sisto Riccardo, Valenza Fulvio, Yusupov Jalolliddin Towards a fully automated
    and optimized network security functions orchestration 2019 4th International
    Conference on Computing, Communications and Security (ICCCS), IEEE (2019), pp.
    1-7 CrossRefGoogle Scholar [108] Nguyen Tri-Hai, Yoo Myungsik A VNF descriptor
    generator for tacker-based NFV management and orchestration 2018 International
    Conference on Information and Communication Technology Convergence (ICTC), IEEE
    (2018), pp. 260-262 CrossRefView in ScopusGoogle Scholar [109] Tacker documentation
    (2021) https://docs.openstack.org/tacker/latest/. Accessed: 2021-05-28 Google
    Scholar [110] Venâncio Giovanni, Garcia Vinícius Fulber, da Cruz Marcuzzo Leonardo,
    Tavares Thales Nicolai, Franco Muriel Figueredo, Bondan Lucas, Schaeffer-Filho
    Alberto Egon, Paula dos Santos Carlos Raniery, Granville Lisandro Zambenedetti,
    Duarte Elias P. Jr. Beyond VNFM: Filling the gaps of the ETSI VNF manager to fully
    support VNF life cycle operations Int. J. Netw. Manage. (2019), Article e2068
    Google Scholar [111] Bhardwaj Aditya, Krishna C. Rama Virtualization in cloud
    computing: Moving from hypervisor to containerization—A survey Arab. J. Sci. Eng.
    (2021), pp. 1-17 CrossRefGoogle Scholar [112] Potdar Amit M., Narayan D.G., Kengond
    Shivaraj, Mulla Mohammed Moin Performance evaluation of docker container and virtual
    machine Procedia Comput. Sci., 171 (2020), pp. 1419-1428 View PDFView articleView
    in ScopusGoogle Scholar [113] Prateek Sharma, Lucas Chaufournier, Prashant Shenoy,
    Y.C. Tay, Containers and virtual machines at scale: A comparative study, in: Proceedings
    of the 17th International Middleware Conference, 2016, pp. 1–13. Google Scholar
    [114] Singh Gursharan, Behal Sunny, Taneja Monal Advanced memory reusing mechanism
    for virtual machines in cloud computing Procedia Comput. Sci., 57 (5) (2015),
    pp. 91-103 View PDFView articleView in ScopusGoogle Scholar [115] Boettiger Carl
    An introduction to Docker for reproducible research Oper. Syst. Rev., 49 (1) (2015),
    pp. 71-79 CrossRefView in ScopusGoogle Scholar [116] Kovács József, Kacsuk Péter,
    Emődi Márk Deploying docker swarm cluster on hybrid clouds using occopus Adv.
    Eng. Softw., 125 (2018), pp. 136-145 View PDFView articleView in ScopusGoogle
    Scholar [117] Modak Arsh, Chaudhary S.D., Paygude P.S., Ldate S.R. Techniques
    to secure data on cloud: Docker swarm or kubernetes? 2018 Second International
    Conference on Inventive Communication and Computational Technologies (ICICCT),
    IEEE (2018), pp. 7-12 CrossRefView in ScopusGoogle Scholar [118] Sayfan Gigi Mastering
    Kubernetes Packt Publishing Ltd (2017) Google Scholar [119] Santos Jose, Wauters
    Tim, Volckaert Bruno, De Turck Filip Towards network-aware resource provisioning
    in Kubernetes for fog computing applications 2019 IEEE Conference on Network Softwarization
    (NetSoft), IEEE (2019), pp. 351-359 CrossRefView in ScopusGoogle Scholar [120]
    Gawel Maciej, Zielinski Krzysztof Analysis and evaluation of kubernetes based
    NFV management and orchestration 2019 IEEE 12th International Conference on Cloud
    Computing (CLOUD), IEEE (2019), pp. 511-513 CrossRefView in ScopusGoogle Scholar
    [121] Vayghan Leila Abdollahi, Saied Mohamed Aymen, Toeroe Maria, Khendek Ferhat
    Microservice based architecture: Towards high-availability for stateful applications
    with Kubernetes 2019 IEEE 19th International Conference on Software Quality, Reliability
    and Security (QRS), IEEE (2019), pp. 176-185 Google Scholar [122] Brandon Thurgood,
    Ruth G. Lennon, Cloud computing with Kubernetes cluster elastic scaling, in: Proceedings
    of the 3rd International Conference on Future Networks and Distributed Systems,
    2019, pp. 1–7. Google Scholar [123] Nguyen Nguyen, Kim Taehong Toward highly scalable
    load balancing in Kubernetes clusters IEEE Commun. Mag., 58 (7) (2020), pp. 78-83
    CrossRefView in ScopusGoogle Scholar [124] Kristiani Endah, Yang Chao-Tung, Huang
    Chin-Yin, Wang Yuan-Ting, Ko Po-Cheng The implementation of a cloud-edge computing
    architecture using OpenStack and kubernetes for air quality monitoring application
    Mob. Netw. Appl. (2020), pp. 1-23 CrossRefGoogle Scholar [125] Li Zheng, Kihl
    Maria, Lu Qinghua, Andersson Jens A. Performance overhead comparison between hypervisor
    and container based virtualization 2017 IEEE 31st International Conference on
    Advanced Information Networking and Applications (AINA), IEEE (2017), pp. 955-962
    CrossRefView in ScopusGoogle Scholar [126] Ojha Vishrant, Parihar Krishna, Choudhary
    Arjun Comparison of the two giants in container platforms: Kubernetes with openstack
    vs. docker Int. J. Adv. Res. Comput. Sci., 9 (3) (2018) Google Scholar [127] Magnum
    user-guide: OpenStack documentation (2021) https://docs.openstack.org/magnum/latest/user/.
    Accessed: 2021-03-25 Google Scholar [128] Hadi Razzaghi Kouchaksaraei, Holger
    Karl, Service function chaining across openstack and kubernetes domains, in: Proceedings
    of the 13th ACM International Conference on Distributed and Event-Based Systems,
    2019, pp. 240–243. Google Scholar [129] Chun Byonggon, Ha Jihun, Oh Sewon, Cho
    Hyunsung, Jeong MyeongGi Kubernetes enhancement for 5G NFV infrastructure 2019
    International Conference on Information and Communication Technology Convergence
    (ICTC), IEEE (2019), pp. 1327-1329 CrossRefView in ScopusGoogle Scholar [130]
    Yala Louiza, Iordache Marius, Bousselmi Ayoub, Imadali Sofiane 5G mobile network
    orchestration and management using open-source 2019 IEEE 2nd 5G World Forum (5GWF),
    IEEE (2019), pp. 421-426 CrossRefView in ScopusGoogle Scholar [131] Slim Farah,
    Guillemin Fabrice, Gravey Annie, Hadjadj-Aoul Yassine Towards a dynamic adaptive
    placement of virtual network functions under ONAP 2017 IEEE Conference on Network
    Function Virtualization and Software Defined Networks (NFV-SDN), IEEE (2017),
    pp. 210-215 CrossRefView in ScopusGoogle Scholar [132] Yilma Girma M, Yousaf Zarrar
    F., Sciancalepore Vincenzo, Costa-Perez Xavier Benchmarking open source NFV MANO
    systems: OSM and ONAP Comput. Commun., 161 (2020), pp. 86-98 View PDFView articleView
    in ScopusGoogle Scholar [133] de Carvalho Leonardo Rebouças, de Araujo Aleteia
    Patricia Favacho Performance comparison of terraform and cloudify as multicloud
    orchestrators 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and
    Internet Computing (CCGRID), IEEE (2020), pp. 380-389 CrossRefView in ScopusGoogle
    Scholar [134] Carella Giuseppe Antonio, Magedanz Thomas Open baton: a framework
    for virtual network function management and orchestration for emerging software-based
    5g networks Newsletter, 2016 (2015), p. 190 Google Scholar [135] Slamnik-Kriještorac
    Nina, Marquez-Barja Johann M. Demo abstract: Assessing MANO performance based
    on VIM platforms within MEC context IEEE INFOCOM 2020-IEEE Conference on Computer
    Communications Workshops (INFOCOM WKSHPS), IEEE (2020), pp. 1338-1339 CrossRefView
    in ScopusGoogle Scholar [136] Makris Nikos, Zarafetas Christos, Valantasis Alexandros,
    Korakis Thanasis Service orchestration over wireless network slices: testbed setup
    and integration IEEE Trans. Netw. Serv. Manag., 18 (1) (2020), pp. 482-497 Google
    Scholar [137] Mijumbi Rashid, Serrat Joan, Gorricho Juan-Luis, Latre Steven, Charalambides
    Marinos, Lopez Diego Management and orchestration challenges in network functions
    virtualization IEEE Commun. Mag., 54 (1) (2016), pp. 98-105 View in ScopusGoogle
    Scholar [138] Yilma Girma M., Yousaf Faqir Zarrar, Sciancalepore Vincenzo, Costa-Perez
    Xavier On the challenges and KPIs for benchmarking open-source NFV MANO systems:
    OSM vs ONAP (2019) arXiv preprint arXiv:1904.10697 Google Scholar [139] Rehman
    A.U., Aguiar Rui L., Barraca João Paulo Testing virtual network functions auto-scaling
    using open-source management and orchestration 2021 Telecoms Conference (ConfTELE),
    IEEE (2021), pp. 1-6 View in ScopusGoogle Scholar [140] Jamshidi Pooyan, Ahmad
    Aakash, Pahl Claus Cloud migration research: a systematic review IEEE Trans. Cloud
    Comput., 1 (2) (2013), pp. 142-157 View in ScopusGoogle Scholar [141] Babar Muhammad
    Ali, Zhang He Systematic literature reviews in software engineering: Preliminary
    results from interviews with researchers 2009 3rd International Symposium on Empirical
    Software Engineering and Measurement, IEEE (2009), pp. 346-355 CrossRefView in
    ScopusGoogle Scholar [142] Singh Karanpreet, Singh Paramvir, Kumar Krishan A systematic
    review of IP traceback schemes for denial of service attacks Comput. Secur., 56
    (2016), pp. 111-139 View PDFView articleView in ScopusGoogle Scholar [143] ACM
    digital library, 2019 (2021) https://dl.acm.org/. Accessed: 2021-03-22 Google
    Scholar [144] Springer digital library, 2019 (2021) https://link.springer.com/.
    Accessed: 2021-03-22 Google Scholar [145] IEEEXplore digital library, 2019 (2021)
    https://ieeexplore.ieee.org/Xplore/home.jsp. Accessed: 2021-03-24 Google Scholar
    [146] ScienceDirecct digital library, 2019 (2021) https://www.sciencedirect.com/.
    Accessed: 2021-03-23 Google Scholar [147] Ma Jiefei, Rankothge Windhya, Makaya
    Christian, Morales Mariceli, Le Frank, Lobo Jorge A comprehensive study on load
    balancers for vnf chains horizontal scaling (2018) arXiv preprint arXiv:1810.03238
    Google Scholar [148] Cheng-Liang Hsieh, Ning Weng, Virtual network functions instantiation
    on SDN switches for policy-aware traffic steering, in: Proceedings of the 2016
    Symposium on Architectures for Networking and Communications Systems, 2016, pp.
    119–120. Google Scholar [149] Ma Yi-Wei, Chen Jiann-Liang, Jhou Jia-Yi Adaptive
    service function selection for network function virtualization networking Future
    Gener. Comput. Syst., 91 (2019), pp. 108-123 View PDFView articleView in ScopusGoogle
    Scholar [150] Thai Minh-Tuan, Lin Ying-Dar, Lin Po-Ching, Lai Yuan-Cheng Towards
    load-balanced service chaining by hash-based traffic steering on softswitches
    J. Netw. Comput. Appl., 109 (2018), pp. 1-10 View PDFView articleView in ScopusGoogle
    Scholar [151] Gabriele Baldoni, Alfio Lombardo, Marcello Melita, Sergio Micalizzi,
    Corrado Rametta, Alessandro Vassallo, An emulation framework for SDN-NFV based
    services, in: Proceedings of the Second International Conference on Internet of
    Things, Data and Cloud Computing, 2017, pp. 1–8. Google Scholar [152] Fancy C.,
    Pushpalatha M. Performance evaluation of SDN controllers POX and floodlight in
    mininet emulation environment 2017 International Conference on Intelligent Sustainable
    Systems (ICISS), IEEE (2017), pp. 695-699 CrossRefView in ScopusGoogle Scholar
    [153] Chou Li-Der, Tseng Chia-Wei, Chou Hsin-Yao, Yang Yao-Tsung A SFC network
    management system in SDN International Conference on Mobile and Wireless Technology,
    Springer (2017), pp. 360-369 CrossRefView in ScopusGoogle Scholar [154] Kim Sang
    Il, Kim Hwa Sung A research on dynamic service function chaining based on reinforcement
    learning using resource usage 2017 Ninth International Conference on Ubiquitous
    and Future Networks (ICUFN), IEEE (2017), pp. 582-586 CrossRefView in ScopusGoogle
    Scholar [155] Medhat Ahmed M., Carella Giuseppe A., Pauls Michael, Magedanz Thomas
    Orchestrating scalable service function chains in a NFV environment 2017 IEEE
    Conference on Network Softwarization (NetSoft), IEEE (2017), pp. 1-5 Google Scholar
    [156] Yang Lyunsik, Van Tung Doan, Kim Minsik, Kim Younghan Alarm-based monitoring
    for high availability in service function chain 2016 International Conference
    on Cloud Computing Research and Innovations (ICCCRI), IEEE (2016), pp. 86-91 View
    in ScopusGoogle Scholar [157] Akhtar Nabeel, Matta Ibrahim, Wang Yuefeng Managing
    NFV using SDN and control theory NOMS 2016-2016 IEEE/IFIP Network Operations and
    Management Symposium, IEEE (2016), pp. 1113-1118 CrossRefView in ScopusGoogle
    Scholar [158] Sun Gang, Xu Zhu, Yu Hongfang, Chen Xi, Chang Victor, Vasilakos
    Athanasios V. Low-latency and resource-efficient service function chaining orchestration
    in network function virtualization IEEE Internet Things J., 7 (7) (2020), pp.
    5760-5772 CrossRefView in ScopusGoogle Scholar [159] Salvatore Renato Ziri, Ahmad
    Tajuddin Samsudin, Christophe Fontaine, Service chaining implementation in network
    function virtualization with software defined networking, in: Proceedings of the
    5th International Conference on Communications and Broadband Networking, 2017,
    pp. 70–75. Google Scholar [160] Yi Bo, Wang Xingwei, Huang Min Design and evaluation
    of schemes for provisioning service function chain with function scalability J.
    Netw. Comput. Appl., 93 (2017), pp. 197-214 View PDFView articleView in ScopusGoogle
    Scholar [161] Shen Gengbiao, Li Qing, Jiang Yong, Wu Yu, Lv Jianhui A four-stage
    adaptive scheduling scheme for service function chain in NFV Comput. Netw., 175
    (2020), Article 107259 View PDFView articleView in ScopusGoogle Scholar [162]
    Soualah Oussama, Mechtri Marouen, Ghribi Chaima, Zeghlache Djamal Energy efficient
    algorithm for VNF placement and chaining 2017 17th IEEE/ACM International Symposium
    on Cluster, Cloud and Grid Computing (CCGRID), IEEE (2017), pp. 579-588 View in
    ScopusGoogle Scholar [163] Davoli Gianluca, Cerroni Walter, Contoli Chiara, Foresta
    Francesco, Callegati Franco Implementation of service function chaining control
    plane through OpenFlow 2017 IEEE Conference on Network Function Virtualization
    and Software Defined Networks (NFV-SDN), IEEE (2017), pp. 1-4 CrossRefView in
    ScopusGoogle Scholar [164] Heideker Alexandre, Zyrianoff Ivan, Kamienski Carlos
    A. Profiling service function chaining behavior for NFV orchestration 2018 IEEE
    Symposium on Computers and Communications (ISCC), IEEE (2018), pp. 01020-01025
    CrossRefGoogle Scholar [165] Özdem Mehmet, Alkan Mustafa Subscriber aware dynamic
    service function chaining Comput. Netw., 194 (2021), Article 108138 View PDFView
    articleView in ScopusGoogle Scholar [166] Gharbaoui Molka, Contoli Chiara, Davoli
    Gianluca, Cuffaro Giovanni, Martini Barbara, Paganelli Federica, Cerroni Walter,
    Cappanera Paola, Castoldi Piero Experimenting latency-aware and reliable service
    chaining in next generation internet testbed facility 2018 IEEE Conference on
    Network Function Virtualization and Software Defined Networks (NFV-SDN), IEEE
    (2018), pp. 1-4 CrossRefGoogle Scholar [167] FED4FIRE+ (2021) https://www.fed4fire.eu.
    Accessed: 2021-06-12 Google Scholar [168] Lee Doyoung, Yoo Jae-Hyoung, Hong James
    Won-Ki Deep Q-networks based auto-scaling for service function chaining 2020 16th
    International Conference on Network and Service Management (CNSM), IEEE (2020),
    pp. 1-9 CrossRefGoogle Scholar [169] Cheng Guozhen, Chen Hongchang, Hu Hongchao,
    Wang Zhiming, Lan Julong Enabling network function combination via service chain
    instantiation Comput. Netw., 92 (2015), pp. 396-407 View PDFView articleView in
    ScopusGoogle Scholar [170] Van Tu Nguyen, Yoo Jae-Hyoung, Hong James Won-Ki Measuring
    end-to-end packet processing time in service function chaining 2020 16th International
    Conference on Network and Service Management (CNSM), IEEE (2020), pp. 1-9 CrossRefGoogle
    Scholar [171] Livi Sergio, Jacquemart Quentin, Pacheco Dino Lopez, Urvoy-Keller
    Guillaume Container-based service chaining: A performance perspective 2016 5th
    IEEE International Conference on Cloud Networking (Cloudnet), IEEE (2016), pp.
    176-181 CrossRefView in ScopusGoogle Scholar [172] Dab Boutheina, Fajjari Ilhem,
    Rohon Mathieu, Auboin Cyril, Diquélou Arnaud An efficient traffic steering for
    cloud-native service function chaining 2020 23rd Conference on Innovation in Clouds,
    Internet and Networks and Workshops (ICIN), IEEE (2020), pp. 71-78 CrossRefView
    in ScopusGoogle Scholar [173] Jeong Seyeon, Kim Heegon, Yoo Jae-Hyoung, Hong James
    Won-Ki Machine learning based link state aware service function chaining 2019
    20th Asia-Pacific Network Operations and Management Symposium (APNOMS), IEEE (2019),
    pp. 1-4 View in ScopusGoogle Scholar [174] Santos José, Wauters Tim, Volckaert
    Bruno, De Turck Filip Towards delay-aware container-based service function chaining
    in fog computing NOMS 2020-2020 IEEE/IFIP Network Operations and Management Symposium,
    IEEE (2020), pp. 1-9 Google Scholar [175] Sekine Hibiki, Kanai Kenji, Katto Jiro,
    Kanemitsu Hidehiro, Nakazato Hidenori IoT-centric service function chaining orchestration
    and its performance validation 2021 IEEE 18th Annual Consumer Communications &
    Networking Conference (CCNC), IEEE (2021), pp. 1-4 Google Scholar [176] DeokGi
    Hong, Jaemin Shin, Shinae Woo, Sue Moon, Considerations on deploying high-performance
    container-based NFV, in: Proceedings of the 2nd Workshop on Cloud-Assisted Networking,
    2017, pp. 1–6. Google Scholar [177] Pattaranantakul Montida, Song Qipeng, Tian
    Yanmei, Wang Licheng, Zhang Zonghua, Meddahi Ahmed, Vorakulpipat Chalee On achieving
    trustworthy service function chaining IEEE Trans. Netw. Serv. Manag. (2021) Google
    Scholar [178] Pahl Claus, Brogi Antonio, Soldani Jacopo, Jamshidi Pooyan Cloud
    container technologies: a state-of-the-art review IEEE Trans. Cloud Comput., 7
    (3) (2017), pp. 677-692 Google Scholar [179] Callegati Franco, Cerroni Walter,
    Contoli Chiara, Santandrea Giuliano Implementing dynamic chaining of virtual network
    functions in OpenStack platform 2015 17th International Conference on Transparent
    Optical Networks (ICTON), IEEE (2015), pp. 1-4 CrossRefGoogle Scholar [180] Ledjiar
    Abderrahmane, Sampin Emmanuel, Talhi Chamseddine, Cheriet Mohamed Network function
    virtualization as a service for multi-tenant software defined networks 2017 Fourth
    International Conference on Software Defined Systems (SDS), IEEE (2017), pp. 168-173
    CrossRefView in ScopusGoogle Scholar [181] Borylo Piotr, Davoli Gianluca, Rzepka
    Michal, Lason Artur, Cerroni Walter Unified and standalone monitoring module for
    NFV/SDN infrastructures J. Netw. Comput. Appl., 175 (2021), Article 102934 View
    PDFView articleView in ScopusGoogle Scholar [182] Wijethilaka Shalitha, Liyanage
    Madhusanka Survey on network slicing for Internet of Things realization in 5G
    networks IEEE Commun. Surv. Tutor., 23 (2) (2021), pp. 957-994 CrossRefView in
    ScopusGoogle Scholar [183] Lee Pei-Hsuan, Lin Fuchun Joseph Tackling IoT scalability
    with 5G NFV-enabled network slicing Adv. Internet Things, 11 (3) (2021), pp. 123-139
    CrossRefGoogle Scholar [184] Alemany Pol, Román Anton, Vilalta Ricard, Pol Ana,
    Bonnet José, Kapassa Evgenia, Touloupou Marios, Kyriazis Dimosthenis, Karkazis
    Panagiotis, Trakadas Panagiotis, et al. A KPI-enabled NFV MANO architecture for
    network slicing with QoS IEEE Commun. Mag., 59 (7) (2021), pp. 44-50 CrossRefView
    in ScopusGoogle Scholar [185] Kraus Sascha, Kanbach Dominik K., Krysta Peter M.,
    Steinhoff Maurice M., Tomini Nino Facebook and the creation of the metaverse:
    Radical business model innovation or incremental transformation Int. J. Entrep.
    Behav. Res. (2022) Google Scholar [186] Mozumder Md. Ariful Islam, Sheeraz Muhammad
    Mohsan, Athar Ali, Aich Satyabrata, Kim Hee-Cheol Overview: Technology roadmap
    of the future trend of metaverse based on IoT, blockchain, AI technique, and medical
    domain metaverse activity 2022 24th International Conference on Advanced Communication
    Technology (ICACT), IEEE (2022), pp. 256-261 CrossRefView in ScopusGoogle Scholar
    [187] Proença Jorge, Cruz Tiago, Simões Paulo, Freitas Miguel, Calé Rui Virtualizing
    customer premises equipment 2021 IEEE Conference on Network Function Virtualization
    and Software Defined Networks (NFV-SDN), IEEE (2021), pp. 104-105 CrossRefView
    in ScopusGoogle Scholar [188] Elangovan Muthuraman, Chen Chien, Chen Jyh-Cheng
    A flexible vCPE framework to enable dynamic service function chaining using P4
    switches 2021 22nd Asia-Pacific Network Operations and Management Symposium (APNOMS),
    IEEE (2021), pp. 342-347 CrossRefView in ScopusGoogle Scholar [189] Abid Muhammad
    Aneeq, Afaqui Naokhaiz, Khan Muazzam A, Akhtar Muhammad Waseem, Malik Asad Waqar,
    Munir Arslan, Ahmad Jawad, Shabir Balawal Evolution towards smart and software-defined
    internet of things AI, 3 (1) (2022), pp. 100-123 CrossRefView in ScopusGoogle
    Scholar [190] Mashaly Maggie Connecting the twins: A review on digital twin technology
    & its networking requirements Procedia Comput. Sci., 184 (2021), pp. 299-305 View
    PDFView articleView in ScopusGoogle Scholar [191] Jiang Zongmin, Guo Yangming,
    Wang Zhuqing Digital twin to improve the virtual-real integration of industrial
    IoT J. Ind. Inf. Integr., 22 (2021), Article 100196 View PDFView articleView in
    ScopusGoogle Scholar [192] Akram Hakiri, Behnam Dezfouli, Towards a blockchain-SDN
    architecture for secure and trustworthy 5G massive IoT networks, in: Proceedings
    of the 2021 ACM International Workshop on Software Defined Networks & Network
    Function Virtualization Security, 2021, pp. 11–18. Google Scholar [193] Jawdhari
    Hayder A., Abdullah Alharith A. The application of network functions virtualization
    on different networks, and its new applications in blockchain: A survey Management
    (2021) Google Scholar [194] Ramanathan Shunmugapriya, Kondepu Koteswararao, Razo
    Miguel, Tacca Marco, Valcarenghi Luca, Fumagalli Andrea Live migration of virtual
    machine and container based mobile core network components: A comprehensive study
    IEEE Access, 9 (2021), pp. 105082-105100 CrossRefView in ScopusGoogle Scholar
    [195] Internet of Things (IoT) connected devices installed base worldwide from
    2015 to 2025 (2021) https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/.
    Accessed: 2021-10-08 Google Scholar [196] Garay Jokin, Matias Jon, Unzilla Juanjo,
    Jacob Eduardo Service description in the NFV revolution: Trends, challenges and
    a way forward IEEE Commun. Mag., 54 (3) (2016), pp. 68-74 View in ScopusGoogle
    Scholar [197] Alvizu Gómez Rodolfo Enrique Advance optical routing techniques
    in the software defined era (2017) Google Scholar [198] Dräxler Sevil, Karl Holger,
    Peuster Manuel, Kouchaksaraei Hadi Razzaghi, Bredel Michael, Lessmann Johannes,
    Soenen Thomas, Tavernier Wouter, Mendel-Brin Sharon, Xilouris George SONATA: Service
    programming and orchestration for virtualized software networks 2017 IEEE International
    Conference on Communications Workshops (ICC Workshops), IEEE (2017), pp. 973-978
    View in ScopusGoogle Scholar [199] Arfaoui Ghada, Vilchez Jose Manuel Sanchez,
    Wary Jean-Philippe Security and resilience in 5G: Current challenges and future
    directions 2017 IEEE Trustcom/BigDataSE/ICESS, IEEE (2017), pp. 1010-1015 View
    in ScopusGoogle Scholar [200] Francescon Antonio, Baggio Giovanni, Fedrizzi Riccardo,
    Ferrusy Ramon, Yahiaz Imen Grida Ben, Riggio Roberto X–MANO: Cross–domain management
    and orchestration of network services 2017 IEEE Conference on Network Softwarization
    (NetSoft), IEEE (2017), pp. 1-5 CrossRefGoogle Scholar [201] Chartsias Panteleimon-Konstantinos,
    Amiras Athanasios, Plevrakis Ioannis, Samaras Ioakeim, Katsaros K, Kritharidis
    Dimitrios, Trouva Eleni, Angelopoulos Ioannis, Kourtis Akis, Siddiqui Muhammad
    Shuaib, et al. SDN/NFV-based end to end network slicing for 5G multi-tenant networks
    2017 European Conference on Networks and Communications (EuCNC), IEEE (2017),
    pp. 1-5 CrossRefGoogle Scholar [202] Jaeger Bernd Security orchestrator: Introducing
    a security orchestrator in the context of the etsi nfv reference architecture
    2015 IEEE Trustcom/BigDataSE/ISPA, Vol. 1, IEEE (2015), pp. 1255-1260 CrossRefView
    in ScopusGoogle Scholar [203] Karamichailidis Panagiotis, Choumas Kostas, Korakis
    Thanasis Enabling multi-domain orchestration using open source MANO, OpenStack
    and OpenDaylight 2019 IEEE International Symposium on Local and Metropolitan Area
    Networks (LANMAN), IEEE (2019), pp. 1-6 CrossRefGoogle Scholar [204] Fischer Andreas,
    Botero Juan Felipe, Beck Michael Till, De Meer Hermann, Hesselbach Xavier Virtual
    network embedding: A survey IEEE Commun. Surv. Tutor., 15 (4) (2013), pp. 1888-1906
    View in ScopusGoogle Scholar [205] Golkarifard Morteza, Chiasserini Carla Fabiana,
    Malandrino Francesco, Movaghar Ali Dynamic VNF placement, resource allocation
    and traffic routing in 5G Comput. Netw., 188 (2021), Article 107830 View PDFView
    articleView in ScopusGoogle Scholar [206] Araújo Samuel M.A., de Souza Fernanda
    S.H., Mateus Geraldo R. A hybrid optimization-machine learning approach for the
    VNF placement and chaining problem Comput. Netw. (2021), Article 108474 View PDFView
    articleView in ScopusGoogle Scholar [207] Gao Tao, Li Xin, Wu Yu, Zou Weixia,
    Huang Shanguo, Tornatore Massimo, Mukherjee Biswanath Cost-efficient VNF placement
    and scheduling in public cloud networks IEEE Trans. Commun., 68 (8) (2020), pp.
    4946-4959 CrossRefView in ScopusGoogle Scholar [208] Bellendorf Julian, Mann Zoltán
    Ádám Specification of cloud topologies and orchestration using TOSCA: a survey
    Computing, 102 (8) (2020), pp. 1793-1815 CrossRefView in ScopusGoogle Scholar
    [209] Ilievski Gjorgji, Latkoski Pero Network traffic classification in an NFV
    environment using supervised ML algorithms J. Telecommun. Inf. Technol. (3) (2021)
    Google Scholar [210] Huang Haojun, Zeng Cheng, Zhao Yangmin, Min Geyong, Zhu Yingying,
    Miao Wang, Hu Jia Scalable orchestration of service function chains in NFV-enabled
    networks: A federated reinforcement learning approach IEEE J. Sel. Areas Commun.,
    39 (8) (2021), pp. 2558-2571 CrossRefView in ScopusGoogle Scholar Cited by (19)
    The concept of network resource control of a 5G cluster focused on the smart city''s
    critical infrastructure needs 2024, Alexandria Engineering Journal Show abstract
    LIGHT: A Compatible, high-performance and scalable user-level network stack 2023,
    Computer Networks Show abstract Service Function Chain Deployment Using Deep Q
    Learning and Tidal Mechanism 2024, IEEE Internet of Things Journal Detection and
    mitigation of link flooding-based DDoS attacks on a software defined network using
    network function virtualisation 2024, International Journal of Communication Networks
    and Distributed Systems The State of Security in Sdn, Nfv, and Network Slicing
    2023, SSRN An In-Depth Survey on Virtualization Technologies in 6G Integrated
    Terrestrial and Non-Terrestrial Networks 2023, arXiv View all citing articles
    on Scopus Karamjeet Kaur received the B.Tech degree from Punjabi University Patiala
    and M.Tech degree from Punjab Technical University, Jalandhar in Computer Science
    Engineering. She is currently pursuing her PHD in Information Technology from
    Panjab University Chandigarh. Her research interest is on Software-Defined Networking,
    Virtualization, Network Function Virtualization and Cloud Computing. She has published
    25 papers in reputed journal, International & National Conferences. She has got
    4 years teaching experience. She is the author of the 1 book. She is UGC-NET qualified.
    Dr. Veenu Mangat received her Masters of Engineering in Computer Science and Engineering
    from Punjab Engineering College (PEC) in 2004 and Ph.D. in Engineering and Technology
    (Computer Science) in 2016 from Panjab University, India. She is currently working
    as Associate Professor in Information Technology at UIET, Panjab University. She
    has a teaching experience of more than 15 years. Her areas of research include
    data mining, machine learning, privacy and security. She is co-Principal Investigator
    in research project on ‘Monitoring of Active Fire Locations and Precision in Allied
    Agricultural Activities using Communication Technologies’ funded by Ministry of
    Electronics & IT of Government of India worth Rs. 75.75 lakhs from 2020-2022.
    She has also worked on research project entitled ‘Pedestrian Detection from Thermal
    Imaging’ funded by Design Innovation Centre of Ministry of HRD and consultancy
    project in the area of machine learning. She has successfully guided 21 Masters
    of Engineering dissertations and is currently guiding 7 Ph.D. scholars. Dr. Krishan
    Kumar is currently Professor in Department of Information Technology, University
    Institute of Engineering and Technology, Panjab University, Chandigarh. He has
    done B. Tech. Computer Science & Engineering from National Institute of Technology,
    Hamirpur in 1995. He completed his Master of Software Systems from Birla Institute
    of Technology & Sciences, Pilani in 2001. He finished his regular Ph.D. from Indian
    Institute of Technology, Roorkee in February, 2008. He has more than 22 years
    of teaching, research and administrative experience. His general research interests
    are in the areas of Network Security and Computer Networks. Specific research
    interests include Intrusion Detection, Protection from Internet Attacks, Web performance,
    Network architecture/protocols, and Network measurement/ modeling. He has published
    2 national and 2 International Books in the field of Computer Science & Network
    security. He has published more than 150 papers in national/International peer
    reviewed/Indexed/impact factor Journals and IEEE, ACM and Springer proceedings.
    His publications are well cited by eminent researchers in the field. View Abstract
    © 2022 Elsevier B.V. All rights reserved. Recommended articles Extensive performance
    analysis of OpenDayLight (ODL) and Open Network Operating System (ONOS) SDN controllers
    Microprocessors and Microsystems, Volume 95, 2022, Article 104715 Avtar Singh,
    …, Harpreet Kaur View PDF A proactive auto-scaling scheme with latency guarantees
    for multi-tenant NFV cloud Computer Networks, Volume 181, 2020, Article 107552
    Guangwu Hu, …, Yu Wu View PDF A survey of application orchestration and OSS in
    next-generation network management Computer Standards & Interfaces, Volume 62,
    2019, pp. 17-31 Guy Saadon, …, Noemie Simoni View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 11 Captures Readers: 43 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computer Networks
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A review on Virtualized Infrastructure Managers with management and orchestration
    features in NFV architecture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - El Khairi A.
  - Caselli M.
  - Knierim C.
  - Peter A.
  - Continella A.
  citation_count: '2'
  description: Container technology has gained ground in the industry for its scalability
    and lightweight virtualization, especially in cloud environments. Nevertheless,
    research has shown that containerized applications are an appealing target for
    cyberattacks, which may lead to interruption of business-critical services and
    financial damage. State-of-the-art anomaly-based host intrusion detection systems
    (HIDS) may enhance container runtime security. However, they were not designed
    to deal with the characteristics of containerized environments. Specifically,
    they cannot effectively cope with the scalability of containers and the diversity
    of anomalies. To address these challenges, we introduce a novel anomaly-based
    HIDS that relies on monitoring heterogeneous properties of system calls. Our key
    idea is that anomalies can be accurately detected when those properties are examined
    jointly within their context. To this end, we model system calls leveraging a
    graph-based structure that emphasizes their dependencies within their relative
    context, allowing us to precisely discern between normal and malicious activities.
    We evaluate our approach on two datasets of 20 different attack scenarios containing
    11,700 normal and 1,980 attack system call traces. The achieved results show that
    our solution effectively detects various anomalies with reasonable runtime overhead,
    outperforming state-of-the-art tools.
  doi: 10.1145/3560810.3564266
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesCCSProceedingsCCSW''22Contextualizing
    System Calls in Containers for Anomaly-Based Intrusion Detection RESEARCH-ARTICLE
    OPEN ACCESS SHARE ON Contextualizing System Calls in Containers for Anomaly-Based
    Intrusion Detection Authors: Asbat El Khairi , Marco Caselli , Christian Knierim
    , + 2 Authors Info & Claims CCSW''22: Proceedings of the 2022 on Cloud Computing
    Security WorkshopNovember 2022Pages 9–21https://doi.org/10.1145/3560810.3564266
    Published:07 November 2022Publication History 0 citation 1,539 Downloads eReaderPDF
    CCSW''22: Proceedings of the 2022 on Cloud Computing Security Workshop Contextualizing
    System Calls in Containers for Anomaly-Based Intrusion Detection Pages 9–21 Previous
    Next ABSTRACT Supplemental Material References Index Terms Recommendations Comments
    ABSTRACT Container technology has gained ground in the industry for its scalability
    and lightweight virtualization, especially in cloud environments. Nevertheless,
    research has shown that containerized applications are an appealing target for
    cyberattacks, which may lead to interruption of business-critical services and
    financial damage. State-of-the-art anomaly-based host intrusion detection systems
    (HIDS) may enhance container runtime security. However, they were not designed
    to deal with the characteristics of containerized environments. Specifically,
    they cannot effectively cope with the scalability of containers and the diversity
    of anomalies. To address these challenges, we introduce a novel anomaly-based
    HIDS that relies on monitoring heterogeneous properties of system calls. Our key
    idea is that anomalies can be accurately detected when those properties are examined
    jointly within their context. To this end, we model system calls leveraging a
    graph-based structure that emphasizes their dependencies within their relative
    context, allowing us to precisely discern between normal and malicious activities.
    We evaluate our approach on two datasets of 20 different attack scenarios containing
    11,700 normal and 1,980 attack system call traces. The achieved results show that
    our solution effectively detects various anomalies with reasonable runtime overhead,
    outperforming state-of-the-art tools. Skip Supplemental Material Section Supplemental
    Material CCSW22-011.mp4 Asbat El Khairi - Contextualizing System Calls in Containers
    for Anomaly-Based Intrusion Detection (CCSW''22) MP4 113.9 MB Play streamDownload
    References Martín Abadi et al. 2015. TensorFlow: Large-Scale Machine Learning
    on Heterogeneous Systems. https://www.tensorflow.org/ Amr S Abed, Charles Clancy,
    and David S Levy. 2015. Intrusion detection system for applications using linux
    containers. In International Workshop on Security and Trust Management. Springer,
    123--135. Rancher Admin. 2020. Runtime Security in Rancher with Falco. Show All
    References Index Terms Contextualizing System Calls in Containers for Anomaly-Based
    Intrusion Detection Security and privacy Intrusion/anomaly detection and malware
    mitigation Intrusion detection systems Recommendations An intelligent intrusion
    detection system (IDS) for anomaly and misuse detection in computer networks In
    this paper, we propose a novel Intrusion Detection System (IDS) architecture utilizing
    both anomaly and misuse detection approaches. This hybrid Intrusion Detection
    System architecture consists of an anomaly detection module, a misuse detection
    ... Read More Unknown Attacks Detection Using Feature Extraction from Anomaly-Based
    IDS Alerts SAINT ''12: Proceedings of the 2012 IEEE/IPSJ 12th International Symposium
    on Applications and the Internet Intrusion Detection Systems (IDSs) play an important
    role detecting various kinds of attacks and defend our computer systems from them.
    There are basically two main types of detection techniques: signature-based and
    anomaly-based. A signature-based IDS ... Read More A Neural Network Based Anomaly
    Intrusion Detection System DESE ''11: Proceedings of the 2011 Developments in
    E-systems Engineering Security system is the immune system for computers which
    is similar to the immune system in the human body. This includes all operations
    required to protect computer and systems from intruders. The aim of this work
    is to develop an anomaly-based ... Read More Comments 66 References View Table
    Of Contents Footer Categories Journals Magazines Books Proceedings SIGs Conferences
    Collections People About About ACM Digital Library ACM Digital Library Board Subscription
    Information Author Guidelines Using ACM Digital Library All Holdings within the
    ACM Digital Library ACM Computing Classification System Digital Library Accessibility
    Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: CCSW 2022 - Proceedings of the 2022 Cloud Computing Security Workshop,
    co-located with CCS 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Contextualizing System Calls in Containers for Anomaly-Based Intrusion Detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhong Z.
  - Xu M.
  - Rodriguez M.A.
  - Xu C.
  - Buyya R.
  citation_count: '33'
  description: Containerization is a lightweight application virtualization technology,
    providing high environmental consistency, operating system distribution portability,
    and resource isolation. Existing mainstream cloud service providers have prevalently
    adopted container technologies in their distributed system infrastructures for
    automated application management. To handle the automation of deployment, maintenance,
    autoscaling, and networking of containerized applications, container orchestration
    is proposed as an essential research problem. However, the highly dynamic and
    diverse feature of cloud workloads and environments considerably raises the complexity
    of orchestration mechanisms. Machine learning algorithms are accordingly employed
    by container orchestration systems for behavior modeling and prediction of multi-dimensional
    performance metrics. Such insights could further improve the quality of resource
    provisioning decisions in response to the changing workloads under complex environments.
    In this article, we present a comprehensive literature review of existing machine
    learning-based container orchestration approaches. Detailed taxonomies are proposed
    to classify the current researches by their common features. Moreover, the evolution
    of machine learning-based container orchestration technologies from the year 2016
    to 2021 has been designed based on objectives and metrics. A comparative analysis
    of the reviewed techniques is conducted according to the proposed taxonomies,
    with emphasis on their key characteristics. Finally, various open research challenges
    and potential future directions are highlighted.
  doi: 10.1145/3510415
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Journal Home Just Accepted Latest
    Issue Archive Authors Editors Reviewers About Contact Us HomeACM JournalsACM Computing
    SurveysVol. 54, No. 10sMachine Learning-based Orchestration of Containers: A Taxonomy
    and Future Directions SURVEY OPEN ACCESS SHARE ON Machine Learning-based Orchestration
    of Containers: A Taxonomy and Future Directions Authors: Zhiheng Zhong , Minxian
    Xu , Maria Alejandra Rodriguez , + 2 Authors Info & Claims ACM Computing SurveysVolume
    54Issue 10sArticle No.: 217pp 1–35https://doi.org/10.1145/3510415 Published:13
    September 2022Publication History 17 citation 6,792 Downloads View all FormatsPDF
    ACM Computing Surveys Volume 54, Issue 10s Previous Next Abstract 1 INTRODUCTION
    2 BACKGROUND 3 TAXONOMY OF MACHINE LEARNING-BASED CONTAINER ORCHESTRATION TECHNOLOGIES
    4 State-of-the-Art in Machine Learning for Orchestration of Containers 5 DISCUSSIONS
    AND FUTURE DIRECTIONS 6 CONCLUSION ACKNOWLEDGMENTS REFERENCES Cited By Index Terms
    Recommendations Comments Skip Abstract Section Abstract Containerization is a
    lightweight application virtualization technology, providing high environmental
    consistency, operating system distribution portability, and resource isolation.
    Existing mainstream cloud service providers have prevalently adopted container
    technologies in their distributed system infrastructures for automated application
    management. To handle the automation of deployment, maintenance, autoscaling,
    and networking of containerized applications, container orchestration is proposed
    as an essential research problem. However, the highly dynamic and diverse feature
    of cloud workloads and environments considerably raises the complexity of orchestration
    mechanisms. Machine learning algorithms are accordingly employed by container
    orchestration systems for behavior modeling and prediction of multi-dimensional
    performance metrics. Such insights could further improve the quality of resource
    provisioning decisions in response to the changing workloads under complex environments.
    In this article, we present a comprehensive literature review of existing machine
    learning-based container orchestration approaches. Detailed taxonomies are proposed
    to classify the current researches by their common features. Moreover, the evolution
    of machine learning-based container orchestration technologies from the year 2016
    to 2021 has been designed based on objectives and metrics. A comparative analysis
    of the reviewed techniques is conducted according to the proposed taxonomies,
    with emphasis on their key characteristics. Finally, various open research challenges
    and potential future directions are highlighted. Skip 1INTRODUCTION Section 1
    INTRODUCTION Our era has witnessed the trend of cloud computing becoming the mainstream
    industrial computing paradigm, providing stable service delivery with high cost-efficiency,
    scalability, availability, and accessibility [1]. Existing mainstream cloud service
    providers, including Amazon Web Services (AWS) [2], Google [3], and Alibaba [4],
    prevalently adopt virtualization technologies including virtual machines (VM)
    and containers in their distributed system infrastructures for automated application
    deployment. In recent years, their infrastructures are evolving from VM centric
    to container centric [5]. Containers employ a logical packing mechanism that binds
    both software and dependencies together for application abstraction [6]. Unlike
    VMs that support hardware-level resource virtualization where each VM has to maintain
    its own operating system (OS), containers virtualize resources at the OS level
    where all the containers share the same OS with less overhead. Therefore, containers
    are more lightweight in nature with high application portability, resource efficiency,
    and environmental consistency. They define a standardized unit for application
    deployment under an isolated runtime system. Thanks to these features, we have
    observed the prevalence of container technologies for automatic application deployment
    under diverse cloud environments. Following this trend of containerization, container
    orchestration techniques are proposed for the management of containerized applications.
    Different from the management of VM lifecycle via VM orchestration at large-grained
    control, container orchestration is the fine-grained and automated management
    process of a container lifecycle, including resource allocation, deployment, autoscaling,
    health monitoring, migration, load balance, security, and network configuration.
    For cloud service providers, which have to handle hundreds or thousands of containers
    simultaneously, a refined and robust container orchestration system is the key
    factor in controlling overall resource utilization, energy efficiency, and application
    performance. Under the surge of cloud workloads in terms of resource demands,
    bandwidth consumption, and quality of service (QoS) requirements, the traditional
    cloud computing environment is extended to fog and edge infrastructures that are
    close to end users with extra computational power [7]. Consequently, this also
    requires current container orchestration systems to be further enhanced in response
    to the rising resource heterogeneity, application distribution, workload diversity,
    and security requirements across hybrid cloud infrastructures. 1.1 Needs for Machine
    Learning-based Container Orchestration Considering the increasingly diverse and
    dynamic cloud workloads processed by existing cloud service providers, it is still
    unclear how to automate the orchestration process for complex heterogeneous workloads
    under large-scale cloud computing systems [8, 9]. Note that containers bear a
    resemblance to VMs from the perspective of orchestration. Machine-learning (ML)
    has been applied for orchestration of VMs. For example, early efforts in the use
    of reinforcement learning (RL) algorithms for auto-configuration of VMs can be
    seen in [10, 11, 12]. In traditional cloud computing platforms, Container Orchestrators
    are usually designed with heuristic policies without consideration of the diversity
    of workload scenarios and QoS requirements [13]. Their main drawbacks are listed
    as follows: (1) Most of these policies are static heuristic methods configured
    offline according to certain workload scenarios at a limited scale. For instance,
    threshold-based autoscaling strategies can only be suitable for managing a set
    of pre-defined workloads [6, 14, 15]. Such policies can not handle highly dynamic
    workloads where applications need to be scaled in/out at runtime according to
    specific behavior patterns (details and solutions are discussed in Sections 4.2.1
    and 4.3.2). (2) The performance of heuristic methods could dramatically degrade
    when the system scales up. For example, bin-packing algorithms, such as best fit
    or least fit algorithms, are widely utilized for solving task scheduling and resource
    allocation [16, 17, 18]. However, such algorithms could perform poorly with high
    task scheduling delays in large-scale compute clusters (details and solutions
    are discussed in Sections 4.3.1 and 4.3.3). (3) Resource contention and performance
    interference between co-located applications are usually ignored. Co-located applications
    could compete for shared resources, which may cause application performance downgrade,
    extra maintenance costs, and service-level agreement (SLA) violations [3, 4, 8]
    (details and solutions are discussed in Section 4.2.3). (4) The dependency structures
    between containerized application components are not considered during resource
    provisioning. Although some existing studies [19, 20, 21, 22] address this issue
    to a certain degree by leveraging machine learning methods, their ML models are
    only feasible for traditional cloud-based applications and relatively simple for
    containerized workload scenarios. For instance, containerized microservice-based
    applications are more lightweight and decentralized in nature, compared with traditional
    monolithic applications. There are internal connections between different microservice
    units within an application. Critical components in a microservice architecture
    are the dependencies of most other microservices and more likely to suffer from
    service level objectives (SLO) violations due to higher resource demands and communication
    costs [23, 24]. These factors should all be taken into account during resource
    provisioning (details and solutions are discussed in Section 4.2.4). (5) Current
    container orchestration methodologies mostly emphasize the evaluation of infrastructure-level
    metrics, while application-level metrics and specific QoS requirements are not
    receiving sufficient consideration. For example, containerized workloads may be
    attached with stricter time constraints compared with traditional cloud workloads,
    such as task deployment delays, task completion time, and communication delays
    [25, 26] (details and solutions are discussed in Section 4.2.2). With such fast-growing
    complexity of application management in cloud platforms, cloud service providers
    have a strong motivation to optimize their container orchestration policies by
    leveraging ML techniques [27]. Through applying mathematical methods to training
    data, ML algorithms manage to build sophisticated analytic models that can precisely
    understand the behavior patterns in data flows or impacts of system operations.
    Thus, ML approaches could be adopted for modeling and prediction of both infrastructure-level
    or application-level metrics, such as application performance, workload characteristics,
    and system resource utilization. These insights could further assist the Container
    Orchestrators to improve the quality of resource provisioning decisions. On the
    other hand, ML algorithms could directly produce resource management decisions
    instead of heuristic methods, offering higher accuracy and lower time overhead
    in large-scale systems [28, 29, 30, 31]. 1.2 Motivation of Research ML-based container
    orchestration technologies have been leveraged in cloud computing environments
    for various purposes, such as resource efficiency, load balance, energy efficiency,
    and SLA assurance. Therefore, we aim at investigating them in depth in this article:
    (1) The ML-based container orchestration technologies have shown promise in application
    deployment and resource management in cloud computing environments. Hence, we
    aim at outlining the evolution and principles of ML-based container orchestration
    technologies in existing studies. (2) We recognize the need for a literature review
    to address the status of ML-based container orchestration researches in such fast-evolving
    and challenging scenarios. Furthermore, we investigate and classify the relevant
    articles by their key features. Our goal is to identify potential future directions
    and encourage more efforts in advanced research. (3) Although an innovative review
    is proposed on container orchestration in Reference [13], the research on this
    field is growing continually by leveraging ML models. Therefore, there is a need
    for fresh reviews of machine learning-based container orchestration approaches
    to find out the advanced research challenges and potential future directions.
    1.3 Our Contributions The main contributions of this work are: (1) We introduce
    a taxonomy of the most common ML algorithms used in the field of container orchestration.
    (2) We present a comprehensive literature review of the state-of-the-art ML-based
    container orchestration approaches, and demonstrate the evolution of ML-based
    approaches used in container orchestration in recent years. (3) We classify the
    reviewed orchestration methods by their key characteristics and conditions. (4)
    We identify the future directions of ML-based container orchestration technologies.
    1.4 Related Surveys As summarized in Table 1, some previous surveys have already
    explored the field of application management in cloud computing environments while
    focusing on different research problems. Weerasiri et al. [32] introduce a comprehensive
    classification framework for analysis and evaluation of cloud resource orchestration
    techniques, while this work mainly focuses on resource provisioning perspectives
    without discussing ML-based approaches. Xu and Buyya [33] survey brownout technologies
    for adaptive application maintenance in cloud computing systems without discussing
    orchestration techniques. Duc et al. [34] look into the research challenge of
    resource management and performance optimization under edge-cloud platforms, with
    emphasis on workload modeling and resource management through ML techniques, where
    container orchestration is not their focus. Singh and Chana [35] depict a classification
    of QoS-aware autonomic resource provisioning under cloud computing environments
    through the methodical analysis of related research. However, this work is designed
    for managing general cloud workloads without adequate analysis of the key characteristics
    of containerized applications. Table 1. Ref. Application Deployment Unit Infrastructure
    Classfication Schemes Investigated Problem VM Container Cloud Fog Edge Application
    Architecure Behavior Modeling and Prediction Resource Provisioning ML-based Orchestration
    Strategies [32] ✓ ✓ ✓ ✓ How to design effective cloud orchestration techniques
    to cope with large-scale heterogeneous cloud? [33] ✓ ✓ ✓ ✓ How to apply brownout
    to support adaptive management of resources and applications in cloud? [34] ✓
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ How to employ machine learning techniques to achieve reliable resource
    provisioning in distributed computing environment? Show More Table 1. A Comparison
    of Our Work with Existing Surveys Based on Key Parameters Furthermore, some recent
    studies also investigate different scopes of container orchestration techniques.
    Rodriguez and Buyya [13] propose a systematic review and taxonomy of the mainstream
    container orchestration systems by classifying their features, system architectures,
    and management strategies. This work pays more attention to system modeling and
    design perspectives rather than detailed orchestration strategies. Casalicchio
    and Iannucci [5] conduct an extensive literature review of the state-of-the-art
    container technologies, focusing on three main issues, including performance evaluation,
    orchestration, and cyber-security. Pahl et al. [36] present a survey and taxonomy
    of cloud container technologies with a systematic classification of the existing
    researches. Nonetheless, the research direction of machine learning-based orchestration
    for containerized applications has not been explored with systematic categories
    in any existing survey and taxonomy. Compared to them, our article extends the
    previous surveys by focusing on how machine learning algorithms can be applied
    to solve complex research problems from a container orchestration perspective,
    such as multi-dimensional workload characterization or autoscaling in hybrid clouds.
    We emphasize the diversity and complexity of orchestration schemes under specific
    application architectures and cloud infrastructures. Furthermore, it also specifies
    the extensive research questions and potential future directions of machine learning-based
    container orchestration techniques. 1.5 Article Structure The rest of this article
    is organized as follows: Section 2 introduces the background of machine-learning
    and container orchestration. Section 3 describes an overview of machine learning-based
    container orchestration technologies, followed by the categories and taxonomy
    of the existing approaches. A description of the reviewed approaches and their
    mappings to the categories are presented in Section 4. Section 5 summarizes a
    discussion the future directions and open challenges. Finally, the conclusion
    of this work is given in Section 6. Skip 2BACKGROUND Section 2 BACKGROUND In this
    section, we present a comprehensive taxonomy of the existing ML models utilized
    in the area of container orchestration and a brief introduction of the high-level
    container orchestration framework. 2.1 Machine-Learning Machine-learning is a
    general concept describing a set of computing algorithms that can learn from data
    and automatically build analytical models through data analysis [37]. One of its
    fundamental objectives is to build mathematical models that can emulate and predict
    the behavior patterns of various applications through training data. Machine-learning
    has gained immense popularity through the past decades, widely accepted in many
    research areas such as image recognition, speech recognition, medical diagnose,
    and smart building [38, 39]. With the continuously growing computational power
    and adoption of GPUs in warehouse-scale datacenters, the capability and data scale
    of machine learning technologies are still soaring [40]. As depicted in Figure
    1, we design a machine learning taxonomy through classifying some of the most
    popular ML models in the field of container orchestration by their optimization
    objectives: Fig. 1. Fig. 1. Machine-learning taxonomy in the context of container
    orchestration by optimization objectives. (1) Regression algorithms predict a
    continuous output variable through analysis of the relationship between the output
    variable and one or multiple input variables. Popular regression algorithms (e.g.,
    support vector regression (SVR), random forest, and polynomial regression) are
    usually used to explore and understand the relation between different performance
    metrics. (2) Classification methods categorize training data into a series of
    classes. The use cases of classification mainly include anomaly detection and
    dependency analysis. For example, K-means is broadly adopted for the identification
    of abnormal system behaviors or components [41, 42], while support vector machine
    (SVM) can be leveraged for decomposing the internal structure of containerized
    applications and identifying key application components [24]. (3) Decision making
    models generate resolutions by simulating the decision making process and identifying
    the choices with the maximized cumulative rewards [43]. As the most common algorithms
    in this category, RL models, including model-free (e.g., Q-Learning and Actor-Critic)
    and model-based RL that belong to the reactive mechanism used to react to the
    current environment, have been widely employed for decision making in resource
    provisioning. (4) Time series analysis is the predictive approach that achieves
    pattern recognition of time series data and forecasts of future time series values
    or trends from past values. Ranging from autoregressive integrated moving average
    (ARIMA) to more advanced recurrent neural network (RNN) models, such algorithms
    are useful for behavior modeling of sequential data, including workload arrival
    rates or resource utilization. To be noted, some ML models can be utilized under
    multiple optimization scenarios. For example, artificial neural network (ANN)
    models can be applied for time series analysis of resource utilization or regression
    of application performance metrics [28, 44]. 2.2 Container Orchestration Container
    orchestration empowers cloud service providers to decide how containerized applications
    are configured, deployed, and maintained under cloud computing environments [5].
    It is targeted at automatic application deployment and dynamic configuration adjustment
    at runtime for a diverse range of workloads. Figure 2 demonstrates a reference
    architecture of machine learning-based container orchestration frameworks, where
    the components included are common to most existing systems. A detailed description
    of each layer is given as below: Fig. 2. Fig. 2. A high-level machine learning-based
    container orchestration framework reference architecture. 2.2.1 Application Workloads.
    Workloads are generally defined as the requests submitted by users to an application
    or the Orchestrator. Workloads could be classified into many categories from different
    perspectives according to their unique features, such as long-running services
    or short-living batch jobs (classified by execution time); computation-intensive,
    data-intensive, or memory-sensitive workloads (classified by resource usage pattern)
    [3, 4, 45]. Each workload is also defined with multi-dimensional resource requirements
    (e.g., CPU, memory, disk, network) and QoS requirements (e.g., time constraints,
    priorities, throughput). The extremely dynamic and unpredictable feature of heterogeneous
    workloads greatly increases the complexity of orchestration mechanisms. 2.2.2
    Compute Cluster. A compute cluster is a group of virtual or physical computational
    nodes that run in a shared network. As the core component in a containerized cluster,
    the Container Orchestrator is responsible for assigning containerized application
    components to worker nodes where containers are actually hosted and executed.
    Its major functionalities involve: (1) Resource Allocation assigns a specific
    amount of resources to a container. Such configurations and limitations are basic
    metrics in managing container placement and resource isolation control between
    containers. (2) Scheduling defines the policy for the initial placement for one
    or a group of containers, by considering conditions such as resource constraints,
    communication costs, and QoS requirements. (3) Scaling is the resource configuration
    adjustment of containerized applications or computational nodes in response to
    any potential workload fluctuations. (4) Migration is designed as a complementary
    mechanism to prevent severe infrastructure-level resource overloading or resource
    contention between co-located applications by relocating one or a group of containers
    from one node to another. (5) Infrastructure Monitoring keeps the track of infrastructure-level
    metrics of the cluster, such as the number of nodes, node states, resource usage
    of each node, and network throughput. Such metrics are the critical information
    for evaluating the health conditions of the cluster and making precise decisions
    in the above resource source provisioning layers. (6) Application Monitoring does
    not only periodically check the application states but also records their resource
    consumption and performance metrics (e.g., response time, completion time, throughput,
    and error rates). This information serves as the crucial reference data in anomaly
    detection and SLA violation measurement. (7) Load Distribution, as the core mechanism
    for load balancing under containerized environments, distributes network traffic
    between containers evenly with policies such as Round-Robin [46]. It plays an
    important role in improving system scalability, availability, and network performance.
    (8) Fault Tolerance ensures the high availability of the system through replica
    control. Each container is maintained with a configurable number of replicas across
    multiple nodes in case of a single point of failure. It is also possible to have
    multiple Orchestrators to deal with unexpected node crashes or system overloading.
    2.2.3 Infrastructure. Thanks to the high portability, flexibility, and lightweight
    nature of containers, it allows containers to be deployed across a multitude of
    infrastructures, including private clouds, public clouds, fog, and edge devices.
    Similar to traditional server farms, private clouds provide exclusive resource
    sharing within a single organization based on its internal datacenter [1]. By
    contrast, public cloud service providers support on-demand resource renting from
    their warehouse-scale cloud datacenters. Since applications and workloads are
    all deployed and processed at cloud data centers in traditional cloud computing,
    this requires a massive volume of data transferred to cloud servers. As a consequence,
    it leads to significant propagation delays, communication costs, bandwidth, and
    energy consumption. Through moving computation and storage facilities to the edge
    of a network, fog and edge infrastructures can achieve higher performance in a
    delay-sensitive, QoS-aware, and cost-saving manner [47, 48]. With the requests
    or data directly received from users, fog or edge nodes (e.g., IoT-enabled devices,
    routers, gateways, switches, and access points) can decide whether to host them
    locally or send them to the cloud. 2.2.4 Machine Learning-based Optimization Engine.
    An ML-based optimization engine builds certain ML models for workload characterization
    and performance analysis, based on analysis of monitoring data and system logs
    received from the Orchestrator. Furthermore, it can produce future resource provisioning
    decisions relying on the generated behavior models and prediction results. The
    engine can be entirely integrated or independent from the Orchestrator. Its major
    components are listed as follows: (1) Workload Modeler is designed for ML-based
    workload characterization through analyzing the input application workloads and
    identifying their key characteristics. (2) Performance Analyzer generates application
    and system behavior models through applying ML algorithms to application and infrastructure-level
    monitoring data acquired from the Orchestrator. (3) Predictor forms forecasts
    of workload volumes or application/system behaviors, relying on the models obtained
    from Workload Modeler and Performance Analyzer. The prediction results could be
    sent either to the Orchestrator or Decision Maker. (4) Decision Maker combines
    the behavior models and prediction results received from the above components
    with certain ML-based optimization methods/schemes to further generate precise
    resource provisioning decisions that are fed back to the Orchestrator. These components
    are common to most container orchestration systems; however, not all of them have
    to be implemented to make the whole system functional. According to different
    design principles, the engine can alternatively produce multi-level prediction
    results to assist the original Orchestrator in making high-quality decisions in
    resource provisioning, or directly generate such decisions instead of the Orchestrator.
    Skip 3TAXONOMY OF MACHINE LEARNING-BASED CONTAINER ORCHESTRATION TECHNOLOGIES
    Section 3 TAXONOMY OF MACHINE LEARNING-BASED CONTAINER ORCHESTRATION TECHNOLOGIES
    Figure 3 presents a taxonomic classification of the literature reviewed in our
    work. Application Architecture describes the behaviors and internal structures
    of containerized application components that together perform as a whole unit.
    Infrastructure indicates the environments or platforms where the applications
    operate. Objectives are the improvements that the proposed ML-based approaches
    attempt to achieve. Behavior Modeling and Prediction leverage ML models for pattern
    recognition and simulation of system and application behaviors, as well as forecasting
    future trends according to previously collected data. Resource Provisioning specifies
    the ML-based or heuristic policies for resource management of containerized applications
    at different phases in a container lifecycle under various scenarios. Fig. 3.
    Fig. 3. Machine learning-based container orchestration taxonomy. 3.1 Application
    Architecture This section presents the most common application architectures that
    define the composition of containerized applications and how they are deployed,
    executed, and maintained. The application architecture also decides the minimum
    orchestration unit that the ML-based approaches will operate. 3.1.1 Monolithic.
    Monolithic applications follow an all-in-one architecture where all the functional
    modules are developed and configured into exactly one deployment unit, namely,
    one container. Such applications could be initially easy to develop and maintain
    at a small scale. For example, Microsoft Azure [49] still supports automated single-container-based
    application deployment for enterprize solutions where the business logic is not
    feasible for building complex multi-component models. However, the consistent
    development and enrichment of monolithic applications would inevitably lead to
    incremental application sizes and complexity [50]. Consequently, the maintenance
    costs can dramatically grow in continuous deployment. Even modification of a single
    module requires retesting and redeployment of the whole application. Furthermore,
    scaling of monolithic applications means replication of the entire deployment
    unit containing all the modules [51]. In most scenarios, only a proportion of
    the modules need to be scaled due to resource shortage. Therefore, scaling the
    whole application would lead to poor overall resource efficiency and reliability.
    Thus, the monolithic architecture is only suitable for small-scale applications
    with simple internal structures. 3.1.2 Microservice. To address the problem of
    high development and maintenance costs caused by colossal application sizes, the
    microservice architecture (MSA) is proposed to split single-component applications
    into multiple loosely coupled and self-contained microservice components [52].
    An example of MSA is given in Figure 4(a). Each microservice unit can be deployed
    and operated independently for different functionalities and business objectives.
    Furthermore, they can interact with each other and collaborate as a whole application
    through lightweight communication methods such as representational state transfer
    (REST). Through decomposing applications into a group of lightweight and independent
    microservice units, MSA has significantly reduced the costs and complexity of
    application development and deployment. Nonetheless, the growing number of components
    and dynamic inter-dependencies between microservices in MSA raises the problem
    of load distribution and resource management at the infrastructure level. A well-structured
    orchestration framework for MSA should be able to maintain multiple parts of an
    application with SLA assurance, granting more control over individual components.
    These challenges can be addressed by ML-based approaches, for example, utilizing
    the ML-based approach to analyze dependencies between different microservices
    and resource usage of isolated microservices. Fig. 4. Fig. 4. Examples of the
    architectures. 3.1.3 Serverless. The serverless architecture defines an event-driven
    application paradigm that undertakes stateless computational tasks, namely, serverless
    functions. Designed to perform certain user-defined functionalities, functions
    are usually short pieces of code hosted in function containers with specific configurations
    and limited execution time. To ensure the successful completion and performance
    requirements of functions, serverless platforms are responsible for managing the
    function execution environments in terms of resource provisioning, energy consumption,
    SLA assurance, and security [53, 54]. Compared with the microservice that tends
    to support continuous requests with long-running lifecycles, the serverless is
    the event-driven handler for ephemeral tasks with finer granularity. As depicted
    in Figure 4(b), a typical serverless application is represented in the form of
    a function chain consisting of a workflow of individual functions. Within a function
    chain, interactions and transitions between functions are conducted through a
    centralized messaging service, such as a message queue or event bus [55]. The
    orchestration problem under such context translates into managing the invocation
    of function chains in terms of function initialization, execution, transition,
    and data flow control. Several previous studies have proved that the significant
    time overhead of function initialization is currently the major performance bottleneck
    in function invocation [56, 57]. Therefore, the current research focus of ML-based
    solutions in this field is on minimization of function invocation delays and resource
    wastage [58, 59, 60]. Currently, as there is no prevailing technology (e.g., containers
    for MSA) to support the handlers in the serverless platform, the serverless utilizes
    containers to realize their platform, which may experience performance slowdown
    [61]. 3.2 Infrastructure A cloud infrastructure consists of a set of hardware
    resources and virtualization software to deliver virtualized resources to users
    for application deployment. The execution of ML-based approaches also needs to
    consume the resources provisioned by infrastructure. In general, we have identified
    three types of cloud infrastructures in the context of container orchestration:
    (1) A single cloud environment is built on resources from only one cloud service
    provider (either a private or public cloud) to host and serve all the applications.
    (2) A multi-cloud environment includes multiple cloud services (e.g., private
    clouds, public clouds, or a mixture of both). As different cloud service providers
    may differ in many aspects, such as resource configurations, price, network latency,
    and geographic locations, this allows more choices for optimization of application
    deployment. (3) A hybrid cloud environment is composed of a mixture of private
    clouds, public clouds, fog, or edge devices. It is not always efficient to deploy
    all the applications and data to public/private clouds, considering the data transmission
    time and network latency from end users to cloud servers. To solve this issue,
    the hybrid cloud enables applications and data to be deployed and processed at
    fog or edge devices that are close to end users. 3.3 Optimization Objectives In
    light of the diversity of application architectures and cloud infrastructures,
    many types of metrics have been considered as optimization objectives during the
    behavior modeling and resource provisioning of containerized applications. As
    shown in Figure 3, we group the existing objective metrics into four major categories.
    Since an orchestration solution usually needs to achieve multiple objectives according
    to specific user requirements, balancing the tradeoff between different optimization
    objectives remains a key concern in automated application deployment and maintenance.
    (1) Resource Efficiency. Infrastructure-level resource usage metrics are usually
    treated as key application performance indicators for energy and cost efficiency.
    They are the fundamental data source of most behavior modeling schemes, such as
    prediction of the resource demands of coming workloads, or discovery of the relationship
    between resource usage patterns and other performance metrics. Such insights could
    be further applied in decision making of resource provisioning to improve the
    overall resource efficiency and application performance. (2) Energy Efficiency.
    Under the continuously growing scale of cloud data centers, the tremendous electricity
    usage consumed by cloud infrastructures has emerged as a critical concern in the
    field of cloud computing [62]. Therefore, various approaches have been proposed
    to minimize energy consumption and optimize energy efficiency [33, 41, 58, 63].
    As the overall electricity usage of a system is estimated as the summation of
    the consumption by each physical machine (PM) where its energy usage is directly
    related to its resource utilization, an essential way to control energy efficiency
    is to adjust and balance the resource utilization of physical machines during
    resource provisioning. (3) Cost Efficiency. Following the pay-as-you-go payment
    model of mainstream cloud service providers, market-based solutions regard cost
    efficiency as one of their principal targets [6, 15, 29, 64]. Through evaluation
    and selection of diverse cloud services according to their pricing models and
    computing capability, an optimized orchestration solution aims at minimizing the
    overall financial costs while satisfying the QoS requirements defined by users.
    (4) SLA Assurance. Containerized applications are mostly configured with specific
    performance requirements, such as response time, initialization time, completion
    time, and throughput. These constraints are mostly expressed as SLA contracts,
    while their violations could lead to certain penalties. Because of the dynamic
    and unpredictable feature of cloud workloads, autoscaling is usually leveraged
    to automate application maintenance with SLA assurance [8, 28, 29, 60, 65, 66,
    67], in response to the frequently changing workloads. 3.4 Behavior Modeling and
    Prediction Behavior modeling is a fundamental step in understanding the overall
    application or system behavior patterns through analysis of application/infrastructure-level
    metrics. The variety of multi-layer metrics related to workloads, application
    performance, and system states significantly complicates the modeling process.
    Nonetheless, a well-structured behavior model that can produce precise prediction
    results based on ML approaches, which is also apparently useful for achieving
    certain optimization objectives during orchestration. The behavior modeling and
    prediction can also provide important information for the decision making in the
    resource provisioning phase. (1) Workload characterization captures the key features
    of application workloads. Because of the dynamic and decentralized nature of containerized
    applications like microservices, the received workloads may differ in many ways,
    such as task structures, resource demands, arrival rates, and distributed locations.
    These factors make it hard to define a robust method for characterization and
    categorization of all the different workloads within an orchestration system.
    However, the knowledge of workload behaviors is necessary to improve the quality
    of resource provisioning decisions by making precise resource assignments in response
    to any incoming workloads. (2) Performance analysis discovers the relation among
    infrastructure (e.g., resource utilization and energy consumption) or application-level
    (e.g., response time, execution time, and throughput) metrics to depict the system
    states and application performance. These insights are important in managing the
    tradeoff between different optimization objectives. (3) Anomaly detection classifies
    and identifies abnormal system behaviors, including security threats, instance
    failure, workload spikes, performance downgrade, and resource overloading. Such
    anomalies could severely harm the system availability and reliability. Therefore,
    fast and accurate localization of their root causes could prevent SLA violations
    or system crashes. (4) Dependency analysis looks into the graph-based internal
    dependencies between containerized application components. It helps to understand
    the workload distribution/pressure among application components and make precise
    resource configurations. Since the dependencies may be dynamically updated at
    runtime, it requires an incremental model that can consistently adjust itself
    and address the chain reactions of individual components to the overall application
    performance. 3.5 Resource Provisioning Considering the various application architectures,
    infrastructures, and optimization objectives discussed in the above sections,
    resource provisioning for containerized applications has become much more challenging.
    The diversity of cloud workloads, resource heterogeneity within hybrid cloud environments,
    and complex application internal structures should be all assessed during resource
    provisioning. Therefore, the state-of-the-art resource provisioning strategies
    are commonly relying on the decisions on the amount of provisioned resources,
    which can exploit ML-based approaches to achieve higher accuracy and shorter computation
    delays. Based on our investigation, the Resource Provisioning category in Figure
    3 can be classified into more detailed categories, we provide Figure 5 to complement
    Figure 3, where the resource provisioning operations can be further classified
    into the following categories: Fig. 5. Fig. 5. Resource provisioning taxonomy.
    (1) Scheduling decides the initial placement of a containerized task unit, which
    could consist of a single task, a group of independent tasks, or a group of dependent
    tasks in graph-based structures. Due to the variety of application architectures
    and task structures, application deployment policies should consider a wide range
    of placement schemes. The quality of scheduling decisions has a direct impact
    on the overall application performance and resource efficiency [9]. (2) Scaling
    is the size adjustment of containerized applications or computational nodes in
    response to any potential workload fluctuations, which ensures the applications
    supported with enough resources to minimize SLA violations. Horizontal scaling
    adjusts the number of container replicas belonging to the applications or the
    number of nodes. By contrast, vertical scaling only updates the amount of resources
    assigned to existing containers or nodes. Moreover, hybrid scaling combines both
    horizontal and vertical scaling to produce an optimized solution. (3) Migration
    is the relocation of one or a group of tasks from one node to another. When resource
    contention, overloading, or underloading occur between co-located applications
    due to poor scheduling decisions, rescheduling is triggered for load balancing
    through task migration within a single cloud. On the other hand, computation offloading
    manages the migration of computation-intensive tasks that are experiencing resource
    bottlenecks to devices with enough demanded resources across hybrid cloud environments.
    Targeted to improve the application performance, a refined offloading policy should
    be able to pick a suitable relocation destination that minimizes the migration
    costs in terms of execution time, bandwidth, and energy consumption. 3.6 Evolution
    of Machine Learning-based Container Orchestration Technologies The optimization
    of the objectives and metrics of ML-based approaches for container orchestration
    has been investigated and multiple methods have been proposed over the years.
    To show the evolution and development of ML-based approaches for container orchestration
    in recent years. Figure 6 demonstrates the evolution of ML-based models since
    2016, with emphasis on their objectives and metrics. As the research related to
    machine-learning for container orchestration starts from 2016, our examination
    for the evolution falls between 2016 and 2021. Fig. 6. Fig. 6. Evolution of machine
    learning-based container orchestration technologies. In 2016, the ARIMA [68] and
    nearest neighbor (NN) [18] algorithms were already leveraged for resource utilization
    prediction of containerized applications. ARIMA is a dynamic stochastic process
    proposed in the 1970s, which has been used for forecasting time series showing
    non-stationarity by identifying the seasonal differences. NN is a proximity search
    approach that can find a candidate that is closest to a given point. As ARIMA
    and NN have been used widely in time series prediction, they were firstly applied
    in container orchestration to predict resource utilization, such as CPU, memory,
    and I/O. However, at this stage, the application models were relatively simple,
    which only considered the time series pattern of infrastructure-level resource
    metrics. In 2017, Shah et al. [69] first adopted the long short-term memory (LSTM)
    model for dependency analysis of microservices. The LSTM is an approach based
    on neural network and well suited to classifying, processing, and forecasting
    based on time series data. Compared with traditional feedforward neural network,
    LSTM also has feedback connections to enhance its performance, which has functioned
    well in handwriting recognition and speech recognition. The model in [69] evaluated
    both the internal connections between microservice units and the time series pattern
    of resource metrics. Furthermore, anomaly detection was built on top of the LSTM
    model for the identification of abnormal behaviors in resource utilization or
    application performance. Besides, Cheng et al. [70] used Gradient Boosting Regression
    (GBR) that can ensemble multiple weak prediction models (e.g., regression trees)
    to form a more powerful model, and applied GBR for resource demand prediction
    in workload characterization. A model-free RL method, namely, Q-Learning, is also
    used for scaling microservices. Q-learning works by learning the action-value
    function to evaluate the reward of taking an action in a particular state. The
    benefit of Q-learning is that it can achieve the expected reward without the model
    of the environment. Xu et al. [29] leveraged Q-Learning to produce vertical scaling
    plans. An optimal scaling decision was targeted to minimize resource wasting and
    computation costs under the assumption of SLA assurance. In 2018, Tang et al.
    [71] employed the bidirectional LSTM (Bi-LSTM) model for the prediction of workload
    arrival rates and application throughput. Their training module had demonstrated
    significant accuracy improvement over ARIMA and LSTM models in terms of time series
    prediction. Ye et al. [44] applied a series of traditional regression methods
    based on the statistical process for relationship estimation, including SVR, linear
    regression (LR), and modern ANN based on deep learning, to conduct performance
    analysis of relevant resource metrics. They attempted to evaluate the relationship
    between resource allocation and application performance. However, only single-component
    applications with limited performance benchmarks were considered within the scope
    of their work. Du et al. [42] designed an anomaly detection engine composed of
    traditional machine learning methods including k-nearest neighbors (KNN), SVM,
    Naive Bayes (NB), and random forest (RF), to classify and diagnose abnormal resource
    usage patterns in containerized applications. Orhean et al. [25] utilized state–action–reward–state–action
    (SARSA), a model-free RL algorithm similar to Q-learning, to manage the graph-based
    task scheduling problem in directed acyclic graph (DAG) structures, aiming at
    minimizing the overall DAG execution time. In 2019, Cheng et al. [72] proposed
    a hybrid gated recurrent unit (GRU) model to further reduce the computational
    costs and error rates of resource usage prediction of cloud workloads. GRU is
    considered as an optimized model of LSTM [73], as LSTM models are relatively complex
    with high computational costs and data processing time. An LSTM cell structure
    consists of three gates, including the input gate, the forget gate, and the output
    gate. GRU simplifies this structure and achieves higher computational efficiency
    by integrating the input gate and the forget gate into one update gate. Performance
    analysis of containerized applications was also explored in depth. Venkateswaran
    and Sarkar [16] leveraged the K-means clustering and polynomial regression (PR)
    to classify the multi-layer container execution structures under multi-cloud environments
    by their feasibility to the application performance requirements. Both K-means
    and polynomial regression can find groups that have not been explicitly labeled
    in the data, which are originally used in signal processing and applied to identify
    the execution structures of containers. According to workload arrival rates and
    resource metrics, Podolskiy et al. [74] applied Lasso regression (LASSO) to forecast
    service level indicators (SLI) in terms of application response time and throughput
    for Kubernetes private cloud. LASSO can generate a linear model via variable selection
    and regularization to improve prediction accuracy. Dartois et al. [75] used the
    decision tree (DT) regression algorithm to analyze the solid state drive (SSD)
    I/O performance under interference between applications. The DT can break down
    the dataset into small subsets and an associated decision tree can be incrementally
    generated, which is easy to interpret, understand and virtualize. In addition,
    DT is not very sensitive to outliers or missing data, and it can handle both categorical
    and numerical variables. As for resource provisioning, deep reinforcement learning
    (DRL) was first applied in the context of task scheduling [26, 31]. Bao et al.
    [26] designed a DRL framework for the placement of batch processing jobs, where
    an ANN model represented the mapping relationship between workload features, system
    states, and corresponding job placement decisions. The Actor-Critic RL algorithm
    was selected to train the ANN model and generate optimal scheduling decisions
    that minimized the performance interference between co-located batch jobs. Compared
    with traditional heuristic scheduling policies like bin packing, their solution
    demonstrated remarkable performance improvement on the Kubernetes cluster regarding
    overall job execution time. Moreover, DRL was also employed to solve the problem
    of computation offloading under fog-cloud environments in Reference [31]. On top
    of a Markov decision process (MDP) model that simulated the interactions of the
    offloading process at a large scale, the deep Q-Learning method optimized the
    migration decisions by minimization of the time overhead, energy usage, and computational
    costs. To explore the efficiency of hybrid scaling mechanisms, Rossi et al. [76,
    77] leveraged model-based RL models to compose a mixture of horizontal and vertical
    scaling operations for monolithic applications, aiming at minimizing the resource
    usage, performance degradation, and adaption costs. In 2020, several RL-based
    scaling approaches were proposed in the form of hybrid ML models [24, 28, 30].
    Qiu et al. [24] adopted the SVM model for dependency analysis of microservices
    and recognition of the key components that are highly likely to experience resource
    bottlenecks and performance downgrade. To prevent severe service level objectives
    (SLO) violations, the Actor-Critic method was utilized to generate the appropriate
    resource assignment decisions for these components through horizontal scaling.
    The approach has been validated and executed on the Kubernetes cluster with significant
    performance improvement compared with Kubernetes’s autoscaling approach. Besides,
    Sami et al. [30] combined MDP and SARSA models to build a horizontal scaling solution
    for monolithic applications under fog-cloud environments. SARAS produced the optimized
    scaling decisions through model training relying on the MDP model that simulated
    the scaling scenarios with the fluctuating workloads and resource availability
    in fog taken into account. In 2021, Zhang et al. [23] proposed a novel approach
    composed of a convolutional neural network (CNN) and boosted trees (BT) for dependency
    and performance analysis of microservices. Their CNN model did not only analyze
    the inter-dependencies between microservice units for system complexity navigation,
    but also the time series metrics related to application performance. Furthermore,
    the BT model is responsible for the prediction of long-term QoS violations. To
    further improve the speed and efficiency of RL-based scaling approaches for microservices
    under hybrid cloud environments, Yan et al. [78] developed a multi-agent parallel
    training module based on SARSA and improved the horizontal scaling policy of Kubernetes,
    supported by the microservice workload prediction results generated by Bi-LSTM.
    Overall, diverse ML algorithms have been utilized in the context of container
    orchestration, ranging from workload modeling to decision making through RL. However,
    there are not many new ML models adopted in the area of container orchestration
    in recent years. To further improve prediction accuracy and computational efficiency,
    the emerging trend of hybrid ML-based solutions targets to combine multiple existing
    ML methods to form a complete orchestration pipeline, including multi-dimensional
    behavior modeling and resource provisioning. The evolution of ML models also contributes
    to the extension of various application architectures and cloud infrastructures.
    Skip 4State-of-the-Art in Machine Learning for Orchestration of Containers Section
    4 State-of-the-Art in Machine Learning for Orchestration of Containers In this
    section, we introduce a literature review of machine learning-based container
    orchestration approaches. To stress the key features in the evaluated studies,
    we use the taxonomy in Section 3 to outline the key characteristics of the approaches
    designed for behavior modeling and prediction, as well as resource provisioning.
    4.1 Article Selection Methodology In this subsection, we introduce the approach
    we followed to reach the articles. The relevant articles have been broadly searched
    in the mainstream academic sources, including ACM Digital Library, IEEEXplore,
    Springer, Elsevier, ScienceDirect, Wiley Interscience, and Google Scholar. We
    focus the search criteria on titles and abstracts, starting with the following
    keywords: container orchestration, application component, microservice, serverless,
    machine-learning, deep learning, characterization, dependency analysis, anomaly
    detection, classification, prediction, monitoring, scheduling, scaling, migration,
    and load distribution. As there are many irrelevant articles in the search results,
    we further refine the results through secondary filtering and reviewing based
    on relevance and quality. Eventually, 44 research articles are selected in the
    field of machine learning-based container orchestration technologies. The study
    distribution by publication sources includes 64% conference papers, 24% journal
    articles, and 12% symposium articles. The articles discussed in each class are
    key representative works selected from literature by several evaluation criteria,
    including robustness of application models, coverage of critical challenges in
    the field, accurate depiction of real-world orchestration scenarios, high scalability
    under complex cloud environments, novelty of orchestration approaches, refined
    combinations of ML-based models, and extensibility to hybrid clouds. 4.2 Behavior
    Modeling and Prediction This section presents the approaches related to behavior
    modeling and prediction. 4.2.1 Workload Characterization. The knowledge of workload
    characteristics and behavior patterns offers important reference data for the
    estimation of resource provisioning and load distribution. Table 2 shows the existing
    studies that leverage ML techniques in workload characterization. Table 2. Ref.
    Mechanism Infrastructure Application Architecture Methods Objectives Advantages
    Limitations [58] Time series analysis Single cloud Serverless LSTM Request arrival
    rate prediction High prediction accuracy Simplicity of application models [8]
    Classification Single cloud Monolithic K-means++ Resource demand prediction High
    scalability Limited accuracy under high load variance [28] Time series analysis
    Single cloud Monolithic ARIMA Request arrival rate prediction Capability of large
    data scales Inaccuracy under trend turning Show More Table 2. Summary of Workload
    Characterization Approaches Many previous studies have tried to model the time
    series pattern of request arrival rates in containerized applications through
    various algorithms. ARIMA, as a classic algorithm for analyzing time series data,
    was utilized in Reference [28]. Compared with other linear models that are mainly
    suitable for linear datasets such as autoregressive (AR), moving average (MA),
    autoregressive moving average (ARMA), and exponentially weighted moving average
    (EWMA), ARIMA enjoys high accuracy for large-scale datasets and even under unstable
    time series by using a set of lagged observations of time series. However, as
    the workload scenario of ARIMA is limited to linear models, it is usually referenced
    as a baseline approach by many studies included in our literature review. To further
    speed up the data training process, LSTM models are adopted in References [58,
    67, 81] to predict the request arrival rates under large dynamic variations and
    avoid unnecessary resource provisioning operations. Unlike general feedforward
    neural networks, LSTM has a more complicated structure with feedback connections
    that can improve prediction accuracy. It produces prediction results by analyzing
    the whole data sequence and is more accurate at identifying new patterns. However,
    LSTM models only train the data in one direction. Bi-LSTM models can overcome
    this limitation by processing the data sequence from both forward and backward
    directions. Therefore, Bi-LSTM models are proposed in References [71, 78] to capture
    more key metrics and improve the prediction accuracy. Another direction in ML-based
    workload characterization is resource demand modeling and prediction. Zhong et
    al. [8] leverage the K-means++ algorithm for task classification and identification
    based on the resource usage (e.g., CPU and memory) patterns of different workloads.
    This advantage of K-means based approach is scalable for multiple types of tasks;
    however, the accuracy is undermined under loads with high variance. Zhang et al.
    [18] introduce the Time Series Nearest Neighbor Regression (TSNNR) algorithm for
    prediction of future workload resource requirements by matching the recent time
    series data trend to similar historical data, which can improve the prediction
    accuracy compared with simple NN approach due to the consideration of time series.
    However, it is constrained to limited workload scenarios. To enhance the ARIMA
    model in the analysis of the nonlinear relationship and trend turning points within
    data sequences, Xie et al. [79] apply ARIMA with triple exponential smoothing
    (ARIMA-TES) in the prediction of container workload resource usage with multi-dimension,
    which can achieve high robustness and accuracy but bring higher time overhead
    compared with ARIMA. Besides, A hybrid association learning architecture is designed
    in Reference [80] through combining the LSTM and Bi-LSTM models. A multi-layer
    structure is built to find the inter-dependencies and relationship between various
    resource metrics, which are generally classified into three distinct groups, including
    CPU, memory, and I/O. To further reduce the error rates of resource usage prediction
    and data training time, GRU-based models are utilized due to their high computational
    efficiency and prediction accuracy. Lu et al. [82] develop a hybrid prediction
    framework consisting of a GRU and a straggler detection model (IGRU-SD) to predict
    the periodical resource demand patterns of cloud workloads on a long-term basis.
    Likewise, Cheng et al. [72] propose a GRU-ES model where the exponential smoothing
    method is used to update the resource usage prediction results generated by GRU
    and reduce prediction errors. Generally, these hybrid solutions can improve the
    performance of a single technique; however, due to the complex architecture, it
    is more difficult to analyze the contributions of different variables. In summary,
    the majority of the reviewed articles in workload characterization have focused
    on the analysis and prediction of request arrival rates and resource usage patterns
    through time series analysis or regression models. The researchers can benefit
    from the investigation in this section by selecting specific techniques according
    to their objectives and techniques’ advantages. For example, given one research
    work aims at predicting resource demand with low error rates, it can utilize GRU-based
    approach in [58], and it can improve the existing work by using more comprehensive
    application models. If another work plans to forecast requests arrival rate with
    high accuracy, the Bi-LSTM approaches in [71] and [78] can be the candidates.
    However, the researchers need to overcome the inaccuracy in long-term prediction.
    4.2.2 Performance Analysis. Performance analysis captures the key infrastructure
    and application-level metrics for evaluation of the overall system status or application
    performance. A summary of the ML-based performance analysis approaches is given
    in Table 3. Table 3. Ref. Mechanism Infrastructure Application Architecture Methods
    Objectives Advantages Limitations [59] Time series analysis, regression Single
    cloud Serverless LSTM, LR Prediction of function invoking time Online prediction
    of function chains Additional resource consumption for model training [83] Regression
    Hybrid cloud Serverless GBR Prediction of costs and end-to-end latency High prediction
    accuracy High computational expenses [84] Regression Hybrid cloud Serverless BO
    Prediction of costs and execution time based on function configurations High prediction
    accuracy High computational complexity Show More Table 3. Summary of Performance
    Analysis Approaches Das et al. [83] design a performance modeler based on GBR
    for predicting the costs and end-to-end latency of input serverless functions
    based on their configurations under edge-cloud environments. Such metrics are
    the key factors in the estimation of the efficiency of function scheduling decisions.
    Similarly, Akhtar et al. [84] leverage the Bayesian Optimization (BO) function
    to achieve the same purpose. The prediction results will be used to estimate the
    optimal function configurations that meet the time constraints in function deployment
    with the lowest costs. Both the GBR and BO can provide high prediction accuracy,
    where the GBR is through the optimization of different loss functions and BO is
    via the direct search based on Bayes Theorem to find the best results of the objective
    function. However, both of these approaches suffer from high computational expenses.
    To estimate the cold start latency in serverless computing platforms, Xu et al.
    [59] propose a two-phase approach. LSTM is used in the first phase to predict
    the invoking time of the first function in a function chain, while the rest of
    the functions is processed through LR. Although this two-phase approach consumes
    additional resources for model training, it achieves a significant reduction of
    prediction error rates and resource wasting in the container pool. Venkateswaran
    and Sarkar [16] try to classify the cloud service providers and container cluster
    configurations under multi-cloud environments through a two-level approach. K-means
    is employed in the first level to precisely classify the comparable container
    cluster compositions by their performance data, while PR is applied in the second
    level for analyzing the relationship between container strength and container
    system performance. The disadvantage of this work is that the evaluated application
    model only contains a limited number of microservices. Some research works have
    investigated the issue of infrastructure-level resource utilization prediction
    [28, 63, 75]. Zhang et al. [28] leverage the ANN model to predict the CPU utilization
    and request response time by modeling a series of metrics, including CPU and memory
    usage, response time, and the request arrival rates mentioned in Section 4.2.1.
    Besides, Dartois et al. [75] look into the research topic of SSD I/O performance
    modeling and interference prevention with a series of regression techniques, including
    boosting, RF, DT, and Multivariate adaptive regression splines (MARS). As SSDs
    are prevalently used by large-scale data center for data storage due to their
    high performance and energy efficiency, their internal mechanisms could directly
    impact application-level behaviors and cause potential SLO violations. Compared
    with the ANN approach, these regression-based approaches can obtain results within
    a shorter data training time, while the prediction accuracy is not as good as
    ANN. On the other hand, some previous studies focus on behavior modeling of application-level
    metrics [23, 66, 74, 85]. RScale [66] is implemented as a robust scaling system
    with high computational complexity leveraging Gaussian process (GP) regression
    for investigation of the interconnections between end-to-end tail latency of microservice
    workloads and internal performance dependencies. Likewise, Sinan [23], as an ML-based
    and QoS-aware cluster manager for containerized microservices, combines the CNN
    and BT model for the prediction of end-to-end latency and QoS violations. Taking
    both the microservice inter-dependencies and the time series pattern of application-level
    metrics into account, Sinan evaluates the efficiency of short-term resource allocation
    decisions as well as long-term application QoS performance. In most cases, the
    approach functions well except overfitting and misprediction may happen occasionally.
    Overall, most studies in this section are concerned with prediction of time constraints
    or resource usage patterns through regression or time series analysis techniques.
    4.2.3 Anomaly Detection. Anomaly detection is a critical mechanism for identifying
    abnormal behaviors in system states or application performance. Table 4 describes
    the reviewed approaches regarding anomaly detection. Table 4. Ref. Mechanism Infrastructure
    Application Architecture Methods Objectives Advantages Limitations [41] Classification
    Hybrid cloud Monolithic K-means, ensemble, and hierarchical Identification of
    overloaded or underloaded nodes Short data training time Limited workload scenarios,
    high space complexity [88] Classification Single cloud Monolithic K-means, KNN,
    and self-organizing map Container vulnerability detection High detection accuracy
    Insufficient anomaly cases [42] Classification Single cloud Microservice KNN,
    SVM, NB, and RF Anomaly detection of microservices according to real-time performance
    metrics Various monitoring metrics Insufficient evaluation of application-level
    anomalies Show More Table 4. Summary of Anomaly Detection Approaches Due to the
    application-centric and decentralized features of containers, they are more likely
    to experience a wide range of security threats, which may lead to potential propagation
    delays in container image dependency management or security attacks [90]. Tunde-Onadele
    et al. [88] classify 28 container vulnerability scenarios into six categories
    and develop a detection model, including both dynamic and static anomaly detection
    schemes with KNN, K-means, self-organization map algorithms, which could reach
    detection coverage up to 86%. High detection accuracy has been achieved by this
    approach; however, the investigated anomaly cases are not comprehensive. To balance
    the energy efficiency and resource utilization of a containerized computing system
    under hybrid cloud environments, Chhikara et al. [41] employ K-means, hierarchical
    clustering algorithms, and ensemble learning for identification and classification
    of underloaded and overloaded hosts. Further container migration operations will
    be conducted between these two groups for load balancing and energy consumption
    reduction. The limitations are that this hybrid solution leads to a large solution
    space and only considers limited workload scenarios. Shah et al. [69] extend the
    LSTM model to analyze the long-term dependencies of real-time performance metrics
    among microservice units. Relying on the static models they constructed, they
    manage to identify critical performance indicators that support anomaly detection
    of various application and infrastructure-level metrics, including network throughput
    and CPU utilization. 4.2.4 Dependency Analysis. Table 5 summarizes the recent
    studies in service dependency analysis of containerized applications. As serverless
    function chains or workflows are usually pre-defined by users [91], current ML-based
    dependency analysis solutions only focus on decomposing the internal structures
    of microservice units and monitoring any dynamic structural updates. Table 5.
    Ref. Mechanism Infrastructure Application Architecture Methods Objectives Advantages
    Limitations [86] Regression Multi-cloud Microservice BO, GP Discovery of optimal
    route of individual microservice Load balance Poor performance under highly dynamic
    environment [24] Classification Single cloud Microservice SVM Identification of
    potential heavy-loaded microservice units High accuracy Implicit explanation of
    the time overhead and computational costs [23] Classification Single cloud Microservice
    CNN Dependency analysis of microservices between pipelined-tiers Navigation of
    system complexity Overfitting and misprediction Show More Table 5. Summary of
    Dependency Analysis Approaches SVM is utilized by Qiu et al. [24] to find the
    microservice units with higher risks causing SLO violations through analysis of
    the performance metrics related to the critical path (CP) of each individual microservice.
    CP is defined as the longest path between the client request and the underlying
    microservice in the execution history graph. Since CP can change dynamically at
    runtime in response to potential resource contention or performance interference,
    the SVM classifier is implemented with incremental learning for dependency analysis
    in a dynamic and consistent manner. However, the relationship between the time
    overhead and computational costs is not detailed discussed. Load balancing between
    microservices under the multi-cloud environment could be rather complex, because
    of the unstable network latency, dynamic service configuration, and fluctuating
    application workloads. To address this challenge, Cui et al. [86] leverage the
    BO search algorithm with GP to produce the optimal load-balanced request chain
    for each microservice unit. Then all the individual request chains are consolidated
    into a tree structure dependency model that can be dynamically updated in case
    of potential environmental changes. This solution can achieve good load balancing
    effects, while performance can be degraded under a highly dynamic environment
    as the updates are quite time-consuming. 4.3 Resource Provisioning In this section,
    we discuss various resource provisioning techniques, including scheduling, scaling,
    and migration. 4.3.1 Scheduling. As shown in Table 6, various algorithms have
    been proposed to solve the scheduling issue for improving system and application
    performance. Table 6. Ref. Infrastructure Application Architecture Methods Task
    Structure Objectives Advantages Limitations [58] Single cloud Serverless Heuristic
    Multiple independent Resource utilization improvement and energy saving Reduction
    of cold start and response latency Poor efficiency for tasks with long lifetimes
    [83] Hybrid cloud Serverless Heuristic Single Cost and latency minimization Multi-objective
    task placement Limited accuracy under high load variance [84] Hybrid cloud Serverless
    Heuristic Graph-based Cost minimization SLA assurance Simplicity of application
    workloads Show More Table 6. Summary of Scheduling Approaches Most of the reviewed
    approaches follow a design pattern of combining ML-based Workload Modelers or
    Performance Analyzers with a heuristic scheduling Decision Maker, as discussed
    in Section 2.2.4. As the system complexity has been navigated by prediction models,
    bin packing and approximation algorithms such as best fit or least fit are commonly
    adopted to make scheduling decisions with improved resource utilization and energy
    efficiency [8, 16, 18, 58, 83, 84]. For example, Venkateswaran and Sarkar [16]
    manage to significantly reduce the complexity of selecting the best-fit container
    system and cluster compositions under multi-cloud environments, standing on their
    prediction model described in Section 4.2.2. To mitigate the resource contention
    between co-located tasks deployed at the same host, Thinakaran et al. [63] implement
    a correlation-based scheduler for handling task co-location by measuring GPU consumption
    correlation metrics, especially consecutive peak resource demand patterns recognized
    by its ARIMA prediction model. Generally, although these heuristic-based approaches
    can achieve acceptable results within a short time, the results are not the optimal
    ones. The rest of the studies choose RL models as the core component in their
    scheduling engine. For instance, Orhean et al. [25] choose two classic model-free
    RL models, namely, Q-Learning and SARSA, to schedule a group of tasks in a DAG
    structure. To reduce the overall DAG execution time, the internal tasks in a DAG
    categorized by their key features and priorities are scheduled under consideration
    of the dynamic cluster state and machine performance. To address the limitation
    of high sample complexity of model-free RL approaches, Zhang et al. [85] attempt
    to handle scientific workflows under microservice architectures through model-based
    RL. An ANN model is trained to emulate the system behavior by identification of
    key performance metrics collected from the microservice infrastructure, so that
    the synthetic interactions generated by ANN could directly be involved in the
    policy training process with Actor-Critical to generate scheduling decisions.
    In such a way, it simplifies the system model and avoids the time consuming and
    computationally expensive interactions within the real microservice environment.
    In RL-based scheduling algorithms, the states and actions require to be carefully
    designed, otherwise, the scalability can be significantly limited due to the large
    solution space. In conclusion, heuristic or RL models are applied in most of the
    investigated works for decision making in task scheduling, to achieve resource
    utilization improvement and task completion time minimization. 4.3.2 Scaling.
    Scaling can dynamically adjust the system states in response to the changing workloads
    and cloud environments. Different from the scheduling introduced in Section 4.3.1,
    the target of scaling is for containerized applications rather than tasks. Currently,
    the dominant scaling mechanisms include horizontal scaling via increasing or decreasing
    instances of containers, vertical scaling via adding or removing hardware resources
    for a single container, and hybrid scaling by combining horizontal scaling and
    vertical scaling. The research works related to scaling are given in Table 7,
    which summarizes the adopted scaling mechanism, optimization objective, advantages,
    and limitations of investigated approaches. The researcher can select the appropriate
    approach based on their research goals. Table 7. Ref. Mechanism Infrastructure
    Application Architecture Methods Objectives Advantages Limitations [59] Horizontal
    Single cloud Serverless Heuristic Reduction of execution latency and resource
    consumption Alleviation of cold starts Instability under changing workloads [60]
    Horizontal Single cloud Serverless Q-Learning SLA assurance Reduction of time
    overhead of cold starts Simplicity of training model [92] Hybrid Single cloud
    Monolithic Ensemble Resource saving High resource efficiency and reliability Limited
    flexibility Show More Table 7. Summary of Scaling Approaches To improve the decision
    quality and accuracy of RL-based autoscalers for monolithic applications with
    SLA assurance, Zhang et al. [28] build a horizontal scaling approach through the
    SARSA algorithm, based on analysis of the application workloads and CPU usage
    predicted by the ARIMA and ANN models. For addressing the same research questions
    under fog-cloud environments, Sami et al. [30] simulate the horizontal scaling
    of containers as an MDP model that considers both the changing workloads and free
    resource capacity in fogs. Then, SARSA is chosen for finding the optimal scaling
    strategy on top of the MDP model through online training at a small data scale.
    Further considering the possibility of hybrid scaling of monolithic applications,
    Rossi et al. [76] propose a model-based RL approach, targeting to find a combination
    of horizontal and vertical scaling decisions that meet the QoS requirements with
    the lowest adaption costs and resource wastage. Furthermore, the authors extend
    this model with a network-aware heuristic method for container placement in geographically
    distributed clouds [77]. Although the RL-based approaches mentioned above can
    improve resource utilization and QoS to a certain degree, their application workloads
    and QoS scenarios are too simple, without enough consideration of the diversity
    and complexity of cloud workloads. Plenty of previous studies [66, 67, 70, 74,
    81] have tried to leverage heuristic methods for microservice scaling, assisted
    by ML-based workload modeling and performance analysis. However, such approaches
    underestimate the inter-dependencies between microservices that are updated dynamically.
    On the other hand, model-based RL algorithms are usually unsuitable for microservice-based
    applications for the same reason. As microservice dependencies could potentially
    change at runtime, the simulation of state transitions could then be invalid.
    Therefore, model-free RL algorithms are more common in the application scaling
    of microservices, as they do not rely on transition models. Qiu et al. [24] implement
    an SLO violation alleviation mechanism using Actor-Critic to scale the critical
    microservices detected by SVM as mentioned in Section 4.2.4. The Actor-Critic
    model produces a horizontal scaling decision to minimize the SLO violations through
    evaluating three crucial features, including SLO maintenance ratio, workload changes,
    and request composition. To speed up the model training process of RL methods,
    Yan et al. [78] design a multi-agent parallel training model based on SARSA for
    horizontal scaling of microservices under hybrid clouds. Assisted with the workload
    prediction results generated by Bi-LSTM, their elastic scaling approach could
    make a more accurate scaling decision of when, where, and how many microservice
    instances should be scaled up/down. In such a way, it achieves significant resource
    utilization improvement and cost reduction under SLA assurance. However, due to
    the high computation complexity, this approach is not suitable for edge devices
    given their limited capability. Cold starts during the invocation of serverless
    functions are a serious performance bottleneck of serverless computing platforms.
    Cold starts are defined as the time overhead of environment setup and function
    initialization. Xu et al. [59] present a container pool scaling strategy, where
    function containers are pre-initialized by evaluating the first function invocation
    time (predicted by LSTM as discussed in Section 4.2.2) and the number of containers
    in each function category. Similarly, Agarwal et al. [60] introduce a Q-Learning
    agent to summarize the function invocation patterns and make the optimal scaling
    decisions of function containers in advance. A series of metrics, including the
    number of available function containers, per-container CPU utilization, and success/failure
    rates, are selected to represent the system states in the Q-Learning model. Due
    to the nature of model-free RL methods, prior information of the input function
    is not necessary. However, the performance is not guaranteed under changing workloads
    as only simple types of workloads are considered in these solutions. In summary,
    there has been a large body of literature in the area of scaling using diverse
    solutions, covering all types of application architectures and cloud infrastructures.
    The majority of the reviewed works are related to autoscaling of microservice-based
    applications, with the model-free RL models as the latest resolution to process
    the dynamically changing workloads and microservice dependencies [24, 29, 78].
    Their common targets mainly focus on SLA assurance and resource efficiency optimization.
    4.3.3 Migration. As depicted in Table 8, Migration is a complementary mechanism
    for resource optimization across cloud environments. Table 8. Ref. Mechanism Infrastructure
    Application Architecture Methods Objectives Advantages Limitations [8] Rescheduling
    Single cloud Monolithic Heuristic SLA assurance Resource contention alleviation
    Long execution delays [31] Offloading Hybrid cloud Monolithic MDP, DNN, and Q-Learning
    Reduction of communication delays, power consumption Low migration costs High
    space complexity [41] Offloading Hybrid cloud Monolithic Heuristic Energy efficiency
    improvement Load balance High time and space complexity Show More Table 8. Summary
    of Migration Approaches Some reviewed approaches manage to bind ML-based behavior
    models with heuristic migration algorithms. Zhong et al. [8] develop a least-fit
    rescheduling algorithm to evict and relocate a set of lower-priority containers
    with the least QoS impact when unexpected resource contention occurs between co-located
    containers. The rescheduling algorithm readjusts the container configuration based
    on runtime performance metrics and selects the node with the most available resources
    evaluated by K-means for relocation. Besides, Chhikara et al. [41] introduce an
    energy-efficient offloading model with a set of heuristic methods, including random
    placement, first-fit, best-fit, and correlation threshold-based placement algorithms.
    It is aimed at resource load balancing under hybrid cloud environments by migrating
    containers from overloaded nodes to underloaded nodes that are identified through
    classification as described in Section 4.2.3. However, since the performance metrics
    are obtained by third-party APIs, these approaches may come with high execution
    delays in large-scale computing systems. A fog-cloud container offloading prototype
    system is presented by Tang et al. [31]. The container offloading process is considered
    as a multi-dimensional MDP model. To reduce the network delays and computation
    costs under potentially unstable environments, the deep Q-Learning algorithm combines
    the deep neural network (DNN) and Q-Learning model to quickly produce an efficient
    offloading plan. Wang et al. [94] further extends the combination of MDP and Q-Learning
    in the context of microservice coordination under edge-cloud environments. The
    process of microservice coordination is assumed as a sequential decision scheme
    and formulated as an MDP model. On top of the MDP model, Q-Learning is used to
    find the optimal solution for service migration or offloading, in light of long-term
    performance metrics, including overall migration delays and costs. However, the
    large solution space of these approaches limits the scalability of a large-scale
    container-based cluster. 4.4 Observations In this section, we summarize the observations
    of the discussed researches based on three important categories, namely, application
    architecture, infrastructure, and hybrid machine learning model. 4.4.1 Application
    Architecture. As described in Figure 7(a), most of the reviewed articles (68%)
    are concerned with modeling and management of microservice-based applications.
    In light of the dynamic and decentralized nature of microservices, diverse ML
    algorithms have been investigated for capturing the key characteristic of microservice
    workloads, performance, and inter-dependencies, such as BGR, LSTM, GRU, SVM, GP,
    ANN, CNN, and DT [23, 69, 70, 81, 82, 85, 86]. As the inter-dependencies of microservice
    units could dynamically update at runtime, the key concern of microservice resource
    provisioning is the chain reactions caused by microservice unit scaling operations.
    By combining ML-based behavior models with heuristic or model-free RL methods
    for scaling, the most recent studies manage to achieve high resource utilization
    optimization, cost reduction, and SLA assurance [24, 66, 67, 70, 74, 78, 81].
    Fig. 7. Fig. 7. Study distribution of (a) application architectures and (b) cloud
    infrastructures. The research works related to the orchestration of single-component
    containerized applications hold a study distribution of 22%. As the workload scenarios
    of these applications are relatively simple, their core drive of workload/behavior
    modeling is to predict their resource demands under certain QoS requirements.
    Assisted with such prediction results, heuristic and RL methods (both model-free
    and model-based) are adopted for optimization of the resource allocation process
    regarding cost, energy, and resource efficiency [8, 16, 28, 44, 63, 79, 80]. Although
    the serverless architecture is currently having the lowest study distribution
    (10%), it is enjoying a growing popularity and becoming a prevalent application
    architecture in cloud computing. Most of the existing ML-based researches in this
    field are trying to alleviate the performance downgrade caused by function cold
    starts. LSTM and GBR models have been employed to estimate the function invocation
    time in serverless function chains, while the allocation of functions and scaling
    of function containers are mainly solved by heuristic methods and Q-Learning [58,
    59, 60, 83]. 4.4.2 Infrastructure. The majority of the included researches (71%)
    only consider application deployment under single cloud environments as demonstrated
    in Figure 7(b). Since single cloud environments are easier to manage, it is not
    necessary to raise the system complexity for most applications. Only 5% of studies
    have attempted behavior modeling and scaling under geographic-distributed multi-cloud
    environments [77, 86], because the interactions under such context are usually
    time-consuming and computation-intensive. As traditional cloud computing commonly
    causes significant propagation delays, bandwidth and energy consumption by hosting
    all the applications and data in cloud servers, edge and fog computing are emerging
    as mainstream computing paradigms. Therefore, the latest studies have investigated
    the possibility of container orchestration under hybrid cloud environments, where
    the crucial research question is to decide where to host the containerized applications
    among cloud/edge devices and the cloud. The proposed solutions for scheduling
    and offloading of containerized applications under hybrid clouds, including MDP,
    RL, DRL, and heuristic methods [16, 30, 31, 41, 78, 83], aim to reduce end-to-end
    latency and optimize resource/energy efficiency. 4.4.3 Hybrid Machine Learning
    Model. Figure 8 shows the study distribution of ML models between 2016 and 2021,
    where we can observe a rising trend of hybrid ML models. A single ML model consisting
    of merely one ML algorithm is designed to solve a specific container orchestration
    problem, either data analysis or resource provisioning. By 2018, only single ML
    models had been adopted in this field. As the internal structures of containerized
    applications like microservices are becoming rather complex and dynamic with continuously
    growing demands of higher modeling/prediction accuracy and lower response time,
    this requires ML models to be more robust and efficient with even lower error
    rates, computation costs, and data training time. Therefore, most studies have
    been investigating hybrid ML models composed of a mixture of multiple ML algorithms
    to solve one or more orchestration problems from 2019 [31, 65, 72, 79, 80, 82,
    94]. Fig. 8. Fig. 8. Study distribution of machine learning models between 2016
    and 2021. Skip 5DISCUSSIONS AND FUTURE DIRECTIONS Section 5 DISCUSSIONS AND FUTURE
    DIRECTIONS Tackling the problem of container orchestration based on ML in cloud
    computing requires addressing a set of sub-problems including workloads characterization,
    microservice inter-dependency analysis, container anomalies detection, and task
    dependency analysis. Various ML-based solutions have been proposed and applied
    to solve these problems. To support the readers to pick up the solutions to their
    target problems, Table 9 summarizes the techniques utilized in the surveyed research
    work. From the present survey, we can conclude that significant efforts have been
    made to address the workloads characterization problem and the dominant ML mechanisms,
    such as LSTM, Bi-LSTM, K-means, and ARIMA have been applied. However, for some
    other problems such as dependency analysis in tasks and communication delays in
    edge devices, they are worth considering the adoption of more ML solutions when
    realizing more efficient solutions to the problems. Table 9. Problems in container
    orchestration Potential ML-based solutions How to characterize workloads by modeling
    and predicting requests behavior and resource usage pattern? LSTM, K-means++,
    ARIMA, Bi-LSTM, GRU, TSNNR, and GBR How to analyze inter-dependency between microservices?
    BO, GP, CNN, and LSTM How to detect anomalies in container orchestration systems?
    K-means, KNN, SVM, NB, RF, LSTM, isolation forest, and LASSO Show More Table 9.
    Problems in Containers Orchestration and Potential ML-based Solutions Although
    the existing studies have covered diverse orchestration schemes, application architectures,
    and cloud infrastructures, some research gaps and challenges are only partially
    addressed. In this section, we discuss a series of open research opportunities
    and potential future directions: (1) Workload Distribution in Microservices. The
    current workload characterization methods are mainly focusing on modeling and
    prediction of the request arrival rates and resource usage patterns. Very few
    works try to address the issue of workload distribution across microservice units.
    The changing workload distribution on individual microservices could potentially
    cause a chain reaction and further impact the overall application performance.
    Therefore, how to simulate and standardize workload distribution between microservices
    for load balance and performance optimization remains an unsolved research question.
    This question can be addressed by using efficient and accurate workload distribution
    prediction methods based on ML, and the efficiency of the tasks scheduling discussed
    in Section 4.3.1 can be further improved. (2) Microservice Dependency Analysis.
    The dynamic inter-dependencies between microservices is a crucial part of application
    complexity navigation. Though some works have attempted to predict application
    performance or identify critical microservice units through dependency analysis,
    there is no existing solution to explicitly address the relationship between the
    status of individual microservices and overall application performance metrics.
    Such analysis models are necessary for scheduling and scaling microservice-based
    applications, which can significantly improve performance optimization and SLA
    assurance. One promising approach is taking advantage of advanced ML-based approaches
    to establish comprehensive analysis models for microservices. (3) Systematic Anomaly
    Detection and Recovery. The current ML-based anomaly detection methods for containerized
    applications are mostly based on resource/performance metrics or security threats.
    There is a need for a systematic approach for anomaly detection, root cause categorization,
    and recovery in a timely and accurate fashion, under different application architectures
    and cloud infrastructures. For example, RL-based approach can be applied for making
    decision on recovery. (4) Graph-based Task Scheduling. Batch processing jobs consisting
    of a group of dependent tasks in DAG structures are common in containerized applications,
    but the literature related to the scheduling problem of such applications is very
    limited. Some previous studies manage to resolve this problem under a simplified
    condition of homogeneous job structures where each task in a DAG is configured
    similarly with the same execution time. This kind of assumption is rather unrealistic
    considering the complex and heterogeneous nature of DAG jobs. A sophisticated
    scheduling strategy should not only consider the overall DAG structure and different
    task configurations (e.g., resource demands, execution time, and replica sizes),
    but also the runtime performance metrics and fault tolerance. One promising way
    is to model these configurations as the states in the RL-based model and the tasks
    can be scheduled by the actions in the model to improve performance. (5) Management
    of Function Chains in Serverless Architectures. The latest ML-based studies on
    serverless architectures are all related to the alleviation of function cold starts,
    with many other research directions left to be discovered. As functions are submitted
    without any prior application-level knowledge, it is tricky to implement a robust
    and efficient solution for workload classification, resource demand estimation,
    and autoscaling. How to optimize the invocation of function chains in an SLO and
    cost-aware manner is also a crucial research question, especially under hybrid
    cloud environments. As an initial exploration, we suggest using SVM or KNN-based
    approaches to conduct some analysis for the invocation of function chains. (6)
    Microservices in Hybrid Clouds. Under the emerging trend of edge and fog computing,
    the most recent studies have made efforts to move microservices to edge/fog devices
    for communication delays reduction and cost saving. However, the extremely heterogeneous
    and geographically distributed computation resources within hybrid clouds significantly
    raise the complexity of application and resource management. A resource provisioning
    solution under such environments must consider the long-term impacts of each microservice
    placement or adjustment decision, regarding multi-dimensional optimization objectives,
    such as costs, energy, and end-to-end latency. Therefore, there is great potential
    for ML-based solutions to simulate the process of scheduling, scaling, and offloading
    for microservices under hybrid clouds. (7) Energy-aware Resource Management. Through
    combing ML-based workload characterization and performance analysis models, the
    overall energy consumption of a system could be precisely predicted based on the
    resource utilization of each PM. Accordingly, these insights grant us more options
    for developing energy-aware orchestration frameworks. For instance, brownout technologies
    could be utilized to activate/deactivate optional microservices for energy efficiency
    optimization, according to the predicted trends in resource utilization and SLA
    violations [33], where the selection of deactivated microservices can be made
    by ML-based approaches. (8) Multi-dimensional Performance Optimization. One of
    the essential optimization problems in container orchestration is to manage the
    tradeoff between different performance metrics, especially SLA violations and
    financial costs. Though some previous studies [58, 63, 83] address this issue
    to a certain degree by balancing several optimization objectives during resource
    provisioning, a standard performance analysis benchmark should be designed to
    accurately decompose the relation between a set of pre-defined key performance
    metrics. The relationship of various performance metrics can be analyzed via ML-based
    solutions, such as nearest neighbour or decision tree to represent co-relationship.
    Such knowledge could be further applied in resource provisioning to produce optimal
    orchestration decisions with multiple performance requirements taken into account.
    (9) Fully Integrated ML-based Optimization Engine. Considering the long data training
    time and large data volumes requested by ML models, a partially integrated ML
    engine enjoys high popularity, where existing ML models are only utilized for
    behavior modeling to assist the original Container Orchestrator in online resource
    provisioning. Following the reference architecture of ML-based optimization engine
    in Section 2.2.4, a fully integrated ML engine should be capable of combining
    multiple ML models, such as integrating both offline training of behavior models
    and fast online decision making of resource provisioning. (10) Edge Intelligence.
    Due to the extreme data scales and geographical distribution of edge devices,
    data transition to a centralized ML-based Optimization Engine across edge networks
    is potentially time-consuming and expensive. Hence, the next-generation edge intelligence
    agents should follow a decentralized architecture where each independent agent
    is deployed at a distributed location close to end users for data collection,
    analytical model construction, and maintenance of serverless or microservice-based
    applications under low propagation delays. However, the current researches are
    mainly conducted with small-scale experiments and the scalability problem with
    multiple agents is not well addressed. A promising solution is to apply the approximate
    solution based on RL to manage agents that can scale for a large-scale environment.
    (11) Costs Analysis of Applying ML. Based on the current survey, ML solutions
    have demonstrated the powerful capacity to address some problems in container
    orchestration. However, in some cases, such as the online decision scenario or
    energy-efficient scenario, the computational costs of ML-based methods can undermine
    the algorithm performance or consume extra energy. Although some researches have
    aimed at reducing the costs of the online decision based on ML [24], the current
    research still lacks a comprehensive analysis of the costs incurred by ML-based
    solutions of container orchestration. One promising approach to address this problem
    is balancing the tradeoffs between offline and online training, for instance,
    in the RL-based approach, the more trained decisions based on historical data
    can be stored in the knowledge pool, and the online decisions can rely more on
    it rather than online training. In addition, the state space should be carefully
    designed. Skip 6CONCLUSION Section 6 CONCLUSION In this work, we have conducted
    an extensive literature review on how container orchestration is managed using
    diverse machine learning-based approaches in the state-of-the-art studies, with
    emphasis on the specific application architectures and cloud infrastructures.
    The last few years have witnessed a continuously growing trend of the adoption
    of machine learning methods in containerized application management systems for
    behavior modeling, as well as resource provisioning of complicated decentralized
    applications. Compared with most traditional heuristic methods, ML-based models
    could produce more accurate orchestration decisions with shorter computation delays,
    under complex and dynamic cloud environments consisting of highly heterogeneous
    and geographically distributed computation resources. We have observed machine
    learning-based methods applied under a wide range of scenarios, but there is no
    systematic approach to build a complete machine learning-based optimization framework
    in charge of the whole orchestration process. Under the emerging trend of moving
    microservice and serverless applications to hybrid clouds, such frameworks are
    necessary to handle the sustainably growing system complexity and balance multi-dimensional
    optimization objectives. This research work will help researchers find the important
    characteristics of ML-based orchestration approaches and will also help to select
    the most suitable techniques for efficient container orchestration with the specific
    requirement under different application architectures. For instance, researchers
    can utilize the GRU-based approach to conduct time series analysis of workloads,
    and exploit the RL-based approach to make resource provisioning decisions. Skip
    ACKNOWLEDGMENTS Section ACKNOWLEDGMENTS We thank Dr. Sukhpal Singh Gill and anonymous
    reviewers for their valuable suggestions on improving this paper. REFERENCES [1]
    Zhang Qi, Cheng Lu, and Boutaba Raouf. 2010. Cloud computing: State-of-the-art
    and research challenges. Journal of Internet Services and Applications 1, 1 (2010),
    7–18. Reference 1Reference 2 [2] Narula Saakshi, Jain Arushi, and Prachi. 2015.
    Cloud computing security: Amazon web service. In Proceedings of the 2015 International
    Conference on Advanced Computing Communication Technologies. 501–505. Reference
    [3] Verma Abhishek, Pedrosa Luis, Korupolu Madhukar, Oppenheimer David, Tune Eric,
    and Wilkes John. 2015. Large-scale cluster management at Google with Borg. In
    Proceedings of the 2015 European Conference on Computer Systems. 1–18. Reference
    1Reference 2Reference 3 [4] Liu Qixiao and Yu Zhibin. 2018. The elasticity and
    plasticity in semi-containerized co-locating cloud workload: A view from alibaba
    trace. In Proceedings of the 2018 ACM Symposium on Cloud Computing. 347–360. Reference
    1Reference 2Reference 3 [5] Casalicchio Emiliano and Iannucci Stefano. 2020. The
    state-of-the-art in container technologies: Application, orchestration and security.
    Concurrency and Computation: Practice and Experience 32, 17 (2020), 1–21. Navigate
    to [6] Zhong Zhiheng and Buyya Rajkumar. 2020. A cost-efficient container orchestration
    strategy in kubernetes-based cloud computing infrastructures with heterogeneous
    resources. ACM Transactions on Internet Technology 20, 2 (2020), 1–24. Reference
    1Reference 2Reference 3 [7] Shi Weisong, Cao Jie, Zhang Quan, Li Youhuizi, and
    Xu Lanyu. 2016. Edge computing: Vision and challenges. IEEE Internet of Things
    Journal 3, 5 (2016), 637–646. Reference [8] Zhong Zhiheng, He Jiabo, Rodriguez
    Maria A., Erfani Sarah, Kotagiri Ramamohanarao, and Buyya Rajkumar. 2020. Heterogeneous
    task co-location in containerized cloud computing environments. In Proceedings
    of the 2020 IEEE International Symposium on Real-Time Distributed Computing. 79–88.
    Navigate to [9] Joseph Christina Terese and Chandrasekaran K.. 2019. Straddling
    the cStraddling the crevasse: A review of microservice software architecture foundations
    and recent advancementsrevasse: A review of microservice software architecture
    foundations and recent advancements. Software: Practice and Experience 49, 10
    (2019), 1448–1484. Reference 1Reference 2 [10] Bu Xiangping, Rao Jia, and Xu Cheng-Zhong.
    2009. A reinforcement learning approach to online web systems auto-configuration.
    In Proceedings of the 2009 29th IEEE International Conference on Distributed Computing
    Systems. IEEE, 2–11. Reference [11] Xu Cheng-Zhong, Rao Jia, and Bu Xiangping.
    2012. URL: A unified reinforcement learning approach for autonomic cloud management.
    Journal of Parallel and Distributed Computing 72, 2 (2012), 95–105. Reference
    [12] Bu Xiangping, Rao Jia, and Xu Cheng-Zhong. 2012. Coordinated self-configuration
    of virtual machines and appliances using a model-free learning approach. IEEE
    Transactions on Parallel and Distributed Systems 24, 4 (2012), 681–690. Reference
    [13] Rodriguez Maria A. and Buyya Rajkumar. 2019. Container-based cluster orchestration
    systems: A taxonomy and future directions. Software: Practice and Experience 49,
    5 (2019), 698–719. Navigate to [14] Reiss Charles, Tumanov Alexey, Ganger Gregory
    R., Katz Randy H., and Kozuch Michael A.. 2012. Heterogeneity and dynamicity of
    clouds at scale: Google trace analysis. In Proceedings of the 2012 ACM Symposium
    on Cloud Computing. 1–13. Reference [15] Chung Andrew, Park Jun Woo, and Ganger
    Gregory R.. 2018. Stratus: Cost-aware container scheduling in the public cloud.
    In Proceedings of the 2018 ACM Symposium on Cloud Computing. 121–134. Reference
    1Reference 2 [16] Venkateswaran Sreekrishnan and Sarkar Santonu. 2019. Fitness-aware
    containerization service leveraging machine learning. IEEE Transactions on Services
    Computing 1, 1 (2019), 1–14. Navigate to [17] Menouer Tarek, Manad Otman, Cérin
    Christophe, and Darmon Patrice. 2019. Power efficiency containers scheduling approach
    based on machine learning technique for cloud computing environment. In Proceedings
    of the 2019 International Symposium on Pervasive Systems, Algorithms and Networks.
    193–206. Reference [18] Zhang Haitao, Ma Huadong, Fu Guangping, Yang Xianda, Jiang
    Zhe, and Gao Yangyang. 2016. Container based video surveillance cloud service
    with fine-grained resource provisioning. In Proceedings of the 2016 IEEE International
    Conference on Cloud Computing. 758–765. Navigate to [19] Barshan Maryam, Moens
    Hendrik, Latre Steven, Volckaert Bruno, and Turck Filip De. 2017. Algorithms for
    network-aware application component placement for cloud resource allocation. Journal
    of Communications and Networks 19, 5 (2017), 493–508. Reference [20] Berral Josep
    Ll., Gavalda Ricard, and Torres Jordi. 2011. Adaptive scheduling on power-aware
    managed data-centers using machine learning. In Proceedings of the 2011 IEEE/ACM
    International Conference on Grid Computing. 66–73. Reference [21] Trneberg William,
    Mehta Amardeep, Wadbro Eddie, Tordsson Johan, Eker Johan, Kihl Maria, and Elmroth
    Erik. 2017. Dynamic application placement in the Mobile cloud network. Future
    Generation Computer Systems 70, 1 (2017), 163–177. Reference [22] Wang Shiqiang,
    Zafer Murtaza, and Leung Kin K.. 2017. Online placement of multi-component applications
    in edge computing environments. IEEE Access 5, 1 (2017), 2514–2533. Reference
    [23] Zhang Yanqi, Hua Weizhe, Zhou Zhuangzhuang, Suh G. Edward, and Delimitrou
    Christina. 2021. Sinan: ML-based and QoS-aware resource management for cloud microservices.
    In Proceedings of the 2021 ACM International Conference on Architectural Support
    for Programming Languages and Operating Systems. 167–181. Navigate to [24] Qiu
    Haoran, Banerjee Subho S., Jha Saurabh, Kalbarczyk Zbigniew T., and Iyer Ravishankar
    K.. 2020. FIRM: An intelligent fine-grained resource management framework for
    SLO-oriented microservices. In Proceedings of the 2020 USENIX Symposium on Operating
    Systems Design and Implementation. 805–825. Navigate to [25] Orhean Alexandru
    Iulian, Pop Florin, and Raicu Ioan. 2018. New scheduling approach using reinforcement
    learning for heterogeneous distributed systems. Journal of Parallel and Distributed
    Computing 117, 1 (2018), 292–302. Navigate to [26] Bao Yixin, Peng Yanghua, and
    Wu Chuan. 2019. Deep learning-based job placement in distributed machine learning
    clusters. In Proceedings of the 2019 IEEE INFOCOM - IEEE Conference on Computer
    Communications. 505–513. Navigate to [27] Bianchini Ricardo, Fontoura Marcus,
    Cortez Eli, Bonde Anand, Muzio Alexandre, Constantin Ana-Maria, Moscibroda Thomas,
    Magalhaes Gabriel, Bablani Girish, and Russinovich Mark. 2020. Toward ML-centric
    cloud platforms. Communications of the ACM 63, 2 (2020), 50–59. Reference [28]
    Zhang Shubo, Wu Tianyang, Pan Maolin, Zhang Chaomeng, and Yu Yang. 2020. A-SARSA:
    A predictive container auto-scaling algorithm based on reinforcement learning.
    In Proceedings of the 2020 IEEE International Conference on Web Services. 489–497.
    Navigate to [29] Xu Yu, Yao Jianguo, Jacobsen Hans-Arno, and Guan Haibing. 2017.
    Cost-efficient negotiation over multiple resources with reinforcement learning.
    In Proceedings of the 2017 IEEE/ACM International Symposium on Quality of Service.
    1–6. Navigate to [30] Sami Hani, Mourad Azzam, Otrok Hadi, and Bentahar Jamal.
    2020. FScaler: Automatic resource scaling of containers in fog clusters using
    reinforcement learning. In Proceedings of the 2020 International Wireless Communications
    and Mobile Computing. 1824–1829. Navigate to [31] Tang Zhiqing, Zhou Xiaojie,
    Zhang Fuming, Jia Weijia, and Zhao Wei. 2019. Migration modeling and learning
    algorithms for containers in fog computing. IEEE Transactions on Services Computing
    12, 5 (2019), 712–725. Navigate to [32] Weerasiri Denis, Barukh Moshe Chai, Benatallah
    Boualem, Sheng Quan Z., and Ranjan Rajiv. 2017. A taxonomy and survey of cloud
    resource orchestration techniques. Computing Surveys 50, 2 (2017), 1–41. Reference
    1Reference 2 [33] Xu Minxian and Buyya Rajkumar. 2019. Brownout approach for adaptive
    management of resources and applications in cloud computing systems: A taxonomy
    and future directions. Computing Surveys 52, 1 (2019), 1–27. Navigate to [34]
    Duc Thang Le, Leiva Rafael García, Casari Paolo, and Östberg Per-Olov. 2019. Machine
    learning methods for reliable resource provisioning in edge-cloud computing: A
    survey. Computing Surveys 52, 5 (2019), 1–39. Reference 1Reference 2 [35] Singh
    Sukhpal and Chana Inderveer. 2015. QoS-aware autonomic resource management in
    cloud computing: A systematic review. Computing Surveys 48, 3, Article 42 (2015),
    46 pages. Reference 1Reference 2 [36] Pahl Claus, Brogi Antonio, Soldani Jacopo,
    and Jamshidi Pooyan. 2019. Cloud container technologies: A state-of-the-art review.
    IEEE Transactions on Cloud Computing 7, 3 (2019), 677–692. Reference 1Reference
    2 [37] Brnabic A. and Hess L. M.. 2021. Systematic literature review of machine
    learning methods used in the analysis of real-world data for patient-provider
    decision making. BMC Medical Informatics and Decision Making 21, 1 (2021), 1–19.
    Reference [38] Djenouri Djamel, Laidi Roufaida, Djenouri Youcef, and Balasingham
    Ilangko. 2019. Machine learning for smart building applications: Review and taxonomy.
    Computing Surveys 52, 2 (2019), 1–36. Reference [39] Abreu Pedro Henriques, Santos
    Miriam Seoane, Abreu Miguel Henriques, Andrade Bruno, and Silva Daniel Castro.
    2016. Predicting breast cancer recurrence using machine learning techniques: A
    systematic review. Computing Surveys 49, 3 (2016), 1–40. Reference [40] Pouyanfar
    Samira, Sadiq Saad, Yan Yilin, Tian Haiman, Tao Yudong, Reyes Maria Presa, Shyu
    Mei-Ling, Chen Shu-Ching, and Iyengar S. S.. 2018. A survey on deep learning:
    Algorithms, techniques, and applications. Computing Surveys 51, 5 (2018), 1–36.
    Reference [41] Obaidat Prateek Chhikara, Rajkumar Tekchandani, Neeraj Kumar, and
    Mohammad S.. 2020. An efficient container management scheme for resource constrained
    intelligent IoT devices. IEEE Internet of Things Journal 1, 1 (2020), 1–13. Navigate
    to [42] Du Qingfeng, Xie Tiandi, and He Yu. 2018. Anomaly detection and diagnosis
    for container-based microservices with performance monitoring. In Proceedings
    of the 2018 International Conference on Algorithms and Architectures for Parallel
    Processing. 560–572. Reference 1Reference 2Reference 3 [43] Yau Kok-Lim Alvin,
    Qadir Junaid, Khoo Hooi Ling, Ling Mee Hong, and Komisarczuk Peter. 2017. A survey
    on reinforcement learning models and algorithms for traffic signal control. Computing
    Surveys 50, 3 (2017), 1–38. Reference [44] Ye Kejiang, Kou Yanmin, Lu Chengzhi,
    Wang Yang, and Xu Cheng-Zhong. 2018. Modeling application performance in docker
    containers using machine learning techniques. In Proceedings of the 2018 IEEE
    International Conference on Parallel and Distributed Systems. 1–6. Navigate to
    [45] Carrera David, Steinder Malgorzata, Whalley Ian, Torres Jordi, and Ayguade
    Eduard. 2012. Autonomic placement of mixed batch and transactional workloads.
    IEEE Transactions on Parallel and Distributed Systems 23, 2 (2012), 219–231. Reference
    [46] Rasmussen Rasmus V. and Trick Michael A.. 2008. Round robin scheduling–a
    survey. European Journal of Operational Research 188, 3 (2008), 617–636. Reference
    [47] Roman Rodrigo, Lopez Javier, and Mambo Masahiro. 2018. Mobile edge computing,
    Fog et al.: A survey and analysis of security threats and challenges. Future Generation
    Computer Systems 78, 1 (2018), 680–698. Reference [48] Mukherjee Mithun, Shu Lei,
    and Wang Di. 2018. Survey of fog computing: Fundamental, network applications,
    and research challenges. IEEE Communications Surveys Tutorials 20, 3 (2018), 1826–1857.
    Reference [49] Bianchini Ricardo, Fontoura Marcus, Cortez Eli, Bonde Anand, Muzio
    Alexandre, Constantin Ana-Maria, Moscibroda Thomas, Magalhaes Gabriel, Bablani
    Girish, and Russinovich Mark. 2020. Toward ML-Centric cloud platforms. Communications
    of the ACM 63, 2 (2020), 50–59. Reference [50] Mosleh Mohsen, Dalili Kia, and
    Heydari Babak. 2018. Distributed or monolithic? A computational architecture decision
    framework. IEEE Systems Journal 12, 1 (2018), 125–136. Reference [51] Ren Zhongshan,
    Wang Wei, Wu Guoquan, Gao Chushu, Chen Wei, Wei Jun, and Huang Tao. 2018. Migrating
    web applications from monolithic structure to microservices architecture. In Proceedings
    of the 2018 Asia-Pacific Symposium on Internetware. 1–10. Reference [52] Kecskemeti
    Gabor, Marosi Attila Csaba, and Kertesz Attila. 2016. The ENTICE approach to decompose
    monolithic services into microservices. In Proceedings of the 2016 International
    Conference on High Performance Computing Simulation. 591–596. Reference [53] Kiran
    Mariam, Murphy Peter, Monga Inder, Dugan Jon, and Baveja Sartaj Singh. 2015. Lambda
    architecture for cost-effective batch and speed big data processing. In Proceedings
    of the 2015 IEEE International Conference on Big Data. 2785–2792. Reference [54]
    Lynn Theo, Rosati Pierangelo, Lejeune Arnaud, and Emeakaroha Vincent. 2017. A
    preliminary review of enterprise serverless cloud computing (function-as-a-service)
    platforms. In Proceedings of the 2017 IEEE International Conference on Cloud Computing
    Technology and Science. 162–169. Reference [55] Daw Nilanjan, Bellur Umesh, and
    Kulkarni Purushottam. 2020. Xanadu: Mitigating cascading cold starts in serverless
    function chain deployments. In Proceedings of the 2020 International Middleware
    Conference. 356–370. Reference [56] Du Dong, Yu Tianyi, Xia Yubin, Zang Binyu,
    Yan Guanglu, Qin Chenggang, Wu Qixuan, and Chen Haibo. 2020. Catalyzer: Sub-millisecond
    startup for serverless computing with initialization-less booting. In Proceedings
    of the 2020 International Conference on Architectural Support for Programming
    Languages and Operating Systems. 467–481. Reference [57] Agache Alexandru, Brooker
    Marc, Iordache Alexandra, Liguori Anthony, Neugebauer Rolf, Piwonka Phil, and
    Popa Diana-Maria. 2020. Firecracker: Lightweight virtualization for serverless
    applications. In Proceedings of the 17th { usenix } Symposium on Networked Systems
    Design and Implementation ( { nsdi } 20). 419–434. Reference [58] Gunasekaran
    Jashwant Raj, Thinakaran Prashanth, Nachiappan Nachiappan C., Kandemir Mahmut
    Taylan, and Das Chita R.. 2020. Fifer: Tackling resource underutilization in the
    serverless era. In Proceedings of the 2020 International Middleware Conference.
    280–295. Navigate to [59] Xu Zhengjun, Zhang Haitao, Geng Xin, Wu Qiong, and Ma
    Huadong. 2019. Adaptive function launching acceleration in serverless computing
    platforms. In Proceedings of the 2019 IEEE International Conference on Parallel
    and Distributed Systems. 9–16. Navigate to [60] Buyya Siddharth Agarwal, Maria
    Alejandra Rodriguez, and Rajkumar. 2021. A reinforcement learning approach to
    reduce serverless function cold start frequency. In Proceedings of the 2021 IEEE/ACM
    International Symposium on Cluster, Cloud, and Internet Computing. 797–803. Navigate
    to [61] Oakes Edward, Yang Leon, Zhou Dennis, Houck Kevin, Harter Tyler, Arpaci-Dusseau
    Andrea C., and Arpaci-Dusseau Remzi H.. 2018. SOCK: Rapid task provisioning with
    serverless-optimized containers. In Proceedings of the 2018 USENIX Conference
    on Usenix Annual Technical Conference. USENIX Association, 57–69. Reference [62]
    Mastelic Toni, Oleksiak Ariel, Claussen Holger, Brandic Ivona, Pierson Jean-Marc,
    and Vasilakos Athanasios V.. 2014. Cloud computing: Survey on energy efficiency.
    Computing Surveys 47, 2 (2014), 1–36. Reference [63] Das Prashanth Thinakaran,
    Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R..
    2019. Kube-Knots: Resource harvesting through dynamic container orchestration
    in GPU-based datacenters. In Proceedings of the 2019 IEEE International Conference
    on Cluster Computing. 1–13. Navigate to [64] Xu Minxian, Toosi Adel N., and Buyya
    Rajkumar. 2020. A self-adaptive approach for managing applications and harnessing
    renewable energy for sustainable cloud computing. IEEE Transactions on Sustainable
    Computing 1, 1 (2020), 1–15. Reference [65] Yu Guangba, Chen Pengfei, and Zheng
    Zibin. 2019. Microscaler: Automatic scaling for microservices with an online learning
    approach. In Proceedings of the 2019 IEEE International Conference on Web Services.
    68–75. Reference 1Reference 2Reference 3 [66] Kang Peng and Lama Palden. 2020.
    Robust resource scaling of containerized microservices with probabilistic machine
    learning. In Proceedings of the 2020 IEEE/ACM International Conference on Utility
    and Cloud Computing. 122–131. Navigate to [67] Toka Laszlo, Dobreff Gergely, Fodor
    Balazs, and Sonkoly Balazs. 2020. Adaptive AI-based auto-scaling for Kubernetes.
    In Proceedings of the 2020 IEEE/ACM International Symposium on Cluster, Cloud
    and Internet Computing. 599–608. Navigate to [68] Meng Yang, Rao Ruonan, Zhang
    Xin, and Hong Pei. 2016. CRUPA: A container resource utilization prediction algorithm
    for auto-scaling based on time series analysis. In Proceedings of the 2016 International
    Conference on Progress in Informatics and Computing. 468–472. Reference [69] Shah
    Syed Yousaf, Yuan Zengwen, Lu Songwu, and Zerfos Petros. 2017. Dependency analysis
    of cloud applications for performance monitoring using recurrent neural networks.
    In Proceedings of the 2017 IEEE International Conference on Big Data. 1534–1543.
    Navigate to [70] Cheng Yi-Lin, Lin Ching-Chi, Liu Pangfeng, and Wu Jan-Jan. 2017.
    High resource utilization auto-scaling algorithms for heterogeneous container
    configurations. In Proceedings of the 2017 IEEE International Conference on Parallel
    and Distributed Systems. 143–150. Navigate to [71] Tang Xuehai, Liu Qiuyang, Dong
    Yangchen, Han Jizhong, and Zhang Zhiyuan. 2018. Fisher: An efficient container
    load prediction model with deep neural network in clouds. In Proceedings of the
    2018 IEEE International Conference on Parallel Distributed Processing with Applications,
    Ubiquitous Computing Communications, Big Data Cloud Computing, Social Computing
    Networking, Sustainable Computing Communications. 199–206. Navigate to [72] Cheng
    Yuming, Wang Chao, Yu Huihuang, Hu Yahui, and Zhou Xuehai. 2019. GRU-ES: Resource
    usage prediction of cloud workloads using a novel hybrid method. In Proceedings
    of the 2019 IEEE International Conference on High Performance Computing and Communications;
    the 2019 IEEE International Conference on Smart City; the 2019 IEEE International
    Conference on Data Science and Systems. 1249–1256. Navigate to [73] Fu Rui, Zhang
    Zuo, and Li Li. 2016. Using LSTM and GRU neural network methods for traffic flow
    prediction. In Proceedings of the 2016 Youth Academic Annual Conference of Chinese
    Association of Automation. 324–328. Reference [74] Podolskiy Vladimir, Mayo Michael,
    Koay Abigail, Gerndt Michael, and Patros Panos. 2019. Maintaining SLOs of cloud-native
    applications via self-adaptive resource sharing. In Proceedings of the 2019 IEEE
    International Conference on Self-Adaptive and Self-Organizing Systems. 72–81.
    Navigate to [75] Dartois Jean-Emile, Boukhobza Jalil, Knefati Anas, and Barais
    Olivier. 2019. Investigating machine learning algorithms for modeling SSD I/O
    performance for container-based virtualization. IEEE Transactions on Cloud Computing
    1, 1 (2019), 1–14. Navigate to [76] Rossi Fabiana, Nardelli Matteo, and Cardellini
    Valeria. 2019. Horizontal and vertical scaling of container-based applications
    using reinforcement learning. In Proceedings of the 2019 IEEE International Conference
    on Cloud Computing. 329–338. Reference 1Reference 2Reference 3 [77] Rossi Fabiana,
    Cardellini Valeria, and Presti Francesco Lo. 2019. Elastic deployment of software
    containers in geo-distributed computing environments. In Proceedings of the 2019
    IEEE Symposium on Computers and Communications. 1–7. Navigate to [78] Zhang Ming
    Yan, XiaoMeng Liang, ZhiHui Lu, Jie Wu, and Wei. 2021. HANSEL: Adaptive horizontal
    scaling of microservices using Bi-LSTM. Applied Soft Computing 105, 1 (2021),
    107216–107230. Navigate to [79] Xie Yulai, Jin Minpeng, Zou Zhuping, Xu Gongming,
    Feng Dan, Liu Wenmao, and Long Darrell. 2020. Real-time prediction of docker container
    resource load based on a hybrid model of ARIMA and triple exponential smoothing.
    IEEE Transactions on Cloud Computing 1, 1 (2020), 1–17. Navigate to [80] Nigam
    Siddhant Kumar, Neha Muthiyan, Shaifu Gupta, A. D. Dileep, and Aditya. 2018. Association
    learning based hybrid model for cloud workload prediction. In Proceedings of the
    2018 International Joint Conference on Neural Networks. 1–8. Navigate to [81]
    Alfailakawi Mahmoud Imdoukh, Imtiaz Ahmad, and Mohammad Gh. 2019. Machine learning-based
    auto-scaling for containerized applications. Neural Computing and Applications
    1, 1 (2019), 1–16. Navigate to [82] Lu Yao, Liu Lu, Panneerselvam John, Yuan Bo,
    Gu Jiayan, and Antonopoulos Nick. 2020. A GRU-based prediction framework for intelligent
    resource management at cloud data centres in the age of 5G. IEEE Transactions
    on Cognitive Communications and Networking 6, 2 (2020), 486–498. Navigate to [83]
    Das Anirban, Imai Shigeru, Patterson Stacy, and Wittie Mike P. 2020. Performance
    optimization for edge-cloud serverless platforms via dynamic task placement. In
    Proceedings of the 2021 IEEE/ACM International Symposium on Cluster, Cloud, and
    Internet Computing. 41–50. Navigate to [84] Akhtar Nabeel, Raza Ali, Ishakian
    Vatche, and Matta Ibrahim. 2020. COSE: Configuring serverless functions using
    statistical learning. In Proceedings of the IEEE INFOCOM 2020 - IEEE Conference
    on Computer Communications. 129–138. Navigate to [85] Yang Zhe, Nguyen Phuong,
    Jin Haiming, and Nahrstedt Klara. 2019. MIRAS: Model-based reinforcement learning
    for microservice resource allocation over scientific workflows. In Proceedings
    of the 2019 IEEE International Conference on Distributed Computing Systems. 122–132.
    Navigate to [86] Cui Jieqi, Chen Pengfei, and Yu Guangba. 2020. A learning-based
    dynamic load balancing approach for microservice systems in multi-cloud environment.
    In Proceedings of the 2020 IEEE International Conference on Parallel and Distributed
    Systems. 334–341. Navigate to [87] Majeed Ayesha Abdul, Kilpatrick Peter, Spence
    Ivor, and Varghese Blesson. 2019. Performance estimation of container-based cloud-to-fog
    offloading. In Proceedings of the 2019 IEEE/ACM International Conference on Utility
    and Cloud Computing Companion. 151–156. Reference [88] Tunde-Onadele Olufogorehan,
    He Jingzhu, Dai Ting, and Gu Xiaohui. 2019. A study on container vulnerability
    exploit detection. In Proceedings of the 2019 IEEE International Conference on
    Cloud Engineering. 121–127. Reference 1Reference 2 [89] Zou Zhuping, Xie Yulai,
    Huang Kai, Xu Gongming, Feng Dan, and Long Darrell. 2019. A docker container anomaly
    monitoring system based on optimized isolation forest. IEEE Transactions on Cloud
    Computing 1, 1 (2019), 1534–1543. Reference [90] Shu Rui, Gu Xiaohui, and Enck
    William. 2017. A study of security vulnerabilities on docker hub. In Proceedings
    of the 2017 ACM on Conference on Data and Application Security and Privacy. 269–280.
    Reference [91] Malawski Maciej, Gajek Adam, Zima Adam, Balis Bartosz, and Figiela
    Kamil. 2020. Serverless execution of scientific workflows: Experiments with HyperFlow,
    AWS Lambda, and Google cloud functions. Future Generation Computer Systems 110,
    1 (2020), 502–514. Reference [92] Rzadca Krzysztof, Findeisen Pawel, Swiderski
    Jacek, Zych Przemyslaw, Broniek Przemyslaw, Kusmierek Jarek, Nowak Pawel, Strack
    Beata, Witusowski Piotr, Hand Steven, and Wilkes John. 2020. Autopilot: Workload
    autoscaling at Google. In Proceedings of the 2020 European Conference on Computer
    Systems. 1–16. Reference [93] Lv Jingze, Wei Mingchang, and Yu Yang. 2019. A container
    scheduling strategy based on machine learning in microservice architecture. In
    Proceedings of the 2019 IEEE International Conference on Services Computing. 65–71.
    Reference [94] Wang Shangguang, Guo Yan, Zhang Ning, Yang Peng, Zhou Ao, and Shen
    Xuemin. 2021. Delay-aware microservice coordination in Mobile edge computing:
    A reinforcement learning approach. IEEE Transactions on Mobile Computing 20, 3
    (2021), 939–951. Reference 1Reference 2Reference 3 Cited By View all Lin W, Xiong
    C, Wu W, Shi F, Li K and Xu M. (2022). Performance Interference of Virtual Machines:
    A Survey. ACM Computing Surveys. 55:12. (1-37). Online publication date: 31-Dec-2024.
    https://doi.org/10.1145/3573009 Daraghmeh M, Agarwal A and Jararweh Y. Cloud Workload
    Categorization Using Various Data Preprocessing and Clustering Techniques. Proceedings
    of the IEEE/ACM 16th International Conference on Utility and Cloud Computing.
    (1-10). https://doi.org/10.1145/3603166.3632131 Iftikhar S, Gill S, Song C, Xu
    M, Aslanpour M, Toosi A, Du J, Wu H, Ghosh S, Chowdhury D, Golec M, Kumar M, Abdelmoniem
    A, Cuadrado F, Varghese B, Rana O, Dustdar S and Uhlig S. (2023). AI-based fog
    and edge computing: A systematic review, taxonomy and future directions. Internet
    of Things. 10.1016/j.iot.2022.100674. 21. (100674). Online publication date: 1-Apr-2023.
    https://linkinghub.elsevier.com/retrieve/pii/S254266052200155X Show All Cited
    By Index Terms Machine Learning-based Orchestration of Containers: A Taxonomy
    and Future Directions Computer systems organization Architectures Distributed
    architectures Cloud computing General and reference Document types General literature
    Recommendations Container orchestration on HPC systems through Kubernetes Abstract
    Containerisation demonstrates its efficiency in application deployment in Cloud
    Computing. Containers can encapsulate complex programs with their dependencies
    in isolated environments making applications more portable, hence are being adopted
    in ... Read More Cloud resource provisioning: survey, status and future research
    directions Cloud resource provisioning is a challenging job that may be compromised
    due to unavailability of the expected resources. Quality of Service (QoS) requirements
    of workloads derives the provisioning of appropriate resources to cloud workloads.
    Discovery ... Read More The Prospects for Multi-Cloud Deployment of SaaS Applications
    with Container Orchestration Platforms Middleware Doctoral Symposium''16: Proceedings
    of the Doctoral Symposium of the 17th International Middleware Conference Recent
    years have seen an increased adoption of container technology for software deployment
    and lightweight virtualization. More recently, container orchestration systems
    provide a platform for container deployment and management of cluster resources.
    Read More Comments 94 References View Issue’s Table of Contents Footer Categories
    Journals Magazines Books Proceedings SIGs Conferences Collections People About
    About ACM Digital Library ACM Digital Library Board Subscription Information Author
    Guidelines Using ACM Digital Library All Holdings within the ACM Digital Library
    ACM Computing Classification System Digital Library Accessibility Join Join ACM
    Join SIGs Subscribe to Publications Institutions and Libraries Connect Contact
    Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library is published
    by the Association for Computing Machinery. Copyright © 2024 ACM, Inc. Terms of
    Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: ACM Computing Surveys
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Machine Learning-based Orchestration of Containers: A Taxonomy and Future
    Directions'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
