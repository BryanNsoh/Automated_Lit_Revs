- DOI: https://doi.org/10.1109/uemcon.2018.8796814
  analysis: '>'
  authors:
  - Jay Shah
  - Dushyant Dubaria
  - John Widhalm
  citation_count: 11
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2018 9th IEEE Annual Ubiquito...
    A Survey of DevOps tools for Networking Publisher: IEEE Cite This PDF Jay Shah;
    Dushyant Dubaria; John Widhalm All Authors 10 Cites in Papers 2121 Full Text Views
    Abstract Document Sections I. INTRODUCTION II. BACKGROUND III. DEVOPS TOOLS IV.
    CI/CD PIPELINE V. CREATING DEVOPS INFRASTRUCTURE Show Full Outline Authors Figures
    References Citations Keywords Metrics Abstract: DevOps is a new methodology that
    combines developers and operations team to closely integrate people, processes
    and technology for an automated software delivery that is agile, scalable and
    cost-effective. DevOps delivery is a combination of two previous methods; agile
    software development and the collaboration between developments and operations
    team by changing the way we think to deploy an infrastructure through the entire
    product life-cycle, from the design through the production phase, we can deliver
    consistent and repeatable designs. By abstracting the configuration needed to
    launch server instances with specific configurations into consistently repeatable
    recipes or manifests, the entire technology stack becomes converged. In order
    to do this, both operation and development skills are required. We have GIT as
    source control tool, Ansible as configuration management, Jenkins as configuration
    integration, Vagrant as provisioning and Docker as containerization tool. With
    all these tools we can build an entire Continuous Integration and Continuous Delivery
    pipeline. Published in: 2018 9th IEEE Annual Ubiquitous Computing, Electronics
    & Mobile Communication Conference (UEMCON) Date of Conference: 08-10 November
    2018 Date Added to IEEE Xplore: 15 August 2019 ISBN Information: DOI: 10.1109/UEMCON.2018.8796814
    Publisher: IEEE Conference Location: New York, NY, USA SECTION I. INTRODUCTION
    DevOps evokes an agile software development approach in an operational environment
    [1]. Deployment is the period between the completion of the code by the developers
    and then placing of the code into normal production. Modern development organizations
    require entire teams of DevOps to automate and reduce the gap between the development
    team and the operation team [14]. This developer/operations collaboration enables
    to manage the complexity of real-world problems. This paper layout the basic concepts,
    reviews and surveys of DevOps as a whole, philosophy, workflow, monitoring methods
    etc. DevOps uses tools such as Git, Vagrant, Docker, Ansible, Jenkins [3]. We
    have integrated all the above tools to work together to host an application-based
    web server. And to test all our we have built a CI/CD pipeline. SECTION II. BACKGROUND
    Fig. 1: DevOps Culture of Collaboration Show All DevOps acknowledges the interdependence
    of software development, quality assurance, and IT operations, and intends to
    help an organization rapidly produce software products and services and to improve
    operations performance [15] [16]. Operations are simply the set of processes and
    services provisioned by IT personnel to meet the requirements of their own internal
    or external clients in support of the business objectives. It is delineated in
    several ways: Infrastructure and Monitoring, Architecture and Planning, Maintenance
    and Support Although not necessarily exclusively, in IT, development refers to
    the process of creating software. It involves the programming, documenting, testing
    and debugging software with application development and the associated software
    release lifecycle. There are several methodologies for doing: Prototyping, Waterfall,
    Agile and Rapid [1]. Implementing DevOps also means measuring and monitoring efficiently.
    As a result, having effective monitoring is crucial across all components of infrastructure
    and networking because various monitoring points can be aggregated and monitored
    as real-time feedback which results in a much more rapid root cause analysis [14].
    Having an effective monitoring solution helps to facilitate the metric of mean
    time to resolve issues. As a result, when a production issue occurs, the source
    of the problem can be isolated and resolved in near-real time fashion [12]. This
    automated approach to issue resolution has obvious benefits in terms of ensuring
    minimal disruption of the business objectives. SECTION III. DEVOPS TOOLS A. VAGRANT
    Virtualization is not a new technology. It has the roots as far as back the early
    1970s when the first IBM solution for Time Sharing was introduced [5]. Virtualization
    simply refers to abstracting the hardware components of a physical machine so
    that it can operate multiple virtual machines sharing local resources. Vagrant
    is one of the DevOps tools that help automate infrastructure provisioning through
    the use of various providers which include VirtualBox, VMware, Hyper-V and AWS
    Cloud. In addition, Vagrant supports a variety of provisioners such as Ansible,
    Chef, Puppet, Salt, etc. Vagrant uses the command line and a text file called
    Vagrantfile to operate. Vagrant is configured such that a directory is created
    for each project [18]. Each machine will have its own directory where the Vagrant
    commands will be issued and directed only to the virtual machine associated with
    that project. In that directory, Vagrant uses the concept of boxes to provide
    an OS image [1]. A Vagrant box is a packaged environment like a template where
    the box can be created to support not only basic operating systems but also any
    application you wish to include. Vagrant boxes are also community driven and Vagrant
    cloud supports a plethora of Vagrant boxes for users to contribute [19]. Vagrant
    supports three modes of network access to the guest VM''s: Port forwarding, Private
    Network and Public Network. Vagrant widely supports the variety of ways to provision
    the virtual machine: Shell scripting, Ansible playbooks, Chef recipes, Puppet
    and Salt. Docker is also a very flexible platform which supports the use of multiple
    providers. B. DOCKER Docker is the way to create Linux containers. Docker is a
    virtualization technology which does not rely on containers instead, it uses a
    Docker engine [2]. Hypervisors are used in virtual machines and Docker helps to
    run multiple software applications on the same machine. Each of which is run in
    an isolated environment called container. The software is packaged in the container
    and, hence, can be run on any machine that has the Docker engine installed. A
    container is a closed environment for the software and bundles all the files and
    libraries that the application needs to function correctly. Multiple containers
    can be deployed on the same machine and share the resources. Containers are application-centric
    and rely on the same physical hardware of the host machine and run the application
    in an isolated environment. Additionally, containers use fewer resources than
    virtual machines [20]. You cannot containerize a Windows or a MAC application.
    Docker can be installed on Windows, Linux, and MAC. But it can only run applications
    designed to work on a Linux kernel. Docker has two versions: the community edition
    (CE) which is free of charge but with community support and the enterprise edition
    (EE) is a paid version with enterprise-level support from Docker. Vagrant uses
    Vagrantfile while Docker uses Dockerfile for building containers and images. Building
    an image is an incremental process for example; you may download the Apache web
    server image (httpd) and use a Dockerfile to install components on top of it like
    a specific version of PHP [5]. You can add new packages, change configuration
    files, create new users and groups, and even copy files and directories to the
    image. C. GIT A Version Control System (VCS) is the management of changes to any
    kind of document but is usually used to control and manage software source code
    or scripts [8]. In addition, a VCS is also known as revision control or source
    control. There are two type of Version control system: Centralized VCS and distributed
    VCS. A centralized version control system is a client-server approach. The client
    checks out a copy from the centralized repository. All these data and history
    is stored and retrieved from the central repository. On the other hand, we have
    the distributed Version control system which is a peer to peer approach. Over
    copy is a repository and contains full project history [7]. Network connection
    is needed when it is to be shared. Version controlling our code provides several
    benefits: The ability to work on the same project by more than one person at the
    same time. Each person will have a copy of the source code to do whatever changes
    required, then changes are merged with the main code, after the necessary approvals.
    The history stored by a version control system would allow a developer to examine
    changes to faulty code. The code can be branched. Developers can work on separate
    branches of the source code to create and test new features. In the end, a branch
    can be merged with the main branch (called master). GitHub is a repository for
    controlled documents which supports Git [21]. It was launched in 2008. It offers
    a version control repository that can be used free or for a fee, a private repository
    that can be used [9]. Git can be integrated with GitHub so that the repository
    can be accessed over the Internet from anywhere in the world instead of having
    to create a local repository server. Fig. 2: GIT Working Flow Show All To keep
    a backup of our important files we use cloud services provided by Dropbox, Google
    Drive. But to store and back up our codes we use GitHub. Git organizes our files
    into a repository. A repository can contain any number of files organized the
    way we want. A single repository usually contains all the code used by a single
    project. Git stores a copy of the repository on a local machine. But to submit
    and to backup your work, you are going to push your changes to the GitHub, Git
    hosting service. D. JENKINS Jenkins is both a Continuous Integration and Continuous
    Deployment tool [6]. Jenkins supports a variety of configuration management tools
    such as Ansible and Chef. We have already introduced this Configuration management
    tool as individual components. However, Jenkins can be used to call Chef cookbooks
    or Ansible playbooks to facilitate the automated delivery of infrastructure configuration
    [17]. Jenkins is used more often in software development for building deployments,
    it supports many plugins that enable anything from deploying scripts to launch
    virtual machines through VMWare or Vagrant to Docker containers across environments.
    Jenkins allows to create and build jobs that do anything from deploying a simple
    software build to the custom creation of a Docker container with specific build
    branches while doing performance and unit testing while reporting results back
    to the team [6]. The variety of plugins you can download and install as well as
    the way you can manage and distribute the building load across multiple slave
    servers ensure that you have a robust and performant environment for automating
    all facets of your build, integration and deployment process. E. ANSIBLE Ansible
    is a configuration management tool like other tools such as Chef, Puppet and Salt
    [2] [4]. Ansible is agent-less which means that no additional software is required
    to be installed on the target machines. Ansible is written in Python and uses
    Playbooks written in YAML. In addition, it relies on SSH to connect to the managed
    hosts and on host configuration file in which the hosts to be managed are placed.
    In addition, password-less login via SSH can be created which allows. To install
    Ansible on any machine you need to first install Python because python is the
    prerequisite to install Ansible as it need python libraries to run. Ansible to
    run in an unattended mode of sorts. Ansible works in two modes: ad-hoc line and
    playbook files. In ad-hoc line mode, you pass the instructions to the remote servers
    as arguments and parameters to an ansible command. In the playbook files mode,
    you type those instructions in a YAML file. YAML is an abbreviation for Yet Another
    Markup Language. It is used the same as JSON files to serialize data in an easy
    format that is readable by both humans and machines. Ansible uses /etc/ansible/hosts
    file to describe the machines you want to be managed or configured by Ansible
    [1]. This file contains machine hostnames, IP addresses or a mix of them. This
    depends on how the server machine (the one with Ansible installed) can communicate
    with target hosts. Hosts can be added either in named groups for easier reference
    or as stand-alone entries. The Ansible hosts files support in a variety of keywords.
    The most common use is a simple file contains hostnames using password-less SSH.
    SECTION IV. CI/CD PIPELINE CI/CD helps increase the velocity of development. This
    helps to finish the development process faster but will also make adding new features
    and resolving software defects much faster. Using Continuous Integration (CI),
    you pull together the various software components-build and test them as a single
    unit. Defects must be addressed immediately and are the reason for real-time feedback
    in builds. The artifacts created by a CI/CD process must always result in a clean
    build. In CI, the application should not stay overnight in a non-stable state.
    This means that any software defects which occurs after the commit must be resolved
    as soon as possible. So, the time to fix bugs will be kept to a minimum as every
    minor failure will be solved early enough before causing more severe bugs. Developers
    will always be able to pull a clean copy of the application from the repository;
    as any failing code will not be allowed to be pushed to the central repository.
    No human intervention exists in the operation, so there are very fewer chances
    of manual error which increases the efficiency. Special tools are designed for
    this such as Jenkins. Combined with a test framework like PHP Unit. Fig. 3: CI/CD
    Pipeline Show All Continuous Integration: It is the one with least concern as
    it has the minimum automation [16]. CI stops when the committed code is tested
    and integrated successfully into the master branch. CI has very less automation
    Continuous Delivery: Automation is a more than CI and delivers the application
    code from the master branch to the non-production servers instead of having to
    pull the changes manually from the repository Continuous Deployment: Over here,
    full automation is achieved. Code is deployed by the application code or to the
    binary artifacts and is given to the live production servers to be consumed by
    end users SECTION V. CREATING DEVOPS INFRASTRUCTURE We are building an environment
    to demonstrate the power of different DevOps tools. Git for the version-control
    system and source-code management. Vagrant to build and configure the lightweight
    development infrastructure [18]. Docker to create and run distributed applications
    or microservices inside the isolated container. Ansible for configuration management,
    application deployment and task automation. Jenkins [17] to centralize all the
    testing environment by building CI/CD pipeline which includes triggering auto
    builds, auto-promote builds from one environment to another, code analysis, auto
    version, etc. In this DevOps infrastructure, we are using Vagrant to spin multiple
    Virtual Machine’s (5VMs) on Virtual Box [10]. To further configure the VMs we
    have Ansible-playbooks to install plugins and packages. A client machine is where
    all the necessary codes is stored and manually pushed to the GitHub repository
    [13]. The Web Server is hosting and running the application. A Database Server
    is acting as a data repository for the application. To test any update in the
    code, we are using a CI/CD pipeline inside a Jenkins Server. We have created 3
    branches: Feature, Integration and Master branch on the GitHub [1]. Upon successful
    testing, the code is pushed to different branches for rigorous testing and after
    finishing all the different tests, the code is pushed back to the Web Development
    Server for the last and final test before it is pushed to live Web Server [10].
    Once the development and operation team are satisfied with the updated version
    of the application, it is finally live on the original Web Server. In case of
    any sudden failure, it will only take a few minutes to make a new server up and
    running. Also, all these tools give us the leverage to achieve consistency in
    the infrastructure. This is very vital in a big network to avoid any version conflicts.
    Fig. 4: DevOps Project Infrastructure Show All SECTION VI. ADVANTAGE By implementing
    a culture of DevOps, it initiates and promotes repeatability, measurement and
    consistency. Implementing automation naturally improves the velocity of change,
    increases the number of deployments a team can do in any given day and improves
    time to market [2]. A byproduct of automation is that the mean time to resolve
    will also become quicker for infrastructure issues. If infrastructure or network
    changes are automated, they can be applied much more efficiently than if they
    were carried out manually [11]. Manual changes depend on the velocity of the engineer
    implementing the change rather than an automated script that can be measured more
    accurately. Utilizing automation and effective monitoring also means that all
    members of a team have access to see how processes work and how fixes and new
    features are pushed out. SECTION VII. CONCLUSION In a nutshell, DevOps brings
    a holistic approach to the complete business delivery system. DevOps combines
    developers and operations team to integrate people, processes and technology for
    an automated software delivery that is agile, scalable and cost-effective. Authors
    Figures References Citations Keywords Metrics More Like This Implementation of
    a Continuous Integration and Deployment Pipeline for Containerized Applications
    in Amazon Web Services Using Jenkins, Ansible and Kubernetes 2020 19th RoEduNet
    Conference: Networking in Education and Research (RoEduNet) Published: 2020 Efficient
    Automation of Web Application Development and Deployment Using Jenkins: A Comprehensive
    CI/CD Pipeline for Enhanced Productivity and Quality 2023 International Conference
    on Self Sustainable Artificial Intelligence Systems (ICSSAS) Published: 2023 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: A Survey of DevOps tools for Networking
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/fie43999.2019.9028598
  analysis: '>'
  authors:
  - Rachel A. Jennings
  - Gerald C. Gannod
  citation_count: 13
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 IEEE Frontiers in Educat...
    DevOps - Preparing Students for Professional Practice Publisher: IEEE Cite This
    PDF Rachel A. Kaczka Jennings; Gerald Gannod All Authors 13 Cites in Papers 652
    Full Text Views Abstract Document Sections I. Introduction II. Background III.
    Course Content IV. Lessons Learned V. Conclusions and Future Investigations Authors
    References Citations Keywords Metrics Abstract: This work in progress paper presents
    a course on DevOps which is a combination of software development skills and software
    operations skills. This new course is for sophomores and juniors in the computer
    science program who want to be prepared for professional software engineering
    careers. Introduction to DevOps Is a hands-on laboratory course that brings students
    through Git for source code management, Capybara for automated testing, AWS, Docker,
    and Ansible for automated virtual machine provisioning and configuration, and
    Jenkins for Continuous Integration. Unlike our current course offerings which
    primarily focus on the single developer context in a localized environment, this
    course prepares students for highly collaborative, team-based projects that use
    cloud resources to facilitate management of the software deployment pipeline.
    We developed this course based on feedback from our external advisory board and
    under consultation from a number of industrial partners. This is complementary
    to our current offerings in software engineering which focus on Agile software
    practices. In this paper we describe the core concepts, the design, learning experiences,
    technologies, and lessons learned through developing and conducting this course.
    In future work we hope to present student perceptions of learning and provide
    data collected through direct assessment of student outcomes. Published in: 2019
    IEEE Frontiers in Education Conference (FIE) Date of Conference: 16-19 October
    2019 Date Added to IEEE Xplore: 12 March 2020 ISBN Information: ISSN Information:
    DOI: 10.1109/FIE43999.2019.9028598 Publisher: IEEE Conference Location: Covington,
    KY, USA SECTION I. Introduction DevOps is a software practice that focuses on
    enabling software practitioners to combine the management of software development
    with infrastructure/operations practices. In its purest form, DevOps integrates
    development and operations practices through the use of tools, practices, and
    automation of those practices in order to streamline the development and delivery
    of software. DevOps includes a wide variety of practices encompassing coding,
    testing, configuration management, integration, packaging, deployment, and monitoring
    (to name a few). A recent survey by 2nd Watch queried over 1,000 IT professionals
    in large companies (> 1000 employees) to determine the degree of adoption of DevOps
    [1]. One of the takeaways of their survey is that while many organizations are
    interested in more widespread adoption of DevOps (70%), only 25% are actively
    engaging in true DevOps practices. Through our conversations with our department
    advisory board, employers and other industry partners, we received feedback on
    the growing desire to have graduates that are educated on the use of approaches
    and tools to better prepare students to engage in DevOps practice. Using that
    feedback, we convened a group of practitioners from private, public, and government
    systems sectors to assist us in defining desired outcomes for what will ultimately
    become a sequence of DevOps courses within our computer science program. In this
    paper, we describe the design, outcomes, topics, and learning activities of a
    course meant to pilot our efforts to institutionalize this sequence. Indeed, neither
    the Software Engineering Body of Knowledge [2] nor the Computer Science Curriculum
    of the Joint ACM and IEEE Task Force [3] make mention of this topic; accordingly,
    we are sharing our experience in attempting to introduce these topics into our
    curriculum. In addition, we share some initial lessons learned through the delivery
    of the first offering of this course. Finally, we describe our future plans and
    investigations towards incorporating these topics more permanently into our computer
    science program. The remainder of this paper is organized as follows. Section
    III summarizes the outcomes of a consulting session we conducted with industry
    partners over perceived needs in the DevOps practice. In Section III we also discuss
    the instructional methodology used in the course as well as a description of the
    various learning activities used throughout the pilot course. In Section IV, we
    describe a number of lessons learned from teaching this course. Finally, in Section
    V, we draw conclusions and discuss future investigations. SECTION II. Background
    DevOps is not yet a clearly defined term. Atlassian, a leader in Agile development,
    defines DevOps as a “Set of practices that automates the processes between software
    development and IT teams” so that these teams can “build, test, and release software
    faster and more reliably” [4]. The Agile Admin describes DevOps as “the practice
    of operations and development engineers participating together in the entire service
    lifecycle” including both development and production support [5]. Bass describes
    DevOps as “a set of practices intended to reduce the time between committing a
    change” and that change “being placed into normal production” [6]. While these
    definitions differ, it is clear that they share at least three common ideas. Firstly,
    DevOps is a combination of developer and operation roles. DevOps practitioners
    must either take on aspects of both of these roles or have a strong collaboration
    between these two, traditionally independent, teams. Secondly, DevOps practices
    are focused on the entire pipeline from the first time a developer makes a change
    to the source code all the way to the code’s life-cycle in a production environment.
    Thirdly, the practices of DevOps need to be repeatable and automated. This allows
    DevOps practitioners to rapidly deploy changes and to maintain consistency between
    users. While not common, some work has been done in the area of teaching DevOps
    in classrooms. For instance, both Udemy and Coursera offer regular courses on
    DevOps related topics. However, as the topic of DevOps has not yet been standardized
    within either the Software Engineering Body of Knowledge [2] nor the Computer
    Science Curriculum Guidelines [3], a consistent set of courses and model for DevOps
    curricula is still in need of development. Rong et al. proposed a system for DevOps
    education called DevOpsEnvy [7]. This system proposes a solution to utilizing
    DevOps technology in the classroom by providing a supporting system. Educators
    can use this system to enforce a particular workflow through a set of predefined
    DevOps tools. Christensen also discusses DevOps education in a classroom with
    Cloud Computing [8]. However, this course content does not prioritize the full
    DevOps pipeline. The focus is instead on Cloud Computing. Other work has shown
    the importance of incorporating DevOps practices and tools into education including
    [9], [10], [11]. SECTION III. Course Content A. Consulting Session Our original
    conversations regarding creating a DevOps course were catalyzed by visits with
    our external advisory board and employers that typically hire graduates of our
    department. Based on this demand we convened a group of practitioners that were
    leaders of infrastructure teams responsible for their respective companies’ DevOps
    practice. We focused the conversation on three primary topics of discussion: 1)
    Motivations, 2) Desired Capabilities, and 3) Resources. In regards to motivation,
    the participants were interested in graduates that have exposure to concepts related
    to DevOps. In this context, they had the perspective that exposure and experience
    in these areas outweigh using specific technologies due to the proliferation of
    tools being used in this space. In regards to desired capabilities, we queried
    the group regarding some of those tools being used in their organizations. The
    categories of choices fell within the areas of configuration management with tools
    like Git [12], containerization with tools like Docker [13], and platforms such
    as AWS [14] and Kubernetes [15]. Finally, in regards to resources, we asked participants
    about the availability of resources that they use to train their staff. Generally,
    the resources identified were unavailable to us, or were resources that come from
    large training organizations. B. General Course Structure The pilot course ran
    as a full semester course with thirty (30) students. Class met twice weekly for
    eighty (80) minutes. Students ranged in year from Sophomore to Senior. Students
    were expected to have completed a prerequisite 1-credit Unix laboratory course
    that teaches the basics of navigation, scripting, and other command-line fundamentals.
    Course instruction prioritized in-class labs with supplemental instructional material
    for students outside of class time in an inverted [16] (or flipped) style. Students
    are meant to read or watch supplemental material from a variety of sources prior
    to the start of a lab. Material varies from documentation to practical examples,
    and tutorials to conceptual information. When appropriate, existing supplemental
    material could not be found, short lectures were used to disseminate the required
    information for the lab assignment. Evaluation of students included twelve in-class
    labs and exams. Labs varied in length from a single day to multiple weeks. Lab
    assignments required students to submit a short write-up related to the lab material
    in a blog format. Ideally, labs were completed utilizing only in-class time, allowing
    students to use out-of-class time to review the supplemental material. Students
    submitted their lab work primarily through an institutionally hosted GitLab, an
    implementation of Git, instance and were marked using Git tags. If a lab required
    additional external services, such as AWS Educate [17], a program designed to
    provide limited free usage of AWS services to educators and their students, or
    Jenkins [18], their work was validated by visiting the appropriate service. C.
    Course Topics, Outcomes, and Laboratories Using all of the information that we
    gathered from our consulting session, we developed a number of outcomes for the
    pilot course. In particular, the desired outcomes are as follows: upon completion
    of the course, • Students understand the high-level purpose, terminology and architecture
    of DevOps pipelines • Students can use code repositories • Students can use technologies
    for automating the management of infrastructure • Students can use container technology
    • Students can use orchestration technologies for automating DevOps pipelines
    Basics of DevOps. Upon completion of this course, students are expected to be
    able to understand the purpose, terms, and underlying approaches of DevOps. Our
    intention is for students to understand the foundations related to the approaches
    in order to eventually use different technology variants that support the approaches.
    We assessed this knowledge with a combination of multiple choice, short answer,
    and other essay or problem solving questions. We also used an initial laboratory
    to get student work environments configured. • Example Exam Question. Explain
    the basic Git workflow. You may use a diagram to aid your explanation. Be sure
    to include what commands are used at each stage. Be sure to include the following
    working directory, staging area, local repository, and remote. • Lab 00 - Preliminaries.
    The first laboratory covered preliminary material from the prerequisite Unix course
    and initial setup of student laptops. Students were expected to review a Git tutorial,
    setup an Ubuntu VM (version 16 or 18), and review basic Unix commands including
    navigation, file management, file creation and editing. Source Code Management
    and Repositories. Upon completion of this course, students are expected to have
    knowledge and experience in the use of Source Code Management (SCM), repositories
    and their associated technologies including, for example, Git. SCM systems are
    essential to managing source code changes among developers. SCM is widely used
    even outside of DevOps practices. An SCM system such as Git allows developers
    to track changes made by themselves or others, the reasons those changes were
    made, and manage multiple versions of their software. Git is one of the most popular
    tools among developers. Websites such as GitHub [19] and GitLab [20] provide a
    central repository system where changes are shared amongst developers [12]. Students
    demonstrate their ability to use SCM tools via actions such as adding, and updating
    files in a repository, creating branches, and merging those branches back into
    the mainline branch of a repository. Hands-on experience was initially provided
    using online resources such as Code Academy [21] and Katacoda [22], and later
    supplemented with laboratory assignments as follows: • Lab 01 - Working with a
    Git Repository. Students were expected to create a Git repository, create a simple
    static website using Jekyll [23], make changes to the site, and use Git to manage
    those changes. • Lab 03 - Git revisited. Students were expected to be able to
    create, manage, and merge Git branches as well as use Git logs. To highlight how
    to use Git branches, multiple branches were made and changes made were committed
    in each branch and then merged. Some of this work was adopted from [24]. • Lab
    02 - Testing. The testing laboratory covered basic automated existence testing.
    Students were expected to create basic automated tests using RSpec [25], Capybara
    [26], and Selenium [27]. Students were given a basic example and the required
    startup code. Students then made changes to their Jekyll sites and verified those
    changes via automated tests. Configuration and Provisioning. Upon completion of
    this course, students are expected to have knowledge and experience in the use
    of configuration and provisioning tools that support Infrastructure as Code (IaC).
    IAC is the concept that infrastructure should not be highly customized, requiring
    special, often manual maintenance or care, but rather infrastructure should be
    the same across as many instances as possible. Handling our infrastructure in
    this way allows us to create a repeatable process that can be automated. We can
    then ‘code’ this infrastructure with a variety of tools. Ansible [28], Puppet
    [29], and Chef [30] are all examples of IaC tools that use automation techniques
    to deploy software, along with it’s dependencies, and manage it’s configuration.
    In our course, this is primarily achieved through the use of Ansible [28]. Students
    demonstrate their ability to use IAC tools via creating scripts that automate
    the process of configuring a virtual machine through installation of dependencies
    and other software needed to support development. Three laboratories were used
    to assess student knowledge for this outcome: Lab 04 - Manual Infrastructure Setup.
    The labs for this outcome covered manual infrastructure setup and an introduction
    to IaC and automation. Students were expected to manually setup an NGINX production
    server on an AWS EC2 instance to host a Jekyll based site. Lab 05 - Infrastructure
    Setup with Ansible. In the next laboratory, students were tasked with creating
    an example Ansible playbook for an Apache server. In this context, students were
    recreating the manual setup of a server. Lab 06 - Production Server with Ansible.
    In the following laboratory, students created an Ansible playbook for the automated
    deployment of their Jekyll site. The playbook installed NGINX and then copied
    their local site files to the remote server for hosting. Containers. Upon completion
    of this course, students are expected to have knowledge and experience in the
    use of containers. Container-based tools, such as Docker [13], approach IaC with
    a similar idea to tools such as Ansible. However, where IaC tools like Ansible
    seek to automate actions normally handled by a user, containers seek to package
    an application, it’s dependencies, and configurations into a single deployable
    unit. This allows infrastructure configurations to be moved from one set of hardware
    to another more rapidly than other IaC tools, and abstract even farther on the
    idea of infrastructure being generic units [31] [32]. Students demonstrate their
    ability to use containers via creating an image template that contains the instructions
    for creating a full virtualized deployment of some target (i.e., developed software).
    In addition, students should be able to fully deploy those images so that the
    target software is executable using an instance of the image. In this course we
    used Docker [13] as the technology of choice. The laboratories for this outcome
    were as follows: Lab 07 Docker Basics. The container-based laboratories covered
    container creation and use in Docker. Students completed an introductory tutorial
    [33] provided by Docker to familiarize themselves with Docker commands. Lab 08
    Docker applied. Students were expected to create a Docker image for both production
    and development environments using Docker files. The production Docker image was
    similar to that of the production environment created in previous labs and the
    development image matched the environment that they used on their individual VMs.
    Students were then expected to run containers, which is similar to running an
    application or initializing a VM, from each Docker image created from Docker files.
    Continuous Integration and Delivery. Upon completion of this course, students
    are expected to have knowledge and experience in the use of Continuous Integration
    (CI). CI is the practice of integrating various source code changes from multiple
    developers frequently, often several times a day [34]. Automated tests can be
    run on each new change and if the tests pass, the change can be integrated with
    the existing code base, in accordance with the SCM [35]. CI allows practitioners
    to use Continuous Delivery tools, which are often the same as the chosen CI tool.
    Continuous Delivery continues ideas of CI and posits that code should not only
    be integrated continuously, but it should also be built continuously. Ideally,
    a built and tested artifact should be ready and waiting for deployment after every
    code change. As such, Continuous Deployment is the process of continuously deploying
    a compiled artifact whenever it is available [35]. Tools for Continuous Integration,
    Delivery, and Deployment are often the same, orchestrating the entire pipeline
    as a single process. Ultimately this allows organizations to deploy updates in
    a matter of seconds [36]. In our course, this is primarily achieved through the
    use of Jenkins [18]. Students demonstrate their ability to use CI via creating
    a fully functioning CI pipeline within Jenkins that includes the use of automated
    testing. Students also demonstrate their ability to use Continuous Delivery tools
    via creating built artifact at the end of their CI process in the form of a deployable
    Docker container. The learning activities for this outcome consisted of three
    laboratories as follows: Lab 09 - Continuous Integration and Delivery. The CI
    labs covered basic continuous integration and delivery using Jenkins. Students
    were expected to establish a connection between their existing Git repository
    and a Jenkins freestyle project and establish a simple automated CI pipeline.
    Lab 10. In this lab, students modified the developed pipeline to merge changes
    into a production branch. A new multi-branch project was used to provide grading
    consistency. Lab 11. The final lab involved modifying the pipeline to orchestrate
    a complete multi-branch project with testing and continuous delivery. SECTION
    IV. Lessons Learned One of our goals with this course is to prepare students for
    Agile software development practices that integrate methodologies such as Scrum
    or Extreme Programming with DevOps. In creating this course we have learned a
    number of important lessons as follows. A. Preparation We have found that the
    level and depth of the prerequisite Unix course was insufficient for being prepared
    for the DevOps course. In particular, the students need more extensive ability
    to navigate the filesystem, edit files, use the commandline environment, and access
    and use appropriate help and documentation. Our intention, as we address below,
    is to split the DevOps course into at least two courses and to integrate the Unix
    laboratory into the first course. B. Content As we moved through the delivery
    of the course, we found that our pace was far too accelerated and lacked the depth
    necessary for a clear understanding of the technologies. In particular, we suffered
    from too much breadth and not enough depth. As we develop the DevOps course sequence
    we are planning on splitting this particular course into two courses: one that
    focuses on Unix fundamentals along with code management, automated test, and provisioning
    of infrastructure as code, and one course that focuses on continuous integration
    and continuous deployment using technologies such as Jenkins. C. Technology Concerns
    In this course we used a number of different technologies including Ansible, Docker,
    AWS, and Jenkins to name a few. Our use of AWS Educate was especially problematic
    given the constraint imposed both by their infrastructure as well as our own institutional
    security policies. In regards to AWS, in order to fully appreciate the DevOps
    pipeline, the Code Pipeline, Code Commit, Code Build, and Code Deploy are needed
    to provide the proper experience. Unfortunately, none of these tools are available
    within AWS Educate. In addition, AWS requires that students adequately monitor
    their allocated infrastructure. If a student, fails to shutdown their resources,
    they use their resource budget (measured in $) and risk being unable to access
    their resources should they expend their entire budget. As such, we used the tools
    we described in this paper. However, these tools are not without their own challenges.
    For instance, the security model for Jenkins precludes a simple method for isolating
    a given student’s work from other students. We are currently looking at other
    solutions and options to address this particular problem. SECTION V. Conclusions
    and Future Investigations There is a growing interest in DevOps as the logical
    progression of an Agile Development practice; it is the means by which organizations
    can achieve continuous integration and continuous delivery. The increased interest
    in DevOps has prompted a demand from employers to provide some level of instruction
    in computing programs. We have sought to work with our external advisory board,
    corporate partners, and potential employers to determine the desired outcomes
    and topics for addressing this need are, and have piloted the course described
    in this paper to better understand the challenges that must be met to institutionalize
    either a course or sequence in DevOps. Informally, the reception of the course
    by the students has primarily been positive, with challenges being related primarily
    to technology startup, lack of experience in pre-requisite topics, and the density
    of concepts introduced in the course. As we evolve this course, our intention
    is to reevaluate our technology stack to determine whether a different set of
    technologies provides a gentler entry into the space, integrate an existing one
    credit Unix laboratory course into this course, and to split the breadth of the
    topics into two separate courses to provide a more in-depth understanding of the
    tools and practices associated with this course, with the first course to cover
    IaC and basic automation and the second course to cover CI and CD. Authors References
    Citations Keywords Metrics More Like This PARES: a software tool for computer-based
    testing and evaluation used in the Greek higher education system IEEE International
    Conference on Advanced Learning Technologies, 2004. Proceedings. Published: 2004
    Teaching Practices of Software Testing in Programming Education 2020 IEEE Frontiers
    in Education Conference (FIE) Published: 2020 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: DevOps - Preparing Students for Professional Practice
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/s21051910
  analysis: '>'
  authors:
  - Aneta Poniszewska-Marańda
  - Ewa Czechowska
  citation_count: 13
  full_citation: '>'
  full_text: ">\nsensors\nArticle\nKubernetes Cluster for Automating Software\nProduction\
    \ Environment\nAneta Poniszewska-Mara´nda *,†\nand Ewa Czechowska †\n\x01\x02\x03\
    \x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\nCitation: Poniszewska-Mara´nda,\
    \ A.;\nCzechowska, E. Kubernetes Cluster\nfor Automating Software Production\n\
    Environment. Sensors 2021, 21, 1910.\nhttps://doi.org/10.3390/s21051910\nAcademic\
    \ Editor: Yuh-Shyan Chen\nReceived: 15 January 2021\nAccepted: 4 March 2021\n\
    Published: 9 March 2021\nPublisher’s Note: MDPI stays neutral\nwith regard to\
    \ jurisdictional claims in\npublished maps and institutional afﬁl-\niations.\n\
    Copyright: © 2021 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article\
    \ is an open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of\
    \ the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nInstitute of Information Technology, Lodz University of Technology, 90-924\
    \ Lodz, Poland; Ewa@kudulab.io\n* Correspondence: aneta.poniszewska-maranda@p.lodz.pl\n\
    † These authors contributed equally to this work.\nAbstract: Microservices, Continuous\
    \ Integration and Delivery, Docker, DevOps, Infrastructure as\nCode—these are\
    \ the current trends and buzzwords in the technological world of 2020. A popular\
    \ tool\nwhich can facilitate the deployment and maintenance of microservices is\
    \ Kubernetes. Kubernetes is\na platform for running containerized applications,\
    \ for example microservices. There are two main\nquestions which answer was important\
    \ for us: how to deploy Kubernetes itself and how to ensure\nthat the deployment\
    \ fulﬁls the needs of a production environment. Our research concentrates on the\n\
    analysis and evaluation of Kubernetes cluster as the software production environment.\
    \ However,\nﬁrstly it is necessary to determine and evaluate the requirements\
    \ of production environment. The\npaper presents the determination and analysis\
    \ of such requirements and their evaluation in the case\nof Kubernetes cluster.\
    \ Next, the paper compares two methods of deploying a Kubernetes cluster:\nkops\
    \ and eksctl. Both of the methods concern the AWS cloud, which was chosen mainly\
    \ because of its\nwide popularity and the range of provided services. Besides\
    \ the two chosen methods of deployment,\nthere are many more, including the DIY\
    \ method and deploying on-premises.\nKeywords: software production environment;\
    \ production environment; Kubernetes; Amazon Web\nServices (AWS); Amazon Elastic\
    \ Kubernetes Service (EKS); operations automation\n1. Introduction\nMicroservices,\
    \ Continuous Integration and Delivery, Docker, DevOps, and Infrastruc-\nture as\
    \ Code—these are the current trends and buzzwords in the technological world of\n\
    2020. A popular tool which can facilitate the deployment and maintenance of microser-\n\
    vices is Kubernetes. It is a platform for running containerized applications,\
    \ for example\nmicroservices. Kubernetes provides mechanisms for maintaining,\
    \ deploying, healing and\nscaling containerized microservices. Thanks to that,\
    \ Kubernetes hides the complexity of\nmicroservices orchestration. It is much\
    \ easier to satisfy non-functional requirements of\nmicroservices deployment by\
    \ using Kubernetes. An example of such requirements may be:\navailability, healing,\
    \ redundancy or autoscaling [1–3]. There are two main questions which\nanswer\
    \ was important for us: how to deploy Kubernetes itself and how to ensure that\
    \ the\ndeployment fulﬁlls the needs of a production environment.\nThere are plenty\
    \ methods of Kubernetes cluster deployment. The methods differ in\nrelation to:\
    \ how much customization they offer, which clouds they support, how much they\n\
    cost. Some of the methods has existed since the Kubernetes was created, the other\
    \ ones,\nsuch as AWS EKS, were invented later. There already exist comparisons\
    \ between different\ndeployment methods of a Kubernetes cluster. However, to the\
    \ best of our knowledge, these\ncomparisons either: are based on theoretical information\
    \ (e.g., technical documentation) or\nare created through a informal, non-academic\
    \ process and presented as a blog post or they\nconsider only a vanilla cluster.\n\
    There exist sources which compare several methods of Kubernetes cluster deployment.\n\
    However, they are either informal sources (e.g., blog posts or Internet tutorials)\
    \ or they\ndo not compare the two methods selected in our research or they do\
    \ not consider the\nSensors 2021, 21, 1910. https://doi.org/10.3390/s21051910\n\
    https://www.mdpi.com/journal/sensors\nSensors 2021, 21, 1910\n2 of 24\nproduction\
    \ environment. Therefore, this paper discussing the comparison of two chosen\n\
    methods offers the aspects not presented before.\nOur goal was to apply a practical\
    \ approach (by really deploying a Kubernetes cluster,\nnot just reading how to\
    \ do it), to do it in an automated manner (so that the experiment can\nbe easily\
    \ reproduced) and to ensure that the cluster is ready to be used in a production\n\
    environment. Thus, the paper aims to compare two methods of deploying a Kubernetes\n\
    cluster: kops and eksctl. Both of the methods concern the AWS cloud, which was\
    \ chosen\nmainly because of its wide popularity and the range of provided services.\
    \ Besides the\ntwo chosen methods of deployment, there are many more, including\
    \ the DIY method and\ndeploying on-premises.\nAny application may comprise several\
    \ components, for example: a backend server,\na frontend server, database. It\
    \ is common knowledge that deploying an application to a\nproduction environment,\
    \ should obey a set of guidelines. It should be easy to view all\nthe log messages\
    \ generated by each of the components of the application, the application\nshould\
    \ be reachable for its end users and also, it would be useful if in a case of\
    \ any\ncomponent failure—that component should be available despite the failure.\
    \ Kubernetes\nfacilitates satisfying such requirements. However our aim is to\
    \ ensure such requirements\nfor Kubernetes itself. A set of requirements were\
    \ selected. Then, several ideas were\nprovided on how to meet each of the chosen\
    \ requirements. This makes us believe that the\nway in which we deployed the Kubernetes\
    \ clusters was a formal, documented way.\nThe process of deploying multiple containers\
    \ of one application can be optimized\nthrough automation. This kind of automation\
    \ is referred to as orchestration [4]. Kubernetes\nwas built to make deployments\
    \ easy, to reduce the time and effort which needs to be poured\ninto them. To\
    \ achieve this goals, Kubernetes offers a wide variety of features [3,5–7]:\n\
    •\ncreating, destroying, replicating containers,\n•\nrolling updates of containers,\n\
    •\nbuilt-in health checks (liveness and readiness probes),\n•\nautoscaling,\n\
    •\nredundancy and failover—to make the applications deployed on top of Kubernetes\n\
    more resilient and reliable,\n•\nbeing provider-agnostic—Kubernetes can be deployed\
    \ on-premises and in the cloud\nand in both cases it will provide the same set\
    \ of features, thus, it may be said that\nKubernetes uniﬁes the underlying infrastructure,\n\
    •\nutilizing provider-speciﬁc (sometimes referred to as: vendor-speciﬁc) features,\
    \ e.g., AWS\nload balancer or Google Cloud load balancer,\n•\nself-healing,\n\
    •\nservice discovery,\n•\nload balancing,\n•\nstorage orchestration.\nOur research\
    \ concentrates on the analysis and evaluation of Kubernetes cluster as\nthe software\
    \ production environment, especially the production deployment that means\nsuch\
    \ a deployment which targets the production environment. In the framework of this\n\
    research we made the comparison of two methods of deploying a Kubernetes cluster:\
    \ kops\nand eksctl.\nFirstly, the paper attempts to stipulate the requirements\
    \ of a production environment.\nThen, the requirements are used as comparison\
    \ criteria to help assess the two methods of\ndeployment. It was expected that\
    \ one method could be easier to use than the other but also\na method could be\
    \ insufﬁcient to satisfy all the production environment requirements. Fur-\nthermore,\
    \ other criteria were used, such as: cost of both methods and amount of problems\n\
    encountered. The paper presents the determination and analysis of such requirements\
    \ and\ntheir evaluation in the case of Kubernetes cluster. They are necessary\
    \ to use as comparison\ncriteria to help assess the two methods of deployment\
    \ the cluster.\nThe ﬁrst novelty of this manuscript is that, while working on\
    \ it, Kubernetes clusters\nwere indeed deployed on a chosen cloud (AWS) and it\
    \ was done so in a formal, documented\nSensors 2021, 21, 1910\n3 of 24\nway. First,\
    \ the 9 requirements of a cluster were gathered, then each of the requirements\
    \ was\nplanned and then, the deployments were implemented, the clusters were tested\
    \ and then\ndestroyed. Thanks to deploying the clusters by the authors, it was\
    \ possible to be sure of\nexactly which and how many of AWS resources (such as\
    \ EC2, ALB, EBS) were created by a\ndeployment tool. Based on this information\
    \ the Section 5.6. was written—it was possible\nto compare how much a Kubernetes\
    \ cluster deployed with AWS EKS and kops would cost\nto run for one month.\nThe\
    \ second novelty is that the deployed Kubernetes clusters were not vanilla clus-\n\
    ters. The authors undertook the challenge of providing a speciﬁcation of a production\n\
    environment. The 9 production environment requirements were collected and an attempt\n\
    was made to satisfy each of them. Thanks to that such a Kubernetes cluster deployment\
    \ is\nmuch closer to a real-life scenario and represents a more practical approach.\n\
    The main contribution of this manuscript is the comparison of two methods of de-\n\
    ploying a Kubernetes cluster: kops and eksctl, described in Sections 4 and 5.\
    \ They contain\nthe helpful information for anyone committed to a Kubernetes cluster\
    \ deployment plan-\nning. Examples of such information are: time needed to create\
    \ a production-grade cluster,\ninformation on how easy it is to conﬁgure a cluster,\
    \ cost of running a cluster for one month.\nThe presented paper is composed as\
    \ follows: Section 2 presents the production de-\nployment requirements—their\
    \ idea, their classiﬁcation and description of each type chosen\nfor the further\
    \ research. Section 3 describes the Kubernetes cluster production deployment\n\
    requirement and methods. Section 4 deals with available Kubernetes cluster deployment\n\
    methods: AWS EKS Managed Service using eksctl and AWS using kops while Section\
    \ 5\npresents the comparison results of the used methods of Kubernetes cluster\
    \ deployment.\n2. Production Deployment Requirements—Background and Related Works\n\
    A Kubernetes cluster may be deﬁned as a collection of storage and networking re-\n\
    sources which are used by Kubernetes to run various workloads [7,8]. Another deﬁnition\n\
    states that a Kubernetes cluster is a single unit of computers which are connected\
    \ to work\ntogether and which are provisioned with Kubernetes components [9].\
    \ A cluster consists\nof two kinds of instances: masters and nodes. An instance\
    \ can be a virtual machine or a\nphysical computer [1,9–11].\nTypically, there\
    \ are two reasons for which multiple environments are in use: to support\na release\
    \ delivery process and to run multiple production instances of the system. The\
    \ ﬁrst\nreason allows having a particular build of an application (e.g., a git\
    \ commit or a speciﬁed\nversion of code) well tested. Such a build has to go through\
    \ many different environments,\ne.g., testing, staging and production. When a\
    \ build does not pass all the stages in the\nformer environments, it will not\
    \ be promoted to the production environment [12,13].\nThe second reason for multiple\
    \ environments is that they are used in order to ensure\nfault-tolerance (when\
    \ one environment fails, the other can take over), scalability (the work\ncan\
    \ be spread among many clusters), segregation (it may be decided to handle a group\
    \ of\ncustomers using one environment and the other group with the other environment,\
    \ e.g., for\nlatency purposes) [12]. Well-known examples of running multiple production\
    \ deployments\ncan be Blue-green deployments or Canary deployments [10].\nThe\
    \ authors of [14] discuss the set of capabilities required for container orchestration\n\
    platform for design principles. It also provides a guide to identifying and implementing\n\
    the key mechanisms required in a container orchestration platform.\nThroughout\
    \ this work, a production deployment means such a deployment which\ntargets the\
    \ production environment. A list of requirements for a production deployment,\n\
    gathered through the literature, is as follows:\n•\nCentral Monitoring—this is\
    \ helpful when troubleshooting a cluster [5,8,15–17].\n•\nCentral Logging—this\
    \ is a fundamental requirement for any cluster with number of\nnodes or pods or\
    \ containers greater than a couple [8,16,18].\n•\nAudit—to show who was responsible\
    \ for which action [17].\nSensors 2021, 21, 1910\n4 of 24\n•\nHigh Availability—authors\
    \ of [8] go even further and state that the cluster should be\ntested for being\
    \ reliable and highly available before it is deployed into production [5,8].\n\
    •\nLive cluster upgrades—it is not affordable for large Kubernetes clusters with\
    \ many\nusers to be ofﬁne for maintenance [8].\n•\nBackup, Disaster Recovery—cluster\
    \ state is represented by deployed containerized\napplications and workloads,\
    \ their associated network and disk resources—it is recom-\nmended to have a backup\
    \ plan for this data [5,8,17].\n•\nSecurity, secrets management, image scanning—security\
    \ at many levels is needed\n(node, image, pod and container, etc.) [5,8,16,17].\n\
    •\nPassing tests, a healthy cluster–‘if you don’t test it, assume it doesn’t work’\
    \ [5,8].\n•\nAutomation and Infrastructure as Code—in production environment a\
    \ versioned,\nauditable and repeatable way to manage the infrastructure is needed\
    \ [8,17].\n•\nAutoscaling—if application deployed on a Kubernetes demand more\
    \ resources, then\na new Kubernetes node should be automatically created and added\
    \ to the cluster [5].\n2.1. Monitoring as Production Environment Requirement\n\
    Monitoring helps to ensure that a cluster is operational, correctly conﬁgured\
    \ and that\nthere are enough resources deployed. Monitoring is also indispensable\
    \ for debugging\nand troubleshooting [8]. Moreover, monitoring system provides\
    \ the historical data that is\nneeded for planning purposes. The monitoring strategy\
    \ should cover four areas [13]:\n•\nconﬁguring the infrastructure in such a way\
    \ that it is possible to collect the data,\n•\nstoring the data,\n•\nproviding\
    \ dashboards, so that data is presented in a clear way,\n•\nsetting up notiﬁcations\
    \ or alarms to let users know about certain events.\nMonitoring provides various\
    \ metrics, e.g., CPU usage, memory use, I/O per disk,\ndisk space, number of network\
    \ connections, response time, etc. Thus, it is helpful at many\ndifferent levels:\
    \ at hardware, operating system, middleware and application level. There\nis a\
    \ wide range of available open source and commercial tools to take care of monitoring:\n\
    Nagios, OpenNMS, Flapjack, Zenoss, Tivoli from IBM, Operations Manager from HP,\n\
    Splunk, etc. [13]. Solutions recommended for a Kubernetes cluster are: Heapster\
    \ combined\nwith InﬂuxDB as backend and Grafana as frontend and also cAdvisor\
    \ [8,19].\nAnother solution for monitoring is Kubernetes dashboard, which is a\
    \ built-in solution\nand doesn’t require any customization. Heapster, InﬂuxDB\
    \ and Grafana are great for heavy-\nduty purposes, whereas Kubernetes dashboard\
    \ is probably able to satisfy the majority\nof monitoring needs of a Kubernetes\
    \ cluster [8,18]. Example dashboard provided by\nKubernetes dashboard is presented\
    \ in Figure 1.\nVersion March 3, 2021 submitted to Journal Not Speciﬁed\n5 of\
    \ 23\nFigure 1. Kubernetes dashboard depicting CPU and Memory usage by Kubernetes\
    \ pods [48].\nsolutions are: Fluentd, Elasticsearch, Kibana [7], Logstash [12]\
    \ and Graylog [40,50]. It is also important\n177\nto consider that log messages\
    \ in Kubernetes cluster are generated from many sources: from end-user\n178\n\
    applications, nodes, Kubernetes system containers and there are also audit logs\
    \ in the form of e.g.\n179\nAPI server events [51]. For the purposes of auditing,\
    \ when deploying on AWS, one can use AWS\n180\nCloudTrail [27].\n181\nFigure 1.\
    \ Kubernetes dashboard depicting CPU and Memory usage by Kubernetes pods [20].\n\
    Sensors 2021, 21, 1910\n5 of 24\nThe authors of [21] proposed KRaft, an incorporation\
    \ of Raft in Kubernetes, a system\nthat manages the containers. The paper presents\
    \ the evaluation of performance and\nresource consumption of KRaft together with\
    \ performance close to Raft executing on\nphysical machines. Moreover, KRaft demands\
    \ more network transmission then Raft which\nexecuted in physical machines needs\
    \ more processing power and memory.\n2.2. Logging as Production Environment Requirement\n\
    Kubernetes dashboard has also a feature which makes it able to show log messages\n\
    of a single container deployed on Kubernetes [8]. Centralized logging is essential\
    \ for a\nproduction cluster, because usually there are a lot of pods (and containers)\
    \ deployed, each\ngenerating many log messages. It is impossible to require a\
    \ Kubernetes administrator to\nlogin into each container for the purpose of getting\
    \ the logs.\nThe second reason for the importance of centralized logging is that\
    \ containers are\nephemeral—the log messages kept inside the containers would\
    \ be lost after a container is\nredeployed. Popular solutions are: Fluentd, Elasticsearch,\
    \ Kibana [8], Logstash [18], and\nGraylog [22,23]. It is also important to consider\
    \ that log messages in Kubernetes cluster\nare generated from many sources: from\
    \ end-user applications, nodes, Kubernetes system\ncontainers and there are also\
    \ audit logs in the form of e.g., API server events [24]. For the\npurposes of\
    \ auditing, when deploying on AWS, one can use AWS CloudTrail [25].\n2.3. High\
    \ Availability as Production Environment Requirement\nWhile administering a Kubernetes\
    \ cluster, there is a high probability that something\nwill go wrong. For example,\
    \ components can fail, the network can fail, the conﬁguration\ncan be incorrect,\
    \ people make mistakes, and software could have bugs. Failure classiﬁcation\n\
    was described in [26]. This has to be accepted and a system should be designed\
    \ in such a\nway that it is reliable and highly available (HA) despite many problems.\
    \ Ideas for how to\nensure high availability are as follows [8]:\n•\nRedundancy—means\
    \ having a spare copy of something. Kubernetes uses Replica\nSets or Replication\
    \ Controllers to provide redundancy for applications deployed on\nKubernetes.\
    \ Five redundancy models were summarized in [27]. Some of them require\nan active\
    \ replica (running) and other passive (or standby).\n•\nHot Swapping—can be explained\
    \ as replacing some failed component on the ﬂy,\nwith minimal or ideally zero\
    \ down-time. Actually, hot swapping is quite easy to\nimplement for stateless\
    \ applications. For stateful applications, one has to keep a\nreplica of a component\
    \ (see redundancy).\n•\nLeader election—it is a pattern used in distributed systems.\
    \ Whenever there are many\nservers fulﬁlling the same purpose to share the load.\
    \ One of the servers must be\nelected a leader and then certain operations must\
    \ go through it. When the leader\nserver experiences a failure, other server can\
    \ be selected as new leader. This is a\ncombination of redundancy and hot swapping.\n\
    •\nSmart load balancing—used to share and distribute the load.\n•\nIdempotenc—means\
    \ that one request (or some operation) is handled exactly once.\n•\nSelf-healing—means\
    \ that whenever a failure of one component happens, it is auto-\nmatically detected\
    \ and steps are taken (also automatically) to get rid of the failure.\n•\nDeploying\
    \ in a cloud—a goal is to be able to physically remove or replace a piece\nof\
    \ hardware, either because of some issues or because of preventative maintenance\n\
    or horizontal growth. Often this is too expensive or even impossible to achieve\
    \ [26].\nTraditional deployments on-premises forced administrators to do a capacity\
    \ planning\n(to predict the amount of computing resources). Thanks to the on-demand\
    \ and elastic\nnature of the clouds, the infrastructure can be closely aligned\
    \ to the actual demand.\nIt is also easy to scale applications deployed on a cloud,\
    \ because of the fundamental\nproperty of the cloud: elasticity [28].\nFurthermore,\
    \ it may be necessary to test high availability. This can be done by inducing\n\
    a predictable failure and verifying if the system behaves as expected [8,29].\
    \ Such a kind of\nSensors 2021, 21, 1910\n6 of 24\ntesting, where one or more\
    \ cluster nodes or pods is killed is called Chaos Monkey, after the\ntool developed\
    \ by Netﬂix. There are also ready to use tools basing on the idea of Chaos\nMonkey:\
    \ chaoskube, kube-monkey, PowerfulSeal [5].\n2.4. Automation as Production Environment\
    \ Requirements\nWhen it comes to automation, many guidelines can be found in [13].\
    \ The most\nimportant guidelines for automation in production environment are\
    \ as follows:\n•\nEvery Change Should Trigger the Feedback Process—means that\
    \ every change in code\nshould trigger some pipeline and should be tested (including\
    \ unit tests, functional\nacceptance tests, non-functional tests). The tests should\
    \ happen in an environment\nwhich is as similar as possible to production. Some\
    \ tests may run in production\nenvironment too [12,13].\n•\nFeedback Must Be Received\
    \ as Soon as Possible—this also involves another rule: fail\nfast. This guideline\
    \ suggests that faster tests (or less resource-intensive tests) should\nrun ﬁrst.\
    \ If theses tests fail, the code does not get promoted to the next pipeline stages,\n\
    which ensures optimal use of resources [13].\n•\nAutomate Almost Everything—generally,\
    \ the build process should be automated to\nsuch extent where speciﬁc human intervention\
    \ or decision is needed. However there\nis no need to automate everything at once\
    \ [12,13].\n•\nKeep Everything in Version Control—this means that not only application\
    \ source code\nbut also tests, documentation, database conﬁguration, deployment\
    \ scripts, etc. should\nbe kept in version control and that it should be possible\
    \ to identify the relevant version.\nFurthermore, any person with access to the\
    \ source code should be able to invoke\na single command in order to build and\
    \ deploy the application to any accessible\nenvironment. Apart from that, it should\
    \ be also clear which version in version control\nsystem was deployed into each\
    \ environment [13].\n•\nIf It Hurts, Do It More Frequently, and Bring the Pain\
    \ Forward—if some part of the\napplication lifecycle is painful, it should be\
    \ done more often, certainly not left to do at\nthe end of the project [13].\n\
    •\nIdempotency—the tools used for automation should be idempotent, which means\
    \ that\nno matter how many times the tool is invoked, the result should stay the\
    \ same [12].\nPresntly, the most essential tools needed to introduce the automated\
    \ application\nlifecycle are the following: ﬁrst, a framework for Conﬁguration\
    \ Management is needed,\ne.g., Puppet, CFEngine [12,13], Chef [30], Ansible [31],\
    \ SaltStack [32]. These tools help to\ndeclaratively deﬁne what packages should\
    \ be installed and how should they be conﬁgured\nin a virtual machine or a container\
    \ or a physical server [13]. They can help prevent\nconﬁguration drift in a large\
    \ number of computers. A conﬁguration drift is a difference\nacross systems that\
    \ were once identical. It can be imposed by a manual amendment and\nalso by automation\
    \ tools which propagated a change only to some of the instances [12].\nThere are\
    \ also stack-oriented tools, which follow the same declarative model: Terraform\
    \ [33]\nand CloudFormation [12]. Another type of needed tools is a building server,\
    \ where the\nexamples are: Jenkins, GoCD, Travis.\n2.5. Security as Production\
    \ Environment Requirement\nSecurity is another essential aspect of production\
    \ deployment and it touches many\nlevels. A node breach is a very serious problem\
    \ and it can happen by someone logging to\nthe instance or having physical access\
    \ to it. The latter is easily mitigated by not deploying\non bare-metal machines\
    \ but on a cloud instead [8]. The former demands some hardening\ndone. There are\
    \ several ideas that can be implemented for a Kubernetes cluster speciﬁcally\n\
    listed below:\n•\nEnsuring that data is encrypted in transit by using secure API\
    \ server protocol (HTTPS\ninstead of HTTP) [8].\n•\nEnsuring proper user and permissions\
    \ management by conﬁguring authentication,\nauthorization, security accounts and\
    \ admission control in API server [8]. When setting\nSensors 2021, 21, 1910\n\
    7 of 24\nup authorization, it is wise to apply the principle of least privilege.\
    \ This principle\nrecommends that only the needed resources or permissions should\
    \ be granted [5].\n•\nUsing Role-Based Access Control (RBAC) to manage access\
    \ to a cluster [5,34,35].\n•\nEnsuring security keys management and exchange by\
    \ implementing for example\nautomated key rotation.\n•\nEnsuring that used Docker\
    \ images are neither malicious (deliberately causing some\nharm) nor vulnerable\
    \ (allowing some attacker to take control) by keeping them up-to-\ndate and maintaining\
    \ them instead of using the publicly available ones or by using a\nprivate Docker\
    \ registry [8].\n•\nUsing minimal Docker images because the fewer programs there\
    \ are installed in an\nimage, the fewer potential vulnerabilities there are [5].\n\
    •\nMaintaining a log or audit system [8].\n•\nUsing network policies which act\
    \ in a white list fashion and can open certain protocols\nand ports [8].\n•\n\
    Using secrets. Kubernetes has a resource called: secret, but the problem is that\n\
    Kubernetes stores secrets as plaintext in etcd. This, in turn, means that steps\
    \ should\nbe taken in order to limit direct access to etcd [8].\n•\nPreferring\
    \ managed services, because they will have many security measures already\nimplemented\
    \ [5].\n•\nAvoid running processes as root user in Docker containers [5].\n•\n\
    Using available programs for security scanning [5].\n2.6. Disaster Recovery as\
    \ Production Environment Requirement\nDisaster recovery can be understood as the\
    \ process which an organization has to\nundergo after a service disruption happened\
    \ in order to resume normal services. It is\nvital to know what actions are necessary\
    \ to overcome the disaster. This set of predeﬁned\nprocedures is known as Disaster\
    \ Recovery Plan, DRP. Furthermore, disaster recovery is not\nthe same as fault\
    \ tolerance. The latter ensures that a system will withstand and resist the\n\
    failure [11].\nDisaster recovery is an essential requirement of any business where\
    \ continuity matters.\nIn order to plan disaster recovery well the following key\
    \ parameters should be considered:\nthe initial cost, the cost of data transfers\
    \ and the cost of data storage. Signiﬁcant costs\nmay be a reason why, in the\
    \ past, around 40–50% of small businesses had no DRP and\ndid not intend to change\
    \ this. However, cloud computing provides affordable solutions,\nbecause of the\
    \ employed model “pay-for-what is used”. Another advantage of the cloud\nis that\
    \ it is fairly easy to use resources deployed in multiple geographical areas.\
    \ This is\ndesired, because one the major concepts in a DRP is the geographical\
    \ separation of the\nsystem servers and its backup [36].\nKey metrics that can\
    \ be taken into consideration while planning disaster recovery\nare [11,36]:\n\
    •\nRecovery Point Objective (RPO),\n•\nRecovery Time Objective (RTO).\nCloud services\
    \ mitigate some risks that persistent data storage has. For example cloud\nservices\
    \ provide high-available data storage by replicating it across different geographical\n\
    locations. However, replication is not the same as backup and it does not protect\
    \ against\naccidentally deleting a volume or against a misconﬁgured application\
    \ overwriting the data.\nThus, backup is still needed. In order to backup Kubernetes,\
    \ the etcd database has to be\nbacked up. Apart from that, each application deployed\
    \ on top of Kubernetes should be\nbacked up on its own [5]. There are already\
    \ available services that help with Kubernetes\nbackup: Velero [5].\n2.7. Testing\
    \ as Production Environment Requirement\nBefore the production Kubernetes cluster\
    \ is ready for end-users, it must be veriﬁed\nthat it works and is healthy. Every\
    \ component and every node should be tested proving that\nSensors 2021, 21, 1910\n\
    8 of 24\nit is working as expected. Sometimes, applications expose a custom health\
    \ endpoint [18],\ne.g., kube-scheduler does that. Thanks to that, it is possible\
    \ to verify regularly that a service\nis performing correctly by sending a request\
    \ to the health endpoint. Usually, a HTTP\nresponse code of 200 indicates correct\
    \ status of the service [37]. Kubernetes can monitor\nthe health endpoints with\
    \ liveness probes. Based on a speciﬁed health endpoint response,\nKubernetes can\
    \ restart the faulty container [38].\nThis section described the production environment\
    \ requirements together with the\nexample ideas of how to satisfy most of them.\
    \ Therefore, it is now possible to choose the\nrequirements for Kubernetes cluster\
    \ deployment.\n3. Kubernetes Cluster Production Deployment Requirements and Methods\n\
    The practical planning and designing the production deployment consider the fol-\n\
    lowing activities: capacity planning, choosing which requirements to satisfy and\
    \ taking\ndifferent deployment and infrastructure related decisions.\nThere are\
    \ numerous requirements for a production deployment of a Kubernetes cluster.\n\
    In general the companies, which deploy Kubernetes and similar systems, obey some\
    \ set of\nbest practices, dedicated to these companies only. Thus, the requirements\
    \ presented in this\nwork do not exhaust the topic.\nIn the empirical part of\
    \ our study, the most important requirements were attempted\nto be satisﬁed:\n\
    •\nhealthy cluster,\n•\nautomated operations,\n•\ncentral logging,\n•\ncentral\
    \ monitoring,\n•\ncentral audit,\n•\nbackup,\n•\nhigh availability,\n•\nautoscaling,\n\
    •\nsecurity.\nThe set entails nine requirements. We determined the acceptance\
    \ criteria needed to\nsatisfy each of the requirements. Based on this, a plan\
    \ for Kubernetes cluster deployment\nwas created.\n3.1. Healthy and Usable Cluster\n\
    Unless all of the Kubernetes control plane components are healthy, it will need\
    \ to\nbe ﬁxed. It should not be a problem with a managed Kubernetes services,\
    \ but for self\nhosted clusters, this should be checked. To monitor the health\
    \ of a Kubernetes cluster,\nthe following could be checked: number of nodes, node\
    \ health status, number of pods per\nnode and overall, resource usage/allocation\
    \ per node, and overall [5].\nTo verify that an application can be deployed on\
    \ top of a Kubernetes cluster, a simple\nApache server will be deployed. A Helm\
    \ Chart will be used [39]. Then, Bats-core tests will\nbe run to check that the\
    \ Apache server works (that the Apache endpoint is reachable from\na remote machine).\n\
    In order for a cluster to be healthy and usable, the following acceptance criteria\
    \ must\nbe met:\n•\nthe cluster should be tested with Bats-core (i.e., particular\
    \ number of worker nodes\nshould be deployed, particular Kubernetes version should\
    \ be used)),\n•\nthe Kubernetes API Server endpoint should be reachable for the\
    \ end users under a\ndomain name (i.e., an end user can connect with the cluster\
    \ using kubectl),\n•\nit should be tested that an application can be deployed\
    \ on top of a Kubernetes cluster\n(i.e., an Apache server will be deployed and\
    \ tested).\nSensors 2021, 21, 1910\n9 of 24\n3.2. Automated Operations\nTo be\
    \ able to perform automated operations, a Kubernetes cluster conﬁguration and\n\
    commands needed to manage the cluster should be stored in a version control system.\
    \ This\nis the goal of Infrastructure as Code. Such a solution will also allow\
    \ cluster changes to be\napplied through reviewable commits and in turn it will\
    \ allow avoiding possible collisions\nfrom simultaneous changes being made.\n\
    To automate the operations, a Bash script will be used. It should be possible\
    \ to use this\nscript in order to: create, test and delete a cluster. It is essential\
    \ to obey the Infrastructure\nas Code policy, because it facilitates repeating\
    \ the operations and the whole experiment of\na cluster deployment. Although in\
    \ this work no CI server is going to be used, it should\nbe possible to invoke\
    \ the automated commands (by using the Bash script) in a potential\nCI pipeline.\n\
    To satisfy the Automated Operations requirement, the following measures will be\
    \ ap-\nplied:\n•\nall the code and conﬁguration needed to deploy a cluster will\
    \ be stored in a Git\nrepository,\n•\ncluster will be conﬁgure with YAML ﬁle,\n\
    •\nBash script will be used to create, test and delete a cluster,\n•\nHelm tool\
    \ will be used to automate a test application deployment,\n•\nit will be possible\
    \ to choose between two deployment environments: testing and\nproduction (templating\
    \ mechanism with Bash variables will be used).\n3.3. Central Logging, Monitoring\
    \ and Audit\nThe plan here is to simply use AWS CloudWatch service. It should\
    \ be easy to enable\nwhen using eksctl. It is not enabled by default due to data\
    \ ingestion and storage costs [40].\nTo satisfy the Central Monitoring requirement,\
    \ we plan to use AWS CloudWatch.\nThere also exist a solution—a program called:\
    \ Kubernetes Metrics Server. In addition, even\nthough it is a server which provides\
    \ source of container resource metrics, it should not be\nused as an accurate\
    \ source of resource usage metrics. Its main usage is for CPU/Memory-\nbased horizontal\
    \ autoscaling and for automatically adjusting/suggesting resources needed\nby\
    \ containers. Thus, it will not be used for Central Monitoring.\nThis requirement\
    \ will be simply met by using the AWS CloudTrail service. This\nservice should\
    \ show us which user is responsible for what changes made to the AWS\nresources\
    \ used. Besides, AWS CloudTrail can be also used to detect unusual activity in\
    \ the\nAWS accounts.\n3.4. Backup\nTo implement and test backup, Velero tool will\
    \ be used. First, Velero will be used to\ncreate a backup. Then a disaster will\
    \ be simulated: a whole Kubernetes namespace will be\ndeleted with all the resources\
    \ deployed inside. The last phase is the backup restore phase.\nIt is expected\
    \ that the namespace and all the resources in it will be restored.\n3.5. Capacity\
    \ Planning and High Availability\nThe needed capacity, meaning: number of machines\
    \ and IT resources (CPU, Memory)\nmust be discussed. The important thing is that\
    \ the to-be-deployed Kubernetes cluster,\nshould have the minimal amount of capacity,\
    \ but still it should be representative for a\nproduction environment. Here chicken-counting\
    \ (0, 1, many) comes to the rescue. Applying\nthe chicken-counting technique means\
    \ that if a production site has 250 web servers, 2 should\nbe enough to represent\
    \ it [13]. Since a Kubernetes cluster consists of a master node and\nworker nodes,\
    \ it was decided that a representative cluster would be made of two worker\nnodes.\
    \ One worker node will be created together with cluster creation and the second\n\
    worker will be added automatically by the Autoscaler. When it comes to number\
    \ of master\nnodes needed, one is enough for the minimal deployment. However in\
    \ order to achieve\nhigh availability at least two master nodes are needed. However,\
    \ having an odd number is\nSensors 2021, 21, 1910\n10 of 24\nrecommended for master\
    \ nodes to avoid issues with consensus/quorum. Thus, at least\nthree master nodes\
    \ are needed [41,42].\n3.6. Autoscaling\nThe idea here is to test whether a cluster,\
    \ which consists of master nodes and worker\nnodes, will be able to, by itself,\
    \ create and delete another worker node. This decision,\nwhether to scale in or\
    \ out, should be taken according to how many resources there are\navailable for\
    \ pods. The demand for more resources will be artiﬁcially created by increasing\n\
    the value of replicaCount in the Apache Helm Chart [39].\n3.7. Security\nThere\
    \ are many security measures that could be applied on a cluster. In order to stay\n\
    secure, access to the Kubernetes cluster should be limited. Any Kubernetes deployment\
    \ on\nAWS will have to deal with networking.\nVirtual Private Cloud (VPC) is an\
    \ AWS resource, providing the networking services.\nWhen a VPC is created, it\
    \ means that a virtual private network is created and assigned to a\nparticular\
    \ AWS account. AWS resources can be launched into that VPC. Each VPC has a\nCIDR\
    \ block assigned. The VPC cannot span more than one AWS Region. The VPC can be\n\
    divided into subnetworks (or subnets). One subnet cannot span more than one Availability\n\
    Zone. Subnets can be either public or private. If a subnet’s trafﬁc is routed\
    \ to an internet\ngateway (which is an AWS resource), the subnet is known as a\
    \ public subnet. Otherwise,\na subnet is private [43]. This means that public\
    \ subnets have access to the Internet.\nBy default, both kops and eksctl deploy\
    \ Kubernetes clusters using public subnets.\nThis means that anyone who knows\
    \ the URL of the Kubernetes master node, can access\nits API Server. Additionally,\
    \ if one has a private SSH key, then they can SSH login into\nthe master node.\
    \ A simple way to restrict access to the cluster is to use Security Groups.\n\
    Security Groups rules will be used to limit SSH access to the master and worker\
    \ nodes and\nalso to limit access to API Server endpoint [44]. In result, only\
    \ one IP address should be\nable to access the endpoints.\nThe deployments operated\
    \ by a bigger team of many Kubernetes cluster administra-\ntors requires that\
    \ one could use a NAT Gateway or NAT instance IP address as a single\nallowed\
    \ IP address. Furthermore, a bastion host could be used to securely SSH login\
    \ into\nthe master node [45].\nTo summarize: in order to make the cluster more\
    \ secure, access to the cluster should\nbe restricted. Only one IP address will\
    \ be allowed to connect.\n4. Available Kubernetes Cluster Deployment Methods\n\
    Kubernetes is not trivial to deploy. In order to deploy a usable cluster, there\
    \ are at\nleast two machines needed: one master and one node. On each of the machines,\
    \ several\ncomponents must be installed.\nFortunately, Kubernetes is a popular\
    \ tool and many methods of deploying it are\nalready described in the literature.\
    \ The available methods may be divided into three cate-\ngories:\n•\nself-hosted\
    \ solutions, on-premises,\n•\ndeployment in a cloud, but not using Managed Services,\n\
    •\ndeployment in a cloud, using Managed Services.\nAccording to the authors of\
    \ [5], the best solution is to use Managed Services. The au-\nthors argue that,\
    \ thanks to this method, one can get a fully working, secure, highly available,\n\
    production-grade cluster in a few minutes and for a little price. Managed Services\
    \ are cer-\ntainly a good way to try Kubernetes out. Then, if one wants to do\
    \ something non-standard\nor to experiment with the cluster, then one could choose\
    \ a custom or a hybrid solution.\nThe self-hosted, deployed on-premises way is\
    \ recommended if the following qualities\nare of a great importance: price, low-network\
    \ latency, regulatory requirements and total\ncontrol over hardware [8].\nSensors\
    \ 2021, 21, 1910\n11 of 24\nManaged Services offer many features, such as built-in\
    \ autoscaling, security, high\navailability, having serverless options. However,\
    \ they may be more expensive (for example\nwhen using AWS EKS, one has to pay\
    \ $0.10 per hour for each Amazon EKS cluster [46])\nand less customizable. Custom\
    \ solutions allow the Kubernetes administrator to broaden\ntheir knowledge and\
    \ grasp the deep understanding on what is going on under the Ku-\nbernetes hood\
    \ (i.e., one can customize how the Kubernetes control plane is deployed or\nset\
    \ up a custom network topology). There is no one right answer which ﬁts all the\
    \ use\ncases. It is always advised to do one’s own research and to try and experiment\
    \ with the\nexisting methods.\nFurthermore, different categorization may be applied.\
    \ For instance, the deployment\nmethods may be categorized by the tools:\n•\n\
    using web interface of a particular cloud, e.g., AWS Management Console (supported\n\
    by AWS),\n•\nusing command-line tools ofﬁcially supported by a particular cloud,\
    \ e.g., awscli or\neksctl (supported by AWS),\n•\nusing command-line tools designed\
    \ exactly to deploy a Kubernetes cluster, but not\nlimited to one particular cloud,\
    \ e.g., kops,\n•\nusing command-line tools, designed for managing computer infrastructure\
    \ resources,\ne.g., Terraform, SaltStack.\nHowever, our focus is on the two particular\
    \ methods, which were evaluated for the\nproduction environment of software deployment:\n\
    •\ndeploying on AWS, using AWS Managed Service (AWS EKS), using eksctl which is\
    \ a\nAWS supported ofﬁcial tool,\n•\ndeploying on AWS, not using any Managed Service,\
    \ using kops which is a command-\nline tool, not ofﬁcially supported by any cloud,\
    \ but designed exactly to deploy a\nKubernetes cluster.\n4.1. Deployment Method:\
    \ On AWS EKS Managed Service Using Eksctl\nThe goal of eksctl method is to have\
    \ the Kubernetes control plane managed by AWS.\nThis means that it is AWS which\
    \ is responsible for managing Kubernetes master com-\nponents, such as: API Server,\
    \ kube-scheduler, kube-controller-manager, and also Etcd.\nThe control plane should\
    \ be already highly available, thanks to being deployed across\nmultiple Availability\
    \ Zones (AZ) [43,45,47].\nThe control plane being already highly available means\
    \ that there should be at least\ntwo API server nodes and three etcd nodes that\
    \ run across three Availability Zones within\na Region. In addition, AWS EKS is\
    \ responsible for automatically detecting and replacing\nunhealthy control plane\
    \ instances. There exists a Service Level Agreement (SLA) especially\nconcerning\
    \ AWS EKS. It is a policy which governs the use of the Amazon Elastic Container\n\
    Service for Kubernetes. The Kubernetes API is exposed via the Amazon EKS endpoint\n\
    which is associated with the cluster. The cluster control plane is fronted by\
    \ an Elastic Load\nBalancing Network Load Balancer and also all the networking\
    \ is taken care of to provide\nconnectivity from the control plane instances to\
    \ the worker nodes. Thanks to that, the AWS\nEKS user can access the Kubernetes\
    \ API Server [48].\nTo use AWS EKS, there are two ways supported: using eksctl\
    \ CLI or using AWS\nManagement Console and AWS CLI. Both of them demand installing\
    \ AWS CLI. In this\nwork, the method with eksctl CLI is applied. Eksctl is ofﬁcially\
    \ the CLI for AWS EKS,\nendorsed by AWS, though it was launched by WeaveWorks.\
    \ The cluster can be conﬁgured\nwith a YAML ﬁle and also by setting eksctl CLI\
    \ ﬂags.\nAfter the cluster is created and after a connectivity with a Kubernetes\
    \ cluster endpoint\nis established, it is now possible to deploy applications\
    \ on top of the cluster. The Figure 2\ndepicts the stages of working with AWS\
    \ EKS.\nSensors 2021, 21, 1910\n12 of 24\nVersion March 3, 2021 submitted to Journal\
    \ Not Speciﬁed\n12 of 23\nFigure 2. Schema presenting the stages of working with\
    \ AWS EKS.\none of the recommended ways to setup a Kubernetes cluster and it is\
    \ a tool which can be used to a\n492\ncreate a production environment [11].\n\
    493\nSimilarly to eksctl, kops can also create a Highly Available Kubernetes cluster.\
    \ However it\n494\ndemands more work than eksctl. The commands which allow to\
    \ create a cluster on AWS are:\n495\n$ kops create cluster $ {CLUSTER_NAME} -state\
    \ \\\n496\n\" s3://$ {K8S_EXP_KOPS_S3_BUCKET}\" -cloud=aws \\\n497\n-zones=us-east-1c\n\
    498\n$ kops update cluster $ {CLUSTER_NAME} -state \\\n499\n\" s3://$ {K8S_EXP_KOPS_S3_BUCKET}\"\
    \ -yes\n500\nBefore these commands can be run, one has to provide some minimal\
    \ DNS conﬁguration via Route53 (an\n501\nAWS resource responsible for networking),\
    \ set up a S3 bucket (another AWS resource, responsible for storage) to\n502\n\
    store the cluster conﬁguration [7] and also conﬁgure the AWS IAM user (an AWS\
    \ resource responsible for access\n503\nmanagement) and create a SSH key pair\
    \ [33–35]. Amazon S3 is an AWS service which provides object storage\n504\nAWS\
    \ IAM is an abbreviation from AWS Identity and Access Management and it is responsible\
    \ for managing\n505\naccess to AWS services and resources.\n506\nThe stages of\
    \ working with a Kubernetes cluster deployed on AWS with kops are similar to the\
    \ stages of\n507\nworking with eksctl, but there is the additional ﬁrst stage\
    \ (Fig. 3).\n508\nFigure 3. Stages of working with Kubernetes cluster deployed\
    \ on AWS with kops.\nKops has been around a long time as an AWS-speciﬁc tool,\
    \ but now it also supports other clouds [4,38]. Its\n509\nmain features such as:\
    \ automated Kubernetes cluster deployment, highly available master or adding a\
    \ variety of\n510\ncustom Kubernetes addons [39,40] indicate that kops is an attractive\
    \ tool, worthy to at least try out.\n511\n4.3. Troubleshooting any Kubernetes\
    \ cluster\n512\nThis subsection presents the ideas how to troubleshoot any Kubernetes\
    \ cluster. The ideas were gathered\n513\nduring the empirical work of our research.\n\
    514\nThere are some general guidelines which help to debug the cluster. When a\
    \ cluster is self-hosted and one\n515\nis fully responsible for master and worker\
    \ nodes deployment, then one should verify whether any node is still\n516\nfunctional.\
    \ It means that problems like: corrupt ﬁle system, kernel deadlock, problems with\
    \ the Docker Daemon\n517\nor problems with hardware may occur and should be mitigated.\
    \ A solution to detect such problems may be: a\n518\nnode problem detector pod\
    \ [7]. One should also run the following commands to check if all the worker nodes\
    \ are\n519\nrunning and if the cluster is generally reachable:\n520\n$ kubectl\
    \ get nodes\n521\n$ kubectl cluster -info\n522\nAnother level of troubleshooting\
    \ is debugging the applications deployed on top of the Kubernetes cluster\n523\n\
    Some ideas are described on the ofﬁcial Kubernetes documentation website [44].\
    \ For example, one can get details\n524\nFigure 2. Schema presenting the stages\
    \ of working with AWS EKS.\n4.2. Deployment Method: On AWS Using Kops\nKops stands\
    \ for Kubernetes Operations. It is a command-line tool which allows\ncreating,\
    \ destroy, upgrade, and maintain production-grade, highly available, Kubernetes\n\
    clusters. It supports multiple clouds: AWS (ofﬁcially), GCE and OpenStack (in\
    \ beta\nsupport) and VMware vSphere (in alpha) [49]. Furthermore, it can be used\
    \ from multiple\noperating systems: Linux, Mac and Windows [50]. Kops is one of\
    \ the recommended ways\nto setup a Kubernetes cluster and it is a tool which can\
    \ be used to a create a production\nenvironment [9].\nAs with eksctl, kops can\
    \ also create a Highly Available Kubernetes cluster. However,\nit demands more\
    \ work than eksctl. The commands which allow creating a cluster on AWS\nare:\n\
    $ kops create cluster $ {CLUSTER_NAME} -state \\\n‘‘ s3://$ {K8S_EXP_KOPS_S3_BUCKET}’’\
    \ -cloud=aws \\\n-zones=us-east-1c\n$ kops update cluster $ {CLUSTER_NAME} -state\
    \ \\\n‘‘ s3://$ {K8S_EXP_KOPS_S3_BUCKET}’’ -yes\nBefore these commands can be\
    \ run, one has to provide some minimal DNS conﬁg-\nuration via Route53 (an AWS\
    \ resource responsible for networking), set up a S3 bucket\n(another AWS resource,\
    \ responsible for storage) to store the cluster conﬁguration [8] and\nalso conﬁgure\
    \ the AWS IAM user (an AWS resource responsible for access management)\nand create\
    \ a SSH key pair [41,44,51]. Amazon S3 is an AWS service which provides object\n\
    storage. AWS IAM is an abbreviation from AWS Identity and Access Management and\
    \ it is\nresponsible for managing access to AWS services and resources.\nThe stages\
    \ of working with a Kubernetes cluster deployed on AWS with kops are\nsimilar\
    \ to the stages of working with eksctl, but there is the additional ﬁrst stage\
    \ (Figure 3).\nVersion March 3, 2021 submitted to Journal Not Speciﬁed\n12 of\
    \ 23\nFigure 2. Schema presenting the stages of working with AWS EKS.\none of\
    \ the recommended ways to setup a Kubernetes cluster and it is a tool which can\
    \ be used to a\n492\ncreate a production environment [11].\n493\nSimilarly to\
    \ eksctl, kops can also create a Highly Available Kubernetes cluster. However\
    \ it\n494\ndemands more work than eksctl. The commands which allow to create a\
    \ cluster on AWS are:\n495\n$ kops create cluster $ {CLUSTER_NAME} -state \\\n\
    496\n\" s3://$ {K8S_EXP_KOPS_S3_BUCKET}\" -cloud=aws \\\n497\n-zones=us-east-1c\n\
    498\n$ kops update cluster $ {CLUSTER_NAME} -state \\\n499\n\" s3://$ {K8S_EXP_KOPS_S3_BUCKET}\"\
    \ -yes\n500\nBefore these commands can be run, one has to provide some minimal\
    \ DNS conﬁguration via Route53 (an\n501\nAWS resource responsible for networking),\
    \ set up a S3 bucket (another AWS resource, responsible for storage) to\n502\n\
    store the cluster conﬁguration [7] and also conﬁgure the AWS IAM user (an AWS\
    \ resource responsible for access\n503\nmanagement) and create a SSH key pair\
    \ [33–35]. Amazon S3 is an AWS service which provides object storage.\n504\nAWS\
    \ IAM is an abbreviation from AWS Identity and Access Management and it is responsible\
    \ for managing\n505\naccess to AWS services and resources.\n506\nThe stages of\
    \ working with a Kubernetes cluster deployed on AWS with kops are similar to the\
    \ stages of\n507\nworking with eksctl, but there is the additional ﬁrst stage\
    \ (Fig. 3).\n508\nFigure 3. Stages of working with Kubernetes cluster deployed\
    \ on AWS with kops.\nKops has been around a long time as an AWS-speciﬁc tool,\
    \ but now it also supports other clouds [4,38]. Its\n509\nmain features such as:\
    \ automated Kubernetes cluster deployment, highly available master or adding a\
    \ variety of\n510\ncustom Kubernetes addons [39,40] indicate that kops is an attractive\
    \ tool, worthy to at least try out.\n511\n4.3. Troubleshooting any Kubernetes\
    \ cluster\n512\nThis subsection presents the ideas how to troubleshoot any Kubernetes\
    \ cluster. The ideas were gathered\n513\nduring the empirical work of our research.\n\
    514\nThere are some general guidelines which help to debug the cluster. When a\
    \ cluster is self-hosted and one\n515\nis fully responsible for master and worker\
    \ nodes deployment, then one should verify whether any node is still\n516\nfunctional.\
    \ It means that problems like: corrupt ﬁle system, kernel deadlock, problems with\
    \ the Docker Daemon\n517\nor problems with hardware may occur and should be mitigated.\
    \ A solution to detect such problems may be: a\n518\nnode problem detector pod\
    \ [7]. One should also run the following commands to check if all the worker nodes\
    \ are\n519\nrunning and if the cluster is generally reachable:\n520\n$ kubectl\
    \ get nodes\n521\nFigure 3. Stages of working with Kubernetes cluster deployed\
    \ on AWS with kops.\nKops has been around a long time as an AWS-speciﬁc tool,\
    \ but now it also supports\nother clouds [5,49]. Its main features such as: automated\
    \ Kubernetes cluster deployment,\nhighly available master or adding a variety\
    \ of custom Kubernetes addons [22,52] indicate\nthat kops is an attractive tool,\
    \ worthy to at least try out.\n4.3. Troubleshooting Any Kubernetes Cluster\nThis\
    \ subsection presents the ideas how to troubleshoot any Kubernetes cluster. The\
    \ ideas\nwere gathered during the empirical work of our research.\nThere are some\
    \ general guidelines which help to debug the cluster. When a cluster is\nself-hosted\
    \ and one is fully responsible for master and worker nodes deployment, then\n\
    one should verify whether any node is still functional. It means that problems,\
    \ such as a\ncorrupt ﬁle system, kernel deadlock, problems with the Docker Daemon,\
    \ or problems with\nSensors 2021, 21, 1910\n13 of 24\nhardware may occur and should\
    \ be mitigated. A solution to detect such problems may be:\na node problem detector\
    \ pod [8]. One should also run the following commands to check if\nall the worker\
    \ nodes are running and if the cluster is generally reachable:\n$ kubectl get\
    \ nodes\n$ kubectl cluster -info\nAnother level of troubleshooting is debugging\
    \ the applications deployed on top of the\nKubernetes cluster. Some ideas are\
    \ described on the ofﬁcial Kubernetes documentation\nwebsite [53,54]. For example,\
    \ one can get details of a pod with the following commands:\n$ kubectl describe\
    \ pods $ {POD_NAME}\n$ kubectl logs $ {POD_NAME}\nEven the pod status may provide\
    \ us with enough information, what steps should be\ntaken next. If a pod is in\
    \ Pending status, then it could mean that there are not enough\nresources in the\
    \ cluster (e.g., the worker nodes use too small EC2 instance types or there are\n\
    too few worker nodes). A pod may be also in status CrashLoopBackOff which indicates\
    \ that\na container is repeatedly crashing after restarting. There may a problem\
    \ also that a Docker\nimage for a container cannot be downloaded and a pod may\
    \ be in state ImagePullBackOff\nor ErrImagePull [55].\nIt is also always a good\
    \ idea to read log messages (so access to log messages should be\nalways supplied).\
    \ The following Kubernetes components log messages may be helpful [56]:\n•\nlogs\
    \ from kube-apiserver, which is responsible for serving the API,\n•\nlogs from\
    \ kube-scheduler, which is responsible for making scheduling decisions,\n•\nlogs\
    \ from kube-controller-manager, which manages replication controllers,\n•\nlogs\
    \ from kubelet, which is responsible for running containers on the node,\n•\n\
    logs from kube-proxy, which is responsible for service load balancing.\nBesides\
    \ all the troubleshooting ideas presented earlier, one can also visit solutions\n\
    dedicated entirely to AWS EKS clusters [57] or to eksctl [58]. Still, some problems\
    \ may be\ntough to handle. However, even then, there are always measures that\
    \ could be applied\nto move further into the correct direction. One may ask a\
    \ question on Stack Overﬂow or\ntalk to a Kubernetes team on Slack or write on\
    \ a Kubernetes forum or ﬁle a bug on the\nKubernetes Github.com project website\
    \ [59].\nTo summarize: there are various ways how to debug a Kubernetes cluster\
    \ or an\napplication deployed on top of it. Some steps may be applied to any Kubernetes\
    \ cluster,\nother concern only the clusters deployed using a particular method.\n\
    5. Comparison of the Used Methods of Kubernetes Cluster Deployment\nSeveral comparison\
    \ criteria are used in order to compare the two chosen methods of\na Kubernetes\
    \ cluster deployment. Even though the two methods: using kops and using\neksctl,\
    \ help to deploy a Kubernetes cluster, they have some differences. For example:\
    \ kops\nworks for many clouds (e.g., AWS, GCP), whereas eksctl supports only AWS.\
    \ Another\ndifference is the default AMI(Amazon Machine Image). kops chooses Debian\
    \ Operating\nSystem, while eksctl uses Amazon Linux 2. These AMIs are, however,\
    \ only defaults, one\ncan always choose another AMI.\nThere were nine production\
    \ environment requirements which were supposed to be\nmet by both tested methods.\
    \ All of them were met. This means that both of the methods can\nbe applied in\
    \ production use cases. Figure 4 provides a brief explanation how each of the\n\
    requirements was satisﬁed for each method. Many requirements were handled in the\
    \ same\nway for both methods. Subjectively, the hardest requirement to meet was:\
    \ security, using\neksctl. There was a problem with ﬁnding a working YAML conﬁguration\
    \ and also ensuring\nthat cluster was healthy. Eksctl provides a more automated\
    \ approach. For example the\nAWS CloudWatch LogGroup, needed for central logging,\
    \ was automatically created when\nusing eksctl, but not when using kops. Also,\
    \ eksctl provides the cluster which is already\nHighly Available. On the other\
    \ hand, kops provides more ﬂexibility. The master node can\nSensors 2021, 21,\
    \ 1910\n14 of 24\nbe accessed with SSH when using kops and not when using eksctl.\
    \ When using kops, it\nis possible to see all the kube-system namespaced pods,\
    \ which is not the case with eksctl.\nWhen using eksctl no parameters of the control\
    \ plane components can be set, with kops it\nis possible.\nVersion March 3, 2021\
    \ submitted to Journal Not Speciﬁed\n14 of 23\nFigure 4. Comparison of how each\
    \ production requirement was satisﬁed using kops and eksctl.\nTable 1. Time needed\
    \ to run Kubernetes management operations using kops and eksctl\nOperation\nUsing\
    \ kops method\nUsing eksctl method\nCreate a minimal cluster\n6 min 12 s\n19 min\
    \ 26 s\nCreate a production-grade\n6 min 35 s\n25 min 40 s\ncluster\nTest a cluster\n\
    6 min 33 s\n6 min 40 s\nDelete a cluster\n2 min 23 s\n13 min 25 s\nThe ﬁles’ names\
    \ used to perform each cluster operation are demonstrated in table 2. The ﬁles\
    \ can be found\n578\nin the public Git repository https://github.com/xmik/MastersThesis.\n\
    579\nTable 2. Files used to deploy minimal and full clusters using: kops and eksctl\n\
    Which cluster\nUsing kops method\nUsing eksctl method\nA minimal cluster\nsrc/kops/cluster-minimal.yaml\n\
    src/eks/cluster-minimal.yaml\nA production cluster\nsrc/kops/cluster.yaml and\
    \ central logging deployed\nsrc/eks/cluster-minimal.yaml\nIn production deployment\
    \ using eksctl, there were one command used to generate cluster conﬁguration\n\
    580\n(eksctl create cluster) and to deploy the cluster. This command did not return\
    \ (meaning: it waited) for the cluster to\n581\nbe ready. First, the control plane\
    \ was deployed, then the worker nodes. For each of these two tasks (deploying\n\
    582\ncontrol plane and deploying worker nodes) a separate CloudFormation stack\
    \ was used. It was done automatically\n583\nby AWS EKS. The control plane is entirely\
    \ managed by AWS and it is run across three AWS availability zones in\n584\norder\
    \ to ensure high availability. The end user has even no access to the control\
    \ plane, meaning: there is no EC2\n585\ninstance with master node visible and\
    \ when listing all Kubernetes nodes, only the worker nodes are visible.\n586\n\
    A few sources claimed that the creation of Kubernetes cluster on AWS EKS should\
    \ take 10-15 minutes [57,70]\n587\nbut in this particular case it took about 20\
    \ minutes (Table 1). Also, to the best of this work’s author knowledge,\n588\n\
    Figure 4. Comparison of how each production requirement was satisﬁed using kops\
    \ and eksctl.\n5.1. Time of Chosen Kubernetes Cluster Operations\nOne of the criterion\
    \ used to compare the two deployment methods was time. The time\nof several operations\
    \ concerning Kubernetes cluster management was measured. Each\nof the operation\
    \ was performed several times and the mean value was used. Time was\nmeasured\
    \ separately for a minimal working cluster and for the full cluster. The minimal\n\
    cluster here means such a cluster that is usable for end user and utilizes the\
    \ default\nconﬁguration (central monitoring, central audit, healthy cluster, automated\
    \ operations\nare met). The production-grade cluster includes also the following\
    \ requirements: HA,\ncentral logging and security. About 5 min of difference between\
    \ creating a minimal and\nfull cluster using eksctl was caused by setting the\
    \ security measures. The full cluster\nhere does not include backup and autoscaling,\
    \ because meeting these requirements is\nnegligible in the terms of time and takes\
    \ the same time for kops and eksctl. Testing a cluster\ninvolves running the automated\
    \ tests—verifying that the cluster is healthy. It does not\ninvolve testing each\
    \ of the production deployment requirements. Both the minimal and full\nclusters\
    \ use t2.micro for kops and t2.small for eksctl for worker nodes. The time observed\n\
    for the cluster operations is given in Table 1.\nTable 1. Time needed to run Kubernetes\
    \ management operations using kops and eksctl.\nOperation\nUsing Kops Method\n\
    Using Eksctl Method\nCreate a minimal cluster\n6 min 12 s\n19 min 26 s\nCreate\
    \ a production-grade cluster\n6 min 35 s\n25 min 40 s\nTest a cluster\n6 min 33\
    \ s\n6 min 40 s\nDelete a cluster\n2 min 23 s\n13 min 25 s\nSensors 2021, 21,\
    \ 1910\n15 of 24\nThe ﬁles’ names used to perform each cluster operation are demonstrated\
    \ in Table 2.\nThe ﬁles can be found in the public Git repository https://github.com/xmik/MastersThesis\n\
    (accessed on 31 January 2021).\nTable 2. Files used to deploy minimal and full\
    \ clusters using: kops and eksctl.\nWhich Cluster\nUsing Kops Method\nUsing Eksctl\
    \ Method\nA minimal cluster\nsrc/kops/cluster-minimal.yaml\nsrc/eks/cluster-minimal.yaml\n\
    A production cluster\nsrc/kops/cluster.yaml and central logging deployed\nsrc/eks/cluster-minimal.yaml\n\
    In production deployment using eksctl, there were one command used to generate\n\
    cluster conﬁguration (eksctl create cluster) and to deploy the cluster. This command\
    \ did\nnot return (meaning: it waited) for the cluster to be ready. First, the\
    \ control plane was\ndeployed, then the worker nodes. For each of these two tasks\
    \ (deploying control plane\nand deploying worker nodes) a separate CloudFormation\
    \ stack was used. It was done\nautomatically by AWS EKS. The control plane is\
    \ entirely managed by AWS and it is run\nacross three AWS availability zones in\
    \ order to ensure high availability. The end user has\neven no access to the control\
    \ plane, meaning: there is no EC2 instance with master node\nvisible and when\
    \ listing all Kubernetes nodes, only the worker nodes are visible.\nA few sources\
    \ claimed that the creation of Kubernetes cluster on AWS EKS should\ntake 10–15\
    \ min [60,61] but in this particular case it took about 20 min (Table 1). Also,\
    \ to the\nbest of this work’s author knowledge, there was no command which allows\
    \ reconﬁguring\nthe cluster using all settings from the YAML conﬁguration ﬁle.\
    \ This means that when there\nwas a need to change e.g., tags of the AWS resources,\
    \ the cluster must have been deleted\nand created from scratch.\nTo make the cluster\
    \ more secure, it was decided to restrict the SSH access and kubectl\naccess.\
    \ Searching through eksctl conﬁguration, there was no setting which allowed to\n\
    whitelist the IP addresses which are allowed to ssh login to worker nodes. However,\n\
    blocking all the SSH access was available and it was chosen. This means now that\
    \ a\ncluster administrator has only kubectl access to cluster and no ssh access\
    \ at all (because\nssh access to worker nodes is blocked and access to master\
    \ nodes is not given by design\nof AWS EKS). It is acknowledged that sometimes,\
    \ not being able to ssh to the node may\nmake troubleshooting more difﬁcult, but\
    \ generally system and EKS logs contain enough\ninformation for diagnosing problems.\
    \ The second security requirement was to restrict the\naccess to cluster through\
    \ kubectl. By default, eksctl exposes the Kubernetes API server\npublicly but\
    \ not directly from within the VPC subnets. This means that the public endpoint\n\
    is by default set to true and the private endpoint to false [62].\nThere were\
    \ problems ﬁnding a working conﬁguration—the main setting was publicAc-\ncessCIDRs.\
    \ Thus, only one IP address should be allowed to communicate with the cluster\n\
    through kubectl. It was tested that the command kubectl worked from such an IP.\
    \ When\nthe same command was run from a different machine, with a different IP,\
    \ this command\nreached a timeout. Whenever there is a timeout when reaching an\
    \ AWS endpoint, it may\nbe caused by a Security Group not allowing such access.\
    \ Thus, this proved that the kubectl\naccess was restricted and that security\
    \ measures were working. Creating the EKS cluster\nwith all the security settings\
    \ took longer—about 25 min (Table 1).\nThe testing procedure in eksctl was the\
    \ same as with kops. Different command was\ninvoked (because all the commands\
    \ to run with eksctl were put into one ﬁle), but all\nthe Bats-core tests were\
    \ the same. Even the same Bats-core ﬁles were used. There was\nobviously no kops\
    \ validate cluster command run, because that command was dedicated to\nkops clusters\
    \ only. Running all the tests took slightly less than 7 min. It took long to have\
    \ a\nworking Load Balancer.\nDeleting a cluster command usually took about 13\
    \ min. It deleted almost all AWS\nresources, but not CloudWatch LogGroup. Not\
    \ deleting the LogGroup may be a good thing,\nbecause one may be interested to\
    \ study the logs after the cluster is long gone.\nSensors 2021, 21, 1910\n16 of\
    \ 24\nTo summarize, it is clearly visible that operating a Kubernetes cluster\
    \ is faster with\nkops. The time of operations is an important criterion, because\
    \ ﬁrstly, the faster it is,\nthe faster one can get the feedback and experiment\
    \ with the cluster. In addition, secondly,\nlonger creation and deletion times\
    \ mean longer time of keeping AWS resources created,\nwhich in turn means that\
    \ they cost more.\n5.2. Additional Steps Needed to Create a Kubernetes Cluster\n\
    To deploy the clusters, many tools were needed. However, these tools were packaged\n\
    inside a Docker image. Therefore, the development environment, provided by the\
    \ Docker\nimage was reliable and easily reproducible. It could be destroyed and\
    \ recreated any time.\nHowever, apart from the development tools, there were also\
    \ some prerequisites needed to\nbe deployed for the production-grade clusters.\n\
    When using kops, one has to use and create an S3 bucket. Kops needs this bucket\n\
    to store cluster conﬁguration. Contrary to that, AWS EKS does not need any additional\n\
    AWS Resources created beforehands. However, this work attempted to deploy Kubernetes\n\
    clusters in a production environment. In order to satisfy the backup requirement,\
    \ Velero\ntool was used. Velero needs some location to store backups. Therefore\
    \ an S3 bucket was\nneeded for both deployments.\nTo summarize, both methods require\
    \ the same steps to work in a production environ-\nment speciﬁed in this work,\
    \ but the S3 bucket was used by choice and it could be replaced\nwith some other\
    \ storage solution.\n5.3. Minimal EC2 Instance Type Needed for a Kubernetes Worker\
    \ Node\nVarious EC2 instance types cost various amounts of money. Thus, it is\
    \ preferable to\nuse the cheapest possible instance type. For this reason, the\
    \ minimal EC2 instance type was\nneeded for a Kubernetes worker node, in a production\
    \ ready cluster. To meet production\nenvironment requirements, this number of\
    \ pods is needed to be deployed on a worker\nnode (additionally to the pods deployed\
    \ by default by eksctl and kops):\n•\nthree pods for a cluster created with eksctl\
    \ (one pod for testing, one pod for backup\nand one pod for autoscaler),\n•\n\
    ﬁve pods for a cluster created with kops (one pod for testing, one pod for backup,\
    \ one\npod for autoscaler, two pods for logging).\nAs far as eksctl method is\
    \ concerned, there are by default four pods deployed on\na worker node. Thus,\
    \ altogether, there have to be 4 + 3 = 7 pods deployed on a worker\nnode. The\
    \ EC2 instance types t2.nano and t2.micro allow for maximally four pods on\none\
    \ EC2 instance [63]. The next, bigger instance type is t2.small and it allows\
    \ for 11 pods.\nThe conclusion is that when it comes to the number of allowed\
    \ pods on a worker node,\nthe minimal EC2 instance type is t2.small.\nAnother\
    \ thing to consider is whether t2.small is enough when it comes to EC2 CPU and\n\
    memory resources. This was checked empirically, by deploying a cluster with eksctl\
    \ and\nthen deploying the three pods needed to satisfy the production environment\
    \ requirements:\neks$ export K8S_EXP_ENVIRONMENT= testing\neks$ ./tasks_create\n\
    eks$ ./tasks_enable_velero\neks$ ./tasks_enable_as\neks$ ./tasks_test\nAll the\
    \ seven pods were in the status—Running—and the tests were successful. Thus,\n\
    it may be concluded that when in comes to EC2 instance CPU and memory resources,\n\
    the EC2 instance type t2.small is enough.\nThe third aspect is that it is possible\
    \ to deploy some pods on a master node or mark\nthe master node as schedulable\
    \ only by Kubernetes system components [38,64]. However,\nthis is not the case\
    \ for AWS EKS. In AWS EKS, the control plane is locked down and\nscheduling any\
    \ workloads on the control plane nodes is forbidden [65]. Contrary to the\nSensors\
    \ 2021, 21, 1910\n17 of 24\nAWS EKS cluster, there are no hardcoded limits for\
    \ the number of pods when using kops.\nFrom the performed experiments, it was\
    \ tested that the EC2 instance type t2.micro is too\nsmall. The experiment was\
    \ repeated with the EC2 instance type t2.small for a worker node\nand with deploying\
    \ all the services needed for a production environment:\nkops$ export K8S_EXP_ENVIRONMENT=\
    \ testing\nkops$ ./tasks_create\nkops$ ./tasks_enable_velero\nkops$ ./tasks_enable_as\n\
    kops$ ./tasks_enable_logging\nkops$ ./tasks_test\nThe tests were successful and\
    \ all the pods were in status Running. Thus, it may be\nconcluded that the minimal\
    \ EC2 instance type is t2.small. To summarize, both methods of\ndeploying Kubernetes\
    \ clusters kops and eksctl allow worker nodes minimally of EC2 in-\nstance type:\
    \ t2.small, in order to satisfy the production deployment requirements. However,\n\
    for experimental environments, kops allows for smaller types (t2.micro can be\
    \ used).\n5.4. Easiness of Conﬁguration\nWith kops, it was harder to generate\
    \ the conﬁg ﬁle. Kops’ documentation recommends\nthat one should use kops create\
    \ cluster command instead of trying to generate it by hand.\nUsing this command,\
    \ one needs to create an S3 bucket ﬁrst, the conﬁguration ﬁle is put\nthere and\
    \ then, the object hosted on an S3 bucket can be exported to a local ﬁle. On the\n\
    contrary, with eksctl, it was easier to generate the YAML ﬁle and it was done\
    \ by hand.\nOn the other hand, kops allows for conﬁguring the parameters of the\
    \ control plane\ncomponents, while eksctl does not. Another thing is that kops\
    \ allows using Golang\ntemplates [66]. Thus, it is possible to use one template\
    \ ﬁle and use it, for example, to deploy\ninto two different environments: testing\
    \ and production. However, since such templating\nsolution was not possible with\
    \ eksctl (to the best knowledge of authors), template ﬁles with\nBash variables\
    \ were used. It was easier to manage the template ﬁles for kops and for eksctl\n\
    in the same way. This solution worked well and accomplished its purpose.\nConsidering\
    \ the encountered problems, there was an unexpected error, when creating\nthe\
    \ cluster with eksctl and applying the security measure to restrict the IP addresses\
    \ which\ncan access the API server. It was, subjectively, quite hard to ﬁnd the\
    \ working conﬁguration\nand also it took long, because creating the not working\
    \ cluster then, resulted in timeout\nafter about 43 min. However, this problem\
    \ was solved.\n5.5. Meeting the Automation Requirement\nIt was extremely important\
    \ for us to automate the cluster operations. This was needed\nfor several reasons:\
    \ we wanted to make the experiment easy to be repeated, we wanted to\nbe able\
    \ to quickly recreate a cluster, and also we wanted to achieve such level of operations\n\
    automation, so that a CICD server could be used to continuously test such a cluster.\n\
    Apart from that, we also decided to follow Infrastructure as Code (IaC) best practices.\n\
    IaC is an approach which suggests treating everything as code. While kops and\
    \ eksctl\nalready provide some level of automation, our work resulted in having\
    \ simple, one-line\nBash commands used to (among others) deploy, test and destroy\
    \ a cluster. For example,\nin order to deploy an EKS cluster, all what was needed\
    \ was a git repository https://github.\ncom/xmik/MastersThesis (accessed on 31\
    \ January 2021) and the following Bash command:\nkops$ ./tasks _create\nThen,\
    \ one had to wait on average 6 min, and the cluster should have been automati-\n\
    cally created. The Bash script name was: tasks. Similarly, to test the cluster,\
    \ one should just\nrun:\nkops$ ./tasks _test\nThe code for the test task can be\
    \ found here: https://github.com/xmik/MastersThesis/\nblob/master/src/\\kops/tasksL165/\
    \ (accessed on 31 January 2021) and all the tests are\nSensors 2021, 21, 1910\n\
    18 of 24\nkept here: https://github.com/xmik/MastersThesis/blob/master/src/tests/\\\
    tests.bats\n(accessed on 31 January 2021).\nWe tested for example: whether a correct\
    \ version of kubectl was installed, whether\nthe worker nodes have status: Ready\
    \ or whether a test application (a microservice) can be\ndeployed on top of such\
    \ a cluster. The Bats-core framework was used.\nIt is worth summarizing how automating\
    \ the cluster operations progressed. Firstly,\nboth kops and eksctl automatically\
    \ edited the local kubeconﬁg ﬁles. This was tremen-\ndously helpful, because immediately\
    \ after cluster creation, one can access a cluster using\nkubectl command.\nSecondly,\
    \ there was a difference concerning the moment in which the cluster creation\n\
    commands return. The kops creation command returned immediately. This means that\
    \ in\norder to automate the cluster creation to the extent that it can be run\
    \ on a CI server, some\nwaiting mechanism was needed. However, the waiting mechanism\
    \ was easy to implement,\nbecause kops provides the kops validate cluster command.\
    \ The command was run in a\nloop (with sleep in between each trial) and after\
    \ is succeeded, the cluster was deemed\ncreated. On the other hand, creating the\
    \ cluster with eksctl returned after all the AWS\nresources were created.\nBesides\
    \ the aforementioned differences, automating the operations was done in a very\n\
    similar manner for both deployment methods. Creating the conﬁguration was harder\
    \ with\nkops, but this was a manual step and it does not need to be run on a CI\
    \ server.\n5.6. Cost of Chosen Kubernetes Cluster Operations\nThanks to really\
    \ deploying the clusters, it was possible to be sure of exactly which\nand how\
    \ many of AWS resources (such as EC2, ALB, EBS) were created by a deployment\n\
    tool. Based on this information it was possible to compare how much a Kubernetes\
    \ cluster\ndeployed with eksctl and kops would cost to run for one month. This\
    \ information comes\nfrom a real-life scenario and could be useful to someone\
    \ who wants to have a Kubernetes\ncluster deployed for a longer time.\nTo be aware\
    \ of the cost generated by AWS resources needed to run the clusters,\nan AWS tag\
    \ was applied to all the AWS resources. Thanks to such a solution, it was\npossible\
    \ to use the AWS Cost Explorer to verify AWS resources cost. The particular tag\n\
    applied was either deployment: eks-testing or deployment: kops-testing, depending\n\
    on the deployment method used. In practice, no cluster was deployed with the label\n\
    deployment: kops-production or deployment: eks-production, because in this work\
    \ there\nwas no difference between the testing and production environment. The\
    \ intention of\nhaving two environments was to prove that all the conﬁguration\
    \ and operations can be\nautomated for multiple environments and, so that in the\
    \ future, it is possible to choose the\nenvironment. Chart taken from the AWS\
    \ Cost Explorer website is presented in Figure 5.\nThis chart should not be treated\
    \ as a single source of truth, because ﬁrstly, the AWS\ntags were enabled later\
    \ in the empirical work phase and secondly, experimenting with\nthe clusters created\
    \ with kops took different amount of time and was performed different\namount\
    \ of times than experimenting with the clusters created with eksctl. The chart\
    \ is just\na way of showing an example how it is possible to monitor Kubernetes\
    \ clusters cost.\nTo be able to compare the cost of a cluster created with kops\
    \ and a cluster created with\neksctl, the following steps were taken:\n1.\nA cluster\
    \ was created with kops, using the src/kops/cluster.yaml ﬁle and central logging\n\
    was deployed.\n2.\nAll the AWS resources decorated with the tag: deployment kops-testing\
    \ were listed\nwith the following command: aws resourcegroupstaggingapi get-resources—tag-\
    \ ﬁlters\nKey=deployment,\nValues=kops-testing.\n3.\nThe kops cluster was deleted.\n\
    4.\nA cluster was created with eksctl, using the src/eks/cluster.yaml ﬁle.\nSensors\
    \ 2021, 21, 1910\n19 of 24\n5.\nAll the AWS resources decorated with the tag:\
    \ deployment eks-testing were listed\nwith the following command: aws resourcegroupstaggingapi\
    \ get-resources –tag- ﬁlters\nKey=deployment,\nValues=eks-testing.\n6.\nThe AWS\
    \ EKS cluster was deleted.\nVersion March 3, 2021 submitted to Journal Not Speciﬁed\n\
    18 of 23\nFigure 5. Cost report available on AWS Cost Explorer, grouped by the\
    \ AWS tag: deployment.\nIn order to be able to compare the cost of a cluster created\
    \ with kops and a cluster created with eksctl, the\n739\nfollowing steps were\
    \ taken:\n740\n1.\nA cluster was created with kops, using the src/kops/cluster.yaml\
    \ ﬁle and central logging was deployed.\n741\n2.\nAll the AWS resources decorated\
    \ with the tag: deployment kops-testing were listed with the following\n742\n\
    command: aws resourcegroupstaggingapi get-resources –tag- ﬁlters Key=deployment,Values=kops-testing.\n\
    743\n3.\nThe kops cluster was deleted.\n744\n4.\nA cluster was created with eksctl,\
    \ using the src/eks/cluster.yaml ﬁle.\n745\n5.\nAll the AWS resources decorated\
    \ with the tag: deployment eks-testing were listed with the following command:\n\
    746\naws resourcegroupstaggingapi get-resources –tag- ﬁlters Key=deployment,Values=eks-testing.\n\
    747\n6.\nThe AWS EKS cluster was deleted.\n748\nThis procedure resulted in two\
    \ lists of all AWS resources, used by each Kubernetes deployment method.\n749\n\
    Next, all the free AWS resources (VPC Subnets, Security Groups, Internet Gateway,\
    \ Route Table) were omitted.\n750\nOverall, using eksctl, the following AWS resources\
    \ were used: 1 NAT Gateway, 2 CloudFormation stacks, AWS\n751\nEKS cluster, 2\
    \ EC2 instances of type: t2.small (used asworker nodes), an S3 bucket, 1 Load\
    \ Balancer (ALB) (to\n752\nexpose API server). For kops, there was a bug and not\
    \ all the AWS resources were tagged [60–62]. That bug\n753\nshould be ﬁxed in\
    \ the future releases of kops, because there is already a PR merged on kops Github.com\
    \ webiste\n754\n[59]. Despite this, it was possible to itemize the not free AWS\
    \ resources used by kops: 3 EC2 instances of type:\n755\nt2.micro (used as master\
    \ nodes), 2 EC2 instances of type: t2.small (used asworker nodes), 1 Load Balancer\
    \ (ALB)\n756\n(to expose API server), 6 EBS volumes (3 for etcd-main and 3 for\
    \ etcd-events) of type gp2 and 20GB each, an S3\n757\nbucket. Whenever Elastic\
    \ IPs were used, they were omitted, because an attached Elastic IP does not cost\
    \ anything.\n758\nThen, the cost for running a cluster for one month was computed.\
    \ It is presented in tables 3 and 4. Each\n759\nof them lists the AWS resources\
    \ used by one deployment method. The last row of each table contains the total\n\
    760\namount, in USD, of the monthly cost of the AWS resources. The calculated\
    \ cost does not include everything (e.g.\n761\ntransfer costs), but it should\
    \ be enough to compare kops to eksctl. Many pricing information is given by Amazon\n\
    762\nper hour. 720 hours in a month was assumed. When it comes to CloudFormation\
    \ resources, there were 2 stacks\n763\nused.\n764\nTable 3. Expected cost of running\
    \ a Kubernetes cluster created with eksctl for one month.\nAWS Resource\nCost\
    \ using eksctl method\n1 NAT Gateway [25]\n720 * $0.048 = $34.56\nClassic Load\
    \ Balancer [28]\n720 * $0.028 = $20.16\n2 CloudFormation stacks [26]\n88 * $0.0009\
    \ = $0.0792\nAWS EKS cluster [22]\n720 * $0.10 = $72\n2 EC2 instances of type:\
    \ t2.small [20]\n2 * 720 * $0.025 = $36\nkeeping ﬁles on S3 bucket [24]\n$0.023\n\
    Total\n$162.8222\nFigure 5. Cost report available on AWS Cost Explorer, grouped\
    \ by the AWS tag: deployment.\nThis procedure resulted in two lists of all AWS\
    \ resources, used by each Kubernetes\ndeployment method. Next, all the free AWS\
    \ resources (VPC Subnets, Security Groups,\nInternet Gateway, Route Table) were\
    \ omitted. Overall, using eksctl, the following AWS\nresources were used: 1 NAT\
    \ Gateway, 2 CloudFormation stacks, AWS EKS cluster, 2 EC2\ninstances of type:\
    \ t2.small (used asworker nodes), an S3 bucket, 1 Load Balancer (ALB)\n(to expose\
    \ API server). For kops, there was a bug and not all the AWS resources were\n\
    tagged [32,42,67]. That bug should be ﬁxed in the future releases of kops, because\
    \ there\nis already a PR merged on kops Github.com website [68]. Despite this,\
    \ it was possible to\nitemize the not free AWS resources used by kops: 3 EC2 instances\
    \ of type: t2.micro (used\nas master nodes), 2 EC2 instances of type: t2.small\
    \ (used asworker nodes), 1 Load Balancer\n(ALB) (to expose API server), 6 EBS\
    \ volumes (3 for etcd-main and 3 for etcd-events) of type\ngp2 and 20GB each,\
    \ an S3 bucket. Whenever Elastic IPs were used, they were omitted,\nbecause an\
    \ attached Elastic IP does not cost anything.\nThen, the cost for running a cluster\
    \ for one month was computed. It is presented in\nTables 3 and 4. Each of them\
    \ lists the AWS resources used by one deployment method.\nThe last row of each\
    \ table contains the total amount, in USD, of the monthly cost of the\nAWS resources.\
    \ The calculated cost does not include everything (e.g., transfer costs), but\
    \ it\nshould be enough to compare kops to eksctl. Many pricing information is\
    \ given by Amazon\nper hour. 720 h in a month was assumed. When it comes to CloudFormation\
    \ resources,\nthere were two stacks used.\nTable 3. Expected cost of running a\
    \ Kubernetes cluster created with eksctl for one month.\nAWS Resource\nCost Using\
    \ Eksctl Method\n1 NAT Gateway [69]\n720 × $0.048 = $34.56\nClassic Load Balancer\
    \ [70]\n720 × $0.028 = $20.16\n2 CloudFormation stacks [71]\n88 × $0.0009 = $0.0792\n\
    AWS EKS cluster [46]\n720 × $0.10 = $72\n2 EC2 instances of type: t2.small [72]\n\
    2 × 720 × $0.025 = $36\nkeeping ﬁles on S3 bucket [73]\n$0.023\nTotal\n$162.8222\n\
    Sensors 2021, 21, 1910\n20 of 24\nTable 4. Expected cost of running a Kubernetes\
    \ cluster created with kops for one month.\nAWS Resource\nCost Using Kops Method\n\
    3 EC2 instances of type: t2.micro [72]\n3 × 720 × $0.0126 = $27.216\n2 EC2 instances\
    \ of type: t2.small [72]\n2 × 720 × $0.025 = $36\nClassic Load Balancer [70]\n\
    720 × $0.028 = $20.16\n6 EBS volumes of type gp2 and 20GB each [74]\n6 × 20 ×\
    \ $0.11 = $13.2\nkeeping ﬁles on S3 bucket [73]\n$0.023\nTotal\n$96.599\nThe ﬁrst\
    \ stack managed 10 resources and the second stack—34. The CloudFormation\npricing\
    \ [71] depends on operations count. 10 + 34 = 44 creation operations and 10 +\
    \ 34 = 44\ndeletion operations were assumed based on the information given. 88\
    \ operations total.\nThe pricing website also informs that for operation durations\
    \ above 30 s per operation, one\nwill be charged $0.00008 per second above the\
    \ threshold. It cannot be assumed that all\nthe resources took the same time to\
    \ create, this was not measured. Thus, this cost is not\nincluded in the below\
    \ calculations.\nWhen it comes to the S3 cost, the storage cost is very cheap—$0.023\
    \ for 1 GB [73].\nThe storage used by either kops or eksctl was for sure less\
    \ than 1 GB, thus $0.023 was\nassumed as monthly usage cost. Transfer costs were\
    \ omitted.\nBased on the calculations provided above it costs 162.83 dollars to\
    \ run a Kubernetes\ncluster created by eksctl and 96.60 dollars to run a Kubernetes\
    \ cluster created by kops,\non AWS, for a 30-day month. Some costs, e.g., data\
    \ transfer, were not included.\n6. Discussion and Conclusions\nThe paper presented\
    \ the comparison of two methods for Kubernetes cluster deploy-\nment, using kops\
    \ and using eksctl. Seven comparison criteria were applied. The criteria\nentailed:\
    \ whether all the production environment requirements were met, time needed\n\
    to operate a cluster, number of additional prerequisites, minimal EC2 instance\
    \ types for\nworker nodes, easiness of conﬁguration, meeting the automation requirement\
    \ and cost of\nchosen Kubernetes cluster operations.\nSome criteria are subjective,\
    \ because one may enjoy having a wide choice of possible\nconﬁguration to set,\
    \ whereas another (or other use case) would prefer to create a cluster\nwithout\
    \ a redundant hassle. Unquestionably: time and cost are the criteria helping us\
    \ to\ndecide objectively which method is better. Considering the two criteria,\
    \ the winner is kops.\nKops also provides more conﬁguration options and can be\
    \ applied on multiple clouds (not\nonly AWS).\nOn the contrary, eksctl provides\
    \ an easier way to conﬁgure the cluster, because it is\nconvenient to create a\
    \ YAML conﬁguration ﬁle by hand. There are also no prerequisites\nneeded before\
    \ performing the deployment with eksctl, unless one needs to satisfy the\nbackup\
    \ requirement. Then, some storage location (such as an S3 bucket) is needed. Kops\n\
    needs an S3 bucket to keep the cluster conﬁguration. Besides, eksctl provides\
    \ a managed\nservice (AWS EKS), thus the Kubernetes cluster administrator does\
    \ not have to worry about\nmanaging the control plane components. The last things\
    \ that should be noted is that eksctl\nis not the same as AWS EKS, even though\
    \ eksctl utilizes AWS EKS resource. Furthermore,\nall the results collected in\
    \ this study may depend on the versions of kops, eksctl and maybe\nKubernetes\
    \ itself. Our works used the particular versions of each tool.\nConcluding each\
    \ method can be used for different use cases. For example: kops allows\nmore conﬁguration,\
    \ whereas eksctl is easier to conﬁgure. The method which was deemed\nfaster (in\
    \ relation to cluster operations) and cheaper was kops.\nThe Kubernetes clusters\
    \ were deployed in a formal, documented way. First, we\ngathered the requirements\
    \ needed for a production environment. We wanted to work\non a cluster which is\
    \ closer to a real-life scenario and represents a more practical ap-\nproach.\
    \ Afterwards, the requirements were planned, implemented and the clusters were\n\
    Sensors 2021, 21, 1910\n21 of 24\ndeployed, tested and destroyed. Our article\
    \ is based on our deployed clusters, not just on\ntechnical documentation.\nThe\
    \ comparison of the two deployment methods could hopefully become useful to\n\
    anyone responsible for a Kubernetes cluster deployment or for planning it. Such\
    \ a person\nmay learn how much time is needed to deploy a running, production-grade\
    \ cluster or how\neasy it is to conﬁgure a cluster or how much running such a\
    \ cluster for one month would\ncost. This information may inﬂuence the deployment\
    \ decisions and may result in realising\na need for more experimentation.\nSimilarly,\
    \ the operations automation approach may also be useful for a pragmatic\nInfrastructure\
    \ Engineer who wants to limit human errors while deploying a cluster and\nto ensure\
    \ a repeatable and easily reproducible process. Our one-line operations are easy\n\
    to invoke and ready to be used with a CICD server. Based on such a solution, one\
    \ could\nexpand it by adding new CICD pipeline stages, for example: to test if\
    \ the cluster can be\nupdated automatically.\nProbably the most obvious way to\
    \ make our comparison better is to add more deploy-\nment methods. Kubernetes\
    \ can be deployed in the cloud, thus other managed services,\nsuch as Azure Kubernetes\
    \ Service (AKS) or Google Kubernetes Engine (GKE) could be tried.\nAlso it could\
    \ be possible to determine more production environment requirements then\nnine\
    \ considered. Ideas for additional requirements are: cluster live upgrades, cluster\
    \ live\nreconﬁguration, dealing with persistent data, obeying some country laws\
    \ (concerning for\nexample how long some data should be kept) and more. Moreover,\
    \ every administrator or\nevery company has their own views on how to satisfy\
    \ each of the requirements. For some\npeople white listing one IP address to have\
    \ access to a Kubernetes API server is enough,\nwhile the other could demand to\
    \ keep all master and worker nodes in a private network\nand use a bastion host.\
    \ Also the comparison criteria could be performance oriented. In such\na case,\
    \ a chosen test application could have had needs for special resources, maybe\
    \ running\nsome Machine Learning tasks, demanding access to GPU, lighting fast\
    \ data transfer and\ntiny latencies.\nBy using Kubernetes technology, users can\
    \ build data science and artiﬁcial intelligence\nplatforms based on the best specialized\
    \ components. With projects such as Kubeﬂow and\nOpen Data Hub, Kubernetes technology\
    \ is becoming the standard and primary AI platform.\nThe scope of artiﬁcial intelligence\
    \ applications in the areas of IT infrastructure opera-\ntion and Dev (Sec) Ops\
    \ is expanding, therefore it should be expected that the interest in\nsuch solutions\
    \ will increase in the case of new enterprises with large market opportunities.\n\
    The core element of such environments will be Kubernetes technology, as it offers\
    \ the\nright possibilities for standardization and automation, especially in combination\
    \ with\nthe concept of Kubernetes operators. Customers can count on an increase\
    \ in reliability,\nquality and scalability both in the production environment\
    \ and in the processes of software\ndevelopment and operation.\nAmazon EC2 P4\
    \ instances powered by Intel Cascade Lake processors contain NVIDIA\nA100 Tensor\
    \ Core GPUs, each connected to the others via NVLink and support NVIDIA\nGPUDirect.\
    \ They can deliver up to 2.5 times the performance of deep learning compared\n\
    to P3 instances. EC2 UltraClusters can withstand the toughest loads in the ﬁeld\
    \ of machine\nlearning and HPC on a supercomputer scale: natural language processing,\
    \ image detection\nand classiﬁcation, scene understanding, seismic analysis, weather\
    \ forecasting, ﬁnancial\nmodelling.\nAuthor Contributions: Conceptualization,\
    \ E.C., A.P.-M.; Investigation, E.C.; Methodology, E.C.;\nValidation, A.P.-M.\
    \ All authors have read and agreed to the published version of the manuscript.\n\
    Funding: This research received no external funding.\nInstitutional Review Board\
    \ Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nConﬂicts\
    \ of Interest: The authors declare no conﬂict of interest.\nSensors 2021, 21,\
    \ 1910\n22 of 24\nReferences\n1.\nVayghan, L.A.; Saied, M.A.; Toeroe, M.; Khendek,\
    \ F. Kubernetes as an Availability Manager for Microservice Applications.\narXiv\
    \ 2019, arXiv:1901.04946.\n2.\nThe Kubernetes Authors. Kubernetes API Server—Security.\
    \ 2020. Available online: https://kubernetes.io/docs/reference/\naccess-authn-authz/\
    \ (accessed on 16 May 2020).\n3.\nThe Kubernetes Authors. Kubernetes Offcial Website.\
    \ 2020. Available online: https://kubernetes.io/ (accessed on 16 May 2020).\n\
    4.\nDiouf, G.M.; Elbiaze, H.; Jaafar, W. On Byzantine Fault Tolerance in Multi-Master\
    \ Kubernertes Clusters.\narXiv 2019,\narXiv:1904.06206.\n5.\nArundel, J.; Domingus,\
    \ J. Cloud Native DevOps with Kubernetes: Building, Deploying, and Scaling Modern\
    \ Applications in the Cloud;\nO’Reilly Media: Newton, MA, USA, 2019; ISBN 978-1492040767.\n\
    6.\nPitchumani, R.; Kee, Y.-S. Hybrid Data Reliability for Emerging Key-Value\
    \ Storage Devices. In Proceedings of the18th USENIX\nConference on File and Storage\
    \ Technologies (FAST 20), Santa Clara, CA, USA, 24–27 February 2020; USENIX Association:\n\
    Berkeley, CA, USA, 2020; pp. 309–322. ISBN 978-1-939133-12-0.\n7.\nNetto, H.V.;\
    \ Lung, L.C.; Correia, M.; Luiz, A.F.; de Souza, L.M.S. State machine replication\
    \ in containers managed by Kubernetes.\nJ. Syst. Archit. 2016, 73, 53–59. [CrossRef]\n\
    8.\nSayfan, G. Mastering Kubernetes, 2nd ed.; Packt Publishing: Birmingham, UK,\
    \ 2018; ISBN 978-1788999786.\n9.\nSaito, H.; Lee, H.-C.; Wu, C.-Y. DevOps with\
    \ Kubernetes; Packt Publishing: Birmingham, UK, 2017; ISBN 978-1-78839-664-6.\n\
    10.\nMai, K. Building High Availability Infrastructure in Cloud. Bachelor’s Thesis,\
    \ Metropolia University of Applied Sciences,\nHelsinki, Finland, 2017.\n11.\n\
    Alshammari, M.M.; Alwan, A.A.; Nordin, A.; Al-Shaikhli, I.F. Disaster Recovery\
    \ in Single-Cloud and Multi-Cloud Environments:\nIssues and Challenges. In Proceedings\
    \ of the 4th IEEE International Conference on Engineering Technologies and Applied\n\
    Sciences (ICETAS 2017), Salmabad, Bahrain, 29 November–1 December 2017.\n12.\n\
    Morris, K. Infrastructure as Code, 1st ed.; O’Reilly Media: Newton, MA, USA, 2016;\
    \ ISBN 978-1491924358.\n13.\nHumble, J.; Farley, D. Continuous Delivery: Reliable\
    \ Software Releases through Build, Test, and Deployment Automation, 1st ed.;\n\
    Addison-Wesley Professional: Boston, MA, USA, 2010; ISBN 978-0321601919.\n14.\n\
    Khan, A. Key Characteristics of a Container Orchestration Platform to Enable a\
    \ Modern Application. IEEE Cloud Comput. 2017, 4,\n42–48. [CrossRef]\n15.\nTurol,\
    \ S.; Gutierrez, C.; Matykevich, S. A Multitude of Kubernetes Deployment Tools:\
    \ Kubespray, Kops, and Kubeadm. Available\nonline: https://www.altoros.com/blog/a-multitude-of-kubernetes-deployment-tools-kubespray-\\\
    kops-and-kubeadm/?utm_\ncampaign=SMM-Val_K8sTools%20Post-Free-Promo-Medium&utm_source=Medium\
    \ (accessed on 20 May 2020).\n16.\nWeaveWorks. Production Ready Checklists for\
    \ Kubernetes. Available online: https://go.weave.works/production-ready-\nkubernetes-checklist.html?LeadSource=Website%20-%20General\\\
    &CampaignID=7014M000001cEod (accessed on 15 April 2020).\n17.\nWeaveWorks. Your\
    \ Guide to a Production Ready Kubernetes Cluster. Available online: https://go.weave.works/WP-Production-\n\
    Ready.html?LSD=Homepage&Source=Website%20-%20General (accessed on 15 April 2020).\n\
    18.\nUphill, T. DevOps: Puppet, Docker and Kubernetes; Packt Publishing: Birmingham,\
    \ UK, 2017; ISBN 978-1788297615.\n19.\nMilenovi, M. How to Monitor Kubernetes\
    \ Cluster with Prometheus and Grafana. Available online: https://linoxide.com/linux-\n\
    how-to/monitor-kubernetes-cluster-prometheus-grafana/ (accessed on 15 April 2020).\n\
    20.\nKubernetes Community. Web UI (Dashboard). Available online: https://kubernetes.io/docs/tasks/access-application-cluster/\n\
    web-ui-dashboard/ (accessed on 12 June 2020).\n21.\nNetto, H.; Oliveira, C.P.;\
    \ Rech, L.; Alchieri, E. Incorporating the Raft consensus protocol in containers\
    \ managed by Kubernetes:\nAn evaluation. Int. J. Parallel Emergent Distrib. Syst.\
    \ 2020, 35, 433–453. [CrossRef]\n22.\nBakker, P. One Year Using Kubernetes in\
    \ Production: Lessons Learned. Available online: https://techbeacon.com/devops/one-\n\
    year-using-kubernetes-production-lessons-learned (accessed on 15 April 2020).\n\
    23.\nGraylog. Graylog Offcial Website. Available online: https://www.graylog.org/\
    \ (accessed on 15 April 2020).\n24.\nGraylog. Improving Kubernetes Clusters’ Efﬁciency\
    \ with Log Management. Available online: https://www.graylog.org/post/\nimproving-kubernetes-clusters-efﬁciency-with-log-management\
    \ (accessed on 15 April 2020).\n25.\nAmazon. AWS CloudTrail. Available online:\
    \ https://aws.amazon.com/cloudtrail/ (accessed on 15 April 2020).\n26.\nCristian,\
    \ F. Understanding Fault-Tolerant Distributed Systems. Commun. ACM 1993, 34, 56–78.\
    \ [CrossRef]\n27.\nKanso, A.; Toeroe, M.; Khendek, F. Comparing redundancy models\
    \ for high availability middleware. Computing 2014, 96, 975–993.\n[CrossRef]\n\
    28.\nVaria, J. Architecting for the Cloud: Best Practices. In AWS Whitepapers;\
    \ Amazon Web Services: Seattle, WA, USA, 2010.\n29.\nGravier, T.W. What Is RAID\
    \ and Why Should You Want It? Available online: https://raid.wiki.kernel.org/index.php/What_is_\n\
    RAID_and_why_should_you_want_it%3F (accessed on 12 June 2020).\n30.\nChef Software\
    \ Inc. Offcial Chef website. Available online: https://www.chef.io/ (accessed\
    \ on 16 April 2020).\n31.\nRed Hat, Inc. Offcial Ansible Website. Available online:\
    \ https://www.ansible.com/ (accessed on 16 April 2020).\n32.\nSaltStack, Inc.\
    \ Offcial SaltStack Website. Available online: https://www.saltstack.com/ (accessed\
    \ on 16 April 2020).\n33.\nHashicorp. Terraform Offcial Website. Available online:\
    \ https://www.terraform.io/ (accessed on 18 May 2020).\n34.\nPoniszewska-Maranda,\
    \ A. Modeling and design of role engineering in development of access control\
    \ for dynamic information\nsystems. Bull. Pol. Acad. Sci. Tech. Sci. 2013, 61,\
    \ 569–580. [CrossRef]\nSensors 2021, 21, 1910\n23 of 24\n35.\nMajchrzycka, A.;\
    \ Poniszewska-Mara´nda, A. Secure Development Model for mobile applications. Bull.\
    \ Pol. Acad. Sci. Tech. Sci.\n2016, 64, 495–503. [CrossRef]\n36.\nAlhazmi, O.H.;\
    \ Malaiya, Y.K. Evaluating Disaster Recovery Plans Using the Cloud; Taibah University:\
    \ Medina, Saudi Arabia, 2013.\n37.\nMicrosoft. Health Endpoint Monitoring Pattern.\
    \ Available online: https://docs.microsoft.com/en-us/azure/architecture/\npatterns/health-endpoint-monitoring\
    \ (accessed on 17 April 2020).\n38.\nKubernetes Community. Conﬁgure Liveness,\
    \ Readiness and Startup Probes. Available online: https://kubernetes.io/docs/\n\
    tasks/conﬁgure-pod-container/conﬁgure-liveness-readiness-startup-probes/ (accessed\
    \ on 17 April 2020).\n39.\nBitnami. Apache Helm Chart. Available online: https://github.com/bitnami/charts/tree/master/bitnami/\\\
    apache (accessed\non 30 May 2020).\n40.\nWeaveWorks. CloudWatch Logging. Available\
    \ online: https://eksctl.io/usage/cloudwatch-cluster-logging/ (accessed on 27\
    \ May\n2020).\n41.\nKops Authors. High Availability (HA). Available online: https://github.com/kubernetes/kops/blob/v1.16.2/docs/operations/\n\
    high_availability.md (accessed on 30 May 2020).\n42.\nSusmel, A. Create a High-Availability\
    \ Kubernetes Cluster on AWS with Kops. Available online: https://www.poeticoding.com/\n\
    create-a-high-availability-kubernetes-cluster-on-aws-with-kops/ (accessed on 29\
    \ May 2020).\n43.\nAmazon. VPCs and Subnets. Available online: https://docs.aws.amazon.com/vpc/latest/userguide/\\\
    VPC_Subnets.html\n(accessed on 25 May 2020).\n44.\nKops Authors. Description of\
    \ Keys in Conﬁg and Cluster.spec. Available online: https://github.com/kubernetes/kops/blob/v1\n\
    .16.2/docs/cluster_spec.md (accessed on 2 June 2020).\n45.\nAmazon. Linux Bastion\
    \ Hosts on the AWS Cloud. Available online: https://docs.aws.amazon.com/quickstart/latest/linux-\n\
    bastion/overview.html (accessed on 25 May 2020).\n46.\nAmazon. Amazon EKS Kubernetes\
    \ Pricing. Available online: https://aws.amazon.com/eks/pricing/ (accessed on\
    \ 18 May 2020).\n47.\nAmazon. What is Amazon EKS? Available online: https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\n\
    (accessed on 18 May 2020).\n48.\nAmazon. Amazon EKS Clusters. Available online:\
    \ https://docs.aws.amazon.com/eks/latest/userguide/clusters.html (accessed\non\
    \ 19 May 2020).\n49.\nKops Authors. Kops Project Page on Github.com. Available\
    \ online: https://github.com/kubernetes/kops (accessed on 18 May\n2020).\n50.\n\
    Kops Authors. Installing. Available online: https://kops.sigs.k8s.io/getting_started/install/\
    \ (accessed on 18 May 2020).\n51.\nKops Authors. Getting Started with kops on\
    \ AWS. Available online: https://kops.sigs.k8s.io/getting_started/aws/ (accessed\
    \ on\n20 May 2020).\n52.\nKops Authors. Kubernetes Addons and Addon Manager. Available\
    \ online: https://github.com/kubernetes/kops/blob/master/\ndocs/operations/addons.md\
    \ (accessed on 20 May 2020).\n53.\nKubernetes Community. Troubleshoot Applications.\
    \ Available online: https://kubernetes.io/docs/tasks/debug-application-\ncluster/debug-application/\
    \ (accessed on 3 June 2020).\n54.\nKubernetes Community. Using Minikube to Create\
    \ a Cluster. Available online: https://kubernetes.io/docs/tutorials/kubernetes-\n\
    basics/create-cluster/cluster-intro/ (accessed on 10 April 2020).\n55.\nGoogle.\
    \ Troubleshooting. Available online: https://cloud.google.com/kubernetes-engine/docs/\\\
    troubleshooting (accessed on\n6 June 2020).\n56.\nKubernetes Community. Debug\
    \ Clusters. Available online: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-\n\
    cluster/ (accessed on 6 June 2020).\n57.\nAmazon.\nAmazon EKS Troubleshooting.\n\
    Available online:\nhttps://docs.aws.amazon.com/eks/latest/userguide/\ntroubleshooting.html\
    \ (accessed on 6 June 2020).\n58.\nWeaveWorks. Troubleshooting. Available online:\
    \ https://eksctl.io/usage/troubleshooting/ (accessed on 6 June 2020).\n59.\nKubernetes\
    \ Community. Troubleshoot Clusters. Available online: https://kubernetes.io/docs/tasks/debug-application-cluster/\n\
    troubleshooting/ (accessed on 6 June 2020).\n60.\nLiran Polak. EKS Done Right—From\
    \ Control Plane to Worker Nodes. Available online: https://spot.io/blog/eks-done-right-\n\
    from-control-plane-to-worker-nodes/ (accessed on 4 June 2020).\n61.\nZhang, H.\
    \ Learning Kubernetes on EKS by Doing Part 1—Setting Up EKS. Available online:\
    \ https://medium.com/faun/learning-\nkubernetes-by-doing-part-1-setting-up-eks-in-aws-50dcf7a76247\
    \ (accessed on 4 June 2020).\n62.\nWeaveWorks. VPC Networking. Available online:\
    \ https://eksctl.io/usage/vpc-networking/ (accessed on 24 May 2020).\n63.\nAmazon.\
    \ File Containing Hard Limits Set on EKS, Limiting the Number of Pods Allowed\
    \ for an EC2 Instance Type. Available\nonline: https://github.com/awslabs/amazon-eks-ami/blob/master/ﬁles/eni-max-pods.txt\
    \ (accessed on 22 May 2020).\n64.\nKubernetes Community. Advanced Kubernetes Scheduling.\
    \ Available online: https://kubernetes.io/blog/2017/03/advanced-\nscheduling-in-kubernetes/\
    \ (accessed on 20 June 2020).\n65.\nGruntwork. Comprehensive Guide to EKSWorker\
    \ Nodes. Available online: https://kubernetes.io/docs/concepts/scheduling-\neviction/assign-pod-node/#per-pod-conﬁgurable-eviction-\\\
    behavior-when-there-are-node-problems-alpha-feature (accessed\non 20 June 2020).\n\
    Sensors 2021, 21, 1910\n24 of 24\n66.\nKops Authors. Kops Golang Package Documentation.\
    \ Available online: https://pkg.go.dev/k8s.io/kops/pkg/apis/kops?tab=\ndoc#ClusterSpec\
    \ (accessed on 20 May 2020).\n67.\nGithub User: Schollii. Not All Resources Get\
    \ Tags from CloudLabels in kops 1.13. Available online: https://github.com/\n\
    kubernetes/kops/issues/8792 (accessed on 7 June 2020).\n68.\nGithub User: Rifelpet.\
    \ Add CloudLabels Tags to Additional AWS Resources. Available online: https://github.com/kubernetes/\n\
    kops/pull/8903 (accessed on 7 June 2020).\n69.\nAmazon. Amazon VPC pricing. Available\
    \ online: https://aws.amazon.com/vpc/pricing/ (accessed on 8 June 2020).\n70.\n\
    Amazon. Elastic Load Balancing Pricing. Available online: https://aws.amazon.com/elasticloadbalancing/pricing/\
    \ (accessed\non 8 June 2020).\n71.\nAmazon. AWS CloudFormation Pricing. Available\
    \ online: https://aws.amazon.com/cloudformation/pricing/ (accessed on\n8 June\
    \ 2020).\n72.\nAmazon. Amazon EC2 Pricing. Available online: https://aws.amazon.com/ec2/pricing/on-demand/\
    \ (accessed on 21 May\n2020).\n73.\nAmazon. Amazon S3 Pricing. Available online:\
    \ https://aws.amazon.com/s3/pricing/ (accessed on 8 June 2020).\n74.\nAmazon.\
    \ Amazon EBS Pricing. Available online: https://aws.amazon.com/ebs/pricing/ (accessed\
    \ on 8 June 2020).\n"
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  pdf_link: https://www.mdpi.com/1424-8220/21/5/1910/pdf?version=1615350002
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: Kubernetes Cluster for Automating Software Production Environment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/sose52839.2021.00020
  analysis: '>'
  authors:
  - Stefan Throner
  - Heiko Hütter
  - Niklas Sänger
  - Michael Schneider
  - Simon Hanselmann
  - Patrick Petrovic
  - Sebastian Abeck
  citation_count: 5
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2021 IEEE International Confe... An Advanced DevOps
    Environment for Microservice-based Applications Publisher: IEEE Cite This PDF
    Stefan Throner; Heiko Hütter; Niklas Sänger; Michael Schneider; Simon Hanselmann;
    Patrick Petrovic; Sebastian Abeck All Authors 6 Cites in Papers 993 Full Text
    Views Abstract Document Sections 1. Introduction 2. Related Work 3. Template-Based
    DevOps Environment 4. Developer''s Perspective on the Devops Environment 5. Security
    Aspects of the DevOps Environment Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: Complex applications consisting of many interdependent
    microservices require an advanced environment that allows their efficient Development
    and Operations (DevOps). One of the central components of a DevOps environment
    is a pipeline concept that supports the Continuous Integration/ Continuous Deployment
    (CI/CD) of single microservices, usually in the form of a container-virtualized
    cloud infrastructure based on advanced technologies such as Docker, Kubernetes,
    or Helm. Although there are available concepts and technologies to implement these
    concepts, it remains unclear how to combine the concepts and technologies into
    an advanced DevOps environment which specifically supports the different roles
    involved in the process. This paper describes the DevOps environment set up to
    develop microservice-based applications and focuses on the following aspects:
    (i) a flexible CI/CD pipeline based on reusable templates, (ii) support for developers
    to use the DevOps environment efficiently, and (iii) the security of the environment
    against attacks. Published in: 2021 IEEE International Conference on Service-Oriented
    System Engineering (SOSE) Date of Conference: 23-26 August 2021 Date Added to
    IEEE Xplore: 15 October 2021 ISBN Information: ISSN Information: DOI: 10.1109/SOSE52839.2021.00020
    Publisher: IEEE Conference Location: Oxford, United Kingdom SECTION 1. Introduction
    Over the last five years, microservices [1] have been gaining more widespread
    attention. Today, many newly developed software systems are based on the microservice
    architecture, and existing monolithic systems are divided into microservices [2],
    which provide several advantages. One advantage is that the microservice architecture
    is based on a domain-driven design [3] which results in well-defined micro service
    APIs [4] that can be reused in many applications. This may improve the maintainability
    of the software system, especially compared to a monolithic system. A second advantage
    is that since each microservice can be built, tested, and deployed on its own,
    such applications support the concept of continuous Integration/Continuous Deployment
    (CI/CD) [5]. In a professional environment, complex applications with many interdependent
    microser-vices are deployed, and this process must be supported by a powerful
    CI/CD pipeline as part of a DevOps setup. A third advantage of microservices concerns
    their operational aspect. As the virtual computing unit of the architecture, a
    microservice fits a container-virtualized infrastructure which uses technologies
    such as Docker and Kubernetes. Such infrastructures support both vertical and
    horizontal scaling, which result in flexible usage of the available computing
    resources by the running micro service-based applications [6]. For developers,
    the additional abstraction and tools require additional knowledge and work, resulting
    in a shift in focus from software development to additional operational overhead
    [7]. The DevOps environment introduced in our approach can drastically reduce
    this overhead and enforce quality and security standards for all services. Furthermore,
    our approach minimizes the maintenance effort and infrastructure costs without
    compromising the developer''s experience. To demonstrate its effectiveness, we
    tested our approach using a project with multiple microservices and validated
    the results. Figure 1 illustrates the central elements of the DevOps environment
    that we have configured to build, test, deploy, and run micro service-based applications
    from various domains. The following chapters take two such applications, PredictiveCarMaintenance
    and ClinicsAssetManage-ment (from the connected car and healthcare domains, respectively)
    as examples. Figure 1. Topic sketch Show All Figure 1 presents the two applications
    and the DevOps environment which we used to build, test, deploy and operate the
    applications. The environment includes a GitLab CI/CD pipeline. By providing reusable
    project templates that contain a CI/CD pipeline configuration, we can share most
    of our configurations by referencing the shared pipeline configuration. It consists
    of the framework-independent configuration files for the release and deployment
    stage as well as all the framework-specific configurations for several tech- nology
    stacks. This results in project templates with a single configuration which solely
    references the shared pipeline configuration. By implementing this structure,
    a single point of configuration can be achieved. This process reduces or even
    eliminates the increased management and maintenance effort required for a decentralized
    DevOps environment. The present article is structured as follows: Chapter 2 presents
    the state-of-the-art microservices and DevOps concepts and technologies. Chapter
    3 describes our DevOps environment, with a focus on the template concept that
    we introduced to simplify the configuration of the CI/CD pipeline by reuse of
    these templates. Chapter 4 provides a software developer''s perspective on the
    DevOps environment by demonstrating the use of the templates through the example
    of a concrete microservice-based application. Subsequently, Chapter 5 discusses
    the actions that we have taken to protect our DevOps environment against attacks
    and introduces a pipeline-securing concept. Lastly, Chapter 6 summarizes the main
    results of our approach as well as the major research issues that we are currently
    working on. SECTION 2. Related Work 2.1. Microservices Balalaie et al. [8] emphasizes
    the synergy of microser-vices and DevOps, which have displayed equal rates of
    growth among IT concepts over the past several years. The main reason is that
    microservices enable a continuous deployment, which is one of the major goals
    of DevOps. The term “micro service” originated in 2011, and a group of software
    architects chose it as the most appropriate name in 2012. Lewis was among the
    first researchers to use this term in a presentation [9]. Three years later, Lewis
    and Fowler [1] published the first comprehensive description on microservices
    and the microservice style. Based on their work, Newman [10] published the first
    well-accepted and often cited book on the topic. One strong motivation behind
    the microservice architectural style is the inherent disadvantage of monolithic
    systems with respect to change cycles and complex deployment since any changes
    to each small part of the application require a complete rebuilding and deployment
    of the entire monolith [2]. 2.2. DevOps Chen [5] mentions that while the microservice
    architecture reduces the complexity of each service, it also increases the number
    of services. This requires an additional effort in terms of interactions and contracts
    between the teams. To mitigate such problems, Wilsenach [11] states that the culture
    of DevOps should contain no silos between development and operations. Although
    there is no precise definition of the term “DevOps”, certain concepts, such as
    automation, collaboration and shared responsibility among teams that are often
    associated with it. Lwakatare et al. [12] analyzed literature surrounding this
    topic and identify the five dimensions of DevOps: collaboration, automation, culture,
    monitoring and measurement. Erich et al. [13] reveal that different companies
    have varied views on the definition of DevOps, but they all share the opinion
    that the team, culture and automation are key elements of the concept. Furthermore,
    Senapathi [14] states that one reason for the adoption of DevOps is that it improves
    user experience due to the shorter response times to customer requests as well
    as higher team productivity resulting from automation and the reduction of communication
    barriers. This can be achieved through agile methods and CI/CD pipelines. Riungu-Kalliosaari
    et al. [15] analyze how different companies have adopted the DevOps concepts.
    Problems in communication and motivation for change are major aspects that mitigate
    the adoption process. Steven [16] claims that DevOps focuses on the culture and
    roles within the teams, and that it complements the processes of agile methods
    and the software development cycle of CI/CD. 2.3. Container Orchestration In their
    paper, Kang et al. [17] note the ways in which containerization benefits the microservice
    and DevOps approach. The orchestration of the microservices plays a major role
    in DevOps as well as the requirements of modern software systems. Asif Khan [18]
    from Amazon lists scheduling, state management, and service discovery as a the
    main characteristics of container orchestration. This enables rolling updates
    of the software without any downtime or the need for a secondary system like,
    as is often required in the case of blue-green deployment. Such requirements are
    typically configured by the operational team. Kubernetes is the widely used form
    of container orchestration [19] [20] which comes with its own complexity and tools.
    Paul Bakker [21] a software architect at Netfiix, observes a significant advantage
    in standardizing the operational aspects. He also mentions that it was not sufficient
    for their use case and difficult to debug or monitor without additional tools
    and know ledge. 2.4. CI/CD Pipeline A micro service architecture enables the efficient
    use of the CI/CD concept [5], for which Shahin et al. [7] list several challenges.
    Beside the lack of suitable infrastructure, the authors also mention common problems
    such as a lack of investment, which involves cost, skills, and time for integration
    and maintenance. Team awareness and communication is another major challenge,
    since CI/CD requires the collaboration of the developers and operational teams
    in the companies. Furthermore, continuous deployment demands additional awareness,
    since it can directly affect a production system. To overcome the issues in the
    adoption of CI/CD pipelines, we introduce a template-based pipeline approach in
    Chapter 3. This approach allows all developer to deploy the services with nearly
    zero effort and test them in a setting that is close to the production environment.
    Since attacks on the DevOps environment and its CI/CD pipeline can cause great
    damage to an organization, adequate security measures must be taken. Ullah et
    al. [22] present five security tactics to protect main components such as the
    respository or CI server of the pipeline from malicious attacks. The effectiveness
    of the protection is then assessed through a comparison with an unsecured pipeline.
    Alternatively, Bass et al. [23] introduce a process to decompose the pipeline
    into a set of trusted and untrusted components, which are then isolated from each
    other. An interesting aspect of this approach is the analysis of the access rights
    and network connections of the pipeline components. In Chapter 5, we apply this
    generic process to the CI/CD pipeline of our concrete DevOps environment. SECTION
    3. Template-Based DevOps Environment Figure 2 introduces our template-based pipeline
    approach, which aims to lower the entry barriers for developers to provide applications
    as well as reduce the maintenance efforts of the operational team. This is achieved
    through the use of templates and the generalization and reuse of the pipeline
    steps. Through the GitLab internal referencing mechanism, single files of individual
    steps can be injected and shared across multiple projects, thereby effectively
    addressing the major issue of the management of multiple projects in [5]. When
    using the pipeline templates, we must also ensure that no breaking changes occur
    due to the maintenance and expansion of the templates. To prevent this problem,
    we have introduced a workflow that ensures compatibility. Another method to achieve
    the above mentioned goals is using project templates; these provide developers
    with a quick start to deploy their software in a Kubernetes cluster. In addition,
    we have also created a service provisioning concept that can the develop of multiple
    features for a single service, without conflicts or additional costs to the environment.
    Our approach therefore addresses many of the existing challenges [7] and ensures
    that quality standards are met. Figure 2. Template-based pipeline approach Show
    All 3.1. Shared Pipeline Configuration Each project template contains a predefined
    pipeline definition, which references the local project variables as well as the
    shared pipeline templates. A pipeline template provides all the necessary pipeline
    steps to execute a certain task. For example, the shared pipeline for Docker images
    includes build, push, and scan the images for known security vulnerabilities.
    The individual pipeline steps are then executed by a referenced pipeline image.
    The include mechanism in GitLab [24] allows the user to reference the relevant
    files-in our case the pipeline templates-in other projects. Additionally a reference
    to a tag or branch of the file can be set. The shared pipeline template mechanism
    make great use of this feature. During our research, we identified two types of
    pipeline steps: framework-independent and framework-specific. Framework-specific
    steps include source code compilation, test execution, code style checks, as well
    as security checks, which are further explained in Chapter 5. The two categories
    differ in the amount of customization and modification that they need to provide.
    Therefore, the developer has the option to extend the framework -specific pipeline
    templates using a local configuration in the project template. To prevent the
    changes to the shared pipeline templates from having a negative impact on the
    existing pipelines, a workflow for the maintenance of the shared pipeline templates
    has been introduced. For the workflow shown in Figure 3, each change to the pipeline
    templates or the deployment image triggers a different tests to prevent any negative
    influences. These tests include unit tests for the bash based script of the deployment
    image as well as the execution of a pipeline within a example project to test
    the interaction of all its pipeline steps. Afterwards, a code review is conducted
    to merge and release the changes into the production environment. To track each
    change, the semantic versioning specification [25] is used. This ensures that
    there are no breaking changes during a major version, and patches or features
    can be delivered to the developers without any modifications from their side.
    To allow the individual pipeline steps to be tested quickly and independently
    from GitLab, the logic has been encapsulated in the pipeline image. For example,
    image.sh and scan.sh contains all the logic for the build, push, and scan steps
    of the shared pipeline template for the Docker images. Therefore, each individual
    pipeline step can be tested locally, and any bugs can be fixed quickly using the
    image release mechanism provided by our shared pipeline template. Figure 3. Maintenance
    workflow for the pipeline templates Show All 3.2. Project Templates To support
    the developer in integrating and delivering the application, specific project
    templates adapted to the framework are offered. These templates contain all the
    artifacts required by the developer for the execution of a CI/CD process. Each
    project template includes a gitlab.yml file, which contains all the required templates
    from the shared pipeline for this framework. Since it is not possible to satisfy
    all the requirements for a project build with a single configuration, we provide
    a project-specific pipeline artifact. This allows us to encapsulate the project-specific
    build and tests the more static configurations of the pipeline templates. In addition,
    a framework-specific Dockerfile, which follows the best practices of Google [26],
    is provided. The Dockerfile builds and defines the port for the service, which
    is then referenced in the Helm chart of the project. The Helm chart in turn contains
    all the required Kubernetes manifests and dependencies as well as the values for
    the project. Predefined variables in the values.yaml file of the chart allow the
    user to add databases or other backing services to the project. To improve the
    usability, a recommended project structure is provided. 3.3. Application Provisioning
    The provisioning for an application is predefined in the project template pipeline.
    Each template follows the process illustrated in Figure 4. Figure 4. Application
    provisioning process from an operational perspective Show All In the first step,
    certain rules are checked, and a linting of the project is performed. Violations
    of company-internal guidelines and faulty configurations can be detected at the
    very beginning. In addition, the prepare step creates a Git-Lab environment that
    allows us to define a stop procedure and a lifespan for the deployment. This lifespan
    is limited to feature branches so as to save costs and still provide a test environment
    for other developers. After the prepare step the software is built and tested.
    All build artifacts are passed on to the next steps. To enable a collision-free
    and configuration-free delivery, a large number of parameters are injected at
    the start of the publishing process. Therefore, predefined GitLab-specific and
    institute-internal environment variables are used. The developers have the authority
    to override these variables by setting them manually. Next, in the publish step,
    the Docker image is built, and template of the Helm chart template is rendered
    and delivered to a Harbor registry. The delivered artifacts are then deployed
    to a predefined Kubernetes cluster. The flow of the pipeline execution varies
    depending on the application. Therefore, a distinction is made among version tags
    in GitLab as well as protected and feature branches, which differ in terms of
    the scope of the tests and the lifetime of the environment. This optimizes the
    build time and minimizes operating costs. In addition to these benefits, our approach
    allows allows the developer to focus more on their actual work and drastically
    reduces the effort spent on the operational aspects. SECTION 4. Developer''s Perspective
    on the Devops Environment We developed the Predictive Car Maintenance (PCM) application
    using the project templates and shared pipeline configuration. The goal of this
    application is to predict maintenance events for cars. The design process resulted
    in three microservices: Vehicle, Diagnosis, and Predictive Maintenance. The application
    also has a frontend and a BackendForFrontend (BFF). Figure 5. PCM application
    in the DevOps environment Show All The microservices are each maintained in a
    Git project. While the Vehicle microservice was developed using the Maven project
    template, the other microservices utilize the Node.js or Go templates. Figure
    5 presents the PCM application in the DevOps environment. During the build step,
    the executable for each micro service is built and stored within GitLab (e.g.,
    for the Vehicle microservice, it stores the vehicle-ms.jar file). The publish
    step takes the executables and parameters and creates the Docker image and Helm
    chart for each project before provisioning the image and the chart to the Harbor
    registry. For the Vehicle microservice, the parameters define a PostgreSQL dependency
    as well as the username and password for the database. Finally, the deploy step
    installs the Helm chart for each PCM service in the cluster. Backing services
    such as PostgreSQL and MongoDB are also installed in the cluster. Every Project
    Template allows the use of these backing services. 4.1. Why Project Templates
    are Used Coming from a monolithic architecture, microservice-based applications
    are a composition of multiple microser-vices (e.g., five microservices instead
    of one monolithic application). While this architecture positively influences
    flexibility, it also introduces complexity regarding DevOps [8]. However, in a
    monolithic application, a DevOps process has to be applied only to a single application.
    In a microservice architecture, this process is applied to all the microservices,
    thereby creating additional work for the developers. Moreover, within a team of
    developers, some must acquire knowledge on how the DevOps concepts work, how these
    concepts can be applied, and how projects that use these concepts have to be maintained.
    The shared pipeline concept and the project templates help developers to simplify
    the DevOps process. Templates are available for several kind of projects; these
    include Go, Node, Java, Gradle, and Maven. Hence, development teams can use different
    programming languages for their projects. The templates assist developers with
    the specification of the DevOps environment without requiring them to know every
    detail of the underlying pipeline steps. For example, the developer does not have
    to know how the resulting Docker images are published to the registry or how the
    service is deployed to the Kubernetes cluster. However, the development team has
    to know how to configure, extend, and use the templates. To this end, the developer
    should be provided with extensive documentation. 4.2. How to Use Project Templates
    Before a development team can start a new project, it has to decide on a programming
    language to use. Since the shared pipeline concept and project templates are currently
    developed using GitLab, a new Git project needs to be created, and the corresponding
    project template has to be imported. The Git project now contains a basic structure
    with the necessary files for a Java application. These include variables.yaml,.
    gitlab-ci.yaml, Dockerfile, and the Helm charts used to define the GitLab pipeline
    (see Figure 2). Of course it is also possible to integrate the shared pipeline
    concept into an existing project. To do so, the existing project has to use an
    programming language that is supported by the project templates. The developer
    must add the abovementioned pipeline configuration files for the programming language
    to the existing project. For a Java application, the configuration files from
    the Maven template are used. Listing 1: Definition of the GitLab pipeline using
    the shared pipeline configuration Show All By default, the pipeline of a project
    is defined in the gitlab-ci.yaml file, which contains references to the shared
    pipeline. An sample configuration file can be found in Listing 1. Lines 2, 5,
    8, and 11 reference the configuration files defined by the shared pipeline, while
    lines 3, 6, 9, and 12 indicate the version to which the referenced configuration
    files have been set. A project may reference different versions of the shared
    pipeline. If version 1 is referenced, the master branch of the shared-pipeline
    project is used. Line 14 references the local variables. yaml file. To configure
    the project template, the developer has to define the parameters in variables.yaml.
    For the Vehicle microser-vice, the build variables MVN_BUILD_CLI_OPTIONS and MVN_TEST_CLI_OPTIONS
    can be set. These variables are used during the build and test stage, respectively.
    If a developer chooses not to set the variables, the default values are used,
    and the pipeline can be run without any additional configurations. If the developers
    require an optional Kuber-netes Ingress (i.e., enable external access), this has
    to be enabled in the configuration file of the Helm chart. If a developer requires
    additional steps for a single project, the GitLab pipeline can be extended by
    adding the steps to the project pipeline configuration. For example, the Vehicle
    microservice uses an additional step to trigger integration tests with the other
    PCM microservices. If the developer needs a template for a different programming
    language (such as C++) that is not currently available, they have to contact the
    pipeline operator to create a new project template and the corresponding framework-specific
    file defining the build and test stages for the shared pipeline configuration.
    This ensures that quality standards are maintained and that framework specific
    knowledge is centrally held by the pipeline operators. When a developer pushes
    a commit to the repository, the pipeline is triggered, and they are presented
    with a detailed pipeline overview in GitLab, as depicted in Figure 6. The developer
    can see the execution status of the pipeline (i.e., whether it passed or failed)
    and the currently executed stage. A detailed log is available for each step. Hence,
    if there is a failure at any stage, the developer is notified and can review the
    logs to find the error. It is also possible to manually trigger the pipeline or
    re-run an exact copy of a previously executed pipeline so as to locate and fix
    bugs in the pipeline template. Figure 6. GitLab pipeline execution overview Show
    All 4.3. Developer Experiences The shared pipeline concept has been used successfully
    to build a variety of microservices. As a result, experiences with the shared
    pipeline were used to identify the existing problems. Overall, the templates have
    been positively received by the developers. Simple issues such as naming conventions
    in configuration files (e.g., paths or special characters) can lead to failures
    in the execution of the pipeline. Developers reported that the execution is quite
    complex and can be difficult to understand. Moreover, the documentation seems
    to be too distributed. These problems have been fixed, and each project template
    now has comprehensive documentation. Some developers noted that the shared pipeline
    concept can introduce a single point of failure. Since all the projects currently
    reference the same shared pipeline configuration, a bug introduced into the central
    pipeline configuration consequently has an effect on every project. The pipeline
    might fail to run, and important updates might not be deployed as a result. Furthermore,
    a pipeline might run successfully but fail to deploy properly. This in turn can
    cause the development team to be unable to directly access the shared pipeline
    configuration or deploy projects manually to the cluster. While the aspect of
    the shared pipeline as a single point of failure is valid, it is unavoidable when
    designing the concept, and we believe that the overall advantages of the shared
    pipeline (such as central configuration) outweigh the drawbacks. Furthermore,
    developers can reference a version of the shared pipeline that remains unchanged
    over time (unless the latest version is referenced). This also allows them to
    quickly fix bugs for a specific pipeline version. In general, management of access
    to the cluster, pipeline runner, or project can be considered a problem. Moreover,
    the developers complained about the time it takes to fully execute the pipeline.
    Although the execution time depends on the number of pipeline steps and the complexity
    of the project, this factor should be kept in mind when adding additional steps.
    4.4. Current Work: Microservice Developer Portal Developers of microservice-based
    applications may wonder if their microservice has been deployed correctly or which
    microservice are running at a given moment. In a Kubernetes system, these questions
    are not trivial, and cluster access is required to answer them, raising additional
    security concerns. Therefore, we are currently developing a MicroserviceDeveloperPortal
    (MDP) to provide developers with all the information that they need regarding
    their software environment. This portal provides users with a list of all the
    running microservices and details on each of them, including the address, API
    specification, service dependencies, personnel responsible, and health state of
    the service. Manually created documentation tends to become outdated [27] and
    we do not want to create another system for developers to maintain. Instead, the
    project templates are used to create a project structure that allows them to automatically
    extract the relevant information, such as whether dependencies can be derived
    from the values.yaml file. The shared pipeline concept will add a registration
    step after a successful deployment to notify the MDP about the newly deployed
    microservice and deliver the extracted data. This ensures that the documentation
    will always be up to date with the code base from which the service is built.
    SECTION 5. Security Aspects of the DevOps Environment Almost any application can
    be targeted by malicious attacks. In addition to the application, its deployment
    environment should also be protected against this threats [28]. Therefore, an
    advanced security concept should be part of a state-of-the-art DevOps environment.
    Continuous security testing can be integrated into the CI/CD pipeline [29]], and
    security checks such as dependency scanning should not require manual initiation
    on the developer''s machine. Instead, this should be done in a reproducible and
    transparent manner within the CI/CD pipeline. Generally, when automated tests
    (such as unit tests) run in a CI/CD pipeline, their execution does not depend
    on the developer''s local environment. Moreover, a significant amount of communication
    overhead is eliminated, as the pipeline execution status is clearly visible to
    all the team members. These advantages are also relevant when running security
    checks within the CI/CD pipeline. Specifically, the concept of breaking the build
    can be applied to security. Ideally, this concept improves security awareness
    among developers and encourages them to prioritize related issues. However, placing
    such a strong focus on application security leads to a fundamental question: What
    if the pipeline itself is not secure? 5.1. Security-First Pipeline Structure The
    CI/CD pipeline can be considered the final element of a software supply chain
    [23]. Therefore, establishing a high level of trust towards the pipeline is essential.
    As trust in all pipeline components is infeasible in practice, DevOps teams are
    strongly discouraged from using monolithic pipelines in which components are not
    isolated from each other. This is because any untrusted component that is not
    sufficiently isolated may compromise the security of the entire pipeline. Accordingly,
    the pipeline introduced in Chapter 3 provides a high degree of isolation. Each
    step is executed in its own Docker container, and the containers are ephemeral,
    meaning that they are destroyed after completing the respective pipeline step.
    Such an architecture has significant security advantages. In a monolithic pipeline,
    each step operates on the same file system as all the other steps. Sensitive data
    can easily be leaked to an untrusted and subsequently executed pipeline step.
    For example, temporary files might be created and not properly cleared on completion.
    In a decomposed pipeline, this is not a problem because an explicit declaration
    is needed for each file or directory in order to make it available to subsequent
    steps, hence accidental leakage of data to untrusted components is much less likely
    to occur. An example of an explicit artifact declaration that is stored in a specific
    directory is shown in Listing 2. In this example, all the binaries generated by
    the step build are supposed to persist. Therefore, files in the bin directory
    are declared as artifacts, while all other files are automatically deleted when
    the container stops running. To make the configuration even more explicit, the
    directory name can be replaced with a specific file name. However, this would
    require additional effort when the build output is changed. Listing 2: Explicit
    declaration for file persistence Show All In addition to the decomposed pipeline
    architecture, the template-based definitions explained in Chapter 3 also improve
    security. Even in an environment with many different projects, some security improvements
    can be implemented in the template files that are used by all the projects. For
    example, a dependency scan step can be introduced to all the existing pipelines
    by adding a new pipeline step definition to an existing template file. This centralized
    approach greatly reduces the amount of administrative effort required. 5.2. Pipeline
    Security Issues in Practice Although a decomposed pipeline structure provides
    strong security advantages, it falls far short of solving all pipeline-related
    security issues. Figure 7 presents an overview of security issues in the pipeline.
    The main issues that have been identified are (1) insufficient handling of credentials,
    (2) unrestricted outbound network access, and (3) insufficient isolation from
    production applications. Figure 7. Pipeline and related security issues Show All
    5.2.1. Credential Management Most pipeline steps need access to some kind of external
    service. For instance, the step publish has to access a Harbor server in order
    to upload new Docker images. Harbor [30] is a Docker image repository that provides
    some additional features, such as a graphical user interface and support for vulnerability
    scanning. A repository access token is needed to access its API. In the pipeline,
    credentials are provided through GitLab environment variables, which are made
    available to the pipeline containers. However, under the default settings, all
    such variables are available to all the pipeline steps. This can cause highly
    sensitive credentials to be visible to entirely unrelated steps; for instance,
    a simple source code formatting check may have access to the credentials used
    for deployment. This strongly violates the principle of least privilege [31].
    Fortunately, GitLab provides environment labels [32] that can be assigned to the
    pipeline steps. Environment variables may then be restricted to a specific label.
    While this approach may be somewhat clumsy to implement due to the potentially
    tedious label assignment process, it does enable step-specific credentials. 5.2.2.
    Unrestricted Network Access Some pipeline com- ponents require access to the public
    Internet. For example, downloading third-party dependencies is an important use
    case for this requirement. However, other components may not need to access any
    external service, apart from GitLab for the reporting of results. Again, the example
    of a source code formatting check may be used to illustrate this point. By default,
    GitLab''s Kubernetes pipeline executor does not impose any outbound networking
    restrictions on pipeline containers. If no default blocking policy is defined
    in the Kubernetes cluster, the pipeline containers can communicate freely with
    the outside world. Moreover, network isolation is not guaranteed between pipeline
    containers and production applications running on the same cluster. The potential
    effects of these issues include unwanted interference with production applications
    and leakage of data to external agents. Moreover, a sophisticated attacker may
    even attempt to access the Kubernetes cluster directly using a reverse shell if
    they manage to inject code into a pipeline component. Therefore, blocking all
    unnecessary outbound network access may be a requirement in highly sensitive environments.
    To the best of our knowledge, step-specific networking restrictions are currently
    not possible in a standard GitLab-or Kubernetes-based pipeline. This is because
    Kubernetes network policies cannot differentiate between the containers responsible
    for different pipeline steps. In a true least-privilege approach, this limitation
    needs to be overcome. This can be done by slightly modifying the GitLab pipeline
    implementation to tag each container with the name of the step that it executes.
    Kubernetes container tags are essentially plain key-value pairs assigned to these
    containers. The crucial point is that network policies can be applied to specific
    tags, and thus the modified pipeline runner enables the implementation of least-privilege
    network access for pipeline containers. An important note regarding the above
    approach is that the internal DNS service of GitLab and Kubernetes have to be
    accessible by all the pipeline containers. Otherwise, the pipeline cannot be executed
    successfully. Another limitation is that Kubernetes network policies are IP-based,
    and there is no direct method to control access to the services available via
    dynamic IP addresses. In this case, the responsible DevOps team may either use
    a proxy server or grant access to whole IP ranges. Nonetheless, the above solution
    provides a simple and effective way for DevOps engineers to set up fine-grained
    network access control in many cases. Since the default implementation makes a
    granular level of control impossible, this solution is already a significant improvement.
    5.3. Pipeline Dependency Scanning Once a robust and well-structured CI/CD pipeline
    has been established, it can be used as a basis for further security measures.
    This section focuses on a highly relevant use case: dependency scanning. The term
    refers to the automated auditing of the third-party dependencies of an application.
    The motivation for doing so is quite clear: modern dependency ecosystems such
    as Node Package Manager (NPM) contain hundreds of thousands of packages, creating
    a huge attack surface. In 2019, the average NPM package had about 87 cumulative
    dependencies [33]. Here, the cumulative means that the transitive closure of dependencies,
    including indirect dependencies, are considered. Critically, vulnerabilities in
    both direct and indirect dependencies can severely impact application security.
    Due to the substantial scale of package ecosystems and the often large number
    of cumulative dependencies, it is difficult to manually audit all the dependencies.
    Ideally, third-party dependencies should only be added to an application if absolutely
    necessary. Enforcing this as a rule can prevent related security issues from occurring
    in the first place. However, existing projects may be deeply invested in using
    many different dependencies that cannot be easily removed. In such cases, dependency
    scanning can help engineers to keep a security overview. Various automated solutions
    such as Snyk [34] or Trivy [35] can be used for this purpose. These solutions
    all search vulnerability databases for matches between application artifacts and
    security advisories. The inner workings of dependency scanners are beyond the
    scope of this article, but it is important to note that they specifically target
    known vulnerabilities. This paper does not intend to recommend any specific dependency
    scanner; rather, it aims to demonstrate how a scanner can be integrated into the
    pipeline. For practical reasons, dependency scanning should not be implemented
    separately for each project; the integration should be available to all projects
    by default. This is exactly where pipeline templates reveal their full potential.
    Specifically, an additional step vulnerability scan can be defined in the shared
    pipeline. By default, this step is executed in all the project-specific pipelines.
    Alternatively, an pipeline step opt-in can be subsequently defined. In that case,
    an environment variable must be set for each project in which the step is to be
    included. The opt-in approach may be useful to organizations in which a large
    number of existing projects have known security issues that cannot be fixed immediately.
    The vulnerability scan step uses the Harbor dependency scanning functionality,
    which scans entire Docker images rather than plain source codes. This also implies
    that the scans are performed outside the pipeline. The pipeline itself is only
    responsible for initiating a scan, waiting for the scan results, and either continuing
    or aborting execution once the results are available. An overview of the architecture
    is given in Figure 8. Figure 8. Pipeline vulnerability scanning Show All As Figure
    8 illustrates, the vulnerability scan step is based on a Python script that interacts
    with the Harbor HTTP API. When it is run, the script initiates a scan of the previously
    pushed image in Harbor. Subsequently, it busy-waits for a result by repeatedly
    polling the Harbor API until the result is available. While this solution may
    not be the most technically elegant, a more advanced approach based on webhooks
    would require more complex and error-prone configurations in Harbor and GitLab.
    The API call used for polling requires very little resources, as it only has to
    check if a result is available. Therefore, scalability is unlikely to become an
    issue in this approach. Finally, once the result is available, the script creates
    a vulnerability report and finishes execution with a successful or unsuccessful
    exit code. The report is presented as a human-readable JSON file, which can be
    used by developers to fix any detected vulnerabilities. One practical issue is
    that the Harbor API is stateless, i.e., the Harbor server does not have any information
    about the execution status of the pipeline. Therefore, the developer needs to
    be ensured that a pipeline execution will always receive the result of the same
    scan that it initiated. Otherwise, it might receive outdated scan results belonging
    to a previous execution of the pipeline. It is possible that while the content
    of a source code repository may have remained the same, the scanner used by Harbor
    or its vulnerability database may have changed. Therefore, merely tagging Docker
    images with commit hashes is not enough to uniquely identify the scan results.
    Instead, the step vulnerability scan relies on the GitLab CI_PIPELINE_IID environment
    variable, which is a project-level unique pipeline identifier. The value of this
    variable increases incrementally with each new pipeline execution. The Docker
    images are then tagged with this value, allowing the scan results to be uniquely
    traced back to a single execution of the pipeline. Although the integration of
    the Harbor vulnerability scan into the CI/CD pipeline requires some effort, its
    advantages outweigh the implementation issues. The most important advantage is
    that Harbor ensures interoperability between pipeline and scanner implementation.
    As a result, a concrete scanner implementation can be replaced without changing
    the pipeline. Developers may benefit even further from having Harbor as a middleware
    between the pipeline and the scanner. One crucial reason is that Harbor provides
    the option to ignore specific vulnerabilities identified through a Common Vulnerabilities
    and Exposures (CVE) identifier, which can be applied on a per-project basis. The
    option to ignore a vulnerability may appear counterproductive at first glance,
    but it can in fact hold pragmatic value for developers. As dependency scanners
    base their analysis solely on metadata-containing artifacts such as package lock
    files, the relevance of a vulnerability to a specific codebase usually cannot
    be determined automatically. As an example, a specific package may only be vulnerable
    if used in combination with untrusted (e.g., user- generated) input. Conversely,
    the input data of said package may always be considered safe in the scope of a
    particular project. In this case, the developers may choose not to immediately
    upgrade the package, especially when confronted with compatibility or time constraints.
    Obviously, ignoring a vulnerability issue may lead to a slippery slope and should
    not be done without thorough inspection. Clear guidelines for such cases should
    be defined with input from the developers. It is worth noting that the dependency
    scanning implementation proposed above is a rather invasive security measure for
    developers. Acceptance among developers is essential, and security absolutism
    may be an obstacle to such approval. The ultimate goal is to help developers deliver
    more secure software while minimizing any impedance to their workflow. Finally,
    the architectural solution proposed in this paper may not be applicable to all
    environments. One caveat is that Harbor does not currently support the use of
    more than one scanner implementation at a time. This may constitute a significant
    limitation for some teams, due to platform requirements that cannot be fulfilled
    by a single scanner. In this scenario, the developers may decide to implement
    Harbor vulnerability scanning only for a subset of the projects or to integrate
    a scanner without using Harbor. Nonetheless, the Harbor-based approach can be
    useful for many teams, and it remains a valuable security achievement for both
    developers and DevOps engineers. SECTION 6. Conclusion and Further Work The DevOps
    concept not only supports the integration of operational features but can also
    contributes to the development of sustainable software products. It allows users
    to automate and generalize aspects of the development process and operational
    concept. In terms of implementation, a thorough understanding of both areas is
    necessary to find a suitable measure between generalization and customization.
    Failing to do so can have negative effects on the acceptance of the approach by
    developers and operations. 6.1. Conclusion Through the DevOps workflow introduced
    above, we provided an environment that supports the deployment process of microservices
    and enforces security patterns in their implementation. One requirement of the
    approach is the delivery of the micro service application in a container orchestration
    system such as Kubernetes. Since most developers are not familiar with Docker
    or Kubernetes, a target was to make the introduction of these technologies and
    the integration of CI/CD concepts as simple as possible. This aim was achieved
    through the use of the project templates, freeing developers from the responsibility
    of creating the Docker or Kubernetes relevant artifacts and delivering them to
    the orchestration system. Publishing, delivery, as well as the base of the delivered
    artifacts were managed through the central pipeline templates. The template-based
    approach drastically reduced the maintenance times as well as the time to market
    for new micro service developments. To protect the pipeline and minimize attack
    vectors, the security-first pipeline structure was used. We effectively mitigated
    the most common security issues for all CI/CD pipelines. Additionally, the integrated
    vulnerability checks created much higher awareness of security issues during development
    and forced the developers to fix critical vulnerabilities as soon as possible.
    As a result, the number of attack vectors targeting the pipeline and the microservices
    could be significantly reduced, making them more secure. 6.2. Further Work Aside
    from the positive aspects of the approach presented, developers missed the insight
    into their own running microservices and those of others. By not allowing them
    to access the service environment, attack vectors were drastically reduced, but
    at the cost of usability on the developer side. The permission restrictions meant
    that the services and interfaces of these microservices were invisible and had
    to be communicated between the individual teams. Since communication is essential
    for the reuse of microservices, we want to address this issue in further research.
    To overcome the problem, a DevOps-oriented mMicroservice Develop Portal (MDP)
    should be created to further support the work between the developer and operational
    teams. The MDP can showcase the mutual benefits of a DevOps-based approach and
    well-defined engineering processes. Authors Figures References Citations Keywords
    Metrics More Like This Facing Service-Oriented System Engineering challenges:
    An organizational perspective 2010 IEEE International Conference on Service-Oriented
    Computing and Applications (SOCA) Published: 2010 Perspectives on Service-Oriented
    Computing and Service-Oriented System Engineering 2006 Second IEEE International
    Symposium on Service-Oriented System Engineering (SOSE''06) Published: 2006 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: An Advanced DevOps Environment for Microservice-based Applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/2723872.2723882
  analysis: '>'
  authors:
  - Carl Boettiger
  citation_count: 747
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Newsletter Home Latest Issue
    Archive Authors Affiliations Award Winners HomeSIGsSIGOPSACM SIGOPS Operating
    Systems ReviewVol. 49, No. 1An introduction to Docker for reproducible research
    RESEARCH-ARTICLE SHARE ON An introduction to Docker for reproducible research
    Author: Carl Boettiger Authors Info & Claims ACM SIGOPS Operating Systems ReviewVolume
    49Issue 1pp 71–79https://doi.org/10.1145/2723872.2723882 Published:20 January
    2015Publication History 573 citation 6,174 Downloads eReaderPDF ACM SIGOPS Operating
    Systems Review Volume 49, Issue 1 Previous Next Abstract References Cited By Recommendations
    Comments Skip Abstract Section Abstract As computational work becomes more and
    more integral to many aspects of scientific research, computational reproducibility
    has become an issue of increasing importance to computer systems researchers and
    domain scientists alike. Though computational reproducibility seems more straight
    forward than replicating physical experiments, the complex and rapidly changing
    nature of computer environments makes being able to reproduce and extend such
    work a serious challenge. In this paper, I explore common reasons that code developed
    for one research project cannot be successfully executed or extended by subsequent
    researchers. I review current approaches to these issues, including virtual machines
    and workflow systems, and their limitations. I then examine how the popular emerging
    technology Docker combines several areas from systems research - such as operating
    system virtualization, cross-platform portability, modular re-usable elements,
    versioning, and a ''DevOps'' philosophy, to address these challenges. I illustrate
    this with several examples of Docker use with a focus on the R statistical environment.
    References Altintas, I. et al. 2004. Kepler: an extensible system for design and
    execution of scientific workflows. Proceedings.16th international conference on
    scientific and statistical database management, 2004. (2004). Barnes, N. 2010.
    Publish your computer code: it is good enough. Nature. 467, 7317 (Oct. 2010),
    753--753. Clark, D. et al. 2014. BCE: Berkeley''s Common Scientific Compute Environment
    for Research and Education. Proceedings of the 13th Python in Science Conference
    (SciPy 2014). (2014). Show All References Recommendations The Practice of Reproducible
    Research: Case Studies and Lessons from the Data-Intensive Sciences Read More
    Reproducible Research for Scientific Computing: Tools and Strategies for Changing
    the Culture This article considers the obstacles involved in creating reproducible
    computational research as well as some efforts and approaches to overcome them.
    Read More Learning Docker: Build, ship, and scale faster, 2nd Edition Read More
    Comments Login options Check if you have access through your login credentials
    or your institution to get full access on this article. Sign in Full Access Get
    this Article Information Contributors Published in ACM SIGOPS Operating Systems
    Review   Volume 49, Issue 1 Special Issue on Repeatability and Sharing of Experimental
    Artifacts January 2015155 pages ISSN: 0163-5980 DOI: 10.1145/2723872 Editors:
    Jeanna Neefe Matthews , Thomas Bressoud Issue’s Table of Contents Copyright ©
    2015 Author Publisher Association for Computing Machinery New York, NY, United
    States Publication History Published: 20 January 2015 Check for updates Qualifiers
    Research-Article Bibliometrics Citations573 Article Metrics 573 Total Citations
    View Citations 6,174 Total Downloads Downloads (Last 12 months) 513 Downloads
    (Last 6 weeks) 72 Other Metrics View Author Metrics PDF Format View or Download
    as a PDF file. PDF eReader View online with eReader. eReader Altintas, I. et al.
    2004. Kepler: an extensible system for design and execution of scientific workflows.
    Proceedings.16th international conference on scientific and statistical database
    management, 2004. (2004). Barnes, N. 2010. Publish your computer code: it is good
    enough. Nature. 467, 7317 (Oct. 2010), 753--753. Clark, D. et al. 2014. BCE: Berkeley''s
    Common Scientific Compute Environment for Research and Education. Proceedings
    of the 13th Python in Science Conference (SciPy 2014). (2014). Collberg, C. et
    al. 2014. Measuring Reproducibility in Computer Systems Research. Dudley, J.T.
    and Butte, A.J. 2010. In silico research in the era of cloud computing. Nat Biotechnol.
    28, 11 (Nov. 2010), 1181--1185. Eide, E. 2010. Toward Replayable Research in Networking
    and Systems. Archive ''10, the nSF workshop on archiving experiments to raise
    scientific standards (2010). FitzJohn, R. et al. 2014. Reproducible research is
    still a challenge. http://ropensci.org/blog/2014/06/09/reproducibility/. Garijo,
    D. et al. 2013. Quantifying reproducibility in computational biology: The case
    of the tuberculosis drugome. {PLoS} {ONE}. 8, 11 (Nov. 2013), e80278. Gil, Y.
    et al. 2007. Examining the challenges of scientific workflows. Computer. 40, 12
    (2007), 24--32. Gilbert, K.J. et al. 2012. Recommendations for utilizing and reporting
    population genetic analyses: the reproducibilityof genetic clustering using the
    program structure. Mol Ecol. 21, 20 (Sep. 2012), 4925--4930. Harji, A.S. et al.
    2013. Our Troubles with Linux Kernel Upgrades and Why You Should Care. ACM SIGOPS
    Operating Systems Review. 47, 2 (2013), 66--72. Howe, B. 2012. Virtual appliances,
    cloud computing, and reproducible research. Computing in Science & Engineering.
    14, 4 (Jul. 2012), 36--41. Hull, D. et al. 2006. Taverna: a tool for building
    and running workflows of services. Nucleic Acids Research. 34, Web Server (Jul.
    2006), W729--W732. Ince, D.C. et al. 2012. The case for open computer programs.
    Nature. 482, 7386 (Feb. 2012), 485--488. Joppa, L.N. et al. 2013. Troubling Trends
    in Scientific Software Use. Science (New York, N.Y.). 340, 6134 (May 2013), 814--815.
    Lapp, Hilmar 2014. Reproducibility / repeatability big- Think (with tweets) @hlapp.
    Storify. http://storify.com/hlapp/reproducibility-repeatability-bigthink. Leisch,
    F. 2002. Sweave: Dynamic Generation of Statistical Reports Using Literate Data
    Analysis. Compstat. W. Härdle and B. Rönz, eds. Physica-Verlag HD. Merali, Z.
    2010. Computational science: ...Error. Nature. 467, 7317 (Oct. 2010), 775--777.
    Nature Editors 2012. Must try harder. Nature. 483, 7391 (Mar. 2012), 509--509.
    Ooms, J. 2013. Possible directions for improving dependency versioning in r. arXiv.org.
    http://arxiv.org/abs/1303. 2140v2. Ooms, J. 2014. The openCPU system: Towards
    a universal interface for scientific computing through separation of concerns.
    arXiv.org. http://arxiv.org/abs/1406.4806. Peng, R.D. 2011. Reproducible research
    in computational science. Science. 334, 6060 (Dec. 2011), 1226--1227. Stodden,
    V. 2010. The scientific method in practice: Reproducibility in the computational
    sciences. SSRN Journal. (2010). Stodden, V. et al. 2013. Setting the Default to
    Reproducible. (2013), 1--19. The Economist 2013. How science goes wrong. The Economist.
    http://www.economist.com/news/leaders/21588069-scientific-research-has-changed-world-now-itneeds-change-itself-how-science-goes-wrong.
    Xie, Y. 2013. Dynamic documents with R and knitr. Chapman; Hall/CRC. 2014. Examining
    reproducibility in computer science. http://cs.brown.edu/~sk/Memos/Examining-
    Reproducibility/. 2012. Mick Watson on Twitter: @ewanbirney @pathogenomenick @ctitusbrown
    you can''t install an image for every pipeline you want... https://twitter.com/BioMickWatson/status/265037994526928896.
    Figures Other None Share this Publication link https://dl.acm.org/doi/10.1145/2723872.2723882
    Copy Link Share on Social Media Share on Twitter LinkedIn Reddit Facebook Email
    28 References View Issue’s Table of Contents Footer Categories Journals Magazines
    Books Proceedings SIGs Conferences Collections People About About ACM Digital
    Library ACM Digital Library Board Subscription Information Author Guidelines Using
    ACM Digital Library All Holdings within the ACM Digital Library ACM Computing
    Classification System Digital Library Accessibility Join Join ACM Join SIGs Subscribe
    to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin
    Feedback Bug Report The ACM Digital Library is published by the Association for
    Computing Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy
    Code of Ethics'
  inline_citation: '>'
  journal: Operating systems review
  limitations: '>'
  pdf_link: null
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: An introduction to Docker for reproducible research
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/s22124637
  analysis: '>'
  authors:
  - Ionut-Catalin Donca
  - Ovidiu Stan
  - Marius Misaros
  - Dan-Ioan Gota
  - Liviu Miclea
  citation_count: 9
  full_citation: '>'
  full_text: '>

    Web Store Add shortcut Name URL Customize Chrome'
  inline_citation: '>'
  journal: Sensors (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/1424-8220/22/12/4637/pdf?version=1655712470
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Method for Continuous Integration and Deployment Using a Pipeline Generator
    for Agile Software Projects
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ms.2016.64
  analysis: '>'
  authors:
  - Armin Balalaie
  - Abbas Heydarnoori
  - Pooyan Jamshidi
  citation_count: 514
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Journals & Magazines >IEEE Software >Volume: 33 Issue: 3 Microservices
    Architecture Enables DevOps: Migration to a Cloud-Native Architecture Publisher:
    IEEE Cite This PDF Armin Balalaie; Abbas Heydarnoori; Pooyan Jamshidi All Authors
    456 Cites in Papers 1 Cites in Patent 20706 Full Text Views Abstract Document
    Sections Devops and Microservices Architectural Concerns for Microservices Migration
    The Migration Lessons Learned Microservices Migration Patterns Authors Figures
    References Citations Keywords Metrics Abstract: This article reports on experiences
    and lessons learned during incremental migration and architectural refactoring
    of a commercial mobile back end as a service to microservices architecture. It
    explains how the researchers adopted DevOps and how this facilitated a smooth
    migration. Published in: IEEE Software ( Volume: 33, Issue: 3, May-June 2016)
    Page(s): 42 - 52 Date of Publication: 18 March 2016 ISSN Information: DOI: 10.1109/MS.2016.64
    Publisher: IEEE Devops and Microservices DevOps is a set of practices that aim
    to decrease the time between changing a system and transferring that change to
    the production environment. However, they also insist on maintaining software
    quality in terms of both code and the delivery mechanism. Any technique that enables
    these goals is considered a DevOps practice.1, 2 Sign in to Continue Reading Authors
    Figures References Citations Keywords Metrics More Like This Virtual Machine Scalability
    on Multi-Core Processors Based Servers for Cloud Computing Workloads 2009 IEEE
    International Conference on Networking, Architecture, and Storage Published: 2009
    Exploring Mobility and Scalability of Cloud Computing Servers Using Logical Regression
    Framework 2024 2nd International Conference on Disruptive Technologies (ICDT)
    Published: 2024 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE software
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: 'Microservices Architecture Enables DevOps: Migration to a Cloud-Native Architecture'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/978-3-030-29157-0_11
  analysis: '>'
  authors:
  - Clauirton Siebra
  - Rosberg Lacerda
  - Italo Cerqueira
  - Jonysberg P. Quintino
  - Fabiana Florentin
  - Fábio Q. B. da Silva
  - André L. M. Santos
  citation_count: 2
  full_citation: '>'
  full_text: '>

    Your privacy, your choice We use essential cookies to make sure the site can function.
    We also use optional cookies for advertising, personalisation of content, usage
    analysis, and social media. By accepting optional cookies, you consent to the
    processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Software Technologies Conference
    paper Empowering Continuous Delivery in Software Development: The DevOps Strategy
    Conference paper First Online: 13 August 2019 pp 247–265 Cite this conference
    paper Access provided by University of Nebraska-Lincoln Download book PDF Download
    book EPUB Software Technologies (ICSOFT 2018) Clauirton Siebra, Rosberg Lacerda,
    Italo Cerqueira, Jonysberg P. Quintino, Fabiana Florentin, Fabio B. Q. da Silva
    & Andre L. M. Santos  Part of the book series: Communications in Computer and
    Information Science ((CCIS,volume 1077)) Included in the following conference
    series: International Conference on Software Technologies 513 Accesses 2 Citations
    Abstract Continuous Delivery refers to a software development practice where members
    of a team frequently integrate their work, so that the process of delivery can
    be easily conducted. However, this continuous integration and delivery requires
    a reliable collaboration between development and IT operation teams. The DevOps
    practices support this collaboration since they enable that the operation staff
    making use of the same infrastructure as developers for their systems work. Our
    study aims at presenting a practical DevOps implementation and analyzing how the
    process of software delivery and infrastructure changes was automated. Our approach
    follows the principles of infrastructure as code, where a configuration platform
    – PowerShell DSC – was used to automatically define reliable environments for
    continuous software delivery. In this context, we defined the concept of “stage
    for dev”, also using the Docker technology, which involves all the elements that
    enable members of a team to have the same production environment, locally configured
    in their personal machines and thus empowering the continuous integration and
    delivery of system releases. Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others DevOpSlang
    – Bridging the Gap between Development and Operations Chapter © 2014 Enabling
    DevOps Collaboration and Continuous Delivery Using Diverse Application Environments
    Chapter © 2015 DevOps: Foundations and Its Utilization in Data Center Chapter
    © 2017 Keywords Continuous delivery DevOps Software deployment 1 Introduction
    The lifecycle of an application involves teams that usually work in distinct areas
    and have incompatible goals. For example, while the development team wants agility;
    the operation team is more focused on stability issues. In such domains, applications
    are manually handed over between these teams with minimal communication. Such
    separation between entities, which are in fact dependent, translates into an increased
    time to market and negatively impacts the software quality, decreasing the actual
    value of the product [1]. The fundamental conflict in the software process environment
    is between developers, which have to produce changes at a rapid pace; and IT Operators,
    which have to maintain infrastructure configuration and availability along these
    changes. The term DevOps, which is a blend of the Developers and Operations words,
    is a concept that assists to facilitate these changes [2]. It builds a living
    bridge between development and operations and gives them an opportunity to work
    and collaborate effectively and seamlessly. According to Loukides [3], DevOps
    is a culture, movement or practice that emphasizes the collaboration and communication
    of both software developers and other information technology (IT) professionals
    while automating the process of software delivery and infrastructure changes.
    It aims at establishing a culture and environment where building, testing, and
    releasing software, can happen rapidly, frequently, and more reliably. Previous
    works on DevOps [4, 5] are mainly focused on propose conceptual frameworks, which
    intend to create a consensus to the own DevOps definition and their features.
    Some elements such as the culture of collaboration, automation and monitoring;
    emerged from these works and seem to be the basis for the implementation of DevOps
    environments. However, while DevOps is becoming very popular between software
    practitioners; there is still a lack in discussions on frameworks that support
    its implementation and reports of real experiences that could assist development
    teams in adopting the DevOps principles [6, 7]. The first focus of this work was
    on the automation dimension, where the definition of practices related to infrastructure
    as code creates the basis for an automated process of continuous integration and
    delivery [8]. Handling infrastructure as code, the following benefits can be obtained
    [9]: A code can be thoroughly tested to reproduce infrastructure consistently
    at scale; Developers could be provided with a simulated production environment,
    which increases testability and reliability; Infrastructure code can be versioned;
    Infrastructure can be provisioned and configured on demand; Proactive recovering
    from failures can be carried out by continuous monitoring of the environment for
    violations, which can trigger automatic execution of scripts for rollback or recovery.
    Our approach followed the principles of infrastructure as code, where a configuration
    platform, PowerShell DSC (Desired State Configuration) was used to automatically
    define reliable environments for continuous software delivery. This initial environment
    was designed to work on virtual machines. Differently, our current study shows
    this approach running together with Docker [10]. Docker is a tool designed to
    easily create, deploy, and run applications by using containers, which allow developers
    to package up an application with all of the parts it needs, such as libraries
    and other dependencies, and ship it all out as one package. In this way, our main
    aim was to define an environment model, which we call “stage for dev”, which involves
    all the elements that enable members of a team to have the same production environment,
    locally configured in their personal machines and thus empowering the continuous
    integration and delivery of system releases. Furthermore, we also present the
    current discussions regarding the integration of Docker with PowerShell DSC, which
    is being conducted in the Windows platform. The remainder of this paper is structured
    as follows: Sect. 2 summarizes the studies on the automation dimension of DevOps,
    where the focus is on the infrastructure as code aspects and their implementations.
    Section 3 presents how DevOps concepts were implemented in our organization and
    the lessons learned from this experience. Section 4 extends this idea, considering
    the use of the Docker technology. Section 5 summarizes the current discussions
    about infrastructure as code, which focus on the integration of Docker and PowerShell
    DSC. Finally, Sect. 6 concludes this work, stressing the challenges of DevOps
    implementation and future works that we intend to carry out. 2 Structure as Code
    The creation of a DevOps environment is based on principles such as culture of
    collaboration [11,12,13,14,15], measurement of development efforts [16,17,18]
    and monitoring of system health [11, 17, 18]. However, according to Ebert et al.
    [19], the most important shift over the adoption of DevOps is to treat infrastructure
    as code, since infrastructure can be shared, tested, and version controlled. Furthermore,
    development and production could share a homogenous infrastructure, reducing problems
    and bugs due to different infrastructure configurations. This section discusses
    the main ideas of this approach and resources that support it. 2.1 Basic Concepts
    Infrastructure as Code (IaC) is a DevOps principle used to address problems regarding
    the manual process of configuration management by means of automatic provision
    and configuration of infrastructural resources. In this way, the IaC concept is
    used to describe the idea that almost all actions performed to the infrastructure
    can be automated. Like any code, developers could create automation logic for
    different tasks such as to deploy, configure and upgrade computational systems
    and infrastructures. Patterns to use the infrastructure as code were proposed
    in [20] and they can be summarized as: Automate Provisioning: automate the process
    of configuring environments to include networks, external services, and infrastructure;
    Behavior-Driven Monitoring: automate tests to verify the behavior of the infrastructure;
    Immune System: deploy software one instance at a time while conducting behavior-driven
    monitoring. If an error is detected during the incremental deployment, a Rollback
    Release must be initiated to revert changes; Lockdown Environments: lock down
    shared environments from unauthorized external and internal usage, including operations
    staff. All changes must be versioned and applied through automation; Production-Like
    Environments: development and production environments must be as similar as possible.
    These patterns show that DevOps pushes automation from the development to the
    infrastructure. Compared with manual infrastructure provisioning, for example,
    con-figuration management tools can reduce production provisioning and configuration
    maintenance complexity while enabling recreation of the production system on the
    development machines. As discussed in [19], tools are a major DevOps enabler and
    they are mandatory in automating these and other patterns and tasks. In fact,
    DevOps considers deliveries with short cycle time. This feature comes from one
    of the Lean/Agile principles, which stands for “Build incrementally with fast
    integrated learning cycles”. Thus, such a strategy requires a high degree of automation,
    so that it is fundamental the appropriate choice of tools. See a list of tools
    in [19]. 2.2 Tools for Configuration Management Configuration management tools
    are the main resources to implement IaC strategies. Such tools aim at replacing
    error-prone shell scripts, which are employed to manage the state of machines
    or environments where development codes are going to execute. Shell scripts are
    potentially complex to maintain and evolve, since they are neither modular nor
    reusable. Thus, the aim of approaches for configuration management was to provide
    languages to specify configuration properties without the limitations (low modularity
    and reusability) of shell scripts. Three examples of these languages, which follow
    different implementation strategies, are: Puppet: domain specific language implemented
    in a common programming language (originally Ruby, but with newer versions in
    C++ and Clojure); Chef: uses an existing language (Ruby) for writing system configuration
    “recipes”; CFEngine: domain specific language also implemented in a common programming
    language (C). These languages are often declarative. This means, they describe
    the desired state of the system rather than a way to achieve it. There are other
    languages such as Nix, which is a purely functional programming language with
    specific properties for configuration; and IBM Tivoli System Automation for Multiplatforms.
    These languages have similar features but may present particular purposes. The
    IBM approach, for example, facilitates the automatic switching of users, applications
    and data from one database system to another in a cluster. Puppet, Chef and CFEngine
    are the most popular configuration management alternatives. Therefore, it is important
    to understand some slight differences among them [21]. Chef and Puppet are very
    similar since they are based on Ruby. However, Chef seems to present fewer security
    vulnerabilities than Puppet. Both languages are more “Ops-friendly” due to its
    model-driven approach. They also present a relatively small learning curve. Differently,
    CFEngine is more “Dev-friendly” and its learning curve is steep. However, as an
    advantage, CFEngine has a dramatically smaller memory footprint, runs faster and
    has far fewer dependencies since it was developed with C. For configuration information,
    CFEngine uses its own declarative language to create “promises,” or policy statements.
    Puppet, on the other hand, uses a Ruby Domain-Specific Language (DSL) to create
    its manifests. So those with some Ruby experience may find themselves in more
    familiar territory with Puppet. A comparison among these and several other open-source
    configuration management approaches can be seen in [22]. 2.3 Frameworks As applications
    need to be developed and tested in production like environments, some organizations
    are using strategies such as virtualization and more recently containerization
    [23] to make such environments portable. However, these approaches are also hard
    to use when they are manually maintained. This scenario motivated the creation
    of frameworks for setup of more complex development environments. Two popular
    examples of frameworks are Vagrant and Docker. Vagrant [24] is a management and
    support framework to virtualization of development environments. Instead of running
    all projects locally on a unique computer, having to rearrange the different requirements
    and dependencies of each project, this framework allows running each project in
    its own dedicated virtual environment. Docker [10] is a container-based approach
    that provides virtualization at the operating system level and uses the host kernel
    to run multiple virtual environments. A difference between these approaches is
    associated with their performances. As discussed in the previous paragraph, Docker
    relies on containerization, while Vagrant utilizes virtualization. In this latter
    approach, each virtual machine runs its own entire operating system inside a simulated
    hardware environment provided by special programs. Thus, each virtual machine
    needs a dedicated amount of static resources (CPU, RAM, storage), generating an
    overhead of such resources. Approaches based on containerization present a higher
    performance since containers simply use whatever resources it needs. This means,
    there is no overhead of resources. Based on this discussion, Docker is lighter
    than Vagrant. A deeper study in such approaches shows that both have advantages
    and disadvantages, so that the final decision must be based on the particular
    features of each project. There is another important difference between these
    approaches. Vagrant cannot create virtual machines or containers without virtualization
    platforms [21] such as VirtualBox, VMware or Docker. Differently, Docker can work
    without Vagrant. In order, the main advantage of vagrant is that it provides an
    easy mechanism to reproduce environments. These frameworks can also be used together
    with configuration management tools/languages to implement more powerful IaC environments.
    Some examples are given in the next section. 2.4 Tools in Practice The previous
    section showed that there are several options regarding frameworks and configuration
    management tools to support the implementation of the infrastructure as code principles.
    However, the literature presents few contributions regarding their practical use
    and the focus of this literature is on the specification of extensions that could
    improve the limitations of current tools rather than descriptions of real case
    studies. The work of Hüttermann [25], for example, integrates Vagrant and Puppet
    and uses them to create a topology for IaC consisting of Vagrant and Puppet artefacts
    that are continuously built and stored in a version control system. While Vagrant
    allows the building of lightweight and portable virtual environments, based on
    a simple textual description; Puppet uses a declarative syntax to describe the
    desired state of a target environment and allows this description to be executed
    to create that state on a target machine. Hummer and colleagues [26] propose and
    evaluate a model-based testing framework for IaC, where an abstracted system model
    is used to derive state transition graphs. The resulting graph is then used to
    derive test cases. Their prototype extends the Chef IaC tool. However the authors
    comment that their approach is general and could be applied to other tools, such
    as Puppet. The work of Artac et al. [27] discusses several technologies involved
    in supporting IaC. Its main focus is on the OASIS TOSCA, which is an industrial
    practice language for automated deployment of technology independent and multi-cloud
    compliant applications. In order, the majority of examples regarding IaC are focused
    on Cloud environment and they are related to specific features of such domain.
    For example, Zhu et al. [28] report results from experiments on reliability issues
    of cloud infrastructure and trade-offs between using heavily-baked and lightly-baked
    images. Their experiments were based on Amazon Web Service (AWS) OpsWorks APIs
    (Application Programming Interfaces) and they also used the Chef configuration
    management tool. Several other works regarding IaC in the Cloud domain are discussed
    in the literature, such as in [18, 29]. The work of Spinellis [30] is another
    example of a study that discusses popular tools in the DevOps domain, which include
    CFEngine, Puppet and Chef. This work stresses the main function of such tools,
    which is to automate a system’s configuration so that users write rules expressing
    how an IT system is to be configured and the tool will set up the system accordingly.
    Wettinger et al. [13] also show that the DevOps community focuses on providing
    pragmatic solutions for the automation of application deployment. Then, the communities
    affiliated with some of the DevOps tools, such as Chef or Puppet, to provide artefacts
    to build deployment plans for certain application tasks. Thus, these two previous
    works [13, 30] confirm the trend to some specific tools (Chef and Puppet) and
    their relation to aspects of automation. Unfortunately, the scientific literature
    does not discuss the use and evaluation of such tools in a DevOps context, including
    the Docker technology, considering real development cases. This is the major contribution
    of our work, as detailed in the next sections. 3 DevOps Implementation: Case Study
    1 This section is divided into four parts. We first describe the object of this
    case study, which is a real application that we call Xsolution (pseudo name due
    to commercial issues). Next we describe the original strategy to deploy this application
    and the metrics that characterize the problems of such a strategy. Then, we present
    the implementation of our infrastructure as code approach, which is based on the
    PowerShell DCS, and how this new strategy significantly improved our deployment
    process. Finally, we stress the advantages of this approach when it is compared
    to other ways to implement infrastructure as code solutions, such as Chef and
    Puppet. 3.1 Tools in Practice Xsolution is a client-server solution that requires
    the deployment of a server and mobile modules to execute. The abstract architecture
    of this application is illustrated in Fig. 1. Fig. 1. (adapted from [8]). The
    high-level architecture of Xsolution Full size image Each of the components in
    this figure (Smartphone, Web server, Internet Information Service – IIS, App Server,
    API Server, DB Server and SQL Server) requires a specific configuration before
    the deployment of the application. This configuration used to be manually carried
    out by the IT team by means of an internal home-made deployment guide that describes
    all the process (step-by-step), as better detailed in Sect. 3.2. In order, to
    prepare the required resources that will support Xsolution, or any other application
    with this architecture (Fig. 1), the next actions must be carried out: (1) Installation
    of packages; (2) Database installation; (3) Installation of Web application requirements;
    (4) Installation of Web application; (5) Configuration of the Admin Web Applications;
    (6) Configuration of the log of errors; and (7) Mobile Web site configuration.
    Each of these actions has multiple steps and the traditional approach to carry
    out this process is to follow guides that describe these steps. This approach
    is described in the next section. 3.2 Manual Deployment Process The manual deployment
    of Xsolution and other applications of our company, used to be manually carried
    out by a group of IT collaborators. In this strategy, each application had an
    associated deployment guide, which describes all the details to prepare the resources
    and environment to run this application. The internal deployment guide of Xsolution,
    for example, is a document with about 60 pages. It is important to understand
    how this manual process used to be carried out, so that we could have an idea
    about its complexity and the reasons it is a so time-consuming and error-prone
    activity. The first step in this manual process is the installation of packages.
    Basically the idea is to create the directory structure, which will contain the
    admin front-end Web build files (related to user interface configuration), admin
    back-end Web build files, mobile android application, back-end mobile build files,
    database structure creation scripts, database initial seed script, and mobile
    user front-end Web build files. The second step is the database installation.
    Xsolution, for example, supports both Active Directory users (through Windows
    authentication) and SQL Server users (with custom login and password). The deployment
    team must also configure the IIS (Internet Information Service) to delegate the
    anonymous authentication configuration to Xsolution. However the main aim of this
    step is the creation of the database structure, which involves several details.
    For example, the structure must only be created in the first application deployment
    and the database scripts depend on the country where the application will be hosted.
    In fact, there are a significant number of details that must be observed in this
    process. These details are described in the guide, such as: “if you update the
    database adding more values for some Enumeration, you must perform the Recycle
    of the Application Pools related to the App Server and API Server. This is necessary
    because the Enumerations present in this table are cached in memory when the application
    starts, rather than updated if changes were made in the database.” This type of
    conditional actions increases the complexity of the configuration and they are
    usually a common source of errors since they are not part of the normal configuration
    flow. The use of further support tools, such as the SQL Server management studio
    to support the database backup procedures, is also described in the guide. In
    addition, there are also issues when databases are updated. For example: “If you
    are upgrading the version of the database, you must sequentially run all scripts
    of the current version to the version you want. If just a script is ignored, the
    next scripts after that may not run correctly.” The third step is the installation
    of the Web application requirements. This step generally involves the installation
    of several third-party resources, which act as the Front-end Admin Web, Back-end
    Admin Web and Mobile User Web. For example, the Xsolution requires the installation
    of the next components: (1) NET Framework 4.5; (2) Internet Information Services
    7.5 or 10; (3) ASP.NET; (4) Windows Management Framework 3.0; and (5) IIS URL
    Rewrite 2.0 module. Each of these components also has their own installation details,
    which must be observed by the deployment team. For example, the IIS module has
    its own manual (24 pages) with instructions about the reverse proxy configuration
    using an IIS server. One of the functions of the IIS is to capture the application
    log. This task is customized and also presents a set of configurations to properly
    work according to the features of each Web application. The version of components
    is another point to observe. Xsolution, for example, allows the use of Windows
    Server 2008 R2 Service Pack 1 or Windows Server 2016. Depending on the choice,
    particular details must be observed along the configuration process. The configuration
    process also has an influence from local laws. For example, due to the new national
    legislation for Internet (Law No. 12,965 - Internet Civil Landmark) [31], information
    about the user access to the application needs to be stored for a period of six
    months. The information required is the IP, the username, the date and time of
    login. Thus, the components must be configured to maintain such information. The
    fourth step is the installation of the own Web application, which involves the
    creation of the application pool, the choice of Website locations and the assignment
    of each site to a specific application pool. In order, application pools are processing
    groups based on specific administrative preferences that isolate Website processes
    from other website processes on the server, offering strong performance and security
    benefits. Again, there are several details in this configuration. For example,
    the Admin Front-end Web and Admin Back-end Web applications could be in the same
    application pool, but it is strongly recommended that the Back-end Mobile application
    stays in a separate application pool. Thus, the configuration of two Web servers
    is required. The fifth step is the configuration of the Admin Web application.
    There are several technical details in this step, which are related to authentication
    options, the configuration of mobile responses and database access permissions.
    In fact, there are a significant number of parameters (about 50) that must be
    set and the deployment team must understand these parameters and know the best
    way to set them. Finally, the sixth and seventh steps are respectively related
    to the configuration of the error log and mobile Website. Similarly to the other
    steps, the guide brings several details and customization options. This description
    illustrates just part of the tasks and details regarding the manual deployment
    process. We can easily observe that this process is prone to errors since it is
    long and has several details. Furthermore, it is hard to identify which configuration
    was not properly performed when an error occurs. To demonstrate these problems
    and characterize this process in terms of software engineering metrics, we carried
    out a simple quantitative analysis of this process using Xsolution as our object
    of study. According to the schedule and documents from the Xsolution project,
    the deployment stage of each Xsolution release took about 16 h in the best case.
    This means when the process was performed without errors. Then, if we had 3 sprints
    per month, a collaborator should be allocated to this task over 6 days (8 h/day)
    to each new version. At each new sprint, all the guide items were executed, starting
    from the first step; while the own guide was also reviewed or updated along with
    each sprint. This ensures a current and future process free of failures. If any
    error was identified, all the process was again started from the initial configuration.
    Thus, the final deployment could spend much more than 16 h. 3.3 Infrastructure
    as Code Implementation The infrastructure as code to support the deployment was
    implemented in our organization as a form to avoid the limitations of the previous
    manual approach (Sect. 3.2). Furthermore, this approach allows that solutions
    can be deployed in any environment without the expertise required by the manual
    approach. Our strategy is based on the PowerShell DSC (Desired State Configuration),
    which is a script language that enables the definition of a set of deployment
    actions. Our experiments showed that several of the previous deployment actions
    could be automated with this language, such as: (1) install or remove server roles
    and features; (2) manage registry settings; (3) manage files and directories;
    (4) start, stop, and manage processes and services; (5) manage local groups and
    user accounts; (6) install and manage packages such as .msi and .exe; (7) manage
    environment variables; (8) fix a configuration that has drifted away from the
    desired state; and (8) discover the actual configuration state on a given node.
    Furthermore, DSC is a platform build into Windows, so that it is a natural choice
    for development projects in such platform. The use of PowerShell DSC involved
    three phases in our experiments. In the first phase (authoring phase), the DSC
    configuration was created by means of the PowerShell Integrated Scripting Environment
    (ISE), which is an authoring tool for DSC configurations. These configurations
    are translated to one or more MOF files, which contain the necessary information
    for the configuration of the nodes. MOF (Managed Object Format) is a schema description
    language used for specifying the interface of managed resources, such as storage,
    networking, etc.). Thus, they are basically made up of a series of class and instance
    declarations. A MOF file used in the Xsolution deployment, for example, accounts
    for the configuration of roles and service roles during the installation of the
    Web application requirements (step 3 discussed in Sect. 3.2). A server role is
    a set of software programs that, when installed and properly configured, allows
    a computer to perform a specific function for multiple users or other computers
    within a network. Role services are software programs that provide the functionality
    of a role. In a manual way, the deployment team must access several configuration
    pages and check a set of options indicated by the manual. Differently, the use
    of MOF scripts is simple and powerful at the same time since we do not need to
    indicate any path for the system variables. The own DSC framework already identifies
    such variables and set them. This process is completely transparent to human operators.
    However, this configuration was simple because the DSC framework has the “WindowsFeature”
    as one of its 12 built-in configuration resources. In order, a DSC resource is
    a Windows PowerShell module, which contains both the schema (the definition of
    the configurable properties) and the implementation (the code that does the actual
    work specified by a configuration) for the resource. A DSC resource schema can
    be defined in a MOF file, and the implementation is performed by a script module.
    Other examples of built-in resources that were used in our study are: DSC File
    Resource: provides a mechanism to manage files and folders on the target node;
    DSC Package Resource: provides a mechanism to install or uninstall packages, such
    as Windows Installer and setup.exe packages, on a target node; DSC Service Resource:
    provides a mechanism to manage services on the target node. The use of the infrastructure
    as code had a huge impact in our deployment efficiency. The deployment time for
    each release was decreased to 30 min. Thus, if we had 3 sprints per month, just
    90 min will be spent in this process for each new version. Furthermore, all the
    process is automatic, so that it can be quickly executed from the beginning and
    the deployment team abandoned both the use of the guide (Sect. 3.2) and its update.
    Modifications are now carried out in the own scripts and maintained by version
    control programs. 3.4 Lessons Learned Some lessons were learned along this first
    case study with DSC and some of them support previous finds from the literature.
    DSC enables IT teams in deploying several times their configuration without risks
    of breaking the infrastructure. Thus, DSC supports the DevOps principle of continuous
    deployment. We observed two important DSC features that optimize this process
    of continuous deployment: Only settings that do not match will be modified when
    the configuration is applied. The remainder configurations are skipped so that
    we obtain a faster deployment time; The definition of the configuration data and
    configuration logic are separated and well-defined. This strategy supports the
    reuse of configuration data for different resources, nodes and configurations.
    A useful DSC strategy is to record errors and events in logs that can be viewed
    in the Event Viewer application. This function was important mainly at initial
    phases of the development since the composition of configuration scripts was challenging
    for members of our team. Thus, the use of logs has facilitated the identification
    and solving of issues. DSC provides a declarative syntax to express configurations
    for infrastructure and information systems. This DSC feature accounts for creating
    a transparent process, where the IT team do not necessarily have to know how DSC
    will provide a specific feature or software installation because the declarative
    syntax is similar to an INI type expression, specifying what should be present
    on the node, as discussed in [22]. DSC has two modes of operation: push and pull.
    The pull mode has its scalability as the primary advantage and it seems to be
    the most used DSC mode. In fact, a single pull server can provide DSC configurations
    to many connected nodes with the additional benefit of specifying how often the
    LCM (Local Configuration Manager) on each node should check back with the pull
    server enforcing a configuration. However, as our task is focused on deployment,
    whose configuration is applied once for a long period, the push mode was chosen
    since we do not need periodic configuration checks. Furthermore, the push mode
    is more appropriate when environments have high security restrictions. Finally,
    we used the ability of DSC to create new resources for configurations that are
    not provided as a built-in resource. This process was straightforward and the
    resultant resources could be reused in several parts of the deployment script.
    Thus, this feature was very useful to a complete automation of our deployment
    process and we also contributed to the open source DSC official repository, which
    is maintained by Microsoft. 4 The Docker Extension: Case Study 2 4.1 Initial Setup
    The previous case study was based on an infrastructure that uses virtualization.
    However, the aspects of a containerization technology, such as Docker, must be
    considered as an additional way to facilitate continuous delivery. Containers
    provide a mechanism for logical packaging in which applications could be abstracted
    from the environment in which they actually run. The principal motivation to use
    containerization in our environment was to enable applications to be easily and
    consistently deployed, regardless of the type of host (e.g. physical machines,
    virtual machines, private or public clouds). These hosts become in fact predictable
    environments, which maintain a default configuration pattern, which we call stage
    for dev. Using containers together with our strategy to Infrastructure as Code,
    we could go beyond the process of software delivery. In order, rather than delivering
    the software together with a script to configure the infrastructure, we can instantiate
    a container, configure the infrastructure in this instance and then delivering
    a container with the software and its infrastructure ready in terms of configuration.
    In this way, we could save resources, facilitate the delivery management, provide
    scalability and portability. In brief, the next definitions of Docker are used
    in our work: Images: a template, sometimes also called recipe, which is used to
    create containers. An important part of these images is composed of a set of steps
    that account for installing and running a particular software; Containers: they
    are created from the instructions defined within the source image. Containers
    act like a compact virtual machine regarding their applications and they share
    the operating system; Client: a client is the interface of the Docker API, which
    is used to access the Docker daemon. The implementation of a daemon only makes
    sense if it is going to run in a different machine than the client; Host: this
    component is represented by a physical or virtual machine, where the daemon is
    running, and contains cached images as well as runnable containers created from
    images. Docker follows a client-server architecture, which uses a remote API to
    create containers based on Docker images. Containers and images have a similar
    relationship than objects and classes of the object-oriented programming. Thus,
    we defined our templates (images) to be later instantiated in several containers.
    The images of our project are used to create four containers: Gateway container:
    contains the gateway module, which is based on a functionality called Reverse
    proxy; Web container: contains the web, web_backend, site and helper modules;
    MobileBackend container: contains the mobile_backend module; SQL container: contains
    the database module. While these containers support the main idea of portability,
    its integration in our DSC based approach enables that their configuration can
    also evolve together with the needs of the applications in development. Next section
    describes the workflow of a traditional Docker application and how/where our approach
    works in this workflow. 4.2 Workflow The typical Docker workflow allows creating
    images, pulling images, publishing images, and running containers. The next schema
    (Fig. 2) illustrates these actions. Fig. 2. Schema of the Docker workflow. Full
    size image This schema shows that images are built from Dockerfiles, which in
    our approach will be evolved by means of DSC updates. These files have the instructions
    regarding the configuration of containers. Furthermore, Dockerfiles may also contain
    instructions on how to pull an image from Docker repositories. After the creation
    of an image in the host environment, we can run such image to create the containers,
    which are in fact an isolated runtime environment with applications and other
    configurations that are specified in the image. In our case, we have three images:
    Dockerfile_Iis (XsolutionWeb and XsolutionBackend containers), Dockerfile_IisHttps
    (XsolutionGatway container) and Dockerfile_Database (XsolutionSQL). These containers
    can be started and stopped and restarted similarly to virtual machines. A container
    can be committed to making a new image that can be later used to create containers
    from it. This must be indicated in its configuration. In our case, for example,
    after the start of a general container, the DSC PowerShell script is executed
    in each container and these four images can be generated. New images can also
    be pulled from repositories, while our images can also be pushed to such repositories.
    This is a possible strategy to share and reuse images already specified by the
    own team, or other teams around the world when these repositories are global.
    4.3 The Docker and PowerShell DSC Integration As illustrated in the previous schema
    (Fig. 2), the Dockerfile represents the main form to build Docker images by means
    of commands for installing and configuring the environment. Together with such
    files, other files located in the same directory could be incorporated as part
    of this build process. Our project extension was focused on how DSC scripts could
    be used to automate the evolution of containers that are created from Dockerfiles.
    In this scenario, the scripts could be evolved by the own development team, together
    with the application code and its needs. The next execution sequence is used in
    our project to promote this extension. First, the Dockerfile is edited, acting
    as a template to define a Docker image. Then, this image is instantiated to create
    one or more containers. Each container has a LCM, which is the DSC engine that
    runs on every host and accounts for parsing and enacting configurations that are
    sent to such host. In other words, the LCM executes DSC scripts. The configurable
    feature of LCM, by means of a meta-configuration, is fundamental in this process.
    We observed that several options for configuration are related to global behaviors.
    However, we can also identify individual behaviors by means of a configuration
    ID, which could identify, for example, an image in particular. An important investigation
    is on the integration of host configurations into the container image. To that
    end, the Dsc-Service must be installed in the Windows so that we can work with
    the DSC strategy. In this process, we observed that the build script of Docker
    must download any DSC resource that is required for configuration. Then, the Dockerfile
    can add this resource and its directories to the image before the configuration
    is applied. Another important activity in this process was the management and
    evolution of the meta-configuration. A possible approach to this management is
    to divide the meta-configuration into two parts. The first part accounts for building
    the image. The second part is more related to the instantiation of this image,
    or the creation of containers. To that end, the Dockerfile must contain the reference
    to a script that applies the static configuration to the LCM. After that, other
    commands can be used to instantiate the containers based on an image. 4.4 Discussion
    Using Docker containers together with our DSC-based approach for configuration,
    we have tested with success our infrastructure on different types of hosts such
    as Windows Server 2012 and Windows Server 2016. This means, even with different
    hosts features, the containers were able to create a unique stage for dev that
    was used by all individuals involved in the development/deployment of the applications.
    Along the process, our developers needed to integrate their codes in a branch
    to try their execution, which usually raised several problems related to the versions
    of APIs of service and outdated databases. Furthermore, the entire environment
    needed to be replicated to avoid problems with services or application instances
    that were offered to other developers. The team also had problems, before the
    stage for dev, regarding the identification of the right moment to delegate the
    maintenance of the code related to the infrastructure (infra-code) to the development
    team. This problem appeared when the DevOps team had an infrastructure script
    almost concluded, but the evolution of the Xsolution was still generating new
    modifications in this script configuration. In order, when the dev team was evolving
    the Xsolution code in a specific sprint, this used to generate a demand for the
    infrastructure evolution, which was carried out by the operation team. However,
    this infrastructure evolution should finish before the next development sprint,
    so that they could use a complete and functional infra-code. After the conclusion
    of the stage of dev, which integrated the Docker containers with DSC, the concept
    of Infrastructure as Code was included in the process of development and this
    team could evolve both the application code and the infrastructure together. The
    main restrictions of our experiment were related to aspects of security. These
    restrictions had a significant impact on the development of our solution. The
    main examples were: No application server could have access to the Internet; The
    communication between the machines should be carried out via just one port (port
    80) and no further ports could be liberated; No network shared directory could
    be created; We should consider that all machines only had the operating system
    installed. From this scenario, any required modification should be exclusively
    carried out by the deployment scripts. Due to these restrictions, libraries (e.g.
    non-native PowerShell resources) or embedded tools (e.g. Java Runtime Environment
    or the URL module for IIS) needed to be sent together with the deployment package.
    Furthermore, we also concluded that the push mode to deployment is more appropriate
    to very restrictive environments. In fact, the pull model requires the use of
    a server that is responsible for providing the configurations (MOF files), or
    the creation of a shared network directory to maintain these files. Thus, due
    to our restrictions regarding security, we could not implement any of these approaches.
    An important lesson learned was to realize that the bakery model [32] was the
    correct choice to manage the containers. According to [32], “A bakery is a form
    of infrastructure entity that embodies the process of acquiring, building and
    releasing machine images to allow repeatable deployments of working code. The
    output of a bakery is a baked image that is used to spin off instances of machines
    (VMs or containers) in any compatible infrastructure. The compatible infrastructure
    represents the required environment in the form of hypervisors or container engines
    that support the deployment of these baked images”. Thus, the bakery output, which
    is represented by baked images, is compatible with several types of infrastructures.
    This feature reduces the need for seamlessly tailored images built for each type
    of infrastructure. In our project, we used the bakery strategy to deliver the
    “ingredients and the recipe” rather than delivering the image already ready to
    use. This means, we deliver a Dockerfile, which comes from the official Microsoft
    image (microsoft/iis), and then we execute the DSC script. The main reason to
    use this strategy is the size of the final image, which is around 11 GB. 5 The
    Current State of PowerShell DSC and Docker The Windows Server 2016 was the first
    version in this platform to natively offer support to Docker. However, PowerShell
    DSC and Docker use to be discussed in the opposite site when the Infrastructure
    as Code is considered. According to Stoneman [33], DSC works with scripts which
    declaratively specify the final state of a host. This specification is modular
    and contains different types of components in different packages, which can be
    obtained from the Microsoft and related community. Thus, using PowerShell DSC
    we can write a single script to define the state of a particular type of machine.
    According to its use, DSC is employed for one-time configuration or configurations
    that periodically verify if hosts are still correctly set up. Thus, when virtual
    machines have their correct state modified, scripts can automatically return this
    state to the correct one. The idea of Docker is to maintain the host as light
    as possible. To that end, the main strategy is to install the base OS and Docker
    tools so that they support the operationalization of applications inside Docker
    containers on the host. Initially, Docker was developed to use Linux features
    that provide ways to isolate processes running on its core. Thus, containers could
    share the same kernel with other Linux processes, avoiding complex strategies
    (e.g. hypervisor) to separate the containers applications from the host’s computer
    resources. To follow this Linux strategy, Windows Server 2016 introduced containers
    as a new layer of virtualization, whose aim was to create a compatible management
    layer based on Docker. Two initial ideas of containers are Windows and Hyper-V
    Containers, whose primary interface managements were implemented in PowerShell.
    However, according to Dille [34], although this interface module was implemented
    in the PowerShell way, it did not relate to the management concepts of Docker.
    New releases of this approach show that the trend is to have Docker as the primary
    management tool while the PowerShell module for containers should be redesigned
    and adapted to this situation. This is still an ongoing process, but as described
    in the experiences of Dille [34], “After investing a few hours of testing and
    playing around with Dockerfiles as well as the build and push commands, I am amazed
    by the current state of the implementation of containers on Windows. Microsoft
    has managed to bring the management experience of Docker on Linux to Windows Server.
    This will make containers on Windows Server part of a huge community”. 6 Conclusion
    This work provides an initial analysis on the use and advantages of applying an
    infrastructure as code strategy for deployment, based on the PowerShell DSC and
    its integration with Docker. In fact, specialized forums and the software engineering
    community comment on this lack. We could find comments such as “I’ve never seen
    anyone with a robust production environment using DSC exclusively yet, however
    there are plenty of examples of Puppet/Chef environments”. Furthermore, PowerShell
    DSC and Docker are technologies that are separately discussed. While PowerShell
    DSC is presented as great for automating the setup of complex components; Docker
    is showed as great for building small, lean components. Thus, this paper is an
    initial academic contribution to show in practice as these two technologies could
    work together since most of the information presented on this subject is from
    different blogs and sites of the Internet. Our analysis with PowerShell DSC and
    Docker was based on a case study, which used a real market application as an object.
    The quantitative analysis of the efficiency of the approaches shows that the use
    of these technologies offers the appropriate resources to the automation of the
    deployment process. Our future researches intend to carry out a better quantitative
    analysis (e.g. development time) since the infrastructure as code is in fact being
    implemented in our organization. Thus, several quantitative and qualitative data
    is going to be generated regarding the real advantages of this deployment approach.
    References Humble, J., Farley, D.: Continuous delivery: reliable software releases
    through build, test, and deployment automation. Addison-Wesley Professional, Boston
    (2010) Google Scholar   Claps, G.G., Svensson, R.B., Aurum, A.: On the journey
    to continuous deployment: technical and social challenges along the way. Inf.
    Softw. Technol. 57(1), 21–31 (2015) Article   Google Scholar   Loukides, M.: What
    is DevOps? Infrastructure as Code. O’Reilly Media, Sebastopol (2012) Google Scholar   Lwakatare,
    L.E., Kuvaja, P., Oivo, M.: Dimensions of DevOps. In: Lassenius, C., Dingsøyr,
    T., Paasivaara, M. (eds.) XP 2015. LNBIP, vol. 212, pp. 212–217. Springer, Cham
    (2015). https://doi.org/10.1007/978-3-319-18612-2_19 Chapter   Google Scholar   Hosono,
    S.: A DevOps framework to shorten delivery time for cloud applications. Int. J.
    Comput. Sci. Eng. 7(4), 329–344 (2012) Google Scholar   Erich, F., Amrit, C.,
    Daneva, M.: Report: Devops literature review. University of Twente, Technical
    report (2014) Google Scholar   Dyck, A., Penners, R., Lichter, H.: Towards definitions
    for release engineering and DevOps. In: Proceedings of the IEEE/ACM 3rd International
    Workshop on Release Engineering (2015) Google Scholar   Siebra, C. et al.: From
    theory to practice: the challenges of a DevOps infrastructure as code implementation.
    In: Proceedings of the 13th International Conference on Software Technologies
    (ICSOFT) (2018) Google Scholar   Punjabi, R., Bajaj, R.: User stories to user
    reality: a DevOps approach for the cloud. In: IEEE International Conference on
    Recent Trends in Electronics, Information & Communication Technology (RTEICT),
    Bangalore, pp. 658–662 (2016) Google Scholar   Miell, I., Sayers, A.H.: Docker
    in Practice. Manning Publications Co., New York (2016) Google Scholar   Bang,
    S.K., Chung, S., Choh, Y., Dupuis, M.: A grounded theory analysis of modern Web
    applications: knowledge, skills, and abilities for DevOps. In: Proceedings of
    the 2nd Annual Conference on Research in Information Technology (RIIT 2013), pp.
    61–62. ACM, New York (2013) Google Scholar   DeGrandis, D.: Devops: so you say
    you want a revolution? Cutter Bus. Technol. J. 24(8), 34–39 (2011) Google Scholar   Wettinger,
    J., Breitenbücher, U., Leymann, F.: Devopslang – bridging the gap between development
    and operations. In: Villari, M., Zimmermann, W., Lau, K.-K. (eds.) ESOCC 2014.
    LNCS, vol. 8745, pp. 108–122. Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-44879-3_8
    Chapter   Google Scholar   Tessem, B., Iden, J.: Cooperation between developers
    and operations in software engineering projects. In: Proceedings of the 2008 International
    Workshop on Cooperative and Human Aspects of Software Engineering, pp. 105–108
    (2008) Google Scholar   Walls, M.: Building a DevOps Culture. O’Reilly Media,
    Sebastopol (2013) Google Scholar   Liu, Y., Li, C., Liu, W.: Integrated solution
    for timely delivery of customer change requests: a case study of using devops
    approach. Int. J. U- E-Serv. Sci. Technol. 7, 41–50 (2014) Article   Google Scholar   Shang,
    W.: Bridging the divide between software developers and operators using logs.
    In: Proceedings of the 34th International Conference on Software Engineering,
    pp. 1583–1586. IEEE Press, New York (2012) Google Scholar   Bruneo, D., et al.:
    Cloudwave: where adaptive cloud management meets DevOps. In: Proceedings of the
    IEEE Symposium on Computers and Communications, pp. 1–6. IEEE Press, New York
    (2014) Google Scholar   Ebert, C., Gallardo, G., Hernantes, J., Serrano, N.: DevOps.
    IEEE Softw. 33(3), 94–100 (2016) Article   Google Scholar   Duvall, M.P.: Continuous
    Delivery Patterns and AntiPatterns in the Software LifeCycle (2011) Google Scholar   Younge,
    A.J., et al.: Analysis of virtualization technologies for high performance computing
    environments. In: IEEE International Conference on Cloud Computing, pp. 9–16 (2011)
    Google Scholar   O’Connor, R., Elger, P., Clarke, P.: Continuous software engineering
    – a microservices architecture perspective. J. Softw. Evol. Process 29(11), e1866
    (2017) Article   Google Scholar   Scheepers, M.J.: Virtualization and containerization
    of application infrastructure: a comparison. In: Proceedings of the 21st Twente
    Student Conference on IT, pp. 1–7 (2014) Google Scholar   Peacock, M.: Creating
    Development Environments with Vagrant. Packt Publishing Ltd., Birmingham (2015)
    Google Scholar   Hüttermann, M.: Infrastructure as Code. In: DevOps for Developers.
    Apress, Berkeley (2012) Chapter   Google Scholar   Hummer, W., Rosenberg, F.,
    Oliveira, F., Eilam, T.: Testing idempotence for infrastructure as code. In: Eyers,
    D., Schwan, K. (eds.) Middleware 2013. LNCS, vol. 8275, pp. 368–388. Springer,
    Heidelberg (2013). https://doi.org/10.1007/978-3-642-45065-5_19 Chapter   Google
    Scholar   Artac, M., Borovssak, T., Di Nitto, E., Guerriero, M., Tamburri, D.:
    Devops: introducing infrastructure-as-code. In: Proceedings of the 39th IEEE International
    Conference on Software Engineering Companion, pp. 497–498 (2017) Google Scholar   Zhu,
    L., Xu, D., Xu, X., Tran, A.B., Weber, I., Bass, L.: Challenges in practicing
    high frequency releases in cloud environments. In: Proceedings of the 2nd International
    Workshop on Release Engineering, Mountain View, USA, pp. 21–24 (2014) Google Scholar   Scheuner,
    J., Leitner, P., Cito, J., Gall, H.: Cloud work bench–infrastructure-as-code based
    cloud benchmarking. In: Proceedings of the IEEE 6th International Conference on
    Cloud Computing Technology and Science (CloudCom), pp. 246–253 (2014) Google Scholar   Spinellis,
    D.: Don’t install software by hand. IEEE Softw. 29(4), 86–87 (2012) Article   Google
    Scholar   Tomasevicius Filho, E.: Marco Civil da Internet: uma lei sem conteúdo
    normativo. Estudos Avançados 30(86), 269–285 (2016). (in Portuguese) Article   Google
    Scholar   Juneja, V.: The Bakery Model for Building Container Images and Microservices.
    TheNewStack (2016). https://thenewstack.io/bakery-foundation-container-images-microservices.
    Accessed 20 Oct 2018 Eagles, H.: DevOps Technology in a Windows World (2016).
    https://blogs.technet.microsoft.com/uktechnet/2016/05/24/devops-technology-in-a-windows-world/.
    Accessed 28 Sept 2018 Dille, N.: Build, Ship, Run Containers on Windows Server
    2016 TP5 with Docker. Automation, DevOps and Containerization (2016). https://dille.name/blog/2016/06/08/build-ship-run-containers-with-windows-server-2016-tp5/.
    Accessed 22 Oct 2018 Download references Author information Authors and Affiliations
    Informatics Center, Federal University of Paraiba, Joao Pessoa PB, Brazil Clauirton
    Siebra CIn/Samsung Laboratory of Research and Development, UFPE, Recife PE, Brazil
    Clauirton Siebra, Rosberg Lacerda, Italo Cerqueira & Jonysberg P. Quintino SIDI/Samsung,
    Campinas, SP, Brazil Fabiana Florentin Centro de Informática, Universidade Federal
    de Pernambuco, Recife PE, Brazil Fabio B. Q. da Silva & Andre L. M. Santos Corresponding
    author Correspondence to Clauirton Siebra . Editor information Editors and Affiliations
    Information Systems Group, University of Twente, Enschede, The Netherlands Marten
    van Sinderen Wrocław University of Economics, Wrocław, Poland Leszek A. Maciaszek
    Rights and permissions Reprints and permissions Copyright information © 2019 Springer
    Nature Switzerland AG About this paper Cite this paper Siebra, C. et al. (2019).
    Empowering Continuous Delivery in Software Development: The DevOps Strategy. In:
    van Sinderen, M., Maciaszek, L. (eds) Software Technologies. ICSOFT 2018. Communications
    in Computer and Information Science, vol 1077. Springer, Cham. https://doi.org/10.1007/978-3-030-29157-0_11
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-030-29157-0_11
    Published 13 August 2019 Publisher Name Springer, Cham Print ISBN 978-3-030-29156-3
    Online ISBN 978-3-030-29157-0 eBook Packages Computer Science Computer Science
    (R0) Share this paper Anyone you share the following link with will be able to
    read this content: Get shareable link Provided by the Springer Nature SharedIt
    content-sharing initiative Publish with us Policies and ethics Sections Figures
    References Abstract Introduction Structure as Code DevOps Implementation: Case
    Study 1 The Docker Extension: Case Study 2 The Current State of PowerShell DSC
    and Docker Conclusion References Author information Editor information Rights
    and permissions Copyright information About this paper Publish with us Discover
    content Journals A-Z Books A-Z Publish with us Publish your research Open access
    publishing Products and services Our products Librarians Societies Partners and
    advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress
    Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature'
  inline_citation: '>'
  journal: Communications in computer and information science (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Empowering Continuous Delivery in Software Development: The DevOps Strategy'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/cseet49119.2020.9206183
  analysis: '>'
  authors:
  - Mark Hills
  citation_count: 1
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Conferences >2020 IEEE 32nd Conference on ... Introducing DevOps Techniques
    in a Software Construction Class Publisher: IEEE Cite This PDF Mark Hills All
    Authors 1 Cites in Paper 302 Full Text Views Abstract Document Sections I. Introduction
    II. Course Overview III. Assignment Details IV. Retrospective V. Related Work
    Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: As more companies adopt techniques related to DevOps and continuous
    deployment, it is critical for students in software engineering courses to gain
    hands-on experience with these techniques. In this paper, we describe a collection
    of assignments given in a graduate software construction class that guides students
    through the process of creating Docker containers, configuring continuous integration
    services, constructing a build pipeline, and automating deployment of new versions
    of a software system when changes are committed to the code repository. We also
    briefly analyze the performance of students on these hands-on exercises, identifying
    areas where additional support is needed to ensure student success. Published
    in: 2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T)
    Date of Conference: 09-12 November 2020 Date Added to IEEE Xplore: 14 October
    2020 Print ISBN:978-1-7281-6807-4 Print ISSN: 2377-570X DOI: 10.1109/CSEET49119.2020.9206183
    Publisher: IEEE Conference Location: Munich, Germany I. Introduction DevOps [1]–[3]
    refers to the integration of development, quality assurance, and operations teams
    in a company. Interest in DevOps has increased over the last decade as companies
    such as Amazon and Etsy have promoted DevOps techniques that are claimed to lead
    to reduced cost and faster time to market. Adopting DevOps practices often requires
    both technical and cultural changes within a company, as cross-functional teams
    replace the formerly separate teams mentioned above. DevOps relies heavily on
    tools and automation to support practices including test automation, build automation,
    continuous integration [4], continuous delivery [5], and continuous deployment
    [6]. Sign in to Continue Reading Authors Figures References Citations Keywords
    Metrics Footnotes More Like This Software Engineering in Small Software Companies:
    Consolidating and Integrating Empirical Literature Into a Process Tool Adoption
    Framework IEEE Access Published: 2021 Using Software Engineering Metrics to Evaluate
    the Quality of Static Code Analysis Tools 2018 1st International Conference on
    Data Intelligence and Security (ICDIS) Published: 2018 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Introducing DevOps Techniques in a Software Construction Class
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icpc52881.2021.00048
  analysis: '>'
  authors:
  - Rodney Rodriguez
  - Xiaoyin Wang
  citation_count: 1
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Loading [MathJax]/extensions/MathZoom.js
    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Conferences >2021 IEEE/ACM 29th Internatio... Understanding Execution Environment
    of File-Manipulation Scripts by Extracting Pre-Conditions Publisher: IEEE Cite
    This PDF Rodney Rodriguez; Xiaoyin Wang All Authors 78 Full Text Views Abstract
    Document Sections I. Introduction II. Example III. Intermediate Language IV. Approach
    V. Evaluation Show Full Outline Authors Figures References Keywords Metrics Footnotes
    Abstract: File manipulation scripts are widely used in software projects to operate
    the file system at run time. Due to the emergence of DevOps practices in software
    industry, developers also use longer and more complicated file manipulations in
    their continuous integration and deployment scripts to automate software build,
    testing, and deployment in different environment configurations. A major challenge
    on understanding these scripts is that they make lots of implicit assumptions
    on the file system they are executed on. Such assumptions are rarely documented
    and often do not hold when a script is moved to another execution environment.
    In this paper, we propose a static-analysis-based technique that statically infer
    the directory tree pre-condition of the file system required to execute a file
    manipulation script. We evaluated our analysis on 58 docker files and the experiment
    shows that our technique is able to generate directory tree preconditions on real
    world scripts efficiently. Published in: 2021 IEEE/ACM 29th International Conference
    on Program Comprehension (ICPC) Date of Conference: 20-21 May 2021 Date Added
    to IEEE Xplore: 28 June 2021 ISBN Information: ISSN Information: DOI: 10.1109/ICPC52881.2021.00048
    Publisher: IEEE Conference Location: Madrid, Spain I. Introduction File systems
    are widely used for data storage in computer systems. To automatically manipulate
    files, various software projects use file manipulation scripts combining basic
    operations such as touch, cp, and rm provided by the operating system. Furthermore,
    the emerging DevOps practice in software industry requires the developers to fully
    automate the software building, testing, and deployment process, which leads to
    more complicated file manipulations in build scripts (e.g., gradle scripts, makefiles),
    deployment scripts (e.g., docker files), and continuous integration scripts (e.g.,
    gitlab.yml, travis.yml). Sign in to Continue Reading Authors Figures References
    Keywords Metrics Footnotes More Like This Practices of Software Testing Techniques
    and Tools in Bangladesh Software Industry 2019 IEEE Asia-Pacific Conference on
    Computer Science and Data Engineering (CSDE) Published: 2019 A Comparative Study
    of Software Model Checkers as Unit Testing Tools: An Industrial Case Study IEEE
    Transactions on Software Engineering Published: 2011 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: Understanding Execution Environment of File-Manipulation Scripts by Extracting
    Pre-Conditions
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3359981
  analysis: '>'
  authors:
  - Leonardo Leite
  - Carla Rocha
  - Fábio Kon
  - Dejan Milojicic
  - Paulo Meirelles
  citation_count: 189
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Journal Home Just Accepted Latest
    Issue Archive Authors Editors Reviewers About Contact Us HomeACM JournalsACM Computing
    SurveysVol. 52, No. 6A Survey of DevOps Concepts and Challenges SURVEY SHARE ON
    A Survey of DevOps Concepts and Challenges Authors: Leonardo Leite , Carla Rocha
    , Fabio Kon , + 2 Authors Info & Claims ACM Computing SurveysVolume 52Issue 6Article
    No.: 127pp 1–35https://doi.org/10.1145/3359981 Published:14 November 2019Publication
    History 216 citation 8,003 Downloads View all FormatsPDF ACM Computing Surveys
    Volume 52, Issue 6 Previous Next Abstract References Cited By Index Terms Recommendations
    Comments Skip Abstract Section Abstract DevOpsis a collaborative and multidisciplinary
    organizational effort to automate continuous delivery of new software updates
    while guaranteeing their correctness and reliability. The present survey investigates
    and discusses DevOps challenges from the perspective of engineers, managers, and
    researchers. We review the literature and develop a DevOps conceptual map, correlating
    the DevOps automation tools with these concepts. We then discuss their practical
    implications for engineers, managers, and researchers. Finally, we critically
    explore some of the most relevant DevOps challenges reported by the literature.
    References X. Bai, M. Li, D. Pei, S. Li, and D. Ye. 2018. Continuous delivery
    of personalized assessment and feedback in agile software engineering projects.
    In Proceedings of the 40th International Conference on Software Engineering: Software
    Engineering Education and Training (ICSE-SEET’18). 58--67. Code: I818. Armin Balalaie,
    Abbas Heydarnoori, and Pooyan Jamshidi. 2016. Microservices architecture enables
    DevOps: Migration to a cloud-native architecture. IEEE Softw. 33, 3 (2016), 42--52.
    Code: A2. A. Basiri, N. Behnam, R. de Rooij, L. Hochstein, L. Kosewski, J. Reynolds,
    and C. Rosenthal. 2016. Chaos engineering. IEEE Softw. 33, 3 (2016), 35--41. Code:
    A76. Show All References Cited By View all Index Terms A Survey of DevOps Concepts
    and Challenges Software and its engineering Software creation and management Collaboration
    in software development Programming teams Software development process management
    Software post-development issues Recommendations Assessing the Maturity of DevOps
    Practices in Software Industry: An Empirical Study of HELENA2 Dataset EASE ''22:
    Proceedings of the 26th International Conference on Evaluation and Assessment
    in Software Engineering Currently, the software development organizations are
    adopting DevOps practices in order to develop quality product. Due to the lack
    of definition of DevOps, the principles, practices, and methods adopted in DevOps
    to determine success have changed ... Read More CMMI guided process improvement
    for DevOps projects: an exploratory case study ICSSP ''16: Proceedings of the
    International Conference on Software and Systems Process Very recently, an increasing
    number of software companies adopted DevOps to adapt themselves to the ever-changing
    business environment. While it is important to mature adoption of the DevOps for
    these companies, no dedicated maturity models for DevOps ... Read More DevOps
    in practice: an exploratory case study XP ''18: Proceedings of the 19th International
    Conference on Agile Software Development: Companion DevOps is a cultural movement
    and technical solution that plays a fundamental role for software-intensive organizations
    whose business greatly depends on how efficient development and operation are.
    DevOps is relatively recent, and thus little is known ... Read More Comments Login
    options Check if you have access through your login credentials or your institution
    to get full access on this article. Sign in Full Access Get this Article Information
    Contributors Published in ACM Computing Surveys   Volume 52, Issue 6 November
    2020806 pages ISSN: 0360-0300 EISSN: 1557-7341 DOI: 10.1145/3368196 Editor: Sartaj
    Sahni Issue’s Table of Contents Copyright © 2019 ACM © 2019 Association for Computing
    Machinery. ACM acknowledges that this contribution was authored or co-authored
    by an employee, contractor or affiliate of a national government. As such, the
    Government retains a nonexclusive, royalty-free right to publish or reproduce
    this article, or to allow others to do so, for Government purposes only. Publisher
    Association for Computing Machinery New York, NY, United States Publication History
    Published: 14 November 2019 Accepted: 1 August 2019 Revised: 1 June 2019 Received:
    1 January 2019 Published in CSUR Volume 52, Issue 6 Permissions Request permissions
    about this article. Request Permissions Check for updates Author Tags DevOpsand
    build processconfiguration managementcontinuous (delivery, deployment, integration)release
    processversioning Qualifiers Survey Research Refereed Bibliometrics Citations216
    Article Metrics 216 Total Citations View Citations 8,003 Total Downloads Downloads
    (Last 12 months) 1,515 Downloads (Last 6 weeks) 178 Other Metrics View Author
    Metrics PDF Format View or Download as a PDF file. PDF eReader View online with
    eReader. eReader HTML Format View this article in HTML Format . View HTML Format
    X. Bai, M. Li, D. Pei, S. Li, and D. Ye. 2018. Continuous delivery of personalized
    assessment and feedback in agile software engineering projects. In Proceedings
    of the 40th International Conference on Software Engineering: Software Engineering
    Education and Training (ICSE-SEET’18). 58--67. Code: I818. Armin Balalaie, Abbas
    Heydarnoori, and Pooyan Jamshidi. 2016. Microservices architecture enables DevOps:
    Migration to a cloud-native architecture. IEEE Softw. 33, 3 (2016), 42--52. Code:
    A2. A. Basiri, N. Behnam, R. de Rooij, L. Hochstein, L. Kosewski, J. Reynolds,
    and C. Rosenthal. 2016. Chaos engineering. IEEE Softw. 33, 3 (2016), 35--41. Code:
    A76. Len Bass. 2018. The software architect and DevOps. IEEE Softw. 35, 1 (2018),
    8--10. Code: I33. Kyle Brown and Bobby Woolf. 2016. Implementation patterns for
    microservices architectures. In Proceedings of the 23rd Conference on Pattern
    Languages of Programs (PLoP’16). The Hillside Group, Article 7, 7:1--7:35 pages.
    Code: A104. Matt Callanan and Alexandra Spillane. 2016. DevOps: Making it easy
    to do the right thing. IEEE Softw. 33, 3 (2016), 53--59. Code: A67. Lianping Chen.
    2015. Continuous delivery: Huge benefits, but challenges too. IEEE Softw. 32,
    2 (2015), 50--54. Code: B15. Henrik Bærbak Christensen. 2016. Teaching DevOps
    and cloud computing using a cognitive apprenticeship and story-telling approach.
    In Proceedings of the ACM Conference on Innovation and Technology in Computer
    Science Education (ITiCSE’16). ACM, 174--179. Code: A47. Sam Chung and Soon Bang.
    2016. Identifying knowledge, skills, and abilities (KSA) for DevOps-aware server
    side web application with the grounded theory. J. Comput. Sci. Coll. 32, 1 (2016),
    110--116. Code: A16. Gerry Gerard Claps, Richard Berntsson Svensson, and Aybüke
    Aurum. 2015. On the journey to continuous deployment: Technical and social challenges
    along the way. Inf. Softw. Technol. 57 (2015), 21--31. Code: B13. Daniel Cukier.
    2013. DevOps patterns to scale web applications using cloud services. In Proceedings
    of the Companion Publication for Conference on Systems, Programming, 8 Applications:
    Software for Humanity (SPLASH’13). ACM, 143--152. Code: A35. Maximilien de Bayser,
    Leonardo G. Azevedo, and Renato Cerqueira. 2015. ResearchOps: The case for DevOps
    in scientific applications. In Proceedings of the IFIP/IEEE International Symposium
    on Integrated Network Management (IM’15). 1398--1404. Code: I40. Rico de Feijter,
    Sietse Overbeek, Rob van Vliet, Erik Jagroep, and Sjaak Brinkkemper. 2018. DevOps
    competences and maturity for software producing organizations. In Enterprise,
    Business-Process and Information Systems Modeling. Springer, 244--259. Code: S805.
    Patrick Debois. 2011. DevOps: A software revolution in the making. Cutter IT J.
    24, 8 (2011), 3--5. Code: B4. Elisa Diel, Sabrina Marczak, and Daniela S. Cruzes.
    2016. Communication challenges and strategies in distributed DevOps. In Proceedings
    of the 11th IEEE International Conference on Global Software Engineering (ICGSE’16).
    24--28. Code: I19. Andrej Dyck, Ralf Penners, and Horst Lichter. 2015. Towards
    definitions for release engineering and DevOps. In Proceedings of the IEEE/ACM
    3rd International Workshop on Release Engineering. 3--3. Code: B26. C. Ebert,
    G. Gallardo, J. Hernantes, and N. Serrano. 2016. DevOps. IEEE Softw. 33, 3 (2016),
    94--100. Code: A54. Dror G. Feitelson, Eitan Frachtenberg, and Kent L. Beck. 2013.
    Development and deployment at Facebook. IEEE Int. Comput. 17, 4 (2013), 8--17.
    Code: B7. Nicole Forsgren and Mik Kersten. 2018. DevOps metrics. Commun. ACM 61,
    4 (2018), 44--48. Code: B21. Jim Gray. 2006. A conversation with Werner Vogels.
    ACM Queue 4, 4 (2006), 14--22. Code: B3. Jez Humble. 2017. Continuous delivery
    sounds great, but will it work here?Queue 15, 6 (2017), 57--76. Code: B22. Jez
    Humble and Joanne Molesky. 2011. Why enterprises must adopt DevOps to enable continuous
    delivery. Cutter IT J. 24, 8 (2011), 6. Code: B5. Waqar Hussain, Tony Clear, and
    Stephen MacDonell. 2017. Emerging trends for global DevOps: A New Zealand perspective.
    In Proceedings of the 12th International Conference on Global Software Engineering
    (ICGSE’17). IEEE Press, 21--30. Code: A25. Martin Gilje Jaatun. 2018. Software
    security activities that support incident management in secure DevOps. In Proceedings
    of the 13th International Conference on Availability, Reliability and Security
    (ARES’18). ACM, 8:1--8:6. Code: A803. Martin Gilje Jaatun, Daniela S. Cruzes,
    and Jesus Luna. 2017. DevOps for better software security in the cloud. In Proceedings
    of the 12th International Conference on Availability, Reliability and Security
    (ARES’17). ACM, Article 69, 69:1--69:6 pages. Code: A85. Hui Kang, Michael Le,
    and Shu Tao. 2016. Container and microservice driven design for cloud infrastructure
    DevOps. In Proceedings of the IEEE International Conference on Cloud Engineering
    (IC2E’16). 202--211. Code: I58. Mik Kersten. 2018. A Cambrian explosion of DevOps
    tools. IEEE Softw. 35, 2 (2018), 14--17. Code: I808. Teemu Laukkarinen, Kati Kuusinen,
    and Tommi Mikkonen. 2017. DevOps in regulated software development: Case medical
    devices. In Proceedings of the 39th International Conference on Software Engineering:
    New Ideas and Emerging Results Track (ICSE-NIER’17). IEEE Press, 15--18. Code:
    A26. M. Leppanen, S. Makinen, M. Pagels, V. Eloranta, J. Itkonen, M. V. Mantyla,
    and T. Mannisto. 2015. The highways and country roads to continuous deployment.
    IEEE Softw. 32, 2 (2015), 64--72. Code: B14. Z. Li, Q. Lu, L. Zhu, X. Xu, Y. Liu,
    and W. Zhang. 2018. An empirical study of cloud API issues. IEEE Cloud Comput.
    5, 2 (2018), 58--72. Code: I802. Lucy Ellen Lwakatare, Teemu Karvonen, Tanja Sauvola,
    Pasi Kuvaja, Helena Holmström Olsson, Jan Bosch, and Markku Oivo. 2016. Towards
    DevOps in the embedded systems domain: Why is it so hard? In Proceedings of the
    49th Hawaii International Conference on System Sciences (HICSS’16). IEEE, 5437--5446.
    Code: A42. Kostas Magoutis, Christos Papoulas, Antonis Papaioannou, Flora Karniavoura,
    Dimitrios-Georgios Akestoridis, Nikos Parotsidis, Maria Korozi, Asterios Leonidis,
    Stavroula Ntoa, and Constantine Stephanidis. 2015. Design and implementation of
    a social networking platform for cloud deployment specialists. J. Int. Serv. Appl.
    6, 1 (2015). Code: S3. Steve Neely and Steve Stolt. 2013. Continuous delivery?
    Easy! Just change everything (well, maybe it is not that easy). In Proceedings
    of the Agile Conference. 121--128. Code: B23. Kristian Nybom, Jens Smeds, and
    Ivan Porres. 2016. On the impact of mixing responsibilities between devs and ops.
    In Proceedings of the International Conference on Agile Software Development (XP’16).
    Springer International Publishing, 131--143. Code: S18. Helena H. Olsson, Hiva
    Alahyari, and Jan Bosch. 2012. Climbing the “stairway to heaven”—A mulitiple-case
    study exploring barriers in the transition from agile development towards continuous
    deployment of software. In Proceedings of the 38th Euromicro Conference on Software
    Engineering and Advanced Applications. 392--399. Code: B17. Candy Pang and Abram
    Hindle. 2016. Continuous maintenance. In Proceedings of the IEEE International
    Conference on Software Maintenance and Evolution (ICSME’16). 458--462. Code: I55.
    Rahul Punjabi and Ruhi Bajaj. 2016. User stories to user reality: A DevOps approach
    for the cloud. In Proceedings of the IEEE International Conference on Recent Trends
    in Electronics, Information Communication Technology (RTEICT’16). 658--662. Code:
    I17. Akond Rahman. 2018. Characteristics of defective infrastructure as code scripts
    in DevOps. In Proceedings of the 40th International Conference on Software Engineering
    (ICSE’18). ACM, 476--479. Code: A806. M. Rajkumar, A. K. Pole, V. S. Adige, and
    P. Mahanta. 2016. DevOps culture and its impact on cloud delivery and software
    development. In Proceedings of the International Conference on Advances in Computing,
    Communication, Automation (ICACCA’16). 1--6. Code: I48. James Roche. 2013. Adopting
    DevOps practices in quality assurance. Commun. ACM 56, 11 (2013), 38--43. Code:
    A74. S. Van Rossem, W. Tavernier, D. Colle, M. Pickavet, and P. Demeester. 2018.
    Introducing development features for virtualized network services. IEEE Commun.
    Mag. 56, 8 (2018), 184--192. Code: I77. Mojtaba Shahin, Muhammad Ali Babar, and
    Liming Zhu. 2016. The intersection of continuous deployment and architecting process:
    Practitioners’ perspectives. In Proceedings of the 10th ACM/IEEE International
    Symposium on Empirical Software Engineering and Measurement (ESEM’16). ACM, 44:1--44:10.
    Code: A64. Alan Sill. 2014. Cloud standards and the spectrum of development. IEEE
    Cloud Comput. 1, 3 (2014), 15--19. Code: I67. Rodrigo Siqueira, Diego Camarinha,
    Melissa Wen, Paulo Meirelles, and Fabio Kon. 2018. Continuous delivery: Building
    trust in a large-scale, complex government organization. IEEE Softw. 35, 2 (2018),
    38--43. Code: B29. Barry Snyder and Bill Curtis. 2018. Using analytics to guide
    improvement during an agile/DevOps transformation. IEEE Softw. 35, 1 (2018), 78--83.
    Code: I7. Johannes Wettinger, Vasilios Andrikopoulos, and Frank Leymann. 2015.
    Automated capturing and systematic usage of DevOps knowledge for cloud applications.
    In Proceedings of the IEEE International Conference on Cloud Engineering. IEEE,
    60--65. Code: A61. Johannes Wettinger, Vasilios Andrikopoulos, and Frank Leymann.
    2015. Enabling DevOps collaboration and continuous delivery using diverse application
    environments. In Proceedings of the On the Move to Meaningful Internet Systems
    Conferences (OTM’15). Springer International Publishing, 348--358. Code: A17.
    Eoin Woods. 2016. Operational: The forgotten architectural view. IEEE Softw. 33,
    3 (2016), 20--23. Code: A82. Hasan Yasar and Kiriakos Kontostathis. 2016. Where
    to integrate security practices on DevOps platform. Int. J. Sec. Softw. Eng. 7,
    4 (2016), 39--50. Code: A58. L. Zhu, D. Xu, A. B. Tran, X. Xu, L. Bass, I. Weber,
    and S. Dwarakanathan. 2015. Achieving reliable high-frequency releases in cloud
    environments. IEEE Softw. 32, 2 (2015), 73--80. Code: B12. xMatters. 2017. xMatters
    Atlassian DevOps Maturity Survey Report 2017. Retrieved from: https://www.xmatters.com/press-release/xmatters-atlassian-2017-devops-maturity-survey-report/.
    YouTube. 2018. How Netflix Thinks of DevOps. Retrieved from: https://www.youtube.com/watch?v&equals;UTKIT6STSVM.
    Nicole Forsgren Velasquez Alanna Brown, Gene Kim, Nigel Kersten, and Jez Humble.
    2016. 2016 State of DevOps Report. Retrieved from: https://puppet.com/resources/whitepaper/2016-state-of-devops-report.
    Hrishikesh Barua. 2015. The Role of Configuration Management in a Containerized
    World. Retrieved from: https://www.infoq.com/news/2015/12/containers-vs-config-mgmt.
    Len Bass, Ingo Weber, and Liming Zhu. 2015. DevOps: A Software Architect’s Perspective.
    Addison-Wesley Professional. Helen Beal. 2015. Where are you on the DevOps Maturity
    Scale Webcast. Retrieved from: https://www.youtube.com/watch?v&equals;a50ArHzVRqk.
    Kent Beck and Cynthia Andres. 2004. Extreme Programming Explained: Embrace Change.
    Addison-Wesley Professional. Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall
    Richard Murphy. 2016. Site Reliability Engineering: How Google Runs Production
    Systems. O’Reilly Media. Jonas Bonér, Dave Farley, Roland Kuhn, and Martin Thompson.
    2014. The Reactive Manifesto. Retrieved from: https://www.reactivemanifesto.org/.
    Rob Brigham. 2015. DevOps at Amazon: A Look at Our Tools and Processes. Retrieved
    from: https://www.youtube.com/watch?v&equals;esEFaY0FDKc. Donovan Brown. 2018.
    Our DevOps Journey—Microsoft’s Internal Transformation Story. Retrieved from:
    https://www.youtube.com/watch?v&equals;cbFzojQOjyA. David Budgen and Pearl Brereton.
    2006. Performing systematic literature reviews in software engineering. In Proceedings
    of the 28th International Conference on Software Engineering (ICSE’06). ACM, 1051--1052.
    Necco Ceresani. 2016. The Periodic Table of DevOps Tools v.2 Is Here. Retrieved
    from: https://blog.xebialabs.com/2016/06/14/periodic-table-devops-tools-v-2/.
    Kathy Charmaz. 2008. Chapter 7: Grounded theory as an emergent method. In Handbook
    of Emergent Methods. The Guilford Press. Gerry Coleman and Rory O’Connor. 2008.
    Investigating software process in practice: A grounded theory perspective. J.
    Syst. Softw. 81, 5 (2008), 772--784. Juliet Corbin and Anselm Strauss. 2014. Basics
    of Qualitative Research: Techniques and Procedures for Developing Grounded Theory
    (4th ed.). SAGE Publications, Inc. Breno B. Nicolau de França, Helvio Jeronimo,
    Jr., and Guilherme Horta Travassos. 2016. Characterizing DevOps by hearing multiple
    voices. In Proceedings of the 30th Brazilian Symposium on Software Engineering
    (SBES’16). ACM, 53--62. Patrick Debois. 2008. Agile Infrastructure Operations.
    At Agile 2008 Toronto. Retrieved on October 2019 from http://www.jedi.be/presentations/agile-infrastructure-agile-2008.pdf.
    Phil Dougherty. 2015. Containers vs. Config Management. Retrieved from: https://blog.containership.io/containers-vs-config-management-e64cbb744a94.
    Floris Erich, Chintan Amrit, and Maya Daneva. 2014. A mapping study on cooperation
    between information system development and operations. In Product-Focused Software
    Process Improvement, Andreas Jedlitschka, Pasi Kuvaja, Marco Kuhrmann, Tomi Männistö,
    Jürgen Münch, and Mikko Raatikainen (Eds.). Springer International Publishing,
    Cham, 277--280. F. M. A. Erich, C. Amrit, and M. Daneva. 2017. A qualitative study
    of DevOps usage in practice. J. Softw.: Evol. Proc. 29, 6 (2017), e1885. Nicole
    Forsgren, Jez Humble, and Gene Kim. 2018. Accelerate: The Science of Lean Software
    and DevOps: Building and Scaling High Performing Technology Organizations. IT
    Revolution Press. Martin Fowler. 2004. Strangler Application. Retrieved from:
    https://www.martinfowler.com/bliki/Strangler-Application.html. Martin Fowler.
    2010. Blue Green Deployment. Retrieved from: https://martinfowler.com/bliki/BlueGreen-Deployment.html.
    Georges Bou Ghantous and Asif Gill. 2017. DevOps: Concepts, practices, tools,
    benefits and challenges. In Proceedings of the 21st Pacific Asia Conference on
    Information Systems (PACIS’17). 96:1--96:12. Peter J. Hager, Howard Jeffrey Scheiber,
    and Nancy C. Corbin. 1997. Designing 8 Delivering: Scientific, Technical, and
    Managerial Presentations. John Wiley 8 Sons. James Hamilton. 2007. On designing
    and deploying internet-scale services. In Proceedings of the 21st Large Installation
    System Administration Conference (LISA’07). USENIX, 231--242. Pete Hodgson. 2017.
    Feature Toggles (aka Feature Flags). Retrieved from: https://martinfowler.com/articles/feature-toggles.html.
    Jonah Horowitz. 2017. Configuration Management Is an Antipattern. Retrieved from:
    https://hackernoon.com/configuration-management-is-an-antipattern-e677e34be64c.
    Jez Humble. 2010. Continuous Delivery vs Continuous Deployment. Retrieved from:
    https://continuousdelivery.com/2010/08/continuous-delivery-vs-continuous-deployment/.
    Jez Humble. 2012. There’s No Such Thing as a “DevOps Team.” Retrieved from: https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/.
    Jez Humble and David Farley. 2010. Continuous Delivery: Reliable Software Releases
    Through Build, Test, and Deployment Automation. Addison-Wesley Professional. Ramtin
    Jabbari, Nauman bin Ali, Kai Petersen, and Binish Tanveer. 2016. What is DevOps?:
    A systematic mapping study on definitions and practices. In Proceedings of the
    Scientific Workshop Proceedings of XP2016 (XP’16 Workshops). ACM, 12:1--12:11.
    Adam Jacob. 2015. Chef Style DevOps Kungfu. Retrieved from: https://www.youtube.com/watch?v&equals;_DEToXsgrPc.
    Dan Kelly. 2016. Configuration Management and Containers: Which Is Better? Retrieved
    from: https://blog.containership.io/configuration-management-and-containers-which-is-better.
    N. Kerzazi and B. Adams. 2016. Botched releases: Do we need to roll back? Empirical
    study on a commercial web app. In Proceedings of the IEEE 23rd International Conference
    on Software Analysis, Evolution, and Reengineering (SANER’16), Vol. 1. 574--583.
    N. Kerzazi and B. Adams. 2016. Who needs release and DevOps engineers, and why?
    In Proceedings of the IEEE/ACM International Workshop on Continuous Software Evolution
    and Delivery (CSED’16). 77--83. Gene Kim. 2012. The Three Ways: The Principles
    Underpinning DevOps. Retrieved from: http://itrevolution.com/the-three-ways-principles-underpinning-devops/.
    Gene Kim, Kevin Behr, and Kim Spafford. 2014. The Phoenix Project: A Novel about
    IT, DevOps, and Helping Your Business Win. IT Revolution. Gene Kim, Jez Humble,
    Patrick Debois, and John Willis. 2016. The DevOps Handbook: How to Create World-Class
    Agility, Reliability, and Security in Technology Organizations. IT Revolution
    Press. Henrik Kniberg. 2014. Spotify engineering culture (part 1). Retrieved from:
    https://labs.spotify.com/2014/03/27/spotify-engineering-culture-part-1. Per Kroll
    and Philippe Kruchten. 2003. The Rational Unified Process Made Easy: A Practitioner’s
    Guide to the RUP. Addison-Wesley Professional. Patrick Kua. 2013. An Appropriate
    Use of Metrics. Retrieved from: https://martinfowler.com/articles/useOfMetrics.html.
    James Lewis and Martin Fowler. 2014. Microservices. Retrieved from: https://www.martinfowler.com/articles/microservices.html.
    Lucy Ellen Lwakatare, Pasi Kuvaja, and Markku Oivo. 2015. Dimensions of DevOps.
    In Agile Processes in Software Engineering and Extreme Programming. Springer International
    Publishing, 212--217. Robert C. Martin. 2008. Chapter 12: Emergence. In Clean
    Code: A Handbook of Agile Software Craftsmanship. Prentice Hall. M. Douglas McIlroy,
    J. M. Buxton, Peter Naur, and Brian Randell. 1968. Mass-produced software components.
    In Proceedings of the NATO Conference on Software Engineering, Software Engineering
    Concepts and Techniques. 88--98. Peter Mell and Timothy Grance. 2011. The NIST
    definition of cloud computing. Retrieved from: http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf.
    Matthew B. Miles and A. Michael Huberman. 1994. Chapter 2: Focusing and bounding
    the collection of data—The substantive start. In Qualitative Data Analysis: An
    Expanded Sourcebook (6th ed.). SAGE Publications. Dejan Milojicic. 2011. Autograding
    in the cloud: Interview with David O’Hallaron. IEEE Int. Comput. 15, 1 (2011),
    9--12. Kief Morris. 2016. Infrastructure as Code: Managing Servers in the Cloud.
    O’Reilly Media. Eueung Mulyana, Rifqy Hakimi, and Hendrawan. 2018. Bringing automation
    to the classroom: A ChatOps-based approach. In Proceedings of the 4th International
    Conference on Wireless and Telematics (ICWT’18). 1--6. Michael T. Nygard. 2009.
    Release It! Design and Deploy Production-Ready Software. Pragmatic Bookshelf.
    Mary Poppendieck and Tom Poppendieck. 2006. Implementing Lean Software Development:
    From Concept to Cash. Addison-Wesley Professional. Roger S. Pressman. 2005. Software
    Engineering: A Practitioner’s Approach (6th ed.). Palgrave Macmillan. Mike Roberts.
    2018. Serverless Architectures. Retrieved from: https://martinfowler.com/articles/serverless.html.
    Kevin Roebuck. 2011. DevOps: High-impact Strategies—What You Need to Know: Definitions,
    Adoptions, Impact, Benefits, Maturity, Vendors. Tebbo. Margaret Rouse. 2015. What
    Is NoOps?—Definition from WhatIs.com. Retrieved from: https://searchcloudapplications.techtarget.com/definition/noops.
    Danilo Sato. 2014. CanaryRelease. Retrieved from: https://martinfowler.com/bliki/CanaryRelease.html.
    Danilo Sato. 2014. Chapter 12: Infrastructure as code. In DevOps in Practice:
    Reliable and Automated Software Delivery. Casa do Código. Alexandra Sbaraini,
    Stacy M. Carter, R. Wendell Evans, and Anthony Blinkhorn. 2011. How to do a grounded
    theory study: A worked example of a study of dental practices. BMC Med. Res. Methodol.
    11, 128 (2011), 1--20. Julia Silge. 2017. How Much Do Developers Earn? Find Out
    with the Stack Overflow Salary Calculator. Retrieved from: https://stackoverflow.blog/2017/09/19/much-developers-earn-find-stack-overflow-salary-calculator/.
    Matthew Skelton and Manuel Pais. 2013. DevOps Topologies. Retrieved from: https://web.devopstopologies.com/.
    Jens Smeds, Kristian Nybom, and Ivan Porres. 2015. DevOps: A definition and perceived
    adoption impediments. In Agile Processes in Software Engineering and Extreme Programming.
    Springer International Publishing, 166--177. Ian Sommerville. 2011. Software Engineering
    (9th ed.). Addison-Wesley. Daniel Stahl, Torvald Martensson, and Jan Bosch. 2017.
    Continuous practices and DevOps: Beyond the buzz, what does it all mean? In Proceedings
    of the 43rd Euromicro Conference on Software Engineering and Advanced Applications
    (SEAA’17). 440--448. Klaas-Jan Stol, Paul Ralph, and Brian Fitzgerald. 2016. Grounded
    theory in software engineering research: A critical review and guidelines. In
    Proceedings of the IEEE/ACM 38th International Conference on Software Engineering
    (ICSE’16). 120--131. Antonio Terceiro, Joenio Costa, João Miranda, Paulo Meirelles,
    Luiz Romário Rios, Lucianna Almeida, Christina Chavez, and Fabio Kon. 2010. Analizo:
    An extensible multi-language source code analysis and visualization toolkit. In
    Proceedings of the Brazilian Conference on Software: Theory and Practice (Tools
    Session) (CBSoft’10), Vol. 29. J. C. van Winkel. 2017. Life of an SRE at Google.
    Retrieved from: https://www.youtube.com/watch?v&equals;7Oe8mYPBZmw. Nicole Forsgren
    Velasquez, Gene Kim, Nigel Kersten, and Jez Humble. 2014. 2014 State of DevOps
    Report. Retrieved from: https://puppet.com/resources/whitepaper/2014-state-devops-report.
    Adam Wiggins. 2011. The Twelve-Factor App. Retrieved from: https://12factor.net/.
    Claes Wohlin. 2014. Guidelines for snowballing in systematic literature studies
    and a replication in software engineering. In Proceedings of the 18th International
    Conference on Evaluation and Assessment in Software Engineering (EASE’14). ACM,
    38:1--38:10. Figures Other Share this Publication link https://dl.acm.org/doi/10.1145/3359981
    Copy Link Share on Social Media Share on Twitter LinkedIn Reddit Facebook Email
    122 References View Issue’s Table of Contents Footer Categories Journals Magazines
    Books Proceedings SIGs Conferences Collections People About About ACM Digital
    Library ACM Digital Library Board Subscription Information Author Guidelines Using
    ACM Digital Library All Holdings within the ACM Digital Library ACM Computing
    Classification System Digital Library Accessibility Join Join ACM Join SIGs Subscribe
    to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin
    Feedback Bug Report The ACM Digital Library is published by the Association for
    Computing Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy
    Code of Ethics'
  inline_citation: '>'
  journal: ACM computing surveys
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: A Survey of DevOps Concepts and Challenges
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ms.2015.62
  analysis: '>'
  authors:
  - Charles Anderson
  citation_count: 210
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Software >Volume: 32 Issue: 3 Docker [Software engineering] Publisher: IEEE
    Cite This PDF Charles Anderson All Authors 189 Cites in Papers 25821 Full Text
    Views Abstract Document Sections Software Engineering Radio Authors Figures Citations
    Keywords Metrics Abstract: In episode 217 of Software Engineering Radio, host
    Charles Anderson talks with James Turnbull, a software developer and security
    specialist who''s vice president of services at Docker. Lightweight Docker containers
    are rapidly becoming a tool for deploying microservice-based architectures. Published
    in: IEEE Software ( Volume: 32, Issue: 3, May-June 2015) Page(s): 102 - c3 Date
    of Publication: 23 April 2015 ISSN Information: DOI: 10.1109/MS.2015.62 Publisher:
    IEEE Show All The Software Engineering Radio podcast welcomes two new hosts this
    year: Josh Long and Sven Johann, whose interview with Software Architecture for
    Developers author Simon Brown is coming soon. Later this year, we''ll publish
    an episode in which I interview all the podcast hosts and some of the editorial
    staff. This will give our listeners the opportunity to find out why busy software
    engineers are volunteering their time to this podcast. In episode 217, host Charles
    Anderson talks with James Turnbull, a software developer and security specialist
    who''s vice president of services at Docker. Lightweight Docker containers are
    rapidly becoming a tool for deploying microservice-based architectures, a topic
    we''ve covered in several shows and in last issue''s column. Portions of the interview
    that aren''t featured in this column owing to space include networking between
    containers, how Docker images are built, the DockerHub repository for sharing
    images, developer use cases for containers, the role of containers in a microservices
    architecture, and Docker''s importance for DevOps. You can download the full episode
    at www.se-radio.net. —Robert Blumen What is Docker? Docker is a container virtualization
    technology. So, it''s like a very lightweight virtual machine [VM]. In addition
    to building containers, we provide what we call a developer workflow, which is
    really about helping people build containers and applications inside containers
    and then share those among their teammates. What problems does it address? There
    are a couple of problems we''re looking at specifically. The first one is aimed
    at the fact that a VM is a fairly large-weight compute resource. Your average
    VM is a copy of an operating system running on top of a hypervisor running on
    top of physical hardware, which your application is then on top of. That presents
    some challenges for speed and performance, and some challenges in an agile sort
    of environment. So, we''re aiming to solve the problem of producing a more lightweight,
    more agile compute resource. Docker containers launch in a subsecond, and you
    can then have a hypervisor that sits directly on top of the operating system.
    So, you can pack a lot of them onto a physical or virtual machine. You get quite
    a lot of scalability. For most people, the most important IT asset they own is
    the code they''re developing, and that code lives on a developer''s workstation
    or laptop or in a dev test environment. It''s not really valuable to the company
    until it actually gets in front of the customer. The process by which it gets
    in front of a customer, that workflow of dev test, staging, and deployment to
    production, is one of the most [tension-fraught] in IT. The DevOps movement, for
    example, emerged from one of the classic stumbling blocks in a lot of organizations.
    Developers build code and applications and ship them to the operations people,
    only to discover that the code and applications don''t run in production. This
    is the classic “it works on my machine; it''s operations'' problem now.” We were
    aiming to build a lightweight computing technology that helped people put code
    and applications inside that resource, have them be portable all the way through
    the dev test, and then be able to be instantiated in production. We made the assumption
    that what you build and run in dev test looks the same as what you build and run
    in production. What are some typical use cases in which a developer or admin might
    want to use Docker? We have two really hot use cases right now. The first one
    is continuous integration and continuous deployment. With Docker being so lightweight,
    developers can build stacks of Docker containers on their laptops that replicate
    some production environments—for example, a LAMP stack or a multitier application.
    They can build and run their application against that stack. You can then move
    these containers around—they''re very portable. Let''s say you have a Jenkins
    continuous-integration environment. Instead of relying on VMs, we have to spin
    up a new VM, install all the software, install your application source code, run
    the tests, and then probably tear it all down again because you may have destroyed
    the VM as part of the test process. Let''s say it would take 10 minutes to build
    those VMs. In the Docker world, you can build those VMs or the containers that
    replace them in a matter of seconds, which means if you''ve cut 10 minutes out
    of your build—test run, that''s an amazing cost saving. It allows you to get much
    more bang out of your buck from the continuous-deployment and continuous-testing
    model. Software Engineering Radio Visit www.se-radio.net to listen to these and
    other insightful hour-long podcasts. Recent Episodes 219—Jeff Meyerson talks with
    Apache Kafka project committer Jun Rao about the popular streaming and messaging
    framework, and the challenges of building a reliable distributed messaging system.
    220—Robert Blumen is on location with Jon Gifford to learn about using logging
    to provide insights into programs'' run-time behavior, modern logging infrastructure
    based on a search engine, and logging as a service in the cloud. 221—Jez Humble
    joins host Johannes Thönes to explore the benefits and challenges of implementing
    continuous delivery, from both a technological and cultural standpoint. Upcoming
    Episodes Sven Johann interviews Simon Brown, author of Software Architecture for
    Developers, about Brown''s approach to developing, documenting, and communicating
    software architecture via a set of simple drawings. Joshua Suereth and Matthew
    Farwell appear with host Tobias Kaatz to discuss software builds, the SBT (a scala
    build tool), and the guests'' new book. Stefan Tilkov sits down with Mark Nottingham
    to learn about the game-changing new version of the venerable HTTP protocol and
    its impact on Web application development. The other area where we''re seeing
    a lot of interest is what we call high capacity. Traditional VMs have a hypervisor,
    which probably occupies about 10 to 15 percent of the capacity of a host. We have
    a lot of customers for whom that 10 to 15 percent is quite an expensive 10 to
    15 percent. They want to say, “Okay, let''s root that out, replace it with a Docker
    host, and then we can run a lot more containers.” We can run hyperscale numbers
    of containers on a host container because without a hypervisor, they sit right
    on top of the operating system and are very, very fast. IBM released some research
    last year that suggested that on a per-transaction basis, the average container
    is about 26 times faster than a VM, which is pretty amazing. Docker is based on
    containers, which provide an isolated environment for user-mode code. Some of
    our listeners might be familiar with earlier container systems such as Solaris
    Zones or FreeBSD jails, or even going back to the chroot system call from Unix
    Version 7. How does the container have its own copy of the file system without
    duplicating the space between identical containers, especially when you''re talking
    about hyperscale? One of the other interesting technologies Docker relies on is
    a concept we call copy-on-write. Many file systems, such as Btrfs, Device Mapper,
    and AuFS, all support this copy-on-write model, which is what the kernel developers
    call a union file system. Essentially, what happens is that you build layers of
    file systems. So, every Docker container is built on what we call an image. The
    Docker image is like a prebaked file system that contains a very thin layer of
    libraries and binaries that are required to make your application work, and perhaps
    your application code and maybe some supporting packages. For example, you might
    have a LAMP stack that might have a container that has Apache in it, and libc,
    and a small number of very thin shims that fake out an operating system. That
    image is saved in what we call a file system layer. If I was to then make a change
    to that image—for example, if I wanted to install another package—I''d say, “I
    want to have PHP as well.” On an Ubuntu system I''d say, “apt-get install PHP,”
    and Docker says, “You want to create a new thing. I''m going to create a new layer
    on top of our existing layer, and I''m only going to add in the things that I
    have changed.” So, for example, “I''m going to add in the new package, and that''s
    it.” That is a layered construction. I end up with a read-only file system with
    multiple layers. You can think about this layer a bit like a git commit or a version
    control commit. As a result, I end up with a very lightweight system that only
    has the things on it that I want. Docker understands that I can cache things.
    So it says, “You''ve already installed PHP. I''m not going to make any changes
    to the environment; I''m not going to have to write anything. So, I''m going to
    just reuse that existing layer, and I''ll drag that in as the PHP layer.” For
    example, if you''re changing your source code, instead of a VM you may be rebuilding
    the DM. With Docker, you say, “Here''s the new commit of my source code. I''m
    going to add it to my Docker image and maybe that''s 10 Kbytes worth of code change.”
    Docker says, “That''s the only thing you want to change; therefore, that''s the
    only thing I''ll write to the file system.” As a result, it''s very lightweight,
    and with the cache, extraordinarily fast to rebuild. Explain how a process in
    a container can only see other processes in the container. Docker relies heavily
    on two pieces of Linux kernel technology. The first one is called namespaces.
    If you run a new process on the Linux kernel, then you''re making a system call
    to the namespaces: “I want to create a new process.” If you want to create a new
    network interface, you''re making a call to the network namespace. The kernel
    assigns you a namespace that has a process and whatever other resources you want.
    For example, it might have some access to the network. It might have access to
    some parts of the file system. It might have access just to memory or CPU. When
    we create a Docker container, we''re basically making a bunch of calls to the
    Linux kernel to say, “Can you build me a box? The box should have access to this
    particular file system, access to CPU and memory, and access to the network, and
    it should be inside this process namespace.” And from inside that process namespace,
    you can''t see any other process namespaces outside it. The second piece of technology
    we use is called control groups, or cgroups. These are designed around managing
    the resources available to a container. It allows us to do things such as “This
    container only gets 128 Mbytes of RAM” or “This container doesn''t have access
    to the network.” You can add and drop capabilities as needed, and that makes it
    fairly powerful to be able to granularly control a container in much the same
    way you would with the point-and-shoot VM interface to say, “Get this network
    interface,” “Get this CPU core,” “Take this bit of memory access to this file
    system,” or “Create a virtual CD-ROM drive.” It''s a similar level of technology.
    Besides files and processes, containers provide an isolated environment for network
    addresses and ports. for example, I can have Web servers running in multiple containers
    all using port 80. By default, the container doesn''t expose network ports to
    the outside. However, they can be exposed manually or intentionally, which is
    a bit like opening up ports on a firewall such as iptables, right? That''s correct.
    Each container has its own network interface, which is a virtual network interface.
    You can run processes that use network ports inside those containers. For example,
    I can run 10 Apache containers inside port 80, inside the container. And then,
    outside the container, I can say, “Expose this port.” And if I want to actually
    expose port 80 to port 80 I can, but obviously I can only do that once, because
    you can only map one port inside the container to the one port outside the host.
    By default, Docker chooses a random port and says, “I''m going to do a network
    address translation between this port 80 inside the container and, say, port 49154
    on the host.” For example, I could have multiple Apache processes running on different
    ports. Then I put a service discovery tool or a load balancer or some sort of
    proxy in front of it. HAProxy is very commonly used. We can use things such as
    nginx or service discovery tools such as Zoo-Keeper, etcd, and Consul that allow
    you to either proxy the connections or provide a way to say, “I want this application.
    Query that particular service discovery tool,” or “That application belongs to
    these 10 containers, and you can choose one of these 10 ports,” which will connect
    to the Apache process. So it''s a very flexible, scalable model. It''s designed
    to run complicated applications. Those are going to be aimed at running containers
    on multiple hosts as well, because so far we''ve been talking about Docker on
    a single host, right? We launched a prototype called libswarm, like a swarm of
    wasps. That tool was designed to prototype how you would get Docker hosts to talk
    to one another, because currently containers on one Docker host have to use the
    network to talk to one another. But they should have a back channel of communication.
    You should essentially be able to have Docker hosts communicating together. We
    look at this like a way to say, “I''m a Docker host and I run Apache Web services,”
    and another Docker host is saying, “I have a database back end that needs an Apache
    front end. Can you link one of your containers with one of my containers?” This
    starts to create some really awesome stories around scalability, autoscaling,
    and redundancy. And, you start to be able to build really complex applications,
    which up until now has been very challenging for a lot of organizations. Authors
    Figures Citations Keywords Metrics More Like This Communication, Knowledge and
    Co-ordination Management in Globally Distributed Software Development: Informed
    by a scientific Software Engineering Case Study 2009 Fourth IEEE International
    Conference on Global Software Engineering Published: 2009 Team Composition and
    Team Factors in Software Engineering: An Interview Study of Project-Based Organizations
    2018 25th Asia-Pacific Software Engineering Conference (APSEC) Published: 2018
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE software
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/52/7093013/07093032.pdf
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: Docker [Software engineering]
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ic2e.2016.26
  analysis: '>'
  authors:
  - Hui Kang
  - Michael Le
  - Tao Shu
  citation_count: 139
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2016
    IEEE International Confe... Container and Microservice Driven Design for Cloud
    Infrastructure DevOps Publisher: IEEE Cite This PDF Hui Kang; Michael Le; Shu
    Tao All Authors 126 Cites in Papers 15 Cites in Patents 7243 Full Text Views Abstract
    Document Sections I. Introduction II. Cloud Infrastructure Devops III. Case Study:
    Applying Containerization and Microservice-Style Design to OpenStack IV. Evaluation
    V. Limitations and Discussion Show Full Outline Authors Figures References Citations
    Keywords Metrics Footnotes Abstract: Emerging container technologies, such as
    Docker, offer unprecedented agility in developing and running applications in
    cloud environment especially when combined with a microservice-style architecture.
    However, it is often difficult to use containers to manage the cloud infrastructure,
    without sacrificing many benefits container offers. This paper identifies the
    key challenges that impede realizing the full promise of containerizing infrastructure
    services. Using OpenStack as a case study, we explore solutions to these challenges.
    Specifically, we redesign OpenStack deployment architecture to enable dynamic
    service registration and discovery, explore different ways to manage service state
    in containers, and enable containers to access the host kernel and devices. We
    quantify the efficiency of the container-based microservice-style DevOps compared
    to the VM-based approach, and study the scalability of the stateless and stateful
    containerized components. We also discuss limitations in our current design, and
    highlight open research problems that, if solved, can lead to wider adoption of
    containers in cloud infrastructure management. Published in: 2016 IEEE International
    Conference on Cloud Engineering (IC2E) Date of Conference: 04-08 April 2016 Date
    Added to IEEE Xplore: 02 June 2016 Electronic ISBN:978-1-5090-1961-8 DOI: 10.1109/IC2E.2016.26
    Publisher: IEEE Conference Location: Berlin, Germany I. Introduction Infrastructure
    as a Service (IaaS) provides users with immediate access to scalable and seemingly
    “infinite” infrastructure resources without the expensive overhead and burden
    of having to maintain and manage their own datacenters. To offer IaaS, providers
    must deploy complex software stacks (aka., cloud infrastructure management software)
    to allocate, manage, monitor, and configure the underlying physical resources,
    e.g., CloudStack [1], Eucalyptus [2], and OpenStack. With IaaS quickly becoming
    commoditized and with businesses relying more on IaaS to serve as a foundation
    on which more lucrative managed services are built, the need to efficiently deploy
    and operate these infrastructure services grows in importance. Sign in to Continue
    Reading Authors Figures References Citations Keywords Metrics Footnotes More Like
    This Long Live The Image: Container-Native Data Persistence in Production 2021
    IEEE 18th International Conference on Software Architecture Companion (ICSA-C)
    Published: 2021 A Novel Solution of Distributed Memory NoSQL Database for Cloud
    Computing 2011 10th IEEE/ACIS International Conference on Computer and Information
    Science Published: 2011 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Container and Microservice Driven Design for Cloud Infrastructure DevOps
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/978-3-319-33313-7_15
  analysis: '>'
  authors:
  - Armin Balalaie
  - Abbas Heydarnoori
  - Pooyan Jamshidi
  citation_count: 90
  full_citation: '>'
  full_text: '>

    Your privacy, your choice We use essential cookies to make sure the site can function.
    We also use optional cookies for advertising, personalisation of content, usage
    analysis, and social media. By accepting optional cookies, you consent to the
    processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Advances in Service-Oriented
    and Cloud Computing Conference paper Migrating to Cloud-Native Architectures Using
    Microservices: An Experience Report Conference paper First Online: 27 April 2016
    pp 201–215 Cite this conference paper Access provided by University of Nebraska-Lincoln
    Download book PDF Download book EPUB Advances in Service-Oriented and Cloud Computing
    (ESOCC 2015) Armin Balalaie, Abbas Heydarnoori & Pooyan Jamshidi  Part of the
    book series: Communications in Computer and Information Science ((CCIS,volume
    567)) Included in the following conference series: European Conference on Service-Oriented
    and Cloud Computing 4311 Accesses 67 Citations 1 Altmetric Abstract Migration
    to the cloud has been a popular topic in industry and academia in recent years.
    Despite many benefits that the cloud presents, such as high availability and scalability,
    most of the on-premise application architectures are not ready to fully exploit
    the benefits of this environment, and adapting them to this environment is a non-trivial
    task. Microservices have appeared recently as novel architectural styles that
    are native to the cloud. These cloud-native architectures can facilitate migrating
    on-premise architectures to fully benefit from the cloud environments because
    non-functional attributes, like scalability, are inherent in this style. The existing
    approaches on cloud migration does not mostly consider cloud-native architectures
    as their first-class citizens. As a result, the final product may not meet its
    primary drivers for migration. In this paper, we intend to report our experience
    and lessons learned in an ongoing project on migrating a monolithic on-premise
    software architecture to microservices. We concluded that microservices is not
    a one-fit-all solution as it introduces new complexities to the system, and many
    factors, such as distribution complexities, should be considered before adopting
    this style. However, if adopted in a context that needs high flexibility in terms
    of scalability and availability, it can deliver its promised benefits. Access
    provided by University of Nebraska-Lincoln. Download conference paper PDF Similar
    content being viewed by others From Monolith to Cloud Architecture Using Semi-automated
    Microservices Modernization Chapter © 2020 Microservices as a Key Enabler of a
    Cloud Native Architecture Chapter © 2023 Towards a Methodology to Form Microservices
    from Monolithic Ones Chapter © 2017 Keywords Cloud migration Microservices Cloud-native
    architectures Software modernization 1 Introduction In recent years, with the
    emergence of cloud computing and its promises, many companies from large to small
    and medium sizes are considering cloud as a target platform for migration [9].
    Despite motivations for migrating to the cloud, most of the applications could
    not benefit from the cloud environment as long as their main intention is to simply
    dump the existing legacy architecture to a virtualized environment and call it
    a cloud application. One of the main characteristics of the cloud environment
    is that failures can happen at any time, and the applications in this environment
    should be designed in a way that they can resist such uncertainties. Furthermore,
    application scalability would not be possible without a scalable architecture.
    Cloud-native architectures like microservices are the ones that have these characteristics,
    i.e., availability and scalability, in their nature and can facilitate migrating
    on-premise architectures to fully benefit from the cloud environments. Microservices
    is a novel architectural style that has been proposed to overcome the shortcomings
    of a monolithic architecture [16] in which the application logic is within one
    deployable unit. For small systems, the monolithic architecture could be the most
    appropriate solution and could become highly available and scalable using simple
    load balancing mechanisms. However, as the size of the system starts growing,
    problems like difficulties in understanding the code, increased deployment time,
    scalability for data-intensive loads, and a long-term commitment to a technology
    stack would start to appear [16]. This is where microservices come to help by
    providing small services that are easy to understand, could be deployed and scaled
    independently, and could have different technology stacks. Most of the current
    approaches on cloud migration are focused on automated migration via applying
    model-driven approaches [1, 2], and reusing of knowledge by migration patterns
    [5, 10, 12] without having cloud-native architectures as their first-class citizens.
    Furthermore, microservices is a new concept and thus, only a few technical reports
    can be found about using them in the literature [15, 17]. Migrating an application’s
    architecture to microservices brings in many complexities that make this migration
    a non-trivial task. In this paper, we report our experience on an ongoing project
    in PegahTech Co. 1, on migrating an on-premise application named SSaaS to microservices
    architecture. Although the migration steps that we describe in this paper are
    specific to our project, the necessity of performing these migration activities
    could be generalized to other projects as well. Furthermore, we summarize some
    of the challenges we faced and the lessons learned during this project. The rest
    of this paper is organized as follows: Sect. 2 briefly explains the background
    behind the microservices architecture. Section 3 describes the architecture of
    SSaaS before its migration to microservices. The target architecture to which
    we migrated SSaaS is described in Sect. 4. Section 5 then discusses our migration
    plan and the steps that we followed in our migration project. Next, Sect. 6 summarizes
    the lessons learned in this project. Finally, Sect. 7 concludes the paper. 2 Background
    Microservices is a new trend that binds closely to some other new concepts like
    Continuous Delivery and DevOps. In this section, we first explain these concepts
    followed by the background on microservices architecture. 2.1 Continuous Delivery
    and DevOps Continuous Delivery [8] is a software development discipline that enables
    on demand deployment of a software to any environment. With Continuous Delivery,
    the software delivery life cycle will be automated as much as possible. It leverages
    techniques like Continuous Integration and Continuous Deployment and embraces
    DevOps. The DevOps is a culture that emphasizes the collaboration between developers
    and operations teams from the beginning of every project in order to reduce time
    to market and bring agility to all the phases of the software development life
    cycle. By adopting microservices, the number of services will be increased. Consequently,
    we need a mechanism for automating the delivery process. 2.2 Microservices Microservices
    is a new architectural style [6] that aims to realize software systems as a package
    of small services, each deployable on a different platform, and running in its
    own process while communicating through lightweight mechanisms like RESTFull APIs.
    In this setting, each service is a business capability which can utilize various
    programming languages and data stores. A system has a microservices architecture
    when that system is composed of several services without any centralized control
    [11]. Resilience to failure is another characteristic of microservices as every
    request in this new setting will be translated to several service calls through
    the system. The Continuous Delivery and DevOps are also needed to be agile in
    terms of development and deployment [6]. To have a fully functional microservices
    architecture and to take advantage of all of its benefits, the following components
    have to be utilized. Most of these components address the complexities of distributing
    the business logic among the services: Configuration Server: It is one of the
    principles of Continuous Delivery to decouple source code from its configuration.
    It enables us to change the configuration of our application without redeploying
    the code. As a microservices architecture have so many services, and their re-deployment
    is going to be costly, it is better to have a configuration server so that the
    services could fetch their corresponding configurations. Service Discovery: In
    a microservices architecture, there exist several services that each of them might
    have many instances in order to scale themselves to the underlying load. Thus,
    keeping track of the deployed services, and their exact address and port number
    is a cumbersome task. The solution is to use a Service Discovery component in
    order to get the available instances of each service. Load Balancer: In order
    to be scalable, an application should be able to distribute the load on an individual
    service among its many instances. This is the duty of a Load Balancer, and in
    this case, it should get available instances from the Service Discovery component.
    Circuit Breaker: Fault tolerance should be embedded in every cloud-native application,
    and it makes more sense in a microservices architecture where lots of dependent
    services are working together. Failure in each of this services may result in
    the failure of the whole system. Leveraging patterns like Circuit Breaker [14]
    can mitigate the corresponding loss to the lowest level. Edge Server: The Edge
    Server is an implementation of the API Gateway pattern [16] and a wall for exposing
    external APIs to the public. All the traffic from outside should be routed to
    internal services through this server. In this way, the clients would not be affected
    if the internal structures of system’s services have changed afterwards. 3 The
    Architecture of SSaaS Before the Migration The SSaaS (Server Side as a Service)
    application was initially started at PegahTech Co. to be a service that provides
    mobile application developers a facility for doing the server side programming
    part of their applications without knowing any server side languages. The PegahTech
    Co. envisions SSaaS as a service that could be scaled to millions of users. The
    first functionality of SSaaS was a RDBMS as a Service. Developers could define
    their database schema in the SSaaS website, and the SSaaS service provides them
    an SDK for their desired target platform (e.g., Android or iOS). Afterwards, the
    developers can only code in their desired platforms using their domain objects,
    and the objects would make some service calls on their behalf in order to fulfill
    their requests. As time goes on, new services are being added to SSaaS like Chat
    as a Service, Indexing as a Service, NoSQL as a Service, and so on. SSaaS is written
    in Java using the Spring framework. The underlying RDBMS is an Oracle 11g. Maven
    is used for fetching dependencies and building the project. All of the services
    were in a Git repository, and the modules feature of Maven was used to build different
    services. At the time of writing this paper, there were no test cases for this
    project. The deployment of services in development machines was done using the
    Maven’s Jetty plugin. However, the deployment to the production machine was a
    manual task that had many disadvantages [8]. Fig. 1. The architecture of SSaaS
    before the migration Full size image In Fig. 1, solid arrows and dashed arrows
    respectively illustrate service calls direction and library dependencies. Figure
    1 also indicates that SSaaS consisted of the following five components before
    the migration: CommonLib: This is a place for putting shared functionalities,
    like utility classes, that are going to be used by the rest of the system. DeveloperData:
    This holds the information of developers who are using the SSaaS service and their
    domain model metadata entities that are shared between the DeveloperServices and
    the ContentServices components. DeveloperServices: This is where the services
    related to managing the domain model of developers’ projects reside in. Using
    these services, developers could add new models, edit existing ones, and so on.
    ContentServices: This holds the services that the target SDK is using in order
    to perform the CRUD operations on the model’s objects. DeveloperWebsite: This
    is an application written in HTML and JQuery and acts as a dashboard for developers.
    For this purpose, it leverages the DeveloperServices component. 3.1 Why Did We
    Plan to Migrate Towards the Microservices? What motivated us to perform a migration
    to a microservices architecture was a problem raised with a requirement for a
    Chat as a Service. To implement this requirement, we chose ejabberd 2 due to its
    known built-in scalability and its ability to run on clusters. To this end, we
    wrote a script in python that enabled ejabberd to perform authentications using
    our system. After preparing everything, the big issue in our service was the on
    demand capability, otherwise our service was useless. In the following, we discuss
    the reasons that motivated us to choose the microservices architecture: The need
    for reusability: To address the above issue, we started to automate the process
    of setting up a chat service. One of these steps was to set up a database for
    each user. We were hoping that this was also a step in creating RDBMS projects
    that we can reuse. After investigating the RDBMS service creation process, we
    recognized that there was not anything to satisfy our new requirement. To clarify
    further, there was a pool of servers in place. Each of these servers had an instance
    of the Oracle DBMS installed and an instance of DeveloperServices running. During
    the creation of a RDBMS project, a server was selected randomly and related users
    and tablespaces were created in the Oracle server. The mentioned design had several
    issues since it was just designed to fulfill the RDBMS service needs, and it was
    tightly coupled to the Oracle server. Nevertheless, we needed MySQL database for
    ejabberd and we should add this functionality to the system. After struggling
    a bit with the system, we recognized that we were just revamping the current bad
    design. What we needed was a database reservation system that both of our services
    could make use of. Thinking more generally, we needed a backing resources reservation
    system. This was the first step towards making cohesive services that can be reused
    by other parts of the system. The need for decentralized data governance: Another
    problem was that every time anyone wanted to add some metadata about different
    services, they were added to the DeveloperData. In other words, it was kind of
    an integration point among the services. It was not a good habit because services
    were independent units that were only sharing their contracts with other parts
    of the system. Consequently, another step was to re-architect the system so that
    any services could govern its own metadata and data by themselves. The need for
    automated deployment: As the number of services was growing, another problem was
    to automate the deployment process and to decouple the build life cycle of each
    service from other services as much as possible. This can happen using the Configuration
    Server and the Continuous Delivery components. The need for built-in scalability:
    As mentioned before, the vision of SSaaS is to serve millions of users. By increasing
    the number of services, we needed a new approach for handling this kind of scalability
    because scaling services individually needs a lot of work and can be error-prone.
    Therefore, to handle this problem, our solution was to locate service instances
    dynamically through the Service Discovery component and balancing the load among
    them using the internal Load Balancer component. To summarize, new requirements
    pushed us to introduce new services, and new services brought in new non-functional
    requirements as mentioned above. Hence, we got advantage of microservices to satisfy
    these new requirements. 4 The Target Architecture of SSaaS After the Migration
    In order to realize microservices architecture and to satisfy our new requirements,
    we transformed the core architecture of our system to a target architecture by
    undergoing some architectural refactorings. These changes included introducing
    microservices-specific components as explained in Sect. 2 and re-architecting
    the current system as will be discussed in this section. The final architecture
    is depicted in Fig. 2. The new technology stack for the development was including
    the Spring Boot 3 for its embedded application server, fast service initialization,
    using the operating system’s environment variables for configuration, and the
    Spring Cloud 4 Context and the Config Server to separate the configuration from
    the source code as recommended by Continuous Delivery. Additionally, we chose
    the Netflix OSS 5 for providing some of the microservices-specific components,
    i.e. Service Discovery, and the Spring Cloud Netflix that integrates the Spring
    framework with the Netflix OSS project. We also chose Eureka for Service Discovery,
    Ribbon as Load Balancer, Hystrix as Circuit Breaker and Zuul as Edge Server, that
    all are parts of the Netflix OSS project. We specifically chose Ribbon among other
    load balancers, i.e. HAProxy 6, because of its integration with the Spring framework
    and other Netflix OSS projects, in particular Eureka. Additionally, it is an internal
    load balancer, so we do not need to deploy an external one. Fig. 2. Target architecture
    of SSaaS after the migration Full size image 4.1 How Did We Re-Architect the System
    and Refactor the Data? In the state-of-the-art about microservices [13, 18], Domain
    Driven Design [4, 19] and Bounded Context [4, 19] are introduced as common practices
    to transform the system’s architecture into microservices. As we did not have
    a complex domain, we decided to re-architect the system based on domain entities
    in DeveloperData. We put every set of cohesive entities into a service, such that
    the only one which can create and update that entity would be that service. For
    example, only the ChatServices service could update or create the chat metadata
    entities. Other services can only have copies of the data that they do not own,
    e.g., for the purpose of caching. However, they should be careful about synchronization
    with the master data as their copy could be stale. With respect to this discussion,
    the list of architectural changes to reach the target architecture is the following:
    Letting the ChatServices service handle its metadata by itself and not inside
    the DeveloperData. Introducing a new Resource Manager service in order to reserve
    resources like databases. The entities related to Oracle server instances will
    be moved from DeveloerData to this service. Introducing a new service to handle
    developer’s information and its registered services. Transforming DeveloperData
    from a library to a service. Therefore, DeveloperServices and ContentServices
    have to be adapted such that they can make service calls to DeveloperData instead
    of method calls. Please note that the remaining data in DeveloperData are just
    RDBMS entities like Table and Column. 5 Migration Steps Migrating the system towards
    the target architecture is not a one-step procedure and should be done incrementally
    and in several steps without affecting the end-users of the system. Furthermore,
    as the number of services is growing, we need a mechanism for automating the delivery
    process. In this section, we describe how we migrated SSaaS using the following
    eight steps: Fig. 3. Transforming DeveloperData to a service Full size image 5.1
    Preparing the Continuous Integration Pipeline Continuous integration is the first
    step for having an effective Continuous Delivery. It allows developers to integrate
    their work with the others’ early and often, and helps to prevent future conflicts
    [8]. To this end, a continuous integration (CI) server, an as-a-service or self-hosted
    code repository, and an artifact repository is needed. We chose Jenkins 7 as the
    CI server, self-hosted Gitlab 8 as the code repository, and Artifactory 9 as the
    artifact repository (cf. Fig. 8). By adopting microservices, the number of services
    will increase. As each of these services can have a number of instances running,
    deploying them by virtualization is not cost-effective and can introduce a lot
    of computational overhead. Furthermore, we may need to use Configuration Management
    systems in order to create the exact test and production environments. Containerization
    is a new trend that is well suited for microservices. By utilizing containers,
    we can deploy service instances with lower overheads than the virtualization,
    and in isolation. Additionally, we would not hear phrases like “this works on
    my machine” anymore because we are using the exact environments and artifacts
    in both of the development and production environments. Another major benefit
    is portability since we can deploy anywhere that supports containerization without
    any changes to our source codes or container images. Many public cloud providers
    such as Google and Amazon now have a support for containerization. Docker 10 is
    a tool for containerization of applications, and it is now becoming the de-facto
    standard for containerization in industry. There is a pool of ready to use images
    in the Docker Hub, the central docker image repository, that can be pulled and
    customized based on specific needs. Docker Registry 11 is another project that
    let organizations to have a private docker image repository. As we are going to
    use Docker, we need Docker Registry to be in our pipeline as well. To summarize,
    in this step, we installed and integrated the Gitlab, Jenkins, Artifactory and
    Docker Registry as a CI pipeline. 5.2 Transforming DeveloperData to a Service
    In this step, we changed DeveloperData to use Spring Boot because of its advantages
    (see Sect. 4). Furthermore as shown in Fig. 3, we changed it to expose its functionalities
    as a REST API. In this way, its dependent services would not be affected when
    the internal structure of DeveloperData changes. Since they have service-level
    dependency, the governance of DeveloperData entities will be done by a single
    service and DeveloperData would not act as an Integration Database [7] for its
    dependent services anymore. Accordingly, we adapted DeveloperServices and ContentServices
    to use DeveloperData as a service and not as a Maven dependency. Fig. 4. Introducing
    configuration server Full size image 5.3 Introducing Continuous Delivery A best
    practice in the Continuous Delivery is to separate the source code, the configuration,
    and the environment specification so that they can evolve independently [8]. In
    this way, we can change the configuration without redeploying the source code.
    By leveraging Docker, we removed the need for specifying environments since the
    Docker images produce the same behavior in different environments. In order to
    separate the source code and the configuration, we ported every service to Spring
    Boot and changed them to use the Spring Cloud Configuration Server and the Spring
    Cloud Context for resolving their configuration values (cf. Fig. 4). In this step,
    we also separated services’ code repositories to have a clearer change history
    and to separate the build life cycle of each service. We also created the Dockerfile
    for each service that is a configuration for creating Docker images for that service.
    After doing all of the mentioned tasks, we created a CI job per service and ran
    them in order to populate our repositories. Having the Docker image of each service
    in our private Docker registry, we were able to run the whole system with Docker
    Compose 12 using only one configuration file. Starting from this step, we had
    an automated deployment on a single server. 5.4 Introducing Edge Server As we
    were going to re-architect the system and it was supposed to change the internal
    service architecture, in this step, we introduced Edge Server to the system to
    minimize the impact of internal changes on end-users as shown in Fig. 5. Accordingly,
    we adapted DeveloperWebsite. Fig. 5. Introducing edge server Full size image 5.5
    Introducing Dynamic Service Collaboration In this step, we introduced Service
    Discovery, Load Balancer and Circuit Breaker to the system as shown in Fig. 6.
    Dependent services should locate each other via the Service Discovery and Load
    Balancer; and the Circuit Breaker will make our system more resilient during the
    service calls. By introducing these components to the system sooner, we made our
    developers more comfortable with these new concepts, and it increased our speed
    for the rest of the migration and of course, in introducing new services. Fig.
    6. Introducing dynamic service collaboration Full size image 5.6 Introducing Resource
    Manager In this step, we introduced the Resource Manager by factoring out the
    entities that were related to servers, i.e. AvailableServer, from DeveloperData
    and introducing some new features, i.e. MySQL database reservation, for satisfying
    our chat service requirements (cf. Fig. 7). Accordingly, we adapted DeveloperServices
    to use this service for database reservations. Fig. 7. Introducing resource manager
    Full size image 5.7 Introducing ChatServices and DeveloperInfoServices As the
    final step in re-architecting the system, we introduced the following services:
    DeveloperInfoServices by factoring out developer related entities (e.g., Developer)
    from DeveloperData. ChatServices for persisting chat service instances metadata
    and handling chat service instance creations. This led us to the target architecture
    as depicted in Fig. 2. 5.8 Clusterization Compared to virtualization, one of the
    main features of containerization is its low overhead. Due to this feature, people
    started to make it more efficient by introducing lightweight operating systems,
    like CoreOS 13 and Project Atomic, that only have the minimal parts to host many
    containers. Google Kubernetes 14, that has a good integration with the CoreOS,
    is a tool for easy deployments of containers on a cluster. Using Kubernetes, a
    container can be easily fetched from a private repository and deployed to a cluster
    with different policies. For example, a service can be deployed with three always
    available instances. In this step, we set up a cluster of CoreOS instances with
    Kubernetes agents installed on them. Next, we deployed our services on this cluster
    instead of a single server. The final delivery pipeline is shown in Fig. 8. Fig.
    8. The final delivery pipeline Full size image In Sect. 5, we described the incremental
    process of migrating the SSaaS application towards the microservices architecture.
    This migration was actually performed in three dimensions: re-architecting the
    current system, introducing new supporting services, and enabling Continuous Delivery
    in the system. The important point to note is that how we incrementally evolved
    the system in all these three dimensions together. Despite the smoothness of the
    explained process, we faced several challenges in this process as well. Section
    6 discusses some of the lessons we learned during this process. 6 Lessons Learned
    Migrating an on-premise application to a microservices architecture is a non-trivial
    task. During this migration, we faced several challenges that we were able to
    solve. In the following, we share some of the lessons we learned in this process
    that we think might be helpful for others who are also trying to migrate to microservices:
    Deployment in the development environment is difficult: Introducing new services
    to the system will put a big burden on developers. It is true that the application’s
    code is now in isolated services. However, to run those services in their machines,
    developers need to deploy the dependent services as well. For example, the service
    registry should be deployed as well in order to have a working system. These kinds
    of deployment complexities are not normal for a novice developer. Hence, there
    should be a facility in place for setting up such a development environment with
    a minimum amount of effort. In our case, we chose the Docker Compose to easily
    deploy dependent services from our private Docker registry. Service contracts
    are double important: Changing so many services that only expose their contracts
    to each other could be an error-prone task. Even a small change in the contracts
    can break a part of the system or even the system as a whole. Service versioning
    is a solution. Nonetheless, it could make the deployment procedure of each service
    even more complex. Therefore, people usually do not recommend service versioning
    in microservices. Thus, techniques like Tolerant Reader [3] are more advisable
    in order to avoid service versioning. Consumer-driven contracts [3] could be a
    great help in this regard, as the team responsible for the service can be confident
    that most of their consumers are satisfied with their service. Distributed system
    development needs skilled developers: Microservices is a distributed architectural
    style. Furthermore, in order for it to be fully functional, it needs some supporting
    services like service registry, load balancer, and so on. Hence, to get the most
    out of microservices, those team members are needed who are familiar with these
    concepts and are comfortable with this type of programming. Creating service development
    templates is important: Polyglot persistence and the usage of different programming
    languages are promises of microservices. Nevertheless, in practice, a radical
    interpretation of these promises could result in a chaos in the system and make
    it even unmaintainable. Consequently, having standards is a must in order to avoid
    chaos. Different languages and data stores can be used, but it should be in a
    controlled and standard way. As a solution, having service development templates
    for each leveraged language is essential. It would reduce the burden of development
    since people can easily fork the template and just start developing. Microservices
    is not a silver bullet: Microservices was beneficial for us because we needed
    that amount of flexibility in our system, and that we had the Spring Cloud and
    Netflix OSS that made our migration and development a lot easier. However, as
    mentioned before, by adopting microservices so many complexities would be introduced
    to the system that require a lot of effort to be addressed. Therefore, these challenges
    should be considered before the adoption of microservices. In other words, maybe
    our problems could be solved more easily by applying another architectural style
    or solution. 7 Conclusions and Future Work In this paper, we explained our experience
    during the migration of an on-premise application to the microservices architectural
    style. In particular, we provided the architecture of our system before and after
    the migration and the steps that we followed for this migration. Furthermore,
    we highlighted the importance of Continuous Delivery in the process of adopting
    microservices. Finally, we discussed the lessons learned during this migration.
    In future, we plan to consolidate these practices and develop a set of reusable
    patterns for migrating on-premise applications to microservices architectural
    style. These patterns should generalize the process that we used in this paper,
    but in a well-defined structure that can be instantiated independently, similarly
    to the approach that we devised in [10]. Notes 1. http://www.pegahtech.ir. 2.
    https://www.ejabberd.im/. 3. http://projects.spring.io/spring-boot. 4. http://projects.spring.io/spring-cloud.
    5. http://netflix.github.io. 6. http://www.haproxy.org. 7. https://jenkins-ci.org.
    8. https://about.gitlab.com. 9. http://www.jfrog.com/open-source. 10. https://www.docker.com.
    11. https://docs.docker.com/registry. 12. https://docs.docker.com/compose. 13.
    https://coreos.com. 14. http://kubernetes.io. References Ardagna, D., di Nitto,
    E., Mohagheghi, P., Mosser, S., Ballagny, C., D’Andria, F., Casale, G., Matthews,
    P., Nechifor, C.S., Petcu, D., Gericke, A., Sheridan, C.: Modaclouds: a model-driven
    approach for the design and execution of applications on multiple clouds. In:
    4th International Workshop on Modelling in Software Engineering (MISE), pp. 50–56,
    June 2012 Google Scholar   Bergmayr, A., Bruneliere, H., Canovas Izquierdo, J.,
    Gorronogoitia, J., Kousiouris, G., Kyriazis, D., Langer, P., Menychtas, A., Orue-Echevarria,
    L., Pezuela, C., Wimmer, M.: Migrating legacy software to the cloud with artist.
    In: 17th European Conference on Software Maintenance and Reengineering (CSMR),
    pp. 465–468, March 2013 Google Scholar   Daigneau, R.: Service Design Patterns:
    Fundamental Design Solutions for SOAP/WSDL and Restful Web Services. Addison-Wesley
    Professional, Reading (2011) Google Scholar   Evans, E.: Domain-driven Design:
    Tackling Complexity in the Heart of Software. Addison-Wesley Professional, Reading
    (2004) Google Scholar   Fehling, C., Leymann, F., Ruehl, S., Rudek, M., Verclas,
    S.: Service migration patterns - decision support and best practices for the migration
    of existing service-based applications to cloud environments. In: 6th IEEE International
    Conference on Service-Oriented Computing and Applications (SOCA), pp. 9–16, December
    2013 Google Scholar   Fowler, M., Lewis, J.: Microservices. http://martinfowler.com/articles/microservices.html.
    Accessed 15 Jun 2015 Hohpe, G., Woolf, B.: Enterprise Integration Patterns: Designing,
    Building, and Deploying Messaging Solutions. Addison-Wesley Professional, Reading
    (2004) Google Scholar   Humble, J., Farley, D.: Continuous delivery: Reliable
    Software Releases through Build, Test, and Deployment Automation. Addison-Wesley
    Professional, Reading (2010) Google Scholar   Jamshidi, P., Ahmad, A., Pahl, C.:
    Cloud migration research: a systematic review. IEEE Trans. Cloud Comput. 1(2),
    142–157 (2013) Article   Google Scholar   Jamshidi, P., Pahl, C., Chinenyeze,
    S., Liu, X.: Cloud migration patterns: a multi-cloud service architecture perspective.
    In: Toumani, F., et al. (eds.) Service-Oriented Computing - ICSOC 2014 Workshops.
    LNCS, vol. 8954, pp. 6–9. Springer, Switzerland (2015) Chapter   Google Scholar   Martin,
    R.: Clean micro-service architecture. http://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html.
    Accessed 15 June 2015 Mendonca, N.: Architectural options for cloud migration.
    Computer 47(8), 62–66 (2014) Article   Google Scholar   Newman, S.: Building Microservices.
    O’Reilly Media, Sebastopol (2015) Google Scholar   Nygard, M.: Release It!: Design
    and Deploy Production-Ready Software. Pragmatic Bookshelf, Raleigh (2007) Google
    Scholar   Calçado, P.: Building products at soundcloud. https://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith.
    Accessed 15 June 2015 Richardson, C.: Microservices architecture (2014). http://microservices.io/.
    Accessed 15 June 2015 Borsje, S.: How we build microservices at karma. https://blog.yourkarma.com/building-microservices-at-karma.
    Accessed 15 June 2015 Stine, M.: Migrating to Cloud-Native Application Architectures.
    O’Reilly Media, Sebastopol (2015) Google Scholar   Vernon, V.: Implementing Domain-driven
    Design. Addison-Wesley Professional, Reading (2013) Google Scholar   Download
    references Acknowledgments The work of Pooyan Jamshidi has been supported by the
    Irish Centre for Cloud Computing and Commerce (IC4) and by the Horizon 2020 project
    no. 644869 (DICE). Author information Authors and Affiliations Department of Computer
    Engineering, Sharif University of Technology, Tehran, Iran Armin Balalaie & Abbas
    Heydarnoori Department of Computing, Imperial College London, London, UK Pooyan
    Jamshidi Corresponding author Correspondence to Abbas Heydarnoori . Editor information
    Editors and Affiliations DICIEAMA, University of Messina, Messina, Italy Antonio
    Celesti Software Evolution and Architecture Lab, University of Zürich Software
    Evolution and Architecture Lab, Zürich, Switzerland Philipp Leitner Rights and
    permissions Reprints and permissions Copyright information © 2016 Springer International
    Publishing Switzerland About this paper Cite this paper Balalaie, A., Heydarnoori,
    A., Jamshidi, P. (2016). Migrating to Cloud-Native Architectures Using Microservices:
    An Experience Report. In: Celesti, A., Leitner, P. (eds) Advances in Service-Oriented
    and Cloud Computing. ESOCC 2015. Communications in Computer and Information Science,
    vol 567. Springer, Cham. https://doi.org/10.1007/978-3-319-33313-7_15 Download
    citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-319-33313-7_15 Published
    27 April 2016 Publisher Name Springer, Cham Print ISBN 978-3-319-33312-0 Online
    ISBN 978-3-319-33313-7 eBook Packages Computer Science Computer Science (R0) Share
    this paper Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Sections Figures References Abstract
    Introduction Background The Architecture of SSaaS Before the Migration The Target
    Architecture of SSaaS After the Migration Migration Steps Lessons Learned Conclusions
    and Future Work Notes References Acknowledgments Author information Editor information
    Rights and permissions Copyright information About this paper Publish with us
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature'
  inline_citation: '>'
  journal: Communications in computer and information science (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: 'Migrating to Cloud-Native Architectures Using Microservices: An Experience
    Report'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/nca.2015.49
  analysis: '>'
  authors:
  - Marcelo Amaral
  - Jordà Polo
  - David Carrera
  - Iqbal Mohomed
  - Merve Unuvar
  - Małgorzata Steinder
  citation_count: 108
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2015
    IEEE 14th International ... Performance Evaluation of Microservices Architectures
    Using Containers Publisher: IEEE Cite This PDF Marcelo Amaral; Jordà Polo; David
    Carrera; Iqbal Mohomed; Merve Unuvar; Malgorzata Steinder All Authors 108 Cites
    in Papers 11 Cites in Patents 4596 Full Text Views Abstract Document Sections
    I. Introduction II. Microservices Architecture Using Containers III. Related Work
    IV. Evaluation V. Conclusions Authors Figures References Citations Keywords Metrics
    Abstract: Micro services architecture has started a new trend for application
    development for a number of reasons: (1) to reduce complexity by using tiny services,
    (2) to scale, remove and deploy parts of the system easily, (3) to improve flexibility
    to use different frameworks and tools, (4) to increase the overall scalability,
    and (5) to improve the resilience of the system. Containers have empowered the
    usage of micro services architectures by being lightweight, providing fast start-up
    times, and having a low overhead. Containers can be used to develop applications
    based on monolithic architectures where the whole system runs inside a single
    container or inside a micro services architecture where one or few processes run
    inside the containers. Two models can be used to implement a micro services architecture
    using containers: master-slave, or nested-container. The goal of this work is
    to compare the performance of CPU and network running benchmarks in the two aforementioned
    models of micro services architecture hence provide a benchmark analysis guidance
    for system designers. Published in: 2015 IEEE 14th International Symposium on
    Network Computing and Applications Date of Conference: 28-30 September 2015 Date
    Added to IEEE Xplore: 07 January 2016 ISBN Information: DOI: 10.1109/NCA.2015.49
    Publisher: IEEE Conference Location: Cambridge, MA, USA I. Introduction Virtual
    Machines are a widely used building block of workload management and deployment.
    They are heavily used in both traditional data center environments and clouds
    (private, public and hybrid clouds). The commonly used term Virtual Machine (VM)
    refers to server virtualization, which can be accomplished via full virtualization
    or paravirtualization. In recent months, there has been a resurgence of interest
    in container technology, which provides a more lightweight mechanism - operating
    system level virtualization. Containers are lightweight and fast - a single x86
    server can reasonably have 100s of containers running (memory usually ends up
    being the scarce resource); moreover, containers start up very quickly - under
    1 to 2 seconds in most cases. There are many reasons for this resurgence but from
    a technical perspective, two of the biggest reasons are (i) the improvements in
    names-pace support in the Linux kernel are available in popular distributions,
    and (ii) a specific implementation of containers - Docker - has successfully created
    an attractive packaging format, useful tools and diverse ecosystem. Sign in to
    Continue Reading Authors Figures References Citations Keywords Metrics More Like
    This NHVM: Design and Implementation of Linux Server Virtual Machine Using Hybrid
    Virtualization Technology 2010 International Conference on Computational Science
    and Its Applications Published: 2010 Performance comparison between Linux containers
    and virtual machines 2015 International Conference on Advances in Computer Engineering
    and Applications Published: 2015 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: Performance Evaluation of Microservices Architectures Using Containers
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icsaw.2017.11
  analysis: '>'
  authors:
  - Wilhelm Hasselbring
  - Guido Steinacker
  citation_count: 111
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2017
    IEEE International Confe... Microservice Architectures for Scalability, Agility
    and Reliability in E-Commerce Publisher: IEEE Cite This PDF Wilhelm Hasselbring;
    Guido Steinacker All Authors 83 Cites in Papers 1 Cites in Patent 6221 Full Text
    Views Abstract Document Sections I. Introduction II. Microservices at Otto.de
    III. Conclusions and Take Away Authors Figures References Citations Keywords Metrics
    Abstract: Microservice architectures provide small services that may be deployed
    and scaled independently of each other, and may employ different middleware stacks
    for their implementation. Microservice architectures intend to overcome the shortcomings
    of monolithic architectures where all of the application''s logic and data are
    managed in one deployable unit. We present how the properties of microservice
    architectures facilitate scalability, agility and reliability at otto.de, which
    is one of the biggest European e-commerce platforms. In particular, we discuss
    vertical decomposition into self contained systems and appropriate granularity
    of microservices as well as coupling, integration, scalability and monitoring
    of microservices at otto.de. While increasing agility to more than 500 live deployments
    per week, high reliability is achieved by means of automated quality assurance
    with continuous integration and deployment. Published in: 2017 IEEE International
    Conference on Software Architecture Workshops (ICSAW) Date of Conference: 05-07
    April 2017 Date Added to IEEE Xplore: 26 June 2017 ISBN Information: DOI: 10.1109/ICSAW.2017.11
    Publisher: IEEE Conference Location: Gothenburg, Sweden I. Introduction Traditionally,
    information system integration aims at achieving high data coherence among heterogeneous
    information sources [1], [2]. However, a great challenge with integrated databases
    is the inherently limited horizontal scalability of transactional database management
    [3]. One of the intentions of microservice architectures is to overcome the limited
    scalability of such monolithic architectures. A system has a microservice architecture
    when that system is composed of many collaborating microservices; typically without
    centralized control [4]. Microservices are built around business capabilities
    and take a full-stack implementation of software for that business area. The following
    topics are of eminent relevance. Sign in to Continue Reading Authors Figures References
    Citations Keywords Metrics More Like This Empirical Study on Recognition of Project
    Situations by Monitoring Application Results of Software Reliability Growth Model
    2017 IEEE International Symposium on Software Reliability Engineering Workshops
    (ISSREW) Published: 2017 Online Monitoring of Software System Reliability 2010
    European Dependable Computing Conference Published: 2010 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Microservice Architectures for Scalability, Agility and Reliability in E-Commerce
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/iiswc.2016.7581269
  analysis: '>'
  authors:
  - Takanori Ueda
  - Takuya Nakaike
  - Maureen O’Hara
  citation_count: 90
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2016
    IEEE International Sympo... Workload characterization for microservices Publisher:
    IEEE Cite This PDF Takanori Ueda; Takuya Nakaike; Moriyoshi Ohara All Authors
    72 Cites in Papers 1 Cites in Patent 3518 Full Text Views Abstract Document Sections
    I. Introduction II. Background III. Measurement Methodology IV. Performance Overview
    V. Hardware-Level Analysis Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: The microservice architecture is a new framework to
    construct a Web service as a collection of small services that communicate with
    each other. It is becoming increasingly popular because it can accelerate agile
    software development, deployment, and operation practices. As a result, cloud
    service providers are expected to host an increasing number of microservices that
    can generate significant resource pressure on the cloud infrastructure. We want
    to understand the characteristics of microservice workloads to design an infrastructure
    optimized for microservices. In this paper, we used Acme Air, an open-source benchmark
    for Web services, and analyzed the behavior of two versions of the benchmark,
    microservice and monolithic, for two widely used language runtimes, Node.js and
    Java. We observed a significant overhead due to the microservice architecture;
    the performance of the microservice version can be 79.2% lower than the monolithic
    version on the same hardware configuration. On Node.js, the microservice version
    consumed 4.22 times more time in the libraries of Node.js than the monolithic
    version to process one user request. On Java, the microservice version also consumed
    more time in the application server than the monolithic version. We explain these
    performance differences from both hardware and software perspectives. We discuss
    the network virtualization in Docker, an infrastructure for microservices that
    has nonnegligible impact on performance. These findings give clues to develop
    optimization techniques in a language runtime and hardware for microservice workloads.
    Published in: 2016 IEEE International Symposium on Workload Characterization (IISWC)
    Date of Conference: 25-27 September 2016 Date Added to IEEE Xplore: 10 October
    2016 ISBN Information: DOI: 10.1109/IISWC.2016.7581269 Publisher: IEEE Conference
    Location: Providence, RI, USA I. Introduction As an increasing number of companies
    provide their services through the cloud, the development speed of Web services
    has a direct impact on business outcomes. Developers use agile development [16]
    with DevOps to accelerate Web-service deliverables. With the widespread use of
    agile development, the microservice architecture proposed by James Lewis and Martin
    Fowler [17] is attracting much attention. The microservice architecture is a design
    methodology for building a Web service with “a suite of small services”. Each
    service communicates with other services “with lightweight mechanisms, often an
    HTTP resource API”. A recent common style in microservice development involves
    a developer building a Web service as a Docker container image on his/her laptop
    then transfers the image to a production cloud. A Docker container image includes
    all the libraries that the service needs to run. With the isolation mechanism
    of the container technology, developers can avoid conflicts among the runtime
    environments, even when they deploy multiple microservices on the same machine.
    Furthermore, the microservice architecture enables developers to continuously
    update an Web service without shutting down the entire service. Web-service developers
    are eager to adopt the microservice architecture to accelerate agile software
    development and DevOps. Sign in to Continue Reading Authors Figures References
    Citations Keywords Metrics More Like This Design and Implementation of the Container
    Terminal Operating System Based on Service-Oriented Architecture (SOA) 2008 International
    Conference on Cyberworlds Published: 2008 A framework for runtime V&V in business-critical
    Service Oriented Architectures 2013 43rd Annual IEEE/IFIP Conference on Dependable
    Systems and Networks Workshop (DSN-W) Published: 2013 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Workload characterization for microservices
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3106237.3106270
  analysis: '>'
  authors:
  - Michael Hilton
  - Nicholas Nelson
  - Timothy Tunnell
  - Darko Marinov
  - Danny Dig
  citation_count: 127
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesFSEProceedingsESEC/FSE
    2017Trade-offs in continuous integration: assurance, security, and flexibility
    RESEARCH-ARTICLE PUBLIC ACCESS SHARE ON Trade-offs in continuous integration:
    assurance, security, and flexibility Authors: Michael Hilton , Nicholas Nelson
    , Timothy Tunnell , + 2 Authors Info & Claims ESEC/FSE 2017: Proceedings of the
    2017 11th Joint Meeting on Foundations of Software EngineeringAugust 2017Pages
    197–207https://doi.org/10.1145/3106237.3106270 Published:21 August 2017Publication
    History 120 citation 2,483 Downloads eReaderPDF ESEC/FSE 2017: Proceedings of
    the 2017 11th Joint Meeting on Foundations of Software Engineering Trade-offs
    in continuous integration: assurance, security, and flexibility Pages 197–207
    Previous Next ABSTRACT References Cited By Index Terms Recommendations Comments
    ABSTRACT Continuous integration (CI) systems automate the compilation, building,
    and testing of software. Despite CI being a widely used activity in software engineering,
    we do not know what motivates developers to use CI, and what barriers and unmet
    needs they face. Without such knowledge, developers make easily avoidable errors,
    tool builders invest in the wrong direction, and researchers miss opportunities
    for improving the practice of CI. We present a qualitative study of the barriers
    and needs developers face when using CI. We conduct semi-structured interviews
    with developers from different industries and development scales. We triangulate
    our findings by running two surveys. We find that developers face trade-offs between
    speed and certainty (Assurance), between better access and information security
    (Security), and between more configuration options and greater ease of use (Flexi-
    bility). We present implications of these trade-offs for developers, tool builders,
    and researchers. References Shay Artzi, Julian Dolby, Simon Holm Jensen, Anders
    Møller, and Frank Tip. 2011. A Framework for Automated Testing of JavaScript Web
    Applications. In ICSE. Alberto Bacchelli and Christian Bird. 2013. Expectations,
    Outcomes, and Challenges of Modern Code Review. In ICSE. Kent Beck. 1999. Embracing
    Change with Extreme Programming. IEEE Computer (1999). Show All References Cited
    By View all Index Terms Trade-offs in continuous integration: assurance, security,
    and flexibility Software and its engineering Software creation and management
    Software development process management Software development methods Agile software
    development Software verification and validation Software defect analysis Software
    testing and debugging Recommendations Usage, costs, and benefits of continuous
    integration in open-source projects ASE ''16: Proceedings of the 31st IEEE/ACM
    International Conference on Automated Software Engineering Continuous integration
    (CI) systems automate the compilation, building, and testing of software. Despite
    CI rising as a big success story in automated software engineering, it has received
    almost no attention from the research community. For example, ... Read More Enabling
    Agile Testing through Continuous Integration AGILE ''09: Proceedings of the 2009
    Agile Conference A Continuous Integration system is often considered one of the
    key elements involved in supporting an agile software development and testing
    environment. As a traditional software tester transitioning to an agile development
    environment it became clear ... Read More The effects of continuous integration
    on software development: a systematic literature review AbstractContext Continuous
    integration (CI) is a software engineering technique that proclaims frequent activities
    to assure the software product health. Researchers and practitioners mention several
    benefits related to CI. However, no systematic study ... Read More Comments 54
    References View Table Of Contents Footer Categories Journals Magazines Books Proceedings
    SIGs Conferences Collections People About About ACM Digital Library ACM Digital
    Library Board Subscription Information Author Guidelines Using ACM Digital Library
    All Holdings within the ACM Digital Library ACM Computing Classification System
    Digital Library Accessibility Join Join ACM Join SIGs Subscribe to Publications
    Institutions and Libraries Connect Contact Facebook Twitter Linkedin Feedback
    Bug Report The ACM Digital Library is published by the Association for Computing
    Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: 'Trade-offs in continuous integration: assurance, security, and flexibility'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3104028
  analysis: '>'
  authors:
  - Claus Pahl
  - Pooyan Jamshidi
  - Olaf Zimmermann
  citation_count: 81
  full_citation: '>'
  full_text: '>

    skip to main content University of Nebraska Lincoln Browse About Sign in Register
    Journals Magazines Proceedings Books SIGs Conferences People Search ACM Digital
    Library Advanced Search Journal Home Just Accepted Latest Issue Archive Authors
    Editors Reviewers About Contact Us HomeACM JournalsACM Transactions on Internet
    TechnologyVol. 18, No. 2Architectural Principles for Cloud Software RESEARCH-ARTICLE
    SHARE ON Architectural Principles for Cloud Software Authors: Claus Pahl , Pooyan
    Jamshidi , Olaf Zimmermann Authors Info & Claims ACM Transactions on Internet
    TechnologyVolume 18Issue 2Article No.: 17pp 1–23https://doi.org/10.1145/3104028
    Published:02 February 2018Publication History 72 citation 2,143 Downloads eReaderPDF
    ACM Transactions on Internet Technology Volume 18, Issue 2 Previous Next Abstract
    References Cited By Index Terms Recommendations Comments Skip Abstract Section
    Abstract A cloud is a distributed Internet-based software system providing resources
    as tiered services. Through service-orientation and virtualization for resource
    provisioning, cloud applications can be deployed and managed dynamically. We discuss
    the building blocks of an architectural style for cloud-based software systems.
    We capture style-defining architectural principles and patterns for control-theoretic,
    model-based architectures for cloud software. While service orientation is agreed
    on in the form of service-oriented architecture and microservices, challenges
    resulting from multi-tiered, distributed and heterogeneous cloud architectures
    cause uncertainty that has not been sufficiently addressed. We define principles
    and patterns needed for effective development and operation of adaptive cloud-native
    systems. References A. Ahmad, P. Jamshidi, and C. Pahl. 2014. Classification and
    comparison of architecture evolution reuse knowledge – A systematic review. J.
    Softw.: Evol. Process 26, 7 (2014), 654--691. N. Antonopoulos and L. Gillam. 2010.
    Cloud Computing: Principles, Systems and Applications. Springer. H. Arabnejad,
    P. Jamshidi, G. Estrada, N. El Ioini, and C. Pahl. 2016. An auto-scaling cloud
    controller using fuzzy Q-learning—Implementation in openstack. In Proceedings
    of the European Conference on Service-Oriented and Cloud Computing (ESOCC’16).
    Show All References Cited By View all Kumar M and Choppella V. Enhancing MVC architecture
    pattern description using its System of Systems model. Proceedings of the 17th
    Innovations in Software Engineering Conference. (1-11). https://doi.org/10.1145/3641399.3641410
    Khan H, Ali F and Nazir S. (2024). Systematic analysis of software development
    in cloud computing perceptions. Journal of Software: Evolution and Process. 36:2.
    Online publication date: 13-Feb-2024. https://doi.org/10.1002/smr.2485 Henning
    S and Hasselbring W. (2024). Benchmarking scalability of stream processing frameworks
    deployed as microservices in the cloud. Journal of Systems and Software. 208:C.
    Online publication date: 1-Feb-2024. https://doi.org/10.1016/j.jss.2023.111879
    Show All Cited By Index Terms Architectural Principles for Cloud Software Software
    and its engineering Software notations and tools Development frameworks and environments
    Software organization and properties Contextual software domains Software infrastructure
    Software system structures Distributed systems organizing principles Cloud computing
    Software architectures Recommendations Architectural Requirements for Cloud Computing
    Systems: An Enterprise Cloud Approach Cloud Computing is a model of service delivery
    and access where dynamically scalable and virtualized resources are provided as
    a service over the Internet. This model creates a new horizon of opportunity for
    enterprises. It introduces new operating and ... Read More DevOps patterns to
    scale web applications using cloud services SPLASH ''13: Proceedings of the 2013
    companion publication for conference on Systems, programming, & applications:
    software for humanity Scaling a web applications can be easy for simple CRUD software
    running when you use Platform as a Service Clouds (PaaS). But if you need to deploy
    a complex software, with many components and a lot users, you will need have a
    mix of cloud services in ... Read More Architectural Strategies for Green Cloud
    Computing: Environments, Infrastructure and Resources Opportunities for improving
    IT efficiency and performance through centralization of resources have increased
    dramatically in the past few years with the maturation of technologies, such as
    service oriented architecture, virtualization, grid computing, ... Read More Comments
    55 References View Issue’s Table of Contents Footer Categories Journals Magazines
    Books Proceedings SIGs Conferences Collections People About About ACM Digital
    Library ACM Digital Library Board Subscription Information Author Guidelines Using
    ACM Digital Library All Holdings within the ACM Digital Library ACM Computing
    Classification System Digital Library Accessibility Join Join ACM Join SIGs Subscribe
    to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin
    Feedback Bug Report The ACM Digital Library is published by the Association for
    Computing Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy
    Code of Ethics Feedback'
  inline_citation: '>'
  journal: ACM transactions on Internet technology
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Architectural Principles for Cloud Software
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.jss.2019.01.001
  analysis: '>'
  authors:
  - Paolo Di Francesco
  - Patricia Lago
  - Ivano Malavolta
  citation_count: 153
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Architecting with microservices
    3. Study design 4. Results - Publication trends (RQ1) 5. Results - research focus
    (RQ2) 6. Results - potential for industrial adoption (RQ3) 7. Orthogonal results
    8. Threats to validity 9. Related work 10. Conclusions Appendix A. Primary studies
    References Vitae Show full outline Cited by (164) Figures (24) Show 18 more figures
    Tables (3) Table 1 Table 2 Table A.1 Journal of Systems and Software Volume 150,
    April 2019, Pages 77-97 Architecting with microservices: A systematic mapping
    study Author links open overlay panel Paolo Di Francesco a, Patricia Lago b, Ivano
    Malavolta b Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.jss.2019.01.001
    Get rights and content Highlights • Classification of 103 primary studies on architecting
    with microservices (MSA). • A reusable classification framework for classifying
    and evaluating MSA solutions. • An up-to-date map of the state of the art in architecting
    with microservices. • An evaluation of the potential for broad industrial adoption
    of existing research. • An evidence-based discussion of the emerging research
    trends, patterns, and gaps. Abstract Context A microservice architecture is composed
    of a set of small services, each running in its own process and communicating
    with lightweight mechanisms. Many aspects on architecting with microservices are
    still unexplored and existing research is still far from being crispy clear. Objective
    We aim at identifying, classifying, and evaluating the state of the art on architecting
    with microservices from the following perspectives: publication trends, focus
    of research, and potential for industrial adoption. Method We apply the systematic
    mapping methodology. We rigorously selected 103 primary studies and we defined
    and applied a classification framework to them for extracting key information
    for subsequent analysis. We synthesized the obtained data and produced a clear
    overview of the state of the art. Results This work contributes with (i) a classification
    framework for research studies on architecting with microservices, (ii) a systematic
    map of current research of the field, (iii) an evaluation of the potential for
    industrial adoption of research results, and (iv) a discussion of emerging findings
    and implications for future research. Conclusion This study provides a solid,
    rigorous, and replicable picture of the state of the art on architecting with
    microservices. Its results can benefit both researchers and practitioners of the
    field. Previous article in issue Next article in issue Keywords MicroservicesSoftware
    architectureSystematic mapping study 1. Introduction Amazon, Netflix, LinkedIn,
    Spotify, SoundCloud and other companies (Fowler, Lewis, Villamizar, Garcés, Castro,
    Verano, Salamanca, Casallas, Gil, 2015, Yahia, Réveillère, Bromberg, Chevalier,
    Cadot, 2016) have evolved their applications towards a microservice architecture
    (MSA). The most acknowledged definition of the microservices architectural style
    is the one provided by Fowler and Lewis (2014), which describes it as an approach
    for developing a single application as a suite of small services, each running
    in its own process and communicating with lightweight mechanisms, often an HTTP
    resource API. Recently, the microservice architectural style has received significant
    attention from a research point of view. However, as of today it is difficult
    for both researchers and practitioners to have a clear view of existing research
    solutions for architecting with microservices. The goal of this paper is to characterize
    the current state of the art for understanding what we know about scientific research
    on architecting with microservices. For achieving this goal we designed and conducted
    a systematic mapping study methodology. Specifically, we select 103 primary studies
    from 532 potentially relevant papers, we rigorously define a classification framework
    for categorizing research results on architecting with microservices, and we apply
    it to the 103 primary studies. Finally, we synthesize the obtained data to produce
    a clear overview of the state of the art in architecting with microservices. Also,
    we assess how research results on architecting with microservices can be potentially
    transferred and adopted in industrial projects. This assessment can play the role
    of reference framework for acting towards a smoother transfer of research results
    to practice. The main contributions of this study include: • a reusable framework
    for classifying, comparing, and evaluating architectural solutions, methods, and
    techniques (e.g., tactics, patterns, styles, views, models, reference architectures,
    or architectural languages) specific for microservices; • an up-to-date map of
    the state of the art in architecting with microservices; • an evaluation of the
    potential for industrial adoption of existing research results on architecting
    with microservices; • an evidence-based discussion of the emerging research trends,
    patterns, and gaps, and their implications for future research on architecting
    with microservices. This study is an extended version of our previous research
    on architecting with microservices (Di Francesco et al., 2017b). The novelties
    added in this study are: (i) the extension of the set of primary studies from
    71 to 103 entries because now we cover publications until the beginning of May
    2017, (ii) a more in-depth elaboration of the extracted data, (iii) an orthogonal
    analysis about the potential interactions between various parameters of the classification
    framework, (iv) the analysis of the research trends over the years. The audience
    of this study is composed of both (i) researchers interested to investigate on
    the microservices architectural style, and (ii) practitioners willing to understand
    and adopt existing research on architecting with microservices. The remainder
    of this paper is organized as follows. In Section 2 we provide basic concepts
    about architecting with microservices. In Section 3 we present the design of the
    study. The elaborated results are reported in 4 Results - Publication trends (RQ1),
    5 Results - research focus (RQ2), and 6. Section 7 discusses the orthogonal findings
    of the study. Threats to validity and related work are described in Sections 8
    and 9, respectively. Section 10 closes the paper. 2. Architecting with microservices
    While there has not been a wide acceptance of a specific definition, a popular
    one was provided by Lewis and Fowler, which define the microservice architectural
    style as an approach to developing a single application as a suite of small services
    each running in its own process and communicating with lightweight mechanisms,
    often an HTTP resource API (Fowler and Lewis, 2014). Recurrent characteristics
    of the microservice architectural style are: (i) organization of the system around
    business capability, (ii) automated deployment, (iii) intelligence in the endpoints,
    and (iv) decentralized control of languages and data. This style allows to design
    architectures that should be flexible, modular and easy to evolve. Microservice
    architectures can provide significant benefits. Among the important ones, there
    is the possibility to design, develop, test and release services with great agility.
    Infrastructure automation allows to reduce the manual effort involved in building,
    deploying and operating microservices, thus enabling continuous delivery. Decentralized
    governance and data management allow services to be independent, and avoid an
    application to standardize on a single technology. Microservice architectures
    are particularly suitable for cloud infrastructures, as they greatly benefit from
    cloud-enabled elasticity and rapid provisioning of resources. Architecting with
    microservices, however, is not an easy task as it requires to manage a distributed
    architecture and its challenges, which include network latency and unreliability,
    fault tolerance, complex services’ orchestration, data consistency and transaction
    management, and load balancing. Cloud infrastructures and new technologies play
    a fundamental role for realizing microservice architectures and managing the associated
    challenges and complexities. An illustrative example of a microservice-based architecture
    is Netflix.1 Netflix is implemented as an ecosystem of small, independently deployable,
    and independently scalable microservices. When requests come from client-side
    devices (e.g., smartphones, TVs, laptops), they reach the Netflix API orchestration
    service, which acts as an API gateway towards the rest of the ecosystem (i.e.,
    it exposes a set of coarse-grained APIs and then routes incoming requests to the
    specific target microservices within the ecosystem). The Netflix API implements
    the logic to route, sequence, and parallelize incoming calls from the devices.
    Microservice-based systems must be designed to cope with failure (Fowler and Lewis,
    2014), meaning that the application must be able to tolerate possible service
    failures either by recovering as fast as possible or by gracefully degrading its
    functionalities. At Netflix, this is achieved by having each microservice manage
    its own layer of persistence (as independently as possible from the other microservices),
    by pushing as much as possible towards stateless microservices, relying on real-time
    monitoring, using libraries for fast recovery of services, and by implementing
    the so-called circuit breaker architectural pattern for avoiding cascading failures
    (e.g., via the open-source Hystrix2 library). Further details about the Netflix
    microservice-based architecture can be found in the Netflix technical blog3. 3.
    Study design In this research we follow the well-established guidelines for systematic
    mapping studies (Petersen, Vakkalanka, Kuzniarz, 2015, Kitchenham, Brereton, 2013).
    In this section we present the key aspects of the design of our study. 3.1. Goal
    and research questions The goal of this study is to identify, classify, and evaluate
    the trends, focus, and potential for industrial adoption of existing research
    in architecting with microservices from a researcher’s and practitioner’s point
    of view. This abstract goal can be refined into the following research questions.
    RQ1: What are the publication trends of research studies about architecting with
    microservices? Rationale: academic research is a dynamic ecosystem, where a multitude
    of researchers and research groups investigate on specific scientific problems
    over time with different degrees of independence and different methodologies.
    Relevance for researchers: the results of this research question help researchers
    in (i) quantifying the intensity of scientific interest on architecting with microservices,
    (ii) identifying the academic venues where related papers about architecting microservices
    are published, and (iii) identifying the academic venues where new results about
    architecting microservices may be better received (and appreciated) by the scientific
    community. Relevance for practitioners: the results of this research question
    help practitioners in identifying the relevant venues where scientific knowledge
    is created, so to (i) take inspiration for solving problems which have been already
    targeted by researchers, (ii) get a more orthogonal and cross-organizational perspective
    with respect to architecting with microservices, and (iii) identify the research
    groups which are prominently contributing in the field, so to catch future collaboration
    opportunities. RQ2: What is the focus of research on architecting with microservices?
    Rationale: architecting microservices is a multi-faceted research topic, where
    researchers can focus on very different aspects (e.g., continuous integration,
    performance analysis, security, deployment elasticity, monitoring and fault tolerance),
    applying very different research methodologies (industrial case studies, empirical
    evaluations, feasibility studies, etc.), and providing different types of contributions
    (e.g., specific architectural tactics, architectural languages, etc.). Relevance
    for researchers: by answering this research question we support researchers by
    providing (i) a solid foundation for classifying existing (and future) research
    on architecting with microservices and (ii) an understanding of current research
    gaps in the state of the art on architecting with microservices. Relevance for
    practitioners: the results of this research question help practitioners in (i)
    positioning themselves according to their organizational and technical needs (thanks
    to the classification framework) and (ii) effectively locate the research results
    which can be reused/customized for solving specific problems related to the microservices
    architectural style (e.g., how to efficiently and correctly perform integration
    testing of microservice-based systems). RQ3: What is the potential for industrial
    adoption of existing research on architecting with microservices? Rationale: while
    it is well known that microservices have their roots in industry, it is a fact
    that there are research groups focusing on them from an academic perspective.
    So it is natural to ask ourselves how the produced research findings and contributions
    can be actually transferred back to industry. Relevance for researchers: by answering
    this research question we support researchers by assessing how and if the current
    state of the art on architecting with microservices is ready to be transferred
    and adopted in industry. Moreover, the results of this research question will
    trigger a discussion about the next steps for successfully transferring research
    products on microservice architectures to industry. Relevance for practitioners:
    the results of this research question help practitioners in identifying those
    research products which are ready to be transferred to industry and which research
    groups are already collaborating with industry. Also, the results of our study
    support practitioners in identifying the solutions which are supported by (open-source)
    tools, and thus are one step closer to their application into an industrial context.
    3.2. Search and selection process In the following we present the stages of our
    search and selection process (see Fig. 1). Download : Download high-res image
    (305KB) Download : Download full-size image Fig. 1. Overview and numbers of the
    search and selection process. 1. Initial search. In this stage we performed4 automatic
    searches on electronic databases and indexing systems. The selection of these
    electronic databases and indexing systems was guided by: (i) the fact that they
    are the largest and most complete scientific databases and indexing systems in
    software engineering (Petersen, Vakkalanka, Kuzniarz, 2015, Kitchenham, Brereton,
    2013), (ii) the fact that they have been recognised as being an effective means
    to conduct systematic literature studies in software engineering (Petersen et
    al., 2015), (iii) their high accessibility, and (iv) their ability to export search
    results to well-defined, computation-amenable formats. Our search string is shown
    in the listing below. For consistency, the search string has been applied to title,
    abstract and keywords of papers in all electronic databases and indexing systems
    considered in this research. 2. Impurity removal. Due to the nature of the involved
    data sources, search results included also elements that were clearly not research
    papers, such as international standards, textbooks, book series, etc. In this
    stage we manually removed such impurius results from our set 3. Merging and duplicates
    removal. In this stage all relevant results from the first stage have been combined
    together into a single dataset. 4. Application of selection criteria. We considered
    all the selected studies and filtered them according to the following inclusion
    and exclusion criteria. I1 - Studies focussing on architectural solutions, methods
    or techniques (e.g., tactics, patterns, styles, views, models, reference architectures,
    or architectural languages) specific for microservices. I2 - Studies providing
    an evaluation of the architectural solution, method or technique (e.g., via formal
    analysis, controlled experiment, exploitation in industry, simple examples, etc.).
    I3 - Studies subject to peer review. I4 - Studies written in English. E1 - Studies
    that, while focusing on microservices, do not explicitly deal with their architecture
    (e.g., studies focussing only on low-level technological aspects, the inner details
    of microservices, etc.). E2 - Studies where microservices are only used as an
    example. E3 - Secondary or tertiary studies (e.g., systematic literature reviews,
    surveys, etc.). E4 - Studies in the form of tutorial papers, editorials, etc.
    because they do not provide enough information. E5 - Studies not available as
    full-text. Even if secondary studies have been excluded because of the E3 exclusion
    criterion, we considered them in our study for: (i) checking the completeness
    of our set of primary studies (i.e., if any relevant paper was missing from this
    study); (ii) identifying important issues to be analysed in this study; (iii)
    defining what is the contribution of this study to the literature (see Section
    9). 5. Snowballing. In this phase we complemented the automatic search with a
    closed recursive backward and forward snowballing activity (Wohlin, 2014). In
    the backward snowballing we focussed on all the references of each considered
    study, whereas for the forward snowballing Google Scholar has been used to obtain
    those studies citing the current one (Wohlin, 2014). 6. Combination. If there
    were multiple papers on the same study, we kept a record of all of them and pointed
    them to a single study. For example, if a primary study was published in more
    than one paper (e.g., a conference paper extended to a journal version), only
    one instance has been considered as a primary study. Generally, the journal version
    has been preferred, since more complete, but both versions have been used in the
    data extraction and analysis of the publication trends (RQ1) phases. This step
    is necessary for ensuring completeness and traceability of the results (Wohlin
    et al., 2012). 3.3. Data extraction In order to have a rigorous data extraction
    process and to ease the management of the extracted data, a well-structured classification
    framework has been rigorously designed. The resulting classification framework
    is shown in Fig. 2 and it is composed of three facets, each of them addressing
    its corresponding research question. In the following we describe each facet of
    our classification framework. Publications trends (RQ1). The parameters we considered
    to collect data about publication trends are: publication year, publication venue
    (e.g., conference, journal, etc.), and research strategy (e.g., solution proposal,
    opinion paper, etc.). Focus of research (RQ2). We followed a systematic process
    called keywording for defining the categories of this facet of our classification
    framework. Goal of the keywording process is to effectively develop a classification
    framework so that it fits the primary studies and takes their research focus into
    account (Petersen et al., 2008). The following details each step of the process
    depicted in Fig. 3: 1. Identify starting set of studies. Two researchers (R1 and
    R2) randomly extracted 5 studies, which have been used as pilot studies. 2. Identify
    keywords and concepts. Three researchers (R1, R2, and R3) collected keywords and
    concepts by reading the full-text of each starting study. 3. Cluster keywords
    and form categories. Two researchers (R1 and R2) clustered the collected keywords
    and concepts into a set of emerging categories. The output of this stage is the
    initial version of the classification framework. Examples of emerging categories
    include: supported architecting activities, scope in the software lifecycle, considered
    design patterns, considered quality attributes (e.g., performance, reliability,
    security), etc. Next steps have been performed for each primary study. 4. Extract
    data from current study. A researcher (R1) extracted information about the current
    primary study to be analysed and (i) collected information according to the parameters
    of the classification framework and (ii) collected any kind of additional information
    that was considered relevant and that did not fit within any parameter of the
    classification framework. If the collected information about the current primary
    study fit completely within the classification framework, then we proceeded to
    analyze the next primary study, otherwise the classification framework was refined
    (this step involved three researchers, R1, R2, and R3)). 5. Refine comparison
    framework. Two researchers (R2 and R3) discussed together on the collected additional
    information. This discussion could result either in the correction of the performed
    classification or in the refinement of the classification framework in order to
    make it a better fit with the primary studies. The above described process ended
    when no primary study to analyze was left. The specific parameters emerging from
    the keywording process are independent from each other and have been extracted
    independently; they are described in Section 5. Finally, in this phase we agreed
    that 9 analysed studies were semantically out of the scope of this research, so
    they have been excluded. Potential for industrial adoption (RQ3). In order to
    analyse the potential for industrial adoption of microservices, we have classified
    and extracted five different parameters: (i) readiness level for assessing the
    maturity of the involved technologies, (ii) industry involvement for understanding
    how academic and industrial researchers collaborate on the topic, (iii) tool support
    for distinguishing between software-based or knowledge-based contributions, (iv)
    open-source test system for identifying existing benchmarks for microservice architectures,
    and (v) number of microservices used for evaluation. 3.4. Data synthesis Our data
    synthesis activity can be divided into three main phases: vertical analysis, trend
    analysis, and horizontal analysis. When performing vertical analysis, we analyzed
    the extracted data to find trends and collect information about each parameter
    of our classification framework. When performing trend analysis, we focussed on
    how each possible value of all parameters of the classification framework evolves
    over time. When performing horizontal analysis, we used contingency tables for
    evaluating the actual existence of relations across different parameters of the
    classification framework, we made comparisons between pairs of parameters, and
    we identified perspectives of interest. In those phases we performed a combination
    of content analysis (for categorizing and coding the studies under broad thematic
    categories) and narrative synthesis (for explaining in details and interpreting
    the findings coming from the content analysis). 3.5. Replicability of the study
    To allow easy replication and verification of our study, a complete replication
    package  Di Francesco et al. (2017a) is publicly available to interested researchers.
    Our replication package includes: the detailed research protocol of this study,
    a document providing a precise definition of all the parameters of the classification
    framework, the list of all selected studies, raw data for each phase of the study,
    and the R scripts for checking, analyzing, and visualizing the extracted data.
    4. Results - Publication trends (RQ1) In this section we present the results we
    obtained when analyzing the publication trends on architecting with microservices.
    In order to provide a complete picture about the number and types of publications
    on the topic, in this section we consider all the selected publications, independently
    of the combination step we performed during the search and selection process (see
    Section 3.2). More specifically, for answering RQ1 we consider the total set of
    119 primary studies, which includes both the entire set of 103 primary studies
    and the 16 primary studies resulting from the combination activity. 4.1. Obtained
    results (RQ1) Publication years. Fig. 4 presents the distribution of publications
    on architecting with microservices over the years. The year 2017 is highlighted
    with a grey background to remark that data within this period is only partial,
    as the search and selection process was performed in May 2017. The Figure emphasizes
    a clear confirmation of the scientific interest on architecting with microservices
    in the years 2015 through 2017. A very small number of publications have been
    produced until 2014, which is actually the first year in which (i) microservices
    started to attract the interest of large organizations, and (ii) the term microservice
    as architectural style was consistently used (Pahl and Jamshidi, 2016). As a confirmation,
    even if the six studies published before 2014 were about systems composed of small-scale
    lightweight services (P9, P60, P61, P62, P104, P105), they were referring to slightly
    different perspectives on microservices as they are considered today. For example,
    P9 considers microservices as low-level software components in the robotic domain,
    whereas P60 considers microservices as mobile services generated by end-users.
    Year 2015 signed a booming in the research field of architecting with microservices,
    with the trend increasing in 2016 and still growing in the first months of the
    year 2017.5 Publication types. Fig. 4 shows the publication types of the primary
    studies over the years. The high number of conference and journal papers indicate
    that architecting with microservices is progressing as research topic despite
    its relative young age; the relatively low number of workshop papers indicate
    that researchers commonly target more scientifically-rewarding publication types
    (like journals and conferences) when working on architecting with microservices.
    Publication venues. We can observe an extreme fragmentation in terms of publication
    venues, where research on architecting with microservices is spread across 91
    venues spanning different research areas like cloud infrastructures, software
    engineering, software services, autonomic computing, software maintenance, etc.
    This result indicates that architecting with microservices is considered as an
    orthogonal research target with many cross-cutting concerns. In Table 1 the most
    targeted publication venues are reported. We can notice that researchers are mainly
    targeting specialized venues on architecting with microservices (i.e., AMS), cloud
    computing venues (i.e., IEEECC) and software architecture venues (i.e., ICSA).
    Researchers and practitioners can consider those venues as their starting points
    for their exploration into the state of the art on architecting with microservices.
    Table 1. Publication venues. Research strategies. Since this parameter is general
    and independent from the research area, we reuse the comparison of research approaches
    proposed by Wieringa et al. (2006). We chose this comparison because (i) it has
    been widely used in various systematic mapping studies (e.g., Engström, Runeson,
    2011, Mehmood, Jawawi, 2013, Petersen, 2011), and (ii) its categories are quite
    cost-effective to be identified by reading a paper without going into its very
    details (Petersen et al., 2008). As shown in Fig. 5, here the clear winner is
    solution proposal (86/119). This result is due to the fact that the microservice
    architectural style is still in its infancy (we recall here that its first well
    acknowledged definition has been provided only in 2014) and not yet consolidated
    in any (not even de facto) standards. This results in a large number of researchers
    trying to propose their own solutions for either recurrent or specific problems
    (see Section 5.1 for the details on which problems are targeted). Validation research
    (43/119) is the second most recurrent research strategy, highlighting the fact
    that researchers are actually providing some level of evidence about their proposed
    solutions, e.g., by simulations, in-the-lab experiments, prototypes, etc. At the
    other end of the spectrum, evaluation research is performed very rarely (1/119),
    meaning that industry- and practitioners-oriented studies (e.g., industrial case
    studies, action research, practitioner-targeted surveys) are not yet in the main
    focus of researchers today. Specifically, in P34 a case study conducted in a software
    company has been presented; in this context, the authors developed a Java application
    using both the monolithic approach and the microservice pattern. The fact that
    evaluation research is rarely performed has a negative impact on the potential
    for transferring current research results in industry. This suggests a gap that
    should be filled by future research on architecting with microservices, especially
    if we want to either (i) solve real problems coming from industrial scenarios,
    or (ii) push further the technology transfer of research results in industry.
    4.2. Trend analysis (RQ1) In this section we report our analysis of the research
    trends over the years for the parameters related to RQ1. We have analysed the
    research trends for all the parameters of the classification framework. However,
    some trends have been already discussed during the vertical analysis (e.g., the
    spike of conference publications in 2015 and 2016, see Fig. 6(a)) or do not have
    enough data points (e.g., the use of architectural languages – see Section 5.2),
    so in the following we focus exclusively on the most essential aspects we could
    observe. For the sake of completeness, we include in the replication package (Di
    Francesco et al., 2017a) of our study the figures showing the research trends
    for all parameters of the classification framework. In each figure, we have highlighted
    with a grey background the year 2017 to recall that the data within this period
    is partial, as the search and selection process was performed in May 2017. For
    what concerns research strategies, we observe a growth of solution proposals from
    2014. Studies proposing validation research are following the trend of solution
    proposals, but with a lower magnitude. Unfortunately, as previously discussed,
    evaluation research (i.e., the one involving industry- and practioners-oriented
    research methodologies) is still lagging behind and its trend over the years seems
    not to be very promising. This confirms the urgency to fill the gap with respect
    to the industrial relevance of the performed evaluations. Main findings: ▸ Year
    2015 signed a booming monotonic increase in publication numbers with particular
    interest in conferences and journals (both increasing). ▸ The field is rooted
    in practice: publication venues are scattered across specific topics or application
    domains, and most publications propose specific solutions and validations thereof.
    ▸ Only one study applied industry- and practioners-oriented research methodologies
    (e.g., industrial case studies, action research), leaving a gap with respect to
    the industrial relevance of the performed evaluations. 5. Results - research focus
    (RQ2) As described in Section 3.3, the part of classification framework related
    to RQ2 has been systematically defined. After this process we obtained two main
    categories related to the research focus on architecting with microservices, namely
    scope of the research (see Section 5.1) and support for architecting (see Section
    5.2). 5.1. Scope of the research With this category we provide information to
    help researchers and practitioners in putting into context research studies on
    architecting with microservices. In the following we discuss the obtained results.
    Target problems. Fig. 7 presents the problems targeted by the primary studies.
    The obtained results confirm that if on the one hand microservices can help in
    achieving a good level of flexibility (e.g., by promoting low services coupling,
    higher maintainability), on the other hand adopting a microservice-based architecture
    may bring higher complexity. Interestingly, the bottom area of Fig. 7 shows problems
    that are related to system-level aspects like time to market, low testability,
    low portability, and security. Moreover, only one paper (P64) is addressing the
    problem of benchmarking microservice-based applications. These aspects have been
    extensively investigated in the software architecture area, but are still new
    to microservice architectures; this result is an indicator of a potentially relevant
    research gap needing attention in the future. Research contribution. By referring
    to Fig. 8, the realization of microservice-based application and the consistent
    number of method studies may indicate that the complexity in the realization of
    these systems is still very high. Interestingly, few papers are investigating
    architectural languages and design patterns for microservices, unveiling interesting
    gaps to be filled by the research community. Main research area. The main research
    area is about the principal area of interest of the research to which the primary
    study belongs, e.g., cloud computing, system migration (see Fig. 9). The focus
    on the system quality (e.g., performance, maintainability) suggests that the microservice
    architectural style has direct impact on the design of a system and that researchers
    are still investigating how to leverage its characteristics. A significant attention
    is also given to the use of microservices in cloud environments. Not surprisingly,
    a significant number of studies are investigating migration techniques in order
    to adopt and benefit of microservices starting from the so-called monolithic applications.
    An industrial survey on the activities and the challenges of migrating towards
    microservices (Di Francesco et al., 2018) provides insights to this topic from
    an industrial perspective. If on the one side microservice architecture have been
    applied to recent technologies like Internet of Things, mobile apps, and other
    domain-specific fields as robotics (P9) and datacenters (P28), on the other side
    a significant number of studies are focusing on other research areas (not reported
    in the figure), such as microservice architecture recovery (P85), distinguishing
    characteristics between microservice- and service-oriented architectures (P77,
    P84), deployment cost models definition (P79). Abstraction layer. As shown in
    Fig. 13, microservices can run on top of (i) a physical machine running an operating
    system, (ii) a machine running a container engine, (iii) a machine running a virtualized
    environment (in this setting the hypervisor is mapped as operating system), or
    (iv) a machine running a container engine on top of a virtualized environment.
    As shown in Fig. 10, more than half of the studies focus on the microservice layer
    only, without considering any other layer. This result is also aligned with the
    recent advent of serverless functions running in the cloud, where the developer
    is asked to provide the business logic that should run in the cloud, whereas the
    operational overhead is taken care by the platform (e.g., AWS Lambda6). In this
    context, serverless platforms are able to transparently manage infrastructural
    and operations aspects of the system, such as its deployment and configuration,
    monitoring and logging (at different levels, like operating system, containers,
    communication, etc.), security facilities and patches, operating system, platforms,
    and libraries updates, management of the services lifecycle, services vertical
    and horizontal scaling, and so on. Differently, other studies not only focus on
    microservices, but also consider the environment as an important aspect of the
    architecture. More specifically, the container and the virtual machine layers
    were discussed respectively in 24 and 19 studies. This particular focus on containerization
    and virtualization confirms them as key enabling technologies for MSA. Moreover,
    a few studies (P16, P27, P39, P75) also consider the possibility to run containers
    on top of virtual machines, thus combining the resource utilization benefits of
    virtual machines and the portability and efficiency of containerization (Jaramillo
    et al., 2016), which seems to be particularly suitable for offering microservices
    on the IaaS cloud model (Khazaei et al., 2016). Software lifecycle scope. As shown
    in see Fig. 11, the number of studies on design is significantly higher than the
    number of those focusing on other lifecycle phases. In 32 primary studies the
    microservice architectural style is related with operations of deployment and
    configuration of the environment needed for the services in general. We have also
    investigated which studies relate the microservice architectural style with DevOps.
    Among the 103 primary studies, 30 of them discussed DevOps with two slightly different
    perspectives. On the one side, some consider DevOps as the set of practices intended
    to reduce the time between committing a change in the code base and rolling it
    out in production, while ensuring high quality (Bass et al., 2015). On the other
    side, DevOps deals with the goal of having the development and operations teams
    work closely together for achieving rapid and continuous release cycles (Fazio
    et al., 2016). A total of 32 studies explicitly discussed on the requirements
    of the microservice approach/application included in the study. This helps define
    the specific context information the microservice architectures are subject to.
    As an example, in P103 the authors discuss both functional and non-functional
    requirements when they use a microservice architecture to address key practical
    challenges in smart city platforms. Finally, given the trend in the scope of the
    studies, we conjecture that the areas of microservices maintenance and testing
    will attract further research when the fields of design, implementation and operation
    will gain more maturity. Microservice architecture definition. In the primary
    studies, microservice architectures have been defined in several ways and in some
    cases even more than one single definition was reported. As shown in Fig. 12,
    the most recurring definition was the one provided by Fowler and Lewis (2014),
    followed by the ones given by Newman (2015), and others. In 27 of the 103 studies,
    the authors have either provided their own definition of microservices or have
    used an informal definition. Nevertheless, the definitions provided by Lewis &
    Fowler and Newman seem to start prevailing over other definitions. Download :
    Download high-res image (66KB) Download : Download full-size image Listing 1.
    Search string used for automatic research studies. Download : Download high-res
    image (1MB) Download : Download full-size image Fig. 2. Classification framework.
    Download : Download high-res image (450KB) Download : Download full-size image
    Fig. 3. Keywording process and data extraction for RQ2. Download : Download high-res
    image (139KB) Download : Download full-size image Fig. 4. Distribution of primary
    studies by year and by type of publication. Download : Download high-res image
    (122KB) Download : Download full-size image Fig. 5. Research strategies. Download
    : Download high-res image (390KB) Download : Download full-size image Fig. 6.
    Trend analysis (RQ1). Download : Download high-res image (216KB) Download : Download
    full-size image Fig. 7. Target problems. Download : Download high-res image (109KB)
    Download : Download full-size image Fig. 8. Research contribution. Download :
    Download high-res image (111KB) Download : Download full-size image Fig. 9. Main
    research area. Download : Download high-res image (89KB) Download : Download full-size
    image Fig. 10. Abstraction layer. Download : Download high-res image (100KB) Download
    : Download full-size image Fig. 11. Software lifecycle scope. Download : Download
    high-res image (91KB) Download : Download full-size image Fig. 12. Microservice
    architecture definition. Download : Download high-res image (496KB) Download :
    Download full-size image Fig. 13. Abstraction layers. Download : Download high-res
    image (152KB) Download : Download full-size image Fig. 14. Architecting activities.
    Download : Download high-res image (123KB) Download : Download full-size image
    Fig. 15. Quality attributes. 5.2. Support for architecting We characterize primary
    studies with respect to how they support architecture-specific concerns and activities,
    such as design patterns, support for specific quality attributes, recurrent infrastructural
    services. Architecting activities. We have based our classification of architecting
    activities according to the introvert/extrovert nature of software architects
    discovered in Malavolta et al. (2013). The introvert nature regards the analysis
    and design of the software activities. It has been refined into the architecting
    activities defined by Li et al. (2013). The extrovert nature regards the communication
    between architects and other stakeholders. It has been further classified into
    the providing information and getting input parameters proposed by Kruchten (2008).
    Highlighted in darker gray in the figure, we can also observe how little investigation
    has been performed on extrovert architecting activities, i.e., providing information
    and getting input from other stakeholders of the system. From a research perspective,
    the low interest in these complementary activities indicates that there are areas
    of improvement in the engagement of customers and users, and also in the project
    management and communication with teams. Quality attributes. Fig. 15 shows that
    performance, maintainability, and functional suitability are by far the most investigated
    quality attributes, while the remaining qualities are almost equally represented.
    Among the primary studies discussing performance (59/103), we have classified
    which of them have a special focus on scalability. It resulted that scalability
    aspects are addressed in 36 out of 59 studies, suggesting that many researchers
    seem to consider scalability as a sub-problem of performance when architecting
    with microservices. Architecture provenance. According to our classification framework,
    an architecture is designed if it is created prior its implementation, otherwise
    it is considered as an extracted architecture. In Fig. 16, the overwhelming focus
    on design suggests that it is not easy to realize microservice architectures unless
    an actual analysis and design of the system is performed prior to its implementation.
    Download : Download high-res image (43KB) Download : Download full-size image
    Fig. 16. Architecture provenance. Architectural language. An architectural language
    can be considered as any form of expression used for architecture description,
    ranging from box-and-line informal notations, UML models, to more formal Architecture
    Description Languages (ADLs) (Malavolta et al., 2013). From the analysis of the
    primary studies has emerged that the majority of the proposed architectures were
    described using informal architectural languages, while in few cases UML was used.
    Interestingly, nine different languages were either used or proposed as suitable
    languages for modeling specific aspects of microservice architectures: BPMN (P32,
    P49), UML (P41), MicroART (P85), OCCIEx (P57), Medley (P50), KDM (P72), Diary
    (P97), Ciudad (P60), and Own-DSL (P64). It is interesting to notice that the Oasis
    Topology and Orchestration Specification for Cloud Applications (TOSCA) (2013)
    standard has not been used by any of the primary studies for designing microservice
    architectures. TOSCA is a standard that can be used for representing portable
    cloud applications and supporting their life-cycle management (Bergmayr et al.,
    2018), and is a promising candidate for the microservice architectures (Ruiu,
    Scionti, Nider, Rapoport, 2016, Lipton, Palma, Rutkowski, Tamburri, 2018, Shalom,
    2017). From a researcher’s point of view, the use of informal architectural languages
    and the lack of a predominant architectural language may lead to difficulties
    in the description and modeling of microservice architectures. We can conjecture
    that this concern can be addressed by working on a standard architecture language,
    which may help in having a shared common, industry-proven representation for microservice
    architectures. Proposing an architectural language for microservices helps architects
    in many activities; for example, it can help in reasoning about the system as
    a whole, performing analyses on the system qualities, coping with the dynamic
    and changing aspects of the application at runtime. Furthermore, an architectural
    language is a powerful communication instrument to enable better communication
    with both technical- (e.g., developers, architects) and non-technical stakeholders
    (e.g., customers and users) at the right level of abstraction and with a shared
    technical vocabulary. Architecture description types. Fig. 17 shows that the architectures
    proposed were mostly described in terms of their structural aspects, while the
    behavioral aspects were addressed less often. The major focus of researchers on
    static rather than dynamic aspects gives another perspective about certain types
    of challenges inherently related to the definition of the microservices, as for
    example finding the proper level of granularity of each service or migrating legacy
    systems. Download : Download high-res image (42KB) Download : Download full-size
    image Fig. 17. Architecture description types. Technology-specific. We have classified
    as technology-specific the studies proposing solutions, methods or techniques
    that are specific to one or more particular technologies (e.g., Docker). A total
    of 75 primary studies were not technology-specific, while the remaining 28 studies
    were technology-specific. The predominance of not technology-specific studies
    is a good indicator because approaches and solutions can be reused across technologies.
    Differently, technology-specific studies bear the advantage of being more detailed,
    but their applicability and portability in the future might be limited. In the
    set of technology-specific studies, Docker is clearly the most recurring technology
    (12/28) while other technologies are quite scattered, with a few occurrences of
    Java EE (P72, P78), Spring framework (P19, P63), Eureka (P19, P28), and other
    specific technologies (e.g., Serfnode (P31), KVM (P23)). Design patterns. Each
    design pattern has been identified as reported in the primary studies, thus each
    pattern has to be considered disjoint from the others (e.g., if the API Gateway
    is used for implementing the load balancing pattern, we report each as a separate
    pattern). The set of discussed patterns is reported in Fig. 18. The most recurring
    design patterns when architecting with microservices are: API gateway, Publish/subscribe,
    Proxy, Circuit breaker, and Discovery patterns. Download : Download high-res image
    (123KB) Download : Download full-size image Fig. 18. Design patterns. It is important
    to note that 7 primary studies (P10, P18, P29, P36, P42, P44, P48) have addressed
    or referred to a set of design patterns which have not been discussed in the other
    studies. In P10, four different patterns for implementing loose coupling in microservices
    are reported, namely: location independence, communication independence, security
    independence, and instance independence patterns. In P29, the authors report about
    the ports and adaptor pattern, also known as hexagonal architecture (Cockburn,
    2007), and the immutable server pattern (Morris, 2014). In P36 a set of data adapter
    patterns for working with data provision mechanisms are addressed. In P42, the
    bulkhead pattern is presented to support fault isolation within a microservice.
    In P44, cloud-focused patterns such as the Twelve-Factor App (Wiggins, 2014),
    and cloud computing patterns are referred. In P48, authors not only address several
    existing patterns, but they propose a new pattern called the database-is-the-service.
    Infrastructure services. These are the infrastructure services supporting non-functional
    tasks, as defined by Richards (2015). As shown in Fig. 19, microservice architectures,
    being inherently distributed, show a clear need for monitoring capabilities (e.g.,
    logging, profiling) but also for system level management (e.g., health management,
    autoscaling) in order to leverage the underlying infrastructure efficiently. A
    significant research interest is pointing to service brokering and service orchestration,
    which confirms that service management capabilities are fundamental to this area.
    Download : Download high-res image (142KB) Download : Download full-size image
    Fig. 19. Infrastructure services. 5.3. Trend analysis (RQ2) Fig. 20 summarizes
    our results for the trend analysis related to RQ2. In the following we will discuss
    only the most relevant trends. Download : Download high-res image (2MB) Download
    : Download full-size image Download : Download high-res image (327KB) Download
    : Download full-size image Fig. 20. Trend analysis (RQ2). When looking at the
    main research areas (see Fig. 20(c)), since 2015 we are seeing a growth of system-level
    quality, microservices in the cloud, and migration; we can conjecture that this
    trend will continue in the next years. Microservices in the context of mobile-enabled
    systems has a negative trend, meaning that in the last years researchers on architecting
    microservices seem to be less interested in microservices deployable on mobile
    devices in favour of microservices deployed in the back-end of mobile-based systems.
    This trend is in line with the classical definition of microservice, which is
    heavily influenced by concepts coming from containerization and cloud computing.
    Research on microservices has a clear trend when considering the software lifecycle
    scope (see Fig. 20(e)), researchers are increasingly focussing on the design of
    microservice-based systems, followed by operations and implementation. Interestingly,
    requirements is recently starting to attract researchers’ attention, hence suggesting
    that this trend might continue in the next years. Looking at Fig. 20(g) (architectural
    activities), it is interesting to observe a spike in the focus on architectural
    analysis in 2015 and 2016, unveiling the fact that researchers are devising and
    applying (new) architecture analysis techniques in the last years. We believe
    that reasoning at the architectural level of abstraction allows those techniques
    to be applicable on large scale systems like the microservice-based ones. It is
    clear that performance is the raising star in terms of quality attribute (see
    Fig. 20(h)), followed by a relatively strong interest over the years on maintainability
    and functional suitability. From the collected data, we can also observe that
    the scientific interest in security and usability is decreasing after 2015. We
    conjecture that the high degree of isolation provided by containers (e.g., by
    using Dockers namespaces) and resources limitations enforced in virtualized environments
    (e.g., limits for CPU load, I/O access, memory usage, and networking) may have
    played a role in this context. When looking at infrastructure services (Fig. 20(l))
    we notice that monitoring (e.g., logging, profiling) and system level management
    (e.g., autoscaling, load balancing) have the most prominent growth in the last
    years. These are clearly the two types of infrastructure services that are attracting
    the strongest attention of researchers. We expect that this trend will continue
    in the future. Main findings: ▸ Research scope involves problems that consolidate
    the need to master the tradeoffs between complexity and flexibility; here we can
    notice a strong focus on cloud and mobile paradigms, and legacy migration. Benchmarking
    is growing in 2017, potentially unveiling a promising future research direction.
    Requirements are starting to attract researchers’ attention in recent years. ▸
    Architecture analysis emerges as the most popular architecting activity. Results
    suggest software architecture as a powerful instrument for stakeholder engagement.
    Extrovert activities are raising since 2015, even if they are still not mainstream.
    ▸ The clear focus on infrastructure services has the potential to help devising
    new related patterns and styles and hence further leveraging cloud-based architecture
    models. In the set of investigated infrastructure services we observed that monitoring
    and system level management (e.g., health management, autoscaling, load balancing)
    have the most prominent growth in the last years. ▸ An industrial standard describing
    the architecture of microservice-based systems does not yet exist. If present,
    it could help support the architecting activities of microservice-based systems
    better. The most promising standard in this direction is the OASIS standard for
    Topology and Orchestration Specification for Cloud Applications (TOSCA) that,
    with the proper customization, could be used in the future to model microservice
    architectures (Lipton et al., 2018). 6. Results - potential for industrial adoption
    (RQ3) 6.1. Obtained results (RQ3) Readiness level. Defined by the systematic measurement
    system for assessing the maturity of a particular technology (Mankins, 1995),
    the technology readiness level (TRL) is an integer n where 1 ≤ n ≤ 9. This measure
    has been used by the Horizon 2020 European Commission for the 2014/2015 work program.7
    We have classified the TRL of each primary study to emphasize the environment
    in which the proposed approach has been validated. Specifically, in the context
    of this study we classify the TRL of each primary study on a 3-level scale: (i)
    low TRL (i.e., TRL  ≤  4) means that a technology is either formulated, validated
    or demonstrated at most in lab-based environments, (ii) medium TRL (i.e., 5  ≤ 
    TRL  ≤  6) means that a technology is either validated or demonstrated in industrially
    relevant environment, and (iii) high TRL (i.e., TRL  ≥  7) means that a technology
    is either completed, demonstrated, or proven in operational environment. Fig.
    21 presents the TRL levels of our primary studies. Download : Download high-res
    image (124KB) Download : Download full-size image Fig. 21. Technology readiness
    levels. The obtained results indicate that (i) research on architecting with microservices
    is still in its initial phase for what concerns the transferability of the developed
    technologies to industry and (ii) there is a relatively large number of studies
    (9/103) (P3, P7, P19, P33, P35, P59, P68, P83, P87) in which the actual system
    has been proven in its operational environment (TRL = 9). Industry involvement.
    Here we classify each primary study as: academic if all authors are affiliated
    with universities or research centers, industrial if all authors are affiliated
    with some companies, or a mix of the previous two categories. As shown in Fig.
    22, the results are encouraging, as in almost half of the primary studies (42/103),
    there is the involvement of at least one industrial researcher or practitioner;
    this suggests some knowledge exchange between academia and industry. Download
    : Download high-res image (79KB) Download : Download full-size image Fig. 22.
    Industry involvement. Tool support. In the context of this study a tool can be
    considered as an instance that may represent a precise version of an automated
    tool or a written procedure (Jaccheri et al., 1998). Based on the given definition,
    we categorize a tool either as software-based or knowledge-based. Overall, 54
    primary studies provided software-based tools and 77 primary studies provided
    knowledg-based tools. From a research point of view, this result indicates the
    need to support knowledge-based tools with more software-based tools in order
    to demonstrate how effective knowledge-based tools are and how they can be compared
    one another. This can help researchers and practitioners to improve the overall
    quality of microservice-based systems. It is important to notice that this parameter
    can be related to the Research contribution parameter of our classification framework
    (see Section 5.1). Indeed, here we are focussing on whether the proposed approach
    is proposing a specific tool, procedure, or guideline (or a combination thereof),
    as opposed to the main research contribution, which may be about the description
    of a problem, a new application of the microservices architectural style, etc.
    Open-source test system. When screening the 103 primary studies we checked if
    an open-source test system for benchmarking microservices-based systems was used,
    discussed or proposed. We identified only one such system called Acme Air, which
    was used and discussed in two different primary studies (P26, P85). Acme Air is
    a web-based system available in two different architectures (i.e., monolithic
    service and microservice) and in two different languages (i.e., Node.js and Java),
    thus providing researchers and practitioners with a very useful benchmark for
    evaluating, measuring, and comparing their own solutions over a common reference
    system. Acme Air is publicly available as open-source repository on GitHub.8 The
    lack of practical systems for benchmarking microservice-based architectures can
    severely impact the knowledge transfer from academia to industry. A first step
    in this direction has been performed with the Acme Air system, which however is
    still far away from being a realistic benchmarking system. Specifically, it is
    composed of only six services and all of them are developed using the same programming
    language and underlying platform; this is rarely the case in real microservice-based
    systems, for example the Netflix software stack is composed of more than 20 technologies,9
    such as Python, Node.js, Java, React, MySQL, PostgreSQL, Cassandra, and Hadoop.
    In the near future it will be fundamental for researchers to have a shared, technologically
    polyglot, open-source benchmarking system that can be used for testing their proposed
    solutions and for increasing the readiness level of their research products. Technically
    it is also possible to (semi-) automatically generate large scale systems composed
    of a large number of heterogeneous microservices; even if a generated system may
    be realistic only from a syntactical and scale perspective (i.e., its microservices
    communicate with each other, but they do not do any meaningful operation from
    a semantic point of view), it may already prove useful for researchers focussing
    on dependability aspects like scalability, performance, security, availability.
    Number of microservices used for evaluation. Most of the primary studies have
    only used a relatively small number of microservices for their evaluations (i.e.,
    less than 10). Only three primary studies (P82, P35, P72) have used a relatively
    significant number of microservices using a total of 27, 28 and 67 microservices
    respectively. In order to put this result into context, a recent industrial survey
    (Di Francesco et al., 2018) showed that the expected number of microservices deployed
    after migrating towards the microservices architectural style varies between 5
    and 250, with an average of 59. In the future, if the research community on microservice
    architectures aims to bring new emerging approaches to maturity and perform realistic
    evaluations, the number of microservices used for evaluation purposes should be
    increased. 6.2. Trend analysis (RQ3) Fig. 23 summarizes our results for the trend
    analysis related to RQ2. In particular, we notice interesting trends with respect
    to industry involvement (Fig. 23(b)). Firstly, academic-only publications are
    increasing at a fast pace since 2014 and publications with both academic and industrial
    researchers are growing in the last two years as well. Finally, publications with
    only industrial authors are decreasing, potentially in favour of publications
    where also academic researchers are involved. Main findings: ▸ In spite of their
    focus on specific solutions, the low TRL scores of most studies suggest that industrial
    transferability is far away. ▸ The studies with high TRL are quite heterogeneous,
    and their contributions range from a component-based gateway middleware (P2),
    to auto scaling services (P33), to the management of mobile and IoT workloads
    (P52), etc. All studies with high TRL involve an industrial case study or an application
    of the proposed solution into on an industrial-scale system. ▸ The balanced involvement
    of industrial and academic authors, however, is promising for knowledge co-creation
    and cross-fertilization. ▸ The industrial relevance of research evaluations shall
    be fostered by having more significant open-source test systems or benchmarking
    applications available. Download : Download high-res image (529KB) Download :
    Download full-size image Fig. 23. Trend analysis (RQ3). 7. Orthogonal results
    Table 2 presents the results of our horizontal analysis.In this phase of the study,
    we firstly automatically computed a contingency table for every possible pair
    of parameters of our classification framework. Then, we collaboratively created
    and discussed a set of 37 potentially relevant insights to be investigated. We
    iteratively analyzed each potentially relevant insight created in the previous
    step in order to check if its contingency table actually confirms or disproves
    its related hypotheses. Finally, we filtered out all the results which were either
    (i) not supported by a sufficient number of data points, or (ii) chaotic, not
    revealing any evident pattern. This filtering step was performed manually and
    collaboratively by three researchers until reaching a full agreement. The full
    list of potentially interesting relations and the contingency tables for evaluating
    the actual existence of those relations are available in our replication package
    (Di Francesco et al., 2017a). Main findings: ▸ Gaps for future research especially
    point toward security and real-time communication in specific areas like IoT and
    mobile. ▸ With an eye on quality: (i) many orthogonal results suggest that quality
    control and security are attracting insufficient research. Given the substantial
    investments in modernizing software solutions with microservices, this can become
    a real issue in industrial practice. Also, (ii) the quality attributes performance
    and maintainability occur with striking frequency together with various target
    problems like low flexibility, low efficiency, complexity, and modernization.
    ▸ With the pervasive coverage of the design lifecycle phase, a few design patterns
    seem to consolidate towards a catalogue of solutions ready for reuse by practitioners.
    ▸ Research so far is missing evaluations (maybe hindered by relatively immature
    technology or lack of representative benchmarks). Not surprisingly, practice confirms
    a strong interest in migration, and again quality. ▸ History repeats by architectural
    languages/descriptions focusing on system modeling and neglecting support for
    analysis/provenance. ▸ Overall, the studies yield a healthy mix of academic and
    industrial authors, hence suggesting synergies that should help the field to mature
    toward quality solutions. Table 2. Orthogonal results. Relation Results Target
    problems - Main research areas When looking at the distribution of main research
    areas over target problems we notice that service composition, resources management,
    low flexibility, and complexity are well covered by all the main research areas.
    Differently, the least covered problems are: benchmarking (only 1 perspective
    over 7), low testability (2/7), security (3/7), real-time communication (3/7),
    low portability (3/7), and time to market (4/7). The research community in software
    architecture can consider those problems as potential good candidates for contributing
    in solving not-yet-explored challenges in architecting with microservices. Empty
    Cell Moreover, some interesting research gaps for the research community on microservice
    architecture emerged: security from the IoT perspective, real-time communication
    from the cloud, IoT and mobile perspectives, low auditability from the IoT and
    mobile perspectives, and data management from the cloud perspectives. Those problem-perspective
    pairs have never been investigated in any of the analyzed primary studies. Target
    problems - Readiness levels There are some problems in which the technology readiness
    is still leaning towards lower values. These are: runtime uncertainty, modernization,
    and low portability. Those problems can be considered as potentially relevant
    for future researchers as there seems to be a barrier to overcome for architecting
    with microservices with runtime uncertainty and high portability. Target problems
    - Software lifecycle scopes In general, design is the most frequently considered
    lifecycle phase independently of the considered target problem (the only exception
    is low testability with zero occurrences), followed by implementation (only exceptions
    are low testability and benchmarking with zero occurrences). During maintenance
    almost all target problems are considered, with no clear interesting trends. Requirements
    are mainly considered when dealing with low flexibility problems (11 occurrences),
    complexity (9), low efficiency (8), and modernization (6). There are target problems
    which have been considered from a very narrow set of lifecycle phases, like: low
    testability (1/6, testing), benchmarking (2/6, design and testing), and security
    (3/6, design, implementation, operations). In the future it will be interesting
    to see if those extremely scoped problems will expand towards a larger number
    of lifecycle phases, such as requirements, implementation, and maintenance. Target
    problems - Quality attributes The most recurrent pairs of target problems and
    quality attributes are: low flexibility with performance (16 occurrences) and
    maintainability (14), low efficiency with performance (15), complexity with performance
    (12) and maintainability (13), service composition with performance (12), modernization
    with maintainability (10). The identified pairs show the interdependencies between
    architectural problems and quality attributes that have been investigated most
    frequently by the community. Nevertheless, some interesting gaps caught our attention,
    revealing potentially fruitful research lines for future research on architecting
    with microservices: (i) portability has not been considered when addressing either
    low auditability or data management, (ii) compatibility, portability, reliability,
    and security have not been considered when addressing real-time communication,
    and (iii) compatibility, reliability, and usability have not been considered when
    dealing with security. Quality attributes - Design patterns Among the most frequently
    used design patterns we can see that the API gateway is benefiting all quality
    attributes with a spike in maintainability (12) and performance (9), while publish/subscribe
    is strongly related to the performance (7), maintainability (7) and compatibility
    (5) quality attributes. The circuit breaker pattern is related to reliability
    and portability (4), whereas proxy is related to performance (5). Differently,
    the discovery patterns are related to maintainability and performance (3). These
    results can be used by practitioners as a catalogue of prepackaged solutions for
    gaining better quality of a microservice architecture, as potentially they have
    been already validated by the research community, or even evaluated in an industrial
    setting. Architecting activities - Architecture description types If on one hand
    structural descriptions cover all the architecting activities, on the other behavioural
    descriptions are only slightly used for describing reuse (1 occurrence) of architectural
    assets (e.g., design elements, decisions, patterns), getting input and providing
    information (3). While it is reasonable to consider the activity of reuse more
    linked to structural concerns of the architecture, it is important to note that
    missing a behavioural viewpoint can be a strong limitation since it may prevent
    the architect from reasoning with other stakeholders on the functionalities delivered
    by the system. Architectural languages - Architecture provenance Architectural
    languages are predominantly used for the design of the architecture of the system.
    Moreover, informal architecture descriptions are the only notations used when
    dealing with extracted architectures. This means that architectural languages
    (like UML) seem to be only used for designing the system, but are neither used
    for understanding nor analysising the current state of the architecture of the
    system (i.e., architecture extraction) – in spite of the strong focus on architecture
    analysis. Industry involvement - Research contributions Industrial contributions
    are mainly present in studies contributing with (i) an application of architectural
    methods, principles or tools (17 academic, 7 industrial, 8 mixed) or (ii) a reference
    architecture (1 academic, 3 industrial, 3 mixed). Differently, academia is spread
    in many types of contributions, with the largest difference with respect to industrial
    contributions in method (23 academic, 2 industrial, 10 mixed) and problem framing
    (12 academic, 3 industrial, 4 mixed). This result indicates that the latter types
    of contributions (i.e., method and problem framing) are the ones in which industrial
    participation is missing the most. Industry involvement - Research strategies
    Even though the majority of solution proposals are authored by academic-only authors
    (44/71), an encouraging result is the fact that 19 (out of 71) primary studies
    have been authored by a mixed type of researchers (i.e., both academic and industrial)
    and 8 (out of 71) by industrial-only authors. With the exception of one paper
    (P70), validation research always involves academic authors (academic-only in
    18 occurrences, and mixed in 10 occurrences), which is another encouraging trend
    since in principle academic researchers can support industrial ones in setting
    up well-designed, reliable experiments by following known methodological guidelines.
    8. Threats to validity In 2015, Petersen et al. (2015) created a checklist for
    objectively assessing the quality of systematic mapping studies. In this context
    a score can be computed as the ratio of the number of actions taken in a study
    versus the total number of actions in the checklist. In our case we achieve a
    score of 65%, far higher than most systematic studies in the literature, which
    have a distribution with a median of 33% and 48% as the absolute maximum value.
    As always, however, threats to validity are unavoidable. The following reports
    on the main threats to validity of our study and how we mitigated them (Fig. 2,
    Fig. 3, Fig. 4, Fig. 5, Fig. 6, Fig. 7, Fig. 8, Fig. 9, Fig. 10, Fig. 11, Fig.
    12, Fig. 13, Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18, Fig. 19, Fig. 20, Fig.
    21, Fig. 22 and A.1). External validity. The most severe potential external threat
    to the validity of our study is on our primary studies not being representative
    of the state of the art on architecting with microservices. To avoid this to happen,
    we applied a search strategy consisting of both automatic search and backward-forward
    snowballing on the selected studies in combination. Specifically, we mitigated
    the presence of potential gaps left out by the automatic search (which is intrinsically
    syntactic) by means of the snowballing technique. Indeed, as recommended in the
    most recent guidelines for systematic studies (Petersen et al., 2015), we extended
    the coverage of the automatic search by complementing it with a snowballing activity,
    thus enlarging the set of relevant studies by considering each study selected
    in the automatic search, and focussing on those papers either citing or cited
    by it. Also, we considered only peer-reviewed papers and excluded the so-called
    grey literature (e.g., white papers, editorials, etc.). This potential bias did
    not impact our study significantly since considered papers have undergone a rigorous
    peer-review process, which is a well-established requirement for high quality
    publications. We also applied well-defined and previously validated inclusion
    and exclusion criteria, which we refined iteratively by considering the pilot
    studies of our review. Specifically, we thoroughly discussed the definition of
    each selection criteria in order to have a minimal, but complete set of selection
    criteria, according to the goal of our study. It is important to note that we
    decided to have the E2 selection criterion while piloting the search string on
    the electronic data sources. In that phase, we noticed that a large number of
    research articles used a very simple example; in those cases, microservices are
    outside the focus of the proposed research (e.g., approaches for self-adaptive
    systems which can be applied to any type of system, approaches focussing on REST
    APIs in general, etc.) and the microservices domain has been used by the authors
    of the articles in order to contextualize their research contributions in a more
    recent technology. We added the E2 selection criterion to avoid this phenomenon.
    Nevertheless, we are aware that the E2 selection criterion could have been risky
    in case of abuses, so during the application of the selection criteria we have
    been extremely rigorous and, when in doubt, we went through the full text of the
    whole study being considered. Moreover, secondary studies have been excluded (criterion
    E3) because they are meta-studies, and they provide a different perspective about
    microservices w.r.t. primary studies, which focus more on proposing specific architectural
    solutions, methods, or techniques. Nevertheless, even if secondary studies have
    been excluded because of the E3 exclusion criterion, we considered them in our
    study for checking the completeness of our set of primary studies, for identifying
    important issues to be considered in our study, and for defining what is the contribution
    of our study to the literature. Internal validity. We rigorously defined our research
    protocol, and we iteratively defined the classification framework by rigorously
    applying the keywording process. The synthesis of the collected data has been
    performed by applying well-assessed descriptive statistics. Also, during the horizontal
    analysis we made a sanity test of the extracted data by cross-analyzing parameters
    of the classification framework. Construct validity. We mitigated this potential
    bias by automatically searching the studies on multiple data sources, independently
    of publishers’ policies or business concerns; also we are reasonably confident
    about the construction of the search string since the terms used are very general
    and suited to our research questions; the automatic search has been complemented
    with snowballing. Also, we rigorously selected the potentially relevant studies
    according to well-documented inclusion and exclusion criteria. This selection
    stage was performed by one researcher and, as suggested in Wohlin et al. (2012),
    a random sample of potentially relevant studies was identified and the inter-researcher
    agreement was ensured. Conclusion validity. We rigorously defined and iteratively
    refined our classification framework, so that we could reduce potential biases
    during the data extraction process. In doing so, we also have the guarantee that
    the data extraction process was aligned with our research questions. More in general,
    we mitigated potential threats to conclusion validity by applying the best practices
    coming from three different guidelines on systematic studies (Petersen, Vakkalanka,
    Kuzniarz, 2015, Kitchenham, Brereton, 2013, Wohlin, Runeson, Höst, Ohlsson, Regnell,
    Wesslén, 2012). We applied those best practices in each phase of our study and
    we documented each phase in a publicly available research protocol, thus making
    our study easy to be checked and replicated by other researchers. 9. Related work
    A systematic mapping on microservices was performed by Pahl et al. on a set of
    21 primary studies from 2014 to 2015 (Pahl and Jamshidi, 2016). It is a classification
    of the research directions in the field and highlights the relevant perspectives
    considered by researchers. Our study differs from Pahl and Jamshidi (2016) as
    follows: (i) we apply a more comprehensive search process by considering studies
    published in any year up to 2017, extending their search string, and complementing
    the automated search with snowballing; (ii) we apply a systematic process for
    defining a classification framework; (iii) we investigate on the potential of
    industrial adoption of research in architecting with microservices. Alshuqayran
    et al. (2016) presented a systematic mapping study on microservice architecture.
    Their study focusses on (i) the architectural challenges faced by microservice-based
    systems, (ii) the architectural diagrams used for representing them, and (iii)
    the involved quality requirements. Their work and ours can be considered as complementary,
    both cutting the topic of architecting with microservices from different perspectives.
    The main difference between the two studies is that ours considers different research
    questions, thus leading to different results, findings, and implications. Dragoni
    et al. (2016) performed an informal survey on microservices. Our study differs
    from their study because (i) we specifically focus on architectural principles,
    method, and techniques, rather than on microservices in general; (ii) we apply
    a rigorous empirical method throughout the study (i.e., systematic mapping), thus
    providing evidence-based results and easing replication of the performed research;
    (iii) the objective of our study is to characterize existing research on architecting
    with microservices, rather than on providing a narrative viewpoint on their historical,
    current, and future traits. Kratzke and Quint (2017) conducted a systematic mapping
    study on cloud-native applications. The main outcome of that study is a clear
    definition of cloud-native applications, which are defined as “distributed, elastic
    and horizontal scalable systems composed of (micro)services which isolate state
    in a minimum of stateful components. The applications and each of their self-contained
    deployment unit are designed according to cloud-focused design patterns and operated
    on a self-service elastic platform”. Even though the subjects of their study is
    different from ours (i.e., cloud-native apps vs architecting with microservices),
    the two studies share the overall goal (building a comprehensive body of knowledge
    about a topic) and some parameters of the classification framework (e.g., research
    strategy, quality attributes, publication trends, etc.). Besides the difference
    in the considered subject, we also investigate on the potential for industrial
    adoption. Finally, Bergmayr et al. (2018) conducted a systematic literature review
    about cloud modeling languages (e.g., TOSCA). The study is motivated by the fact
    that existing modeling languages for the cloud have different goals, scope, and
    (partially overlapping) modeling concepts. The main contributions of Bergmayr
    et al. (2018) are: (i) a common classification for cloud modeling languages, (ii)
    a comparison framework for cloud modeling languages, and (iii) the elicitation
    and discussion of a set of relevant findings about the state of the art. Our study
    differs from the one by Bergmayr et al. because: (i) the focus of our study is
    on architecting with microservices in general, not only about the modeling aspect,
    (ii) we focus on microservices, and not on cloud-specific concerns, (iii) our
    goal is broader and aims at building a map of the state of the art in order to
    provide an overview of the research area, instead of critically evaluating and
    interpreting studies on a specific research topic such as cloud modeling languages
    (Napoleão et al., 2017), (iv) we investigate on the potential of industrial adoption
    of research contributions, instead of zooming into one specific aspect of each
    analyzed study. 10. Conclusions By following the suggestion in Dragoni et al.
    (2016), the purpose of this study is to provide a broad survey investigating relationships
    among research contributions on microservices. Specifically, we performed a systematic
    mapping of 103 primary studies and produced a clear overview of the state of the
    art on architecting with microservices. We have investigated the research on architecting
    with microservices under three main perspectives: publication trends, focus of
    research, and potential for industrial adoption. Using the data that we have extracted
    from the primary studies, we have performed both a vertical and horizontal analyses.
    Further, we have performed a detailed trend analysis on the data in order to understand
    how the research on architecting with microservices has been evolving over time.
    For each research question, the paper has already summarized (in the boxes titled
    Main findings) the findings we consider the most interesting. In addition, the
    following reports our key associated reflections. The scientific interest in microservices
    exploded in 2015 - hence we expect that the next few years will witness great
    advances. Our analysis shows that most papers discuss specific solutions and related
    validation, fact which calls for more fundamental research, reusable practices
    and lessons learned. Maybe due this bottom-up approach (generalizing from practical
    solutions), more fundamental principles and claimed benefits have still to be
    proven. Among them, our analysis of the research focus shows that the quality
    (and especially performance, functional suitability and maintainability) delivered
    by microservices architectures is a main research focus, but also yet to be proven;
    the promised flexibility might come to the cost of a much-higher complexity than
    expected; and the architecture practices upon which industry can rely are still
    to be identified. The pervasive role technology is playing in engineering for,
    and migrating toward, microservices will hopefully shape some of these architecture
    practices. For example, the increasing utilization of virtualization and containerization
    technologies might push microservices in the back-end to address, among others,
    scalability and elasticity concerns in cloud-based solutions. In a similar vein,
    the increasing popularity of mobile software might give raise to new microservice-based
    patterns for the front-end. Both are definitely directions deserving much-needed
    research. Finally, investigating the above-mentioned tradeoff between flexibility
    and complexity calls for intensive synergy between researchers and practitioners,
    especially because significant microservice-based systems must consist of much
    larger numbers of microservices than the toy examples covered by the publications
    so far. Appendix A. Primary studies Table A.1 reports the full list of the 103
    primary studies. Table A.1. Primary studies. ID Title Authors Year P1 A Reference
    Architecture for Real-time Microservice API Consumption Cristian Gadea and Mircea
    Trifan and Dan Ionescu and Bogdan Ionescu 2016 P2 Apache Airavata As a Laboratory:
    Architecture and Case Study for Component-Based Gateway Middleware Suresh Marru
    and Marlon Pierce and Sudhakar Pamidighantam and Chathuri Wimalasena 2015 P3 Synapse:
    A Microservices Architecture for Heterogeneous-database Web Applications Nicolas
    Viennot and Mathias Lécuyer and Jonathan Bell and Roxana Geambasu and Jason Nieh
    2015 P4 Emergent Software Services Nicolas Cardozo 2016 P5 Bifrost: Supporting
    Continuous Deployment with Automated Enactment of Multi-Phase Live Testing Strategies
    Gerald Schermann and Dominik Schoni and Philipp Leitner and Harald C. Gall 2016
    P6 Sustaining Runtime Performance While Incrementally Modernizing Transactional
    Monolithic Software Towards Microservices Holger Knoche 2016 P7 Case Study: Microservice
    Evolution and Software Lifecycle of the XSEDE User Portal API Walter Scarborough
    and Carrie Arnold and Maytal Dahan 2016 P8 TopoLens: Building a CyberGIS Community
    Data Service for Enhancing the Usability of High-resolution National Topographic
    Datasets Hao Hu and Xingchen Hong and Jeff Terstriep and Yan Y. Liu and Michael
    P. Finn and Johnathan Rush and Jeffrey Wendel and Shaowen Wang 2016 P9 Service-Oriented
    Robotic Swarm Systems: Model and Structuring Algorithms G. Zhou; Y. Zhang; F.
    Bastani; I. L. Yen 2012 P10 Practical Use of Microservices in Moving Workloads
    to the Cloud D. S. Linthicum 2016 P11 Container and Microservice Driven Design
    for Cloud Infrastructure DevOps H. Kang; M. Le; S. Tao 2016 P12 Towards Integrating
    Microservices with Adaptable Enterprise Architecture J. Bogner; A. Zimmermann
    2016 P13 Security-as-a-Service for Microservices-Based Cloud Applications Y. Sun;
    S. Nanda; T. Jaeger 2015 P14 TeNOR: Steps towards an orchestration platform for
    multi-PoP NFV deployment J. F. Riera; J. Batallé; J. Bonnet; M. Dias; M. McGrath;
    G. Petralia; F. Liberati; A. Giuseppi; A. Pietrabissa; A. Ceselli; A. Petrini;
    M. Trubian; P. Papadimitrou; D. Dietrich; A. Ramos; J. Melian; G. Xilouris; A.
    Kourtis; T. Kourtis; E. K. Markakis 2016 P15 Vendor Malware: Detection Limits
    and Mitigation O. Lysne; K. J. Hole; C. Otterstad; O. Ytrehus; R. Aarseth; J.
    Tellnes 2016 P16 Leveraging microservices architecture by using Docker technology
    D. Jaramillo; D. V. Nguyen; R. Smart 2016 P17 Scalable microservice based architecture
    for enabling DMTF profiles D. Malavalli; S. Sathappan 2015 P18 The Design and
    Architecture of Microservices A. Sill 2016 P19 JMesh – A Scalable Web-Based Platform
    for Visualization and Mining of Passive Acoustic Data X. Mouy; P. A. Mouy; D.
    Hannay; T. Dakin 2015 P20 CYCLOPS: A micro service based approach for dynamic
    rating, charging & billing for cloud S. Patanjali; B. Truninger; P. Harsh; T.
    M. Bohnert 2015 P21 A Reusable Automated Acceptance Testing Architecture for Microservices
    in Behavior-Driven Development M. Rahman; J. Gao 2015 P22 Architecture of an interoperable
    IoT platform based on microservices T. Vresk; I. Cavrak 2016 P23 Performance Evaluation
    of Microservices Architectures Using Containers M. Amaral; J. Polo; D. Carrera;
    I. Mohomed; M. Unuvar; M. Steinder 2015 P24 A microservices architecture for collaborative
    document editing enhanced with face recognition C. Gadea; M. Trifan; D. Ionescu;
    M. Cordea; B. Ionescu 2016 P25 Gru: An Approach to Introduce Decentralized Autonomic
    Behavior in Microservices Architectures L. Florio; E. D. Nitto 2016 P26 Workload
    characterization for microservices T. Ueda; T. Nakaike; M. Ohara 2016 P27 Challenges
    in Delivering Software in the Cloud as Microservices C. Esposito; A. Castiglione;
    K. K. R. Choo 2016 P28 Microservice-based architecture for the NRDC V. D. Le;
    M. M. Neff; R. V. Stewart; R. Kelley; E. Fritzinger; S. M. Dascalu; F. C. Harris
    2015 P29 Microservices approach for the internet of things B. Butzin; F. Golatowski;
    D. Timmermann 2016 P30 Towards microservices architecture to transcode videos
    in the large at low costs O. Barais; J. Bourcier; Y. D. Bromberg; C. Dion 2016
    P31 Distributed Systems of Microservices Using Docker and Serfnode J. Stubbs;
    W. Moreira; R. Dooley 2015 P32 Microservice Based Tool Support for Business Process
    Modelling S. Alpers; C. Becker; A. Oberweis; T. Schuster 2015 P33 Polyglot Application
    Auto Scaling Service for Platform as a Service Cloud S. R. Seelam; P. Dettori;
    P. Westerink; B. B. Yang 2015 P34 Evaluating the monolithic and the microservice
    architecture pattern to deploy web applications in the cloud M. Villamizar; O.
    Garcés; H. Castro; M. Verano; L. Salamanca; R. Casallas; S. Gil 2015 P35 Swiss
    TSO integrated operational planning, optimization and ancillary services system
    D. Tchoubraev; D. Wiczynski 2015 P36 Experience on a Microservice-Based Reference
    Architecture for Measurement Systems M. Vianden; H. Lichter; A. Steffens 2014
    P37 Microservices and Their Design Trade-Offs: A Self-Adaptive Roadmap S. Hassan;
    R. Bahsoon 2016 P38 Designing a Smart City Internet of Things Platform with Microservice
    Architecture A. Krylovskiy; M. Jahn; E. Patti 2015 P39 Open Issues in Scheduling
    Microservices in the Cloud M. Fazio; A. Celesti; R. Ranjan; C. Liu; L. Chen; M.
    Villari 2016 P40 Migrating web applications to clouds with microservice architectures
    J. Lin; L. C. Lin; S. Huang 2016 P41 The ENTICE approach to decompose monolithic
    services into microservices G. Kecskemeti; A. C. Marosi; A. Kertesz 2016 P42 Gremlin:
    Systematic Resilience Testing of Microservices V. Heorhiadi; S. Rajagopalan; H.
    Jamjoom; M. K. Reiter; V. Sekar 2016 P43 Automated Fault-Tolerance Testing A.
    Nagarajan; A. Vaddadi 2016 P44 ClouNS-a Cloud-Native Application Reference Model
    for Enterprise Architects Kratzke N., Peinl R. 2016 P45 SeCoS: Web of Things platform
    based on a microservices architecture and support of time-awareness Zeiner H.,
    Goller M., Expósito Jiménez V.J., Salmhofer F., Haas W. 2016 P46 Multi cloud deployment
    with containers Jambunathan B., Kalpana Y. 2016 P47 Micro service cloud computing
    pattern for next generation networks Potvin P., Nabaee M., Labeau F., Nguyen K.-K.,
    Cheriet M. 2016 P48 The database-is-the-service pattern for microservice architectures
    Messina A., Rizzo R., Storniolo P., Tripiciano M., Urso A. 2016 P49 Service cutter:
    A systematic approach to service decomposition Gysel M., Kölbener L., Giersche
    W., Zimmermann O. 2016 P50 Medley: An event-driven lightweight platform for service
    composition Yahia E.B.H., Réveillère L., Bromberg Y.-D., Chevalier R., Cadot A.
    2016 P51 Native cloud applications why virtual machines, images and containers
    miss the point Leymann F., Fehling C., Wagner S., Wettinger J. 2016 P52 Location
    and Context-Based Microservices for Mobile and Internet of Things Workloads Bak
    P., Melamed R., Moshkovich D., Nardi Y., Ship H., Yaeli A. 2015 P53 An ontology-based
    reasoning framework for context-aware applications Anderson C., Suarez I., Xu
    Y., David K. 2015 P54 A methodology and tool support for widget-based web application
    development Nicolaescu P., Klamma R. 2015 P55 Learning-based testing of distributed
    microservice architectures: Correctness and fault injection Meinke K., Nycander
    P. 2015 P56 Microservices validation: Methodology and implementation Savchenko
    D., Radchenko G. 2015 P57 Automated Deployment of a Microservice-based Monitoring
    Infrastructure Ciuffoletti A. 2015 P58 An ecosystem of user-facing microservices
    supported by semantic models Versteden A., Pauwels E., Papantoniou A. 2015 P59
    User-aware location management of prosumed micro-services Klein B., Lopez-De-Ipina
    D., Guggenmos C., Velasco J.P. 2014 P60 m:Ciudad: Enabling end-user mobile service
    creation Davies M., Carrez F., Heinila J., Fensel A., Narganes M., Danado J.C.S.
    2011 P61 Curation micro-services: A pipeline metaphor for repositories Abrams
    S., Cruse P., Kunze J., Minor D. 2011 P62 Towards a platform for user-generated
    mobile services Tacken J., Flake S., Golatowski F., Prüter S., Rust C., Chapko
    A., Emrich A. 2010 P63 Migrating to Cloud-Native Architectures Using Microservices:
    An Experience Report Balalaie, A; Heydarnoori, A; Jamshidi, P 2016 P64 Model-driven
    Generation of Microservice Architectures for Benchmarking Performance and Resilience
    Engineering Approaches Thomas F. Dullmann and Andrévan Hoorn 2017 P65 Towards
    Effective Virtualization of Intrusion Detection Systems Nuyun Zhang and Hongda
    Li and Hongxin Hu and Younghee Park 2017 P66 Publishing Linked Data Through Semantic
    Microservices Composition Ivan Salvadori and Alexis Huf and Ronaldo dos Santos
    Mello and Frank Siqueira 2016 P67 An Architecture to Automate Performance Tests
    on Microservices André; de Camargo and Ivan Salvadori and Ronaldo dos Santos Mello
    and Frank Siqueira 2016 P68 Design and implementation of a decentralized message
    bus for microservices Kookarinrat, Pakorn and Temtanapat, Yaowadee 2016 P69 Telecom
    strategies for service discovery in microservice environments C. Rotter; J. Illés;
    G. Nyìri; L. Farkas; G. Csatári; G. Huszty 2017 P70 A VNF-as-a-service design
    through micro-services disassembling the IMS A. Boubendir; E. Bertin; N. Simoni
    2017 P71 Requirements Reconciliation for Scalable and Secure Microservice (De)composition
    M. Ahmadvand; A. Ibrahim 2016 P72 Towards the understanding and evolution of monolithic
    applications as microservices D. Escobar; D. Cárdenas; R. Amarillo; E. Castro;
    K. Garcés; C. Parra; R. Casallas 2016 P73 A scalable routing mechanism for stateful
    microservices N. H. Do; T. Van Do; X. Thi Tran; L. Farkas; C. Rotter 2017 P74
    A new efficient distributed computing middleware based on cloud micro-services
    for HPC F. Z. Benchara; M. Youssfi; O. Bouattane; H. Ouajji 2016 P75 Efficiency
    Analysis of Provisioning Microservices H. Khazaei; C. Barna; N. Beigi-Mohammadi;
    M. Litoiu 2016 P76 A microservice based reference architecture model in the context
    of enterprise architecture Yale Yu; H. Silveira; M. Sundaram 2016 P77 Reflections
    on SOA and Microservices Z. Xiao; I. Wijegunaratne; X. Qiang 2016 P78 An open
    IoT framework based on microservices architecture L. Sun; Y. Li; R. A. Memon 2017
    P79 Modelling and Managing Deployment Costs of Microservice-Based Cloud Applications
    P. Leitner; J. Cito; E. Stöckli 2016 P80 The evolution of distributed systems
    towards microservices architecture T. Salah; M. Jamal Zemerly; Chan Yeob Yeun;
    M. Al-Qutayri; Y. Al-Hammadi 2016 P81 Microservice Ambients: An Architectural
    Meta-Modelling Approach for Microservice Granularity Sara Hassan, Nour Ali, Rami
    Bahsoon 2017 P82 Workload-Based Clustering of Coherent Feature Sets in Microservice
    Architectures Sander Klock, Jan Martijn E. M. Van Der Werf, Jan Pieter Guelen,
    Slinger Jansen 2017 P83 Microservice Architectures for Scalability, Agility and
    Reliability in E-Commerce Wilhelm Hasselbring and Guido Steinacker 2017 P84 Differences
    Between Model-driven Development of Service-oriented and Microservice Architecture
    F. Rademacher, S. Sachweh and A. Zündorf 2017 P85 Towards Recovering the Software
    Architecture of Microservice-based Systems G. Granchelli, M. Cardarelli, P. Di
    Francesco, I. Malavolta, L. Iovino and A. Di Salle 2017 P86 Decision Guidance
    Models for Microservice Monitoring S. Haselbock and R. Weinreich. 2017 P87 From
    monolith to microservices - Lessons learned on an industrial migration to a Web
    Oriented Architecture J. Gouigoux and D. Tamzalit 2017 P88 A Dashboard for Microservice
    Monitoring and Management B. Mayer and R. Weinreich 2017 P89 Self-managing cloud-native
    applications: Design, implementation, and experience Giovanni Toffetti and Sandro
    Brunner and Martin Blochlinger and Florian Dudouet and Andrew Edmonds 2017 P90
    The IPOL demo system: A scalable architecture of microservices for reproducible
    research Arévalo M., Escobar C., Monasse P., Monzón N., Colom M. 2017 P91 Microflows:
    Automated planning and enactment of dynamic workflows comprising semantically-annotated
    microservices Oberhauser R. 2017 P92 Continuous software engineering-A microservices
    architecture perspective O’Connor R.V., Elger P., Clarke P.M. 2017 P93 A microservice
    architecture for the Intranet of Things and energy in smart buildings Bao K.,
    Mauser I., Kochanneck S., Xu H., Schmeck H. 2016 P94 Domain Driven Design and
    Provision of Micro-services to build Emerging Learning Systems Khemaja M. 2016
    P95 Cloudware: An emerging software paradigm for cloud computing D. Guo; W. Wang;
    G. Zeng; Z. Wei 2016 P96 MORe: A micro-service oriented aggregator Gavrilis D.,
    Nomikos V., Kravvaritis K., Angelis S., Papatheodorou C., Constantopoulos P. 2016
    P97 Incremental integration of microservices in cloud applications Zuniga-Prieto,
    Miguel and Insfran, Emilio and Abrahao, Silvia and Cano-Genoves, Carlos 2016 P98
    Trident: Scalable compute archives: Workflows, visualization, and analysis Gopu
    A., Hayashi S., Young M.D., Kotulla R., Henschel R., Harbeck D. 2016 P99 A Service-Oriented
    Approach to Crowdsensing for Accessible Smart Mobility Scenarios Mirri S., Prandi
    C., Salomoni P., Callegati F., Melis A., Prandini M. 2016 P100 On micro-services
    architecture Namiot, Dmitry and Sneps-Sneppe, Manfred 2014 P101 Towards a Technique
    for Extracting Microservices from Monolithic Enterprise Systems Levcovitz, Alessandra
    and Terra, Ricardo and Valente, Marco Tulio 2016 P102 A dynamic deployment method
    of micro service oriented to SLA ZL Ji, Y Liu 2016 P103 InterSCity: A Scalable
    Microservice-based Open Source Platform for Smart Cities Arthur de M. Del Esposte,
    Fabio Kon, Fabio M. Costa and Nelson Lago 2017 References Alshuqayran, Ali, Evans,
    2016 N. Alshuqayran, N. Ali, R. Evans A systematic mapping study in microservice
    architecture Service-Oriented Computing and Applications (SOCA), 2016 IEEE 9th
    International Conference on, IEEE (2016), pp. 44-51 View in ScopusGoogle Scholar
    Bass, Weber, Zhu, 2015 L. Bass, I. Weber, L. Zhu DevOps: A Software Architect’s
    Perspective Addison-Wesley Professional (2015) Google Scholar Bergmayr, Breitenbücher,
    Ferry, Rossini, Solberg, Wimmer, Kappel, Leymann, 2018 A. Bergmayr, U. Breitenbücher,
    N. Ferry, A. Rossini, A. Solberg, M. Wimmer, G. Kappel, F. Leymann A systematic
    review of cloud modeling languages ACM Comput. Surv. (CSUR), 51 (1) (2018), p.
    22 View in ScopusGoogle Scholar Cockburn Cockburn, A., 2007. Hexagonal architecture.
    Google Scholar Di Francesco, Malavolta, Lago Di Francesco, P., Malavolta, I.,
    Lago, P., 2017a. Replication package. http://cs.gssi.infn.it/JSS2017ReplicationPackage.
    Google Scholar Di Francesco, Malavolta, Lago, 2017b P. Di Francesco, I. Malavolta,
    P. Lago Research on architecting microservices: trends, focus, and potential for
    industrial adoption Software Architecture (ICSA), 2017 IEEE International Conference
    on, IEEE (2017), pp. 21-30 View in ScopusGoogle Scholar Di Francesco, Malavolta,
    Lago, 2018 P. Di Francesco, I. Malavolta, P. Lago Migrating towards microservice
    architectures: an industrial survey 2018 IEEE International Conference on Software
    Architecture, ICSA 2018, Seattle, USA, April 30, - May 4, 2018 (2018), pp. 29-38
    View in ScopusGoogle Scholar Dragoni, Giallorenzo, Lafuente, Mazzara, Montesi,
    Mustafin, Safina Dragoni, N., Giallorenzo, S., Lafuente, A. L., Mazzara, M., Montesi,
    F., Mustafin, R., Safina, L., 2016. Microservices: yesterday, today, and tomorrow.
    arXiv:1606.04036v1. Google Scholar Engström, Runeson, 2011 E. Engström, P. Runeson
    Software product line testing - a systematic mapping study Inf. Softw. Technol.,
    53 (1) (2011), pp. 2-13 View PDFView articleView in ScopusGoogle Scholar Fazio,
    Celesti, Ranjan, Liu, Chen, Villari, 2016 M. Fazio, A. Celesti, R. Ranjan, C.
    Liu, L. Chen, M. Villari Open issues in scheduling microservices in the cloud
    IEEE Cloud Comput., 3 (5) (2016), pp. 81-88 View in ScopusGoogle Scholar Fowler,
    Lewis Fowler, M., Lewis, J., 2014. Microservices a definition of this new architectural
    term. http://martinfowler.com/articles/microservices.html. Google Scholar Jaccheri,
    Picco, Lago, 1998 M.L. Jaccheri, G.P. Picco, P. Lago Eliciting software process
    models with the e 3 language ACM Trans. Softw. Eng.Methodol. (TOSEM), 7 (4) (1998),
    pp. 368-410 View in ScopusGoogle Scholar Jaramillo, Nguyen, Smart, 2016 D. Jaramillo,
    D.V. Nguyen, R. Smart Leveraging microservices architecture by using docker technology
    SoutheastCon, 2016, IEEE (2016), pp. 1-5 CrossRefGoogle Scholar Khazaei, Barna,
    Beigi-Mohammadi, Litoiu, 2016 H. Khazaei, C. Barna, N. Beigi-Mohammadi, M. Litoiu
    Efficiency analysis of provisioning microservices Cloud Computing Technology and
    Science (CloudCom), 2016 IEEE International Conference on, IEEE (2016), pp. 261-268
    CrossRefView in ScopusGoogle Scholar Kitchenham, Brereton, 2013 B. Kitchenham,
    P. Brereton A systematic review of systematic review process research in software
    engineering Inf. Softw. Technol., 55 (12) (2013), pp. 2049-2075 View PDFView articleView
    in ScopusGoogle Scholar Kratzke, Quint, 2017 N. Kratzke, P.-C. Quint Understanding
    cloud-native applications after 10 years of cloud computing-A systematic mapping
    study J. Syst. Softw., 126 (2017), pp. 1-16 View PDFView articleView in ScopusGoogle
    Scholar Kruchten, 2008 P. Kruchten What do software architects really do? J. Syst.
    Softw., 81 (12) (2008), pp. 2413-2416 View PDFView articleView in ScopusGoogle
    Scholar Li, Liang, Avgeriou, 2013 Z. Li, P. Liang, P. Avgeriou Application of
    knowledge-based approaches in software architecture: a systematic mapping study
    Inf. Softw. Technol., 55 (5) (2013), pp. 777-794 View PDFView articleView in ScopusGoogle
    Scholar Lipton, Palma, Rutkowski, Tamburri, 2018 P. Lipton, D. Palma, M. Rutkowski,
    D.A. Tamburri Tosca solves big problems in the cloud and beyond! IEEE Cloud Comput.
    (2018) Google Scholar Malavolta, Lago, Muccini, Pelliccione, Tang, 2013 I. Malavolta,
    P. Lago, H. Muccini, P. Pelliccione, A. Tang What industry needs from architectural
    languages: a survey IEEE Trans. Softw. Eng., 39 (6) (2013), pp. 869-891 View in
    ScopusGoogle Scholar Mankins Mankins, J. C., 1995. Technology readiness levels.
    White Paper, April 6. Google Scholar Mehmood, Jawawi, 2013 A. Mehmood, D.N. Jawawi
    Aspect-oriented model-driven code generation: a systematic mapping study Inf.
    Softw. Technol., 55 (2) (2013), pp. 395-411 View PDFView articleView in ScopusGoogle
    Scholar Special Section: Component-Based Software Engineering (CBSE), 2011 Morris
    Morris, K., 2014. Immutable server. http://martinfowler.com/bliki/ImmutableServer.html.
    [Online], [last accessed on June 15, 2018]. Google Scholar Napoleão, Felizardo,
    de Souza, Vijaykumar, 2017 B.M. Napoleão, K.R. Felizardo, É.F. de Souza, N.L.
    Vijaykumar Practical similarities and differences between systematic literature
    reviews and systematic mappings: a tertiary study The 29th International Conference
    on Software Engineering and Knowledge Engineering (2017) Google Scholar Newman,
    2015 S. Newman Building Microservices “ O’Reilly Media, Inc.” (2015) Google Scholar
    Oasis Topology and Orchestration Specification for Cloud Applications (TOSCA),
    2013 Oasis Topology and Orchestration Specification for Cloud Applications (TOSCA)
    Organization for the Advancement of Structured Information Standards (OASIS) (2013)
    Google Scholar Tech. Rep Pahl, Jamshidi, 2016 C. Pahl, P. Jamshidi Microservices:
    asystematic mapping study Proceedings of the 6th International Conference on Cloud
    Computing and Services Science, Volume 1, Rome, Italy, April 23–25 (2016), pp.
    137-146 CrossRefView in ScopusGoogle Scholar Petersen, 2011 K. Petersen Measuring
    and predicting software productivity: a systematic map and review Inf. Softw.
    Technol., 53 (4) (2011), pp. 317-343 View PDFView articleView in ScopusGoogle
    Scholar Special section: Software Engineering track of the 24th Annual Symposium
    on Applied Computing Software Engineering track of the 24th Annual Symposium on
    Applied Computing Petersen, Feldt, Mujtaba, Mattsson, 2008 K. Petersen, R. Feldt,
    S. Mujtaba, M. Mattsson Systematic mapping studies in software engineering Proceedings
    of the 12th International Conference on Evaluation and Assessment in Software
    Engineering, British Computer Society, Swinton, UK, UK (2008), pp. 68-77 Google
    Scholar Petersen, Vakkalanka, Kuzniarz, 2015 K. Petersen, S. Vakkalanka, L. Kuzniarz
    Guidelines for conducting systematic mapping studies in software engineering:
    an update Inf. Softw. Technol., 64 (2015), pp. 1-18 View PDFView articleView in
    ScopusGoogle Scholar Richards Richards, M., 2015. Microservices vs. service-oriented
    architecture. Google Scholar Ruiu, Scionti, Nider, Rapoport, 2016 P. Ruiu, A.
    Scionti, J. Nider, M. Rapoport Workload management for power efficiency in heterogeneous
    data centers Complex, Intelligent, and Software Intensive Systems (CISIS), 2016
    10th International Conference on, IEEE (2016), pp. 23-30 View in ScopusGoogle
    Scholar Shalom, 2017 N. Shalom Building large scale services with microservices
    (2017) white paper, URL: https://cloudify.co/whitepaper/microservices-orchestration-large-scale-services/.
    Google Scholar Villamizar, Garcés, Castro, Verano, Salamanca, Casallas, Gil, 2015
    M. Villamizar, O. Garcés, H. Castro, M. Verano, L. Salamanca, R. Casallas, S.
    Gil Evaluating the monolithic and the microservice architecture pattern to deploy
    web applications in the cloud Computing Colombian Conference (10CCC), 2015 10th,
    IEEE (2015), pp. 583-590 CrossRefView in ScopusGoogle Scholar Wieringa, Maiden,
    Mead, Rolland, 2006 R. Wieringa, N. Maiden, N. Mead, C. Rolland Requirements engineering
    paper classification and evaluation criteria: a proposal and a discussion Requirements
    Eng., 11 (1) (2006), pp. 102-107 CrossRefView in ScopusGoogle Scholar Wiggins
    Wiggins, A., 2014. The twelve-factor app. URL: http://12factor.net/. [last accessed
    on June 15, 2018]. Google Scholar Wohlin, 2014 C. Wohlin Guidelines for snowballing
    in systematic literature studies and a replication in software engineering Proceedings
    of the 18th International Conference on Evaluation and Assessment in Software
    Engineering, ACM, New York, NY, USA (2014), pp. 38:1-38:10 Google Scholar Wohlin,
    Runeson, Höst, Ohlsson, Regnell, Wesslén, 2012 C. Wohlin, P. Runeson, M. Höst,
    M. Ohlsson, B. Regnell, A. Wesslén Experimentation in Software Engineering Springer
    (2012) Google Scholar Computer Science Yahia, Réveillère, Bromberg, Chevalier,
    Cadot, 2016 E.B.H. Yahia, L. Réveillère, Y.-D. Bromberg, R. Chevalier, A. Cadot
    Medley: an event-driven lightweight platform for service composition International
    Conference on Web Engineering, Springer (2016), pp. 3-20 Google Scholar Cited
    by (164) Towards defining industry 5.0 vision with intelligent and softwarized
    wireless network architectures and services: A survey 2024, Journal of Network
    and Computer Applications Show abstract Monitoring tools for DevOps and microservices:
    A systematic grey literature review 2024, Journal of Systems and Software Show
    abstract Automatic Generation of a Microservice Data Structure from ReLEL 2024,
    Procedia Computer Science Show abstract Microservice-based projects in agile world:
    A structured interview 2024, Information and Software Technology Show abstract
    Catalog and detection techniques of microservice anti-patterns and bad smells:
    A tertiary study 2023, Journal of Systems and Software Show abstract Software
    architecture for quantum computing systems — A systematic review 2023, Journal
    of Systems and Software Show abstract View all citing articles on Scopus Paolo
    Di Francesco is a Ph.D. student in Computer Science at Gran Sasso Science Institute,
    L’Aquila, Italy. His research interests are software engineering, software architecture,
    and modeldriven engineering. His Ph.D. research is focused on microservice-based
    architectures, where he investigates techniques for architecture recovery, architecture
    modeling, and migration of legacy applications towards microservices. In 2012,
    he received his master double degree in Computer Science from L’Aquila University
    (Italy) and Mälardalens Högskola (Sweden), as part of the Global Software Engineering
    European Master Programme (GSEEM). After he completed his master studies and before
    pursuing his Ph.D., he was architect and developer for an industrial company,
    and then an IT consultant freelancer. In 2015 he was awarded a research grant
    from the University of L’Aquila. More information is available at http://www.paolodifrancesco.com.
    Patricia Lago is Full Professor in software engineering at the Vrije Universiteit
    Amsterdam, where she leads the Software and Services (S2) research group in the
    Computer Science Department. Her research is in software architecture and software
    quality with a special emphasis on sustainability. She has a PhD in Control and
    Computer Engineering from Politecnico di Torino and a Master in Computer Science
    from the University of Pisa, both in Italy. She is initiator and coordinator of
    the Computer Science Master Track in Software Engineering and Green IT, and co-founder
    of the Green Lab, a place where researchers, students and companies collaborate
    to measure the energy footprint of software solutions and the impact on software
    quality. She is member of the Steering Committees of IEEE ICSA, ECSA and the ICT4S
    conference series, member of the IFIP 2.10 Working group on Software Architecture,
    the IFIP 2.14 Working group on Services-based Systems, and the Dutch Knowledge
    Network on Green Software. More information is available at www.cs.vu.nl/~patricia.
    Ivano Malavolta is Assistant Professor at the Vrije Universiteit Amsterdam, the
    Netherlands. His research focuses on data-driven software engineering, software
    engineering for mobile development, software architecture, model-driven engineering
    (MDE), and robotics. He is applying empirical methods to assess practices and
    trends in the field of software engineering. He is program committee member and
    reviewer of international conferences and journals in his fields of interest.
    He authored more than 80 papers in international journals and peer-reviewed international
    conferences proceedings. He received a PhD in computer science from the University
    of L’Aquila in 2012. He is a member of ACM and IEEE, Amsterdam Data Science, and
    VERSEN. More information is available at http://www.ivanomalavolta.com. 1 https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/.
    2 https://github.com/Netflix/hystrix 3 http://techblog.netflix.com. 4 The automatic
    searches were performed on May 8, 2017. 5 Our search process covers the research
    studies published until 8th May 2017. 6 https://aws.amazon.com/lambda 7 http://ec.europa.eu/research/participants/data/ref/h2020/wp/2014_2015/annexes/h2020-wp1415-annex-g-trl_en.pdf.
    8 https://github.com/acmeair/acmeair 9 http://stackshare.io/netflix View Abstract
    © 2019 Elsevier Inc. All rights reserved. Part of special issue Architecting Autonomous
    and Smart Systems Edited by Patrizio Pelliccione, Jan Bosch, Mikic Marija View
    special issue Recommended articles Introduction to the special issue on architecting
    autonomous and smart systems Journal of Systems and Software, Volume 150, 2019,
    pp. 1-2 Patrizio Pelliccione, …, Marija Mikic View PDF Implementing secure applications
    in smart city clouds using microservices Future Generation Computer Systems, Volume
    99, 2019, pp. 308-320 Michel Krämer, …, Arjan Kuijper View PDF On revisiting energy
    and performance in microservices applications: A cloud elasticity-driven approach
    Parallel Computing, Volume 108, 2021, Article 102858 Igor Fontana de Nardin, …,
    Harald Köstler View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 138 Captures Readers: 418 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: "Journal of systems and software/\x98The \x9CJournal of systems and software"
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Architecting with microservices: A systematic mapping study'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ares.2016.92
  analysis: '>'
  authors:
  - Vaishnavi Mohan
  - Lotfi ben Othmane
  citation_count: 54
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2016 11th International Confe...
    SecDevOps: Is It a Marketing Buzzword? - Mapping Research on Security in DevOps
    Publisher: IEEE Cite This PDF Vaishnavi Mohan; Lotfi Ben Othmane All Authors 47
    Cites in Papers 3276 Full Text Views Abstract Document Sections I. Introduction
    II. Research Methodology III. Overview of the Current Research Aspects in Secdevops
    IV. Discussion V. Conclusion Authors References Citations Keywords Metrics Abstract:
    DevOps is changing the way organizations develop and deploy applications and service
    customers. Many organizations want to apply DevOps, but they are concerned by
    the security aspects of the produced software. This has triggered the creation
    of the terms SecDevOps and DevSecOps. These terms refer to incorporating security
    practices in a DevOps environment by promoting the collaboration between the development
    teams, the operations teams, and the security teams. This paper surveys the literature
    from academia and industry to identify the main aspects of this trend. The main
    aspects that we found are: definition, security best practices, compliance, process
    automation, tools for SecDevOps, software configuration, team collaboration, availability
    of activity data and information secrecy. Although the number of relevant publications
    is low, we believe that the terms are not buzzwords, they imply important challenges
    that the security and software communities shall address to help organizations
    develop secure software while applying DevOps processes. Published in: 2016 11th
    International Conference on Availability, Reliability and Security (ARES) Date
    of Conference: 31 August 2016 - 02 September 2016 Date Added to IEEE Xplore: 15
    December 2016 ISBN Information: DOI: 10.1109/ARES.2016.92 Publisher: IEEE Conference
    Location: Salzburg, Austria SECTION I. Introduction DevOps is a trending technology
    term. It refers to improving the performance of software development operations
    by involving the development team and the operations team in one process. This
    helps to increase the frequency of deployments, which helps to service the customers
    faster [1]. In fact, CA Technologies expects that 1254 out of 1425 organizations
    will adopt DevOps in the next five years [2]. DevOps helps organizations to [1]:
    improve the collaboration among the developers and operations teams, enhance the
    frequency and easiness of deployments, increase the flexibility in accommodating
    customer requirements, improve the quality of code due to developer collaboration.
    Security is among the major concerns that limit the adoption of DevOps processes.
    This triggered the coining of the terms SecDevOps and DevSecOps. Both refer to
    incorporating security practices in the DevOps processes by promoting collabor-ation
    between the development teams, the operations teams, and the security teams. Other
    concerns include: reliable crossteam management process, recognized training methodology,
    availability of professionals for a complete DevOps implementation, awareness
    about security and compliance aspects, and balance of workload among the teams
    involved [1]. Security experts in the industry and academia have started investigating
    the security aspects of DevOps. Rahman et al. [2] surveyed the perceptions of
    DevOps practitioners towards security in DevOps. They identified the DevOps activities
    that potentially impact the security of the software and identified the security
    practices that organizations use to integrate secur-ity into DevOps. Our paper
    surveys the literature about the security aspects in DevOps from academia and
    industry. (We limited our search to presentations and papers published by OWASP
    AppSec and RSA for the case of industry publications.) It identifies the aspects
    that are being discussed in the SecDevOps or DevSecOps literature. The results
    could be a basis for researchers who want to investigate security problems related
    to DevOps. The rest of this paper is organized as follows. Section II describes
    the research methodology, Section III describe the literature on security in DevOps
    and categorizes the rel-evant information extracted from the selected publications,
    Section IV summarizes the information extracted from the analyzed publications,
    and discusses the results and limitations of the study, and Section V concludes
    the paper. SECTION II. Research Methodology We used in this work the systematic
    mapping research method [9]. The systematic mapping research method is commonly
    used to survey the state of the art of research areas that are not yet mature
    [10]. Such studies help to summarize a particular area of research by categorizing
    the topics investigated in the relevant research publications. The following subsections
    describe the activities that we performed. A. Definition of Research Questions
    The main aim of the study is to identify the aspects that the literature related
    to SecDevOps or DevSecOps discusses. It addresses the question: What are the aspects
    that the research community believes are related to SecDevOps and DevSecOps? B.
    Search for Primary Studies We derived a set of keywords based on the defined research
    question. We did an initial search of the Web and we identified a set of terms
    that the community uses when discussing SecDevOps/DevSecOps topics. The terms
    are: SecDevOps, Secure DevOps, Security in DevOps, Safe DevOps, SecOps, (secure)AND(DevOps),
    and DevSecOps. Table I Summary of the identified publications and presentations.
    Table II Summary of the secdevops aspects identified by the mapping study. We
    used these keywords to search for the research papers that address the question.
    First, we searched the Google Scholar database, and the IEEE Xplore Digital Library
    for academic papers. We identified 66 research publications that match the research
    criteria. Second, we searched for presentations at OWASP and RSA conferences that
    also much the criteria. We identified 5 presentations. In general, talks at industry
    events are not reviewed/verified (and therefore not trustworthy), which is not
    the case for OWASP and RSA events. C. Screening of Papers for Inclusion and Exclusion
    Papers, that did not comply with the motivation of this research, are excluded
    from the study. In addition, papers that did not fit into at least one of the
    following criteria are excluded: defines the term SecDevOps or Security in DevOps,
    highlights the need for security in DevOps, describes security concerns or protection
    mechanisms for DevOps components, describes security practices that can be used
    to incorporate security into DevOps. We found that only 5 publications out of
    the 66 and 3 presentations from OWASP AppSec and RSA conferences out the identified
    5 presentations, are relevant to our research question. Again, the search for
    conference talks was not exhaustive and was confined to targeted conferences.
    Table I gives the list of the identified publications. D. Keywords for the Classification
    To identify research context, the various sections of the identified papers were
    condensed to a set of keywords. The keywords from all the papers were then grouped
    together to form aspects, which were used to classify the results from relevant
    publications. Those aspects were further refined based on common topics. The final
    set of aspects are: definition, security best practices, compliance, process automation,
    tools for SecDevOps, software configuration, team collaboration, availability
    of activity data and information secrecy. The description of each of the selected
    aspects is given below. Table II lists these aspects. E. Data Extraction and Mapping
    Studies We analyzed all selected papers and presentations to verify that they
    included ‘concrete’ information related to each of the aspects identified in Table
    II. Then, we classified the selected publications and presentations into the aspects.
    We observed that most of the publications discuss multiple SecDe-vOps/DevSecOps
    aspects. Table III summarizes the different aspects that the selected publications
    and presentation comprise. Table III Classification of secdevops publication into
    the identified aspects SECTION III. Overview of the Current Research Aspects in
    Secdevops We report in this section the treatment of the different aspects that
    we identified in Section II-E in each of the selected papers. Note that we will
    use, starting from this section, the term SecDevOps to refer also to DevSecOps,
    Secure DevOps, Security in DevOps, Safe DevOps, and SecOps. A. Definition Researchers
    from industry and academia agree that SecDe-vOps and DevSecOps imply integration
    of security practices in the DevOps processes. For example, Cash et al. [4] refer
    to the practice of incorporating quality security technologies into DevOps as
    SecDevOps. They consider that DevOps fits developing software for the cloud and
    consider that security in influencing enterprise decisions to utilize the cloud.
    Rahman et al. [2] consider that DevSecOps, SecDevOps, SecOps, and RuggedOps are
    aliases for Security in DevOps. They consider that the terms refer to the integration
    of security principles in DevOps by promoting the collaboration between the security
    teams, the development teams and operations teams. B. Security Best Practices
    Farroha et al. [1] call for the integration into SecDevOps the following set of
    best practices: automating tests to detect non-compliance, tracking compliance
    breaches through automated reporting of violations, continuous monitoring and
    mainten-ance of a service catalog with tested and certified services. The paper
    also depicts successful strategies to maintain the incorporated security. Cash
    et al. [4] call for integrating into SecDevOps security scanning and configuration
    automation. Rahman et al. [2] believes that DevOps activities impact security
    in 2 opposite ways: positive or negative. They believe that automation activities,
    such as automated monitoring, automated deployment pipeline, and automated testing
    contribute positively to the security of the software. However, selecting the
    wrong automated deployment tools, using the wrong software metrics and unsupervised
    collaboration contribute negatively towards the security of the software. In addition,
    5 out of 9 survey [2] respondents believe that the fast software deployment that
    DevOps processes support pushes organizations to overlook security tests, which
    leads to deploying vulnerable software. the majority of the survey respondents
    also believe that security policies, manual security tests, and security configuration
    are prevalent in DevOps organizations. Schneider [6] describes the different stages
    of dynamic security scanning that can be used by organizations to integrate security
    into DevOps. The four levels of scanning are: (1) pre-authentication scanning,
    that involves scanning the public attack surface; (2) post-authentication scanning,
    that involves session maintenance, user role management, and logout and auto-relogin
    detection; (3) backend scanning of the various application layers independently;
    and (4) scanning workflows specific to the targeted application. Vries [8] believes
    that security activities need to adopt concepts used in DevOps. The talk advocates
    the collaboration between the development team and the business owner of the software
    to set the security goals. The talk also calls for automating security tests and
    security scans in SecDevOps processes. C. Compliance Schneider [6] introduces
    the SecDevOps Maturity Model(SDOMM). The model is a manual to help projects achieve
    certain security aspects through automation in a continuous integration (CI) build
    chain. The model is useful for organizations willing to make the change to SecDevOps.
    It comprises four axes: (1) dynamic depth, the extent of dynamic scans in a CI
    chain, (2) static depth, the extent of static code analyses in a CI chain, (3)
    intensity, the impact of the executed attacks and (4) consolidation, the effectiveness
    of handling findings. Storms [7] believes that DevOps fails to include security
    throughout the process and leaves it to the end. This is supported by CA technologies
    survey [11] that found that more than one-fourth of the surveyed companies are
    willing to adopt DevOps but state that security and compliance concerns stop them
    from doing so. The security problems due to DevOps that Storms lists include the
    high pace of deployments, the unclear access restrictions, and the lack of audit
    and control points. Farroha et al. [1] suggest a set of requirements for compliance
    policies which include prohibiting unauthorized access, maintaining a log for
    accesses to sensitive data, and monitoring data operations. In addition, they
    illustrate how the collection of data could affect the policies for compliance.
    D. Process Automation Vries [8] explains how the traditional security approach
    with a focus on documentation, manual processes and tools are unfit for continuous
    deployment environments and need to be replaced by a more modern approach to suit
    DevOps. Rahman et al. [2] describe the impact of automated de-ployment on security.
    Automated deployment pipelines enable organizations to ship software changes at
    a rapid rate. Though, this increase in deployment speed is beneficial to the organizations,
    the speed might lead to overlooking the necessary security reviews and checks
    that need to be approved before delivery. If the security team is not a part of
    this rapid develop and deploy iteration, it might increase chances of production
    of vulnerable software. Their study also describes the security techniques prevalent
    among organizations to integrate security into DevOps. Use of automation activities
    like automated code review, automated monitoring, automated testing are popular
    automation activities for security integration in DevOps environments. Storms
    [7] explains that the integration of security expertise in DevOps pipelines and
    processes, helps to enable the De-vOps and Security teams work together more efficiently.
    His talk highlights the ways in which existing DevOps tools can be utilized to
    strengthen security. He also suggests process changes to move towards secure DevOps.
    Matteti et al. [3] describe the need to secure Linux containers, which are considered
    a break-through for DevOps because of their contribution to simplifying automated
    deployments. Linux containers expose file systems, networks and kernels to attacks.
    These resources cannot be protected by the existing security measures that only
    protect specific applications rather than entire environments. Linux containers
    are considered more prone to attacks than VMs. Kernel exploits, attacks on shared
    Linux host resources, misconfiguration, side channels and data leakage are some
    vulnerabilities in Linux containers. Matteti et al. describe also mechanisms to
    protect container environments. The security hardening mechanisms, including AppArmor
    and SELinux, and host based intrusion detection systems are not easy to be used
    within Linux container environments. The authors propose the LiCShield Framework
    that provides protection to hosts, by confining accesses of containers and container
    management daemons to perform only the operations observed in testing environments
    and restricting container operations, by tightening the internal noisy environments.
    The authors describe also the protection mechanisms available in Docker, a container
    management daemon. Docker is protected by AppArmor/SELinux profiles that secure
    the critical host locations from modifications. This security mechanism of Docker
    does not secure the container workloads, nor does it provide protection against
    the vulner-ability of the Docker daemon itself. LiCShield Framework has been created
    to overcome these drawbacks. Bass et al. [5] elucidate the vulnerabilities in
    a deployment pipeline. They describe three scenarios to subvert a deployment pipeline
    that range from deploying an invalid image, to deploying an image without performing
    all the necessary checks, to having an unprotected production environment. They
    also indicate the possible attacks on host and network security that might lead
    to subversion of the deployment pipeline. The authors also classify components
    involved in the deployment pipeline as trustworthy and untrustworthy. Their study
    portrays mechanisms to create trustworthiness in a de-ployment pipeline. Security
    testing, static analysis and formal verification are some techniques that may
    be used to secure a pipeline. The authors propose a method to secure the pipeline
    by restricting the attack surface of the code base. This involves restricting
    the reach of parts of the code base from critical parts. The approach suggests
    that untrustworthy components need to communicate via trustworthy components to
    reach sensitive parts of the pipeline. The paper also describes a step-by-step
    process of how the hardening of a deployment pipeline can be performed. The result
    of the process is re-architecture of untrustworthy components to have restricted
    access or converting them into trustworthy components. Table IV Tools for integrating
    security in devops [1] E. Tools for SecDevOps Farroha et al. [1] identified a
    set of tools that could be integrated to DevOps to support security, monitoring,
    and logging. Table IV lists these tools. Also, Schneider [6] provides some examples
    of open source tools that enable security in DevOps environments. Table V lists
    these tools. Storms [7] showed how security features available in open source
    software, such as Git, Chef and Jenkins can be utilized to include security into
    the development and deployment processes. He also suggests the use of a set of
    monitoring and logging tools, which are included in Table V. F. Software Configuration
    Rahman et al. [2] suggest that the relationship between the use of automation
    activities in DevOps environments and its influence on the software quality could
    be a useful line of research for DevOps enthusiasts. They also suggest that the
    relationship between collaboration, the use of security activities and security
    practices as a scope for future research. Table V Security tools useful in devops
    environments - industry perspective Bass et al. [5] propose studying the fetching
    of code from third party libraries for the vulnerabilities fixes, and the security
    of the cloud where the image is deployed. G. Team Collaboration Farroha et al.
    [1] advocate the importance of involving software stakeholders to build a secure
    system. The rights to protect sensitive data and ensure compliance need to be
    granted to stakeholders to enable security. Rahman et al. [2] reviewed the literature
    about SecDevOps and found that most artifacts propose enforcing the collaboration
    among the security team, the development team, and the operations team for a better
    integration of security principles into DevOps. In addition, they found that the
    literature suggests training the developers to build security tools, which could
    then be integrated into SecDevOps processes. Rahman et al. [2] also conducted
    a survey with DevOps practicing enterprises. The survey revealed the enterprises''
    awareness towards collaboration and indicated that at least 7 out of the 9 surveyed
    organizations observed at least a moderate level of collaboration amongst the
    development team, the operations team, and the security team. H. Availability
    of Activity Data Farroha et al. [1] suggest how the characteristics of data in
    today''s era of Big Data affects the policies for compliance. They enumerate 9
    characteristics of Big Data: volume, variety, velocity, veracity, validity, volatility,
    visualization, vulnerability, and value. They believe that these characteristics
    influence the formulation of compliance policies. In addition, the availability
    of data generated by SecDevOps processes and the easy and instant access to this
    data raises concerns about the privacy of the developers and about the secrecy
    of this information. Schneider [6] lists a set of tools that support developing
    secure software. The data from these tools could be utilized to identify false
    positives, to develop custom logic, to flag unstable builds, to classify the severity
    of attacks on builds, and to identify trends in focused areas. The data could
    be utilized to improve security measures. I. Information Secrecy Rahman et al.
    [2] believe that collaboration among teams, if left unsupervised, would lead to
    deterioration of the system''s security. When teams collaborate closely, the information
    exchanged is not restricted and this might be a threat to the system''s security.
    Well-defined policies regarding information exchange across teams should be in
    place to prevent security threats due to collaboration. Policies for data acquisition
    and data protection are critical for the industry. In addition, the amount of
    Personally Identifiable Information(PII) allowable to be collected for analysis
    is one of the major privacy concerns that need to be addressed. SecDevOps allows
    to collect data related to the development activities. Farroha et al. [1] believe
    that the increase in the volume of this type of data, their diversity, and their
    availability is alarming. They propose setting information governance policies
    to control the use of such data. SECTION IV. Discussion This section summarizes
    the findings of the paper, discusses the impact of these findings and the limitations
    of the work. A. Summary We reviewed 5 peer-reviewed papers and 3 presentations
    from AppSec and RSA conferences related to SecDevOps. We observe from the literature
    survey that there is a trend in the industry in adopting DevOps processes. This
    enthusiasm is manifested by the trend of producing tools (often as open sources)
    that automate security activities that needs to be integrated in SecDevOps process.
    This will help organizations to adopt security practices in an agile way and to
    make security techniques more usable. The aspects discussed in the reviewed papers
    are: definition, security best practices, compliance, process automation, tools
    for SecDevOps, software configuration, team collaboration, availability of activity
    data, and information secrecy. The variety of these aspects demonstrates that
    SecDevOps is not only about process automation and integrating security activities
    in DevOps processes, it concerns also the factors that support that integration
    (i.e., tools, teams collaboration, and configuration management), and side effects
    of such integration, i.e., availability of activity data and information secrecy.
    The variety of these aspects suggest, despite the low number of surveyed papers,
    that SecDevOps is not a buzzword, or at least it is not anymore. It refers a topic
    that is starting to have its own merit: need, aspects, and potential foundations.
    B. Impact of the Results DevOps is promoting frequent software deployments, which
    challenge the adoption of security activities in the process. This is pushing
    for developing tools that automate security activities to streamline SecDevOps
    processes. This work demonstrates that SecDevOps is more than integrating security
    best practices in DevOps and automating security activities. SecDevOps implies
    also, for example, strengthening the collaboration between the development teams,
    the operation teams, and the security teams. The aspects of SecDevOps that we
    identified in this paper could be used as a research map to address the challenges
    related to the adoption of SecDevOps. The result could be a basis for researchers
    who want to work on open problems affecting the security of DevOps organizations,
    and provide them with an overview of the current state of research in SecDevOps.
    We observed from the study that collaboration among the development team, the
    security team and the operation team is crucial to the success of SecDevOps. However,
    collaboration implies sharing information, which may threaten e.g., the trade
    secrets of organizations. Organizations need to define policies for sharing information
    among the participants in SecDevOps processes to ensure that critical information
    is not leaked during collaboration. The integration of development processes to
    operation processes and increased collaboration among the development team, the
    operation team, and the security team would allow to collect data about the process.
    The data could be used, for example, to identify weaknesses, assess the performance
    of the teams, and check compliance with standards. The data could be also be used
    to develop tools that support the development team in their tasks, such as tools
    to mine patterns for false positives. Unfortunately, such data may include information
    private to the process participants. Use of such data may lead to violation of
    their privacy. Research to ensure privacy of developers and secrecy of information
    (such as trade secret) should be performed. C. Limitations of the Study This study
    has several limitations. First, we identified only 5 relevant publications out
    of 66 publications found by the search. We cannot assure the completeness in the
    set of relevant publications related to SecDevOps, some may have been missed.
    This also applies to presentations in RSA and APPSEC conferences. The number of
    relevant publications is low because the field is relatively young. Second, the
    extraction of data from the papers and presentations may be flawed due to human
    perception and understanding of the text. We were careful in the coding, we read
    the text several times, and we performed peer-reviews to reduce the impact of
    this threat. The extracted data were grouped into SecDevOps aspects (see Table
    III). The two authors discussed the data and the aspects classification to have
    more objective opinions. However, we acknowledge that we may have missed some
    aspects. SECTION V. Conclusion We surveyed in this paper publications and presentations
    related to SecDevOps. We found that the community is currently investigating the
    following aspects: definition, security best practices, compliance, process automation,
    tools for SecDe-vOps, software configuration, team collaboration, availability
    of activity data and information secrecy. The variety of these aspects demonstrates
    that SecDevOps is not a buzzword. We believe that the scope of research on SecDevOps
    should move from integrating security activities into DevOps processes towards
    addressing related concerns, such as collaboration, information secrecy, and learning
    from available process data. We believe that organizations adopting SecDevOps
    should at least harden their deployment environment, implement supervised collaborations
    between teams, restrict the access of untrustworthy components of a DevOps environment,
    and ensure security practices are followed during the execution of DevOps activities.
    As future work, we are performing a case study on auto-mating a manual waterfall-based
    deployment process into a secure DevOps process for a leading software organization.
    Organizations willing to make the change to DevOps definitely need to consider
    the impacts on the security of their software and adapt essential security measures
    to implement secure DevOps. ACKNOWLEDGMENT This work was supported, in part, by
    the Hessian LOEWE excellence initiative within CASED. Authors References Citations
    Keywords Metrics More Like This BP: Security Concerns and Best Practices for Automation
    of Software Deployment Processes: An Industrial Case Study 2018 IEEE Cybersecurity
    Development (SecDev) Published: 2018 Enterprise-Driven Open Source Software: A
    Case Study on Security Automation 2021 IEEE/ACM 43rd International Conference
    on Software Engineering: Software Engineering in Practice (ICSE-SEIP) Published:
    2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: 'SecDevOps: Is It a Marketing Buzzword? - Mapping Research on Security in
    DevOps'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/2899415.2899426
  analysis: '>'
  authors:
  - Henrik Bærbak Christensen
  citation_count: 41
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesITICSEProceedingsITiCSE
    ''16Teaching DevOps and Cloud Computing using a Cognitive Apprenticeship and Story-Telling
    Approach RESEARCH-ARTICLE SHARE ON Teaching DevOps and Cloud Computing using a
    Cognitive Apprenticeship and Story-Telling Approach Author: Henrik Bærbak Christensen
    Authors Info & Claims ITiCSE ''16: Proceedings of the 2016 ACM Conference on Innovation
    and Technology in Computer Science EducationJuly 2016Pages 174–179https://doi.org/10.1145/2899415.2899426
    Published:11 July 2016Publication History 40 citation 622 Downloads eReaderPDF
    ITiCSE ''16: Proceedings of the 2016 ACM Conference on Innovation and Technology
    in Computer Science Education Teaching DevOps and Cloud Computing using a Cognitive
    Apprenticeship and Story-Telling Approach Pages 174–179 Previous Next ABSTRACT
    References Cited By Index Terms Recommendations Comments ABSTRACT DevOps is a
    new way of developing software that is challenging from a teaching perspective.
    In this paper, we outline these challenges and propose teaching methods that focus
    on skill acquisition and technical practices that focus on performant virtualization
    to overcome them. We describe central elements from our course Cloud Computing
    and Architecture that has been designed and executed upon these methods and practices
    and report our experiences and lessons learned. References ACM. Computer science
    curricula 2013. Technical report, IEEE Computer Society, 2013. http://www.acm.org/education/CS2013-final-report.pdf.
    Manifesto for agile software development. http://www.agilemanifesto.org/. C. Anderson.
    Docker. IEEE Software, pages 102--105, May/June 2015. Show All References Cited
    By View all Jha A, Teri R, Verma S, Tarafder S, Bhowmik W, Kumar Mishra S, Appasani
    B, Srinivasulu A and Philibert N. (2023). From theory to practice: Understanding
    DevOps culture and mindset. Cogent Engineering. 10.1080/23311916.2023.2251758.
    10:1. Online publication date: 31-Dec-2024. https://www.tandfonline.com/doi/full/10.1080/23311916.2023.2251758
    Ferino S and Kulesza U. Unveiling the Teaching Methods Adopted in DevOps Courses.
    Proceedings of the XXII Brazilian Symposium on Software Quality. (355-357). https://doi.org/10.1145/3629479.3629483
    Pfeiffer R, Lungu M and Tell P. (2023). Live Is Life: Teaching Software Engineering
    on Live Systems. IEEE Software. 40:6. (117-125). Online publication date: 1-Nov-2023.
    https://doi.org/10.1109/MS.2023.3311376 Show All Cited By Index Terms Teaching
    DevOps and Cloud Computing using a Cognitive Apprenticeship and Story-Telling
    Approach Applied computing Education Computer-assisted instruction Social and
    professional topics Professional topics Computing education Computing education
    programs Computer science education Software and its engineering Software creation
    and management Software verification and validation Software defect analysis Software
    testing and debugging Recommendations DevOps patterns to scale web applications
    using cloud services SPLASH ''13: Proceedings of the 2013 companion publication
    for conference on Systems, programming, & applications: software for humanity
    Scaling a web applications can be easy for simple CRUD software running when you
    use Platform as a Service Clouds (PaaS). But if you need to deploy a complex software,
    with many components and a lot users, you will need have a mix of cloud services
    in ... Read More Analysis and Research of Cloud Computing System Instance ICFN
    ''10: Proceedings of the 2010 Second International Conference on Future Networks
    As a kind of emerging business computational model, Cloud Computing distribute
    computation task on the resource pool which consists of massive computers, accordingly
    ,the application systems can gain the computation strength, the storage space
    and ... Read More Cloud Storage as the Infrastructure of Cloud Computing ICICCI
    ''10: Proceedings of the 2010 International Conference on Intelligent Computing
    and Cognitive Informatics As an emerging technology and business paradigm, Cloud
    Computing has taken commercial computing by storm. Cloud computing platforms provide
    easy access to a company’s high-performance computing and storage infrastructure
    through web services. With cloud ... Read More Comments 15 References View Table
    Of Contents Footer Categories Journals Magazines Books Proceedings SIGs Conferences
    Collections People About About ACM Digital Library ACM Digital Library Board Subscription
    Information Author Guidelines Using ACM Digital Library All Holdings within the
    ACM Digital Library ACM Computing Classification System Digital Library Accessibility
    Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Teaching DevOps and Cloud Computing using a Cognitive Apprenticeship and
    Story-Telling Approach
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/mic.2018.032501519
  analysis: '>'
  authors:
  - George Pallis
  - Demetris Trihinas
  - Athanasios Tryfonos
  - Marios D. Dikaiakos
  citation_count: 41
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Internet Computing >Volume: 22 Issue: 3 DevOps as a Service: Pushing the
    Boundaries of Microservice Adoption Publisher: IEEE Cite This PDF Demetris Trihinas;
    Athanasios Tryfonos; Marios D. Dikaiakos; George Pallis All Authors 42 Cites in
    Papers 2180 Full Text Views Abstract Document Sections Microservices Challenges
    Enter the Unicorn Framework Conclusion Authors Figures References Citations Keywords
    Metrics Abstract: Software teams of all sizes are embracing the DevOps philosophy
    to rapidly deliver applications by adopting the microservice paradigm. Industry
    trends show clearly that microservice adoption is expanding rapidly. However,
    microservices are not without challenges. Deployment at scale calls for implementing
    autonomic principles and solutions, which lead to higher complexity and increased
    failure risk. Published in: IEEE Internet Computing ( Volume: 22, Issue: 3, May./Jun.
    2018) Page(s): 65 - 71 Date of Publication: 11 June 2018 ISSN Information: DOI:
    10.1109/MIC.2018.032501519 Publisher: IEEE Microservices Microservices are now
    being studied as part of the cloud infrastructure. But, why are microservices
    so important to the future of DevOps? Unlike software-heavy VMs, microservices
    can share the core of the underlying OS, which enables faster deployments in the
    cloud without diminishing performance. Thus, instead of all application services
    being part of one enormous monolith, business capabilities are self-contained
    with well-defined interfaces that avoid synchronous and blocking-calls whenever
    possible. By adopting the DevOps “ideology,” separate software teams are each
    responsible for different aspects of the end application allowing both the team
    and software core to develop, test, handle failures and scale independently. Sign
    in to Continue Reading Authors Figures References Citations Keywords Metrics More
    Like This Two Level Based Privacy Protection Approach for Internet of Things Users
    in Cloud Computing 2018 21st Saudi Computer Society National Computer Conference
    (NCC) Published: 2018 Current security and privacy issues, and concerns of Internet
    of Things (IoT) and Cloud Computing: A review 2022 International Conference on
    Computing, Communication, and Intelligent Systems (ICCCIS) Published: 2022 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE internet computing
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'DevOps as a Service: Pushing the Boundaries of Microservice Adoption'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.future.2018.10.034
  analysis: '>'
  authors:
  - Shaohua Wang
  - Yisheng Zhong
  - Erqi Wang
  citation_count: 59
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. An integrated
    GIS platform architecture for spatiotemporal big data 4. Implementation of cloud-terminal
    integration GIS for spatiotemporal big data 5. Experiments 6. Conclusion and future
    work Acknowledgments References Vitae Show full outline Cited by (71) Figures
    (14) Show 8 more figures Tables (1) Table 1 Future Generation Computer Systems
    Volume 94, May 2019, Pages 160-172 An integrated GIS platform architecture for
    spatiotemporal big data Author links open overlay panel Shaohua Wang a b, Yang
    Zhong c, Erqi Wang d e f Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.future.2018.10.034
    Get rights and content Highlights • We propose an integrated GIS platform architecture
    designed to meet the requirements of processing and analyzing spatiotemporal big
    data. • Cloud-terminal Integration GIS for spatiotemporal big data is developed.
    • The experiments for spatiotemporal big data showed SuperMap GIS spatiotemporal
    big data engine achieved excellent performance. Abstract With the increase in
    smart devices, spatiotemporal data has grown exponentially. To deal with challenges
    caused by an increase data requires a scalable and efficient architecture that
    can store, query, analyze, and visualize spatiotemporal big data. This paper describes
    a Cloud-terminal integrated GIS platform architecture designed to meet the requirements
    of processing and analyzing spatiotemporal big data. Cloud-terminal Integration
    GIS is developed according to the architecture. Extensive experiments deployed
    on the internal organization cluster using real-time datasets showed that the
    SuperMap GIS spatiotemporal big data engine achieved excellent performance. Previous
    article in issue Next article in issue Keywords Spatiotemporal big dataDistributed
    computing frameworkCloud-terminal integration GISSuperMap GIS 1. Introduction
    With the advancement of information technology, the demands of processing, analyzing
    and visualizing spatiotemporal big data have dramatically increased [1]. In this
    era of big data, the Geographic Information System (GIS) is facing new challenges.
    To overcome the difficulties caused by big data, GIS must evolve its technologies
    to cope with big data [2], [3]. Some of the challenges for GIS include analyzing
    and processing the spatiotemporal big data, clustering and distributing spatial
    big data, indexing and managing big data, and computing and visualizing the big
    data in the system while maintaining a high performance [4], [5]. Currently, popular
    big data platforms (such as Hadoop and Spark) do not have the capacity to perform
    of spatial analysis, spatial computation, or spatial data mining. To recognize
    breakthroughs and innovations for the large scale spatial data of distributed
    storage and management, distributed spatial computing, real-time big data processing,
    and visualization [6], it is necessary for GIS to integrate the general big data
    technology. In the face of the increase in data volume and the growing number
    of data types, traditional relational database is prone to bottleneck problems,
    such as low storage efficiency, weak concurrent access ability, and difficulty
    in horizontal scaling. It is imperative to develop new spatial data storage technology
    [7], [8]. Container technology (such as Docker.) facilitates rapid and large-scale
    deployment of GIS. Optimal synchronization and discovery mechanisms in load-balancing
    provide support for the dynamic scaling and disaster recovery of GIS services.
    If a GIS system seeks to consume data to perform queries or generate maps, the
    output data from Spark must be converted and transferred into GIS platforms. The
    process is typically time and storage-consuming. Additionally, the traditional
    GIS system only executes the computing tasks in the job queue, it cannot process
    streaming data. The conventional GIS software and stand-alone processing architecture
    cannot be analyzed a large volume (For example, over 1 billion records) of spatiotemporal
    big data. Moreover, these integration processes require high specs of computer
    hardware and a rewriting of most of the algorithms for big data in GIS [9], [10].
    In this paper, we focus on the design and implementation of an integrated GIS
    platform architecture for spatiotemporal big data. The paper is organized into
    four parts. Related work is illustrated in Section 2. Section 3 introduces our
    integrated GIS platform architecture. Section 4 discusses its implementation.
    Case tests and results follow in Section 5. 2. Related work Hadoop extensions
    like Hadoop-GIS [11], [12], [13] and SpatialHadoop [13], [14], [15] support spatial
    query using a MapReduce framework. The issue with these extensions, however, is
    that they save intermediate results to the disk reducing efficiency. One advantage
    that Spark’s framework has over Hadoop is its speed. The memory-based parallel
    computing architecture performs better than the MapReduce model in Hadoop. With
    the use of RDD, distributed computing leads to better performance by two orders
    of magnitude. Additionally, Spark offers more support to big data computing, its
    enhanced stream processing, graph computing, and machine learning sub-systems
    are versatile. These are the reasons that Spark based framework was chosen in
    this study. A few solutions of processing spatial data such as GeoSpark [16],
    [17], SpatialSpark [18], [19], LocationSpark [20], and Simba [21] offer limited
    functions within Spark. There are two ways for Spark to execute GIS functions.
    The first approach is to have GIS computing run outside of Spark, this allows
    for managing task orders and visualizing the output of analysis. The other approach
    is to have it run internally. Using this method, we can perform a various of tasks,
    including generating a spatial index, executing the spatial query, and performing
    spatial analysis and computing. Considering GIS core features and applications,
    we prefer the latter: running GIS directly inside the Spark framework so as to
    take full advantage of its potential. Most of the big data frameworks such as
    Spark, HDFS, MongoDB, and ZooKeeper are based on Linux [22]. In Windows environments,
    these frameworks are being mostly used for research and study purposes. Hence,
    the best way to have GIS and big data framework work together seamlessly is to
    have a cross-platform GIS system. A cross-platform GIS system can directly support
    Linux from its core functions but also work in Windows environments. GIS functions
    have to be cross-platform [23]. In spatiotemporal data computing, the system is
    required to process a large volume of data and to manage dynamic changes. Spatial
    online analytical processing for real-time data based on SpatialHadoop is usability
    [24], it can be improved by Apache Storm [25] or Spark Streaming [26], [27]. Also,
    cloud computing demands high processing performance and the capability to manage
    dynamic changes. To take full advantage of the optimizing cloud computing, the
    ability to support the virtual machine or Docker’s quick deployment is also critical
    in ensuring the high efficiency of this spatiotemporal analysis engine. Our research
    develops an integrated GIS platform architecture which enables spatiotemporal
    big data storage, processing, visualization and analysis. 3. An integrated GIS
    platform architecture for spatiotemporal big data A variety of big data platforms
    yielded low spatial data storage, spatial analysis and spatiotemporal visualization
    performance. We proposed an integrated GIS platform architecture for spatiotemporal
    big data (Fig. 1), which contains large-scale virtual storage, a distributed computing
    framework, cloud computing and integration, stream data processing, 3D and virtual
    reality, being rapidly applied across the multi-terminal, the open source community,
    and container and continuous delivery. Download : Download high-res image (742KB)
    Download : Download full-size image Fig. 1. Architecture of the GIS platform for
    spatiotemporal big data. 3.1. Massive spatial virtual storage In big data systems,
    a critical issue is data storage. As the data are being generated with high data
    type variety and low-value density, the traditional file systems and databases
    can no longer maintain a high performance while continuing to satisfy big data
    storage requirements. In recent years, technologies and solutions in virtual storage
    have emerged, many of these have been widely used by internet platforms. For geospatial
    data, there is also a need to evolve traditional file systems and relational database
    storage solutions to distributed, virtual and software defined storage system
    so that storage scalability and processing capability can meet future challenges.
    The virtual storage system can be classified into three categories: the distributed
    file system, the distributed relationaldatabase, and the NoSQL/NewSQL storage
    system. The distributed file system is mainly used to solve the issue of limited
    storage space and the high cost of a single machine system. Running concurrent
    I/O with multi-replication copies not only increases the computing bandwidth,
    but also enhances the system’s load balance, error tolerance, and dynamic scalability.
    The system can be deployed in a cloud computing environment with the support of
    a large file size, memory cache, space sharing, and REST web services. One popular
    database of this type is Hadoop; Other similar systems include Ceph, and IPFS.
    The distributed relational database is mainly implemented by adding newly distributed
    clusters and distributed transaction processing features in traditional databases
    (Examples of implementation include PostgreSQL cluster, MySQL cluster, and CrateDB
    based on Docker technology). Because of the high compatibility with the original
    databases, these systems can better support SQL and transaction processing. Since
    the original management methods and software can still be applied, data migration
    and system scaling becomes easier. As most of these systems are open-source, the
    cost is relatively low; this is important especially when the system needs to
    be deployed in multi-nodes cluster environment. The NoSQL/NewSQL storage system
    focused on reducing the number of ACID transactions so that its data processing
    performance can be significantly improved. When managing various unstructured
    data, the system not only simplifies the development and maintenance processes,
    but also lowers the total cost of operation (TCO). This kind of solution has been
    widely used in many internet platforms as well, such as MongoDB, HBase, Cassandra,
    Redis, etc. Today, many different virtual storage systems exist in various kinds
    of environment and are being used in diverse ways. How do we fully utilize the
    advantages of each system while enabling the sharing and transferring of resources
    between systems? How to provide a unified way of visiting, read and write data
    while having the ability to store data in diverse platforms so that the data become
    more valuable? To solve these issues, we designed and developed a virtual spatiotemporal
    integrated service system — DaaS (Data as a Service) based on a seamless integration
    of multi-source spatial data in SDX+ and the interface in GDB-CLI [28]. We implemented
    a unified REST service framework that can easily connect with multiple types of
    data storage systems and work with the existing connected database systems at
    the same time. This system supports distributed, multi-level spatial database
    storage services, and cloud/local data management in one portal. By using its
    unified data interface, the system can connect with Hadoop storage ecosystem,
    the MongoDB storage system, the PostgreSQL cluster, the MySQL cluster, and other
    existing databases (Fig. 2). With an increase of in the demand for storage space
    and an increase in maintenance cost, the value of data has been on the decline.
    If we can consume data within a reasonable amount of time, it may become a more
    precious asset. In contrast, if data is not being used properly, it can become
    a burden to a business. For example, without enough investment in data security,
    a company runs the risk of leaking sensitive data, which could be detrimental
    to the company. Simply owning data does not benefit a business. In fact, how productively
    data is used determines its value. The more that we consume data, the more value
    we can derive from it. Therefore, establishing a continuous data processing infrastructure
    to meet the needs of the application is vital. Additionally, maintaining and applying
    the data value is a critical aspect in developing a big data system. Download
    : Download high-res image (646KB) Download : Download full-size image Fig. 2.
    From SDX+ to DaaS. 3.2. A distributed computing framework When Moore’s law reaches
    its end, it is difficult to pursue further processor speed by increasing the CPU’s
    clock rate. Instead, multicore CPU becomes the new normal. By using multi thread
    and process technologies to manage and parallel process tasks or using graphic
    cards’ CUDA and the OpenCL parallel computing mechanism, the system can break
    through the limitation of computing capability within a single CPU. In SuperMap
    8C, the multi-thread support, the multi-process service, and the spatial analysis
    algorithm based on OpenMP, CUDA significantly improves the efficiency of spatial
    data processing and model analysis. It enables the object visualization capability
    to operate in real-time. With the merging of big data, computing power encounters
    its bottleneck. The multi core CPU and large-scale cluster system are needed to
    adapt the changes. Designed for batch processing, MapReduce module in Hadoop is
    considered a pioneer in the new generation of distributed computing. However,
    it has a number of weaknesses. These weaknesses include a slow starting speed,
    complicated deployment, and an inability to perform regression computing. Built
    on a distributed memory computing model and on Flink which better supports stream
    computing the module has started to be replaced by Spark. The Hadoop/Spark open
    source ecosystem led by the Apache software foundation has become the standard
    in the big data field. Many business solutions have been built based on this framework.
    (These business solutions include the big data service clouds from Databricks,
    Amazon, IBM, and Oracle.) With the advancement of GPS systems, satellite imagery,
    drone photography, and smart measurement devices, the requirements for spatial
    data storage and processing have been rapidly increasing. Thus, importing GIS
    functionality into the Spark framework to build an integrated distributed spatial
    and temporal data processing platform has taken on new importance. The latest
    SuperMap GIS platform provides full support to the Spark computing framework.
    It establishes a complete big data solution with three main components, a GIS
    core engine, a client SDK, and an application system. The GIS core engine can
    either be imported into the Spark environment as Scala or be implemented in a
    different frontend big data analytics software by supporting Python. By integrating
    iObjects for the Spark service into the iServer product series, a distributed
    spatial analysis model computing service can be exposed via REST. Its return results
    can be consumed and visualized easily at applications with iObjects, iDesktop,
    iDesktop Cross, iMobile, iClient, and with other 2D/3D linkage clients (Fig. 3).
    It is a huge advancement toward moving the GIS core functions from single core
    CPU to a distributed computing framework. With this move, the GIS system will
    be able to take full advantage of the capabilities of large-scale storage, distributed
    memory, cluster management, and its deployment brought on by modern computing
    hardware and data centers. This move will also solve issues in the traditional
    GIS software: such as a lack of storage and deficient computing power. It makes
    it possible to build a large-scale application system or to conduct spatial relationship
    research at a high accuracy level. We will likely be seeing numerous types of
    applications and breakthrough development in geography spatial models or algorithms.
    Not only will it bring GIScience and geography science to a new level, but it
    will also improve the efficiency in environmental management, disaster management,
    urban planning, etc. Download : Download high-res image (440KB) Download : Download
    full-size image Fig. 3. The Architecture of massive GIS clusters. 3.3. Cloud computing
    integration Cloud computing provides a set of models and methods for sharing computing
    resources. Allocating computing resources dynamically, not only enhances system
    utilization efficiency, but also makes gathering large scale computing power in
    a short amount of time possible. Amazon, Google, Microsoft, and IBM all provide
    cloud data center services on a large scale. In China, Ali Cloud, Baidu Cloud,
    and Tencent Cloud also offer diverse cloud computing services. In recent years,
    many startups have begun to provide services based on Docker technology, such
    as Qiniu and QingCloud. All of these cloud computing platforms allow users to
    manage the computing resources, lease resources based on demand and quickly establish
    a large-scale cloud computing cluster. In the past, the traditional server leasing
    service was the main focus. Today, the distributed cloud computing cluster based
    on Hadoop/Spark has become the standard service of large data centers. With the
    rapid development of Docker container technology, the cloud computing service
    it is based on can further lower the cost of maintenance and provide more flexible
    and agile solutions to allocate and deploy resources. Services’s migration between
    different data centers or between public cloud and private cloud centers, is also
    made significantly easier with Docker technology. To sum up, the cloud computing
    service has been moving from server leasing services based on the virtual machine
    to the distributed cluster services and micro services based on recent technologies
    such as Docker, Hadoop/Spark, etc. In Docker, cloud services can be encapsulated
    by a business component as micro services and can be assembled based on demand
    during deployment. The Docker instance can be developed, tested, run, and deployed
    as needed within the public cloud, dedicated cloud, industry cloud, and private
    cloud in a streamlined way. This will greatly reduce the maintenance cost and
    development difficulty of a cloud computing service. A GIS cloud computing integration
    infrastructure must fully integrate with the Docker technology, and design, develop,
    and deploy the system based on the micro services concept model. SuperMap iServer,
    iExpress, iPortal and iManager have already been supporting Docker; The service
    based on its technical standards and micro services structure can be deployed
    to diverse cloud computing data centers. Other features like integration among
    different types of computing infrastructures along with functions for automatic
    management system are all included as well. Additionally, the enterprise user
    and the personal user can directly access these services via Dituhui or the online
    portal (Fig. 4). By implementing the micro service infrastructure based on Docker,
    the GIS system can be deployed as a cloud computing module, and the multi-clouds
    integration and management can be unified. We can also fully integrate geospatial
    big data into the cloud computing infrastructure. All of these features have become
    the core capabilities of a modern data center and have even become essential system
    components in the smart city, environmental resources, and many other industries.
    It also serves the following core functions of geospatial data management, spatial
    pattern analysis, geospatial data visualization, API sharing, and other application
    services. Download : Download high-res image (610KB) Download : Download full-size
    image Fig. 4. The micro services architecture based on cloud computing and Docker.
    3.4. Streaming data processing With the development of GIS technologies, the data
    sources of the GIS system have changed tremendously. In the past, data mainly
    came from traditional map digitization and measurement input via devices like
    the plane table, total station, etc. The common data format was the static vector
    map, which lacks update accuracy, and currency. The new measurement works extensively
    using photogrammetry method to collect the raw data. The main data sources include
    imagery, video, radar, and GPS data, which are generated from satellite, airplane,
    drone, and measurement vehicles. The latest devices such as panoramic camera,
    the street view camera, the observation satellite, and the LiDAR system are capable
    of retrieving omnidirectional images and spatial information. Some of these devices
    support streaming services so that the data can be dynamically transferred to
    users. Today, the traditional static data storage, static cartography, and scheduled
    data update methods have become less critical. It also leads to a tremendous change
    in traditional data storage, processing, analysis, and usage methods. LiveGIS
    – a new feature of GIS – has the ability to generate, process, and consume live
    data via streaming [29]. Due to the changes in data types and the increase in
    processing data volume, the GIS system structure has been evolving to adapt this
    revolution. There are currently several preferred streaming practices using distributed
    computing as the system structure, using Spark Stream as the stream data framework,
    integrating message-oriented middleware such as Kafka, and combining the message
    receiving, processing, and high efficiency data storage with real-time spatial
    analysis as a spatial temporal integrated software platform which satisfies LiveGIS
    demands. There are already numerous successful solutions have been used in eCommerce,
    social media, logistics, and transportation industries. The latest SuperMap GIS
    platform has integrated this system solution with advanced GIS functions so that
    the streaming data can take advantage of GIS spatial analysis and the visualization
    feature. This platform greatly enriches the traditional GIS system’s capability
    and usages. The capability of backend processing along with the flexibility of
    mobile applications can provide a reliable platform for IoT and smart devices
    to process its spatial temporal data (Fig. 5). Not only is it scalable with the
    development of business scope, but it can also quickly migrate between environments.
    To sum up, it has become the core foundation of smart city’s development and operation.
    Download : Download high-res image (850KB) Download : Download full-size image
    Fig. 5. The process diagram and architecture for GeoStreaming data. 3.5. 3D and
    virtual reality In recent years, 3D related information technologies have been
    making great progress. With the advancement of graphic’ card processing capability,
    the supported software standards and techniques such as OpenGL, OpenCL, WebGL,
    etc. have rapidly evolved. The breakthrough of the VR/AR headset and glasses brings
    digital 3D application into a new era. Owing to the IT revolution, two critical
    improvements in GIS have been made: oblique photography integration and 2D–3D
    linkage capabilities. From retrieving full range 3D geospatial data to building
    models, the process of consuming data in the end terminal application has been
    streamlined. The SuperMap platforms’ 3D GIS technique has been built in every
    product, providing comprehensive solution for importing data, publishing service,
    analyzing application, gaining web access, and improving mobile Apps. It is compatible
    with diverse server types, components, mobile platforms, web, desktop software,
    existing databases, cloud computing services, and other IT infrastructures. By
    integrating BIM technology with GIS, we can further apply this GIS system to several
    micro management areas. For example, it can be used for building parts and managing
    component objects. We can also use it to develop the support system for smart
    buildings or IoT networks. By integrating VR/AR with GIS, the city planning and
    management can offer a richer user experience which could enhance the public service
    quality in land management, municipal administration, urban planning, etc. The
    mobile 3D GIS feature not only simplifies the data collection process, but also
    provides powerful on site management function. Furthermore, it creates a public
    IT platform that allows users to work on further spatial planning, application,
    and optimization (Fig. 6). The current 3D GIS has become a core component in the
    spatial temporal big data system. However, the future 3D GIS will go beyond the
    current 3D GIS and simulate the real world. It will also support the actual instance
    model’s bool operation. Additionally, importing the physics engine and the collision
    detection algorithm into GIS will make the simulation of the model and of the
    spatiotemporal environment more realistic. It will advance the business applications
    in planning, designing, pipeline, transportation, construction, etc. The future
    3D GIS will also influence new advancements like high-accuracy navigation, the
    self-driving car, and airport management. Download : Download high-res image (683KB)
    Download : Download full-size image Fig. 6. The 3D GIS technology and solution.
    3.6. Quick multi-terminal application Software is like a magnifying glass for
    data value. The more that data is being used, the more value that it generates
    and the more software compatibility is required. Not only does the software need
    to have powerful data capability, it also has to have diverse application compatibility.
    Lastly, it should work in different environments and with all mobile devices.
    The client end can be classified in devices, operating systems, hardware infrastructures,
    and programming languages. The more types of client side it supports, the more
    compatible it will be. It also means that more users are exposed to generating
    more value for the data. The SuperMap GIS product family offers very rich client-end
    support. The iDesktop based on.NET and iDesktop built on Java can directly access
    cloud computing resources and a large volume of storage. It has the functions
    for professional GIS users to process data, generate maps, and analyze spatial
    patterns. The iClient provides WebGIS functions which are compatible with different
    browsers. Its functions including accessing server shared data, executing online
    analysis, and visualizing scenes can be used on multiple operating systems without
    plugin software installation. The iMobile not only provides SDK for iOS and Android
    development, it also supports YuanXin OS and other embedded operating systems.
    Since GIS functions are made to be easily accessible and portable, numerous applications
    have been developed on handheld platforms by SuperMap partners or other vehicle
    measurement devices to meet their own professional needs. SuperMap is the GIS
    platform that supports the largest number of terminals. It offers SDK on desktop,
    web, mobile, and supports accessing cloud services via API (Fig. 7). Users can
    develop all-purpose applications by using the given SDK and plugin framework from
    SuperMap. Many native Chinese CPU brands are supported, such as FeiTeng, and Loongson,
    along with the Chinese operating system, Kiron OS. In sum, the progress made in
    improving the compatibility of the GIS system will reveal system’s extra potential
    from the big data and generate more data value. Download : Download high-res image
    (657KB) Download : Download full-size image Fig. 7. The multi-client GIS products.
    3.7. Open source and the open source community development Open source and its
    communities are considered boosters for the modern software industry. Especially,
    the emergence of github.com makes it possible to have developers around the world
    work on the same code repository and communicate with other developers, significantly
    improving software development productivity. In China, there are several platforms
    similar to Github, which offers source code management and sharing services (One
    such platform is Oschina.net). The open source software and online source control
    model change the traditional development of closed research and development. It
    makes the development process public and connects the developer with entire user
    community. Thus, in turn, enhances developers’ productivity and the quality of
    their work. Large scale development cooperation becomes possible. The open source
    software system represented by Apache Hadoop/Spark has become the essential foundation
    in the big data ecosystem. There are already many published and business solutions
    which were built based on this framework. By integrating with the Apache big data
    ecosystem and working with the open source community, the GIS system can continue
    to evolve at its steady pace. The open source big data software provides unprecedented
    computing power via its distributed structure methodology. This leads to tremendous
    improvements in traditional GIS solutions. Adding geospatial data types to the
    open source big data solution also benefits the open source community users’ usage
    of the spatial data. The latest version of the SuperMap GIS platform is built
    based on the Spark 2.x framework. It implements the big data analysis engine and
    the distributed computing feature. The big data processing, and analyzing efficiency
    greatly improves, and the maximum supported data size increases as well. Additionally,
    the open source iClient, iDesktop Cross, and iObjects’ Python scripts can all
    process and publish big data (Fig. 8). Open source and closed source software
    have their pros and cons. The advantages of open source software are strong developer
    involvement, fast update speed, and a high level of innovation. On the other hand,
    closed source software excels at testing, version control, persistence, and professional
    support. Today, it is rare to see a solution that is completely open source or
    closed source. Combining the advantage from the two to build the final solution
    will be the main realistic method. Download : Download high-res image (959KB)
    Download : Download full-size image Fig. 8. The integration architecture of GIS
    and open source software. 3.8. Container technology and continuous delivery Boosting
    by internet technologies, the software development methodology has been evolved
    completely. With the emergence of Git/Gitlab/Github, distributed version control
    has replaced the traditional centralized software development methodology. Today,
    community development, public code review, auto testing, and continuous integration
    have become the standard development methodology. Compared with virtualization,
    the Docker container can be deployed at the bottom level of the system and run
    directly on top of the Linux kernel. Docker allows the user to compile software
    as a package and to isolate the running environment. By implementing Docker, a
    customizable micro services system framework can be easily established. Docker
    also shortens the system deployment time and simplifies the migration process
    between data centers. To meet the latest demands of the online platform, the continuous
    delivery concept and the DevOps method have been making great progress. The integration
    with the cluster management system such as Mesos, Kubernets, etc. has been developed
    as well. The automated processes on the container framework and the continuous
    delivery methodology greatly shorten the time of the software update and bugs-fixing
    and improve the software development’s responsiveness. This quick iteration leads
    to a faster software innovation and to fewer system risks. To implement the quick
    iteration, live testing, runtime verification, and controlled delivery features,
    we categorized the system into three deployment zones, the development zone, the
    validation zone, and the production zone. includes development tools, the use
    case library, and the testing system, in addition to testing data and the source
    code. The validation zone includes the validation data, the validation system,
    and the evaluation system. The production zone includes the production system,
    which contains the current running system and the latest updated system. This
    makes it possible for gray release, which is the act of migrating to the new version
    of the system via the AB testing method. To ensure the reliability of its GIS
    platform software and to solve the complications brought on by the multi-version
    issue, the SuperMap research team has developed the continuous delivery system,
    which covers the entire solution. It establishes the automated workflow of software
    development, integration, and testing. To move toward the online platform, the
    Dituhui and online service portal have gradually built the framework to support
    continuous delivery and DevOps. So far, SuperMap iServer, iExpress, iPortal, and
    iManager products have already endorsed Docker and the Micro service framework.
    These features have been integrated into some continuous delivery systems and
    have become some of the core components of the IT infrastructure (Fig. 9). The
    spatial temporal big data system must have the capability to process the dynamic
    streaming data. Also, the software system itself should be able to evolve and
    update. The container and continuous delivery methods make software migration
    a smoother process, ensuring that the system and the data can be deployed as needed.
    This leads to better efficiency and improved system usability. The development,
    testing, validation, deployment, and production maintenance/management/update
    in traditional software development will all be integrated. The system will achieve
    a fast response time, runtime bug fixing, and update features that do not require
    downtime. In short, using the cloud computing framework, the virtualization technique,
    and the container technique to build a system with continuous delivery and DevOps
    workflow, will be the mainstream trend in future software development. It will
    become a necessary step to adapting to the challenges of big data. Download :
    Download high-res image (641KB) Download : Download full-size image Fig. 9. DevOps
    and continuous delivery flowchart. 4. Implementation of cloud-terminal integration
    GIS for spatiotemporal big data The SuperMap GIS platform is based on the Cloud-terminal
    integration technology, which consists of the intensive GIS cloud platforms, the
    diversified GIS terminals, and the integration of GIS cloud platforms and GIS
    terminals. The intensive GIS cloud platforms make full use of the cloud computing
    resources and provide high available GIS services. The diversified GIS terminals
    integrate technology of desktop GIS, web GIS, and mobile GIS to support construction
    of GIS applications across multi terminal devices. The GIS cloud platforms and
    GIS terminals are integrated through technology of cloud-terminal interconnection
    and cooperation, realizing efficient interconnection and collaboration between
    clouds and terminals. In SuperMap GIS, the core GIS functions are based on the
    cross-platform universal GIS core (UGC) which was developed by standard C++. Because
    of the high compatibility of UGC with Spark and Scala, SuperMap GIS can be executed
    in Spark seamlessly. The Cloud-terminal integration GIS platform for spatiotemporal
    big data that we have designed (see Fig. 10) not only builds the iObject for Java
    inside Spark, but also supports extended development. For this reason, SuperMap
    GIS can support using PostgreSQL, Elasticsearch, HDFS and MongoDB to store its
    data, using the open-sourced GIS cross-platform desktop (SuperMap iDesktop Cross)
    to effectively visualize the data, and using the GIS application server (SuperMap
    iServer) to publish its data as services. It also supports many high-performance
    cloud computing infrastructures such as Docker. Based on the SuperMap iObjects
    for Java and the latest Apache Spark framework, we implemented the spatiotemporal
    big data engine shown in Fig. 11. Download : Download high-res image (473KB) Download
    : Download full-size image Fig. 10. The SuperMap GIS framework. The engine uses
    HDFS, MongoDB, PostgreSQL and Elasticsearch to store spatiotemporal big data.
    It also supports the processing different data formats such as vector, image,
    and stream data. It is compatible with all of the functions in the existing GIS
    system, with an enhancement of its data services and spatial analysis functions.
    By using the distributed data storage system and Spark’s cluster computing, GIS
    users can manage and analyze big data, which the traditional relation databases
    cannot accommodate. Download : Download high-res image (670KB) Download : Download
    full-size image Fig. 11. Technology roadmap of the spatiotemporal big data engine.
    Some customizations were made to the engine to extend Spark’s Resilient Distributed
    Datasets (RDD). The engine can not only support traditional GIS analysis and basic
    computing (e.g., spatial query, nearby analysis, interpolation analysis, buffer
    zone analysis, and overlap analysis), but also many advanced GIS computing and
    data analyses like cluster analysis, density analysis, hot zone analysis, map
    matching, and traffic computing. By using Spark streaming, users are also able
    to analysis the spatial stream data. Additionally, with the built-in GIScript,
    an open-sourced data processing system, and the R data statistic tool, users can
    use Python and R to perform spatial data analysis and spatiotemporal data mining.
    With this engine, iServer gains the distributed computing capability. The iDesktop
    users can utilize the web services offered by iServer to read and write the data
    from distributed cluster and to submit computing or analysis tasks back to the
    server. Designed to run on the delegated multi-user infrastructure, the engine
    was deployed in the OpenStack cloud computing environment and in the Docker container.
    The Docker container makes it possible for the user to build and manage a distributed
    computing cluster quickly and effectively. 5. Experiments To test the performance
    of the spatiotemporal big data processing and analysis in real-time using the
    architecture described in the previous section, we choose Apache Spark Streaming.
    For the storage of the real-time data, we used Elasticsearch. We propose a set
    of modules to build the program for processing the real-time data stream dynamically.
    Based on the combination of these modules and the Spark Streaming –to prove the
    increased usability of data processing and the substantially improved performance
    and throughput –we share an application example. We used over 1 billion records
    of global flight and around 0.15 billion records of worldwide shipping to carry
    out the experiments. An Apache Spark cluster was constructed based on 5 machines.
    The operating system of single node is 64-bit Ubuntu Linux (16.04). And the CPU
    is Intel core i7-6700 K. quad core processor. Other hardware configurations are
    quad core processors with a master frequency of 4 GHz, a hard disk of 1 TB, a
    memory of 16 GB and a hard disk of 16 GB. SuperMap iObjects Java and BDT Spark
    extension package are installed and configured on each node. Moreover, The Hadoop
    version is 2.7.3, and the Spark version is 2.1.0. Fig. 12 depicts the spatiotemporal
    big data stream processing in a flow chart. Real-time data, including location
    data (such as vessel or vehicle location), or other form of state data (such as
    air temperature, barometric pressure, PM2.5), can be received through Socket,
    HTTP and JMS. The parser allows CSV, JSON and GeoJSON data as input. Real-time
    big data computing components are used for receiving, filtering, mapping and processing.
    Output data can be exported to various locations and saved to Kafka/HDFS. Outputs
    can also be sent to JMS, Active MQ, and Socket. Results can be shown in many types
    of clients. Fig. 13(a) depicts the spatial query results of global shipping based
    on Elasticsearch. Fig. 13(b) displays where shipping routes were rebuilt. Download
    : Download high-res image (505KB) Download : Download full-size image Fig. 12.
    Flow chart of spatiotemporal big data stream processing. Fig. 14(a) shows real-time
    data of the monitor aircraft. A tracking map illustrates that 10 thousand long-distance
    air lines were tracked from 1 billion tracing points (Fig. 14(b)). We use spatial
    association relation near, determine whether the time interval of the two planes
    is within 10 s. It takes two minutes of submitting the task, the analysis results
    are showed on the screen. But in traditional GIS, it takes more than thirty minutes
    to finish same task (Table 1). Download : Download high-res image (432KB) Download
    : Download full-size image Fig. 13. (a) Grid spatial query; (b) Rebuild shipping
    routes. Table 1. Performance result. Data Operators Traditional GIS (min) Our
    method (min) 0.15 billion points Spatial query 3.23 0.45 0.15 billion points Routes
    built 8.37 1.18 1 billion tracing points Spatiotemporal query 31.12 2.01 Download
    : Download high-res image (403KB) Download : Download full-size image Fig. 14.
    (a) Real-time data of monitor aircraft; (b) Rebuild tracking line from global
    flight. 6. Conclusion and future work In this paper, we propose an integrated
    GIS platform architecture for spatiotemporal big data, which contains large-scale
    virtual storage, a distributed computing framework, cloud computing and integration,
    stream data processing, 3D and virtual reality, being rapidly applied across the
    multi-terminal, the open source community, and the container and continuous delivery
    model. This makes GIS integrate into the mainstream of IT and provide geospatial
    information technology with unprecedented opportunities. With the advancement
    in IT technology, key capabilities for processing spatiotemporal big data in GIS
    have improved tremendously. This progress has generated tremendous added data
    value for users. Large scale virtual storage technology allows users to maintain
    their data assets in long term, and maximize their data’s value. The distributed
    computing architecture allows users to process a big volume of data instantly
    and to analyze a spatial model in a short amount of time. Cloud computing technologies
    enhance the resources management and maintenance in data centers. Its integration
    between the public cloud and the private cloud helps to better satisfy the requirements
    from professional users. The capability to manage stream data makes spatialized
    instant collaboration possible, and also makes a contribution to social network
    analysis, IoT systems, and smart city systems. It has solved the issues of storing
    and processing data. The 3D and virtual reality technologies create a ‘digital
    image’ of the real world. They provide digitalized solutions with high precision
    for spatial resources management, city operation, and city maintenance. It can
    improve management efficiency and service quality in these fields. The multi-type
    platforms make it possible to use spatiotemporal big data anytime and anywhere.
    They help to fully recognize the value of big data. The development of open source
    software and its community not only helps the technologies to evolve and related
    professions to grow, but also helps users to solve issues, such as limitations
    in system suppliers, high system risks, and high maintenance costs. The container
    (Docker, etc.) technology and continuous delivery model make the integrated DevOps
    process become possible. It shortens the cycles of software release and bug fixing,
    facilitates the problem solving of system complications caused by an increase
    in software scale, and reduces the cost of unit operation. By integrating the
    latest big data technologies, software development methods, continuous delivery
    methodologies, 3D and virtual reality, and cloud computing with GIS, we have begun
    a new chapter in GIS. On one hand, it enhances the spatiotemporal analysis and
    visualization in various information systems, on the other hand, it provides a
    powerful geospatial data foundation to support diverse fields, such as resources,
    environment, energy, and city development. GIS has been transformed from the traditional
    applications for static cartography to the systems that could process and analyze
    dynamic, real-time spatiotemporal big data. It not only heralds greater potentials
    for sustainable development, geoformation science, and geography science, but
    also generates many unprecedented opportunities for related industries as well.
    Acknowledgments Our research was supported by the National Key R&D Plan (2016YFB0502004),
    the Independent Research Project of State Key Laboratory of Resources and Environmental
    Information Systems, Chinese Academy of Sciences (088RAC00YA), the Project of
    Beijing Excellent Talents (201500002685XG242), and the National Postdoctoral International
    Exchange Program (Grant No. 20150081). References [1] L. Qingquan, L. Deren, Big
    data GIS, Geomatics and Information Science of Wuhan University, Vol. 39, 2014,
    pp. 641–644. Google Scholar [2] Erqi W., Shaohua W. Technology trends prospects
    of the future development of GIS Bull. Surv. Mapp., 2015 (2015), pp. 66-69 Google
    Scholar [3] Li S., Dragicevic S., Castro F.A., Sester M., Winter S., Coltekin
    A., Pettit C., Jiang B., Haworth J., Stein A. Geospatial big data handling theory
    and methods: A review and research challenges ISPRS J. Photogramm. Remote Sens.,
    115 (2016), pp. 119-133 View PDFView articleView in ScopusGoogle Scholar [4] Yang
    C., Huang Q., Li Z., Liu K., Hu F. Big Data and cloud computing: innovation opportunities
    and challenges Int. J. Digit. Earth, 1 (2017), pp. 13-53 CrossRefView in ScopusGoogle
    Scholar [5] Yang C., Yu M., Hu F., Jiang Y., Li Y. Utilizing Cloud Computing to
    address big geospatial data challenges Comput. Environ. Urban Syst., 61 (2017),
    pp. 120-128 View PDFView articleView in ScopusGoogle Scholar [6] S. Wang, E. Zhong,
    E. Wang, Y. Zhong, W. Cai, S. Li, S. Gao, GISpark: A geospatial distributed computing
    platform for spatiotemporal big data, in: AGU Fall Meeting Abstracts, 2016. Google
    Scholar [7] Li Z., Yang C., Jin B., Yu M., Liu K., Sun M., Zhan M. Enabling big
    geoscience data analytics with a cloud-based, MapReduce-enabled and service-oriented
    workflow framework PLoS One, 10 (2015), Article e0116781 CrossRefView in ScopusGoogle
    Scholar [8] Li Z., Yang C., Liu K., Hu F., Jin B. Automatic scaling hadoop in
    the cloud for efficient process of big geospatial data ISPRS Int. J. Geo-Inf.,
    5 (2016), p. 173 View PDFView articleCrossRefGoogle Scholar [9] Sui D. Opportunities
    and impediments for open GIS Trans. GIS, 18 (2014), pp. 1-24 CrossRefView in ScopusGoogle
    Scholar [10] Dianzhi S., Xinyue Y., Tian G. Open GIS for big data: opportunities
    and impediments Prog. Geogr., 33 (2014), pp. 723-737 Google Scholar [11] Aji A.,
    Sun X., Vo H., Liu Q., Lee R., Zhang X., Saltz J., Wang F. Demonstration of Hadoop-GIS:
    a spatial data warehousing system over MapReduce Proceedings of the 21st ACM SIGSPATIAL
    International Conference on Advances in Geographic Information Systems, ACM (2013),
    pp. 528-531 CrossRefGoogle Scholar [12] F. Wang, R. Lee, Q. Liu, A. Aji, X. Zhang,
    J. Saltz, Hadoop-gis: A high performance query system for analytical medical imaging
    with mapreduce, Atlanta–USA: Technical report, Emory University, 2011, pp. 1-13.
    Google Scholar [13] Vo H., Aji A., Wang F. Sato: A spatial data partitioning framework
    for scalable query processing Proceedings of the 22nd ACM SIGSPATIAL International
    Conference on Advances in Geographic Information Systems, ACM (2014), pp. 545-548
    CrossRefView in ScopusGoogle Scholar [14] Eldawy A., Mokbel M.F. SpatialHadoop:
    A MapReduce framework for spatial data Data Engineering (ICDE), 2015 IEEE 31st
    International Conference on, IEEE (2015), pp. 1352-1363 CrossRefView in ScopusGoogle
    Scholar [15] Gao S., Li L., Li W., Janowicz K., Zhang Y. Constructing gazetteers
    from volunteered big geo-data based on Hadoop Comput. Environ. Urban Syst., 61
    (2017), pp. 172-186 View PDFView articleGoogle Scholar [16] Yu J., Wu J., Sarwat
    M. Geospark: A cluster computing framework for processing large-scale spatial
    data Proceedings of the 23rd SIGSPATIAL International Conference on Advances in
    Geographic Information Systems, ACM (2015), p. 70 View PDFView articleCrossRefGoogle
    Scholar [17] Yu J., Wu J., Sarwat M. A demonstration of GeoSpark: A cluster computing
    framework for processing big spatial data Data Engineering (ICDE), 2016 IEEE 32nd
    International Conference on, IEEE (2016), pp. 1410-1413 CrossRefView in ScopusGoogle
    Scholar [18] Zhang J., You S., Gruenwald L. Large-scale spatial data processing
    on GPUs and GPU-accelerated clusters SIGSPATIAL Spec., 6 (2015), pp. 27-34 View
    PDFView articleCrossRefGoogle Scholar [19] You S., Zhang J., Gruenwald L. Large-scale
    spatial join query processing in cloud Data Engineering Workshops (ICDEW), 2015
    31st IEEE International Conference on, IEEE (2015), pp. 34-41 CrossRefView in
    ScopusGoogle Scholar [20] Tang M., Yu Y., Malluhi Q.M., Ouzzani M., Aref W.G.
    Locationspark: a distributed in-memory data management system for big spatial
    data Proc. VLDB Endowment, 9 (2016), pp. 1565-1568 CrossRefGoogle Scholar [21]
    Xie D., Li F., Yao B., Li G., Zhou L., Guo M. Simba: Efficient in-memory spatial
    analytics Proceedings of the 2016 International Conference on Management of Data,
    ACM (2016), pp. 1071-1085 CrossRefView in ScopusGoogle Scholar [22] Zheng K.,
    Fu Y. Research on vector spatial data storage schema based on Hadoop platform
    Int. J. Database Theory Appl., 6 (2013), pp. 85-94 CrossRefGoogle Scholar [23]
    Wenwen C., Shaohua W., Ershun Z., Chenpu H., Liu X. Design and implementation
    of a new cross-platform open source GIS desktop software Bull. Surv. Mapp., 2017
    (2017), pp. 122-125 Google Scholar [24] Lee J.-G., Kang M. Geospatial big data:
    challenges and opportunities Big Data Res., 2 (2015), pp. 74-81 View PDFView articleView
    in ScopusGoogle Scholar [25] Zhang F., Zheng Y., Xu D., Du Z., Wang Y., Liu R.,
    Ye X. Real-time spatial queries for moving objects using storm topology ISPRS
    Int. J. Geo-Inf., 5 (2016), p. 178 CrossRefView in ScopusGoogle Scholar [26] Galić
    Z. Spatio-temporal data streams and big data paradigm Spatio-Temporal Data Streams,
    Springer (2016), pp. 47-69 CrossRefView in ScopusGoogle Scholar [27] Galić Z.,
    Mešković E., Osmanović D. Distributed processing of big mobility data as spatio-temporal
    data streams GeoInformatica, 21 (2017), pp. 263-291 CrossRefView in ScopusGoogle
    Scholar [28] Shaojun L., Ershun Z., Shaohua W., Xun Z., Qin Z., Jiong X. Research
    on Opening Geospatial database connectivity Computer Science and Network Technology
    (ICCSNT), 2013 3rd International Conference on, IEEE (2013), pp. 462-466 Google
    Scholar [29] Zhong E. Geocontrol and live geography: Some thoughts on the direction
    of GIS J. Geo-Inf. Sci., 15 (2013), pp. 783-792 Google Scholar Cited by (71) ReCovNet:
    Reinforcement learning with covering information for solving maximal coverage
    billboards location problem 2024, International Journal of Applied Earth Observation
    and Geoinformation Show abstract Urban flood risk assessment using Sentinel-1
    on the google earth engine: A case study in Thai Nguyen city, Vietnam 2023, Remote
    Sensing Applications: Society and Environment Show abstract A novel design and
    application of spatial data management platform for natural resources 2023, Journal
    of Cleaner Production Show abstract WS4GEE: Enhancing geospatial web services
    and geoprocessing workflows by integrating the Google Earth Engine 2023, Environmental
    Modelling and Software Show abstract Big data analysis and optimization and platform
    components 2022, Journal of King Saud University - Science Citation Excerpt :
    As IT technology goes Internet and mobile, data sources become more and more abundant,
    and unstructured data, such as website log, IoT device data, APP buried data,
    etc., appear on the basis of the original business database, which are several
    orders of magnitude larger than the previous structured data and put forward higher
    requirements for ETL process and storage (Sun and Zhang, 2020). The online nature
    of the Internet also pushes business needs to real-time, and it is becoming more
    and more common to adjust strategies based on current customer behavior, such
    as inventory management and operation management during promotions (i.e., both
    medium- and long-term strategies and short-term operations) (Wang et al., 2019).
    At the same time, the company''s business after the Internet has led to a sharp
    increase in the number of customers served at the same time, and some situations
    are difficult to fully handle manually, which requires automatic machine decision-making.
    Show abstract The architectural design and implementation of a digital platform
    for Industry 4.0 SME collaboration 2022, Computers in Industry Citation Excerpt
    : HewaNadungodage et al. (2019) introduce a Digital Environment to Enable Data-driven
    Science (DEEDS) which is a cross-domain, self-serve platform for data and computing
    that supports the entire end-to-end research investigation process. Wang et al.
    (2019) propose an integrated geographic information system platform architecture
    for processing and analyzing spatiotemporal big data. Kratzwald and Feuerriegel
    (2019) analyze different architectures used in question-and-answering (Q&A) systems,
    such as ontology-based and content-based Q&A systems. Show abstract View all citing
    articles on Scopus Dr. Shaohua Wang received a Diploma degree in mathematics from
    Beijing University of Chemical Technology, China, in 2006. He received Ph.D. degree
    in the field of Cartography and GIS from the University of Chinese Academy of
    Science in 2013. From 2013 to 2017, he is Postdoctoral Research Assistant at Institute
    of Geographic Sciences and Natural Resources Research, CAS. From 2016 to 2017,
    he is a visiting scholar in Geography Department of Geography, University of California,
    Santa Barbara. His major research interests include spatial cloud computing, spatiotemporal
    big data, spatial visualization and spatial optimization. Mr. Yang Zhong is a
    M.S. student in Information Systems & Technology with a concentration in GIS Solutions
    Development at Claremont Graduate University, California. He earned his B.A. degree
    in Software Engineering from Beijing JiaoTong University, China. He is currently
    conducting research in geospatial data visualization and integrating social network
    data into GIS. Prof. Erqi Wang received a Diploma degree and a Master degree both
    in Geography from Beijing Normal University, China. From 1998 to 2017, he is a
    Chief Architect at SuperMap Software Co., Ltd., Beijing. His primary research
    interests include spatial cloud computing, spatiotemporal big data, Open source.
    View Abstract © 2018 Elsevier B.V. All rights reserved. Part of special issue
    Spatiotemporal Big Data Challenges, Approaches, and Solutions Edited by Dimitris
    Zissis, Dimitris Askounis, Luca Cazzantti, Minos Garofalakis, Fabio Mazzarella
    View special issue Recommended articles Urban Mobility Data Management – The OPTICITIES
    Project and the Madrid Standardization Proposal Transportation Research Procedia,
    Volume 14, 2016, pp. 1260-1269 Jorge Alfonso, …, Tomás Melero View PDF Development
    of an enterprise Geographic Information System (GIS) integrated with smart grid
    Sustainable Energy, Grids and Networks, Volume 14, 2018, pp. 25-34 Atefeh Dehghani
    Ashkezari, …, Mahammed Albadi View PDF Spatial query based virtual reality GIS
    analysis platform Neurocomputing, Volume 274, 2018, pp. 88-98 Weixi Wang, …, Yan
    Yan View PDF Show 3 more articles Article Metrics Citations Citation Indexes:
    60 Captures Readers: 201 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply.'
  inline_citation: '>'
  journal: Future generation computer systems
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: An integrated GIS platform architecture for spatiotemporal big data
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ic2e.2015.23
  analysis: '>'
  authors:
  - Johannes Wettinger
  - Vasilios Andrikopoulos
  - Frank Leymann
  citation_count: 38
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2015 IEEE International Confe...
    Automated Capturing and Systematic Usage of DevOps Knowledge for Cloud Applications
    Publisher: IEEE Cite This PDF Johannes Wettinger; Vasilios Andrikopoulos; Frank
    Leymann All Authors 32 Cites in Papers 1174 Full Text Views Abstract Document
    Sections I. Introduction and Problem Statement II. Motivation III. Devops Knowledge
    Management Methodology IV. Devops Knowledgebase V. Devops Knowledge Utilization
    Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: DevOps is an emerging paradigm to actively foster the collaboration
    between system developers and operations in order to enable efficient end-to-end
    automation of software deployment and management processes. DevOps is typically
    combined with Cloud computing, which enables rapid, on-demand provisioning of
    underlying resources such as virtual servers, storage, or database instances using
    APIs in a self-service manner. Today, an ever-growing amount of DevOps tools,
    reusable artifacts such as scripts, and Cloud services are available to implement
    DevOps automation. Thus, informed decision making on the appropriate approach
    (es) for the needs of an application is hard. In this work we present a collaborative
    and holistic approach to capture DevOps knowledge in a knowledgebase. Beside the
    ability to capture expert knowledge and utilize crowd sourcing approaches, we
    implemented a crawling framework to automatically discover and capture DevOps
    knowledge. Moreover, we show how this knowledge is utilized to deploy and operate
    Cloud applications. Published in: 2015 IEEE International Conference on Cloud
    Engineering Date of Conference: 09-13 March 2015 Date Added to IEEE Xplore: 23
    April 2015 Electronic ISBN:978-1-4799-8218-9 DOI: 10.1109/IC2E.2015.23 Publisher:
    IEEE Conference Location: Tempe, AZ, USA SECTION I. Introduction and Problem Statement
    Today, it is vital for many software vendors and providers, especially in the
    field of Web applications and software-as-a-service (SaaS) applications to frequently
    and continuously roll out updates of an application. This is because users and
    customers expect new features and bug fixes to be available as quickly as possible.
    Consequently, a critical competitive advantage can be achieved by meeting these
    expectations and implementing mechanisms to drastically shorten application release
    cycles. Thorough and holistic automation is required to enable frequent and continuous
    software delivery [1]. DevOps is an emerging paradigm [2]–[4] to eliminate the
    split and barrier between developers and operations personnel: “DevOps is a mix
    of patterns intended to improve collaboration between development and operations.
    DevOps addresses shared goals and incentives as well as shared processes and tools.
    Because of the natural conflicts among different groups, shared goals and incentives
    may not always be achievable. However, they should at least be aligned with one
    another.”[5] Tight collaboration between developers and operations (DevOps) as
    well as holistic end-to-end automation (e.g., in the form of a fully automated
    deployment pipeline [1]) is required to enable continuous delivery. A huge and
    confusing variety of tools, reusable artifacts, and services are available today.
    Prominent examples are the Chef configuration management framework [6], the Jenkins
    continuous integration server, and Docker as an efficient container virtualization
    approach. The open-source communities affiliated with these tools are publicly
    sharing reusable artifacts to package, deploy, and operate middleware and application
    components. For instance, Chef''s Ruby-based domain-specific language [7] can
    be used to create and maintain cookbooks, which are basically scripts to automate
    the deployment and wiring of different components of an application stack. Alternatively
    or additionally, Docker container images can be built to package certain components
    in an isolated and reusable manner. There are further DevOps tools and corresponding
    artifacts such as Juju with Juju charms and Puppet [8], [9] with Puppet modules
    that can be used alternatively or complementary. DevOps approaches are typically
    combined with Cloud computing [10] to enable on-demand provisioning of resources
    such as virtual servers and storage in a self-service manner. Different interfaces
    are offered by Cloud providers (e.g., Amazon) such as graphical user interfaces,
    command line interfaces, and APIs to provision and manage these resources. Especially
    APIs and command line interfaces are an efficient means to integrate DevOps automation
    approaches with Cloud resource management programmatically. This can be done for
    public, private, and hybrid Cloud scenarios. As an example, Amazon''s APIs can
    be utilized to provision virtual servers on which Chef cookbooks or Juju charms
    are executed to deploy a certain application stack. Moreover, some Cloud providers
    offer higher-level services such as middleware services (e.g., runtime-as-a-service,
    database-as-a-service, etc.) and operations automation services, abstracting from
    the underlying infrastructure. These services can be used alternatively or complementary
    to the lower-level infrastructure services. Because DevOps automation approaches
    and Cloud services appear, change, and disappear rapidly, it is hard to choose
    the most appropriate solutions and combinations thereof to implement holistic
    DevOps automation for a specific application. Informed decision making is hard
    because of the huge variety of options. DevOps knowledge is practically available
    at large scale, but it is not systematically captured and managed. However, providing
    solutions for this DevOps knowledge management problem is of the utmost importance
    to end up with an efficient DevOps automation and collaboration process to enable
    continuous delivery. The major contributions of this paper can therefore be summarized
    by: A holistic approach toward systematic DevOps knowledge management. The design
    and implementation of a DevOps knowledgebase and the tooling required to populate
    it. An automated crawling approach to discover and capture DevOps knowledge. The
    remainder of this paper is structured as follows: Section II outlines a motivating
    scenario that is used as running example for this paper. We present a holistic
    methodology to enable DevOps knowledge management in Section III. Because the
    DevOps knowledgebase is the core of our methodology, Section IV discusses DevOps
    knowledge classification and a prototype implementation in detail. Section V discusses
    the utilization of DevOps knowledge based on specific DevOps requirements. Finally,
    Section VI concludes the paper. SECTION II. Motivation As a motivating scenario
    and running example for our work we consider the deployment and operations requirements
    of WordPress, which is a popular open-source blog application. WordPress requires:
    (i) a MySQL database server version 5.0 or greater, (ii) a PHP runtime version
    5.2.4 or greater, and (iii) a Web server such as Apache HTTP Server or Nginx,
    In order to enable continuous delivery of new versions of WordPress (bug fixes,
    new features, etc.), these requirements have to be considered when implementing
    an end-to-end DevOps automation and collaboration process. Figure 1 presents an
    overview of the middleware components required to run WordPress. Moreover, different
    deployment options are outlined to make these middleware components available
    to the application, satisfying WordPress'' requirements. As an example, an existing
    Amazon Machine Image (AMI) could be used to run a complete LAMP stack on a single
    virtual machine (VM) to operate WordPress. However, in this case resources are
    limited to a single VM, and as such scalability options are limited. Thus, open-source
    deployment scripts such as the MySQL cookbook and the PHP application cookbook
    (Chef) could be used to split the middleware across two VMs, running in a Linux
    or Windows VM. In case the scalability of the MySQL database server needs to be
    further improved, the MySQL charm may be used as a set of operations scripts to
    run a MySQL database cluster in a master/slave setup: data that are written to
    the master instance are consistently replicated to the slave instances, so reading
    requests can be load-balanced between slave instances. However, there is a constraint:
    Juju charms can only be deployed on Ubuntu-based VMs. Fig. 1. Middleware components
    of WordPress and deployment alternatives Show All To go one step further and provide
    elastic middleware for WordPress to scale in and out dynamically and automatically
    depending on the current workload, solutions such as database-as-a-service and
    runtime-as-a-service [11] offerings can be used instead of maintaining VMs. For
    instance, Amazon Elastic Beanstalk and Google App Engine provide PHP runtime environments
    transparently as a service without seeing or maintaining the underlying infrastructure
    such as VMs. Amazon Relational Database Service (RDS) may be used as MySQL database-as-a-service
    to satisfy the MySQL requirement of WordPress. One downside of this elastic middleware-as-a-service
    approach is its limited configurability. For instance, a MySQL database hosted
    on a VM can be arbitrarily tuned and configured, whereas a database instance offered
    as a service only provides predefined configuration options Therefore, even for
    an application with a few deployment and operations requirements such as WordPress,
    there exists a great variety of alternatives. Because of the individual benefits
    and drawbacks of each alternative, it becomes difficult for application designers
    to decide on how to easily and efficiently deploy their application. Thus, the
    major challenge we tackle with our work is how to systematically capture, link,
    and utilize DevOps knowledge as a foundation for informed decision making during
    application design and deployment. For this purpose, a methodology and system
    to collaboratively maintain and use the captured knowledge is required because
    different parties such as developers and operations personnel may be involved.
    The following section presents a holistic approach toward this goal. SECTION III.
    Devops Knowledge Management Methodology When developing and operating applications
    such as WordPress, a large variety of alternatives exist in choosing appropriate
    infrastructure and middleware solutions to fulfill DevOps requirements. Figure
    2 outlines our proposal for a holistic DevOps knowledge management approach with
    the goal to provide the means to systematically resolve DevOps requirements. More
    specifically, DevOps knowledge is currently spread across different sources in
    the form of unstructured and semi-structured data. Public repositories, for instance,
    provide semi-structured data in the form of reusable artifacts such as scripts,
    templates, and images (e.g., preconfigured VM images) to operate middleware and
    application components. Prominent examples are the Chef cookbook repository, Puppet
    Forge, Docker Hub, and Amazon Web Services'' Marketplace. Crawlers can be used
    to discover such artifacts in an automated manner because the artifacts are annotated
    with meta data such as dependencies, categories, and input/output data specifications.
    Ratings associated with these artifacts may be discovered by the crawlers, too.
    Figure 2. Holistic devops knowledge management (overview) Show All Discovering
    unstructured data in a fully automated manner is much more challenging because
    natural language processing techniques such as document classification [12] need
    to be utilized. These approaches do not always lead to correct results, so the
    resulting knowledgebase may become inconsistent quickly. Thus, additional sources
    of input can be used to discover and rate DevOps knowledge. For instance, experts
    are able to analyze the documentation and sources of DevOps tools (Chef [6], Puppet
    [8], etc.) to operate middleware and application components, libraries (fog, jclouds,
    etc.) to manage Cloud resources, and services (Heroku, Google App Engine, etc.)
    to utilize middleware-as-a-service offerings. However, discovery performed by
    humans is not limited to experts. Crowdsourcing approaches [13] may be used alternatively
    or in a complementary fashion. As an example, the growing open-source community
    centered around DevOps tools and artifacts such as Chef, Puppet, Juju, and Docker
    may follow such an approach to systematically consolidate DevOps knowledge. The
    knowledge discovered by crawlers, experts, and crowdsourcing efforts can then
    be captured, linked, and optionally refined in a DevOps Knowledgebase (KB) in
    a collaborative manner. This DevOps KB covers different levels of resources such
    as provisioning libraries on the level of infrastructure, deployment scripts on
    the level of middleware, and templates on the level of application stacks. As
    shown in Figure 3, the knowledge discovery and capturing is meant to be continuously
    repeated to refine and update the DevOps KB. As shown in Figure 2 and Figure 3,
    developers and operations personnel define DevOps requirements for a particular
    application (e.g., WordPress requires MySQL and PHP, as discussed in Section II).
    These requirements can be used to query against the DevOps KB to find viable options
    to resolve the requirements. Alternatively, the KB may be browsed, for instance,
    to get an impression what options are available to host a MySQL database. Then,
    the DevOps requirements of a particular application may be refined accordingly.
    Moreover, the knowledgebase can be updated, e.g., by adding deployment scripts
    for new middleware components required by an application. All these tasks are
    performed to eventually resolve the DevOps requirements of an application in order
    to deploy and operate it. By end-to-end monitoring the application, occurring
    issues on different levels can be identified. For example, if there are problems
    with a certain middleware component, its DevOps requirements may have to be refined
    and adapted. As such, the proposed approach promotes the continuous and iterative
    accumulation, organization, and utilization of knowledge over the lifetime of
    applications. Figure 3. Methodology to manage and use devops knowledge Show All
    SECTION IV. Devops Knowledgebase In the previous section we proposed a methodology
    to enable holistic DevOps knowledge management. The DevOps KB is the central component
    to implement a collaborative knowledge management system. Collaboration based
    on the KB is not limited to the efficient collaboration between developers and
    operations personnel. It further includes the collaborative discovery and capturing
    of DevOps knowledge by experts, crawlers, and crowdsourcing. In this section we
    focus on the conceptual structure of the DevOps KB to enable collaborative DevOps
    knowledge management as discussed in the previous section. Technically, the DevOps
    KB may be composed of multiple, possibly distributed knowledge stores. For instance,
    there could be public knowledge stores that are maintained by open-source communities.
    These stores may be focused on artifacts of certain kinds such as Chef cookbooks
    and Docker images. Private knowledge stores may be maintained by companies or
    departments for knowledge that is either very specific and/or is not meant to
    be available outside the scope of one organization. Consequently, the actual KB
    is a composition of multiple public and/or private knowledge stores. A. Knowledge
    Classification In order to classify existing DevOps tools, artifacts, and services
    in a systematic way, we propose a set of base taxonomies to categorize abstract
    entities and implementations. Figure 4. Middleware taxonomy Show All More specifically,
    knowledge is organized around middleware, infrastructure, providers, and DevOps
    automation tooling. Figure 4 presents an extract of our middleware taxonomy classifying
    different kinds of middleware such as runtimes, Web servers, and databases. These
    are captured as abstract entities, whereas implementations such as PHP application
    cookbook, LAMP Dockerfile, and MySQL charm can be instantiated to resolve specific
    DevOps requirements. This middleware taxonomy is based on the middleware categorizations
    of Cloud providers and DevOps tooling providers such as Heroku, Google, IBM Bluemix,
    and Chef. In addition, an infrastructure taxonomy providing categorized infrastructure
    implementations is required to define dependencies for middleware implementations
    that are not offered as a service. For instance, the MySQL cookbook middleware
    implementation requires an operating system such as Ubuntu or Amazon Linux to
    be hosted on, whereas MySQL on RDS can be used as a service provided by Amazon
    RDS without having to resolve further infrastructure requirements. Middleware
    implementations may not only require infrastructure implementations. Additional
    tooling may be required by middleware implementations such as operations tools
    (e.g. Chef, Docker, Juju, etc.). Moreover, tooling to build, test, and integrate
    middleware and application components may be part of the DevOps requirements for
    a particular application. All these supporting tools covering both development
    and operations, as well as integration aspects are categorized using an additional
    DevOpsware taxonomy. Some middleware implementations are offered as a service
    by certain providers such as MySQL on RDS. An additional provider taxonomy to
    classify provider implementations such as RDS offered by Amazon. These provider
    implementations are linked to the service implementations in the middleware taxonomy
    to define, for instance, that MySQL on RDS is hosted on RDS. To capture such relations
    between implementations (hosted on, requires, etc.) and further characteristics
    in the knowledgebase, properties are added as annotations to implementations.
    In the following we present an initial set of properties to annotate implementations:
    Figure 5. Properties of implementations Show All HOSTED_ON refers to one or more
    entities, either infrastructure or provider entities, on which this implementation
    can be hosted on. For instance, the MySQL cookbook may be hosted on Ubuntu, Amazon
    Linux, or Red Hat Enterprise Linux (RHEL). • REQUIRES refers to one or more entities,
    most likely DevOpsware entities, that are needed to operate this implementation.
    For instance, the MySQL cookook requires Chef. REPLACES refers to one or more
    entities that can be replaced by this entity. For instance, Amazon OpsWorks can
    be used to replace a Chef server. INT_WITH refers to one or more entities that
    are integrated with this entity. For instance, Amazon OpsWorks is integrated with
    Amazon RDS. VERSIONS defines one or more tuples to express which versions of this
    implementation are supported. For instance, a MySQL implementation supporting
    two versions: (‘MySQL’, ‘5.5’), (‘MySQL’, ߢ5.1’). SCALING defines the scaling
    mode (e.g., cluster, master-slave, etc.) of this implementation if scaling is
    supported at all. ELASTIC defines whether this implementation is elastic, meaning
    if it scales automatically and dynamically depending on the current load. Figure
    5 provides examples of properties for several implementations of different types.
    For instance, two of the four listed MySQL implementations support scaling in
    a master-slave manner, meaning additional slave instances can be added as replicas
    to load-balance read requests between them; the other two do not support scaling.
    Considering implementations of a PHP runtime, some of them are elastic, whereas
    others do not have any scaling capabilities. Furthermore, some implementations
    are bound to certain providers such as PHP on Google App Engine or LAMP AMI; others
    only have to be hosted on certain operating systems that may run on arbitrary
    VMs at any provider or even on-premise. Additional properties may be defined to
    further characterize implementations. As an example, runtime-as-a-service offerings
    such as PHP on Google App Engine do not always provide unlimited filesystem access.
    Moreover, files stored in the filesystem may not be persistent. Such information
    can be captured using properties because application components may rely on corresponding
    features depending on their architecture and design. Figure 6. Overview of prototype
    implementation Show All B. Devops KB Prototype Implementation An overview of our
    prototype implementation of a DevOps KB is presented in Figure 6, following the
    methodology proposed in Section III and the classification discussed in the previous.
    More specifically, we have discovered and systematically captured unstructured
    data from documentations and feature descriptions of Google App Engine and Amazon
    Web Services in corresponding knowledge stores. Currently, each knowledge store
    is represented as a single YAML file stored in a Git repository. Based on the
    provider taxonomy discussed in the previous Section IV-A, we captured infrastructure
    and middleware implementations offered by these two providers. We then categorized
    these implementations based on the infrastructure and middleware taxonomies outlined
    before. Furthermore, we implemented a crawling framework to automatically discover
    semi-structured data such as reusable scripts and configuration definitions such
    as Chef cookbooks from public repositories. Corresponding knowledge stores are
    automatically generated by the crawler as shown in Figure 6. In order to eventually
    build and render a consolidated, interlinked DevOps knowledgebase we implemented
    a knowledgebase builder as a Node.js module to merge individual knowledge stores.
    This is done by reading the contents of all knowledge stores and merging it into
    a hierarchically structured database. The technical foundation could be a single
    file rendered in JSON, XML, or YAML as it is implemented in our first prototype.
    But it could also be based on a database server such as a graph-based database
    server or a relational database server to use and implement more efficient query
    mechanisms. The resulting DevOps KB currently holds roughly 4000 implementations,
    including 1430 Chef cookbooks, 2190 Puppet modules, and 278 Juju charms captured
    as middleware implementations by the crawling framework; the rest are additional
    implementations of types infrastructure, provider, and middleware derived from
    provider offerings as well as DevOpsware implementations. Finally, we implemented
    a knowledgebase renderer as a Node.js module to render the knowledgebase in different
    formats such as JSON, YAML, and XML. This eases the usage of the knowledgebase
    in very different contexts. Both the knowledgebase builder and the renderer can
    be used programmatically and through a command line interface. SECTION V. Devops
    Knowledge Utilization In order to logically specify DevOps requirements for an
    application such as WordPress as outlined in Section II, predicates can be defined
    and combined in the form of Boolean expressions. These expressions can then be
    utilized as queries against the DevOps KB. Let''s assume E E is the domain of
    all entities captured in the taxonomies that are part of our DevOps KB. The predicate
    P requires : E→ {true, false} then assigns each entity (abstract entity or implementation)
    a Boolean value. The P requires predicate returns true if the given entity is
    an implementation or there is at least one implementation that inherits from the
    given entity; otherwise it returns false. In addition to E , let''s assume p is
    the domain of all properties and V is the domain of all property values; the predicate
    P propertyEq :E×P×V→ {true, false} then assigns each combination of an entity,
    a property, and a property value a Boolean value. The P propertyEq predicate returns
    true if the given entity owns the given property with the given value (in the
    DevOps KB), or there is at least one entity with the given property and value
    that inherits from the given entity; otherwise it returns false. The predicate
    P provertvEoGr :E×P×V→ . {true, false} is very similar to P propertyEq but it
    also returns true if the actual property value is greater than the given value.
    For instance, this predicate can be used to express version dependencies in the
    sense of a particular version or greater is required. The predicates P propertyEq
    and P propertyEqGr implicitly cover the P requires predicate because if a certain
    property of an entity needs to be equal to or greater than a given value, the
    entity itself is obviously required. As an example, we could use the expression
    P requires (‘Middleware/DB/…/MySQL’) to specify the need for a MySQL database
    in our application stack. In case we require specific versions, we can go with
    a refined expression such as P propertyEqGr (‘Middleware/DB/…/MySQL’, ‘versions’,
    ‘5.0’). Beside expressing application-specific DevOps requirements an organization
    may enforce further constraints that are valid independent of a specific application.
    Such constraints, as shown by the following examples, can be expressed as additional
    requirements: (i) MySQL has to be hosted on an Ubuntu VM; (ii) operating any component
    on Amazon is forbidden; (iii) using Chef for deployment is forbidden. Additional
    predicates may have to be defined to cover such requirements. For instance, we
    have to define the predicate P excludes : E→ {true, false} to express that we
    do not allow Amazon to be involved in any application stack that we operate: P
    excludes ( ′ Provider/Amazo n ′ ) . Such expressions can be merged with application-specific
    requirements as outlined before, to get a consolidated Boolean expression to be
    used as a query against the DevOps KB. We use the standardized Web Services Policy
    Framework (WS-Policy) [14] to render expressions as policies and merge such expressions.
    Finally, merged expressions can be transformed to WS-Policy normal form (basically
    a disjunctive normal form) to ease the processing of corresponding expressions
    and their usage for query purposes. As an example for the WordPress application
    discussed in Section II we can express its minimum requirements as: WordPres s
    minimum = P propertyEqGr (‘Middleware/DB/…/ MySQL ′ , ‘ versions ′ , ‘ 5.0 ′ )
    ∧ P propertyEqGr (‘Middleware/Runtime/ PHP ′ , ‘ versions ′ , ‘ 5.2.4 ′ ) ∧ P
    requires (Middleware/Web Serve r ′ ) View Source The WordPres s minimum expression
    can then be evaluated against the DevOps KB to see whether there are appropriate
    implementations to operate WordPress. Moreover, the expression is used to derive
    possibly multiple alternatives and combinations to operate the application based
    on the implementations captured in the knowledgebase. A few examples drawn from
    the prototype implementation discussed in Section IV-B are: LAMP AMI (middleware)
    hosted on Amazon EC2 (provider). LAMP charm (middleware) hosted on Ubuntu (infrastructure)
    hosted on Amazon EC2 (provider). PHP application cookbook (middleware) hosted
    on Ubuntu (infrastructure) hosted on Amazon EC2 (provider) & MySQL charm (middleware)
    hosted on Ubuntu (infrastructure) hosted on Amazon EC2 (provider). PHP on Elastic
    Beanstalk (middleware) hosted on Amazon Elastic Beanstalk (provider) & MySQL on
    RDS (middleware) hosted on Amazon RDS (provider). PHP on Google App Engine (middleware)
    hosted on Google App Engine (provider) & MySQL charm (middleware) hosted on Ubuntu
    (infrastructure) hosted on Amazon EC2 (provider). These are just several selected
    examples how the WordPress application can be deployed and operated. The requirements
    can be refined to limit the alternatives by additional constraints, e.g., by requiring
    a scalable MySQL implementation: WordPres s scalableDB = P propertyEqGr (‘Middleware/DB/…/
    MySQL ′ , ‘ versions ′ , ‘ 5.0 ′ ) ∧ (  P propertyEq ( ′ Middleware/…/MySQ L ′
    ,  ′ scalin g ′ ,  ′ master−slav e ′ ) ⊕ P propertyEq ( ′ Middleware/…/MySQ L
    ′ ,  ′ scalin g ′ ,  ′ cluste r ′ )) ∧ … View Source Similarly, expressions can
    be further refined by adding constraints such as P propertyEq (‘Middleware/Runtime/PHP’,
    ‘elastic’, true) and P excludes (''Provider/Amazon‘). These expressions are then
    rendered, merged, and normalized using WS-Policy to be used as queries against
    the DevOps KB. SECTION VI. Conclusion The DevOps paradigm is rising in prominence
    in contemporary information systems as the means for efficient, seamless end-to-end
    automated software management. The combination of DevOps approaches with Cloud
    computing solutions enables the rapid provisioning of infrastructure resources
    on demand. An ever expanding wealth of existing reusable DevOps artifacts and
    related Cloud services means that application designers and developers have ample
    opportunities for putting tried and tested DevOps knowledge into practice. However,
    deciding how to use this knowledge is hindered by the multitude of existing approaches,
    the scattering of knowledge between different communities and experts, and the
    width of the available design space in application design. In order to address
    the need for informed decision making, in the previous sections we presented our
    proposal for a holistic DevOps knowledge management methodology. More specifically,
    we identified a set of knowledge sources that can be (publicly) accessed, and
    the means to harvest the knowledge that is contained in these sources. This can
    be done in an automated manner based on our crawling approach. Finally, we discussed
    how to reflect, organize, store, and utilize the knowledge using a DevOps knowledgebase,
    predicate logic, and WS-Policy. Future work includes the improvement of the knowledge
    management approach by populating the DevOps KB with more offerings from Cloud
    providers, the evaluation of crawling techniques for the automatic capturing of
    such offerings, e.g., based on the providers'' API documentations, and finally,
    we aim to enable the automated generation of alternative application topologies
    based on our previous research [15]. ACKNOWLEDGMENT This work is partially funded
    by the FP7 EU-FET project 600792 ALLOW Ensembles. Authors Figures References Citations
    Keywords Metrics Footnotes More Like This Storming the cloud: A look at denial
    of service in the Google App Engine 2015 International Conference on Computing,
    Networking and Communications (ICNC) Published: 2015 The design and development
    of the multi-user collaboration operation examination system of the novel marine
    engine simulation platform 2017 29th Chinese Control And Decision Conference (CCDC)
    Published: 2017 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: Automated Capturing and Systematic Usage of DevOps Knowledge for Cloud Applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
