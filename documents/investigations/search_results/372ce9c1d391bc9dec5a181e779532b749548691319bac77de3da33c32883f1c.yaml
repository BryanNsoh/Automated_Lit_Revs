- DOI: https://doi.org/10.1016/j.future.2016.08.025
  analysis: '>'
  authors:
  - Zhanibek Kozhirbayev
  - Richard O. Sinnott
  citation_count: 148
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Background and related
    work 3. Comparison of micro-hosting environments 4. Evaluation methodology and
    benchmarking 5. Conclusions and future work Acknowledgment References Show full
    outline Figures (7) Show 1 more figure Tables (9) Table 1 Table 2 Table 3 Table
    4 Table 5 Table 6 Show all tables Future Generation Computer Systems Volume 68,
    March 2017, Pages 175-182 A performance comparison of container-based technologies
    for the Cloud Author links open overlay panel Zhanibek Kozhirbayev a, Richard
    O. Sinnott b Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.future.2016.08.025
    Get rights and content Highlights • The key features of the micro-service hosting
    technologies for the Cloud were identified. • We perform test cases to evaluate
    virtualization performance of these technologies. • There were roughly no overheads
    on memory utilization or CPU by the examined technologies. • I/O and operating
    system interactions incurred some overheads. Abstract Cloud computing allows to
    utilize servers in efficient and scalable ways through exploitation of virtualization
    technology. In the Infrastructure-as-a-Server (IaaS) Cloud model, many virtualized
    servers (instances) can be created on a single physical machine. There are many
    such Cloud providers that are now in widespread use offering such capabilities.
    However, Cloud computing has overheads and can constrain the scalability and flexibility,
    especially when diverse users with different needs wish to use the Cloud resources.
    To accommodate such communities, an alternative to Cloud computing and virtualization
    of whole servers that is gaining widespread adoption is micro-hosting services
    and container-based solutions. Container-based technologies such as Docker allow
    hosting of micro-services on Cloud infrastructures. These enable bundling of applications
    and data in a manner that allows their easy deployment and subsequent utilization.
    Docker is just one of the many such solutions that have been put forward. The
    purpose of this paper is to compare and contrast a range of existing container-based
    technologies for the Cloud and evaluate their pros and cons and overall performances.
    The OpenStack-based Australia-wide National eResearch Collaboration Tools and
    Resources (NeCTAR) Research Cloud (www.nectar.org.au) was used for this purpose.
    We describe the design of the experiments and benchmarks that were chosen and
    relate these to literature review findings. Previous article in issue Next article
    in issue Keywords Container-based virtualization technologiesCloud computingPerformance
    comparison 1. Introduction Nowadays Cloud platforms and associated virtualization
    technologies are in great demand. Many software companies such as VMware (VMware),
    Citrix (Xen), Microsoft (Hyper-V) dominate the virtualization market with their
    solutions and companies which are oriented to hardware such as Intel and AMD now
    offer advanced processors to support virtualization. Collectively these technologies
    are utilized for server consolidation—typically in data centers that offer large
    collections of servers for external communities in a flexible manner through for
    example elastic scaling. There has been much research related to virtualization
    performance. Some of these works concentrate on HPC facilities  [1], [2] whilst
    others focus on Cloud environments. Previous research identified that technologies
    which utilize hypervisor-based virtualization, face high performance overheads.
    In addition they suffer from I/O limitations and hence are normally avoided in
    HPC environments. Recently container-based virtualization and support for microhosting
    services has gained significant acceptance since it provides a lightweight solution
    that allows bundling applications and data in a simpler and more performance-oriented
    manner that can run on different Cloud infrastructures. This way of dealing with
    virtualization offers horizontally scalable, deployable systems without the difficulty
    of high-performance challenges of traditional hypervisors and the overheads of
    managing large scale Cloud infrastructures  [3]. In this work, we undertake a
    review of microhosting services and perform a number of experiments to provide
    a comprehensive performance evaluation of container-based virtualization technologies
    for the Cloud. We focus in particular on representative systems: Docker  [4],
    [5], [6] and Flockport (LXC)  [7], [8] as leading offerings. The purpose of this
    work is to compare the performance of container-based virtualization technologies
    on the Cloud. This work focused specifically on CPU, memory as well as I/O devices
    capacities. In order to meet these criteria four objectives were defined: – Critically
    review performance experiments of related works to measure the performance of
    different existing virtualization technologies on different environments; – Identify
    performance-oriented case studies in order to evaluate the performances of virtualization
    technologies on the Cloud; – Implement several case studies to evaluate the performances
    of the microhosting technologies; and – Compare the results obtained from the
    performed experiments and identify the pros and cons of microhosting technologies
    under these experimental conditions. This paper is organized as follows: Section  2
    presents an overview of various technologies for virtualization and related work
    on benchmarking applications. More precisely, it describes container-based virtualization
    and hypervisor-based virtualization as well as representative examples of these
    solutions including Docker, LXC (Flockport) and CoreOs Rocket. Section  3 compares
    the key features of the container-based technologies that may influence to their
    performance. The design of the experiments executed to evaluate virtualization
    performance is described in Section  4 including the Cloud environment, the system
    architecture and benchmarking tools that were used to perform the benchmarking
    case studies. The implementation details of the experiments and the results of
    the performance comparison are presented in Section  4. The summary of the performance
    comparison and areas of further research are given in Section  5. 2. Background
    and related work Virtualization of resources typically includes utilizing an additional
    software layer above the host operating system with an eye to handle multiple
    resources. Such virtual machines (VMs) can be considered as a separate execution
    environment. Several approaches are used for virtualization purposes  [9]. One
    popular technique is hypervisor-based virtualization. Well-known solutions based
    on hypervisor-based virtualization are: KVM and VMware. In order to use this kind
    of technology there should be a virtual machine monitor above the underlying physical
    system. Each virtual machine also has support for (isolated) guest operating systems.
    It is quite possible that one host operating system may support many guest operating
    systems within this virtualization approach  [9]. Container-based virtualization
    technology represents another approach. In this model, the hardware resources
    are divided by implementing many instances with (secure) isolation properties  [9].
    The difference between the two technologies can be seen in Fig. 1. Here, guest
    processes obtain abstractions immediately with container-based technologies as
    they operate through the virtualization layer directly at the operating system
    (OS) level. In hypervisor-based approaches however there is typically one virtual
    machine per guest OS  [9]. One OS kernel is typically shared among virtual instances
    in container-based solutions. Therefore, there is an assumption that the security
    of this kind of approach is weaker than with hypervisors. From a users perspective,
    containers operate as autonomous OSs, which appear able to run independently of
    hardware and software  [10]. Download : Download full-size image Fig. 1. Comparison
    of container-based and hypervisor-based approaches. Biederman  [11] argues that
    kernel namespaces are responsible for handling the isolation property of containers.
    This is considered as a Linux kernel characteristic attribute, which allows processes
    to obtain the necessary levels of abstraction. Despite the fact that there is
    no interaction between containers and outside of a namespace layer, there is isolation
    between the Host OS and Guest Processes and each container has its own operating
    system. According to Biederman  [11], file systems, process identifiers, networks
    as well as inter-process communication are considered to be isolated over namespaces.
    However, there is a restriction of the resource utilization in accordance with
    process groups in container-based virtualization technologies. This procedure
    is managed by cgroups   [12]. To be precise, cgroups are responsible for determining
    the priority for CPU, memory as well as I/O utilization in container-based virtualization.
    However certain technologies, which use containers implement their management
    of the resources in accordance with the consistency of cgroups. Using such container-based
    solutions allows for the dynamic deployment and use of micro-services in bundled
    hosting environments. Micro-service patterns are not a new idea in software architecture.
    Nowadays they are widely recognized as an efficient solution to develop applications.
    Prior to the micro-services architecture, the general approach for service development
    was to create largely monolithic applications. From a functional point of view,
    this required a single environment that handled all the things. In Cloud environments,
    many of these issues can be overcome through scripting approaches supporting Infrastructure-as-a-Service
    (IaaS), Platform-as-a-Service (PaaS) and Software-as-a-Service (SaaS). However,
    such solutions are challenged when there is many ad hoc projects and communities
    involved with their own diverse software and data demands, e.g. where community-specific
    virtual machine images are not available for the Cloud. In such environments,
    light-weight Cloud-enabled solutions are beneficial. Micro-services are one such
    model. The main concept of the micro-services architecture is “divide and conquer”.
    Fundamentally, micro-services replace a single larger code base with multiple
    small-scale code foundations controlled by small, agile groups. These code foundations
    have a single API relationship with one another. The benefits behind this concept
    are that every group may work in isolation and be protected/disconnected from
    one another—so called exemption cycles. However, these exemption cycles may be
    connected in certain cases, e.g. when there is dependency between services in
    different groups  [13]. A range of container-based micro-hosting services now
    exists. The most established of these are Docker, CoreOS and LXC. Docker provides
    a less complicated way to wrap an application into a container including the accessories
    it requires for execution. This action is conducted through a targeted set of
    tools and integrated Application Programming Interface Guiding technologies with
    kernel-level structure, e.g. Linux containers, control groups as well as a copy-on-write
    file system. Acting as a file system for containers Docker is dependent on Advanced
    Multi-Layered Unification File system (AuFS). AuFS is able to explicitly superimpose
    single or multiple available file systems. It enables Docker to utilize images
    required for the container’s foundation. For instance, a person may use a Ubuntu
    image that in turn might provide the foundation for multiple other containers.
    With the help of the Advanced Multi-Layered Unification File system Docker utilizes
    a single copy of Ubuntu. This dramatically saves storage and reduces the use of
    memory in line with prompt launching of containers. AuFS has one more advantage
    which enables it to create image versions. Every latest version is marked as diff,
    which is a file collation benefit that identifies the difference between two files.
    This allows for example for image files to be reduced in volume. Another efficiency
    of AuFS is that every modification made on image versions can be tracked—much
    like software development code versioning systems  [4]. CoreOS is a comparatively
    recent distribution of Linux, which has been designed to provide characteristics
    required to operate stacks of software systems. This technology provides a reduced
    Linux kernel to decrease overheads to the maximum. Moreover, CoreOS supports a
    cluster with tools in order to ensure redundancy as well as methods of protecting
    systems from failure. Recently, CoreOS introduced a new product—rocket container
    runtime (rkt)  [14], which runs the application container specification. The main
    purpose of this technology is to build a specified model for a container. This
    approach also supports Amazon Machine Images. The rocket container runtime is
    an option to Docker, which includes advanced security as well as other demands
    necessary for production activities on servers. The rocket container runtime is
    aligned with the Application Container specification, which offers a novel set
    of formats for containers that allows them to be easily carried or moved. In Docker,
    every process runs via a daemon and from security point of view it does not provide
    as much assurance as Rocket. In order to correct this phenomenon it has been suggested
    that Docker should be rewritten completely. LXC is maintained in the standard
    Linux kernel and the project enables instruments to manage container and OS images.
    Containers can be considered as lightweight OS facilities which execute together
    with the host OS. Containers do not imitate the hardware layer and therefore they
    can execute at almost native speed in the absence of other performance overheads.
    In their standard utilization, applications as well as web stacks are established
    and put in a particular form in bare-metal servers for testing purposes. For example,
    PHP, MySQL, Nginx and Drupal can be installed and configured to run in parallel.
    However, currently applications are dedicated to the machine where they have been
    established and cannot simply be migrated. A virtual machine can be installed
    and migrated and it may give some sense of being easily moved but this compromises
    performance. The LXC container can give almost bare-metal throughput and the capability
    to simply migrate throughout systems by establishing similar stack into containers.
    An LXC container has improved performance and flexibility, leading to the illusion
    that there is a separate server. The containers can be cloned, backed up and snapshotted.
    LXC makes it easy to manage containers and introduces new degrees of flexibility
    in executing and launching apps. Flockport ensures web stacks as well as software
    in LXC containers can be launched on any Linux-based machines. LXC containers
    directly support flexibility and throughput, whereas Flockport is a tool to distribute
    LXC containers and simplify their utilization. Several researchers have concentrated
    on reducing the difference between the virtualization technologies and non-virtualized
    approaches in terms of performance and optimization. The methodology and tools
    used are typically different in each of these researches. Moreover, the examined
    and compared compositions of techniques vary also. For instance, the gap between
    native systems and visualized versions of them and the performance differences
    between container-based and hypervisor-based virtualization technologies are examined
    and compared in recent research papers. We note that this is a fast moving field
    and hence some of the earlier papers are based on out of date software. Moreover,
    they do not perform analysis on recent visualization technologies. Four different
    hypervisor-based virtualization technologies were compared by Hwang et al. in  [15].
    They did not discover a hypervisor with any noticeably higher performance. Accordingly,
    the suggestion offered by them was to use various software as well as hardware
    platforms in Cloud facilities in order to satisfy customer requirements. Abdellatief
    et al.  [16] performed a performance comparison of technologies such as VMware,
    Microsoft Hyper-V, and Citrix Xen in different scenarios. The methodology of evaluation
    they used was to apply customized SQL instances. With the help of this method,
    they simulated millions of products, customers and orders. The same kind of analysis
    was performed by Varrette et al.  [17], but with different technologies and associated
    testbed environment. They utilized kernel-based virtual machines in place of Microsoft
    Hyper-V, with experiments related to high performance computing. Their experiments
    were focused on the consumption of power, energy efficiency as well as scalability.
    Despite the fact that there was an inconsistent demonstration of virtualization
    overheads,  [17] identified that the virtualization layer for almost every hypervisor
    provided significant influence on the performance of the virtualized environments,
    especially for high performance computing domains. Recent publications consider
    the similarity or dissimilarity between hypervisors with container approaches.
    According to Dua et al.  [18], containers are becoming popular in supporting PaaS
    facilities. The representatives of both types of virtualization namely KVM, Xen,
    and LXC were benchmarked by Estrada et al.  [19]. The main methodology of their
    research was to measure the similarities as well as the differences of the runtime
    performance of each mentioned technology. The basis of their experiments and benchmarking
    was supporting sequence-based applications. Felter et al.  [5] also compared technologies
    for hypervisor and container-based system namely KVM and Docker respectively.
    They conducted a comprehensive analysis in terms of CPU, memory, storage as well
    as networking bandwidth and latencies. According to their benchmark results, the
    performance of the Docker container was almost the same as the “bare metal” system.
    Their benchmarks were based on memory transfers, floating point handling, network
    resources, block I/O as well as database capacities. However, the authors work
    did not examine methodically the influence of containers on traditional hypervisors.
    Utilizing containers to deploy applications in an efficient and repeatable manner
    was introduced in the Heroku PaaS provider  [20]. Heroku offers a container as
    a process with additional isolation properties instead of considering it as a
    virtual server. As a consequence application deployment containers provide a lightweight
    technology with insignificant overheads and almost the same isolation as virtual
    machines. It also has resource sharing properties as standard processes. Such
    containers were heavily utilized by Google in their infrastructure. Moreover,
    a standard format for images as well as management tools for application containers
    was offered by Docker. The main distinction compared to previous publications
    and this work is that the analysis made in this work is specifically related to
    benchmarking the performance of open source container-based technologies and hence
    identifying their associated advantages and disadvantages. 3. Comparison of micro-hosting
    environments Each container-based technology has its own features. This section
    presents some key characteristics of the current leading container-based technologies
    that may have impact on the performance. We note that even though CoreOs Rocket
    was identified in the previous section, the performance evaluation for Rocket
    was not conducted since CoreOS have not yet released an official version of their
    product. 3.1. Docker Docker utilizes several Linux kernel features in order to
    run containers in an isolated way  [9]. • namespaces: Docker employs namespaces
    to deploy containers. There are several types of namespaces utilized by Docker
    to perform the tasks of creating isolated containers  [4]: – Docker uses pid as
    a base for containers which ensures that all processes in the container are not
    allowed to affect processes in other containers; – It uses net in order to manage
    network interfaces or more precisely, it provides isolation of the system resources
    regarding networking; – ipc is used to provide isolation to specific inter-process
    communication (IPC) resources, namely System V IPC objects and POSIX message queues.
    This means that each IPC namespace has its own inter-process communication resources.
    – In order to allow processes to have their own view of a filesystem and of their
    mount points Docker uses mnt namespace. – Isolation of kernel and version identifiers
    is performed through uts. • control groups: Docker executes cgroups so that existing
    containers can share available hardware resources and it is possible that there
    might be a limitation in use of these resources at a given time. • union file
    system is a file system that functions by establishing layers which are utilized
    to provide the building blocks for containers. • container format is considered
    as a wrapper that integrates all of the fore-mentioned mechanisms. 3.2. LXC Linux
    Container is a container-based virtualization technology that enables the building
    of lightweight Linux containers without difficulty by use of a common and flexible
    API and associated implementations  [21]. On the other hand, Docker is an application-centric
    technology based on containers. They share some common features but have numerous
    differences. Firstly, LXC is an operating-system-level virtualization technique
    for executing several isolated Linux containers on a single LXC host. It does
    not make use of a virtual machine, but rather allows utilizing a virtual environment
    which has its own CPU, memory, blocking I/O, network as well as the resource control
    mechanism. This is offered through the namespaces and  cgroups features in the
    Linux kernel on the LXC host. It is similar to a chroot, but provides much more
    isolation. Various virtual network types and devices are supported by LXC. Secondly
    Docker utilizes fixed layers to enable reuse of well-structured designs, but this
    can be at the cost of complexity as well as throughput. The restriction of one
    application per container reduces the utilization possibilities. With LXC, single
    and/or multiple applications may be created. Furthermore LXC allows multiple system
    containers to be created, which may be clones of just one sub-volume that can
    be run by utilizing a btrfs file. This feature of LXC can tackle sophisticated
    issues of file system levels. Thirdly, LXC offers an extensive list of facilities
    and privileges to design as well as execute containers. Finally LXC enables the
    creation of unprivileged containers that ensure non-root users can build containers.
    Docker does not yet support this feature. 3.3. CoreOS rocket Rocket  [14] is a
    container-based technology introduced by CoreOS, which provides an alternative
    to Docker. Both Rocket and Docker introduce automation of the application deployment
    in the form of virtual containers that can execute independently based on the
    server’s characteristics. However, whilst Docker has developed into a sophisticated
    environment, which supports a diversity of requirements as well as operations,
    Rocket is structured to perform simple functions but in a secure manner targeted
    to application deployment. Rocket functions as a command-line instrument for executing
    application containers that are descriptions of image designs. Rocket is focused
    on the application container specification introduced by CoreOS as a composition
    of descriptions that allows for a container to be easily migrated. As recognized
    by Polvi  [14], Rocket may be more difficult to use compared to Docker since Docker
    simplifies the whole process of constructing a container through its descriptive
    interface.  [14] argues that Rocket should stay as a command-line based environment
    and be less likely to change. 4. Evaluation methodology and benchmarking There
    are many perspectives that can be used to compare technologies, especially from
    a performance perspective. In order to evaluate container-based technologies from
    the perspective of their overheads, it was necessary to understand (measure) the
    overheads incurred based upon non-virtualized environments. The analysis undertaken
    here focused upon a range of performance criteria: the performance of CPU, memory,
    network bandwidth and latency and storage overheads. In all of the benchmarking
    multiple experiments were repeated 15 times to assess the accuracy and consistency
    of the various results. Average timing and standard deviation was recorded. The
    Cloud environment that was used for these activities was the Australia-wide National
    eResearch Collaboration Tools and Resources (NeCTAR) Research Cloud (www.nectar.org.au).
    NeCTAR provides a Cloud environment for all researchers across Australia. It offers
    access to 30,000 servers across eight availability zones—typically located in
    the State capitals (Melbourne, Canberra, Hobart etc.). The NeCTAR project is led
    by the University of Melbourne and funded by the Department of Education. NeCTAR
    utilizes the OpenStack middleware to realize the Cloud infrastructure. The performance
    studies are all executed on NeCTAR Research Cloud instances. The following instance
    configurations were used for performing the experiments: Model: Processor: AMD
    Opteron 62xx class @ 2.60 GHz; Processor ID: AuthenticAMD Family 21 Model 1 Stepping
    2; Memory: 3955 MB; OS: Ubuntu 12.04 (64-bit). The virtualization technologies
    and their versions are given in Table 1. Table 1. The virtualization technologies
    and their versions. Virtualization technologies Version Docker 1.4.0 Flockport
    (LXC) 1.1.2 For conformity, all Docker and Flockport (LXC) containers utilized
    a Ubuntu 64 bit system image. Moreover, both of them were running on the host
    OS which itself was based on Ubuntu 12.04 64-bit. The outcomes of the performed
    experiments are demonstrated in this section. As mentioned previously, the chosen
    benchmark tools evaluate CPU, memory, network bandwidth and latency, storage overhead
    performances. 4.1. CPU performance The first scenario to evaluate the CPU performance
    was based on use of a compressor. Compression is a regularly utilized module of
    Cloud environments processing. PBZIP2  [22] is a parallel realization of the bzip2
    block-sorting data compression utility. It utilizes number of threads and can
    reach an almost linear acceleration. pbzip2-1.1.12 version was used in order to
    compress a file. An input file (enwik8)  [23], which is 100 MB dump data and frequently
    applied for compression testing purposes. In order to concentrate on compression
    900 kB BWT Block Size and 900 kB File Block Size was used. The performance of
    the pbzip2 compressor is presented in Table 2. From the perspective of CPU evaluation,
    Docker performs slightly better than LXC. In case of Flockport, average elapsed
    time is 14.9 s and standard deviation is , whilst average time by Docker is 14.8
    s and standard deviation is . However, the size of input file should be taken
    into consideration. If the size increases, the difference of results can be significant.
    Table 2. pbzip2 compressor results. Platforms Wall clock (s) Native 13.7 Docker
    14.8 Flockport (LXC) 14.9 The second CPU benchmarking tool is Y-cruncher  [24],
    which is used to compute Pi. It is typically executed as a stress-testing tool
    for CPUs and frequently used as a test for multi-threaded tools running in multi-core
    systems. Besides calculating the value of Pi, Y-cruncher can also compute a range
    of other constants. Y-cruncher performs various outcomes such as multi-core efficiency,
    computation time, and total execution time. The total time is applied to confirm
    the outputs, and it includes the total computation time added to the time requested
    to exploit and perform the outcome. The performance results of the Y-cruncher
    benchmarking tool can be seen in Fig. 2. In terms of computation time, Docker
    performs similarly to the native (non-virtualized) system, whereas Flockport takes
    on average about 2 s longer. The multi-core efficiency results of these systems
    are presented in Table 3. This assessment describes how the CPU is effectively
    utilized in calculation of Pi. This pattern also shows that Docker shows marginally
    better performance than Flockport. Download : Download full-size image Fig. 2.
    Performance results of the Y-cruncher benchmark. Table 3. Multi-core efficiency
    results from Y-cruncher. Platform Multi-core efficiency Native 99.2% Docker 99.3%
    Flockport 99.4% The next benchmarking tool explored was the standard HPC benchmarking
    tool: Linpack. There are two options for this tool. The first one is an optimized
    version by Intel  [25], whereas the second one  [26] allows to operate on all
    machines and not only Intel machines. Linpack finds the solution for a system
    of linear correspondences utilizing an approach that performs ‘lower upper’ decomposition
    of numerical analysis with partial pivoting. A great number of computational operations
    consist of multiplying a process of a scalar with a vector in double-precision
    floating-point format as well as processes for adding the outcomes to different
    vectors. The benchmarking tool is built on the basis of linear algebra functions
    which are modified for the chosen computer architecture. The main Linpack function
    utilizes a random matrix of size and a vector which is determined as . The Linpack
    benchmark tool performs two steps as follows: ‘Lower Upper’ decomposition of ,
    and subsequently the ‘Lower Upper’ decomposition applied to solve the linear problem
    . The outcomes of Linpack are typically provided in MegaFLOPS—floating-point operations
    per second. Executing this tool and increasing the value of N, it can be seen
    from experiment that the behavior of the CPU usage changes and various phases
    can be identified: rising zone, where no challenge occurs in local memory or processor;
    flat zone, where challenges occur in processor functionality, and the decaying
    zone, where challenges occur in the local cache memory. The results of Linpack
    run in the micro-hosting environments can be seen in Fig. 3. These outcomes are
    obtained by applying a specific scenario with . As seen, even though the gap between
    them is relatively small, the performance of Flockport in executing the Linpack
    benchmark is slightly better than Docker. Download : Download full-size image
    Fig. 3. Linpack results (where ). The final tool used to evaluate the performance
    of the CPU was Geekbench  [27]. This tool is useful to test performance of the
    Floating Point Unit as well as the throughput of memory systems. An upper bound
    for the above-mentioned throughput features was assessed by this application.
    In comparison with Y-cruncher, Geekbench supports the evaluation of single as
    well as multi-core architectures. It can execute various workloads generating
    multiple indexes such as Integer Performance, Floating Point Performance, and
    Memory Performance. Moreover, the index of the complete system can be generated.
    As can be seen from Fig. 4, Fig. 5, there is no significant difference in either
    single-core or multi-core testing of results. However, regarding memory performance,
    Flockport produces approximately 100 points and 200 points more than Docker performance
    in single-core and multi-core testing respectively. Download : Download full-size
    image Fig. 4. Geekbench results for single-core testing. Download : Download full-size
    image Fig. 5. Geekbench results for multi-core testing. 4.2. Disk I/O performance
    A key aspect of performance is the evaluation of disk I/O performance, specifically
    volumes given as a non-ephemeral storage, were attached to instances. These volumes
    were created in the same availability zone as the associated instances. The first
    applied benchmarking tool used to evaluate the micro-hosting environments was
    Bonnie++ [28]. Bonnie++ is an open-source application that describes disk throughput.
    Bonnie++ has to be configured for different cases so a common test file was used.
    A 4 Gb dataset was used for this purpose. Fig. 6 presents the Bonnie++ outcomes
    for Block Output as well as Block Input based upon sequential write and read respectively.
    All three systems produce almost same results in terms of sequential write. However
    Flockport shows a slightly better performance compared to Docker regarding sequential
    reading of files. Download : Download full-size image Fig. 6. Bonnie++ results
    for sequential write and sequential read. Through the Bonnie++ benchmarking application,
    the speed can be assessed when reading, writing and flushing processes take place
    in a file. Table 4 shows the Random Write Speed as well as Random Seeks for each
    system. Table 4. Random write speed and random seeks. Platform Random write speed
    (kb/s) Random seeks Native 24 829 % 3741 % Docker 22 494 −9.4% 389.2 −89.6% Flockport
    24 524 −1.2% 3961 +5.9 There is relative alignment between random write speed
    outcomes at which a file is read and then written and flushed to the disk as depicted
    in Fig. 7. However, the arrangement of the systems is different in their random
    seek evaluation which shows the number of blocks which Bonnie++ can seek to per
    second. In this case, Flockport has 100% better results than Docker and is almost
    6% better than the native platform. Download : Download full-size image Fig. 7.
    STREAM results. To further evaluate the disk I/O throughput, the Sysbench benchmark
    tool  [29] was utilized. This tool involves many modules as the basis of design.
    It also can be used as a cross-platform as well as multi-threaded measurement
    software for measuring operating system characteristics. The main concept of Sysbench
    is to gain a representation at a fast speed about system throughput without deploying
    database systems. Actual versions of the suite allow evaluating the system characteristics
    such as file I/O throughput, memory allocation and transferring speed scheduler
    throughput. In performing this scenario, the tool was used to assess the platforms
    performance to read as well as write from specified files. In order to evaluate
    file IO performance, a test file should be created that should be bigger in size
    than the available RAM. The size of the test file used here was 35 GB—specifically
    128 files of 270 Mb each. The results of this measurement are given in Table 5.
    Total time taken by event execution on Docker and Flockport is 27.488 s with standard
    deviation of and 27.491 s with standard deviation of respectively. There is no
    significant difference. Table 5. Sysbench results. Platform Read (Gb) Written
    (Mb) Total transferred (Gb) Throughput (Mb/s) Elapsed time (s) Native 1.22 834.17
    2.04 20.85 27.568 Docker 1.21 829.38 2.02 20.73 27.488 Flockport 1.17 796.88 1.95
    19.92 27.491 The results achieved by Sysbench reflect those obtained through Bonnie++.
    Native as well as Flockport showed approximately the same outcome without any
    discernible difference. However, Docker performed better than both platforms in
    some runs. For the purpose of thoroughness, it should be noted that a default
    file format was utilized for disk images for Native as well as Flockport, whereas
    Docker applies the advanced multi layered unification file system that ensures
    layering as well as image versioning. As such, other tests should be conducted
    in this experiment. 4.3. Memory performance The evaluation of Memory I/O performance
    is presented in this subsection. The benchmark tool used to test the micro-hosting
    environments was the STREAM software  [30]. STREAM assesses memory throughput
    utilizing straightforward vector kernel procedures. The outcomes of four procedures
    namely Copy, Scale, Add as well as Triad are generated by this tool. These procedures
    and how they are calculated are shown in Table 6. Table 6. Stream procedures.
    Procedure Kernel Copy Scale Add Triad According to the STREAM software, there
    is a hard relation between the evaluated throughput and the size of the CPU cache.
    Moreover, there is rule that every stream array has to be at least four times
    the accessible cache memory size. Therefore, the size of the stream array in the
    software must be established correctly. As seen in Fig. 7, the difference between
    the results is not significantly high, but Docker produces slightly better results
    than Flockport and is nearly the same as the native platform. 4.4. Network I/O
    performance This subsection describes the performance of container-based environments
    with regards to their Network I/O. Any containers running on the same host, more
    precisely on the same host bridge, can contact each other through IP. Network
    address translation (NAT) networking principles was used for all container traffic.
    This makes possible for all containers to contact outside world including other
    Docker containers on different hosts, but it does not allow the outside network
    to communicate with the containers. This regulation might be avoided by mapping
    container ports to ports on the host network interface. Linux containers have
    the same networking. The test cases were conducted on two Docker containers located
    on two different hosts. The same condition was applied to Flockport containers
    also. In both cases, one acted as a server and the other as a client. For native
    conditions, two identical NeCTAR Research Cloud instances were used. The Netperf
    benchmark tool  [31] was used to measure Network I/O. This tool has many predetermined
    subtests to evaluate network throughput between a server and clients. It enables
    execution of data transfers operating in a single direction either with TCP or
    UDP protocol. Time spent to establish connections between the netperf client and
    the netserver is not comprised in these evaluations. The assessment outcomes present
    the throughput of arriving packets is shown in Table 7. Table 7. Netperf TCP_STREAM
    and UDP_STREAM results. Platform TCP_STREAM (Mbps) UDP_STREAM (Mbps) Docker 1308.38
    721.32 Flockport 1080.48 586.16 Native 1455.45 780.24 Table 7 shows the outputs
    for both Docker and Flockport and for both TCP_STREAM and UDP_STREAM. For the
    TCP_STREAM test case, Docker performed almost 200 Mbps better than Flockport,
    whereas for the UDP_STREAM test case, Docker again offers approximately 150 Mbps
    better performance. Moreover, as mentioned, the Netperf benchmark tool has assessment
    cases including request and response, which measure the amount of TCP as well
    as UDP transactions. The following establishment connection occurs during these
    cases: the netperf client dispatches requests to the netserver and the netperf
    server dispatches a response to the netperf client. As depicted in Table 8, Flockport
    again produces the worst results for both TCP_RR as well as UDP_RR test cases.
    Table 8. Netperf TCP_RR and UDP_RR results. Platform TCP_RR (Transfer rate per
    second ) UDP_RR (Transfer rate per second) Docker 44363.03 45093.28 Flockport
    39321.02 40625.07 Native 48451.11 49221.17 In order to get a thorough understanding
    of Network I/O performance, a second tool was applied to test Network throughput:
    the Iperf suite  [32]. Iperf offers a complete suite for evaluating connection
    performances using either TCP or UDP protocols. Table 9 shows the results of Iperf
    tests for both TCP and UDP traffic. Table 9. Iperf results. Platform Empty Cell
    TCP UDP Docker Interval 0.0–10.0 s 0.0–10.0 s Empty Cell Transfer 966 MB 11.9
    MB Empty Cell Bandwidth 810 Mb/s 10.0 Mb/s Flockport Interval 0.0–10.0 s 0.0–10.0
    s Empty Cell Transfer 1.34 GB 11.9 MB Empty Cell Bandwidth 1.15 Gb/s 10.0 Mb/s
    Native Interval 0.0–10.0 s 0.0–10.0 s Empty Cell Transfer 1.64 GB 1.19 MB Empty
    Cell Bandwidth 1.41 Gb/s 1.00 Mb/s It can be seen from Table 9 that Docker is
    slower when compared to the previous test case on I/O performance. The possible
    reason is the TCP window size and the UDP buffer size. That is, the quantity of
    data that can be buffered in the time window of a given connection without verification
    differs. The sizes of data can be between the range of 2 and 65,535 bytes, but
    they were small in Iperf by default (60.0 kB) and were not tuned. 5. Conclusions
    and future work Container-based technologies are challenging hypervisor-based
    approaches as the basis for Clouds. Modern container-based approaches are considered
    to be lightweight. In this paper, a thorough performance assessment of leading
    micro-hosting virtualization approaches was presented with specific focus on Docker
    and Flockport and their comparison with native platforms. As noted, CoreOS was
    not considered due to the unavailability of the systems at the time of writing.
    Regarding the results of performed experiments, many common patterns can be seen.
    As shown there were roughly no overheads on memory utilization or CPU by either
    Docker or Flockport, whilst I/O and operating system interactions incurred some
    overheads. The overheads in these cases appear by way of additional cycles for
    every input–output operation. Therefore, applications with increased input–output
    operations have more disadvantages compared to applications with lower input–output
    demands. As a consequence, input–output latency is exacerbated by these overheads.
    The CPU cycles required for utility tasks can also cause performance degradation.
    Docker includes several other capabilities such as Network Address Translation,
    which helps to reduce some of the difficulties of Docker container utilization.
    However, these capabilities directly impact input on throughput quality. As a
    result, Docker containers utilizing no extra features can be no quicker compared
    to Flockport in some cases. Software that is either file system or disk intensive
    must use the advanced multi layered unification file system by applying volumes.
    The impact of Network Address Translation may be removed by utilizing nethost.
    However, this negates the advantages of network namespaces. Eventually, the design
    of one IP address for each Docker container as suggested by the Kubernetes  [33]
    may support quality assurance as well as throughput. The generated results here
    may provide some direction of how the architecture of Cloud environments should
    be designed. Thus current considerations are that IaaS is more to utilization
    of virtual machines and PaaS developed to utilize containers. If IaaS is designed
    to use containers instead, they can provide better throughput as well as simpler
    deployment opportunities. Moreover, containers can reduce the difference between
    IaaS and “bare metal” systems because they support the management and give almost
    the same performance as native systems. Another question here is launching containers
    within virtual machines, thus these can conflict with the throughput overheads
    of virtual machines while providing no advantages compared to launching containers
    immediately on native hosts. Such pragmatic considerations should be factored
    in to the choices of the technologies used to build and manage IaaS and PaaS systems
    for performance demanding communities. Multi-tenancy is a further very important
    problem in Clouds, and containers or micro-service applications typically consist
    of multiple services running in separate containers sharing the same resources.
    The implications and performance impact on shared resources will be conducted
    in future work. Acknowledgment The authors would like to thank the NeCTAR Research
    Cloud (www.nectar.org.au) for the resources used to perform these investigations.
    References [1] P. Padala, X. Zhu, Z. Wang, S. Singhal, K. Shin, Performance Evaluation
    of Virtualization Technologies for Server Consolidation, Enterprise Systems and
    Software Laboratory HP Laboratories Palo Alto HPL-2007-59, 2007. Google Scholar
    [2] N. Regola, J. Ducom, Recommendations for virtualization technologies in high
    performance computing, in: 2010 IEEE Second International Conference on Cloud
    Computing Technology and Science, CloudCom, 30 2010-dec. 3 2010, pp. 409–416.
    Google Scholar [3] N. Slater, Using Containers to Build a Microservices Architecture,
    viewed 1 April 2015, URL https://medium.com/aws-activate-startup-blog/using-containers-to-build-a-microservices-architecture.
    Google Scholar [4] Docker–Build, Ship, and Run Any App, Anywhere, viewed 1 April
    2015, URL http://www.docker.com. Google Scholar [5] W. Felter, A. Ferreira, R.
    Rajamony, J. Rubio, An Updated Performance Comparison of Virtual Machines and
    Linux Containers, viewed 1 April 2015, URL http://www.research.ibm.com/. Google
    Scholar [6] D. Merkel Docker: Lightweight linux containers for consistent development
    and deployment Linux J., 2014 (239) (2014) Google Scholar [7] Flockport, viewed
    1 April 2015, URL http://www.flockport.com. Google Scholar [8] P. Rubens, Docker
    Not the Only Container Option in 2015, IT Business Edge, 2015. Google Scholar
    [9] M. Xavier, M. Neves, F. Rossi, T. Ferreto, T. Lange, C. De Rose Performance
    evaluation of container-based virtualization for high performance computing environments
    21st Euromicro International Conference on Parallel, Distributed and Network-Based
    Processing, (PDP), IEEE (2013) Google Scholar [10] S. Soltesz, H. Potzl, M. Fiuczynski,
    A. Bavier, L. Peterson Container-based operating system virtualization: a scalable,
    high-performance alternative to hypervisors SIGOPS Oper. Syst. Rev., 41 (3) (2007),
    pp. 275-287 View in ScopusGoogle Scholar [11] E.W. Biederman, Multiple instances
    of the global linux namespaces, in: Proceedings of the Linux, 2006. Google Scholar
    [12] P. Menage, Control groups definition, implementation details, examples and
    api, viewed 1 April 2015, URL http://www.kernel.org/doc/Documentation/cgroups/cgroups.txt.
    Google Scholar [13] L. Marsden, The Microservice Revolution: Containerized Applications,
    Data and All, viewed 1 April 2015, URL http://www.infoq.com/articles/microservices-revolution.
    Google Scholar [14] A. Polvi, CoreOS is building a container runtime, Rocket,
    viewed 1 April 2015, URL http://coreos.com/blog/rocket. Google Scholar [15] J.
    Hwang, S. Zeng, T. Wood A component-based performance comparison of four hypervisors
    2013 IFIP/IEEE International Symposium on Integrated Network Management, (IM 2013),
    IEEE (2013) Google Scholar [16] E. Abdellatief, N. Abdelbaki Performance evaluation
    and comparison of the top market virtualization hypervisors 2013 8th International
    Conference on Computer Engineering & Systems, (ICCES), IEEE (2013) Google Scholar
    [17] S. Varrette, M. Guzek, V. Plugaru, V. Besseron, P. Bouvry HPC performance
    and energy-efficiency of Xen, KVM and VMware hypervisors 25th International Symposium
    on Computer Architecture and High Performance Computing, (SBAC-PAD), IEEE (2013)
    Google Scholar [18] R. Dua, A.R. Raja, D. Kakadia Virtualization vs containerization
    to support PaaS 2014 IEEE International Conference on Cloud Engineering, (IC2E),
    IEEE (2014) Google Scholar [19] Z. Estrada, Z. Stephens, C. Pham, Z. Kalbarczyk,
    R. Iyer A performance evaluation of sequence alignment software in virtualized
    environments 2014 14th IEEE/ACM International Symposium on Cluster, Cloud and
    Grid Computing, (CCGrid), IEEE (2014) Google Scholar [20] Heroku, viewed 1 April
    2015, URL https://heroku.com. Google Scholar [21] Linux Containers, viewed 1 April
    2015, URL http://linuxcontainers.org. Google Scholar [22] PBZIP2, viewed 1 April
    2015, URL http://www.compression.ca/pbzip2/. Google Scholar [23] enwik8, viewed
    1 April 2015, URL http://mattmahoney.net/dc/textdata. Google Scholar [24] Y-cruncher–A
    Multi-Threaded Pi-Program, viewed 1 April 2015, URL http://www.numberworld.org/Y-cruncher.
    Google Scholar [25] Intel®Math Kernel Library–LINPACK Download, viewed 1 April
    2015, URL https://software.intel.com/en-us/articles/intel-math-kernel-librarylinpack-download.
    Google Scholar [26] LINPACK_BENCH–The LINPACK Benchmark, viewed 1 April 2015,
    URL http://people.sc.fsu.edu/~jburkardt/c_src/linpack_bench/linpack_bench.html.
    Google Scholar [27] Geekbench 3–Cross-Platform Processor Benchmark, viewed 1 April
    2015, URL http://www.primatelabs.com/geekbench. Google Scholar [28] Bonnie++,
    viewed 1 April 2015, URL http://www.coker.com.au/bonnie++. Google Scholar [29]
    Sysbench in Launchpad - SysBench: a system performance benchmark, viewed 1 April
    2015, URL https://launchpad.net/sysbench. Google Scholar [30] J. McCalpin STREAM:
    Sustainable Memory Bandwidth in High Performance Computers, a continually updated
    technical report (1991-2007) (2015) viewed 1 April URL http://www.cs.virginia.edu/stream
    Google Scholar [31] The Netperf Homepage, viewed 1 April 2015, URL http://www.netperf.org.
    Google Scholar [32] Iperf, viewed 1 April 2015, URL http://www.iperf.fr. Google
    Scholar [33] Kubernetes, viewed 1 April 2015, URL http://kubernetes.io. Google
    Scholar Cited by (0) View Abstract © 2016 Elsevier B.V. All rights reserved. Recommended
    articles Using data virtualisation to detect an insider breach Computer Fraud
    & Security, Volume 2017, Issue 8, 2017, pp. 5-7 George Smyth View PDF A few open
    problems and solutions for software technologies for dependable distributed systems
    Journal of Systems Architecture, Volume 73, 2017, pp. 1-5 Marisol García-Valls,
    …, Hans P. Reiser View PDF An unsupervised approach to online noisy-neighbor detection
    in cloud data centers Expert Systems with Applications, Volume 89, 2017, pp. 188-204
    Tania Lorido-Botran, …, Borja Sanz View PDF Show 3 more articles Article Metrics
    Citations Citation Indexes: 149 Captures Readers: 236 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Future generation computer systems
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: A performance comparison of container-based technologies for the Cloud
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.12688/f1000research.7536.1
  analysis: '>'
  authors:
  - François Moreews
  - Olivier Sallou
  - Hervé Ménager
  - Yvan Le Bras
  - Cyril Monjeaud
  - Christophe Blanchet
  - Olivier Collin
  citation_count: 38
  full_citation: '>'
  full_text: '>

    search file_uploadSUBMIT YOUR RESEARCH BROWSE GATEWAYS & COLLECTIONS HOW TO PUBLISH
    ABOUT BLOG MY RESEARCH SIGN IN Home Browse BioShaDock: a community driven bioinformatics
    shared Docker-based... ALL METRICS 4034 VIEWS 678 DOWNLOADS Get PDF Get XML Cite
    Export Track Share ▬ SOFTWARE TOOL ARTICLE BioShaDock: a community driven bioinformatics
    shared Docker-based tools registry [version 1; peer review: 2 approved] François
    Moreews1, Olivier Sallou2, Hervé Ménager3, Yvan Le bras2, Cyril Monjeaud2, Christophe
    Blanchet2, Olivier Collin4 Author details    This article is included in the ELIXIR
    gateway. This article is included in the Container Virtualization in Bioinformatics
    collection. Abstract Linux container technologies, as represented by Docker, provide
    an alternative to complex and time-consuming installation processes needed for
    scientiﬁc software. The ease of deployment and the process isolation they enable,
    as well as the reproducibility they permit across environments and versions, are
    among the qualities that make them interesting candidates for the construction
    of bioinformatic infrastructures, at any scale from single workstations to high
    throughput computing architectures. The Docker Hub is a public registry which
    can be used to distribute bioinformatic software as Docker images. However, its
    lack of curation and its genericity make it difﬁcult for a bioinformatics user
    to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused
    Docker registry, which provides a local and fully controlled environment to build
    and publish bioinformatic software as portable Docker images. It provides a number
    of improvements over the base Docker registry on authentication and permissions
    management, that enable its integration in existing bioinformatic infrastructures
    such as computing platforms. The metadata associated with the registered images
    are domain-centric, including for instance concepts deﬁned in the EDAM ontology,
    a shared and structured vocabulary of commonly used terms in bioinformatics. The
    registry also includes user deﬁned tags to facilitate its discovery, as well as
    a link to the tool description in the ELIXIR registry if it already exists. If
    it does not, the BioShaDock registry will synchronize with the registry to create
    a new description in the Elixir registry, based on the BioShaDock entry metadata.
    This link will help users get more information on the tool such as its EDAM operations,
    input and output types. This allows integration with the ELIXIR Tools and Data
    Services Registry, thus providing the appropriate visibility of such images to
    the bioinformatics community. Keywords bioinformatics, docker, container, deployment,
    interoperability, maintainability, community driven registry Corresponding author:
    François Moreews Competing interests: No competing interests were disclosed. Grant
    information: Funding was provided from the Western France e-science project supported
    by Brittany and Pays de la Loire regions (e-Biogenouest/052012).  The funders
    had no role in study design, data collection and analysis, decision to publish,
    or preparation of the manuscript. Copyright:  © 2015 Moreews F et al. This is
    an open access article distributed under the terms of the Creative Commons Attribution
    License, which permits unrestricted use, distribution, and reproduction in any
    medium, provided the original work is properly cited. How to cite: Moreews F,
    Sallou O, Ménager H et al. BioShaDock: a community driven bioinformatics shared
    Docker-based tools registry [version 1; peer review: 2 approved]. F1000Research
    2015, 4:1443 (https://doi.org/10.12688/f1000research.7536.1) First published:
    14 Dec 2015, 4:1443 (https://doi.org/10.12688/f1000research.7536.1) Latest published:
    14 Dec 2015, 4:1443 (https://doi.org/10.12688/f1000research.7536.1) Introduction
    The life sciences are becoming more and more digital and nowadays data analysis
    methods represent a key factor of the discovery process. In the case of bioinformatics,
    software is widely provided by the research community. Developers favor open source
    approaches and many software tools are available online. It is commonly agreed
    that such a distributed and free creation process accelerates discoveries in the
    life sciences1,2. However, this view must be nuanced, as multiple factors still
    hinder the discovery, integration, and maintenance of these software tools. First,
    domains such as genomics, where technological innovation leads to a exponential
    growth of data to analyse, also generate an ever-increasing number of new software
    methods. However, the discovery of new interesting tools by potential users remains
    limited by unstructured descriptions, lack of metadata and deprecated source codes.
    In this context, dedicated search engines like the ELIXIR Tools and Data Services
    Registry3,4 (hereafter referred as the "ELIXIR registry") have emerged as a potential
    solution to search, find and locate available and maintained tools. Secondly,
    the implementation methods of bioinformatic software are heterogeneous and their
    deployment requires multiple technical skills. The installation process is therefore
    expensive, in terms of human resources. It is worth recalling that the cost in
    supporting operating systems and hardware diversity can be high, the code compilation
    process is error prone and the required software dependencies are often conflicting
    with installed libraries. Consequently, the audience of a software can be limited
    to highly motivated and technical users or large bioinformatics facilities. The
    recent development of user-friendly data analysis environments like Galaxy5 ease
    access for biologists and bio-analysts to bioinformatic tools. These software
    workbenches provide a generic web user interface for command line based scientific
    applications, but do not solve the tools’ deployment issue. Even if the task can
    be submitted inside a container, it is the tool designer’s responsibility to provide
    a readily deployable component6 and the proportion of container based components
    in repositories such as the Galaxy Toolsheds7 is currently low. Finally, traditional
    academic publishing and funding processes emphasize the production of software
    with short-term goals, these being the publication of the method and/or results.
    Such an environment does not favor a software engineering-oriented approach to
    software development8, and this affects directly the portability and maintainability
    of the software products9. This in turn impacts the reproducibility of analyses,
    experiments or benchmarks described in published articles. However, even if various
    emerging initiatives are developing frameworks10–12 to enable a new kind of "executable
    format" of scientific publication, few journals have an innovative publishing
    policy that includes the long term storage of the source codes on a dedicated
    public web platform. Nevertheless, today containerization brings new pragmatic
    solutions. Linux containers are a mature technology that has the potential to
    dramatically facilitate scientific software deployment and analysis reproducibility.
    Docker, one of the most popular container solutions13,14, is now used in a variety
    of computation environments, from commercial clouds15 to clusters with dedicated
    middleware16. It has been positively evaluated for data intensive computation,
    a recent study showing that the performance of bioinformatic workflows composed
    by medium or long running tasks are only very slightly affected by containerization17.
    Container technology has the potential to impact audiences, developers and end-users.
    In the scientific field, it can effectively improve reproducibility, ease deployment
    and facilitate the building of software collections and search engines dedicated
    to a specific scientific domain or topic. For these reasons, we created the BioShaDock
    registry that promotes the use of container technologies in bioinformatics. The
    BioShaDock registry provides a web entry point to deploy, search and discover
    ready to use bioinformatics tools, encapsulated in Docker containers. Future works
    will focus on better integration with domain-centric registries as well as bioinformatic
    integrated environments, to enable the seamless discovery, integration, and execution
    of the BioShaDock containers. Our project will also greatly benefit from discussions
    with other existing bioinformatic container initiatives. Methods Registration
    BioShaDock is a web server based system that allows the description, registration
    and automated building of Docker images (Figure 1). These images are publicly
    available on the web server for search, download and execution. Users can authenticate
    using local LDAP or Google/GitHub credentials. LDAP users have the possibility
    to push new images. External users (Google, etc.) can request those privileges
    by contacting the support team. This mechanism allows non local users to have
    access to the registry to provide new tools while keeping a controlled access
    on the submission of new tools to the registry, where contributions are based
    on trust. Figure 1. The BioShaDock web interface. The interface enables the creation
    of Dockerfiles and allows to search the repository using full text queries. Download
    as a PowerPoint slide Once authenticated, the user can proceed to the registration
    of a Docker container. The information required includes: the set of instructions
    to build the image, i.e. the Dockerfile and the associated source code. These
    can be provided by pasting directly the Dockerfile contents in the web interface,
    by pointing to a Git repository that contains the Dockerfile and the source code,
    or by pointing to the source code repository and manually providing the Dockerfile.
    In the case of Git repository registration, it is also possible to configure the
    branch and location of the Dockerfile in the repository. additional metadata which
    is required to describe the contents of the image in scientific terms to its potential
    users. Such metadata includes for instance free tags, as well as EDAM18 terms.
    Following the completion of container registration, the image construction and
    integration steps (Figure 2) are automatically run on a dedicated server. The
    trigger of a new build is based on Dockerfile update or via a link (URL with an
    API Key), shown in the web interface when the user is the owner of the tool (created
    it). The creation of a tag on the image uses the same link mechanism. Such a link
    can be used directly (copy/paste in the brower) or via external tools or hooks
    (GitHub web hooks for example). The API also provides the possibility to trigger
    it manually, or to tag a container (i.e. set a version). Figure 2. The BioShaDock
    Docker container processing steps. Download as a PowerPoint slide The Docker images,
    once built and stored in BioShaDock, can be registered in the ELIXIR registry
    (using some LABEL metadata in the Dockerfile). It is also possible to add a link
    to an existing ELIXIR registry entry. By linking its contents to and from the
    ELIXIR registry, BioShaDock enables the discovery of Docker images from a more
    generic system where users might look for a given software without specifically
    searching for container solutions. It hence maximizes the visibility of its images
    and contributes to better software dissemination. Search and execution Listing
    1. An example of Docker image command line invocation using BioShaDock. After
    an automatic download, the container is executed. Here, the program BWA is called
    by default. sudo docker run docker-registry.genouest.org/bioinfo/\ bwa Unable
    to find image \ ’docker-registry.genouest.org/bioinfo/bwa:latest’\ locally latest:
    Pulling from bioinfo/bwa [...] Status: Downloaded newer image for \ docker-registry.genouest.org/bioinfo/bwa:latest
    Program: bwa (alignment via Burrows-Wheeler \ transformation) Version: 0.7.5a-r405
    [...] The images provided by BioShaDock can be executed in various ways (Figure
    3): Figure 3. The BioShaDock use cases. The Docker repository acts as a platform
    that facilitates the dissemination of bioinformatics tools by providing ready
    to use Docker images. Download as a PowerPoint slide • on a personal computer
    with a Linux system (Windows and Mac are supported with the Docker Toolbox), in
    a command line (Listing 1), directly using Docker14; • on a cluster integrating
    a Docker scheduler front-end like GO-DOCKER (v1.0)16; • in any software implementing
    the CWL (Common Workflow Language) specification (draft 3)19,20 such as Arvados21
    or Rabix (v0.6.5)22; • in the D4 workflow portal23 (v0.6); • in the Galaxy environment6
    (v15.10); • in the cloud of the French Institute of Bioinformatics with the help
    of the Docker virtual machine image24. As an illustration, we created a set of
    Galaxy tool descriptors based on Docker images stored by BioShaDock25 available
    in our Toolshed26. Thus, the stacks RADSeq pipeline27 is available as a Galaxy
    tool xml descriptor28 that calls a container stored in BioShaDock29. Implementation
    Listing 2. A container ’Dockerfile’ that defines the automated image build process.
    The LABEL instructions represent metadata. LABEL  name="Emboss" LABEL  homepage="http://emboss.sourceforge.net/"
    LABEL  resourceType="Tool" LABEL  interfaceType="Command line" LABEL  description="The
    European Molecular \   Biology Open Software Suite" LABEL  topic="Data processing
    and validation" #EDAM operation LABEL  functionName="Sequence processing" FROM
    biodckr/biodocker:latest USER root # Install EMBOSS package RUN apt-get update
    && \     apt-get install -y \       emboss=6.6.0-1 && \     apt-get clean && \     apt-get
    purge && \     rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*  USER biodocker WORKDIR
    /data CMD ["embossdata"] MAINTAINER Adam Smith <asmithswx@cnrs.fr> BioShaDock
    is a web application written in python (>=2.7). It manages the container’s build
    and metadata. It is also in charge of authenticating the user against a local
    Docker registry and authorizing the user to push or pull a container according
    to their role (admin, editor, etc.) or rights. A user can give other users access
    to their repository for collaborative work in the edition page of the tool. Collaborators
    can have read only (for private repositories) or read/write access to the tool.
    The backend is based on a local instance of a Docker registry. A script extracts
    the metadata written by the image’s maintainer (Listing 2). Listing 3. An XML
    container metadata description generated from the LABEL instructions by BioShaDock
    and used to publish the container metadata in bio.tools, the ELIXIR registry.  <?xml
    version="1.0" encoding="UTF-8"?> <resources xmlns="http://bio.tools">  <resource>   <name>ngs_multi_vendor_read_corrector</name>   <homepage>http://resourcename.org</homepage>   <resourceType>Tool</resourceType>   <interface>    <interfaceType>Command
    line</interfaceType>    </interface>   <description>    software analysis package
    specially developed for the needs of the molecular biology user community   </description>   <topic
    uri="http://edamontology.org/topic_0220"> Data processing and validation   </topic>   <function>    <functionName
    uri="http://edamontology.org/operation_2446">     Sequence processing    </functionName>    </function>    <contact>    <contactEmail>     asmithswx@cnrs.fr    </contactEmail>   </contact>  </resource>
    </resources> Then, an integrated REST python client (v1.0) manages the container
    indexation in bio.tools (Listing 3). The first version of the registry integrates
    80 Docker images that are versioned and can be re-built when the sources are updated.
    A REST API enables programmatic interaction with the server. For example, it can
    be used by external tools to extract the list of available images for job submissions.
    GO-DOCKER (v1.0) and the D4 workflow portal (v0.6) integrate this feature. The
    access to the images is public. To ensure the quality of available images, BioShaDock
    manages the authentication and ACL (access control list) to restrict the creation
    and update of its images to identified trustful contributors. The current implementation
    (v1.0) enables authentication using LDAP, Google or GitHub. Discussion The aim
    of BioShaDock is to contribute to the aggregation and standardization of bioinformatic
    tools and utilities. Maintaining ready to use validated and versioned software
    is key in ensuring the reproducibility needed in an open science approach. Thereby,
    the creation of a collection of tools embedded in Docker containers, as provided
    by BioShaDock, is a pragmatic solution to this major bottleneck. A number of other
    projects also focus on the provision of bioinformatic Docker images. BioDocker30
    is a community based initiative to encourage the use of Docker images in bioinformatics.
    A GitHub repository stores a list of Dockerfiles that define the construction
    of images for the corresponding bioinformatic tool, with an open yet controlled
    contribution mechanism. Bioboxes31 is an open source project that defines guidelines
    to build bioinformatic tool images using compatible interfaces for images which
    perform the same task, independent of the underlying tool, hence favoring interoperability
    between tools. It is therefore, among other characteristics, very well suited
    to automate tool and pipeline benchmarks. It has been applied to the assessment
    of different types of NGS data processing methods that concern assembly software
    as well as metagenomics tool. Dockstore32 is an open platform that enables the
    registration of Docker images described using CWL. It integrates with a number
    of external services for source code and image hosting, and focuses on the provision
    of images that can be integrated in CWL-ready environments. BioShaDock shares
    with these existing efforts the use of Docker as a container technology to facilitate
    the distribution and integration of bioinformatic tools. However, none of these
    systems are designed to provide local image building and storage options. Furthermore,
    we believe the integration of BioShaDock with external domain-centric and platform-agnostic
    registries such as the ELIXIR registry will significantly raise the visibility
    of both the images provided and the container technology itself to the community
    of bioinformatic tool users. Because the files that describe the image building
    process (Dockerfiles) are usually freely available online, the interoperability
    issues between Docker registry initiatives are potentially very limited. Conclusions
    Computer scientists and bioinformaticians can more easily disseminate their programs
    and find potential users using a dedicated domain-centric Docker registry. There
    is a wide range of perspective uses for container registries in bioinformatics:
    repositories managed at a community level, based on tools embedded in containers,
    promote the ability to exchange and replicate data analyses. In addition, the
    association between workflow models, data references and containerized tools could
    lead to the creation of interoperable and ready to use analysis components and
    pipeline collections maintained by many contributors. The development of such
    specifications is already in progress as illustrated by the CWL (Common Workflow
    Language)20 and the A-SCDFM (Autonomous Semi-Concrete Data Flow Model)33 portable
    workflow formats that are natively compatible with containers. In this case, the
    integration of programs in a container registry like BioShaDock and the formalization
    of the data processing following one of these new portable workflow specifications
    could simplify the creation of reproducible benchmarks, teaching material, demos
    and the production of use case prototypes. It could also be used by article reviewers
    to quickly evaluate a software. The spread of container usage in the bioinformatics
    community and their indexing in repositories can be a solution to capture and
    share a large collection of data analysis methods. A wide set of bioinformatics
    components available on demand could induce better data analysis by simplifying
    tests and benchmarks. Software availability Server • BioShaDock registry: https://docker-ui.genouest.org
    • BioShaDock home page: http://bioshadock.genouest.org Source code • BioShaDock
    client and tools: https://github.com/fjrmoreews/bioshadock_client • BioShaDock
    local server: https://bitbucket.org/osallou/bioshadock • Archived source code
    at the time of publication (client): https://zenodo.org/record/3458834 • Archived
    source code at the time of publication (server): https://zenodo.org/record/3458735
    License Apache 2.0 Author contributions FM and OS conceived the software and developed
    the web interface and the build system. HM participated to the meta-data publishing
    feature design. YLB and CM designed some of the first Dockerfile and integrated
    Docker images in our Galaxy toolshed. OC and CB managed the deployment and infrastructure
    availability. All authors helped prepare the manuscript. Competing interests No
    competing interests were disclosed. Grant information Funding was provided from
    the Western France e-science project supported by Brittany and Pays de la Loire
    regions (e-Biogenouest/052012). I confirm that the funders had no role in study
    design, data collection and analysis, decision to publish, or preparation of the
    manuscript. Acknowledgements We would like to thank the IFB/Genouest platform
    for hosting the software and the use of its cluster to build the images. We thank
    the IFB-CORE team, especially Marie Grosjean and Sandrine Perrin, for creating
    containers and for supporting the deployment of the registry coupled with the
    IFB cloud facility. Finally, we also thank the BioDocker core team, Felipe da
    Veiga Leprevost, Yasset Perez-Riverol and Saulo Alves Aflitos for collaborative
    efforts. Supplementary material BioShaDock API documentation: http://www.genouest.org/api/bioshadock-api
    References 1.  Woelfle M, Olliaro P, Todd MH: Open science is a research accelerator.
    Nat Chem. 2011; 3(10): 745–748. PubMed Abstract | Publisher Full Text 2.  Stajich
    JE, Lapp H: Open source tools and toolkits for bioinformatics: significance, and
    where are we? Brief Bioinform. 2006; 7(3): 287–296. PubMed Abstract | Publisher
    Full Text 3.  Ison J, Rapacki K, Ménager H, et al.: Tools and data services registry:
    a community effort to document bioinformatics resources. Nucleic Acids Res. 2015;
    pii: gkv1116. PubMed Abstract | Publisher Full Text 4.  Connor BO, Kartashov A,
    Yuen D, et al.: ELIXIR Tools and Data Services Registry. 2015. Reference Source
    5.  Goecks J, Nekrutenko A, Taylor J, et al.: Galaxy: a comprehensive approach
    for supporting accessible, reproducible, and transparent computational research
    in the life sciences. Genome Biol. 2010; 11(8): R86. PubMed Abstract | Publisher
    Full Text | Free Full Text 6.  Aranguren ME: Merging OpenLifeData with SADI services
    using Galaxy and Docker. BioRxiv, Cold Spring Harbor Labs. 2015. Publisher Full
    Text 7.  Blankenberg D, Von Kuster G, Bouvier E, et al.: Dissemination of scientific
    software with Galaxy ToolShed. Genome Biol. 2014; 15(2): 403. PubMed Abstract
    | Publisher Full Text | Free Full Text 8.  Lawlor B, Walsh P: Engineering bioinformatics:
    building reliability, performance and productivity into bioinformatics software.
    Bioengineered. 2015; 6(4): 193–203. PubMed Abstract | Publisher Full Text | Free
    Full Text 9.  Prins P, de Ligt J, Tarasov A, et al.: Toward effective software
    solutions for big biology. Nat Biotechnol. 2015; 33(7): 686–687. PubMed Abstract
    | Publisher Full Text 10.  Van Gorp P, Mazanek S: SHARE: a web portal for creating
    and sharing executable research papers. Procedia Comput Sci. 2011; 4: 589–597.
    Publisher Full Text 11.  Granger B, Avila D, Perez F, et al.: Jupyter: Open source,
    interactive data science and scientific computing across over 40 programming languages.
    2015. Reference Source 12.  Kanterakis A, Kuiper J, Potamias G, et al.: PyPedia:
    using the wiki paradigm as crowd sourcing environment for bioinformatics protocols.
    Source Code Biol Med. 2015; 10(1): 14. PubMed Abstract | Publisher Full Text |
    Free Full Text 13.  Merkel D: Docker: Lightweight Linux containers for consistent
    development and deployment. Linux J. 2014; (239). Reference Source 14.  Docker.
    2013; [Online; accessed 16-Nov-2015]. Reference Source 15.  google Inc: Google
    Container Engine. 2015. Reference Source 16.  Sallou O, Monjeaud C: GO-Docker:
    Batch scheduling with containers. IEEE Cluster 2015. 2015. Reference Source 17.  Di
    Tommaso P, Palumbo E, Chatzou M, et al.: The impact of Docker containers on the
    performance of genomic pipelines. PeerJ. 2015; 3: e1273. PubMed Abstract | Publisher
    Full Text | Free Full Text 18.  Ison J, Kalas M, Jonassen I, et al.: EDAM: an
    ontology of bioinformatics operations, types of data and identifiers, topics and
    formats. Bioinformatics. 2013; 29(10): 1325–1332. PubMed Abstract | Publisher
    Full Text | Free Full Text 19.  Peter A, Nebojsa T, Stian SR, et al.: Beyond Galaxy:
    portable workflows and tool definitions with the CWL. In Galaxy Community Conference
    2015. Norwich, United Kingdom, 2015. Reference Source 20.  Amstutz P, Chilton
    J, Crusoe MR, et al.: Common Workflow Language. 2015. Reference Source 21.  Arvados.
    2015. Reference Source 22.  Rabix. 2015. Reference Source 23.  Francois M: D4
    Workflow Portal. 2015. Reference Source 24.  IFB cloud: The academic cloud of
    the French Institute of Bioinformatics. Online; accessed 2015-09-24. Reference
    Source 25.  Moreews F, Sallou O, Bras YL, et al.: A curated Domain centric shared
    Docker registry linked to the Galaxy toolshed. In Galaxy Community Conference
    2015. Norwich, United Kingdom, 2015. Reference Source 26.  Bras YL, Monjeau C:
    GUGGO Galaxy ToolShed. 2014. [Online; accessed 05-Nov-2015]. Reference Source
    27.  Catchen J, Amores A, Hohenlohe P, et al.: STACKS, a software pipeline for
    building loci from short-read sequence. 2010. [Online; accessed 02-Dec-2015].
    Reference Source 28.  Bras YL, Monjeaud C: STACKS pipeline, galaxy tool descriptor.
    2010. [Online; accessed 02-Dec-2015]. Reference Source 29.  Bras YL, Monjeaud
    C: STACKS pipeline, docker container. 2010. [Online; accessed 02-Dec-2015]. Reference
    Source 30.  BioDocker. 2015. Reference Source 31.  Belmann P, Dröge J, Bremges
    A, et al.: Bioboxes: standardised containers for interchangeable bioinformatics
    software. Gigascience. 2015; 4: 47. PubMed Abstract | Publisher Full Text | Free
    Full Text 32.  Connor BO, Kartashov A, Yuen D, et al.: DockStore. 2015. Reference
    Source 33.  Moreews F: Design and share data analysis workflows. Application to
    bioinformatics intensive treatments. Thesis, université de rennes 1. 2015. 34.  Francois
    M, Olivier S: BioShaDock client. Zenodo. 2015. Data Source 35.  Olivier S, Francois
    M: BioShaDock server. Zenodo. 2015. Data Source Comments on this article Version
    1 Comment                     keyboard_arrow_left Open Peer Review Reviewer Status
    info_outline Reviewer Reports Invited Reviewers 1 2 Version 1 14 Dec 15 read read
    Rodrigo Lopez, European Bioinformatics Institute, Cambridge, UK Björn A. Grüning,
    University of Freiburg, Freiburg, Germany Comments on this article All Comments(0)
    Add a comment Sign up for content alerts SIGN UP Browse by related subjects Bioinformatics
    Biological sciences Computer and information sciences Natural sciences Operating
    system Programming language           An innovative open access publishing platform
    offering rapid publication and open peer review, whilst supporting data deposition
    and sharing. BROWSE GATEWAYS COLLECTIONS HOW IT WORKS BLOG CONTACT FOR DEVELOPERS
    COOKIE NOTICE PRIVACY NOTICE RSS SUBMIT YOUR RESEARCH Follow us © 2012-2024 F1000
    Research Ltd. ISSN 2046-1402 | Legal | Partner of Research4Life • CrossRef • ORCID
    • FAIRSharing   Cookies Button About Cookies On This Site We and our partners
    use cookies to enhance your website experience, learn how our site is used, offer
    personalised features, measure the effectiveness of our services, and tailor content
    and ads to your interests while you navigate on the web or interact with us across
    devices. By clicking "Continue" or continuing to browse our site you are agreeing
    to our and our partners use of cookies. For more information seePrivacy Policy
    CONTINUE'
  inline_citation: '>'
  journal: F1000Research
  limitations: '>'
  pdf_link: null
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: 'BioShaDock: a community driven bioinformatics shared Docker-based tools
    registry'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/tst.2017.7830891
  analysis: '>'
  authors:
  - Zhenhua Li
  - Zhang Yun
  - Yunhao Liu
  citation_count: 29
  full_citation: '>'
  full_text: '>

    Scheduled Maintenance: On Tuesday, 16 April, IEEE Xplore will undergo scheduled
    maintenance from 1:00-5:00 PM ET (1700-2100 UTC). During this time, there may
    be intermittent impact on performance. We apologize for any inconvenience. IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account
    Personal Sign In Personal Sign In * Required *Email Address *Password Forgot Password?
    Sign In Don''t have a Personal Account? Create an IEEE Account now. Create Account
    Learn more about personalization features. IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: Tsinghua science and technology/Tsinghua Science and Technology
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/5971803/7830888/07830891.pdf
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Towards a full-stack devops environment (platform-as-a-service) for cloud-hosted
    applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.future.2017.01.006
  analysis: '>'
  authors:
  - Matthias Geiger
  - Simon Harrer
  - Jörg Lenhard
  - Guido Wirtz
  citation_count: 56
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. Analysis
    of BPMN 2.0 implementers 4. Assessment of current BPMN support 5. Evolution of
    BPMN support 6. Conclusion and future work Acknowledgments References Vitae Show
    full outline Cited by (62) Figures (7) Show 1 more figure Tables (4) Table 1 Table
    2 Table 3 Table 4 Future Generation Computer Systems Volume 80, March 2018, Pages
    250-262 BPMN 2.0: The state of support and implementation Author links open overlay
    panel Matthias Geiger a, Simon Harrer a, Jörg Lenhard b, Guido Wirtz a Show more
    Add to Mendeley Share Cite https://doi.org/10.1016/j.future.2017.01.006 Get rights
    and content Highlights • Only three out of 47 vendor implementations of BPMN do
    support the direct execution of the native BPMN format. • These three implementations
    only support around half of the features of BPMN. • The feature support is sufficient
    to implement at least 13 out of 20 workflow control-flow patterns. • Erroneous
    process models are often not rejected at deployment. • There is only a limited
    increase in feature support over the last four years. Abstract The Business Process
    Model and Notation 2.0 (BPMN) standard has been hailed as a major step in business
    process modeling and automation. Recently, it has also been accepted as an ISO
    standard. The expectation is that vendors of business process management systems
    (BPMS) will switch to the new standard and natively support its execution in process
    engines. This paper presents an analysis of the current state and evolution of
    BPMN 2.0 support and implementation. We investigate how BPMN 2.0 implementers
    deal with the standard, showing that native BPMN 2.0 execution is an exception.
    Only three out of 47 BPMS considered support the execution format defined in the
    standard, although all of them claim to comply to the BPMN 2.0 standard. Furthermore,
    we evaluate three process engines that do provide native BPMN support, namely
    camunda BPM, jBPM and activiti, and examine the evolution of their degree of support
    over a period of more than three years. This lets us delimit the areas of the
    standard that are considered important by the implementers. Since there is only
    a limited increase in supported features over the past years, it seems that the
    implementation of the standard is more or less concluded from the perspective
    of the implementers. Hence, it is unlikely that features which are not available
    by now will be implemented in the future. Previous article in issue Next article
    in issue Keywords BPMNProcess engineSoftware evolutionConformance testing 1. Introduction
    Business process management (BPM) is a broad discipline with influence on other
    areas of computing research, such as service composition  [1]. BPM is strongly
    tailored to the modeling of organizational processes in dedicated modeling languages
    and the subsequent implementation of process models in executable software. Among
    the many languages for process modeling that are available today (examples are
    discussed in  [2]), the Business Model and Notation (BPMN)  [3] has emerged as
    a widely accepted candidate  [4], in industry and academia alike. Its potential
    for consolidating the market of process standards is emphasized by its subsequent
    acceptance as an ISO standard  [5].1 With the publication of BPMN 2.0 in January
    2011, support for the native execution of BPMN process models and a standardized
    serialization format has been added to the standard. The intention underlying
    this addition is to mitigate the gap between the modeling of processes on the
    one hand and their execution through software on the other hand  [4]. When bridging
    this gap by translating a model into executable software, a variety of problems
    arise that are the topic of various publications, e.g., [6], [7], [8], regardless
    of the concrete modeling language used. Especially for industry-size process models,
    this manual translation step is non-trivial and can lead to a deviation of the
    actual behavior of the process implementation from the desired execution semantics
    captured in the process model. The direct execution of a process model, enabled
    by BPMN 2.0, intends to minimize the distance between desired and actual behavior.
    It avoids the necessity for a translation step in the first place and has the
    potential to preempt the behavioral deviation of an implementation from a corresponding
    model. The specification of the execution semantics of a process model in a standards
    document, as achieved with BPMN 2.0  [5], is a first step towards the goal of
    direct execution. As a next step, it is necessary that the execution semantics
    are correctly and completely implemented by vendors supporting the standard. In
    the case of BPMN, the second step faces two major obstacles: Firstly, the implementation
    of the specified execution semantics needs to be feasible, which unfortunately
    cannot be guaranteed by the purely theoretical and informal discussion of the
    execution semantics in the BPMN 2.0 standard  [9]. Secondly, the vendors need
    to actually implement the same semantics and not interpretations or customizations
    thereof. Since there is no certification authority for BPMN, any vendor can claim
    implementation completeness and full standard conformance without proof of this
    claim. On the other hand, it may well be possible that only a subset of BPMN is
    valuable from a practitioner’s point of view, as indicated in  [10]. As a result,
    vendors might want to implement only a limited part of the standard. In this paper,
    we provide a comprehensive investigation of the current products of BPMN 2.0 implementers.
    First, we analyze the products of a broad list of vendors that claim to implement
    BPMN 2.0. This shows that a vast majority of current self-acclaimed BPMN 2.0 implementations
    is limited to visualization and does not actually implement the execution semantics
    of BPMN 2.0. Thereafter, we extend the standard conformance analysis from prior
    work  [11] with a more sophisticated set of tests, covering a higher degree of
    the executable part of the standard. Moreover, we also analyze whether common
    feature combinations, defined by the so-called workflow patterns  [12], are supported.
    We compare multiple revisions of several engines that support the native execution
    of BPMN 2.0 over a period of more than three years. This lets us see how BPMN
    2.0 support has evolved since its adoption as an ISO standard. The current development
    indicates that the implementation of BPMN 2.0 is consolidating at a level that
    excludes a large part of the actual execution semantics of the standard. This
    article builds upon  [13] and addresses several gaps and weakness points: (a)
    insights on engine quality are limited, as the test suite only encompasses tests
    for conformance and expressiveness, (b) the longitudinal study is limited to a
    period of only two years, and last but not least, (c) new engine versions were
    published in the meantime, limiting the currentness of the data underlying the
    findings. In this work, we mitigated these issues as follows. The test suite has
    been extended with a new category of static analysis tests. This addition doubles
    the amount of tests and provides us with more detailed insights on engine quality.
    Furthermore, we enhanced the presentation of the conformance data with several
    new graphics. Through the addition of new engine versions, we were able to extend
    the scope of our longitudinal study by one third. The added results gained through
    this addition reinforce our findings from  [13] and harden the interpretation
    of the data. Last but not least, we updated both, the related work section taking
    into account more recent results that were published in the meantime, and the
    engine conformance data through repeating the benchmarks with the most recent
    engine versions available at the time of writing. The remainder of this paper
    is structured as follows: The next section outlines related work. In Section  3,
    we evaluate 47 products that claim to comply to BPMN 2.0. Thereafter, in Section  4,
    we detail our procedure for assessing standard support of BPMN engines and show
    the results of three evaluated products. The outcomes of our longitudinal study,
    showing the evolution of support over the course of more than three years, round
    off our findings in Section  5. Finally, Section  6 concludes the work. 2. Related
    work Work related to this paper separates in three major areas: First, we discuss
    work that concerns the native serialization format and execution of BPMN 2.0 process
    models, including problems experienced during this task. Second, we outline approaches
    for evaluating and benchmarking BPMN 2.0 engines. Third, we present approaches
    for building reproducible benchmarks, which is what we aim for in this paper.
    2.1. BPMN 2.0 serialization format and native execution Since its first publication,
    BPMN has been evaluated in various studies regarding its appropriateness for business
    process modeling. An overview of relevant work is given by Aagesen and Krogstie  [14].
    There are some studies investigating the appropriateness of the BPMN serialization
    format, its pitfalls, and its support in BPMN modeling tools  [15], [16], [17].
    When it comes to the usage of BPMN 2.0 execution semantics and the serialization
    format, Chinosi and Trombetta [4] present first results based on a survey. Already
    at that time, shortly after the publication of BPMN 2.0, 40% of the survey respondents
    claimed to use the new native serialization format. But an analysis of modeling
    systems performed at that time by Evéquoz and Sterren  [16] documents only rudimentary
    support, especially for exporting in the standardized format. In  [15] issues
    in the specification of the serialization format of BPMN are discussed and a preliminary
    analysis of the BPMN support in modeling tools is presented. The most recent study
    by Kurz  [17] shows that the situation has improved considerably in the last five
    years. One important reason, as indicated in  [17], is the work of the BPMN Model
    Interchange Working Group (BPMN MIWG), which targets the improvement of the model
    portability between modeling tools.2 Although previous BPMN versions are analyzed
    and criticized often  [18], [19], [20], studies covering the BPMN 2.0 execution
    semantics are rare, and studies investigating BPMN 2.0 process engines even more
    so. As discussed by Börger  [9], the description of the BPMN 2.0 execution semantics
    is still inaccurate and ambiguous in various places. As a result, the goal of
    fully standard-conformant BPMN 2.0 implementations with identical execution behavior
    is elusive  [9]. The same aspect is confirmed by Gutschier et al.  [21], as the
    authors discuss underspecifications for a particular language construct, the service
    task, which are likely to result in differing runtime behavior for different implementations.
    Also Kossak et al.  [22] target this aspect and provide a comprehensive discussion
    of the execution semantics of BPMN 2.0 and their shortcomings. To improve the
    situation, Kossak et al.  [22] refine the specification using abstract state machines.
    Another approach to formalize and analyze the BPMN 2.0 execution semantics using
    graph rewrite techniques is presented in  [23]. Gorp and Dijkman  [23] claim that
    their implementation of the execution semantics can be seen as a reference implementation,
    and can be used for tool vendors to check their conformance to the standard. In
    contrast to our approach, this conformance check has to be performed manually,
    whereas we support fully automated conformance checking. 2.2. Evaluating and benchmarking
    BPMN 2.0 engines When it comes to the evaluation of BPMN 2.0 process engines,
    our prior work on standard conformance  [11], [13] is obviously related. Since
    we build on this approach, it will be described in more detail in the following
    sections. A short case study that analyzes several capabilities of a set of BPMN
    2.0 engines is presented by Dirndorfer et al.  [24]. At that time, practically
    none of the engines under focus was able to correctly import models in the native
    BPMN serialization format. This is an interesting result which is confirmed by
    our analysis of BPMN implementers in Section  3. However, it contradicts the already
    mentioned results from  [4]. There seems to be a discrepancy between modeling
    tools which are able to create and consume standard compliant BPMN models and
    execution engines which are unable to work with those models. Apart from the evaluation
    of standard conformance, also other aspects of BPMN 2.0 engines have been investigated.
    A notable approach for evaluating the performance characteristics of BPMN engines
    is the BenchFlow project  [25]. Kluza et al.  [26] present another comparative
    study which evaluates the three BPMN engines activiti, jBPM and camunda BPM with
    regard to their general architecture and their extensibility. However, the focus
    in  [26] is on the integration of semantic techniques, such as ontologies, to
    improve the quality of the processes to be executed. An even more general study,
    not only covering BPMN engines but BPMS as a whole, is  [27]. The focus of this
    study lies more on the general suitability of BPMS for business needs than on
    their conformance to process standards. Therefore, standard support is discussed
    only on a high level. 2.3. Computational reproducibility Reproducible research,
    especially computational reproducibility, is key in today’s science  [28]. Many
    scientific results are obtained with the help of extensive software tooling, without
    which it is not possible to reproduce or to check the correctness of results.
    To enable computational reproducibility, Merali  [28] suggests to make necessary
    material freely available, preferably via a version-control system. This encompasses
    the documented, tested, and modular code on which scientific computations are
    based, as well as all other needed resources and the results themselves. But even
    following these guidelines, researchers trying to reproduce experiments still
    face “significant barriers”  [29], e.g., imprecise documentation which is hard
    to follow or steep learning curves for the tools used in experiments. To increase
    the reproducibility of scientific computations, Boettiger  [29] and Chamberlain
    and Schommer  [30] suggest to use Docker3 as the basis for computational environments,
    since this technology lowers several of the aforementioned barriers. In Docker,
    one can create light-weight Linux containers based on Docker images. These images
    can be shared, reused, archived, put under version control, and deployed on different
    platforms  [29]. What is more, such an image can be built automatically by passing
    a Dockerfile (i.e., a list of commands) to Docker itself, easing the reproduction
    of the software environment creation for third parties. To enable reproducibility
    of the benchmark presented in this paper, its execution is based on Docker. This
    is in line with related benchmarking approaches  [25]. 3. Analysis of BPMN 2.0
    implementers To gather insights on the state of BPMN implementation, it is necessary
    to investigate the current market of systems that implement the standard. To this
    end, we evaluate public listings of BPMN implementers and judge the state of their
    implementations when it comes to the execution of BPMN 2.0 processes in this section.
    Only a small subset of the existing implementations is mature enough to justify
    an evaluation of the degree of standard conformance they exhibit. This section
    describes the selection of such mature engines from the actual market. Section  4
    analyzes the degree of standard conformance of the selected engines. At a first
    glance, the market of BPM suites and engines supporting BPMN seems to be rather
    active. A list of BPMN implementers4 contains more than 60 entries at the time
    of writing. However, these implementers provide not only BPMN engines but also
    BPMN modeling tools, or other tools claiming to implement (some) BPMN aspects.
    With regard to BPMN execution semantics conformance, only fully fledged BPMS suites
    or dedicated BPMN engines are of relevance, as they should be able to execute
    BPMN process models. Another “list of active BPMN 2.0 Engines” is available at
    Wikipedia,5 which has been created and is maintained by members of the BenchFlow
    project.6 At the time of writing it lists 27 entries. After comparing and consolidating
    both lists, we arrived at 47 BPM suites that claim to conform to the BPMN standard.
    This is an addition of two BPM suites in comparison to prior work  [13]. 3.1.
    Requirements for BPMN engines The standard document  [5] is rather strict regarding
    Process Execution Conformance: “The tool claiming BPMN Execution Conformance type
    MUST fully support and interpret the operational semantics and Activity life-cycle
    specified in sub clause 14.2.2. […] Conformant implementations MUST fully support
    and interpret the underlying metamodel”  [5]. Following these statements, there
    are two main aspects relevant for BPMN engines and BPMS: All language constructs
    defined in the BPMN metamodel must be supported, and for each language construct
    the underlying execution semantics must be implemented correctly. The only exceptions
    from this are a few “non-operational elements” defined in  [5]. Derived from those
    two requirements, BPMN engines should also be able to detect BPMN process models
    which are incorrect regarding the BPMN metamodel: If such an invalid process is
    deployed on a BPMN engine, its runtime behavior is not predictable. The process
    execution might fail silently, a runtime exception could occur, or other unintended
    behavior may happen. Therefore, BPMN compliant engines should be able to detect
    violations of the BPMN specification  [5] in the process models to be deployed,
    i.e., they should perform static analysis checks at deploy-time. The purpose of
    the standardization of BPMN is to achieve portability and interoperability between
    different modeling tools and engines. A common workflow for implementing processes
    is to model “basic” business process models in a BPMN modeling tool first, and,
    subsequently, to enrich them with execution details specific to the BPMS used
    for execution. A strong indicator for the importance of this use case is the ongoing
    effort put into interchange of models by various vendors of modeling tools and
    engines in the BPMN MIWG  [17]. Therefore, we require BPMN engines and BPMS to
    support the import of the standardized XSD-based serialization format. To summarize
    the requirements stated above, a conforming BPMN implementation must: 1. support
    the usage of all constructs defined in the standard, 2. implement the behavior
    of the constructs as specified, 3. be able to detect invalid BPMN process models,
    and 4. be able to consume the standardized serialization format. Only products
    that fulfill these requirements can be evaluated to support process execution
    conformance, as defined in the standard. 3.2. Requirements evaluation for BPMS
    We evaluated the requirements stated in the previous section for all 47 BPMN implementations
    mentioned before using a structured methodology. First, we checked the publicly
    available information on the vendor websites. If a version of the BPMN implementation
    was available for free usage or evaluation purposes, we downloaded this version.
    For each available product, we checked whether existing integrated modeling tools
    support the BPMN metamodel at least partially, i.e., whether at least some BPMN
    elements with their defined attributes are available. If this was the case, or
    if direct deployment of BPMN files was possible, we tried to import and deploy
    an existing BPMN process model. Only products that pass all these tests can, in
    principle, be classified as an implementation of the execution semantics of BPMN.
    The results for the 47 tested products, depicted in Fig. 1, are rather surprising.
    For 26 out of the 47 products, evaluation was not possible for various reasons:
    The development and distribution of five products has been discontinued. Eleven
    products provide no suitable public information (too little information to meaningfully
    evaluate BPMN support, or information is not available in English). Two products
    offer some modeling and execution functionality, but neither the used shapes nor
    the used serialization format correspond to the BPMN standard. Hence, it is unclear,
    why these tools are listed as BPMN implementers to begin with. For eight products,
    a closer analysis is not possible in the context of this paper, due to licensing
    restrictions. For instance, some vendors explicitly prohibit the benchmarking
    and evaluation of their products, which forces us to omit them from a closer investigation.
    Download : Download high-res image (161KB) Download : Download full-size image
    Fig. 1. Results of the product analysis. The usage of graphical BPMN shapes to
    model processes is supported by twelve BPMS, but the definition of attributes
    does not conform to the standard and/or the import of BPMN process models stored
    in the standardized format is not possible. As a result, we conclude that the
    usage of standard compliant models is not possible. Therefore, of the original
    47 products, only 9 support at least parts of the BPMN 2.0 metamodel, can import
    standard compliant models, and provide enough information for a detailed assessment.
    Of those remaining BPMS, three engines require the usage of extensions that are
    not covered by the standard, or restrict the usage of essential features such
    as IDs and task types. As a result, the models consumed or produced by these systems
    are substantially different from the BPMN 2.0 standard, although they share some
    syntactical elements. Considering all these aspects, only six systems justify
    a closer analysis regarding the degree to which they actually implement the BPMN
    standard. The remaining products are able to digest process models conforming
    to the BPMN standard, but for three of these, model import, deployment or process
    execution is only possible via manual operations using a graphical interface.
    This severely hampers computational reproducibility and, thus, is prohibitive
    for our approach of conformance checking (see Section  4.1): We check each language
    construct of the executable part of BPMN in each possible configuration. Furthermore,
    we want to guarantee isolation of tests by providing a new process engine instance
    for each test run, and by running each test various times to check reproducibility
    of results. Those aspects are only feasible if automated deployment and execution
    is possible. Therefore, only three engines can be assessed here: These engines
    are activiti, camunda BPM, and jBPM.7 3.3. Threats to completeness Our final selection
    of three out of the initial 47 products is subject to several threats to completeness.
    First, the evaluation was conducted manually and is therefore prone to human errors.
    To mitigate this risk, we peer-reviewed our evaluation and discussed the findings
    with domain experts. Second, it is possible that we did not include relevant products
    in our initial set. We try to avoid this by integrating two public listings that
    are maintained by other groups, including an official list by the authors of the
    standard. Nevertheless, it could be the case that a BPMN implementation exists,
    which is not part of either of these lists. Third, not every product that may
    be feasible to evaluate can be evaluated, due to legal reasons. Some products
    are shipped with licenses that forbid the user to evaluate them and publish the
    results. For other products, evaluation copies are simply not available. This
    evaluation barrier is the most problematic threat to completeness. We contacted
    some vendors, whose products fell into this category, but they were either not
    willing to provide us with a license or required the signing of a non-disclosure
    agreement, which would have inhibited the publication of any results. Thus, it
    was not possible to test more BPMN engines here. 4. Assessment of current BPMN
    support In this section, we benchmark the BPMN support of the three engines selected
    in the previous section. We use the newest version that was available on June
    30th 2016, namely, activiti 5.20.0, camunda BPM 7.5.0 and jBPM 6.4.0, thereby
    updating the versions assessed in previous work  [11], [13]. The setup of the
    benchmark is explained in Section  4.1, followed by the test suite, which consists
    of tests for native BPMN support, tests for workflow control-flow pattern support,
    and static analysis tests for constraint violations, in Section  4.2. The results
    of our assessment are given in Section  4.3. 4.1. Benchmark setup The updated
    study presented in this article is conducted on an improved version of the BPEL/BPMN
    Engine Test System (betsy) as presented in  [13]. In previous work, betsy has
    been used to benchmark BPEL  [31] as well as BPMN engines  [11]. For  [13], we
    have improved and extended betsy by (1) adding the ability to test several versions
    of activiti, jBPM, and camunda BPM, (2) enabling betsy to distinguish real concurrency
    and pseudo-parallelism during process execution, (3) adding a test suite for evaluating
    workflow control-flow pattern support in BPMN, and (4) porting betsy to Linux
    to enable execution with Docker. For this work another test suite to test the
    static analysis capabilities of the BPMN engines has been added, which will be
    described in detail in Section  4.2. Betsy has its own domain specific test language
    to express conformance tests and their results  [31], [11]. A test corresponds
    to an executable process that should be used to test a single language construct,
    a workflow control-flow pattern, or a static analysis rule, depending on the test
    suite. For example, a process in the language construct test suite uses a specific
    BPMN 2.0 language construct, e.g., a ScriptTask or an ExclusiveGateway. Each test
    has at least one test case, consisting of one or more test steps. These steps
    define inputs to the test, in the form of string or integer variables. Test inputs
    are injected into the process during execution as properties. Moreover, test steps
    define expected test outputs in form of test assertions. An assertion specifies
    execution traces of a process model and is used to verify the correctness of a
    test after its execution. Execution traces are written during execution by the
    means of BPMN 2.0 ScriptTasks. By comparing the expected execution trace with
    the actual one, it is possible to determine whether a test case was executed successfully.
    The testing of concurrency within a process has been implemented by adding the
    ability to store labeled timestamps into the log trace. By checking if time stamps
    overlap, it is possible to detect concurrent execution. Generally, the test results
    state for each test case, (a) if the respective process could be deployed on a
    particular engine, and (b) if the test case was executed successfully. A test
    is considered to be passing, if the process can be deployed and all test cases
    are executed successfully. An exception is the test suite for static analysis
    rules which expects the process to be rejected at deploy time. Due to special
    schedulers in both activiti and camunda BPM, some test cases are not completely
    reproducible on each run, but will eventually show that a feature is supported.
    Because of this, we reran the test suites multiple times. On starting a benchmark,
    betsy executes a test workflow for each test and each engine under test. First,
    the native BPMN format of a test is adapted to the engine under test. For instance,
    a concrete scripting language needs to be set (this aspect is not treated by the
    BPMN standard). To ensure that each test is executed in isolation, a fresh instance
    of the engine under test is installed and started. Next, the process containing
    the language feature under test is deployed on the running engine. The deployment
    may require the creation of a deployment package, depending on the engine under
    test. Next, the test itself is executed by triggering all test cases with their
    test steps subsequently. Following this, execution traces are gathered from the
    log file and compared to the expected traces, thereby determining test success
    or failure. Last, the engine under test is shut down. We used the following steps
    to ensure that the benchmark conducted as part of this work can be considered
    reproducible: Betsy itself, including its test suites, is open source and freely
    available on a version control system.8 This holds true for the engines under
    test as well. In addition, the functional correctness and quality of betsy is
    safeguarded using a continuous integration pipeline. The benchmark is done within
    a fixed Docker-based environment9 for betsy. The pre-built image on which the
    experiment was conducted is also available.10 4.2. Test suites for BPMN conformance
    assessment There are three test suites for assessing the conformance of engines
    to the BPMN standard: The first test suite bundles tests for the language constructs
    described in the BPMN specification  [5]. The second one focuses on more complex
    construct combinations that are frequently needed in process models, captured
    in the form of workflow control-flow patterns  [12]. The last test suite comprises
    process models which violate the BPMN specification and should be rejected by
    the engines under test. 4.2.1. Language constructs The first test suite has already
    been used in  [11] and covered 27 different language constructs (such as ExclusiveGateway)
    which have been tested by 70 feature tests divided in five different groups. Using
    feature tests we are able to determine whether all possible configurations (e.g,
    usage of a default SequenceFlow for ExclusiveGateways) are supported by the engines
    under test. Although the 27 language constructs represent the majority of the
    BPMN constructs, we added further constructs for  [13] to tackle several limitations
    mentioned in  [11]: First, we added another construct group, data, which contains
    basic tests for the language constructs DataObject and Property. Second, we added
    further tests for inter-process communication performed with the help of Signals,
    MessageEvents, SendTasks and ReceiveTasks. Moreover, we now also test MultipleEvents,
    the usage of EventDefintionRefs, AdHocSubProcesses and different startQuantity
    and completionQuantity settings of activities (subsumed as TokenCardinality).
    Apart from those newly added language constructs we refined and added feature
    tests for various other language constructs. In comparison to  [13] we removed
    the group error which contained invalid processes that should be detected by the
    engines. The reason for this is that we specifically test invalid processes with
    the new static analysis test suite. Thus, the eight feature tests for four different
    language constructs mentioned in  [13] are now part of the new test suite and
    no longer integrated in the feature test suite. The resulting test suite for checking
    native BPMN support consists of 33 language constructs tested by 105 feature tests.
    An overview of all covered constructs and the number of associated feature tests
    is shown as part of the results in Table 1. Table 1. Tested BPMN features and
    results. Group Language construct Features Activiti camunda BPM jBPM 33 105 48
    60 56 Basics 4 6 5 5 3 Empty Cell Lanes 1 1 1 1 Empty Cell Participant 1 1 1 1
    Empty Cell SequenceFlow 1 1 1 1 Empty Cell SequenceFlowConditional 3 2 2 0 Activities
    9 27 10 10 4 Empty Cell AdHocSubProcess 2 0 0 0 Empty Cell CallActivity 2 1 1
    1 Empty Cell LoopTask 6 1 1 1 Empty Cell MultiInstanceTask 8 5 5 0 Empty Cell
    ReceiveTask 2 0 0 0 Empty Cell SendTask 1 0 0 0 Empty Cell SubProcess 1 1 1 1
    Empty Cell TokenCardinality 4 1 1 0 Empty Cell Transaction 1 1 1 1 Gateways 6
    14 12 12 11 Empty Cell ComplexGateway 1 0 0 0 Empty Cell EventBasedGateway 2 2
    2 2 Empty Cell ExclusiveGateway 3 3 3 2 Empty Cell InclusiveGateway 2 2 2 2 Empty
    Cell MixedGatewayCombinations 4 4 4 4 Empty Cell ParallelGateway 2 1 1 1 Events
    12 56 19 31 36 Empty Cell CancelEvent 1 1 1 0 Empty Cell CompensationEvent 6 2
    6 5 Empty Cell ConditionalEvent 5 0 0 0 Empty Cell ErrorEvent 4 4 4 4 Empty Cell
    EscalationEvent 7 0 7 7 Empty Cell EventDefinitionRef 4 0 0 0 Empty Cell LinkEvent
    1 0 1 1 Empty Cell MessageEvent 3 0 0 0 Empty Cell MultipleEvents 6 2 0 4 Empty
    Cell SignalEvent 9 5 5 9 Empty Cell TerminateEvent 1 1 1 1 Empty Cell TimerEvent
    9 4 6 5 Data 2 2 2 2 2 Empty Cell DataObject 1 1 1 1 Empty Cell Property 1 1 1
    1 4.2.2. Workflow control-flow patterns Standard conformance of all language features
    is necessary to execute arbitrary standard-conformant processes on a given engine.
    Any process that can be expressed in the standard, should be executable on an
    engine. Closely related to this aspect is the expressive power of the language
    dialect supported by an engine. The concept of “expressiveness” was adapted to
    discuss and compare workflow languages and their implementations by Kiepusewski  [32].
    Expressive power in this sense does not refer to a formal notion of expressiveness
    but describes the ease of expressing frequently needed structures in a system  [32].
    This definition of expressive power is common in the workflow management community,
    and it is frequently used in the form of workflow patterns that capture structures
    which should be easily expressible  [12]. Although the relevance of the concrete
    patterns is not undisputed  [9], they are applied often in related studies. Additionally,
    using workflow patterns for assessing expressive power eases the comparison of
    this work to others. In this paper, we use the original 20 workflow control-flow
    patterns from  [12], since these are most widely known, and no extensions or derivations
    thereof. We built upon the pattern-based analysis for BPMN 1.0 presented in  [33].
    Most of the pattern implementations described in the paper can directly be applied
    to BPMN 2.0. In the rare cases, where a modification of a pattern implementation
    was necessary, we followed the rationale of Wohed et al.  [33] to provide a solution.
    Table 2 lists the patterns sorted by pattern category, along with the highest
    degree of pattern support that can be achieved for BPMN 2.0. For the sake of brevity,
    we do not describe every pattern here, but refer the interested reader to  [12],
    [33]. Pattern support is rated in a trivalent rating of (direct support), , (partial
    support), or (no direct support). Again, we follow Wohed et al.  [33] in the judgment
    of the degree of support that is possible in BPMN. It can be seen in Table 2 that
    two patterns (MI without A Priori Run-Time Knowledge and Milestone) cannot be
    directly implemented in BPMN. We were unable to find workarounds based on the
    extended vocabulary of BPMN 2.0 that could compensate for this. Thus, we exclude
    these patterns from further discussion. For the remaining patterns, we implemented
    at least one test case, according to the structures from  [33], that led to pattern
    support. In case at least one engine failed the initially built test, we implemented
    an additional test that led to the same or a reduced support rating, to see if
    the engine supports an alternate structure. The reasoning for this is that an
    engine is considered to support a pattern if it implements at least one solution
    that grants support. Full support of all possible, equivalent solutions is not
    required. Finally, if an engine supports none of the solutions presented by  [33]
    (fails all related tests), we consider it as not supporting a pattern. Table 2.
    Workflow control-flow pattern support. Control-flow pattern BPMN 2.0 Activiti
    camunda BPM jBPM Basic control-flow patterns WCP-1 Sequence WCP-2 Parallel split
    WCP-3 Synchronization WCP-4 Exclusive choice WCP-5 Simple merge  Advanced branching
    and synchronization patterns WCP-6 Multi-choice WCP-7 Structured synchronizing
    merge WCP-8 Multi merge WCP-9 Structured discriminator  Structural patterns WCP-10
    Arbitrary cycles WCP-11 Implicit termination  Multiple Instances (MI) patterns
    WCP-12 MI without synchronization WCP-13 MI with a priori design-time know. WCP-14
    MI with a priori run-time know. WCP-15 MI without a priori run-time know.  State-based
    patterns WCP-16 Deferred choice WCP-17 Interleaved parallel routing WCP-18 Milestone  Cancellation
    patterns WCP-19 Cancel activity WCP-20 Cancel case 4.2.3. Static analysis rules
    The BPMN metamodel is vast and complex, and it defines more than 600 different
    constraints which have to be respected by valid processes  [15], [34]. In  [34],
    a detailed categorization and description of those constraints is provided. 330
    (or 54%) of the constraints are already covered by the normative XSDs provided
    by the standardization organization11   [15]. However, as shown in  [15], most
    of the so-called extended (EXT) constraints and the correctness of references
    between BPMN elements is not checked by a mere XML schema validation. For these
    constraints, more sophisticated checks have to be implemented to be able to detect
    violations. The last test suite is used to test the static analysis capabilities
    of the engines to detect and reject invalid BPMN processes. To achieve this, 301
    incorrect BPMN process models have been implemented. 17 processes violate reference
    requirements. For example, the specification states that a Message element has
    to be referenced by the messageRef attribute of a MessageEvent   [5]. If another
    element type, e.g., an Escalation definition is referenced, this is a violation
    of the specification. Moreover, we provide testable process files for 143 EXT
    constraints. As for some of these constraints different configurations are possible,
    284 processes are used to check the constraint violations. In contrast to the
    other test suites, we do not expect a valid runtime behavior upon execution and,
    therefore, we do not evaluate execution traces. Instead, we require the engines
    to be able to detect the invalid process. This implies that we do not provide
    test specific configurations, but simply try to deploy the process and reuse the
    same single test assertion, which states that deployment should fail. Therefore,
    we consider an engine to be able to correctly identify and reject violations of
    a single constraint, if all processes violating this requirement are rejected
    during deployment. 4.3. Benchmark results We obtained the results by executing
    the benchmark setup (described in Section  4.1) for the three engines (selected
    in Section  3) using the test suites (described in Section  4.2). This section
    discusses the results for the test suite addressing native BPMN support, followed
    by the discussion of workflow control-flow pattern support, and finally presenting
    insights in the ability of the engines to detect processes violating the constraints
    stated in the BPMN specification. 4.3.1. Language constructs The results of the
    evaluation of native BPMN support are shown in Table 1. Activiti supports 48 out
    of the 105 features, jBPM supports 56, and camunda BPM provides the highest amount
    of support with 60 features. Translated to percentage values, support ranges from
    46% up to 57%, being approximately half of the tested features. It is interesting
    that all three engines support roughly the same amount of features, differing
    by at most twelve features. Activiti and camunda BPM differ in their support by
    twelve features. Previously, camunda BPM supported all the features that activiti
    supported  [11], but this is no longer the case. There are no differences in all
    categories, except for the events category: Activiti successfully passes two feature
    tests related to MultipleEvents, which are failed by camunda BPM. However, taking
    all other differences into account, camunda BPM is better in its support for events
    as it provides higher support for Compensation-, Escalation-, Link-, and TimerEvents.
    The third engine, jBPM, shows strength with its support for five additional event
    feature tests for Multiple- and SignalEvents, compared to camunda BPM. Nevertheless,
    it is only ranked second in the number of passed feature tests, as it supports
    fewer features than camunda BPM in the basics, activities, and gateways category.
    Looking in more detail at the supported language constructs by group presented
    in Fig. 2, we can see that the data group is supported completely. Within the
    gateways group, all three engines support EventBased-, Exclusive-, Inclusive-,
    and MixedGateway combinations. However, all fail to support ComplexGateways. ParallelGateways
    are only partially supported, since none of the engines actually supports concurrency,
    but only pseudo-parallelism. The language constructs of the basics group, namely,
    Lanes, Participants and SequenceFlows are supported by every engine. Conditional
    SequenceFlows are supported partially by activiti and camunda BPM, whereas jBPM
    provides no support for this construct. Regarding the activities group, three
    features, namely, ReceiveTask, SendTask, and AdHocSubProcess, are unsupported.
    Nevertheless, every engine supports SubProcesses and Transactions. Regarding the
    LoopTask, only one out of six different tests is supported by every engine. In
    contrast, support for MultiInstanceTasks is relatively high, with five out of
    eight successful tests by activiti and camunda BPM, but it is completely unsupported
    by jBPM. The customization of consumed and produced tokens is barely supported
    as only activiti and camunda BPM pass one out of four tests. Within the events
    group MessageEvents, ConditionalEvents, and EventDefinitionRefs are unsupported
    by every engine, but widespread support can be diagnosed for Error- and TerminateEvents.
    Both, camunda BPM and jBPM support all EscalationEvents, which are unsupported
    by activiti. Furthermore, jBPM supports all SignalEvents, which are supported
    with five successful tests out of nine by activiti and camunda BPM, respectively.
    Approximately half of the tests for TimerEvents are passed by the engines. Failures
    of TimerEvents in jBPM and activiti are mainly caused by using timers for start
    events. CompensationEvents are fully supported by camunda BPM, and almost fully
    supported by jBPM, whereas in contrast activiti only supports two out of the six
    different tests. CancelEvents are unsupported by jBPM only, and LinkEvents are
    unsupported by activiti alone. Regarding MultipleEvents, jBPM supports two thirds,
    activiti one third, and camunda BPM none. Download : Download high-res image (125KB)
    Download : Download full-size image Fig. 2. Group support per engine in percentages.
    Looking at the number of language features that are not supported, as shown in
    Fig. 3, we can distinguish between two different cases: rejection at deployment
    and failure at runtime. Activiti rejects 23, camunda BPM 18, and jBPM 23 valid
    language features at deployment. Of the unsupported language features, activiti
    accepts 34, camunda BPM 27, and jBPM 26 language features at deployment, but at
    runtime the corresponding tests reveal incorrect execution semantics. This is
    a real issue for users of the engines, as they might encounter unexpected behavior
    in their own deployed processes, which is even worse in case it remains undetected.
    Download : Download high-res image (97KB) Download : Download full-size image
    Fig. 3. Number of supported, failing and undeployable features per engine. 4.3.2.
    Workflow control-flow patterns The support of workflow control-flow patterns is
    shown in Table 2. Overall, every engine supports at least 13 out of the 20 original
    workflow control-flow patterns. However, as mentioned before, the patterns WCP-15
    and WCP-18 are excluded from the analysis, since they cannot be directly supported
    in BPMN to begin with. Eleven patterns are supported as expected by all three
    engines. The three patterns WCP-9, WCP-12 and WCP-17, for which at least partial
    support is possible in BPMN, are unsupported by all engines under test. For four
    patterns, the results vary from engine to engine, but they are always supported
    by two engines. WCP-13, WCP-14, and WCP-19 are directly supported by activiti
    and camunda BPM, but not supported by jBPM. This reveals a major weakness of jBPM
    caused by missing supported for needed language constructs. The pattern WCP-10
    is correctly executed on jBPM and activiti, but not on camunda BPM. This is also
    the only difference of the pattern support between camunda BPM and activiti. Looking
    at the pattern groups, it is obvious that all basic control-flow patterns are
    supported. Both, structural and cancellation patterns, are also supported on almost
    every engine, except for one failure in each of the groups. Within the group of
    advanced branching and synchronization patterns, only one pattern is completely
    unsupported, but the remaining three are supported at the best possible level
    by every engine. Of the state-based patterns only WCP-16 is supported by all engines.
    WCP-17, which could be supported partially, is not supported by any of the engines
    under test and WCP-18 has not been tested as already the BPMN 2.0 specification
    does not allow for any valid pattern implementation. The remaining group of multiple
    instances patterns is least supported, as WCP-12 is completely unsupported by
    all engines and both WCP-13 and WCP-14 are unsupported by jBPM. When looking at
    the reasons for failures in pattern support, the most common issue is that the
    BPMN process containing the pattern is not deployable, because a construct used
    therein (such as an AdHocSubProcess) is not supported by an engine. Because of
    this, jBPM supports none of the multiple instances patterns, due to missing support
    for the MultiInstanceTask, and the Cancel Activity pattern is unsupported, due
    to missing support for the CancelEvent. The Arbitrary Cycles pattern could be
    expected to be supported by camunda BPM, as all used language features are supported,
    but the test fails with a runtime exception caused by the composition of these
    language features. To sum up, pattern support of the three engines can be considered
    high and balanced, as it is ranging between 13 up to 15 out of 18 patterns. It
    is interesting that activiti supports 15 out of the 18 patterns with only 48 out
    of 105 language features. Hence, only a moderate degree of support for language
    features is required to implement a large set of the patterns. Despite supporting
    56 out of the 105 language features, the engine jBPM supports only 13 patterns.
    The lack of pattern support by jBPM is mainly caused by rejecting processes containing
    the pattern already at deployment time. Out of the ten failed tests, nine pattern
    tests are being rejected during deployment. For activiti and camunda, only one
    out of six and three out of seven tests are rejected upon deployment. 4.3.3. Static
    analysis rules As stated before, BPMN engines should be able to detect the errors
    in the 301 test files of the BPMN constraint violation test suite. Fig. 4 shows
    both, the absolute numbers of rejected and non-rejected processes. Download :
    Download high-res image (80KB) Download : Download full-size image Fig. 4. Number
    of (not) rejected invalid processes per engine. Of the most recent engine versions,
    jBPM, with 236 of 301 (or approx. 78.4%) detected errors and rejected process
    files is far more capable of detecting invalid processes than the other two tested
    engines. Activiti rejects only 49.8% (150 of 301) and camunda BPM only 46.8% (141
    of 301) of the tested invalid processes. An analysis of the log files created
    by the engines during the test runs reveals three different reasons why an invalid
    BPMN process is rejected during deployment: First, each engine has to parse the
    process defined in the normative XML notation. During this parsing, all engines
    are performing an XML schema validation using the normative XML Schema Definitions
    (XSDs). As 13 test processes contain XML schema violations, those invalid processes
    are directly detected by the XSD validation mechanism of the engines. Interestingly,
    jBPM performs a schema validation, but in some cases, if the more sophisticated
    checks do not detect an error, the process is deployed despite of the (detected)
    schema errors. Second, dedicated checking mechanisms detect invalid processes.
    After the initial schema validation, the process is transformed into an internal
    representation for further processing and execution. During this transformation,
    the engines can check whether all mandatory information which will be required
    during execution is present, and whether all information has the expected format.
    For instance, in the tests for the constraints EXT.132-13412 the timer definition
    which should follow the ISO-8601 date format is invalid. Of the three engines
    under test, only jBPM is able to detect the violation at deploy time and rejects
    the process accordingly. Third, the engines reject processes either due to an
    exception during the deployment phase or due to stricter requirements. In contrast
    to the implemented checks, some invalid processes simply crash the deployment
    operation. This happens for example if a feature used in an erroneous process
    is not supported, or if an invalid value causes uncaught exceptions. Moreover,
    especially jBPM has some stricter requirements for process deployment. For instance,
    an invalid process might not get rejected because of its invalidity, but because
    jBPM imposes requirements which are not covered by the specification (e.g., a
    process must have a StartEvent, which is not necessary for initiating EventBasedGateways).
    Although this behavior is debatable, it is still preferable to a situation in
    which an invalid process is deployed, as runtime exceptions or unpredictable behavior
    can be considered as more serious  [35]. Overall, it can be concluded that jBPM
    has the best coverage of static analysis rules, detecting the highest amount of
    incorrect process models at deploy time. Activiti and camunda BPM do not perform
    equally strict checks during deployment and, therefore, are not able to detect
    as many invalid processes. However, relying on correctly modeled processes for
    enactment is problematic, as most modeling tools do not enforce the correctness
    of the created models  [15]. Our test suite comprises rather simple models with
    only a few modeled elements and control-flow branches. It is to be expected that
    real-world process models violating the BPMN specification contain more elements
    and branches, which may leave problematic errors undetected for a long time, resulting
    in runtime failures in production use. 5. Evolution of BPMN support As seen in
    the previous section, there are substantial limitations in the current implementation
    of the BPMN standard. These limitations could be attributed to the fact that implementation
    is still in progress and support of the standard will increase with time. An alternative
    interpretation is that implementations are limited to the features of the standard
    that are relevant in practice and the remaining part of the standard will likely
    never be implemented. The latter interpretation is supported by studies that show
    that the features used in process models by practitioners are limited to a modest
    part of the standard  [10], [36]. Here, we try to address this aspect from the
    direction of process engines. If, on the one hand, the feature set supported by
    engines constantly increases, this is an indication that the implementation of
    the standard is still in progress. On the other hand, if the feature set supported
    by engines stays rather constant over time, this is an indication that the implementation
    of the standard has stopped in practice and language features that have not been
    supported by now are considered as irrelevant. To address this aspect, we investigate
    the evolution of BPMN support over time in this section. We benchmark different
    versions of the engines discussed in Section  4 that have been published over
    a period of more than three years, using the exact same methodology as before.
    An overview of the engine versions we consider is given in Section  5.1. The results
    of the benchmarks, describing the evolution of BPMN language feature and workflow
    control-flow pattern support, are shown in Section  5.2 and Section  5.3. Section  5.4
    describes the changes over time for the third test suite regarding static analysis
    checks, and Section  5.5 sums up the results. 5.1. Compared engine versions Section  4
    details the benchmarks of the latest versions of the engines activiti, jBPM, and
    camunda BPM. Here, we add benchmarks for at least four additional prior versions
    of each said engine. The concrete version numbers of the engines are shown in
    Fig. 5 along with their release date. For the current version, we have used June
    30th, 2016 as a reference date. The engine vendors apply semantic versioning for
    setting release numbers.13 Version numbers are encoded in the form MAJOR.MINOR.PATCH
    and numbers are incremented depending on the nature of the changes made in a revision.
    According to semantic versioning, an increment of the patch level should only
    reflect bug fixes, the minor level is incremented when adding functionality which
    is backwards compatible, and the major version is changed when introducing API-breaking
    changes. Within the last years, several minor or patch level updates have been
    released for all engines, but no major release has been made. Here, we focus on
    the last four minor version updates and always use the highest available patch
    level.14 Each of the releases are at least three months and on average six months
    apart. Therefore, we can expect to see signs of progress in BPMN feature implementation,
    given the implementation has not been concluded yet. Download : Download high-res
    image (150KB) Download : Download full-size image Fig. 5. Engine release cycles.
    Each engine has a different release strategy, as can be seen in the visualization
    of the release dates in Fig. 5. Whereas activiti has a relatively short release
    cycle, releasing bug fix releases between planned, minor releases, both jBPM and
    camunda BPM have longer, but more consistent, release cycles. As we only use the
    last five or six minor version releases, we do not cover the same amount of time
    for each engine. This is not a problem, since we are interested in increasing
    feature support between releases, and not within a fixed period of time. 5.2.
    Evolution of language feature support Since page space is limited, we cannot present
    the complete benchmark results for all engine versions. Instead, an overview of
    the changes in native BPMN support is shown in Table 3. To reveal these changes,
    we applied a form of regression testing, which is defined as the “retesting of
    a system or component to verify that modifications have not caused unintended
    effects and that the system or component still complies with its specified requirements”  [37].
    For that, we define feature regressions as features that were supported in an
    earlier version and stopped working in subsequent version. Some regressions were
    fixed with subsequent releases. Other features which were supported in some version
    and stopped working for all subsequent versions are explicitly stated, as unfixed
    feature regressions. Table 3. Evolution of native BPMN support. Feature Change
    Engine 4 events Supported since camunda BPM 7.1.0 2 events Supported since camunda
    BPM 7.3.0 10 events Supported since camunda BPM 7.4.0 3 events Unsupported since
    camunda BPM 7.4.0 1 gateway and 5 events Supported since jBPM 6.1.0 First of all,
    it becomes obvious that there is little change in the support of BPMN features.
    Feature support for all three engines increased by the number of merely 19 additional
    features during the past years. Especially when considering the immense number
    of potential features and the moderate degree to which engines implement the standard
    so far, as described in Section  4, this number is very low. Regarding activiti,
    we do not see any evolution of native BPMN support. In the groups of activities,
    basics, and data we cannot diagnose any changes at all. There are minor improvements
    in the gateways and events group, but the changes are minimal. For instance, one
    test shifts from being undeployable to a partial support of some variants of a
    language feature, which does not justify to be judged as an increase in feature
    support. In contrast to activiti, there are more differences between the tested
    versions of camunda BPM. However, the support for the basics, data, and gateway
    groups stays the same for all versions. With the change from camunda BPM 7.0.0
    to 7.1.0, four new features within the events group are supported, and with version
    7.3.0, camunda BPM gains support for two additional features in the same group.
    The biggest increase is caused by moving to version 7.4.0, which supports ten
    additional event feature tests, but three previously supported event tests are
    failing due to runtime exceptions. A negligible change, similar to the changes
    for activiti, can be found in the activities group, between camunda BPM 7.0.0
    and 7.1.0. Overall, 13 additional feature tests for BPMN events are supported
    in the latest version in contrast to the earliest version we have tested. The
    main difference is the newly added support for EscalationEvents in camunda BPM
    7.4.0 with seven passing feature tests and the improvement of the implementation
    of CompensationEvents which was only rudimentary in early camunda BPM versions.
    Looking at jBPM, we can see that all the versions exhibit the same degree of support
    for activities, basics, and the data group. Upgrading jBPM 6.0.1–6.1.0 brought
    support for six new features, namely five new events and one gateway. Upgrading
    from jBPM 6.1.0 to 6.2.0, however, results in five regression errors as this time
    four events and one gateway option fails. With jBPM 6.3.0, these failures are
    fixed again. To sum up, over the time of the last five minor versions, jBPM introduced
    six new features in total. 5.3. Evolution of workflow control-flow patterns support
    An overview of the evolution of workflow control-flow pattern support is shown
    in Table 4. As it can be seen, there are even fewer changes and these are not
    limited to an increase in feature support, but instead also include a decrease.
    Table 4. Evolution of pattern support. Pattern Change Engine WCP06 Supported since
    jBPM 6.1.0 WCP10 Supported until camunda BPM 7.1.0 For activiti, we have no visible
    improvements in pattern support over the last six minor releases. This can be
    expected, since there is also no increase in native language features. The only
    change is that processes which contain complex gateways are no longer marked as
    not deployable, affecting the variants of WCP06 and WCP09 that make use of complex
    gateways. This change happens between the versions 5.16.3 and 5.17.0. However,
    this has no effect on pattern support, as said pattern variants do not work as
    expected at runtime anyway. Regarding jBPM, we can see a single improvement in
    the variant of WCP06 that is implemented with an inclusive gateway. This is unsupported
    in jBPM 6.0.1 but supported in every subsequent version we have tested. Looking
    at camunda BPM, the support for workflow patterns even declines. While WCP10 is
    still supported in the versions 7.0.0 and 7.1.0, all three subsequent versions
    lack support of this pattern. 5.4. Static analysis rules evolution In analogy
    to the other test suites, we also compared the static analysis rule implementations
    for all different releases of the engines under test. Fig. 6 gives an overview
    of the evolution from release to release. Download : Download high-res image (109KB)
    Download : Download full-size image Fig. 6. Evolution of the static analysis capabilities.
    As can be seen, the detection rate of camunda BPM is rather stable. Only the first
    release 7.0.0 can detect noticeably fewer violations (125 invalid processes),
    whereas the other releases are able to reveal 139 resp. 141 invalid processes,
    resulting in a detection rate of approx. 46.8%. However, looking at the details
    reveals that there are some regressions and newly passed checks from version to
    version. For instance, a process omitting a mandatory DataOutput element is correctly
    rejected by camunda BPM 7.1.0, but all further versions are not rejecting this
    process. In total there are three regressions of checks that have been passed
    in earlier versions. Activiti starts at an even lower level than camunda BPM,
    with only 118 detected constraint violations in version 5.15.1. Although the number
    of detected flaws increases (to 127 resp. 126) in the next two releases, activiti
    is still falling behind camunda BPM. However, the 5.18.0 release increases the
    ability to detect constraint violations by nearly 20% to 151 detected invalid
    processes. This maximum slightly decreases to 150 detections, i.e., 49.8% in the
    two most recent activiti versions, as there are two regressions regarding the
    incorrect usage of a ComplexGateway and a SequenceFlow. From the first to the
    last tested engine version, jBPM generally has the best detection rate in comparison
    to the other two tested engines. The versions 6.0.1–6.3.0 are all capable of finding
    185–190 invalid process definitions. Interestingly the number is decreasing slightly
    from version to version. Since version 6.1.0 two regressions have been introduced,
    and in 6.3.0 four additional tests are failing. However, since 6.3.0, an invalid
    DateTime format is recognized. With the last release most regressions have been
    fixed and, moreover, the general static analysis ability has been massively increased,
    as can be seen in an increased detection rate from 61.7% (185 of 301) to 78.7%
    (236 of 301 correct detections). 5.5. Summary Overall, it seems that the evolution
    of feature support in BPMN engines has stopped. Improvements to feature support
    are confined to BPMN events, as shown by increases for camunda BPM and jBPM. Fig.
    7 visualizes this development. It can be seen that feature support is hardly rising
    except for camunda BPM which shows modest progress. Interestingly, activiti, which
    supports the least number of BPMN events of the three engines, has not improved
    in that area within the last six releases. Download : Download high-res image
    (139KB) Download : Download full-size image Fig. 7. Evolution of native BPMN support
    for events. This halt in feature improvements of these three major BPMN engines
    can be interpreted as a sign that the implementation of the BPMN 2.0 standard
    has concluded at the current level. It seems that neither the implementers of
    these engines view the remaining features of the standard as valuable for their
    implementations, nor are they pressured by their customer base to implement further
    features. Otherwise, we would see a stronger increase in new features. Since all
    three engines seem to be successful in the market, we can conclude that the feature
    set they provide is sufficient for BPM systems in practice. In contrast to the
    stagnation at the feature level, the improvement of the detection rate of invalid
    processes can be seen as a sign that the engine vendors are currently putting
    more effort in stabilization and quality management. Especially the approach of
    jBPM seems to be reasonable: A high rejection rate of invalid process, and also
    the highest rejection rate of valid processes using unsupported features (cf.
    Section  4.3.1) are indications that jBPM is strict at deployment time to ensure
    that only valid processes with supported features are actually executed on the
    engine. However, there is still room for improvement as for the current jBPM version
    still 31 deployable feature tests are failing at runtime and 65 invalid processes
    are not rejected. For camunda BPM and activiti the situation is different: Both
    engines are rather weak in detecting BPMN constraint violations. Also, correct
    processes, which are not supported correctly, are not detected very well. Especially
    camunda BPM can be improved with more sophisticated deployment checks, as only
    46.8% of all invalid processes and only 18 of the 49 unsupported features are
    detected at deploy time. 6. Conclusion and future work In this paper, we have
    presented a comprehensive analysis of the current state and the evolution of the
    implementation of the BPMN 2.0 standard. The results of this analysis can be summarized
    as follows: The data presented in this paper show that there is hardly an increase
    in the features supported by BPMN 2.0 engines over a period of more than three
    years. This stable trend suggests that there will be no sudden burst in feature
    support and, thus, the implementation of BPMN 2.0 in practice has largely been
    concluded at the current level. Remaining features of the standard that are not
    yet supported seem to be irrelevant in practice. The first part of this study,
    presented in Section  3, was a comprehensive analysis of products that claim to
    implement the BPMN 2.0 standard. Of the 47 products that classify as BPMS or process
    engines, only three remain that are able to consume the required serialization
    format and permit a closer automated evaluation. In Section  4, we performed such
    an evaluation of these engines. We analyzed conformance to the BPMN standard and,
    on top of that, support for common workflow control-flow patterns. Moreover, we
    also checked the ability of the engines to detect violations of the static analysis
    rules defined in the specification  [5]. Following this analysis of the most recent
    versions, we analyzed the same aspects for up to five additional prior versions
    of the engines that have been published in the last years. Results are exhibited
    in Section  5. We could show that only in the events category new features have
    been added over this time by some engines. Although, even the best engine supports
    only 57% of all tested features for language constructs, support for the basic
    workflow control-flow patterns is rather good. Hence, we can conclude that for
    both, the implementers and the users, the currently implemented features are sufficient
    to express the most common needs. The investigation of static analysis checks
    applied during deployment showed that the engine developers use different approaches
    to deal with invalid processes at deploy time. Only one engine imposes rather
    strict requirements and has a high rejection rate during deployment. This is the
    preferable approach as the detection of invalid processes at deployment prevents
    problems at runtime. The results do have implications for process engine vendors
    and users. Users should not expect that features that are currently missing will
    be added soon, with the notable exception of additional event types. Moreover,
    as a lot of errors within process models are not detected by the engines, users
    should always test their process models to prevent costly runtime errors in production.
    In addition, users should test the update of an engine, because of potential feature
    regressions. In turn, vendors should use test suites like the ones described in
    this paper to prevent feature regressions, and improve their overall product,
    e.g., through better detection of errors in process models or through a higher
    degree of feature support. To facilitate the adoption of our results in practice,
    we published the data underlying this and previous work in an interactive dashboard.15
    Multiple directions of future work follow from this. Firstly, it would be desirable
    to investigate further BPMN engines. We hope to obtain access to further products,
    but this depends on the availability of academic licenses. Secondly, we work on
    extending the coverage of our test suite. Currently, this test suite does not
    cover all aspects of the standard and a higher level of coverage would be desirable.
    With respect to the results presented in this paper, it can be expected that a
    higher level of feature coverage leads to a diagnosis of even more insufficiencies
    in contemporary implementations. The refinement of the standard can be seen as
    a third direction. Currently, the BPMN standard is quite extensive and studies
    that analyze its usage in practice, such as this one, consistently show that many
    of the features are simply not needed. A reduction of the feature set of BPMN
    to a core subset could help in many ways, e.g., by easing the implementation of
    the standard and reducing the complexity for its users. Similar approaches exist
    for related standards  [38] and could be valuable for BPMN as well. Fourthly,
    more work is needed that takes process models developed and deployed by practitioners
    into account. As indicated in related work  [10], [36], it can be expected that
    realistic process models make use of a limited feature set as well, which would
    support the central conclusion of this paper. Finally, based on this research,
    we aim to develop a generic process engine benchmarking framework which can be
    used to benchmark a variety of important process engine capabilities, so that
    end-users can compare the available alternatives and choose the best-fitting engine
    for their purpose. Acknowledgments We would like to thank the students of the
    University of Bamberg who contributed to the benchmarking system used in this
    paper during software development projects. Thanks go to Adrian Bazyli, Annalena
    Bentele, Christian Kremitzl, Lea-Louisa Maaß, Frederik Müller and Severin Sobetzko.
    References [1] W.M.P. van der Aalst Business process management: A comprehensive
    survey ISRN Softw. Eng. (2013) Google Scholar [2] H. Mili, G. Tremblay, G.B. Jaoude,
    E. Lefebvre, L. Elabed, G.E. Boussaidi Business process modeling languages: Sorting
    through the alphabet soup ACM Comput. Surv., 43 (1) (2010), pp. 4:1-4:56 Google
    Scholar [3] Business Process Model and Notation (BPMN) Version 2.0, OMG (Object
    Management Group), v2.0, 2011. Google Scholar [4] M. Chinosi, A. Trombetta BPMN:
    An introduction to the standard Comput. Stand. Interfaces, 34 (1) (2012), pp.
    124-134 View PDFView articleView in ScopusGoogle Scholar [5] ISO/IEC, ISO/IEC
    19510:2013–Information technology - Object Management Group Business Process Model
    and Notation, v2.0.2, 2013. Google Scholar [6] B. Hofreiter, C. Huemer A model-driven
    top-down approach to inter-organizational systems: From global choreography models
    to executable BPEL 2008 10th IEEE Conference on E-Commerce Technology and the
    Fifth IEEE Conference on Enterprise Computing, E-Commerce and E-Services, IEEE
    (2008), pp. 136-145 CrossRefView in ScopusGoogle Scholar [7] I. Weber, J. Haller,
    J.A. Mulle Automated Derivation of Executable Business Processes from Choreographies
    in Virtual Organisations Int. J. Bus. Process Integr. Manag., 3 (2) (2008), pp.
    85-95 CrossRefView in ScopusGoogle Scholar [8] C. Ouyang, M. Dumas, Wil M.P. van
    der Aalst, Arthur H.M. ter Hofstede, J. Mendling From business process models
    to process-oriented software systems ACM Trans. Softw. Eng. Methodol., 19 (2)
    (2009), pp. 1-37 CrossRefGoogle Scholar [9] E. Börger Approaches to modeling business
    processes. A critical analysis of BPMN, workflow patterns and YAWL Softw. Syst.
    Model., 11 (3) (2012), pp. 305-318 CrossRefView in ScopusGoogle Scholar [10] M.
    zur Muehlen, J. Recker How much language is enough? Theoretical and practical
    use of the business process modeling notation Advanced Information Systems Engineering,
    20th International Conference, CAiSE 2008, Montpellier, France, June 16–20, 2008,
    Proceedings, Springer, Berlin, Heidelberg (2008), pp. 465-479 View in ScopusGoogle
    Scholar [11] M. Geiger, S. Harrer, J. Lenhard, M. Casar, A. Vorndran, G. Wirtz
    BPMN conformance in open source engines 9th IEEE International Symposium on Service-Oriented
    System Engineering, SOSE, IEEE, San Francisco Bay, CA, USA (2015), pp. 21-30 CrossRefView
    in ScopusGoogle Scholar [12] W. van~der Aalst, A. ter Hofstede, B. Kiepuszewski,
    A. Barros Workflow patterns Distrib. Parallel Databases, 14 (1) (2003), pp. 5-51
    View in ScopusGoogle Scholar [13] M. Geiger, S. Harrer, J. Lenhard, G. Wirtz,
    On the Evolution of BPMN 2.0 Support and Implementation, in: 10th IEEE International
    Symposium on Service-Oriented System Engineering (SOSE), Oxford, UK, 2016, pp.
    120–128. Google Scholar [14] G. Aagesen, J. Krogstie BPMN 2.0 for modeling business
    processes J. vom Brocke, M. Rosemann (Eds.), Handbook on Business Process Management
    1, International Handbooks on Information Systems, 978-3-642-45099-0, Springer
    (2015), pp. 219-250 CrossRefView in ScopusGoogle Scholar [15] M. Geiger, G. Wirtz
    BPMN 2.0 Serialization - Standard Compliance Issues and Evaluation of Modeling
    Tools 5th International Workshop on Enterprise Modelling and Information Systems
    Architectures, St. Gallen, Switzerland (2013), pp. 177-190 View in ScopusGoogle
    Scholar [16] F. Evéquoz, C. Sterren Waiting for the Miracle–Comparative Analysis
    of Twelve Business Process Management Systems Regarding the Support of BPMN 2.0
    Palette and Export, Tech. Rep. IIG-TR 2011.03 University of Applied Sciences Western
    Switzerland (2011) Google Scholar [17] M. Kurz BPMN model interchange: The quest
    for interoperability Proceedings of the 8th International Conference on Subject-oriented
    Business Process Management, S-BPM’16, ACM (2016), pp. 6:1-6:10 Google Scholar
    [18] R.M. Dijkman, M. Dumas, C. Ouyang Semantics and analysis of business process
    models in BPMN Inf. Softw. Technol., 50 (12) (2008), pp. 1281-1294 View PDFView
    articleView in ScopusGoogle Scholar [19] M. Chinosi, A. Trombetta Modeling and
    validating BPMN diagrams 2009 IEEE Conference on Commerce and Enterprise Computing,
    IEEE (2009), pp. 353-360 CrossRefView in ScopusGoogle Scholar [20] P.Y. Wong,
    J. Gibbons A process semantics for BPMN Formal Methods and Software Engineering,
    Lecture Notes in Computer Science, vol. 5256, Springer (2008), pp. 355-374 CrossRefView
    in ScopusGoogle Scholar [21] C. Gutschier, R. Hoch, H. Kaindl, R. Popp, A pitfall
    with BPMN execution, in: Second International Conference on Building and Exploring
    Web Based Environments, Chamonix, France, 2014, pp. 7–13. Google Scholar [22]
    F. Kossak, C. Illibauer, V. Geist, J. Kubovy, C. Natschläger, T. Ziebermayr, T.
    Kopetzky, B. Freudenthaler, K.-D. Schewe A Rigorous Semantics for BPMN 2.0 Process
    Diagrams Springer (2014) Google Scholar [23] P.V. Gorp, R. Dijkman A Visual Token-based
    Formalization of BPMN 2.0 Based on In-place Transformations Inf. Softw. Technol.,
    55 (2) (2013), pp. 365-394 Google Scholar [24] M. Dirndorfer, H. Fischer, S. Sneed
    Case study on the interoperability of business process management software Communications
    in Computer and Information Science, Springer (2013), pp. 229-234 CrossRefView
    in ScopusGoogle Scholar [25] V. Ferme, A. Ivanchikj, C. Pautasso A framework for
    benchmarking BPMN 2.0 workflow management systems 13th International Conference
    on Business Process Management, (BPM 2015), Springer, Innsbruck, Austria (2015),
    pp. 251-259 CrossRefView in ScopusGoogle Scholar [26] K. Kluza, K. Kaczor, G.J.
    Nalepa, M. Ślażyński, Opportunities for Business Process Semantization in Open-source
    Process Execution Environments, in: Proceedings of the 2015 Federated Conference
    on Computer Science and Information Systems, 2015, pp. 1307–1314. Google Scholar
    [27] A. Delgado, D. Calegari, P. Milanese, R. Falcon, E. García A systematic approach
    for evaluating BPM systems: case studies on open source and proprietary tools
    Open Source Systems: Adoption and Impact, IFIP Advances in Information and Communication
    Technology, vol. 451, 978-3-319-17836-3, Springer (2015), pp. 81-90 CrossRefView
    in ScopusGoogle Scholar [28] Z. Merali Computational science: Error, why scientific
    programming does not compute Nature, 467 (7317) (2010), pp. 775-777 CrossRefView
    in ScopusGoogle Scholar [29] C. Boettiger An introduction to Docker for reproducible
    research ACM SIGOPS Oper. Syst. Rev., 49 (1) (2015), pp. 71-79 CrossRefView in
    ScopusGoogle Scholar [30] R. Chamberlain, J. Schommer Using Docker to Support
    Reproducible Research, Tech. Rep., Technical report Invenshure, LLC. figshare
    (2014) 1101910 Google Scholar [31] S. Harrer, J. Lenhard, G. Wirtz BPEL conformance
    in open source engines Proceedings of the 5th IEEE International Conference on
    Service-Oriented Computing and Applications, SOCA’12, Taipei, Taiwan, IEEE (2012),
    pp. 237-244 Google Scholar [32] B. Kiepusewski Expressiveness and suitability
    of languages for control flow modelling in workflows (Ph.D. thesis) Queensland
    University of Technology, Brisbane (2003) Google Scholar [33] P. Wohed, W.M.P.
    van~der Aalst, M. Dumas, A.H.M. ter Hofstede, N. Russell On the suitability of
    BPMN for business process modelling Lecture Notes in Computer Science, Springer
    (2006), pp. 161-176 CrossRefGoogle Scholar [34] M. Geiger BPMN 2.0 Process Model
    Serialization Constraints, Bamberger Beiträge zur Wirtschaftsinformatik und Angewandten
    Informatik, no. 92 Otto-Friedrich Universität Bamberg (2013) Google Scholar [35]
    V.R. Basili, B.T. Perricone Software errors and complexity: An empirical investigation
    Commun. ACM, 27 (1) (1984), pp. 42-52 View in ScopusGoogle Scholar [36] J. Lenhard,
    M. Geiger, G. Wirtz On the measurement of design-time adaptability for process-based
    systems 2015 IEEE Symposium on Service-Oriented System Engineering, IEEE (2015),
    pp. 1-10 CrossRefView in ScopusGoogle Scholar [37] IEEE, IEEE Standard Glossary
    of Software Engineering Terminology, IEEE Std 610.12-1990, 1990. Google Scholar
    [38] E. Højsgaard, T. Hallwyl Core BPEL: syntactic simplification of WS-BPEL 2.0
    Proceedings of the 27th Annual ACM Symposium on Applied Computing, ACM (2012),
    pp. 1984-1991 CrossRefView in ScopusGoogle Scholar Cited by (62) How to benefit
    more from business process documentation? Framework for calculation personality
    - process role fit 2022, Procedia Computer Science Show abstract A Methodological
    Approach towards Cyber Risk Management in Land Administrations Systems 2024, Land
    A Software Testing Workflow Analysis Tool Based on the ADCV Method 2023, Electronics
    (Switzerland) A secure remote health monitoring model for early disease diagnosis
    in cloud-based IoT environment 2023, Personal and Ubiquitous Computing Balancing
    Flexibility and Compliance in Response to Long-Tailed Business Process Changes
    2023, SSRN An elderly health monitoring system based on biological and behavioral
    indicators in internet of things 2023, Journal of Ambient Intelligence and Humanized
    Computing View all citing articles on Scopus Matthias Geiger is a Ph.D. student
    at the Distributed Systems Group of the University of Bamberg, Germany. The focus
    of this research is the analysis of quality aspects in the BPMN ecosystem. This
    comprises not only analyses of BPMN engines but also the investigation of standard
    compliance issues and other quality aspects in process models and modeling tools.
    Simon Harrer is a Ph.D. student at the Distributed Systems Group of the University
    of Bamberg. His research interests focus on benchmarking and evaluating software
    in general and workflow engines in particular. Apart from that, he also conducts
    research within the field of service-oriented computing and the cloud. Jörg Lenhard
    is a postdoctoral research fellow in the Software Engineering Research Group at
    the Department of Mathematics and Computer Science of Karlstad University, Sweden.
    He received his Ph.D. from the University of Bamberg, Germany. Jörg’s research
    interests include the measurement and comparison of software quality, service-oriented
    computing, and process-aware information systems. His work has been distinguished
    by several scientific awards. Guido Wirtz is a full professor of computer science,
    heads the Distributed Systems Group of the University of Bamberg, and is Vice
    President of Technology and Innovation of the University of Bamberg. He received
    his Ph.D. from the University of Bonn and his habilitation from the University
    of Siegen. His main research areas are service-oriented architectures, business-to-business-integration
    with QoS, multi-agent technology, and visual programming and design languages.
    1 For the remainder of the paper, we will refer to the ISO version of the standard  [5],
    instead of  [3]. 2 The work of the group, their intentions and preliminary results
    are presented at http://www.omgwiki.org/bpmn-miwg/doku.php. 3 See https://www.docker.io
    for details. 4 Available at http://www.bpmn.org/#tabs-implementers, the official
    BPMN website. 5 The list is available at https://en.wikipedia.org/wiki/List_of_BPMN_2.0_engines.
    6 The project web site is available at http://www.iaas.uni-stuttgart.de/forschung/projects/benchflow.php.
    7 Further information is available at the respective websites located at http://www.activiti.org/,
    http://www.camunda.org/, and http://www.jbpm.org/. 8 The system is located at
    https://github.com/uniba-dsg/betsy. 9 The Dockerfile is available at https://github.com/uniba-dsg/betsy-docker.
    10 To use the pre-built image, please execute the command docker pull simonharrer/betsy-docker
    on the command line. More information is available on the Docker Hub project at
    https://hub.docker.com/r/simonharrer/betsy-docker/. 11 The XSDs are available
    at http://www.omg.org/spec/BPMN/2.0.2/. 12 See the documentation at http://bpmnspector.org/ConstraintList_EXT.html#EXT132.
    13 See http://semver.org/ for details. 14 The only exception to this rule is activiti
    5.16.4 which contained a bug that severely hampered process deployment. Since
    the fix for this bug has not been backported to activiti 5.16.X, we used the previous
    patch level, namely, activiti 5.16.3. 15 Available at https://peace-project.github.io.
    View Abstract © 2017 Elsevier B.V. All rights reserved. Part of special issue
    Service-Oriented System Engineering Edited by Nik Bessis, Xiaojun Zhai, Stelios
    Sotiriadis View special issue Recommended articles NewWave: Workflow engine Science
    of Computer Programming, Volume 203, 2021, Article 102581 Sebastijan Kaplar, …,
    Stéphane Ducasse View PDF BPMN extensions for automating cloud environments using
    a two-layer orchestration approach Journal of Visual Languages & Computing, Volume
    47, 2018, pp. 31-43 Robert Dukaric, Matjaz B. Juric View PDF uBPMN: A BPMN extension
    for modeling ubiquitous business processes Information and Software Technology,
    Volume 74, 2016, pp. 55-68 Alaaeddine Yousfi, …, Anind K. Dey View PDF Show 3
    more articles Article Metrics Citations Citation Indexes: 60 Captures Readers:
    343 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: Future generation computer systems
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'BPMN 2.0: The state of support and implementation'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.jocs.2015.04.012
  analysis: '>'
  authors:
  - Meng Hu
  - Rupa Kommineni
  - Quan Pham
  - R. W. Gardner
  - Tanu Malik
  - Douglas Thain
  citation_count: 20
  full_citation: '>'
  full_text: '>

    403 Forbidden Code: AccessDenied Message: Access Denied RequestId: C54QA4ACXXFPHSBS
    HostId: q+PiKR8aPw6lD3e6ZoJOdG3F9CdnfzyWDh7gOamscuU2oTZH4M+rrQhbptYpdT921oLdPBQpLyw='
  inline_citation: '>'
  journal: Journal of computational science
  limitations: '>'
  pdf_link: http://manuscript.elsevier.com/S1877750315000502/pdf/S1877750315000502.pdf
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: An invariant framework for conducting reproducible computational science
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s11042-021-11681-7
  analysis: '>'
  authors:
  - Carlos Gómez-Huélamo
  - Javier Del Egido
  - Luis M. Bergasa
  - Rafael Barea
  - Elena López-Guillén
  - Felipe Arango
  - Javier Araluce
  - Joaquín López
  citation_count: 7
  full_citation: '>'
  full_text: ">\nMultimedia Tools and Applications (2022) 81:4213–4240\nVol.:(0123456789)\n\
    https://doi.org/10.1007/s11042-021-11681-7\n1 3\nTrain here, drive there: ROS\
    \ based end‑to‑end \nautonomous‑driving pipeline validation in CARLA simulator\
    \ \nusing the NHTSA typology\nCarlos Gómez‑Huélamo1  · Javier Del Egido1 · Luis M. Bergasa1 ·\
    \ Rafael Barea1 · \nElena López‑Guillén1 · Felipe Arango1 · Javier Araluce1 ·\
    \ Joaquín López2\nReceived: 1 February 2021 / Revised: 26 August 2021 / Accepted:\
    \ 13 October 2021 \n© The Author(s) 2021\nAbstract\nUrban complex scenarios are\
    \ the most challenging situations in the field of Autonomous \nDriving (AD). In\
    \ that sense, an AD pipeline should be tested in countless environments \nand\
    \ scenarios, escalating the cost and development time exponentially with a physical\
    \ \napproach. In this paper we present a validation of our fully-autonomous driving\
    \ architec-\nture using the NHTSA (National Highway Traffic Safety Administration)\
    \ protocol in the \nCARLA simulator, focusing on the analysis of our decision-making\
    \ module, based on \nHierarchical Interpreted Binary Petri Nets (HIBPN). First,\
    \ the paper states the importance \nof using hyper-realistic simulators, as a\
    \ preliminary help to real test, as well as an appropri-\nate design of the traffic\
    \ scenarios as the two current keys to build safe and robust AD tech-\nnology.\
    \ Second, our pipeline is introduced, which exploits the concepts of standard\
    \ com-\nmunication in robotics using the Robot Operating System (ROS) and the\
    \ Docker approach \nto provide the system with isolation, flexibility and portability,\
    \ describing the main mod-\nules and approaches to perform the navigation. Third,\
    \ the CARLA simulator is described, \noutlining the steps carried out to merge\
    \ our architecture with the simulator and the advan-\ntages to create ad-hoc driving\
    \ scenarios for use cases validation instead of just modular \nevaluation. Finally,\
    \ the architecture is validated using some challenging driving scenarios \nsuch\
    \ as Pedestrian Crossing, Stop, Adaptive Cruise Control (ACC) and Unexpected Pedes-\n\
    trian. Some qualitative (video files: Simul ation Use Cases) and quantitative\
    \ (linear velocity \nand trajectory splitted in the corresponding HIBPN states)\
    \ results are presented for each \nuse case, as well as an analysis of the temporal\
    \ graphs associated to the Vulnerable Road \nUsers (VRU) cases, validating our\
    \ architecture in simulation as a preliminary stage before \nimplementing it in\
    \ our real autonomous electric car.\nKeywords CARLA · Autonomous Driving · ROS ·\
    \ Simulation · Decision-Making · \nNHTSA typology\n * Carlos Gómez-Huélamo \n\
    \ \ncarlos.gomezh@edu.uah.es\nExtended author information available on the last\
    \ page of the article\nPublished online: 3 December 2021\n/\nMultimedia Tools\
    \ and Applications (2022) 81:4213–4240\n1 3\n1 Introduction\nAccording to the\
    \ World Health Organization, nearly one third of the world population will \n\
    live in cities by 2030, leading to an overpopulation in most of them. Aware of\
    \ this prob-\nlem, the European Commission published the Transport White Paper\
    \ in 2011 [21] indi-\ncating that new forms of mobility ought to be proposed to\
    \ provide sustainable solutions \nfor goods and people safety. In that sense,\
    \ regarding safety, it set the ambitious goal of \nhalving the overall number\
    \ of road deaths in the EU between 2010 and 2020. However, \nonly in 2014 more\
    \ than 25,700 people died on the EU roads, many of them caused by an \nimproper\
    \ behaviour of the driver on the road. On top of that, the National Highway Traf-\n\
    fic Safety Administration (NHTSA), US agency responsible of reducing vehicle-related\
    \ \ncrashes related to transportation safety, stated [59] that human beings are\
    \ involved in 94 % \nof road accidents, in which an inadequate velocity, driver\
    \ distraction or factors inherent to \nage, such as lack of sight and reflexes\
    \ or joint mobility.\nConsidering this, Autonomous Driving (AD) have held the\
    \ attention of technology \nenthusiasts and futurists for some time, evidenced\
    \ by the continuous research and develop-\nment in this field of study over the\
    \ past decades, being one of the emerging technologies of \nthe Fourth Industrial\
    \ Revolution, and particularly of the Industry 4.0. Despite the fact these \n\
    pipelines still present myriad scenarios with unsolved issues, the advent of Deep\
    \ Learning \n[36], supported by the breakthroughs in Computer Vision, accumulated\
    \ knowledge in vehi-\ncle dynamics and availability of new sensor modalities,\
    \ datasets and hyper-realistic simula-\ntors have catalyzed [69] AD research and\
    \ industrial implementation, representing one of \nthe most challenging engineering\
    \ tasks of our era. The launch of reliable and economi-\ncally affordable autonomous\
    \ vehicles will create a huge impact on society, affecting social \n(mitigating\
    \ traffic jams, transporting the mobility impaired), demographic (preventing traf-\n\
    fic accidents), environmental (reducing emissions) and economic (consuming Mobility\
    \ as a \nService (MaaS), reallocating of driving time, logistics revolution) aspects.\n\
    Despite all impressive efforts in the development of AD technology [13], fully-autonomous\
    \ \nnavigation in arbitrarily complex environments is still years away. The reason\
    \ for this is two-\nfold: Firstly, autonomous systems that are operated in complex\
    \ dynamic environments require \nintelligent systems that generalizes to unpredictable\
    \ situations in a timely manner. Secondly, \ninformed decision-making requires\
    \ accurate perception. Most of the existing perception sys-\ntems produce errors\
    \ at a rate not acceptable for autonomous driving [30].\nAutonomous Vehicles (AVs)\
    \ are expected to be driven throughout arbitrarily complex \nscenarios with a\
    \ reliability greater than human beings and full autonomy, performing driv-\n\
    ing decisions based on the offline and online information processed by the vehicle.\
    \ The \noffline information may be identified as the prior knowledge of the system,\
    \ like the vehicle \ndynamic, traffic rules based on behavioural decision-making\
    \ systems [41] or even the topo-\nlogical relations and geographic information\
    \ of the environment based on HDMaps (High-\nDefinition maps) [47, 40]. On the\
    \ other hand, the online information, also known as the traf-\nfic situation,\
    \ is obtained through the use of a global perception system of the vehicle, which\
    \ \ninvolves different on-board sensors as: Inertial Measurement Unit (IMU), Light\
    \ Detection \nAnd Ranging (LiDAR), RAdio Detection And Ranging (RADAR), Differential-Global\
    \ \nNavigation Satellite System (D-GNSS), Wheel odometers or Cameras. For this\
    \ purpose, a \nself-driving car must sense its environment and moving safely with\
    \ little or even no human \nintervention. Considering a modular architecture [65, 39],\
    \ which are the far more common \nthan end-to-end architectures [10, 68] at the\
    \ time of writing this article, the core functions \nof an AV pipeline may be\
    \ summarized [49] as: vehicle control, localization and mapping, \n4214\nMultimedia\
    \ Tools and Applications (2022) 81:4213–4240\n1 3\nplanning, perception, decision-making\
    \ and V2U (Vehicle-to-User), sometimes referred as \nHMI (Human-Machine Interface).\
    \ The main idea of these modular based architectures is to \ngenerate motor commands\
    \ as a result of the whole stream, which start with feeding raw sensor \ninputs\
    \ to the perception [12] and localization modules [64] to perceive the surrounding\
    \ envi-\nronment and estimate the ego-vehicle position in real-time. In terms\
    \ of the perception module, \nfor fully-autonomous driving architectures in complex\
    \ scenarios, estimating the position of the \nobjects in the 2D/2.5D/3D space\
    \ with a very low error is insufficient. Then, identifying the \nstatic and dynamic\
    \ objects of the environment (output of the tracking stage [24]), in addition\
    \ \nto their heading angle and velocity, is mandatory as a preliminary stage of\
    \ scene prediction. \nThis, combined with the control and mapping/planning layer\
    \ information, feeds the decision-\nmaking layer of the ego-vehicle in order to\
    \ successfully predict the future trajectory and most \nprobable behaviour of\
    \ the dynamic obstacles to avoid collisions and plan the optimal route \nin the\
    \ road in common urban use cases such as predicting the trajectory of a pedestrian\
    \ in a \ncrosswalk, predict the presence of an adversary vehicle in the next give-way\
    \ or even evaluat-\ning the risk when carrying out the Adaptive Cruise Control\
    \ (ACC) use case, in which the ego-\nvehicle adjusts the vehicle speed to maintain\
    \ a safe distance with ahead vehicles.\nThe scope of this paper is the validation\
    \ of our ROS based AD driving architecture (Agent \nDocker in Fig. 1), focusing\
    \ on the behavioural decision-making layer, in the context of some \nchallenging\
    \ driving scenarios inspired in the CARLA [14] Autonomous Driving Challenge \n\
    and based in the NHTSA protocol. This paper is an extension of our previous conference\
    \ pub-\nlication [23]. In this previous work, our results were limited to the\
    \ analysis of the linear veloc-\nity given the corresponding states of the HBIPNs,\
    \ whilst in the present work a more detailed \nanalysis of temporal graph associated\
    \ to the Vulnerable Road Users (VRU) use cases, in order \nto analyze in depth\
    \ the performance of our architecture. The validation with CARLA, a novel \nopen-source\
    \ autonomous driving simulator, featured by its flexibility, hyper-realism and\
    \ real-\ntime working, reinforces the simulation design stage. We hope that our\
    \ distributed system can \nserve as a solid baseline on which future research\
    \ can build on to advance the state-of-the-art \nin validating fully-autonomous\
    \ driving architectures using hyper-realistic simulation, as a pre-\nliminary\
    \ stage before implementing our architecture in our real electric car prototype.\n\
    The remaining content of this work is organized as follows. Section 2 presents\
    \ some chal-\nlenges associated to this research, focused on the decision-making\
    \ layer and the importance \nof simulation in the context of AV, as well as a\
    \ brief survey among different simulators for \ntesting self-driving cars. Section 3\
    \ presents our autonomous navigation architecture. Section 4 \ndescribes the main\
    \ features of the CARLA simulator, its integration with our AV architecture, \n\
    the way in which the sensors perceive the environment and how this information\
    \ is processed \nto feed the decision-making system, including both the prior\
    \ knowledge and traffic situation. \nSections 5 and 6 describes some interesting\
    \ use cases, giving rise to some quantitative and \nqualitative results to illustrate\
    \ the proposed architecture performance. Finally, Sect. 7 deals \nwith the future\
    \ works and concludes the paper.\n2  Related works\nAs mentioned in Sect. 1, a\
    \ fully-autonomous driving architecture (L5 in the J3016 SA [61]) \nis still years\
    \ away, mainly due to technical challenges, but also due to social and legal ones\
    \ \n[42]. The Society of Automotive Engineers (SAE) stated a taxonomy with five\
    \ levels [61] \nof driving automation, in which the level zero stands for no automation\
    \ and level five for \nfull-automation. Level one (L1) includes primitive ADAS\
    \ (Advanced Driver Assistance \n4215\nMultimedia Tools and Applications (2022)\
    \ 81:4213–4240\n1 3\nSystems) such as Adaptive Cruise Control, stability control\
    \ or anti-lock braking systems \n[52]. Once the system has longitudinal and lateral\
    \ control in a specific use case (although \nthe driver has to monitor the system\
    \ at all times), level two (L2) becomes a feasible tech-\nnology. Then, the real\
    \ challenge arises above this level. Level three (L3) is conditional \nautomation,\
    \ that is, the system has longitudinal and lateral control in specific use cases,\
    \ so \nFig. 1  Our autonomous driving architecture under CARLA simulation. GUI\
    \ = Graphical User Interface; \nHW = Hardware; RG = RoboGraph; GPS = Global Positioning\
    \ System; ROS = Robot Operating System; \nLiDAR = Light Detection and Ranging;\
    \ IPC = Inter-Process Communication\n4216\nMultimedia Tools and Applications (2022)\
    \ 81:4213–4240\n1 3\nthe driver does not have to monitor the system at all times.\
    \ However, the system recognizes \nthe performance limits and the driver is requested\
    \ to resume control (s/he must be in posi-\ntion) within a sufficient time margin.\
    \ In that sense, the takeover maneuver (transition from \nautomatic to manual\
    \ mode) is an issue yet to be solved. Since recent studies [20, 43] have \ndemonstrated\
    \ how it increases the likelihood of an accident during the route, in particular\
    \ \nif the driver is not aware of the navigation.\nBesides this, we find levels\
    \ four and five, in which human attention is not required any-\nmore in specific\
    \ use cases (L4) or any weather condition and road conditions (L5), which \nrepresent\
    \ an open and challenging problem. The environment variables, from surrounding\
    \ \nbehaviour to weather conditions, are highly stochastic and difficult to model.\
    \ In that sense, \nno industry organization has shown a ratified testing methodology\
    \ for L4/L5 autonomous \nvehicles. The autonomous driving community gives a simple\
    \ reason: despite the fact that \nsome regulations have been defined for these\
    \ levels and current automotive companies/\nresearch groups are very good at testing\
    \ the individual components of the AD architec-\nture using the corresponding\
    \ datasets [19, 9, 70], there is a need to test intelligent vehicles \nfull of\
    \ advanced sensors [57] in an end-to-end way. In this context, artificial intelligence\
    \ is \nincreasingly being involved in processes such as detecting the most relevant\
    \ objects around \nthe car (DL based multi-object tracking systems), or evaluating\
    \ the current situation of the \nvehicle to conduct the safest decision (e.g.\
    \ Deep Reinforcement Learning applied to behav-\nioural systems). Moreover, it\
    \ is important to consider the presence of sensor redundancy \nin order to establish\
    \ a safe navigation in such a way the different sensors and associated \nalgorithms\
    \ are integrated together, to validate the whole system and not just individual\
    \ \ncomponents.\nRegarding urban environment complexity, in order to validate\
    \ a whole AD architecture \nthe system must be tested in countless environments\
    \ and scenarios, which would escalate \nthe cost and development time exponentially\
    \ with the physical approach. Considering this, \nthe use of photo-realistic simulation\
    \ (virtual development and validation testing) and an \nappropriate design of\
    \ the driving scenarios are the current keys to build safe and robust AV. \nThese\
    \ simulators have evolved from merely simulating vehicle dynamics to also simulating\
    \ \nmore complex functionalities. Simulators intended to be used for test self-driving\
    \ technol-\nogy must have requirements that extend from simulating physical car\
    \ models to several \nsensor models, path planning, control and so forth and so\
    \ on. Some state-of-the-art simula-\ntors [33] for testing self-driving vehicles\
    \ are as following:\n– MATLAB/Simulink [38] published its Automated Driving Toolbox,\
    \ that provides \nseveral tools which facilitate the design, simulation and testing\
    \ of automated driving \nsystems and Advanced Driver Assistance Systems (ADAS).\
    \ One of its key features is \nthat OpenDRIVE [15] road networks can be imported\
    \ into MATLAB and may be used \nfor varios testing and design purposes. This Automated\
    \ Driving toolbox also supports \nHardware-in-the-Loop (HIL) testing as well as\
    \ C/C++ generation, which enables faster \nprototyping.\n– CarSim [6] is a vehicle\
    \ simulator commonly used by academia and industry. Its new-\nest version supports\
    \ moving objects and sensors that benefit simulations involving self-\ndriving\
    \ tecnology and ADAS. These moving objects may be linked to 3D objects with \n\
    their own embedded animations, such as vehicles, byciclists or pedestrians.\n\
    – PreScan [63] provides a simulator framework to design self-driving cars and\
    \ ADAS. It \npresents PreScan’s automatic traffic generator which enables manufacturers\
    \ to validate \ntheir autonomous navigation architectures providing a variety\
    \ of realistic environments \n4217\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\nand traffic conditions. This simulator also supports HIL simulation, quite\
    \ common for \nevaluating Electronic Control Units (ECUs) used in real-world applications.\n\
    – CARLA (CAR Learning to Act) [14] is an open-sources simulator which main objec-\n\
    tive is to democratize the autonomous driving research area. It is developed based\
    \ on \nthe Unreal Engine [56], consisting of a scalable client-server architecture\
    \ in which the \nthe simulation tasks are deployed at the serve, including the\
    \ updates on the world-state \nand its sensor rendering, computation of physics,\
    \ actors, etc.\n– Gazebo [35] is an scalable, open-source, flexible and multi-robot\
    \ 3D simulator. It sup-\nports the recreation of both outdoor and indoor environments\
    \ in which there are two \ncore elements that define the 3D scene, also known\
    \ as world and model. The world \nis used to represent the 3D scene, defined in\
    \ a Simulation Description File (SDF) and \na model is basically any 3D object.\
    \ Gazebo uses Open Dynamic Engine (ODE) as its \ndefault physic engine.\n– LGSVL\
    \ (LG Electronics America R&D Center) [55] is the most recent simulator \nfor\
    \ testing autonomous driving technology, focused on multi-robot simulation. It\
    \ is \nbased on the Unity game engine [26], providing different bridges for message\
    \ passing \nbetween the simulator backbone and the autonomous driving stack. LGSVL\
    \ provides a \nPythonAPI to control different environment entitities, such as\
    \ weather conditions, the \nposition of the adversaries, etc. in a similar way\
    \ to the CARLA simulator. It also pro-\nvides Functional Mockup Interface (FMI)\
    \ so as to integrate vehicle dynamics platform \nto the external third party dynamics\
    \ models.\nIn order to choose the right simulator, there is a set of criteria\
    \ [33] that may serve as a \nmetric to identify which simulators are most suitable\
    \ for our purposes, such as percep-\ntion (sensors), multi-view geometry, traffic\
    \ infrastructure, vehicle control, traffic scenario \nsimulation, 3D virtual environment,\
    \ 2D/3D groundtruth, scalability via a server multi-\nclient architecture and\
    \ last but not the least, open-source. In that sense, we identify that \nMATLAB/Simulink\
    \ is designed to simulate simple scenarios, with efficient plot functions \nand\
    \ computation. It is usually connected to CarSim, where the user can control the\
    \ vehicle \nmodels from CarSim and build their upper control algorithms in MATLAB/Simulink\
    \ to \ndo a co-simulation project, but the realism, the quality of the sensors\
    \ and the complexity \nis limited. PreScan presents better capabilities to build\
    \ realistic environments and simulate \ndifferent weather conditions, unlike MATLAB\
    \ and CarSim. Gazebo is quite popular as a \nrobotic simulator, but the effort\
    \ and time needed to create complex and dynamic scenes \ndoes not make it the\
    \ first choice for testing self-driving technology. Then, we have two \nsimulators\
    \ as our final options: LGSVL and CARLA. At the moment of writing this paper,\
    \ \nthey are the most suited simulators for end-to-end testing of unique functionalities\
    \ offered \nby autonomous vehicles, such as perception, mapping, vehicle control\
    \ or localization. Most \nof their features, summarized in [33] are identical\
    \ (open-source, traffic generation simula-\ntion, portability, 2D/3D groundtruth,\
    \ flexible API and so forth and so on), with the only \ndifference that LGSVL\
    \ does not present camera calibration to perform multi-view geom-\netry or Simultaneous\
    \ Localization and Mapping (SLAM). In that sense, we decided to use \nthe CARLA\
    \ simulator since the performance is very similar to LGSVL and the group had \n\
    previous experience in the use of this simulator. For a deeper understanding about\
    \ how we \nintegrate our architecture with this simulator, we refer the reader\
    \ to Sect. 4.\nMoreover, as stated in Sect. 1, this work focuses on the behavioural\
    \ decision-making \nlayer of the pipeline and its relation with the end-to-end\
    \ validation. The decision-making \nlayer must provide tools to model the sequence\
    \ of events and action, based on some pre-\ndefined traffic rules, that can take\
    \ place in the different traffic scenarios. In terms of AV, \n4218\nMultimedia\
    \ Tools and Applications (2022) 81:4213–4240\n1 3\ndifferent approaches have been\
    \ proposed to design the decision-making layer, including \ndifferent heuristic\
    \ solutions [65], that lie in identifying a set of driving scenarios or driv-\n\
    ing contexts (e.g. intersection handling or lane driving), reducing the number\
    \ of environ-\nmental features to which the vehicle must be focused, according\
    \ to each driving context. \nThe design of the decision-making layer for AV is\
    \ challenging due to uncertainty in the \nknowledge about the driving situation\
    \ and the state of the vehicle. This uncertainty comes \nfrom different sources,\
    \ such as the estimation of the continuous state of nearby external \nagents,\
    \ like other vehicles or pedestrians, whose behaviour is usually unpredictable.\
    \ Hence, \nin order to design an optimal decision system, uncertainty must be\
    \ considered. Despite \nthe fact that Partially Observable Markov Decision Processes\
    \ (POMDPs) [37] offer a great \nframework to manage uncertainty, they are not\
    \ scalable to real-world scenarios due to the \nassociated complexity. Other approaches\
    \ tackle this layer using simple discrete events sys-\ntems, which are not enough\
    \ complex to model real-world driving scenarios. Decision Trees \n(DT) [16], Hierarchical\
    \ Finite-State Machines (HFSM) [3] and Finite-State Machines \n(FSM) [46] are\
    \ among the most popular approaches to design a decision making system. \nSeveral\
    \ teams in the Defense Advanced Research Projects Agency (DARPA) Urban Chal-\n\
    lenge [41] used some of these approaches.\nThe approach used by the winning team\
    \ (Tartan team) was a HFSM to implement their \nbehaviour generation component.\
    \ Then, the main task is hierarchically broken down into \na set of top-level\
    \ hierarchy behaviours, identified with intersection handling, lane-driving \n\
    and achieving a pose [65].\nNevertheless, for complex problems in which common\
    \ urban driving scenarios are \nincluded, the difficulty in representing the system\
    \ as FSM lies in dealing with the need \nto implement the potentially high number\
    \ of transitions due to the state explosion prob-\nlem. Behaviours Trees (BT)\
    \ can get rid of these drawbacks since the transitions among the \nstates are\
    \ implicit to their control structure formulated as a tree, giving rise to a higher\
    \ flex-\nibility, maintainability and extensibility with respect to FSMs.\nHowever,\
    \ simple discrete event systems and BTs share a common problem: A naive \nimplementation\
    \ usually gives rise to blocking behaviour, which make them unsuitable for \n\
    autonomous driving. In that sense, concurrency and parallel activities can be\
    \ easily pro-\ngrammed using Petri Nets (PN). PNs are a powerful tool to design,\
    \ model and analyze \nconcurrent, sequential and distributed systems. While in\
    \ a FSM there is always a single \ncurrent state, in PNs there may be several\
    \ states that can change the state of the PN. In par-\nticular, in this work we\
    \ model every behaviour as Hierarchical Interpreted Binary Petri Net \n(HIBPN)\
    \ [18, 22], as shown in Fig. 2b, where a PN can start/stop another PN depending\
    \ on \nits hierarchy.\n3  Autonomous navigation architecture\nTo accomplish the\
    \ AD task, we present an architecture (Fig. 1) that exploits the concepts \nof\
    \ portability, flexibility and isolation in terms of software development using\
    \ Docker [44] \ncontainers and standard communications in robotics using the Robot\
    \ Operating System \n(ROS) [51]. It is featured by a modular architecture where\
    \ individual modules process the \ninformation in an asynchronous way. These modules\
    \ are standalone processes that commu-\nnicate each other using the ROS Inter-Process\
    \ Communication (IPC) system. The publish/\nsubscribe concept is used to provide\
    \ non-blocking communications. These software mod-\nules are organized in four\
    \ layers as following:\n4219\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\n– Hardware drivers layer Set of programs that implements different hardware\
    \ devices, \nsuch as sensors and actuators.\n– Control layer Set of programs that\
    \ implements the vehicle control and navigation func-\ntionality. These programs\
    \ include the path planning (map manager), localization, reac-\ntive control (local\
    \ navigator) and a program to process most of the perception sensors to \ndetect\
    \ and track relevant events (event monitor).\n– Executive layer Set of programs\
    \ that coordinates the sequence of actions required to be \nexecuted to perform\
    \ the current behaviour.\n– Interface layer Set of processes that interact with\
    \ the user and enable the communica-\ntion to other processes for multi-robot\
    \ applications.\nFurthermore, our workflow is summarized in Fig. 3. In order to\
    \ implement and validate \nnew algorithms for some specific use case in our real-world\
    \ autonomous electric car, we \nfirst evaluate not only the proposals of each\
    \ individual layer, but also the whole archi-\ntecture, which is actually the\
    \ real purpose of automotive companies. We employ both \nVersion Control (Git)\
    \ and lightweight Linux containers (Docker) for consistent develop-\nment to ensure\
    \ the transfer learning between simulation and the real-world. Our motion \ncontrol\
    \ is broken down into high-level (HL) planning and low-level (LL) reactive con-\n\
    trol. First of all, as observed in Figs. 3 and 1, the map manager (control layer)\
    \ loads \nthe map made up by a sequence of lanelets [5], in which the user must\
    \ specify the start \nand goal position of the route. Then, a lanelet path is\
    \ generated using an A* algorithm \n[8]. Besides this, the map manager serves\
    \ other queries from other modules related to \nthe map, such as providing the\
    \ most relevant lanes around the vehicle (also referred as \nmonotorized lanes),\
    \ like intersection lanes or contiguous lanes for Stop and Overtaking \nuse cases\
    \ respectively, or providing the position of the regulatory elements in the route.\
    \ \nFinally, a global path planner calculates a suitable path to be followed by\
    \ the car. Since \nthe A* algorithm belongs to the goal-directed category ([2]\
    \ differentiates route plan-\nner into four categories: goal-directed, separator-based,\
    \ hierarchical and bounded-hop, \nproviding the fastest route but not the most\
    \ desirable one, since no road hierarchies or \nprecomputed distances between\
    \ selected vertexes are taken into account. In that sense, \na local navigation\
    \ module is used to safely follow this path, keeping the vehicle within \nthe\
    \ driving lane and modifying the vehicle dynamic as a function of the behaviour\
    \ con-\nstraints established by the HL planning. To do that, the local navigation\
    \ system calcu-\nlates the curvature to guide the car from its current position\
    \ to a look-at-head position \nFig. 2  (a) Simulation example: On the left, the\
    \ CARLA simulator illustrates the urban scenario our sensors \nface. On the right,\
    \ the RVIZ simulator shows the detection in the coloured point cloud (b) Start\
    \ Petri Net of \nour behaviour decision-making system\n4220\nMultimedia Tools\
    \ and Applications (2022) 81:4213–4240\n1 3\nplaced in the center of the lane\
    \ using the Pure Pursuit algorithm [34]. This curvature is \nused as the reference\
    \ for the reactive control, where an obstacle avoidance method is \nimplemented\
    \ based on the Beam Curvature Method (BCM) [17]. Combining both the \nPure Pursuit\
    \ algorithm and BCM, the local navigation system keeps the vehicle centered \n\
    in the driving lane while is able to avoid unknown obstacles that can partially\
    \ block the \nlane.\nIn terms of environment perception, we perform a sensor fusion\
    \ [22] between LiDAR \nand camera in order to detect and track the most relevant\
    \ objects in the scene. First, a \nsemantic segmentation is performed in the RGB\
    \ image to detect the different obstacles and \nthe drivable area. In the present\
    \ work we use the groundtruth of the semantic segmentation \nprovided by CARLA\
    \ pseudo-sensor instead of using some algorithm to process the RGB \ncamera and\
    \ obtain the segmented image. We will conduct this preprocessing step in future\
    \ \nworks. In the present work we focus on validating the autonomous navigation\
    \ architecture \nand its relationship with the decision-making layer. We merge\
    \ the 3D LiDAR point cloud \ninformation and the 2D segmented pixels so as to\
    \ obtain a coloured point cloud, where the \npoints out of the FoV (Field of View)\
    \ of the camera are not coloured. Then, we carry out a \ncoloring based clustering\
    \ [28], obtaining the most relevant objects in the scene, as shown \nin Fig. 2a.\
    \ Then, Multi-Object Tracking is performed by combining the Precision Tracker\
    \ \napproach [28], BEV (Bird’s Eye View) Kalman Filter [32] and Nearest Neighbour\
    \ algo-\nrithm [4]. The output of the perception module is then published through\
    \ the event monitor \nmodule (see Fig. 1).\nFinally, the behavioural decision-making\
    \ is implemented using HIBPNs, where the \nmain Petri Net is fed with the inputs\
    \ provided by the local perception (event monitor mod-\nule), ego-vehicle dead-reckoning\
    \ in the map and the map manager information. To imple-\nment these HIBPNs, we\
    \ use the RoboGraph tool [41], employed by the authors in other \nmobile robot\
    \ applications.\nFig. 3  Workflow to develop safe AD technology. Version Control\
    \ (Git) and lightweight Linux containers \n(Docker) are employed for consistent\
    \ development and deployment to ensure the transfer learning between \nsimulation\
    \ and the real-world\n4221\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\n4  Simulation stage\nFatalities caused by immature technology (software,\
    \ hardware or integration) under-\nmine public acceptance of self-driving technology.\
    \ In 2014, in the Hyundai competi-\ntion one of the teams crashed due to the rain\
    \ [62]. In 2016, the Google car hit a bus \n[11] while conducting the lane changing\
    \ maneuver since it failed to estimate the speed \nof the bus. From L3 to L4 the\
    \ vehicles goes from human centered autonomy, in which \nartificial intelligence\
    \ is not fully responsible, to a stage in which the implemented \nsoftware and\
    \ hardware are fully responsible in each particular real-world situation. \nRegarding\
    \ this, several ethical dilemmas arise. In the event of an accident, the legal\
    \ \nresponsibility would lie with the automotive company rather than the human\
    \ driver. \nMoreover, in an inevitable accident situation, which should be the\
    \ most accurate sys-\ntem behaviour? Then, reliability certification and risk\
    \ is another task to be solved. \nSome promising works have been proposed such\
    \ as DeepTest [62], but a workflow of \ndesign ⟶ simulation ⟶ test ⟶ redesign\
    \ has not been established by the automotive \nindustry yet. Then, it can be observed\
    \ that, a fully-autonomous driving architecture \n(L5 in this classification)\
    \ is still years away, not only due to technical challenges, but \nalso due to\
    \ social and legal ones. To solve this challenge, virtual testing is becom-\n\
    ing increasingly importance in terms of AV, even more so considering L3 or higher\
    \ \narchitectures, in order to provide an endless generation of traffic scenarios\
    \ based on \nreal-world behaviours. Since the urban environment is highly complex,\
    \ the navigation \narchitecture must be tested in countless environments and scenarios,\
    \ which would esca-\nlate the cost and development time exponentially with the\
    \ physical approach. Some \nof the most used simulators in the field of AV are\
    \ Microsoft Airsim [58], recently \nupdated to include AV although though it was\
    \ initially designed for drones, NVIDIA \nDRIVE PX [7], aimed at providing AV\
    \ and driver assistance functionality powered by \ndeep learning, ROS development\
    \ studio [45], fully based on the cloud concept where \na cluster of computers\
    \ allows the parallel simulation of as many vehicles as required, \nV-REP [54],\
    \ with an easy integration with ROS and a countless number of vehicles \nand dynamic\
    \ parameters and CARLA [14], which is the newest open-source simulator \nfor AV\
    \ based on Unreal engine. In [22] we validated the proposed navigation archi-\n\
    tecture using the V-REP simulator due to the experience of our research group\
    \ with \nsmall mobile robots. However, the paradigms of real-time and high realism\
    \ could not \nbe deeply analysed since V-REP was not designed to create high realistic\
    \ urban driving \nscenarios, in such a way the analysis of traffic use cases is\
    \ difficult. Since our project \nis characterized by being open-source, we decided\
    \ to integrate our autonomous driving \narchitecture with the open-source CARLA\
    \ simulator. This offers a much more inter-\nesting environment in terms of traffic\
    \ scenarios, perception, real-time and flexibility, \nwhich are key concepts to\
    \ design a reliable AV pipeline. In this work we have used the \n0.9.6 version\
    \ of CARLA and the corresponding versions of the CARLA ROS Bridge \nand the Scenario\
    \ Runner. Additional tools needed for the simulation, were configured \naccording\
    \ to this version. In our previous work [22], we run the simulation environ-\n\
    ment, the V-REP ROS bridge and the autonomous navigation architecture in the same\
    \ \nhost machine, running on Ubuntu 16.04. On the contrary, in the present work\
    \ we \nexploit the concept of lightweight Linux containers using Docker [44] images,\
    \ where \nthe first image contains the CARLA world and CARLA ROS Bridge information\
    \ and \nthe second image contains our autonomous navigation architecture, as shown\
    \ in Fig. 1.\n4222\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\n\
    4.1  Environment\nCARLA is implemented as an open-source layer over Unreal Engine\
    \ 4 (UE4) [56] \ngraphic engine. This simulation engine provides CARLA an hyper-realistic\
    \ physics and \nan ecosystem of interoperable plugins. In that sense, CARLA simulates\
    \ a dynamic world \nand provides a simple interface between an agent that interacts\
    \ with the world and itself. \nIn order to support this functionality, CARLA was\
    \ designed as a server-client system, \nwhere the simulation is run and rendered\
    \ by the server. CARLA environment is made \nup by 3D models of static objects\
    \ like infrastructure, buildings or vegetation, as well as \ndynamic objects such\
    \ as vehicles, cyclists or pedestrians. They are designed using low-\nweight geometric\
    \ models and textures but maintaining visual realism due to a variable \nlevel\
    \ of detail and carefully crafting the materials. On the other hand, the maps\
    \ provided \nby CARLA are in OpenDrive [15] format, whilst we use the lanelet\
    \ approach based on \nOpenStreetMap (OSM) [27] service. We transform the OpenDrive\
    \ format into OSM for-\nmat by using the converter proposed by [1]. Then, based\
    \ on this lanelet map, we manu-\nally include the regulatory traffic information\
    \ to generate an enriched topological map \nuseful for navigation. Furthermore,\
    \ the OSM map uses WGS84 coordinates (latitude, \nlongitude and height) whilst\
    \ the simulator provides Cartesian coordinates (UTM) relative \nto an origin (usually\
    \ the center of the map). Then, geometric transformations between \nboth systems\
    \ are calculated using the libraries implemented in the ROS geodesy package.\n\
    4.2  Vehicle\nIn order to link the vehicle in CARLA and its corresponding model\
    \ in RVIZ (Fig. 1a), we \nmodify the ROS bridge associated to the CARLA simulator.\
    \ The CARLA ROS bridge is \na set of algorithms package that aims at providing\
    \ a bridge between CARLA and ROS, \nbeing able to send the data captured by the\
    \ on-board sensors and other variables associ-\nated to the vehicle in the form\
    \ of topics and parameters understood by ROS. In that sense, \nwe modify some\
    \ parameters of the acceleration and speed PIDs (Proportional Integral \nDerivative)\
    \ controllers, the sensors position, orientation and resolution, the Ackermann\
    \ \ncontrol node and the waypoint publisher to adjust the CARLA ego-vehicle parameters\
    \ \nto our project. The reference system of the vehicle is centered on the rear\
    \ axle, and the \norientation of that frame is based on the LiDAR frame, that\
    \ is, X-axis pointing to the \nfront, Y-axis to the left and Z-axis above. The\
    \ angular velocity at Z is positive according \nto the right hand rule, so the\
    \ right turns are considered negative. The speed commands \ngenerated by the simulator\
    \ are interpreted by the local navigation system (in particular, \nthe low-level\
    \ controller), giving rise the corresponding accelerations and turning angle.\n\
    4.3  Sensors\nFrom the sensors perspective, the agent sensor suite can be modified\
    \ in a flexible way, \nbeing possible to modify both the number of sensors and\
    \ their associated features, such \nas the position, orientation, resolution or\
    \ frequency. Most common sensors in CARLA \nworld are LiDAR, GNSS and RGB cameras\
    \ as well as their corresponding pseudo-\nsensors that provide semantic segmentation\
    \ and groundtruth depth. Moreover, cam-\nera parameters include 3D orientation\
    \ and position with respect to the car coordinate \nsystem, field-of-view and\
    \ depth of field. The original configuration of the CARLA \n4223\nMultimedia Tools\
    \ and Applications (2022) 81:4213–4240\n1 3\nego-vehicle on-board sensors had\
    \ a 800 × 600 RGB camera sensor placed at the front \nof the vehicle with a FoV\
    \ of 100 º, a 32-channels LiDAR, a collision sensor, a GNSS \nsensor and a lane\
    \ invasion sensor. In our case, the position of the LiDAR and camera \nsensors\
    \ are manually configured in order to obtain the same frames distribution that\
    \ \nin our real-world vehicle. LiDAR information is published in PointCloud2 ROS\
    \ format \nusing the bridge with the Z axe pointing up, Y left and the X inwards.\
    \ Image messages \nare published with the Z axe inwards, the Y down and the X\
    \ pointing right in the image \nplane. As mentioned above, in order to colour\
    \ the 3D point cloud and perform coloring \nclustering, we project the semantic\
    \ segmentation into the cloud. This semantic segmen-\ntation is carried out based\
    \ on a 1280 × 720 RGB camera as input, which is the resolu-\ntion of the ZED camera\
    \ equipped in our real-world electric car. Moreover, the position \nof the GNSS\
    \ sensor is displaced to the center of the rear axle, required by the local navi-\n\
    gation system. The remaining sensors keep unmodified. As observed, CARLA provides\
    \ \na straightforward way to add or remove sensors from the vehicle or even modify\
    \ their \nparameters, to adjust the simulation to the real-world as best as possible.\n\
    5  NHTSA based use cases\nOne of the best advantages of CARLA is the possibility\
    \ to create ad-hoc urban lay-\nouts, useful to validate the navigation architecture\
    \ in challenging driving scenarios. \nThis code can be downloaded from the ScenarioRunner\
    \ repository, associated to the \nCARLA GitHub. The ScenarioRunner is a module\
    \ that allows the user to define and \nexecute traffic scenarios for the CARLA\
    \ simulator. In terms of crash typologies, there \nare several New Car Assessment\
    \ Programs (NCAPs), which are a set of protocols to \nevaluate the safety of vehicles.\
    \ Currently, evaluations are focused on the structure of \nvehicle and Advanced\
    \ Driver Assistance Systems (ADAS), such as: Adult Occupant \nProtection (AOP),\
    \ Child Occupant Protection (COP), Autonomous Emergency Braking \n(AEB), Speed\
    \ Assist Systems (SAS), etc. Crash typologies provide an understanding of \ndistinct\
    \ crash types and scenarios and explain why they occur. They serve as a tool to\
    \ \nidentify intervention opportunities, set research priorities and direction\
    \ in technology \ndevelopment, and evaluate the effectiveness of selected crash\
    \ countermeasure systems. \nThe most important NCAPs, at the moment of writing\
    \ this paper, are:\n– European New Car Assessment Program (Euro-NCAP): Founded\
    \ in 1997, is the \nmost widely used NCAP, within the scope of the collaboration\
    \ of the countries of the \nEuropean Union [66].\n– China New Car Assessment Program\
    \ (C-NCAP): Research and development bench-\nmark for vehicle manufacturers in\
    \ Asia, founded in 2006. Most of its program is \nbased on the Euro-NCAP [25].\n\
    – National Highway Transportation Safety Administration (NHTSA): Agency of the\
    \ \nUnited States of America federal government, part of the Department of Transporta-\n\
    tion, founded in 1970. They have published research reports, guidance documents,\
    \ \nand regulations on vehicles equipped with ADAS [60].\n– Autoreview Car Assessment\
    \ Program (ARCAP): is the first independent evaluation pro-\ngram for cars in\
    \ Russia. They publish their studies in a newspaper called Autoreview \n[29].\n\
    4224\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\nIn the present\
    \ case, the scenarios are inspired in the CARLA Autonomous Driving Chal-\nlenge\
    \ (CADC) 4, selected from the NHTSA pre-crash typology [48], which can be defined\
    \ \nusing a Python interface or the OpenSCENARIO [31] standard. Then, the user\
    \ can prepare \nits AD pipeline (also referred as the agent) for evaluation by\
    \ creating complex routes and \ntraffic scenarios with the presence of additional\
    \ obstacles, start condition for the adver-\nsary (e.g. pedestrian or vehicle),\
    \ position and orientation of the dynamic obstacles in the \nenvironment, weather\
    \ conditions and so on. Table 1 shows the main HIBPNs features used \nto manage\
    \ the logic of these use cases (inputs, input modules, outputs, output modules,\
    \ \nnumber of nodes and number of transitions). Below the particular HIBPNs covered\
    \ in this \npaper are explained from the behavioural decision-making layer perspective.\n\
    5.1  Stop behaviour\nIntersections represent some of the most common danger areas\
    \ on the road, since they pose \nmany challenges as vehicles must deal with different\
    \ maneuvers such as traffic signals, \nturning lanes, merging lanes, and other\
    \ vehicles in the same area. Intersections are highly \nvaried, and some require\
    \ the car to stop, while others only require a give way.\nIn our case, this behaviour\
    \ (Fig. 5) is triggered if the distance between the Stop regu-\nlatory element\
    \ and our ego-vehicle is equal or lower than a certain threshold Dre (that \n\
    stands for distance to regulatory element, usually 30 m). Then, the perception\
    \ system \nwill try to check if there is some car in the lanelets that intersect\
    \ with our current trajec-\ntory. If no car is detected, the car resumes the path,\
    \ not modifying the current trajectory. \nNevertheless, even if no car is detected\
    \ and the distance to the reference line (position \nof the regulatory element)\
    \ is lower than a more restrictive threshold Drl (that stands for \nFig. 4  Our\
    \ selected CARLA Autonomous Driving challenge traffic scenarios, based on the\
    \ pre-crash sce-\nnario typology for crash avoidance defined by the NTHSA organization\n\
    4225\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\ndistance to\
    \ reference line, usually 10 m), due to the traffic rules of the stop behaviour,\
    \ \nthe ego-car must always stop, so the ego-vehicle sends a stop command to the\
    \ local \nnavigation module. At this point, while waiting for the car to stop,\
    \ the messages regard-\ning the car velocity from the local navigation module\
    \ will be received until the speed \nreaches zero (stopped transition). After\
    \ waiting a few seconds for safety, it will proceed \nto follow the path if the\
    \ event monitor publishes a message indicating that it is safe to \nmerge. While\
    \ merging, when the car reaches the intersection, it considers that the safest\
    \ \nmaneuver to do is to continue and it ends the behavior.\nOn the other hand,\
    \ if a vehicle is detected before reaching the intersection, a cor-\nresponding\
    \ message is received from the event monitor and the safeMerge transition is \n\
    set to 0. In this case, the car will stop (stop place) sending the corresponding\
    \ stop mes-\nsage to the local navigator and waits until the event monitor module\
    \ reports a safeMerge \nmessage. After stopping in front of the reference line,\
    \ and waiting for the detected car \nto take out of our perception system, the\
    \ car will proceed to keep on the path and finish \nthe behaviour.\nTable 1  Summary\
    \ of the main features, including inputs, outputs and involved modules, for the\
    \ main Petri \nNets of our work. MM = Map Manager; EM = Event Monitor; LN = Local\
    \ Navigator; RGD = RoboGraph \nDispatch; PC = Pedestrian Crossing; N = Nodes;\
    \ T = Transitions\nPetri Net\nInputs\nInput modules\nOutputs\nOutput modules\n\
    N / T\nBackground\nMan/auto\nGUI User\nRun Selector\nRGD\n8/9\ngoToPoint\nGUI\
    \ User\nStop selector\nRGD\nSelector PN\nReg. Element\nMM\nRun PC\nRGD\n21/29\n\
    Dist. Reg Element\nMM\nStop PC\nRGD\nEnd Reg. Element\nMM\nRun GiveWay\nRGD\n\
    Reg. Element\nEM\nStop GiveWay\nRGD\nEnd Reg. Element\nEM\nRun Stop\nRGD\nFrontCarVel\n\
    EM\n...\n...\nOdom\nBase\nPedestrian\nNoPedestrian\nEM\nWatchforPedetrians\nEM\n\
    10/13\nCrossing\nPedestrian\nEM\nSetMaxVel\nLN\nDist. To PC\nMM\nStopAtPoint\n\
    LN\nPC Over\nMM\nForce End\nRGD\nStopped\nLN\nStop\nSafetoMerge\nEM\nCheckSafeMerge\n\
    EM\n9/12\nNotSafetoMerge\nEM\nSetMaxVel\nLN\nDistToStop\nMM\nStopAtPoint\nLN\n\
    StopOver\nMM\nForce End\nRGD\nStopped\nLN\nAdaptive\nCurrent Velocity\nMM\nSetMaxVel\n\
    LN\n4/6\nCruise\nFrontCarVel\nEM\nControl (ACC)\nDistToFrontCar\nMM\n4226\nMultimedia\
    \ Tools and Applications (2022) 81:4213–4240\n1 3\nFig. 5  Stop Petri Net\n4227\n\
    Multimedia Tools and Applications (2022) 81:4213–4240\n1 3\n5.2  Pedestrian crossing\
    \ behaviour\nThe pedestrian crossing use case (Fig. 6), also referred as crosswalk\
    \ use case in the lit-\nerature, is the most basic regulatory element that helps\
    \ people to cross a road. It is started \nwhen the ego-vehicle is approaching\
    \ an area with the corresponding regulatory element \nand the distance to the\
    \ regulatory element is lower than a certain threshold Dre (usually 30 \nm), in\
    \ a similar way to the Stop behaviour. Then, the node watchForPedestrians sends\
    \ a \nmessage to the event monitor module to watch for pedestrians crossing or\
    \ trying to cross on \nthe crosswalk. The watchForPedestrians output transition\
    \ includes three possibilities:\n– Non-pedestrian detected A confirmation from\
    \ the event monitor notifying that it \nreceived the message and so far no pedestrian\
    \ has been detected (!pedestrian transition) \nin the pedestrian crossing area.\n\
    – Pedestrian detected A pedestrian has been detected (pedestrian transition) in\
    \ the \npedestrian crossing area.\nFig. 6  Pedestrian Crossing Petri Net\n4228\n\
    Multimedia Tools and Applications (2022) 81:4213–4240\n1 3\n– Non-confirmation\
    \ There has been some problem and no confirmation has been \nreceived from the\
    \ event monitor, so a timeout message is received and the car must stop \nits\
    \ trajectory.\nWhile the current state is followPath, the local navigation module\
    \ will keep following \nthe lane provided by the map manager module. At this moment,\
    \ the system is receptive to \ntwo events: pedestrian and distToRegElem < D.\n\
    The event pedestrian corresponds to a message published by the event monitor module:\
    \ \nIf this message is received, the pedestrian transition will be fired, switching\
    \ the PN from \nfollowPath node to stop node. The transition distToRegElem < Dre\
    \ takes place when the car \nis closer than a threshold distance Dre (usually\
    \ 30 m) to the crosswalk. In this case, every \ntime the localization module publishes\
    \ a new position message, the distance between the \ncar position and the pedestrian\
    \ crossing (in particular its reference line) is calculated and \ncompared with\
    \ this threshold Dre.\nWhen the car gets close to the pedestrian crossing, it\
    \ reduces its velocity, even if there is \nno pedestrian presence, and keeps following\
    \ the path (reduceVel node) until the reference \nline of the pedestrian crossing\
    \ is reached (distToRegElem < Drl transition). Meanwhile, if a \npedestrian is\
    \ detected by the event monitor module, the message issued by this module will\
    \ \nfire the transition labeled pedestrian. The Petri net will switch the token\
    \ to the second stop \nnode, where a stop message will be sent to the local navigation\
    \ module, forcing the vehicle \nto stop in front of the reference line of the\
    \ pedestrian crossing. The Petri net will leave the \nstop mode when a notPedestrian\
    \ message is received (!pedestrian transition), meaning that \nthe car can resume\
    \ its path, ending the Petri net and allowing the Selector PN to enable \nanother\
    \ behaviour in the trajectory, if required.\nFurthermore, as observed in Fig. 6b,\
    \ no high-level behaviours are launched with the \nAdaptive Cruise Control (ACC)\
    \ or Unexpected Pedestrian use cases since they do not \ndepend of the presence\
    \ of a regulatory element, but they are always running in the back-\nground of\
    \ the Selector PN, that is, always in execution. The Adaptive Cruise Control \n\
    (ACC) is a behaviour that represents an available cruise control system for vehicles\
    \ on the \nroad, automatically adjusting the vehicle velocity to keep a safe distance\
    \ to ahead vehicles. \nAs mentioned in Sect. 1, it is a well mature technology,\
    \ widely regarded as a key compo-\nnent of any future generations of autonomous\
    \ vehicles. It improves the driver safety as well \nas increasing the capacity\
    \ of roads by maintaining optimal separation between vehicles \nan reducing driver\
    \ errors. On the other hand, the Unexpected Pedestrian is a special use \ncase\
    \ consisting on a pedestrian, bicycle, etc. jumps into the lane without the presence\
    \ of a \npedestrian crossing. Then, an emergency break must be conducted until\
    \ the car is stopped \nin front of the obstacle to ensure safety, and resumes\
    \ the navigation once the obstacle \nleaves the driving lane.\n6  Experimental\
    \ results\nIn our previous work [22] we study the linear velocity and the odometry\
    \ of the ego-vehicle \nthroughout the following ScenarioRunner use cases: Stop\
    \ with car detection, Stop with no \ncar detection, Pedestrian Crossing, Adaptive\
    \ Cruise Control (ACC) and Unexpected Pedes-\ntrian, inspired in the Traffic Scenario\
    \ 09 (Right turn at an intersection with crossing traffic), \nTraffic Scenario\
    \ 04 (Obstacle avoidance with prior action), Traffic Scenario 02 (Longitu-\ndinal\
    \ control after leading vehicle’s brake) and Traffic Scenario 03 (Obstacle avoidance\
    \ \n4229\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\nwithout\
    \ prior action) of the CADC respectively. Furthermore, in this work we focus on\
    \ the \nanalysis of Vulnerable Road Users (VRU), in particular the pedestrian\
    \ crossing and unex-\npected pedestrian, through the analysis of their corresponding\
    \ temporal graphs. A video \ndemonstration for each use cases can be found in\
    \ the following play list Simul ation Use \nCases1.\n6.1  Stop use case\nThe resulting\
    \ graphics are segmented using different colours, corresponding to the current\
    \ \nnode of the associated PN. Figure 7 shows the two different situations regarding\
    \ the Stop \nuse case: with and without the presence of an adversary vehicle.\
    \ The vehicle carries out \nthe Stop Petri net (BHstop, init) once the distance\
    \ to the regulatory element is lower than \n30 m. Then, the car reduces its velocity\
    \ from 11 m/s (most common urban speed limita-\ntion) to 0 m/s, and wait for a\
    \ safe situation (BHstop). In that sense, it is appreciated that \nthe ego-vehicle\
    \ waits 2.3 s in front of the Stop line in the use case of Stop with no detec-\n\
    tion (Fig. 7(b) and (d)), which is quite similar to time that the ego-vehicle\
    \ waits with the \npresence of an adversary vehicle (Fig. 7(a) and (c)). This\
    \ behaviour is coherent, since the \nFig. 7  First and second row represent, respectively,\
    \ the linear velocities and described trajectory projected \nonto the corresponding\
    \ CARLA scenario for Stop with car detection (a, c) and Stop with no car detection\
    \ \n(b, d) behaviours\n1 Simulation Use Cases link: https:// cutt. ly/ prUzQ Li\n\
    4230\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\nFig. 8  First\
    \ and second row represent, respectively, the linear velocities and described\
    \ trajectory projected \nonto the corresponding CARLA scenario for Pedestrian\
    \ Crossing (a, c) and Unexpected Pedestrian (b, d) \nbehaviours\nFig. 9  Pedestrian\
    \ Crossing Temporal diagram. At the top, the events produced by the monitors and\
    \ map \nmanager modules. In the middle, the evolution of the pedestrian crossing\
    \ use case, selector, and start (back-\nground) PNs. At the bottom, the velocity\
    \ of the car throughout the route\n4231\nMultimedia Tools and Applications (2022)\
    \ 81:4213–4240\n1 3\nStop PN presents a transition that requests 0 m/s for the\
    \ ego-vehicle in addition to n extra \nseconds to wait for safety, in the same\
    \ way that a real-world car would do, which is usually \nenough time for the adversary\
    \ to leave the intersection lane and enable again the navigation \n(BHstop, ResumingPath)\
    \ until the car faces another regulatory element.\n6.2  Pedestrian crossing use\
    \ case\nFigure 8(a) and (c) show the linear velocity and odometry projected onto\
    \ the CARLA sim-\nulator associated to the pedestrian crossing behaviour as it\
    \ was presented in our previous \npublication [22]. Figure 9 represents a temporal\
    \ diagram with the sequence of events, evo-\nlution of the PNs states and the\
    \ actual velocity of the car throughout the same scenario for \na new experiment.\
    \ Note that this diagram does not exactly match with their corresponding \nlinear\
    \ velocities and ego-vehicle odometry shown in Fig. 8 since corresponds to a different\
    \ \nexperiment with different stochastic processes involved on it.\nThe incorporation\
    \ of this temporal diagram, also presented in [41], is a powerful \nmanner to\
    \ validate the architecture in an end-to-end way, since we can observe how \n\
    the car behaves considering the different actions and events provided by the executive\
    \ \nlayer, which is actually the output of the whole architecture before sending\
    \ commands \nto the motor. The scenario starts with the vehicle stopped 40 meters\
    \ away from the reg-\nulatory element. Since this distance exceeds the previously\
    \ defined threshold Dre , the \nvelocity is set at maximum at the beginning (Main,\
    \ ACC limit max speed to 11,111). \nAs observed in Fig. 8(c), an additional safety\
    \ area (yellow rectangle) is defined around \nthe pedestrian crossing, in order\
    \ to activate the pedestrian detection, not only consid-\nering the lane but also\
    \ a small area of the sidewalk. Note that the shown velocity is \nnot the velocity\
    \ commanded by the control layer, which is usually more homogeneous, \nbut it\
    \ is the actual velocity of the ego-vehicle, considering the physic constraints\
    \ of the \ncar. About second 18, a message from the map manager module indicates\
    \ the proxim-\nity of the pedestrian crossing regulatory element which triggers\
    \ the pedestrian cross-\ning behaviour ( cwfollowpath ). Then, as illustrated\
    \ above, once the distance is lower than \nthe more restrictive threshold Drl\
    \ , represented as the transition close2CrossWalk, the \nexecutive layer indicates\
    \ the reactive control to gradually stop in front of the pedestrian \ncrossing\
    \ reference line. The reason for this is simple: A good practice when driving\
    \ is \nto reduce the velocity of the vehicle in the proximity of a regulatory\
    \ element, further-\nmore if it is a pedestrian crossing, to safely conduct the\
    \ maneuver, even without the \npresence of a pedestrian in the monitorized area.\
    \ In our case, above second 22 a pedes-\ntrian is detected, so a stop signal is\
    \ sent to the reactive control, which actually does not \nneed to abruptally change\
    \ the velocity of the car since it is already moving slowly. It is \nimportant\
    \ to note that the vehicle must stop in front of the reference line if there is\
    \ one \nobstacle in the monitorized area Fig. 2(a). In this case, the pedestrian\
    \ leaves the relevant \nlane (second 30) before the vehicle reaches the line,\
    \ so it does not reduce its velocity \nat all. Once the reference line is over\
    \ (second 31), the pedestrian crossing behaviour is \nfinished and the vehicle\
    \ continuous the route.\nFig. 10  Analysis of the ACC use case with variable adversary\
    \ vehicle velocity. (a) represents the ego-vehicle \nlinear velocity, (b) analysis\
    \ of the road distance to the adversary throughout the use case, (c) the ego-vehicle\
    \ \nodometry projected onto the CARLA world\n▸\n4232\nMultimedia Tools and Applications\
    \ (2022) 81:4213–4240\n1 3\n4233\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\n6.3  Adaptive Cruise Control (ACC) use case\nRegarding the Adaptive Cruise\
    \ Control (ACC) use case Fig. 10, we faced several problems \nsimulating this\
    \ behaviour in V-REP since this environment does not offer the possibility \n\
    to configure a variable velocity for the adversary, being the ACC limited to decrease\
    \ the \nego-vehicle velocity till the adversary fixed velocity. In the present\
    \ case, we take advantage \nof the possibilities offered by CARLA, in particular\
    \ ScenarioRunner, to configure the spa-\ntiotemporal variables of the adversary\
    \ in this use case. In that sense, Fig. 10(a) shows the \nlinear velocity under\
    \ the effects of the ACC in blue, being the linear velocity of the ego-\nvehicle\
    \ adjusted to the variable adversary linear velocity. It can be observed that\
    \ the ACC \nbehaviour starts at the exact moment in which the adversary vehicle\
    \ is detected. Moreover, \nFig. 10(c) shows how the road distance to the vehicle\
    \ is decreased till 22/23 m, where the \nACC is kept until the traffic scenario\
    \ is concluded. It is important to note that we do not \ncalculate the distance\
    \ Fig. 10(c) to the adversary as a front distance (X-frame considering \nLiDAR\
    \ frame) or even the Euclidean distance, since it could lie in the false hypothesis\
    \ of \nalways carrying out this scenario in a straight lane, but we calculate\
    \ the distance to the \nobstacle calculating the nearest node of the lane, and\
    \ then the accumulated position from \nour global position to that node taking\
    \ into account the intermediate nodes of the lane.\n6.4  Unexpected pedestrian\
    \ use case\nThe last use case we validate in this paper is inspired in the Traffic\
    \ Scenario 03 (Obstacle \navoidance without prior action) of the CADC, which is\
    \ one of the key scenario to validate \nVRU based behaviours. In this traffic\
    \ scenario, the ego-vehicle suddenly finds an unexpected \nobstacle on the road\
    \ and must perform an avoidance maneuver or an emergency break. In \nour case,\
    \ we design our own scenario in such a way an unexpected pedestrian jumps on the\
    \ \nroad, being initially totally occluded by a bus marquee. As expected, Fig. 8(b)\
    \ and (d) show \nFig. 11  Unexpected Pedestrian Temporal diagram. At the top,\
    \ the events produced by the monitors and map \nmanager modules. In the middle,\
    \ the selector, and start (background) PNs. At the bottom, the velocity of the\
    \ \ncar throughout the route\n4234\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\nthat no high-level behaviour is launched because this situation is not included\
    \ as a specific \nuse case (with its corresponding HBIPN), but it is always running\
    \ in the background similar \nto the ACC behaviour. Figure 11 represents a temporal\
    \ diagram with the sequence of events \nand the actual velocity of the car throughout\
    \ the scenario in a similar way that the shown in \nFig. 9. The ego-vehicle starts\
    \ far away from the adversary and starts its journey. At second \n22 a pedestrian\
    \ that is in the sidewalk is detected, so s/he is tracked and forecasted in the\
    \ \nshort-term. After that, our prediction module intersects the ego-vehicle forecasted\
    \ trajectory \nand the pedestrian forecasted trajectory. If the Intersection over\
    \ Union (IoU) is greater than \na threshold (in this case, 0.01), a predicted\
    \ collision flag is activated and the low-level (reac-\ntive) control, which is\
    \ always running in the background, performs an emergency break until \nthe car\
    \ is stopped in front of the obstacle. Navigation is resumed once the obstacle\
    \ leaves the \ndriving lane. This temporal graph shows a good performance in unexpected\
    \ situations due \nto the high frequent execution of our reactive control.\nA\
    \ robustness analysis of our reactive control is depicted in Table 2. We show\
    \ a combi-\nnation of different “jumping distances” D in meters and different\
    \ nominal speed of the ego \nTable 2  Reactive control analysis on CARLA simulator.\
    \ NV: Nominal Velocity of the ego-vehicle (km/h). \nD: Minimum distance between\
    \ the adversary and the ego-vehicle to start its trajectory. Blue and Gray cells\
    \ \nindicate that a collision has not occurred and there has been, respectively\n\
    Fig. 12  Unexpected Pedestrian scenario. The pedestrian can be found behind the\
    \ bus stop, so the perception \nsystems detects it at the moment it is entering\
    \ the road\n4235\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\n\
    vehicle NV in km/h. Blue cells indicate no collision has taken place. The D parameter\
    \ rep-\nresents the initial Euclidean distance between the adversary pedestrian\
    \ and the ego-vehicle \nBird’s Eye View (BEV) centroid, as illustrated in Fig. 12.\
    \ This distance corresponds with \nthe initial condition of the pedestrian, placed\
    \ at the sidewalk, 1.5 m away from the road. \nIn a similar way to the pedestrian\
    \ crossing scenario, we always monitorize an additional \nsafety area (the sidewalk)\
    \ to detect the presence of VRUs and forecast them to ensure a \nsafety navigation.\
    \ In this case, since the bus marquee is totally occluding the VRU, s/he \nis\
    \ tracked and forecasted once jumps on the lane, since we still do not present\
    \ a system to \nforecast the trajectories of dynamic obstacles in occluded obstacles\
    \ [67, 50], which is a \nquite interesting state-of-the-art topic in the field\
    \ of AD. As expected, the faster the vehi-\ncle goes, the lower the distance at\
    \ which the reactive control detects for the first time the \npedestrian inside\
    \ the lane. This is due to the ego-vehicle has travelled a greater distance, \n\
    increasing the likelihood of colliding with the pedestrian, and the ego-vehicle\
    \ must per-\nform the emergency break in a shorter distance.\n7  Conclusions and future\
    \ works\nThis work presents the validation of our ROS-based fully-autonomous driving\
    \ architecture, \nfocusing in the decision-making layer, with CARLA, a hyper-realistic,\
    \ real-time, flexible \nand open-source simulator for autonomous vehicles. The\
    \ simulator and its bridge, respon-\nsible of communicating the CARLA environment\
    \ with our ROS-based architecture, on the \none hand, and the navigation architecture,\
    \ on the other hand, have been integrated in two \nDocker images, in order to\
    \ gain flexibility, portability and isolation. The decision-making is \nbased\
    \ on Hierarchical Interpreted Binary Petri Nets (HIBPN), and our perception is\
    \ based \non the fusion of GNSS, camera (including semantic segmentation) and\
    \ LiDAR. The vali-\ndation has consisted on the study of some traffic scenarios\
    \ inspired on the National High-\nway Safety Traffic Administration (NHTSA) typology,\
    \ and in particular in the CARLA \nAutonomous Driving Challenge (CADC), such as\
    \ Stop, Pedestrian Crossing, Adaptive \nCruise Control and Unexpected Pedestrian.\
    \ In particular, the work was extended from our \nprevious publication with several\
    \ interesting temporal graphs to analyze in a holistic way \nthe sequence of events\
    \ and its impact on the physical behavior of the vehicle. We hope \nthat our distributed\
    \ system can serve as a solid baseline on which others can build on to \nadvance\
    \ the state-of-the-art in validating fully-autonomous driving architectures using\
    \ \nvirtual testing. As future works, we will study a comparison among different\
    \ autonomous \ndriving architectures using our ongoing Autonomous Driving Benchmark\
    \ Development \nKit which will help us to compare in terms of score and quantitative\
    \ results different navi-\ngation pipelines, such as Baidu Apollo or Autoware\
    \ [53]. Moreover, sensor errors (such as \nlocalization and perception raw data)\
    \ and associated uncertainty will be thoroughly studied \nin future works as an\
    \ ablation study to observe the influence of these modifications in the \nfinal\
    \ validation of the architecture. Regarding these pseudo-sensors, we plan to use\
    \ tradi-\ntional semantic segmentation algorithms, or even panoptic segmentation\
    \ networks, instead \nof our traditional Precision-Tracking approach, since using\
    \ panoptic directly provides an \nidentifier for each object in every class, which\
    \ would facilitate the process of data asso-\nciation and creation of trackers.\
    \ In that sense, a Deep Learning based 3D Multi-Object \nTracking and Motion Prediction\
    \ algorithm, an enhanced version of our reactive control, an \nstate-of-the-art\
    \ Human-Machine Interface (HMI) and new behaviours in the behavioural \ndecision-making\
    \ layer will be implemented as well as the validation of the architecture in \n\
    4236\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\nmore challenging\
    \ situations to improve the reliability, effectiveness and robustness of our \n\
    system as a preliminary stage before implementing it in our real autonomous electric\
    \ car.\nFunding Open Access funding provided thanks to the CRUE-CSIC agreement\
    \ with Springer Nature. \nThis work has been funded in part from the Spanish MICINN/FEDER\
    \ through the Techs4AgeCar project \n(RTI2018-099263-B-C21) and from the RoboCity2030-DIH-CM\
    \ project (P2018/NMT- 4331), funded by \nProgramas de actividades I+D (CAM) and\
    \ cofunded by EU Structural Funds.\nOpen Access This article is licensed under\
    \ a Creative Commons Attribution 4.0 International License, \nwhich permits use,\
    \ sharing, adaptation, distribution and reproduction in any medium or format,\
    \ as long \nas you give appropriate credit to the original author(s) and the source,\
    \ provide a link to the Creative Com-\nmons licence, and indicate if changes were\
    \ made. The images or other third party material in this article \nare included\
    \ in the article’s Creative Commons licence, unless indicated otherwise in a credit\
    \ line to the \nmaterial. If material is not included in the article’s Creative\
    \ Commons licence and your intended use is not \npermitted by statutory regulation\
    \ or exceeds the permitted use, you will need to obtain permission directly \n\
    from the copyright holder. To view a copy of this licence, visit http:// creat\
    \ iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Althoff M, Urban S,\
    \ Koschi M (2018) Automatic conversion of road networks from opendrive to lane-\n\
    lets. In: 2018 IEEE International Conference on Service Operations and Logistics,\
    \ and Informatics \n(SOLI). IEEE, pp 157–162\n 2. Bast H, Delling D, Goldberg\
    \ A, Müller-Hannemann M, Pajor T, Sanders P, Wagner D, Werneck RF \n(2016) Route\
    \ planning in transportation networks. In: Algorithm engineering. Springer, pp\
    \ 19–80\n 3. Beeson P, O’Quin J, Gillan B, Nimmagadda T, Ristroph M, Li D, Stone\
    \ P (2008) Multiagent inter-\nactions in urban driving. Journal of Physical Agents\
    \ 2(1):15–29\n 4. Beis JS, Lowe DG (1997)  Shape indexing using approximate nearest-neighbour\
    \ search in high-\ndimensional spaces. In: Proceedings of IEEE computer society\
    \ conference on computer vision and \npattern recognition. IEEE, pp 1000–1006\n\
    \ 5. Bender P, Ziegler J, Stiller C (2014) Lanelets: Efficient map representation\
    \ for autonomous driving. \nIn: Intelligent Vehicles Symposium Proceedings, 2014\
    \ IEEE. IEEE, pp 420–425\n 6. Benekohal RF, Treiterer J (1988) Carsim: Car-following\
    \ model for simulation of traffic in normal \nand stop-and-go conditions. Transp\
    \ Res Rec 1194:99–111\n 7. Bojarski M, Del Testa D, Dworakowski D, Firner B, Flepp\
    \ B, Goyal P, Jackel LD, Monfort M, \nMuller U, Zhang J et al (2016) End to end\
    \ learning for self-driving cars. arXiv preprint arXiv: 1604. \n07316\n 8. Brandes\
    \ U (2001) A faster algorithm for betweenness centrality. J Math Sociol 25(2):163–177\n\
    \ 9. Caesar H, Bankiti V, Lang AH, Vora S, Liong VE, Xu Q, Krishnan A, Pan Y,\
    \ Baldan G,  Beijbom O \n(2020) nuscenes: A multimodal dataset for autonomous\
    \ driving. In : Proceedings of the IEEE/CVF \nconference on computer vision and\
    \ pattern recognition. pp 11621–11631\n 10. Chen C, Seff A, Kornhauser A, Xiao\
    \ J (2015) Deepdriving: Learning affordance for direct perception \nin autonomous\
    \ driving. In: Proceedings of the IEEE International Conference on Computer Vision. pp\
    \ \n2722–2730\n 11. Davies A (2016) Google’s self-driving car caused its first\
    \ crash. Wired\n 12. Del Egido J, Gómez-Huélamo C, Bergasa LM, Barea R, López-Guillén\
    \ E, Araluce J, Gutiérrez R, \nAntunes M (2020) 360 real-time 3d multi-object\
    \ detection and tracking for autonomous vehicle navi-\ngation. In: Workshop of\
    \ Physical Agents. Springer, pp 241–255\n 13. Dickmanns ED, Mysliwetz B, Christians\
    \ T (1990) An integrated spatio-temporal approach to auto-\nmatic visual guidance\
    \ of autonomous vehicles. IEEE Trans Syst Man Cybern 20(6):1273–1284\n 14. Dosovitskiy\
    \ A, Ros G, Codevilla F, López A, Koltun V (2017) Carla: An open urban driving\
    \ simula-\ntor. arXiv preprint arXiv: 1711. 03938\n 15. Dupuis M, Strobl M, Grezlikowski\
    \ H (2010) Opendrive 2010 and beyond–status and future of the de \nfacto standard\
    \ for the description of road networks. In: Proc. of the Driving Simulation Conference\
    \ \nEurope. pp 231–242\n4237\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\n 16. Fernandez C, Izquierdo R, Llorca DF, Sotelo MA (2015) A comparative\
    \ analysis of decision trees \nbased classifiers for road detection in urban environments.\
    \ In: 2015 IEEE 18th International Confer-\nence on Intelligent Transportation\
    \ Systems. IEEE, pp 719–724\n 17. Fernández JL, Sanz R, Benayas J, Diéguez AR\
    \ (2004) Improving collision avoidance for mobile robots \nin partially known\
    \ environments: the beam curvature method. Robot Auton Syst 46(4):205–219\n 18.\
    \ Fernández JL, Sanz R, Paz E, Alonso C (2008) Using hierarchical binary petri\
    \ nets to build robust \nmobile robot applications: Robograph. In: 2008 IEEE International\
    \ Conference on Robotics and Auto-\nmation. IEEE, pp 1372–1377\n 19. Geiger A,\
    \ Lenz P, Urtasun R (2012) Are we ready for autonomous driving? The Kitti vision\
    \ benchmark \nsuite. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE,\
    \ pp 3354–3361\n 20. Gold C, Körber M, Lechner D, Bengler K (2016) Taking over\
    \ control from highly automated vehicles \nin complex traffic situations: the\
    \ role of traffic density. Hum Factors 58(4):642–652\n 21. Golinska P, Hajdul\
    \ M (2012) European union policy for sustainable transport system: Challenges\
    \ and \nlimitations. In: Sustainable transport. Springer, pp 3–19\n 22. Gómez-Huelamo\
    \ C, Bergasa LM, Barea R, López-Guillén E, Arango F, Sánchez P (2019) Simulat-\n\
    ing use cases for the UAH autonomous electric car. In: 2019 IEEE Intelligent Transportation\
    \ Systems \nConference (ITSC). IEEE, pp 2305–2311\n 23. Gómez-Huélamo C, Del Egido\
    \ J, Bergasa LM, Barea R, López-Guillén E, Arango F, Araluce J, López \nJ (2020) Train\
    \ here, drive there: Simulating real-world use cases with fully-autonomous driving\
    \ archi-\ntecture in CARLA simulator. In: Workshop of Physical Agents. Springer,\
    \ pp 44–59\n 24. Gómez-Huélamo C, Del  Egido J, Bergasa LM, Barea R, Ocana M,\
    \ Arango F, Gutiérrez-Moreno \nR (2020) Real-time bird’s eye view multi-object\
    \ tracking system based on fast encoders for object \ndetection. In:  2020 IEEE\
    \ 23rd International Conference on Intelligent Transportation Systems \n(ITSC). IEEE,\
    \ pp 1–6\n 25. Guo K, Yan Y, Shi J, Guo R, Liu Y (2017) An investigation into\
    \ C-NCAP AEB system assessment \nprotocol. In: SAE Technical Paper. SAE International.\
    \ https:// doi. org/ 10. 4271/ 2017- 01- 2009\n 26. Haas J (2014) A history of\
    \ the unity game engine. Diss, Worcester Polytechnic Institute\n 27. Haklay M,\
    \ Weber P (2008) Openstreetmap: User-generated street maps. IEEE Pervasive Comput\
    \ \n7(4):12–18\n 28. Held D, Levinson J, Thrun S (2013)  Precision tracking with\
    \ sparse 3d and dense color 2d data. \nIn: 2013 IEEE International Conference\
    \ on Robotics and Automation. IEEE, pp 1138–1145\n 29. Ivanov A, Kristalniy S,\
    \ Popov N (2021) Russian national non-commercial vehicle safety rating system\
    \ \nruncap. In: IOP Conference Series: Materials Science and Engineering, vol. 1159.\
    \ IOP Publishing, p \n012088\n 30. Janai J, Guney F, Wulff J, Black MJ, Geiger\
    \ A (2017) Slow flow: Exploiting high-speed cameras for \naccurate and diverse\
    \ optical flow reference data. In: Proceedings of the IEEE Conference on Computer\
    \ \nVision and Pattern Recognition. pp 3597–3607\n 31. Jullien JM, Martel C, Vignollet\
    \ L, Wentland M (2009) Openscenario: a flexible integrated environment \nto develop\
    \ educational activities based on pedagogical scenarios. In: 2009 Ninth IEEE International\
    \ \nConference on Advanced Learning Technologies. IEEE, pp 509–513\n 32. Kalman\
    \ RE et  al (1960) A new approach to linear filtering and prediction problems.\
    \ J Basic Eng \n82(1):35–45\n 33. Kaur P, Taghavi S, Tian Z, Shi W (2021) A survey\
    \ on simulators for testing self-driving cars. arXiv \npreprint arXiv: 2101. 05337\n\
    \ 34. Ko NY, Simmons RG (1998) The lane-curvature method for local obstacle avoidance.\
    \ In: Proceedings. \n1998 IEEE/RSJ International Conference on Intelligent Robots\
    \ and Systems. Innovations in Theory, \nPractice and Applications (Cat. No. 98CH36190) vol. 3.\
    \ IEEE, pp 1615–1621\n 35. Koenig N, Howard A (2004) Design and use paradigms\
    \ for gazebo, an open-source multi-robot simu-\nlator. In Intelligent Robots and\
    \ Systems, 2004 (IROS 2004). Proceedings. 2004 IEEE/RSJ International \nConference\
    \ on, vol. 3. IEEE, pp 2149–2154\n 36. Krizhevsky A, Sutskever I, Hinton GE (2012)\
    \ Imagenet classification with deep convolutional neural \nnetworks. Adv Neural\
    \ Inf Proces Syst 25:1097–1105\n 37. Kurniawati H, Hsu D, Lee WS (2008) Sarsop:\
    \ Efficient point-based POMDP planning by approxi-\nmating optimally reachable\
    \ belief spaces. In: Robotics: Science and systems , vol.  2008.  Zurich, \nSwitzerland\n\
    \ 38. Lattarulo R, Pérez J, Dendaluce M (2017) A complete framework for developing\
    \ and testing auto-\nmated driving controllers. IFAC-PapersOnLine 50(1):258–263\n\
    \ 39. Levinson J, Askeland J, Becker J, Dolson J, Held D, Kammel S, Kolter JZ,\
    \ Langer D, Pink O, Pratt \nV et al (2011) Towards fully autonomous driving: Systems\
    \ and algorithms. In: 2011 IEEE Intelligent \nVehicles Symposium (IV). IEEE, pp\
    \ 163–168\n4238\nMultimedia Tools and Applications (2022) 81:4213–4240\n1 3\n\
    \ 40. Liu R, Wang J, Zhang B (2020) High definition map for automated driving:\
    \ Overview and analysis. \nJ Navig 73(2):324–341\n 41. López J, Sánchez-Vilariño\
    \ P, Sanz R, Paz E (2020) Implementing autonomous driving behaviors \nusing a\
    \ message driven petri net framework. Sensors 20(2):449\n 42. Matthaeia R, Reschkaa\
    \ A, Riekena J, Dierkesa F, Ulbricha S, Winkleb T, Maurera M (2015) Auton-\nomous\
    \ driving: Technical, legal and social aspects\n 43. Merat N, Jamson AH, Lai FC,\
    \ Daly M, Carsten OM (2014) Transition to manual: Driver behaviour \nwhen resuming\
    \ control from a highly automated vehicle. Transportation Research Part F: Traffic\
    \ \nPsychology and Behaviour 27:274–282\n 44. Merkel D (2014) Docker: Lightweight\
    \ Linux containers for consistent development and deploy-\nment. Linux Journal\
    \ 2014(239):2\n 45. Michal DS, Etzkorn L (2011) A comparison of player/stage/gazebo\
    \ and Microsoft robotics devel-\noper studio. In: Proceedings of the 49th Annual\
    \ Southeast Regional Conference. ACM, pp 60–66\n 46. Montemerlo M, Roy N, Thrun\
    \ S (2003) Perspectives on standardization in mobile robot program-\nming: The\
    \ Carnegie Mellon navigation (Carmen) toolkit. In: Proceedings 2003 IEEE/RSJ Interna-\n\
    tional Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453),\
    \ vol. 3. \nIEEE, pp 2436–2441\n 47. Murciego E, Huélamo CG, Barea R, Bergasa\
    \ LM, Romera E, Arango JF, Tradacete M, Sáez Á \n(2018) Topological road mapping\
    \ for autonomous driving applications. In: Workshop of Physical \nAgents. Springer,\
    \ pp 257–270\n 48. Najm WG, Smith JD, Yanagisawa M et al (2007) Pre-crash scenario\
    \ typology for crash avoidance \nresearch. Tech. rep., United States. National\
    \ Highway Traffic Safety Administration\n 49. Paden B, Čáp M, Yong SZ, Yershov\
    \ D, Frazzoli E (2016) A survey of motion planning and control \ntechniques for\
    \ self-driving urban vehicles. IEEE Transactions on Intelligent Vehicles 1(1):33–55\n\
    \ 50. Park JS, Manocha, D (2020) HMPO: Human motion prediction in occluded environments\
    \ for safe \nmotion planning. arXiv preprint arXiv: 2006. 00424\n 51. Quigley\
    \ M, Conley K, Gerkey B, Faust J, Foote T, Leibs J, Wheeler R, Ng AY (2009) Ros:\
    \ An \nopen-source robot operating system. In: ICRA workshop on open source software,\
    \ vol. 3. Kobe, \nJapan, p 5\n 52. Rajamani R (2011) Vehicle dynamics and control.\
    \ Springer Science & Business Media\n 53. Raju VM, Gupta V, Lomate S (2019) Performance\
    \ of open autonomous vehicle platforms: Auto-\nware and Apollo. In:  2019 IEEE\
    \ 5th International Conference for Convergence in Technology \n(I2CT). IEEE, pp\
    \ 1–5\n 54. Robotics C (2015) V-rep user manual. http:// www. coppe liaro botics.\
    \ com/ helpF iles/. Ultimo acesso \n13, 04 \n 55. Rong G, Shin BH, Tabatabaee\
    \ H, Lu Q, Lemke S, Možeiko M, Boise E, Uhm G, Gerow M, Mehta \nS et al (2020) LGSVL\
    \ simulator: A high fidelity simulator for autonomous driving. In: 2020 IEEE \n\
    23rd International Conference on Intelligent Transportation Systems (ITSC). IEEE,\
    \ pp 1–6\n 56. Sanders A (2016) An introduction to unreal engine 4. AK Peters/CRC\
    \ Press\n 57. Schöner H (2017) The role of simulation in development and testing\
    \ of autonomous vehicles. In: \nDriving Simulation Conference, Stuttgart\n 58.\
    \ Shah S, Dey D, Lovett C, Kapoor A (2018) Airsim: High-fidelity visual and physical\
    \ simulation for \nautonomous vehicles. In: Field and service robotics. Springer,\
    \ pp 621–635\n 59. Singh S (2015) Critical reasons for crashes investigated in\
    \ the national motor vehicle crash causa-\ntion survey. Tech. rep.\n 60. Takács Á,\
    \ Drexler DA, Galambos P, Rudas IJ, Haidegger T (2018) Assessment and standardization\
    \ \nof autonomous vehicles. In: 2018 IEEE 22nd International Conference on Intelligent\
    \ Engineering \nSystems (INES). pp 000185–000192\n 61. Taxonomy S (2016) Definitions\
    \ for terms related to driving automation systems for on-road motor \nvehicles\
    \ (j3016). Technical report, Society for Automotive Engineering, Tech. rep.\n\
    \ 62. Tian Y, Pei K, Jana S, Ray B (2018) Deeptest: Automated testing of deep-neural-network-driven\
    \ \nautonomous cars. In: Proceedings of the 40th International Conference on Software\
    \ Engineer-\ning. pp 303–314\n 63. Tideman M, Van Noort M (2013) A simulation\
    \ tool suite for developing connected vehicle systems. \nIn: 2013 IEEE Intelligent\
    \ Vehicles Symposium (IV). IEEE, pp 713–718\n 64. Tradacete M, Sáez Á, Arango\
    \ JF, Huélamo CG, Revenga P, Barea R, López-Guillén E, Bergasa LM \n(2018) Positioning\
    \ system for an electric autonomous vehicle based on the fusion of multi-GNSS\
    \ RTK \nand odometry by using an extented Kalman filter. In: Workshop of Physical\
    \ Agents. Springer, pp 16–30\n4239\nMultimedia Tools and Applications (2022) 81:4213–4240\n\
    1 3\n 65. Urmson C, Anhalt J, Bagnell D, Baker C, Bittner R, Clark M, Dolan J,\
    \ Duggins D, Galatali T, Geyer \nC et al (2008) Autonomous driving in urban environments:\
    \ Boss and the urban challenge. J Field Rob \n25(8):425–466\n 66. van Ratingen\
    \ M, Williams A, Lie A, Seeck A, Castaing P, Kolke R, Adriaenssens G, Miller A\
    \ (2016) The \neuropean new car assessment programme: A historical review. Chin\
    \ J Traumatol 19(2):63–69. https:// doi. \norg/ 10. 1016/j. cjtee. 2015. 11. 016. https://\
    \ www. scien cedir ect. com/ scien ce/ artic le/ pii/ S1008 12751 60001 10\n 67.\
    \ Wang L, Ye H, Wang Q, Gao Y, Xu C, Gao F (2020) Learning-based 3D occupancy\
    \ prediction for \nautonomous navigation in occluded environments. arXiv preprint\
    \ arXiv: 2011. 03981\n 68. Xu H, Gao Y, Yu F, Darrell T (2017) End-to-end learning\
    \ of driving models from large-scale video \ndatasets. In: Proceedings of the\
    \ IEEE Conference on Computer Vision and Pattern Recognition. pp \n2174–2182\n\
    \ 69. Yurtsever E, Lambert J, Carballo A, Takeda K (2020) A survey of autonomous\
    \ driving: Common prac-\ntices and emerging technologies. IEEE Access 8:58443–58469\n\
    \ 70. Zhan W, Sun L, Wang D, Shi H, Clausse A, Naumann M, Kummerle J, Konigshof\
    \ H, Stiller C, \nde La Fortelle A et al (2019) Interaction dataset: An international,\
    \ adversarial and cooperative motion \ndataset in interactive driving scenarios\
    \ with semantic maps. arXiv preprint arXiv: 1910. 03088\nPublisher’s Note Springer\
    \ Nature remains neutral with regard to jurisdictional claims in published maps\
    \ and \ninstitutional affiliations.\nAuthors and Affiliations\nCarlos Gómez‑Huélamo1\
    \  · Javier Del Egido1 · Luis M. Bergasa1 · Rafael Barea1 · \nElena López‑Guillén1 ·\
    \ Felipe Arango1 · Javier Araluce1 · Joaquín López2\n \nJavier Del Egido \n \n\
    javier.egido@edu.uah.es\n \nLuis M. Bergasa \n \nluism.bergasa@uah.es\n \nRafael\
    \ Barea \n \nrafael.barea@uah.es\n \nElena López-Guillén \n \nelena.lopezg@uah.es\n\
    \ \nFelipe Arango \n \njuanfelipe.arango@edu.uah.es\n \nJavier Araluce \n \njavier.araluce@edu.uah.es\n\
    \ \nJoaquín López \n \njoaquin@uvigo.es\n1 \nElectronics Department, University\
    \ of Alcalá (UAH), Madrid, Spain\n2 \nDepartment of Systems Engineering and Automation,\
    \ University of Vigo (UVIGO), Pontevedra, \nSpain\n4240\n"
  inline_citation: '>'
  journal: Multimedia tools and applications
  limitations: '>'
  pdf_link: https://link.springer.com/content/pdf/10.1007/s11042-021-11681-7.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Train here, drive there: ROS based end-to-end autonomous-driving pipeline
    validation in CARLA simulator using the NHTSA typology'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/sose.2016.39
  analysis: '>'
  authors:
  - Matthias Geiger
  - Simon Harrer
  - Jörg Lenhard
  - Guido Wirtz
  citation_count: 10
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2016 IEEE Symposium on Servic...
    On the Evolution of BPMN 2.0 Support and Implementation Publisher: IEEE Cite This
    PDF Matthias Geiger; Simon Harrer; Jörg Lenhard; Guido Wirtz All Authors 11 Cites
    in Papers 526 Full Text Views Abstract Document Sections I. Introduction II. Related
    Work III. Analysis of BPMN 2.0 Implementers IV. Assessment of Current BPMN Support
    V. Evolution of BPMN Support Show Full Outline Authors Figures References Citations
    Keywords Metrics Footnotes Abstract: The Business Process Model and Notation 2.0
    (BPMN) standard has been hailed as a major step in business process modeling and
    automation. Recently, it has also been accepted as an ISO standard. The expectation
    is that vendors of business process management systems (BPMS) will switch to the
    new standard and natively support its execution in process engines. This paper
    presents an analysis of the current state and evolution of BPMN 2.0 support and
    implementation. We investigate how current BPMN 2.0 implementers deal with the
    standard, showing that native BPMN 2.0 execution still is an exception. Most BPMS
    do not support the execution format, despite claiming to be BPMN 2.0 compliant.
    Furthermore, building on past work, we evaluate three process engines that do
    provide native BPMN support and examine the evolution of their degree of support
    over a three-year period. This lets us delimit the areas of the standard that
    are considered important by the implementers. Since there is hardly an increase
    in supported features over the past three years, it seems that the implementation
    of the standard is more or less seen as finished by vendors and it is unlikely
    that features which are not available by now will be implemented in the future.
    Published in: 2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)
    Date of Conference: 29 March 2016 - 02 April 2016 Date Added to IEEE Xplore: 19
    May 2016 Electronic ISBN:978-1-5090-2253-3 DOI: 10.1109/SOSE.2016.39 Publisher:
    IEEE Conference Location: Oxford, UK SECTION I. Introduction Business process
    management (BPM) is a broad discipline with influence on other areas of computing
    research, such as service composition [1]. BPM is strongly tailored to the modeling
    of organizational processes in dedicated modeling languages and the subsequent
    implementation of process models in executable software. Among the many languages
    for process modeling that are available today (examples are discussed in [2]),
    the Business Model and Notation (BPMN) [3] has emerged as a widely accepted candidate.
    Its importance and potential for consolidating the market of process standards
    is emphasized by its recent acceptance as an ISO standard [4] 1. With the publication
    of BPMN 2.0 in January 2011, support for the native execution of BPMN process
    models and a standardized serialization format has been added to the standard.
    The intention underlying this addition is to mitigate the gap between modeled
    processes on the one hand and the execution of processes on the other hand [5].
    When bridging this gap by translating a model into executable software, a variety
    of problems arise that are the topic of various publications, e.g., [6]–[9], regardless
    of the concrete modeling language used. Especially for industry-size process models,
    this translation step is non-trivial and can lead to a deviation of the actual
    implementation of the model from the desired execution semantics captured in the
    model. The direct execution of a process model intends to minimize the distance
    between desired and actual behavior. This avoids the necessity for a translation
    step in the first place and has the potential to preempt the deviation of a model
    from its implementation. The specification of the execution semantics of a process
    model in a standards document, as achieved with BPMN 2.0 [4], is a first step
    towards the goal of direct execution. As a next step, it is necessary that the
    execution semantics are correctly and completely implemented by vendors of the
    standard. In the case of BPMN, the second step faces two major obstacles: Firstly,
    the implementation of the specified execution semantics needs to be feasible,
    which unfortunately cannot be guaranteed by the purely theoretical and informal
    discussion of the execution semantics in the BPMN 2.0 standard [10]. Secondly,
    the vendors need to actually implement the same semantics and not interpretations
    or customizations thereof. Since there is no certification authority for BPMN,
    any vendor can claim full implementation conformance and support for the standard
    without proof of this claim. Furthermore, it may well be possible that only a
    subset of BPMN is valuable from a practitioner''s point of view, as indicated
    in [11]. As a result, vendors might want to implement only a limited part of the
    standard. The paper intents to cast light on the latter issue. In prior work [12],
    we analyzed three well-known BPMN 2.0 engines and investigated the degree of standard
    conformance they provide. In this paper, we build upon this work with a comprehensive
    study of BPMN 2.0 implementers in the current market. We analyze the products
    of a broad list of vendors that claim to implement BPMN 2.0. This shows that a
    vast majority of current self-acclaimed BPMN 2.0 implementations is limited to
    visualization and does not actually implement the execution semantics of BPMN
    2.0. Thereafter, we extend the standard conformance analysis from [12] with a
    more sophisticated set of tests, covering a higher degree of the executable part
    of the standard. Moreover, we also analyze whether common feature combinations,
    defined by the so-called workflow patterns [13], are supported. We compare multiple
    revisions of several engines that support the native execution of BPMN 2.0 over
    a three-year period. This lets us see how BPMN 2.0 support has evolved since its
    adoption as an ISO standard. The current development indicates that the implementation
    of BPMN 2.0 is consolidating at a level that excludes a large part of the actual
    execution semantics of the standard. The remainder of this paper is structured
    as follows: The next section outlines related work on BPMN 2.0 execution and the
    evaluation and benchmarking of process engines. In Section III, we present and
    discuss data of 45 vendors that claim to implement the BPMN 2.0 standard. Thereafter,
    in Section IV, we detail our procedure for evaluating standard support in a BPMN
    engine and show the results of a detailed assessment of the supported feature
    set of three products. The results of our longitudinal study, showing the evolution
    of feature support over the course of the last three years, round off our findings
    in Section V. Finally, Section VI summarizes the main findings of the study and
    points to future work. SECTION II. Related Work Work related to this paper separates
    in three major areas: First, we discuss work that concerns the native serialization
    format and execution of BPMN 2.0 process models and problems experienced during
    this task. Second, we outline approaches for evaluating and benchmarking BPMN
    2.0 engines. Third, we present approaches for building reproducible benchmarks,
    which is what we aim for in this paper. A. Serialization Fromat and Native Execution
    of BPMN 2.0 The set of features that is to be supported by a BPMN 2.0 implementation
    is vast and so is the description of their execution semantics. As discussed in
    [10], this description is inaccurate and unambiguous in various places. As a result,
    the goal of fully standard-conformant BPMN 2.0 implementations with identical
    behavior is elusive, according to [10]. The same aspect is confirmed by [14],
    whose authors discuss underspecifications for a particular language construct,
    the service task, which are likely to result in differing behavior in different
    implementations. Also [15] targets this aspect and provides a comprehensive discussion
    of the execution semantics of BPMN 2.0 [4] and their shortfalls. To improve the
    situation, [15] refines the specification using abstract state machines. Finally,
    also [16] discusses issues in the specification of the serialization format of
    BPMN and provides a preliminary analysis of the BPMN support in modeling tools.
    When it comes to the usage of the BPMN 2.0 execution semantics and the serialization
    format, [5] presents first results based on a survey. Already at the time shortly
    after the publication of BPMN 2.0, 40% of the survey respondents claimed to use
    the new native serialization format. It can be expected that this number has increased
    since the publication of [5]. B. Evaluating and Benchmarking BPMN 2.0 Engines
    When it comes to the evaluation of BPMN 2.0 process engines, our prior work on
    standard conformance [12] is obviously related. Since we build on this approach,
    it is described in more detail in the following sections. A short case study that
    analyses several capabilities of a set of BPMN 2.0 engines is presented in [17].
    At the time, practically none of the engines under focus was able to correctly
    import models in the native BPMN serialization format. This is an interesting
    result which is confirmed by our analysis of BPMN implementers in Sect. III. It
    contradicts the already mentioned results from [5]. There seems to be a discrepancy
    between modeling tools which are able to create and consume standard compliant
    BPMN models and execution engines which are unable to work with those models.
    Apart from the evaluation of standard conformance, also other aspects of BPMN
    2.0 engines are investigated. A notable approach for evaluating the performance
    characteristics of BPMN engines is [18]. Another comparative study is [19] which
    evaluates the three BPMN engines activiti, jBPM and camunda BPM with regard to
    their general architecture and their extensibility. However, the focus in [19]
    is on the integration of semantic techniques such as ontologies to improve the
    quality of the processes to be executed. C. Computational Reproducibility Reproducible
    research, especially computational reproducibility, is key in today''s science
    [20]. Many scientific results are obtained with the help of extensive software
    tooling, without which it is not possible to reproduce or to check the correctness
    of results. To enable computational reproducibility, [20] suggests to make the
    documented, tested, and modular code on which scientific computations are based,
    as well as all related material and the results freely available, preferably on
    a version-control system. But even following these guidelines, researchers trying
    to reproduce the experiment still phase “significant barriers” [21], p. 72], e.g.,
    imprecise documentation which is hard to follow or steep learning curves for the
    tools used in experiments. To increase the reproducibility of scientific computations,
    [21], [22] suggest to use Docker2 as the basis for computational environments,
    since it lowers several of the aforementioned barriers. In Docker, one can create
    lightweight Linux containers based on Docker images. These images can be shared,
    reused, archived, put under version control, and deployed on different platforms
    [21]. What is more, such an image can be built automatically by passing a Dockerfile
    (i.e., a list of commands) to Docker itself, making the step to create the software
    environment for an experiment reproducible for third parties. Here, we perform
    a benchmark for obtaining core results. To enable reproducibility of this benchmark,
    it is executed based on Docker. SECTION III. Analysis of BPMN 2.0 Implementers
    To gather insights on the state of BPMN implementation, it is necessary to investigate
    the current market of systems that implement the standard. To this end, we evaluate
    public listings of BPMN implementers and judge their state of implementation when
    it comes to the execution of BPMN 2.0 processes in this section. Only a small
    subset of the existing implementations is mature enough to justify an evaluation
    of the degree of standard conformance they exhibit. This section describes the
    selection of such mature engines from the actual market.Sect. IV analyses the
    degree of standard conformance of the selected engines. At a first glance, the
    market of BPM suites and engines supporting BPMN seems to be rather active. A
    list of BPMN implementers3 contains 75 implementations at the time of writing.
    These implementers can be divided into diagramming tools, Business Process Management
    Systems (BPMS), Business Process Analysis (BPA) systems, and Case Management systems.
    With regard to BPMN execution semantics conformance, especially the fully fledged
    BPMS suites are of relevance. We identified 31 candidates which stated directly
    or implicitly that they also target the execution of BPMN processes. Another “list
    of active BPMN 2.0 Engines” at Wikipedia4, which has been created and is mainly
    maintained by members of the BenchFlow project5, currently lists 24 entries. Here,
    we compared and consolidated both lists, arriving at 45 BPM suites and engines,
    which claim to conform to the BPMN standard. These systems are the focus of the
    following sections. A. Requirements for BPMN Engines The standard document [4]
    is rather strict regarding Process Execution Conformance: “The tool claiming BPMN
    Execution Conformance type MUST fully support and interpret the operational semantics
    and Activity life-cycle specified in sub clause 14.2.2. […] Conformant implementations
    MUST fully support and interpret the underlying metamodel.” [4], p. 10] Following
    these statements, there are two main aspects relevant for BPMN engines and BPMS:
    Firstly, all language constructs defined in the BPMN meta-model must be supported.
    Secondly, for each language construct the underlying execution semantics must
    be implemented correctly. The only exceptions from this are a few “non-operational
    elements” defined in [4]. The purpose of the standardization of BPMN is to achieve
    portability and interoperability between different modeling tools and engines.
    A common workflow for implementing processes is to first model “basic” business
    process models in a BPMN modeling tool. Subsequently, these models are enriched
    with execution details specific to the BPMS used for execution. A strong indicator
    for the importance of this use case is the ongoing effort put into interchange
    of models by various vendors of modeling tools and engines in the BPMN Model Interchange
    Working Group (MIWG)6. Therefore, we require BPMN engines and BPMS to support
    the import of the standardized XSD-based serialization format as a third important
    requirement. To summarize these requirements, a conforming BPMN implementation
    must: support the usage of all constructs defined in the standard, implement the
    behavior of the constructs as specified, and be able to consume the standardized
    serialization format. Only products that fulfill these requirements can be evaluated
    to support process execution conformance, as defined in the standard. B. Requirements
    Evaluation for BPMS We evaluated the requirements stated in the previous section
    for all 45 BPMN implementations mentioned before using a structured methodology.
    First, we checked the publicly available information on the websites of the different
    vendors. If a version of the BPMN implementation was available for free usage
    or evaluation purposes, we downloaded this version. For each available product,
    we checked whether existing integrated modeling tools support the BPMN meta-model
    at least partially, i.e., whether at least some BPMN elements with their defined
    attributes are available. If this was the case, or if direct deployment of BPMN
    files was possible, we tried to import and deploy an existing BPMN process model.
    Only products that pass all these tests can, in principle, be classified as an
    implementation of the execution semantics of BPMN. The results for the 45 tested
    products, depicted in Fig 1, are rather surprising: For 25 out of the 45 products,
    evaluation was not possible for various reasons. The development and distribution
    of five products has been discontinued. For ten products no suitable public information
    is available (too little information to meaningfully evaluate BPMN support, or
    information is not available in English). Two products offer some modeling and
    execution functionality but neither the shapes used nor the used serialization
    format correspond to the BPMN standard. Hence, it is unclear, why these tools
    are listed as BPMN implementers to begin with. For eight products, a closer analysis
    is not possible in the context of this paper, due to licensing restrictions. For
    instance, some vendors explicitly prohibit the benchmarking and evaluation of
    their products, which forces us to omit them from a closer investigation. Figure
    1. Results of the product analysis Show All The usage of graphical BPMN shapes
    to model processes is supported by eleven BPMS, but the definition of attributes
    does not conform to the standard and/or the import, and as a result, usage of
    standard compliant models is not possible. Therefore, of the original 45 products,
    only 9 support at least parts of the BPMN 2.0 meta-model, can import standard
    compliant models and provide enough information for a detailed assessment. Of
    those remaining BPMS, three engines require the usage of extensions that are not
    covered by the standard, or restrict the usage of essential features such as IDs
    and task types. As a result, the models consumed or produced by these systems
    are substantially different from the BPMN 2.0 standard, although they share some
    syntactical elements. Considering all these aspects, only six systems justify
    a closer analysis regarding the degree to which they actually implement the BPMN
    standard. The remaining products are able to digest process models conforming
    to the BPMN standard, but for three of these, model import, deployment or the
    process execution is only possible via manual operations using a graphical interface.
    This severely hampers computational reproducibility and, thus, is prohibitive
    for our approach of checking the conformance, which will be described comprehensively
    in the following section: We assess the conformance of the whole spectrum of BPMN.
    To achieve this, each BPMN language construct has to be tested in each possible
    configuration. Furthermore, we want to guarantee isolation of tests by providing
    a fresh process engine instance for each test run, and by running each test various
    times to check reproducibility of results. Those aspects are only feasible if
    automated deployment and execution is possible. Therefore, only three engines
    can be assessed here: These engines are activiti, camunda BPM, and jBPM 7. SECTION
    IV. Assessment of Current BPMN Support In this section, we benchmark the BPMN
    support of the three engines selected in the previous section. We use the newest
    version that was available on 2015-10-30, namely, activiti 5.18.0, jBPM 6.3.0,
    and camunda BPM 7.3.0, thereby updating the versions assessed in [12]. The setup
    of the benchmark is explained in Section IV-A, followed by the test suite, which
    consists of both, tests for native BPMN support and tests for workflow control-flow
    pattern support, in Section IV-B. The results of our assessment are given in Section
    IV-C. A. Benchmark Setup The benchmark is conducted with an improved version of
    the BPEL/BPMN Engine Test System (betsy). In previous work, betsy has been used
    to benchmark BPEL [23] as well as BPMN engines [12]. For this study, we have improved
    and extended betsy by 1) adding the ability to test several versions of activiti,
    jBPM, and camunda BPM, 2) enabling betsy to distinguish real concurrency and pseudo-parallelism
    during process execution, 3) adding a test suite for evaluating workflow control-flow
    pattern support in BPMN, and 4) porting betsy to Linux to enable execution with
    Docker. Betsy has its own domain specific test language to express conformance
    tests and their results [12], [23]. A test corresponds to an executable process
    that uses a specific BPMN 2.0 language feature, e.g., a ScriptTask or an ExclusiveGateway.
    Each test has at least one test case, consisting of one or more test steps. These
    steps define inputs to the test, in the form of string or integer variables. Test
    inputs are injected into the process during execution as properties. Moreover,
    test steps define expected test outputs in form of test assertions. An assertion
    specifies execution traces of a process model and is used to verify the correctness
    of a test after its execution. Execution traces are written during execution by
    the means of BPMN 2.0 ScriptTasks. By comparing the expected execution trace with
    the actual one, it is possible to determine whether a test case was executed successfully.
    The testing of real concurrency within a process has been implemented by adding
    the ability to store labeled timestamps into the log trace. By checking if time
    stamps overlap, it is possible to detect concurrent execution. The test results
    state for each test case, a) if the respective process could be deployed on a
    particular engine, and b) if the test case was executed successfully. A language
    feature is considered to be supported by an engine given the process can be deployed
    and all test cases are executed successfully. On starting a benchmark, betsy executes
    a test workflow for each test and each engine under test. First, the native BPMN
    format of a test is adapted to the engine under test. For instance, a concrete
    scripting language needs to be set (this aspect is not treated by the BPMN standard).
    To ensure that each test is executed in isolation, a fresh instance of the engine
    under test is installed and started. Next, the process containing the language
    feature under test is deployed on the running engine. The deployment may require
    the creation of a deployment package, depending on the engine under test. Next,
    the test itself is executed by triggering all test cases with their test steps
    subsequently. Following this, execution traces are gathered from the log file
    and compared to the expected traces, thereby determining test success or failure.
    Last, the engine under test is shut down. We used the following steps to ensure
    that the benchmark conducted as part of this work can be considered reproducible:
    Betsy itself including its test suites is open source and freely available on
    a version control system8. This holds true for the engines under test as well.
    In addition, the functional correctness and quality of betsy is ensured using
    a continuous integration pipeline. The results of our experiments can be found
    in version control as well9. The benchmark is done within a fixed Docker-based
    environment10 for betsy, which required the porting of betsy to Linux. The pre-built
    image on which the experiment was conducted is also available11. B. Test Suites
    for BPMN Conformance Assessment There are two test suites for assessing the conformance
    of engines to the BPMN standard: The first test suite bundles tests for the constructs
    and features described in the BPMN specification [4]. The second focuses on more
    complex constructs that are frequently needed in process models, captured in the
    form of workflow control-flow patterns [13]. 1) Feature Tests for Language Constructs
    The first test suite has already been used in [12] and covered 27 different language
    constructs (such as ExclusiveGateway) which have been tested by 70 feature tests
    divided in five different groups. Using feature tests we are able to determine
    whether all possible configuration possibilities (e.g, usage of a default SequenceFlow
    for ExclusiveGateways) are supported by the engines under test. Although the 27
    language constructs represent the majority of the BPMN constructs, we added further
    constructs to tackle several limitations mentioned in [12]: First, we added a
    sixth construct group, data, which contains basic tests for the language constructs
    DataObject and Property. Second, we added further tests for inter-process communication
    performed with the help of Signals, MessageEvents, SendTasks and ReceiveTasks.
    Moreover, we now also test MultipleEvents, the usage of EventDefintionRefs, AdHocSubProcesses
    and different startQuantity and completionQuantity settings of activities (subsumed
    as TokenCardinality). Apart from those newly added language constructs we refined
    and added feature tests for various other language constructs. The resulting test
    suite for checking the native BPMN support consists of 38 language constructs
    tested by 113 feature tests. This is a 40% increase in the amount of language
    constructs covered. An overview over all covered constructs and the number of
    associated feature tests is shown as part of the results in Table I. 2) Workflow
    Control-Flow Patterns Standard conformance of all language features is necessary
    to execute arbitrary standard-conformant processes on a given engine. Any process
    that can be expressed in the standard, should be executable on an engine. Closely
    related to this aspect is the expressive power of the language dialect supported
    by an engine. Expressive power is captured by the ease with which structures that
    are frequently needed in a system can be expressed. The easier commonly needed
    structures can be built, the more expressive a language or system is. In the case
    of process languages, expressive power is frequently assessed by the means of
    workflow patterns [13]. Although the relevance of the concrete patterns is not
    undisputed [10], they are frequently applied in related studies. Additionally,
    using workflow patterns for assessing expressive power eases the comparison of
    this work to others. In this paper, we use the original 20 workflow control-flow
    patterns from [13], and no extensions or derivations thereof, since these are
    most widely known. We built upon the pattern-based analysis for BPMN 1.0 presented
    in [24]. Most of the pattern implementations described in the paper can directly
    be applied to BPMN 2.0. In the rare cases, where a modification of a pattern implementation
    was necessary, we followed the rationale of [24] to provide a solution. Table
    II lists the patterns sorted by pattern category, along with the highest degree
    of pattern support that can be achieved for BPMN 2.0. Due to page constraints,
    we cannot describe every pattern here, but refer the interested reader to [13],
    [24]. Pattern support is rated in a trivalent rating of + (direct support), +/−,
    (partial support), or − (no direct support). Again, we follow [24] in the judgement
    of the degree of support that is possible in BPMN. It can be seen in Table II
    that two patterns (MI without A Priori Run-Time Knowledge and Milestone) cannot
    be directly implemented in BPMN. We were unable to find workarounds based on the
    extended vocabulary of BPMN 2.0 that could compensate for this. Thus, we exclude
    these patterns from further discussion. For the remaining patterns, we implemented
    at least one test case, according to the structures from [24], that led to pattern
    support. In case at least one engine failed the initially built test, we implemented
    an additional test that led to the same or a reduced support rating, to see if
    the engine supports an alternate structure. The reasoning for this is that an
    engine is considered to support a pattern if it implements at least one solution
    that grants support. Full support of all possible, equivalent solutions is not
    required. Finally, if an engine supports none of the solutions presented by [24]
    (fails all related tests), we consider it as not supporting a pattern. C. Benchmark
    Results The results are obtained by executing the benchmark setup, described in
    Section IV-A, for the three engines, selected in Section III, using the test suites
    from the previous section. The next section discusses the results for the test
    suite addressing native BPMN support, followed by the discussion of workflow control-flow
    pattern support. 1) Feature Tests for Language Constructs The results of the evaluation
    of native BPMN support are shown in Table I. Activiti supports 51 out of the 113
    features, camunda BPM supports 55, and jBPM provides the highest amount of support
    with 59 features. Translated to percentage values, support ranges from 45% up
    to 52%, being approx. half of the tested features. It is interesting that all
    three engines support roughly the same amount of features, differing by at most
    eight features. In comparison to previous work [12], we added 43 new feature tests,
    but the support of activiti has only risen by 12 features, of camunda BPM by 11
    features, and of jBPM by 15 features. Hence, the data reveals that at most 35%
    of the added feature tests are supported. Activiti and camunda BPM differ in their
    support of four features. Previously [12], camunda BPM supported all the features
    activiti that supported, but this is no longer the case. Activiti successfully
    detects two errors related to Multiple-Events, which are missed by camunda BPM
    at deployment time. However, taking all other differences into account, camunda
    BPM is better than activiti. Camunda BPM has higher support for Compensation-,
    Signal- and TimerEvents, as well as for detecting invalid loop conditions. The
    third engine, jBPM, shows strength with its support for twelve additional event
    types compared to camunda BPM, but fails to support conditional SequenceFlows
    and the MultiInstanceTask. Apart from these differences, jBPM behaves similar
    to the other two engines. Looking at the supported language constructs by group,
    we can see that the data group is supported completely. Within the gateways group,
    all three engines support EventBased-, Exclusive-, Inclusive-, and MixedGateway
    combinations. However, all fail to support ComplexGateways. Parallel-Gateways
    are only partially supported, since none of the engines actually supports concurrency,
    but only pseudo-parallelism. The language constructs of the basics group, namely,
    Lanes, Participants and SequenceFlows are supported by every engine. Conditional
    SequenceFlows are supported partially by activiti and camunda BPM, whereas jBPM
    has no support for this construct. Regarding the activities group, three features,
    namely, ReceiveTask, SendTask and AdHocSubProcess, are unsupported. Nevertheless,
    every engine supports SubProcesses and Transactions. Regarding the LoopTask, only
    one out of six different tests is supported by every engine. In contrast, support
    for MultiInstanceTasks is relatively good, with five out of eight successful tests
    by activiti and camunda BPM, but it is completely unsupported by jBPM. The usage
    of tokens is barely supported as only activiti and camunda BPM pass one out of
    four tests. In previous work, all feature tests within the errors group were supported.
    As opposed to this, most of the newly added error tests fail, except for two checks
    for the usage of multiple events and invalid loop conditions by activiti and jBPM.
    Within the events group, MessageEvents, Conditional-Events, and EventDefinitionRefs
    are unsupported by every engine, whereas widespread support can be diagnosed for
    Error- and TerminateEvents. The engine jBPM supports all EscalationEvents, which
    are unsupported by the other two engines. Furthermore, it supports all SignalEvents,
    which are supported with five and six successful tests out of nine by activiti
    and camunda BPM, respectively. Approximately half of the tests for TimerEvents
    and CompensationEvents are passed by the engines. Failures of the TimerEvents
    in jBPM and activiti are mainly caused by using timers for start events. Cancel-
    and LinkEvents are unsupported by jBPM and activiti, respectively, but supported
    by the camunda BPM. Half of the tests for MultipleEvents are passed by jBPM only.
    Looking at the number of language features that were not supported, we can distinguish
    between two different cases: rejection at deployment and failure at runtime. Activiti
    rejects 21, camunda BPM 24, and jBPM 23 valid language features at deployment.
    Of the unsupported language features, activiti accepts 41, camunda BPM 34, and
    jBPM 31 language features at deployment, but at runtime the corresponding tests
    reveal wrong execution semantics. This is a real issue for users of the engines
    as they might encounter unexpected behavior in their own deployed processes, which
    is even worse in case it remains undetected. 2) Workflow Control-Flow Patterns
    The support of work-flow control-flow patterns is shown in Table II. Overall,
    every engine supports at least 13 out of the 20 original workflow patterns. However,
    as mentioned before, the patterns WCP-15 and WCP-18 are excluded from the analysis,
    since they cannot be directly supported in BPMN to begin with. Eleven patterns
    are supported as expected by all three engines. The three patterns WCP-9, WCP-12
    and WCP-17, for which partial support is possible in BPMN, are unsupported by
    all engines under test. For four patterns the results vary from engine to engine,
    but are always supported by two engines. WCP-13, WCP-14 and WCP-19 are directly
    supported by activiti and camunda BPM, but not supported by jBPM. This reveals
    a major problem with the jBPM engine. The pattern WCP-10 is correctly executed
    on jBPM and activiti, but not on camunda BPM. This is also the only difference
    of the pattern support between camunda BPM and activiti. Table I Native BPMN support
    Looking at the pattern groups, it is obvious that all basic control-flow patterns
    are supported. Both, structural and cancellation patterns, are also supported
    on almost each engine, except for one failure in each of the groups. Within the
    group of advanced branching and synchronization patterns, only one pattern is
    completely unsupported, but the remaining three are directly supported by every
    engine. The same holds true for the state-based patterns. The remaining group
    of multiple instances patterns is least supported, as WCP-12 is completely unsupported
    by all engines and both WCP-13 and WCP-14 are unsupported by jBPM. When looking
    at the reasons for failures in pattern support, the most common issue is that
    the BPMN process containing the pattern is not deployable, because a construct
    used therein (such as an AdHocSubProcess) is not supported by an engine. Table
    II Workflow control-flow pattern support Because of this, jBPM supports none of
    the multiple instances patterns due to missing support for the MultilnstanceTask
    and the WCP-19 Cancel Activity due to missing support for the CancelEvent. The
    pattern WCP-10 Arbitrary Cycles should in general be supported by camunda BPM
    as all used language features are supported, but the test fails with a runtime
    exception caused by the composition of these language features. To sum up, pattern
    support of the three engines can be considered high and balanced, as it is ranging
    between 13 up to 15 out of 18 patterns. It is interesting that activiti supports
    14 out of the 18 patterns with only 51 out of 113 language features. In contrast,
    the engine jBPM supports 59 out of the 113 language features, but only supports
    13 patterns. Hence, only a moderate degree of support for language features is
    required to implement a large set of the patterns. What is more, the lack of pattern
    support by jBPM is mainly caused by rejecting processes containing the pattern
    already at deployment time. Out of the ten failed tests, nine pattern tests were
    being rejected during deployment. For activiti and jBPM, only one out of six and
    three out of seven are rejected upon deployment. SECTION V. Evolution of BPMN
    Support As seen in the previous section, there are substantial limitations in
    the current implementation of the BPMN standard. These limitations could be attributed
    to the fact that implementation is still in progress and support of the standard
    will increase with time. An alternative interpretation is that implementations
    are limited to the features of the standard that are relevant in practice and
    the remaining part of the standard will likely never be implemented. The latter
    interpretation is supported by studies that show that the features used in process
    models by practitioners are limited to a modest part of the standard [11], [25].
    Here, we try to address this aspect from the direction of process engines. If,
    on the one hand, the feature set supported by engines constantly increases, this
    is an indication that the implementation of the standard is still in progress.
    On the other hand, if the feature set supported by engines stays rather constant
    over time, this is an indication that the implementation of the standard has stopped
    in practice and language features that have not been supported by now are considered
    as irrelevant. To address this aspect, we investigate the evolution of BPMN support
    over time in this section. We benchmark different versions of the engines discussed
    in Section IV that have been published over a three year period, using the exact
    same methodology as before. An overview of the engine versions we consider is
    given in Section V-A. The results of the benchmarks, describing the evolution
    of native BPMN support and workflow control-flow pattern support, are shown in
    Section V-B and Section V-C, respectively, and are summed up in Section V-D. The
    results support the second hypothesis, i.e., that the implementation of the BPMN
    standard is concluded in practice. A. Compared Engine Versions Section IV details
    the benchmarks of the latest versions of the engines activiti, jBPM, and camunda
    BPM. Here, we add benchmarks for three additional prior versions of each said
    engine. The concrete version numbers of the engines are shown in Table III, along
    with their release date and the number of days each version has served as latest
    stable release. The engine vendors apply semantic versioning for setting release
    numbers12. Version numbers are encoded in the form MAJOR.MINOR.PATCH and numbers
    are incremented depending on the nature of the changes made in a revision. According
    to semantic versioning, an increment of the patch level should only reflect bug
    fixes, the minor level is incremented when adding functionality which is backwards
    compatible, and the major version is only changed when introducing API-breaking
    changes. Within the last three years, several minor or patch level updates have
    been released for all engines, but no major release has been made. Here, we focus
    on the last four minor version updates and always use the highest available patch
    level13. Each of the releases are at least three month and on average six months
    apart. Therefore, we can expect to see signs of progress in BPMN feature implementation,
    given the implementation has not been concluded yet. Table III The BPMN engines
    under test Each engine has a different release strategy, as can be seen in the
    release dates and the number of days between each release in Table III. Whereas
    activiti has a relatively short release cycle, both jBPM and camunda have longer
    release cycles. As we only use the last four minor version releases, we do not
    cover the same amount of time for each engine. This is not a problem, since we
    are interested in increasing feature support between releases, and not within
    a fixed period of time. B. Evolution of Native BPMN Support Since page space is
    limited, we cannot present the complete benchmark results for all engine versions.
    Instead, an overview of the changes in native BPMN support is shown in Table IV.
    Feature regressions, i.e., features that were supported in the earliest version,
    stopped working in an intermediate version, and resumed functioning in a subsequent
    version, are excluded from this overview. Table IV Evolution of native BPMN support
    First of all, it becomes obvious that there is very little change in the support
    of BPMN features. Feature support for all three engines increased by the number
    of merely eleven additional features during the past three years. Especially when
    considering the immense number of potential features and the moderate degree to
    which engines implement the standard so far, as described in Section IV, this
    number is very low. Regarding activiti, we do not see any evolution of native
    BPMN support. In the groups of activities, basics, data, and errors, we cannot
    diagnose any changes at all. There are minor improvements in the gateways and
    events group, but the changes are minimal. For instance, one test shifts from
    being undeployable to a partial support of some variants of a language feature,
    which does not justify to be judged as an increase in feature support. The only
    factual change that does happen is a regression in activiti 5.16.3 which is fixed
    in activiti 5.17.0 for a single event type. In contrast to activiti, there are
    more differences between the tested versions of camunda BPM. However, the support
    for the basics, data, errors, and gateway groups stays the same for all versions.
    With the change from camunda BPM 7.0.0 to 7.1.0, four new features within the
    events group are supported and with the latest version 7.3.0, camunda BPM gains
    support for two additional features in the events group. A negligible change,
    similar to the changes for activiti, can be found in the activities group, between
    camunda BPM 7.0.0 and 7.1.0. Overall, six new BPMN events are supported in the
    latest version in contrast to the earliest version we have tested. Looking at
    jBPM, we can see that all the versions have the same support for activities, basics,
    data, and the errors group. Upgrading jBPM 6.0.1 to 6.1.0 brought support for
    five new features, namely four new events and one gateway. Upgrading from jBPM
    6.1.0 to 6.2.0, however, results in five regression errors as again four events
    and one gateway option fails. With jBPM 6.3.0, these failures are fixed again.
    To sum up, over the time of the last four minor versions, jBPM introduced five
    new features in total. C. Evolution of Workflow Control-Flow Patterns Support
    An overview of the evolution of workflow control-flow pattern support is shown
    in Table V. As it can be seen, there are even fewer changes and these are not
    limited to an increase in feature support, but instead also include a decrease.
    Table V Evolution of pattern support For activiti, we have no visible improvements
    in pattern support over the last four minor releases. This can be expected, since
    there is also no increase in native language features. The only change is that
    processes which contain complex gateways are no longer marked as not deployable,
    affecting the variants of WCP06 and WCP09 that make use of complex gateways. This
    change happens between the versions 5.16.3 and 5.17.0. However, this has no effect
    on pattern support, as said pattern variants do not work as expected at runtime
    anyway. Regarding jBPM, we can see a single improvement in the variant of WCP06
    that is implemented with an inclusive gateway. This is unsupported in jBPM 6.0.1
    but supported in every subsequent version we have tested. Looking at camunda BPM,
    the support for workflow patterns even declines. While WCP10 is still supported
    in both versions 7.0.0 and 7.1.0, versions 7.2.0 and 7.3.0 lack support of this
    pattern. D. Summary Overall, it seems that the evolution of feature support in
    BPMN engines has stopped. Improvements to feature support are confined to BPMN
    events. as shown by increases for camunda BPM and jBPM. Fig. 2 visualizes this
    development. It can be seen that feature support is hardly rising. Interestingly,
    activiti, which supports the least number of BPMN events of the three engines,
    has not improved in that area within the last four releases. Figure 2. Evolution
    of native BPMN support for events Show All This halt in feature improvements of
    these three major BPMN engines can be seen as a sign that the implementation of
    the BPMN standard has concluded at the current level. It seems that neither the
    implementers of these engines view the remaining features of the standard as valuable
    for their implementations, nor are they pressured by their customer base to implement
    further features. Otherwise, we would see a stronger increase in new features.
    Since all three engines are successful in the market, we can conclude that the
    feature set they provide is sufficient for BPM-systems in practice. SECTION VI.
    Conclusion and Future Work In this paper, we have presented a comprehensive analysis
    of the current state and the evolution of the implementation of the BPMN 2.0 standard.
    The results of this analysis can be summarized as follows: Evidence suggests that
    the implementation of the standard in practice has concluded at the current level.
    Remaining features of the standard that are not yet supported seem to be irrelevant
    in practice. The first part of this study, presented in Section III, was a comprehensive
    analysis of products that claim to implement the BPMN 2.0 standard. Of the 45
    products that classify as BPMS or process engines, only three remain that are
    able to consume the required serialization format and permit a closer evaluation.
    In Section IV, we performed such a closer evaluation of feature support provided
    by the most recent versions of these engines. We analyzed conformance to the BPMN
    standard and, on top of that, support for common workflow control-flow patterns.
    Following this analysis, we extended the study to three additional prior versions
    of the engines that have been published over the last three years in Section V.
    We could show that hardly any features have been added over this time. Hence,
    we can conclude that neither the implementers, nor the users of the standard are
    interested in the remaining features of the standard. Multiple directions of future
    work follow from this. Firstly, it would be desirable to investigate further BPMN
    engines. We hope to obtain access to further products, but this depends on the
    availability of academic licenses. Secondly, we work on extending the coverage
    of our test suite. Currently, this test suite does not cover all aspects of the
    standard and a higher level of coverage would be desirable. With respect to the
    results presented in this paper, it can be expected that a higher level of feature
    coverage leads to a diagnosis of even more insufficiencies in contemporary implementations.
    The refinement of the standard can be seen as a third direction. Currently, the
    BPMN standard is quite extensive and studies that analyse its usage in practice,
    such as this one, consistently show that many of the features are simply not needed.
    A reduction of the feature set of BPMN to a core subset could help in many ways,
    e.g., by easing the implementation of the standard and reducing the complexity
    for its users. Similar approaches exist for related standards [26] and could be
    valuable for BPMN as well. ACKNOWLEDGMENTS We would like to thank the students
    of the University of Bamberg who contributed to the benchmarking system used in
    this paper during software development projects. Thanks go to Adrian Bazyli, Annalena
    Bentele, Christian Kremitzl, Lea-Louisa Maaß, Frederik Müller and Severin Sobetzko.
    Authors Figures References Citations Keywords Metrics Footnotes More Like This
    Targeting an Audience of Robots: Search Engines and the Marketing of Technical
    Communication Business Websites IEEE Transactions on Professional Communication
    Published: 2009 A Rule-Based Knowledge Discovery Engine Embedded Semantic Graph
    Knowledge Repository for Retail Business 2016 International Conference on Advanced
    Cloud and Big Data (CBD) Published: 2016 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: On the Evolution of BPMN 2.0 Support and Implementation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.sysarc.2018.05.007
  analysis: '>'
  authors:
  - Marisol García-Valls
  - Abhishek Dubey
  - Vicente Botti
  citation_count: 62
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Computing paradigms: definitions
    and evolution 3. Social dispersed computing 4. Enabling social dispersed computing
    5. Challenges in social dispersed computing 6. Discussion and conclusions References
    Vitae Show full outline Cited by (63) Figures (4) Journal of Systems Architecture
    Volume 91, November 2018, Pages 83-102 Introducing the new paradigm of Social
    Dispersed Computing: Applications, Technologies and Challenges Author links open
    overlay panel Marisol García-Valls a, Abhishek Dubey b, Vicent Botti c Show more
    Add to Mendeley Share Cite https://doi.org/10.1016/j.sysarc.2018.05.007 Get rights
    and content Under a Creative Commons license open access Abstract If last decade
    viewed computational services as a utilitythen surely this decade has transformed
    computation into a commodity. Computation is now progressively integrated into
    the physical networks in a seamless way that enables cyber-physical systems (CPS)
    and the Internet of Things (IoT) meet their latency requirements. Similar to the
    concept of “platform as a service” or “software as a service”, both cloudlets
    and fog computing have found their own use cases. Edge devices (that we call end
    or user devices for disambiguation) play the role of personal computers, dedicated
    to a user and to a set of correlated applications. In this new scenario, the boundaries
    between the network node, the sensor, and the actuator are blurring, driven primarily
    by the computation power of IoT nodes like single board computers and the smartphones.
    The bigger data generated in this type of networks needs clever, scalable, and
    possibly decentralized computing solutions that can scale independently as required.
    Any node can be seen as part of a graph, with the capacity to serve as a computing
    or network router node, or both. Complex applications can possibly be distributed
    over this graph or network of nodes to improve the overall performance like the
    amount of data processed over time. In this paper, we identify this new computing
    paradigm that we call Social Dispersed Computing, analyzing key themes in it that
    includes a new outlook on its relation to agent based applications. We architect
    this new paradigm by providing supportive application examples that include next
    generation electrical energy distribution networks, next generation mobility services
    for transportation, and applications for distributed analysis and identification
    of non-recurring traffic congestion in cities. The paper analyzes the existing
    computing paradigms (e.g., cloud, fog, edge, mobile edge, social, etc.), solving
    the ambiguity of their definitions; and analyzes and discusses the relevant foundational
    software technologies, the remaining challenges, and research opportunities. Previous
    article in issue Next article in issue Keywords Social dispersed computingIoTFog
    ComputingCloud ComputingDispersed ComputingSocial ComputingEdge ComputingDistributed
    computingCyber physical systemsReal timeMiddlewareVirtualizationContainersMicroservicesDistributed
    transactionsBlockchainMulti agent systemsDistributed coordinationComplex event
    processingNetworking 1. Introduction Social computing applications are smart applications,
    where the results received by the end users or the performance that they experience
    is affected by the other users using the same application. A classical example
    of this kind is traffic routing, implemented by many commercial mobility planning
    solutions like Waze and Google. The routes provided to the end users depend upon
    the interaction that other users in the systems have had with the application.
    An effective route planning solution will be proactive in the sense that it will
    analyze the demands being made by users and will use the dynamic demand model
    for effectively distributing vehicle and people across space, time, and modes
    of transportation, improving the efficiency of the mobility system and leading
    to a reduction of congestion. However, due to its nature, this computing application
    requires large scale real-time data ingestion, analysis, and optimization. We
    call such applications social computing applications. With the burst of the cloud
    computing paradigm, systems requiring intensive computations over large data volumes
    have relied on the usage of shared data centers to which they transfer their data
    for processing. This is a powerful scheme for application scenarios that benefit
    from deep processing and data availability, but it brings in non negligible problems
    to meet the time requirements of time sensitive social computing applications.
    While not necessarily real-time in the strict sense, such applications have built
    in penalty (user aversion) if they are not responsive; they must be low-latency;
    however, the traditional cloud computing architecture is problematic in a number
    of application domains that are latency sensitive. Precisely, the delay incurred
    by data propagation across the backhaul is not suited to the needs of applications
    that require (near) real-time response or high quality of service guarantees.
    Backhaul data handling latency is severe in the unpredictable occasions where
    the network throughput is limited. Furthermore, a community deploying such smart
    applications often finds it difficult to scale the system to the cloud due to
    economic constraints. To alleviate these situations, engineers have looked around
    towards “what is available”, i.e., to leverage the computing power of the available
    near by resources, leading to a profound discussion on the opportunistic usage
    of the computing resources dispersed in the community. Out of this new scenario,
    we have identified this new computing approach that we call “Social Dispersed
    Computing”. This is a powerful paradigm that can significantly improve the performance
    experienced by applications in what concerns latency and available throughput
    that will, in turn, have an indirect impact on other measures such as the energy
    consumption. Unlike cloud computing, resource scalability comes from the participatory
    nature of the system, i.e., having a larger number of users. The key driver is
    the social benefit behind the achieved collaboration and the great value obtained
    from the aggregation of the individual information. Users have to perceive hardly
    no entry barriers to use these applications; barrier elimination is done by fulfilling
    the technical requirements of these applications such as providing low cost computation
    resources, reliability, and data privacy guarantees, over a low overhead management
    structure that achieves low latency in service provisioning. Enabling social dispersed
    computing. The next computing generation is one in which the computing platform
    and the social applications will be tightly integrated. For example, sharing computing
    resources can be used as incentive for participation. Moreover, providing the
    users with the capability of deciding where their computations will run for security
    and privacy concerns will likely be a major factor for enrolling in application
    usage. To enable this, the corresponding transformations are already happening
    in the communications and persistent storage mechanisms. For example, Software
    Defined Network [83] addresses the required mechanisms to create a flexible overlay
    network over dispersed resources. The concept of decentralized distributed ledgers
    like Ethereum [5] and other similar ones enable immutable event chronology across
    computing resources. New concepts such as the inter-planetary files system (IPFS)
    [29] extend blockchains and the concept of distributed file systems to provide
    a shared, decentralized, and world-wide persistent information store. In this
    paper, we claim that social dispersed computing systems require fog infrastructures
    to take a predominant role. Fog infrastructures will support the mobility of the
    users, enabling them to offload heavy tasks such as those that run machine learning
    services to more powerful nodes in their vicinity. However, the great push of
    relatively very novel computation paradigms such as fog, edge, cloud, social,
    and dispersed computing (among other computing paradigms) has resulted in a non-negligible
    level of terminology confusion in the community. In different research contributions,
    the reader can find these terms being used differently. This paper aims at shedding
    some light by clarifying the meanings, and defining the boundaries (where possible)
    of these paradigms, guided by their goals and application-level motivation. Paper
    outline. This paper is structured as follows. Section 2 defines a number of computing
    paradigms that are simultaneously used nowadays; some of these paradigms are very
    recent and still the scientific community has not fully agreed on what they actually
    are; we clarify the paradigms and introduce the concept of social dispersed computing.
    Section 3 describes the concept of social dispersed computing and illustrates
    it through a set of application scenarios in domains such as energy, social routing
    and distributed traffic congestion analysis. Section 4 presents the enabling technologies
    that will allow the development of social dispersed applications. To do this,
    a selected set of computational approaches are presented, followed by a selection
    of supporting software tools. Section 5 compiles the main challenges for the design
    and development of social dispersed applications. Finally, Section 6 draws the
    conclusions presented as the opportunities for research. 2. Computing paradigms:
    definitions and evolution Distributed computing systems date back decades ago
    enabled by the first communication schemes for remote machines. Fig. 1 shows a
    general view since the 90''s; a time where a number of important software and
    hardware developments came together, and hardware and software schemes started
    to become more sophisticated and powerful. This led to subsequent productive decades,
    resulting in the introduction of a number of new and refined concepts and terms,
    sometimes over short periods of time. Download : Download high-res image (419KB)
    Download : Download full-size image Fig. 1. Evolution of computing: a general
    view on the evolution of personal devices over the years. Especially through the
    last decade, a number of keywords have appeared that imply different computing
    paradigms such as cloud, mobile cloud, fog, or edge, among others. However, the
    rapid proliferation of contributions on these paradigms, even prior to the real
    consolidation of a wide accepted definition for some of them, has introduced some
    confusion on their definitions. For example, the definition of edge computing
    diverges across a number of works. In [136], edge computing is defined as “any
    computing and network resources along the path between data sources and cloud
    data centers”; whereas [145] defines edge computing as a paradigm belonging to
    the sphere of the pure network infrastructure that connects the user devices (that
    it refers to as “edge nodes”) to the cloud. This last vision of edge computing
    is also shared by [51] although it refers to the user devices as “end nodes” in
    a more consistent manner. All these concepts have led us to the point where we
    are ready to realize the potential of social computing using resources from either
    the cloud, the fog, or locally dispersed computing resources. Nevertheless, it
    is first important to clarify the terminology and, for this reason, we initially
    provide a comprehensive definition of key computing paradigms present in modern
    literature, with the aim to establish a common understanding. These definitions
    are based on the most accepted significations of the research community. The goal
    is to draw a clean separation (wherever possible) among the different computing
    paradigms also explaining their evolution, motivation, and purpose. 2.1. Cloud
    computing Cloud computing (CC) is a service model where computing services that
    are available remotely allow users to access applications, data, and physical
    computation resources over a network, on demand, through access devices that can
    be highly heterogeneous. In cloud computing [59], resources are rented in an on
    demand and pay-per-use fashion from cloud providers. Just as a huge hardware machine,
    cloud computing data centers deliver an infrastructure, platform, and software
    applications as services that are available to consumers. This facilitates offloading
    of highly consuming tasks to cloud servers. The National Institute of Standards
    and Technology (NIST) is responsible for developing standards and guidelines for
    providing security to all assets. [108] provides an insight into the cloud computing
    infrastructure which consists of three service models, four deployment models,
    and five essential characteristics which are: on-demand self-service, broad network
    access, resource pooling, rapid elasticity, and measured service. A cloud service
    model represents the packaging of IT resources required by the consumers as a
    service that is provided by the cloud vendor. The three cloud service models are:
    • Software as a service (SaaS): The consumers are granted the capability to run
    the applications of the provider, but they have no control over the cloud infrastructures
    like operating system, servers, or storage. • Platform as a service (PaaS): The
    consumers have the capability to deploy either own or acquired applications to
    the cloud. The consumer does not have any control on the cloud infrastructure,
    but has control over the deployed application. • Infrastructure as a service (IaaS):
    The consumers can use the applications provided on the cloud without the need
    to download the application to the consumer’s computer. Consumers can manage the
    underlying infrastructure at the cloud such as virtual machines, the operating
    systems, and other resources. Additionally, with the wide increase of data processing
    and storage in the cloud, larger data volumes circulate over the network, increasing
    their exposure to third parties and attackers. This brings in the need for data
    security and privacy mechanisms. Data security in particular is a vital challenge
    that has been analyzed in [114]. In the paper, the authors have addressed the
    security problem from the perspective of different stakeholders like cloud providers,
    service providers, and consumers. It also summarizes the security issues in each
    one of the service delivery models of IaaS, PaaS, and SaaS, where some of the
    identified problems are responsibility of the cloud vendor while the other issues
    are that of the consumers. The authors also identified the various holes in the
    security loop of the cloud computing model, suggesting fixes that would make the
    whole model more secure. Apart from security there are other obstacles in using
    and implementing cloud services. For example, while the main the advantage of
    the large amount of data storage and analytics capabilities of the cloud, some
    of its disadvantages (e.g., unreliable data latency, immobility and lack of location
    awareness) are important drawbacks in some domains; and this has made way to other
    technologies like mobile cloud computing or fog computing. 2.2. Mobile cloud computing
    The proliferation of mobile personal devices led to Mobile Cloud Computing (MCC).
    MCC appeared as a natural evolution and enhancement of cloud computing with the
    goal of offering specific services to mobile users with powerful computational
    and storage resources. Task offloading strategies are one of the most studied
    problems in this domain because mobile devices have strict resource limitations
    if compared to cloud servers. As explained in [56], MCC combines mobile computing,
    mobile Internet, and cloud computing for providing task offloading. The literature
    gives different definitions for MCC as explained in [77]. Infrastructure based
    MCC refers to a model that uses the cloud data centers hardware to serve mobile
    users; and ad-hoc MCC defines the concept of mobile cloud as made up of nearby
    mobile nodes acting as a resource cloud that grants access to the Internet (including
    other cloud services) for other mobile users. Using the nearby mobile devices
    has several advantages like the possibility of using a faster LAN network that
    is comparable to the available servers interconnection inside a cloud data center.
    Also, MCC is “cloudlet” based (a rather parallel concept that is defined below).
    The paper [20] provides an overview of MCC along with its evolution from cloud
    computing, and its advantages and disadvantages, as well as its applications.
    Some of the mentioned noteworthy advantages of MCC are flexibility, storage, cost
    efficiency, mobility and availability, scalability, and performance. Some discussed
    disadvantages are security, privacy, compliance, compatibility, and dependency.
    The authors also enumerate a few open challenges faced by MCC which are low bandwidth
    and quality of service (QoS) parameters like congestion, network disconnection,
    and interoperability. These non-negligible security and privacy challenges of
    MCC arise from the integration of mobile devices with cloud computing. Along with
    the similar security concerns of cloud computing, some new issues on security
    and privacy arise in MCC as there is a wireless medium for transferring data between
    the mobile device and the cloud. In [113], the authors identify the main security
    and privacy challenges as data security, virtualization security, partitioning
    and offloading security, mobile cloud application security, mobile device security,
    data privacy, location privacy and identity privacy; and solutions to each of
    these questions have also been discussed by citing prior literature work. Given
    the increase in the number of mobile users and applications, security and privacy
    requirements are vital for MCC; and addressing them will likely increase the computation
    and communication overhead that will have to be dealt with by the users. With
    the integration of mobile devices and cloud computing, MCC overcomes the limitation
    of immobility and lack of location awareness in cloud computing; also, it provides
    an attractive and convenient technology for moving all the data-rich mobile computation
    to the cloud. However advantageous this idea of MCC may look, there are still
    open issues like the associated high network latency and power consumption of
    data transmission from the mobile devices to the cloud, which are not handled
    by MCC. 2.3. Cloudlet Cloudlet is defined as a small scale cloud data center formed
    by resource-rich and trusted computing devices near the vicinity of mobile users
    that can be used to process data jointly over a local area network connection.
    It is a major technological enabler for MCC, defined at the convergence of MCC
    and cloud computing. It defines a virtualized architecture [132] as a computational
    resource accessible by mobile users at range, i.e., within their physical vicinity.
    This has the objective of empowering mobile devices providing them the capabilities
    to access computationally intensive services that could not be run by their own
    limited resources. Examples of such as services are speech recognition, processing
    of natural language, machine learning, or augmented reality. As discussed in [132],
    even with the increased computation and storage capacity, mobile devices are not
    able to process rich media content locally with their own resources. MCC aimed
    at solving the above issue by offloading all the data from the mobile device to
    the cloud for computation. However, MCC could not provide a feasible solution
    for applications with tight latency requirements (i.e., real time applications),
    and this led to the concept of cloudlet. Additionally, as discussed in [147],
    mobile users can utilize the cloudlets virtual machines to run the required software
    applications closer to the mobile devices that aims to solve the latency issues
    by moving the virtual machine closer to the mobile devices. However, there is
    a notable drawback of mobile users being dependent on network service providers
    to deploy cloudlets into the LAN network for the mobile devices to utilize them.
    The authors in [147] present the architecture of cloudlets where the applications
    are managed at the component level and evaluate it by implementing it for a use
    case of augmented reality classify the architecture into two categories: ad hoc
    cloudlet and elastic cloudlet. The evolution of the cloudlet concept is further
    discussed in [132], [147], placing the concept between cloud computing and MCC.
    In cloudlet, the jobs of the mobile users are not transmitted all the way to the
    cloud but to a nearby cloudlet; this tends to reduce the power consumption of
    mobile devices and also the transmission delay. Thus, at this point, cloudlet
    makes an advantageous evolution from MCC. In order to reduce the power consumption
    of mobile devices and the network communication latency, [74] merges the concepts
    of MCC and cloudlets. This proposal has an advantage as it can support real time
    processing on the cloudlet; other non-real time data processing and storage can
    be run on the cloud. These claims for reduced power consumption and transmission
    delay are properly supported by their analysis and evaluation. 2.4. Internet of
    Things Internet of Things (IoT, that includes IoE – Internet of Everything) is
    an extension of the classical sensor network paradigm, providing support for large
    scale sensor data aggregation, cloud based data processing, and decision support
    systems. The concept of pervasive computing emerged before IoT to refer to the
    provisioning of computation anytime, anywhere. One of the novelties of this concept
    was the fact that computation devices could be personal devices, among others.
    This idea was also expressed and referred to as ambient intelligence or everywhere.
    IoT is a similar concept except that in IoT the emphasis is placed on the physical
    object. The range of possible devices in IoT was enlarged as compared to those
    considered in pervasive computing. As technology improved, the IoT vision was
    to flood the market with computation nodes that were deeply immersed in the environment:
    from sensors to small embedded computers that could be connected to the Internet
    as direct and uniquely addressable end points. The primary evolution in the IoT
    paradigm compared to the sensor networks is the support for complex event processing
    (CEP) [46] which is typically executed on the integrated cloud platform. CEP engines
    can be run over the intermediate IoT node resources in the network, and queries
    can be placed on the incoming continuous data streams from the end devices1 like
    sensors and RFID tags. As compared to the previous paradigm where end nodes sent
    data streams to the cloud that would process them, performing such processing
    on the available IoT nodes could reduce the latency and bandwidth requirements
    of the IoT network. An overview of CEP for IoT and its applications is provided
    in [41], consisting of a deep insight into the distributed CEP architecture based
    on the client-server model which can be realized on the IoT devices to perform
    queries like filtering, passing data, and placing windows on the incoming data.
    Some of the advantages of using CEP over IoT are: (1) distributed CEP in the network
    will balance the workload better; (2) ease of CEP engine deployment; and (3) the
    data traffic can be significantly reduced by removing unwanted data using queries
    of filtering, data passing, etc. Additionally, there are other works like [63]
    that address the idea of distributing the data analytics between the IoT nodes
    and the cloud. For example, they use genetic algorithms to optimize the query
    mapping to the end devices. While the integration of IoT and CEP is a well studied
    concept, the challenges of security, privacy, adaptability, scalability, and interoperability
    still remain. To deal with the complexity and heterogeneity of IoT environments,
    a number of high level flexible layered architectures have been contributed. Heterogeneity
    has led to different sets of requirements, with different needs for complexity
    and varied performance, which has affected the design of architectures. This has
    led to a scenario in which solutions have not yet converged to a reference model,
    which causes issues of interoperability across systems [84]. 2.5. Cyber-Physical
    Systems Cyber-Physical Systems (CPS) are networked systems in which the computational
    (cyber) part is tightly integrated with the physical components. That is, the
    computational components sense the state of the system and environment and then
    provide continuous feedback for controlling the system and actuating on the environment.
    Physical components include energy sources, transmission and distribution lines,
    loads, and control devices. Cyber components include energy management systems
    (EMS), supervisory control and data acquisition (SCADA) systems, and embedded
    implementations of control algorithms. The interplay of computational and physical
    systems yields new capabilities. The network is a key component in cyber-physical
    systems as it provides the backplane that guarantees timely transmission of the
    information (from the physical to the computational world) and of the commands
    (from the computation to the physical world). Traditionally, these systems had
    been mostly self-contained in the sense that they have included all needed computational
    parts with little interaction with external elements. For example, the traditional
    architecture for the Smart Grid transfers all SCADA data to centralized utility
    servers [142]. An evolution of the design of such systems arrived with the rising
    of the cloud computing paradigm as many of the analytics functions were deployed
    in the cloud [139]. However, even with the availability of on-demand resources
    in the cloud, the critical CPS often are unable to transfer the time-critical
    control tasks to the cloud due to communication latencies [36], [38]. This centralized
    SCADA architecture is changing with recent developments like Fog Computing [64],
    [153], which have advertised the use of dual purpose sensing and computation nodes
    (that are end nodes) that are closer to the physical phenomenon that is observed
    or analyzed. For example, the SCALE-2 [30] platform provides the capability to
    run air-quality monitoring sensors, whereas the Paradrop architecture [148] provides
    the capability to run containerized applications in network routers. Nowadays,
    cyber physical systems research has to consider the highly dynamic nature of CPS;
    it is not possible to perform static system design as the full operation conditions
    are unknown at design time. For this reason, a number of contributions are appearing
    that support the online verification of these systems as they face new situations
    that require them to adapt; examples of these contributions are [92] and [126].
    As a direct consequence of the evolution of the computing paradigm from “central
    data-centers” to “shared cloud computing resources” to “distributed edge (meaning
    end) computing resources plus shared cloud resources”, critical CPS like smart
    grids can distribute the intelligence further down into the network, away from
    the centralized utility servers. For example, this capability provides us with
    the means to build energy management applications of the future that are both
    distributed and coordinated, with heavy reliance on communication and coordination
    among local sensing and control algorithms, while also obeying strategic energy
    management decisions made on a higher level of the control hierarchy. We discuss
    this concept of providing “scalable” and “extensible” computation services near
    the physical process (i.e., fog computing) next. 2.6. Fog computing Fog computing
    (FC) was introduced to solve the problem of having billions of IoT devices and
    cyber physical systems that cannot operate by simply having connectivity to servers
    in the cloud; and instead, computations are pushed closer to these end nodes and
    devices. Unlike the traditional computation model, the fog computing model, pioneered
    by the Open Fog Consortium, suggests the use of shared computation servers, similar
    to the vision of cloudlet described by [132]. However, the key difference lies
    in the software as a service pioneered by fog computing. For example, instead
    of just providing the computation resources, a fog computing machine often provides
    machine learning stack as a service [7]. Also, a difference with respect to cloud
    is that fog computing supports user mobility. Nevertheless, fog and cloud are
    not independent paradigms as in a fog computing environment there is the need
    for interacting with the cloud to achieve coherent data management. As mentioned
    in [154], the unresolved issues in cloud computing of latency and mobility have
    been overcome by providing services and elastic resources at the end of the information
    chain, close to the sensors. [156] defines fog computing and discusses the characteristics
    related to it like fog networking, quality of service, interfacing and programming
    model, computation offloading, accounting, billing and monitoring, provisioning
    and resource management, and security and privacy. Along with providing insights
    into the issues related to fog computing, it also mentions paradigmatic applications
    like augmented reality (AR) and real-time video analytics, content delivery and
    caching, and mobile big data analytics which will promote fog computing. All of
    the computation paradigms discussed have big security and privacy challenges.
    Some of the main security issues faced by fog computing [116] are trust, authentication,
    secure communication, privacy at the end user''s node, and malicious insider attacks.
    A number of papers have contributed to identifying the security and privacy concerns
    of fog computing, and similarly a number of solutions for each of the above stated
    security challenges have also been analyzed in the literature. Similar to [116],
    also [141] mentions different security issues in fog computing. All smart appliances
    (e.g., fog computing nodes like smart meters) have an IP address, and here, authentication
    problems are a big threat. A malicious attacker may try to hack the device and
    tamper the data associated to it; e.g., in case of a smart meter this may imply
    providing false meter reading. Similar to authentication problems, man-in-the-middle
    attack is also a prominent type of attack on fog computing nodes (FCN), where
    the devices may be compromised or replaced by fake ones. This problem arises because
    the FCN under this type of attack utilize only a small amount of the processor
    and memory, and normal intrusion and anomaly detection techniques will not be
    able to detect it. The authors also provide an insight into the solution to the
    man-in-the-middle attack and list a number of privacy issues in fog computing
    and different solutions available in the literature. Overall, it must be acknowledged
    that fog computing provides a number of advantages that are of key importance
    for most applications: low latency, location awareness, real time operation, heterogeneity,
    and end device mobility; all these make it an attractive computation paradigm.
    But, the security and privacy challenges of trust, authentication and man-in-the-middle
    attack discussed above make it challenging to implement FCN in daily life applications.
    2.7. Edge computing Edge computing (EC) is an overloaded concept, defined differently
    across the literature. The most commonly mentioned meaning of edge is that of
    end, meaning that edge computing is carried out by the end devices or user devices
    (also called edge devices in many works). Although the latter one pulls the focus
    away from the network elements and their associated challenges, it is probably
    the most extended term up to the present time. However, the networking community
    has started to use edge computing to refer to the computation performed by the
    network elements. If we view the Internet as a graph that connects computation
    nodes (computers), the term edge is assigned to the connecting line between the
    central nodes (cloud servers) and the end nodes (the devices at the end of the
    network or user devices). Here, edge computing refers to the computation done
    at the network backhaul. After presenting both usages of edge computing, we use
    this term in the networking sense in the remainder of this paper. This way, we
    refer to end or user devices as the leaf nodes of the Internet graph, and we use
    the term edge computing as to the computation done at the network elements and
    backhaul that will support offloading and will speed up the service time to end
    devices by partly performing heavy computations in the network segments. In the
    first presented meaning of the term, the idea behind edge computing is to perform
    computation and storage locally within the resources available at the end devices.
    For this type of nodes, [136] targets at addressing the potential issues of response
    time requirements, battery life constraints, data security and privacy, and bandwidth
    reduction; this paper also discusses the evolution of the edge computing from
    the concepts of cloud computing and IoT, providing a definition for edge computing
    and several case studies that support this paradigm and show the inherent advantages
    that it offers. Similarly, an insight into edge computing is provided in [51]
    along with the comparison among the different edge computing implementations of
    fog computing, mobile edge computing, and cloudlets. Some simple differences among
    the three are: • Characteristics of nodes. Fog computing nodes use off-the-shelf
    devices and provide them with computation and storage capabilities which make
    them slower as compared to the dedicated devices of mobile edge computing and
    cloudlets. • Proximity to end devices. Fog computing nodes may not be the first
    hop for end devices due to the use of off-the-shelf computing devices; whereas
    for MEC and cloudlet, the devices can connect directly to the end nodes using
    WiFi for cloudlets and mobile base station for mobile edge computing. • Access/communication
    mechanisms between the devices. Fog computing nodes can use WiFi, Bluetooth or
    even mobile networks; mobile edge computing devices can only utilize mobile networks;
    and Cloudlets use WiFi. • Diversity and heterogeneity in the off-the-shelf devices.
    The fog computing paradigm requires an abstraction layer; whereas mobile edge
    computing and cloudlets do not require this because of the dedicated connections
    that devices use. Additionally, the authors have also mentioned the use case based
    selection of the three edge computing implementations in terms of power consumption,
    access medium, context awareness, proximity, and computation times. As a result
    from the literature analysis, it appears that the genesis of edge computing has
    made way to other edge computing implementations of fog computing, mobile edge
    computing, and cloudlets which tend to tackle the disadvantages of cloud computing
    and mobile cloud computing. However, there are several open issues of edge computing
    as indicated in [95] and these are security and privacy, resilience, robustness,
    achieving openness in the networks, supporting multi-services and operation. 2.8.
    Mobile edge computing Mobile-Edge Computing (MEC) was motivated by the growth
    of the network traffic generated by the proliferation of smart phones and their
    applications that require intensive data exchange and processing. MEC intends
    to reduce the latency and to support location awareness in order to increase the
    capacity of the applications that run on mobile devices. MEC started development
    in 2014 led by ETSI2 with the goal of achieving a sustainable business strategy
    [132]. For this, it brought together mobile operators, service providers, mobile
    users, and over-the-top (OTT) players. Different metrics can be improved by deploying
    services over MEC. On the functional side: latency, energy efficiency, throughput,
    goodput, packet loss, jitter, and QoS. On the non-functional side: service availability,
    reliability, service load, and number of invocation requests. MEC servers are
    located near the base stations. Smart devices offload activities and the cellular
    data and offloaded activities are processed on such servers; them, the edge servers
    decrease the traffic and congestion on the backhaul. Thus, MEC aims at placing
    the computational and storage resources at the mobile base stations so that mobile
    users can widely use the additional features it has to provide. [27] provide technical
    insight into MEC along with its limitations by identifying the applications of
    MEC. Various applications and use cases of content scaling, offloading, augmentation,
    edge content delivery, aggregation and local connectivity are evaluated in terms
    of power consumption, delay, bandwidth and scalability. A few of the listed advantages
    of MEC for different stakeholders of end users, network operators and application
    service providers are: (1) end users benefit from reduced communication delay;
    (2) network operators benefit with bandwidth reduction and scalability; (3) application
    service providers benefit with faster service and scalability; and (4) augmentation
    enables the application providers to integrate cellular network specific information
    into the application traffic. A comprehensive overview of MEC is found in [104]
    that introduces features of MEC along with its paradigm shift from MCC. A comparison
    of MEC and MCC has been made to support the advantages of paradigm shift from
    MCC. The advantages of MEC like low latency, mobile energy savings, context awareness
    and, privacy and security enhancement are discussed along with examples. Some
    of the mentioned technical challenges of MEC are: security, network integration,
    application portability, performance, regulatory and legal consideration, resilience
    and operation. The literature also mentions some use case scenarios of MEC like
    video stream analysis, augmented reality, IoT, and connected vehicles. In contrast
    to the cloudlet model, which is available to specific users in the vicinity of
    the cloudlet, MEC is available to all mobile users as MEC servers are deployed
    in mobile base stations to deliver additional features such as location and mobility
    information. Fog or cloudlet nodes are managed typically by individuals and can
    be deployed at any location that they judge convenient. MEC servers are owned
    by mobile operators; servers have to be deployed near the base stations to facilitate
    that users have access to the mobile network over the radio access network (RAN)
    [131]. The MEC model has been prototyped on a few scenarios such as edge video
    orchestration in which users access live video streams enabled by an orchestration
    application running on a MEC server. MEC servers can be deployed at different
    locations on the networking infrastructure: an LTE base station,3 3G Radio Network
    Controllers (RNC), or a mix of both. Security and privacy issues are shared by
    fog, cloud, and MEC. Moreover, in MEC the congestion of a server may affect the
    service provided to a number of mobile users, resulting in high monetary costs.
    Therefore, increasing the computation power at the edge servers is a real need.
    2.9. Mist computing Mist Computing is a concept explained in [125]. There is lack
    of consensus as to the precise definition of mist computing. In some works, mist
    computing is defined as the paradigm that takes advantage of every processing
    capacity available everywhere, from the end nodes (sensors and actuators) to the
    cloud servers. Some of these works also provide definitions for other concepts
    that collide with the mainstream trend, e.g., edge computing acquires fog computing
    capabilities [73]. As there is no clean definition of what mist actually provides,
    we are inclined to either use cloud, fog, or edge. As in [126], fog computing
    performs the computation at the network using the gateway devices, but in mist
    computing this is performed by the actual end devices, i.e., sensors and actuators.
    We know that the closer the computation is to the end devices, the bigger is the
    decrease in the network latency and transmission delay, which improves the user
    experiences in real time applications. 2.10. Social computing Social Computing
    [78] is a paradigm for analyzing and modeling social behaviors of users on media
    and platforms to extract added value information and create intelligent and interactive
    applications and data. It involves a multi-disciplinary approach that encompasses
    computing, sociology, social psychology, communication theory, computer-science,
    and human-machine interaction (HMI). For this purpose, social computing focuses
    essentially on studying the relations among people within a group to analyze how
    the information flows; the collaboration manner to extract positive and negative
    patterns; how communities are built, and how grouping is achieved. The target
    systems for analysis are social media, social networks, social games, social bookmarking
    and tagging systems, social news, and knowledge sharing, among others. Among these
    scenarios, social computing and social software are capable of providing big data
    that can be processed and analyzed with complex algorithms and computation techniques
    [78] capable of extracting essential social knowledge that creates high value
    for society, industry, or individuals. Social computing is a part of computer
    science at the confluence area between social behavior and computational systems.
    By means of using software systems and computer science technology, social computing
    recreates social conventions and social contexts. Software applications for social
    communication and interaction are the building block of social computing and illustrate
    this concept. Among these software elements, one may find public web based content,
    blogs, email, instant messaging, social network services, wikis, social tagging
    and bookmarking. Since the wide availability of Internet and powerful personal
    computers, social computing took a phenomenal growth. This paradigm shifts the
    computing towards the end of the network for the end users to engage in social
    communities, share information and ideas, and collectively build and use new tools.
    Social communities with common ideas, tools and interests are formed which can
    improve the experience of using tools and sharing common problems and solutions.
    As an example, Wikipedia is an open source encyclopedia that works like an information
    sharing tool formed by collaborative authoring which can be reviewed and changed
    upon the feedback of users. Though this social tool helps the community in sharing
    information through a common platform called wiki, the credibility of information
    is at stake, as it is an open source tool with collaborative authorship. Some
    other notable examples of social computing platforms are YouTube, Word press,
    Tumblr, Facebook, Twitter, or LinkedIn. 2.11. Dispersed computing Dispersed computing
    [18] involves algorithms and protocols for mission-aware computation and communication
    across broad-scale, physically dispersed objects for designing scalable, secure,
    and robust decision systems that are collectively driven by a global mission.
    These systems can operate under highly variable and unpredictable (possibly also
    degraded) network connectivity conditions. For this reason, dispersed computing
    envisions opportunistic and convenient design of mobile code and data relations
    as needed by the users, the applications, or the mission. For cloud computing
    and mobile computing, users offload the real time data on to the cloud for processing
    and data analytics. We have also discussed a few limitations of high network latency
    and transmission delay, that lead to the genesis of the different paradigms of
    edge computing, fog computing and mobile edge computing based on the idea of utilizing
    the computational resources of the end devices in the network to process the data
    locally. Similar to this idea, dispersed computing seeks to provide a scalable
    and robust computing system which collectively uses heterogeneous computing platforms
    to process large data volumes. This paradigm is typically deployed in situations
    where there is degraded network connectivity that leads to higher data latency
    and transmission delay. Among the first works on dispersed computing, we find
    [140] that defines the term as an alternative model derived from the consolidation
    of a number of contributions on data transmission, data storage, and code execution.
    Still that work is very preliminar and much targeted at surveying the existing
    distributed computing models according to various criteria and highly related
    to cloud. Other meanings of disperse computing rather point at the edge computing
    elements, such as DARPA’s definition [18] where NCPs (the network control points)
    are placed at the core of the computations. Dispersed computing systems run software
    partly inside the programmable platforms within the network, the NCPs. As mentioned
    earlier, NCPs are capable of running code for both, users/applications and for
    the network protocol stack. For implementing the dispersed computing paradigm,
    the application-level logic will need resources available at the end points (the
    computation devices) and at the NCPs. 3. Social dispersed computing In this paper,
    we coin the term social dispersed computing that is at the intersection of social
    computing and dispersed computing. On the one hand, dispersed computing [18] has
    the goal of providing scalable, secure, and robust decision systems that are collectively
    driven by a global mission. Dispersed computing is a computing paradigm for designing
    systems that can operate under highly variable and unpredictable (possibly also
    degraded) network connectivity conditions. For this, such a computing paradigm
    envisions opportunistic and convenient design of mobile code and data relations
    as needed by the users, the applications, or the mission. On the other hand, the
    social dispersed computing paradigm takes an agent or actor based approach, connecting
    the users with each other via messages, enabling them to obtain globally useful
    analysis, while performing local computations. Further, decisions on what users
    do are influenced not only by the users’ personal preference and desire but also
    by what other users are doing. These models demand complex, flexible, and adaptive
    systems, in which components cannot simply be passive nor can reactive entities
    be managed by only one organization [144]. Nevertheless, instead of being a solitary
    activity, computation becomes rather an inherently social activity, leading to
    new ways of conceiving, designing, developing, and handling computational systems
    [138]. Considering the emergence of distributed paradigms such as web services,
    service-oriented computing, grid computing, peer-to-peer technologies, autonomic
    computing, etc., large systems can be viewed as the services that are offered
    and consumed by different entities, enabling a transactive paradigm. Formally,
    social dispersed computing applications can be approximated as multi agent systems.
    For example, they can be thought of as collections of service-provider and service-consumer
    components interlinked by dynamically defined workflows [103]. Agents are autonomous
    entities with given behaviours that interact with other agents that also have
    their own behaviours. As a result of these interactions, individual behaviours
    (or even objectives, preferences, etc.) may be affected, emerging a global (or
    aggregated) behaviour of the whole system. Intelligent software agents are a new
    class of software that act on behalf of the user to find and filter information,
    negotiate for services, easily automate complex tasks, or collaborate with other
    software agents to solve complex problems. This concept of intelligent agent provides
    support to build complex social dispersed computing systems as components with
    higher levels of intelligence, which demand complex ways of interaction and cooperation
    in order to solve specific problems and achieve the given objectives. However,
    while procedures, functions, methods and objects are familiar software abstractions
    that software developers use every day, software agents, are a fundamentally new
    paradigm unfamiliar to many software developers. Thus, new platforms and programming
    abstractions are required. We describe some of these paradigms in the sections
    on market based approaches later in the paper. 3.1. Multi agent systems It should
    be noted that the concept of social dispersed systems borrows heavily from the
    paradigm of multi-agent systems and integrates social behaviors and incentives
    (to encourage participation) in to the mix. Multi Agent Systems (MAS) is an important
    and exciting research area that has arisen in the field of Information Technologies
    in the last decade [102]. According to [150], an agent is defined by its flexibility,
    which implies that an agent is reactive as it must answer to its environment;
    proactive as it must try to fulfill its own plans or objectives; and social because
    an agent has to be able to communicate with other agents by means of some kind
    of language. A Multi Agent System consists of a number of agents that interact
    with one-another [149]. The most promising application of MAS technology is its
    use for supporting open distributed systems [102]. Open systems are characterized
    by the heterogeneity of their participants, non-trustworthy members, existence
    of conflicting individual goals and a high possibility of non-accordance with
    specifications [25]. The main feature of agents in these systems is autonomy.
    It is this autonomy that requires regulation, and norms are a solution for this
    requirement. In these types of systems, problems are solved by means of cooperation
    among several software agents [103]. Norms prescribe what is permitted, forbidden,
    and mandatory in societies. Thus, they define the benefits and responsibilities
    of the society members and, as a consequence, agents are able to plan their actions
    according to their expected behaviour. When developing applications based on the
    new generation of distributed systems, developers and users require infrastructures
    and tools that support essential features in Multi Agent Systems (such as agent
    organizations, mobility, etc.) and that facilitate the system design, management,
    execution, and evaluation [48], [57]. Agent infrastructures are usually built
    using other technologies such as grid systems, service-oriented architectures,
    P2P networks, etc. In this sense, the integration and interoperability of such
    technologies in Multi Agent Systems is also a challenging issue in the area of
    both tools and infrastructures. What is more, agent technologies can provide concepts
    and tools that give possible answers to the challenges of practical development
    of such systems by taking into consideration issues such as decentralization and
    distribution of control, flexibility, adaptation, trust, security, and openness
    [35]. Finally, in order for Multi Agent Systems to be included in real domains
    like media and Internet, logistics, e-commerce and health care, their associated
    infrastructures and tools should provide efficiency, scalability, security, management,
    monitoring and other features related to building real applications. 3.2. Social
    dispersed computing illustration To illustrate the concept of social dispersed
    computing, we consider three examples: two from the transportation domain and
    one from the energy domain. 3.2.1. Next generation electrical energy systems Transactive
    energy systems (TES) [45], [80], [91], [109] have emerged in anticipation of a
    shift in the electricity industry, away from centralized, monolithic business
    models characterized by bulk generation and one-way delivery, towards a decentralized
    model in which end users play a more active role in both production and consumption
    [109]. The main actors of this system are the consumers, which are comprised primarily
    of residential loads and prosumers who operate distributed energy resources (DERs).
    Examples of such DERs include photovoltaics, batteries, and schedulable loads
    (electric vehicle charging, laundry, etc.). Additionally, a distribution system
    operator (DSO) manages the connection between the microgrid and the primary grid.
    Such installations are equipped with an advanced metering infrastructure, which
    consists of TE-enabled smart meters. In addition to the standard functionality
    of smart meters (i.e., the ability to measure line voltages, power consumption
    and production, and communicate these to the DSO), TE-enabled smart meters are
    capable of communicating with other smart meters, have substantial on-board computational
    resources, and are capable of accessing the Internet and cloud computing services
    as needed. Examples of such installations include the well-known Brooklyn Microgrid
    Project [17]. At its core, transactive energy systems are market based social
    applications that have to dynamically balance the demand and supply across the
    electrical infrastructure [109]. In this approach, customers on the same feeder
    (i.e., those sharing a power line link) can operate in an open market, trading
    and exchanging generated energy locally. Distribution system operators can be
    the custodians of this market, while still meeting the net demand [47]. Implementing
    such systems requires either a centralized or decentralized market framework that
    is robust, resilient, and secure. Fog computing resources provide an ideal opportunity
    to schedule the operation of the market activities in the community as most of
    the activity remains within the community and each home has access to a set of
    smart inverters and computers attached to the smart inverters that can be part
    of the fog computing layer. 3.2.2. Social mobility Social routing platforms address
    the problem of urban transportation and congestion by directly engaging individual
    commuters. Due to widespread use of smart devices, users are becoming active agents
    in the shared mobility economy. This favors the use of algorithms for designing
    active incentives that encourage users to take mobility decisions considering
    the overall system effect, rather than myopic individual utilities, that focuses
    on what is best for each individual from his or her local perspective, as implemented
    by commercially available mobility solutions [130]. Such services require a for
    information sharing, and a transactive platform that: (a) provides multimodal
    routing algorithms, which extend existing optimization techniques for solving
    the multimodal transit problem by incorporating probabilistic representations
    of events in cities, creating a near-optimal distributed algorithm by employing
    sub-modularity and folding incentive mechanisms into the optimization problem;
    (b) provide high-fidelity analytics and simulation capabilities for service providers,
    informing them about how users are consuming transportation resources, which enables
    them to develop mechanisms for improving services; and (c) provide an immutable
    and auditable record of all transactions in the system. Again a market-based distributed
    system running across these agents will be able to create a dynamic offer with
    incentive-based route assignment logic that can ensure that transportation resources
    are shared efficiently without causing congestion. Clearly, such a platform is
    also an extension of the transaction management platform by: (1) making individual
    consumers the participants; and (2) making the apps running on their smart phones
    the transaction agents and the transaction management platform provided by the
    transportation agency. A solution to this problem requires a social computing
    and information sharing platform that overcomes the incentive gap between individuals
    and municipalities. This platform must offer mixed-mode routing suggestions and
    general system information to travelers and, in turn, supply service providers
    with high-fidelity information about how users are consuming different transportation
    resources. At the same time, this system must also consider the investment required
    by the cities in the computing infrastructure required to solve the problem at
    scale. Alternatively, a social dispersed computing approach that utilizes the
    various end computing resources available in the city, including the mobile devices
    of the commuters, can be employed by municipalities to improve efficiency within
    their cities with little investment. This scenario precisely leads to the problem
    of secure and trustworthy computing. Privacy of individuals is an important aspect
    of a suitable solution; the usage of individuals’ smart devices as both data sources
    and computational resources could expose the end users to a risk of privacy breach.
    Seemingly innocuous data, such as transit mode or route choice, can lead to inferences
    of private information, such as real-time tracking of an individual’s position
    [82], likelihood of affairs [115], and forecasting trip destinations [50]. Therefore,
    again localized computing resources which are managed under the legal jurisdiction
    are more attractive to use for implementing the transaction management. 3.2.3.
    Distributed traffic congestion analysis Another example is traffic congestion
    analysis in cities. Traffic congestion in urban areas has become a significant
    issue in recent years. Because of traffic congestion, people in the United States
    traveled an extra 6.9 billion hours and purchased an extra 3.1 billion gallons
    of fuel in 2014. The extra time and fuel cost were valued up to 160 billion dollars
    [133]. Congestion that is caused by accidents, roadwork, special events, or adverse
    weather is called non-recurring congestion (NRC) [65]. Compared with the recurring
    congestion that happens repeatedly at particular times in the day, weekday and
    peak hours, NRC makes people unprepared and has a significant impact on urban
    mobility. For example, in the US, NRC accounts for two-thirds of the overall traffic
    delay in urban areas with a population of over one million [100]. Driven by the
    concepts of the Internet of Things (IoT) and smart cities, various traffic sensors
    have been deployed in urban environments on a large scale, and many techniques
    for knowledge discovery and data mining that integrate and utilize the sensor
    data have been also developed. Traffic data is widely available by using static
    sensors (e.g., loop detectors, radars, cameras, etc.) as well as mobile sensors
    (e.g., in-vehicle GPS and other crowdsensing techniques that use mobile phones).
    The fast development of sensor techniques enables the possibility of in-depth
    analysis of congestion and their causes. The problem of finding anomalous traffic
    patterns is called traffic anomaly detection. Understanding and analyzing traffic
    anomalies, especially congestion patterns, is critical to helping city planners
    make better decisions to optimize urban transportation systems and reduce congestion
    conditions. To identify faulty sensors, many data-driven and model-driven methods
    have been proposed to incorporate historical and real-time data [62], [101], [129],
    [156]. Some researchers [75], [81], [146], [152] have worked on detecting traffic
    events such as car accidents and congestion using videos, traffic, and vehicular
    ad hoc data. There are also researchers who have explored the root causes of anomalous
    traffic [19], [44], [86], [87], [99], [151]. Most existing work still focuses
    mostly on a road section or a small network region to identify traffic congestion,
    but few studies explore non-recurring congestion and its causes for a large urban
    area. Recently, deep learning techniques have gained great success in many research
    fields (including image processing, speech recognition, bioinformatics, etc.),
    and provide a great opportunity to potentially solve the NRC identification and
    classification problem. However, the state of the art still is to collate the
    data into a server and then perform the NRC classification periodically. The concept
    of Mobile Edge Computing and Fog Computing provide a new opportunity. Consider
    a network of micro-devices running on the transit buses, on kiosks at the bus
    stops, and the metro data center can be used to not only provide the transit schedule
    analysis services to the end customer but can also be used to provide analysis
    of non-recurring congestion (NRC). Compared with the recurring congestion that
    happens repeatedly at a particular time in the day, weekday and peak hours, NRC
    usually shows specific patterns associated with the causing events. It is important
    to identify and correlate the traffic data gathered by individual road sensors,
    including cameras, and solve a coordinated analysis of traffic conditions across
    the region. Clearly, sending all the data in real-time to the cloud or the metro
    data center is inefficient and the data should be only sent when the likelihood
    of NRC is high. Detection of NRC events is important in communities as the local
    traffic operation centers and emergency responders can take proactive actions.
    Once an NRC event is detected, it is possible to do further analysis to identify
    if it can be explained due to an existing event or if it can be explained as a
    failure of one or more traffic sensors [62], which can then be repaired. 4. Enabling
    social dispersed computing While fog computing, edge computing, and mobile edge
    computing provide the required computation resources, the resilience, timeliness,
    and security requirements impose the need for additional middleware technology
    with improved services. While traditionally middleware was thought of as the “networking”
    glue, these days middleware is often used as the term to also describe “useful
    platform” services. These platform services provide reusable capabilities like
    distributed transactions, time synchronization, fault-tolerance, etc. This section
    describes some of these core computation services. The reader must think of them
    as core-enablers, which when combined appropriately with the underlying computation
    substrates enable useful social dispersed computing applications. 4.1. Distributed
    transaction management At its core, agents in the social dispersed computing domain
    are executing a set of related operations. These operations and their sequence
    can be grouped into a transaction to enable fault tolerance, specially providing
    the capability of roll back. A distributed transaction is a set of operations
    that involve two or more networked nodes that, in turn, provide resources that
    are used and probably updated by the operations. In a traditional transaction,
    there is the notion of the transaction manager that manages the execution of the
    constituent operations and their access to the distributed resources. Typical
    transaction systems such as [49], [111] use techniques for faster execution like
    compensating transactions, optimism, and isolation without locking. However, the
    concept of centralized management will have to be revisited for social dispersed
    computing applications; these are highly distributed applications, potentially
    involving large numbers of participants with high mobility, that produce large
    data volumes, and that manage data selectively. Social computing applications
    are transactive by nature because they often involve exchange of digital assets
    between participants. The state transition of the system also depends upon the
    confirmed past state of the system. Examples include transactive ride-share systems
    [155], transactive health-care systems [26], and transactive energy systems [45],
    [80], [109]. Typically, there are three different kinds of subsystems required
    to settle the transactions in a social dispersed computing application. The first
    subsystem is a distributed ledger (e.g. Blockchains), which is responsible for
    keeping track of (and log) all events of interest. For instance, in the energy
    domain these events are trades, energy transfers, and financial transactions.
    In the health care domain, the events record the time of access of the health
    care data. The data is not stored in the blockchain due to the size and privacy
    issues. Rather, the data is stored in the second layer, which can be implemented
    by either a cloud or a decentralized storage service like Storj [3] or IPFS [119].
    The second subsystem is the IoT layer, which is responsible for sensing and control.
    The third subsystem is the communication layer and is typically implemented using
    messaging middlewares like MQTT [120] or DDS [121]. A new enabling technology
    for transaction management can be IPFS (Inter Planetary File System) that is a
    peer to peer distributed file system with the goal of connecting all computing
    devices through a single global file system. In IPFS, nodes do not need to trust
    each other: it uses a distributed hashtable and a self-certifying namespace, and
    has no single point of failure. IPFS is similar to the web, but it tries to mimic
    the exchange of files through a Git type of repository for all devices by providing
    a content-addressed block storage model with content-addressed hyper links. This
    connection type will form a data structure (Merkle DAG) that can be used for providing
    blockchains, versioned file systems, or a permanent web. 4.2. Blockchain Blockchains
    combine the storage of transaction information with advanced protocols in a way
    that ensures that there is a consensus on the operations that were executed. It
    is a public database where new data are stored in a container called a block.
    Each block is added to an immutable chain that has data added in the past. Data
    stored in blockchains can be of any type. The perfect illustration of this technology
    is inevitably related to Bitcoins, a cryptocurrency whose transactions are recorded
    chronologically and publicly on the database, where each block is a transaction.
    The evolution of blockchain technology ancestors until today is depicted in Fig.
    2. Download : Download high-res image (828KB) Download : Download full-size image
    Fig. 2. Evolution of Blockchain. Current transactions require that people trust
    on a third party to complete the transaction. This third party can be a bank or
    a national authority for the case of transactions involving money. Blockchain
    technology is radically challenging the current way of operating transactions.
    Blockchain relies on the use of mathematical tools and cryptography to provide
    an open decentralized database as a global and decentralized source of trust recording
    every transaction that involves value, money, goods, property, work, or even votes.
    Every transaction is recorded on a public and distributed ledger accessible by
    anyone who has an Internet connection. It consists of creating and managing a
    record whose authenticity can be verified by the entire user community. Distributed
    property and trust can, then, be enabled in a way in which every user with access
    to the Internet can get involved in blockchain-based transactions, and third party
    trust organizations may no longer be necessary. Blockchain technology can be used
    in an endless number of applications: tax collection, money transfers without
    bank intervention, or health care management. How it work is explained in what
    follows. When someone requests a transaction, such transaction is broadcast to
    a peer-to-peer network consisting of computation nodes, simply known as nodes,
    that form a completely decentralized network. The network of nodes validates the
    transaction and the user’s status applying algorithms. When this transaction is
    verified, it is combined with other transactions to create a new block of data
    that is placed in the ledger. After, the new block is added to the existing blockchain
    permanently and inmutably. Social dispersed applications are candidates for using
    blockchain technology given their highly distributed nature. Overall, the blockchain
    database is stored in a distributed way, and the records it keeps are public and
    easily verifiable. As no centralized version of such information exists, it is
    secured from hacker attacks. 4.3. Distributed market platform As discussed in
    the earlier examples, there is a need for incentives to participate as a resource
    in the social dispersed computing as well as to be eager to provide information.
    A market based distributed framework can provide this foundation: one in which
    all interactions generated in the social computing application are safely stored.
    As mentioned previously, such interactions are found in other sharing economy
    driven applications [135], e.g., ride-sharing [79], [94], car-sharing [66] and
    transactive energy systems [31], [85], [92]. However, these exchange of data and
    resource raises the concerns of integrity, trust, and above all the need for fair
    and optimal solutions to the problem of resource allocation, motivating the requirement
    for a management platform. Specifically, such a market based platform involves
    a number of self-interested agents that interact with each other by submitting
    offers to buy or sell the goods, while satisfying one or more of the following
    requirements: (1) anonymity of participant identities, i.e., individual agents
    shall not have the means to infer the identities of other agents, or who trades
    with whom; (2) confidentiality of market information, which includes individual
    bids and transaction information, output of trade verification processes, and
    finalized trading data that are yet executed; (3) market integrity and non-repudiation
    transactions; (4) availability and auditability of all events and data which can
    take the form of encrypted or non-encrypted data. Blockchains form a key component
    of such market based platforms because they enable participants to reach a consensus
    on the value of any state variable in the system, without relying on a trusted
    third party or trusting each other. Distributed consensus not only solves the
    trust issue, but also provides fault-tolerance since consensus is always reached
    on the correct state as long as the number of faulty nodes is below a threshold.
    Further, blockchains can also enable performing computation in a distributed and
    trustworthy manner in the form of smart contracts. However, while the distributed
    integrity of a blockchain ledger presents unique opportunities, it also introduces
    new assurance challenges that must be addressed before protocols and implementations
    can live up to their potential. For instance, smart contracts deployed in practice
    are riddled with bugs and security vulnerabilities. Another problem with blockchain
    based implementation is that the computation is relatively expensive on blockchain-based
    distributed platforms and solving the trading problem using a blockchain-based
    smart contract is not scalable in practice. Fig. 3 describes an example of such
    a market platform called SolidWorx [54]. It allows agents to post offers using
    predefined programming interfaces. A directory actor provides a mechanism to look
    up connection endpoints, including the address of a deployed smart contract. The
    smart contract functions check the correctness of each offer and then stores it
    within the smart contract. Mixer services can be used to obfuscate the identity
    of the prosumers [31]. By generating new anonymous addresses at random periodically,
    prosumers can prevent other entities from linking the anonymous addresses to their
    actual identities [31], [91], thereby keeping their activities private. Solver
    actors, which are pre-configured with constraints and an objective function, can
    listen to smart-contract events, which provide the solvers with information about
    offers. Solvers run at pre-configured intervals, compute a resource allocation,
    and submit the solution allocation to the smart contract. The directory, acting
    as a service director, can then finalize a solution by invoking a smart-contract
    function, which chooses the best solution from all the allocations that have been
    submitted. Once a solution has been finalized, the prosumers are notified using
    smart-contract events. To ensure correctness, the smart contract of SolidWorx
    is generated and verified using a finite-state machine (FSM) based language called
    FSolidM [106]. Download : Download high-res image (237KB) Download : Download
    full-size image Fig. 3. An example of a distributed market platform managing the
    interaction of the agents in a social computing setting described in [54]. 4.4.
    Time synchronization Satisfying time deterministic requirements during code execution
    on a node is crucial but not enough for a distributed system like social dispersed
    computing. In these applications, we sometimes need to establish a common synchronized
    time base and need to align each node’s local clock(s) to this global reference.
    Even slight differences in each node’s local clock—typically a few tens of parts
    per million (ppm)—accumulate fast and become apparent over time. Based on environmental
    factors (temperature, humidity, and voltage stability), the frequency differences
    are not constant. Thus, to provide an accurate globally synchronized time base,
    the supporting services need to periodically measure and compensate for these
    differences. The periodic adjustment of the local time on the node requires careful
    considerations to avoid disruption of the local event scheduler [52]. Fortunately,
    there are two well established technologies for solving this problem, both are
    supported by any modern Linux kernel. The Network Time Protocol (NTP) [71] is
    a ubiquitous time synchronization service using heuristic software algorithms
    with no special requirements on the networking hardware and communication infrastructure.
    The Precision Time Protocol (PTP, IEEE-1588) on the other hand is built on accurate
    end-to-end hardware-level timestamping capabilities. It is no surprise that the
    attainable accuracy of the two methods differ by orders of magnitudes: tens of
    milliseconds with NTP vs. microseconds with PTP [117]. PTP has also been implemented
    over wireless [42]. The PTP protocol achieves excellent accuracy if used within
    a local area network and/or all network equipment in the packet forwarding path
    participate in the protocol. The basic building blocks of the protocol are: (1)
    a hierarchical master/slave clock tree strategy supported by a leader-election
    (“best master”) protocol, (2) accurate time-of-flight measurement of network packets
    with the built-in assumption that these delays are symmetrical (3) support for
    measuring and compensating for intermediate delays across the communication medium
    (4) using level-2 LAN frames or IPv4/IPv6 UDP messages as the transport mechanism
    (5) support for co-existing independent PTP clock domains on the same LAN. At
    its core, the master-slave clock synchronization mechanism is implemented by periodic
    beacon frames broadcast by the master and containing the master clock value at
    the beginning of the beacon message generation. If the networking hardware is
    not capable of inserting this time value during frame transmission, a second non
    time critical frame is sent by the master containing this value. With properly
    maintained estimates on frame transmission delays, each slave can adjust its local
    clock to the master. The delay estimation is based on periodic round-trip requests
    from the slaves to the master. The request message is transmit-timestamped by
    the slave and received-timestamped by the master. The server then replies with
    a non real time message which contains the received-timestamp for the slave to
    have a good estimate on the current network delay. 4.5. Distributed coordination
    services Social dispersed computing applications will aggregate large numbers
    of users participating as sensing actors and will also receive and use data produced
    by the applications themselves. Interactions across these users will be possibly
    made on the basis of user groups that can change dynamically. Services for grouping/membership
    management and distributed coordination and consensus will have to be put in place
    to enable consistent interoperation with coherent state management. An application
    may be deployed on a variable number of nodes. Nodes can be added or removed from
    the network at any time, either by a controlling authority or unintentionally
    due to a fault condition. It is possible for an application to operate on a subset
    of nodes (or groups), while another application operates on another subset of
    nodes. It is possible for a node to migrate from one subset to another subset.
    A distributed coordination service provides common services for coordination among
    actors that run on a network of nodes. The distributed coordination service includes:
    (1) group membership, (2) leader election, (3) distributed consensus, and (4)
    time-synchronized coordinated action; these are explained below: • Group membership
    maintenance: It is a basic building block that maintains the logical lists of
    components (i.e., users) that register with the service. All the distributed coordination
    features are available inside a logical group. • Leader election: Choosing a leader
    is a process where a single node becomes designated as an organizer of tasks among
    several distributed nodes. • Distributed consensus: A process where group members
    form agreement on some data value. • Time-synchronized coordinated action: Time
    synchronized activities take the clock value as the trigger for their execution.
    In a distributed scenario, several nodes will have to agree on when to schedule
    a task of this kind, and for this, their clocks must be synchronized. More in
    detail, coordination services are needed to maintain shared state in a consistent
    and fault-tolerant manner. Achieving fault tolerance is done by using replication
    that is typically based on running a quorum (majority) based protocol such as
    Paxos [88], [89]. Paxos manages the state updates history with acceptors, and
    each update is voted by a quorum of acceptors. The leader that manages the voting
    process is one acceptor. Paxos also has learners that are light weight services
    that get the notifications of updates after the quorum accepts them; learners
    do not participate in the voting. Different technologies have implemented this
    protocol; a few selected ones will be presented in the next section. A major criticism
    to Paxos is that it is not an easy-to-understand protocol. Raft [122] is similar
    to Paxos, however, according to the authors it is more understandable, the implementation
    phase is shorter, and it is designed to have fewer states. Often, distributed
    hash tables are also used to store the information that can be used for distributed
    coordination. For example [55] uses OpenDHT [128] to store, query, and disseminate
    details of publishers and subscribers across the network. OpenDHT is a fast, lightweight
    Distributed Hash Table (DHT) implementation. The dissemination does not mean full
    data replication on all nodes. OpenDHT stores the registered value locally and
    forwards it to a maximum of eight neighbors. The distributed hash table for service
    discovery does not distinguish the nodes, i.e. there are no “server” or “client”
    nodes; nodes are peers and each one operates with the same rules. If a node disconnects
    from the network, the DHT service on the other nodes is still able to register
    new services or run queries. 4.6. Software technologies 4.6.1. Virtualization
    Since 1966 when the term “hypervisor” was first used until today, virtualization
    technology has undergone a strong revolution (see (Fig. 4) up to the point in
    which virtualization technology has been one of the key enablers of cloud computing
    [59]; and we believe that it will also play a major role in social dispersed computing.
    The partial computations from user groups will have to happen in servers in their
    vicinity that will aggregate the data received from users, possibly maintaining
    a state of the group, and communicate back to the users and to other neighboring
    servers and the cloud. These servers will have to run other applications besides
    the social computing application; in this way, virtualization can be used to isolate
    the execution of the different applications in the same physical node, avoiding
    interference and preserving performance. In a computer system, virtualization
    refers to the creation of a virtual (not actual) version of some other system;
    that includes processor, storage, or network virtualization. There are different
    types of virtualization. A few of them are provided in what follows. Download
    : Download high-res image (486KB) Download : Download full-size image Fig. 4.
    Looking back at 60 years of virtualization history. Machine virtualization. It
    provides an abstraction of the real hardware resources or subsystems, mapping
    the virtual resource to the actual one, offering applications an abstract view
    through interfaces of the hardware platform and resources that are provided underneath.
    In this context, the host machine is used for referring to the physical machine
    on which virtualization occurs; and guest machine is the virtual machine that
    is created on the physical machine. The hypervisor or virtual machine monitor
    (VMM) is a program (whether software, firmware, or hardware) that creates virtual
    machines on an actual host machine. Virtualization allows applications to be run
    in software environments that are separated from their underlying hardware infrastructure
    by a layer of abstraction. This enables different applications to be split into
    virtualized machines that can run over different operating systems running over
    the same hardware. A virtual machine (VM) is an execution environment in its own:
    it is a software implementation of a physical execution platform, machine, or
    computer, capable of running the same programs that the physical machine can run.
    Virtual environments can be designed from either a hardware partitioning or hypervisor
    design side. Hardware partitioning does not support the benefits that resource
    sharing and emulation offered by hypervisors can provide. There are two main types
    of hypervisors. On the one hand, bare metal (namely type 1) hypervisors execute
    directly on the physical hardware platform that virtualizes the critical hardware
    devices offering several independent isolated partitions. Examples of these are
    VMWare ESX, Xen, or Microsoft Hyper-V; and others such as WindRiver Hypervisor
    or XtratuM for real-time systems. These can also include network virtualization
    models like VMware NSX. On the other hand, type 2 hypervisors are hosted ones
    as they run over a host operating system. Containers. Containers are a different
    virtualization model in which different applications and services can run on a
    single operating system as a host, instead of virtual machines which allow to
    run different operating systems. The idea behind containers was providing software
    code in a way that it can be quickly moved around to run on servers using Linux
    OS; such software form can even be connected together to run a distributed application
    in the cloud. The benefit is, then, maximized by the possibility of speeding up
    the building of large cloud applications that are scalable. Containerization was
    originally developed as a way to separate namespaces in Linux for security reasons
    for protecting the kernel from the execution of applications that could have questionable
    security or authenticity. After this came the idea of making these “partitions”
    efficient and portable. LXC [10] was probably the first true container system,
    and it was developed as part of Linux. Additionally, Docker [4] was then developed
    as a system capable of deploying LXC containers on a PaaS environment. The applications
    running with containers are virtualized. In the specific case of Docker’s native
    environment, there is no hypervisor. There is a daemon in the kernel that provides
    the isolation across containers and connects the existing workloads to the kernel.
    Modern containers usually include a minimal operating system (e.g. VMWare’s Photon
    OS) with the sole objective of providing basic local services for the hosted applications.
    Microservices. The concept of microservices has a natural fit to containers, and
    it provides an alternative to the monolithic architecture pattern that is the
    traditional architectural style of enterprise applications. The microservice architecture
    structures applications as collections of loosely coupled, small, modular services
    that provide business capabilities and in which every service runs a unique process
    and communicates through well-defined, lightweight mechanisms. Microservices are
    functions that can operate for different applications like libraries, that contact
    them via an API to produce a discrete output. In monolithic applications, these
    functions would be instantiated redundantly: one per application. Netflix [118]
    streaming video service provider uses microservices. Modern containers include
    only the basic services needed for a given system. Orchestration services such
    as Kubernetes and Mesosphere Marathon manage the replication and removal of container
    images depending on the traffic patters to/from the workloads of microservices.
    Different protocols are possible for communication across microservices like HTTP;
    however, DevOps professionals mostly choose REST (Representational State Transfer)
    given its lower complexity as compared to other protocols. Microservices support
    the continuous delivery/deployment of large, complex applications, that yields
    agile software provisioning. Given its scalability, it is considered a particularly
    interesting pattern when it is needed to support a broad range of platforms and
    devices. 4.6.2. Cloud deployment and management There are various alternatives
    to designing and developing a cloud computing infrastructure and manage it such
    as Amazon Elastic Compute Cloud (Amazon EC2) [21], Microsoft Azure [110], CloudStack
    [22], OpenStack [12], OpenNebula [11], Eucalytus [6], or IBM Cloud [9], among
    others. They offer compute and storage services on the basis of an IaaS model,
    except for Google App Engine [8] and Azure; the latter offers a PaaS model on
    which it is possible to deploy web applications and scalable mobile backends.
    The technologies that provide an IaaS model are typically based on low-level virtual
    machine monitors (VMMs) that support the construction of virtual execution environments
    or virtual machines. Most of the previous technologies are based on either Xen
    [15], VMware [14], or KVM [96] VMMs and have a native Linux host. This is true
    except for IBM Cloud that also uses the above virtualization. On the other hand,
    the technologies that provide PaaS are based on lighter weight virtualization
    models such as application containers in the case of Google App Engine or OS virtualization
    for Microsoft Azure. Among the main benefits of this model is the maintenance
    cost as users do not have to configure nor fine tune any backend server. User
    applications deployed in this type of environments can use APIs to access a number
    of available services just as data base interfacing (through SQL queries, etc.)
    or user authentication. In addition, applications availability is also managed
    by the platform, and they are automatically scaled depending of the amount of
    incoming traffic, so users only pay for the amount of resources used. A number
    of problems have been addressed over the last decade for data center management.
    Precisely, virtual machine placement has been one of the most popular problems
    addressed by the scientific community that has produced many contributions such
    as [105]. Energy consumption has also received great attention; some researchers
    have contributed algorithms to optimize virtual machine placement and energy consumption
    such as [43] through live migration based on values of usage thresholds considering
    multiple resources, therefore targeting two of the greatest problems of data centers.
    Another research problem in cloud is QoS-aware data delivery to users. One of
    the bottlenecks in a data center that hinders performance is the networking across
    servers with kilometers of cables and terabytes of exchanged data across inhouse
    servers. Quality of service provisioning is concerned also with a number of very
    common activities such as effective resource management strategies [28] including
    virtual machine migration, service scaling, service migration, or on-the-fly hardware
    configuration changes. These may all affect the quality experienced by data delivery
    to users. One of todays’ open problems in cloud computing management is managing
    the complexity introduced by geographically distributed data centers. Some authors
    have proposed the design of an integrated control plane [40] that brings together
    both computation resources and network protocols for managing the distribution
    of data centers. Timely traffic delivery is essential to guaranteeing quality
    of service to applications, services, and users. Traffic engineering relies on
    the appropriate networking mechanism over LSP (Label Switched Paths) that are
    set at core networks and are controlled by the control plane. Path computation
    is essential to achieving the goals of traffic engineering. Actually, the IETF
    (Internet Engineering Task Force) promoted the Path Computation Element (PCE)
    architecture as a means to overcome the inefficiencies encountered by the lack
    of visibility of some distributed network resources. The core idea of PCE is a
    dedicated network entity destined to the path computation process. A number of
    initiatives for using the PCE also for cloud provisioning have been further researched
    like [124]. Predictable cloud computing technologies. The penetration of virtualization
    technology has paved the way for the integration of different functions over the
    same physical platform. This effect of virtualization technology has also arrived
    to the real-time systems area supporting the integration of a number of functions
    of heterogeneous criticality levels over the same physical platform. The design
    of mixed criticality systems (MCS) [39] is an important trend that supports the
    execution of various applications and functions of different criticality levels.
    Real-time research applied to technologies has improved the capacities of hypervisors
    to ensure full isolation across virtual machines that are called partitions. Partitions
    are fully independent and are scheduled by the hypervisor according to some scheduling
    policy. To comply with the real-time requirements, hierarchical scheduling is
    used most of the time due to its simplicity that favors timeliness; however, still
    the most complex point in this domain is the integration of the communication
    and distribution technology into partitioned systems. In [60], it is shown how
    a distributed partitioned system can be naturally integrated with a hierarchical
    scheduling mechanism to ensure timeliness of the communications when using distribution
    software under a number of still severe restrictions. 4.6.3. Messaging middleware
    Existing middleware solutions still have much room for improvement in order to
    fulfill the requirement interconnecting large numbers of devices in IoT scenarios,
    as many IoT devices are resource constrained. To overcome this, a variety of solutions
    have recently been developed and new ones are progressively emerging. We survey
    a few of the most popular solutions used in connecting IoT devices in what follows.
    Message Queuing Telemetry Transport (MQTT). MQTT [70] was originally developed
    in 1999 and has recently become an OASIS standard starting from version 3.1.1.
    It is a connectivity protocol to support machine-to-machine (M2M) communications
    in IoT. Since the goal was to support the IoT resource-constrained devices, it
    is designed to be a lightweight technology. MQTT supports a publish/subscribe
    messaging transport. Example use cases include sensors communicating to a broker
    via a satellite link, over occasional dial-up connections with health care providers,
    and in a range of home automation and small device scenarios. Even mobile applications
    can make use of MQTT because of its support for small size, low power usage, smaller
    data packet payloads, and efficient distribution of information to one or many
    receivers. Its publish/subscribe communication model uses the term “client” to
    refer to entities that either publish data related to given topics or subscribe
    to topics to receive their associated data; while the term “server” refers to
    mediators/brokers that relay messages between the clients. MQTT operates over
    TCP or any other transport protocol that supports ordered, lossless message communication.
    MQTT supports three levels of QoS for message delivery: (a) at-most-once, (b)
    at-least-once, and (c) exactly once. Message Brokers. MQTT is somehow an example
    of a publish/subscribe message broker. In addition to MQTT, a number of message
    brokers like Apache Kafka, AMQP (Advanced Message Queue Protocol), and Active
    MQ are finding applications in areas of IoT. Apache Kafka [23] is an open source
    distributed streaming platform used to build real-time data pipelines between
    different systems or applications. They provide high throughput, low latency and
    fault tolerant pipelines for streaming data with a tradeoff between performance
    and reliability. They are deployed as a cluster of servers which handles the messaging
    system with the help of four core APIs, namely, producers, consumers, streams,
    and connectors. The other important part of the Kafka architecture is the topic,
    broker, and records. Here, data is divided into topics, which is further divided
    into partitions for the brokers to handle them. Apache Zookeeper is used to provide
    synchronization between multiple brokers. In addition, among the most popular
    data buses is the data-centric DDS (Data Distribution Service) [72] which has
    been extended in a number of ways such as [61] for supporting real-time reconfiguration
    in dynamic service-oriented applications. Constrained Application Protocol (CoAP).
    The CoAP protocol [37], which is defined as an Internet Standard in RFC 7252,
    is a web transfer protocol for use by resource-constrained devices of IoT, e.g.,
    8-bit microcontrollers with small ROM and RAM. Like MQTT, CoAP is also meant to
    support M2M communications. CoAP provides a request/response interaction model
    in contrast to the publish/subscribe model between application endpoints. It provides
    built-in discovery of services and resources. CoAP supports key web-related concepts
    such as URIs (Uniform Resource Identifier) and Internet media types. It leverages
    the REST architectural pattern that has been highly successful in the traditional
    HTTP realm. Thus, in CoAP, servers make their resources available as URLs and
    clients can use commands such as GET, PUT, POST, and DELETE to avail of these
    resources. Due to the use of the REST architectural pattern, it is seamless to
    combine HTTP with CoAP thereby allowing traditional web clients to access an IoT
    sensor device. CoAP uses UDP as its transport layer. Other protocols like DTLS
    (Datagram Transport Layer Security) are also applicable. Like HTTP, CoAP allows
    payloads of multiple different types, e.g., XML, JavaScript Object Notation (JSON),
    or Concise Binary Object Representation (CBOR). Node-RED. Node-RED [34] is technically
    not a middleware but rather a browser-based model-driven tool to wire the flows
    between IoT devices. The tool then allows a one-click approach to deploy the capabilities
    in the runtime environment. Node-RED uses Node.js (a JavaScript execution engine)
    behind the scenes. The flows are stored as JSON objects. Thus, we can consider
    Node-RED as a model-driven middleware capability. Akka.  [1] Akka is an open-source
    event-driven middleware framework that uses the Actor Model [67] to provide a
    platform to build scalable, resilient, and responsive distributed and concurrent
    applications. Akka runs on a Java virtual machine (JVM) and supports actors written
    in Java and Scala. Actors in Akka are lightweight event-driven processes that
    provide abstractions for concurrency and parallelism. Akka follows the “let it
    crash” model for fault-tolerance in order to support applications that self-heal
    and never stop. Distributed applications in Akka are made of multiple actors distributed
    amongst a cluster of member nodes. Cluster membership is maintained using Gossip
    Protocol, where the current state of a cluster is randomly propagated through
    the cluster with preference to members who have not seen the latest state. Actors
    within a cluster can communicate with each other using mediators that facilitate
    point-to-point as well as pub/sub interaction patterns. Each node can host a single
    mediator in which case discovery becomes decentralized, or particular nodes of
    a cluster can be designated to host a mediator in which case discovery becomes
    centralized. Akka’s message delivery semantics facilitates three different QoS
    policies – (a) at-most-once, (b) at-least-once, and (c) exactly-once. Robot Operating
    System (ROS). ROS  [13] is a framework that provides a collection of tools, libraries,
    and conventions to write robust, general-purpose robot software. It is designed
    to work with various robotic platforms. ROS nodes are processes that perform computation,
    and these nodes combined together form a network (graph) of nodes that communicate
    with each other using pub/sub or request/response interaction patterns. Pub/sub
    interaction is facilitated via topics. Multiple publishers and subscribers can
    be associated with a topic. Request/response interaction, on the other hand, is
    done via a service. A node that provides a service, offers its service under a
    string name, and a client calls a provided service by sending the request message
    and awaiting the reply. Both, topics and services, are monitored by the ROS Master.
    Therefore, the master is a single point of failure that performs the task of matching
    nodes that need to communicate with each other, regardless of the interaction
    pattern. 4.6.4. Complex Event Processing (CEP) CEP is used in multiple points
    for IoT analytics (e.g. Edge, Cloud etc). In general, event processing is a method
    for tracking and analyzing streams of data and deriving a conclusion from them,
    while the data is in motion. A number of CEP engines like Siddhi, Apache flink
    and Esper are available for stream processing. These CEP tools allow the users
    to write queries over the arriving stream of data which can be utilized to determine
    anomalies, sequences, and patterns of interest. For example, Siddhi [143] is an
    open source CEP server with a very powerful SQL query like language for event
    stream processing. It allows the users to integrate the data from any input system
    like Kafka, MQTT, file, and websocket with data in different formats like XML,
    JSON, or plain text. After the data has been received at the input adapters, queries
    like patterns, filters, sequences, windows and pass through can be applied on
    the data at the even stream to perform some real time event processing. The data
    obtained after processing can be published over web-based analytics dashboard
    to monitor the meaningful processed data. 4.6.5. Transaction management During
    2016, several open-source platforms for transaction management for the financial
    services industry have appeared, e.g. Hyperledger, Chain Core, or Corda, besides
    other open-source platforms such as Ethereum and Monax that were released in precedent
    years. Among the most relevant transaction management systems we find Hyperledger4,
    that is a Linux implementation for blockchain. Hyperledger (or the Hyperledger
    project) is an umbrella project of open source blockchains and related tools [53]
    started in December 2015 by the Linux Foundation [16]. Hyperledger’s goal is to
    develop blockchain-based distributed ledgers following the Linux philosophy of
    collaborative development. The Hyperledger project is partitipated by a large
    number of partners contributing different tools individually or in collaboration.
    Burrow5 is a blockchain client that includes a virtual machine (Ethereum). Fabric,6
    is an architecture that defines the execution of smart contracts (namely chaincode
    in Fabric); the processes for consensus and membership, and the roles of the participating
    nodes. Iroha7 is another Hyperledger tool similar to Fabric but targeted at mobile
    aplications. Lastly, Sawtooth8 is a tool that provides the Proof of Elapsed Time
    consensus protocol based on a lottery-design consensus protocol; this tool is
    based on trusted execution environments such as SGX9. 4.6.6. Service configuration
    and deployment technologies One of the most complex problem in distributed computing
    is remote management of computation environment and resources. This includes management,
    update and configuration of the computation environment as well as remote deployment
    of tasks. We highlight a few of the representative technologies that provide this
    functionality. Kubernetes. It is an open source platform that facilitates the
    task of running applications in clouds, whether private or public. It supports
    the automatic deployment and operation of application containers. Applications
    can be scaled on the fly, and the usage of hardware can be limited to required
    resources only. Whenever an application need be released, Kubernetes allows generating
    container images; it can schedule and run application containers on clusters of
    physical or virtual machines. One of the most interesting characteristics is that
    it supports continuous development, integration, and deployment with quick rollbacks.
    Also, it raises the level of abstraction as compared to running an operating system
    on a virtualized hardware; in this way, it is an application that is run on an
    operating system that uses logical resources. In Kubernetes, applications are
    composed of smaller microservices that are independent pieces of code that can
    be deployed and managed dynamically. Paradrop. It is a platform that offeres computing
    and storage resources over the end nodes supporting the development of services
    [98]. A key element is the WiFi access point as it has all information about its
    end devices and manages all the traffic across them. Paradrop provides an API
    for third party developers to create and manage their services across different
    access points, that are isolated in containers (called chutes). Also, it provides
    a cloud backend to install dynamically the access points and the third party containers,
    and to instantiate and revoke them. Paradrop uses lightweight Linux containers
    [97] instead of virtual machines as the virtualization mechanism to deploy services
    on the network routers. The computational requirements of social dispersed computing
    applications make it necessary to provide efficient execution over the nodes.
    In this way, control over the execution of all nodes, especially on resource limited
    ones, needs to be put in place. Following, we describe one of the technologies
    that provides such a functionality. Mesos [68] is a thin software acting as a
    resource manager that enables fine-grained sharing across different and highly
    diverse cluster computing frameworks by providing them with a common interface
    to access the cluster resources. Control of task scheduling and execution is taken
    by the frameworks; this allows each framework to decide on execution of activities
    according to its specific needs and better supports the independent evolution
    of frameworks. Mesos consists of a master and slave daemons, frameworks, and tasks.
    The master process manages the slave daemons running on each cluster node. Moreover,
    frameworks run tasks on these slave daemons. Each framework running on Mesos has
    two components: a scheduler and a executor. The scheduler registers with the master
    in order to be offered resources; the executor process is launched on the slave
    daemons to run the tasks. Fine-grained resource sharing across the frameworks
    is implemented using resource offers, that are lists of free resources on multiple
    slaves. The organizational policies (priority or fair sharing) determine how the
    master decides on how many resources to offer to each framework. Mesos defines
    a plugable allocation module to let organizations define their own allocation
    policies. An important characteristic is that Mesos provides performance isolation
    between framework executors running on the same slave by leveraging existing isolation
    mechanisms of operating systems. 4.6.7. Service coordination Distributed systems
    also need technologies that can ensure that the related services remain coordinated.
    We discuss a few state of the art technologies here. Zookeeper. It is an open
    source technology [24] that provides key services for large scale systems containing
    large numbers of distributed processes; these services are configuration, synchronization,
    group services, and naming registry. Typically, these services can be highly complex
    to design and implement and they are used by the vast majority of distributed
    applications. Zookeeper has a simple architecture in the form of a shared hierarchical
    namespace to facilitate process coordination. Also, it is a reliable system that
    can continue to run in the presence of a node failure; it provides redundant services
    for ensuring high availability. Data storage is performed in a hierarchical name
    space such as a file system or a tree data structure. It supports data updates
    in a totally ordered manner as in an atomic broadcast system. Fault tolerance
    and security is an important characteristic in coordination services that must
    be well supported not only considering simple faults (crashes) or attacks (invalid
    access). DeepSpace [33] is a distributed coordination service that provides Byzantine
    fault tolerance [90] in a tuple space abstraction. It provides secure, reliable,
    and available operation in the presence of less than a third of faulty service
    replicas. Also, it has a content-addressable confidentiality scheme that allows
    to store critical data. The maturity level, community, services, and penetration
    of Zookeeper is, however, not comparable. Girafe. It is a scalable coordination
    service [137] for cloud services. It organizes the coordination of servers by
    means of interior-node-disjoint trees; it uses a Paxos protocol for strong consistency
    and fault tolerance; and it supports hierarchical data organization for high throughput
    and low latency. ZooNet. It is a coordination service [93] that addresses the
    problems of coordination of applications running in multiple geographic regions;
    these applications need to trade-off between performance and consistency, and
    ZooNet provides a modular composition design for this purpose. Consul. Consul
    is a system that enables service discovery and configuration in a distributed
    infrastructure [2]. Consul clients provide services (e.g. MySQL) and other clients
    can discover the providers of such given service. Health checks for services are
    also enabled with respect to specific characteristics such as if a service is
    up and running or if it is using a certain memory size. Health checks can be used
    to route traffic avoiding unhealthy hosts. It also provides multi-region datacenters.
    Consul is based on agents. Each node that is part of Consul (i.e., that provides
    services to it) runs a Consul agent that is responsible for health checking the
    services on the node as well as the node itself. Agents interact with Consul servers
    that store data and replicate it. Servers elect a leader. Components that need
    to locate a service query any of the servers or any of the agents; agents automatically
    forward requests to the servers. Location of services residing in remote data
    centers is performed by the local servers that forward the queries to the remote
    data center. Etcd. Etcd is a key value store [112] which internally uses raft
    [123] consensus algorithm. Etcd can be used to build a discovery service. However,
    it is primarily used to store information across a set of nodes. Kubernetes uses
    etcd for managing the configuration data across the cluster. 4.6.8. Networking
    technologies Networking is a concept that is critical for distributed computing,
    including edge computing. The recent avdances in software defined networking have
    provided mechanisms to increase the flexibility of this crucial layer. We provide
    a brief overview here. Software defined networks (SDN). Social dispersed computing
    applications require flexible network connections to support the dynamic geographic
    distribution of end users and fine tune the parameters of the communication. Although
    the advances in network technology and bandwidth increase have been impressive,
    still IP networks have until recently been structured in an manner that did not
    achieve sufficient flexibility. Actually, the boost of Internet has occurred over
    IP networks that are vertically integrated [134] in which control and data planes
    are bundled together [58] inside the network devices. However, this design makes
    it hard to reconfigure in the event of adverse load conditions, faults, etc. The
    control plane is the logic that decides how to handle the network packets; whereas
    the data plane is the logic in charge of forwarding the packets as indicated by
    the control plane. Network operators configure each network device individually
    using low-level (and sometimes vendor specific) logic; all data packets are treated
    the same by the switch that starts sending every packet going to the same destination
    along the same path. Originally, SDN focused exclusively on the separation of
    the control and data planes. Software defined networking brings in the promise
    for solving the above limitations in a flexible way by providing the needed mechanisms
    for a network that will be programmable. Kreutz et al.[83] provide a comprehensive
    survey of the technologies towards SDN and its adoption. It presents the main
    differences of the conventional networking as compared to SDN, describing the
    role of the SDN controller over which a number of network applications (like MAC
    learning, routing algorithms, intrusion detection system, and load balancer) run.
    The above is the classic SDN scenario in which a controller (that is an application
    running somewhere on some server) sends the rules for handling the packets to
    the switch; then, switches that are the data plane devices request guidance to
    the controller whenever needed and provide the controller with information about
    the traffic that they handle. The communication between the controllers and the
    switches happens through well defined interfaces. The interface that enables communication
    between the SDN controller and the network nodes (that are physical and virtual
    switches and routers) is called the controller’s southbound interface, and it
    is typically the OpenFlow [107] specification. OpenFlow has become the most important
    architecture for managing large scale complex networks and has, therefore, become
    the major bet for SDN. This is a specification that need be applied in matured
    systems through implementations. Hu et al. [69] provide a survey of the target
    applications, the language abstraction, the controller functions and inner workings,
    the virtualization that is achieved, quality of service properties, security issues
    and its integration in different networks. OpenFlow security issues are very relevant
    especially in large scale deployments. Kandoi and Antikainen [76] describe the
    two types of denial of service (DoS) attacks that are specific to OpenFlow SDN
    networks discovering some key configurations (like the timeout value of a flow
    rule and the control plane bandwidth) that directly affect the capability of a
    switch and it identifies mitigation actions for them. The research in SDN proceeds
    in parallel with the improvement of the control plane algorithms searching for
    better and more efficient ways to route traffic. Especially cloud services with
    soft real-time requirements experience the delays of wide area IP network interconnects
    across geographically distributed locations. To address this problem, Bessani
    et al. [32] propose a routing mechanism for providing latency and reliability
    assurances for control traffic in wide-area IP networks with a just in time routing
    that routes deadline constrained messages that are control messages at the application
    layer with the goal of achieving a non-intrusive solution for timely and reliable
    communication. 5. Challenges in social dispersed computing Having explained the
    different computation technologies that cover the range from utility cloud computing
    to edge computing, we can now revisit the concept of social dispersed computing
    and identify the key challenges that still exist. For researchers, these points
    also serve as a summary of current research interests and opportunities for the
    community. The primary challenge of social dispersed computing is mobility. Consider
    that nodes in the social routing application described earlier are mobile, the
    system must be cognizant of intermittent connectivity caused due to high mobility.
    Thus, new mechanisms have to be built for implementing handover strategies that
    account for multi-tenancy on a local cloud in which multiple service providers
    can be present to ensure backup. Additionally, given the high mobility of users,
    managing volatile group formation may play a key role in the efficient collection
    of data and in the transmission of only the needed data that are relevant for
    particular groups. For this, it will be needed to incorporate dynamic transaction
    management functionality. The second challenge emanates from the resource constraints
    of the system, which suggests that only required applications should be running
    on the computation platforms. However, this leads to an interesting question of
    what are the required applications. In the past, “goal-driven” computing has been
    used in high criticality, but mostly static systems [127]. However, a social dispersed
    application implies that the end nodes or user nodes act in a social way; they
    will exchange information, sharing part of their computations among the participant
    users, the local fog nodes, and partly with the cloud nodes. A number of different
    services may run at these three layers: user/end and fog, edge, and cloud. Also,
    some services may be split across the different layers. As all participant nodes
    are driven by a shared goal, they will have to share part of their data and computations
    in a synchronized way and the exchanged data will have to be appropriately tagged
    in the temporal domain to meet the global goal. Thus goal-driven service orchestration
    is a third challenge in these systems. Another challenge includes service synchronization
    and orchestration. In the cloud model, services are provided to clients in a client-server
    type of interaction. In social dispersed computing, end nodes come into play,
    requiring interaction not only with the cloud servers. End nodes will interact
    with other end nodes for fast information exchange; with the fog nodes for data
    bulk exchange and for low latency gathering of information derived from heavy
    processing; and with the cloud servers for obtaining results derived from more
    complex data intensive computations like machine learning services for longer
    term prediction. Social dispersed computing applications will need that supporting
    architectures add an abstraction layer that meets the coordination and orchestration
    requirements by providing smooth cooperation through the end nodes. This layer
    will contain the required logic to orchestrate the interaction between fog servers
    and the central cloud, as well as the interaction across fogs. Timely operation
    and stringent quality of service demands is yet another challenge. Some social
    dispersed applications need to provide real-time services to users. This requires
    to put in place a number of physical resource management policies that ensure
    time bounded operation. Fog servers will have high consolidation, so virtualization
    techniques will have to be properly applied in conjunction with scheduling policies
    that ensure timely operation for those real-time services and avoidance of execution
    interference among applications in the presence of possibly computationally greedy
    functions. Understanding that failures will be more common in social dispersed
    computing applications is important. Thus, the soft state of applications must
    be properly managed. End nodes may interact heavily in social dispersed applications,
    and these interactions may not assume that data nor the infrastructure are available
    at all times. There is a noticeable difference with respect to the cloud model
    that handles hard state and persistent data. Considering soft state brings in
    much more complex scenarios in which fall back operations will need to be considered
    for the end nodes to run recovery actions. In social dispersed computing, the
    focus shifts towardstheserviceand thedata, and other characteristics such as the
    location become less important. A service may reside on a number of fog servers
    as well as partly in the cloud. Then, the traditional client-server structure
    falls short as IP based operations become inappropriate for handling service and
    data centric interactions across nodes (mainly the fog and end nodes). A service
    centric design that relies on data centric interaction and information exchange
    better adjusts to this level of complexity. As a result, service offloading strategies
    and selection of target infrastructure processing point are going to be a difficult
    problem. In a social dispersed applications, it will be beneficial to draw a clever
    server processing hierarchy. Where to process, whether at the cloud, at the edge,
    or at the fog, and why are decisions that will have to be taken based on a per
    application basis. We believe that the target point for running a specific service
    should be selected according to the computational complexity of the service itself
    (e.g. online video streaming probably at the edge servers, face recognition probably
    at the fog). There is strong need for designing efficient service partitioning
    schemes that make use of the end, fog, edge, and cloud infrastructures as a complementary
    overall execution platform that will speed up the dispersed computations for the
    social interactions. Lastly, autonomy, interaction, mobility and openness are
    the characteristics that the Multi Agent System (MAS) paradigm covers from a theoretical
    and practical perspective. MAS technology provides models, frameworks, methods
    and algorithms for constructing large-scale open distributed computer systems
    and allows to cope with the (high) dynamicity of the systems topology and with
    semantic mismatches in the interaction, both natural consequences of the distributed
    and autonomous nature of the components. Open distributed systems are going to
    be the norm in the software development industry of the future, and the interoperation
    of the software entities will need to rely on a declarative concept of agreement
    that is autonomously signed and executed by the entities themselves. The generation
    of agreements between entities will need to integrate semantic, normative, organization,
    negotiation and trust techniques. As evidenced by the partial list of technical
    problems given above, there is a complex technical challenge in the design and
    development of social dispersed computing applications that is multi-faceted.
    Addressing some of these problems simultaneously may result in the appearance
    of emerging problems that have still not been envisioned. 6. Discussion and conclusions
    A number of computing paradigms have appeared through the years that, currently,
    put in practice in the development of a number of systems across different application
    domains. Newer computing paradigms coexist with the more classical ones and each
    has brought into scene their accompanying set of tools and technologies in their
    support. This large number of computation design options allows us to implement
    systems of a complexity that was not previously imagined and that increases day
    by day. Some of the more recent paradigms have still not been sufficiently applied
    in practice and even create confusion as to what they mean to different scientific
    communities. This paper situates the computing paradigms from the times where
    they were used as a utility to provide practical solutions to given problems that
    could be solved in a faster and more efficient manner; up to today where users
    are progressively accustomed to having commodity solutions at reach which go some
    steps beyond the mere practicality of automating tasks to a point in which they
    even consent to share information and knowledge online to ease their lives in
    some way or to obtain some other non primary benefit in exchange. In this paper,
    we presented a review of core computing paradigms that have appeared in the distributed
    system community in the last two decades, focusing on cloud computing, edge computing,
    and fog computing; and we have analyzed how they are refered to across the literature
    to disambiguate their definition. We identify a new paradigm, social dispersed
    computing, that borrows from some existing paradigms and is essentially enabled
    by powerful technology and tools that are available today. Then, we described
    such a set of current computing technologies or services, which when augmented
    with the computing paradigms can enable interesting social dispersed computing
    applications. We exemplify this new paradigm by describing three example applications,
    two from the transportation domain and one from the energy domain. These applications
    can run successfully on both edge and fog computing devices. However, as we imagine
    more complex and integrated applications, we must start considering the challenges
    we mentioned in Section 5. Current computing technologies only partially meet
    these challenges, giving the community a great opportunity to explore this broad
    research space. References [1] Akka, (https://www.akka.io). Last accessed January
    2018. Google Scholar [2] Consul, (https://www.consul.io). Last accessed February
    2018. Google Scholar [3] Distributed Cloud Storage, (https://www.storj.io). Last
    accessed February 2018. Google Scholar [4] Docker, (https://www.docker.com). Last
    accessed February 2018. Google Scholar [5] Ethereum Block chain app platform,
    (https://www.ethereum.org/). Last accessed February 2018. Google Scholar [6] Eucalyptus
    cloud computing platform, (https://github.com/eucalyptus/eucalyptus). Last accessed
    January 2018. Google Scholar [7] Foghorn, (https://www.foghorn.io). Last accessed
    February 2018. Google Scholar [8] Google cloud platform – App Engine, (https://cloud.google.com/appengine/).
    Last accessed January 2018. Google Scholar [9] IBM Cloud, (https://www.ibm.com/cloud/).
    Last accessed January 2018. Google Scholar [10] LinuX Containers, (https://www.linuxcontainers.org).
    Last accessed January 2018. Google Scholar [11] Opennebula, (http://www.opennebula.orga).
    Last accessed January 2018. Google Scholar [12] OpenStack, (https://www.openstack.org/b).
    Last accessed January 2018. Google Scholar [13] ROS, (http://www.ros.org). Last
    accessed January 2018. Google Scholar [14] VMWare, (http://www.vmware.com). Last
    accessed January 2018. Google Scholar [15] The Xen project 4.8.1 version April
    2017, (http://www.xenproject.org). Last accessed January 2018. Google Scholar
    [16] The Linux Foundation. Linux Foundation unites industry leaders to advance
    Blockchain technology, 2015. Last accessed December 2017. Google Scholar [17]
    Brooklyn microgrid, (https://medium.com/thebeammagazine/can-the-brooklyn-microgrid-project-revolutionise-the-energy-market-ae2c13ec0341).
    Last accessed June 2018. Google Scholar [18] DARPA Defense Advanced Research Projects
    Agency, Dispersed Computing, 2017, Last accessed on October 2017. Google Scholar
    [19] R. Al Mallah, A. Quintero, B. Farooq Distributed classification of urban
    congestion using VANET IEEE Trans. Intell. Transp. Syst. (2017) Google Scholar
    [20] A. Alzahrani, A. Alalwan, M. Sarrab Mobile cloud computing: advantage, disadvantage
    and open challenge Proceedings of the Seventh Euro American on Telematics and
    Information Systems, (EATIS 2014), Valparaiso, Chile (2014), pp. 1-4, 10.1145/2590651.2590670
    Google Scholar [21] Amazon, Amazon Elastic Compute Cloud (Amazon EC2), (https://aws.amazon.com/es/ec2/).
    Last accessed January 2018. Google Scholar [22] Apache Software Foundation, Apache
    CloudStack, (https://cloudstack.apache.org/). Last accessed January 2018. Google
    Scholar [23] Apache Software Foundation, Apache Kafka - A distributed streaming
    platform, v0.8.2.0, 2015, (https://kafka.apache.org). Last accessed December 2017.
    Google Scholar [24] Apache Software Foundation, Apache Zookeeper v3.5.3, 2017,
    (https://zookeeper.apache.org). Last accessed December 2017. Google Scholar [25]
    A. Artikis, J. Pitt A formal model of open agent societies Proceedings of the
    Fifth International Conference on Autonomous Agents, AGENTS ’01, ACM (2001), pp.
    192-193, 10.1145/375735.376108 View in ScopusGoogle Scholar New York, NY, USA
    [26] A. Azaria, A. Ekblaw, T. Vieira, A. Lippman Medrec: using blockchain for
    medical data access and permission management Proceedings of the International
    Conference on Open and Big Data (OBD), IEEE (2016), pp. 25-30 CrossRefView in
    ScopusGoogle Scholar [27] M. Beck, M. Werner, S. Feld, S. Schimper Mobile edge
    computing: taxonomy Proceedings of the Sixth International Conference on Advances
    in Future Internet (2014) Google Scholar [28] A. Beloglazov, R. Buyya Managing
    overloaded hosts for dynamic consolidation of virtual machines in cloud data centers
    under quality of service constraints IEEE Trans. Parallel Distrib. Syst., 24 (7)
    (2013), pp. 1366-1379, 10.1109/TPDS.2012.240 View in ScopusGoogle Scholar [29]
    J. Benet, IPFS – content addressed, versioned, P2P file system (draft 3), (https://www.ipfs.io).
    Last accessed February 2018. Google Scholar [30] K. Benson, C. Fracchia, G. Wang,
    Q. Zhu, S. Almomen, J. Cohn, L. D’arcy, D. Hoffman, M. Makai, J. Stamatakis, N.
    Venkatasubramanian Scale: safe community awareness and alerting leveraging the
    internet of things IEEE Commun. Mag., 53 (12) (2015), pp. 27-34, 10.1109/MCOM.2015.7355581
    View in ScopusGoogle Scholar [31] J. Bergquist, A. Laszka, M. Sturm, A. Dubey
    On the design of communication and transaction anonymity in Blockchain-based transactive
    microgrids Proceedings of the First Workshop on Scalable and Resilient Infrastructures
    for Distributed Ledgers (SERIAL), ACM (2017), pp. 3:1-3:6, 10.1145/3152824.3152827
    Google Scholar [32] A. Bessani, N.F. Neves, P. Veríssimo, W. Dantas, A. Fonseca,
    R. Silva, P. Luz, M. Correia Jiter: just-in-time application-layer routing Comput.
    Netw., 104 (2016), pp. 122-136, 10.1016/j.comnet.2016.05.010 View PDFView articleView
    in ScopusGoogle Scholar [33] A.N. Bessani, E.P. Alchieri, M. Correia, J.S. Fraga
    Depspace: a byzantine fault-tolerant coordination service SIGOPS Oper. Syst. Rev.,
    42 (4) (2008), pp. 163-176, 10.1145/1357010.1352610 Google Scholar [34] M. Blackstock,
    R. Lea Toward a distributed data flow platform for the web of things (distributed
    node-red) Proceedings of the Fifth International Workshop on Web of Things, ACM
    (2014), pp. 34-39 CrossRefView in ScopusGoogle Scholar [35] O. Boissier, R.H.
    Bordini, J.F. Hübner, A. Ricci, A. Santi Multi-agent oriented programming with
    Jacamo Sci. Comput. Program., 78 (6) (2013), pp. 747-761, 10.1016/j.scico.2011.10.004
    View PDFView articleView in ScopusGoogle Scholar [36] F. Bonomi, R. Milito, Zhu
    J., S. Addepalli Fog computing and its role in the Internet of Things Proceedings
    of the First Edition of the MCC Workshop on Mobile Cloud Computing, MCC’12, ACM,
    New York, NY, USA (2012), pp. 13-16, 10.1145/2342509.2342513 Google Scholar [37]
    C. Bormann, A.P. Castellani, Z. Shelby COAP: an application protocol for billions
    of tiny Internet nodes IEEE Internet Comput., 16 (2) (2012), pp. 62-67 View in
    ScopusGoogle Scholar [38] A. Botta, W. De Donato, V. Persico, A. Pescapé On the
    integration of cloud computing and Internet of Things Proceedings of the 2014
    International Conference on Future Internet of Things and Cloud (FiCloud), IEEE
    (2014), pp. 23-30 CrossRefView in ScopusGoogle Scholar [39] A. Burns, R. Davis,
    Mixed criticality systems – a review, 2016, Report. University of York. Google
    Scholar [40] J. Buysse, M.D. Leenheer, L.M. Contreras, J.I. Aznar, J.R. Martinez,
    G. Landi, C. Develder NCP+: an integrated network and its control plane for cloud
    computing Opt. Switch. Netw., 11 (2014), pp. 137-152 View PDFView articleView
    in ScopusGoogle Scholar [41] Chen C., Fu J., Sung T., Wang P., Jou E., Feng M.
    Complex event processing for the Internet of Things and its applications Proceedings
    of the IEEE International Conference on Automation Science and Engineering (2014)
    Google Scholar [42] Cho H., Jung J., Cho B., Jin Y., Lee S.W., Baek Y. Precision
    time synchronization using IEEE 1588 for wireless sensor networks Proceedings
    of the 2009 International Conference on Computational Science and Engineering,
    2 (2009), pp. 579-586, 10.1109/CSE.2009.264 View in ScopusGoogle Scholar [43]
    A. Choudhary, S. Rana, K.J. Matahai A critical analysis of energy efficient virtual
    machine placement techniques and its optimization in a cloud computing environment
    Procedia Comput. Sci., 78 (2016), pp. 132-138 View PDFView articleGoogle Scholar
    [44] A.H. Chow, A. Santacreu, I. Tsapakis, G. Tanasaranond, Cheng T. Empirical
    assessment of urban traffic congestion J. Adv. Transp., 48 (8) (2014), pp. 1000-1016
    CrossRefView in ScopusGoogle Scholar [45] W. Cox, T. Considine Structured energy:
    microgrids and autonomous transactive operation Proceedings of the 2013 IEEE PES
    Innovative Smart Grid Technologies (ISGT), IEEE (2013), pp. 1-6 CrossRefGoogle
    Scholar [46] G. Cugola, A. Margara Processing flows of information: from data
    stream to complex event processing ACM Comput. Surv. (CSUR), 44 (3) (2012), p.
    15 View in ScopusGoogle Scholar [47] O. Dag, B. Mirafzal On stability of islanded
    low-inertia microgrids Proceedings of the of 2016 Clemson University Power Systems
    Conference (PSC) (2016), pp. 1-7 CrossRefGoogle Scholar [48] E. Denti, A. Omicini,
    A. Ricci Coordination tools for MAS development and deployment Appl. Artif. Intell.,
    16 (2002), pp. 721-752 View in ScopusGoogle Scholar [49] Compensating transactions:
    when ACID is too much, JBoss Developer. (developer.jboss.org). Last accessed February
    2018. Google Scholar [50] R. Dewri, P. Annadata, W. Eltarjaman, R. Thurimella
    Inferring trip destinations from driving habits data Proceedings of the Twelfth
    ACM Workshop on Workshop on Privacy in the Electronic Society, WPES ’13, ACM,
    New York, NY, USA (2013), pp. 267-272, 10.1145/2517840.2517871 View in ScopusGoogle
    Scholar [51] K. Dolui, S.K. Datta Comparison of edge computing implementations:
    fog computing, cloudlet and mobile edge computing Proceedings of the Global Internet
    of Things Summit, Geneva, Switzerland (2017), pp. 1-6 Google Scholar [52] A. Dubey,
    G. Karsai, S. Abdelwahed Compensating for timing jitter in computing systems with
    general-purpose operating systems Proceedings of the 2009 IEEE International Symposium
    on Object/Component/Service-Oriented Real-Time Distributed Computing (2009), pp.
    55-62, 10.1109/ISORC.2009.28 View in ScopusGoogle Scholar [53] F. Ehsani, Blockchain
    in finance: from buzzword to watchword in 2016, (www.coindesk.com). 2016. Google
    Scholar [54] S. Eisele, A. Laszka, A. Mavridou, A. Dubey, Solidworx: a resilient
    and trustworthy transactive platform for smart and connected communities, ArXiv
    e-prints, 2018. Google Scholar [55] S. Eisele, I. Mardari, A. Dubey, G. Karsai
    Riaps: Resilient information architecture platform for decentralized smart systems
    Proceedings of the 2017 IEEE 20th International Symposium on Real-Time Distributed
    Computing (ISORC) (2017), pp. 125-132, 10.1109/ISORC.2017.22 View in ScopusGoogle
    Scholar [56] Gai K., Qiu M., Zhao H., Tao L., Zong Z. Dynamic energy-aware cloudlet-based
    mobile cloud computing model for green computing J. Netw. Comput. Appl., 59 (Supplement
    C) (2016), pp. 46-54, 10.1016/j.jnca.2015.05.016 View PDFView articleView in ScopusGoogle
    Scholar [57] A. García-Fornes, J.F. Hübner, A. Omicini, J.A. Rodríguez-Aguilar,
    V.J. Botti Infrastructures and tools for multiagent systems for the new generation
    of distributed systems Eng. Appl. AI, 24 (7) (2011), pp. 1095-1097, 10.1016/j.engappai.2011.06.012
    View PDFView articleView in ScopusGoogle Scholar [58] M. García-Valls, R. Baldoni
    Adaptive middleware design for CPS: Considerations on the OS, resource managers,
    and the network run-time Proceedings of the Fourteenth International Workshop
    on Adaptive and Reflective Middleware, ARM 2015 (2015), pp. 3:1-3:6 Google Scholar
    [59] M. García-Valls, T. Cucinotta, Lu C. Challenges in real-time virtualization
    and predictable cloud computing J. Syst. Archit., 60 (9) (2014), pp. 726-740,
    10.1016/j.sysarc.2014.07.004 View PDFView articleView in ScopusGoogle Scholar
    [60] M. García-Valls, J. Domínguez-Poblete, Touahria I.E., C. Lu Integration of
    Data Distribution Service and distributed partitioned systems J. Syst. Archit.,
    83 (2018), pp. 23-31, 10.1016/j.sysarc.2017.11.001 View PDFView articleView in
    ScopusGoogle Scholar [61] M. García-Valls, I.R. Lopez, L. Fernández-Villar iLAND:
    an enhanced middleware for real-time reconfiguration of service oriented distributed
    real-time systems IEEE Trans. Indust. Inf., 9 (1) (2013), pp. 228-236, 10.1109/TII.2012.2198662
    View in ScopusGoogle Scholar [62] A. Ghafouri, A. Laszka, A. Dubey, X. Koutsoukos
    Optimal detection of faulty traffic sensors used in route planning Proceedings
    of the Second International Workshop on Science of Smart City Operations and Platforms
    Engineering, ACM (2017), pp. 1-6 CrossRefView in ScopusGoogle Scholar [63] R.
    Ghosh, Y. Simmhan Distributed scheduling of event analytics across edge and cloud
    CoRR (2016) Google Scholar 1608.01537 [64] OpenFog architecture overview, White
    Paper, The OpenFog Consortium Architecture Working Group, 2016. Google Scholar
    [65] R.W. Hall Non-recurrent congestion: how big is the problem? Are traveler
    information systems the solution? Transp. Res. Part C Emerg. Technol., 1 (1) (1993),
    pp. 89-103 View PDFView articleView in ScopusGoogle Scholar [66] Y. Hara, E. Hato
    A car sharing auction with temporal-spatial OD connection conditions Transp. Res.
    Part B Methodol. (2017) Google Scholar [67] C. Hewitt, P. Bishop, R. Steiger A
    universal modular actor formalism for artificial intelligence Proceedings of the
    3rd International Joint Conference on Artificial Intelligence, Morgan Kaufmann
    Publishers Inc. (1973), pp. 235-245 Google Scholar [68] B. Hindman, A. Konwinski,
    M. Zaharia, A. Ghodsi, A.D. Joseph, R. Katz, S. Shenker, I. Stoica MESOS: a platform
    for fine-grained resource sharing in the data center Proceedings of the Eighth
    USENIX Conference on Networked Systems Design and Implementation, NSDI’11, USENIX
    Association, Berkeley, CA, USA (2011), pp. 295-308 View in ScopusGoogle Scholar
    [69] Hu F., Hao Q., Bao K. A survey on Software-Defined Network and Openflow:
    from concept to implementation IEEE Commun. Surv. Tutor., 16 (4) (2014), pp. 2181-2206,
    10.1109/COMST.2014.2326417 View in ScopusGoogle Scholar [70] U. Hunkeler, H.L.
    Truong, A. Stanford-Clark MQTT-S—a publish/subscribe protocol for wireless sensor
    networks Proceedings of the 3rd International Conference on Communication Systems
    Software and Middleware and workshops, 2008. COMSWARE 2008, IEEE (2008), pp. 791-798
    CrossRefView in ScopusGoogle Scholar [71] IETF, RFC 5905. Network Time Protocol
    (NTP) version 4, 2018, (https://www.ietf.org/rfc/rfc5905.txt). Google Scholar
    [72] Real-Time Innovations, Data Distribution Service, (http://www.rti.com/products/dds/index.html).
    Last accessed January 2018. Google Scholar [73] Intellinium, Fog, edge, cloud
    and mist computing, (https://intellinium.io). Last accessed November 2017. Google
    Scholar [74] Y. Jararweh, L. Tawalbeh, F. Ababneh, F. Dosari Resource efficient
    mobile computing using cloudlet infrastructure Proceedings of the IEEE 9th International
    Conference on Mobile Ad-hoc and Sensor Networks (MSN) (2013), pp. 373-377 CrossRefView
    in ScopusGoogle Scholar [75] S. Kamijo, Y. Matsushita, K. Ikeuchi, M. Sakauchi
    Traffic monitoring and accident detection at intersections IEEE Trans. Intell.
    Transp. Syst., 1 (2) (2000), pp. 108-118 Google Scholar [76] R. Kandoi, M. Antikainen
    Denial-of-service attacks in Openflow SDN networks Proceedings of the 2015 IFIP/IEEE
    International Symposium on Integrated Network Management (IM) (2015), pp. 1322-1326,
    10.1109/INM.2015.7140489 View in ScopusGoogle Scholar [77] A.R. Khan, M. Othman,
    S.A. Madani, S.U. Khan A survey of mobile cloud computing application models IEEE
    Commun. Surv. Tutor., 16 (1) (2014), pp. 393-413, 10.1109/SURV.2013.062613.00160
    View in ScopusGoogle Scholar [78] King I., Li J., Chan K.T. A brief survey of
    computational approaches in social computing Proceedings of the 2009 International
    Joint Conference on Neural Networks (2009), pp. 2699-2706 Google Scholar [79]
    A. Kleiner, B. Nebel, V.A. Ziparo A mechanism for dynamic ride sharing based on
    parallel auctions Proceedings of the International Joint Conference on Artificial
    Intelligence, IJCAI, 11 (2011), pp. 266-272 View in ScopusGoogle Scholar [80]
    K. Kok, S. Widergren A society of devices: integrating intelligent distributed
    resources with transactive energy IEEE Power Energy Mag., 14 (3) (2016), pp. 34-45
    View in ScopusGoogle Scholar [81] Kong X., Song X., Xia F., Guo H., Wang J., A.
    Tolba LOTAD: long-term traffic anomaly detection based on crowdsourced bus trajectory
    data World Wide Web (2017), pp. 1-23 Google Scholar [82] F. Koufogiannis and G.
    J. Pappas, Diffusing Private Data over Networks, in IEEE Transactions on Control
    of Network Systems, 2017, 1-11. https://doi.org/10.1109/TCNS.2017.2673414. Google
    Scholar [83] D. Kreutz, F.M.V. Ramos, P.J.E. Veríssimo, C.E. Rothenberg, S. Azodolmolky,
    S. Uhlig Software-defined networking: a comprehensive survey Proc. IEEE, 103 (1)
    (2015), pp. 14-76, 10.1109/JPROC.2014.2371999 View in ScopusGoogle Scholar [84]
    S. Krčo, B. Pokrić, F. Carrez Designing IoT architecture(s): a European perspective
    Proceedings of the 2014 IEEE World Forum on Internet of Things (WF-IoT) (2014),
    pp. 79-84, 10.1109/WF-IoT.2014.6803124 View in ScopusGoogle Scholar [85] K. Kvaternik,
    A. Laszka, M. Walker, D.C. Schmidt, M. Sturm, M. Lehofer, A. Dubey Privacy-preserving
    platform for transactive energy systems CoRR, abs/1709.09597 (2017) Google Scholar
    [86] S. Kwoczek, S. Di Martino, W. Nejdl Predicting and visualizing traffic congestion
    in the presence of planned special events J. Vis. Lang. Comput., 25 (6) (2014),
    pp. 973-980 View PDFView articleView in ScopusGoogle Scholar [87] S. Kwoczek,
    S. Di Martino, W. Nejdl Stuck around the stadium? An approach to identify road
    segments affected by planned special events Proceedings of the 2015 IEEE Eighteenth
    International Conference on Intelligent Transportation Systems (ITSC), IEEE (2015),
    pp. 1255-1260 View in ScopusGoogle Scholar [88] L. Lamport The Part-Time Parliament
    ACM Trans. Comput. Syst., 16 (2) (1998), pp. 133-169, 10.1145/279227.279229 View
    in ScopusGoogle Scholar [89] L. Lamport Paxos made simple ACM Sigact News, 32
    (4) (2001), pp. 18-25 Google Scholar [90] L. Lamport, R. Shostak, M. Pease The
    byzantine generals problem ACM Trans. Program. Lang. Syst., 4 (3) (1982), pp.
    382-401 View in ScopusGoogle Scholar [91] A. Laszka, A. Dubey, M. Walker, D.C.
    Schmidt Providing privacy, safety, and security in IoT-based transactive energy
    systems using distributed ledgers CoRR, abs/1709.09614 (2017) Google Scholar [92]
    Online verification in cyber‐physical systems: Practical bounds for meaningful
    temporal costs. Journal of Software: Evolution and Process, vol. 30(3). March
    2018. Google Scholar [93] K. Lev-Ari, E. Bortnikov, I. Keidar, A. Shraer Modular
    composition of coordination services Proceedings of the 2016 USENIX Conference
    on Usenix Annual Technical Conference, USENIX ATC ’16, Berkeley, CA, USA (2016),
    pp. 251-264 View in ScopusGoogle Scholar [94] M.W. Levin, K.M. Kockelman, S.D.
    Boyles, T. Li A general framework for modeling shared autonomous vehicles with
    dynamic network-loading and dynamic ride-sharing application Comput. Environ.
    Urban Syst., 64 (2017), pp. 373-383 View PDFView articleView in ScopusGoogle Scholar
    [95] Li H., Shou G., Hu Y., Guo Z. Mobile edge computing: progress and challenges
    Proceedings of the 2016 Fourteenth IEEE International Conference onMobile Cloud
    Computing, Services, and Engineering (MobileCloud), IEEE (2016), pp. 83-84 Google
    Scholar [96] Linux, KVM – Kernel based Virtual Machine, (http://www.linux-kvm.com).
    Last accessed January 2018. Google Scholar [97] Linux Containers, Infrastructure
    for container projects, (http://www.linuxcontainers.org). Last accessed February
    2018. Google Scholar [98] P. Liu, D. Willis, S. Banerjee Paradrop: enabling lightweight
    multi-tenancy at the network’s extreme edge Proceedings of the 2016 IEEE/ACM Symposium
    on Edge Computing (SEC) (2016), pp. 1-13 Google Scholar [99] Liu W., Zheng Y.,
    S. Chawla, Yuan J., Xing X. Discovering spatio-temporal causal interactions in
    traffic data streams Proceedings of the Seventeenth ACM SIGKDD International Conference
    on Knowledge Discovery and data Mining, ACM (2011), pp. 1010-1018 CrossRefView
    in ScopusGoogle Scholar [100] S. Lockwood The 21st century operation oriented
    state dots (2006) Google Scholar NCHRP project 20–24 [101] X.-Y. Lu, P. Varaiya,
    R. Horowitz, J. Palen Faulty loop data analysis/correction and loop fault detection
    Proceedings of the Fifteenth World Congress on Intelligent Transport Systems and
    ITS America’s 2008 Annual Meeting (2008) Google Scholar [102] M. Luck, P. McBurney,
    Computing as interaction: agent and agreement technologies, Proceedings of the
    IEEE SMC conference on distributed human-machine systems, pp 1–6, 2008. Google
    Scholar [103] M. Luck, P. McBurney, O. Shehory, S. Willmott Agent Technology:
    Computing as Interaction (A Roadmap for Agent Based Computing) AgentLink (2005)
    Google Scholar [104] Mao Y., You C., Zhang J., Huang K., K.B. Letaief A survey
    on mobile edge computing: the communication perspective Proceedings of the IEEE
    Communications Surveys and Tutorials, 19 (2017), pp. 2322-2358, 10.1109/COMST.2017.2745201
    View in ScopusGoogle Scholar [105] M. Masdari, S.S. Nabavi, V. Ahmadi An overview
    of virtual machine placement schemes in cloud computing. J. Netw. Comput. Appl.,
    66 (2016), pp. 106-127 View PDFView articleView in ScopusGoogle Scholar [106]
    A. Mavridou, A. Laszka Designing secure Ethereum smart contracts: a finite state
    machine based approach Proceedings of the Twenty-second International Conference
    on Financial Cryptography and Data Security (FC) (2018) Google Scholar [107] N.
    McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford Openflow:
    enabling innovation in campus networks Proceedings of the ACM SIGCOMM Computer
    Communication Review (2008), pp. 69-74 CrossRefView in ScopusGoogle Scholar [108]
    P. Mell, T. Grance The NIST Definition of Cloud Computing,v15 NIST (2009) Google
    Scholar [109] R.B. Melton Gridwise transactive energy framework (draft version)
    Technical Report, Pacific Northwest National Laboratory, Richland, WA (2013) Google
    Scholar [110] Microsoft, Microsoft Azure, (http://www.azure.microsoft.com/Azure).
    Last accessed January 2018. Google Scholar [111] Sun Microsystems. Java Transaction
    API (JTA), (http://www.java.sun.com:80/javaee/technologies/jta/). Last accessed
    February 2018. Google Scholar [112] R. Mocevicius CoreOS Essentials Packt Publishing
    Ltd (2015) Google Scholar [113] M.B. Mollah, M.A.K. Azad, A. Vasilakos Security
    and privacy challenges in mobile cloud computing: Survey and way ahead J. Netw.
    Comput. Appl., 84 (2017), pp. 38-54 View PDFView articleView in ScopusGoogle Scholar
    [114] M.A. Morsy, J. Grundy, I. Müller An analysis of the cloud computing security
    problem Proceedings of APSEC 2010 Cloud Workshop, Sydney, Australia (2010) Google
    Scholar [115] K. Mueffelmann Uber’s Privacy Woes Should Serve as a Cautionary
    Tale for All Companies Wired Magazine (2015) Google Scholar [116] M. Mukherjee,
    R. Matam, L. Shu, L. Maglaras, M.A. Ferrag, N. Choudhury, V. Kumar Security and
    privacy in fog computing: challenges Proceedings of IEEE Access, 5 (2017), pp.
    19293-19304, 10.1109/ACCESS.2017.2749422 View in ScopusGoogle Scholar [117] T.
    Neagoe, V. Cristea, L. Banica NTP versus PTP in com puter networks clock synchronization
    Proceedings of the 2006 IEEE International Symposium on Industrial Electronics,
    1 (2006), pp. 317-362, 10.1109/ISIE.2006.295613 Google Scholar [118] Netflix,
    Netflix video streaming, (https://www.netflix.com/). Last accessed January 2017.
    Google Scholar [119] P.B. Nichols, The permanent web for healthcare with IPFS
    and Blockchain, 2017, (https://www.cio.com/article/3174144/innovation/the-permanent-web-for-healthcare-with-ipfs-and-blockchain.html).
    Google Scholar [120] OASIS, Message Queue Telemetry Transport (MQTT) v3.1.1, (http://www.docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html).
    Last accessed Feb 2018. Google Scholar [121] OMG, The Data Distribution Service
    specification, v1.2, 2007, (http://www.omg.org/spec/DDS/1.2). Google Scholar [122]
    D. Ongaro, J. Ousterhout In search of an understandable consensus algorithm Proceedings
    of the 2014 USENIX Conference on USENIX Annual Technical Conference, USENIX ATC’14,
    USENIX Association, Berkeley, CA, USA (2014), pp. 305-320 View in ScopusGoogle
    Scholar [123] D. Ongaro, J. Ousterhout In search of an understandable consensus
    algorithm Proceedings of the USENIX Annual Technical Conference (2014), pp. 305-320
    View in ScopusGoogle Scholar [124] F. Paolucci, F. Cugini, A. Giorgetti, P.C.
    N. Sambo A survey on the Path Computation Element (PCE) architecture IEEE Commun.
    Surv. Tutor., 15 (4) (2013), pp. 1819-1841 View in ScopusGoogle Scholar [125]
    J.S. Preden, K. Tammemäe, A. Jantsch, M. Leier, A. Riid, E. Calis The benefits
    of self-awareness and attention in fog and mist computing Computer (Long Beach
    Calif), 48 (7) (2015), pp. 37-45, 10.1109/MC.2015.207 View in ScopusGoogle Scholar
    [126] M. García-Valls, D. Perez-Palacin, R. Mirandola Time-Sensitive Adaptation
    in CPS through Run-Time Configuration Generation and Verification 38th Annual
    Computer Software and Applications Conference (COMPSAC), IEEE (2014) Google Scholar
    [127] R.D. Rasmussen Goal-based fault tolerance for space systems using the mission
    data system Proceedings of the IEEE Aerospace Conference, 2001, 5, IEEE (2001),
    pp. 2401-2410 Google Scholar [128] S. Rhea, B. Godfrey, B. Karp, J. Kubiatowicz,
    S. Ratnasamy, S. Shenker, I. Stoica, Yu H. OpenDHT: a public DHT service and its
    uses Proceedings of the ACM SIGCOMM Computer Communication Review, 35, ACM (2005),
    pp. 73-84 View in ScopusGoogle Scholar [129] S.P. Robinson The development and
    application of an urban link travel time model using data derived from inductive
    loop detectors, University of London (2006) Ph.D. thesis Google Scholar [130]
    C. Samal, L. Zheng, F. Sun, L.J. Ratliff, A. Dubey, Towards a socially optimal
    multi-modal routing platform, ArXiv e-prints (2018). https://arxiv.org/abs/1802.10140.
    Google Scholar [131] M. Sapienza, E. Guardo, M. Cavallo, G.L. Torre, G. Leombruno,
    O. Tomarchio Solving critical events through mobile edge computing: An approach
    for smart cities Proceedings of the 2016 IEEE International Conference on Smart
    Computing (SMARTCOMP) (2016), pp. 1-5, 10.1109/SMARTCOMP.2016.7501719 Google Scholar
    [132] M. Satyanarayanan, P. Simoens, Xiao Y., P. Pillai, Chen Z., Ha K., Hu W.,
    Amos B. Edge analytics in the Internet of Things IEEE Pervasive Comput., 14 (2015),
    10.1109/MPRV.2015.32 Google Scholar [133] D. Schrank, B. Eisele, T. Lomax, J.
    Bak, 2015 Urban Mobility Scorecard (2015). https://static.tti.tamu.edu/tti.tamu.edu/documents/mobility-scorecard-2015.pdf.
    Last accessed June 2018. Google Scholar [134] S. Shenker, The future of networking
    and the past of network protocols, 2011, (http://www.opennetsummit.org/archives/oct11/shenker-tue.pd).
    Open Network Summit. Google Scholar [135] A. Sheth, P. Anantharam, C. Henson Physical-cyber-social
    computing: an early 21st century approach IEEE Intell. Syst., 28 (1) (2013), pp.
    78-82 View in ScopusGoogle Scholar [136] Shi W., Cao J., Zhang Q., Li Y., Xu L.
    Edge computing: vision and challenges IEEE Internet Things J., 3 (2016), pp. 637-646
    View in ScopusGoogle Scholar [137] Shi X., Lin H., Jin H., Zhou B.B., Yin Z.,
    Di S., Wu S. Giraffe: a scalable distributed coordination service for large-scale
    systems Proceedings of the 2014 IEEE International Conference on Cluster Computing
    (CLUSTER) (2014), pp. 38-47, 10.1109/CLUSTER.2014.6968766 View in ScopusGoogle
    Scholar [138] C. Sierra, V. Botti, S. Ossowski Agreement computing Knstl. Intell.,
    25 (2011), pp. 57-61 CrossRefView in ScopusGoogle Scholar [139] Y. Simmhan, S.
    Aman, A. Kumbhare, Liu R., S. Stevens, Zhou Q., V. Prasanna Cloud-based software
    platform for big data analytics in smart grids Comput. Sci. Eng., 15 (4) (2013),
    pp. 38-47, 10.1109/MCSE.2013.39 View in ScopusGoogle Scholar [140] J. Spillner,
    A. Schill Towards dispersed cloud computing Proceedings of the 2014 IEEE International
    Black Sea Conference on Communications and Networking (BlackSeaCom) (2014), pp.
    170-174, 10.1109/BlackSeaCom.2014.6849032 View in ScopusGoogle Scholar [141] I.
    Stojmenovic, Wen S. The fog computing paradigm: Scenarios and security issues
    Proceedings of the 2014 Federated Conference on Computer Science and Information
    Systems (FedCSIS), 2, Warsaw, Poland (2014), pp. 1-8, 10.15439/2014F503 View in
    ScopusGoogle Scholar [142] H.L. Storey Implementing an integrated centralized
    model-based distribution management system Proceedings of the 2011 IEEE Power
    and Energy Society General Meeting (2011), pp. 1-2, 10.1109/PES.2011.6038994 Google
    Scholar [143] S. Suhothayan, K. Gajasinghe, I.L. Narangoda, S. Chaturanga, S.
    Perera, V. Nanayakkara SIDDHI: a second look at complex event processing architectures
    Proceedings of the ACM Workshop on Gateway Computing Environments (2011), 10.1145/2110486.2110493
    Google Scholar [144] E. del Val, M. Rebollo, V. Botti Enhancing decentralized
    service discovery in open service-oriented multi-agent systems Auton. Agent Multi
    Agent Syst., 28 (2014), pp. 1-30 CrossRefView in ScopusGoogle Scholar [145] B.
    Varghese, N. Wang, S. Barbhuiya, P. Kilpatrick, D.S. Nikolopoulos Challenges and
    opportunities in edge computing Proceedings of the 2016 IEEE International Conference
    on Smart Cloud (SmartCloud) (2016), pp. 20-26, 10.1109/SmartCloud.2016.18 View
    in ScopusGoogle Scholar [146] H. Veeraraghavan, P. Schrater, N. Papanikolopoulos
    Switching Kalman filter-based approach for tracking and event detection at traffic
    intersections Proceedings of the 2005 IEEE International Symposium on, Mediterrean
    Conference on Control and Automation Intelligent Control, 2005, IEEE (2005), pp.
    1167-1172 CrossRefView in ScopusGoogle Scholar [147] T. Verbelen, P. Simoens,
    F. Turck, B. Dhoedt Cloudlets: bringing the cloud to the mobile user Proceedings
    of the 3rd ACM Workshop on Mobile Cloud Computing and Services, Low Wood Bay,
    UK (2012), pp. 29-36, 10.1145/2307849.2307858 Google Scholar [148] D. Willis,
    A. Dasgupta, S. Banerjee ParaDrop: a multi-tenant platform to dynamically install
    third party services on wireless gateways Proceedings of the Ninth ACM Workshop
    on Mobility in the Evolving Internet Architecture, ACM (2014), pp. 43-48 CrossRefGoogle
    Scholar [149] M. Wooldridge An Introduction to Multiagent Systems (second ed.),
    Wiley Publishing (2009) Google Scholar [150] M. Wooldridge, N.R. Jennings Intelligent
    agents: theory and practice Knowl. Eng. Rev., 10 (1995), pp. 115-152 View in ScopusGoogle
    Scholar [151] Xu L., Yue Y., Li Q. Identifying urban traffic congestion pattern
    from historical floating car data Procedia-Soc. Behav. Sci., 96 (2013), pp. 2084-2095
    View PDFView articleGoogle Scholar [152] Yang S., K. Kalpakis, A. Biem Detecting
    road traffic events by coupling multiple timeseries with a nonparametric Bayesian
    method IEEE Trans. Intell. Transp. Syst., 15 (5) (2014), pp. 1936-1946 View in
    ScopusGoogle Scholar [153] Yi S., Li C., Li Q. A survey of fog computing: Concepts,
    applications and issues Proceedings of the 2015 Workshop on Mobile Big Data, Mobidata
    ’15, ACM, New York, NY, USA (2015), pp. 37-42, 10.1145/2757384.2757397 View in
    ScopusGoogle Scholar [154] Yi S., Li C., Li Q. A survey of fog computing: concepts,
    applications and issues Proceedings of the 2015 Workshop on Mobile Big Data, ACM
    (2015) Google Scholar [155] Yuan Y., Wang F.-Y. Towards blockchain-based intelligent
    transportation systems Proceedings of the Intelligent Transportation Systems (ITSC),
    2016 IEEE 19th International Conference on, IEEE (2016), pp. 2663-2668 Google
    Scholar [156] N. Zygouras, N. Panagiotou, N. Zacheilas, I. Boutsis, V. Kalogeraki,
    I. Katakis, D. Gunopulos Towards detection of faulty traffic sensors in real-time.
    MUD@ ICML (2015), pp. 53-62 Google Scholar Cited by (63) EdgeOptimizer: A programmable
    containerized scheduler of time-critical tasks in Kubernetes-based edge-cloud
    clusters 2024, Future Generation Computer Systems Show abstract Mobility-aware
    fog computing in dynamic networks with mobile nodes: A survey 2023, Journal of
    Network and Computer Applications Show abstract A survey on downlink–uplink decoupled
    access: Advances, challenges, and open problems 2022, Computer Networks Citation
    Excerpt : Compared with cloud computing, edge computing servers are deployed closer
    to UEs, which allows processes to take place in BSs and other aggregation points
    in the network. With the development of 5G UDNs, MEC has become a promising technique
    to satisfy the latency, capacity, and security requirements of many emerging services,
    such as augmented & virtual reality, live streaming/gaming, V2X, and smart factories
    [61]. Another emerging technology that can reduce long-distance transmission latency
    and alleviate backhaul traffic is content caching technology [62]. Show abstract
    Towards highly-concurrent leaderless state machine replication for distributed
    systems 2022, Journal of Systems Architecture Show abstract Integration of multi
    access edge computing with unmanned aerial vehicles: Current techniques, open
    issues and research directions 2022, Physical Communication Show abstract An overview
    of low power hardware architecture for edge computing devices 2022, 5G IoT and
    Edge Computing for Smart Healthcare Show abstract View all citing articles on
    Scopus Marisol García Valls is Associate Professor in the Department of Telematics
    Engineering of Universidad Carlos III de Madrid, Spain, where she has led the
    Distributed Real Time Systems Lab for more than 10 years. Her research interests
    focus on the design of efficient, timely, and secure execution and interoperation
    mechanisms for time-sensitive distributed systems, IoT, and cyber-physical systems.
    http://www.it.uc3m.es/mvalls/. Abhishek Dubey is an Assistant Professor of Electrical
    Engineering and Computer Science at Vanderbilt University, Senior Research Scientist
    at the Institute for Software-Integrated Systems and co-lead for the Vanderbilt
    Initiative for Smart Cities Operations and Research (VISOR). He directs the Smart
    computing laboratory (http://scope.isis.vanderbilt.edu/) at the university. His
    research interests are resilient cyber-physical systems, fault-tolerant distributed
    systems and applied machine learning. https://my.vanderbilt.edu/dabhishe. Vicent
    Botti is full professor of computer systems at Universitat Politècnica de València,
    Spain. His current research activities include the following interdisciplinary
    areas: agreement technologies; virtual organizations, automatic negotiation, argumentation,
    trust, reputation and privacy; multi agent systems; architectures and platforms;
    development technologies, agent based social simulation, agent based intelligent
    manufacturing systems, multiagent adaptive systems, agreement networks, decentralized
    services management. http://www.users.dsic.upv.es/vbotti/. 1 By end devices, we
    refer to the nodes at the leaf position of the information flow graph, typically
    intelligent sensors, smartphones, embedded computers, etc. 2 European Telecommunications
    Standards Institute. http://www.etsi.org. 3 Long-Term Evolution (LTE) is a telecommunications
    standard –a registered trademark of ETSI – for high-speed wireless communication
    in mobile devices and data terminals; it increases the capacity and speed by using
    a different radio interface together with core network improvements 4 http://www.hyperledger.org
    5 Burrow was contributed by Monax. 6 Fabric was originally contributed by IBM
    and Digital Asset. 7 Contributed by Soramitsu. 8 Contributed by Intel. 9 Software
    Guard Extensions by Intel. © 2018 The Authors. Published by Elsevier B.V. Part
    of special issue Adaptive Middleware for Smart Systems Edited by Marisol García-Valls,
    Abhishek Dubey View special issue Recommended articles Compositional Human Pose
    Regression Computer Vision and Image Understanding, Volumes 176–177, 2018, pp.
    1-8 Shuang Liang, …, Yichen Wei View PDF An introduction to delay and disruption-tolerant
    networks (DTNs) Advances in Delay-Tolerant Networks (DTNs), 2015, pp. 1-21 J.J.P.C.
    Rodrigues, V.N.G.J. Soares Appearance based pedestrians’ head pose and body orientation
    estimation using deep learning Neurocomputing, Volume 272, 2018, pp. 647-659 Mudassar
    Raza, …, Peng Bao View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 57 Captures Readers: 323 Social Media Shares, Likes & Comments: 26 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: Journal of systems architecture
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'Introducing the new paradigm of Social Dispersed Computing: Applications,
    Technologies and Challenges'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.12688/f1000research.110875.1
  analysis: '>'
  authors:
  - Ihda Chaerony Siffa
  - Jan Schäfer
  - Markus M. Becker
  citation_count: 1
  full_citation: '>'
  full_text: ">\nSOFTWARE TOOL ARTICLE\nAdamant: a JSON schema-based metadata editor\
    \ for research \ndata management workflows [version 1; peer review: 2 \napproved]\n\
    Ihda Chaerony Siffa\n, Jan Schäfer\n, Markus M. Becker\nLeibniz Institute for\
    \ Plasma Science and Technology (INP), Greifswald, Felix-Hausdorff-Straße 2, 17489,\
    \ Germany \nFirst published: 29 Apr 2022, 11:475  \nhttps://doi.org/10.12688/f1000research.110875.1\n\
    Latest published: 19 Jul 2022, 11:475  \nhttps://doi.org/10.12688/f1000research.110875.2\n\
    v1\n \nAbstract \nThe web tool Adamant has been developed to systematically collect\
    \ \nresearch metadata as early as the conception of the experiment. \nAdamant\
    \ enables a continuous, consistent, and transparent research \ndata management\
    \ (RDM) process, which is a key element of good \nscientific practice ensuring\
    \ the path to Findable, Accessible, \nInteroperable, Reusable (FAIR) research\
    \ data. It simplifies the creation \nof on-demand metadata schemas and the collection\
    \ of metadata \naccording to established or new standards. The approach is based\
    \ on \nJavaScript Object Notation (JSON) schema, where any valid schema can \n\
    be presented as an interactive web-form. Furthermore, Adamant \neases the integration\
    \ of numerous available RDM methods and \nsoftware tools into the everyday research\
    \ activities of especially small \nindependent laboratories. A programming interface\
    \ allows \nprogrammatic integration with other software tools such as electronic\
    \ \nlab books or repositories. The user interface (UI) of Adamant is \ndesigned\
    \ to be as user friendly as possible. Each UI element is self-\nexplanatory and\
    \ intuitive to use, which makes it accessible for users \nthat have little to\
    \ no experience with JSON format and programming \nin general. Several examples\
    \ of research data management workflows \nthat can be implemented using Adamant\
    \ are introduced. Adamant \n(client-only version) is available from: https://plasma-\n\
    mds.github.io/adamant.\nKeywords \nResearch Data Management, JSON Schema, FAIR\
    \ Principles\n \nThis article is included in the Research on \nResearch, Policy\
    \ & Culture gateway.\nOpen Peer Review\nApproval Status  \n \n \n1\n2\n3\nversion\
    \ 2\n(revision)\n19 Jul 2022\nview\nversion 1\n29 Apr 2022\nview\nview\nFelix\
    \ Shaw\n, Earlham Institute, Norwich, \nUK\n1. \nFrank Krüger\n, University of\
    \ Rostock, \nRostock, Germany\n2. \nMartin Thomas Horsch\n, University of \nRostock,\
    \ Rostock, Germany\n3. \nAny reports and responses or comments on the \narticle\
    \ can be found at the end of the article.\n \nPage 1 of 19\nF1000Research 2022,\
    \ 11:475 Last updated: 27 NOV 2023\nCorresponding author: Ihda Chaerony Siffa\
    \ (ihda.chaeronysiffa@inp-greifswald.de)\nAuthor roles: Chaerony Siffa I: Conceptualization,\
    \ Methodology, Software, Writing – Original Draft Preparation, Writing – Review\
    \ & \nEditing; Schäfer J: Conceptualization, Writing – Review & Editing; Becker\
    \ MM: Conceptualization, Funding Acquisition, Methodology, \nProject Administration,\
    \ Supervision, Writing – Original Draft Preparation, Writing – Review & Editing\n\
    Competing interests: No competing interests were disclosed.\nGrant information:\
    \ The work was funded by the Federal Ministry of Education and Research (BMBF)\
    \ under the grant mark 16QK03A \ngranted to the Leibniz Institute for Plasma Science\
    \ and Technology (INP). The responsibility for the content of this publication\
    \ lies with \nthe authors. \nThe funders had no role in study design, data collection\
    \ and analysis, decision to publish, or preparation of the manuscript.\nCopyright:\
    \ © 2022 Chaerony Siffa I et al. This is an open access article distributed under\
    \ the terms of the Creative Commons Attribution \nLicense, which permits unrestricted\
    \ use, distribution, and reproduction in any medium, provided the original work\
    \ is properly cited.\nHow to cite this article: Chaerony Siffa I, Schäfer J and\
    \ Becker MM. Adamant: a JSON schema-based metadata editor for research \ndata\
    \ management workflows [version 1; peer review: 2 approved] F1000Research 2022,\
    \ 11:475 \nhttps://doi.org/10.12688/f1000research.110875.1\nFirst published: 29\
    \ Apr 2022, 11:475 https://doi.org/10.12688/f1000research.110875.1 \n \nPage 2\
    \ of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\nIntroduction\n\
    The demand of funding organizations and publishers to make research data discoverable,\
    \ accessible, interoperable,\nand reusable in the sense of the FAIR principles1\
    \ and the associated need for digitization and automation of research\ndata management\
    \ (RDM) processes pose new challenges for some research fields. In particular,\
    \ for smaller laboratories\nin physics departments that operate away from large-scale\
    \ experiments in astrophysics, or high-energy physics, new\nprocesses for research\
    \ data management must be introduced and established. While the aspects of findability\
    \ and\naccessibility can be realized without much effort by means of a data publication\
    \ on a generic platform, the interoperability\nand reusability of the (domain-specific)\
    \ data and metadata often pose a major challenge and requires certain conventions\n\
    and standards in the communities. There are hardly any common research data management\
    \ processes in areas such as\noptics and low-temperature plasma physics, and handwritten\
    \ laboratory notebooks are still the standard in many places.2\nOn the other hand,\
    \ electronic laboratory notebook (ELN) systems, repositories, collaborative tools,\
    \ and (meta-)data\nstandards already exist that can support the implementation\
    \ of the FAIR principles and Open Science practices. A major\nchallenge is to\
    \ integrate these tools and standards into the everyday research activities. This\
    \ is especially challenging\nwhen extremely diverse needs have to be addressed\
    \ at institutions, no specific standard procedures and (meta-)data\nstandards\
    \ exist, and only a few scientists at a time are dealing with similar instruments\
    \ and data.\nAdamant has been developed to support especially these often small\
    \ laboratories and departments in the implementation\nof digital research data\
    \ management processes and the step-by-step adoption of the FAIR principles. To\
    \ this end, a tool is\nprovided for the easy use or compilation and creation of\
    \ domain-specific metadata and metadata schemas based on\nthe widely used JavaScript\
    \ Object Notation (JSON) schema standard, which recently has been gaining traction\
    \ in several\nscientific communities.3–7 The use of JSON schema in Adamant enables\
    \ straightforward validation of the metadata\nensuring its quality.8,9 Furthermore,\
    \ storing the metadata in JSON format maintains the adaptability of the created\n\
    metadata with different RDM tools and processes, and increases the findability\
    \ of the research data to which the metadata\nis attached, in view of the fact\
    \ that JSON documents are human- and machine-readable.8,10,11 An Application\n\
    Programming Interface (API) based integration with other systems enables the seamless\
    \ embedding of Adamant into\ninstitutional research data management processes.\
    \ This positions Adamant directly alongside electronic laboratory\nnotebook systems\
    \ and makes it suitable for ensuring standards-compliant documentation of scientific\
    \ studies as early as\nthe planning and execution of experiments.\nIn its current\
    \ form, Adamant (v1.0.0) offers various features that can support diverse workflows\
    \ within RDM activities.\nThe features are as follow:\n•\nRendering of interactive\
    \ web-form based on a valid JSON schema\n•\nUser-friendly editing process of the\
    \ rendered web-form and the corresponding schema\n•\nCreating a valid JSON schema\
    \ and web-form from scratch\n•\nLive validation for various field types\n•\nQuick\
    \ re-use of existing schemas from a list\n•\nDownloadable JSON schema and its\
    \ form data\n•\nAPI-based integration as various form submission functionalities\n\
    Methods\nThis section describes the thought process of selecting the technology\
    \ stack for the development of Adamant, and the\nimplementation details of the\
    \ tool’s functionalities. Subsequently, detailed descriptions on how to set up\
    \ and operate\nAdamant are described. The code is available from GitHub and is\
    \ archived with Zenodo.29\nImplementation\nTechnology stack and architecture\n\
    Adamant has been developed as a web-based tool. This decision was motivated by\
    \ the end user experience, where end\nusers can use Adamant directly on a web\
    \ browser available on their machines and handheld devices with no prior\ninstallation\
    \ and configurations required. We considered several things when choosing the\
    \ right technology stack for\nPage 3 of 19\nF1000Research 2022, 11:475 Last updated:\
    \ 27 NOV 2023\ndeveloping Adamant, such as the availability and ease of use of\
    \ the technology, whether its community is active and big,\nand the possibility\
    \ of the technology still being relevant in the coming years, often indicated\
    \ by its adoption rate and\npopularity. For these reasons, we adopted a technology\
    \ stack consisting solely of open-source software, such as ReactJS12\nand Flask13\
    \ (as the main development frameworks). This stack utilizes two of the most popular\
    \ programming languages,\nnamely JavaScript and Python14 (Python Programming Language,\
    \ RRID:SCR_008394). On top of that, we utilized\nDocker (Docker Desktop, RRID:SCR_016445)\
    \ and Docker Compose for straightforward packaging and deployment of\nthe developed\
    \ tool.15 Like a typical web tool or application, Adamant consists of front-end\
    \ and back-end layers. The\nfront-end part serves as the presentation layer in\
    \ the form of a Graphical User Interface (GUI) for end users to interact\nwith.\
    \ The back-end part is a server-side part of the tool with the main task of providing\
    \ processes that are not suitable to be\nrun on the client-side[1]. Lastly, the\
    \ front-end and back-end layers are bridged using an API built upon the Hypertext\n\
    Transfer Protocol (HTTP) request methods, such as POST and GET.\nThe front-end\
    \ has been developed in the Node.js16 (v14.15.5) JavaScript runtime environment\
    \ making use of ReactJS\n(v17.0.2), which is a popular JavaScript library for\
    \ developing a highly interactive browser-based GUI. ReactJS is\ncombined with\
    \ the Material-UI library17 (v4.12.3), which contains many pre-existing user interface\
    \ (UI) components.\nThe pre-existing components can be modified to one’s need\
    \ straightforwardly thus accelerating the UI development. The\nmain bulk of the\
    \ features are implemented on the front-end. This includes the automatic rendering\
    \ process of a JSON\nschema into a web-form, form editing logic (editing of the\
    \ rendered web-form or creating one from scratch), and input\nvalidation processes.\
    \ The back-end is written in Python (v3.8.7) using the Flask library (v2.0.2).\
    \ Flask is a lightweight\nweb framework and straightforward to use making it suitable\
    \ for the scope of Adamant development. The back-end’s\ntasks include providing\
    \ the front-end with available JSON schemas from the server, data preparation,\
    \ necessary e-mail\nnotification for users, and API calls for communication with\
    \ external applications (API based integration), e.g., ELN and\nonline data repository\
    \ systems.\nFigure 1 provides an overview of Adamant’s software architecture and\
    \ the main functionalities of each application layer\nalong with the corresponding\
    \ technologies.\nFront-end: JSON schema to editable web-form\nA JSON schema, like\
    \ every other JSON document, contains key (word)-value pair instances, where the\
    \ value parts may\ncontain another set of keyword-value pairs, i.e., nested objects.8,10,11\
    \ In a JSON schema, there are a number of schema-\nspecific keywords that are\
    \ mainly used for validation and annotation of the elements in a schema.8,9 For\
    \ example, a typical\nFigure 1. Overview of Adamant’s software architecture. API,\
    \ Application Programming Interface; JSON, JavaScript\nObject Notation; HTTP,\
    \ Hypertext Transfer Protocol.\n1In this paper, the terms “front-end” and “client-side”\
    \ are used interchangeably, as well as “back-end” and “server-side”.\nPage 4 of\
    \ 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\nset of schema-specific\
    \ keywords found in a JSON schema and its sub-schema contains $id/id, $schema,\
    \ title,\ntype, and properties keywords.9 On many occasions, more specific keywords\
    \ are used to further annotate or\ndescribe the elements in a schema. The specific\
    \ functions of these keywords are dictated by the schema specification\nversion\
    \ a JSON schema is specified in, which is declared at the very beginning of the\
    \ schema under the $schema\nkeyword. A comprehensive documentation of the JSON\
    \ schema standard can be found at https://json-schema.org/. An\nexample of a JSON\
    \ schema with the specification version draft 4 is shown in Listing 1, which contains\
    \ the typical\nkeywords stated earlier, and several field elements. One of the\
    \ main features of Adamant is to create a web-form\nrepresentation of a given\
    \ JSON schema, where users can fill it in and create a JSON document containing\
    \ the form data for\nanything they want to describe, e.g., a lab experiment or\
    \ a dataset. It is worth noting that there are several freely-available\nlibraries\
    \ and tools for JSON form rendering that conform to the JSON schema specifications.18–21\
    \ Unfortunately, the\navailable libraries and tools, though some offer direct\
    \ editing of the schema, do not provide a user-friendly interface for\nschema\
    \ (or form) creation and editing right out of the box as what we have envisioned\
    \ with Adamant. Therefore, we\ndecided to build a custom JSON schema form renderer\
    \ from the ground up in order to achieve this, which allows us to\nmaintain full\
    \ control as well as flexibility during the development of Adamant’s current and\
    \ future features. At the time of\nwriting this paper, Adamant (v1.0.0) supports\
    \ the rendering and editing of JSON schemas with a specification version\ndraft\
    \ 4 or 7.\nAdamant employs a recursive form field rendering process that makes\
    \ accessing of the keyword-value pairs located\nwithin nested objects possible.\
    \ The rendering process starts by determining the value of the type keyword in\
    \ the\nuppermost object (or parent object) of the schema. The type keyword in\
    \ this location commonly has a string value of\nobject. This value indicates that\
    \ a properties keyword is present within the same location. The properties\nkeyword’s\
    \ value is a set of keyword-value pairs (an object), where each pair contains\
    \ the information that the process\nneeds to render the field. As the process\
    \ iterates over these keyword-value pairs, it renders a certain form field type\n\
    according to its type keyword’s value. The acceptable types in a JSON schema are\
    \ string, number, integer,\nboolean, array, and object. If the process stumbles\
    \ upon a field keyword that has the type of object again, then a\nListing 1. Example\
    \ of a draft-4 JSON schema containing typical schema-specific keywords presented\
    \ in blue\nwith their values presented in black, and field element keywords presented\
    \ in red. JSON, JavaScript Object\nNotation.\nPage 5 of 19\nF1000Research 2022,\
    \ 11:475 Last updated: 27 NOV 2023\ncontainer will be rendered, and the recursion\
    \ process is initiated. This process iterates over the current properties\nkeyword’s\
    \ value following the same procedure as before, and renders each field within\
    \ the newly created container. In\nthis way, the process will not terminate until\
    \ all form fields are rendered. This is a condition where the process does not\n\
    find any field keyword with the type of object anymore. Apart from the type keyword’s\
    \ values, the renderer also\nmakes use of other keywords’ values to adorn the\
    \ form field with useful information, such as the values of title and\ndescription\
    \ keywords (annotation keywords). The main steps of this form field rendering\
    \ process is also represented\nin a flowchart diagram as shown in Figure 2.\n\
    Another main feature of Adamant is the capability of editing the rendered fields.\
    \ This feature allows users to change the\nvalues of relevant keywords in the\
    \ schema, and re-order the rendered fields within the same container. To achieve\
    \ this\nfeature, apart from the rendering of the form fields, each rendered field\
    \ is also supplied with several editing interfaces\nalong with the editing and\
    \ input handling logic. Creating a schema (or form) from scratch is another feature\
    \ made possible\nby using the same principle of this form editing feature, which\
    \ basically presents the user with a blank schema and lets\nthem add the field\
    \ elements as required.\nFor a certain use case, a file upload functionality can\
    \ be included in a rendered form. This functionality is intended as an\nalternative\
    \ way of enriching the metadata when texts are no longer sufficient. For example,\
    \ an experiment may have a\ngraphical description of its set-up. In this case,\
    \ the user can include an image file within the metadata using this file upload\n\
    functionality. To add this functionality, a string-type field element, with the\
    \ addition of “contentEncoding”:“base64”\nkeyword-value pair, must be specified\
    \ in the schema. This particular keyword-value pair will inform the rendering\n\
    function to create a file upload field element, where the selected file is read\
    \ as a string of base64-encoded binary data.\nFortunately, adding the file upload\
    \ functionality can be done in only a couple of mouse clicks in Adamant. For a\
    \ smooth\nexperience, the uploaded file is recommended to have the size of less\
    \ than 500 kilobytes.\nLastly, Table 1 presents a complete list of the implemented\
    \ field types and their relevant keywords in the current version\nof Adamant (v1.0.0).\n\
    Figure 2. JSON schema to web-form rendering flowchart. JSON, JavaScript Object\
    \ Notation.\nPage 6 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\n\
    Front-end: JSON form data validation using Ajv\nAs the user fills in the form,\
    \ a JSON document containing the entered data is created and updated accordingly\
    \ and\nsimultaneously, this JSON document is called JSON form data. Upon finishing\
    \ the form filling process, the created JSON\nform data can be used for other\
    \ operations. To ensure the correctness or quality of the entered data, the JSON\
    \ form data is\nvalidated against its schema prior further operation. For this\
    \ validation purpose, Adamant takes the advantage of the Ajv\nlibrary (v8.8.2).22\
    \ By using this library, any discrepancies between the JSON form data and its\
    \ schema can be identified.\nFor example, if a field is required to be filled\
    \ and the user does not provide any input, then the process will throw a\nvalidation\
    \ error with relevant error messages. Many other keyword specific validations\
    \ are included in this library, which\nhelps with ensuring the quality of the\
    \ form data with respect to its schema.\nBack-end: API-based integration\nMany\
    \ existing web applications provide APIs for seamless integration with other (external)\
    \ web tools or applications.\nFor instance, web applications that are commonly\
    \ used in the RDM community include online data repository systems,\nELNs, collaborative\
    \ and version control applications, etc. These applications, more often than not,\
    \ provide API endpoints\nthat are straightforward to use. Several web tools have\
    \ also been developed each addressing certain challenges in\nRDM, such as COPO,3\
    \ and Dendro.23 Adamant is equipped with a back-end layer whose main purpose is\
    \ to ensure the\npossibility of seamless integration with such web applications.\
    \ The back-end is written in Python (v3.8.7) using the Flask\nlibrary (v2.0.2),\
    \ which is advantageous given numerous web applications provide Python libraries\
    \ for their API calls. The\nconnection between Adamant’s front-end and back-end\
    \ is achieved primarily using the POST and GET HTTP request\nmethods. The POST\
    \ method is used to send data from the front-end to the back-end, which is then\
    \ pre-processed\n(if needed) and relayed to the external application (using the\
    \ provided API). The GET method is used when the client-side\nrequires information\
    \ or data only available in the server-side or from an external application, e.g.,\
    \ when retrieving\nschemas from the server, or retrieving tags and database items\
    \ from an external application. With this straightforward\napproach, various functionalities\
    \ can be implemented in the server-side that can help with realizing application-specific\n\
    RDM workflows.\nOperation\nMany features of Adamant can be explored directly in\
    \ the client-only version available online at https://plasma-mds.\ngithub.io/adamant.\
    \ This version only lacks the submission-related features, which require the server-side\
    \ to operate. To\naccess the full features, Adamant can be set up and deployed\
    \ on a local machine or a server that allows for running both the\nclient and\
    \ server sides. In most instances, Adamant is not resource intensive, and can\
    \ be used on a typical smartphone and\nlaptop. However, for deployment and development,\
    \ we recommend to set up Adamant on a system with at least a 64-bit\nCPU (with\
    \ 4 cores) and 8GB of RAM. At the time of writing, Adamant has been tested on\
    \ Firefox (91.5.1esr), Chrome\n(v97.0.4692.99), and Chrome Mobile (v97.0.4692.98)\
    \ web browsers. The following subsections introduce the deploy-\nment process\
    \ of Adamant and how users can interact with it.\nTable 1. Implemented JSON schema\
    \ field types and their relevant keywords in Adamant v1.0.0. Note that the\nid\
    \ keyword only works with the JSON schema specification version draft 4, whereas\
    \ $id is used for the newer\nspecification drafts. Lastly, the contentEncoding\
    \ keyword is intended to be used with the specification version\ndraft 7 or newer.\
    \ JSON, JavaScript Object Notation.\nField\ntype\nImplemented keywords\nNote\n\
    String\ntitle, id, $id, description, type, enum,\ncontentEncoding, default, minLength,\
    \ maxLength\ncontentEncoding can only\nreceive a string value of\n“base64”\nNumber\n\
    title, id, $id, description, type, enum, default,\nminimum, maximum\nInteger\n\
    title, id, $id, description, type, enum, default,\nminimum, maximum\nBoolean\n\
    title, id, $id, description, type, default\nArray\ntitle, id, $id, description,\
    \ type, default, items,\nminItems, maxItems, uniqueItems\nObject\ntitle, id, $id,\
    \ description, type, properties, required\nPage 7 of 19\nF1000Research 2022, 11:475\
    \ Last updated: 27 NOV 2023\nSetting up Adamant on a local machine\nFor development\
    \ purpose, Adamant can be set up on a local machine. For a Linux system (tested\
    \ on Ubuntu 20.04.4 LTS),\nthe following steps can be taken for this:\n1.\n$ git\
    \ clone https://github.com/plasma-mds/adamant.git — clone the repository\n2.\n\
    $ cd adamant — go to adamant project directory\n3.\nadamant$ npm install — install\
    \ the dependencies for the client-side\n4.\nadamant$ cd backend — go to backend\
    \ directory\n5.\nadamant/backend$ python3 -m venv venv — create a python virtual\
    \ environment\n6.\nadamant/backend$ source./venv/bin/activate — activate the virtual\
    \ environment\n7.\nadamant/backend$ pip install -r requirements.txt — install\
    \ the dependencies for the\nback-end\n8.\nadamant/backend$ ./venv/bin/flask run\
    \ --no-debugger — start the back-end\n9.\nStart a new terminal\n10.\nadamant$\
    \ npm start — on the new terminal, in the adamant project directory, start the\
    \ client-side\nTypically, a web-browser presenting the client-side will open automatically\
    \ following the last command. In any case,\nAdamant can be accessed at http://localhost:3000.\n\
    Deploying Adamant using Docker\nAdamant is intended to be deployed on an institutional\
    \ server in the internal network. In this way, anyone who is\nconnected to the\
    \ institute’s network can use Adamant directly. We recommend using Docker to deploy\
    \ the production\nbuild of Adamant, which can be done with the following steps:\n\
    1.\n$ git clone https://github.com/plasma-mds/adamant.git — clone the repository\n\
    2.\n$ cd adamant — go to adamant project directory\n3.\nadamant$ docker\x01compose\
    \ build — build the docker images for both back-end and front-end\n4.\nadamant$\
    \ docker\x01compose up -d — start both client and server containers, i.e., the\
    \ whole system\nBy default, the deployed system can be accessed at http://localhost:3000.\n\
    General overview of the Adamant UI\nThe general overview of the Adamant UI is\
    \ presented in Figure 3. Adamant renders a given JSON schema into a web-\nform\
    \ by uploading the schema from a local drive using the “BROWSE SCHEMA” button,\
    \ or if already existing, the\nschema can be selected from a list next to the\
    \ browse button. Creating a JSON schema from scratch is possible by clicking\n\
    the “CREATE FROM SCRATCH” button. After uploading or selecting the schema, the\
    \ schema is checked by Adamant\nto see whether the schema file type and structure\
    \ are valid. If the schema is valid, the schema can be rendered by clicking\n\
    the “RENDER” button. The rendering can be undone by clicking the “CLEAR” button,\
    \ which also discards the current\nschema.\nAs a demonstration, Figure 3 shows\
    \ the rendered web-form based on the schema represented in Listing 1. Each rendered\n\
    field is shown along with its editing interfaces, and the field container’s header\
    \ is rendered in blue for a quick\nPage 8 of 19\nF1000Research 2022, 11:475 Last\
    \ updated: 27 NOV 2023\nidentification. Furthermore, the field container can be\
    \ expanded and collapsed for a good user experience. The editing\ninterfaces found\
    \ in each field consists of edit and remove button icons, and a drag handle icon.\
    \ These interfaces allow\nthe user to change the values of relevant keywords in\
    \ the schema (directly affecting the rendered field), and re-order the\nrendered\
    \ fields by dragging and dropping the field to the desired order within the same\
    \ container. The changes are\nreflected to the rendered form immediately, which\
    \ promotes a What You See Is What You Get (WYSIWYG) application\nexperience for\
    \ the users. On top of that, each field container is equipped with the “ADD ELEMENT”\
    \ button for adding a\nnew field within the corresponding container. By default,\
    \ Adamant renders the form in the edit mode right after a schema\nis rendered.\
    \ The editing mode can be concluded by pressing the “COMPILE” button, which removes\
    \ all editing interfaces\n(thus also removes the editing functionalities) and\
    \ readies the form for use. In case of needing to edit the form again, the\nuser\
    \ is able to initiate the editing mode again, and without losing the already entered\
    \ data.\nAdding and removing schemas\nA number of schemas can be fixed in the\
    \ back-end. The back-end serve these stored schemas to the front-end, from which\n\
    the end users can select. This is important for a quick re-use of the schemas,\
    \ especially the ones that are regularly used. For\nthis purpose, one can simply\
    \ add the desired schemas to adamant/backend/schemas/ directory. Likewise, a\n\
    schema can be removed from this directory. The list of the schemas in the front-end\
    \ is updated every time the front-end is\nrefreshed.\nUse cases\nAt its core,\
    \ Adamant is a JSON schema form renderer-editor and JSON form data creator presented\
    \ in an interactive\nand user-friendly UI. It can be operated by anyone with no\
    \ prior knowledge of JSON format and coding. This\nsection elaborates the general\
    \ and ad hoc use cases of Adamant that it can be suited to.\nCollection of structured\
    \ and standardized metadata\nThe creation or use of a valid JSON schema and the\
    \ collection of corresponding form data being consistent with the\nschema can\
    \ be considered as the general use case of Adamant, which simplifies the collection\
    \ of structured and\nFigure 3. Overview of the Adamant UI with a rendered web-form\
    \ based on the schema in Listing 1 as an\nexample. (A) Main corpus of the UI;\
    \ (1) from left to right: JSON schema viewer, auto-populate form, edit schema\n\
    description, revert all changes; (2) remove form field; (3) collapse or expand\
    \ the field container; (4) field drag handle;\n(5) edit field description and\
    \ (B) field editing panel (as a pop-up on top of the main UI) triggered by clicking\
    \ (5) the edit\nbutton. UI, user interface; JSON, JavaScript Object Notation.\n\
    Page 9 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\nstandardized\
    \ metadata. This is particularly relevant for laboratories that are not embedded\
    \ in large research clusters\nproviding access to established standards and digital\
    \ research data management workflows. On the one hand, the ability\nto define\
    \ required metadata fields and formats supports metadata quality assurance, and\
    \ on the other hand, storing\nmetadata in JSON format enables further (automated)\
    \ processing of metadata. This both directly contributes to the\n“FAIRness” of\
    \ metadata.\nCreation of an eLabFTW experiment with schema-compliant metadata\n\
    The use case introduced here aims to highlight how the previously described ability\
    \ to interface with external\nsystems through API-based integration can be used\
    \ to generate schema-compliant experiment descriptions in eLabFTW.\nThe open source\
    \ ELN system eLabFTW24,25 (elabFTW, RRID:SCR_013971) is becoming increasingly\
    \ popular,\nespecially at universities and research institutions. Given Adamant’s\
    \ features (JSON schema-based metadata creation,\ninput validations, etc.), one\
    \ can see why it could be beneficial to collect structured metadata using Adamant\
    \ rather than\ndoing it directly in the ELN system, especially when the ELN system\
    \ is designed to be as general as possible in order to\naccommodate various kinds\
    \ of experiments from different scientific fields. Note that eLabFTW already offers\
    \ the\npossibility to use predefined templates and to edit JSON files. However,\
    \ a user-friendly possibility to work directly with\nJSON schemas and to validate\
    \ the entered metadata on the basis of these predefined schemas is missing so\
    \ far.\nFigure 4 shows the workflow for creating an experiment using Adamant in\
    \ the eLabFTW system from the end-user’s\nperspective. The user generates the\
    \ experiment form by uploading their experiment schema, or selecting it from the\
    \ list if\nit already exists. Alternatively, the user can create the form from\
    \ scratch. If necessary, the user can edit the form (affecting\nthe given schema).\
    \ For example, changing the title of a certain field, removing a field, etc. When\
    \ satisfied, the user can\ncompile the rendered form and proceed to fill it in.\
    \ Upon completing the form, Adamant validates the entered data against\nthe given\
    \ schema and notifies the user if discrepancies between the entered data and the\
    \ schema are found, which the user\ncan rectify thereafter. When the entered data\
    \ are valid, the user can proceed to review the form. After a thorough review,\n\
    the user can proceed and submit the form. During the submission process, the user\
    \ will be prompted to input the URL of\ntheir eLabFTW system and their eLabFTW\
    \ API token. Optionally, the user can provide the title and the tags for their\n\
    experiment. Upon submission, Adamant’s back-end prepares the content and creates\
    \ a new experiment in eLabFTW\nFigure 4. Workflow for creating a new experiment\
    \ in the eLabFTW system with Adamant.\nPage 10 of 19\nF1000Research 2022, 11:475\
    \ Last updated: 27 NOV 2023\naccordingly, making use of the available eLabFTW\
    \ API for python, namely elabapy (v0.8.2).26,27 The user will be\nnotified upon\
    \ a successful submission, and the created experiment can be viewed in the eLabFTW\
    \ system.\nThe eLabFTW system (v4.2.2) makes use of the TinyMCE text editor28\
    \ for free text descriptions of experiments. Here,\nHTML content can be loaded\
    \ and rendered. Based on this functionality, the present implementation of the\
    \ described\nworkflow generates an HTML description list content based on the\
    \ submitted JSON form data and its schema. The title of\nthe field (obtained from\
    \ the schema) is used and paired with the value of this field, as shown in Figure\
    \ 5(a), where the titles\nare encapsulated within the <dt> tags, the values within\
    \ the <dd> tags, and the complete pairs in the <dl> tags. The\ndescription list\
    \ is rendered in a way to attain or simulate the look of a form, as presented\
    \ in Figure 5(b), which increases the\nreadability of the content. This result\
    \ is achieved by using a custom Cascading Style Sheets (CSS) styling, which can\
    \ be\nprovided to the eLabFTW system. The generation of this description list\
    \ content is carried out on Adamant’s front-end\nside. Note that it is equally\
    \ possible to upload the experiment information prepared in Adamant directly to\
    \ the eLabFTW\nexperiment in JSON format. However, this would present it in the\
    \ form of additional information to the experiment\ndescription. Since we consider\
    \ the structured metadata to be the main component of the experiment documentation,\
    \ the\ndescribed workaround based on the HTML description lists in the main body\
    \ has been chosen. Still, the schema, JSON\nform data, and uploaded files (if\
    \ available) are sent as attachments to the eLabFTW experiment. In this way, the\
    \ schema\nand form data that describe the experiment are preserved and available\
    \ for re-use, while retaining the high readability of\nthe experiment body for\
    \ the user.\nSubmission of metadata for research lab workflow\nResearch institutions\
    \ are often equipped with advanced laboratory instruments, such as Scanning Electron\
    \ Microscopy\n(SEM) and Mass Spectrometry (MS) instruments just to name a few.\
    \ These instruments are pivotal for various research\ninvestigations in many scientific\
    \ fields. However, not every researcher at the institute is able to operate and\
    \ has access to\nsuch instruments. To this end, we propose a job request workflow\
    \ using Adamant that also incorporates the use of\neLabFTW for experiment documentation.\n\
    The job request workflow represented in Figure 6 aims to streamline the process\
    \ of requesting a job or an investigation of\nan object of interest using a laboratory\
    \ instrument that needs to be carried out by the designated instrument operator.\
    \ From\nAdamant’s perspective, this workflow involves two users, namely the researcher\
    \ who wants to get something done with\nan instrument they do not know how to\
    \ use (or do not have access to), denoted as the “requester”, and the instrument\n\
    operator, denoted as the “operator”. From the documentary point of view, the requester\
    \ provides the information required\nto execute the job, while the operator can\
    \ add metadata related to the instrument and the analysis method. Hence, the\n\
    schemas used for this workflow can be divided into two, the request schema and\
    \ the experiment schema. The request\nschema is used by the requester, and is\
    \ a subset of the experiment schema including only the form fields for the request\n\
    details. The experiment schema is used by the operator, which contains every form\
    \ field necessary to describe the\nexperiment using the selected instrument. Examples\
    \ of a request schema and its corresponding experiment schema\n(complete schema)\
    \ are available here and here, respectively. Both users are notified automatically\
    \ per e-mail for relevant\nevents within the workflow, such as when the requester\
    \ submits the job request and the operator starts working on the\nrequest. The\
    \ e-mail notifications are made sure to work accordingly by including the necessary\
    \ information of the\nrequester and operator within the used schemas, and by setting\
    \ up the e-mail notification configuration for the relevant\nschemas. An example\
    \ of the e-mail notification configuration file can be found in adamant/backend/conf/,\
    \ in\nwhich each configuration parameter is explained. For using more than one\
    \ job request workflow, one can simply add\nanother schema-specific configuration\
    \ into the confList keyword. The right configuration is then automatically\nselected\
    \ based on the relevant schema titles.\nFigure 5. HTML description list rendering\
    \ in eLabFTW. (a) Title-value pairs in the HTML description list format, and\n\
    (b) the rendered description list with a custom CSS styling. CSS, Cascading Style\
    \ Sheets.\nPage 11 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\n\
    Conclusions\nWe have developed a web-based tool named Adamant with the aim of\
    \ easing the implementation of digital research data\nmanagement processes. With\
    \ this, we intent to support the adoption of the FAIR data principles in independent\
    \ labs,\nwhich do not (yet) have access to established research data management\
    \ infrastructures and domain-specific standards.\nAdamant provides straightforward\
    \ compilation and creation of metadata and metadata schemas based on the JSON\n\
    schema standard, and API-based integration with other available research data\
    \ management software tools. The intuitive\nand self-explanatory UI of Adamant\
    \ increases the accessibility for users with little to no experience with JSON\
    \ format and\nprogramming in general. A flagship implementation of Adamant is\
    \ the use of the tool, generally, in a core facility or\nlaboratory of a research\
    \ institute that often hosts advanced scientific equipment, such as the scanning\
    \ electron microscopy\ninstrument. This is often an advanced expertise that different\
    \ teams want to use, while a small competence group (experts\nin such an instrument)\
    \ ensures that the investigation tasks are optimally processed. Such a use case\
    \ is demonstrated in the\nAdamant’s submission of metadata for research lab workflow.\
    \ With that as an example, Adamant can be suited to many\nspecific use cases by\
    \ extending it with relevant submission functionalities.\nData availability\n\
    Underlying data\nAll data underlying the results are available as part of the\
    \ article and no additional source data are required.\nFigure 6. Job request workflow\
    \ using Adamant involving two users: a requester and an operator. JSON,\nJavaScript\
    \ Object Notation.\nPage 12 of 19\nF1000Research 2022, 11:475 Last updated: 27\
    \ NOV 2023\nSoftware availability\n•\nAdamant (client-only version) available\
    \ from: https://plasma-mds.github.io/adamant\n•\nSource code available from: https://github.com/plasma-mds/adamant\n\
    •\nArchived source code at time of publication: https://doi.org/10.5281/zenodo.639618229\n\
    •\nLicense: MIT\nAcknowledgements\nThe authors would like to thank Nick Plathe\
    \ for their helpful suggestions and Laura Vilardell Scholten for developing the\n\
    description list CSS code.\nReferences\n1.\nWilkinson MD, Dumontier M, Aalbersberg\
    \ IJ, et al.: The FAIR guiding\nprinciples for scientific data management and\
    \ stewardship. Sci.\nData. 2016; 3: 160018.\nPubMed Abstract|Publisher Full Text\n\
    2.\nIsrael H, Tobschall E, Tristram F: Dataset for the publication\n“Umfrage zum\
    \ Forschungsdatenmanagement in der Physik”.\n2021.\nPublisher Full Text\n3.\n\
    Shaw F, Etuk A, Minotto A, et al.: COPO: a metadata platform for\nbrokering FAIR\
    \ data in the life sciences [version 1; peer review:\n1 approved, 1 approved with\
    \ reservations]. F1000Res. 2020; 9: 495.\nPublisher Full Text\n4.\nFranke S, Paulet\
    \ L, Schäfer J, et al.: Plasma-MDS, a metadata\nschema for plasma science with\
    \ examples from plasma\ntechnology. Sci. Data. 2020; 7: 439.\nPublisher Full Text\n\
    5.\nP.-K. consortium: PDBe-KB: a community-driven resource for\nstructural and\
    \ functional annotations. Nucleic Acids Res. 10 2019;\n48: D344–D353.\nPublisher\
    \ Full Text\n6.\nThe Human Cell Atlas Metadata Standards: JSON Schemas: 2022.\n\
    Accessed: 2022-04-12.\nReference Source\n7.\nMetadata-Schemas-for-Materials-Science:\
    \ 2022. Accessed: 2022-04-\n12.\nReference Source\n8.\nPezoa F, Reutter JL, Suarez\
    \ F, et al.: Foundations of JSON Schema.\nProceedings of the 25th International\
    \ Conference on World Wide Web.\n2016; pp. 263–273. International World Wide Web\
    \ Conferences\nSteering Committee.\nPublisher Full Text\n9.\nDroettboom M, et\
    \ al.: Understanding JSON Schema. 2022.\nAccessed: 2022-02-04.\nReference Source\n\
    10.\nIntroducing JSON: 2022. Accessed: 2022-03-01.\nReference Source\n11.\nBray\
    \ T: The JavaScript Object Notation (JSON) Data Interchange\nFormat. RFC 7158,\
    \ 2014.\n12.\nReactJS API: 2022. Accessed: 2022-02-04.\nReference Source\n13.\n\
    Flask: 2022. Accessed: 2022-02-04.\nReference Source\n14.\nVan Rossum G, Drake\
    \ FL: Python 3 Reference Manual. Scotts Valley,\nCA: CreateSpace; 2009.\n15.\n\
    Merkel D: Docker: lightweight linux containers for consistent\ndevelopment and\
    \ deployment. Linux journal. 2014; 2014(239): 2.\n16.\nNode.js: 2022. Accessed:\
    \ 2022-04-12.\nReference Source\n17.\nMaterial-UI: 2022. Accessed: 2022-02-04.\n\
    Reference Source\n18.\nreact-jsonschema-form: 2022. Accessed: 2022-02-04.\nReference\
    \ Source\n19.\njsonform: 2022. Accessed: 2022-02-04.\nReference Source\n20.\n\
    jsonforms.io: 2022. Accessed: 2022-02-04.\nReference Source\n21.\njson-editor:\
    \ 2022. Accessed: 2022-02-04.\nReference Source\n22.\nAjv JSON schema validator:\
    \ 2022. Accessed: 2022-02-04.\nReference Source\n23.\nRocha da Silva J, Aguiar\
    \ Castro J, Ribeiro C, et al.: Dendro:\nCollaborative research data management\
    \ built on linked open\ndata. The Semantic Web: ESWC 2014 Satellite Events. Presutti\
    \ V,\nBlomqvist E, Troncy R, et al., editors. Cham: Springer International\nPublishing;\
    \ 2014; pp. 483–487.\nPublisher Full Text\n24.\neLabFTW: a free and open source\
    \ electronic lab notebook: 2022.\nAccessed: 2022-02-04.\nReference Source\n25.\n\
    Carpi N, Minges A, Piel M: eLabFTW: An open source laboratory\nnotebook for research\
    \ labs. J. Open Source Softw. 2017; 2(12): 146.\nPublisher Full Text\n26.\neLabFTW\
    \ API: 2022. Accessed: 2022-02-04.\nReference Source\n27.\nelabapy at PyPI: 2022.\
    \ Accessed: 2022-02-04.\nReference Source\n28.\nTinyMCE: 2022. Accessed: 2022-02-04.\n\
    Reference Source\n29.\nChaerony Siffa I, Schäfer J, Becker MM: plasma-mds/adamant:\n\
    Adamant Initial Release v1.0.0 (v1.0.0). Zenodo. 2022.\nPublisher Full Text\n\
    Page 13 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\nOpen Peer\
    \ Review\nCurrent Peer Review Status:  \n \nVersion 1\nReviewer Report 07 June\
    \ 2022\nhttps://doi.org/10.5256/f1000research.122529.r136576\n© 2022 Krüger F.\
    \ This is an open access peer review report distributed under the terms of the\
    \ Creative Commons \nAttribution License, which permits unrestricted use, distribution,\
    \ and reproduction in any medium, provided the \noriginal work is properly cited.\n\
    Frank Krüger \n  \nInstitute of Communications Engineering, University of Rostock,\
    \ Rostock, Germany \nThe FAIR Guiding Principles require, among other things,\
    \ the provision of rich metadata for \nresearch data and the generating processes.\
    \ For this purpose, structured information that reuses \nexisting vocabulary in\
    \ a standardized way is desired. While such thorough documentation \nimproves\
    \ the reusability of the research data, it demands knowledge about appropriate\
    \ \nvocabularies (e.g. DCAT, DDI...) and some minimum technical understanding\
    \ of structured \ndocumentation (e.g. JSON, JSON-LD, RDF...) of researchers from\
    \ all scientific domains. \nFurthermore, mechanisms to ensure the provision of\
    \ some minimal required information are \nnecessary. These requirements increase\
    \ the complexity of providing FAIR data and create the \nneed for appropriate\
    \ tool support. \n \nAdamant is a web-based tool for the provision of structured\
    \ metadata which guides the \nresearcher through the documentation process and\
    \ provides a means for validation of the \ncontent and completeness by providing\
    \ a schema-based form. The underlying schema can either \nbe loaded from a library\
    \ or particularly designed for the research process at hand. By providing a \n\
    rich API, Adamant satisfies further requirements for the integration into complex\
    \ workflows based \non virtual research environments. \n \nFrom the technical\
    \ perspective, Adamant employs JSON Schema, a technology to specify the \nstructure\
    \ of JSON documents including required and optional information. A web-form, based\
    \ on \nPython/Flask, is generated from the schema that is presented to the end-user\
    \ to provide the \nnecessary information. Adamant is publicly hosted for testing\
    \ purposes and can easily be \ndeployed by using the provided docker image. All\
    \ code is available from github and archived at \nZenodo. The released version\
    \ worked without any problems for me. I tested the web-form and the \nintegration\
    \ with elabFTW. \n \nWhile I like Adamant and the article very much, I have a\
    \ few suggestions/questions:\nAdamant is based on JSON Schema and JSON, but recently\
    \ I recognized an increase in RDF-\nbased (meta)data management. I was wondering\
    \ if RDF/S and SHACL would also be a viable \nbase for Adamant and why the authors\
    \ didn't employ those. \n1. \n \nPage 14 of 19\nF1000Research 2022, 11:475 Last\
    \ updated: 27 NOV 2023\n \nI further agree with Felix Shaw (Reviewer 1) about\
    \ the integration of ontologies for the \nstructured and semantic documentation\
    \ of research data.\n2. \n \nIs the rationale for developing the new software\
    \ tool clearly explained?\nYes\nIs the description of the software tool technically\
    \ sound?\nYes\nAre sufficient details of the code, methods and analysis (if applicable)\
    \ provided to allow \nreplication of the software development and its use by others?\n\
    Yes\nIs sufficient information provided to allow interpretation of the expected\
    \ output datasets \nand any results generated using the tool?\nYes\nAre the conclusions\
    \ about the tool and its performance adequately supported by the \nfindings presented\
    \ in the article?\nYes\nCompeting Interests: No competing interests were disclosed.\n\
    Reviewer Expertise: Data Science, Provenance, Metadata\nI confirm that I have\
    \ read this submission and believe that I have an appropriate level of \nexpertise\
    \ to confirm that it is of an acceptable scientific standard.\nAuthor Response\
    \ 12 Jul 2022\nIhda Chaerony Siffa \nWe thank Frank Krüger for their valuable\
    \ comments. The following are our responses to \neach individual comment:\nResponse\
    \ for comment 1: From our understanding, it is feasible to use RDF(S) \nformats\
    \ and SHACL for generating web-forms and validation. Our arguments on \nusing\
    \ JSON and JSON Schema instead of RDF(S) and SHACL are (in no particular \norder):\n\
    RDF(S)/SHACL are ontology-related standards that are surely the means of \nchoice\
    \ in cases where well-defined ontologies already exist. Note that Adamant \nspecifically\
    \ targets use cases for which ontologies or metadata standards do \nnot yet exist.\
    \ \n \n○\nJSON Schema supports a set of data types that are analogous to those\
    \ found in \nmany programming languages (e.g., Python, JavaScript). This allows\
    \ \nstraightforward implementation of specific input types and their validations.\
    \ \n○\n1. \n \nPage 15 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV\
    \ 2023\n \nTo the best of our knowledge, the JSON format is the de-facto standard\
    \ of API \ndata interchange. Since Adamant can be used with different external\
    \ \napplications by means of API integration, describing the metadata and its\
    \ \nschema in the JSON format seems sensible. \n \n○\nMany existing metadata schemas\
    \ are available in the JSON format. \n \n○\nOn top of that, we plan on using ontologies,\
    \ serialized in an RDF format, to \ndescribe the schema elements that are used\
    \ in JSON schemas. A Uniform \nResource Identifier (URI) can be assigned to each\
    \ schema element in the JSON \nformat, which allows the interoperability of these\
    \ schema elements across both \nformats. \n \n○\nResponse for comment 2: Please\
    \ see the response under Reviewer 1’s comments.\n2. \n \nCompeting Interests:\
    \ No competing interests were disclosed.\nReviewer Report 03 May 2022\nhttps://doi.org/10.5256/f1000research.122529.r136392\n\
    © 2022 Shaw F. This is an open access peer review report distributed under the\
    \ terms of the Creative Commons \nAttribution License, which permits unrestricted\
    \ use, distribution, and reproduction in any medium, provided the \noriginal work\
    \ is properly cited.\nFelix Shaw \n  \nData Infrastructure and Algorithms, Earlham\
    \ Institute, Norwich, UK \nStandards for experimental metadata are in most cases\
    \ sub-optimal or non-existent. Being able to \ndescribe protocols, samples, instruments,\
    \ etc. in a consistent way is vital to the reusability of data \nand the reproducibility\
    \ of work. This situation will only continue with the prevalence of machine \n\
    learning techniques in science which rely heavily on labeled data. Adamant is\
    \ a web tool for \nauthoring generic experimental assays and then filling out\
    \ such assays. Users are presented with \nan interface in which they can add fields\
    \ of different data types, and various input restrictions (e.g. \nenumerations,\
    \ character lengths, etc.). The system then creates a JSON document that records\
    \ \nthese fields and can be used later for validation and display. Users can then\
    \ fill out these \ngenerated forms with real assay data which is validated against\
    \ the JSON document, and in turn, \nproduces another JSON document with the assay\
    \ data itself. \n \nThis is a good tool, is well presented, and is easy to use.\
    \ In functionality, it seems comparable with \nthe ISATools suite of software,\
    \ although less mature and (arguably) more intuitive. It uses \nindustry-standard\
    \ software (Python, Flask, React, etc.) and is under an MIT license on Github,\
    \ \nmeaning anyone can download and modify it. This means if uptake is good, there\
    \ is a fair \n \nPage 16 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV\
    \ 2023\npossibility of community maintenance. There are precious few tools for\
    \ authoring metadata \nschemas, and this is a welcome step in the right direction.\
    \ I downloaded and tested the docker \nversion and it worked without any problem.\
    \ I didn't download and run the development version. \n \nThere are some things\
    \ I would like to see:\nCreating metadata standards is in my experience very much\
    \ a community effort. Whilst this \ntool makes the \"process\" of making metadata\
    \ standards easy, there needs to be an \nemphasis on community agreed standards.\
    \ Otherwise, we risk the proliferation of a \nmultitude of standards that fit\
    \ only very niche applications. \n \n○\nTalking of community agreed standards,\
    \ it would be nice to have a large selection of \nexisting standard schemas available\
    \ for users to select, and then possibly edit. This would \nbe a big time-saver\
    \ for a lot of researchers. \n \n○\nThere doesn't seem to be a way of using ontology\
    \ terms for field names, values, or units. \nOntologies are the best way of disambiguating\
    \ metadata, and services such as EMBL/EBI's \nOntology Lookup Service make real-time\
    \ searching for ontology terms easy. I would like to \nsee this implemented in\
    \ the authoring stage. \n \n○\nI have a feeling this will not scale particularly\
    \ well. It's fine to fill out a web form for a few \nsamples or assays, but in\
    \ practice, researchers will have hundreds or thousands of records. \nFilling\
    \ out web forms on this scale is impracticable. I would like to know the author's\
    \ views \non how to overcome this issue. \n \n○\nWhilst the authors give the use\
    \ case of submitting an assay to eLabFTW, it would be nice to \nsee some other\
    \ integrations, such as with Dataverse, Dspace, or CKAN repositories, which \n\
    are often used by smaller research facilities (the apparent target audience for\
    \ this work).\n○\n \nIs the rationale for developing the new software tool clearly\
    \ explained?\nYes\nIs the description of the software tool technically sound?\n\
    Yes\nAre sufficient details of the code, methods and analysis (if applicable)\
    \ provided to allow \nreplication of the software development and its use by others?\n\
    Yes\nIs sufficient information provided to allow interpretation of the expected\
    \ output datasets \nand any results generated using the tool?\nYes\nAre the conclusions\
    \ about the tool and its performance adequately supported by the \nfindings presented\
    \ in the article?\nYes\nCompeting Interests: No competing interests were disclosed.\n\
     \nPage 17 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\nReviewer\
    \ Expertise: Data Science, Open Science, Metadata Standards, Web Development\n\
    I confirm that I have read this submission and believe that I have an appropriate\
    \ level of \nexpertise to confirm that it is of an acceptable scientific standard.\n\
    Author Response 12 Jul 2022\nIhda Chaerony Siffa \nWe thank Felix Shaw for their\
    \ insightful and constructive comments and helpful \nsuggestions. Some of the\
    \ comments are in line with what we already have in mind for \nfurther development\
    \ of the tool which we will include in the next version of the manuscript \nas\
    \ an outlook. Nevertheless, we will still attempt to address each individual comment\
    \ briefly \nas follows:\nResponse for comment 1: We definitely agree and have\
    \ experienced this matter first \nhand. At the very early stage, Adamant is intended\
    \ to be used to create and design \nthe initial experiment schema, which later\
    \ can be shared/made public in a publicly \navailable collaborative version control\
    \ platform (like GitHub). Once the schema is \nmade public, the community can\
    \ participate in the further development and \nmaintenance of the said schema.\
    \ \n \n1. \nResponse for comment 2: Thank you very much for this suggestion. Besides\
    \ the \npublic maintenance of already available schemas for the domain of plasma\
    \ \ntechnology (see our community effort at https://www.plasma-mds.org) it is\
    \ indeed an \ninteresting idea to provide easy access to established standards\
    \ like Dublin Core, \nDataCite, etc., via Adamant. We will think further in that\
    \ direction. \n \n2. \nResponse for comment 3: We agree with the comments regarding\
    \ ontologies and \nwe have already thought of something in this direction. We\
    \ will definitely implement \nfeatures related to the use of ontologies in the\
    \ future release of Adamant. One thing \nthat we have planned is to have a domain-specific\
    \ ontology that can be attached to \nthe tool where it can be used to populate\
    \ the domain-specific knowledge graph. \n \n3. \nResponse for comment 4: This\
    \ is true. In the field of low-temperature plasma \nscience and technology, there\
    \ is rarely any case that requires the researcher to fill out \nhundreds or even\
    \ thousands of entries in an experiment. However, to make sure that \nAdamant\
    \ can be adopted in different research fields, this requirement is, of course,\
    \ \ncrucial. One thing that we have in mind is to have a feature of uploading\
    \ a tabular file, \nwhich is then translated to its JSON representation. Such\
    \ a feature would also enable \nthe collection and further processing of standard-conform\
    \ metadata in “offline” field \nexperiments. \n \n4. \nResponse for comment 5:\
    \ Currently, we are looking into implementing a feature of \ndataset publication\
    \ in the DKAN-based data platform INPTDAT \n(https://www.inptdat.de/). The idea\
    \ is to re-use the collected experiment metadata \n(created with Adamant and stored\
    \ in the eLabFTW ELN) and bundle it with the dataset \nfor publication. Note that\
    \ the back-end can be easily adapted to use the existing APIs \nof the mentioned\
    \ repository platforms to publish metadata, once it is available in a \n5. \n\
     \nPage 18 of 19\nF1000Research 2022, 11:475 Last updated: 27 NOV 2023\nstandardized\
    \ format.\n \nCompeting Interests: No competing interests were disclosed.\nThe\
    \ benefits of publishing with F1000Research:\nYour article is published within\
    \ days, with no editorial bias\n•\nYou can publish traditional articles, null/negative\
    \ results, case reports, data notes and more\n•\nThe peer review process is transparent\
    \ and collaborative\n•\nYour article is indexed in PubMed after passing peer review\n\
    •\nDedicated customer support at every stage\n•\nFor pre-submission enquiries,\
    \ contact research@f1000.com\n \nPage 19 of 19\nF1000Research 2022, 11:475 Last\
    \ updated: 27 NOV 2023\n"
  inline_citation: '>'
  journal: F1000Research
  limitations: '>'
  pdf_link: https://f1000research.com/articles/11-475/v1/pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'Adamant: a JSON schema-based metadata editor for research data management
    workflows'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.35662/unine-thesis-2812
  analysis: '>'
  authors:
  - Rafael Pires
  citation_count: 0
  full_citation: '>'
  full_text: '>

    This object is not longer available Take me to the home page Portal overview User
    guide Open Access strategy Open Access directive Research at UniNE Open Access
    ORCID What''s new Service information scientifique & bibliothèques Rue Emile-Argand
    11 2000 Neuchâtel contact.libra@unine.ch Powered by DSpace, DSpace-CRIS & 4Science
    | v2022.02.00 Vos données personnelles sont récupérées et utilisées dans les contextes
    suivants : authentification, préférences, consentement et statistiques. Pour plus
    d''informations, veuillez vous référer à la privacy policy. Personnaliser RefuserThat''s
    ok'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: 'Distributed systems and trusted execution environments : trade-offs and
    challenges'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
