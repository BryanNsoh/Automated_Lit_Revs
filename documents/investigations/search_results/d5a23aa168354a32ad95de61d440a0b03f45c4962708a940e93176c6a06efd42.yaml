- DOI: https://doi.org/10.1109/mic.2019.2955784
  analysis: '>'
  authors:
  - Fred Douglis
  - Jason Nieh
  citation_count: 12
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet Computing
    >Volume: 23 Issue: 6 Microservices and Containers Publisher: IEEE Cite This PDF
    Fred Douglis; Jason Nieh All Authors 12 Cites in Papers 2179 Full Text Views Abstract
    Document Sections IN THIS ISSUE Authors Citations Keywords Metrics Abstract: The
    articles in this special section focus on microservices and containers. These
    services allow an application to be comprised of many independently operating
    and scalable components, have become a common service paradigm. The ability to
    construct an application by provisioning these interoperating components has various
    advantages, including the isolation and independent development of tools such
    as key-value stores, authentication, logging, and many others. Containers are
    one type of system infrastructure that is commonly used to support microservices.
    With container management systems like Docker and orchestration systems like Kubernetes
    to control applications and dynamically provision their resources, cloud services
    can be extremely scalable, reliable, and reactive. However, other systems beyond
    containers can be used to support microservices, and many applications other than
    microservices benefit from containerization. Published in: IEEE Internet Computing
    ( Volume: 23, Issue: 6, 01 Nov.-Dec. 2019) Page(s): 5 - 6 Date of Publication:
    01 Nov.-Dec. 2019 ISSN Information: DOI: 10.1109/MIC.2019.2955784 Publisher: IEEE
    Microservices, which allow an application to be comprised of many independently
    operating and scalable components, have become a common service paradigm. The
    ability to construct an application by provisioning these interoperating components
    has various advantages, including the isolation and independent development of
    tools such as key-value stores, authentication, logging, and many others. Containers
    are one type of system infrastructure that is commonly used to support microservices.
    With container management systems like Docker and orchestration systems like Kubernetes
    to control applications and dynamically provision their resources, cloud services
    can be extremely scalable, reliable, and reactive. However, other systems beyond
    containers can be used to support microservices, and many applications other than
    microservices benefit from containerization. Containers should be contrasted with
    another virtualization technique, traditional virtual machines. Back in March
    2013, IEEE Internet Computing published a special issue on virtualization. By
    then, virtual machines had become a popular way to isolate applications, as a
    specialized server could run in its own virtual machine while sharing a pool of
    available resources (such as a VMware ESXi server). Virtual machines provide a
    convenient way to encapsulate state (so that machines can be migrated among servers)
    and to deploy a service in a predictable fashion. For instance, a service can
    be wrapped in a deployable template that can be installed into a virtualization
    environment with just a few configuration steps. Containers, by comparison, provide
    a way to virtualize and isolate the operating system, allowing multiple applications
    to run in a single operating system; i.e., it is the software rather than the
    hardware that is virtualized. Without the need to run multiple operating system
    instances, containers can be more lightweight and potentially easier to manage.
    But like virtual machine templates, containers can have specifications that define
    exactly what software environment an application is to be run within. For instance,
    a container might be created with a specific version of Ubuntu, in which a specific
    version of python and python libraries would execute. Given the ability to create
    services with various interacting containers, which may execute on one or many
    nodes, complex applications can be synthesized by combining these services in
    interesting ways. In particular, as demand varies, the individual microservices
    can be replicated to scale with demand or reduced to fit current requirements.
    To support this adaptivity, the overall service must be architected to handle
    parallelism of individual microservices and to perform appropriate selection of
    the available instances to maximize performance. IN THIS ISSUE There are three
    articles in this theme issue on microservices and containers. The first two focus
    on how microservices should best be used, whereas the third provides a case study
    of containerization in a specific application context. The first article reports
    on work from the Standard Performance Evaluation Corporation (SPEC) Research Group
    on Cloud Computing (https://research.spec.org/working-groups/rgcloud.html). In
    “The SPEC-RG Reference Architecture for FaaS: From Microservices and Containers
    to Serverless Platforms,” Van Eyk et al. describe best practices for Functions-as-a-Service.
    FaaS is a key example of the elastic scalability of microservices described above:
    instantiate functions when they are needed, and eliminate them when not. The authors
    evaluate numerous existing examples of serverless computing, which support the
    FaaS model, and describe how it is best deployed. They compare a number of platforms,
    such as Kubernetes, services from Amazon Web Services, Apache, and others; and
    many more. Akbulut and Perros in “Performance Analysis of Microservice Design
    Patterns,” focus more specifically on the performance of microservices. They study
    several metrics (query response time, efficient hardware usage, hosting costs,
    and packet loss rate) as applied to three design patterns. An API gateway, which
    acts as a load balancer and a guard against overload (note that this gateway bears
    some resemblance to the application described in the third paper, below). A chain
    of microservices that pass data from stage to stage for different types of processing.
    Asynchronous messaging, using RabbitMQ. Finally, Amirante and Romano authored
    “Container NATs and Session-Oriented Standards: Friends or Foe?” This article
    takes a very different view of the areas covered by this special issue, with the
    focus on containers and in particular the network isolation that is common in
    container environments. In particular, they observe that Docker isolates applications
    by providing them one Internet Protocol (IP) address and then translating that
    using “network address translation” for use outside the container. Some applications
    need to know the IP address by which they are reached from the outside, and forcing
    NATs on such applications raise a level of complexity. Amirante and Romano explain
    the issues and propose some possible work-arounds. Authors Citations Keywords
    Metrics More Like This Lightweight Virtualization Approaches for Software-Defined
    Systems and Cloud Computing: An Evaluation of Unikernels and Containers 2019 Sixth
    International Conference on Software Defined Systems (SDS) Published: 2019 Performability
    analysis of I/O bound application on container-based server virtualization cluster
    2014 IEEE Symposium on Computers and Communications (ISCC) Published: 2014 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE internet computing
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/4236/8970628/08970636.pdf
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Microservices and Containers
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.future.2019.05.062
  analysis: '>'
  authors:
  - Tamás Kiss
  - James DesLauriers
  - Gregoire Gesmier
  - Gábor Terstyánszky
  - Gabriele Pierantoni
  - Osama Abu Oun
  - Simon J. E. Taylor
  - Anastasia Anagnostou
  - József Kovács
  citation_count: 15
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. MiCADO
    — Microservices-based Cloud Application-level Dynamic Orchestrator 4. Experiment
    structure 5. JQueuer design and implementation 6. JQueuer integration with MiCADO
    7. Deadline-based execution of an agent-based simulation 8. Conclusion and future
    work Acknowledgement References Vitae Show full outline Figures (10) Show 4 more
    figures Future Generation Computer Systems Volume 101, December 2019, Pages 99-111
    A cloud-agnostic queuing system to support the implementation of deadline-based
    application execution policies Author links open overlay panel Tamas Kiss a, James
    DesLauriers a, Gregoire Gesmier a, Gabor Terstyanszky a, Gabriele Pierantoni a,
    Osama Abu Oun b, Simon J.E. Taylor c, Anastasia Anagnostou c, Jozsef Kovacs d
    Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.future.2019.05.062
    Get rights and content Under a Creative Commons license open access Highlights
    • JQueuer is a queuing system to be used in conjunction with container technologies.
    • JQueuer supports the execution of jobs and the enforcement of scaling policies.
    • JQueuer is integrated with the MiCADO orchestration framework. • Deadline-based
    execution policies for job submission/batch processing applications. • Deadline
    based execution of agent-based simulation application using REPAST. Abstract There
    are many scientific and commercial applications that require the execution of
    a large number of independent jobs resulting in significant overall execution
    time. Therefore, such applications typically require distributed computing infrastructures
    and science gateways to run efficiently and to be easily accessible for end-users.
    Optimising the execution of such applications in a cloud computing environment
    by keeping resource utilisation at minimum but still completing the experiment
    by a set deadline has paramount importance. As container-based technologies are
    becoming more widespread, support for job-queuing and auto-scaling in such environments
    is becoming important. Current container management technologies, such as Docker
    Swarm or Kubernetes, while provide auto-scaling based on resource consumption,
    do not support job queuing and deadline-based execution policies directly. This
    paper presents JQueuer, a cloud-agnostic queuing system that supports the scheduling
    of a large number of jobs in containerised cloud environments. The paper also
    demonstrates how JQueuer, when integrated with a cloud application-level orchestrator
    and auto-scaling framework, called MiCADO, can be used to implement deadline-based
    execution policies. This novel technical solution provides an important step towards
    the cost-optimisation of batch processing and job submission applications. In
    order to test and prove the effectiveness of the solution, the paper presents
    experimental results when executing an agent-based simulation application using
    the open source REPAST simulation framework. Previous article in issue Next article
    in issue Keywords Cloud computingContainer technologiesDeadline-based auto-scalingJQueuerMiCADOAgent-based
    simulation 1. Introduction Cloud computing offers scalable and on-demand access
    to large amounts of computational and data resources. Operating System (OS) level
    virtualization, also known as container-based virtualization has recently attracted
    much attention due to its near-native performance and low virtualization overhead
    [1]. In science gateways, containers simplify packaging applications so as to
    run on any cloud independently of the cloud’s configuration. A container orchestration
    engine takes multiple resources in the cloud, combines them into a single pool,
    and provides an abstracted layer between the cloud resources and the application
    container that run on these resources. Most applications can be containerised
    along with all their libraries so as to run in any cloud without the need to install
    any prerequisites. Containers are stateless which makes them suitable for services
    that perform single functions and do not need to store data in the containers.
    A Web service is an example of these stateless services where it is possible to
    create/clone several containers so as to process HTTP requests concurrently. On
    the other hand, several types of batch processing applications in science and
    business gateways, for example discrete-event and agent-based simulation or image/video
    processing, require a mechanism to launch containers and provide each one of them
    with the jobs or data which should be processed. These applications typically
    consist of hundreds or thousands of scenarios which need to be executed, usually
    independently from each other. Some scenarios are lightweight allowing several
    of them to be executed at the same time on the same machine in different containers,
    while others consume large amount of resources and need longer time to be accomplished.
    These applications are all dealing with the submission of jobs where the results
    need to be kept after execution. The need to provide the containers with jobs
    or data and collect the output after execution is not the only difference between
    stateless services and batch processing applications. Typical policies applied
    when scaling containers are also different. Scaling up/down the containers of
    a stateless service may depend on the load on its containers, CPU and memory consumption,
    number of requests, average response time, etc. However, in batch processing applications,
    we might need to take scaling decisions depending on completely different types
    of policies such as the time required to process a job, the deadline to finish
    all jobs in the queue, or the length of the queue. In some cases, we do not need
    to scale up the containers if there are no more jobs in the queue, even if all
    containers are consuming 100% of their resources. The time required to finish
    a job (job duration) is one of the most important factors to be taken into consideration
    in order to decide the number of containers needed to process jobs in the queue.
    Job duration may differ completely from one job to another making this estimation
    even more complex. For example, a queue of video files to be processed might contain
    videos of different lengths, from one minute to one hour. If the overall execution
    of the experiment needs to be finished by a given deadline, then this results
    in the need to periodically auto-scale up/down the containers based on user-defined
    scaling policies. While Stateless Services are widely supported by current technologies,
    there is very limited or no support for job-queuing, execution and related policy-based
    auto-scaling mechanisms which are required by batch processing applications. The
    lack of these components forces application developers to spend time and money
    to develop proprietary tools or to customise open source libraries to work in
    container-based environments. These components should also be available to run
    on public and private clouds based on various technologies. Therefore, the solutions
    should be agnostic to the underlying cloud middleware. In this paper we propose
    JQueuer, a cloud-agnostic queuing system that supports the scheduling of a large
    number of jobs in containerised cloud environments. We also demonstrate that JQueuer
    can be integrated with a managed container platform, such as MiCADO, Microservices-based
    Cloud Application-level Dynamic Orchestrator that was presented in [2]. MiCADO
    originally supported the scaling of stateless services only based on performance
    based metrics, such as memory or CPU utilisation. By extending MiCADO with JQueuer
    as an external service, it is possible to realise deadline-based execution policies
    required by job submission or batch processing applications. The job queuing and
    scheduling is provided by JQueuer, while MiCADO is responsible for the implementation
    and enforcement of suitable auto-scaling policies. The applicability of the implemented
    solution is demonstrated by designing a deadline-based execution policy for an
    agent-based simulation application using the open source REPAST framework [3]
    and executing several experiments to assess its efficiency. While these experiments
    demonstrate that JQueuer and MiCADO can be efficiently utilised to execute such
    experiments, it also has to be noted that the focus of this paper is to demonstrate
    the technical concepts and their usability, and not to design and optimise deadline
    based execution policies. This latter was out of our scope and will only be covered
    in future work. Based on the above, the major contribution of this paper is a
    novel cloud agnostic queuing system for containerised environments. JQueuer is
    a stand-alone component that can be easily reused by other researchers or developers
    seeking a suitable job-queue for container-based execution. Additionally, the
    paper also provides an example for the utilisation of this component when implementing
    deadline-based execution policies. The major motivation behind our work was inspired
    by the requirements of real-life industry, public sector and research focused
    use-cases currently being investigated in two European research projects. In the
    COLA (Cloud Orchestration at the Level of Application) project [4], Saker Solutions
    Ltd. [5], a UK-based simulation consultancy company is developing a discrete-event
    based simulation application for evacuation modelling. In the CloudiFacturing
    (Cloudification of Production Engineering for Predictive Digital Manufacturing)
    [6] project DSS Consulting [7], a Hungarian technology company is working together
    with a truck component manufacturer to optimise its production processes, also
    using discrete-event simulation. In both scenarios large scale simulation experimentation
    is required that needs to be significantly speeded up using cloud computing resources.
    These simulation runs need to finish by given deadlines otherwise timely, sometimes
    critical decisions cannot be made. On the other hand, the number of jobs to execute
    in these simulations and their duration can significantly change between scenarios.
    Therefore, it is not possible to reliably predict the volume of resources required.
    In current practice, both companies use fixed internal resources that are limited,
    not scalable and also expensive to maintain. When migrating their applications
    to the cloud, they both would like to utilise its elastic nature fully, leaving
    it to the underlying layers to define the optimal volume of resources to be utilised,
    and optimise the execution for reaching the given deadline while minimising costs.
    The agent-based simulation scenario presented in this paper, although it comes
    from academic research, is very similar in nature to these industry use-cases
    and therefore it provides a good basis for experimentation. The rest of this paper
    is organised as follows. Section 2 discusses the state of the art and related
    work. Section 3 describes the MiCADO framework that will be used as the managed
    container platform to be integrated with JQueuer and where the deadline-based
    execution policy is implemented. Section 4 explains the structure of an experiment
    which will be used when designing JQueuer. Section 5 describes the design and
    implementation of JQueuer, while Section 6 explains how it has been integrated
    with MiCADO. The implementation of a REPAST experiment and its performance results
    are presented in Section 7. Finally, Section 8 concludes this paper and outlines
    future work. 2. Related work A number of solutions and studies have been proposed
    to tackle the problem of auto-scaling jobs and services execution in container-based
    cloud environments. This section first compares the job-queuing and auto-scaling
    capabilities of three widely used container orchestration engines, then looks
    at open-source job scheduling systems, and finally analyses some managed solutions
    offered by hosted cloud platforms. Docker Swarm [8] is an orchestration tool which
    manages a cluster of Docker Engines running Docker containers. Docker provides
    the service concept in which the services are “containers in production”. A service
    only runs one image, but it codifies the way that image runs, what ports it should
    use, how many replicas of the container should exist, and so on. Scaling a service
    changes the number of container replicas running that piece of software, assigning
    more computing resources to the service in the process. The containers in a service
    are stateless. It is widely used and relatively simple to interface with, but,
    out-of-the-box, Swarm is not suitable for queueing jobs. Kubernetes [9] is another
    orchestration tool for Docker, as well as for other container runtimes. Kubernetes
    is a more complex orchestrator, and as such, offers a larger variety of workloads,
    including the Kubernetes Job. The Job controller can deploy and scale sets of
    containers in parallel and ensure that a specific number of containers run to
    successful completion. This offers more certainty in job completion that does
    Swarm, and permits batching jobs, but Kubernetes itself offers no built-in system
    for queuing and no scaling policy for meeting a deadline. Apache Mesos is a cluster
    manager that provides efficient resource isolation and sharing across distributed
    applications [10]. There are several projects which have grown out of Mesos, including
    container orchestration. Mesos also features a Job framework called Metronome
    [11]. Less mature than Kubernetes Jobs, Metronome allows for the creation of containers
    on a schedule, with set resources. However, at the time of writing, there is not
    yet any mechanism built-in to Metronome to support a job queue that executes to
    a set deadline. In another example, Mesos container orchestration is interfaced
    with Jenkins, an open-source automation server, to deploy and scale jobs in containers
    according to a fixed scaling policy based on job queue length [12]. Although Jenkins
    and this Mesos plugin provide a working model for a scalable job queuing system,
    its scaling policy is fixed and static. While these container orchestrators offer
    scalability, they do not provide monitored job-queuing support or complex, deadline-based
    scaling policies out-of-the-box. In comparison, JQueuer monitors and executes
    queued jobs in containers, while MiCADO provides flexible deadline-based scaling
    policies and auto-scaling of the underlying cloud resources. There are several
    widely used job scheduling systems in the open-source domain that are starting
    to support containerised environments. However, these solutions are far too heavy
    and their Docker support is currently limited. HTCondor is an open-source high
    throughput computing software framework for coarse-grained distributed parallelisation
    of computationally intensive tasks [13]. HTCondor has support for launching containers
    but it does not communicate with container orchestrators which prevents the applications
    from using services offered by these orchestrators, such as networking between
    containers or Docker Compose. SLURM (Simple Linux Utility for Resource Management)
    [14] is another open source, fault-tolerant, and highly scalable cluster management
    and job scheduling system for large and small Linux clusters. While both HTCondor
    and SLURM support integration with an elastic cluster to auto-scale the underlying
    compute nodes, they are far from being cloud-agnostic, relying on the proprietary
    cloud services of only a few specific cloud service providers. Furthermore, the
    auto-scaling logic of these clusters are generally inflexible and do not support
    queue monitoring and deadline-based scaling out-of-the-box. In comparison, JQueuer
    and MiCADO seek to avoid such vendor lock-in and provide flexible scaling rules.
    CQueue is a promising job scheduling service, fully supporting Docker containers,
    which offers job queue submission and the parallel execution of those jobs in
    containers on any cloud provider [15]. CQueue, similarly to HTCondor and SLURM,
    takes a stateless approach to the queue. It creates a new container when a job
    is pulled from the queue and kills that container when the job completes. However,
    large-scale scientific and industry simulation software may be resource demanding
    in which case creating a new container for each job can generate large overheads.
    These overheads lead to an overall decrease in efficiency on completing the queue.
    The design described in this paper takes a more stateful route to the container,
    running it as a service which is able to pick up job after job from the queue,
    and is only killed when a policy enforces a scale-down, or the experiment ends.
    Within the domain of large hosted providers, Amazon Web Services (AWS) offers
    AWS Batch [16] as a part of the Elastic Container Service (ECS). This is a full
    featured queue system which schedules, scales and executes jobs in containers
    across multiple virtual machine nodes and offers optimised scaling based on job
    resource requirements. AWS Batch does not scale to a deadline by default and is
    only available inside AWS. Therefore, it is not a solution for private or community
    clouds, or any other public cloud. Microsoft Azure offers a very similar solution,
    also called Batch [17]. Just like the AWS solution, it offers container and virtual
    machine scaling and submission to a queue with job execution in the container,
    and even has a promising custom metric auto-scaler. However, just like the AWS
    solution, it ties any user to one specific cloud. Within academic research, there
    is ample work which compares scaling algorithms for deadline-constrained workflows
    in the cloud, done at a theoretical level using a cloud simulator to mimic the
    behaviour of scalable virtual machines [18], [19]. Other research efforts look
    at implementing scaling mechanisms which manage auto-scaling virtual machines
    in order to complete queued jobs before a given deadline [20], [21]. However,
    each of these papers is focused at the level of virtual machines, and does not
    delve into container environments. According to our best knowledge, no research
    to date has been dealing with the queued execution of jobs in containers which
    scale automatically along with the underlying cluster in order to finish the queue
    of jobs before a set deadline. When compared to the above detailed solutions,
    JQueuer has been designed to implement containers as services, support flexible
    scaling policies based on custom metrics, and be platform and cloud-independent.
    In addition, it can work with any container orchestration engine. 3. MiCADO —
    Microservices-based Cloud Application-level Dynamic Orchestrator MiCADO is an
    application-level multi-cloud orchestration and auto-scaling framework that is
    currently being developed in the European H2020 COLA project [4]. The concept
    of MiCADO is described in detail in [2]. In this section only a high-level overview
    of the framework is provided to explain its architecture, building blocks and
    implementation. The generic, technology independent architecture of MiCADO is
    presented in Fig. 1. MiCADO consists of two main logical components: Master Node
    and Worker Node. Master Node is the head of the cluster performing the collection
    of information on microservices, the calculation of optimised resource usage,
    the decision making, and the realisation of decisions related to handling resources
    and to scheduling microservices. Worker Nodes are volatile components representing
    execution environments for the microservices. These nodes are continuously allocated/released
    based on the dynamically changing requirements of the running microservices. Once
    a new Worker Node is allocated and attached to the cluster, the Master Node utilises
    its resources by allocating microservices on it. Download : Download high-res
    image (406KB) Download : Download full-size image Fig. 1. Generic, technology
    independent architecture of MiCADO. The MiCADO Master Node (box with dashed line
    on the left in Fig. 1) includes six components. MiCADO Submitter is the primary
    service endpoint for creating an infrastructure to run an application, and managing
    this infrastructure and the application itself. The incoming description is interpreted
    by the MiCADO Submitter and related parts are forwarded to other key components.
    Creating new MiCADO Worker Nodes and deploying application containers on these
    Worker Nodes are the responsibility of Cloud Orchestrator and Container Orchestrator
    components, respectively. The Cloud Orchestrator is responsible for communication
    with the Cloud API for allocating and releasing resources, and building up and
    shutting down new MiCADO Worker Nodes when necessary. The Container Orchestrator
    allocates new microservices (realised by containers) on the Worker Nodes, keeps
    track of their execution and destroys them if necessary. The Monitoring System
    collects information on load of the resources and on resource usage of the container
    services, and provides this information for the other components on the Master
    Node. It also provides alerting functionality in relation to the measured attributes
    to detect values that require reaction. The Policy Keeper implements policies
    and makes decisions related to allocating/releasing cloud resources and scheduling
    container services among the Worker Nodes. Moreover, this component makes sure
    that the Cloud and the Container Orchestrator are instructed in a synchronised
    way during the operation of the entire system. The Execution Optimizer is a background
    microservice performing long-running calculations on demand for finding optimised
    setup of both cloud resources and container infrastructures. MiCADO Worker Nodes
    (boxes with dashed line on the right in Fig. 1) contain the Node/container monitor
    that is responsible for measuring the load of the resources and the resource usage
    of the container services. The measured attributes are then provided to the Monitoring
    System running on the Master Node. The Container Executor starts, executes and
    destroys containers upon requests from the Container Orchestrator. Container components
    are realising the user services defined in the (container) infrastructure description
    submitted through the MiCADO Submitter on the Master Node. The current implementation
    of MiCADO utilises Occopus [22], an open source multi-cloud orchestration solution
    as Cloud Orchestrator that is capable of launching virtual machines on various
    private (e.g. OpenStack or OpenNebula-based) or public (e.g. Amazon Web Services
    or CloudSigma [23]) cloud infrastructures, and also via the CloudBroker Platform
    [24] (please note that in the current implementation of MiCADO all virtual machines
    must be mapped to the same cloud, therefore multi-cloud applications where virtual
    machines of the same application are running in different clouds, are not supported).
    For Container Orchestration the MiCADO prototype applied in this paper uses Docker-Swarm
    [8]. However, it is worth mentioning that the latest version of MiCADO also supports
    Kubernetes [9] as this component. The monitoring component is based on Prometheus
    [25], a lightweight, low resource consuming, but powerful monitoring tool. The
    MiCADO Submitter and Policy Keeper components were custom implemented during the
    COLA Project. The current MiCADO prototype does not include the Optimiser component,
    its design and development has just started at the time of writing this paper.
    Infrastructure and policy descriptions (left hand side of Fig. 1) are provided
    in the form of a TOSCA-based (Topology and Orchestration Specification for Cloud
    Applications) [26] Application Description Template (ADT) (for further details
    regarding MiCADO ADTs please see [27]). Finally, application logic is deployed
    in Docker containers on the MiCADO worker nodes. 4. Experiment structure In previously
    mentioned batch processing or job submission applications, for example simulations
    or image/video processing, there are always numerous scenarios that need to be
    completed on large computational resources. However, as these application areas
    evolved independently, the vocabulary used to identify the different units of
    execution are rather varied. In order to avoid any confusion or misunderstanding,
    in this section we define and present these units of execution as experiment,
    job and task, as they are illustrated in Fig. 2. The figure illustrates the structure
    of the JSON (JavaScript Object Notation) [28] file that is required to define
    and submit an experiment in JQueuer, and therefore it indicates the purpose and
    role of these various untis of execution. 4.1. Experiment An experiment consists
    of two parts. The first part is a set of global parameters (upper part of Fig.
    2), and the second part consists of a list of jobs (lower part of Fig. 2). Global
    parameters define the desired cloud infrastructure and scaling properties. For
    launching the cloud infrastructure, these include the endpoint of the container
    management platform, the application’s Docker image and the resources of the virtual
    machine workers to be provisioned. For scalability, the necessary parameters include
    the deadline by which the experiment should be finished, and an estimated average
    execution time for each job. The full list of global parameters can be seen in
    the upper part of Fig. 2. The second part contains the experiment jobs and tasks
    — the data necessary to run the intended workloads on the infrastructure defined
    by the global parameters. An experiment (including jobs and tasks) might be stored
    using various formats. In case of JQueuer this format is JSON, but other possible
    options include XML (Extensible Markup Language) or YAML (Yet Another Markup Language).
    The next sections describe the definition of jobs and tasks within an experiment.
    Download : Download high-res image (114KB) Download : Download full-size image
    Fig. 2. Definition of global parameters and list of jobs and tasks for an experiment
    (experiment.json). 4.2. Job A job consists of three parts: Pre-job Command (1),
    Tasks (2) and Post-job Command (3). While the first and third parts are optional,
    the second part is required. Pre-Job, Post-Job and task commands will be invoked
    within the container so as to launch an application or execute a system call,
    for example. 1. Pre-Job Command (Optional): It is the command that should be invoked
    in the container at the beginning of each job and before running the tasks. The
    command might be used to initialise the parameters or to reserve the resources
    which are needed to execute a task. 2. Tasks (Required): It is a list of tasks
    that should all be executed sequentially in the same container. If any task fails
    for any reason, the whole job will be considered as “failed” and it will be re-queued
    or cancelled, depending on the configuration of the system. Quite often, each
    job consists of one task only. However, in some experiments tasks are depending
    on each other and need to be executed in a certain order inside a job (e.g. the
    first task would parse the argument and download files from a server, the second
    task would run the application, while the third task will upload the results to
    a server). Another motivation to put multiple tasks inside one job is to enhance
    network utilisation and reduce overhead by fetching and executing multiple inputs
    in a batch (e.g. fetching of multiple images at once in order to be analysed sequentially
    instead of fetching one image at a time). 3. Post-Job Command (Optional): This
    command should be executed after finishing all the tasks of the job and before
    getting a new job from the job queue. It might be used to free the resources,
    reset the parameters, etc. A job is considered “accomplished” when all its tasks
    have executed successfully. 4.3. Task A task is the smallest unit in this structure.
    It contains the command line that should be called in the container and the parameters
    (arguments) which should be passed along with this command. An example of the
    above structure is a simulation experiment. The experiment has global parameters
    including the container’s image. Let there be a thousand jobs in this experiment
    and let each job consists of one task. The task in this case will contain the
    command line of the simulation application that needs to be executed inside a
    container and the different sets of parameters that this command requires. 5.
    JQueuer design and implementation JQueuer is a queuing system that can be used
    in conjunction with container technologies to support the execution of a large
    number of jobs. JQueuer is a distributed system that is composed of two independent
    components: JQueuer Manager and JQueuer Agent (Fig. 3). In the following, we are
    going to discuss the structure and functionality of each of these. Download :
    Download high-res image (209KB) Download : Download full-size image Fig. 3. JQueuer
    Manager and Agent — design and implementation with a generic container management
    platform. 5.1. JQueuer Manager design JQueuer Manager is the main component of
    the JQueuer system. It runs externally of any container management platform (running
    Docker Swarm/Kubernetes) as a standalone component. JQueuer Manager consists of
    several sub-components, named in non-italicised font in Fig. 3. Each sub-component
    has a different set of tasks. The sub-components and their tasks are described
    as follows: 1. Experiment Receiver: A RESTful web service which provides a standard
    API to submit the experiment file/object to the JQueuer system via HTTP Request.
    When an experiment is received, the “Experiment Receiver” will generate an “Experiment
    ID” which will be used to identify this experiment in the system. The experiment
    sender will receive this ID as a HTTP response. Experiments to JQueuer are described
    in JSON format as illustrated in Fig. 2 and explained in Section 4. 2. Container
    Management Platform Handler: This component offers communication with a managed
    container platform cluster (Swarm/Kubernetes/Mesos cluster). The Handler is responsible
    for ensuring that the container and virtual machine infrastructure is built, and
    for defining a set of scaling rules for that infrastructure. For the current set
    of experiments, JQueuer integrates with an external MiCADO platform, so the Handler
    accomplishes building the infrastructure by generating and submitting an ADT (see
    Section 3) through the MiCADO Submitter API. 3. Experiment Queue: A list of the
    experiment IDs which have been submitted. Each experiment has two important items
    in this queue: the Experiment Service Name and the Job Queue ID. JQueuer Agents
    use this list to recognise whether the containers running (on their virtual machines)
    should be controlled or not. 4. Job Parser: The Job Parser is responsible for
    extracting the job and task data from the experiment.json file and adding the
    jobs, and their tasks to a job queue dedicated to this experiment. 5. Job Queues:
    Each experiment depends on a dedicated Job Queue that has its own ID. The mechanism
    used to dispatch jobs from Job Queues is discussed in the next section. 6. Monitoring
    System: The Monitoring System contains the monitoring data related to all experiments.
    The monitoring data will be exposed as Prometheus metrics via a Prometheus exporter.
    5.2. JQueuer Agent design An instance of JQueuer Agent component should be running
    on each virtual machine node in the managed container platform cluster. The JQueuer
    Agent is responsible for controlling the service containers of the experiments,
    fetching jobs from the Job Queues, monitoring the execution and sending data to
    the JQueuer Manager. From functional point of view, this component can be divided
    into sub-components, also shown in Fig. 3, as follows: 1. Experiment Checker:
    This sub-component monitors the Experiment’s Queue in the JQueuer Manager. When
    a new experiment is added, the Experiment Checker will fetch the Experiment Service
    ID and the Job Queue ID items. 2. Job Queue Fetcher: It uses the Job Queue ID
    which has been obtained from the Experiment Checker so as to fetch the jobs from
    an experiment job queue and execute them on the containers of the corresponding
    Experiment Service. 3. Monitoring Updater: It monitors job execution on local
    containers and sends data and statistics to the Monitoring System in the JQueuer
    Manager. 5.3. JQueuer implementation In this section, the technologies and tools
    that have been used in the first implementation of JQueuer are described. They
    are visualised in italics in Fig. 3, next to their respective components discussed
    in Sections 5.1 JQueuer Manager design, 5.2 JQueuer Agent design above. The aim
    was to reuse existing open source products as components, and as a result minimise
    development time and effort. The choice of technologies was a result of thorough
    investigation and comparison. However, due to limitations in length, this selection
    process is not detailed in this paper. The two main components of the designed
    architecture, JQueuer Manager and JQueuer Agent have been developed using Python
    3 and were prepared as Docker images. These components include Celery [29], an
    asynchronous distributed task/job queuing system that was used together with Rabbitmq
    [30], a message broker for job queuing, and Redis [31], an in-memory database
    for capturing results. Redis was also applied for experiment queuing and simplifying
    data exchange between the manager and the agents. Statsd [32] was selected for
    monitoring and exporting statistics and events of the JQueuer Agents as Prometheus
    metrics. We used the official Docker images of each of these components together
    with Docker compose, a tool for defining and running multi-container Docker applications
    in order to group all containers and simplify the deployment and communication
    among them. As input to JQueuer Manager, experiments are described in JSON format,
    as it was presented in Fig. 2. JQueuer Agent has two main components: Container
    Updater and Container Manager. 1. Container Updater (Experiment Checker): The
    main function of this subcomponent is to monitor the containers on the local virtual
    machine node to distinguish which containers belong to a particular experiment.
    Each Docker container shows in its information the name of its Docker Swarm Service.
    The Container Updater will check the container services against the list of experiments
    on the Redis server. If the container is new and it belongs to one of the experiments,
    a new Container Manager will be forked to manage this container and it will be
    added to the list of containers that this agent is responsible for. 2. Container
    Manager (Job Queue Fetcher): This component is responsible for managing and controlling
    an experiment container. The life cycle of a Container Manager starts when fetching
    a job from the Job Queue that corresponds to its container. It then executes any
    pre-job script in the container and goes through the list of tasks. Tasks are
    executed sequentially, and after finishing them successfully, the Container Manager
    will run any post-job script. It sends statistics to the StatsD server such as:
    job starting/finishing time and task starting/finishing time. If the job fails
    for any reason, it informs StatsD of the time spent before the job has failed,
    and it signals this failure to the Celery server. After finishing the job, it
    fetches another job and starts executing it. Container Manager continues working
    until the Job Queue of its experiment becomes empty. Containers from different
    experiments can coexist on the same machine. JQueuer Agent will provide each Container
    Manager with a Container ID and a Job Queue ID. The Container List contains only
    those containers that belong to an experiment, and have been assigned to managers.
    6. JQueuer integration with MiCADO This section describes JQueuer’s integration
    with a managed container platform, in our case with MiCADO. As a standalone queue,
    JQueuer relies on an external service to provision the infrastructure of virtual
    machines and containers where the experiment jobs can be executed. This service
    should also allow for the definition of policies or rules that control the scaling
    of the infrastructure at both virtual machine and container levels. MiCADO was
    selected as the managed container platform for this integration because it supports
    Docker Swarm and Kubernetes orchestration, offers automated cloud orchestration
    through Occopus, and provides a flexible approach to defining scaling rules. Integrating
    JQueuer with MiCADO enables the realisation of deadline based execution policies,
    and the managed execution of large number of jobs in various container-based cloud
    computing environments. It should be noted that JQueuer does not depend on any
    cloud or container technology, and it is also agnostic to the cloud middleware
    and resources where the applications are executed. Therefore, it depends entirely
    on the managed container platform for which container technologies and which cloud
    resources can be used for executing the jobs queued by JQueuer. In the implemented
    solution, as MiCADO was applied as the managed container platform, containers
    are managed by Docker Swarm, and virtual machines can be instantiated on clouds
    supported by Occopus (i.e. CloudSigma, Amazon, OpenStack, OpenNebula or CloudBroker).
    Download : Download high-res image (318KB) Download : Download full-size image
    Fig. 4. High-level architecture of integrating JQueuer as an external component
    to MiCADO. The high-level architecture of the integrated JQueuer/MiCADO solution
    is illustrated in Fig. 4. As it can be seen in the figure, JQueuer is an external
    component to MiCADO that receives the Experiment.json file as input and (specifically
    for this integrated solution) generates the necessary ADT as input for MiCADO.
    This ADT describes both the necessary virtual machine and container infrastructures
    (including the dockerised version of the job executable, together with the JQueuer
    Agent), and also contains the auto-scaling rules. Based on the ADT, MiCADO deploys
    the worker nodes and containers with the JQueuer Agent deployed on them. This
    JQueuer Agent communicates with the JQueuer Master to receive the next job from
    the queue. Once the job is completed, JQueuer Agent asks for the next one until
    all jobs in the queue are finished. The number of MiCADO worker nodes and containers
    are managed by the MiCADO Master component based on the scaling rules and policies.
    The Policy Keeper of MiCADO, based on the scaling policy received from JQueuer,
    is responsible for deploying new workers or destroying existing ones (i.e. scaling
    up or down). Although this integration, especially the utilisation of the ADT
    and the MiCADO Policy Keeper is specific to MiCADO, it also has to be noted that
    similar managed container platforms can also be used in relation to JQueuer in
    order to achieve containerised execution and auto-scaling. The more detailed JQueuer/MiCADO
    interactions are visualised in Fig. 5. These interactions can be broadly divided
    into two separate tasks: (1) JQueuer submitting the experiment to MiCADO, and
    (2) MiCADO analysing JQueuer metrics to make scaling decisions. Download : Download
    high-res image (190KB) Download : Download full-size image Fig. 5. JQueuer integrating
    with MiCADO via the Container Management Platform (MiCADO) Handler. 6.1. JQueuer
    submitting the experiment to MiCADO The first task, where JQueuer submits an experiment
    to an external service – in this case MiCADO – begins with the JSON configuration
    file (experiment.json in the top left corner of Fig. 5). This file is responsible
    for defining two subsets of information which are described in detail in Section
    4: a. the experiment jobs and tasks b. the properties of the experiment to be
    launched, which include: container image, virtual machine image, scaling thresholds,
    and experiment deadlines. This second subset contains the information required
    by the managed container platform to build the infrastructure and define the scaling
    rules for the experiment. As shown in Fig. 5, in order to pass this data to MiCADO,
    it must first be converted into the TOSCA-compliant Application Description Template
    (ADT) format supported by the framework. To accomplish this, a generic ADT is
    written following the Jinja2 [33] templating language. The Jinja2 engine then
    generates an ADT specific to a given experiment by automatically filling key placeholders
    in the ADT with the corresponding information from JQueuer’s experiment.json configuration
    file. The generated and now complete ADT is then submitted to the MiCADO framework
    via the TOSCASubmitter API, which then manages the creation of container and virtual
    machine infrastructure and attaches the unique scaling rules. The actions of translation
    to the ADT format and submission to the MiCADO API are supported by the MiCADOHandler
    class on the JQueuer Master. The handler contains the generic template to be filled,
    instantiates the Jinja2 engine to generate the final ADT, and makes a POST request
    to the MiCADO API endpoint to submit the infrastructure and rules. The design
    supports agnosticism in JQueuer, as a handler can be written for any managed container
    service, as long as it can translate the experiment.json configuration into a
    compliant format, and send the generated data to the service in order to launch
    an infrastructure. 6.2. MiCADO analysing JQueuer metrics to make scaling decisions
    The second task sees the Policy Keeper component in MiCADO managing virtual machine
    and container scaling within the infrastructure in order to complete the experiment
    by a set deadline. It does this based on a pre-defined set of scaling rules included
    in the generic ADT used in the previous task. These scaling rules, which define
    expressions and queries based on JQueuer-specific metrics, are combined with the
    scaling thresholds, limits and deadlines defined by the user in experiment.json
    to build a complete scaling policy for the experiment. Generally, the Policy Keeper
    of MiCADO provides a simplistic approach to building policies based on Prometheus
    metrics. Given that Prometheus can be extended with community-developed exporters
    to extract metrics from nearly any piece of software, it is an ideal candidate
    for a pluggable policy enforcer such as Policy Keeper. Policy Keeper can natively
    attach to any Prometheus exporter, either internal or external to the cluster
    it runs on. Extremely generic policies can then be abstracted to suit a wide variety
    of needs and use-cases. Creating the policies for JQueuer involves little more
    than taking a generic deadline-based, job-completion policy from Policy Keeper,
    and extending it with specific queries and expressions related to JQueuer. Policy
    Keeper supports querying Prometheus to build expressions, alerts and constants
    that can then be used to express scaling logic using the Python scripting language.
    Conditional statements based on the queried metrics determine when the containers
    and nodes should scale up or down. The Python logic is written into the generic
    ADT, which, after Jinja2 templating, resolves as a complete and working scaling
    policy for the infrastructure of the current experiment. The JQueuer-specific
    metrics are stored in the statsd server on the JQueuer Manager and are exposed
    via a Prometheus exporter built into the statsd container. The endpoint for this
    statsd server is fed into the ADT during Jinja2 templating, and on submission
    to MiCADO, Policy Keeper instructs Prometheus to connect to it. Once connected,
    the JQueuer-specific metrics become available to Policy Keeper and the expressions
    and queries defined in the ADT are resolved so that metric-based scaling can take
    place. The generic scaling policy for a deadline-based job-completion is created
    thusly: First, a set of constants are defined for the user-supplied estimated
    average execution time, experiment deadline, maximum virtual machine workers,
    and maximum containers per worker. Then, queries are fetched via Prometheus for
    remaining time, jobs in queue, jobs completed, and calculated average execution
    time. Python is then used to express the following logic: The user-supplied estimated
    average execution time is used to calculate the initial number of containers required
    to complete the total number of jobs in queue before the user-supplied experiment
    deadline. After having completed 5% of the jobs in queue, the queried calculated
    average execution time is used to calculate the new number of containers required
    to complete the experiment in time. This average is updated as new jobs are completed.
    Up-scaling of containers and nodes can occur at any time, however, to prevent
    constant scaling, down-scaling will only occur when the change in containers is
    greater than three, or the change in worker nodes is greater than one. The user-defined
    maximums ensure that the infrastructure does not scale out of bounds. Although
    the above expressed logic implements a suitable deadline-based policy, a particular
    problem with down-scaling can happen if Docker Swarm, the container orchestrator
    component of MiCADO decides to kill a container that currently executes a job.
    In this case, significant execution time can be lost as the job will be killed
    and rescheduled to a different container. In order to prevent this, outside of
    the ADT, the following measures are taken to prevent a job-in-execution being
    killed during down-scaling: The container image itself is edited so that PID 1
    inside the container points to a shell script. This script points to the normal
    entry point of the container, but adds protection by using Linux trap [34] to
    catch any interrupt signal forwarded to the container. When Policy Keeper instructs
    Swarm to scale down the number of containers, it sends such a signal. On catching
    the interrupt signal, the shell script ensures that the job runs to completion
    before the container is killed. In order to avoid holding these resources infinitely,
    such as in a situation where a job hangs, container orchestrators offer a grace-period
    setting — the time to wait after having sent an interrupt signal (SIGINT), before
    sending a kill signal (SIGKILL). We override this value using the user-supplied
    estimated average execution time and, since a SIGKILL cannot be caught by trap,
    if the grace-period elapses, the container will be killed regardless of whether
    the job has completed, and consequently the killed job will be rescheduled. 7.
    Deadline-based execution of an agent-based simulation Simulation is a technique
    commonly used in science and industry to study a variety of problems across a
    wide range of domains by building and experimenting with models under difficult
    conditions [35]. Agent-based Simulation (ABS) is a widely used type of simulation
    [36]. It has roots in complex systems, complex adaptive systems and artificial
    life. ABS allows modellers to represent loosely structured systems in terms of
    actors (or agents) and their interactions with each other and their environment.
    For example, ABS has been used to study social networks, healthcare, supply chains,
    economic growth, climate change, power distribution systems and physical activity.
    An ABS typically consists of a set of autonomous agents (with attributes that
    describe the state of the agent), a set of agent relationships (how each agent
    interacts with other agents and its environment) and the environment (the “world”
    in which the agents exist). The use of a single computer restricts these to being
    executed in sequence. As with other forms of simulation (e.g. discrete-event simulation),
    empirical studies also often require many experiments to be run (e.g. the simulation
    of a model with different sets of parameters). Further, as some models can be
    stochastic, replications need to be run to build confidence intervals. This can
    lead to extremely lengthy or prohibitive experimentation time, especially if the
    run time of a single simulation is large. To investigate the performance of the
    deadline based execution approach presented earlier, we have used an agent-based
    simulation of an infection network [37]. The model illustrates how simulation
    can be used to study the changes in individual behaviour and the impact on the
    spread of a disease. It is written using REPAST, a widely used agent-based simulation
    tool [3]. The simulation consists of three types of agents that move in an environment
    and interact with each other, representing the susceptible, infected and recovered
    population. The overall architecture diagram of the experiment is illustrated
    in Fig. 6. As preparation for the experiment with MiCADO and JQueuer, the REPAST
    model was first compressed into a model.tar file and each set of parameters were
    saved in separate files. These input files were placed into an external file server
    (right hand side of Fig. 6) that is also used to store the generated outputs.
    Each job in this experiment consists of a single task, with each task is a simulation
    run that does not require any pre-job or post-job tasks. In the next step, the
    experiment’s global and experiment definition related parameters were prepared
    and placed into the experiment.json file (left hand side of Fig. 6). Some parameters,
    such as MiCADO endpoint, Docker image, specification of worker nodes (CPU and
    RAM), and the estimated duration of a simulation run (from previous experience)
    were fixed for all experiments. As REPAST is computationally demanding, it was
    only efficient to run a single container on a VM. Therefore, the number of containers
    per VM was also fixed and set to one. Some other parameters, including deadline
    and maximum number of VMs were variables that were changed in the different experiments
    to illustrate and test the auto-scaling capabilities of MiCADO. The input data
    to an experiment consists of the URL to the compressed model, the input parameters
    XML file, the URL to an external file (FTP) server where the input/output files
    are stored, the credentials to access the server, and the command for the execution
    of the application. Download : Download high-res image (904KB) Download : Download
    full-size image Fig. 6. Executing REPAST simulation jobs with MiCADO. To compare
    our deadline-based scheduling approach to a fixed resource approach, we used an
    implementation of REPAST running on the CloudBroker (CB) Platform with fixed resource
    allocation [24]. For both experiments (MiCADO and CB) resources of the CloudSigma
    cloud [23] were used (2.2 GHz CPU/2GB RAM instances). A test scenario of 200 REPAST
    infection model runs was executed for comparison. Generally, our approach was
    to run the test scenario using a fixed number of VMs via CloudBroker and then
    to use information from these runs to set the deadline to test MiCADO’s performance
    with different upper limits of VMs. In the first set of experiments (Experiments
    1–4 in Fig. 7) we used two variants of job submission via CloudBroker (manual
    allocation and round robin scheduling) to reflect two general forms of fixed resource
    systems and to give context to MiCADO’s performance (Experiments 1–2). The best
    of the average execution times was used to set the deadline for MiCADO. Two MiCADO
    experiments were then performed (Experiments 3–4) to study deadline behaviour
    against different upper limits of VMs. In the second set of experiments (experiments
    5–6) we compared the fixed approach (this time just with round robin scheduling)
    on 5 instances with MiCADO under more relaxed resource constraints. Finally, we
    put MiCADO under real pressure. In Experiment 7 we set the maximum number of virtual
    machines in MiCADO to 10 but restricted the available resources from the cloud
    service provider’s side in a way that MiCADO could not launch more than 6 worker
    nodes (the cloud account was restricted in such way). In Experiment 8 we significantly
    increased the number of overall jobs to execute from 200 to 1000 and set the deadline
    beyond 5 h. Download : Download high-res image (319KB) Download : Download full-size
    image Fig. 7. Description of experiments comparing fixed resource allocation with
    deadline-based scaling. When executing the experiments, five runs of both manual
    and round robin scheduling were performed with CloudBroker on 10 VMs first (Experiments
    1–2). The best average performance (with rounding) was set as the deadline for
    MiCADO which was the result of the manual allocation and a time of 31 min. In
    the next step, the same experiments were run on MiCADO (Experiments 3–4) with
    the above defined deadline, and setting the maximum number of instances to 10
    and 15, respectively. Fig. 8 illustrates the performance of MiCADO for each of
    the VM maximums (blue area) and compares these to the direct CB execution scenarios
    (red rectangular area). Please note that each experiment was repeated three times
    to check the relative consistency of the results. However, for illustration, only
    one of these runs is represented in the figures. The figures demonstrate how MiCADO
    adjusted the number of VMs depending on the progress being made against the deadline.
    Both figures show how the number of VMs processing jobs changes. At the beginning
    of the experiments there is some notable and unavoidable overhead required to
    set up the necessary infrastructure and start deploying VMs and containers. At
    the end of the experiments, once all jobs have finished, the virtual machines
    are shut down automatically and at the same time by MiCADO. The length of the
    experiment is measured until this automated shut-down. (Please note that in case
    of manual scheduling and execution, VMs needed to be shut down manually, putting
    significant burden on the operator to avoid unnecessary resource usage.) The adjustment
    of VMs in Experiment 3 simply means scaling up to the maximum number of VMs as
    MiCADO is stretched and using the maximum number of instances most of the time.
    In this scenario MiCADO was given the impossible task to match (or better) the
    performance of the optimised manual job distribution, using a priori knowledge
    about the jobs, that is not always available. In doing so, MiCADO scaled up the
    number of resources continuously until reaching the maximum, and then kept processing
    jobs using this maximum number of VMs. As it was expected, the deadline could
    not be reached as MiCADO needed some time to react and realise the need to scale
    up (causing some delay at the beginning of the experiment). On the other hand,
    MiCADO managed to almost match the best fixed resource performance and to process
    the same work in approximately the same time (with only 2–3 min delay). Additionally,
    although the deadline was missed, the utilisation of cloud resources is still
    slightly better than in the manual scenario, providing close to optimal resource
    utilisation. It was also noted, that MiCADO performed much better than the round
    robin scheduling that picked up and executed jobs individually on CloudBroker,
    and took almost twice as long as the optimised manual job distribution. Download
    : Download high-res image (208KB) Download : Download full-size image Fig. 8.
    Performance of automated scaling with maximum instances set to 10 and 15 and with
    deadline from batched job distribution (Experiments 3 and 4).(For interpretation
    of the references to colour in this figure legend, the reader is referred to the
    web version of this article.) In Experiment 4 the scaling up and down is better
    visible and illustrates that MiCADO finished the jobs by the set deadline comfortably
    (as it can scale up to 15 VMs maximum). It can also be observed that in Experiment
    4 MiCADO makes a pre-mature scaling down decision which it needs to adjust later
    on. Such, seemingly unnecessary decisions are justified by the fact that the actual
    job execution time is unpredictable and therefore shorter or longer jobs can turn
    up randomly any time. MiCADO’s decision is based on the latest monitoring information
    and therefore on data that happened in the past. This also means that in case
    of “extreme” job distribution (e.g. if a very long job is turning up at the end
    of an experiment), the current MiCADO implementation can miss the deadline as
    it was not expecting this “out of character” behaviour based on previous monitoring
    information. The machine learning-based optimiser component of MiCADO that is
    currently under development is expected to improve this situation. In all scenarios
    and runs, we have also calculated the average number of VMs that were used by
    MiCADO. This number varied between 7.87 to 9.44 in Experiment 3, and 8.48 and
    9.24 in Experiment 4 which demonstrates that MiCADO used slightly lower number
    of instances on average than the fix allocation. This is due to the fact that
    not all jobs have the same execution time and this execution time cannot be predicted
    beforehand. In the manual allocation we simply allocated the same number of jobs
    per VM. However, when leaving the scheduling and auto-scaling to MiCADO, it has
    the ability to better optimise variable length jobs and allow the execution of
    variable number of jobs by workers that can result in a better overall execution
    time. As conclusion, it can be observed that although MiCADO has been put under
    real pressure in this scenario as it was compared to an ideal, specifically generated
    and pre-batched job distribution (which in most real-life scenarios is not possible
    due to various length of simulation jobs), it was preforming reasonably well by
    using less resources than the manual allocation and completing the tasks by the
    deadline or with less than 10% extra time used. In the next set of experiments
    (Experiments 5 and 6) a more relaxed scenario was prepared for MiCADO to better
    illustrate its auto-scaling capability. Five runs of round robin scheduling were
    performed with CloudBroker on 5 VMs. The average execution time from these experiments
    was set as the approximate deadline for MiCADO (1 h 4 min 55 s). In the MiCADO
    experiments the maximum number of VMs were set to 10 and the experiment was repeated
    three times. Fig. 9 shows two of these runs as examples of the performance of
    MiCADO in this context. Download : Download high-res image (210KB) Download :
    Download full-size image Fig. 9. Performance of automated scaling with maximum
    instances set to 10 with relaxed deadline (Experiment 6). In both examples, MiCADO
    ran faster than the best fixed resource runtime and finished the jobs by the deadline.
    In all runs MiCADO consumed a maximum of 6 VMs and, as shown in the figure, this
    again was “peak” consumption that only existed for a short time. Overall an average
    of 3.86 VMs were used, less than the 5 VMs of the fixed resource implementation
    (this again was possible due to the unpredictable and variable execution times
    of the individual jobs). Overall, MiCADO used less VMs to “beat” the deadline
    by a small margin. Additionally the figure also demonstrates that MiCADO scaled
    up differently in different runs based on the actual performance of the VMs that
    differ slightly in time when using CloudSigma. This unpredictable behaviour coupled
    with variable job execution time leads to some scaling up/down adjustments performed
    by MiCADO, as it is visible on both graphs. Download : Download high-res image
    (214KB) Download : Download full-size image Fig. 10. Automated scaling in resource
    constrained scenario (Experiment 7) and with 1000 jobs (Experiment 8). Experiment
    7 was designed to test how MiCADO behaves in an impossible scenario when the resources
    are restricted by the cloud service provider and not available in the expected
    volume. In this experiment the maximum number of resources to be utilised by MiCADO
    were set to 10. However, the user account on the CloudSigma cloud was restricted
    in a way that MiCADO could not launch more than 6 worker nodes at a time. The
    results of this experiment are shown in the left hand pane of Fig. 10. As it is
    evident, MiCADO scales-up to the maximum of 6 workers. However, as it was expected,
    this amount of resources is not enough to complete the tasks by the set deadline
    which is missed by a large margin due to these physical constraints. Please note
    that MiCADO does not provide error message or warning to the user in such scenarios
    (a feature which can be implemented in the future). However, the scaling of the
    virtual machines can be observed on the MiCADO dashboard where the user can see
    the restricted scaling behaviour. In Experiment 8 the aim was to execute a larger
    use-case scenario. Therefore, we increased the number of jobs to 1000 and we primarily
    tested the scalability and robustness of our solution. As it is evidenced in the
    right hand pane of Fig. 10, MiCADO coped well with the pressure and completed
    all 1000 jobs by the deadline. It is important to note that in this case we have
    not actually performed the fixed resource experiment, only used the outcomes of
    Experiment 5 to estimate and set a reasonable deadline. As in the previous cases,
    multiple runs were performed, with the graph showing some of these as examples
    only. The above experiments demonstrate the performance of MiCADO for a relatively
    small set of experiments. Performance was comparable with a fixed resource implementation,
    even under challenging deadlines. Significantly, in this limited demonstration,
    the experiments show that less VM time was used in the MiCADO implementation than
    in the fixed resource ones. Arguably this shows that, if an appropriate deadline
    is set, simulation users can merely set the upper limit of VM expenditure and
    allow MiCADO to make efficient use of available cloud resources. However, further
    experimentation with a range of simulation models will be required to generalise
    this observation. 8. Conclusion and future work This paper presented a set of
    technologies that enable the efficient deadline-based execution of large number
    of jobs in containerised cloud environments. As such experiments are typically
    computationally demanding and require access to distributed computing infrastructures,
    these building blocks can be efficiently used when implementing science gateways.
    The presented components include JQueuer, a cloud-agnostic distributed system
    designed to support the scheduling of large number of jobs in containers and virtual
    machines. It was also demonstrated how JQueuer can be integrated with a managed
    container platform, such as MiCADO, to realise deadline-based scaling policies.
    Both JQueuer and MiCADO are open source and available at https://github.com/micado-scale.
    Finally, an agent-based simulation application implemented in REPAST was used
    to test the behaviour of the combined MiCADO JQueuer solution in various scenarios.
    As REPAST is used widely, especially by the academic and research communities,
    a public gateway, based on the CloudSME AppCenter and its related technologies
    [38] is currently being set up where such simulations can be executed on cloud
    computing resources on a pay-as-you-go basis. Similarly to the REPAST solution,
    an open source discrete-event simulation software package, called JaamSim [39]
    is also being prototyped with the deadline-based scaling solution and will be
    offered as public service via the gateway. The results of this work impact both
    forms of simulation. In parallel, experimentation for developing more efficient
    scaling policies and implementing the optimiser component of MiCADO using machine
    learning techniques is currently ongoing work in the COLA project. This work also
    incorporates the further investigation of cost models and how cost optimisation
    can also be considered besides deadline-based execution. Additionally, MiCADO
    is being extended with several new features and capabilities, for example to support
    multiple applications/experiments and multiple users under the same MiCADO installation.
    Acknowledgement This work was funded by the European Commission through COLA Cloud
    Orchestration at the level of Applications Project No. 731574. Conflict of interest
    None. Declaration of competing interest The authors declared that they had no
    conflicts of interest with respect to their authorship or the publication of this
    article. References [1] S. Wu, C. Niu, J. Rao, H. Jin, X. Dai, Container-based
    cloud platform for mobile computation offloading, in: 2017 IEEE International
    Parallel and Distributed Processing Symposium, IPDPS, May 2017, pp. 123–132. Google
    Scholar [2] Kiss T., et al. MiCADO - Microservices-based Cloud Application-level
    Dynamic Orchestrator Future Gener. Comput. Syst. (2017), 10.1016/j.future.2017.09.050
    Google Scholar [3] North M.J., et al. Complex adaptive systems modeling with repast
    simphony Complex Adapt. Syst. Model., 1 (1) (2013), p. 3, 10.1186/2194-3206-1-3
    View in ScopusGoogle Scholar [4] COLA – Cloud Orchestration at the Level of Application,
    https://project-cola.eu/. (Accessed Online: 2019-01-25). Google Scholar [5] .
    Saker Solutions, Company Website, [Online]. https://www.sakersolutions.com/. (Accessed:
    5 May 2019). Google Scholar [6] CloudiFacturing – Cloudification of Production
    Engineering for Predictive Digital Manufacturing [Online]. https://www.cloudifacturing.eu/.
    (Accessed: 5 May 2019). Google Scholar [7] DSS Consulting Company Website, [Online].
    http://www.dssconsulting.com/. (Accessed: 5 May 2019). Google Scholar [8] Docker
    swarm, https://docs.docker.com/engine/swarm/. (Accessed Online: 2018-03-16). Google
    Scholar [9] Burns B., Grant B., Oppenheimer D., Brewer E., Wilkes J. Borg, omega,
    and kubernetes Queue, 14 (1) (2016), p. 10, 10.1145/2898442.2898444 24 pages Google
    Scholar [10] . Apache mesos, http://mesos.apache.org/documentation/latest/frameworks/.
    (Accessed Online: 2018-03-16). Google Scholar [11] . Apache Mesos framework, https://github.com/dcos/metronome.
    (Accessed Online: 2019-01-27). Google Scholar [12] D. Abdurachmanov, Optimizing
    CMS build infrastructure via Apache Mesos. in: Proceedings, 21st International
    Conference on Computing in High Energy and Nuclear Physics, CHEP 2015, April 13–17,
    2015, Okinawa, Japan. Google Scholar [13] Thain D., Tannenbaum T., Livny M. Distributed
    computing in practice: the condor experience Concurr. Comput.: Pract. Exper.,
    17 (24) (2004), pp. 323-356 Google Scholar [14] . Slurm Workload Manager, Version
    18.08, https://slurm.schedmd.com/. (Accessed Online: 2019-01-31). Google Scholar
    [15] Lovas R., et al. Orchestrated platform for cyber-physical systems Complexity,
    2018 (2018), pp. 1-16, 10.1155/2018/8281079 Google Scholar [16] What is AWS Batch?
    https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html. (Accessed
    Online: 2019-01-28). Google Scholar [17] . Batch, https://azure.microsoft.com/en-gb/services/batch/.
    (Accessed Online: 2019-01-28). Google Scholar [18] Malawski M., Juve G., Deelman
    E., Nabrzyski J. Algorithms for cost-and deadline-constrained provisioning for
    scientific workflow ensembles in IaaS clouds Future Gener. Comput. Syst., 48 (2015),
    pp. 1-18 View PDFView articleView in ScopusGoogle Scholar [19] Le G., Xu K., Song
    J. Dynamic resource provisioning and scheduling with deadline constraint in elastic
    cloud 2013 International Conference on Service Sciences, ICSS, IEEE (2013), pp.
    113-117 View in ScopusGoogle Scholar [20] Mao M., Li J., Humphrey M. Cloud auto-scaling
    with deadline and budget constraints 2010 11th IEEE/ACM International Conference
    on Grid Computing, IEEE (2010), pp. 41-48 CrossRefView in ScopusGoogle Scholar
    [21] Shi J., Luo J., Dong F., Zhang J. A budget and deadline aware scientific
    workflow resource provisioning and scheduling mechanism for cloud Proceedings
    of the 2014 IEEE 18th International Conference on Computer Supported Cooperative
    Work in Design, CSCWD, IEEE (2014), pp. 672-677 View in ScopusGoogle Scholar [22]
    Kovacs J., Kacsuk P. Occopus: a multi-cloud orchestrator to deploy and manage
    complex scientific infrastructures J. Grid Comput., 16 (1) (2018), pp. 19-37 CrossRefView
    in ScopusGoogle Scholar [23] . Cloudsigma Holding AG, Cloud servers & Hosting.
    [Online]. https://www.cloudsigma.com/. (Accessed: 7 Feb 2019). Google Scholar
    [24] Taylor S.J.E., et al. Enabling cloud-based computational fluid dynamics with
    a Platform as a Service solution IEEE Trans. Ind. Inf., 15 (1) (2019), pp. 85-94,
    10.1109/TII.2018.2849558 Google Scholar [25] . Prometheus, https://prometheus.io/.
    (Accessed Online: 2018-03-16). Google Scholar [26] OASIS Topology and Orchestration
    Specification for Cloud Applications Version 1.0 [Online]. http://docs.oasis-open.org/tosca/TOSCA/v1.0/TOSCA-v1.0.html.
    (Accessed: 29-Mar-2018). Google Scholar [27] G. Pierantoni, T. Kiss, G. Gesmier,
    J. DesLauriers, G. Terstyanszky, J.M.M. Rapún, Flexible deployment of social media
    analysis tools, in: International Workshop on Science Gateways, 13–15 June 2018,
    Edinburgh, UK. Google Scholar [28] Introducing JSON, https://www.json.org/. (Accessed
    Online: 2019-01-17). Google Scholar [29] . Celery, http://www.celeryproject.org/.
    (Accessed Online: 2018-03-16). Google Scholar [30] . Rabbitmq, https://www.rabbitmq.com/.
    (Accessed Online: 2018-03-16). Google Scholar [31] . Redis, https://redis.io/.
    (Accessed Online: 2018-03-16). Google Scholar [32] . statsd, https://github.com/etsy/statsd.
    (Accessed Online: 2019-01-17). Google Scholar [33] . Jinja2, http://jinja.pocoo.org/docs/2.10/.
    (Accessed Online: 2019-01-15). Google Scholar [34] . trap man page, http://man7.org/linux/man-pages/man1/trap.1p.html.
    (Accessed Online: 2019-01-18). Google Scholar [35] Law A.M. Simulation Modeling
    and Analysis (fifth ed.), McGraw-Hill (2015) Google Scholar [36] C.M. Macal, N.J.
    North, Tutorial on agent-based modelling and simulation, J. Simul. 4 (3) 151–162.
    Google Scholar [37] Taylor S.J.E., et al. Open science: Approaches and benefits
    for modeling & simulation Chan W.K.V., et al. (Eds.), Proceedings of the 2017
    Winter Simulation Conference, IEEE., Piscataway, New Jersey (2017), pp. 535-549
    CrossRefView in ScopusGoogle Scholar [38] Taylor S.J.E., et al. The CloudSME simulation
    platform and its applications: A generic multi-cloud platform for developing and
    executing commercial cloud-based simulations Future Gener. Comput. Syst., 88 (2018),
    pp. 524-539, 10.1016/j.future.2018.06.006 View PDFView articleView in ScopusGoogle
    Scholar [39] D.H. King, H.S. Harrison, Open Source simulation software “JaamSim”,
    in: R. Pasupathy, S.-H. Kim, A. Tolk, R. Hill, and M. E. Kuhl (Eds.), Proceedings
    of the 2013 Winter Simulation Conference. Google Scholar Cited by (0) Prof. Dr.
    Tamas Kiss is a Professor in Distributed Computing at the Department of Computer
    Science and the Director of the University Research Centre for Parallel Computing
    at the University of Westminster. He holds Master’s Degrees in Electrical Engineering,
    and Computer Science and Mathematics, and Ph.D. in Distributed Computing. His
    research interests include distributed and parallel computing, cloud, cluster
    and grid computing. He has attracted over £20 Million research funding and leads
    national and European research projects related to enterprise applications of
    cloud computing technologies. He co-authored more than 120 papers in journals,
    conference proceedings and as chapters of edited books James DesLauriers is a
    Research Associate at the University of Westminster working on the COLA Cloud
    Orchestration at the Level of Application project and leading the development
    activities of the MiCADO orchestration framework at the University of Westminster.
    He holds an M.Sc. in Cyber Security & Forensics, where his dissertation focused
    on the forensic analysis of the Docker container runtime. His current research
    interests include the descriptive cloud language TOSCA, generic container solutions
    for IaaS clouds, and cloud & container security Grégoire Gesmier is a Research
    Associate at the University of Westminster. During his Bachelor Degree in computing
    science, which took place at the University of Besançon in France he did a placement
    as part of his grade at the University of Westminster. After graduating, in 2014,
    he joined the ranks of the CPC (Centre for Parallel Computing) of the University
    where he worked on different project funded under the FP7 body. He worked on developing
    and supporting application on different technology, such as Cloud computing, Desktop
    Grid and Cluster. Prof. Dr. Gabor Terstyanszky is a Professor in Distributed Computing
    at the University of Westminster. His research interests include distributed and
    parallel computing focusing on targeting research issues of Big Data and Cloud
    Computing. He has been involved in more than 15 research projects as either Principal
    or Co-Investigator. He also had several research grants at various universities
    in Germany, Spain, and United Kingdom. He published over 140 papers at conferences
    and journals. He was member of programming committees of several conferences and
    workshops. He supervised more than 10 Ph.D. students. He has taught several B.Sc.
    and M.Sc. modules on distributed computing such as Distributed Computing, Service
    Oriented Architecture, Web Services. Currently, he is teaching the Client Server
    Architecture and Advanced Big Data Analytics module. Dr. Gabriele Pierantoni is
    a Lecturer in Distributed Computing and Deputy Director of CPC at the University
    of Westminster. He holds a Ph.D. in Distributed Computing. His research interests
    include distributed and parallel computing, cloud, cluster and grid computing.
    He has been involved in several FP6–FP7 research projects, Currently he is participating
    in two H2020 projects, COLA — Cloud Orchestration at the Level Application, and
    ASCLEPIOS — Advanced Secure Cloud Encrypted Platform for Internationally Orchestrated
    Solutions in Healthcare, He co-authored two book and more than 20 scientific papers
    in journals, conference proceedings and as book chapters. Dr. Osama Abu Oun is
    a Research Associate at the Cyber Security Group, School of Computing, University
    of Kent, Canterbury, United Kingdom. His research interests include technical
    aspects of cyber Security, virtualisation, fog/cloud computing and internet of
    things. Prof. Dr. Simon J.E. Taylor is a Professor of Computing and the director
    of the Modelling & Simulation Group in the Department of Computer Science, Brunel
    University London (https://tinyurl.com/ya5zjh8z). He has a successful track record
    of research in Modelling & Simulation, particularly with high performance distributed
    simulation in industry through digital infrastructures. He has developed these
    solutions with over 20 companies resulting in over 5M GBP in cost savings and
    increased production. He has successfully applied these experiences in the development
    of digital infrastructures in Africa and has led innovation in over 10 African
    countries. He has also led the development of international standards in these
    areas. He regularly audits and chairs commercial project reviews in distributed
    computing and Modelling & Simulation. He a member of the ACM SIGSIM Steering Committee
    and founder of the Journal of Simulation. He has chaired several major conferences
    and his published over 150 articles. Dr. Anastasia Anagnostou is a Lecturer in
    Computer Science at the College of Engineering, Design and Physical Sciences,
    Department of Computer Science, Brunel University London, UK. She holds a Ph.D.
    in Hybrid Distributed Simulation, an M.Sc. in Telemedicine and e-Health Systems
    and a B.Sc. in Electronics Engineering. Her research interests are related to
    the application of modelling and simulation techniques in the Healthcare and Industry.
    Dr. Jozsef Kovacs is a Senior Research Fellow at the Laboratory of Parallel and
    Distributed Systems (LPDS) at the Institute for Computer Science and Control (SZTAKI)
    of the Hungarian Academy of Sciences (MTA). He got his B.Sc. (1997), M.Sc. (2001)
    and Ph.D. (2008) in the field of parallel computing. His research topic was parallel
    debugging and checkpointing, clusters, grids and desktop grid systems, web portals.
    Recently, he is focusing on cloud and container computing especially on infrastructure
    orchestration and management. He gave numerous scientific presentations and lectures
    at conferences, universities and research institutes in many places in Europe
    and outside. He is reviewer at several scientific journals and holds various positions
    on conferences. He is author and co-author of more than 60 scientific publications
    including conference papers, book chapters and journals. © 2019 The Authors. Published
    by Elsevier B.V. Part of special issue Science Gateway Workshops Special Issue
    2018 Edited by Malcolm Atkinson, Josue Balandrano Coronel, Nancy Wilkins-Diehr
    View special issue Recommended articles On the impact of process replication on
    executions of large-scale parallel applications with coordinated checkpointing
    Future Generation Computer Systems, Volume 51, 2015, pp. 7-19 Henri Casanova,
    …, Dounia Zaidouni View PDF Energy-efficient joint BS and RS sleep scheduling
    in relay-assisted cellular networks Computer Networks, Volume 100, 2016, pp. 45-54
    Hongbin Chen, …, Feng Zhao View PDF Availability and Comparison of Four Retrial
    Systems with Imperfect Coverage and General Repair Times Reliability Engineering
    & System Safety, Volume 212, 2021, Article 107642 Chia-Huang Wu, …, Kuo-Hsiung
    Wang View PDF Show 3 more articles Article Metrics Citations Citation Indexes:
    15 Captures Readers: 61 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply.'
  inline_citation: '>'
  journal: Future generation computer systems
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: A cloud-agnostic queuing system to support the implementation of deadline-based
    application execution policies
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3154842.3154845
  analysis: '>'
  authors:
  - Seetharami Seelam
  - Yubo Li
  citation_count: 3
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesMIDDLEWAREProceedingsDIDL
    ''17Orchestrating deep learning workloads on distributed infrastructure SHORT-PAPER
    SHARE ON Orchestrating deep learning workloads on distributed infrastructure Authors:
    Seetharami R. Seelam , Yubo Li Authors Info & Claims DIDL ''17: Proceedings of
    the 1st Workshop on Distributed Infrastructures for Deep LearningDecember 2017Pages
    9–10https://doi.org/10.1145/3154842.3154845 Published:11 December 2017Publication
    History 3 citation 515 Downloads eReaderPDF DIDL ''17: Proceedings of the 1st
    Workshop on Distributed Infrastructures for Deep Learning Orchestrating deep learning
    workloads on distributed infrastructure Pages 9–10 Previous Next ABSTRACT Cited
    By Recommendations Comments ABSTRACT Containers simplify the packaging, deployment
    and orchestration of diverse workloads on distributed infrastructure. Containers
    are primarily used for web applications, databases, application servers, etc.
    on infrastructure that consists of CPUs, Memory, Network and Storage. Accelerator
    hardware such GPUs are needed for emerging class of deep learning workloads with
    unique set of requirements that are not addressed by current container orchestration
    systems like Mesos, Kuberentes, and Docker swarm. In this extended abstract, we
    discuss the requirements to support GPUs in container management systems and describe
    our solutions in Kubernetes. We will conclude with a set of open issues that are
    yet to be addressed to fully support deep learning workloads on distributed infrastructure.
    Operating system (OS) allows flexible sharing and load balancing of resources
    like CPU, Memory and Network among multiple processes and containers. Unlike these
    GPU''s are unique quantities (GPU 0, GPU 1, ...) and they must be allocated accordingly
    (e.g., allocate GPU 0 to Container 1). GPU topology, will heavily affect the bandwidth
    of GPU to GPU communication and must take into consideration. Moreover, GPU topology
    even affects GPU capabilities. In some systems, for example, GPUs on different
    CPU socket cannot have Peer to Peer communication capability. To address these
    issues, firstly, we have enabled GPU support on Kubernetes. We implemented a GPU
    allocator module to record GPU number-to-device mapping. Kubernetes users only
    request number of GPUs need for their workload; GPU allocator module maps the
    number to actual GPU devices according to required scheduling policy and expose
    the allocated GPUs to application inside the container. Secondly, we have developed
    two advanced GPU schedulers, a bin-packing scheduler and a topology-aware scheduler,
    to improve GPU utilization and GPU performance. Bin-packing scheduler tries to
    bundle GPU jobs to fewer servers, so that other idle servers can be reserved for
    potentially large jobs. Topology-aware scheduler can automatically collect GPU
    topology information of each worker node, and assign nodes that deliver the highest
    possible bandwidth to the application. Access to CPU, Memory, Network and Storage
    devices are abstracted by operating system (OS) application programming interface
    (API) calls. The OS translates the application calls into device specific calls
    internally. Unlike these resources, GPUs have device access calls that are not
    yet abstracted under OS API''s so applications that require access to GPU devices
    need those GPU devices mounted inside the container, they need access to auxiliary
    device interfaces (like nvidia-uvm), and they need the GPU drivers inside that
    container. The device driver inside the container must exactly match the driver
    on the host for proper operation. To solve these issues, we enhanced Kubernetes
    to gather the device drivers on kubelet startup and mount these drivers into the
    container automatically. This ensures portability of the workloads across systems
    with potentially different drivers. A similar approach is taken by Mesos, Nvidia
    Docker and other systems. In addition, unlike CPU and memory, GPU is an external
    PCIe device and in our experience it experiences software and hardware failures
    far more frequently than the rest of the system. Failures could include bad connection
    to the PCIe slot, GPU kernel crash, bad power supply, and so on. To deal with
    such issues, we enabled GPU liveness check on Kubernetes (like the liveness check
    in any cloud service). The kubelet periodically checks the healthiness of GPU
    devices. Once the GPU failure is detected, the GPU will automatically be removed
    out from the resources pool. Finally to support multiple users, we added GPU quota
    support in Kubernetes so that GPU resources can be limited by different namespaces;
    We auto-labeled GPU devices model to Kubernetes worker nodes so that the job can
    use such information to filter GPUs. All these new features are originated from
    our real requirements and aim to enhance the usability of GPUs in a cloud context.
    We consider GPU related scheduling policy and algorithm to improve both GPU performance
    and utility as open issues. We plan to extend CPU topology and affinity support
    to Kubernetes, so that we can make CPU-GPU joint topology optimized scheduling.
    The exploration of CPU-GPU bandwidth will bring more possibilities for performance
    improvement especially for servers with NVLink technology (like IBM Minsky) between
    CPU-GPU. Cited By View all Li J, Zhang R, Cen M, Wang X and Jiang M. (2021). Depression
    Detection Using Asynchronous Federated Optimization 2021 IEEE 20th International
    Conference on Trust, Security and Privacy in Computing and Communications (TrustCom).
    10.1109/TrustCom53373.2021.00110. 978-1-6654-1658-0. (758-765). https://ieeexplore.ieee.org/document/9724416/
    Thinakaran P, Gunasekaran J, Sharma B, Kandemir M and Das C. (2019). Kube-Knots:
    Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters
    2019 IEEE International Conference on Cluster Computing (CLUSTER). 10.1109/CLUSTER.2019.8891040.
    978-1-7281-4734-5. (1-13). https://ieeexplore.ieee.org/document/8891040/ Song
    S, Deng L, Gong J and Luo H. (2018). Gaia Scheduler: A Kubernetes-Based Scheduler
    Framework 2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications,
    Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing
    & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom).
    10.1109/BDCloud.2018.00048. 978-1-7281-1141-4. (252-259). https://ieeexplore.ieee.org/document/8672301/
    Recommendations KubeShare: A Framework to Manage GPUs as First-Class and Shared
    Resources in Container Cloud HPDC ''20: Proceedings of the 29th International
    Symposium on High-Performance Parallel and Distributed Computing Container has
    emerged as a new technology in clouds to replace virtual machines~(VM) for distributed
    applications deployment and operation. With the increasing number of new cloud-focused
    applications, such as deep learning and high performance ... Read More CLBlast:
    A Tuned OpenCL BLAS Library IWOCL ''18: Proceedings of the International Workshop
    on OpenCL This work introduces CLBlast, an open-source BLAS library providing
    optimized OpenCL routines to accelerate dense linear algebra for a wide variety
    of devices. It is targeted at machine learning and HPC applications and thus provides
    a fast matrix-... Read More An efficient and non-intrusive GPU scheduling framework
    for deep learning training systems SC ''20: Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis Efficient GPU
    scheduling is the key to minimizing the execution time of the Deep Learning (DL)
    training workloads. DL training system schedulers typically allocate a fixed number
    of GPUs to each job, which inhibits high resource utilization and often ... Read
    More Comments 0 References View Table Of Contents Footer Categories Journals Magazines
    Books Proceedings SIGs Conferences Collections People About About ACM Digital
    Library ACM Digital Library Board Subscription Information Author Guidelines Using
    ACM Digital Library All Holdings within the ACM Digital Library ACM Computing
    Classification System Digital Library Accessibility Join Join ACM Join SIGs Subscribe
    to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin
    Feedback Bug Report The ACM Digital Library is published by the Association for
    Computing Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy
    Code of Ethics'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Orchestrating deep learning workloads on distributed infrastructure
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1504/ijwgs.2020.110944
  analysis: '>'
  authors:
  - Khouloud Boukadi
  - Molka Rekik
  - Jorge Bernal Bernabé
  - Jaime Lloret
  citation_count: 2
  full_citation: '>'
  full_text: '>

    Login Help Sitemap Home For Authors For Librarians Orders Inderscience Online
    News Home Full-text access for editors Container description ontology for CaaS
    by Khouloud Boukadi; Molka Rekik; Jorge Bernal Bernabe; Jaime Lloret International
    Journal of Web and Grid Services (IJWGS), Vol. 16, No. 4, 2020  Abstract: Besides
    its classical three service models (IaaS, PaaS, and SaaS), container as a service
    (CaaS) has gained significant acceptance. It offers without the difficulty of
    high-performance challenges of traditional hypervisors deployable applications.
    As the adoption of containers is increasingly wide spreading, the use of tools
    to manage them across the infrastructure becomes a vital necessity. In this paper,
    we propose a conceptualisation of a domain ontology for the container description
    called CDO. CDO presents, in a detailed and equal manner, the functional and non-functional
    capabilities of containers, Dockers and container orchestration systems. In addition,
    we provide a framework that aims at simplifying the container management not only
    for the users but also for the cloud providers. In fact, this framework serves
    to populate CDO, help the users to deploy their application on a container orchestration
    system, and enhance interoperability between the cloud providers by providing
    migration service for deploying applications among different host platforms. Finally,
    the CDO effectiveness is demonstrated relying on a real case study on the deployment
    of a micro-service application over a containerised environment under a set of
    functional and non-functional requirements. Online publication date: Mon, 02-Nov-2020
    The full text of this article is only available to individual subscribers or to
    users at subscribing institutions.   Existing subscribers: Go to Inderscience
    Online Journals to access the Full Text of this article. Pay per view: If you
    are not a subscriber and you just want to read the full contents of this article,
    buy online access here. Complimentary Subscribers, Editors or Members of the Editorial
    Board of the International Journal of Web and Grid Services (IJWGS): Login with
    your Inderscience username and password:     Username:        Password:          Forgotten
    your password?  Want to subscribe? A subscription gives you complete access to
    all articles in the current issue, as well as to all articles in the previous
    three years (where applicable). See our Orders page to subscribe. If you still
    need assistance, please email subs@inderscience.com    Keep up-to-date Our Blog
    Follow us on Twitter Visit us on Facebook Our Newsletter (subscribe for free)
    RSS Feeds New issue alerts Return to top Contact us About Inderscience OAI Repository
    Privacy and Cookies Statement Terms and Conditions Help Sitemap © 2024 Inderscience
    Enterprises Ltd.'
  inline_citation: '>'
  journal: International journal of web and grid services
  limitations: '>'
  pdf_link: null
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: Container description ontology for CaaS
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1002/cpe.6436
  analysis: '>'
  authors:
  - Yasser Aldwyan
  - Richard Sinnott
  - Glenn Jayaputera
  citation_count: 2
  full_citation: '>'
  full_text: '>

    UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Concurrency and Computation: Practice and Experience RESEARCH ARTICLE
    Full Access Elastic deployment of container clusters across geographically distributed
    cloud data centers for web applications Yasser Aldwyan,  Richard O. Sinnott,  Glenn
    T. Jayaputera First published: 09 June 2021 https://doi.org/10.1002/cpe.6436 SECTIONS
    PDF TOOLS SHARE Abstract Containers such as Docker provide a lightweight virtualization
    technology. They have gained popularity in developing, deploying and managing
    applications in and across Cloud platforms. Container management and orchestration
    platforms such as Kubernetes run application containers in virtual clusters that
    abstract the overheads in managing the underlying infrastructures to simplify
    the deployment of container solutions. These platforms are well suited for modern
    web applications that can give rise to geographic fluctuations in use based on
    the location of users. Such fluctuations often require dynamic global deployment
    solutions. A key issue is to decide how to adapt the number and placement of clusters
    to maintain performance, whilst incurring minimum operating and adaptation costs.
    Manual decisions are naive and can give rise to: over-provisioning and hence cost
    issues; improper placement and performance issues, and/or unnecessary relocations
    resulting in adaptation issues. Elastic deployment solutions are essential to
    support automated and intelligent adaptation of container clusters in geographically
    distributed Clouds. In this article, we propose an approach that continuously
    makes elastic deployment plans aimed at optimizing cost and performance, even
    during adaptation processes, to meet service level objectives (SLOs) at lower
    costs. Meta-heuristics are used for cluster placement and adjustment. We conduct
    experiments on the Australia-wide National eResearch Collaboration Tools and Resources
    Research Cloud using Docker and Kubernetes. Results show that with only a 0.5
    ms sacrifice in SLO for the 95th percentile of response times we are able to achieve
    up to 44.44% improvement (reduction) in cost compared to a naive over-provisioning
    deployment approach. 1 INTRODUCTION Cloud-based web applications often need to
    be intelligently deployed to specific geographical locations to improve end user
    experiences, for example, regarding performance. Containers, a lightweight virtualization
    technology, have gained popularity for deploying applications efficiently in such
    distributed environments.1-3 They provide application packaging that allows consistent,
    portable deployment across multiple Clouds4 as well as abstracting away many of
    the overheads when deploying and managing containers and infrastructures.4, 5
    These infrastructures used for container deployment models in Clouds are usually
    clusters of virtual machines (VMs), where each cluster of VMs has a container
    cluster management system, for example, Kuberenetes,6 that is used to deploy,
    manage and scale containers across Cloud resources. Container management platforms,
    such as Google Anthos7 and Rancher,8 take all management responsibilities to automate
    the deployment and simplify the management of such clusters and containers across
    Clouds.1 Most of these platforms support multi-cluster deployment models, giving
    application providers capabilities to deploy/remove clusters in Clouds to fit
    their needs, for example, isolation, location or application scaling.3, 7, 9 The
    multi-cluster deployment models for distributed Clouds are well suited for modern
    web applications that can exhibit global fluctuations over time based on the user
    base demand. This requires intelligent dynamic global deployments of container
    clusters,10 including application containers, to data centers, for example, they
    should be deployed in proximity to end users to maintain performance as shown
    in Figure 1. FIGURE 1 Open in figure viewer PowerPoint Container-based multi-cluster
    deployment of web applications across geographically distributed clouds However,
    due to the absence of automated elastic deployment across multiple distributed
    Clouds, adapting such deployments to handle spatial workload variations is a daunting
    task. A key issue is to decide when and how to adapt the deployments of clusters,
    in terms of their quantity and location to maintain performance and minimize operating
    and adaptation costs. In particular, we consider the geographical (spatial) distribution
    of workloads, that is, the locations of an application can depend on the where
    user requests are concentrated and this should be used to determine the optimal
    data center(s) for hosting the application. Spatial fluctuations in user workloads
    can occur due to the variations in application popularity across countries and/or
    cities over time. For instance, tracing the web traffic of 6.5 million user check-ins
    to the social media platform Gowalla over a two-year period (2009 and 2010), showed
    obvious variations in monthly rate in user growth across many (global) locations.11
    The application became popular in some areas due to local events, for example,
    festivals. However the resources that deliver this platform may be deployed in
    distant Cloud data centers. Geographical distance typically increases network
    latency,11-13 which ultimately has negative impacts on businesses with poor user
    experience resulting in lost revenues14 and on given service level objectives
    (SLOs) that have been set. Such increased latencies incur delays in the response
    times of user requests that are often unacceptable for zero-tolerance-to-delay
    web applications, for example, e-commerce web sites. According to Forrester,15
    40% of customers leave online e-commerce sites if loading a page takes more than
    3 s. Additionally, applications running on clusters in given areas may become
    less popular over time resulting in monetary costs that are incurred with no benefits
    to the users. Although most container management platforms in distributed Clouds
    are relatively mature with many advanced features, for example, automation and
    governance,2 they do not currently offer elastic deployment techniques to handle
    spatial workload variations. Instead, application providers typically decide (manually)
    to add/remove clusters across Clouds. Such manual adaptation decisions are naive
    and inefficient as they can lead to costly, over-provisioning issues (excessive
    deployment of clusters), performance issues (improper cluster placement) and/or
    adaptation issues, for example, performance degradation during adaptation and/or
    unnecessary cluster relocation. Much of the prior work on elastic container deployment
    problems in the Cloud does not consider spatial aspects of workload variations
    as they focus on local techniques within a single data center to handle fluctuations
    in workload volumes through localized auto-scaling of containers,16-19 clusters,20-22
    or both.23, 24 Other work proposes techniques to handle geo-workload variations,11,
    25 however, they do not use containers and hence do not benefit from the benefits
    of container-based solutions, and they do not maintain application performance
    during adaptation. Container-aware elastic deployment techniques for handling
    spatial workload fluctuations are essential to support automated deployment adaptation
    of container clusters in geo-distributed Cloud environments to efficiently maintain
    cost and performance during adaptation. They need to make intelligent decisions
    to add, relocate and/or remove clusters across data centers as required. Also,
    they need to consider latencies between data centers when making such adaptation
    decisions to maintain performance (response times) and SLOs during the adaptation.
    In this article we propose an elastic deployment approach for web applications
    using container solutions. We argue that container management platforms should
    support elastic deployment techniques to support web application Quality of Service
    (QoS) and support SLOs at lower costs. This work makes three key contributions.
    Firstly, we present an elastic deployment technique that automatically and continuously
    makes proper deployment plans to optimize the number and placement of clusters.
    The core idea is that sacrificing an acceptable level of performance can help
    to reduce operating cost. For cluster placement, genetic algorithms are used that
    consider proximity to users and cost of adaptation (i.e., number of relocated/new
    clusters and inter-data center latencies), while a heuristic is introduced for
    adjusting cluster quantity. Secondly, we present a framework to demonstrate how
    container platforms can support automated elastic deployment of container clusters
    in geographically distributed Clouds. Thirdly, we carry out experiments using
    case studies based on Kubernetes on the Australia-wide and highly distributed
    NeCTAR Research Cloud. Results show that with only a 0.5 ms sacrifice in the SLO
    for the 95th percentile of response times, our approach achieves 16.67%–44.44%
    reduction in cost compared to static and over-provisioning deployment solutions.
    The rest of the article is organized as follows. In Section 2, we cover related
    work. Section 3 describes the application and the container deployment models
    that are adopted as well as provides the problem definition. Section 6 discusses
    the proposed solution. We evaluate the proposed approach in Section 22. Finally
    in Section 49 we provide conclusions and identify potential future research directions.
    2 RELATED WORK Container deployment in distributed clouds. Significant efforts
    have been made in container deployment across distributed Clouds to tackle different
    challenges such as automation, migration and multi-cluster management. Regarding
    automation, solutions like Kops26 and Kubespray27 automate the deployment of Kubernetes
    clusters across multiple Clouds. Orchestration solutions automate the deployment
    of containers across multi-zone clusters and across multi-region/multi-Cloud clusters,
    for example, Nomad.28 Moreover, migration solutions can relocate container clusters
    across data centers, either via rescheduling10 or live migration.29, 30 Container
    management platforms, like Rancher,8 Google Anthos,7 and OpenShift31 make container
    clusters easier to deploy in distributed clouds.1 In addition to multi-cluster
    governance and visibility, they provide application providers the ability to easily
    adapt the deployment of container clusters across data centers through a unified
    user interface or API. However, as it is the application provider''s responsibility
    to make the deployment and adaptation plans, and these are unlikely to be optimal.
    These plans require accurate workload analysis that correctly estimate workloads.32
    Hence, automated elastic container deployment techniques are needed to fill in
    this gap to provide accurate workload estimation and adapt the deployment to changes
    to maintain application performance and cost requirements. Elastic container deployment
    in cloud computing. The problem of elastic container deployment in Cloud computing
    has been studied intensively at different resource levels: container deployment,16-19
    cluster deployment20-22 or both.23, 24 These approaches use horizontal methods,17,
    21-23 vertical methods18, 24 or hybrid approaches16, 19, 20 depending on the elasticity
    dimensions. However these solutions lack the ability to include spatial aspects
    in their adaptation processes, which is essential to reduce network latency—a
    key performance factor for global web applications. Instead they focus on local,
    auto-scaling techniques, that is, within a single data center, to support scalability,
    elasticity and utilization of Cloud resources to handle variations in workloads
    in a cost-efficient manner for both application and Cloud providers. Geo-elastic
    container deployment techniques are complementary mechanisms to these local solutions,
    and needed to adapt application deployment in geo-distributed Cloud environments,
    exploiting the lightweight and portable nature of containers. Spatial workload
    management. The problem of spatial workload management has been tackled in different
    computing environments, for example, Edge and Fog computing, although they have
    different demands and associated scenarios. Geographical load balancing is a common
    approach for managing spatial workloads. Domain Name System (DNS)-based geographical
    load balancing solutions, like AWS Route 5333 and Azure traffic manager34 can
    distribute load to different Cloud data centers based user geo-locations to reduce
    latency and other factors such as energy savings.35, 36 Centralized geographical
    load balancers gather all incoming requests and distribute them to an appropriate
    data center based on one or more factors, for example, carbon footprint and energy
    costs.37 These centralized solutions add extra latency to every request and can
    limit the benefit of distributing application replicas. Decentralized agent-based
    geographical load balancing solutions avoid issues with centralized solutions
    since each data center running applications has an individual load balancer realized
    as an agent. Agents coordinate with each other in a decentralized manner to distribute
    load. In Reference 38, authors propose a decentralized geographical load balancing
    solution suitable for Edge computing and Internet of Things (IoT) applications.
    They assume a multi-cluster architecture at the edge layer where each cluster
    consists of edge nodes and has an orchestrator used to manage workload distribution,
    either locally or globally across clusters. The aim is to optimize end-to-end
    latency of IoT applications in Edge infrastructures. Similarly, the authors in
    Reference 39 present a decentralized geographical load balancing solution suitable
    for multi-Cloud web applications to manage short-term spatial workload variations.
    This approach, however, is not adequate when managing long-term spatial variations
    of web applications, for example, with monthly/seasonal variations as user requests
    are usually distributed to predefined and static locations. New workloads can
    arrive from new areas that may be distant from those static locations and such
    distances can incur latency issues for user requests and thus affect the overall
    QoS. Another approach to handle spatial workloads is to use a geographical load
    balancing solution to direct users to appropriate, possibly new data centers according
    to given factors, for example, latency, with auto-scaling capability at each data
    center. SeaClouds40 provides a platform for the seamless management of applications
    on multi-Cloud environments based on this approach. It uses a geographical load
    balancer to redirect requests to the application replica closer to the user and
    uses a policy, called follow-the-sun, to auto-scale resources for applications
    with possibility to move replicas closer to the user. In Reference 41 a centralized
    geographical load balancing and adaptive resource provisioning solution is presented.
    The geographical load balancer in this solution acts as an entry point to the
    application and selects an appropriate data center for users according to regulation
    requirements and other factors, for example, latency, with resources auto-scaled
    at each data center. Even though such an approach can handle long-term spatial
    workloads, it is not suitable to our needs as it only considers optimizing latency
    for each user individually and this can lead to deployment of application replicas
    at excessive number of data centers close to users. This can be costly as each
    data center will run their own container cluster. Reducing the number of clusters
    would reduce the number of master nodes and thus reduce the operating costs. Therefore,
    a technique to adapt the deployment and placement of container clusters in distributed
    Clouds, according to accumulated workloads for all clusters, is required to achieve
    including optimizing the overall latency with minimum costs. A better approach
    for managing workload variations is to use deployment optimization techniques
    to intelligently adapt the deployment of applications across distributed computing
    environments, when needed, to maintain application needs, for example, QoS and
    cost. In Reference 42, the authors present a solution to support the adaptive
    deployment of multi-component IoT applications to Fog infrastructure factoring
    in limited infrastructure capabilities, latency, and bandwidth to achieve QoS.
    This solution is not applicable to multi-replica web application deployment here
    as Cloud infrastructures provide scalable, unlimited resources and advanced data
    center networks.39 In the context of distributed Cloud and web applications, solutions
    such as Reference 11, 25 propose geo-elastic deployment techniques of multi-replica
    web applications to maintain performance to support SLOs at lower costs. These
    solutions, along with geographical load balancers, can manage long-term spatial
    workload variations as they can dynamically adapt the number and placement of
    web application replicas across geo-distributed data centers. Work in Reference
    11 assumes cross-data center eventual data consistency whereas the solution in
    Reference 25 targets web applications requiring strong consistency between data
    centers. Furthermore, the solution in Reference 25 considers the number of cross-data
    center application relocations as a cost of adaptation that should be minimized
    while11 does not, hence this can lead to needless relocation of application. However,
    none of these approaches consider inter-data center latency as a cost of adaptation
    (i.e., latencies between data centers for the current deployment and data centers
    for a new deployment) when choosing new data centers for new deployment plans
    to help to maintain application performance. Such considerations would minimize
    the latency between source and destination data centers and thus reduce the geo-replication
    overheads needed for web applications that require to maintain the state (e.g.,
    user sessions). Containerization provides a lightweight portable runtime to facilitate
    the elastic deployment of applications to distributed heterogeneous Cloud platforms,43
    compared to heavyweight VM-based models used in References 11, 25. Elastic deployment
    techniques using containers allow rapid adaptation of application deployment in
    geographically distributed clouds since they eliminate the significant latency
    incurred with VM-based models when provisioning new applications and new instances.
    Copying container images across data centers is also much faster than copying
    large-sized VM images. To reduce provisioning latency in VM-based models, pre-copying
    optimization techniques were proposed in Reference 11. However, they require determination
    of new potential future Cloud locations in advance and periodically copying VM
    images which limits the capabilities of deployment optimization techniques to
    choose Cloud locations other than pre-selected ones when needed. Also, by using
    containers such deployment techniques can utilize more Cloud locations as they
    provide consistent, portable deployment of applications across data centers regardless
    of the underlying Cloud infrastructure. Container images provide an abstraction
    that can isolate the application environment from the underlying deployment infrastructure.4
    Container-based solutions also provide a cross-Cloud overlay networking that facilitates
    the process of geo-replication and migration. Thus container-based solutions have
    many direct benefits for elastic deployment demands compared to historic Infrastructure-as-a-Service
    solutions. Application placement in distributed computing environments. Placement
    solutions to tackle latency management have been studied in different distributed
    computing environments. A solution in Reference 42 adaptively places IoT application
    components across Fog and Cloud infrastructure, while considering inter-fog node
    and fog-Cloud latency issues and other factors such as bandwidth constraints.
    Similarly, in integrated edge-Cloud environments, the authors in Reference 44
    propose a dynamic placement solution for IoT requests, aimed at reducing task
    latency times and system power consumption. Also, work in Reference 38 presented
    an approach to place IoT requests across local or remote cluster of edge nodes
    (or Cloud servers), considering inter-edge-node and edge-Cloud communications,
    queuing and processing delays, to achieve better response times. In the distributed
    Cloud context, a body of work has been proposed considering proximity to users,
    for example, Reference 11, 45, 46, inter-data center latency issues, for example,
    References 47 or both 12, 25, 48. Work has explored place virtual desktops45 and
    service-oriented solutions,46, 48 web solutions11, 12, 25 or diverse47 applications
    across data centers. Works have explored either a static12, 47 or dynamic11, 25,
    45, 48 deployments using VMs11, 25, 45-48 or containers12 to achieve inter-data
    center strong data consistency and performance.25 Other works have considered
    high availability and performance even after complete or partial Cloud outages12
    factoring in budget and performance issues46 or performance issues alone, for
    example, References 11, 45, 47, 48. Dynamic placement (replacement) solutions
    such as Reference 25, 48 consider inter-data center latency issues to optimize
    performance at deployment times. None of them however consider inter-data center
    latency to optimize performance during the adaptation. Solutions that tackle real
    time deployment scenarios are thus needed. In this work, we consider eventual
    data consistency between data centers, inter-data center latency during deployment
    is not considered. 3 PROBLEM FORMULATION This section provides an overview of
    the assumptions made for container deployment and containerized web applications
    in distributed Cloud environments. A specification of the problem definition is
    also provided. 3.1 Assumptions for container and application deployment models
    As shown in Figure 1, we assume that multiple container clusters are deployed
    on top of Infrastructure-as-a-Service (IaaS) distributed Clouds. Each Cloud has
    a number of geographically distributed data centers. We assume one cluster per
    data center and that clusters are deployed at different geo-areas. Each cluster
    should ideally serve users within its given geo-area. A geo-location DNS (geo-DNS)
    service, for example, Azure Traffic Manager34 can be used to determine the traffic
    to appropriate clusters based on the user geo-location. This can be obtained using
    IP-to-Location mapping services such as IP2Location*. These can associate user
    requests to the nearest cluster. In this work, we assume each cluster runs a full
    copy of containerized web applications as this model enables an application to
    scale globally,3 which fits our needs. We assume each application service, for
    example, web server and database, is auto-scaled to cope with dynamic local workload
    volumes. For the underlying web application data model, at least one full copy
    is assumed to be present at each data center running a cluster. While data consistency
    model between replicas within a data center can be supported, eventual consistency
    between inter-data center copies is required to improve scalability without performance
    loss. A cluster infrastructure consists of a number of VMs that can be scaled
    elastically by provisioning/terminating VMs from a data center through the associated
    Cloud APIs. Each VM is assumed to have a container runtime, for example, a Docker
    engine49 installed that allows it join the cluster as a worker node to run containers.
    On top of the cluster, a container orchestration and cluster management platform
    is assumed, for example, Kubernetes. Kubernetes has management components such
    as an API server and scheduler that provide a control plane for the cluster. The
    control plane runs on one or more master nodes for availability. We also assume
    a multi-cluster container management platform that runs on an individual VM and
    logically runs on top of the running clusters. This should include a set of management
    services, for example, for migration and cross-cluster workload monitoring, required
    to add new clusters or relocate/migrate existing ones. 3.2 Problem definition
    As stated, a geo-elastic container deployment technique for multi-cluster deployment
    of web applications in distributed Clouds is essential to maintain application
    QoS and SLOs at lower costs. Any solution needs to be able to modify the current
    state of the cluster deployment to a new, desired state, by adding, relocating
    and/or removing container clusters, whenever a geo-workload fluctuation leads
    to unacceptable SLO violations and/or one or more idle or redundant clusters gives
    rise to unnecessary costs. The key challenges for deployment modifications are
    how to decide how many clusters are required; where they should be placed, which
    existing clusters should be removed/replaced and the underlying capabilities needed
    for cross-data center migration and replication to reach a desired deployment
    state. This work focuses on cluster quantity adjustment and associated, dynamic
    cluster replacement strategies. To adjust the number of clusters, a mechanism
    is needed to minimize the number of running clusters, where possible, to reduce
    the associated operating costs. For the cluster replacement problem, it can be
    formulated as an optimization problem where an objective function is used to reduce
    the violation rates and decrease costs. Maximizing the function should help to
    achieve such aims. Using symbols in Table 1, the objective function can be represented
    as follows. (1) where is the estimated amount of reduction in the violation rate
    when changing the deployment () to the candidate one () and where is the estimated
    cost of that adaptation for a given workload () at time (). TABLE 1. Terms used
    in the models Term Meaning current Current deployment of container clusters n_clusters
    Size of the current deployment cand New deployment candidate A Set of available
    data centers D Set of the data centers of the current deployment (current) F Set
    of selected data centers of the candidate deployment (cand) K Set of new and/or
    relocated clusters Workloads collected at time Number of required clusters for
    the new deployment Number of sessions in data center at time Number of violated
    sessions in data center at time Estimated round-trip-time (RTT) network latency
    between user and data center if that user is served by an application running
    a cluster in Estimated inter-data center (RTT) network latency between data center
    and data center (migration) Upper bound of acceptable Session Based Violates Rates
    () Gain threshold to be considered an improvement in when is below Consecutive
    periods for cool down Time interval Pause interval Formally, the cluster replacement
    problem can be defined as follows. Using the terms defined in Table 1 at a given
    a point in time (t), for N required clusters and workload for a system with clusters,
    we want to select a set of Cloud data centers, , where , to (re)place container
    clusters across data centers in F such that the objective function in Equation
    1 is maximized. 4 PROPOSED GEO-ELASTIC DEPLOYMENT APPROACH To handle the above-mentioned
    issues, we present a geo-elastic deployment approach. In this approach, we propose
    two mechanisms that work together as one technique to make appropriate elastic
    scaling decisions, when needed. The first mechanism is a geo-elastic deployment
    controller that determines how many clusters are required. It implements an SLO-based
    heuristic that attempts to establish an optimal size adjustment of the current
    deployment () among different possible adjustments to avoid/minimize SLO violations
    under cost and performance constraints. Within each possible adjustment, the controller
    uses the second mechanism, a cluster replacement method, to handle spatial aspects
    of elasticity, that is, finding the best location for cluster deployments across
    geographically distributed Cloud data centers (). A candidate deployment plan
    with minimal cost of adaptation and acceptable SLO violation rate is selected
    as the new deployment plan. Moreover, we present a framework to support the automated
    elastic deployment of container clusters in geographically distributed Clouds.
    This factors in the geographical distance between users and data centers running
    application containers and between data centers themselves as key factors that
    affect network latency (performance). We refer to this as geo-elastic deployment.
    4.1 Requirements and assumptions The approach requires pre-agreed SLOs for user
    requests to be provided by application providers. Periodic collection of workloads
    (i.e., user requests) from running container clusters is also obtained. Each cluster
    is assumed to have a load balancer that distributes incoming user requests to
    appropriate application containers. Such requests are stored in log files. The
    request logs are assumed to have information about users including their IP addresses
    that can be mapped to geo-locations. The geo-locations of data centers are assumed
    to be known and available. Moreover, the approach depends on knowing the network
    latency between users and data centers and between data centers themselves. Round-trip-time
    network latency data can be obtained using third party services, for example,
    Ookla,50 or by latency estimators to estimate latencies between users and data
    centers, for example, Reference 51, or through empirical measurement. Additionally
    we assume that the processing time of requests is constant and that clusters have
    sufficient Cloud resources and negligible internal communication overheads within
    data centers due to the high-speed networking capabilities. Finally, we assume
    that homogeneous VMs (in terms of size and price) exist for clusters running at
    different Clouds. 4.2 Adaptation triggers In this section, we investigate the
    dynamic characteristics of web application geo-workloads and identify how to trigger
    the adaptation process to make appropriate elastic actions. We identify three
    cases. Case 1: Geographical growth of workloads. Applications may gain more popularity
    in particular areas. In this case, geo-workloads on current clusters running application
    containers can violate SLOs because of the potentially large geographical distances
    between users and the data centers clusters and the associated network latencies
    that can arise. This situation should trigger the geo-elastic deployment approach
    to geo-expand the current deployment. Case 2: Geographical shrinkage of workloads.
    This case is the opposite of the previous one. Applications at some point in time
    may lose popularity in some regions causing clusters running in data centers within
    those regions to become underutilized. This can cause unnecessary expenses to
    be incurred due to the over-provisioned deployment of clusters. This condition
    should be a trigger to geo-shrink the current deployment. Case 3: Geographic shift
    in workloads. In this case, the popularity of applications can shift between regions,
    hence clusters already running in data centers may need to be partially redeployed
    to other ones. This case should be used as a trigger to relocate some of the running
    clusters at new data centers to meet the geo-area needs of users at that time.
    4.3 Geo-elastic deployment framework As shown in Figure 2, the framework''s components
    are divided into two main categories: decision-making and action-taking. This
    work mainly focuses on the decision-making components. Components in such multi-cluster
    platforms communicate with container cluster platforms running in data centers
    through agents. An agent, which can be deployed as a containerized service, receives
    commands from components and executes them on the local cluster platform. Agents
    can also communicate with other agents running in other cluster platforms in different
    data centers, when needed, to provide inter-cluster management services such as
    container relocation and data replication between clusters. FIGURE 2 Open in figure
    viewer PowerPoint A framework of enabling automated elastic deployment of container
    clusters in geographically distributed Clouds The functionality of the framework''s
    components and how they interact are discussed below. 4.3.1 Decision-making components
    Decision-making components such as a geo-elastic deployment controller and cluster
    replacement component are responsible for making elastic decisions in terms of
    the quantity and placement of container clusters as required, for example, to
    periodically assess and produce new, desired states of the deployment to meet
    evolving web application requirements. Once a new state of deployment is determined,
    it is passed as a deployment plan to the deployment executor component. Geo-elastic
    deployment controller. The geo-elastic deployment controller component is responsible
    for deciding on the optimal quantity of container clusters based on current geo-workloads
    and pre-agreed SLOs. It needs to strike a balance between acceptable SLO violation
    rates and the least possible number of container clusters distributed geographically
    across different Cloud data centers. An SLO-based violation model to estimate
    SLO violation rates of a given deployment is discussed in Section 11. This component
    implements a decision-making algorithm for the geo-elastic deployment controller
    mechanism, which is discussed in more detail in Section 12. The geo-scaling decisions
    include: geo-expanding, geo-shrinking and geo-relocation. Within the algorithm,
    the cluster replacement component is called to determine the optimal placement
    of clusters for any new, deployment plan produced as part of a given scaling decision.
    Cluster replacement. The cluster replacement component aims to automatically handle
    the spatial aspect of the adaptation process by finding the optimal placement
    of container clusters for any potential deployment plan. It implements a meta-heuristic
    using genetic algorithms for dynamic cluster placement as discussed in Sections
    14 and 19. 4.3.2 Action-taking components Action-taking components consist of
    components that take appropriate actions based on any new deployment plan obtained
    from the geo-elastic deployment controller. Each container cluster in a given
    deployment plan has a set of possible conditions for a given cluster: new, migrating
    and leave-as-is. A container cluster with a new condition requires creation of
    a new cluster while a cluster with a migrating condition indicates that the cluster
    already exists however it needs to be relocated to another data center. A cluster
    with a leave-as-is condition implies the cluster is already running at a data
    center and should remain there. Deployment executor. This component is responsible
    for taking elastic actions to change the current state of the deployment to a
    new, desired state. Specifically, it determines the condition of container clusters
    involved in any proposed deployment plan, and subsequently makes appropriate actions
    for each container cluster based on its current condition and the intended future
    state. Only clusters with new and/or migrating conditions require actions to be
    taken. Both conditions initially require container clusters to be prepared for
    a given selected data center. Following this, clusters with a new condition, require
    creation of application containers to be deployed, while for migrating clusters,
    containers need to be relocated to the remote (selected) data centers. The required
    implementations of those actions should be abstracted in the individual cluster
    management components as discussed below. Individual cluster management components.
    Individual cluster management has three components: cluster infrastructure provisioning;
    container deployment and container management. Each component is responsible for
    providing different elastic actions. Cluster infrastructure provisioning. This
    component automates the process of providing the infrastructure-related actions
    that are responsible for making a container platform ready at a new, selected
    data center. It implements all infrastructure automation capabilities required
    for this action including provisioning VMs through Cloud APIs to create a new
    cluster as well as installing required container-related software (e.g., Docker
    and Kubernetes) on the cluster nodes. This minimizes risks related to human errors
    and expedites the deployment process. This automation should abstract the different
    implementations required to make the proposed geo-elastic deployment feature suitable
    for multi-Cloud environments. Multi-Cloud libraries, for example, Apache Libcloud†and
    jclouds‡are examples of technologies that can be used to manage Cloud resources
    from different Cloud providers using a unified API. For Kubernetes, automation
    tools like Kops26 or Kubespray27 can be used here. Container deployment. This
    component supports the deployment of new application containers at container clusters
    with a new condition set. It abstracts the implementation details needed to perform
    the container deployment including dealing with the related data. To achieve this,
    this component sends a deployment request (e.g., in the form of a YAML or JSON
    configuration file) to a given agent. The agent then geo-replicates the application
    data from the nearest data center that is already running a cluster or from the
    data storage located in a data center running the multi-cluster container management
    platform. Following this, the agent passes the deployment file to the local container
    cluster platform (e.g., Kubernetes) via the cluster APIs. Then the local container
    platform pulls the required container images from a nominated container image
    registry. Container management. This component provides a migration action for
    existing clusters to relocate containers and their data from a source cluster
    to a destination cluster at a new, selected data center. It implements all techniques
    and services required to migrate running clusters for clusters with the migrating
    condition set. It needs to provide agents in the source and destination clusters
    with the required inter-cluster management services to complete the migration
    process. Solutions for such services have been proposed in several other works,
    for example, References 30, 52, 53. 4.4 SLO-based violation model In this section,
    we introduce a SLO-based violation model to estimate the violation rate of an
    application deployment at a given time, , based on pre-agreed SLOs. This is used
    as a metric, Session Based Violation Rate (SBVR), to evaluate the performance
    of a given deployment. Using terms defined in Table 1, the violation model is
    defined as follows: (2) At a given point in time, , and given a deployment (i.e.,
    the container clusters to be deployed at data centers in X) as well as the current
    user geo-workloads, , collected from running clusters at time , then the SBVR
    of the deployment can be calculated as the total number of violated user sessions,
    , divided by the total number of user sessions, , where each in X represents a
    data center running or potentially running a cluster. In this work, a user session
    consists of a set of successive requests. These are considered to be violated
    if the average response time of the requests is beyond the defined SLOs. Since
    we assume the processing times of requests are constant, an SLO refers to the
    acceptable network latency, plus the (processing) constant. 4.5 Multi-cluster
    Geo-elastic deployment controller algorithm In this section, we present a decision-making
    algorithm (Algorithm 1) for automatically controlling the size of multi-cluster
    deployment according to the geo-dynamics of workloads. The algorithm aims to provide
    a balance between performance and cost. When adjusting the size of the deployment,
    it relies on a cluster replacement method, which will be discussed in the following
    sections, to modify the actual location of the clusters. As discussed, there are
    three types of elastic decisions: geo-shrinking, geo-relocation and geo-expanding,
    and each one is a form of adaptation trigger. Selecting the right decision requires
    detecting changes in workloads. Using the terms defined in Table 1, the algorithm
    uses the SBVR of a deployment as a performance indicator to detect changes in
    workloads and as the basis for making appropriate decisions. Once the current
    workloads are obtained, the SLO-based violation model presented in the previous
    section is used to calculate the violation rates, SBVR, for both current and any
    candidate deployments. If the SBVR of the current deployment is beyond a pre-defined
    upper bound of acceptable SBVR, , then geo-relocation or geo-expanding decisions
    should be made to reduce the network latency and thus reduced the SBVR (below
    ). On the other hand, if the SBVR of the current deployment has remained under
    , there are two possible courses of action. The first is to try geo-shrinking
    the deployment to reduce the cost. The second one, which should be used if the
    resultant candidate deployment is not able to maintain an acceptable violation
    rate, is to consider geo-relocation decisions and accept them if the new, candidate
    deployment is more likely to improve the SBVR beyond a predefined gain threshold,
    . Algorithm 1. Decision-making Algorithm for the Multi-cluster Geo-elastic Deployment
    Controller In more detail, the input of the algorithm is the initial () deployment
    of the container clusters. The genetic algorithm-based cluster replacement algorithm
    in Section 19 is used throughout as part of the function. The function should
    pass the accepted, candidate deployment to the deployment executor introduced
    previously. It should be noted that the intervals between consecutive decisions
    should be determined carefully to avoid unnecessarily loading the system with
    many status request updates. To address this we introduce a pre-configured parameter,
    pause time (), to enforce these intervals. After the initialization steps, the
    algorithm runs a control loop. At every time interval, , if a decision was made
    in the last iteration, , then we pause the algorithm for (Lines 5–8) to ensure
    that the system is not perpetually asking for update information. Then, the current
    user workloads, , are collected from the container clusters comprising the current
    deployment (). It will try to geo-shrink the deployment if the SBVR of the current
    deployment has remained under a given threshold () for a consecutive number of
    periods ( as shown in Lines 10–18). It then gets the potential, candidate deployments,
    , by decreasing the number of clusters by one and then replacing the clusters,
    when needed, through the cluster replacement method. If the violation rate of
    the new, candidate deployment is below the threshold, , then the algorithm will
    call the function to pass the new deployment plan to the deployment executor to
    take the appropriate actions and update the current deployment and terminate the
    execution of the current iteration of the loop whilst waiting for a new interval,
    . Following this the algorithm continues to explore geo-relocation decisions (Lines
    19–33). When a geo-shrinking decision is not made or the SBVR of the current deployment
    is beyond the threshold, . It obtains the candidate deployment by replacing clusters
    only. This decision can be made when one of the two following conditions is satisfied.
    Firstly when the candidate deployment can help to reduce the unacceptable SBVR
    of the current deployment to be under the threshold, (Lines 21–24). Secondly when
    the SBVR of the current deployment is acceptable and the candidate deployment
    can improve the SBVR for a value that is greater than the gain threshold, (Lines
    25–28). Lastly, a geo-expanding decision is made when the SBVRs of the current
    deployment as well as the candidate one, produced in the previous step, remain
    greater than the threshold, (Lines 34–40). 4.6 Cluster replacement method for
    spatial adaptation The cluster replacement method, which is the second proposed
    mechanism of our geo-elastic deployment solution handles the spatial aspect of
    adaptation to improve the performance, geo-scalability and cost-effectiveness.
    This is an optimization problem as discussed in Section 3. It requires the following
    challenges to be addressed. One challenge is to determine how to estimate the
    improvement in performance as well as the cost of adaptation of a candidate deployment
    plan to be used by the objective function. Another challenge is to design algorithms
    to rapidly establish near-optimal solutions (i.e., finding potential data centers
    for candidate deployments) to support dynamic and near-real time scaling decisions.
    4.6.1 Cost of adaptation The cost of adaptation refers to the potential cost of
    changing the current deployment to a new one. To improve web application demands
    for cost-effectiveness and performance, this cost needs to aim at minimizing the
    operational cost as well as the adaptation time (i.e., time to complete the adaptation
    process). While the former can be reduced by minimizing the number of clusters
    that are deployed and/or relocated to new cloud data centers, the latter can be
    reduced by minimizing the total inter-data center network latencies. These latencies
    can occur between a data center running a multi-cluster platform or data centers
    of a current deployment, D, and selected data centers put forward for a candidate
    deployment, F. Using the terms in Table 1, the cost of an adaptation function
    that can be used as the denominator of the objective function in Equation 1 is
    defined as: (3) where is the number of new and/or relocated clusters, is a data
    center and is the weight of the total inter-data center latencies needed to make
    clusters in K ready. It should be noted that one is added to avoid division by
    zero in our objective function when there is no cost of adaptation. Since the
    aim is to minimize the cost of adaptation and maximize the reduction in the violation
    rate, a candidate deployment that helps to realize this aim should be selected
    by the cluster replacement algorithms. These algorithms use the objective function
    to propose new data centers in F to be geographically near the currently running
    ones (to reduce the overheads of inter-data center latencies during adaptation
    to speed up the adaptation process) as well as near new geo-workloads (to reduce
    user-to-data center latency after new deployment takes place). Inter-data center
    latency consideration during adaptation help to reduce the geo-replication overheads
    and thus its benefits can be realized in two ways. One obvious benefit is that
    it speeds up the adaptation process since it reduces the network latencies by
    expediting the relocation of containers and/or data. Another one is that it helps
    to maintain performance (e.g., response times) during the adaptation by lowering
    the overheads involved in geo-replicating the state (e.g., current user sessions)
    until the adaptation finishes and DNS records are updated. For example, the green
    and red lines represent the amount of inter-data center latencies between some
    data centers in Figure 2, where it is clear that adding 15 ms to the response
    time as an overhead is better than adding 30–50 ms. 4.6.2 Violation rate improvement
    One way to improve the performance is to let the cluster replacement method select
    a candidate deployment that can produce the maximum reduction in SBVR. Selecting
    a deployment in this way can be costly due to the high possibility of over-provisioning.
    In other words, reaching the maximum reduction amount in SBVR can result in provisioning
    more clusters than required. Another way is to find a balance between performance
    and cost when adapting deployments. In some situations, the operational cost can
    be reduced by minimizing the number of running clusters albeit with an acceptable
    sacrifice in performance. This may cause an increase in the SBVR but this increase
    may not go beyond the upper bound of acceptable SBVR, . However it may also give
    rise to increased network latency with no increase in the SBVR since the response
    times of requests cause an increased delay, yet still be under the threshold of
    the defined SLOs. To achieve this, we propose two cases for the reduction function,
    , that is used in the objective function in Equation 1 and represented as follows.
    (4) Choosing which case to be used in the reduction function depends on the SBVR
    of the current deployment or the suggested elastic decision made by the controller.
    The first case is chosen if at least one of the following conditions are satisfied.
    The first condition is met when the SBVR of the current deployment, , is above
    the acceptable SBVR, . In this condition, the first case requires a candidate
    deployment that reduces the SBVR below the threshold and maximizes the reduction
    amount in SBVR from the threshold. The second condition of the first case is satisfied
    when the suggested elastic decision is to geo-shrink the deployment, . In this
    condition, any candidate deployment with smaller size and the maximum reduction
    amount in SBVR from the threshold will be chosen. In this second condition, the
    cost is reduced by possibly increasing the SBVR, however it should still be under
    the upper bound for acceptable SBVR, . If none of the above-mentioned conditions
    are met, the second case of the reduction function will be selected. This case
    aims to choose a candidate deployment that can maximize the reduction amount in
    SBVR from its current status. It is noted that considering both cases is necessary
    in Equation 4 since ignoring one of them can lead to improper elastic decisions
    being made under certain conditions. Consider the following example. Using Equations
    1 and 3 and assuming the inter-data center network latency is constant at 1, the
    objective function can be presented as . Now assume at some time the violation
    rate SBVR of the current deployment is 30% and the upper bound of acceptable SBVR
    is 10%. Also assume that there are two candidate deployments, cand1 with SBVR:
    15% and K: 1 and cand2 with SBVR: 8% and K: 2. If the second case is only considered,
    then for cand1 and cand2 will be 7.5 and 7.3 respectively. Since the aim is to
    maximize the function, cand1 will be returned by the cluster replacement algorithm
    (Algorithm 1). That is, in Algorithm 1, the decision is geo-expanding as the if-condition
    in Line 34 is met. On the other hand, if we consider the two cases for , then
    for cand1 and cand2 will be -2.5 and 0.6 respectively. Therefore, the right candidate
    deployment cand2 will be selected and returned. In this case, in Algorithm 1 the
    if-condition in Line 21 is met and the decision is geo-relocate. Hence, considering
    both cases for improves the reduction in SBVR to be under without the need to
    increase the number of clusters while the one-case-only approach fails to choose
    the right candidate deployment and hence increases the cost as it requires an
    increased number of clusters. 4.6.3 Hardness of the problem Since the cluster
    replacement problem considers the cost of network latencies between users and
    data centers as well as the cost of adaptation, it falls into the class of Mobile
    Facility Location Problems54 that are hard to solve.55, 56 These problems are
    a form of problem of moving each facility from one location to another and assigning
    each client to some facility such that the total costs of moving facilities and
    client assignments are minimized. They also generalize the NP-hard k-median problem,54
    which given a set of points, involves identifying k centers such that the total
    distances of the points to their closest centers are minimized. Our problem can
    be shown to be NP-hard by restriction, which is a method of showing that an already-known
    NP-hard problem is a special case of the target problem. We prove the hardness
    of our problem by showing that the NP-hard k-median problem is a special case
    of this problem. Proof. Suppose at some point in time, , a given reduction in
    a potentially remote workload occurs (i.e., Case 2 in Section 7) and the predefined,
    constant is set to 1. This causes a geo-shrinking decision of the current deployment,
    , to be triggered and hence the number of required clusters, , for the new candidate
    deployment, , becomes . If we let and K be zero in Equation 3 since no new/existing
    cluster can be deployed or relocated. In this case, the cost of adaptation is
    , which is constant. Since is satisfied in Equation 4 and the is constant, then
    . By applying the last two steps to and in Equation 1 and transforming the maximization
    problem to a minimization one, the problem can be represented as . Given a fixed
    amount of estimated SLO violations between any user, , and data center, , and
    given a set of available data centers, A where N data centers are to be chosen
    from A for deploying clusters for users in, . Minimizing the total amount of SLO
    violations is the k-median problem. Hence the cluster replacement problem NP-hard.
    As our problem is NP-hard, we need algorithms that can approximate global optimization
    by finding good solutions in polynomial time. 4.6.4 Cluster replacement algorithm
    To address this optimization problem, we present an approach based on genetic
    algorithms. This meta-heuristic suits our problem for several reasons. First,
    it is used for approximating global optimization for many problems as it generally
    finds good global solutions and has the power to evade local optima.57 It has
    also been used to solve a variety of related problems in diverse contexts as it
    is able to make a good trade-off between the quality of solutions and the completion
    time since it solves problems efficiently.12, 25, 48, 58, 59 Theoretically, by
    considering the complexity of the algorithm and assuming all inputs are of the
    same size , then the worst-case complexity is , which is polynomial. Generally,
    in practice the number of required clusters, , is relatively small for cost and
    administrative reasons and thus we can say the upper bound of the running time
    of the algorithm is . Empirically, this approach provides good solutions to our
    problem within timeframes that are acceptable in our context. As the aim of this
    algorithm is to maintain SLO violation rates below a pre-defined upper bound ,
    genetic algorithms are able to achieve this for all runs requiring only 19 iterations.
    Additionally, meta-heuristics, like genetic algorithms, provide ease of use and
    flexibility to add new selection criteria to given objective functions, for example,
    data center failure rate to improve availability. Finally, even though algorithms
    based on genetic algorithm can be used to solve large-sized problems, they can
    suffer from long computational complexity. However such meta-heuristic can be
    easily parallelized as it implicitly supports parallelization techniques.60 Furthermore,
    the resources required to realize parallel versions of genetic algorithms should
    not be an issue in our context because Cloud computing promises scalable, powerful
    compute-optimized resources. However, for our problem we found that proposed normal,
    non-parallelized versions of this algorithm was effective as will be discussed
    in Section 22. 4.7 Genetic algorithm for cluster replacement A genetic Algorithm
    provides a meta-heuristic approach based on the realization of natural iterative
    improvements in population genetics. It relies on a set of bio-inspired operators,
    for example, selection, crossover, and mutation to iteratively modify populations
    and thus evolve successive populations. Using a genetic algorithm to design an
    algorithm requires a genetic representation used to encode candidate solutions
    based on the problem domain and a fitness function to evaluate the quality of
    each solution. For the genetic representation in this work, a chromosome (or an
    individual) can be represented by a non-duplicated set of data centers (i.e.,
    a current/candidate deployment with D/F) where an individual gene of the individual
    is represented by a data center. Duplicated genes in an individual are forbidden
    and the order of the genes are unimportant. For increased efficiency, genes are
    enumerated to rapidly detect duplicate genes. For the fitness function, the objective
    function in Equation 1 is used to select an individual (i.e., a candidate deployment,
    ) with the highest fitness. A fitness function is presented as: (5) Algorithm
    2. Genetic Algorithm for Cluster Replacement A genetic algorithm-based cluster
    replacement algorithm is shown in Algorithm 2. Using the symbols defined in Tables
    1 and 2, the algorithm works as follows. The inputs of the algorithm are the number
    of required data centers of a candidate deployment (i.e., number of required genes
    to form an individual), the current deployment, , as well as accumulated workloads
    collected from running clusters at time , . The algorithm initializes a population,
    , by randomly generating candidate deployments using available data centers in
    A (Line 2). Duplicate individuals are not allowed in a population. The algorithm
    then evaluates each candidate deployment in the population, , and then sorts candidate
    deployments and the number of generations, (Lines 3–6). TABLE 2. Terms used in
    cluster replacement algorithm Term Meaning Crossover rate Mutation rate Number
    of generations (iterations) Number of individuals (feasible solutions) in the
    population Maximum number of generations cands Candidate deployments Best candidate
    deployment (individual) with maximum fitness The algorithm then iterates to produce
    new successive generations by applying the bio-inspired operators: selection,
    crossover and mutation (Lines 7–23). At each iteration, it creates an empty new
    generation, (Line 8). Then it applies a selection operator by selecting candidate
    deployments from the start of the population, , and adds them to the new population,
    , (Lines 9–10). The crossover operator is then applied (Lines 11-13). This operator
    selects pairs of candidate deployments from the beginning of the population, .
    For each pair, it produces two children (two new candidate deployments) by randomly
    selecting a data center from each pair of members and then swaps the two selected
    data centers. It then adds all children to the new population, . The algorithm
    then applies the mutation operator (Lines 14–15) by randomly choosing candidate
    deployments from the new population, , and replaces a randomly selected data center
    from each chosen deployment with a randomly selected data center from A. It updates
    the population, , and then evaluates and sorts candidates in descending order
    (Lines 16–18). If the fitness of the best candidate deployment is less than the
    fitness of the first candidate deployment in the current population, , it updates
    the best candidate deployment with the first candidate deployment in the current
    population (Lines 19–21). Then, the number of generations, , is incremented by
    1 (Line 22). The algorithm terminates when the number of generations, , reaches
    the maximum number of generations, . The algorithm returns the best candidate
    deployment, , that has the maximum fitness (Line 24). In terms of the time complexity
    of the genetic algorithm, the approach requires calculating the running time of
    an iteration (generation) using the terms given in Tables 1 and 2. The selection,
    crossover and mutation operators at each iteration require , and , respectively.
    With regards to the evaluation phase, computing the fitness of each individual
    in a population, , requires finding the best data center (gene) with the least
    network latency overheads in for each and this requires comparisons. As a result,
    evaluating an individual requires thus evaluating all individuals in the population
    requires . Also, sorting individual of a population requires . Hence, the evaluation
    phase runs in . As a consequence, the running time of an iteration is given by
    . The time complexity of genetic algorithm is therefore given by . 5 PERFORMANCE
    EVALUATION To evaluate the proposed geo-elastic deployment solution, experiments
    were carried out on the Australia-wide National eResearch Collaboration Tools
    and Resources (NeCTAR - www.nectar.org.au) research Cloud. Experiments are classified
    into three sets. In the first set, we study the behavior of our geo-elastic deployment
    approach towards geo-workload variations to maintain performance and cost using
    the SLO-based violation rates, SBVR, and the number of container clusters as performance
    and cost metrics respectively. We also examine the cost of adaptation. In the
    second set of experiments, we evaluate the proposed geo-elastic deployment approach
    on the NeCTAR Cloud. This set considers end-to-end response times to requests
    after adapting deployments according to geo-workload changes, as well as the number
    of container clusters as the key evaluation metrics. In the third set of experiments,
    we evaluate the effectiveness of the proposed cluster replacement algorithm, genetic
    algorithm, using the metrics, SBVR and the execution time. To clarify, the operating
    costs of a given deployment in the Cloud are proportional to the total number
    of container clusters from multiple perspectives. First, each container cluster
    incurs a management fee, for example, Kubernetes clusters in Google and Amazon
    Web Services costs $0.10 per cluster per hour. A deployment of 3 clusters can
    reduce the total cost of management fees by 50% compared to one with 6 clusters.
    Secondly, more container clusters used require more VMs to run master and worker
    nodes. For master nodes, highly available clusters require at least 2 master nodes
    per cluster (i.e., 2 VMs). A deployment of 3 clusters requires 6 VMs while a single
    cluster requires only 2 VMs. For worker nodes, increasing the number of clusters
    can reduce the chance of condensing containers to fewer VMs. For instance, assume
    each worker node runs on a VM that consists of 4 CPUs and 2 user workloads from
    different locations require 2 and 6 CPUs respectively. If containers handle these
    two loads run in 2 separate clusters, the worker nodes for clusters will require
    1 and 2 VMs respectively, that is, a total of 3 VMs. However, if they can run
    in one cluster and ensuring that the Cloud location is carefully selected to optimize
    network latency for both user workloads, their containers will be condensed into
    only two 2 VMs. Hence this deployment reduces the cost by 33% compared to the
    one with two clusters. In all experiments, we refer to the SLO as the pre-agreed
    upper level of a response time for a user request, which includes the network
    latency between a user and a data center, the processing time of the request and
    communication overheads when interacting with the data center. This represents
    the threshold that application/platform providers should preserve for their response
    times to achieve a satisfactory user experience. Common settings among all experiment
    sets are explained in the next section. Individual settings are described separately
    within each experiment set. 5.1 General settings 5.1.1 Cloud data centers and
    compute resources settings We ran extensive experiments on the NeCTAR Research
    Cloud. The NeCTAR Cloud is a geographically distributed Cloud comprising 19 availability
    zones (data centers in our context) distributed around Australia. For cluster
    replacement problem, since this number of available data centers is relatively
    small, we added another 41 data centers (60 in total) scattered across the globe
    including AWS Singapore, London, Cape Town, Stockholm and numerous others. This
    expands the search space and thus allows to accurately evaluate the performance
    of the search algorithms in finding optimal solutions from an expanded and realistic
    global Cloud search space. To make sure that the candidate deployments with SLO
    violation rates were below a pre-defined upper bound for acceptable violation
    rates, we selected global data centers specifically to be highly distributed and
    hence likely to incur network latencies. Therefore, selecting one (or more) of
    these data centers will increase the estimated violation rate and hence increase
    the likelihood that it will go beyond the upper bound of the acceptable violation
    rate. This will make the search algorithms explore more solutions and ideally
    demonstrate that they select NeCTAR data centers for their optimal deployment
    locations. With regards to compute resources, every VM instance used in all experiments
    was assigned the following resources: 4 virtual CPUs and 16 GB of RAM running
    Ubuntu 18.04 LTS (Bionic) as the operating system. 5.1.2 Geo-workload variation
    scenarios Five different locations around Australia were chosen to be sources
    of workloads: Brisbane, Canberra, Melbourne, Sydney and Tasmania. To simulate
    real-world geo-workload change scenarios over time, we synthesized a series of
    consecutive geo-workload variation scenarios as shown in Figure 3 and Table 3.
    This was used to generate spatial workload variations in user growth across different
    locations, for example, cities, across Australia. We targeted workloads within
    Australia because, as discussed earlier, our experiments were carried on the Australia-wide
    NeCTAR Research Cloud. Also, we refer to time periods as blocks of time where
    in the beginning of each block of time, a geo-workload variation scenario occurs.
    FIGURE 3 Open in figure viewer PowerPoint Geographic workload changes over time
    TABLE 3. Workload locations over time periods for the different experiments. Time
    period Workload locations 1 Canberra, Melbourne 2 Canberra, Melbourne, Brisbane
    3 Canberra, Melbourne, Brisbane, Sydney 4 Canberra, Melbourne, Sydney, Tasmania
    5 Melbourne, Sydney, Tasmania 6 Melbourne, Tasmania In experiments sets 1 and
    2, we used these scenarios to reflect spatial changes in workloads over time to
    understand how the proposed and baseline approaches react to those changes. Additionally,
    in experiment set 2 we set the length of each period to 1200s. Changes in geo-workloads
    were carefully chosen to allow geo-elastic deployment approaches to make three
    possible elastic decisions: geo-shrinking, geo-relocation and geo-expanding. It
    is noted that to infer an elastic decision at any period in these experiments,
    we calculate the difference between the size of the deployment at that period
    and the one beforehand. The size of deployments over periods are shown in Figures
    4C,D and 6B. If the difference is greater than 0, then the decision is geo-expanding.
    If it is less than 0, then it is geo-shrinking. Otherwise, the decision is geo-relocation
    or no change. FIGURE 4 Open in figure viewer PowerPoint Performance and cost comparison
    with three different approaches for multi-cluster deployment against geographic
    workload changes over six time periods using various SLOs. The lower the SBVR
    and size of deployment the better. Each approach with different SLOs ran 32 times
    5.1.3 The proposed geo-elastic deployment and baselines settings In these experiments,
    the default settings of the geo-elastic deployment controller''s parameters ,
    , , , were set to 2 min, 20 min, 1 period, 10% and 3% respectively. For genetic
    algorithm parameters, we set r, m, p, and to 0.7, 0.2, 60, and 120 respectively.
    Tuning genetic algorithm parameters for our problem is discussed in Section 42.
    We refer to our approach as Geo-elastic (proposed). Regarding baselines, we have
    two baseline approaches: over-provisioning and static deployment. Over-provisioning
    approach is a latency-sensitive, cost-unaware geo-elastic deployment solution
    that only considers proximity to users. It adapts deployments with the aim of
    improving the performance by selecting data centers for clusters such that the
    network latency between application users and data centers hosting container clusters
    regardless is minimized. It is independent of the cost of the adaptation. This
    approach is similar to work described in Reference 11 as it only considers network
    latency between users and data centers and completely ignores inter-DC latency.
    Furthermore work in Reference 25 only considers inter-DC latency and ignores inter-data
    center latency as the cost of adaptation. We refer to this approach as Over-provisioning.
    Nevertheless, the parallels of using over-provisioning for the baseline is consistent
    with these other works. The static deployment approach, as its name suggests,
    is a non-elastic approach used for the number and location of clusters, that is,
    it does not include the spatial aspect of adaptation. It also only supports local
    elasticity. In all experiments, the deployment size of this approach was set to
    two container clusters and the clusters were statically located at two data centers.
    We refer to this approach as Static. This approach has no cost of adaptation since
    it is static. 5.1.4 Network latency data To estimate unknown network latencies
    between users and data centers, we use an approach based on our previous work12
    that relies on the distance between them. The approach simply relies on a correlation
    between network latencies and geographic distances between data centers. We empirically
    measured the round trip time among all data centers using the ping utility to
    obtain network latencies between data centers. To calculate the geographical distances
    we used the Harversine formula. We use this approach because the correlation coefficient
    is found to be strongly positive (0.97). 5.2 Experiment set 1: evaluation of the
    proposed geo-elastic deployment approach The aim of this experiment set is to
    investigate the effect of our geo-elastic deployment approach on maintaining the
    performance as well as the cost of deployment and adaptation, when adapting cluster
    deployments to handle geo-workload changes. In this set, the delay, caused by
    the processing time of a user request as well as all inner-data center communication
    overheads, is constant and fixed to 10 ms. We run experiments 32 times (except
    for the static case) . 5.2.1 User setting To simulate realistic workloads (user
    request logs), , to be used as input to geo-elastic deployment approaches, we
    first model 300 user sessions at each workload location and set the number of
    requests per session to 10. Then, we obtain realistic geo-locations of users for
    each workload location by using Twitter data collected from each workload location,
    extracting the geo-locations of tweeters and then assigning those geo-locations
    to the users in our model. When generating user request log files for container
    clusters, we inject the geo-location of a user instead of the IP address at each
    request record. The user geo-locations and number of requests for each user are
    key information needed from the workloads. Finally, user workloads at each time
    period of geo-workload variation scenarios as shown in Figure 3 and Table 3 are
    generated depending on the number and locations of geo-workloads over that period.
    5.2.2 Experimental procedure In the first experiment, we set SLO to 20 ms and
    the current deployment to the initial deployment using two clusters. Then, we
    run experiments for each stochastic approach 32 times independently on a VM. At
    each run, the geo-elastic deployment controller iterates 6 times to simulate the
    number of time periods shown in Figure 3 and Table 3. At each period, the controller
    retrieves the workloads over that period. Then, the controller either makes an
    elastic decision and produces a new deployment plan or leaves the current deployment
    as it is. Then, we evaluate the new/current deployment and record the SBVR for
    the number of clusters. We also record the cost of any adaptation (the number
    of relocated/new clusters and the total inter-data center latency) if there is
    a change in the deployment at that period that is put forward. For the static
    deployment approach, we run the experiment once since it is a deterministic approach.
    We simply evaluate the single, static deployment against workloads over all periods.
    In the second experiment, we set SLO to 25 ms and then repeat the same steps followed
    in the first experiment. 5.2.3 The impact of geo-elastic deployment on performance
    and cost In this section, we present the impact of considering geo-elastic deployment
    solutions as well as the importance for those solutions on both performance and
    cost when adapting deployments of container clusters with geo-workload variations
    across geo-distributed data centers. Figure 4A,B indicate that all geo-elastic
    deployment approaches (geo-elastic and over-provisioning) undoubtedly show very
    low SLO violation rates, SBVR, at all periods for both SLO settings (20 and 25
    ms). They adapt deployments against workload changes that result in successfully
    maintaining SBVR below the pre-defined upper bound of acceptable SBVR () in all
    cases. On the other hand, the static approach unsurprisingly incurs significantly
    higher SBVR and exceeds 10% at all periods for both SLO settings except for period
    1 when SLO is 25 ms. Furthermore, while the geo-elastic and over-provisioning
    approaches have the same SBVR level in all cases, our approach as shown in Figure
    4C,D, successfully reduce the cost in most cases, especially when SLO is relaxed
    further (SLO = 25 ms). For SLO with 20 ms, our approach shrinks the size of deployment
    by one cluster at periods 3 and 4 when compared to the over-provisioning approach.
    Moreover Figure 4D shows that relaxing SLO by only 5 ms (i.e., from 20 to 25)
    is exploited to reduce the number of running clusters by one at every period,
    compared to the 20-ms SLO setting. Overall, our approach shows a 37.5% improvement
    in cost when the SLO is relaxed to 25 ms as the total number of clusters for all
    periods decreases from 16 to 10 clusters. As these results show, the over-provisioning
    approach is very costly since it requires a total of 18 clusters for all periods,
    even after relaxing the SLO to maintain the system performance. The reason behind
    this improvement is that the cost-effectiveness in our approach, as discussed
    in Section 15, balance performance and cost (where possible) by sacrificing an
    acceptable amount of performance to reduce the cost, that is, they tolerate acceptable
    increases in user-to-data center network latencies to select data centers that
    are moderately distant from users with a lower number of clusters. This sacrifice
    need not result in response times that violate the defined SLOs and upper level
    threshold for the SBVR. On the other hand, the over-provisioning approach only
    considers improving performance by maximizing the reduction in SBVR resulting
    in provisioning more clusters at different data centers close to geo-distributed
    users. From these results, it is evident that geo-elastic deployment with cost
    and latency awareness plays a crucial rule in improving performance under cost
    constraints. 5.2.4 The cost of adaptation In this section, we evaluate the impact
    of the cost of adaptation on geo-elastic deployment approaches. As discussed,
    we refer to the cost here as the number of relocated/new container clusters combined
    with the total inter-data center network latency when a geo-elastic deployment
    solution adapts a given deployment. As illustrated in Figure 5, our geo-elastic
    approach avoids the cost of adaptation at period 3 and at periods, 1 and 3, when
    the SLO set to 20 and 25 ms respectively. Over-provisioning solutions, on the
    other hand, incur costs at periods, 1, 2, 3, and 4, for both SLO settings. It
    should be noted that all approaches have no cost of adaptation at periods, 5 and
    6, because the elastic decision is for geo-shrinking or no change required at
    those periods. The static deployment solution is ignored since it does not have
    an adaptation ability. FIGURE 5 Open in figure viewer PowerPoint Cost of adaptation
    of 2 different geo-elastic deployment approaches against geographic workload changes
    over 6 time periods using various SLOs. Each approach has different SLOs and is
    run 32 times. Overall, in terms of the number of relocated/new clusters, our approach
    reduces the cost of relocating running clusters or adding new clusters by 25%
    for all periods of SLO with 20 ms, compared to the over-provisioning approach
    as shown in Figure 5A. When the SLO is relaxed as shown in Figure 5B, our solution
    takes advantage of this relaxation and decreases the cost by 50%, compared to
    the over-provisioning case. With regards to the total inter-data center network
    latency, as shown in Figure 5C,D, our approach obviously has less inter-data center
    latencies than those of over-provisioning in all cases. Thus our approach is capable
    of speeding up the adaptation of cluster deployments as well as maintaining better
    response times during adaptation. We can conclude that such cost of adaptation
    considerations can help geo-elastic deployment solutions to minimize the number
    of relocated/new clusters as well as the total inter-data center network latencies.
    5.3 Experiment set 2: evaluation of the proposed geo-elastic deployment solution
    in a real cloud context The aim of this experiment set is to evaluate the deployments
    made by the geo-elastic deployment approaches in real distributed-Cloud contexts
    using the NeCTAR Cloud to cope with geo-workload variations. Workloads here are
    realistically generated from different locations. We show that our geo-elastic
    deployment approach is capable of maintaining performance and meeting SLOs (or
    at least minimizing SLO violations) at lower operational costs. A key web application
    metric, end-to-end response times, and the number of operating container clusters
    are used as evaluation metrics for performance and cost. The assumptions considered
    regarding processing times of requests as well as internal-data center communication
    delays are relaxed here because these times and delays vary in real Cloud experiments.
    Since our geo-elastic deployment approach considers performance related to network
    latencies (to improve geo-scalability), we consider mitigating the impact of this
    variation and avoid any possible overload issues by taking the two following steps.
    First, we generate workloads for experiments in a moderated way. Second, we provision
    more resources for each cluster to allow the cluster platform (Kubernetes) auto-scale
    the containers when needed (i.e., providing enough resources as local elasticity
    out of scope). 5.3.1 Experimental set-up SLO and the geo-elastic deployment settings
    We set the SLO to 25 ms. As with standard benchmarks, we use the 95th percentile
    of response time as a benchmark where it should be within 25 ms. We set the upper
    bound of acceptable SBVR, to 5%. Realistic workload generation and request routing
    To generate realistic workloads from the five locations discussed in Section 23,
    we use a workload generator running on a VM at each location and a single workload
    manager running on a separate VM. Each workload generator consists of Locust,
    an open-source modern load testing framework and an agent. The agent runs as a
    daemon and waits for commands from the manager to activate/deactivate the generation
    of user requests. It also checks the health of the target cluster using heartbeats
    and records the response times of requests. We set the number of concurrent user
    sessions at each generator to 80 users. Each user session consists of 10 requests:
    7 read and 3 write requests. The workload manager simulates geo-workload changes
    over different time periods (see Figure 3 and Table 3) and provides a request
    routing service similar to Geo-DNS services. At the beginning of each period,
    the manager selects the required workload generator at that period and sends appropriate
    commands to agents to activate workload generators or deactivate unnecessary workload
    generators related to previous period (if required). The request routing service
    routes traffic for workload generators at each period (first case) and reroutes
    traffic when adapting deployments requires changes in the IP addresses of clusters
    (second case). In the first case, when a workload generator is activated, it asks
    the request routing service for an IP of a cluster (similar to Geo-DNS lookup
    for IPs). The request routing service responds with an IP of a cluster in the
    current deployment at that time with the least network latency. In the second
    case, once a new deployment of clusters takes place, the request routing service
    is updated with new IPs of clusters, if any. Then, if any agent of running workload
    generators does not receive any response from the target cluster, it asks the
    request routing service for a new IP of the cluster. The request routing service
    then responds with a new IP (i.e., redirecting traffic). Sample application and
    container cluster platforms For web application benchmarking, we use a real-world
    transactional web e-Commerce benchmark (TPC-W) application,61 which simulates
    business-oriented activities of an online bookstore. A Java implementation of
    TPC-W is used where the main application components are containerized individually
    using Docker. Those components include Tomcat (v8.5) as a web server, Couchbase
    database (v5.5.0) as a user session manager and MySQL database (v5.7) as the application
    database. A copy of the application data exists at each data center prior to any
    load on the system. Whenever a cluster is required at a data center, 3 VMs are
    provisioned at that data center as cluster nodes. For each node we use Docker
    (v18.06.2)49 as a container Runtime and Kubernetes (v1.16.3)6 as a cluster platform
    with HAProxy load balancer (v1.9) as an ingress controller for Kubernetes. One
    node is a master while the others are workers. The initial deployment of application
    services has 3 web servers, 1 session manager and 1 database deployed as containers.
    Multi-cluster container platform The multi-cluster container platform runs on
    a VM. It has the components for the geo-elastic deployment framework implemented
    in Python. Since this work focuses on decision-making components, only the necessary
    aspects of the action-components are implemented to complete the evaluation. Therefore,
    when adapting a deployment, if the provisioned cluster is with a new condition,
    then a deployment configuration (YAML file) is passed to the master, which pulls
    container images from the image registry. If the cluster is with a migrating condition
    then several steps are necessary. First, a new master VM at the destination data
    center joins the running cluster at the source data center as another master replica
    (to share the current cluster state). Then, the two new destination workers join
    the cluster as worker nodes. The Kubernetes cluster is then instructed to drain
    the two source workers to evict all running pods (containers). In this case, all
    evicted pods will be scheduled to run at the new workers at the destination data
    center. Finally all nodes at the source are removed and hence the cluster relocation
    is complete. 5.3.2 Experimental procedure We run experiments for each approach
    three times. For all experiments, we fix the locations of two clusters for the
    initial deployment. At each run, once the clusters of the initial deployment are
    ready at the pre-determined data centers, their IPs are registered at the request
    routing service. Then, the workload manager iterates over the 6 periods of time.
    At each period, the following steps are repeated. First, the workload manager
    selects workload generators to start generating requests for that period. Next,
    the geo-elastic deployment approach collects log files. It makes scaling decisions
    and actions if needed to adapt the current deployment. Whether the deployment
    has been adapted or not, the request routing service is updated with IPs and the
    number of clusters for that period is recorded. Once the request routing service
    is updated, the workload manager instructs running workload generators to start
    recording response times for the length of the period (1200 seconds with a resolution
    of one second). For the static deployment approach, the workload manager, at each
    period, selects workload generators and records the response times immediately.
    5.3.3 Results and discussions Maintaining performance at lower cost Figure 6 displays
    the 95th percentile of response time as well as the size of the deployment of
    the three approaches. In terms of performance, as Figure 6A shows, the 95th of
    response time of the over-provisioning for every period unsurprisingly satisfies
    the SLO, 25 ms. For our approach, the 95th percentiles of response time, for periods,
    1, 2, and 3, meet the SLO while for the other periods, 4, 5, and 6, they exhibit
    very mild violations (i.e., only 0.5 ms above the SLO). The static approach shows
    higher SLO violations by at least 5 ms for most periods. It should be noted that
    the 95th percentile of response time of the static approach for the first period
    meets the SLO because geo-workloads at the first period were close to the clusters
    of the static deployment approach by chance. FIGURE 6 Open in figure viewer PowerPoint
    Performance and cost comparison of three different approaches for multi-cluster
    deployment running on the Australia-wide NeCTAR Cloud and reacting against geographic
    workload variations over six time periods. At each period, workloads are generated
    from different locations around Australia to simulate geographic changes over
    time. Each period lasts for 20 min (1200s). Each experiments was run three times
    (SLO = 25 ms) With regard to the cost, Figure 6B shows that the improvement in
    performance for over-provisioning comes at high cost for all periods. Our approach,
    when compared to the over-provisioning solution, shows a noticeably lower cost
    at every period. This gives a justification for our approach, that is, we sacrifice
    small amounts of performance to reduce costs. In more detail, we improve the cost-effectiveness
    by reducing the size of deployment by 1 at periods (1, 2, 5, and 6) and by 2 at
    periods (3 and 4) compared to the over-provisioning approach. In other words,
    costs are reduced by 33% at periods (2 and 5) and by 50% at periods (1, 3, 4,
    and 6). Table 4 shows the results comparing the three approaches using t-test.
    This indicates that over-provisioning and our approach, on average improve the
    response time by 5.89 ms and 3.16 ms respectively compared to the static approach.
    While this performance improvement requires an over-provisioning solution that
    increases the cost by 50%, our approach reduces the cost by approximately 16%.
    Moreover, compared to the over-provisioning approach, our approach incurs only
    minor delays in response time, on average only 2.71 ms whilst reducing the cost
    by 44.44%. As a consequence, it is evident that our approach has the ability to
    preserve performance and minimize SLO violations with at greatly reduced cost.
    TABLE 4. The difference in response time and cost for different approaches for
    all time periods. Each approach runs 3 times. Geo: Geo-elastic (proposed). OP:
    Over-provisioning Difference in response time mean in ms Difference in cost 95%
    CI Estimate -value % Geo-Static (3.24, 3.07) 3.16 0.00 2 16.67 OP  -Static (5.95,
    5.81) 5.89 0.00    6     50.00 Geo-OP ( 2.68,  2.77)     2.72 0.00 8 44.44 5.4
    Experiment set 3: evaluation of the proposed cluster replacement algorithm In
    this experiment set, we evaluate the genetic algorithm-based cluster replacement
    algorithm. Experiments here are classified into two groups. In the first group,
    the aim of the experiments is to tune the genetic algorithm parameters for our
    problem. In the second group, we show the benefits of network latency awareness
    when (re)placing clusters as well as examine the performance of genetic algorithm.
    We use SBVR and the execution time as key evaluation metrics. For all experiments
    here, we set problem-specific parameters as introduced in the next section. 5.4.1
    Problem-specific parameter settings We set SLO and processing time to 20 ms and
    10 ms, respectively. For the workload, , we choose five locations to generate
    workload and set the number of user sessions to 240 per location, that is, we
    have a total of 1200 users. The size and SBVR of the current deployment is 3%
    and 76.40% respectively. The number of required clusters for the new deployment
    is set to 4. As mentioned before, there are 60 potential data centers available,
    hence we have a total of 487,635 candidate deployments (feasible solutions). 5.4.2
    Group 1: genetic algorithm parameter tuning Two key parameters of the genetic
    algorithm are the crossover rate, , and the mutation rate, . These need to be
    tuned for the cluster replacement problem. Therefore, we run 16 experiments with
    the genetic algorithm where each experiment has a different parameter setting.
    To obtain these 16 parameter settings, we combine different values of the two
    parameters. The values for and are set to 0.2, 0.5, 0.7, and 0.9 and to 0.1, 0.2,
    0.5, 0.7 respectively. Tables 5 shows the best five parameter settings for the
    genetic algorithm ordered based on their speed rate (i.e., number of iterations)
    to converge on the problem over the 32 runs. From the results, it is evident that
    the fastest rate of convergence occurs when setting and to 0.7 and 0.2 respectively.
    The convergence curve using these parameter settings is indicated in Figure 7A.
    TABLE 5. Estimate and standard error (SE) of the average fitness and SBVR for
    genetic algorithm (GA) over 32 runs as well as the number of generations for all
    runs to converge using different GA parameter settings. The higher the fitness
    the better. The lower the the better. GA parameters Iterations to Convergence
    (all runs) Average fitness Average SBVR (%) Crossver Mutation Estimate SE Estimate
    SE 0.7 0.2 54 0.89 0.02 7.39 0.09 0.5 0.1 61 0.9 0.02 7.46 0.09 0.7 0.1 61 0.87
    0.02 7.33 0.09 0.7 0.7 68 0.85 0.02 7.22 0.08 0.5 0.5 88 0.88 0.02 7.32 0.09 FIGURE
    7 Open in figure viewer PowerPoint Convergence curve and SBVR improvement for
    proposed genetic algorithm. The algorithm is run 32 times and converges at the
    54th iteration. The higher the objective function value, the better (the optimal
    value is -0.52). The lower the SBVR the better (the optimal SBVR value is 5.6%).
    5.4.3 Group 2: effectiveness of genetic algorithm Proposed algorithms and baselines
    settings Genetic algorithm parameter settings were discussed in Section 24. Regarding
    the baselines, we have two baseline algorithms: Brute force algorithm, which examines
    all possible solutions in the solution space and latency unaware algorithm, which
    does not consider network latency and randomly selects data centers for cluster
    (re)placement. For stochastic algorithms genetic algorithm and latency unaware,
    we run experiments on each algorithm 32 times. We also run the deterministic algorithm,
    brute force, once. The algorithms run independently on the VMs. Impact of network
    latency consideration Table 6 indicates that the latency unaware algorithm shows
    a very high violation rate in SBVR, 90.41%, while our genetic algorithm, which
    takes into account network latency, has very low SBVR violation rates, 7.39%.
    As shown in Table 7, the genetic algorithm improves the performance since it reduces
    SBVR by at least 83.02%. In other words, compared to latency unaware algorithm,
    the genetic algorithm achieves 91.83% improvement in SBVR. It can therefore clearly
    be concluded that network latency considerations reduce SLO violation rates and
    hence improve performance. TABLE 6. 95% confidence interval (CI), estimate and
    standard error (SE) of the mean of the objective function value and for different
    algorithms run 32 times. The higher the objective function value, the better.
    The lower the the better. The optimal SBVR value is 5.6%. GA: Genetic Algorithm.
    LU: Latency Unaware. Algorithm Objective function value SBVR (%) 95% CI Estimate
    SE 95% CI Estimate SE GA (0.96, 0.82)   0.89 0.04 (7.03, 7.75)    7.39 0.18 LU
    (19.22, 17.21) 18.22 0.51 (85.46, 95.36) 90.41 2.53 TABLE 7. 95% confidence interval
    (CI), estimate and -value of the difference in means of the objective function
    value and between different algorithms. Each algorithm ran 32 times. GA: Genetic
    Algorithm. LU: Latency Unaware. Difference in Mean Objective function value SBVR
    (%) 95% CI Estimate -value 95% CI Estimate -value GA-LU (16.29, 18.37) 17.33 0.00
    (88.16, 77.88) 83.02 0.00 The performance of genetic algorithm In this section,
    we examine the performance of the proposed genetic algorithm from two aspects:
    speed and accuracy. For speed, we use two metrics: the execution time (in seconds)
    as well as rate of convergence (number of iterations to converge for all runs).
    Regarding the accuracy, we evaluate the accuracy of the algorithm by measuring
    the average SBVR and determining how far this average differs over iterations
    from the optimal SBVR. The optimal SBVR value, which is obtained from the brute
    force algorithm, is 5.6%. With regards to execution time, the execution time of
    brute force algorithm is 27,859s (approximately 8 h). The estimate and 95% confidence
    interval (CI) of the average execution time over the 32 runs for the genetic algorithm
    are 347.6s (about 6 min) and (347.11s, 348.08s) respectively. Compared to brute
    force algorithm, the genetic algorithm is dramatically faster. Regarding the rate
    of convergence, as Figure 7A shows, the genetic algorithm converges at the 54th
    iteration. With respect to accuracy, as shown in Tables 6 and 7, the distance
    for our genetic algorithm to the optimal value is only 1.79%. Moreover, as indicated
    in Figure 7B, the genetic algorithm obviously meets the upper bound of acceptable
    violation rate for all runs at the 18th iteration. This shows that it provides
    acceptable level of accuracy. We can conclude that the genetic algorithm achieves
    good quality of solutions. 6 CONCLUSIONS AND FUTURE DIRECTIONS We have proposed
    a geo-elastic container deployment approach for multi-cluster deployment that
    leverages the capabilities of distributed, potentially global scale Cloud environments
    to elastically and intelligently scale web applications. The approach enables
    container platforms to automatically adapt the deployment of container clusters
    based on geographically diverse workload variations. The aim is to maintain system
    performance to meet/support SLOs even during the adaptation process, whilst minimizing
    operational costs. For cluster replacement, a genetic algorithm, considering proximity
    to users and the cost of adaptation, that is, the number of relocated/new clusters
    and inter-data center latencies, was explored. A heuristic for cluster quantity
    adjustment was presented. We also presented a framework to show how automated
    elastic multi-cluster deployment is enabled. To evaluate our approach we carried
    out extensive experiments on the NeCTAR Research Cloud using Kubernetes clusters
    and TPC-W web application and demonstrated optimal deployment solutions that minimize
    cost and meet performance demands. Our future work will focus on cross-cluster
    resource management as Cloud-based elasticity solutions are not suited to handle
    unexpected, large scale and bursty overloads due to the overheads of provisioning
    VMs. This overhead usually lasts for a few minutes before the cluster node is
    ready to run new containers. During this time, users requests may be dropped or
    they may experience delays in response times. To handle this problem, we intend
    to propose a cross-cluster resource management mechanisms that allows overloaded
    clusters to borrow already-running (idle) VMs from other clusters with normal
    or reduced loads in different Cloud locations. This mechanism is more suited to
    handle sudden spikes in loads due to the warm-started VMs. Open Research REFERENCES
    Volume33, Issue21 10 November 2021 e6436 Figures References Related Information
    Recommended Enabling Docker for HPC Jonathan Sparks Concurrency and Computation:
    Practice and Experience Implementing Hadoop Container Migrations in OpenNebula
    Private Cloud Environment P. Kalyanaraman,  K.R. Jothi,  P. Balakrishnan,  R.G.
    Navya,  A. Shah,  V. Pandey Role of Edge Analytics in Sustainable Smart City Development:
    Challenges and Solutions, [1] Elastic management of web server clusters on distributed
    virtual infrastructures Rafael Moreno-Vozmediano,  Ruben S. Montero,  Ignacio
    M. Llorente Concurrency and Computation: Practice and Experience A formal approach
    for Docker container deployment Mahendra Pratap Yadav,  Nisha Pal,  Dharmendra
    Kumar Yadav Concurrency and Computation: Practice and Experience The state‐of‐the‐art
    in container technologies: Application, orchestration and security Emiliano Casalicchio,  Stefano
    Iannucci Concurrency and Computation: Practice and Experience Download PDF Additional
    links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About Cookies Manage
    Cookies Accessibility Wiley Research DE&I Statement and Publishing Policies Developing
    World Access HELP & SUPPORT Contact Us Training and Support DMCA & Reporting Piracy
    OPPORTUNITIES Subscription Agents Advertisers & Corporate Partners CONNECT WITH
    WILEY The Wiley Network Wiley Press Room Copyright © 1999-2024 John Wiley & Sons,
    Inc or related companies. All rights reserved, including rights for text and data
    mining and training of artificial technologies or similar technologies.'
  inline_citation: '>'
  journal: Concurrency and computation
  limitations: '>'
  pdf_link: null
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: Elastic deployment of container clusters across geographically distributed
    cloud data centers for web applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.12783/dtetr/icca2016/6039
  analysis: '>'
  authors:
  - Roman Sorokin
  - Vinay Singla
  citation_count: 1
  full_citation: '>'
  full_text: '>

    USER Username Password Remember me    FONT SIZE  Login Register Home > DPI Proceedings  STRUCTURAL
    HEALTH MONITORING 2023 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER PROCEEDINGS
    OF THE AMERICAN SOCIETY FOR COMPOSITES-THIRTY-EIGHTH TECHNICAL CONFERENCE VIEW
    PROCEEDINGS | CURRENT ISSUE | REGISTER PROCEEDINGS OF THE AMERICAN SOCIETY FOR
    COMPOSITES-THIRTY-SEVENTH TECHNICAL CONFERENCE VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER STRUCTURAL HEALTH MONITORING 2021 VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER THERMAL CONDUCTIVITY 34/THERMAL EXPANSION 22 VIEW PROCEEDINGS | CURRENT
    ISSUE | REGISTER 32ND INTERNATIONAL SYMPOSIUM ON BALLISTICS VIEW PROCEEDINGS |
    CURRENT ISSUE | REGISTER 2021 3RD INTERNATIONAL WORKSHOP ON STRUCTURAL HEALTH
    MONITORING FOR RAILWAY SYSTEM(IWSHM-RS 2021) VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER PROCEEDINGS OF THE AMERICAN SOCIETY FOR COMPOSITESÂ€”THIRTY-SIXTH TECHNICAL
    CONFERENCE ON COMPOSITE MATERIALS VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    ASPHALT PAVING TECHNOLOGY 2018 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER 31ST
    INTERNATIONAL SYMPOSIUM ON BALLISTICS VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    STRUCTURAL HEALTH MONITORING 2019 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    THERMAL CONDUCTIVITY 33/THERMAL EXPANSION 21 VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER DESIGN, MANUFACTURING AND APPLICATIONS OF COMPOSITES - TWELFTH JOINT
    CANADA-JAPAN WORKSHOP ON COMPOSITES VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    PROCEEDINGS OF THE AMERICAN SOCIETY FOR COMPOSITES Â€” THIRTY-FOURTH TECHNICAL
    CONFERENCE VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER 2ND INTERNATIONAL WORKSHOP
    ON STRUCTURAL HEALTH MONITORING FOR RAILWAY SYSTEM (IWSHM-RS 2018) VIEW PROCEEDINGS
    | CURRENT ISSUE | REGISTER PROCEEDINGS OF THE AMERICAN SOCIETY FOR COMPOSITES
    Â€” THIRTY-THIRD TECHNICAL CONFERENCE VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    THE 21ST IAPRI WORLD CONFERENCE ON PACKAGING VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER MECHANICAL BEHAVIOR OF THICK COMPOSITES VIEW PROCEEDINGS | CURRENT
    ISSUE | REGISTER PARALLEL COMPUTATIONAL FLUID DYNAMICS VIEW PROCEEDINGS | CURRENT
    ISSUE | REGISTER THERMAL CONDUCTIVITY 31 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    STRUCTURES IN FIRE VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER SIXTH INTERNATIONAL
    CONFERENCE ON NONLINEAR MECHANICS (ICNM-VI) VIEW PROCEEDINGS | CURRENT ISSUE |
    REGISTER ADVANCES IN HETEROGENEOUS MATERIAL MECHANICS 2011 VIEW PROCEEDINGS |
    CURRENT ISSUE | REGISTER DESIGN, MANUFACTURING AND APPLICATIONS OF COMPOSITES
    2014 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER DESIGN, MANUFACTURING AND APPLICATIONS
    OF COMPOSITES 2012 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER THE WATERBORNE
    COATINGS SYMPOSIUM 2013 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER THE WATERBORNE
    COATINGS SYMPOSIUM 2012 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER STRUCTURAL
    HEALTH MONITORING 2013 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER STRUCTURAL
    HEALTH MONITORING 2011 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER RESPONSE OF
    STRUCTURES UNDER EXTREME LOADING (PROTECT 2015) VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER ASPHALT PAVING TECHNOLOGY 2017 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    AUTOMATED COMPOSITES MANUFACTURING - THIRD INTERNATIONAL SYMPOSIUM VIEW PROCEEDINGS
    | CURRENT ISSUE | REGISTER 30TH INTERNATIONAL SYMPOSIUM ON BALLISTICS VIEW PROCEEDINGS
    | CURRENT ISSUE | REGISTER PROCEEDINGS OF THE AMERICAN SOCIETY FOR COMPOSITES
    Â€” THIRTY-SECOND TECHNICAL CONFERENCE VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    STRUCTURAL HEALTH MONITORING 2017 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    ASPHALT PAVING TECHNOLOGY 2016 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER DESIGN,
    MANUFACTURING AND APPLICATIONS OF COMPOSITES VIEW PROCEEDINGS | CURRENT ISSUE
    | REGISTER 1ST INTERNATIONAL WORKSHOP ON STRUCTURAL HEALTH MONITORING FOR RAILWAY
    SYSTEM VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER PROCEEDINGS OF THE AMERICAN
    SOCIETY FOR COMPOSITES: THIRTY-FIRST TECHNICAL CONFERENCE VIEW PROCEEDINGS | CURRENT
    ISSUE | REGISTER DURACOSYS VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER OTHER TECHNICAL
    ONLINE PROCEEDINGS VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER 29TH INTERNATIONAL
    SYMPOSIUM ON BALLISTICS VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER AMERICAN SOCIETY
    OF COMPOSITES - 30TH TECHNICAL CONFERENCE VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    STRUCTURAL HEALTH MONITORING 2015 VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER
    COMPOSITE SCIENCE AND TECHNOLOGY VIEW PROCEEDINGS | CURRENT ISSUE | REGISTER'
  inline_citation: '>'
  journal: DEStech transactions on engineering and technology research
  limitations: '>'
  pdf_link: http://dpi-proceedings.com/index.php/dtetr/article/download/6039/5650
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: 'Microservices Architecture for Tenant-oriented Cloud-based Control Software
    Services: POC and Proposals'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/mcc.2015.51
  analysis: '>'
  authors:
  - Claus Pahl
  citation_count: 296
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Cloud Computing >Volume:
    2 Issue: 3 Containerization and the PaaS Cloud Publisher: IEEE Cite This PDF Claus
    Pahl All Authors 285 Cites in Papers 11 Cites in Patents 9655 Full Text Views
    Abstract Document Sections Virtualization and the Need for Containerization Containerization
    for Lightweight Virtualization and Application Packaging Containerization in Paas
    Clouds Container Orchestration and Clustering Authors Figures References Citations
    Keywords Metrics Abstract: Containerization is widely discussed as a lightweight
    virtualization solution. Apart from exhibiting benefits over traditional virtual
    machines in the cloud, containers are especially relevant for platform-as-a-service
    (PaaS) clouds to manage and orchestrate applications through containers as an
    application packaging mechanism. This article discusses the requirements that
    arise from having to facilitate applications through distributed multicloud platforms.
    Published in: IEEE Cloud Computing ( Volume: 2, Issue: 3, May-June 2015) Page(s):
    24 - 31 Date of Publication: 15 July 2015 ISSN Information: DOI: 10.1109/MCC.2015.51
    Publisher: IEEE The cloud relies on virtualization techniques to achieve elasticity
    of large-scale shared resources. Virtual machines (VMs) have been the backbone
    at the infrastructure layer providing virtualized operating systems (OSs). Containers
    are a similar but more lightweight virtualization concept; they''re less resource
    and time-consuming, thus they''ve been suggested as a solution for more interoperable
    application packaging in the cloud. Although VMs and containers are both virtu-alization
    techniques, they solve different problems. Containers are tools for delivering
    software-that is, they have a platform-as-a-service (PaaS) focus-in a portable
    way aiming at greater interoperability while still utilizing OS virtualization
    principles.[1] VMs, on the other hand, are about hardware allocation and management
    (machines that can be turned on/off and be provisioned)-that is, there''s an infrastructure-as-a-service
    (IaaS) focus on hardware virtualization. Containers can be used as a replacement
    for VMs where the allocation of hard-ware resources is done through containers
    by componentizing workloads in between clouds. For portable, interoperable applications
    in the cloud, we need a lightweight distribution of packaged applications for
    deployment and management.[2] One solution, containerization, provides a lightweight
    portable runtime; the capability to develop, test, and deploy applications to
    a large number of servers; and the capability to interconnect containers. David
    Bernstein discusses the importance of container-based application deployment and
    cluster management for the cloud computing infra-structure.[3] This article reviews
    the virtualization principles behind containers, particularly in comparison with
    VMs. Specifically, I investigate the relevance of the new container technology
    for PaaS clouds, although containers also relate to the IaaS level through their
    sharing and isolation aspects. Because today''s applications are distributed,
    I also discuss the resulting requirements for application packaging and interoperable
    orchestration over clusters of containers. I aim to clarify how containers can
    change the PaaS cloud as a virtualization technique, specifically PaaS as a platform
    tech-nology. I go beyond Bernstein,[3] addressing what''s needed to evolve PaaS
    significantly further as a distributed cloud software platform, resulting in a
    discussion of achievements and limitations of the state of the art. To illustrate
    concepts, I''ll discuss some example technologies that exemplify technology trends.
    Virtualization and the Need for Containerization Historically, virtualization
    technologies have developed out of the need for scheduling processes as manageable
    container units. The processes and resources in question are the file system,
    memory, network, and system information. VMs as the cloud''s core virtualization
    construct have been improved successively by addressing scheduling, packaging,
    and resource access (security) problems. VM instances acting as guests use large,
    isolated files on their hosts to store their entire file system and typically
    run a single, large process on the host. Although security concerns are usually
    addressed through isolation, several limitations remain. Full guest OS images
    are required for each VM in addition to the binaries and libraries necessary for
    the applications. Full images create a space concern that translates into RAM
    and disk storage requirements and is slow on startup (booting might take from
    1 to more than 10 minutes[4]) as in Figure 1, which shows the different architectural
    settings. Packaging and application management is a requirement that PaaS clouds
    need to address. In a virtualized environment, a solution must be grounded in
    technologies that allow the sharing of the underlying platform and infrastructure
    in a secure but also portable and interoperable way. Containers can meet these
    requirements, but a more in-depth elicitation of specific concerns is needed.
    A container holds packaged, self-contained, ready-to-deploy parts of applications
    and, if neces-sary, middleware and business logic (in binaries and libraries)
    to run applications,[5] as Figure 1 il-lustrates. An example is a Web interface
    component with a Tomcat server. Successful tools like Docker are frameworks built
    around container engines that allow container engines to act as a portable mechanism
    to package and run applications as containers.[6] This means that a container
    covers an application tier or node in a tier, which results in the problem of
    managing dependencies between containers in mul-titier applications. An orchestration
    plan describes components, their dependencies, and their life cy-cle. A PaaS then
    enacts the workflows from the plan through agents (which could be a container
    runtime engine). PaaSs can support the deployment of applications from containers.
    Figure 1. Virtualization architecture. The two possible scenarios, a traditional
    hypervisor architecture on the left and a container-based architecture on the
    right, differ in their management of guest operating system components. Show All
    In PaaSs, there''s a need to define, deploy, and operate cross-platform-capable
    cloud services using lightweight virtualization, for which containers are a solution.[7]
    There''s also a need to transfer cloud deployments between cloud providers, which
    requires lightweight virtualized clusters for container orches-tration.[3] Some
    PaaSs are lightweight virtualization solutions in this sense. Containerization
    for Lightweight Virtualization and Application Packaging Recent OS advances have
    improved their multi-tenancy capabilities—that is, the capability to share a resource.
    Linux Containers As an example of OS virtualization advances, new Linux distributions
    provide kernel mechanisms such as namespaces and control groups to isolate processes
    on a shared OS, supported through the Linux Container (LXC) project. N amespace
    isolation allows groups of processes to be separated, preventing them from seeing
    resources in other groups. Container technologies use different namespaces for
    process isolation, network interfaces, access to interprocess communication, and
    mount points, and for isolating kernel and version identifiers. Control groups
    manage and limit resource access for process groups through limit enforcement,
    accounting, and isolation—for example, by limiting the memory available to a specific
    container. This ensures that containers are good multitenant citizens on a host.
    It also provides better isolation between possibly large numbers of isolated applications
    on a host. Control groups allow containers to share available hardware resources
    and, if required, the control groups can set up limits and constraints. Figure
    2. Container image architecture. Based on namespace and cgroup extensions of a
    Linux kernel, images are layered over each other, with a writable container image
    at the top. Show All Docker builds its solution on LXC techniques. A container-aware
    daemon, such as dockerd for Dock-er, can start containers as application processes
    and plays a key role as the root of the user space''s process tree. Docker Container
    Images Containers are OS virtualization techniques based on namespaces and cgroups
    and are particularly suitable for application management in the PaaS cloud. A
    container is represented by lightweight images; VMs are also based on images but
    full, monolithic ones. Processes running in a container are almost fully isolated.
    Container images are the building blocks from which containers are launched. Because
    it''s currently the most popular container solution, I''ll use Docker to illustrate
    how containerization works. A Docker image is made up of file systems layered
    over each other, similar to the Linux virtualization stack, using the LXC mechanisms,
    as Figure 2 illustrates. In a traditional Linux boot, the kernel first mounts
    the root file system as read-only, then checks its integrity before switching
    the rootfs volume to read-write mode. Docker mounts the rootfs as read-only as
    in a traditional boot, but instead of changing the file system to read-write mode,
    it uses a union mount to add a writable file system on top of the read-only file
    system. There might be multiple read-only file systems stacked on top of each
    other. Using union mount, several file systems can be mounted on top of each other,
    which allows for creating new images by building on top of base images. Each of
    these file system layers is a separate image loaded by the container engine for
    execution. Only the top layer is writable. This is the container itself, which
    can have state and is executable. It can be thought of as a directory that contains
    everything needed for execution. Containers can be made into stateless images
    (and reused in more complex builds), however. A typical layering could include
    (top to bottom in Figure 2) a writable container image for applications, an Apache
    image and an Emacs image as sample platform components, a Linux image (a distribution
    such as Ubuntu), and the rootfs kernel image. Containers are based on layers composed
    from individual images built on top of a base image that can be extended. Complete
    Docker images form portable application containers. They''re also building blocks
    for application stacks. The approach is lightweight because single images can
    be changed and distributed easily. Containerizing Applications and Managing Containers
    The container ecosystem consists of an application container engine to run images
    and a repository or registry operated via push and pull operations to transfer
    images to and from host-based engines. The repositories play a central role in
    providing access to possibly tens of thousands of reusable private and public
    container images, such as for platform components like MongoDB or Node.js. The
    container API allows creating, defining, composing, and distributing containers,
    running/starting im-ages, and running commands in images. Containers for applications
    can be created by assembling them from individual images, possibly based on base
    images from the repositories, as in Figure 2, which shows a containerized application.
    Containers can encapsulate several application components through the image layering
    and extension process. Different user applications and platform components can
    be combined in a container. Figure 3. Container-based application architectures.
    These illustrate different architectural configurations with apps running on top
    of (a) management components such as load balancers and autoscalers, (b and c)
    binaries and libraries, and (d) databases. Show All Figure 3 shows different scenarios
    using the container capability of combining images for platform and application
    components. The granularity of containers (that is, the number of applications
    inside a container) varies. Some favor the one-container-per-app approach, which
    still allows composing new stacks easily (for example, changing the webserver
    in an application) or reusing common components (for example, monitoring tools
    or a single storage service like memcached, either locally or predefined from
    a repository such as the Docker Hub). Apps can be built or rebuilt and managed
    easily. The downside is a larger number of containers with the respective interaction
    and management overhead compared to multi-app containers, although container efficiency
    should facilitate this. Containers as application packages for interoperable and
    distributed contexts must facilitate storage and network management. There are
    two ways data is managed in Docker-data volumes and data volume containers. Data
    storage features can add data volumes to any container created from an im-age.
    A data volume is a specially designated directory within one or more containers
    that bypasses the union file system to provide features for persistent or shared
    data. Volumes can be shared and reused between containers, as Figure 4 illustrates.
    A data volume container enables sharing persistent data between application containers
    through a dedicated, separate data storage container. Figure 4. Container-based
    cluster architecture. Clusters assemble host nodes with container and data volumes,
    joined through links. Show All Network management is based on two methods for
    assigning ports on a host—network port mappings and container linking. Applications
    can connect to a service or application running inside a Docker container via
    a network port. Container linking allows linking multiple containers together
    and sending information between them. Linked containers can transfer data about
    themselves via environment variables. To establish links and some relationship
    types, Docker relies on containers'' names, which must be unique, meaning that
    links are often limited to containers of the same host (managed by the same daemon).
    Comparison Table 1 compares traditional VMs and containers. Some sources are also
    concerned about security, suggesting that it''s preferable to run, for instance,
    only one Docker instance per host to avoid isolation limitations.[3] Different
    Container Models A range of other container technologies exist for different operating
    systems types (I single out Linux and Windows here) as well as specific or generic
    solutions for PaaS platforms[8]: Linux (Docker, LXC, OpenVZ, and others for variants
    such as BSD, HP-UX, and Solaris), Windows (Sandboxie), and Cloud PaaS (Warden/Garden
    (in Cloud Found-ry) and LXC (in OpenShift). Table 1. Virtual machine versus container-based
    application architectures There''s still an ongoing evolution of OS virtualization
    and containerization, aiming at providing OS support through standard APIs and
    tools for container management, network management, and more visible and manageable
    resource utilization. The tool landscape is equally in evolution. For example,
    Rocket is a new container runtime from the CoreOS project (CoreOS is Linux for
    massive server deployments), which is an alternative to the Docker runtime. It''s
    specifically designed for com-posability, security, and speed. These concerns
    highlight the teething concerns that the community is still engaged with. Containerization
    in Paas Clouds Although VMs are ultimately the medium to provision PaaS platform
    and application components at the infrastructure layer, containers appear to be
    more suitable for application packaging and management in PaaS clouds. PaaS Features
    A PaaS generally provides mechanisms for deploying applications, designing applications
    for the cloud, pushing applications to their deployment environment, using services,
    migrating databases, mapping custom domains, IDE plugins, or a build integration
    tool. PaaSs have features such as built farms, routing layers, or schedulers that
    dispatch workloads to VMs. A container solution supports these problems through
    interoperable, lightweight, and virtualized packaging. Containers for application
    building, deployment, and management (through a runtime) provide interoperability.
    Containers produced outside a PaaS can be moved into the PaaS so that the container
    encapsulates the application. Existing PaaSs have embraced the momentum caused
    by containerization and standardized application packaging driven by Docker. Many
    PaaSs have a container foundation for running platform tools. PaaS Evolution The
    evolution of PaaS is moving toward container-based, interoperable PaaSs. The first
    generation consisted of classical fixed proprietary platforms such as Azure or
    Heroku. The second generation was built around open source solutions such as Cloud
    Foundry and OpenShift, which let users run their own PaaS (on-premise or in the
    cloud), already built around containers. OpenShift has now adopted the Docker
    container model, as has Cloud Foundry through its internal Diego solution. The
    current third generation includes platforms such as Dawn, Deis, Flynn, Octohost,
    and Tsuru, which are built on Docker from scratch and are deployable on a company''s
    own servers or on public IaaS clouds. Open PaaSs such as Cloud Foundry and Open-Shift
    treat containers differently, however. Whereas Cloud Foundry supports stateless
    applications through containers, stateful services run in VMs. OpenShift doesn''t
    distinguish these. Service Orchestration Development and architecture are central
    PaaS concerns. Recently developed micro service architectures break monolithic
    application architectures into service-oriented architecture (SOA)-style independently
    deployable services, which are well supported by container architectures. Services
    are loosely coupled, independent, and can be rapidly called and mapped to whatever
    business process is required. The microservice architectural style is an approach
    to developing a single application as a suite of small services, each running
    in its own process and communicating with lightweight mecha-nisms. These services
    are independently deployable by a fully automated deployment and orchestration
    framework. They must be able to deploy often and independently at arbitrary schedules,
    instead of requiring synchronized deployments at fixed times. Containerization
    provides an ideal mechanism for their deployment and orchestration, particularly,
    if they''re to be PaaS-provisioned. Container Orchestration and Clustering Containerization
    facilitates the step from a single host to clusters of container hosts to run
    containerized applications over multiple clusters in multiple clouds.[9] The built-in
    interoperability makes this possible. Container Clusters A container-based cluster
    architecture groups hosts in clusters.[10] Figure 4 illustrates an abstract architectural
    scenario based on common container and cluster concepts. Container hosts are linked
    in a cluster configuration: Each cluster consists of several (host) nodes, where
    nodes are virtual servers on hypervisors or possibly bare-metal servers. Each
    host node holds several containers with common services such as scheduling, load
    balancing, and applications. Each container can hold continually provided services
    such as their payload service, which are one-off services (for example, print)
    or functional (middleware service) components. Application services are logical
    groups of containers from the same image. Application services allow scaling an
    application across nodes. Volumes are used for applications requiring data persistence.
    Containers can mount volumes. Data stored in these volumes persists even after
    a container is terminated. Links allow two or more containers, typically on a
    single host, to connect and communicate. This configuration creates an abstraction
    layer for cluster-based service management that goes beyond container solutions
    like Docker. A cluster management architecture has the following components: The
    deployment of distributed applications through containers is supported using a
    virtual scalable service node (cluster) with high internal complexity (supporting
    scaling, load balancing, failover) and reduced external complexity. An API allows
    operating clusters from the creation of services and container sets to other lifecycle
    functions. A platform service manager looks after the software packaging and management.
    An agent manages the container life cycles (at each host). A cluster head node
    service is the master that receives commands from the outside and relays them
    to container hosts. This architecture allows development without regard to the
    network topology and requires no manual configurntion.[11] A cluster architecture
    is composed of engines to share service discovery (for example, through shared
    distributed key-value stores) and orchestration/de-ployment (load balancing, monitoring,
    scaling, and also file storage, deployment, pushing, and pulling). This satisfies
    some of the requirements N ane Kratzke lists for cluster architectures.[8] A lightweight
    virtualized cluster architecture should provide several management features as
    part of the abstraction on top of the container hosts: hosting containerized services
    and providing secure communication between these services, autoscalability and
    load-balancing support, distributed and scalable service discovery and orchestration,
    and transfer/migration of service deployments between clusters. Mesos is an example
    of a cluster management platform. This Apache project binds distributed hardware
    resources into a single pool of resourc-es. Application frameworks can use Mesos
    to efficiently manage workload distribution. Mesos is a distributed systems kernel
    following the same principles as the Linux kernel but at a different level of
    abstraction. The Mesos kernel runs on all cluster machines and provides applications
    with APIs for resource management and scheduling across cloud environments. It
    natively supports LXC and also supports Docker. An example clustering management
    solution at a higher level than Mesos is the Kubernetes architecture, which is
    supported by Google. Kubernetes can be configured to allow orchestrating Docker
    containers on Mesos at scale. Kubernetes is based on processes that run on Docker
    hosts that bind hosts into clusters and manage containers. Minions are container
    hosts that run pods (that is, sets of con-tainers) on the same host. OpenShift
    has adopted Kubernetes. Google expertise incorporated in Ku-bernetes competes
    here with platform-specific evolution toward container-based orchestration. Cloud
    Foundry, for instance, uses Diego as an orchestration engine for containers. Figure
    5. Cluster topology orchestration (adapted from the Topology and Orchestration
    Specification for Cloud Applications [TOSCA] by applying the generic TOSCA service
    template to the container and cluster technology context). Show All Network and
    Data Challenges Containers in distributed systems require advanced network support.
    Containers provide an abstraction that makes each container a self-contained unit
    of computation. Traditionally, containers were exposed on the network via the
    shared host machine''s address. In Kubernetes, each group of containers (or pods)
    receives its own unique IP address, reachable from any other pod in the cluster,
    whether colocated on the same physical machine or not. This requires advanced
    routing features based on network virtualization. Data storage is another problem
    in distributed container management. Managing containers in Kubernetes clusters
    might be hampered in terms of flexibility and efficiency by the need for pods
    to co-locate with their data. What is needed is to pair a container with a storage
    volume that, regardless of the container''s location in the cluster, follows it
    to the physical machine. Orchestration Scenarios Container cluster-based multi-PaaS
    is a solution for managing distributed software applications in the cloud, but
    this technology still faces challenges. These include formal descriptions or user-defined
    metadata for containers beyond image tagging with simple IDs but also clusters
    of containers and their orchestration. The topology of distributed container architectures
    needs to be specified and its deployment and execution orchestrated, as Figure
    5 illustrates. There''s currently no accepted solution for the orchestration problem;
    however, I briefly illustrate its relevance using a possible solution. Although
    Docker has started to develop its own orchestration solution and Kubernetes also
    provides an orchestration mechanism for containers onto nodes, a more comprehensive
    solution that would tackle orchestration of complex application stacks could involve
    Docker orchestration based on the Topology and Orchestration Specification for
    Cloud Applications (TOSCA),[12] a topology-based service orchestration standard
    that''s supported, for example, by the Cloudify PaaS. Cloudify uses TOSCA to enhance
    the portability of cloud applications and services (see Figure 5). TOSCA enables
    the interoperable description of application and infrastructure cloud services
    (here, containers hosted on nodes), the relationships between parts of the service
    (here, service compositions and links, as illustrated in Figure 4), and the operational
    behavior of these services (for example, deploy, patch, and shutdown) in an orchestration
    plan. The TOSCA framework is independent of the supplier creating the service
    and any particular cloud provider or hosting technology. TOSCA will also make
    it possible to associate higher-level operational behavior with cloud infrastructure
    man-agement. Using TOSCA templates for container clusters and abstract node and
    relationship types, an application stack template can be specified. Observations
    Some PaaSs have started to address limitations in the context of programming (such
    as orchestration) and DevOps for clusters. The examples I''ve used allow for some
    observations. First, containers are by now largely adopted for PaaS clouds.[3]
    Second, standardization through adoption of emerging de facto standards such as
    Docker or Kubernetes is also taking place, although at a slower pace. Third, development
    and operations are still at an early stage. Cloud management platforms are still
    at an earlier stage than the container platforms they build on. Whereas clusters
    in general are about distribution, the question emerges as to what extent this
    distribution reaches the edge of the cloud with small devices and embedded systems
    and whether devices running small Linux distributions such as the Debian-based
    DSL (which requires around 50 Mbytes of storage) can support container host and
    cluster management. Container technology has a huge potential to substantially
    advance PaaS technology toward distributed heterogeneous clouds through light-weightness
    and interoperability, as Bernstein and other have recognized.[3] However, we still
    need significant improvements to deal with data and network management aspects
    as well as an abstract development and architecture layer. ACKNOWLEDGMENT This
    work was supported in part by the Irish Centre for Cloud Computing and Commerce
    (IC4), an Irish National Technology Centre funded by Enterprise Ireland and the
    Irish Industrial Development Authority, and by Science Foundation Ireland grant
    13/RC/2094 to Lero, the Irish Software Research Centre. Authors Figures References
    Citations Keywords Metrics More Like This Verification of Linux device drivers
    using device virtualization 2015 2nd International Conference on Computing for
    Sustainable Global Development (INDIACom) Published: 2015 NHVM: Design and Implementation
    of Linux Server Virtual Machine Using Hybrid Virtualization Technology 2010 International
    Conference on Computational Science and Its Applications Published: 2010 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE cloud computing
  limitations: '>'
  pdf_link: null
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: Containerization and the PaaS Cloud
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icac57685.2022.10025077
  analysis: '>'
  authors:
  - Lasal Sandeepa Hettiarachchi
  - Senura Vihan Jayadeva
  - Rusiru Abhisheak Vikum Bandara
  - Dilmi Palliyaguruge
  - Udara Srimath S. Samaratunge Arachchillage
  - Dharshana Kasthurirathna
  citation_count: 1
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 4th International Confer...
    Expert System for Kubernetes Cluster Autoscaling and Resource Management Publisher:
    IEEE Cite This PDF Lasal Sandeepa Hettiarachchi; Senura Vihan Jayadeva; Rusiru
    Abhisheak Vikum Bandara; Dilmi Palliyaguruge; Udara Srimath S.Samaratunge Arachchillage
    All Authors 1 Cites in Paper 242 Full Text Views Abstract Document Sections I.
    Introduction II. Background and Literature III. Methodology IV. Results and Discussion
    Authors Figures References Citations Keywords Metrics Abstract: The importance
    of orchestration tools such as Kubernetes has become paramount with the popularity
    of software architectural styles such as microservices. Furthermore, advancements
    in containerization technologies such as Docker has also played a vital role when
    it comes to advancements in the field of DevOps, enabling developers and system
    engineers to deploy are manage applications much more effectively. However, infrastructure
    configuration and management of resources are still challenging due to the disjointed
    nature of the infrastructure and resource management tools’ failure to comprehend
    the deployed applications and create a holistic view of the services. This is
    partly due to the extensive knowledge required to operate these tools or due to
    the inability to perform specific tasks. As a result, multiple tools and platforms
    need to conFigure together to automate the deployment, monitoring and management
    processes to provide the optimal deployment strategy for the applications. In
    response to this issue, this research proposes an expert system that creates a
    centralized approach to cluster autoscaling and resource management, which also
    provides an automated low-latency container management system and resiliency evaluation
    for dynamic systems. Furthermore, the time series load prediction is done using
    a BiLSTM and periodically creates an optimized autoscaling policy for cluster
    performance, thus creating a seamless pipeline from deployment, monitoring scaling,
    and troubleshooting of distributed applications based on Kubernetes. Published
    in: 2022 4th International Conference on Advancements in Computing (ICAC) Date
    of Conference: 09-10 December 2022 Date Added to IEEE Xplore: 31 January 2023
    ISBN Information: DOI: 10.1109/ICAC57685.2022.10025077 Publisher: IEEE Conference
    Location: Colombo, Sri Lanka SECTION I. Introduction A concept called “Micro-Web-Services”
    was introduced by Peter Rodgers in 2005 as a specialized version of Service Oriented
    Architecture (SOA) [1]. However, the term ”microservice” was first introduced
    in 2011 in a software architect workshop. The microservice architecture was widely
    embraced in many large tech companies such as Amazon, LinkedIn, and Netflix due
    to independent development, deployment, increased scalability, technology heterogeneity
    and greater autonomy [2]. Containerization works conveniently with microservice
    applications due to their ability to encapsulate all the necessary libraries and
    dependencies. Due to services such as Docker, containerization has become the
    most prominent method to deploy microservices. However, the application deployment
    and management still become problematic when the number of microservices increases.
    Kubernetes (k8s) were introduced in 2014 by Google to address this issue. Kubernetes
    provides a framework that manages load balancing, storage orchestration, automated
    rollouts and rollbacks, configuration management and self-healing for organizations
    to deploy and run distributed applications resiliently. However, although Docker
    and Kubernetes solve many problems in microservice applications, it also introduces
    new challenges, particularly in resource management. Furthermore, the current
    resource management tools struggle to provide a holistic view of the deployment
    of microservice applications in the Kubernetes cluster, making it challenging
    to find the optimal deployment strategy that allocates cluster resources effectively.
    Nevertheless, even though there are tools such as Istio, Prometheus, Kiali and
    Grafana, due to their disjointed nature, it becomes hard to configure those tools
    together to get a holistic view of the application [3]. The proposed research
    introduces a model to provide a centralized approach to produce an optimal deployment
    strategy that effectively allocate cluster resources for microservice applications.
    The primary components of the proposed model are as follows: An automated low
    latency container management system that lazily push container images into the
    Kubernetes private container registry. A matrix monitoring system to collect app
    level and pod level data to obtain the cluster behavior and feed them through
    an API to the optimal algorithm. An optimal algorithm which determines the deployment
    strategy and allocates cluster resources effectively to the microservice application.
    A resilience evaluation component that detects faults of the deployed application
    in the Kubernetes cluster. The remaining sections of the publication are organized
    as follows. Section II comprises the background and the related work literature
    referenced in developing the proposed model. Section III discusses the architecture
    of the proposed model and the methodology that followed in developing the proposed
    model. Section IV discusses the results collected using the developed model, and
    finally, the conclusion of this publication is outlined in Section V. SECTION
    II. Background and Literature The requirement for an improved resource management
    system for Kubernetes-based microservice deployment, along with some issues such
    as container management, service monitoring and fault management, has been emphasized
    in several publications throughout the years. Container management is a main aspect
    in microservice deployments. Among four deployment methods proposed in the publication
    [4] containerized application deployment is the one that substantially use in
    the industry. However, according to publication [5], the main drawback in the
    current container management systems is the network overhead added during the
    container image deployment. The main reason for to increase the in network overhead
    is the remote calls generated through the internet. Another significant challenge
    in Kubernetes deployments is the lack of proper monitoring solutions that provide
    a holistic view of the deployed applications which is a crucial aspect of the
    microservice ecosystem. A study done by authors such as [6] shows that it is important
    to have a solution to monitor cluster performance and traffic in a microservice
    application. Moreover, articles such as [7] discuss major challenges that come
    with deploying microservices and the apparent need for tools for monitoring application
    performance, especially those deployed in containers. The decentralized approach
    to container orchestration in the microservices approach entails those individual
    services can be scaled as required without affecting the other services to handle
    dynamic workloads. Most of the approaches in literature such as Elastic docker
    in publication [8] try to address this issue at a container level or a Virtual
    Machine (VM) level by allocating and deallocating resources based on designated
    workloads. The proposed solutions only consider the vertical scaling aspect of
    containers. Since there is an upper bound to the number of resources that can
    be provided, they may fail under large workloads. The Kubernetes scheduler is
    responsible for considering the global resource availability in terms of nodes
    and scheduling pods according to the requirement of resources. However, the governance
    of resources and the responsibility of auto-scaling are given to the Horizontal
    Pod Auto-scaler(HPA) and the Vertical Pod Auto-scaler(VPA). This is primarily
    achieved through rule-based auto-scaling [9] that uses polices that guide orchestrator
    to allocate resources. The policies are defined based on pod-level matrices such
    as CPU and memory utilization to make informed decisions on how to scale up the
    deployments as required to satisfy the objectives of the application owner [10].
    However, there are certain issues inherent to this approach, including the inability
    to adapt to the dynamicity [11], monitoring of only high-level matrices and the
    inability to de-provision resources when they are not required. The resiliency
    evaluation is also a crucial aspect in distributed systems, as failures are also
    distributed within a distributed application. However, even though the failures
    are identified, it is not possible to address them immediately. Therefore, Netflix
    provides a concept called chaos engineering to identify the system faults [12],
    and the invention of chaos engineering led to introduce new tools such as Chaos
    Monkey and Chaos Toolkit [13]. Even though fault tolerance and resiliency evaluations
    have been performed on microservices, the results obtained are only used in the
    identification of the weaknesses of the system. Resilience evaluation is, therefore,
    vital in determining critical services in microservice deployment. All the autoscaling
    policies discussed in the above section are reactive processes. Thus, the system
    operates at a suboptimal level until the specified state is acquired. The novelty
    aspect of this research is that it creates a dynamic autoscaling policy that is
    triggered proactively based on the load prediction of the optimization server
    that is done by a BiLSTM model. Further, it creates a private container registry
    inside the cluster to store container images hence improving the push times drastically.
    Thus the primary objective of this manuscript is to propose a seamless pipeline
    from container creation, orchestration, monitoring, troubleshooting, and performance
    optimizing in a Kubernetes-based environment. The following section discusses
    how these results are achieved. SECTION III. Methodology The centralized model
    proposed in this publication possesses four main components, each aimed to assist
    the optimal deployment of the microservices with the leading intent of gaining
    a more holistic view of the microservice deployment. Fig. 1 illustrates an overview
    of the developed resource management system with its key components. Fig. 1: Overview
    of the implemented solution Show All A. Developing the Fast Provider Server The
    Fast Provider server was implemented using the combination of three sub-components.
    Table I present the subcomponents and a brief description of each component. Fig.
    2. High-level diagram of the components in Fast Provider server Show All TABLE
    I Sub Components of the Fast Provider Server The proposed model uses Docker Software
    Development Kit (SDK), GitHub API and Kubernetes API to develop the Fast Provider
    server. The Fast Provider server runs inside a VM. Therefore, Docker SDK and Kubernetes
    API are thereby configured in the VM to facilitate the low latency container management
    system. a) Fast Provider Server The Fast Provider server utilized in the proposed
    model provides a unified interface for developers to register the container applications.
    Once the services are registered, developers can use Fast Provider server to trigger
    releases to Kubernetes cluster. The server stores the references to the code repository.
    Therefore, in a new release the server uses the reference to download the latest
    code base from the version control system (VCS). The Build method then executed
    by passing the service name and the version tag as parameters to function. The
    Dockerfile extracted from the code base and pass to the Docker Daemon [14] as
    build option along with parameters mentioned above. Furthermore, in the build
    time, the missing base image layers are downloaded from the Docker Hub and store
    in the local registry for the future builds. Once the build process is completed,
    the Push method is executed to push the built container image layers to the container
    registry over the local machine network. The metadata of the building and pushing
    processes are stored in a No-SQL database that developed on cloud [15] and served
    to the dashboard to provide a holistic view of the release. b) Kubernetes Private
    Container Registry The private container registry is created in the Kubernetes
    cluster to store the microservice container images. A separate Kubernetes deployment
    YAML [16] file is constructed and apply to the cluster to create the private container
    registry. However, to access the private container registry by the Fast Provider
    server and the deployed Pods, the registry needs to be revealed as a Nodeport
    service that assigns a static IP address to the registry. Therefore, the Fast
    Provider server and deployed Pods can access the private container registry using
    the exposed IP address. Therefore, the Fast Provider server can connect to the
    registry and transfer the container images. c) Node JS Server A Node JS server
    is developed to generate requests to fetch the container images to the Kubernetes
    private container registry. The Node JS server deployment occurs before the microservice
    applications are deployed to the cluster using a separate YAML file. Once the
    Node JS server is deployed, it makes an API request to the Fast Provider server
    to get the registered container image information. An array of container image
    information with the container name, version and build ID are sent as the response
    to the request. The Node JS server gets Pod-level data by sending a request to
    the Kubernetes API once the container image provisioned. After receiving the Pod-level
    data, the Node JS server extracts the container information from the Pod. Subsequently,
    the Node JS server compares the container image version in the cluster and the
    container image version received from the Fast Provider server. However, if a
    different container version is available in the Fast Provider server that is not
    available in the Kubernetes private container registry, the Node JS server generates
    an API request to the Fast Provider server to receive the missing container image
    version. Once the request is made to the Fast Provider server, it transfers the
    requested container image to the Kubernetes private container registry. The Fast
    Provider server is developed to send only the missing container image layers instead
    of sending the complete image to optimize the performance of the proposed model.
    B. Developing the Matrics Server Fig. 3 illustrate the server that was used to
    fetch deployed microservice metadata to provide the holistic view of the applications.
    The matrices server is implemented using the combination of three sub-components.
    Table II present the sub-components and a brief description of each component.
    Fig. 3. High level diagram of the Matrices server Show All TABLE II The Sub Components
    of the Metrics Server The proposed solution is to set up the Istio service mesh
    [17] in the Kubernetes cluster and uses Prometheus, Kiali, Grafana, and Kubernetes
    APIs to fetch the time-series metrics from the Kubernetes cluster. Node JS is
    used to develop the system, and MongoDB stores the structured data. The Metrics
    server is deployed on a VM available in the cloud environment because this process
    is dramatically faster due to the usage of the local network when connecting with
    the optimization server. The Metrics server systematically fetches the metrics
    regarding the network, CPU, and memory usage of pod and app levels using the aforementioned
    tools to create the mechanism to query the metrics server when the Cron job is
    called to get the pod-level matrices of CPU utilization and dependencies. Those
    metrics are structured in a readable manner for the use of optimization server
    to generate the optimal solution, and those metrics are stored in a persistent
    database. Metrics queried by the Metric server expose as a RESTful service and
    generates a CSV (Comma Separated Values) file to develop a holistic view that
    helps to optimize the deployment strategy of a Kubernetes cluster. Based on the
    optimal solution received by the optimization server YAML Generator service of
    the metrics server regenerates a YAML file and applies it to the Kubernetes cluster
    with optimal deployment. C. Developing the optimization Server The primary objective
    of the load prediction and optimization server is to facilitate the prediction
    of future workloads on the microservice deployment using the data that is continuously
    queried from the matrix server and scale the Kubernetes cluster according to its
    resource requirement. Fig. 4: High level diagram of the optimization server Show
    All The load prediction process is based on the resource utilization metrices
    gathered from the cluster by the metrics server. Pod level data such CPU utilization
    and Memory utilization, as well as cluster level data for a particular period
    repeatedly captured by the metrics server using a 5-minute cron job, are taken
    into account for the load prediction procedure which is primarily based on a time
    series forecasting. The optimization server has the capabilities of querying the
    data from the metrics server through an exposed API. Essentially, the responsibility
    of the Data access layer is to act as a gateway between the optimization gateway
    and to query the necessary data from the metrics server. The queried data are
    then pre-processed and utilize by other layer to make inferences based on them.
    The forecasting mechanism is performed through the application of Bidirectional
    Short-Term Long Memory (LSTM) [18]. Although there are other techniques specially
    based on statistical approaches such as ARIMA, for time series forecasting, BiLSTM
    [19] approach was used since it yields much higher accuracy with less amount of
    data. The BiLSTM was preferred over other model since it allows to learn long
    term dependencies due to its memory. The predictions of Memory usage and CPU usage
    and other matrices were used to create the auto-scaling policy with the help of
    the Horizontal/Vertical pod auto scaler and the Custer auto-scaler. The load prediction
    and optimization server are primarily responsible for performing 3 essential tasks.
    Predicting the future resource utilization metrices using the data that gathered
    by the metrics server. Creating the optimal auto-scaling mechanism using the Horizontal
    pod auto scaler and cluster auto scaler [20] based on the predictions Imposing
    the policy upon the cluster. The server which is deployed inside an Azure VM,
    is entirely responsible for this load prediction and optimization process. This
    modular approach allows the instrumentation and optimization process to happen
    without hindrance to the cluster itself since the computations necessary for the
    load prediction process are performed outside the cluster. The solution is exposed
    to other components as APIs. Primarily, the solution is implemented using Python
    programming language due to its flexibility and adaptability and scalability under
    dynamic workloads. The support for data augmentation processes and the availability
    of a well-developed set of libraries to facilitate the development of the BiLSTM
    played a vital role as well in this decision. Especially, libraries such as Tenser
    flow library for python and Keras were used for the model building and data augmentation
    process. The prediction was based on a BiLSTM, and the featurization table for
    the prediction model was as follows. Fig. 5: Featurization table of the prediction
    model Show All The load prediction model has a forecast horizon of 20-time steps.
    Since the amount of data generated is limitless, the lookback amount can be configured
    to any number of time steps. But for demonstration purposes and the optimal time
    utilization of the prediction, the lookback was set to 150-time degrees. Since
    the requirement was to create a prediction upper bound for resource utilization
    upon which the autoscaling policy is created, the need was to fit a prediction
    model precisely to the trend of the resource utilization data points. Hence the
    model was trained using data for 100 epochs to achieve the overfitting. The prediction
    upper bound was calculated as follows. r i n −t the i th term of the array containing
    values for n for cast horizons. r pred ={a∣a∈ r n ,a≥ r i n } (predicted metric
    value) Using the predicted metric value, the desired number of replicas were calculated
    d – desired number of replicas c – current number of replicas r cur – current
    metric value d ⎧ ⎩ ⎨ ⎪ ⎪ ⌈ c∗ r pred  r cur  ⌉ if  r pred  ≥ r cur  ⌈ c∗ r cur  r
    pred  ⌉ if  r cur  > r pred  (1) View Source Using the ceiling function, after
    calculating the desired number of replicas the policy is imposed upon the Kubernetes
    cluster using the Kubernetes API. According to the piecewise function it can be
    observed that both scaling up and scaling down is handled using this approach.
    Thus, optimizing the cluster performance and resource utilization based on the
    predicted workload D. Developing the Resiliency Management Component The resilience
    evaluation component is based on chaos engineering principles. To make use of
    co-dependence network, the most visible nodes will be targeted for carrying out
    chaos experiments. In a brief, this component will primarily oversee conducting
    chaos experiments to identify system flaws. Fig. 6: High level diagram of the
    Monitoring server Show All System weakness is identified through a chaos experimenting
    tool called Chaos Toolkit and here, a series of operations are carried out on
    each node to see how the system responds to the changes made. As example delays
    can be created using chaos toolkit extensions such as Istio Fault Injection to
    investigate how the system responds. We could take advantage of the Chaos Toolkit-Prometheus
    extension to get contextual information such as CPU usage, storage space, and
    bandwidth. The metrics can be gathered with this add on while using various circumstances
    for the system. Additionally, numerous experiments are done to alter the system’s
    parameters in a way that it is possible to pinpoint the system’s resilience. Constantly
    latency attacks can be performed to measure the system behavior and the latency
    of the database response. To make this process more convenient a scheduler has
    been implemented. SECTION IV. Results and Discussion To evaluate the above-proposed
    model, the authors have used sample microservice applications that developed using
    Java, Python, C# and JavaScript languages. The application is deployed in the
    Azure Kubernetes Service (AKS) cluster that consists of 4 nodes with one master
    node and 3 worker nodes. To evaluate the proposed Fast Provider model, the container
    deployment time was compared with the existing deployment method that use remote
    registry over the Fast Provider server. Table III presents the performance improvement
    of microservice deployment with Fast Provider server. The deployment time recorded
    in seconds. TABLE III Performance Improvement of the Deployments The evaluation
    of the matric prediction and optimization policy was done to a sample microservice
    provided in [21]. The application consists of 11 microservices. The languages
    and descriptions pertaining to the services can be found in their documentation.
    The microservices include a worker application that can be utilized to generate
    load as required to test the application. By providing GitHub repository links
    of the microservices, the container images pertaining to the microservices were
    created and pushed to the local repository inside the Kubernetes cluster. The
    Kubernetes cluster was created in AKS (Azure Kubernetes Service) for evaluation,
    but the application is interoperable among other service providers as well. After
    the local container registry was created, the images were then used to develop
    the microservices. The istio service mesh was enabled on top of the Kubernetes
    cluster after the microservices were built. The aforementioned feature set in
    section 2, Fig [5] was then gathered by the matrices server corresponding to the
    time stamp at a frequency of 1 minute for 2 days, gathering 4320 records. The
    data gathered by the matrices serve are then used to train the BiLSTM model and
    to create the autoscaling policy to be imposed upon the cluster. The following
    results were obtained by setting the frequency of the optimization servers cron
    job to 5 minutes. The objective of this publication was to improve cluster performance.
    Several parameters were used to evaluate the effectiveness of the dynamic policy
    to the cluster performance. The first was the Average response time of services.
    The second was CPU/Memory usage reduction of service pods after scaleup was triggered.
    Primarily a mojor improvement could be observed in the RT (AVG): average response
    time for frontend services when operating under the model proposed in the publication
    compared to the existing model. TABLE IV RT Comparison of the Model and the Existing
    Solution The second metric used to evaluate the effectiveness of the scaling policy
    was the reduction in CPU/Memory utilization in pods. The following instances show
    how the parameters were reduced after a scale-up was triggered by the proposed
    policy. Fig. 7: Before the autoscaling policy was enacted Show All Fig. 8: After
    the autoscaling policy was enacted. Show All The BiLSTM model that was used to
    predict the matric values were evaluated separately along with several other timeseries
    prediction approaches to find the most accurate model on [22], and the following
    results were obtained. The evaluation was carried out for 9000 time-series data
    points using AutoML and TensorFlow libraries TABLE V The Root Mean Square Error
    and the Mean Absolute Percentage Error of Different Time Series Prediction Algorithms
    for the Predictions On [23] From the above results, the significance of BiLSTM
    model in making an accurate prediction in less time is apparent. Further, major
    improvements can be observed pertaining to cluster performance, as discussed in
    the above section, since the response time is reduced significantly as mentioned
    above. Authors Figures References Citations Keywords Metrics More Like This Prediction
    of Software Fault Detection and Correction Processes With Time Series Analysis
    2020 Asia-Pacific International Symposium on Advanced Reliability and Maintenance
    Modeling (APARM) Published: 2020 Time series analysis for bug number prediction
    The 2nd International Conference on Software Engineering and Data Mining Published:
    2010 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Expert System for Kubernetes Cluster Autoscaling and Resource Management
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ficloud.2015.35
  analysis: '>'
  authors:
  - Claus Pahl
  - Brian Lee
  citation_count: 179
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2015 3rd International Confer...
    Containers and Clusters for Edge Cloud Architectures -- A Technology Review Publisher:
    IEEE Cite This PDF Claus Pahl; Brian Lee All Authors 164 Cites in Papers 7 Cites
    in Patents 3812 Full Text Views Abstract Document Sections I. Introduction II.
    Towards Edge Clouds III. Virtualisation Principles IV. Container Virtualisation
    for Lightweight Application Packaging V. Paas Clouds and Containerisation Show
    Full Outline Authors Figures References Citations Keywords Metrics Abstract: Cloud
    technology is moving towards more distribution across multi-clouds and the inclusion
    of various devices, as evident through IoT and network integration in the context
    of edge cloud and fog computing. Generally, lightweight virtualisation solutions
    are beneficial for this architectural setting with smaller, but still virtualised
    devices to host application and platform services, and the logistics required
    to manage this. Containerisation is currently discussed as a lightweight virtualisation
    solution. In addition to having benefits over traditional virtual machines in
    the cloud in terms of size and flexibility, containers are specifically relevant
    for platform concerns typically dealt with Platform-as-a-Service (PaaS) clouds
    such as application packaging and orchestration. For the edge cloud environment,
    application and service orchestration can help to manage and orchestrate applications
    through containers as an application packaging mechanism. We review edge cloud
    requirements and discuss the suitability container and cluster technology of that
    arise from having to facilitate applications through distributed multi-cloud platforms
    build from a range of networked nodes ranging from data centres to small devices,
    which we refer to here as edge cloud. Published in: 2015 3rd International Conference
    on Future Internet of Things and Cloud Date of Conference: 24-26 August 2015 Date
    Added to IEEE Xplore: 26 October 2015 ISBN Information: DOI: 10.1109/FiCloud.2015.35
    Publisher: IEEE Conference Location: Rome, Italy SECTION I. Introduction Cloud
    computing is moving from centralised, large-scale data centres to a more distributed
    multi-cloud setting comprised of a network of larger and smaller virtualised infrastructure
    runtime nodes. Virtualising reaches the network and allows Internet-of Things
    (IoT) infrastructures to be integrated. These architectures and their setting
    are often referred to as edge clouds, edge computing or fog computing [4]. As
    a challenge resulting from distribution, we need a more lightweight solutions
    than the current virtual machine (VM)-based virtualisation technology. Furthermore,
    as another challenge, the orchestration of lightweight virtualised runtimes is
    needed. Regarding the first challenge, the cloud relies on virtualisation techniques
    to achieve elasticity of large-scale shared resources. Virtual machines (VMs)
    have been at the core of the compute infrastructure layer providing virtualised
    operating systems. We will investigate containers, which are a lightweight virtualisation
    concept, i.e., less resource and time consuming. VMs and containers are both virtualisation
    techniques, but solve different problems. Containers are a solution for more interoperable
    application packaging in the cloud and should therefore address the PaaS concerns.
    Contrary to VMs, containers can be seen as more flexible tools for packaging,
    delivering and orchestrating both software infrastructure services and applications,
    i.e., tasks that are typically a PaaS (Platform-as-a-Service) focus. Containers
    built on recent advances in virtualisation, allowing a more portable way aiming
    at more interoperability [19] while still utilising operating systems (OS) virtualisation
    principles. VMs on the other hand are about hardware allocation and management
    (machines turned on/off and provisioned). With them, there is an IaaS (Infrastructure-as-a-Service)
    focus on hardware virtualisation. Containers as a replacement for VMs are only
    a specific use case where the allocation of hardware resources is done through
    containers by componentising workloads in-between clouds. The basic ideas of containerisation
    are: (i) a lightweight portable runtime, (ii) the capability to develop, test
    and deploy applications to a large number of servers and (iii) the capability
    to interconnect containers. Containers address concerns at the cloud PaaS level.
    They also related to the IaaS level through sharing and isolation aspects that
    exemplify the evolution of OS and virtualisation technology. Regarding the second
    challenge, for portable and interoperable software applications in a distributed
    cloud architecture, we need a lightweight distribution of packaged applications
    for deployment and management [6]. Again, the solution can be containerisation,
    but would need to be extended to deal with the orchestration needs. In order to
    assess the concerns here, we look into managing clusters of containers and their
    orchestration in a cloud setting. This article reviews the suitability of container
    technology for edge clouds and similar settings, starting with summarsing the
    virtualisation principles behind containers and identifying key technical requirements
    of edge cloud architectures. The relevance of the new container technology for
    PaaS cloud concerns with application packaging and orchestration concerns shall
    be specifically investigated. This research shall clarify how containers can change
    PaaS clouds as a platform technology. As we consider distributed clouds, the resulting
    requirements for application packaging and interoperable orchestration over clusters
    of containers are central. We discuss what is needed to evolve PaaS technology
    further as a distributed cloud software platform resulting in a discussion of
    achievements and limitations of the state-of-the-art. In order to illustrate the
    technologies in question, we will refer to sample technologies they exemplify
    technology trends. As part of this investigation, we abstract concepts from various
    technology platforms and condense them into sets of common principles, supplemented
    by architecture frameworks that represent best practice. We start with a more
    detailed review of the architectural setting in Section II and discuss the resulting
    virtualisation and management needs to Section III. We then introduce container-based
    virtualisation in Section IV. In Section V, we focus the investigation on PaaS
    cloud concerns. Finally, clustering and orchestration are discussed in Section
    VI, before ending with some conclusions. SECTION II. Towards Edge Clouds Cloud
    edge computing is pushing computing applications, data, and services away from
    centralized cloud data centre architectures to the edges of the underlying network
    [5]. The objective is to allow analytics and knowledge generation services to
    be placed at the source of the data. Cloud computing at the edge of the network
    links into the internet of things (Io''T). The core cloud provides a globalised
    view; edge clouds are responsible for localised views (to host services close
    to or at endpoints of networks), on-device, private cloud like infrastructures.
    We can classify distributed clouds into three architectural models, ranging from
    tightly coupled to highly dispersed ones: Multi-datacentre clouds with multiple,
    but tightly coupled data centers under control of the same provider. Loosely coupled
    multi-service clouds combine services from different cloud providers. Decentralized
    edge clouds utilize edge resources to provide data and compute resources in a
    highly dispersed manner. A. Edge Cloud Technology Requirements In order to support
    specifically the edge clouds, we need for instance location-awareness and computation
    placement, replication, and recovery. For example, consider a content analysis
    application that processes digital multimedia content hosted throughout the Internet.
    Edge resources would be required for both computation and data storage to address
    the wide data distribution. The necessary edge resources could be dedicated resources
    spread across content distribution networks. Various virtualised resources exist
    in this setting that can support edge clouds [15] - all programmable, but different
    in size and type, such as nodes and edges (the latter are actually nodes of the
    network itself). This results in different resource restrictions calling for lightweightness
    with respect to the virtualisation approach [23]. In centre and edge clouds, but
    also the IoT objects linked to, compute and storage resources, platform services
    and applications need to be managed, i.e., packaged, deployed and orchestrated,
    see Figure 1. For the network, virtualisation capacity is required as well - cf.
    work on software-defined networks (SDN). We need to support data transfer between
    virtualised resources and to provide compute, storage, network resources between
    end devices and traditional cloud computing data centres. Concrete requirements
    arising from this are location awareness, low latency and mobility support to
    manage cloud end points with rich (virtualised) services. SDN-virtualised nodes
    and also virtualised edges and connectors require a lightweight virtualisation
    technology of efficiently deploy portable services at edges. This type of virtualised
    infrastructure might provide end-user access and IoT links - through possibly
    private edge clouds (which are technically miniclouds). Fig. 1. Resources architecture
    as cluster-based container architecture Show All These need to be configured and
    updated in a secure way - this particularly applies to the service management.
    We would also need a development layer on top to provision and manage applications
    on these infrastructures. Solutions here could comprise common topology patterns,
    controlling application lifecycle and an easy-to-use API. The right abstractions
    for edge cloud oriented management at a typical PaaS layer would be beneficial.
    B. Architectural Principles An architecture addressing the challenges can be organised
    into layers (bottom to top): at the bottom a smart things network (smart sensors
    network, wireless sensor and actuator networks, mobile and ad-hoc networks - possibly
    with a MQTT protocol on top with a pub/sub model), a field area network (such
    as 3/4G, LTE, WIFI) and then the IP core infrastructure, and a virtual compute/storage/network
    cloud with applications on top. The operation and management of this architecture
    will see service providers push out (i.e., deploy) service in suitable application
    packages (such as containers) to clustered edge clouds. While some solutions like
    Docker container architectures for clouds exist [22], there is still a need for
    a topology specification and a derived orchestration/choreography plans. Existing
    solutions in this space include Kubernetes, but, as we will see, leave some orchestration
    questions unanswered. C. Challenges-Development and Operations We assume the requirements
    to include multi-cloud deployment (via lightweight application packaging, distribution
    and support of topology specification and management. The aim is to allow, in
    virtualised form, various services such as security and analysis services deployed
    on these resources [9]. Specifically, the development of these architectures needs
    to be supported through orchestration based on topology patterns reflecting common
    and reference architectures. Several technologies exist that might contribute
    to the solution: Application Packaging Through Containerisation: Containers can
    be used to distribute service and applications (sometimes called appliances) to
    the edge. Docker has already been used to do this (providing plugins link to agents
    which could be Docker runtimes). Programmability: Orchestration can be supported
    through topology specification based on TOSCA topology patterns [2]. Overall,
    service composition (orchestration) needs to cover the whole life-cycle - deploy,
    patch, shutdown. Operations are mapped to cloud infrastructure management and
    a TOSCA engine runs on top of here edge cloud infrastructure [3]. We now specifically
    look at edge clouds and fog computing from a PaaS perspective taking lightweight
    application packaging and topology specification into account. SECTION III. Virtualisation
    Principles Virtualisation is an answer to the need for scheduling processes as
    manageable container units. Processes and resources in this context are at the
    operating systems level the file system, memory or the network. We review these
    technology principles in order to allow us to judge the capability of the technology
    for edge clouds later on. In the cloud as a virtualised architecture, virtual
    machines (VMs) are the core virtualisation mechanism. We briefly review the development
    of virtualisation over the years. VMs have been improved over the years by enhancing
    scheduling, packaging and resource access (security). VM instances as guests use
    isolated large files on their host to store their file system and to run a single,
    large process on the host. Here, some concerns such as security are addressed
    through isolation. However, limitations remain. For instance, full guest OS images
    are needed for each VM in addition to binaries and libraries necessary for the
    applications, which is a space concern that means additional RAM and disk storage
    requirements. It also causes performance issues as this is slow on startup (boot),
    see Fig. 2. Furthermore, multi-tenant clouds require the sharing of disk space
    and CPU. In a virtualised environment, this has to managed such that the underlying
    platform and infrastructure can be shared in a secure, but also portable and interoperable
    way [18]. Fig. 2. VM (left) and container (right) virtualisation architecture.
    Show All At the platform service level, packaging and application management is
    an additional requirement. Containers can match these requirements, but a more
    in-depth elicitation of specific concerns is needed. Container technology is a
    development that meets the needs. A container is essentially a packaged self-contained,
    ready-to-deploy set of parts of applications, that might even include middleware
    and business logic in the form of binaries and libraries to run the applications
    [21], see Fig. 2. A typical example is a Web interface component with a Tomcat
    server. Container tools like Docker are frameworks built around container engines
    [22]. They make containers a portable way to package applications to run in containers.
    In terms of a tiered application, a container includes an application tier (or
    node in a tier). Two challenges remain, however: Managing dependencies between
    containers in multi-tier applications is a problem that emerges. As discussed,
    an orchestration plan can describe components, their dependencies and their lifecycle.
    A PaaS cloud can then enact the workflows from the plan through agents (which
    could be a container runtime engine). Software platform services can support packaging
    and deployment of applications from containers. The second challenge is to define,
    deploy and operate cross-platform capable cloud services using a lightweight virtualisation
    mechanism such as containers. There is also a need to transfer cloud deployments
    between cloud providers in a distributed context, which requires lightweight virtualised
    clusters for container orchestration. Some PaaS are lightweight virtualisation
    solutions in this sense. SECTION IV. Container Virtualisation for Lightweight
    Application Packaging The evolution of virtualisation has resulted in more lightweight
    solutions. This is specifically relevant for application packaging at a software
    platform and application level. Recent virtualisation advances have improved multi-tenancy
    capabilities, i.e., the capability to share a resource in a cloud. A. Lxc Linux
    and Docker Containers Recent Linux distributions - part of the Linux container
    project LXC - provide kernel mechanisms such as namespaces and cgroups to isolate
    processes on a shared operating system [21]. These are examples of OS virtualisation
    advances. Namespace isolation allows groups of processes to be separated. This
    ensures that they cannot see resources in other groups. Different namespaces are
    the used for process isolation, network interfaces, access to interprocess communication,
    mount-points or for isolating kernel and version identifiers. cgroups (control
    groups) manage and limit resource access for process groups through limit enforcement,
    accounting and isolation, e.g., limiting the memory available to a specific container.
    This enables better isolation between isolated applications on a host. This restricts
    containers in multi-tenant host environments. Control groups allow sharing available
    hardware resources between containers and, if required, setting up limits and
    constraints. Containers are, as a consequence of these properties, virtualisation
    techniques suitable for application management in PaaS clouds. A container is
    represented by lightweight images - VMs are also based on images, but full monolithic
    ones. Processes running in a container are almost fully isolated. Container images
    are the building blocks from which containers are launched. There is still an
    ongoing evolution of OS virtualisation and containerisation, aiming at providing
    OS support through standard APIs and tools for container management, network management
    and making resource utilisation more visible and manageable. Docker is a container
    solution that builds on top of Linux LXC techniques. Docker is the most popular
    container solution at the moment and shall be used to illustrate containerisation.
    A Docker image is made up of file systems layered over each other, similar to
    the Linux virtualisation stack, using the LXC mechanisms, see Fig. 3. A container-aware
    daemon, called systemd, starts containers as application processes. It plays a
    key role as the root of the user''s process tree. Boot process: In a traditional
    Linux boot, the kernel first mounts the root file system as read-only, before
    checking its integrity. It then switches the rootfs volume to read-write mode.
    Docker mounts the rootfs as read-only (as in a traditional Linux boot), but instead
    of changing the file system to read-write mode, it uses a union mount to add a
    writable file system on top of the read-only file system. Mounting: This allows
    multiple read-only file systems to be stacked on top of each other. Using union
    mount, several file systems can be mounted on top of each other. This property
    can be used to create new images by building on top of base images. Each of these
    file system layers is a separate image loaded by the container engine for execution.
    Container: Only the top layer is writable, which is the container itself. The
    container can have state and is executable. It is a kind of directory for everything
    needed for execution. While they are normally stateful, containers can be made
    into stateless images to be reused in more complex builds. Fig. 3. Container image
    architecture. Show All A typical layering could include, from top to bottom (Fig.
    3), a writable container image for applications, an Apache image and an Emacs
    image as sample platform components, a Linux image (a distribution such as Ubuntu),
    and the rootfs kernel image. Containers are based on layers composed from individual
    images built on top of a base image that can be extended. Complete Docker images
    form portable application containers. They are also building blocks for application
    stacks. This approach is called lightweight as single images can easily be changed
    and distributed. B. Application Containerisation and Container Management Fig.
    3 shows a sample containerised application, the writable container. Containers
    can encapsulate a number of application components through the image layering
    and extension process. Different user applications and platform components can
    be combined in a container. Fig. 4 is an illustration of different scenarios using
    the container capability of combining images for platform and application components.
    Fig. 4. Container-based application architecture. Show All A container solution
    consists of two main components - (i) an application container engine to run images
    and (ii) a repository/registry that is operated via push and pull operations to
    transfer images to and from host-based container engines. These repositories play
    a key role in providing access to possibly reusable private and public container
    images, which might be tens of thousands. Examples of popular images are platform
    components such as MongoDB or Node.js. The container API supports life-cycle operations
    like creating, defining, composing, distributing containers, running/starting
    images and running commands in images. Container creation for applications is
    done by assembling them from individual images, which can be base images extracted
    from repositories. In addition to these basic features, storage and network management
    are two specific functions that containers as application packages for distributed
    edge clouds also require. Firstly, there are two ways data is managed in Docker
    - through data volumes and data volume containers. Data storage operations can
    add data volumes to any container. A data volume is a designated directory within
    one or more containers that bypasses the union file system. This allows to provide
    features for persistent or shared data. Volumes can then be shared and reused
    between containers (Fig. 5). A data volume container enables sharing persistent
    data between application containers through a dedicated, separate data storage
    container. Fig. 5. Container-based cluster architecture - an architectural framework
    based on common concepts. Show All Secondly, network management is based on two
    methods for assigning ports on a host - through network port mappings and container
    linking. Applications connect to an application running inside a Docker container
    via a network port. Container linking allows linking multiple containers together
    and sending information between them. Linked containers can transfer their data
    using environment variables. C. Comparison In order to summarise traditional VMs
    and containers, we compare the two technologies in Table 1. Table I. VM vs. Container
    comparison. We have used Docker earlier on to illustrate some container concepts,
    but a range of other container technologies exist for different operating systems
    types. For widely used OS like Linux and Windows several ones exist, but also
    specific solutions for PaaS, see Table II) [13]. Common to all is providing isolation
    for applications to address security problems. Table II. Container models [adapted
    from [13]]. The tool landscape supporting containerisation is equally in evolution.
    As one example, Rocket is a new container runtime from the CoreOS project (CoreOS
    is a Linux derivate for massive server deployments). Rocket is an alternative
    to the Docker runtime. It is specifically designed for composability, security,
    and speed - important properties in the edge cloud domain. The concerns specifically
    addressed by Rocket are good examples of ongoing concerns. SECTION V. Paas Clouds
    and Containerisation VMs are today the format to provision platform and application
    components at the infrastructure layer. Containers, however, appear as a highly
    suitable technology for application packaging and management in PaaS clouds. PaaS
    provide mechanisms for deploying applications, designing applications for the
    cloud, pushing applications to their deployment environment’ using services, migrating
    databases, mapping custom domains, IDE plugins, or a build integration tool. PaaS
    exhibit features like built farms, routing layers, or schedulers that dispatch
    workloads to VMs [8]. A. Evolution of Paas Container frameworks address the application
    deployment problems through interoperable, lightweight and virtualised packaging.
    Containers for application building, deployment and management (through a runtime)
    provide interoperability. Containers are interoperable - those produced outside
    a PaaS can be migrated in since the container encapsulates the application. Some
    PaaS are now aligned with containerisation and standardised application packaging.
    Many PaaS use Docker, some have their own container foundation for running platform
    tools. This development is part of an evolution of PaaS, moving towards container-based,
    interoperable PaaS. The first PaaS generation included classical fixed proprietary
    platforms such as Azure or Heroku. The second PaaS generation was built around
    open-source solutions such as Cloud Foundry or OpenShift that allow users to run
    their own PaaS (on-premise or in the cloud), already with a built-in support of
    containers. Openshift moves now from its own container model to the Docker container
    model, as does Cloud Foundry through its internal Diego solution. The current
    third generation of PaaS includes platforms like Dawn, Deis, Flynn, Octohost and
    Tsuru, which are built on Docker from scratch and are deployable on own servers
    or on public IaaS clouds. However, open PaaS platforms like Cloud Foundry and
    Open-shift treat containers differently. Cloud Foundry supports stateless applications
    through containers, but stateful services run in VMs. Openshift on the other hand
    does not distinguish between them. Flynn and Deis, two sample 3rd generation PaaS,
    have created a micro-PaaS concept where small PaaS can be run on limited hardware
    with very little overhead. They have adopted elements of CoreOS for their clustered,
    distributed architecture management, building on the mechanism of lightweight,
    decoupled services facilitated by Docker. This aids distributed multi-tenancy
    cloud on reduced capability resources. B. Service/Microservice Orchestration Recently,
    microservice architectures have been discussed, which aim to break up monolithic
    application architectures into SOA-style independently deployable services, which
    are well supported by container architectures. Services are loosely coupled, independent
    software components that can be rapidly called and mapped to any business process
    are required. The microservices architectural style is an approach to developing
    a single application as a suite of small services, each running in its own process
    and communicating with lightweight mechanisms. Microservices are independently
    deployable, usually supported by a fully automated deployment and orchestration
    framework. They require the ability to deploy often and independently at arbitrary
    schedules, instead of requiring synchronized deployments at fixed times. The microservice
    development and architecture concerns are central PaaS concerns. Containerisation
    provides an ideal mechanism for their flexible deployment schedules and orchestration
    needs, particularly, if these are to be PaaS-provisioned. SECTION VI. Clustering
    and Orchestrating Container The next concern is to facilitate the step from a
    single container host to clusters of container hosts to run containerised applications
    over multiple clusters in multiple clouds in order to meet the edge cloud requirements
    [10]. The built-in interoperability of containers can make this possible. A. Container
    Clusters A container-based cluster architecture groups hosts into clusters [12].
    Fig. 5 illustrates an architectural framework using common container and cluster
    concepts. Container hosts are linked into a cluster configuration. Central concepts
    are clusters, containers, application services, volumes and links. Each cluster
    consists of several (host) nodes - where nodes are virtual servers on hypervisors
    or possibly bare-metal servers. Each (host) node holds several containers with
    common services such as scheduling, load balancing and applications. Each container
    in a cluster can hold continually provided services such as their payload service,
    so-called jobs, which are once-off services (e.g., print), or functional (mid-dleware
    service) components. Next, application services are logical groups of containers
    from the same image. Application services allow scaling an application across
    nodes. Volumes are used for applications that require data persistence. Containers
    can mount volumes. Data stored in these volumes persists, even after a container
    is terminated. Finally, links allow two or more containers to connect and communicate.
    Resulting from this architectural scenario is an abstraction layer for cluster-based
    service management different from the container features provided by for instance
    Docker. A cluster management architecture has the following components: the service
    node (cluster), an API, a platform service manager, a lifecycle management agent
    and a cluster head node service. The deployment of distributed applications through
    containers is supported using a virtual scalable service node (cluster), with
    high internal complexity (supporting scaling, load balancing, failover) and reduced
    external complexity. An API allows operating clusters from the creation of services
    and container sets to other life-cycle functions. A platform service manager looks
    after the software packaging and management. An agent manages the container life-cycles
    (at each host). A cluster head node service is the master that receives commands
    from the outside and relays them to container hosts. This allows development of
    for instance edge cloud architecture without consideration of the underlying network
    topology and avoids manual configuration [8]. A cluster architecture is composed
    of engines to share service discovery (e.g., through shared distributed key value
    stores) and orchestration/deployment (load balancing, monitoring, scaling, and
    also file storage, deployment, pushing, pulling). This satisfies some requirements
    put forward for these cluster architectures by Kratzke [13]. A lightweight virtualised
    cluster architecture building on containerisation should, according to Kratzke,
    provide a number of management features as part of the abstraction on top of the
    container hosts: Hosting containerised services and providing secure communication
    between these services [17], Auto-scalability and load balancing support [11],
    Distributed and scalable service discovery and orchestration [9], Transfer/migration
    of service deployments between clusters [10]. Similar to Docker, Diego, Warden
    and others in the container space, several products have emerged in the cluster
    space. One such cluster management platform is Mesos - an Apache project that
    binds distributed hardware resources into a single pool of resources. These resources
    can be used by application frameworks to manage workload distribution. It is a
    distributed systems kernel following the same principles as the Linux kernel,
    but here at a different level of abstraction. The Mesos kernel runs on all machines
    in the cluster. It facilitates applications with APIs for resource management
    and scheduling across cloud environments. In terms of interoperability, it natively
    supports LXC and also Docker. Another clustering management solution, albeit at
    a higher level than Mesos, is the Kubernetes architecture. Kubernetes, which is
    supported by Google, can be configured to orchestrate Docker containers on Mesos.
    Kubernetes is based on processes that run on Docker hosts. These bind hosts into
    clusters and manage the containers. So-called minions are container hosts that
    run pods, which are sets of containers on the same host. Openshift is a PaaS example
    that has adopted Kubernetes. Kubernetes competes here with the platform-specific
    evolution towards container-based orchestration. Cloud Foundry, for instance,
    uses Diego as a new orchestration engine for containers. Clustered containers
    in distributed systems require advanced network support. Containers provide an
    abstraction that makes each container a self-contained unit of computation. Traditionally,
    containers are exposed on the network via the shared hosts address. In Kubernetes,
    each group of containers (called pods) receives its own unique IP address, reachable
    from any other pod in the cluster, whether co-located on the same physical machine
    or not. This requires advanced routing features based on network virtualisation.
    Distributed container management also needs to address data storage. Managing
    containers in Kubernetes clusters might cause difficulties with regard to flexibility
    and efficiency because of the need for the Kubernetes pods to co-locate with their
    data. What is needed is to combine a container with a storage volume that follows
    it to the physical machine, regardless of the container location in the cluster.
    B. Orchestration and Topology The management solution provided by cluster solutions
    needs to be combined with development and architecture support. Multi-PaaS based
    on container clusters is a solution for managing distributed software applications
    in the cloud, but this technology still faces challenges. These include a lack
    of suitable formal descriptions or user-defined metadata for containers beyond
    image tagging with simple IDs. Description mechanisms need to be extended to clusters
    of containers and their orchestration as well [1]. The topology of distributed
    container architectures needs to be specified and its deployment and execution
    orchestrated, see Fig. 6. Fig. 6. Reference framework for cluster topology orchestration
    [adapted from the TOSCA standard to an edge cloud setting]. Show All There is
    currently no widely accepted solution for the orchestration problems. We briefly
    illustrate the significance of this problem through a possible solution that we
    want to propose here as a possible reference framework. Docker has started to
    develop its own orchestration solution and Kubernetes is another relevant project,
    but a more comprehensive solution that would address the orchestration of complex
    application stacks could involve Docker orchestration based on the topology-based
    service orchestration standard TOSCA, which is for instance supported by the Cloudify
    PaaS. Cloudify uses TOSCA (Topology and Orchestration Specification for Cloud
    Applications [2]) to enhance the portability of cloud applications and services,
    see Fig. 6. TOSCA supports a number of features: the interoperable description
    of application and infrastructure cloud services - here implemented as containers
    hosted on nodes in an edge cloud, the relationships between parts of the service
    - here service compositions and links as relationships, as illustrated in Fig.
    5, the operational behaviour of these services (such as deploy, patch or shutdown)
    in an orchestration plan. This has the advantage of being independent of a supplier
    creating the service and also any particular cloud provider or hosting technology.
    TOSCA can also be used to associate higher-level operational behaviour with cloud
    infrastructure management. TOSCA templates can be used to define container clusters,
    abstract node and relationship types, and application stack templates. Cloud applications
    can run in TOSCA containers - enacted through a TOSCA engine based on TOSCA topology
    and orchestration descriptions [3]. The topology specification is based on nodes
    (e.g., Web server or sensor/device) and has life-cycle interfaces, which allows
    an architect to define how to create, configure, start, stop and delete resources.
    An orchestration plan is defined in YAML. The orchestration plan is used to orchestrate
    the deployment of applications as well as the post-deployment automation processes.
    The orchestration plan describes the applications and their lifecycle, and the
    relationships between components. This includes the connections between applications
    and where they are hosted - features that we have already discussed as vital for
    the edge cloud environment. With TOSCA, we can describe the infrastructure, the
    platform middleware tier and application layer on top of these. A PaaS product
    like Cloudify supporting TOSCA would take a TOSCA orchestration plan and then
    enacts this using workflows that traverse the graph of the components and issues
    commands to agents. These agents create the application components and glue them
    together. The agents use extensions, called plugins, which are adaptors between
    for instance a Cloudify configuration and the various infrastructure as a service
    (IaaS) and automation tool APIs. TOSCA framework and engine provide an orchestrator
    tool that allows the description of complex topologies and deployments. A dedicated
    topology and orchestration specification allows to specify the deployment of each
    service in the most ideal way. For simple stateless microservices, the best approach,
    for instance, is to use Docker and Kubernetes. A TOSCA blueprint can be used for
    more complex topologies that require more orchestration. Examples here could include
    a replicated and sharded mongo DB cluster or a more complex microservice. Though,
    a TOSCA blueprint can also be used for the basic container case to, for example,
    spawn a number of instances of a Docker image. Currently, TOSCA deals with container
    technologies, such as Docker and Rocket, as well as container management technologies,
    such as Kubernetes and Mesos, in an agnostic way. Due to this abstraction and
    interoperability, TOSCA is a suitable platform for defining a standard container
    orchestration specification that is portable across various cloud environments
    and container providers - which is, despite the current success and leadership
    of for instance Docker, a valuable property. In another direction that singles
    TOSCA out, there is a growing interest in NFV (Network Functions Virtualization)
    within the TOSCA community - an aspect at the infrastructure level (cf. earlier
    SDN comments) due to our focus on application and service management at the platform
    level. SECTION VII. Conclusion Edge clouds move the focus from heavy-weight data
    centre clouds to more lightweight virtualised resources, distributed to bring
    services to the users. They do, however, create challenges. We have identified
    lightweight virtualisation and the need to orchestrate the deployment of these
    service as key challenges. We looked at platform (PaaS) specifically as the application
    service packaging and orchestration is a key PaaS concern (through of course not
    limited to PaaS). Our aim here was to start with the recently emerging container
    technology and container cluster management to determine the suitability of these
    approaches for edge clouds through a technology review. The observations here
    support the current enthusiasm for this technology, but have also identified limitations.
    Some PaaS have started to address limitations in the context of programming (such
    as orchestration) and DevOps for clusters. The examples used above allow some
    observations. Firstly, containers are largely adopted for PaaS clouds. Secondly,
    standardisation by adopting emerging defacto standards like Docker or Kubernetes
    is also happening, though currently at a slower pace. Thirdly, development and
    operations are still at an early stage, particularly if complex orchestrations
    on distributed topologies are in question. We can observe that cloud management
    platforms are still at an earlier stage than the container platforms that they
    build on [14], [16]. While clusters in general are about distribution, the question
    emerges as to which extent this distribution reaches the edge of the cloud with
    small devices and embedded systems. Whether devices running small Linux distributions
    such as the Debian-based DSL (which requires around 50MB storage) can support
    container host and cluster management is a sample question. Recent 3rd generation
    PaaS are equally lightweight and aim to support the build-your-own-PaaS idea that
    is a first step. Container technology has the potential to substantially advance
    PaaS technology towards distributed heterogeneous clouds through lightweightness
    and interoperability. However, we can also conclude that significant improvements
    are still required to deal with data and network management aspects, as is providing
    an abstract development and architecture layer. Orchestration, as far as it is
    supported in implemented cluster solutions, is ultimately not sufficient. Cloud
    Computing and Commerce (IC4), an Irish national Technology Centre funded by Enterprise
    Ireland and the Irish Industrial Development Authority. ACKNOWLEDGMENTS This work
    was supported, in part, by Science Foundation Ireland grant 13/RC/2094 and co-funded
    under the European Regional Development Fund through the Southern & Eastern Regional
    Operational Programme to Lero - the Irish Software Research Centre (www.lero.ie)
    and by the Irish Centre for Cloud Computing and Commerce (IC4), an Irish national
    Technology Centre funded by Enterprise Ireland and the Irish Industrial Development
    Authority. Authors Figures References Citations Keywords Metrics More Like This
    Study of Security Issues in Platform-as-a-Service (PaaS) Cloud Model 2020 International
    Conference on Electrical, Communication, and Computer Engineering (ICECCE) Published:
    2020 Service-Level Interoperability Issues of Platform as a Service 2015 IEEE
    World Congress on Services Published: 2015 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2015
  relevance_score1: 0
  relevance_score2: 0
  title: Containers and Clusters for Edge Cloud Architectures -- A Technology Review
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/2898442.2898444
  analysis: '>'
  authors:
  - Brendan Burns
  - Brian Grant
  - David Oppenheimer
  - Eric Brewer
  - John Wilkes
  citation_count: 272
  full_citation: '>'
  full_text: ">\nacmqueue | january-february 2016   70\nsystem evolution\nT\nhough\
    \ widespread interest \nin software containers \nis a relatively recent \nphenomenon,\
    \ at Google we \nhave been managing Linux containers at scale for \nmore than\
    \ ten years and built three different container-\nmanagement systems in that time.\
    \ Each system was heavily \ninfluenced by its predecessors, even though they were\
    \ \ndeveloped for different reasons. This article describes the \nlessons we’ve\
    \ learned from developing and operating them. \nThe first unified container-management\
    \ system \ndeveloped at Google was the system we internally call Borg.7 \nIt was\
    \ built to manage both long-running services and batch \njobs, which had previously\
    \ been handled by two separate \nsystems: Babysitter and the Global Work Queue.\
    \ The latter’s \narchitecture strongly influenced Borg, but was focused on \n\
    batch jobs; both predated Linux control groups. Borg shares \nmachines between\
    \ these two types of applications as a \nway of increasing resource utilization\
    \ and thereby reducing \ncosts. Such sharing was possible because container support\
    \ \nin the Linux kernel was becoming available (indeed, Google \ncontributed much\
    \ of the container code to the Linux kernel), \nwhich enabled better isolation\
    \ between latency-sensitive \nuser-facing services and CPU-hungry batch processes.\
    \ \nLessons  \nlearned from \nthree container-\nmanagement \nsystems over  \n\
    a decade \nBRENDAN BURNS, \n BRIAN GRANT, \n DAVID OPPENHEIMER, \n ERIC BREWER,\
    \ AND \n JOHN WILKES, \n GOOGLE INC. \n1 of 24\nTEXT  \nONLY \nBorg, Omega, and\
    \ \nKubernetes\nacmqueue | january-february 2016   71\nsystem evolution\nAs more\
    \ and more applications were developed to run \non top of Borg, our application\
    \ and infrastructure teams \ndeveloped a broad ecosystem of tools and services\
    \ for \nit. These systems provided mechanisms for configuring \nand updating jobs;\
    \ predicting resource requirements; \ndynamically pushing configuration files\
    \ to running jobs; \nservice discovery and load balancing; auto-scaling; machine-\n\
    lifecycle management; quota management; and much more. \nThe development of this\
    \ ecosystem was driven by the \nneeds of different teams inside Google, and the\
    \ result was \na somewhat heterogeneous, ad-hoc collection of systems \nthat Borg’s\
    \ users had to configure and interact with, using \nseveral different configuration\
    \ languages and processes. \nBorg remains the primary container-management system\
    \ \nwithin Google because of its scale, breadth of features, and \nextreme robustness.\
    \ \nOmega,6 an offspring of Borg, was driven by a desire to \nimprove the software\
    \ engineering of the Borg ecosystem. \nIt applied many of the patterns that had\
    \ proved successful \nin Borg, but was built from the ground up to have a more\
    \ \nconsistent, principled architecture. Omega stored the state \nof the cluster\
    \ in a centralized Paxos-based transaction-\noriented store that was accessed\
    \ by the different parts of the \ncluster control plane (such as schedulers),\
    \ using optimistic \nconcurrency control to handle the occasional conflicts. \n\
    This decoupling allowed the Borgmaster’s functionality to \nbe broken into separate\
    \ components that acted as peers, \nrather than funneling every change through\
    \ a monolithic, \ncentralized master. Many of Omega’s innovations (including \n\
    2 of 24\nacmqueue | january-february 2016   72\nsystem evolution\nmultiple schedulers)\
    \ have since been folded into Borg. \nThe third container-management system developed\
    \ at \nGoogle was Kubernetes.4 It was conceived of and developed \nin a world\
    \ where external developers were becoming \ninterested in Linux containers, and\
    \ Google had developed \na growing business selling public-cloud infrastructure.\
    \ \nKubernetes is open source—a contrast to Borg and Omega, \nwhich were developed\
    \ as purely Google-internal systems. \nLike Omega, Kubernetes has at its core\
    \ a shared persistent \nstore, with components watching for changes to relevant\
    \ \nobjects. In contrast to Omega, which exposes the store \ndirectly to trusted\
    \ control-plane components, state in \nKubernetes is accessed exclusively through\
    \ a domain-\nspecific REST API that applies higher-level versioning, \nvalidation,\
    \ semantics, and policy, in support of a more \ndiverse array of clients. More\
    \ importantly, Kubernetes \nwas developed with a stronger focus on the experience\
    \ of \ndevelopers writing applications that run in a cluster: its main \ndesign\
    \ goal is to make it easy to deploy and manage complex \ndistributed systems,\
    \ while still benefiting from the improved \nutilization that containers enable.\
    \ \nThis article describes some of the knowledge gained \nand lessons learned\
    \ during Google’s journey from Borg to \nKubernetes. \nCONTAINERS \nHistorically,\
    \ the first containers just provided isolation of the \nroot file system (via\
    \ chroot), with FreeBSD jails extending \n3 of 24\nacmqueue | january-february\
    \ 2016   73\nsystem evolution\nthis to additional namespaces such as process IDs.\
    \ Solaris \nsubsequently pioneered and explored many enhancements. \nLinux control\
    \ groups (cgroups) adopted many of these ideas, \nand development in this area\
    \ continues today. \nThe resource isolation provided by containers has \nenabled\
    \ Google to drive utilization significantly higher than \nindustry norms. For\
    \ example, Borg uses containers to co-\nlocate batch jobs with latency-sensitive,\
    \ user-facing jobs on \nthe same physical machines. The user-facing jobs reserve\
    \ \nmore resources than they usually need—allowing them to \nhandle load spikes\
    \ and fail-over—and these mostly-unused \nresources can be reclaimed to run batch\
    \ jobs. Containers \nprovide the resource-management tools that make this \npossible,\
    \ as well as robust kernel-level resource isolation \nto prevent the processes\
    \ from interfering with one another. \nWe achieved this by enhancing Linux containers\
    \ concurrently \nwith Borg’s development. The isolation is not perfect, though:\
    \ \ncontainers cannot prevent interference in resources that \nthe operating-system\
    \ kernel doesn’t manage, such as level \n3 processor caches and memory bandwidth,\
    \ and containers \nneed to be supported by an additional security layer (such\
    \ as \nvirtual machines) to protect against the kinds of malicious \nactors found\
    \ in the cloud. \nA modern container is more than just an isolation \nmechanism:\
    \ it also includes an image—the files that make up \nthe application that runs\
    \ inside the container. Within Google, \nMPM (Midas Package Manager) is used to\
    \ build and deploy \ncontainer images. The same symbiotic relationship between\
    \ \nthe isolation mechanism and MPM packages can be found \n4 of 24\nacmqueue\
    \ | january-february 2016   74\nsystem evolution\nbetween the Docker daemon and\
    \ the Docker image registry. \nIn the remainder of this article we use the word\
    \ container to \nencompass both of these aspects: the runtime isolation and \n\
    the image. \nAPPLICATION-ORIENTED INFRASTRUCTURE \nO\nver time it became clear\
    \ that the benefits of \ncontainerization go beyond merely enabling higher \n\
    levels of utilization. Containerization transforms \nthe data center from being\
    \ machine-oriented to \nbeing application-oriented. This section discusses \n\
    two examples: \n3 Containers encapsulate the application environment, \nabstracting\
    \ away many details of machines and operating \nsystems from the application developer\
    \ and the deployment \ninfrastructure. \n3 Because well-designed containers and\
    \ container images \nare scoped to a single application, managing containers \n\
    means managing applications rather than machines. This \nshift of management APIs\
    \ from machine-oriented to \napplication oriented dramatically improves application\
    \ \ndeployment and introspection. \nApplication environment\nThe original purpose\
    \ of the cgroup, chroot, and namespace \nfacilities in the kernel was to protect\
    \ applications from \nnoisy, nosey, and messy neighbors. Combining these with\
    \ \ncontainer images created an abstraction that also isolates \napplications\
    \ from the (heterogeneous) operating systems \n5 of 24\nacmqueue | january-february\
    \ 2016   75\nsystem evolution\non which they run. This decoupling of image and\
    \ OS makes \nit possible to provide the same deployment environment in \nboth\
    \ development and production, which, in turn, improves \ndeployment reliability\
    \ and speeds up development by \nreducing inconsistencies and friction. \nThe\
    \ key to making this abstraction work is having a \nhermetic container image that\
    \ can encapsulate almost all \nof an application’s dependencies into a package\
    \ that can \nbe deployed into the container. If this is done correctly, \nthe\
    \ only local external dependencies will be on the Linux \nkernel system-call interface.\
    \ While this limited interface \ndramatically improves the portability of images,\
    \ it is not \nperfect: applications can still be exposed to churn in the OS \n\
    interface, particularly in the wide surface area exposed by \nsocket options,\
    \ /proc, and arguments to ioctl calls. Our \nhope is that ongoing efforts such\
    \ as the Open Container \nInitiative (https://www.opencontainers.org/) will further\
    \ \nclarify the surface area of the container abstraction. \nNonetheless, the\
    \ isolation and dependency minimization \nprovided by containers have proved quite\
    \ effective at \nGoogle, and the container has become the sole runnable \nentity\
    \ supported by the Google infrastructure. One \nconsequence is that Google has\
    \ only a small number of OS \nversions deployed across its entire fleet of machines\
    \ at any \none time, and it needs only a small staff of people to maintain \n\
    them and push out new versions. \nThere are many ways to achieve these hermetic\
    \ images. \nIn Borg, program binaries are statically linked at build time \nto\
    \ known-good library versions hosted in the company-wide \n6 of 24\nacmqueue |\
    \ january-february 2016   76\nsystem evolution\nrepository.5 Even so, the Borg\
    \ container image is not quite as \nairtight as it could have been: applications\
    \ share a so-called \nbase image that is installed once on the machine rather\
    \ than \nbeing packaged in each container. This base image contains \nutilities\
    \ such as tar and the libc library, so upgrades to \nthe base image can affect\
    \ running applications and have \noccasionally been a significant source of trouble.\
    \ \nMore modern container image formats such as Docker \nand ACI harden this abstraction\
    \ further and get closer to the \nhermetic ideal by eliminating implicit host\
    \ OS dependencies \nand requiring an explicit user command to share image data\
    \ \nbetween containers. \nContainers as the unit of management \nBuilding management\
    \ APIs around containers rather than \nmachines shifts the “primary key” of the\
    \ data center from \nmachine to application. This has many benefits: (1) it relieves\
    \ \napplication developers and operations teams from worrying \nabout specific\
    \ details of machines and operating systems; \n(2) it provides the infrastructure\
    \ team flexibility to roll out \nnew hardware and upgrade operating systems with\
    \ minimal \nimpact on running applications and their developers; and (3) \nit\
    \ ties telemetry collected by the management system (e.g., \nmetrics such as CPU\
    \ and memory usage) to applications \nrather than machines, which dramatically\
    \ improves \napplication monitoring and introspection, especially when \nscale-up,\
    \ machine failures, or maintenance cause application \ninstances to move. \nContainers\
    \ provide convenient points to register \n7 of 24\nB\nuilding \nmanage-\nment\
    \ APIs \naround \ncontainers \nrather than  \nmachines shifts \nthe “primary \n\
    key” of the data \ncenter from  \nmachine to  \napplication.\nacmqueue | january-february\
    \ 2016   77\nsystem evolution\ngeneric APIs that enable the flow of information\
    \ between \nthe management system and an application without \neither knowing\
    \ much about the particulars of the other’s \nimplementation. In Borg, this API\
    \ is a series of HTTP \nendpoints attached to each container. For example, the\
    \ \n/healthz endpoint reports application health to the \norchestrator. When an\
    \ unhealthy application is detected, it \nis automatically terminated and restarted.\
    \ This self-healing \nis a key building block for reliable distributed systems.\
    \ \n(Kubernetes offers similar functionality; the health check \nuses a user-specified\
    \ HTTP endpoint or exec command that \nruns inside the container.)\nAdditional\
    \ information can be provided by or for \ncontainers and displayed in various\
    \ user interfaces. For \nexample, Borg applications can provide a simple text\
    \ status \nmessage that can be updated dynamically, and Kubernetes \nprovides\
    \ key-value annotations stored in each object’s \nmetadata that can be used to\
    \ communicate application \nstructure. Such annotations can be set by the container\
    \ itself \nor other actors in the management system (e.g., the process \nrolling\
    \ out an updated version of the container). \nIn the other direction, the container-management\
    \ system \ncan communicate information into the container such as \nresource limits,\
    \ container metadata for propagation to \nlogging and monitoring (e.g., user name,\
    \ job name, identity), \nand notices that provide graceful-termination warnings\
    \ in \nadvance of node maintenance. \nContainers can also provide application-oriented\
    \ \nmonitoring in other ways: for example, Linux kernel cgroups \n8 of 24\nacmqueue\
    \ | january-february 2016   78\nsystem evolution\nprovide resource-utilization\
    \ data about the application, \nand these can be extended with custom metrics\
    \ exported \nusing HTTP APIs, as described earlier. This data enables the \ndevelopment\
    \ of generic tools like an auto-scaler or cAdvisor3 \nthat can record and use\
    \ metrics without understanding the \nspecifics of each application. Because the\
    \ container is the \napplication, there is no need to (de)multiplex signals from\
    \ \nmultiple applications running inside a physical or virtual \nmachine. This\
    \ is simpler, more robust, and permits finer-\ngrained reporting and control of\
    \ metrics and logs. Compare \nthis to having to ssh into a machine to run top.\
    \ Though it is \npossible for developers to ssh into their containers, they \n\
    rarely need to. \nMonitoring is just one example. The application-oriented \n\
    shift has ripple effects throughout the management \ninfrastructure. Our load\
    \ balancers don’t balance traffic \nacross machines; they balance across application\
    \ instances. \nLogs are keyed by application, not machine, so they can \neasily\
    \ be collected and aggregated across instances without \npollution from multiple\
    \ applications or system operations. \nWe can detect application failures and\
    \ more readily ascribe \nfailure causes without having to disentangle them from\
    \ \nmachine-level signals. Fundamentally, because the identity \nof an instance\
    \ being managed by the container manager lines \nup exactly with the identity\
    \ of the instance expected by the \napplication developer, it is easier to build,\
    \ manage, and debug \napplications. \nFinally, although so far we have focused\
    \ on applications \nbeing 1:1 with containers, in reality we use nested containers\
    \ \n9 of 24\nacmqueue | january-february 2016   79\nsystem evolution\nthat are\
    \ co-scheduled on the same machine: the outermost \none provides a pool of resources;\
    \ the inner ones provide \ndeployment isolation. In Borg, the outermost container\
    \ \nis called a resource allocation, or alloc; in Kubernetes, \nit is called a\
    \ pod. Borg also allows top-level application \ncontainers to run outside allocs;\
    \ this has been a source of \nmuch inconvenience, so Kubernetes regularizes things\
    \ and \nalways runs an application container inside a top-level pod, \neven if\
    \ the pod contains a single container. \nA common use pattern is for a pod to\
    \ hold an instance \nof a complex application. The major part of the application\
    \ \nsits in one of the child containers, and other child containers \nrun supporting\
    \ functions such as log rotation or click-\nlog offloading to a distributed file\
    \ system. Compared to \ncombining the functionality into a single binary, this\
    \ makes \nit easy to have different teams develop the distinct pieces \nof functionality,\
    \ and it improves robustness (the offloading \ncontinues even if the main application\
    \ gets wedged), \ncomposability (it’s easy to add a new small support service,\
    \ \nbecause it operates in the private execution environment \nprovided by its\
    \ own container), and fine-grained resource \nisolation (each runs in its own\
    \ resources, so the logging \nsystem can’t starve the main app, or vice versa).\n\
    \ \nOrchestration is the beginning, not the end \nThe original Borg system made\
    \ it possible to run disparate \nworkloads on shared machines to improve resource\
    \ \nutilization. The rapid evolution of support services in \nthe Borg ecosystem,\
    \ however, showed that container \n10 of 24\nacmqueue | january-february 2016\
    \   80\nsystem evolution\nmanagement per se was just the beginning of an environment\
    \ \nfor developing and managing reliable distributed systems. \nMany different\
    \ systems have been built in, on, and around \nBorg to improve upon the basic\
    \ container-management \nservices that Borg provided. The following partial list\
    \ gives \nan idea of their range and variety: \n3  Naming and service discovery\
    \ (the Borg Name Service, or \nBNS). \n3 Master election, using Chubby.2 \n3 Application-aware\
    \ load balancing. \n3  Horizontal (number of instances) and vertical (size of\
    \ an \ninstance) autoscaling. \n3  Rollout tools that manage the careful deployment\
    \ of new \nbinaries and configuration data. \n3  Workflow tools (e.g., to allow\
    \ running multijob analysis \npipelines with interdependencies between the stages).\
    \ \n3  Monitoring tools to gather information about containers, \naggregate it,\
    \ present it on dashboards, and use it to trigger \nalerts. \nThese services were\
    \ built organically to solve problems \nthat application teams experienced. The\
    \ successful ones \nwere picked up, adopted widely, and made other developers’\
    \ \nlives easier. Unfortunately, these tools typically picked \nidiosyncratic\
    \ APIs, conventions (such as file locations), and \ndepth of Borg integration.\
    \ An undesired side effect was to \nincrease the complexity of deploying applications\
    \ in the Borg \necosystem. \nKubernetes attempts to avert this increased complexity\
    \ \nby adopting a consistent approach to its APIs. For example, \n11 of 24\nacmqueue\
    \ | january-february 2016   81\nsystem evolution\nevery Kubernetes object has\
    \ three basic fields in its \ndescription: ObjectMetadata, Specification (or Spec),\
    \ \nand Status. \nThe Object Metadata is the same for all objects in \nthe system;\
    \ it contains information such as the object’s \nname, UID (unique identifier),\
    \ an object version number (for \noptimistic concurrency control), and labels\
    \ (key-value pairs, \nsee below). The contents of Spec and Status vary by object\
    \ \ntype, but their concept does not: Spec is used to describe the \ndesired state\
    \ of the object, whereas Status provides read-\nonly information about the current\
    \ state of the object. \nThis uniform API provides many benefits. Learning the\
    \ \nsystem is simpler: similar information applies to all objects, \nand writing\
    \ generic tools that work across all objects \nis simpler, which in turn enables\
    \ the development of a \nconsistent user experience. Learning from Borg and Omega,\
    \ \nKubernetes is built from a set of composable building blocks \nthat can readily\
    \ be extended by its users. A common API \nand object-metadata structure makes\
    \ that much easier. For \nexample, the pod API is usable by people, internal Kubernetes\
    \ \ncomponents, and external automation tools. To further \nthis consistency,\
    \ Kubernetes is being extended to enable \nusers to add their own APIs dynamically,\
    \ alongside the core \nKubernetes functionality. \nConsistency is also achieved\
    \ via decoupling in the \nKubernetes API. Separation of concerns between API \n\
    components means that higher-level services all share \nthe same common basic\
    \ building blocks. A good example of \nthis is the separation between the Kubernetes\
    \ replication \n12 of 24\nacmqueue | january-february 2016   82\nsystem evolution\n\
    controller and its horizontal auto-scaling system. A \nreplication controller\
    \ ensures the existence of the desired \nnumber of pods for a given role (e.g.,\
    \ “front end”). The \nautoscaler, in turn, relies on this capability and simply\
    \ \nadjusts the desired number of pods, without worrying about \nhow those pods\
    \ are created or deleted. The autoscaler \nimplementation can focus on demand—and\
    \ usage—\npredictions, and ignore the details of how to implement its \ndecisions.\
    \ \nDecoupling ensures that multiple related but different \ncomponents share\
    \ a similar look and feel. For example, \nKubernetes has three different forms\
    \ of replicated pods: \n3 ReplicationController: run-forever replicated \ncontainers\
    \ (e.g., web servers). \n3 DaemonSet: ensure a single instance on each node in\
    \ the \ncluster (e.g., logging agents). \n3 Job: a run-to-completion controller\
    \ that knows how to run \na (possibly parallelized) batch job from start to finish.\
    \ \nRegardless of the differences in policy, all three of these \ncontrollers\
    \ rely on the common pod object to specify the \ncontainers they wish to run.\
    \ \nConsistency is also achieved through common design \npatterns for different\
    \ Kubernetes components. The idea \nof a reconciliation controller loop is shared\
    \ throughout \nBorg, Omega, and Kubernetes to improve the resiliency of \na system:\
    \ it compares a desired state (e.g., how many pods \nshould match a label-selector\
    \ query) against the observed \nstate (the number of such pods that it can find),\
    \ and takes \nactions to converge the observed and desired states. \n13 of 24\n\
    acmqueue | january-february 2016   83\nsystem evolution\nBecause all action is\
    \ based on observation rather than a \nstate diagram, reconciliation loops are\
    \ robust to failures and \nperturbations: when a controller fails or restarts\
    \ it simply \npicks up where it left off. \nThe design of Kubernetes as a combination\
    \ of \nmicroservices and small control loops is an example \nof control through\
    \ choreography—achieving a desired \nemergent behavior by combining the effects\
    \ of separate, \nautonomous entities that collaborate. This is a conscious \n\
    design choice in contrast to a centralized orchestration \nsystem, which may be\
    \ easier to construct at first but tends to \nbecome brittle and rigid over time,\
    \ especially in the presence \nof unanticipated errors or state changes. \nTHINGS\
    \ TO AVOID \nW\nhile developing these systems we have learned \nalmost as many\
    \ things not to do as ideas that \nare worth doing. We present some of them here\
    \ \nin the hopes that others can focus on making \nnew mistakes, rather than repeating\
    \ ours. \nDon’t make the container system manage port numbers \nAll containers\
    \ running on a Borg machine share the host’s \nIP address, so Borg assigns the\
    \ containers unique port \nnumbers as part of the scheduling process. A container\
    \ will \nget a new port number when it moves to a new machine \nand (sometimes)\
    \ when it is restarted on the same machine. \nThis means that traditional networking\
    \ services such as the \nDNS (Domain Name System) have to be replaced by home-\n\
    14 of 24\nacmqueue | january-february 2016   84\nsystem evolution\nbrew versions;\
    \ service clients do not know the port number \nassigned to the service a priori\
    \ and have to be told; port \nnumbers cannot be embedded in URLs, requiring name-based\
    \ \nredirection mechanisms; and tools that rely on simple IP \naddresses need\
    \ to be rewritten to handle IP:port pairs. \nLearning from our experiences with\
    \ Borg, we decided \nthat Kubernetes would allocate an IP address per pod, \n\
    thus aligning network identity (IP address) with application \nidentity. This\
    \ makes it much easier to run off-the-shelf \nsoftware on Kubernetes: applications\
    \ are free to use \nstatic well-known ports (e.g., 80 for HTTP traffic), and \n\
    existing, familiar tools can be used for things like network \nsegmentation, bandwidth\
    \ throttling, and management. All of \nthe popular cloud platforms provide networking\
    \ underlays \nthat enable IP-per-pod; on bare metal, one can use an SDN \n(Software\
    \ Defined Network) overlay or configure L3 routing \nto handle multiple IPs per\
    \ machine. \nDon’t just number containers: give them labels \nIf you allow users\
    \ to create containers easily, they tend \nto create lots of them, and soon need\
    \ a way to group and \norganize them. Borg provides jobs to group identical tasks\
    \ \n(its name for containers). A job is a compact vector of one \nor more identical\
    \ tasks, indexed sequentially from zero. This \nprovides a lot of power and is\
    \ simple and straightforward, \nbut we came to regret its rigidity over time.\
    \ For example, \nwhen a task dies and has to be restarted on another machine,\
    \ \nthe same slot in the task vector has to do double duty: to \nidentify the\
    \ new copy and to point to the old one in case \n15 of 24\nacmqueue | january-february\
    \ 2016   85\nsystem evolution\nit needs to be debugged. When tasks in the middle\
    \ of the \nvector exit, the vector ends up with holes. The vector makes \nit very\
    \ hard to support jobs that span multiple clusters in \na layer above Borg. There\
    \ are also insidious, unexpected \ninteractions between Borg’s job-update semantics\
    \ (which \ntypically restarts tasks in index order when doing rolling \nupgrades)\
    \ and an application’s use of the task index (e.g., to \ndo sharding or partitioning\
    \ of a dataset across the tasks): if \nthe application uses range sharding based\
    \ on the task index, \nBorg’s restart policy can cause data unavailability, as\
    \ it \ntakes down adjacent tasks. Borg also provides no easy way \nto add application-relevant\
    \ metadata to a job, such as role \n(e.g., “frontend”), or rollout status (e.g.,\
    \ “canary”), so people \nencode this information into job names that they decode\
    \ \nusing regular expressions. \nIn contrast, Kubernetes primarily uses labels\
    \ to identify \ngroups of containers. A label is a key/value pair that contains\
    \ \ninformation that helps identify the object. A pod might \nhave the labels\
    \ role=frontend and stage=production, \nindicating that this container is serving\
    \ as a production \nfront-end instance. Labels can be dynamically added, \nremoved,\
    \ and modified by either automated tools or users, \nand different teams can manage\
    \ their own labels largely \nindependently. Sets of objects are defined by label\
    \ selectors \n(e.g., stage==production && role==frontend). Sets \ncan overlap,\
    \ and an object can be in multiple sets, so labels \nare inherently more flexible\
    \ than explicit lists of objects \nor simple static properties. Because a set\
    \ is defined by a \ndynamic query, a new one can be created at any time. Label\
    \ \n16 of 24\nacmqueue | january-february 2016   86\nsystem evolution\nselectors\
    \ are the grouping mechanism in Kubernetes, and \ndefine the scope of all management\
    \ operations that can span \nmultiple entities. \nEven in those circumstances\
    \ where knowing the identity \nof a task in a set is helpful (e.g., for static\
    \ role assignment and \nwork-partitioning or sharding), appropriate per-pod labels\
    \ \ncan be used to reproduce the effect of task indexes, though \nit is the responsibility\
    \ of the application (or some other \nmanagement system external to Kubernetes)\
    \ to provide \nsuch labeling. Labels and label selectors provide a general \n\
    mechanism that gives the best of both worlds.\n \nBe careful with ownership \n\
    In Borg, tasks do not exist independently from jobs. Creating \na job creates\
    \ its tasks; those tasks are forever associated \nwith that particular job, and\
    \ deleting the job deletes the \ntasks. This is convenient, but it has a major\
    \ drawback: \nbecause there is only one grouping mechanism, it needs \nto handle\
    \ all use cases. For example, a job has to store \nparameters that make sense\
    \ only for service or batch jobs \nbut not both, and users must develop workarounds\
    \ when the \njob abstraction doesn’t handle a use case (e.g., a DaemonSet \nthat\
    \ replicates a single pod to all nodes in the cluster). \nIn Kubernetes, pod-lifecycle\
    \ management components \nsuch as replication controllers determine which pods\
    \ \nthey are responsible for using label selectors, so multiple \ncontrollers\
    \ might think they have jurisdiction over a single \npod. It is important to prevent\
    \ such conflicts through \nappropriate configuration choices. But the flexibility\
    \ of labels \n17 of 24\nacmqueue | january-february 2016   87\nsystem evolution\n\
    has compensating advantages—for example, the separation \nof controllers and pods\
    \ means that it is possible to “orphan” \nand “adopt” containers. Consider a load-balanced\
    \ service \nthat uses a label selector to identify the set of pods to send \n\
    traffic to. If one of these pods starts misbehaving, that pod \ncan be quarantined\
    \ from serving requests by removing \none or more of the labels that cause it\
    \ to be targeted by \nthe Kubernetes service load balancer. The pod is no longer\
    \ \nserving traffic, but it will remain up and can be debugged in \nsitu. In the\
    \ meantime, the replication controller managing \nthe pods that implements the\
    \ service automatically creates \na replacement pod for the misbehaving one. \n\
    Don’t expose raw state \nA key difference between Borg, Omega, and Kubernetes\
    \ is \nin their API architectures. The Borgmaster is a monolithic \ncomponent\
    \ that knows the semantics of every API operation. \nIt contains the cluster management\
    \ logic such as the state \nmachines for jobs, tasks, and machines; and it runs\
    \ the \nPaxos-based replicated storage system used to record \nthe master’s state.\
    \ In contrast, Omega has no centralized \ncomponent except the store, which simply\
    \ holds passive \nstate information and enforces optimistic concurrency \ncontrol:\
    \ all logic and semantics are pushed into the clients of \nthe store, which directly\
    \ read and write the store contents. In \npractice, every Omega component uses\
    \ the same client-side \nlibrary for the store, which does packing/unpacking of\
    \ data \nstructures, retries, and enforces semantic consistency. \nKubernetes\
    \ picks a middle ground that provides the \n18 of 24\nA \nkey  \ndifference \n\
    between \nBorg, \nOmega,  \nand Kubernetes \nis in their API \narchitectures.\n\
    acmqueue | january-february 2016   88\nsystem evolution\nflexibility and scalability\
    \ of Omega’s componentized \narchitecture while enforcing system-wide invariants,\
    \ \npolicies, and data transformations. It does this by forcing \nall store accesses\
    \ through a centralized API server that \nhides the details of the store implementation\
    \ and provides \nservices for object validation, defaulting, and versioning. As\
    \ \nin Omega, the client components are decoupled from one \nanother and can evolve\
    \ or be replaced independently (which \nis especially important in the open-source\
    \ environment), \nbut the centralization makes it easy to enforce common \nsemantics,\
    \ invariants, and policies. \nSOME OPEN, HARD PROBLEMS\nE\nven with years of container-management\
    \ experience, \nwe feel there are a number of problems that we still \ndon’t have\
    \ good answers for. This section describes \na couple of particularly knotty ones,\
    \ in the hope of \nfostering discussion and solutions. \nConfiguration\nOf all\
    \ the problems we have confronted, the ones over \nwhich the most brainpower,\
    \ ink, and code have been spilled \nare related to managing configurations—the\
    \ set of values \nsupplied to applications, rather than hard-coded into them.\
    \ \nIn truth, we could have devoted this entire article to the \nsubject and still\
    \ have had more to say. What follows are a \nfew highlights. \nFirst, application\
    \ configuration becomes the catch-\nall location for implementing all of the things\
    \ that the \n19 of 24\nacmqueue | january-february 2016   89\nsystem evolution\n\
    container-management system doesn’t (yet) do. Over the \nhistory of Borg this\
    \ has included: \n3  Boilerplate reduction (e.g., defaulting task-restart policies\
    \ \nappropriate to the workload, such as service or batch jobs). \n3  Adjusting\
    \ and validating application parameters and \ncommand-line flags. \n3  Implementing\
    \ workarounds for missing API abstractions \nsuch as package (image) management.\
    \ \n3  Libraries of configuration templates for applications. \n3 Release-management\
    \ tools. \n3 Image version specification. \nTo cope with these kinds of requirements,\
    \ configuration-\nmanagement systems tend to invent a domain-specific \nconfiguration\
    \ language that (eventually) becomes Turing \ncomplete, starting from the desire\
    \ to perform computation \non the data in the configuration (e.g., to adjust the\
    \ amount \nof memory to give a server as a function of the number of \nshards\
    \ in the service). The result is the kind of inscrutable \n“configuration is code”\
    \ that people were trying to avoid by \neliminating hard-coded parameters in the\
    \ application’s source \ncode. It doesn’t reduce operational complexity or make\
    \ \nthe configurations easier to debug or change; it just moves \nthe computations\
    \ from a real programming language to a \ndomain-specific one, which typically\
    \ has weaker development \ntools such as debuggers and unit test frameworks. \n\
    We believe the most effective approach is to accept \nthis need, embrace the inevitability\
    \ of programmatic \nconfiguration, and maintain a clean separation between \n\
    computation and data. The language to represent the data \n20 of 24\nacmqueue\
    \ | january-february 2016   90\nsystem evolution\nshould be a simple, data-only\
    \ format such as JSON or YAML, \nand programmatic modification of this data should\
    \ be done \nin a real programming language, where there are well-\nunderstood\
    \ semantics, as well as good tooling. Interestingly, \nthis same separation of\
    \ computation and data can be seen \nin front-end development with frameworks\
    \ such as Angular \nthat maintain a crisp separation between the worlds of \n\
    markup (data) and JavaScript (computation). \nDependency management \nStanding\
    \ up a service typically also means standing up a \nseries of related services\
    \ (monitoring, storage, Continuous \nIntegration / Continuous Deployment (CI/CD),\
    \ etc). If an \napplication has dependencies on other applications, \nwouldn’t\
    \ it be nice if those dependencies (and any transitive \ndependencies they may\
    \ have) were automatically \ninstantiated by the cluster-management system? \n\
    To complicate things, instantiating the dependencies is \nrarely as simple as\
    \ just starting a new copy—for example, it \nmay require registering as a consumer\
    \ of an existing service \n(e.g., Bigtable as a service) and passing authentication,\
    \ \nauthorization, and billing information across those transitive \ndependencies.\
    \ Almost no system, however, captures, \nmaintains, or exposes this kind of dependency\
    \ information, so \nautomating even common cases at the infrastructure level \n\
    is nearly impossible. Turning up a new application remains \ncomplicated for the\
    \ user, making it harder for developers to \nbuild new services, and often results\
    \ in the most recent best \n21 of 24\nacmqueue | january-february 2016   91\n\
    system evolution\npractices not being followed, which affects the reliability\
    \ of \nthe resulting service. \nA standard problem is that it is hard to keep\
    \ dependency \ninformation up to date if it is provided manually, and at the \n\
    same time attempts to determine it automatically (e.g., by \ntracing accesses)\
    \ fail to capture the semantic information \nneeded to understand the result.\
    \ (Did that access have to \ngo to that instance, or would any instance have sufficed?)\
    \ \nOne possible way to make progress is to require that an \napplication enumerate\
    \ the services on which it depends, and \nhave the infrastructure refuse to allow\
    \ access to any others. \n(We do this for compiler imports in our build system.1)\
    \ The \nincentive would be enabling the infrastructure to do useful \nthings in\
    \ return, such as automatic setup, authentication, and \nconnectivity. \nUnfortunately,\
    \ the perceived complexity of systems \nthat express, analyze, and use system\
    \ dependencies has \nbeen too high, and so they haven’t yet been added to a \n\
    mainstream container-management system. We still hope \nthat Kubernetes might\
    \ be a platform on which such tools can \nbe built, but doing so remains an open\
    \ challenge. \nCONCLUSIONS\nA \ndecade’s worth of experience building container-\n\
    management systems has taught us much, and \nwe have embedded many of those lessons\
    \ into \nKubernetes, Google’s most recent container-\nmanagement system. Its goals\
    \ are to build on \nthe capabilities of containers to provide significant gains\
    \ \n22 of 24\nacmqueue | january-february 2016   92\nsystem evolution\nin programmer\
    \ productivity and ease of both manual and \nautomated system management. We hope\
    \ you’ll join us in \nextending and improving it.\nReferences \n1.  Bazel: {fast,\
    \ correct}—choose two; http://bazel.io. \n2.  Burrows, M. 2006. The Chubby lock\
    \ service for loosely \ncoupled distributed systems. Symposium on Operating \n\
    System Design and Implementation (OSDI), Seattle, WA. \n3. cAdvisor; https://github.com/google/cadvisor.\
    \ \n4. Kubernetes; http://kubernetes.io/.\n5.  Metz, C. 2015. Google is 2 billion\
    \ lines of code—and it’s \nall in one place. Wired (September);  http://www.wired.\n\
    com/2015/09/google-2-billion-lines-codeand-one-place/. \n6.  Schwarzkopf, M.,\
    \ Konwinski, A., Abd-el-Malek, M., Wilkes, \nJ. 2013. Omega: flexible, scalable\
    \ schedulers for large \ncompute clusters. European Conference on Computer \n\
    Systems (EuroSys), Prague, Czech Republic. \n7.  Verma, A., Pedrosa, L., Korupolu,\
    \ M. R., Oppenheimer, D., \nTune, E., Wilkes, J. 2015. Large-scale cluster management\
    \ \nat Google with Borg. European Conference on Computer \nSystems (EuroSys),\
    \ Bordeaux, France. \nLOVE IT, HATE IT? LET US KNOW feedback@queue.acm.org\nBrendan\
    \ Burns (@brendandburns) is a software engineer \nat Google, where he co-founded\
    \ the Kubernetes project. \nHe received his Ph.D. from the University of Massachusetts\
    \ \nAmherst in 2007. Prior to working on Kubernetes and cloud, \n23 of 24\nacmqueue\
    \ | january-february 2016   93\nsystem evolution\nhe worked on low-latency indexing\
    \ for Google’s web-search \ninfrastructure.\nBrian Grant is a software engineer\
    \ at Google. He was \npreviously a technical lead of Borg and founder of the Omega\
    \ \nproject and is now design lead of Kubernetes.\nDavid Oppenheimer is a software\
    \ engineer at Google and a \ntech lead on the Kubernetes project. He received\
    \ a PhD from \nUC Berkeley in 2005 and joined Google in 2007, where he \nwas a\
    \ tech lead on the Borg and Omega cluster-management \nsystems prior to Kubernetes.\n\
    Eric Brewer is VP Infrastructure at Google and a professor at \nUC Berkeley, where\
    \ he pioneered scalable servers and elastic \ninfrastructure.\nJohn Wilkes has\
    \ been working on cluster management and \ninfrastructure services at Google since\
    \ 2008. Before that, he \nspent time at HP Labs, becoming an HP and ACM Fellow\
    \ in \n2002. He is interested in far too many aspects of distributed \nsystems,\
    \ but a recurring theme has been technologies that \nallow systems to manage themselves.\
    \ In his spare time he \ncontinues, stubbornly, trying to learn how to blow glass.\n\
    Copyright © 2016 by the ACM. All rights reserved. \n24 of 24\n"
  inline_citation: '>'
  journal: ACM queue
  limitations: '>'
  pdf_link: http://dl.acm.org/ft_gateway.cfm?id=2898444&type=pdf
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Borg, Omega, and Kubernetes
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/tcc.2017.2702586
  analysis: '>'
  authors:
  - Claus Pahl
  - Antonio Brogi
  - Jacopo Soldani
  - Pooyan Jamshidi
  citation_count: 211
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Cloud Co... >Volume:
    7 Issue: 3 Cloud Container Technologies: A State-of-the-Art Review Publisher:
    IEEE Cite This PDF Claus Pahl; Antonio Brogi; Jacopo Soldani; Pooyan Jamshidi
    All Authors 207 Cites in Papers 9911 Full Text Views Abstract Document Sections
    1 Introduction 2 Container Architectures and Their Management 3 Research Methodology
    4 A Classification Framework for Cloud Container Technologies 5 Results and Visualisation
    Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: Containers as a lightweight technology to virtualise applications have
    recently been successful, particularly to manage applications in the cloud. Often,
    the management of clusters of containers becomes essential and the orchestration
    of the construction and deployment becomes a central problem. This emerging topic
    has been taken up by researchers, but there is currently no secondary study to
    consolidate this research. We aim to identify, taxonomically classify and systematically
    compare the existing research body on containers and their orchestration and specifically
    the application of this technology in the cloud. We have conducted a systematic
    mapping study of 46 selected studies. We classified and compared the selected
    studies based on a characterisation framework. This results in a discussion of
    agreed and emerging concerns in the container orchestration space, positioning
    it within the cloud context, but also moving it closer to current concerns in
    cloud platforms, microservices and continuous development. Published in: IEEE
    Transactions on Cloud Computing ( Volume: 7, Issue: 3, 01 July-Sept. 2019) Page(s):
    677 - 692 Date of Publication: 09 May 2017 ISSN Information: DOI: 10.1109/TCC.2017.2702586
    Publisher: IEEE Funding Agency: SECTION 1 Introduction Containerisation is a technology
    to virtualise applications in a lightweight way that has resulted in a significant
    uptake in cloud applications management. How to orchestrate the construction and
    deployment of containers individually and in clusters has become a central problem
    [13]. There has not been a secondary study of research on container technologies
    in the cloud that would allow to assess the maturity in general and identify trends,
    research gaps and future directions. Given the growing interest in containers,
    their management and orchestration in cloud, there is a need to explore current
    research. Secondary studies identify, classify and synthesise a comparative overview
    of state-of-the-research and enable an assessment of ongoing work [7], [15]. We
    opt for a systematic mapping study (SMS) as it is more suitable in mapping out
    and structuring new areas of investigation. We identify, taxonomically classify
    and systematically compare the existing research body on container technologies
    and its application in the cloud, aiming to extract a better understanding of
    Platform-as-a-Service (PaaS) as middleware built on containers for application
    packaging and as a deployment infrastructure. We have conducted a systematic mapping
    study of 46 selected studies (Table 2), spanning over a decade from 2007 onwards.
    We classified and compared the selected studies based on a characterisation framework.
    TABLE 1 Research Questions (RQ) TABLE 2 Publications Selected-Reference Data Our
    mapping study resulted in a knowledge base of current research approaches, methods,
    techniques, best practices and experiences used in cloud architecture, with a
    particular attention to cloud application development and management. Our study
    revealed that container technologies research is still in a formative stage. More
    experimental and empirical evaluation of benefits is needed. Our study also showed
    a lack of tool support to automate and facilitate container management and orchestration,
    specifically in clustered cloud architectures. The results of our mapping study
    show growing interests and usage of container-based technologies (such as LXC
    or Docker) as lightweight virtualisation solutions at Infrastructure-as-a-Service
    (IaaS) level, and as application management solutions at PaaS level. We can observe
    that containers positively impact on both development and deployment aspects.
    For instance, architecting in the cloud moves towards DevOps-based approaches,
    supporting a continuous development and deployment pipeline taking into account
    cloud-native architecture solutions based on containers and their orchestration
    (Brunnert et al., 2015). The results show that containers can support continuous
    development in the cloud based on cloud-native platform services for development
    and deployment, but do require advanced orchestration support. Container-based
    orchestration techniques hence emerge as a mechanism to orchestrate computation
    in cloud-based, clustered environments. The results of our study show that such
    techniques are seen to balance the need of technical quality management, e.g.,
    optimised resource utilisation and performances, which is a cost factor in the
    cloud (due to its utility pricing principle). Our systematic mapping study aims
    to benefit, first, researchers in software engineering, distributed systems and
    cloud computing, who need an identification of relevant studies. A systematic
    presentation of research provides a body of knowledge to develop theory and solutions,
    analyse research implications and establish future dimensions. It also benefits
    practitioners interested in understanding the available methods, techniques and
    tools as well as their constraints and maturity level. This paper is structured
    as follows. Section 2 describes background and related research to position this
    work. Section 3 explains the research methodology, research questions and scope.
    Section 4 provides a characterisation framework for cloud container orchestration.
    Section 5 presents the results of the mapping study, followed by an analysis of
    its limitations. Section 6 discusses findings, implications and trends. SECTION
    2 Container Architectures and Their Management The cloud uses virtualisation techniques
    to achieve elasticity of large-scale shared resources [12]. Virtual machines (VMs)
    are typically the backbone at the infrastructure layer. Containerisation in contrast
    allows a lightweight virtualisation through the bespoke construction of containers
    as application packages from individual images (generally retrieved from an image
    repository) that consume less resources and time. They also support a more interoperable
    application packaging needed for portable, interoperable software applications
    in the cloud [13]. Containerisation is based on the capability to develop, test
    and deploy applications to a large number of servers and also to interconnect
    these containers. Containers address consequently concerns at the cloud PaaS level.
    Given the overall importance of the cloud, a consolidating view on current activities
    is important. 2.1 Container Technology Principles A container holds packaged self-contained,
    ready-to-deploy parts of applications and, if necessary, middleware and business
    logic (in binaries and libraries) to run the applications. Tools like Docker are
    built around container engines where containers act as portable means to package
    applications. This results in the need to manage dependencies between containers
    in multi-tier applications. An orchestration plan can describe components, their
    dependencies and their lifecycle in a layered plan. A PaaS cloud can then execute
    the workflows from the plan through agents (like a container engine). PaaS clouds
    can consequently support the deployment of applications from containers. Orchestration
    subsumes here their coordinated construction, deployment and ongoing management
    [10]. Many container solutions are based on Linux LXC techniques. Recent Linux
    distributions-part of the Linux container project LXC-provide kernel mechanisms
    such as namespaces and cgroups to isolate processes on a shared operating system
    [S5]. Docker is the most popular container solution at the moment and shall be
    used to illustrate containerisation. A Docker image is made up of file systems
    layered over each other, similar to the Linux virtualisation stack, using the
    LXC mechanisms, see Fig. 1 (right). Docker uses a union mount to add a writable
    file system on top of the read-only file system. This allows multiple read-only
    file systems to be stacked on top of each other. This property can be used to
    create new images by building on top of base images. Only the top layer is writable,
    which is the container itself. Fig. 1. Container cluster architectures. Show All
    Containerisation facilitates the step from single applications in containers to
    clusters of container hosts that can run containerised applications across cluster
    hosts [S9]. The latter benefits from the built-in interoperability of containers.
    Individual container hosts are grouped into interconnected clusters, illustrated
    in Fig. 1 (left). Each cluster consists of several (host) nodes. Application services
    are logical groups of containers from the same image. Application services allow
    scaling an application across different host nodes. Volumes are mechanisms used
    for applications that require data persistence. Containers can mount these volumes
    for storage. Links allow two or more containers to connect and communicate. The
    set-up and management of these container clusters requires orchestration support
    for inter-container communication, links and service assemblies [13]. 2.2 Cloud-Based
    Container Architectures Container orchestration deals not only with turning applications
    on or off (i.e., start or stop containers) and moving them among servers. We define
    orchestration as constructing and continuously managing possibly distributed clusters
    of container-based software applications. Container orchestration allows users
    to define how to coordinate the containers in the cloud when a multi-container
    application is deployed. Container orchestration defines not only the initial
    deployment of containers, but also the management of the multi-containers as a
    single entity. It takes care of availability, scaling and networking of containers.
    Essentially cloud-based container construction is a form of orchestration within
    the distributed cloud environment. The cloud can be seen as a distributed and
    tiered architecture, see Fig. 2, with core infrastructure, platform and software
    application tiers distributed across multi-cloud environments [1]. Container technologies
    can help. As such, container technologies will play a central role in the future
    of application management, in particular in the cloud PaaS context. Fig. 2. Cloud
    reference architecture model. Show All Recen tly popular microservice-based architectures
    can be realised in this cloud framework through containers [8], [9]. Given this
    change in architecting, a secondary study can help practitioners for their decision
    making in terms of the correct technology choice. 2.3 State-of-the-Art The mechanism
    we use to review cloud container techniques is that of a systematic mapping study.
    Reviews can be distinguished into two forms. Systematic Literature Reviews (SLR)
    suit summative analyses of mature fields, based on possibly larger bodies of literature.
    Systematic Mapping Studies are suitable to determine the structure of the type
    of research reports, visual categorisation, useful if there is a lack of high-quality
    primary studies. SMSs are typically less detailed, but they more appropriate for
    our purposes. As part of our paper selection process, we extracted six review
    papers that are related to our aim (see Table 2). Five out of these six qualify
    as technology reviews, i.e., they overview and assess container technologies.
    Study [S3] covers virtualisation basics and container construction/management.
    The focus is more on deployment than development. Study [S4] clearly addresses
    virtualisation basics only, but from deployment and development perspectives.
    Study [S6] is then more comprehensive, including clusters and both deployment
    and development perspectives. Study [S9] is similar to [S6], but has less quality
    management concerns covered. Study [S19] focusses like [S4] on virtualisation
    basics, but specifically on performance in HPC and computation/storage intensive
    applications. Slightly different in the approach is study [S17], which is more
    organised around research coverage rather than only technology. However, a systematic
    coverage of literature (like the one we propose in this paper) is missing. SLRs
    and SMSs entail to identify, classify and compare existing evidence on the use
    of container technologies specifically in cloud environments through a characterisation
    framework. As highlighted above, some technology reviews exist, but these concentrate
    on technology and do not capture research efforts and directions systematically.
    SECTION 3 Research Methodology 3.1 A Systematic Mapping Process SMSs reduce bias
    through a rigorous sequence of methodological steps to search and classify literature.
    They rely on well-defined and evaluated review protocols to extract, analyse and
    document results. We follow the process presented in [15] with a three-step review
    that includes planning, conducting and documenting. The review is complemented
    by an evaluation of each step''s outcome. Furthermore, we provide an additional
    characterisation framework for the study context. We have adapted and applied
    a systematic mapping to cloud technology in a study focusing on container orchestration.
    The essential process steps of our systematic mapping study are definition of
    research questions, conducting the search for relevant papers, screening of papers,
    keywording of abstracts and data extraction and mapping. Each process step has
    an outcome, the final outcome of the process being the systematic map: Now, the
    individual steps of the three-step process above will be outlined. Based on the
    objectives, we first specify the research questions and the review scope in order
    to formulate search strings for literature extraction. Identify the Scope of our
    SMS. We already discussed the need for a SMS. We can also clarify the general
    goal and scope of the study using the Population, Intervention, Comparison, Outcome
    (PICO) criteria [7]: Define and Evaluate Review Protocol. We developed a protocol
    based on [15] and on our experience with SLRs [4], [5]. Conducting the review
    starts with the study selection and results in extracted data and synthesised
    information. We specifically focus on orchestration to capture the trend towards
    distributed container architectures from a research perspective (using orchestration
    as a broad inclusive term). 3.2 Definition of Research Questions (Review Scope)
    As the next activity, we define the research questions to help shaping the review
    protocol, see Table 1. The main goal of a systematic mapping study is to provide
    an overview of a research area and to identify the quantity and type of research
    and results available within it. We can map the frequencies of publication over
    time to identify trends. A secondary goal is to identify the forums in which research
    has been published. These goals are reflected in the research questions (RQs).
    3.3 Search for Primary Studies The selection of search terms is based on [15]
    and guided by the research questions. The primary studies are typically identified
    by using search strings on scientific databases or browsing manually through conference
    proceedings or journals. A common approach to identify the search string is to
    structure them in PICO terms, which takes into account the research questions.
    Keywords for the search string can be taken from each aspect of the structure.
    It is worth noting that, differently from what is suggested by Petersen et al.
    [15], we do not consider specific outcomes or experimental designs in our study.
    We avoided this restriction since we wanted a broad overview of the research area
    as a whole. If we had only considered certain types of studies the overview could
    have been biased and the map incomplete. Some sub-topics might be over- or under-represented
    for certain study methods. This difference is also reflected in the search string
    (clou d ∗ ∨PaaS)∧(containe r ∗ ) ∧(orchestrat e ∗ ∨cluste r ∗ ∨manag e ∗ ), View
    Source where ‘*’ matches lexically related terms. Based on the PICO criteria,
    we chose for 1) Population, here specifically the Technology perspective, the
    search string (cloud* OR PaaS) AND (container*) AND (orchestrate* OR (cluster*
    OR manage*)). Initially, further PICO categories were considered, but not applied
    in the search term: Population-Product perspective: Docker OR Kubernetes OR Diego
    OR Rocket OR LXC OR ... Intervention: experimental OR empirical OR technical Comparison:
    n/a Outcome: framework OR theory OR architecture OR design OR language OR use
    case OR case study The aspects 1 (Product perspective), 2 and 4 were not applied
    to avoid any incompleteness, but have been considered in the following inclusion/exclusion
    consideration (part III.D below). Given that this is a recent concern in cloud
    computing, the corresponding forums are possibly not fully indexed, causing the
    need for an initial wider, partly manual search. We started with a wider search
    (i.e., reduced list of terms as suggested above) to establish an overall body
    of research, which we then narrowed down towards focus and quality. The choice
    of databases we considered is: IEEE Xplore, ACM Digital Library, Science Direct,
    ISI Web of Science, SpringerLink, INSPEC, EI Compendex, DBLP. Given the recency
    of the field and concerns with indexing, Google Scholar played the key role for
    the initial selection before the inclusion and exclusion stage. Since we used
    our primary search criteria on title and abstract, this resulted in a high number
    of irrelevant studies, which were further refined with a secondary search and
    manual screening-focusing on relevance and resulting then in 46 studies after
    inclusion/exclusion and quality control. 3.4 Screening of Papers for Inclusion/Exclusion
    The Initial Selection step includes screening of titles and abstracts of potential
    studies. We use inclusion and exclusion criteria to exclude studies that are not
    relevant to answer the research questions. The criteria below show that the research
    questions influence the inclusion and exclusion criteria: We started the selection
    with publications from 2007 with the emergence of the LXC Linux Container technology
    as a solution for cloud virtualisation and included all relevant publications
    until the end of 2016. The Final Selection step is a validation scan of the studies,
    considering methods for cloud container orchestration and tool support and details
    of the evaluation approach. At the end, 46 studies were selected, listed in Table
    2. Qualitative Assessment of Included Studies. For the 46 included studies, we
    primarily focused on the technical rigor of content presented. We based our assessment
    on having peer-reviewed contributions and selecting those where the content actually
    matches the title and abstract based initial selection. Data Extraction and Synthesis.
    In order to record extracted data from the selected studies, we followed [15]
    using a structured format based on characterisation dimensions that cover technical,
    domain-specific categories as well as generic study classification categories.
    3.5 Keywording of Abstracts (Classification Scheme) Key to our process is keywording
    to develop the classification scheme and ensuring that the scheme takes the existing
    studies into account. First, the reviewers involved read abstracts of papers and
    looked for keywords and concepts that demonstrate the contribution and also the
    context of each paper. The set of keywords from the different papers considered
    were then combined into a classification framework to develop a high level understanding
    of the subject, which is representative of the underlying population. We combined
    a number of top-level categories, both technical domain-specific concerns as well
    as those on the organisation of the research. The generic categories are adopted
    from the literature, e.g., the contribution type, as in the table below. The remaining
    categories are specific to container and cloud technology and aim to answer the
    research questions (specifically RQ1.1 to RQ1.5) in a targeted way. Generic Concerns:
    Research Contribution, Evaluation Method, Forum, Community, Domain, and Motivation
    for Technology Adoption. Technology-Specific Concerns: Technology Stack, Management
    Services, Architecture Setting, and Tools/Platforms/Technology. This technical
    perspective is highlighted in Fig. 3 where these technical concerns are visualised.
    The four main concerns Technology Stack, Architecture, Management and Tools/Platforms/Technology
    are aligned with the research questions RQ1.2 to RQ1.4, whereby we split RQ1.3
    into a conceptual, methodology-oriented Architecture aspect and a platform-oriented
    Management aspect that covers the available support services. The concerns are
    centred around the Technology Stack which is about the container technology itself
    and its internal mechanisms, whereas the surrounding three others are about the
    cloud environment in which containers are used and supported. The categorisations
    within the top-level concerns have emerged from the keyword extraction and have
    been aligned with the 4 top-level RQ1-oriented categories. Fig. 3. Categories
    of cloud container orchestration concerns. Show All A first-round keyword extraction
    has been used to validate the categories, i.e., that concrete terms extracted
    from the studies occur as instances of the categorisation scheme. 3.6 Data Extraction
    and Mapping of Studies (Systematic Map) When the classification scheme is defined,
    the actual data extraction is the process of categorising the relevant studies
    into the scheme1 The classification scheme has evolved during the data extraction
    (adding new categories and merging or splitting existing ones), based on further
    input processing and feedback received from independent experts. Data during this
    process has been gathered in a spreadsheet table to document the data extraction
    process. From the final table, the frequencies of publications in each category
    can be calculated. This allows to see which categories have been emphasized in
    past research and thus to identify gaps and possibilities for future research.
    We combined two maps-on generic research concerns and on domain-specific technology
    concerns. For the two maps, we use different ways of presenting and analysing
    the results. Common generic map targets are: Trends (by year), Forums, and Frequency
    (by topic). The technology map targets construction aspects (Technology Stack)
    and management aspects (Architecture and Management Services or Tools, Platforms
    and Technologies). The mapping is here from an enriched classification framework
    to the current research coverage (as shown by the selected papers). The map is
    illustrated in Section 5 using statistics in form of tables and pie charts, showing
    the frequencies of publications in each category. SECTION 4 A Classification Framework
    for Cloud Container Technologies We first introduce a reference model for an architecture-centric
    classification of cloud container technologies that helps to demonstrate current
    research at a conceptual level and identify trends and research directions. We
    propose this classification framework, see Table 3, to categorise the primary
    studies. This framework uses common terms from software engineering methods with
    methodological support, architecture, tool support and applications. The top-level
    headings (column 1) were already discussed in their alignment with the research
    questions and illustrated in Fig. 3. The subheadings (columns 2 to 4, if applicable)
    were identified, taking into account the research questions, after a first-round
    scan of selected studies in order to ensure the validity of the framework. These
    align with categorisations found in technology reviews [13] for the technology
    stack, management services, cloud architecture settings and the cloud technologies.
    The concrete terms (last column on the right-hand side of each row) are the terms
    extracted from the study. We organised them manually into the subheadings above.
    TABLE 3 Classification Framework The framework has initially undergone several
    iterations between the authors of this study and have then been validated by five
    external experts at three organisations in three countries. This has resulted
    in some corrections and amendments of the first version based on the extraction.
    SECTION 5 Results and Visualisation Table 4 overviews all selected studies based
    on the most important categories. The table positions each study using the classification
    framework, but also to obtain a first overview of the coverage of all studies
    together. For the aspects ‘technology stack’ and ‘management’, we only capture
    the focus as in these categories multiple occurrences of terms were possible (for
    which there is not sufficient space in the table). For the architecture setting
    and the research aspects, we listed all terms extracted. It is worth noting that
    review papers (a) tend to cover the technology stack more widely and solution
    papers tend to focus more on specific layers, (b) there is more work on deployment
    and management services than design and architecture concerns, (c) there is an
    equal spread between IaaS and PaaS focus, and (d) that mainly experiments and
    case studies are used for evaluation. TABLE 4 Study Comparison-Selected Categories.
    We categorise the selected papers in terms of concerns such as publication format,
    forum and technical contribution. We also present the key terms extracted from
    the studies. The results are discussed and the validity of the results and their
    impact for future research is addressed. The two maps-generic and technology-specific-are
    discussed separately in this section. We hereafter present the results of our
    study in more detail (generic and domain-specific maps are presented separately).
    We also discuss the validity of our results and their potential impact for future
    research. 5.1 Overview of Primary Studies-Generic Attributes In order to examine
    the current state of research on cloud container orchestration, the following
    questions apply: When did research on container technologies become active in
    the cloud computing community? What are the fora in which work on cloud container
    technologies has been published? On which cloud-related communities does the focus
    lie? How is cloud container technologies research reported and what is the maturity
    level of the research in this field within cloud computing? 5.1.1 Temporal Overview
    of Studies With only a few studies on container orchestration in the early years
    after the LXC introduction, there has been a dramatic increase since the emergence
    of tools like Docker, signalling a significant concern, see Fig. 4. The 2007 paper
    ‘Container-based operating system virtualisation: a scalable, high-performance
    alternative to hypervisors’ is however the most cited one. It can be considered
    a pioneer in looking at containers as a solution for cloud computing, then confirmed
    by the interests raised in that paper three years later. This paper brings container
    technology into the cloud research domain. However, there was not much open source
    technology in existence that researchers could consider. As a consequence, only
    after the recent introduction of Docker containers, researcher picked up this
    direction in research more strongly. Fig. 4. Study distribution by Year. At the
    time of writing, the amount of (already indexed) paper published in 2016 is lower
    than that of 2015. This is most probably because of the delays induced by the
    online publication of proceedings, by the review processes in journals, and by
    online indexing leaving the 2016 list of papers inevitably incomplete. Show All
    5.1.2 Publication Fora/Communities and Formats We distinguish communities and
    formats. Fora/Communities. We have categorised the publication fora into the computing
    fields as follows (Fig. 5): Software Engineering, Autonomic Computing, Distributed
    Systems, Cloud and Big Data, Network/OS, Application. Fig. 5. Study distribution
    by community. Show All Publications did occur mainly in the now established cloud
    computing field, as containers can play an important role in cloud (especially
    PaaS) virtualisation. Other areas include the networks, distributed systems and
    software engineering communities. We can note that there is not much work in terms
    of software development for containers and potentials of auto-scaling with containers.
    Formats. Regarding the sources, we recognise and distinguish the following peer-reviewed
    publication formats: journals, magazines, conferences and workshops, and books,
    see Fig. 6. At this formative stage, we can see mainly workshops and conferences,
    with magazines and journals often being survey papers. Researchers go mainly for
    conferences/workshops, as they can provide feedback and publish results in a shortest
    time, which is of high value in this fast developing area. Fig. 6. Study distribution
    by publication format. Show All 5.1.3 Research Methods Contribution Type. In Fig.
    7, the primary studies are summarised according to their contribution type. Solution
    proposals dominate, with little on validation and evaluation at this (formative)
    stage. Reviews are technology reviews rather than SLRs. More comparisons among
    existing solutions are needed (for which our classification scheme can be a compass
    to organise such solutions). Fig. 7. Study distribution by contribution type.
    Show All Evaluation Method. Given the relative immaturity of the domain, the evaluations
    lack detailed experience reports and proofs, while sample implementations as experience
    reports and some controlled experiments have been reported-which matches the solution
    proposals as the contribution types. Fig. 8 looks at the evaluation methods used.
    Figs. 10, 11, 12, 13, 14, 15, and 16 complement this by looking more specifically
    at the technical contribution itself from different perspectives. No mathematical/theoretical
    foundations are behind the solutions we analysed (as all of them are technological
    solutions, rather than theoretical approaches). This provides a pointer for further
    investigation/future work on providing container orchestration with theoretical
    foundations. Summary. We can observe a relatively young field, gaining more and
    more interest over the last years, especially (for conferences) in the cloud domain..
    Fig. 8. Study distribution by evaluation method. Show All 5.2 Overview of Primary
    Studies-Domain-Specific Attributes Now we focus on the technical aspects of the
    classification. We will visualise the data again in pie-charts (%-based) and provide
    the respective total numbers in tables. We also provide some additional groupings
    of terms to clarify further the priorities set by research.2 5.2.1 Motivation
    Ease of deployment and lightweight resource management are seen as the key benefits.
    Large-scale and automatic deployment are also key motivations, see Fig. 9. Hence,
    ease of deployment/management of large-scale heterogenous applications are the
    main drivers (even before lightweight resource management). Fig. 9. Study distribution
    by motivation. Show All 5.2.2 Technology Stack The technology stack covers the
    layered construction of virtual resources, which links platform to middleware
    and orchestration aspects within PaaS, Fig. 10. The focus is on container construction
    and management (more management than construction), with a good amount of work
    on virtualisation basics, but also some recent work on clusters. Fig. 10. Study
    distribution by technology stack layer. Show All Technology Stack-Virtualisation
    Basics: The results here are quite obvious (containerisation is a form of virtualisation),
    but there is some notable interest in isolation. This is mainly geared towards
    addressing the security concerns that are due to the container-based virtualisation
    (where containers are essentially guest processes on a shared operating system,
    and malicious users could cause more security issues with respect to virtual machines
    managed by hypervisors). However, according to [S3], containers promise the same
    level of isolation and security as VMs. No dependence on hardware emulation provides
    performance benefits over full virtualisation, but restricts the number of supported
    operating systems. As this is not often required by PaaS providers, containers
    are suited for providing low-overhead isolation. Container features like cgroups
    can limit device access for containers from a security perspective, but filesystem,
    network and memory isolation need more attention. Technology Stack-Container Construction
    is more about (functional) composition. The focus is on the actual image-based
    construction (assembly) process. Technology Stack-Container Management is on the
    other hand operations and management oriented. Core execution and wider (quality)
    management are equally considered; there is some interest in communications of
    multi-container applications. Technology Stack-Cluster Construction and Management
    as distributed architectures reflect the previous two observations for containers.
    There is a considerable interest in the core virtualisation aspects of containers,
    looking into the optimisation of performance for storage [S19] and network aspects
    [S13]. Other directions beyond orchestration are security concerns [11] such as
    isolation. 5.2.3 Management Services The Management Services address both development
    and operations/management perspectives by looking at the support services that
    are provided by a container platform. For example, software engineering activities
    can be supported by middleware/platform services, software architecture and quality
    assurance, performance engineering and distributed systems concerns. These are
    categorised into architecture/construction, execution management and quality management,
    see Fig. 11. Fig. 11. Study distribution by management service. Show All Management
    Services-Architecture/Construction: The main focus is on classical single-system
    architecture concerns, but also on system integration (architecture and distribution/topology
    management) and some quality and change management can be noted: A grouping shows
    that construction of application containers and their integration are the top
    concerns: Integration in environments with interoperability challenges due to
    heterogeneity is the second strongest group. The benefit of container technology
    to aid the orchestration of applications in distributed topologies is highlighted
    in [S9], the emerging trend towards edge cloud computing and its orchestration
    needs are also noted. Cluster management is instead covered on [S21], [S25] and
    [S26]. Management Services-Execution Management: the following concerns have been
    recorded: This can again be grouped. There is a balance between set up/preparation,
    core execution management, wider (quality) management and, almost as much, enabling
    network/infrastructure technology. While performance and scalability are accepted
    IaaS concerns, managing performance in container-based application architectures
    is also a PaaS concern [S24,S32]. Performance is central in all aspects here,
    from load management to configuration to provisioning and efficient network management.
    Management Services-Quality Management results are below: The three categories
    monitorable / non-monitorable / testable are split 56 /8/ 6, respectively. Monitorable
    and testable quality aspects are in the majority and this can be attributed to
    the large number of experimental studies in the systems community. There is a
    balance between infrastructure quality parameters, (external/given/provided) service-level
    objectives (SLO) and some parameters describing the adaptation mechanism in-between
    (e.g., elastic), as the grouping below shows: The fact that the aim is gain in
    performance (less overhead) through containers as virtualisation mechanisms is
    noteworthy [S13,S19,S30,S35,S36,S39]. [S1] reports that containers are more resource
    efficient and more scalable due to the significantly lower RAM consumption (29.4
    times smaller than VM). High-performance computing (HPC), where both isolation
    and performance is needed, benefits since container-based virtualisation provides
    less overhead than hypervisor-based environments and isolation concerns are less
    prevalent in HPC due to limited resource sharing needs [S36,S42]. There is interest
    both in both SLAs and infrastructures. Concerning SLAs the predominant parameter
    is performance, while for infrastructures the predominant parameters are resource
    utilisation and portability. What emerges here is a stakeholder dimension: SLA
    concerns are important for the consumers/users of the application, infrastructure
    concerns are related to the providers and the system/management category addresses
    an internal perspective. 5.2.4 Architecture Setting Wider software engineering
    concerns are also covered by our classification scheme, which extends the Architecture/Construction
    and Management Services, but at a higher level. ‘Stage’ refers to the development
    stage, Fig. 12, which can be summarised into three groups of concerns: Fig. 12.
    Study distribution by stage. Show All The concern is mainly deployment, with less
    design stage activities and only a few cross-cutting concerns (such as DevOps).
    This confirms other observations above. The operational concerns of deployment
    and runtime monitoring and development aspects like testing would benefit from
    a tighter integration. For instance, [S34] shows that the combination of automated
    testing and deployment improves the speed and efficiency. Continuous deployment
    and automation of activities in a DevOps-style can be the solution. Architecture
    Concerns, see Fig. 13: On a term-by-term basis, ‘flexibility’ emerges as a key
    benefit for clouds and containers, but also that applications have to be integrated
    and run under quality requirements. Fig. 13 is detailed in the table below. We
    can observe an equal balance between construction and management (change and adaptivity)
    towards DevOps and continuous development [S20,S40]. Other qualities are less
    relevant. Fig. 13. Study distribution by architecture concerns. Show All Fig.
    14. Study distribution by cloud delivery model. Show All Fig. 15. Study distribution
    by container technology. Show All Fig. 16. Study distribution by container type.
    Show All 5.2.5 Cloud and Container Technology Space Cloud Delivery Model: we observe
    a mix of IaaS and PaaS. The data we collected clearly shows that containerisation
    is useful at the level of IaaS and PaaS, with both IaaS and PaaS almost equally
    distributed. A few more contributions target IaaS from a virtualisation and orchestration
    perspective, rather than PaaS with its application packaging focus. IaaS is often
    present in the analysed papers. Many of them also address virtual machines and
    compare how containers perform due to having a lighter virtualisation approach
    [2], [3]. VMs are the traditional model for IaaS, but containers are similar as
    image-based machines, but also oriented towards PaaS through application packaging
    aspects, typically running on top of a virtual machine rather than running directly
    on a host OS [S3]. Container Technologies: Docker and to a smaller extent LXC
    dominate, other trending ones are covered (Kubernetes, CoreOS, OpenVZ, Diego,
    Rocket). LXC can be considered the de-facto standard for containerisation on Linux/Unix,
    while Docker has gained momentum and it is becoming the reference technology for
    containerisation. Open-sourcing, a rich ecosystem with image repositories and
    community support are here the main drivers of success. The popularity of Docker
    is probably linked to the open source approach and early release of technology.
    Though, as Bernstein points out [S5], a more neutral governance/collaboration
    structure around Docker (a start-up company) and Kubernetes (still controlled
    by Google) will allow an agreement on a wider common packaging and deployment
    approach. An industry perspective is added in [S8] by VMware, demonstrating an
    OVF-based standardised containerisation approach. Standard containers that cleanly
    separate application providers from infrastructure providers are a key success
    factor [6]. Container Type: Here, we see largely containers, then clusters (confirming
    above observations). However, looking at publication times, we could image that
    containers are more popular than clusters because the latter has been only started
    very recently with publications almost exclusively from mid 2015, e.g., [S9,S15,S43,S44].
    Summary. We can note that container-based solutions target both IaaS and PaaS,
    with a consolidated interest for single-container solutions, and a growing interest
    for solutions for clusters of containers. Managing qualities such as performance
    and resource utilisation are key concerns. 5.3 Completing the Conceptual Map Furthermore,
    we looked at the Technology Stack and Management Services categories in more detail,
    as these were only summarised in the initial study summary in Table 4, where we
    compared the studies based on some core classification categories. In Figs. 17
    and 18, we mapped the generic contribution types (Solution, Evaluation, Experience
    Report, Review) to the two above specific technical categories relevant in the
    container orchestration context. The bubble size indicates the number of studies
    of that type. Fig. 17. Contribution type by stack concern. [Sol:Solution, Val:Validation,
    Eval:Evaluation, Exp: Experience Report, Rev: Review], [VB: Virtualisation Basics,
    ConC: Container Construction, ConM: Container Management, CluC: Cluster Construction,
    CluM: Cluster Management]. Show All Fig. 18. Contribution type by management service.
    [Sol:Solution, Val: Validation, Eval:Evaluation, Exp: Experience Report, Rev:
    Review], [Const: Construction/Architecture, Exec: Execution, QuMon: Quality-Monitorable,
    QuNMon: Quality-Non-Monitorable, QuTest: Quality-Testable]. Show All The Technology
    Stack view, Fig. 17, shows how containers and container-based clusters are internally
    constructed: mainly solutions are reported (largest bubbles), which primarily
    cover container construction and management, but also address all other aspects
    to some extent; evaluations focus on construction, less on management; equally,
    experimentation is construction-focussed; reviews cover the technology stack more
    completely. In the Management Services view, Fig. 18, similar observations can
    be made: mainly solutions are reported, which here more clearly focus on container
    execution and quality management (regarding the latter, monitorable aspects prevail);.
    evaluation, experimentation and examples are patchy, focussing on the quality
    management; reviews cover all service types more comprehensively. Summary. The
    conceptual map links the studies with the classification scheme. For each term,
    we recorded the number of occurrences of the extracted terms from the studies.
    Higher numbers of occurrences in each of the categories indicate a stronger research
    concern and/or stronger consensus on the importance of the aspect. In Fig. 19
    Fig. 19. Key terms extracted, organised into different concerns and presented
    with the numbers of times these terms occur. Show All , the term occurrences are
    associated to the technical concerns of the classification scheme. The figure
    summarises earlier discussions and defines the cloud container technologies space
    into internal construction of containers and their clusters, methodological support
    for construction and architecture, infrastructure services for development, execution
    management and quality control, technology and tool platforms. The relative weight
    of the individual concerns within each of the four core categories is given. This
    shows the current focus on developing solutions for runtime management of execution
    and quality, but also the need to: (methodological aspect) evaluate and experiment
    more, (technical aspect) integrate the runtime focus more with development aspects
    into a coherent framework, (quality aspect) broaden the quality concerns beyond
    the monitorable ones. Particularly the Architecture and Management categories
    define what are the expected functions of a cloud container technologies (with
    methods and techniques). 5.4 Threats to Validity We discuss threats to the validity
    of this work in the different mapping study steps. Threats to the Relevance of
    the Selected Studies. Innovation in the area emerges from industrial practice.
    While industrial white papers or blogs as mechanisms to communicate the innovations
    are available, these are difficult to systematically retrieve and quality assess
    through SLRs or SMSs. We have therefore complemented the protocol-based results
    by reviewing relevant technologies such as Docker, cloud and cloud-native architectures
    and including exemplary blogs in the discussion. Threats to the Identification
    of Primary Studies. In our search, we aimed to retrieve as many studies as possible
    to avoid any bias. A challenge was to determine the scope of our study, since
    container orchestration relates to different computing and IT communities including
    software engineering, distributed systems, operating systems, information systems
    and cloud. These communities use different terminologies for the same concepts.
    To cover all and avoid any bias, we searched for the container orchestration term
    in different contexts. While this approach decreases bias, it significantly increases
    search effort. To identify relevant studies and ensure an unbiased selection,
    a review protocol was developed. Threats to Selection and Data Extraction Consistency.
    The formulation of the research questions has helped in selecting studies of relevance,
    as did the Characterisation Framework. However, we did include peer-reviewed magazine
    contributions and books here to capture trends and activities. Threats to Data
    Synthesis and Results. This reliability threat is mitigated as far as possible
    by having a unified characterisation scheme and following a standard protocol
    where several steps were piloted and externally evaluated. SECTION 6 Conclusions
    Containerisation provides cloud application managment based on lightweight virtualisation.
    The increasing interest in cloud container technologies shows the importance of
    their management and orchestration in this context. The scientific contributions
    we reviewed are a mix of technology reviews, solutions and use case architectures
    (conceptual and implemented). As a good part of this is still conceptual, it can
    be seen as a sign of the immaturity of an emerging field. This interpretation
    is strengthened by the fact that only some use case validations and no large-scale
    empirical evaluations exist. There is also a noticeable imbalance of contribution
    formats compared to more mature domains: Larger number of technical contributions
    (solution proposals): the number of journal publications is low (with short communications
    in magazines published only). Higher number of use cases than technology solutions:
    i.e., an emerging technology to be formatively validated through use cases rather
    than summative evaluations. What has been demonstrated is the better resource
    efficiency of containers compared to VMs, and also the increased flexibility as
    an application management framework. However, proven technologies are lacking
    to fully support container architectures for cloud environments. An example pointed
    out in [S21] is failure management. Finding root causes and dealing with anomalous
    events requires better resource monitoring and log analysis. We can conclude that
    the field is moving towards container middleware (and even container PaaS) with
    isolation, construction, quality management, orchestration and distribution management
    as core concerns of a container PaaS middleware, but key features such as failure
    management are still missing. We see here middleware as the core set of features
    necessary to host and provide applications, facilitating the application and Web
    tier. A PaaS is a more comprehensive solution encompassing (virtualised) infrastructure
    features and data(base) storage. Container orchestration is a key concern in the
    current cloud PaaS context. For instance, the Cloud Native Computing Foundation
    CNCF (https://cncf.io/) will be integrating the orchestration layer of the container
    ecosystem. The CNCF uses Kubernetes container cluster orchestration as its containerisation
    technology to deal with network storage and cluster management. Industry reports,
    such as [S21] about Google''s Borg cluster management, highlight current limitations-e.g.,
    Borg has no first-class mechanism to manage an entire multi-job service as a single
    entity. Better cluster orchestration support is the required solution. Management
    becomes inherently distributed, with features like the scheduler, admission control,
    vertical and horizontal auto-scaling, re-packing, periodic job submission, workflow
    management and archiving. The aim is to scale up the workload and feature set
    without sacrificing performance or maintainability. Organizations are packaging
    applications in containers and need to orchestrate multiple containers across
    cloud servers: The benefits of container-based orchestration include adjustable
    cluster sizes for the deployment of containers, easier cluster maintenance and
    also quicker deployment. This is confirmed by the motivations cited, which prioritise
    the easy, automated container deployment and management in larger settings allowing
    for flexible migration and reconfiguration. Containerisation positively impacts
    on both development and deployment aspects such as testing and monitoring of container-based
    applications, which we found both covered by the studies in terms of a mix of
    architecture design and continuous, quality-driven management. The quality distribution
    shows a strong interest in optimised resource utilisation (effectively a cost
    factor) and performance, complemented by portability/interoperability and security.
    Interesting is also the emergence of two equally important perspectives: quality
    aspects directly observable by the consumers and the quality aspects relevant
    to the management of the applications within the platform itself. A theme that
    emerges is the support of continuous development through containers, joining both
    construction as well as operations and management. In the cloud, cloud-native
    platform services for development and deployment do exist, but require advanced
    PaaS orchestration support. The trend towards cluster-based orchestration, combined
    with the interoperability of successful container technologies, also allows the
    management of highly distributed topologies of smaller virtualised devices beyond
    centralised clouds as in the edge/fog cloud domain [S9,S21,S25,S26,S43,S44]. Containers
    can run on single-board devices and can be deployed on clusters of these devices,
    making them suitable for edge and IoT computing [14]. This results in a need for
    research beyond the performance and isolation concerns for container virtualisation,
    particularly regarding methodological and tool support: A DevOps approach to manage
    the continuous development and deployment would benefit-the cross-stage nature
    is obvious. While most papers look at the two sides separately, the need to link
    these in a continuous process and to feed back monitored operational data into
    development becomes obvious. Containers represent a progression from virtual machines
    towards lightweight application management. Lately, there is an observable continuing
    trend towards serverless architectures and other mechanisms to manage orchestration
    and deployment complexity such as unikernel technology towards more lightweightness.
    Similar to Docker kick-starting research on container technology as an active
    open-source environment, a similar impact may be expected with the emergence of
    similar technologies in the cluster space such as Kubernetes and Mesos. There
    is a strong increase in research papers from mid 2015 that evidences this. Edge
    or fog computing is an architectural setting that needs clustering in which containers
    can help to address interoperability needs. ACKNOWLEDGMENTS The work presented
    here was partly supported by project PRA_2016_64 “Through the fog” funded by the
    University of Pisa and by project “Edge Cloud Orchestration” funded by the Free
    University of Bozen-Bolzano. Authors Figures References Citations Keywords Metrics
    Footnotes More Like This Empirical rapid and accurate prediction model for data
    mining tasks in cloud computing environments 2014 International Congress on Technology,
    Communication and Knowledge (ICTCK) Published: 2014 Cloud computing security risks
    with authorization access for secure Multi-Tenancy based on AAAS protocol TENCON
    2015 - 2015 IEEE Region 10 Conference Published: 2015 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE transactions on cloud computing (Online)
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Cloud Container Technologies: A State-of-the-Art Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ic2e.2014.41
  analysis: '>'
  authors:
  - Rajdeep Dua
  - A Reddy Raja
  - Dharmesh Kakadia
  citation_count: 217
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2014 IEEE International Confe... Virtualization vs
    Containerization to Support PaaS Publisher: IEEE Cite This PDF Rajdeep Dua; A
    Reddy Raja; Dharmesh Kakadia All Authors 179 Cites in Papers 16 Cites in Patents
    10124 Full Text Views Abstract Document Sections I. Introduction II. Virtual Machine
    and Container Comparison III. Paas Requirements IV. Container Overview V. Containers-Current
    State Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: PaaS vendors face challenges in efficiently providing services with
    the growth of their offerings. In this paper, we explore how PaaS vendors are
    using containers as a means of hosting Apps. The paper starts with a discussion
    of PaaS Use case and the current adoption of Container based PaaS architectures
    with the existing vendors. We explore various container implementations - Linux
    Containers, Docker, Warden Container, lmctfy and OpenVZ. We look at how each of
    this implementation handle Process, FileSystem and Namespace isolation. We look
    at some of the unique features of each container and how some of them reuse base
    Linux Container implementation or differ from it. We also explore how IaaSlayer
    itself has started providing support for container lifecycle management along
    with Virtual Machines. In the end, we look at factors affecting container implementation
    choices and some of the features missing from the existing implementations for
    the next generation PaaS. Published in: 2014 IEEE International Conference on
    Cloud Engineering Date of Conference: 11-14 March 2014 Date Added to IEEE Xplore:
    22 September 2014 Electronic ISBN:978-1-4799-3766-0 DOI: 10.1109/IC2E.2014.41
    Publisher: IEEE Conference Location: Boston, MA, USA SECTION I. Introduction Platform
    as a Service has brought developer productivity to the forefront making it possible
    to create a layer of abstraction on laaS for deployment of services easily and
    quickly. This has lead to unique challenges and opportunities for Infrastructure
    as a Service-laaS laver. In this paper, we look at various aspects of this abstraction
    and explore implementations, which use Host OS, Virtual Machines and Containers.
    Some of the PaaS vendors use containers to reduce overhead of creating a new VM
    for each application, hence reducing the cost of running a PaaS application while
    maintaining Isolation at Process, Network and FileSystem level. In this paper,
    we explore various container technologies Warden Container [1], Openvz [4], Docker
    [5] and Imctfy [6]. We also look at benefits of adopting containers for todays
    PaaS implementations. SECTION II. Virtual Machine and Container Comparison System
    Hypervisors allow Multiple Guest OS to run together on a single machine. Each
    Guest OS abstracts Compute, Storage and Network Components. Hypervisor itself
    could run on bare-metal (ESXi) or be part of an OS (KVM). Guest ISA is translated
    to Host ISA using multiple techniques like Hardware Virtualization or Binary Translation.
    A container is a light weight operating system running inside the host system,
    running instructions native to the core CPU, eliminating the need for instruction
    level emulation or just in time compilation. Containers provide savings in resource
    consumption without the overhead of virtualization, while also providing isolation.
    Table 1 compares Virtual Machines and Containers on multiple factors like performance,
    isolation security, networking, storage. Table I VM and container feature comparision
    SECTION III. Paas Requirements PaaS, focuses on developer productivity rather
    than managing Network, Storage and Compute. This makes it possible to abstract
    out the implementation of application runtime environment. Some of the key requirements
    for a PaaS to host application are, Application isolation at Network, Compute
    and Storage level Lower Performance overhead-Virtual Machines are great for isolation
    but there is a performance overhead for the translation happening from Guest ISA
    to Host ISA. Support multiple languages and runtimes. Fair share of CPU time Easy
    management of Application Lifecycle events Application state persistence and migration
    Fig. 1. Concept of container in a VM Show All A. Paas Use Case Platform as a Service
    focuses on providing Language Runtime and Services to developers leaving Infrastructure
    Provisioning and Management to the underlying Layer. PaaS could use Host as, VM
    or Container as hosting Environment for applications. PaaS can be implemented
    using one of the three scenarios. Option 1: Each App runs in an Application level
    container directly on Host as where PaaS layer runs Option 2: Each App runs in
    a VM on PaaS: Each App runs in its own VM Option 3: Each App runs in a container
    which shares a common VM or VMs for hosting Apps Option 3 is one of the common
    implementation choice for most of the PaaS vendors as a combination of the VM
    and container meets some of the requirements of Isolation, Lifecycle management,
    fair share of CPU time. B. Using Vms in Paas Paas has an important use for VMs
    to abstract out the Runtime as from the underlying hardware. Need for each application
    to have its own VM is the point of contention. Since containers tend to be much
    lighter in memory footprint, they are preferred architecture as compared to a
    VM for each application. VM is are used to host the containers. Other Services
    in Paas like Authentication, Routing, Cloud Controller, Persistent Storage is
    still run directly on Virtual Machines. C. Container Adoption Some of the PaaS
    implementations use container abstraction layer to optimize on resources. Table
    II provides the mapping between the PaaS Product and the container implementation
    used. Some of the large PaaS players don''t seem to use any container relying
    on VMs or Application runtime containers for various reasons. As an example Google
    App Engine uses some form of Application Container to isolate tenant applications
    but does not use VMs or as Level containers. Table II Container adoption by pass
    providers SECTION IV. Container Overview Containers are a means of providing isolation
    and resource management in Linux environment. The term is derived from shipping
    containers a standard method to store and ship any kind of cargo. An operating
    system container provides a generic way of isolating a process from the rest of
    the system. The containment applies to all the child processes. Hence, one can
    boot an entire operating system by spawning the init process. It promises same
    level of isolation and security as a virtual machine and is more tightly integrated
    with the host operating system. No dependence on hardware emulation provides performance
    benefits over full virtualization but restricts the number of supported operating
    systems which can be spawned as guest operating systems; one cant boot Windows
    in a LinuxlUnix Container. This is typically, not a requirement for PaaS providers
    and thus containers are well suited for providing low-overhead isolation. A. Chroot
    Chroot (change root) is a Linux command to change the root directory of the current
    process and its children to a new directory. Some of the containers use chroot
    to isolate and share the filesystem across virtual environments. B. Cgroups Cgroups
    is part of kernel subsystem, provides a fine grained control over sharing of resources
    like CPU, memory, group of processes etc. Almost all the containers use cgroups
    as the underlying mechanisim for resource management. Cgroups set the limits on
    the usage of these resources. C. Kernel Namespaces N amespaces create barriers
    between containers at various levels. The pid namespace allows each container
    its own process id. The net namespace allows each container to have its network
    artifacts like routing rable, iptables, loopback interface. The IPC namespace
    provides isolation for various IPC mechanisms namely semaphore, message queues
    and shared memory segments. The mnt namespace provides each container with its
    own mountpoint. The UTS namespace ensures that different containers can view and
    change hostnames specifically to them. Fig. 2. Linux container running in a linux
    host Show All SECTION V. Containers-Current State In this section, we provide
    a brief summary of popular container implementations. A. Linux Containers Linux
    Containers (LXC) are light weight kernel containment implementation supported
    on few flavors or Linux like Ubuntu and Oracle Linux. Key Characteristics of the
    Linux Containers are, Process Each container is assigned a unique PID. Each container
    can run a single process. Resource Isolation Uses cgroups and namespaces to isolate
    resources Network Isolation Containers get a private IP address and veth interface
    connecting to a linux bridge on the host File System Isolation Each container
    gets a private file system by leveraging chroot. Linux containers use Apparmor
    Security profile for hardening of the host. It also uses cgroups to limit device
    access for the containers. LXC uses a separate file system for each container
    which can be backed by an LVM. Figure 2 shows a HostOS running two Linux containers
    LXC 1 and LXC 2, having their own file system under/var/lib/lxc. In this figure
    both the containers are connected to a linux bridge lxcbro and assigned IP and
    assigned private network components. The key advantage of a LXC is a lightweight
    implementation which performs at native speeds which providing better network
    and filsystem isolation. Currently, LXC suffers from following the limitations:
    Containers use a Shared kernel Limitations in terms of secure containment environment
    [3] Limited to Linux based environments Implementation closely tied to a Linux
    Kernel B. Warden Container Warden container provides a kernel independent containment
    implementation which can be plugged to multiple underlying Host OS. This container
    implementation is used by Cloud Foundry project to host applications. Some of
    the Key attributes of Warden container are, Uses namespaces for isolation of Process
    and Network. Uses cgroups concept from Linux to isolate resources Each container
    can run multiple processes. Supported only on ubuntu but the design makes it OS
    neutral. C. Docker Docker is a daemon which provides ability to manage Linux containers
    as self contained images. It utilizes LXC (Linux Containers) for the container
    implementation and adds image management and Union File System capability to it.
    Key attribute of Docker Containers are Process-Each Container is assigned a unique
    process id and a private IP. Cannot run multiple processes in a single container
    Resource Isolation -Uses cgroups and namespaces concept from Linux Containers
    Network Isolation -Leverages LXC functionality File System Isolation Leverages
    LXC functionality Container Lifecycle-Managed using a daemon and command line
    tool Container State-Docker allows ability to store and retrieve container state
    High level Goals of Docker project to improve security (which are limitations
    of Linux Containers) Map root user of the container to non-root user of docker
    Make docker daemon run as a non root user D. Google Lmctfy Google lmctfy provides
    a resource configuration API which simplifies container management. It provides
    intent based configuration without the need to understand cgroups and removes
    the difficulties of unstable LXC APIs. It also improves the resource sharing and
    can support performance guarantees. Key attribute of lmctfy Containers are Process-Use
    of PID namespace. Recursive containers are supported. Resource Isolation-Resources
    isolation using cgroups. Hierarchy splitting for easy management and higher resource
    utilization. Network Isolation-Plan to use net namespace in next version CL2 File
    System Isolation-Plan to use mnt namespace in next version CL2 Container Lifecycle-Plan
    to support freeze, chck-point/restore and disk import/export. Monitoring-To be
    supported through higher level. E. Openvz Open VZ uses a modified Linux Kernel
    with a set of extensions. Open VZ manages multiple physical and Virtual servers,
    by using dynamic realtime partitioning. Like contain-ers, Open VZ has little overhead
    and offers higher performance and can be managed better than Hypervisor technologies.
    Just like other containers, Open VZ uses cgroups and N amespaces. Open VZ additionally
    provides templates that help in pre-created Virtual environments. Process-Each
    container has it own PID namespace, IPC namespace with its own shared memory,
    semaphores and messages. Resource Management-Manages resource sharing using Bean
    Counters, Fair Share CPU Scheduler, disk quotas based on users and Containers,
    and can manage per continer disk I/O priority Network Isolation-Uses net namespace,
    has its own virtual network device, its own IP Address, filters and routing tables
    File System Isolation-Provides isolation to Application Files, System Libraries
    etc Container Lifecycle-Supports create, start, change and stop functions as part
    the lifecycle. Can be backed up, migrated. Supports remote management with the
    Libvirt API Container State-Provides Checkpointing feature for storing and recovering
    the last known state. The Complete state of the container like running processes,
    openfiles, network connections, buffers, memory segments can be stored on a file.
    This enables Live Migration of Open VZ containers SECTION VI. Container Comparison
    TABLE III summarizes the containers discussed above on the following paramters:
    Process, Network, Filesystem Isolation. We did not cover lmctfy as the container
    implementation is still a work in progress. It also discusses differences employed
    in managing these containers. It is clear from the table that all the implementations
    rely on cgroups for resource limits, chroot for process, namespaces for network
    and resource isolation. SECTION VII. Choosing the Right Container Choosing the
    right container for most of the PaaS implementations will be based on the following
    factors: Ecosystem for prebuilt containers Hardened layer for isolation of Process,
    Network, CPU and Filesystem Tools to manage lifecycle of a container Ability to
    migrate containers between hosts Support for multiple OS and kernels Fig. 3. Comparing
    various containers Show All Table III Container implementation comparison SECTION
    VIII. Cloud Orchestration Support for Containers Most of the PaaS products are
    built on top of existing Cloud Orchestration products like AWS, vSphere and OpenStack.
    In the current implementations Containers lie at a layer above laaS as a part
    of PaaS implementation. These containers typically run on top of a Virtual Machine
    rather than running directly on Host OS. Openstack with its Havana release supports
    managing Containers (docker in particular) along with VMs using a libvirt driver
    for docker. We could see containers being part of the laaS layer and being managed
    by the Cloud Orchestration layer rather than the PaaS implementation in some of
    the new PaaS implementations This approach will make the containers integration
    with PaaS pluggable and not tied to a particular container imple-mentation. SECTION
    IX. Next Generation Paas Support The usage of containers in PaaS is becoming mainstream,
    and the Linux Containers are becoming a defacto standard but have some way to
    go before widespread adoption. Some of the Cloud Providers are already using Containers
    to provide a fine-grained control over resource sharing. Some of the features
    listed below would help improve adoption of containers in PaaS platforms Standardization
    It is very important to come up with a container file format-similar to OVF in
    VMsso that there is interoperability between container formats. This will also
    help in building an ecosystem of pre-built container images. Security Containers
    need to be more secure both from the perspective of filesystem, network and memory
    isolation. Most popular container implementations like LXC and Docker seem to
    be lacking in this regard. OS Independence Containers should have abstraction
    layer so that they are not tied to a particular Kernel or its userspace. Warden
    Container seems to be in the right direction in this regard, but we doubt it will
    get wide adoption beyond Cloud Foundry. SECTION X. Conclusion Containers have
    an inherent advantage over VMs because of Performance improvements and reduced
    startup time. There are multiple flavors of container implementations available,
    all of them open source. Each implementation having its own pros and cons. Common
    Linux containment principles like chroot or namespaces are being used by all of
    them. Docker adds additional layers on top on Linux Containers making it little
    different and perhaps most relevant for PaaS implementors. Open VZ is perhaps
    the most secure though it needs a modified kernel. Since Open VZ is working on
    bringing some of these features into Linux kernel, at some point in time Open
    VZ and LXC will converge. Only few PaaS implementations notably the ones which
    are open source or are new are using containers as opposed to older ones like
    GAE, Azure which do not use this concept. Containers have a bright future specially
    in the PaaS use case provided there is more standardization and abstraction from
    the underlying kernel and Host OS. Authors Figures References Citations Keywords
    Metrics More Like This Enhancing security of Docker using Linux hardening techniques
    2016 2nd International Conference on Applied and Theoretical Computing and Communication
    Technology (iCATccT) Published: 2016 Analysis and Study of Security Mechanisms
    inside Linux Kernel 2008 International Conference on Security Technology Published:
    2008 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2014
  relevance_score1: 0
  relevance_score2: 0
  title: Virtualization vs Containerization to Support PaaS
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/mcc.2016.100
  analysis: '>'
  authors:
  - Théo Combe
  - Antony Martin
  - Roberto Di Pietro
  citation_count: 192
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Cloud Computing >Volume: 3 Issue: 5 To Docker or Not to Docker: A Security
    Perspective Publisher: IEEE Cite This PDF Theo Combe; Antony Martin; Roberto Di
    Pietro All Authors 166 Cites in Papers 2 Cites in Patents 11049 Full Text Views
    Abstract Document Sections Containerization and Dockerization in a Growing Ecosystem
    Docker Security Overview Docker Usages: Security Challenges Authors Figures References
    Citations Keywords Metrics Abstract: The need for ever-shorter development cycles,
    continuous delivery, and cost savings in cloud-based infrastructures led to the
    rise of containers, which are more flexible than virtual machines and provide
    near-native performance. Among all container solutions, Docker, a complete packaging
    and software delivery tool, currently leads the market. This article gives an
    overview of the container ecosystem and discusses the Docker environment''s security
    implications through realistic use cases. The authors define an adversary model,
    point out several vulnerabilities affecting current Docker usage, and discuss
    further research directions. Published in: IEEE Cloud Computing ( Volume: 3, Issue:
    5, Sept.-Oct. 2016) Page(s): 54 - 62 Date of Publication: 11 November 2016 ISSN
    Information: DOI: 10.1109/MCC.2016.100 Publisher: IEEE Containerization and Dockerization
    in a Growing Ecosystem Cloud applications have typically leveraged virtualization.
    However, several factors—including acceleration of the development cycle (such
    as agile methods and DevOps), an increasingly complex application stack (mostly
    Web services and their frameworks), and market pressure to densify applications
    on servers—have triggered the need for a fast, easy-to-use way of pushing code
    into production. Sign in to Continue Reading Authors Figures References Citations
    Keywords Metrics More Like This Competitive study of engineering change process
    management in manufacturing industry using product life cycle management — A case
    study 2017 International Conference on Inventive Computing and Informatics (ICICI)
    Published: 2017 Developing a Framework for Cost-Benefit Analysis of Cloud Computing
    Adoption by Higher Education Institutions in Saudi Arabia 2018 International Conference
    on Smart Computing and Electronic Enterprise (ICSCEE) Published: 2018 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE cloud computing
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: 'To Docker or Not to Docker: A Security Perspective'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/2890784
  analysis: '>'
  authors:
  - Brendan Burns
  - Brian Grant
  - David Oppenheimer
  - Eric Brewer
  - John Wilkes
  citation_count: 224
  full_citation: '>'
  full_text: ">\n50    COMMUNICATIONS OF THE ACM   |  MAY 2016  |  VOL. 59  |  NO.\
    \ 5\npractice\nIMAGE BY ALEX KOLOKYTHAS PHOTOGRAPHY\nDOI:10.1145/2890784\n \n\
    \ \n  Article development led by \n \nqueue.acm.org\nLessons learned from three\
    \ container-\nmanagement systems over a decade. \nBY BRENDAN BURNS, BRIAN GRANT,\
    \ DAVID OPPENHEIMER,  \nERIC BREWER, AND JOHN WILKES\nTHOUGH WIDESPREAD INTEREST\
    \ in software containers \nis a relatively recent phenomenon, at Google we have\
    \ \nbeen managing Linux containers at scale for more \nthan 10 years and built\
    \ three different container-\nmanagement systems in that time. Each system is\
    \ \nheavily influenced by its predecessors, even though \nthey were developed\
    \ for different reasons. This article \ndescribes the lessons we’ve learned from\
    \ developing \nand operating them. \nThe first unified container-management system\
    \ \ndeveloped at Google was the system we internally \ncall Borg.7 It was built\
    \ to manage both long-running \nservices and batch jobs, which had previously\
    \ been \nhandled by two separate systems: Babysitter and the \nGlobal Work Queue.\
    \ The latter’s architecture strongly \ninfluenced Borg, but was focused on batch\
    \ jobs; both \npredated Linux control groups. Borg shares machines \nbetween these\
    \ two types of applications as a way of \nincreasing resource utilization and\
    \ \nthereby reducing costs. Such sharing \nwas possible because container sup-\n\
    port in the Linux kernel was becom-\ning available (indeed, Google contrib-\n\
    uted much of the container code to \nthe Linux kernel), which enabled bet-\nter\
    \ isolation between latency-sensitive \nuser-facing services and CPU-hungry \n\
    batch processes. \nAs more applications were devel-\noped to run on top of Borg,\
    \ our ap-\nplication and infrastructure teams \ndeveloped a broad ecosystem of\
    \ tools \nand services for it. These systems pro-\nvided mechanisms for configuring\
    \ and \nupdating jobs; predicting resource \nrequirements; dynamically pushing\
    \ \nconfiguration files to running jobs; \nservice discovery and load balancing;\
    \ \nauto-scaling; machine-lifecycle man-\nagement; quota management; and \nmuch\
    \ more. The development of this \necosystem was driven by the needs of \ndifferent\
    \ teams inside Google, and \nthe result was a somewhat heteroge-\nneous, ad hoc\
    \ collection of systems \nthat Borg’s users had to configure \nand interact with,\
    \ using several dif-\nferent configuration languages and \nprocesses. Borg remains\
    \ the primary \ncontainer-management system within \nGoogle because of its scale,\
    \ breadth of \nfeatures, and extreme robustness. \nOmega,6 an offspring of Borg,\
    \ was \ndriven by a desire to improve the soft-\nware engineering of the Borg\
    \ ecosys-\ntem. It applied many of the patterns \nthat had proved successful in\
    \ Borg, but \nwas built from the ground up to have \na more consistent, principled\
    \ archi-\ntecture. Omega stored the state of the \ncluster in a centralized Paxos-based\
    \ \ntransaction-oriented store accessed \nby the different parts of the cluster\
    \ \ncontrol plane (such as schedulers), us-\ning optimistic concurrency control\
    \ to \nhandle the occasional conflicts. This \ndecoupling allowed the Borgmaster’s\
    \ \nfunctionality to be broken into sepa-\nrate components that acted as peers,\
    \ \nrather than funneling every change \nthrough a monolithic, centralized \n\
    master. Many of Omega’s innovations \nBorg, \nOmega, and \nKubernetes\nMAY 2016\
    \  |  VOL. 59  |  NO. 5  |  COMMUNICATIONS OF THE ACM    51\n52    COMMUNICATIONS\
    \ OF THE ACM   |  MAY 2016  |  VOL. 59  |  NO. 5\npractice\nreclaimed to run batch\
    \ jobs. Contain-\ners provide the resource-management \ntools that make this possible,\
    \ as well as \nrobust kernel-level resource isolation \nto prevent the processes\
    \ from interfer-\ning with one another. We achieved this \nby enhancing Linux\
    \ containers concur-\nrently with Borg’s development. The \nisolation is not perfect,\
    \ though: con-\ntainers cannot prevent interference in \nresources that the operating-system\
    \ \nkernel does not manage, such as level \n3 processor caches and memory band-\n\
    width, and containers need to be sup-\nported by an additional security layer\
    \ \n(such as virtual machines) to protect \nagainst the kinds of malicious actors\
    \ \nfound in the cloud. \nA modern container is more than \njust an isolation\
    \ mechanism: it also \nincludes an image—the files that make \nup the application\
    \ that runs inside the \ncontainer. Within Google, MPM (Midas \nPackage Manager)\
    \ is used to build and \ndeploy container images. The same \nsymbiotic relationship\
    \ between the \nisolation mechanism and MPM pack-\nages can be found between the\
    \ Docker \ndaemon and the Docker image registry. \nIn the remainder of this article\
    \ we use \nthe word container to encompass both \nof these aspects: the runtime\
    \ isolation \nand the image. \nApplication-Oriented Infrastructure \nOver time\
    \ it became clear that the ben-\nefits of containerization go beyond \nmerely\
    \ enabling higher levels of utili-\nzation. Containerization transforms \nthe\
    \ data center from being machine-\noriented to being application-oriented. \n\
    This section discusses two examples: \n \n˲ Containers encapsulate the appli-\n\
    cation environment, abstracting away \nmany details of machines and operating\
    \ \nsystems from the application developer \nand the deployment infrastructure.\
    \ \n \n˲ Because well-designed containers \nand container images are scoped to\
    \ \na single application, managing con-\ntainers means managing applications \n\
    rather than machines. This shift of \nmanagement APIs from machine-ori-\nented\
    \ to application-oriented dramati-\ncally improves application deploy-\nment and\
    \ introspection. \nApplication environment. The origi-\nnal purpose of the cgroup,\
    \ chroot, and \nnamespace facilities in the kernel was \nto protect applications\
    \ from noisy, nos-\n(including multiple schedulers) have \nsince been folded into\
    \ Borg. \nThe third container-management \nsystem developed at Google was Ku-\n\
    bernetes.4 It was conceived of and \ndeveloped in a world where external \ndevelopers\
    \ were becoming interested \nin Linux containers, and Google had \ndeveloped a\
    \ growing business selling \npublic-cloud infrastructure. Kuber-\nnetes is open\
    \ source—a contrast to \nBorg and Omega, which were devel-\noped as purely Google-internal\
    \ sys-\ntems. Like Omega, Kubernetes has at \nits core a shared persistent store,\
    \ with \ncomponents watching for changes to \nrelevant objects. In contrast to\
    \ Ome-\nga, which exposes the store directly \nto trusted control-plane components,\
    \ \nstate in Kubernetes is accessed exclu-\nsively through a domain-specific REST\
    \ \nAPI that applies higher-level version-\ning, validation, semantics, and policy,\
    \ \nin support of a more diverse array of \nclients. More importantly, Kubernetes\
    \ \nwas developed with a stronger focus on \nthe experience of developers writing\
    \ \napplications that run in a cluster: its \nmain design goal is to make it easy\
    \ to \ndeploy and manage complex distribut-\ned systems, while still benefiting\
    \ from \nthe improved utilization that contain-\ners enable. \nThis article describes\
    \ some of the \nknowledge gained and lessons learned \nduring Google’s journey\
    \ from Borg to \nKubernetes. \nContainers \nHistorically, the first containers\
    \ just \nprovided isolation of the root file sys-\ntem (via chroot), with FreeBSD\
    \ jails ex-\ntending this to additional namespaces \nsuch as process IDs. Solaris\
    \ subse-\nquently pioneered and explored many \nenhancements. Linux control groups\
    \ \n(cgroups) adopted many of these ideas, \nand development in this area contin-\n\
    ues today. \nThe resource isolation provided by \ncontainers has enabled Google\
    \ to drive \nutilization significantly higher than in-\ndustry norms. For example,\
    \ Borg uses \ncontainers to co-locate batch jobs with \nlatency-sensitive, user-facing\
    \ jobs on \nthe same physical machines. The user-\nfacing jobs reserve more resources\
    \ than \nthey usually need—allowing them to \nhandle load spikes and fail-over—and\
    \ \nthese mostly unused resources can be \nOver time it \nbecame clear that \n\
    the benefits of \ncontainerization \ngo beyond merely \nenabling higher \nlevels\
    \ of utilization.\nMAY 2016  |  VOL. 59  |  NO. 5  |  COMMUNICATIONS OF THE ACM\
    \    53\npractice\ney, and messy neighbors. Combining \nthese with container images\
    \ created an \nabstraction that also isolates applica-\ntions from the (heterogeneous)\
    \ oper-\nating systems on which they run. This \ndecoupling of image and OS makes\
    \ it \npossible to provide the same deploy-\nment environment in both develop-\n\
    ment and production, which, in turn, \nimproves deployment reliability and \n\
    speeds up development by reducing in-\nconsistencies and friction. \nThe key to\
    \ making this abstraction \nwork is having a hermetic container im-\nage that\
    \ can encapsulate almost all of \nan application’s dependencies into a \npackage\
    \ that can be deployed into the \ncontainer. If this is done correctly, the \n\
    only local external dependencies will \nbe on the Linux kernel system-call in-\n\
    terface. While this limited interface \ndramatically improves the portability\
    \ \nof images, it is not perfect: applica-\ntions can still be exposed to churn\
    \ in \nthe OS interface, particularly in the \nwide surface area exposed by socket\
    \ \noptions, /proc, and arguments to \nioctl calls. Our hope is that ongo-\ning\
    \ efforts such as the Open Container \nInitiative \n(https://www.opencontain-\n\
    ers.org/) will further clarify the surface \narea of the container abstraction.\
    \ \nNonetheless, the isolation and de-\npendency minimization provided by \ncontainers\
    \ have proved quite effective \nat Google, and the container has be-\ncome the\
    \ sole runnable entity support-\ned by the Google infrastructure. One \nconsequence\
    \ is that Google has only a \nsmall number of OS versions deployed \nacross its\
    \ entire fleet of machines at \nany one time, and it needs only a small \nstaff\
    \ of people to maintain them and \npush out new versions. \nThere are many ways\
    \ to achieve \nthese hermetic images. In Borg, pro-\ngram binaries are statically\
    \ linked at \nbuild time to known-good library ver-\nsions hosted in the company-wide\
    \ re-\npository.5 Even so, the Borg container \nimage is not quite as airtight\
    \ as it \ncould have been: applications share a \nso-called base image that is\
    \ installed \nonce on the machine rather than being \npackaged in each container.\
    \ This base \nimage contains utilities such as tar \nand the libc library, so\
    \ upgrades to \nthe base image can affect running ap-\nplications and have occasionally\
    \ been \na significant source of trouble. \nMore modern container image for-\n\
    mats such as Docker and ACI harden \nthis abstraction further and get closer \n\
    to the hermetic ideal by eliminating \nimplicit host OS dependencies and re-\n\
    quiring an explicit user command to \nshare image data between containers. \n\
    Containers as the unit of manage-\nment. Building management APIs \naround containers\
    \ rather than ma-\nchines shifts the “primary key” of the \ndata center from machine\
    \ to applica-\ntion. This has many benefits: [1] it re-\nlieves application developers\
    \ and op-\nerations teams from worrying about \nspecific details of machines and\
    \ op-\nerating systems; [2] it provides the in-\nfrastructure team flexibility\
    \ to roll out \nnew hardware and upgrade operating \nsystems with minimal impact\
    \ on run-\nning applications and their develop-\ners; [3] it ties telemetry collected\
    \ by \nthe management system (for example, \nmetrics such as CPU and memory us-\n\
    age) to applications rather than ma-\nchines, which dramatically improves \napplication\
    \ monitoring and introspec-\ntion, especially when scale-up, ma-\nchine failures,\
    \ or maintenance cause \napplication instances to move. \nContainers \nprovide\
    \ \nconvenient \npoints to register generic APIs that en-\nable the flow of information\
    \ between \nthe management system and an appli-\ncation without either knowing\
    \ much \nabout the particulars of the other’s \nimplementation. In Borg, this\
    \ API is \na series of HTTP endpoints attached \nto each container. For example,\
    \ the  \n/healthz endpoint reports applica-\ntion health to the orchestrator.\
    \ When an \nunhealthy application is detected, it is \nautomatically terminated\
    \ and restart-\ned. This self-healing is a key building \nblock for reliable distributed\
    \ systems. \n(Kubernetes offers similar functional-\nity; the health check uses\
    \ a user-speci-\nfied HTTP endpoint or exec command \nthat runs inside the container.)\n\
    Additional \ninformation \ncan \nbe \nprovided by or for containers and dis-\n\
    played in various user interfaces. For \nexample, Borg applications can pro-\n\
    vide a simple text status message that \ncan be updated dynamically, and Ku-\n\
    bernetes provides key-value annota-\ntions stored in each object’s metadata \n\
    that can be used to communicate ap-\nplication structure. Such annotations \n\
    can be set by the container itself or oth-\ner actors in the management system\
    \ \n(for example, the process rolling out an \nupdated version of the container).\
    \ \nIn the other direction, the container-\nmanagement system can communicate\
    \ \ninformation into the container such as \nresource limits, container metadata\
    \ \nfor propagation to logging and moni-\ntoring (for example, user name, job\
    \ \nname, identity), and notices that pro-\nvide graceful-termination warnings\
    \ in \nadvance of node maintenance. \nContainers can also provide ap-\nplication-oriented\
    \ \nmonitoring \nin \nother ways: for example, Linux kernel \ncgroups provide\
    \ resource-utilization \ndata about the application, and these \ncan be extended\
    \ with custom met-\nrics exported using HTTP APIs, as \ndescribed earlier. This\
    \ data enables \nthe development of generic tools like \nan auto-scaler or cAdvisor3\
    \ that can \nrecord and use metrics without un-\nderstanding the specifics of\
    \ each ap-\nplication. Because the container is \nthe application, there is no\
    \ need to  \n(de)multiplex signals from multiple \napplications running inside\
    \ a physi-\ncal or virtual machine. This is sim-\npler, more robust, and permits\
    \ fin-\ner-grained reporting and control of \nmetrics and logs. Compare this to\
    \ hav-\ning to ssh into a machine to run top. \nThough it is possible for developers\
    \ to \nssh into their containers, they rarely \nneed to. \nMonitoring is just\
    \ one example. \nThe application-oriented shift has \nripple effects throughout\
    \ the man-\nagement infrastructure. Our load \nbalancers don’t balance traffic\
    \ across \nmachines; they balance across ap-\nplication instances. Logs are keyed\
    \ \nby application, not machine, so they \ncan easily be collected and aggregated\
    \ \nacross instances without pollution \nfrom multiple applications or system\
    \ \noperations. We can detect application \nfailures and more readily ascribe\
    \ fail-\nure causes without having to disentan-\ngle them from machine-level signals.\
    \ \nFundamentally, because the identity \nof an instance being managed by the\
    \ \ncontainer manager lines up exactly \nwith the identity of the instance ex-\n\
    pected by the application developer, it \nis easier to build, manage, and debug\
    \ \napplications. \nFinally, although so far we have fo-\ncused on applications\
    \ being 1:1 with \n54    COMMUNICATIONS OF THE ACM   |  MAY 2016  |  VOL. 59 \
    \ |  NO. 5\npractice\nown APIs dynamically, alongside the \ncore Kubernetes functionality.\
    \ \nConsistency is also achieved via de-\ncoupling in the Kubernetes API. Sepa-\n\
    ration of concerns between API com-\nponents means higher-level services all \n\
    share the same common basic build-\ning blocks. A good example of this is \nthe\
    \ separation between the Kubernetes \nreplica controller and its horizontal \n\
    auto-scaling system. A replication con-\ntroller ensures the existence of the\
    \ de-\nsired number of pods for a given role \n(for example, “front end”). The\
    \ auto-\nscaler, in turn, relies on this capability \nand simply adjusts the desired\
    \ number \nof pods, without worrying about how \nthose pods are created or deleted.\
    \ The \nautoscaler implementation can focus \non demand and usage predictions,\
    \ and \nignore the details of how to implement \nits decisions. \nDecoupling ensures\
    \ multiple re-\nlated but different components share \na similar look and feel.\
    \ For example, \nKubernetes has three different forms \nof replicated pods: \n\
    \ \n˲ ReplicationController: run-\nforever replicated containers (for ex-\nample,\
    \ Web servers). \n \n˲ DaemonSet: ensure a single in-\nstance on each node in\
    \ the cluster (for \nexample, logging agents). \n \n˲ Job: a run-to-completion\
    \ con-\ntroller that knows how to run a (possi-\nbly parallelized) batch job from\
    \ start \nto finish. \nRegardless of the differences in pol-\nicy, all three of\
    \ these controllers rely on \nthe common pod object to specify the \ncontainers\
    \ they wish to run. \nConsistency \nis \nalso \nachieved \nthrough common design\
    \ patterns for \ndifferent Kubernetes components. The \nidea of a reconciliation\
    \ controller loop is \nshared throughout Borg, Omega, and \nKubernetes to improve\
    \ the resiliency \nof a system: it compares a desired state \n(for example, how\
    \ many pods should \nmatch a label-selector query) against \nthe observed state\
    \ (the number of such \npods that it can find), and takes actions \nto converge\
    \ the observed and desired \nstates. Because all action is based on \nobservation\
    \ rather than a state dia-\ngram, reconciliation loops are robust \nto failures\
    \ and perturbations: when \na controller fails or restarts it simply \npicks up\
    \ where it left off. \nThe design of Kubernetes as a com-\ncontainers, in reality\
    \ we use nested \ncontainers that are co-scheduled on \nthe same machine: the\
    \ outermost one \nprovides a pool of resources; the inner \nones provide deployment\
    \ isolation. \nIn Borg, the outermost container is \ncalled a resource allocation,\
    \ or alloc; in \nKubernetes, it is called a pod. Borg also \nallows top-level\
    \ application containers \nto run outside allocs; this has been a \nsource of\
    \ much inconvenience, so Ku-\nbernetes regularizes things and always \nruns an\
    \ application container inside a \ntop-level pod, even if the pod contains \n\
    a single container. \nA common use pattern is for a pod \nto hold an instance\
    \ of a complex appli-\ncation. The major part of the applica-\ntion sits in one\
    \ of the child containers, \nand other containers run support-\ning functions\
    \ such as log rotation or \nclick-log offloading to a distributed \nfile system.\
    \ Compared to combining \nthe functionality into a single binary, \nthis makes\
    \ it easy to have different \nteams develop the distinct pieces of \nfunctionality,\
    \ and it improves robust-\nness (the offloading continues even \nif the main application\
    \ gets wedged), \ncomposability (it is easy to add a new \nsmall support service,\
    \ because it oper-\nates in the private execution environ-\nment provided by its\
    \ own container), \nand fine-grained resource isolation \n(each runs in its own\
    \ resources, so the \nlogging system cannot starve the main \napp, or vice versa).\n\
    Orchestration is the beginning, \nnot the end. The original Borg system \nmade\
    \ it possible to run disparate work-\nloads on shared machines to improve \nresource\
    \ utilization. The rapid evolu-\ntion of support services in the Borg \necosystem,\
    \ however, showed that con-\ntainer management per se was just the \nbeginning\
    \ of an environment for devel-\noping and managing reliable distrib-\nuted systems.\
    \ Many different systems \nhave been built in, on, and around \nBorg to improve\
    \ upon the basic con-\ntainer-management services that Borg \nprovided. The following\
    \ partial list \ngives an idea of their range and variety: \n \n˲ Naming and service\
    \ discovery (the \nBorg Name Service, or BNS). \n \n˲ Master election, using Chubby.2\
    \ \n \n˲ Application-aware load balancing. \n \n˲ Horizontal (number of instances)\
    \ \nand vertical (size of an instance) auto-\nscaling. \n \n˲ Rollout tools that\
    \ manage the care-\nful deployment of new binaries and \nconfiguration data. \n\
    \ \n˲ Workflow tools (for example, to \nallow running multi-job analysis pipe-\n\
    lines with interdependencies between \nthe stages). \n \n˲ Monitoring tools to\
    \ gather infor-\nmation about containers, aggregate it, \npresent it on dashboards,\
    \ and use it to \ntrigger alerts. \nThese services were built organi-\ncally to\
    \ solve problems that application \nteams experienced. The successful ones \n\
    were picked up, adopted widely, and \nmade other developers’ lives easier. Un-\n\
    fortunately, these tools typically picked \nidiosyncratic APIs, conventions (such\
    \ \nas file locations), and depth of Borg in-\ntegration. An undesired side effect\
    \ was \nto increase the complexity of deploying \napplications in the Borg ecosystem.\
    \ \nKubernetes attempts to avert this \nincreased complexity by adopting a \n\
    consistent approach to its APIs. For \nexample, every Kubernetes object has \n\
    three basic fields in its description: \nObjectMetadata, Specification \n(or Spec),\
    \ and Status. \nThe ObjectMetadata is the same \nfor all objects in the system;\
    \ it con-\ntains information such as the object’s \nname, UID (unique identifier),\
    \ an ob-\nject version number (for optimistic \nconcurrency control), and labels\
    \ (key-\nvalue pairs, see below). The contents \nof Spec and Status vary by object\
    \ \ntype, but their concept does not: Spec \nis used to describe the desired state\
    \ of \nthe object, whereas Status provides \nread-only information about the cur-\n\
    rent state of the object. \nThis uniform API provides many \nbenefits. Learning\
    \ the system is sim-\npler: similar information applies to \nall objects, and\
    \ writing generic tools \nthat work across all objects is simpler, \nwhich in\
    \ turn enables the develop-\nment of a consistent user experience. \nLearning\
    \ from Borg and Omega, Ku-\nbernetes is built from a set of compos-\nable building\
    \ blocks that can readily \nbe extended by its users. A common \nAPI and object-metadata\
    \ structure \nmakes that much easier. For example, \nthe pod API is usable by\
    \ people, in-\nternal Kubernetes components, and \nexternal automation tools.\
    \ To further \nthis consistency, Kubernetes is being \nextended to enable users\
    \ to add their \nMAY 2016  |  VOL. 59  |  NO. 5  |  COMMUNICATIONS OF THE ACM\
    \    55\npractice\nbination of microservices and small \ncontrol loops is an example\
    \ of control \nthrough choreography—achieving a de-\nsired emergent behavior by\
    \ combining \nthe effects of separate, autonomous \nentities that collaborate.\
    \ This is a con-\nscious design choice in contrast to a \ncentralized orchestration\
    \ system, which \nmay be easier to construct at first but \ntends to become brittle\
    \ and rigid over \ntime, especially in the presence of un-\nanticipated errors\
    \ or state changes. \nThings to Avoid \nWhile developing these systems we \nhave\
    \ learned almost as many things not \nto do as ideas that are worth doing. We\
    \ \npresent some of them here in the hopes \nthat others can focus on making new\
    \ \nmistakes, rather than repeating ours. \nDon’t make the container system \n\
    manage port numbers. All contain-\ners running on a Borg machine share \nthe host’s\
    \ IP address, so Borg assigns \nthe containers unique port numbers \nas part of\
    \ the scheduling process. A \ncontainer will get a new port number \nwhen it moves\
    \ to a new machine and \n(sometimes) when it is restarted on \nthe same machine.\
    \ This means tradi-\ntional networking services such as the \nDNS (Domain Name\
    \ System) have to \nbe replaced by home-brew versions; \nservice clients do not\
    \ know the port \nnumber assigned to the service a pri-\nori and have to be told;\
    \ port numbers \ncannot be embedded in URLs, requir-\ning name-based redirection\
    \ mecha-\nnisms; and tools that rely on simple \nIP addresses need to be rewritten\
    \ to \nhandle IP:port pairs. \nLearning from our experiences \nwith Borg, we decided\
    \ Kubernetes \nwould allocate an IP address per pod, \nthus aligning network identity\
    \ (IP \naddress) with application identity. \nThis makes it much easier to run\
    \ off-\nthe-shelf software on Kubernetes: \napplications are free to use static\
    \ \nwell-known ports (for example, 80 \nfor HTTP traffic), and existing, famil-\n\
    iar tools can be used for things like \nnetwork segmentation, bandwidth \nthrottling,\
    \ and management. All of \nthe popular cloud platforms provide \nnetworking underlays\
    \ that enable IP-\nper-pod; on bare metal, one can use \na Software Defined Network\
    \ (SDN) \noverlay or configure L3 routing to \nhandle multiple IPs per machine.\
    \ \nDon’t just number containers; give \nthem labels. If you allow users to cre-\n\
    ate containers easily, they tend to cre-\nate lots of them, and soon need a way\
    \ \nto group and organize them. Borg \nprovides jobs to group identical tasks\
    \ \n(its name for containers). A job is a \ncompact vector of one or more identi-\n\
    cal tasks, indexed sequentially from \nzero. This provides a lot of power and\
    \ \nis simple and straightforward, but we \ncame to regret its rigidity over time.\
    \ For \nexample, when a task dies and has to \nbe restarted on another machine,\
    \ the \nsame slot in the task vector has to do \ndouble duty: to identify the\
    \ new copy \nand to point to the old one in case it \nneeds to be debugged. When\
    \ tasks in \nthe middle of the vector exit, the vector \nends up with holes. The\
    \ vector makes \nit very hard to support jobs that span \nmultiple clusters in\
    \ a layer above Borg. \nThere are also insidious, unexpected \ninteractions between\
    \ Borg’s job-up-\ndate semantics (which typically re-\nstarts tasks in index order\
    \ when doing \nrolling upgrades) and an application’s \nuse of the task index\
    \ (for example, to \ndo sharding or partitioning of a data-\nset across the tasks):\
    \ if the application \nuses range sharding based on the task \nindex, Borg’s restart\
    \ policy can cause \ndata unavailability, as it takes down ad-\njacent tasks.\
    \ Borg also provides no easy \nway to add application-relevant meta-\ndata to\
    \ a job, such as role (for exam-\nple, “frontend”), or rollout status (for \n\
    example, “canary”), so people encode \nthis information into job names that \n\
    they decode using regular expressions. \nIn contrast, Kubernetes primar-\nily\
    \ uses labels to identify groups of \ncontainers. A label is a key/value pair\
    \ \nthat contains information that helps \nidentify the object. A pod might have\
    \ \nthe \nlabels \nrole=frontend \nand \nstage=production, indicating this \n\
    container is serving as a production \nfront-end instance. Labels can be dy-\n\
    namically added, removed, and modi-\nfied by either automated tools or users,\
    \ \nand different teams can manage their \nown labels largely independently. Sets\
    \ \nof objects are defined by label selectors \n(for example, stage==production\
    \ \n&& \nrole==frontend). Sets can \noverlap, and an object can be in multi-\n\
    ple sets, so labels are inherently more \nflexible than explicit lists of objects\
    \ or \nsimple static properties. Because a set \nThe design  \nof Kubernetes \
    \ \nas a combination \nof microservices \nand small control \nloops is an example\
    \ \nof control through \nchoreography.\n56    COMMUNICATIONS OF THE ACM   |  MAY\
    \ 2016  |  VOL. 59  |  NO. 5\npractice\nreplication controller managing the \n\
    pods that implement the service au-\ntomatically creates a replacement pod \n\
    for the misbehaving one. \nDon’t expose raw state. A key differ-\nence between\
    \ Borg, Omega, and Kuber-\nnetes is in their API architectures. The \nBorgmaster\
    \ is a monolithic component \nthat knows the semantics of every API \noperation.\
    \ It contains the cluster man-\nagement logic such as the state ma-\nchines for\
    \ jobs, tasks, and machines; \nand it runs the Paxos-based replicated \nstorage\
    \ system used to record the mas-\nter’s state. In contrast, Omega has no \ncentralized\
    \ component except the store, \nwhich simply holds passive state infor-\nmation\
    \ and enforces optimistic concur-\nrency control: all logic and semantics \nare\
    \ pushed into the clients of the store, \nwhich directly read and write the store\
    \ \ncontents. In practice, every Omega com-\nponent uses the same client-side\
    \ library \nfor the store, which does packing/un-\npacking of data structures,\
    \ retries, and \nenforces semantic consistency. \nKubernetes picks a middle ground\
    \ \nthat provides the flexibility and scal-\nability of Omega’s componentized\
    \ \narchitecture while enforcing system-\nwide invariants, policies, and data\
    \ \ntransformations. It does this by forcing \nall store accesses through a centralized\
    \ \nAPI server that hides the details of the \nstore implementation and provides\
    \ \nservices for object validation, default-\ning, and versioning. As in Omega,\
    \ the \nclient components are decoupled from \none another and can evolve or be\
    \ re-\nplaced independently (which is espe-\ncially important in the open source\
    \ \nenvironment), but the centralization \nmakes it easy to enforce common se-\n\
    mantics, invariants, and policies. \nSome Open, Hard Problems\nEven with years\
    \ of container-manage-\nment experience, we feel there are a \nnumber of problems\
    \ that we still don’t \nhave answers for. This section de-\nscribes a couple of\
    \ particularly knotty \nones, in the hope of fostering discus-\nsion and solutions.\
    \ \nConfiguration. Of all the problems \nwe have confronted, the ones over \n\
    which the most brainpower, ink, and \ncode have been spilled are related \nto\
    \ managing configurations—the set \nof values supplied to applications, \nrather\
    \ than hard-coded into them. In \nis defined by a dynamic query, a new \none can\
    \ be created at any time. Label \nselectors are the grouping mechanism \nin Kubernetes,\
    \ and define the scope of \nall management operations that can \nspan multiple\
    \ entities. \nEven in those circumstances where \nknowing the identity of a task\
    \ in a set \nis helpful (for example, for static role \nassignment and work-partitioning\
    \ or \nsharding), appropriate per-pod labels \ncan be used to reproduce the effect\
    \ of \ntask indexes, though it is the respon-\nsibility of the application (or\
    \ some \nother management system external to \nKubernetes) to provide such labeling.\
    \ \nLabels and label selectors provide a \ngeneral mechanism that gives the best\
    \ \nof both worlds.\nBe careful with ownership. In Borg, \ntasks do not exist\
    \ independently from \njobs. Creating a job creates its tasks; \nthose tasks are\
    \ forever associated with \nthat particular job, and deleting the job \ndeletes\
    \ the tasks. This is convenient, \nbut it has a major drawback: because \nthere\
    \ is only one grouping mecha-\nnism, it needs to handle all use cases. \nFor example,\
    \ a job has to store param-\neters that make sense only for service \nor batch\
    \ jobs but not both, and users \nmust develop workarounds when the \njob abstraction\
    \ doesn’t handle a use \ncase (for example, a DaemonSet that \nreplicates a single\
    \ pod to all nodes in \nthe cluster). \nIn Kubernetes, pod-lifecycle man-\nagement\
    \ components such as repli-\ncation controllers determine which \npods they are\
    \ responsible for using \nlabel selectors, so multiple control-\nlers might think\
    \ they have jurisdiction \nover a single pod. It is important to \nprevent such\
    \ conflicts through appro-\npriate configuration choices. But the \nflexibility\
    \ of labels has compensating \nadvantages—for example, the separa-\ntion of controllers\
    \ and pods means it \nis possible to “orphan” and “adopt” \ncontainers. Consider\
    \ a load-balanced \nservice that uses a label selector to \nidentify the set of\
    \ pods to send traffic \nto. If one of these pods starts misbe-\nhaving, that\
    \ pod can be quarantined \nfrom serving requests by removing one \nor more of\
    \ the labels that cause it to be \ntargeted by the Kubernetes service load \n\
    balancer. The pod is no longer serving \ntraffic, but it will remain up and can\
    \ be \ndebugged in situ. In the meantime, the \nIf you allow users  \nto create\
    \ containers \neasily, they tend to \ncreate lots of them, \nand soon need  \n\
    a way to group  \nand organize them.\nMAY 2016  |  VOL. 59  |  NO. 5  |  COMMUNICATIONS\
    \ OF THE ACM    57\npractice\ntruth, we could have devoted this en-\ntire article\
    \ to the subject and still have \nhad more to say. What follows are a \nfew highlights.\
    \ \nFirst, application configuration be-\ncomes the catch-all location for imple-\n\
    menting all of the things the contain-\ner-management system doesn’t (yet) \n\
    do. Over the history of Borg this has \nincluded: \n \n˲ Boilerplate reduction\
    \ (for exam-\nple, defaulting task-restart policies ap-\npropriate to the workload,\
    \ such as ser-\nvice or batch jobs). \n ˲ Adjusting and validating application\
    \ \nparameters and command-line flags. \n \n˲ Implementing workarounds for \n\
    missing API abstractions such as pack-\nage (image) management. \n \n˲ Libraries\
    \ of configuration tem-\nplates for applications. \n \n˲ Release-management tools.\
    \ \n \n˲ Image version specification. \nTo cope with these kinds of require-\n\
    ments, \nconfiguration-management \nsystems tend to invent a domain-\nspecific\
    \ configuration language that \n(eventually) becomes Turing complete, \nstarting\
    \ from the desire to perform \ncomputation on the data in the con-\nfiguration\
    \ (for example, to adjust the \namount of memory to give a server as \na function\
    \ of the number of shards in \nthe service). The result is the kind of in-\nscrutable\
    \ “configuration is code” that \npeople were trying to avoid by elimi-\nnating\
    \ hard-coded parameters in the \napplication’s source code. It doesn’t \nreduce\
    \ operational complexity or make \nthe configurations easier to debug or \nchange;\
    \ it just moves the computations \nfrom a real programming language to \na domain-specific\
    \ one, which typically \nhas weaker development tools such as \ndebuggers and\
    \ unit test frameworks. \nWe believe the most effective ap-\nproach is to accept\
    \ this need, embrace \nthe inevitability of programmatic \nconfiguration, and\
    \ maintain a clean \nseparation between computation and \ndata. The language to\
    \ represent the \ndata should be a simple, data-only \nformat such as JSON or\
    \ YAML, and \nprogrammatic modification of this \ndata should be done in a real\
    \ pro-\ngramming language, where there are \nwell-understood semantics, as well\
    \ as \ngood tooling. Interestingly, this same \nseparation of computation and\
    \ data \ncan be seen in front-end development \nmanagement system. We still hope\
    \ Ku-\nbernetes might be a platform on which \nsuch tools can be built, but doing\
    \ so \nremains an open challenge. \nConclusion\nA decade’s worth of experience\
    \ build-\ning container-management systems \nhas taught us much, and we have em-\n\
    bedded many of those lessons into \nKubernetes, Google’s most recent \ncontainer-management\
    \ \nsystem. \nIts \ngoals are to build on the capabilities of \ncontainers to\
    \ provide significant gains \nin programmer productivity and ease \nof both manual\
    \ and automated system \nmanagement. We hope you’ll join us in \nextending and\
    \ improving it.  \n  Related articles  \n  on queue.acm.org\nStorage Virtualization\
    \ Gets Smart\nKostadis Roussos\nhttp://queue.acm.org/detail.cfm?id=1317404\nUnikernels:\
    \ Rise of the Virtual Library \nOperating System\nAnil Madhavapeddy and David\
    \ J. Scott\nhttp://queue.acm.org/detail.cfm?id=2566628\nReferences \n1. Bazel.\
    \ {fast, correct}—choose two; http://bazel.io. \n2. Burrows, M. The Chubby lock\
    \ service for loosely \ncoupled distributed systems. In Proceedings from \nthe\
    \ Symposium on Operating System Design and \nImplementation (OSDI). Seattle, WA,\
    \ 2006. \n3. cAdvisor; https://github.com/google/cadvisor. \n4. Kubernetes; http://kubernetes.io/.\n\
    5. Metz, C. Google is 2 billion lines of code—and it’s all \nin one place. Wired\
    \ (Sept. 2015); http://www.wired.\ncom/2015/09/google-2-billion-lines-codeand-one-place/.\
    \ \n6. Schwarzkopf, M., Konwinski, A., Abd-el-Malek and \nM., Wilkes, J. Omega:\
    \ Flexible, scalable schedulers \nfor large compute clusters. In Proceedings from\
    \ \nthe European Conference on Computer Systems \n(EuroSys). Prague, Czech Republic,\
    \ 2013. \n7. \nVerma, A., Pedrosa, L., Korupolu, M. R., Oppenheimer, \nD., Tune,\
    \ E., Wilkes, J. Large-scale cluster \nmanagement at Google with Borg. In Proceedings\
    \ \nfrom the European Conference on Computer Systems \n(EuroSys). Bordeaux, France,\
    \ 2015. \nBrendan Burns (@brendandburns) is a software engineer \nat Google, where\
    \ he co-founded the Kubernetes project. \nPrior to working on Kubernetes and cloud,\
    \ he worked on \nlow-latency indexing for Google’s web-search infrastructure.\n\
    Brian Grant is a software engineer at Google. He was \npreviously a technical\
    \ lead of Borg and founder of the \nOmega project and is now design lead of Kubernetes.\n\
    David Oppenheimer is a software engineer at Google \nand a tech lead on the Kubernetes\
    \ project. He was a tech \nlead on Google’s Borg and Omega cluster-management\
    \ \nsystems prior to Kubernetes.\nEric Brewer is VP Infrastructure at Google and\
    \ a \nprofessor at UC Berkeley, where he pioneered scalable \nservers and elastic\
    \ infrastructure.\nJohn Wilkes has been working on cluster management \nand infrastructure\
    \ services at Google since 2008. \nPreviously, he spent time at HP Labs. His research\
    \ \ninterests include technologies that allow systems to \nmanage themselves.\
    \ \nCopyright held by authors/owner.  \nPublication rights licensed to ACM. $15.00.\n\
    with frameworks such as Angular that \nmaintain a crisp separation between \n\
    the worlds of markup (data) and Java-\nScript (computation). \nDependency management.\
    \ Stand-\ning up a service typically also means \nstanding up a series of related\
    \ services \n(monitoring, storage, CI/CD, etc). If an \napplication has dependencies\
    \ on oth-\ner applications, wouldn’t it be nice if \nthose dependencies (and any\
    \ transitive \ndependencies they may have) were au-\ntomatically instantiated\
    \ by the cluster-\nmanagement system? \nTo complicate things, instantiat-\ning\
    \ the dependencies is rarely as sim-\nple as just starting a new copy—for \nexample,\
    \ it may require registering as \na consumer of an existing service (for \nexample,\
    \ Bigtable as a service) and \npassing authentication, authoriza-\ntion, and billing\
    \ information across \nthose transitive dependencies. Al-\nmost no system, however,\
    \ captures, \nmaintains, or exposes this kind of \ndependency information, so\
    \ auto-\nmating even common cases at the \ninfrastructure level is nearly impossi-\n\
    ble. Turning up a new application re-\nmains complicated for the user, mak-\n\
    ing it more difficult for developers to \nbuild new services, and often results\
    \ \nin the most recent best practices not \nbeing followed, which affects the\
    \ reli-\nability of the resulting service. \nA standard problem is that is hard\
    \ \nto keep dependency information up to \ndate if it is provided manually, and\
    \ at \nthe same time attempts to determine \nit automatically (for example, by\
    \ trac-\ning accesses) fail to capture the se-\nmantic information needed to under-\n\
    stand the result. (Did that access have \nto go to that instance, or would any\
    \ \ninstance have sufficed?) One possible \nway to make progress is to require\
    \ that \nan application enumerate the services \non which it depends, and have\
    \ the in-\nfrastructure refuse to allow access to \nany others. (We do this for\
    \ compiler \nimports in our build system.1) The in-\ncentive would be enabling\
    \ the infra-\nstructure to do useful things in return, \nsuch as automatic setup,\
    \ authentica-\ntion, and connectivity. \nUnfortunately, the perceived com-\nplexity\
    \ of systems that express, analyze, \nand use system dependencies has been \n\
    too high, and so they haven’t yet been \nadded to a mainstream container-\n"
  inline_citation: '>'
  journal: Communications of the ACM
  limitations: '>'
  pdf_link: https://dl.acm.org/doi/pdf/10.1145/2890784?download=true
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Borg, Omega, and Kubernetes
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/tsc.2017.2711009
  analysis: '>'
  authors:
  - Yahya Al-Dhuraibi
  - Fawaz Paraïso
  - Nabil Bachir Djarallah
  - Philippe Merle
  citation_count: 253
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Transactions on Services... >Volume: 11 Issue: 2 Elasticity in Cloud Computing:
    State of the Art and Research Challenges Publisher: IEEE Cite This PDF Yahya Al-Dhuraibi;
    Fawaz Paraiso; Nabil Djarallah; Philippe Merle All Authors 241 Cites in Papers
    5430 Full Text Views Abstract Document Sections 1 Introduction 2 Elasticity 3
    Containerization 4 Open Issues and Research Challenges 5 Related Work Show Full
    Outline Authors Figures References Citations Keywords Metrics Abstract: Elasticity
    is a fundamental property in cloud computing that has recently witnessed major
    developments. This article reviews both classical and recent elasticity solutions
    and provides an overview of containerization, a new technological trend in lightweight
    virtualization. It also discusses major issues and research challenges related
    to elasticity in cloud computing. We comprehensively review and analyze the proposals
    developed in this field. We provide a taxonomy of elasticity mechanisms according
    to the identified works and key properties. Compared to other works in literature,
    this article presents a broader and detailed analysis of elasticity approaches
    and is considered as the first survey addressing the elasticity of containers.
    Published in: IEEE Transactions on Services Computing ( Volume: 11, Issue: 2,
    01 March-April 2018) Page(s): 430 - 447 Date of Publication: 01 June 2017 ISSN
    Information: DOI: 10.1109/TSC.2017.2711009 Publisher: IEEE Funding Agency: 1 Introduction
    Cloud computing has been gaining more popularity in the last decade and has received
    a great deal of attention from both industrial and academic worlds. The main factor
    motivating the use of cloud platforms is their ability to provide resources according
    to the customer''s needs or what is referred to as elastic provisioning and de-provisioning.
    Therefore, elasticity is one of the key features in cloud computing that dynamically
    adjusts the amount of allocated resources to meet changes in workload demands
    [1]. Sign in to Continue Reading Authors Figures References Citations Keywords
    Metrics More Like This Lightweight Virtualization Approaches for Software-Defined
    Systems and Cloud Computing: An Evaluation of Unikernels and Containers 2019 Sixth
    International Conference on Software Defined Systems (SDS) Published: 2019 Cloud
    Computing: Resource Management, Categorization, Scalability and Taxonomy 2023
    2nd Edition of IEEE Delhi Section Flagship Conference (DELCON) Published: 2023
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE transactions on services computing
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'Elasticity in Cloud Computing: State of the Art and Research Challenges'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3007748.3007777
  analysis: '>'
  authors:
  - Paolo Bellavista
  - Alessandro Zanni
  citation_count: 126
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesICDCNProceedingsICDCN
    ''17Feasibility of Fog Computing Deployment based on Docker Containerization over
    RaspberryPi RESEARCH-ARTICLE SHARE ON Feasibility of Fog Computing Deployment
    based on Docker Containerization over RaspberryPi Authors: Paolo Bellavista ,
    Alessandro Zanni Authors Info & Claims ICDCN ''17: Proceedings of the 18th International
    Conference on Distributed Computing and NetworkingJanuary 2017Article No.: 16Pages
    1–10https://doi.org/10.1145/3007748.3007777 Published:05 January 2017Publication
    History 110 citation 2,151 Downloads eReaderPDF ICDCN ''17: Proceedings of the
    18th International Conference on Distributed Computing and Networking Feasibility
    of Fog Computing Deployment based on Docker Containerization over RaspberryPi
    Pages 1–10 Previous Next ABSTRACT References Cited By Recommendations Comments
    ABSTRACT Fog computing is strongly emerging as a relevant and interest-attracting
    paradigm+technology for both the academic and industrial communities. However,
    architecture and methodological approaches are still prevalent in the literature,
    while few research activities have specifically targeted so far the issues of
    practical feasibility, cost-effectiveness, and efficiency of fog solutions over
    easily-deployable environments. In this perspective, this paper originally presents
    i) our fog-oriented framework for Internet-of-Things applications based on innovative
    scalability extensions of the open-source Kura gateway and ii) its Docker-based
    containerization over challenging and resource-limited fog nodes, i.e., RaspberryPi
    devices. Our practical experience and experimental work show the feasibility of
    using even extremely constrained nodes as fog gateways; the reported results demonstrate
    that good scalability and limited overhead can be coupled, via proper configuration
    tuning and implementation optimizations, with the significant advantages of containerization
    in terms of flexibility and easy deployment, also when working on top of existing,
    off-the-shelf, and limited-cost gateway nodes. References A. Faisal, D. Petriu,
    M. Woodside, "Network Latency Impact on Performance of Software Deployed Across
    Multiple Clouds", Int. Conf. Center for Advanced Studies on Collaborative Research
    (CASCON), 2013. W. Wang, K. Lee, D. Murray, "Integrating Sensors with the Cloud
    using Dynamic Proxies", IEEE Int. Symp. Personal Indoor and Mobile Radio Communications(PIMRC),
    2012. N.D. Lane, E. Miluzzo, H. Lu, D. Peebles, T. Choudhury, A.T. Campbell, "A
    Survey of Mobile Phone Sensing", IEEE Communications Magazine, Vol. 48, No. 9,
    pp. 140--150, Sept. 2010. Show All References Cited By View all Pallewatta S,
    Kostakos V and Buyya R. (2024). MicroFog: A framework for scalable placement of
    microservices-based IoT applications in federated Fog environments. Journal of
    Systems and Software. 10.1016/j.jss.2023.111910. 209. (111910). Online publication
    date: 1-Mar-2024. https://linkinghub.elsevier.com/retrieve/pii/S0164121223003059
    Shen Z, Jin J, Tan C, Tagami A, Wang S, Li Q, Zheng Q and Yuan J. (2023). A Survey
    of Next-generation Computing Technologies in Space-air-ground Integrated Networks.
    ACM Computing Surveys. 56:1. (1-40). Online publication date: 31-Jan-2024. https://doi.org/10.1145/3606018
    Chahoud M, Sami H, Mourad A, Otoum S, Otrok H, Bentahar J and Guizani M. On-Demand-FL:
    A Dynamic and Efficient Multicriteria Federated Learning Client Deployment Scheme.
    IEEE Internet of Things Journal. 10.1109/JIOT.2023.3265564. 10:18. (15822-15834).
    https://ieeexplore.ieee.org/document/10097281/ Show All Cited By Recommendations
    A Pattern for Fog Computing VikingPLoP ''16: Proceedings of the 10th Travelling
    Conference on Pattern Languages of Programs Fog Computing is a new variety of
    the cloud computing paradigm that brings virtualized cloud services to the edge
    of the network to control the devices in the IoT. We present a pattern for fog
    computing which describes its architecture, including its ... Read More Kubernetes:
    Towards Deployment of Distributed IoT Applications in Fog Computing ICPE ''20:
    Companion of the ACM/SPEC International Conference on Performance Engineering
    Fog computing has been regarded as an ideal platform for distributed and diverse
    IoT applications. Fog environment consists of a network of fog nodes and IoT applications
    are composed of containerized microservices communicating with each other. ...
    Read More Container-based fog computing architecture and energy-balancing scheduling
    algorithm for energy IoT Abstract The traditional architecture of fog computing
    is for one data center and multiple fog nodes. It is unable to adapt to the current
    development of private cloud. In addition, virtual machines used for cloud computing,
    are also used for ... Read More Comments 30 References View Table Of Contents
    Footer Categories Journals Magazines Books Proceedings SIGs Conferences Collections
    People About About ACM Digital Library ACM Digital Library Board Subscription
    Information Author Guidelines Using ACM Digital Library All Holdings within the
    ACM Digital Library ACM Computing Classification System Digital Library Accessibility
    Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Feasibility of Fog Computing Deployment based on Docker Containerization
    over RaspberryPi
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/2988336.2988337
  analysis: '>'
  authors:
  - Prateek Sharma
  - Lucas Chaufournier
  - Prashant Shenoy
  - Y. C. Tay
  citation_count: 158
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesMIDDLEWAREProceedingsMiddleware
    ''16Containers and Virtual Machines at Scale: A Comparative Study RESEARCH-ARTICLE
    SHARE ON Containers and Virtual Machines at Scale: A Comparative Study Authors:
    Prateek Sharma , Lucas Chaufournier , Prashant Shenoy , + 1 Authors Info & Claims
    Middleware ''16: Proceedings of the 17th International Middleware ConferenceNovember
    2016Article No.: 1Pages 1–13https://doi.org/10.1145/2988336.2988337 Published:28
    November 2016Publication History 148 citation 2,667 Downloads eReaderPDF Middleware
    ''16: Proceedings of the 17th International Middleware Conference Containers and
    Virtual Machines at Scale: A Comparative Study Pages 1–13 Previous Next ABSTRACT
    References Cited By Recommendations Comments ABSTRACT Virtualization is used in
    data center and cloud environments to decouple applications from the hardware
    they run on. Hardware virtualization and operating system level virtualization
    are two prominent technologies that enable this. Containers, which use OS virtualization,
    have recently surged in interest and deployment. In this paper, we study the differences
    between the two virtualization technologies. We compare containers and virtual
    machines in large data center environments along the dimensions of performance,
    manageability and software development. We evaluate the performance differences
    caused by the different virtualization technologies in data center environments
    where multiple applications are running on the same servers (multi-tenancy). Our
    results show that co-located applications can cause performance interference,
    and the degree of interference is higher in the case of containers for certain
    types of workloads. We also evaluate differences in the management frameworks
    which control deployment and orchestration of containers and VMs. We show how
    the different capabilities exposed by the two virtualization technologies can
    affect the management and development of applications. Lastly, we evaluate novel
    approaches which combine hardware and OS virtualization. References Amazon EC2
    Container Service. https://aws.amazon.com/ecs, June 2015. Docker. https://www.docker.com/,
    June 2015. Google container engine. https://cloud.google.com/container-engine,
    June 2015. Show All References Cited By View all Kappes G and Anastasiadis S.
    (2023). Diciclo: Flexible User-level Services for Efficient Multitenant Isolation.
    ACM Transactions on Computer Systems. 42:1-2. (1-47). Online publication date:
    31-May-2024. https://doi.org/10.1145/3639404 Urblik L, Kajati E, Papcun P and
    Zolotová I. (2024). Containerization in Edge Intelligence: A Review. Electronics.
    10.3390/electronics13071335. 13:7. (1335). https://www.mdpi.com/2079-9292/13/7/1335
    Wang K, Wu S, Li S, Huang Z, Fan H, Yu C and Jin H. (2023). Precise control of
    page cache for containers. Frontiers of Computer Science. 10.1007/s11704-022-2455-0.
    18:2. Online publication date: 1-Apr-2024. https://link.springer.com/10.1007/s11704-022-2455-0
    Show All Cited By Containers and Virtual Machines at Scale: A Comparative Study
    Software and its engineering Software organization and properties Contextual software
    domains Recommendations Traffic-sensitive live migration of virtual machines CCGRID
    ''15: Proceedings of the 15th IEEE/ACM International Symposium on Cluster, Cloud,
    and Grid Computing In this paper we address the problem of network contention
    between the migration traffic and the Virtual Machine (VM) application traffic
    for the live migration of co-located Virtual Machines. When VMs are migrated with
    pre-copy, they run at the source ... Read More Live gang migration of virtual
    machines HPDC ''11: Proceedings of the 20th international symposium on High performance
    distributed computing This paper addresses the problem of simultaneously migrating
    a group of co-located and live virtual machines (VMs), i.e, VMs executing on the
    same physical machine. We refer to such a mass simultaneous migration of active
    VMs as "live gang migration". ... Read More Optimizing virtual machines using
    hybrid virtualization Minimizing virtualization overhead and improving the reliability
    of virtual machines are challenging when establishing virtual machine cluster.
    Paravirtualization and hardware-assisted virtualization are two mainstream solutions
    for modern system ... Read More Comments 53 References View Table Of Contents
    Footer Categories Journals Magazines Books Proceedings SIGs Conferences Collections
    People About About ACM Digital Library ACM Digital Library Board Subscription
    Information Author Guidelines Using ACM Digital Library All Holdings within the
    ACM Digital Library ACM Computing Classification System Digital Library Accessibility
    Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Containers and Virtual Machines at Scale
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s10723-016-9366-y
  analysis: '>'
  authors:
  - René Peinl
  - Florian Holzschuher
  - Florian Pfitzer
  citation_count: 96
  full_citation: '>'
  full_text: '>

    Your privacy, your choice We use essential cookies to make sure the site can function.
    We also use optional cookies for advertising, personalisation of content, usage
    analysis, and social media. By accepting optional cookies, you consent to the
    processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Grid Computing Article
    Docker Cluster Management for the Cloud - Survey Results and Own Solution Published:
    13 April 2016 Volume 14, pages 265–282, (2016) Cite this article Download PDF
    Access provided by University of Nebraska-Lincoln Journal of Grid Computing Aims
    and scope Submit manuscript René Peinl, Florian Holzschuher & Florian Pfitzer  2347
    Accesses 82 Citations 1 Altmetric Explore all metrics Abstract Docker provides
    a good basis to run composite applications in the cloud, especially if those are
    not cloud-aware, or cloud-native. However, Docker concentrates on managing containers
    on one host, but SaaS providers need a container management solution for multiple
    hosts. Therefore, a number of tools emerged that claim to solve the problem. This
    paper classifies the solutions, maps them to requirements from a case study and
    identifies gaps and integration requirements. We close some of these gaps with
    our own integration components and tool enhancements, resulting in the currently
    most complete management suite. Article PDF Similar content being viewed by others
    Virtualization in Cloud Computing: Moving from Hypervisor to Containerization—A
    Survey Article 13 April 2021 Strategies and Methods for Cloud Migration Article
    01 April 2014 The evolution of distributed computing systems: from fundamental
    to new frontiers Article 30 January 2021 References Pahl, C.: Containerization
    and the PaaS Cloud. IEEE Cloud Comput., 24–31 (2015) Scheepers, M. J.: Virtualization
    and Containerization of Application Infrastructure: A Comparison. Presented at
    the 21st Twente Student Conference on IT , Twente The Netherlands June 23 (2014)
    Pahl, C., Lee, B.: Containers and clusters for edge cloud architectures-a technology
    review (2015) Ranjan, R.: The cloud interoperability challenge. IEEE Cloud comput.
    1, 20–24 (2014) Article   Google Scholar   Rosen, R.: Linux containers and the
    future cloud. Linux J 2014, 3 (2014) Google Scholar   Kratzke, N.: Lightweight
    virtualization cluster how to overcome cloud vendor Lock-In. J. Comput. Commun.
    2, 1–7 (2014) Article   Google Scholar   Rubens, P.: Docker No Longer the Only
    Container Game in Town, http://bit.ly/1IlkI0s (2015) Hecht, L.: How Open Source
    Communities Power Docker and the Container Ecosystem, http://bit.ly/1LSIoLW (2015)
    Turnbull, J.: The Docker Book: Containerization is the new virtualization James
    Turnbull (2014) Lewis, J., Fowler, M.: Microservices, http://bit.ly/1dI7ZJQ (2014)
    Papazoglou, M. P.: Service-Oriented Computing: Concepts, characteristics and directions.
    In: Web Information Systems Engineering (WISE 2003). 4Th Int. Conf. On. Pp. 3–12.
    IEEE (2003) Mietzner, R., Leymann, F., Papazoglou, M. P.: Defining Composite Configurable
    SaaS Application Packages Using SCA, variability descriptors and multi-tenancy
    patterns. In: ICIW 2008. IEEE (2008) Cockcroft, A.: State of the Art in Microservices.
    Presented at the DockerCon Europe 14 , Amsterdam Netherlands December 4 (2014)
    Evans, E.: Domain Driven Design: Tackling Complexity in the Heart of Software.
    Addison-Wesley, Boston (2003) Google Scholar   Binz, T., Breitenbücher, U., Kopp,
    O., Leymann, F.: TOSCA: Portable Automated Deployment and Management of Cloud
    Applications. In: Advanced Web Services. Pp. 527–549. Springer (2014) Roßbach,
    P.: Docker poster. Entwickler mag docker spez (2014) Docker Ecosystem Mindmap,
    http://bit.ly/1BjDgtW Peinl, R.: Docker ecosystem on Google Docs, http://bit.ly/1DJ0eS4
    Docker, Inc.: About, http://bit.ly/1OjEBLl Crane, C.: The Container Ecosystem
    Project, http://bit.ly/1RkyBTu Wallner, R.: A breakdown of layers and tools within
    the container and microservices ecosystem, http://bit.ly/21cttZN (2015) Williams,
    A.: The Docker & Container Ecosystem The New Stack (2015) Chauhan, M. A., Babar,
    M. A.: Migrating Service-Oriented System to Cloud Computing: an Experience Report.
    In: Cloud Computing (CLOUD) 2011, IEEE Int. Conf. On. Pp. 404–411. IEEE (2011)
    Coffey, J., White, L., Wilde, N., Simmons, S.: Locating Software Features in a
    SOA Composite Application. In: 8Th European Conf. on Web Services (ECOWS 2010).
    Pp. 99–106. IEEE (2010) Sefraoui, O., Aissaoui, M., Eleuldj, M.: Openstack: toward
    an open-source solution for cloud computing. Int. J. Comput. Appl. 55, 38–42 (2012)
    Google Scholar   Koukis, V.: Flexible Storage for HPC Clouds with Archipelago
    and Ceph. In: 8Th Workshop on Virtualization in High-Performance Cloud Computing.
    ACM (2013) Chadwick, D. W., Siu, K., Lee, C., Fouillat, Y., Germonville, D.: Adding
    federated identity management to openstack. J. Grid Comput 12, 3–27 (2014) Article   Google
    Scholar   Piraghaj, S. F., Dastjerdi, A. V., Calheiros, R. N., Buyya, R.: Efficient
    Virtual Machine Sizing for Hosting Containers as a Service. In: IEEE World Congress
    on Services. Pp. 31–38. IEEE (2015) Jain, R., Paul, S.: Network virtualization
    and software defined networking for cloud computing: a survey. Commun. Mag. IEEE
    51, 24–31 (2013) Article   Google Scholar   Day, S.: Docker Registry V2 - A New
    Model for Image Distribution. In: Docker Con 2015. , San Francisco (2015) Hausenblas,
    M.: Docker Registries: the Good, the Bad & the Ugly, http://bit.ly/1OsrnIu Mills,
    K., Filliben, J., Dabrowski, C.: Comparing VM Placement Algorithms for On-Demand
    Clouds. In: 3Rd Int. Conf. on Cloud Computing Technology and Science (Cloudcom).
    Pp. 91–98. IEEE (2011) Verma, A., Pedrosa, L., Korupolu, M., Tune, D. O. E., Wilkes,
    J.: Large-Scale Cluster Management at Google with Borg. In: 10Th European Conference
    on Computer Systems. P. 18. ACM (2015) Schwarzkopf, M., Konwinski, A., Abd-El-Malek,
    M., Wilkes, J.: Omega: Flexible, Scalable Schedulers for Large Compute Clusters.
    In: 8Th ACM European Conf. on Computer Systems. Pp. 351–364. 1 (2013) Bucchiarone,
    A., Gnesi, S.: A Survey on Services Composition Languages and Models. In: Intl.
    Workshop on Web Services–Modeling and Testing (WS-Mate 2006). P. 51 (2006) Caballer,
    M., Blanquer, I., Moltó, G., de Alfonso, C.: Dynamic management of virtual infrastructures.
    J. Grid Comput 13, 53–70 (2015) Article   Google Scholar   Bachlechner, D., Siorpaes,
    K., Fensel, D., Toma, I.: Web Service Discovery-A Reality Check. In: 3Rd European
    Semantic Web Conference (2006) Vukojevic-Haupt, K., Haupt, F., Karastoyanova,
    D., Leymann, F.: Service Selection for On-demand Provisioned Services. In: 18Th
    Intl. Enterprise Distributed Object Computing Conference (EDOC’14). Pp. 120–127.
    IEEE (2014) Serebryany, I., Rhoads, M.: SmartStack: Service Discovery in the Cloud,
    http://bit.ly/1bRcjo2(2013) Swan, C.: ClusterHQ Launches Flocker to Facilitate
    Robust Stateful Docker Containers, http://bit.ly/1KH3zG3 (2014) Hall, S.: Five
    Storage Companies That Speak To Docker’s Next Wave, http://bit.ly/1VBHYzp (2015)
    Han, S.: Getting Started With the Docker RBD Volume Plugin, http://bit.ly/1RYVONi
    (2015) Lorido-Botran, T., Miguel-Alonso, J., Lozano, J. A.: A review of auto-scaling
    techniques for elastic applications in cloud environments. J. Grid Comput 12,
    559–592 (2014) Article   Google Scholar   Costache, C., Machidon, O., Mladin,
    A., Sandu, F., Bocu, R.: Software-Defined Networking of Linux Containers. In:
    13Th Roedunet Conference. IEEE (2014) Drutskoy, D., Keller, E., Rexford, J.: Scalable
    network virtualization in software-defined networks. IEEE Internet Comput. 17,
    20–27 (2013) Article   Google Scholar   Rimal, B. P., Jukan, A., Katsaros, D.,
    Goeleven, Y.: Architectural requirements for cloud computing systems: an enterprise
    cloud approach. J. Grid Comput 9, 3–26 (2011) Article   Google Scholar   Liu,
    H., Wee, S.: Web Server Farm in the Cloud: Performance Evaluation and Dynamic
    Architecture. In: Cloud Computing. Pp. 369–380. Springer (2009) Aceto, G., Botta,
    A., De Donato, W., Pescapè, A.: Cloud monitoring: a survey. Comput. Netw 57, 2093–2115
    (2013) Article   Google Scholar   Ward, J. S., Barker, A.: Observing the clouds:
    a survey and taxonomy of cloud monitoring. J. Cloud Comput. Adv. Syst. Appl. 3,
    40 (2014) Article   Google Scholar   Yegulalp, S.: Docker Datacenter promises
    end-to-end container control for enterprises, http://bit.ly/1QYXoK1 (2016) Polvi,
    A.: The Security-minded Container Engine by CoreOS: rkt Hits 1.0, http://bit.ly/1S3iyw5
    (2016) Kratzke, N.: About Microservices, Containers and their Underestimated Impact
    on Network Performance. CLOUD Comput. 2015, 180 (2015) Google Scholar   Kazemier,
    A.: BalancerBattle, https://github.com/observing/balancerbattle Felter, W., Ferreira,
    A., Rajamony, R., Rubio, J.: An upyeard performance comparison of virtual machines
    and linux containers. Research Report RC25482, IBM Almaden (2014) Seo, K. -T.,
    Hwang, H. -S., Moon, I. -Y., Kwon, O. -Y., Kim, B. -J.: Performance comparison
    analysis of linux container and virtual machine for building cloud. Adv. Sci.
    Technol. Lett. Netw. Commun 66, 105–107 (2014) Article   Google Scholar   Kabhal:
    Introduce multiple scaling strategies mesosphere/marathon Issue #1477, http://bit.ly/1QAETBl(2015)
    Lindner, M., Galán, F., Chapman, C., Clayman, S., Henriksson, D., Elmroth, E.:
    The Cloud Supply Chain: a Framework for Information, Monitoring, Accounting and
    Billing 2Nd Int. Conf. on Cloud Comp (2010) Sreelakshmi, S.: OpenContrail – Kubernetes
    Integration, http://bit.ly/1Q91Jun (2015) Peinl, R., Holzschuher, F.: The Docker
    Ecosystem Needs Consolidation. In: 5Th Intl. Conf. on Cloud Computing and Services
    Science (CLOSER 2015) 535-542 SCITEPRESS, Lisbon, Portugal (2015) Kazemi, S.:
    CRIU Support in Docker for Native Checkpoint and Restore. In: Linux Plumbers Conference
    2015. , Seattle, Washington, USA (2015) Berman, L.: Are Diego and Docker Really
    Good Friends?, http://bit.ly/1WGpAW4 (2015) Dadgar, A.: Nomad, http://bit.ly/1MV4bYB
    (2015) Hashicorp: Nomad vs. Other Software, http://bit.ly/1OsqLCG (2015) Owens,
    K.: Building Cisco’s IoE PaaS with Mantl, http://bit.ly/1KFP9Ck (2015) Yegulalp,
    S.: Hypernetes unites Kubernetes, OpenStack for multitenant container management,
    http://bit.ly/1QvVrUS (2015) Crisp Research Open cloud alliance - openness as
    an imperative crisp research (2014) Kratzke, N.: A lightweight virtualization
    cluster reference architecture derived from open source PaaS platforms. Open J.
    Mob. Comput. Cloud Comput 1, 17–30 (2014) Google Scholar   Download references
    Author information Authors and Affiliations Institute of Information Systems,
    Hof University, Alfons-Goppel-Platz 1, Hof, Germany René Peinl, Florian Holzschuher
    & Florian Pfitzer Corresponding author Correspondence to René Peinl. Rights and
    permissions Reprints and permissions About this article Cite this article Peinl,
    R., Holzschuher, F. & Pfitzer, F. Docker Cluster Management for the Cloud - Survey
    Results and Own Solution. J Grid Computing 14, 265–282 (2016). https://doi.org/10.1007/s10723-016-9366-y
    Download citation Received 27 October 2015 Accepted 31 March 2016 Published 13
    April 2016 Issue Date June 2016 DOI https://doi.org/10.1007/s10723-016-9366-y
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Cloud computing Management tools Microservices System integration
    Docker Container Use our pre-submission checklist Avoid common mistakes on your
    manuscript. Sections References Abstract Article PDF References Author information
    Rights and permissions About this article Advertisement Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature'
  inline_citation: '>'
  journal: Journal of grid computing
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Docker Cluster Management for the Cloud - Survey Results and Own Solution
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/w-ficloud.2016.36
  analysis: '>'
  authors:
  - Claus Pahl
  - Sven Helmer
  - Lorenzo Miori
  - Julian Sanin
  - Brian Lee
  citation_count: 101
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2016
    IEEE 4th International C... A Container-Based Edge Cloud PaaS Architecture Based
    on Raspberry Pi Clusters Publisher: IEEE Cite This PDF Claus Pahl; Sven Helmer;
    Lorenzo Miori; Julian Sanin; Brian Lee All Authors 94 Cites in Papers 6 Cites
    in Patents 3291 Full Text Views Abstract Document Sections I. Introduction II.
    Edge Cloud Architectures III. Containers IV. Edge Clouds-Container and Orchestration
    V. Clustering and Orchestrating Containers Show Full Outline Authors Figures References
    Citations Keywords Metrics Footnotes Abstract: Cloud technology is moving towards
    multi-cloud environments with the inclusion of various devices. Cloud and IoT
    integration resulting in so-called edge cloud and fog computing has started. This
    requires the combination of data centre technologies with much more constrained
    devices, but still using virtualised solutions to deal with scalability, flexibility
    and multi-tenancy concerns. Lightweight virtualisation solutions do exist for
    this architectural setting with smaller, but still virtualised devices to provide
    application and platform technology as services. Containerisation is a solution
    component for lightweight virtualisation solution. Containers are furthermore
    relevant for cloud platform concerns dealt with by Platform-as-a-Service (PaaS)
    clouds like application packaging and orchestration. We demonstrate an architecture
    for edge cloud PaaS. For edge clouds, application and service orchestration can
    help to manage and orchestrate applications through containers. In this way, computation
    can be brought to the edge of the cloud, rather than data from the Internet-of-Things
    (IoT) to the cloud. We show that edge cloud requirements such as cost-efficiency,
    low power consumption, and robustness can be met by implementing container and
    cluster technology on small single-board devices like Raspberry Pis. This architecture
    can facilitate applications through distributed multi-cloud platforms built from
    a range of nodes from data centres to small devices, which we refer to as edge
    cloud. We illustrate key concepts of an edge cloud PaaS and refer to experimental
    and conceptual work to make that case. Published in: 2016 IEEE 4th International
    Conference on Future Internet of Things and Cloud Workshops (FiCloudW) Date of
    Conference: 22-24 August 2016 Date Added to IEEE Xplore: 18 October 2016 ISBN
    Information: DOI: 10.1109/W-FiCloud.2016.36 Publisher: IEEE Conference Location:
    Vienna, Austria I. Introduction Cloud computing is moving from large-scale centralised
    data centres to more distributed multi-cloud settings. These can consist of networks
    of larger and smaller virtualised infrastructure runtime nodes that connect to
    IoT (Internet-of-Things) devices with centralised data centres. To meet the flexibility,
    elasticity and cost requirements of smaller devices, virtualisation needs to be
    applied throughout, requiring Internet-of Things (IoT) infrastructures to be integrated.
    These architectures are often referred to as edge clouds or fog computing architectures
    [4]. Resulting from smaller devices and distribution, a more lightweight solutions
    than the current virtual machine (VM)-based virtualisation technology is needed.
    Furthermore, as another challenge, an architecture supporting the orchestration
    of lightweight virtualised runtimes is needed. Sign in to Continue Reading Authors
    Figures References Citations Keywords Metrics Footnotes More Like This Containers
    and Clusters for Edge Cloud Architectures -- A Technology Review 2015 3rd International
    Conference on Future Internet of Things and Cloud Published: 2015 An Efficient
    Online Placement Scheme for Cloud Container Clusters IEEE Journal on Selected
    Areas in Communications Published: 2019 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: A Container-Based Edge Cloud PaaS Architecture Based on Raspberry Pi Clusters
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2017.2704444
  analysis: '>'
  authors:
  - Roberto Morabito
  citation_count: 198
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Journals & Magazines >IEEE Access >Volume: 5 Virtualization on Internet
    of Things Edge Devices With Container Technologies: A Performance Evaluation Publisher:
    IEEE Cite This PDF Roberto Morabito All Authors 183 Cites in Papers 10726 Full
    Text Views Open Access Comment(s) Abstract Document Sections I. Introduction II.
    Related Work III. Enabling Technologies IV. Methodology and Experimental Setup
    V. Measurement Results and Analysis Show Full Outline Authors Figures References
    Citations Keywords Metrics Footnotes Abstract: Lightweight virtualization technologies
    have revolutionized the world of software development by introducing flexibility
    and innovation to this domain. Although the benefits introduced by these emerging
    solutions have been widely acknowledged in cloud computing, recent advances have
    led to the spread of such technologies in different contexts. As an example, the
    Internet of Things (IoT) and mobile edge computing benefit from container virtualization
    by exploiting the possibility of using these technologies not only in data centers
    but also on devices, which are characterized by fewer computational resources,
    such as single-board computers. This has led to a growing trend to more efficiently
    redesign the critical components of IoT/edge scenarios (e.g., gateways) to enable
    the concept of device virtualization. The possibility for efficiently deploying
    virtualized instances on single-board computers has already been addressed in
    recent studies; however, these studies considered only a limited number of devices
    and omitted important performance metrics from their empirical assessments. This
    paper seeks to fill this gap and to provide insights for future deployments through
    a comprehensive performance evaluation that aims to show the strengths and weaknesses
    of several low-power devices when handling container-virtualized instances. A
    performance evaluation of container-based virtualization on low-power Single-Board
    Computers. Published in: IEEE Access ( Volume: 5) Page(s): 8835 - 8850 Date of
    Publication: 17 May 2017 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2017.2704444
    Publisher: IEEE Funding Agency: SECTION I. Introduction Over the last decade,
    the approach used to enhance the network efficiency and cope with the increasing
    number of connected devices to the Internet has been to move computation, control,
    and data storage into the cloud [11]. However, cloud computing now faces several
    challenges to meet the more stringent performance requirements of many application
    services, especially in terms of latency and bandwidth. Edge computing is an emerging
    paradigm that aims to increase infrastructure efficiency by delivering low-latency,
    bandwidth-efficient and resilient end-user services [12]. This edge cloud is not
    intended to replace centralized cloud-based infrastructure in its entirety, but
    rather to complement it by increasing the computing and storage resources available
    on the edge by adopting platforms that provide intermediate layers of computation,
    networking, and storage [13]. Especially in Internet of Things (IoT) scenarios,
    several edge processing tasks must be performed at the network edge on hardware
    characterized by processing, memory, and storage capabilities lower than is typical
    of server machines [7]. As an example, a Single-Board Computer (SBC) or comparable
    devices such as Micro-Servers can contain suitable hardware for performing such
    operations [28]. However, because of the limited computational capabilities of
    such devices, it is not always possible to deploy processing operations at the
    network edge. In this case, data centers are necessary to manage heavier computational
    requirements. Considering the heterogeneity of the entire scenario, there may
    be divergence (e.g., in terms of CPU architecture) between the various nodes involved.
    Simultaneously, however, the same software may need to be deployed at either the
    edge or in the data center. One way to ensure that data centers and constrained
    edge entities execute complementary software arises from the possibility of using
    lightweight virtualization technologies—in particular, containers. Container virtualization
    allows hardware resources to be decoupled from software, enabling packaged software
    to execute on multiple hardware architectures. Compared to alternative virtualization
    solutions such as hypervisors, container technologies provide several benefits
    such as rapid construction, instantiation, and initialization of virtualized instances.
    In addition, systems that rely on containers benefit from higher application/services
    allocations because of the smaller dimensions of the virtual images [14]. These
    container features are well-matched to the requirements of IoT/Edge scenarios.
    Previous works have demonstrated the feasibility of using container technologies
    on IoT resource-constrained devices [6], [21]. However, the number of devices
    tested thus far has been extremely limited. For example, only the older version
    of the Raspberry Pi board has been considered as a suitable device. Furthermore,
    important performance metrics such as power consumption and energy efficiency
    have not been considered in previous analyses. By investigating the performance
    of container virtualization on a wide set of low-power edge devices for the IoT,
    the objective of this paper is to provide insights for optimally using SBCs during
    the execution of virtualized instances. We adopt an empirical approach to quantify
    the overhead introduced by the virtualization layer under computing-intensive
    scenarios and networking-intensive traffic. Additionally, power consumption, energy
    efficiency, and device temperatures are considered in our analysis. The remainder
    of this paper is organized as follows. Section II lists the related work. Section
    III provides background information about the hardware and software technologies
    employed in our study. Section IV gives a detailed description of the methodology
    and experimental setup used to carry out the empirical investigation. In Section
    V, we evaluate the impact of using container virtualization on top of different
    SBCs by taking various aspects into consideration. Section VI concludes the paper
    and provides final remarks. SECTION II. Related Work The current literature includes
    several proposals for solutions in which virtualization technologies are employed
    at the network edge and/or on low-power nodes such as SBCs. However, Raspberry
    Pi is usually the only SBC considered for such deployments. Ismail et al. in [4]
    evaluated Docker containers as an enabling technology for deploying an edge computing
    platform. The authors evaluated the technology in terms of (i) deployment and
    termination of services, (ii) resource and service management, (iii) fault tolerance,
    and (iv) caching capabilities, concluding that Docker represents a good solution
    for use in edge computing contexts. Petrolo et al. [5] and Morabito and Beijar
    [6] included lightweight virtualization technologies in the design of an IoT gateway.
    Petrolo et al. [5] employed virtualized software to provide a dense deployment
    of services at the gateway level. Particularly interesting is their analysis of
    the possible interactions between IoT sensors and gateways. Such analysis suggests
    how the dynamic allocation of services, by means of containers, provides several
    benefits from a gateway performance perspective. In our previous work [6], we
    proposed a design for an IoT gateway that can also be efficiently employed in
    edge computing architectures. In that study, we showed how to efficiently and
    flexibly use Docker containers to customize an IoT platform that offers several
    virtualized services such as (i) Device Management capabilities, (ii) Software
    Defined Networking (SDN) support, and (iii) Orchestration and Data Management
    capabilities. In [7], container technologies were also used in a Capillary Network
    scenario. This study used Docker containers for packaging, deployment, and execution
    of software both in the cloud and in more constrained environments such as local
    capillary gateways. The dual purpose of these latter entities is to provide connectivity
    between short-range and cellular networks and to make different software components
    available for local device management and instantiation of distributed cloud processes.
    In [8] container-based virtualization and SBCs were identified as enabling technologies
    to enhance the provisioning of IoT Cloud services. Krylovskiy identified and analyzed
    the requirements for efficiently designing IoT gateways in [9]. The author considered
    the Docker containers to be suitable technology for meeting such requirements.
    In that study, several synthetic and application benchmarks were used to quantify
    the overhead introduced by the virtualization layer. However, the study’s performance
    analysis was limited; it considered only the Raspberry Pi board. Furthermore,
    it lacked a comprehensive power consumption and energy efficiency evaluation.
    The cloudlet architecture proposed by the Carnegie-Mellon university [10], which
    represents an efficient approach for mobile-cloud computation can also be considered
    as linked to our work. Unlike the previous studies the granularity for virtualized
    instances was Virtual Machines (VMs). Pahl et al. [25] proposed a Container-based
    Edge Cloud PaaS Architecture based on Raspberry Pi clusters, in which container
    technologies are used to favor migrating toward edge cloud architectures. The
    authors claim that the deployment of the Raspberry Pi clusters represents an enabling
    hardware technology to ensure important requirements such as cost-efficiency and
    low power consumption. However, this work lacked an empirical investigation to
    support its claims. Only the Raspberry Pi was considered for the edge cluster
    deployment; other hardware alternatives were not considered. Similarly, [26] used
    Docker containers and Raspberry Pi to leverage the deployment of a framework for
    Fog Computing networks. In addition, the evaluation of the overhead introduced
    by containers in SBC was limited when assessing the impact of different file-system
    configurations on disk performance. Specifically, Docker container performances
    were compared with native executions, and the study provided estimates of the
    overhead and delays introduced by different file systems (AUFS, Device mapper,
    and OverlayFS). Finally, Hajji et al. [27] presented a detailed performance evaluation
    to understand the performance of a Raspberry Pi cloud when handling big data applications
    packaged through Docker containers. Before moving on to the next section, we provide
    a brief overview on the real-world state of modern IoT applications. This overview
    can help readers understand—in conjunction with the empirical discussion that
    follows in this paper—what kinds of applications will perform better in the SBCs
    tested here; it is based on application performance requirements such as low energy
    consumption, high CPU and/or RAM performance, many simultaneous tasks, high-speed
    data connections, and so forth. The IoT encompasses several domains including
    E-health, Intelligent Transport Systems (ITS), Smart Cities, Smart Homes, Smart
    Industries, etc [42]–[44]. The applications in these categories give rise to a
    considerable number of use cases, and their performance requirements vary from
    case to case as do the ways such applications interact with IoT/Edge gateways.
    Indeed, the number of interactions between an IoT device and a gateway can vary
    significantly. Some types of sensors provide measured values to the gateway periodically
    (e.g., once per hour), as in, for example, environment sensing, location sensing,
    and location info sharing use cases [44]. The applications used in the gateway
    to handle such scenarios must be particularly efficient in terms of disk I/O responsiveness.
    In contrast, other types of devices such as sensors with cameras, may continuously
    generate significant amounts of data—e.g., for remote control use cases. For this
    second group of sensors, the gateway operations required are rather different
    compared to the first group. For example, video applications usually demand high
    CPU processing and network bandwidth. Furthermore, video data might be characterized
    by a high degree of redundancy. Data compression applications, which are particularly
    challenging in terms of CPU and memory, can be employed in the gateway to reduce
    the amount of data that must be transmitted over the uplink from the gateway to
    the cloud [6]. The energy efficiency of key IoT/Edge entities is particularly
    crucial for battery-powered wireless sensor networks [41]. From this viewpoint,
    understanding which SBCs ensure the best performance/energy efficiency tradeoff
    facilitates the design of such constrained networks. By considering a wide set
    of workloads, knowledge of the boards’ energy efficiencies can help in estimating
    the energy lifetime of battery-powered nodes, and help SBCs offer support for
    cases where they must be battery powered. Finally, recent works have revealed
    a growing trend toward designing gateways shared between different tenants [40].
    In many cases, the container is the technology used to ensure isolation between
    different users. This represents a scenario in which there is a need to understand
    how IoT/Edge entities behave when receiving simultaneous virtualized instances
    and heterogeneous workloads. SECTION III. Enabling Technologies In this section,
    we provide an overview of the different technologies involved in our study. A.
    Container Virtualization Technology Compared to hypervisors, container-based virtualization
    provides different abstraction levels regarding virtualization and isolation.
    Hypervisors virtualize hardware and device drivers, generating a higher overhead.
    In contrast, containers avoid such overhead by implementing process isolation
    at the operating system level [14]. A single container instance combines the application
    with all its dependencies; it runs as an isolated process in user space on the
    host operating system (OS), while the OS kernel is shared among all the containers
    (Fig. 1). The lack of the need for hardware/driver virtualization, together with
    the shared kernel feature, provide the ability to achieve a higher virtualized
    instance density because the resulting disk images are smaller. In our performance
    evaluation, we used Docker1 containers to package virtualized instances. Docker
    introduces an underlying container engine and a functional API that supports easy
    construction, management and removal of containerized applications. Within a Docker
    container, one or more processes/applications can run simultaneously. Alternatively,
    an application can be designed to work in multiple containers, which can interact
    with each other through a linking system. This also guarantees that no conflicts
    will occur with other application containers running on the same machine. FIGURE
    1. Container-based virtualization architecture. Show All B. ARM-Based SBCs The
    ARM architecture is becoming increasingly widespread due primarily to its low-power
    characteristics, low cost, and its use in smartphones, tablets, and other devices
    [18]. Despite these characteristics, modern ARM multi-core processors can compete
    with general purpose CPUs [19]. Most current ARM processors are 32-bit, although
    the use of more powerful 64-bit devices is growing. These hardware enhancements
    are enabling the spread of solutions such as ultra-low power clusters [20] based
    on SBCs powered by ARM architectures. SECTION IV. Methodology and Experimental
    Setup This section provides information about the methodology and experimental
    setup used to perform this empirical investigation. A. Tested Single-Board Computers
    The wide range of SBCs used in our evaluation allows characterizing the performance
    and gaining a deep understanding of the potentialities of a wide set of devices.
    The devices selected for the analysis are described below. The Raspberry Pi2 model
    B (RPi2) is the second generation of the Raspberry Pi platform (Fig. 2a). Released
    in February 2015, RPi2 increased the performance and other hardware characteristics
    compared to previous versions. It includes a quad-core ARM Cortex-A7 CPU and 1
    GB of RAM. FIGURE 2. Single-Board Computer under test: (a) RPi2. (b) OC1+. (c)
    OC2. (d) OXU4. (e) RPi3. Show All TheRaspberry Pi3 model B (RPi3) is the third
    generation of the Raspberry Pi platform (Fig. 2e) and was released in February
    2016. It is the first model with a 64-bit CPU. The new model also integrates Bluetooth
    modules (4.1 and Low Energy) and Wi-Fi 802.11n (2.4 GHz). Odroid is a series of
    SBCs manufactured by Hardkernel Co., Ltd. Most of the Odroid systems are capable
    of running both regular Linux and Android distributions. Odroid C1+4 (OC1+) was
    released in July 2015, replacing Odroid C1 with improved and enhanced hardware
    features (Fig. 2b). Although OC1+ represents the most similar board to the RPi2
    in terms of its hardware characteristics, it includes a faster CPU and its Network
    Interface Card (NIC) has a faster line speed. Odroid C25 (OC2) is the most recent
    Odroid SBC (released by Hardkernel in March 2016) and is the first Odroid model
    with a 64-bit CPU (Fig. 2c). Compared to OC1+, it includes additional RAM (2 GB).
    Odroid XU46 (OXU4) features an ARM Octa-Core with big.LITTLE computing architecture
    (Fig. 2d). This chipset is characterized by a particularly heterogeneous CPU architecture
    that features two groups of cores: four ARM Cortex-A7 LITTLE cores (1.4 GHz),
    and four ARM Cortex-A15 big cores (2 GHz). The first group of cores reduces the
    power consumption at the expense of slower performance. In contrast, the second
    group consumes more power but features faster execution. The peculiarity of this
    architecture lies in the fact that if one core group is active, the other one
    is either powered down or used only if the first group saturates its resources.
    An application cannot work on both groups of cores at the same time. Table 1 summarizes
    the hardware characteristics of the SBCs used in our study. The most relevant
    differences between the boards occur in terms of their CPU, flash storage, and
    Ethernet capabilities. TABLE 1 Raspberry Pi and Odroid Hardware Features From
    a software perspective, for the Raspberry Pi we used an image provided by Hypriot
    running Raspbian Jessie with the Linux kernel 4.4.10 as a base OS. This image
    provides a lightweight environment optimized for executing Docker container technologies
    on top of Raspberry Pi devices. For the Odroid platforms, we used the following
    Hardkernel OS stable releases: Ubuntu version 14.04 for OC1+, Ubuntu version 16.04
    for OC2, and Ubuntu 15.10 for OXU4. All the SBCs were equipped with a 16 GB Transcend
    Premium 400x Class 10 UHS-I microSDHC™ memory card for storage. B. Setup For Virtualized
    Environment The configuration used to customize the virtualized environment was
    similar for all the SBCs under evaluation. Docker version 1.12.0 was used as the
    container technology. The base Docker images—which represent the basic entity
    from which Docker containers are created—used to virtualize the software tools
    used for our benchmarking tests were as follows: ARMv7 armhf/debian image, with
    RPi2, OC1+, and OXU4. ARMv8 aarch64/debian image, with OC2 and RPi3. In virtualization,
    vCPU pinning (or processor affinity) is a relevant aspect that influences the
    performance. vCPU pinning indicates the possibility of dedicating a physical CPU
    to a single instance or a set of virtual CPUs. Several vCPU pinning configurations
    exist. However, in the one selected for this analysis, each vCPU can run on any
    physical CPU core (Fig. 3). Such a configuration is typically termed a random
    setup and provides the advantage of higher CPU utilization [16]. FIGURE 3. CPU
    affinity setup. Show All C. Testbed Setup for Power and Network Measurements The
    power consumption of the SBCs was measured using an external voltage meter (USB-1608FS-Plus),
    characterized by a 16-bit resolution and a setup similar to the one used in [15].
    The measurements involving network communications were performed using an Intel
    Core 2 Duo PC running Linux 3.13.0 with an Intel 82567LM Gigabit Ethernet card.
    The PC was directly connected to the NIC of the SBC under test. Fig. 4 shows the
    entire testbed environment setup. FIGURE 4. Testbed setup. Show All D. Workloads
    for the Evaluation As mentioned earlier, the main goal of our study was to understand
    how different SBCs react to specific workloads generated by applications running
    within Docker containers. Specifically, we want to characterize the performance
    from two different aspects. First, we want to define an upper bound for the performance
    of such devices when handling virtualized applications that challenge a particular
    segment of the underlying hardware. This kind of evaluation represents a key aspect
    because it allows us to verify that the virtualization layer does not generate
    excessive overhead that affects overall board performance. To achieve this, we
    used different benchmark tools to generate intensive CPU, Disk I/O, Memory, and
    Network I/O workloads. For the CPU and Network tests, we performed all the measurements
    using up to eight/sixteen guest domains. Second, we also considered heterogeneous
    virtualized instances to quantify any possible overhead introduced by containers;
    these measurements were made using applications that are closer to real-world
    workloads. We adopted native performance (i.e., running the benchmark tool without
    including any virtualization layer) as a base case to quantify the overhead. We
    also repeated each measurement target with different tools, to analyze the consistency
    between different results. The results were averaged over 20 runs. The tables
    and graphs in this paper show the averages of such measurements. SECTION V. Measurement
    Results and Analysis In this section, we present the results of our performance
    analysis. The paragraph is organized into different subsections according to the
    specific workload being considered. Before beginning the different subsections,
    Table 2 shows the power consumption of the five SBCs under test when in an idle
    state, without any virtualized entity running on them. The sleep Unix command
    was used to set the devices to idle, and the experiment lasted 300 seconds. OXU4
    consumes the greatest amount of power—more than double the idle energy consumption
    of the Raspberry Pi. TABLE 2 Power Consumption in Idle A. CPU Performance We tested
    CPU performance using sysbench.7 This stress test is designed to challenge the
    CPU by calculating prime numbers. The computation is made by dividing each candidate
    number with sequentially increasing numbers and verifying whether the remainder
    (modulo calculation) is zero. Fig. 5 shows the execution time (measured in seconds)
    with up to sixteen concurrent running instances–both native and virtualized. FIGURE
    5. Sysbench CPU stress test. Show All From Fig. 5, three main insights can be
    disclosed: i) the container engine introduces a negligible impact on CPU performance,
    with an approximately 2% percentage difference in the worst case; ii) OC2 significantly
    outperforms the rest of the tested SBCs; iii) for all devices, performance degradation
    can be observed when the number of concurrent instances exceeds four; however,
    the observed performance degradation when the number of instances to be managed
    exceeds four units is strictly related to the CPU architecture of the tested devices.
    For all the devices featuring a 4-core CPU, for four instances the CPU shares
    its resources among the different instances in a fair and effective manner. As
    the number of instances increases, the CPU must schedule and share its resources
    differently between the running instances, as four concurrent instances already
    saturate the maximum CPU capacity. Consequently, increasing the number of instances
    produces a gradual performance degradation. As Fig. 5 shows, the execution time
    doubles when the number of instances rises from four to eight and doubles again
    from eight to sixteen instances, revealing a linear increase. However, a different
    trend can be observed for the OXU4 SBC, which embeds an 8-core CPU. First, it
    must be clarified that if the OXU4 had an embedded CPU with 8-cores characterized
    by the same CPU clock, we would not have observed a performance degradation for
    fewer than eight instances because the CPU would have had enough resources to
    fairly manage all the virtualized instances. However, as discussed earlier, the
    OXU4 features a heterogeneous 8-core CPU in which the device begins using the
    lower-speed cores only when the faster cores are fully saturated. This CPU’s architectural
    peculiarity produces the effect of a higher execution time as soon as the group
    of lower-speed cores begins working. In any case, in contrast to the 4-core CPU
    devices, the execution time grows linearly only when OXU4 is managing more than
    eight containers, corresponding to the point at which the OXU4’s CPU resources
    are fully saturated. From a power consumption perspective (Fig. 6), the consumption
    of the Raspberry Pi boards, OC1+, and OC2 vary, ranging from 1.88W (RPi2) to 3.26W
    (OC2). This variation range highlights the high energy efficiency of these devices.
    FIGURE 6. Power consumption of the SBCs under evaluation while performing the
    sysbench test. Show All However, there is clearly a performance/power consumption
    tradeoff that varies from device to device. This analysis will be discussed at
    a later stage. OXU4 consumes more power consumption compared to the other tested
    devices. Specifically, its power consumption increases linearly from 7W to 14W
    as the number of containers varies from one to four. This significant power consumption
    variation is due to its use of the higher-speed core group. In fact, it can be
    observed that the increase diminishes as soon as the lower-speed cores start handling
    newly allocated instances: this change occurs when the fifth container is allocated.
    Another interesting aspect is that power consumption becomes constant as soon
    as the number of concurrent instances exceeds four for all the devices (with the
    exception of OXU4, which exhibits such behavior at more than eight instances).
    As discussed above, this trend depends strictly on the CPU architecture. When
    the number of running containers is greater than or equal to four (eight for OXU4)
    the CPU works at its maximum capacity and its resources are already saturated.
    Consequently, allocating additional instances does not increase power consumption;
    instead, such increases occur at the expense of performance as shown in Figure
    5. Linpack8 tests system performance using a simple linear algebra problem. Specifically,
    this algorithm uses a random matrix A (of size N), and a right-hand side vector
    B defined as follows: A ∗ X = B. Linpack provides the output result in MegaFLOPS
    (Millions of Floating Point Operations Per Second): mflops=ops/(cp u ∗ 1000000)
    View Source where ops denotes the number of operations per second performed, and
    cpu denotes the number of CPU cycles. In our evaluation, N was set to 1000. Fig.
    7 depicts the outcomes from the Linpack test. Similar to the previous case, the
    Docker virtualization layer introduces no relevant overhead. FIGURE 7. Linpack
    test results. Line chart represents the power consumption. Show All Analysis of
    the results shows that the MFLOPS oscillate around the same values regardless
    of the number of running instances. A partial exception can be seen for OXU4,
    which shows this trend only when the number of running containers is larger than
    eight. Furthermore, in contrast with the sysbench test results, no performance
    degradation can be observed as the number of concurrent instances increases. This
    is because a rating based on MFLOPS is strongly dependent on and limited to the
    program being executed—Linpack in this case. This specific test scenario shows
    that even with a high number of concurrent instances, all the devices can execute
    the Linpack test at the same efficiency because they produce, on average, the
    same number of MFLOPS. The reason for the initial OXU4 performance deterioration
    is again attributable to its heterogeneous CPU architecture. Indeed, on OXU4,
    the execution of the benchmark test is initially allocated to the faster cores.
    When the resources of the faster cores are saturated, the lower-speed cores are
    activated to handle additional instances but with a reduced MFLOP capacity. This
    degrades the average performance. However, when the number of running containers
    is greater than eight, the MFLOPS oscillate around a constant value, as observed
    for the rest of devices. It can also be observed that OXU4 outperforms all the
    other SBCs. With regard to power consumption, similar to the sysbench test, power
    consumption remains constant once four or more containers are in execution. However,
    the tested devices differ in how the power consumption increases from one to four
    instances. In particular, the devices featuring an ARMv7 CPU (RPi2 and OC1+) produce
    an increase of approximately 35%, while OC2 and RPi3, which both feature ARMv8
    CPUs, generate an increase of roughly 50%. The CPU clock rate also influences
    SBC power consumption. In fact, among the ARMv8 devices, OC2 consumes an average
    of 1W more than RPi3; while among the ARMv7 devices, OC1+ results in a higher
    power consumption on the order of 0.9W. Therefore, these results reveal ways in
    which CPU architecture, CPU clock rate, and power consumption are related. B.
    Memory Performance To test RAM memory performance, we used the Unix command mbw,9
    which determines the available memory bandwidth by copying large arrays of data
    into memory. We also performed three other tests (memcpy, dumb, and mcblock).
    Native and container performance can be considered comparable for each tested
    device (Fig. 8) with the exception of OXU4, which introduced an overhead of around
    16% during the memcpy and mcblock tests. Comparing only the boards with 1 GB RAM,
    OC1+ always outperformed RPi2. This probably occurred due to the different RAM
    I/O Bus Clock frequencies and data transfer rates of the two devices (RPi2 uses
    LPDDR2 RAM, while OC1+ uses LPDDR3 RAM). RPi3 performed better than OC1+ on the
    memcpy and mcblock tests, but not on the dumb test. When evaluating this latter
    result, it must be considered that OC1+ has a faster data transfer rate (DDR)
    than RPi3, which uses RAM memory with a higher Bus Clock frequency. OC2 and OXU4
    clearly produce higher Average Speed results compared to the other boards. These
    results can easily be explained by the larger RAM capacity of both boards. Nevertheless,
    OXU4 outperforms OC2 during the memcpy and mcblock tests despite having the same
    RAM hardware features. This result may be explained by the fact that these two
    operations require data to move over the system bus. The CPU may regulate data
    migration over the system bus. Consequently, the greater CPU computational resources
    of OXU4 compared to OC2 affect this result. Another interesting aspect of the
    RAM performance analysis comes from comparing OC1+ and OC2. Although a significant
    performance difference exists between the two boards (OC2 achieved roughly double
    the average speed of OC1+) the power consumption of both SBCs was approximately
    3W. FIGURE 8. Memory RAM performance comparison. The red markers represent power
    consumption. Show All C. Disk I/O Performance We used fio10 10 2.0.8 to run sequential
    read/write instances for a 6 GB file stored on the MicroSD card. Sysbench was
    used to test random read/write disk operations with the embedded MultiMediaCard
    (eMMC). Fig. 9 shows the sequential read and write performance averaged over 60
    seconds, using a typical 1 MB I/O block size. Docker introduce negligible overhead
    in both tests for all the tested SBCs. with the exception of the Raspberry Pi
    boards, where the only significant overhead was introduced during the sequential
    write test, amounting to nearly 50% for RPi2 and approximately 37% for RPi3. We
    used the disk performance analysis tool iostat11 to investigate the reason behind
    these Raspberry Pi results. The iostat tool can be used to monitor and report
    CPU statistics and system I/O device loading. From an analysis of the iostat logs,
    we noticed that the overhead may be caused by a high percentage of iowait periods.
    According to its definition, iowait indicates the percentage of time that the
    CPU is idle while the system services an outstanding disk I/O request. As explained
    in [45], a high iowait value indicates that the system has an application problem,
    an inefficient I/O subsystem configuration, or a memory shortage. The latter is
    possibly the reason the aforementioned overhead occurs. This disk stress test
    was performed with a high and intensive workload; therefore, devices with lower
    resources can experience issues in optimally scheduling disk-writing operations.
    When managing smaller files, it is reasonable to expect that such overhead would
    decrease, as was shown in [6] and [21]. FIGURE 9. Disk I/O performance for sequential
    read/write tasks. Show All As discussed earlier, Disk I/O evaluation was performed
    using a MicroSD card as a storage device. However, unlike the Raspberry Pi family,
    all the Odroid boards provide integrated support for eMMC cards. This alternative
    storage solution offers superior performance in terms of read/write speed. We
    executed a random read/write benchmark test to explore the higher capabilities
    of eMMC storage solutions. The results are shown in Fig. 10, which shows that
    the eMMC cards can reach disk speeds on the order of hundreds of MBs per second—while
    for the MicroSD cards, memory speed oscillates around a range of hundreds of Mbs
    per second. FIGURE 10. Odroid boards eMMC performance test. Show All However,
    we note that disk performance is highly dependent on the type of MicroSD card
    used. As demonstrated in [46], above-average disk performance can be achieved
    based on the type of MicroSD used. D. Network Performance The network configuration
    used for our test is shown in Fig. 11. The Virtual NIC for all the running containers
    shares the same network bridge, which in turn is mapped to the physical Ethernet
    card. Each hardware platform performs network operations (e.g., packet forwarding,
    packet buffering, scheduling, etc.) dependent upon the design and implementation
    of their different software components (operating system, drivers, etc.), that
    could have different performance impacts. FIGURE 11. Network configuration setup.
    Show All We used the tool iperf312 for the network performance analysis. Iperf
    measures network performance between hosts, generating bidirectional data transfers
    of both TCP and UDP traffic. Taking into account that the NIC uses different code
    paths when sending and receiving TCP traffic, we performed bidirectional tests
    to quantify the overhead produced by the virtualization layer when the SBC is
    both receiving and sending network traffic. On the tested SBCs, both iperf server
    and iperf client inside one or multiple Docker containers were executed. Docker
    uses NAT as its default network configuration. With the alternative configuration
    –net=host, Docker uses the host interface directly, avoiding NAT. This alternative
    configuration improves performance at the expense of security. In our previous
    work [6], we tested the –net=host option for each experiment and found that this
    setup eliminated any overhead, allowing the systems to achieve nearly native performance.
    However, in this study we report the results only with the NAT setup because that
    configuration is most commonly used in real-world environments. For the TCP traffic
    analysis, to improve the readability of the graphs, we discuss the performance
    of Raspberry Pi and Odroid separately because they are characterized by a different
    NIC speed. In the case of a TCP server (Fig. 12), the container engine does not
    impact the throughput performance of RPi2 and RPi3 when executing a single network
    instance. However, the overhead increases for the RPi2 according to the number
    of concurrent running instances (for eight simultaneous TCP flows, the overhead
    is approximately 30%). FIGURE 12. Raspberry Pi 2 and Raspberry Pi 3 network performance
    while receiving TCP traffic. The line charts represent power consumption. Show
    All A similar trend can be observed in Fig. 13, which depicts the case when RPi2/RPi3
    are acting as clients. A throughput degradation of approximately 26% can be observed
    when sending even four TCP flows. FIGURE 13. Raspberry Pi 2 and Raspberry Pi 3
    network performance while sending TCP traffic. The line charts represent power
    consumption. Show All The main findings of the TCP traffic analysis for the Odroid
    boards (Fig. 14) are summarized below. FIGURE 14. Odroid boards TCP traffic results.
    TCP Server: (a) OC1+, (b) OC2, (c) OXU4, TCP Client: (d) OC1+, (e) OC2, (f) OXU4.
    The line charts represent power consumption. Show All 1) OC1+ For native execution,
    a substantial throughput difference exists between the server (Fig. 14a) and client
    (Fig. 14d) cases: the client throughput is approximately half of the server throughput.
    This result occurs because of the different send and receive code paths that the
    OS uses for TCP traffic [29]. In terms of virtualization overhead, for the TCP
    client case the containers do not introduce any relevant impact; however, when
    OC1+ is receiving TCP traffic, Docker introduces an overhead of approximately
    50% compared to native execution—for a single virtual instance. However, the performance
    of OC1+ improves as the number of instances increases. Such behavior is attributable
    to the fact that a single TCP flow is unable to saturate a 1 Gb/s link, while
    a combination of multiple streams overcomes this limitation [30]. 2) OC2 The network
    performance of OC2 can be considered the desired outcome: both native and Docker
    performances are essentially the same (Fig. 14b–e). The only outlier can be identified
    in Fig. 14e, which depicts the overhead introduced by Docker when eight containers
    are simultaneously sending TCP traffic. The measured overhead is on the order
    of 10%. Such performance degradation is attributable to the CPU overload generated
    by managing eight concurrent virtualized streams. Fig. 14e shows a substantial
    increase in CPU context switching and cycles consumed compared to the native case.
    3) OXU4 On average, the TCP server test shows that Docker negligibly impacts performance
    compared with native execution (Fig. 14c). Neverthless, the overall throughput
    decreases (by up to 23%) as the number of simultaneous connections increases.
    In contrast with OC1+, the simultaneous presence of parallel streams does not
    saturate the 1 Gb/s link; instead, it generates a performance degradation due
    to the CPU overload produced by the multi-stream flow—which is similar to the
    OC2 client case. However, different from the OC2 case, we can observe such results
    for both the native and virtualized cases. In the TCP client test, the virtualization
    layer produces a significant overhead ranging from 27% to 70% (Fig. 14f). Unlike
    the server case, in this test, the performance degradation affects only the virtualized
    instances. For UDP traffic, we want to quantify the power consumption of the different
    SBCs when handling the same amount of sent/received traffic—90 Mb/s in our example.
    As Fig. 15 shows, the power consumption differs slightly from the idle power consumption
    reported in Table 2, which implies that the NIC generates extremely low overhead
    when handling UDP traffic. Furthermore, no tangible differences can be observed
    between the native and Docker cases. FIGURE 15. Power consumption of the different
    SBC, when 90 Mb/s of UDP traffic is sent/received. Show All The TCP network performance
    analysis showed the existence of a non-negligible overhead introduced by Docker
    for a subset of cases in some of the tested boards (RPi2, OC1+, OXU4, and sin
    some cases, for OC2). Furthermore, we observed how the performance differs between
    servers and clients in such cases. To understand the reasons behind these results,
    we used the Linux hardware performance analysis tool Perf13 to collect system-level
    statistics. This tool reveals how hard the CPU works to deliver network traffic.
    As introduced in the result discussion, by analyzing the Perf logs, we discovered
    that the higher overhead generated in such SBCs is generated by an increasing
    number of CPU context switches and consumed cycles. This occurs because—in contrast
    to the native case—network packets must be processed by extra layers in a containerized
    environment. Another important insight of this analysis is related to the better
    network performance of both 64-bit CPU boards (RPi3 and OC2) compared to the 32-bit
    CPU boards (RPi2, OC1+, and OXU4). These results reflect the fact that Docker
    officially supports only 64-bit CPU systems; consequently, it appears to be better
    optimized for devices featuring 64-bit ARM architectures than those with 32-bit
    architectures. Finally, it must be pointed out that the inability to use the full
    capacity of the NIC interface in the Odroid boards has also been acknowledged
    by the manufacturer through similar benchmark tests [31], [32]. From the power
    consumption perspective, we can draw some general conclusions from the above results
    and discussion that apply to all the tested devices. The devices’ power consumption
    follows the trend of the native network throughput in every single case. An increase/decrease
    in network throughput produces a consequent power consumption increase/decrease;
    although the network throughput variation is not as noticeable, this behavior
    can be observed in Figs. 14a and 14f. Although this result is expected, it is
    interesting to observe how the devices’ power consumption for the Docker case
    follows the same trend as the native case. This occurs even when relevant overhead
    introduced by the virtualization layer exists—again, this trend is particularly
    noticeable in Figs. 14a and 14f. This outcome represents a bottleneck: the devices
    cannot be as efficient as in the native case, but at the same time they use identical
    rates of power consumption. Therefore, this result does not reflect a favorable
    tradeoff between performance and power consumption. As explained previously, the
    causes behind such bottlenecks are that the OS and/or a lack of software optimization
    require too many CPU clock cycles and overtax your system unexpectedly, which
    also impacts the power consumption. E. Mixed Load Performance The performance
    evaluation presented in the previous subsections was conducted using benchmark
    software tools that stress a specific hardware segment of the device. This represents
    a reasonable approach because it allows us to define an upper bound for the performance
    of each portion of the system hardware. However, real-world applications challenge
    the hardware in a more distributed manner. Therefore, we performed further tests
    to evaluate the impact introduced by container technologies when the SBCs are
    handling heterogeneous workloads. This evaluation was conducted using the stress14
    benchmark tool, which is a workload generator that allocates a configurable amount
    of load in the system in terms of CPU, memory, generic I/O, and disk operations.
    We defined three different workloads characterized by an increasing computational
    cost: (i) Low Load, (ii) Average Load, and (iii) High Load. Table 3 describes
    workload characteristics in more detail. TABLE 3 Mixed Workload Characterization
    For this set of tests, the metric that we want to monitor is system load, which
    indicates the overall amount of computational work that a system performs and
    includes all the processes or threads waiting on I/O, networking, database, etc
    [22]. The average load represents the average system load over a period of time.
    In the evaluation, the time interval was set to 300 seconds. The aim of this evaluation
    was twofold. First, it allowed us further assess the impact of virtualization
    when executing applications characterized by heterogeneous characteristics. Furthermore,
    it allowed us to quantify the difference between the average system load of the
    different SBCs when assigned the same workload. The aforementioned metric can
    be collected by means of Unix tools such as dstat.15 Fig. 16 shows the 1-min average
    system load for the Odroid boards and RPi3. Various insights can be drawn from
    these results. First, for all the different workloads, native and Docker executions
    behave comparably. These results represent an important outcome: they confirm
    the lightweight characteristics of container technologies even when the SBC is
    handling mixed workloads. This result was confirmed when the system was assigned
    a High workload, which was defined to heavily challenge the system. A comparison
    of the different devices reveals that RPi3 produces the highest system load when
    compared to the Odroid boards. FIGURE 16. Average system load (1-min) for three
    different heterogenous workloads. Show All The difference increases with the workload
    complexity. Compared to the Odroid boards, RPi3 introduces a higher average system
    load. This increase is approximately 40% for the low and average workloads, and
    approximately 45% for a high workload. Fig. 17 shows the power consumption increase
    for the low and high workloads. The highest rise is produced by OXU4, on the order
    of 18%. For OC1+ and OC2, the increase is approximately 14%, although the base
    power consumption is higher in OC2 (which consumes roughly 1 W more than OC1+).
    In addition, the RPi3 generates a power consumption increase on the order of 17%
    when handling extremely different workloads. FIGURE 17. SBC power consumption
    when executing the Low and High mixed workloads. Show All F. Energy Efficiency
    Evaluation In the preceding subsections, we used the term “power consumption”
    to indicate the device’s average power consumption while handling a specific workload.
    In this section, we want to evaluate the energy efficiency of the tested hardware
    to assess which SBC is the most energy efficient. In this context, energy consumption
    can be defined as the power consumption of a device over time: energy=∫power dt[Joule]
    (1) View Source To determine the energy of an SBC, we must consider the amount
    of computational work the SBC performs when executing a particular task. The computational
    work varies according to the hardware segment analyzed and the benchmark tool
    used to characterize its performance. Table 4 summarizes all the performance metrics
    used in our empirical investigation. TABLE 4 Benchmark Metrics Summary Similar
    to [17], regardless of the considered metric and the particular test, we can conventionally
    and indistinctly define transactions per time unit as the number of transactions
    per second (tps): n tps= p m transaction second (2) View Source Also, as stated
    in [17], “Because of the transactions’ dependency on the specific application
    scenario, only results from the same benchmark are comparable. Such performance
    figures must always be qualified by the respective benchmark.” In our context,
    the energy efficiency expresses how efficiently an SBC completes a specific task
    using a certain amount of energy. Energy efficiency can be defined as follows:
    energy efficiency= number of transactions energy consumption (3) View Source It
    can also be defined as the amount of work done per time unit given a certain amount
    of power: energy efficiency= tps Watts (4) View Source Based on the above definition,
    the higher the energy efficiency is, the better an SBC transforms electricity
    into effective computation. Fig. 18 depicts the energy efficiency of the different
    SBCs. The graphs show the percentage difference with respect to the SBC that performs
    most efficiently on each test, from an energy perspective. FIGURE 18. Energy Efficiency
    Results: (a) CPU, (b) Memory, (c) Disk I/O. Network I/O: (d) TCP Client, (e) TCP
    Server, (f) UDP. Show All To improve the readability of the graph, we consider
    only native performance because we have already empirically demonstrated that
    containers introduce no significant overhead for most tests. The following insights
    were revealed from this analysis. CPU. SBC efficiency changes as the number of
    concurrent instances increases. The behavior of the OXU4 board is particularly
    interesting; it the most efficient SBC when a single instance is running but the
    least efficient when processing eight simultaneous instances (Fig. 18a). Memory.
    The results of the memory stress (Fig. 18b) show that OC2+ is the most energy
    efficient board regardless of the benchmark tool used (stress or mbw); the other
    SBCs behave differently depending on the software used to perform the test. Disk
    I/O. Similar to the memory case, the energy efficiency also varies in the disk
    I/O analysis based on the type of operation performed by the device (Fig. 18c).
    OC2 is the most efficient for sequential read operations, while RPi2 is the most
    efficient for sequential write operations. Network. The results of the TCP network
    performance analysis were similar for most of the SBCs (RPi2, RPi3, and OXU4).
    Particularly interesting is that, in contrast to the TCP client test (Fig. 18d),
    OC1+ outperforms all the other boards in the TCP server evaluation (Fig. 18e).
    Finally, the UDP evaluation shows that the Raspberry boards are more efficient
    when compared to the Odroid boards (Fig. 18f). This result is expected based on
    the analysis discussed in the previous subsection. Fig. 19 shows the energy efficiency
    results of the mixed workload performance analysis. OC1+ performs best because
    it considers the tradeoff between managing heterogeneous workloads and power consumption.
    However, the other boards also guarantee a medium-high level of efficiency. FIGURE
    19. SBC energy efficiency when executing the Mixed Load Test (Low and High workload
    configurations). Show All G. Container Activation Time Analysis We also evaluated
    the variation in a container’s activation time when the SBC is managing a workload
    that gradually becomes more complex. Activation time represents an issue for heavier
    types of system virtualization (i.e., Virtual Machines) on a server because booting
    a VM can require minutes depending on the server load. Container activation time
    was evaluated for server machines in [23]; however, a full characterization of
    container activation time on low-power nodes is lacking. For this evaluation,
    we used stress to allocate increasing loads to the different systems. We tested
    the activation time in four different cases: when the SBCs are in idle state and
    then when two, four, or eight CPU-bound processes are imposed on the system. Fig.
    20 shows the results of this evaluation. The RPi3 is the device that maintains
    the activation time within a very short range—on average between 1300 ms and 1400
    ms—even when handling heavier workloads. The Odroid boards showed no relevant
    difference between idle and the stress -c 2 case. FIGURE 20. Container activation
    time for OC1+, OC2, OXU4, and RPi3 under different workloads. Show All However,
    heavier workloads impact the activation time of the different Odroids differently.
    The percentage increase between the idle and the stress -c 8 case is 20% for the
    OC1+, 61.57% for the OC2, and 181% for the OXU4. The OXU4 SBC substantially increases
    the activation time as the complexity of its workload increases. This result is
    consistent with the energy efficiency analysis, which showed the lower efficiency
    of OXU4 when managing heavy workloads and/or several concurrent instances. However,
    considering all the available results, container activation time remains below
    2000 ms in most cases. This represents a significant result if we consider the
    reduced hardware capabilities of SBCs. Moreover that activation time can be further
    reduced through alternative container-engine setups [23]. H. Board Temperature
    Measurements The temperatures reached by each SBC during the execution of the
    different benchmark tests is another interesting parameter that deserves to be
    analyzed. Temperature can be relevant in scenarios where SBCs are used on a large
    scale such as replacing server machines with SBC clusters, which can provide a
    better energy efficiency/monetary cost tradeoff as demonstrated in [24] and [25].
    Here, we want to estimate the maximum temperature reached by the different devices,
    which can help when designing efficient cooling systems for clusters of SBCs.
    The vcgencmd measure_temp command returns the CPU temperature of the Raspberry
    Pi, while the CPU temperature of the Odroid boards can be accessed using the command
    line /sys/devices/virtual/thermal/temp. It is worth mentioning that, by default,
    RPi2 and RPi3 are passively cooled boards that do not include any heat sink or
    fan, while the Odroid boards require auxiliary components to ensure an effective
    cooling system. The processors on the OC1+ and OC2 boards have a relatively small
    area to dissipate heat. Therefore, both boards use a heat sink to improve heat
    dissipation. The OXU4 uses a software-controlled fan in addition to the heat sink.
    Table 5 shows the temperature of each SBC when in an idle state. TABLE 5 Board
    Temperature at Idle As in the previous subsection, we show only a subset of the
    full results (Fig. 21). The CPU analysis allows us to understand how the temperature
    increases as the number of concurrent virtualized instances (denoted by CPU #1
    and CPU #8) increases. The RPi3 exhibits the highest temperature increase—approximately
    25%. The temperature increase for the Odroid boards varies between 15% for OC1+
    and OC2 to 19% for OXU4, and the RPi2 exhibits the same behavior. In the Memory
    I/O test, we included the results of both mbw and stream tests to determine whether
    any connection exists between temperature increases and the use of different benchmark
    tools. Regardless of the memory benchmark tool used, the boards’ temperatures
    are roughly equivalent. The same logic applies to both the disk I/O and network
    analysis. For the former, we wanted to detect any potential difference between
    read and write disk operations and for the latter, we wanted to detect any potential
    differences when the devices were sending or receiving TCP traffic. FIGURE 21.
    Maximum temperatures reached by the SBCs when executing different computing tasks.
    Show All SECTION VI. Conclusions The main goal of this paper was to conduct an
    extensive performance evaluation to assess the feasibility of running virtualized
    instances on a broad range of low-power nodes such as SBCs. The motivation behind
    this study lies in the increasing employment of such devices in specific Edge-IoT
    scenarios. Our in-depth analysis of the empirical characteristics of SBCs generated
    fundamental insights about the performance of such devices, including the following:
    Employing container-virtualization technologies on SBCs produces an almost negligible
    impact in terms of performance when compared to native executions. This result
    remains valid even when running several virtualized instances simultaneously.
    By considering the tradeoff between performance and power consumption (energy
    efficiency) under a wide set of workloads, we empirically demonstrated the energy
    efficiency of the SBCs. Energy efficiency represents a crucial aspect in scenarios
    in which the devices are battery powered. Indeed, from our study is possible to
    estimate the battery duration of a device based on expected workload characteristics.
    Overall, the Odroid C2 outperforms all the other tested devices in most of the
    performance tests. The Odroid boards can efficiently manage data-intensive applications
    (e.g., Big Data applications) thanks to their support for eMMC cards, which improve
    performance considerably compared to MicroSD. The Odroid C2 and the Odroid XU4
    are the most suitable boards for executing memory-intensive applications, thanks
    to their higher RAM capacities. In general, the network performance analysis showed
    that 64-bit CPU devices do not introduce any tangible overhead compared to 32-bit
    CPU devices. Raspberry Pi boards are highly efficient when handling low volumes
    of network traffic—especially UDP traffic. This result can be useful in creating
    efficient IoT gateway designs that are specifically intended for executing lightweight
    IoT applications, e.g., the Constrained Application Protocol ( CoAP) and Message
    Queuing Telemetry Transport (MQTT) protocols. By considering the limited resources
    of SBCs compared to server machines, we showed that the container activation time
    required by SBCs remains relatively small even when the SBCs are overloaded. The
    maximum temperatures reached by the various tested boards varies depending on
    the applied workload. The choice of one device rather than another may vary based
    on the different requirements of service providers and applications. Therefore,
    the empirical insights achieved by this study can aid in efficiently designing
    integrations of the analyzed devices in different scenarios according to specific
    requirements of different Edge-IoT applications. For the sake of completeness,
    it is worth highlighting that although our study has shown how container-based
    virtualization can represent an efficient and promising way to enhance the features
    of IoT architectures, several aspects still deserve further investigation, especially
    studies that improve our understanding of where this technology can be applied
    most efficiently. As an example, referring to the works mentioned in Section II,
    the advantages that accrue from executing containerized applications on IoT/Edge
    gateways are clear. However, there is still a lack of research to evaluate the
    interactions among multiple gateways while considering that Docker does not currently
    fully provide support to perform live container migrations between different entities.
    Furthermore, in such more complex scenarios, whether the strict requirements of
    many IoT applications can still be preserved should also be investigated. The
    mobility of edge entities in several IoT/Edge use cases is another aspect that
    should be considered. Such scenarios introduce even more strict requirements in
    terms of latency. Farris et al. [33] proposed an approach for provisioning ultra-short
    latency applications in MEC environments by exploiting the potential of container
    technologies. The proposed framework supports proactive service migrations only
    for stateless applications. The authors included a set of challenges that future
    research needs to address. Examples are support for stateful applications, a problem
    closely related to the lack of full support for live container migration, and
    defining specific policies aimed at optimizing container management. Many concerns
    have been expressed about the level of security guaranteed by applications developed
    within containers [34]. One of the main concerns was due to the lack of namespace
    isolation, which made dockerized applications more vulnerable. The latest released
    versions of Docker include several security enhancements to cope with these issues
    [36]. Nonetheless, Docker continuously provides detailed guidelines for developing
    safer Docker ecosystems [37], [38]. A further effort to ensure better security
    in dockerized systems is represented by the collaboration between Docker and the
    Center for Internet Security, which has led to the release of the Docker Security
    Benchmark, a developer’s tool that can check for a wide variety of known security
    issues within virtualized applications [39]. Referring specifically to IoT contexts,
    it is crucial to encourage the development of more specific security mechanisms
    that consider the strict requirements of IoT applications/scenarios but do not
    impair the lightweight features of container-based technologies. From this point
    of view, several Linux-Docker-based frameworks have already been proposed as solutions
    that can enhance IoT security [35]. ACKNOWLEDGMENTS The author would like to thank
    Nicklas Beijar for his helpful feedback, and the reviewers for their insightful
    comments on the paper, as these comments led to an improvement of the work. Authors
    Figures References Citations Keywords Metrics Footnotes More Like This Performance
    Evaluation of Container Management Tasks in OS-Level Virtualization Platforms
    2023 IEEE International Conference on Enabling Technologies: Infrastructure for
    Collaborative Enterprises (WETICE) Published: 2023 Performance Characterization
    of Hypervisor-and Container-Based Virtualization for HPC on SR-IOV Enabled InfiniBand
    Clusters 2016 IEEE International Parallel and Distributed Processing Symposium
    Workshops (IPDPSW) Published: 2016 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE access
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: 'Virtualization on Internet of Things Edge Devices With Container Technologies:
    A Performance Evaluation'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2019.2911732
  analysis: '>'
  authors:
  - Sari Sultan
  - Imtiaz Ahmad
  - Tassos Dimitriou
  citation_count: 136
  full_citation: '>'
  full_text: '>

    Scheduled Maintenance: On Tuesday, 16 April, IEEE Xplore will undergo scheduled
    maintenance from 1:00-5:00 PM ET (1700-2100 UTC). During this time, there may
    be intermittent impact on performance. We apologize for any inconvenience. IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account
    Personal Sign In Personal Sign In * Required *Email Address *Password Forgot Password?
    Sign In Don''t have a Personal Account? Create an IEEE Account now. Create Account
    Learn more about personalization features. IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/8600701/08693491.pdf
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Container Security: Issues, Challenges, and the Road Ahead'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.comcom.2018.03.011
  analysis: '>'
  authors:
  - Antony Martin
  - Simone Raponi
  - Théo Combe
  - Roberto Di Pietro
  citation_count: 114
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Technology background 3. Related
    work 4. Docker 5. Docker containers security architecture 6. Typical Docker use-cases
    7. Docker vulnerability-oriented analysis 8. Discussion 9. Conclusion Acknowledgments
    References Show full outline Cited by (120) Figures (7) Show 1 more figure Tables
    (4) Table 1 Table 2 Table 3 Table 4 Computer Communications Volume 122, June 2018,
    Pages 30-43 Docker ecosystem – Vulnerability Analysis Author links open overlay
    panel A. Martin a, S. Raponi b, T. Combe a, R. Di Pietro b Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.comcom.2018.03.011 Get rights and content
    Abstract Cloud based infrastructures have typically leveraged virtualization.
    However, the need for always shorter development cycles, continuous delivery and
    cost savings in infrastructures, led to the rise of containers. Indeed, containers
    provide faster deployment than virtual machines and near-native performance. In
    this paper, we study the security implications of the use of containers in typical
    use-cases, through a vulnerability-oriented analysis of the Docker ecosystem.
    Indeed, among all container solutions, Docker is currently leading the market.
    More than a container solution, it is a complete packaging and software delivery
    tool. In this paper we provide several contributions: we first provide a thorough
    survey on related work in the area, organizing them in security-driven categories,
    and later we perform an analysis of the containers security ecosystem. In particular,
    using a top-down approach, we identify in the different components of the Docker
    environment several vulnerabilities—present by design or introduced by some original
    use-cases. Moreover, we detail real world scenarios where these vulnerabilities
    could be exploited, propose possible fixes, and, finally discuss the adoption
    of Docker by PaaS providers. Previous article in issue Next article in issue Keywords
    SecurityContainersDockerVirtual MachinesDevOpsOrchestration 1. Introduction Virtualization-rooted
    cloud computing is a mature market. There are both commercial and Open Source
    driven solutions. For the former ones, one may mention Amazon’s Elastic Compute
    Cloud (EC2) [1], Google Compute Engine [2], [3], VMware’s vCloud Air, Microsoft’s
    Azure, while for the latter ones examples include OpenStack combined with virtualization
    technologies such as KVM or Xen. Recent developments have set the focus on two
    main directions. First, the acceleration of the development cycle (agile methods
    and devops) and the increase in complexity of the application stack (mostly web
    services and their frameworks) trigger the need for a fast, easy-to-use way of
    pushing code into production. Further, market pressure leads to the densification
    of applications on servers. This means running more applications per physical
    machine, which can only be achieved by reducing the infrastructure overhead. In
    this context, new lightweight approaches such as containers or unikernels [4]
    become increasingly popular, being more flexible and more resource-efficient.
    Containers achieve their goal of efficiency by reducing the software overhead
    imposed by virtual machines (VM) [5], [6], [7], thanks to a tighter integration
    of guest applications into the host operating system (OS). However, this tighter
    integration also increases the attack surface, raising security concerns. The
    existing work on container security [8], [9], [10], [11] focuses mainly on the
    relationship between the host and the container. This focus is completely justified
    by the fact that, while virtualization exposes well-defined resources to the guest
    system (virtual hardware resources), containers expose (with restrictions) the
    host’s resources (e.g., IPC / file-system) to the applications. However, the latter
    feature represents a threat to both confidentiality and availability of applications
    running on the same host. Containers are now part of a complex ecosystem - from
    container to various repositories and orchestrators - with a high level of automation.
    In particular, container solutions embed automated deployment chains [12] meant
    to speed up code deployment processes. These deployment chains are often composed
    of third parties elements, running on different platforms from different providers,
    raising concerns about code integrity. This can introduce multiple vulnerabilities
    that an adversary can exploit to penetrate the system. To the best of our knowledge,
    while deployment chains are fundamental for the adoption of containers, the security
    of their ecosystem has not been fully investigated yet. The vulnerabilities we
    consider are classified, relatively to a hosting production system, from the most
    remote ones to the most local ones, using Docker as a case study. We actually
    focus on Docker’s ecosystem for three reasons. First, Docker successfully became
    the reference on the market of container and associated DevOps ecosystem. Indeed,
    with more than 29 billions of Docker container downloads, 900.000 Dockerized apps
    in the Docker Hub and 3.000 community contributors [13], Docker is the world’s
    leading software container platform. Second, data protection is still one of the
    biggest barriers for container adoption in the enterprise for production workloads
    [14]. Finally, Docker is already running in some environments which enable experiments
    and exploring the practicality of some attacks. Contributions In this paper, we
    provide several contributions. First, we provide a thorough survey of related
    work, classifying them into security-driven categories. Later, we make a detailed
    list of security issues related to the Docker ecosystem, and run some experiments
    on both local (host-related) and remote (deployment-related) aspects of this ecosystem.
    Further on, we show that the design of this ecosystem triggers behaviours (captured
    in three use-cases) that lower security when compared to the adoption of a VM
    based solution, such as automated deployment of untrusted code. This is the consequence
    of both the close integration of containers into the host system and of the incentive
    to scatter the deployment pipeline at multiple cloud providers. Finally, we argument
    on the fact that these use-cases trigger and increase specific vulnerabilities,
    which is a factor rarely taken into account in vulnerability assessment. Roadmap
    This paper is organized as follows: in Section 2, we provide the background information
    for virtualization and its alternatives. In Section 3 we survey the related work
    in the area, organizing them in security-driven categories. We then focus on Docker’s
    architecture in Section 4, including its ecosystem. In Section 5 we outline Docker’s
    security architecture. In Section 6, we present Docker’s use cases from several
    points of view and show how they differ from VM and other containers (Linux-VServer,
    OpenVZ, etc.) use cases. From these typical use cases we build, in Section 7,
    a vulnerability-oriented risk analysis, classifying vulnerabilities into five
    categories. Finally, in Section 8, we discuss the implications of these vulnerabilities
    in a cloud-based infrastructure and the issues they trigger. Conclusions are drawn
    in Section 9. 2. Technology background Cloud applications have typically leveraged
    virtualization, which can also be used as a security component [15] (e.g., to
    provide monitoring of VMs, allowing easier management of the security of complex
    cluster, server farms, and cloud computing infrastructure). Given some fundamental
    constraints such as performance overhead, flexibility, and scalability, alternatives
    to virtualization have emerged: unikernels as well as containers. All these approaches
    are summarized in Fig. 1, and detailed below. Download : Download high-res image
    (355KB) Download : Download full-size image Fig. 1. Application runtime models.
    2.1. Virtual machines (VM) Virtual machines are the most common way of performing
    cloud computing: they are fully functional OS, running on top of an emulated hardware
    layer, provided by the underlying hypervisor. The hypervisor can either be running
    directly on hardware (Fig. 1b) (Xen) or on a host OS (Fig. 1c) (for instance KVM).
    VM can be cloned, installed within minutes, and booted within seconds, hence allowing
    to stack them with centralized tools. However, the presence of two operating systems
    (host and guest) along with an additional virtual hardware layer introduces significant
    overhead in performance. Hardware support for virtualization dramatically reduces
    the cited overhead, but performance is far from bare metal, especially for I/O
    operations [5], [6], [7]. 2.2. Containers Containers (Fig. 1d) provide near bare
    metal performance as opposed to virtualization (fig. 1a and (1 b) [5] [6] [7]
    with the further possibility to run seamlessly multiple versions of applications
    on the same machine. For instance, new instances of containers can be created
    quasi-instantly to face a customer demand peak. Containers have existed for a
    long time under various forms, which differ by the level of isolation they provide.
    For example, BSD jails [16] and chroot can be considered as an early form of container
    technology. As for recent Linux-based container solutions, they rely on a kernel
    support, a userspace library to provide an interface to syscalls, and front-end
    applications. There are two main kernel implementations: LXC-based implementation,
    using cgroups and namespaces, and the OpenVZ patch. The most popular implementations
    and their dependencies are shown in Table 1. Table 1. Container solutions. Base
    Container Library Kernel dependence Other dependencies LXC Docker libcontainer
    cgroups + namespaces + capabilities + kernel version 3.10 or above iptables, perl,
    Apparmor, sqlite, GO LXC liblxc cgroups + namespaces + capabilities GO LXD liblxc
    cgroups + namespaces + capabilities LXC, GO Rocket AppContainer cgroups + namespaces
    + capabilities + kernel version 3.8 or above cpio, GO, squashfs, gpg Warden custom
    tools cgroups + namespaces debootstrap, rake OpenVZ OpenVZ libCT patched kernel
    specific components: CRIU, ploop, VCMMD Containers may be integrated in a multi-tenant
    environment, thus leveraging resource-sharing to increase average hardware use.
    This goal is achieved by sharing the kernel with the host machine. Indeed, in
    opposition to VM, containers do not embed their own kernel but run directly on
    the host kernel. This shortens the syscalls execution path by removing the guest
    kernel and the virtual hardware layer. Additionally, containers can share software
    resources (e.g., libraries) with the host, avoiding code duplication. The absence
    of kernel and some system libraries (provided by the host) makes containers very
    lightweight (image sizes can shrink to a few megabytes). It makes the boot process
    very fast (about one second [17]). This short startup time is convenient to spawn
    containers on-demand or to quickly move a service, for instance when implementing
    Network Function Virtualization (NFV). The deployment of such containers —agnostic
    of each other even though running on the same shared kernel— requires isolation.
    In 4 Docker, 5 Docker containers security architecture, 6 Typical Docker use-cases,
    7 Docker vulnerability-oriented analysis, we will discuss the cgroups and namespaces
    based containers, and especially Docker’s containers. Indeed, Docker popularity,
    coupled with the extended privileges on the machines it is run, make it a target
    with a high payoff for any adversary, and that is why we concentrate on the vulnerabilities
    it is subject to, and possible countermeasures. 2.3. Unikernels Unikernels consist
    of very lightweight operating systems (OS), specifically designed to run in VM
    (Fig. 1e). Unikernels were originally designed to run on the Xen hypervisor [18].
    They are more performant than classical VM by shortening the syscalls execution
    path: they do not embed drivers and the hypervisor does not emulate a hardware
    layer. The interaction is achieved through a specific API, enabling optimizations
    that were impossible with the legacy model where an unmodified kernel was running
    on top of emulated hardware [19]. However, these modifications make unikernels
    dependent on the hypervisor they run on. Indeed, currently most unikernels development
    are bound to the Xen hypervisor. Furthermore, unikernels are usually designed
    to run programs written in a specific programming language. Thus, they only embed
    the libraries needed by this specific language and are optimized for this specific
    language (e.g., HaLVM for Haskell, OSv for Java). It decreases the induced overhead
    relatively to a VM. A detailed performance comparison of OSv against Docker and
    KVM is currently available [6]. A study on this topic [19] shows that unikernels
    achieve better performance than VM, and addresses some of the security concerns
    containers suffer from. This is possible since applications running in unikernels
    do not share the host OS kernel. The study concludes that unikernels implementations
    are not mature enough for widespread deployment in production. However, latest
    developments consider unikernels as a serious concurrent to containers in the
    longer term [4] [20], both for security reasons and for their significantly short
    startup time. 2.4. Comparison Each of these alternatives provides a different
    trade-off between multiple factors, including performance, isolation, boot time,
    storage, OS adherence, density and maturity. Most of these factors are performance-related,
    but security and maturity of the solutions are also at stake. According to the
    relative importance of these factors, and driven by the intended use, one specific
    solution may be preferred. On the one hand, traditional VM are quite resource-consuming
    and slow to boot and deploy. However, they provide a strong isolation that has
    been experienced in production for many years. On the other hand, containers are
    lightweight, very fast to deploy and boot, impose low performance overhead —at
    the price of less isolation— and provide a new environment with almost no feedback
    from production uses. Unikernels try to achieve a compromise between VMs and containers,
    providing a fast and lightweight —but still experimental— execution environment
    running in an hypervisor. 3. Related work In this section we provide a thorough
    survey on related work in the area. We first describe the studies that provide
    a comparison between containers and virtualization techniques. Then we organize
    the related work into security-driven groups according to the type of security
    contribution provided (i.e., security aspects of containers, defense against specific
    attacks, vulnerability analysis, and use-case driven vulnerability analysis).
    3.1. Container and virtualization comparison A comparison between containers and
    virtualization is provided in [21]. This article is a general-purpose survey of
    containers, showing containers concepts, pros and cons, and detailing features
    of some container solutions. It concludes on the prediction (the paper dates back
    to 2014) that containers will be a central technology for future PaaS solutions.
    However, the comparison is essentially made on a performance basis, and the only
    security concern mentioned is the need for a better isolation between containers.
    A thorough performance evaluation of virtualization and containerization technologies
    —Docker, LXC, KVM, OSv (unikernel)— is provided in [6]. The authors use multiple
    benchmark tests (Y-Cruncher, NBENCH, Noploop, Linpack, Bonnie++, STREAM, Netperf)
    to assess CPU, memory and network throughput and disk I/O in different conditions.
    They show that containers are significantly better than KVM in network and disk
    I/O, with performance almost equal to native applications. They conclude by mentioning
    security as the trade-off of performance for containers. A similar performance
    evaluation is made in [22]. 3.2. Security aspects of containers The security aspect
    of containers is discussed in more detail in [8]. This article details Docker’s
    interaction with the underlying system, e.g., on the one hand internal security
    relying on namespaces and cgroups, intended and achieved isolation, and per-namespace
    isolation features. On the other hand, it details operating system-level security,
    including host hardening (with a short presentation of Apparmor and SELinux) and
    capabilities. The authors insist on the need to run containers with the least
    privilege level (e.g non-root) and conclude that with this use and the default
    Docker configuration, containers are fairly safe, providing a high level of isolation.
    A security-driven comparison among Operating System-level virtualization systems
    is provided in [9]. In the approach of OS-level virtualization, a number of distinct
    user space instances (often referred to as containers) are executed on top of
    a shared operating system kernel. The authors propose a generic model for a typical
    OS-level virtualization setup with the associated security requirements and compare
    a selection of OS-level virtualization solutions with respect to this model. Bacis
    et al. in [23] focuses on SELinux profiles management for containers. The authors
    propose an extension to the Dockerfile specification to let developers include
    the SELinux profile of their container in the built image, to simplify their installation
    on the host. This solution attempts to address the problem that the default SELinux
    Docker profile gives all containers the same type, and all Docker objects the
    same label, so that it does not protect containers from other containers [24].
    3.3. Defense against specific attacks With the wide spread usage of containerization
    technology, numerous articles related to specific defenses have come to light.
    Given that containers can directly communicate with the kernel of the host, an
    attacker can perform several escape attacks in order to compromise both the container
    and the host environment. Once out of the container, she has the potential to
    cause serious damages by obtaining rights through privilege escalation techniques
    or by causing the block of the system (and therefore of all the related containers
    hosted) making use of DoS attacks. In [25], the authors analyze Docker escape
    attacks and propose a defense method that exploits the status namespaces. This
    method provides for a dynamic detection of namespace status at runtime (i.e.,
    during the execution of the processes). The resulting monitoring is able to detect
    anomalous processes and can prevent their escape behaviours, increasing the security
    and the reliability of the container operations. In [26] the authors proposes
    a technique based on the limitation of container memory to reduce the Docker attack
    surface and protects the container technology from DoS attacks. 3.4. Vulnerability
    Analysis In [27], the authors provide an analysis of the content of the images
    available to download on the Docker Hub, from a security point of view. The authors
    show that a significant amount of official and unofficial images on the Docker
    Hub embed packages with known security vulnerabilities, that can therefore be
    exploited by an attacker. Detailed results show that 36% of official images contain
    high-priority CVE vulnerabilities, and 64% contain medium or high-priority vulnerabilities.
    Another recent work related to the vulnerabilities inside the images of Docker
    Hub is described in [28]. The authors of the paper analyze the Docker Hub images
    by using the framework DIVA (Docker Image Vulnerability Analysis). With the analysis
    of exactly 356.218 images they show that both official and unofficial images have
    more than 180 vulnerabilities on average. Furthermore, many images have not been
    updated for hundreds of days and the vulnerabilities commonly tend to propagate
    from parent images to child ones. Lu et al. in [29] the authors study the typical
    penetration testing process under Docker environment related to common attacks
    such as DoS, container escape and side channel. A. Mouat, in [30], provides an
    overview of some container vulnerabilities, such as Kernel exploits, DoS attacks,
    container breakouts, poisoned images, and compromising secrets. The study describes
    the Docker technology and provides security tips in order to limit the related
    attack surface. Although some vulnerabilities in our paper are common to those
    of the book, our work faces them from a different point of view (e.g., both works
    analyze the vulnerabilities inside the images but only ours considers the vulnerabilities
    due to the image automatic construction starting from the software development
    platform GitHub). Besides, in our work we study the security implications of the
    use of containers taking into account the typical use-cases. 3.5. Use-cases driven
    vulnerability analysis To the best of our knowledge, our work is the first one
    that provides a use-cases driven vulnerability analysis. We evaluate the impact
    of vulnerabilities in the three most used use-cases: Docker recommended use-case,
    wide-spread use-case (e.g., casting containers as Virtual Machines), and Cloud
    Provider CaaS use-case. The approach is new since it does not directly concern
    the Docker software but the code distribution process. The same authors of this
    paper have produced a magazine article [31] addressing (at a magazine level) just
    a subset of the topics discussed here. 4. Docker In this section, Docker’s ecosystem
    and main components, i.e., specification, kernel support, daemon, Docker Hub and
    dedicated OS that emerged around Docker, are presented. The term Docker is overloaded
    with a few meanings. It is first a specification for container images and runtime,
    including the Dockerfiles allowing a reproducible building process (Fig. 2 - component
    a). It is also a software that implements this specification (the Docker daemon,
    named Docker Engine: Fig. 2 - component b), a central repository where developers
    can upload and share their images (the Docker Hub: Fig. 2 - component c), and
    other unofficial repositories (Fig. 2 - component d), along with a trademark (Docker
    Inc.) and bindings with third parties applications (Fig. 2 - component e). The
    build process implies fetching code from external repositories (containing the
    packages that will be embedded in the images: Fig. 2 - component g). An orchestrator
    (Fig. 2 - component f) can be used for managing the lifecycle of the operational
    infrastructure. Download : Download high-res image (895KB) Download : Download
    full-size image Fig. 2. Overview of the Docker ecosystem. Arrows show code path,
    with associated commands on them (docker <action>). The Docker project is written
    in Go language and was first released in March 2013. Since then, it has experienced
    an explosive diffusion and widespread adoption [13]. 4.1. Docker specification
    The specification’s scope is container images and runtime. Docker images are composed
    of a set of layers along with metadata in JSON format. They are stored at /var/lib/docker/<driver>/
    where <driver> stands for the storage driver used (e.g., AUFS, BTRFS, VFS, Device
    Mapper, OverlayFS). Each layer contains the modifications done to the file-system
    relatively to the previous layer, starting from a base image (generally a lightweight
    Linux distribution). This way, images are organized in trees and each image has
    a parent, except from base images that are roots of the trees (Fig. 3). This structure
    allows to ship in an image only the modifications specifically related to that
    image (app payload). Therefore, if many images on a host inherit from the same
    base image, or have the same dependencies, they will be fetched only once from
    the repositories. Additionally, if the local storage driver allows it (with a
    union file-system, i.e., a read-only file system, and some sort of writable overlay
    on top [32]), it will be stored only once on the disk, leading to substantial
    resource savings. The detailed specification for Docker images and containers
    can be found at [33]. Download : Download high-res image (99KB) Download : Download
    full-size image Fig. 3. Example of image inheritance trees. Images metadata contain
    information about the image itself (e.g., ID, checksum, tags, repository, author...),
    about its parent (ID) along with (optional) default runtime parameters (e.g.,
    port re-directions, cgroups configuration). These parameters can be overridden
    at launch time by the docker run command. The build of images can be done in two
    ways. It is possible to launch a container from an existing image (docker run),
    perform modifications and installations inside the container, stop the container
    and then save the state of the container as a new image (docker commit). This
    process is close to a classical VM installation, but has to be performed at each
    image rebuild (e.g., for an update); since the base image is standardized, the
    sequence of commands is exactly the same. To automate this process, Dockerfiles
    allow to specify a base image and a sequence of commands to be performed to build
    the image, along with other options specific to the image (e.g., exposed ports,
    entry point...). The image is then built with the docker build command, resulting
    in another standardized tagged image that can be either run or used as a base
    image for another build. The Dockerfile reference is available in [34]. 4.2. Docker
    internals Docker containers rely on creating a wrapped and controlled environment
    on the host machine in which arbitrary code could (ideally) be run safely. This
    isolation is achieved by two main kernel features, kernel namespaces [35] and
    control groups (cgroups). Note that these features were merged starting from the
    Linux kernel version 2.6.24 [36]. There are currently 7 different namespaces in
    the kernel, each one addressing a specific aspect of the system [37]: • PID: provides
    a separate process tree with PID numbers relative to the namespace (two processes
    in different namespaces can have the same local PID). Each PID in a namespace
    is mapped to a unique global PID. • IPC (inter-process communication): provides
    POSIX message queues, SystemV IPC, shared memory, etc. • NET: provides network
    resources — each NET namespace contains its own network stack, including interfaces,
    routing tables, iptables rules, network sockets, etc. • MNT: provides file-system
    mountpoints: each container has its own view of the file-system and mount points
    —like an enhanced chroot— in order to avoid path traversals, chroot escapes, or
    information leak / injection through /proc, /sys and /dev directories. • UTS:
    provides hostname and domain isolation. • USER: provides a separate view of users
    and groups, including UIDs, GIDs, file permissions, capabilities... • CGROUP:
    provides a virtualization of the process’ cgroups view — each cgroup namespace
    has its own set of cgroup root directories, that represents its base points. Each
    of these namespaces has its own kernel internal objects related to its type, and
    provides to processes a local instance of some paths in /proc and /sys file-systems.
    For instance, NET namespaces have their own /proc/net directory. A thorough list
    of per-namespace isolated paths is provided by [37] and their isolation role is
    detailed in [9]. New namespaces can be created by the clone() and unshare() syscalls,
    and processes can change their current namespaces using setns(). Processes inherit
    namespaces from their parent. Each container is created within its own namespaces.
    Hence, when the main process (the container entry point) is launched, all container’s
    children processes are restricted to the container’s view of the host. cgroups
    are a kernel mechanism to restrict the resource usage of a process or group of
    processes. They prevent a process from taking all available resources and starving
    other processes and containers on the host. Controlled resources include CPU shares,
    RAM, network bandwidth, and disk I/O. 4.3. The Docker daemon The Docker software
    itself (Fig. 2b) runs as a daemon on the host machine. It can launch containers,
    control their level of isolation (cgroups, namespaces, capabilities restrictions
    and SELinux / Apparmor profiles), monitor them to trigger actions (e.g restart)
    and spawn shells into running containers for administration purposes. It can change
    iptables rules on the host and create network interfaces. It is also responsible
    for the management of container images: pull and push images on a remote registry
    (e.g the Docker Hub), build images from Dockerfiles, sign them, etc.. The daemon
    itself runs as root (with full capabilities) on the host, and is remotely controlled
    through a UNIX socket. The ownership of this socket determines which users can
    manage containers on the host using the docker command. Alternatively, the daemon
    can listen on a classical TCP socket, enabling remote container administration
    without requiring a shell on the host. 4.4. The Docker Hub The Docker Hub (Fig.
    2c) is an online repository that allows developers to upload their Docker images
    and let users download them. Developers can sign up for a free account, in which
    all repositories are public, or for a pay account, allowing the creation of private
    repositories. Repositories from a developer are namespaced, i.e., their name is
    “developer/repository”. There also exist official repositories, directly provided
    by Docker Inc, whose name is “repository”. These official repositories stand for
    most used base images to build containers. They are “a curated set of Docker repositories
    that are promoted on Docker Hub” [38]. The Docker daemon, along with the Docker
    Hub and the repositories are similar to a package manager, with a local daemon
    installing software on both the host and the remote repositories. Some of the
    repositories are official while others are unofficial, provided by third parties.
    From this point of view, the Docker Hub security can be compared to that of a
    classical package manager [39]. This similarity guided our vulnerability analysis
    study in Section 7. 4.5. The Docker Store The Docker Store [40] is an online self-service
    portal that allows Docker developers to sell and distribute their Docker images
    to Docker users. The Store contains free and open-source images, as well as software
    directly sold by publishers. Every user can sign for a free account and can both
    download freeware images, or purchase premium ones. Users can also become publishers
    to share their Docker images, hence getting several benefits. Among the many,
    publishers have greater visibility, the possibility to achieve the Docker certified
    quality marks, the possibility to use the Docker Store licensing support (to limit
    access to their software based on the type of users), and a communication channel
    with the customers, i.e., every customer is notified in case of Docker image updates
    or upgrades. Although Docker has just its own registry for containers (Docker
    Hub), the Docker Store is specifically oriented to the needs of enterprises. It
    offers enterprises with commercially supported software from trusted and verified
    publishers. 4.6. Docker dedicated operating systems In addition to the Docker
    package in mainstream distributions, a number of dedicated distributions have
    been developed specifically to run Docker or other container solutions. They allow
    running Docker on host OS other than Linux when run inside a VM, without the complexity
    of a full Linux distribution. We experimented three of these distributions: •
    Boot2docker [41], a distribution based on TinyCoreLinux, meant to be very lightweight
    (the bootable .iso weights 27MiB). It is mainly used to run Docker containers
    on OS other than Linux (e.g., running in VirtualBox on Windows Server). The security
    advantage, when compared to mainstream distributions, is the reduced attack surface
    due to the minimal installation. • CoreOS [42], a distribution dedicated to containers.
    It can run Docker, along with Rocket, for which it was designed. Rocket is a fork
    of Docker that only runs containers: in opposition to the monolithic design of
    Docker, interaction with the ecosystem and image builds are managed by other tools
    in CoreOS. The OS integrates with Kubernetes [43] to orchestrate container clusters
    on multiple hosts. • RancherOS [44], an OS entirely based on Docker, meant to
    run Docker containers. The init process is a Docker daemon (system-docker) and
    system services run in (privileged) containers. One of these services is another
    Docker daemon (user-docker) that spawns itself user-level containers. All installed
    applications on the system run in Docker containers, so that Docker commands are
    used to install and update software on the host. No external package manager is
    required. 5. Docker containers security architecture By construction, Docker security
    relies on three components: 1.) isolation of processes at userspace level managed
    by the Docker daemon; 2.) enforcement of the cited mechanism at kernel level;
    and, 3.) network operations security. In the following section, we develop each
    component. 5.1. Isolation Docker containers rely exclusively on Linux kernel features,
    including namespaces, cgroups, hardening and capabilities. Namespace isolation
    and capabilities drop are enabled by default, but cgroup limitations are not,
    and must be enabled on a per-container basis through -a -c options on container
    launch. The default isolation configuration is relatively strict, the only flaw
    is that all containers share the same network bridge, enabling ARP poisoning attacks
    between containers on the same host. However, the global security can be lowered
    by options, triggered at container launch, giving (to containers) extended access
    to some parts of the host (–uts=host, –net=host, –ipc=host, –privileged, –cap-add=<CAP>,
    etc.). These features enhance containers convenience, enabling containers to interact
    with the host system at the price of introducing possible vulnerabilities. For
    example, when given the option –net=host at container launch, Docker does not
    place the container into a separate NET namespace and therefore gives the container
    full access to the host’s network stack (enabling network sniffing, reconfiguration,
    etc.). Additionally, security configuration can be set globally through options
    passed to the Docker daemon. This includes options lowering security, like the
    –insecure-registry option, disabling TLS certificate check on a particular registry.
    Options increasing security are available, such as the –icc=false parameter, that
    forbids network communications between containers and mitigates the ARP poisoning
    attacked described before. However, they prevent multi-container applications
    from operating properly, hence are rarely used. 5.2. Host hardening Host hardening
    through Linux Security Modules is a means to enforce security related limitations
    constraints imposed to containers (e.g., compromise of a container and escape
    to the host OS). Currently, SELinux, Apparmor and Seccomp are supported, with
    available default profiles. These profiles are generic and not restrictive (for
    instance, the docker-default Apparmor profile [45] allows full access to file-system,
    network and all capabilities to Docker containers). Similarly, the default SELinux
    policy puts all Docker objects in the same domain. Therefore, default hardening
    does protect the host from containers, but not containers from other containers.
    This must be addressed by writing specific profiles, that depend individually
    on the containers. SELinux experiments were conducted in 2012 on LXC containers
    [24], showing that the default SELinux profile gives all containers the same type,
    and all objects the same label. This configuration does not protect containers
    from other containers. 5.3. Network security Network resources are used by Docker
    for image distribution and remote control of the Docker daemon. Concerning image
    distribution, images downloaded from a remote repository are verified with a hash,
    while the connection to the registry is made over TLS (except if explicitly specified
    otherwise). Moreover, starting from version 1.8 issued in August 2015, the Docker
    Content Trust [46] architecture allows developers to sign their images before
    pushing them to a repository. Content Trust relies on TUF (The Update Framework
    [47]). It is specifically designed to address package manager flaws [39]. It can
    recover from a key compromise, mitigate replay attacks by embedding expiration
    timestamps in signed images, etc.. The trade-off is a complex management of keys.
    It actually implements a PKI where each developer owns a root key (“offline key”)
    that is used to sign (“signing keys”) Docker images. The signing keys are shared
    among every entity needing to issue an image (possibly including automated signatures
    in an automated code pipeline, meaning that third-parties have access to the keys).
    The distribution of the (numerous) root public keys is also an issue. The daemon
    is remote-controlled through a socket. By default, the socket used to control
    the daemon is a UNIX socket, located at /var/run/docker.sock and owned by root:docker.
    Access to this socket allows to pull and run any container in privileged mode,
    therefore giving root access to the host. In case of a UNIX socket, a user member
    of the docker group can gain root privileges, and in case of a TCP socket, any
    connection to this socket can give root privileges to the host. Therefore the
    connection must be secured with TLS (–tlsverify). This enables both encryption
    and authentication of the two sides of the connection (while adding additional
    certificate management). 6. Typical Docker use-cases Most of the security discussions
    about containers compare them to VMs, thus assuming both technologies are equivalent
    in terms of design. Although VMs equivalence is the aim of some container technologies
    (e.g., OpenVZ used to spawn Virtual Private Servers), recent “lightweight” container
    solutions such as Docker were designed to achieve completely different objectives
    than the ones achieved by VMs [48]. Therefore, it is a key point to develop the
    Docker typical use-cases here, as an introduction to the vulnerability analysis
    and to put the vulnerabilities (Section 7) in perspective of each context. We
    can distinguish three types of Docker usages: • Recommended use-case, i.e., the
    usages Docker was designed for, as explained in the official documentation; •
    Wide-spread use-case, i.e., the common usages done by application developers and
    system administrators; • Cloud provider’s CaaS use-case, i.e., the usages guided
    by the Cloud providers implementations to cope with both security and integration
    within their infrastructure. 6.1. Recommended use-case Docker developers recommend
    a micro-services approach [49], meaning that a container must host a single service,
    in a single process (or a daemon spawning children). Therefore a Docker container
    is not considered as a VM: there is no package manager, no init process, no sshd
    to manage it. All administration tasks (container stop, restart, backups, updates,
    builds...) have to be performed via the host machine, which implies that the legitimate
    containers admin has root access to the host. Indeed, Docker was designed to isolate
    applications that would otherwise run on the same host, so this root access is
    assumed to be granted. From a security point of view, isolation of processes (through
    namespaces) and resources management (through cgroups) makes it safer to deploy
    Docker applications compared to not using container technology but rather usual
    processes on the host. The main advantage of Docker is the ease of application
    deployment. It was designed to completely separate the code plane from the data
    plane: Docker images can be built anywhere through a generic build file (Dockerfile)
    which specifies the steps to build the image from a base image. This generic way
    of building images makes the image generation process and the resulting images
    almost host-agnostic, only depending on the kernel and not on the installed libraries.
    The considerable effort and associated benefits of adopting the micro-services
    approach are developed in [50]. Airpair [51] lists eight proven real world Docker
    use cases, that fit in the official recommendations: • Simplifying configuration;
    • Code pipeline management; • Developer productivity; • App isolation; • Server
    consolidation; • Debugging capabilities; • Multi-tenancy; and, • Rapid deployment.
    6.2. Wide-spread use-case According to Forrester consulting [14], one of the main
    reasons for adopting containers is to increase developers efficiency rather than
    favoring micro-service architectures (i.e., the recommended use-case). In fact,
    two of the most popular images on Docker Hub are Ubuntu (by far) and CentOS [52],
    two VM-oriented container images. Some sysadmins or developers use Docker as a
    way of shipping complete virtual environments and updating them on a regular basis,
    casting containers’ usage as VM [53]. Although this is convenient since it limits
    system administration tasks to the bare minimum (e.g., docker pull), it has several
    security implications. First, embedding more software than the payload the container
    was designed for, increases the attack surface of resulting container images.
    Additional packages and libraries could lead to vulnerabilities that would otherwise
    be avoided. Moreover, this software bloat makes containers management more complex
    and leads to wasted resources (e.g., larger images, more bandwidth and storage
    needed do deploy them, more processes in the containers). Then, with containers
    embedding enough software to run a full system (logging daemon, ssh server, even
    sometimes an init process), it is tempting to perform administration tasks from
    within the container itself. This is completely opposed to Docker’s design. Indeed,
    some of these administration tasks need root access to the container. Some other
    administration actions (e.g., mounting a volume in a container) may need extra
    capabilities that are dropped by Docker by default. This kind of usage tends to
    increase the attack surface. Indeed, it enables more communication channels between
    host and containers, and between co-located containers, increasing the risk of
    attacks, such as privilege escalation. Eventually, with the acceleration of software
    development cycles allowed by Docker, developers cannot maintain each version
    of their product and only maintain the latest one (tag “latest” on Docker repositories).
    As a consequence, old images are still available for downloading, but they have
    not been updated for hundreds of days and can introduce several vulnerabilities
    [28]. A study [27] has shown that more than 30% of images on the Docker Hub contain
    high severity CVE vulnerabilities, and up to 70% contain high or medium severity
    vulnerabilities. Note that these images cannot always be reproducibly built: although
    the Dockerfile is public on the Docker Hub, it often includes a statement ADD
    start.sh /start.sh or similar, that copies an install script from the maintainer’s
    computer (and not available on the Dockerhub) to the image and runs it, without
    appearing in the Dockerfile. Some maintainers even remove this script after execution.
    6.3. Cloud providers CaaS use-case In this section, we present the integration
    of Docker as provided by the main Cloud Providers. We focused on Amazon Web services,
    Google Container Engines, and Microsoft Azure as they are three market leaders.
    Further, we experimented on them as per the provided level of security, and results
    are reported in the following. Amazon provides a recent support for Docker containers
    in its Elastic Compute Cloud (EC2) orchestration tool (generally available since
    April 2015). It allows users to launch containers in EC2 VM instances and to control
    them with a dedicated interface (EC2 Container Service (ECS)). Users must first
    create VM from their EC2 account, install Docker and the ECS daemon on them, register
    them to their ECS cluster, and, finally, they can launch Docker containers via
    the web interface [1] or via a command-line tool. Concerning Docker security,
    the host configuration is up to the users since the host is an EC2 instance, i.e.,
    a VM. Standard images do not provide either Apparmor or SELinux (but they can
    be installed) and the host network configuration is dependent on the EC2 private
    cloud [1]. Similarly, Google provides support for Docker in its Compute Engine
    infrastructure. It has recently issued its Kubernetes orchestrator [43]. It allows
    to automatically create a cluster of VM on which Docker is installed and configured,
    running on a private overlay network. Containers are grouped in “pods”: sets of
    containers sharing the same NET namespace (and so interfaces and IP addresses)
    and optionally cgroups, enabling direct communication between them. Highly coupled
    containers (multiple micro-services composing the same application) typically
    run in the same pod. Pods are the base entity of the Kubernetes orchestrator,
    just as VM are for classical cloud infrastructures. Pods are automatically instantiated
    and placed on VMs in the cluster, according to redundancy and availability constraints
    defined by “replication controllers”. These replication controllers are themselves
    part of “services”: entities that define the global parameters of an application
    (external port mappings, etc.). All nodes in a cluster run a daemon (kubelet)
    that controls the local Docker daemon, and a central node performs the orchestration.
    Cluster administration is performed via the kubectl command, and a web interface
    is expected soon. Kubernetes installation is native in Google Container Engine,
    which automatically creates the VMs of the cluster. Users can only chose a template
    for their VMs. kubectl commands can be run from any machine with credentials to
    access the API on the central node. We experimented this setup, choosing the template
    “n1-standard-1” for the VMs. Installation is also possible on a number of other
    commercial platforms [54] (including Amazon EC2) via distribution and platform-specific
    scripts. Installation from scratch on a custom platform is also possible. Microsoft,
    instead, provides support for Docker both with Azure Container Instances, and
    with Azure Container Services (AKS). The former is a solution for any scenario
    that can operate in a single isolated container, including simple applications
    and build jobs, while the latter is a solution for a full container orchestration,
    including service discovery across multiple containers, automatic scaling, and
    coordinated application upgrades. Azure Container Instances allows a user to download
    or build a container image. These images can be pushed in an Azure-based private
    registry called Azure Container Registry. Users can interact with both the images
    using the Docker CLI tools, and with containers using the Azure CLI [55]. AKS
    allows to rapidly distribute Docker Swarm, DC/OS, and Kubernetes clusters [56].
    AKS has the sole task of installing and deploying the cluster. The orchestrators,
    in turn, have the goal of managing the containers and services itself. A swarm
    represents a cluster of nodes that can be either physical or virtual machines.
    There are two types of nodes: manager and worker nodes. The manager nodes maintain
    the state of the swarm, while the worker ones have the burden to execute containers
    [57]. An application image can be deployed by creating a service. When a user
    deploys the service to the swarm, the swarm manager has the responsibility to
    schedule it as one o more tasks that run independently of each other on nodes
    in the swarm. A task is the atomic unit of scheduling within a swarm which is
    instantiated inside a container. Docker Swarm provides two types of service deployments:
    replicated and global. Using the replicated service, the user must specify the
    number of identical tasks she wants to run. A global service instead runs one
    task of every node, each time the user adds a node to the swarm the orchestrator
    creates a task and the scheduler assigns the task to the new node [57]. Concerning
    security, Docker Swarm uses the swarm mode PKI. The manager node generates a new
    root CA which are used to secure communication with other nodes that join the
    swarm. Each time a node joins the swarm, the manager issues a certificate to the
    node [57]. The aforementioned Cloud Provider approaches are similar and their
    orchestrators have corresponding main components (Table 2). The main differences
    lie between Tasks and Pods. Indeed, while tasks are groups of containers that
    are launched together on the same host, the Kubernetes pod approach differs from
    Docker’s micro-service approach since containers of a same pod share some resources.
    A pod is closer to a VM than to a container from a functional point of view, albeit
    with no kernel. Another difference concerns the scaling of the services. In fact,
    Docker Swarm uses Docker Compose to scale an application [58] — being Docker Compose
    a tool for defining and running multi-container Docker applications [59]. Table
    2. Correspondence between Amazon ECS, Kubernetes and Docker Swarm main components.
    Amazon ECS Kubernetes Docker Swarm Container instance (VM) Node Node ECS agent
    Kubelet Manager Swarm Task Pod Task Service Replication controller N. A. Task
    definition Service Service Cluster Cluster Swarm 7. Docker vulnerability-oriented
    analysis For each of the three typical Docker use-cases detailed in the previous
    section, the chosen approach is to define first an adversary model, and then to
    perform a vulnerability-oriented security analysis [60]. 7.1. Adversary model
    Given the ecosystem and use-cases description, we consider two main categories
    of adversaries, i.e., direct and indirect. A direct adversary is able to sniff,
    block, inject, or modify network and system communications. She targets directly
    the production machines. Locally or remotely, she can compromise: • in-production
    containers (e.g., from an Internet facing container service, she gains root privileges
    on the related container; from a compromised container, she makes a DoS on co-located
    containers, i.e., containers on the same host OS); • in-production host OS (e.g.,
    from a compromised container, she gains access to critical host OS files, i.e.,
    a container’s escape); • in-production Docker daemons (e.g., from a compromised
    host OS, she lowers the default security parameters to launch Docker containers);
    • the production network (e.g., from a compromised host OS, she redirects network
    traffic). An indirect adversary has the same capabilities of a direct one, but
    she leverages the Docker ecosystem (e.g., the code and images repositories) to
    reach the production environment. Depending on the attack phase, we identified
    the following targets: containers, host OS, co-located containers, code repositories,
    images repositories, management network. To subvert a dockerized environment,
    we consider here a subset of all the potential attack vectors, i.e., Docker containers,
    code repositories, and images repositories. We consider primarily these attack
    vectors as they are associated with services and interfaces publicly available.
    Other attack vectors may include host OS, management network or physical access
    to systems. 7.2. Vulnerabilities identification In this section we analyze separately
    the main components of the Docker ecosystem, revealing (part of) their attack
    surface. Following a top-down approach, we identified five categories of vulnerabilities,
    each one related to a different layer of the ecosystem. Typical use-cases (see
    Section 6), already observed vulnerabilities (e.g., CVE [61]), and scope of the
    mitigations (e.g., SELinux, seccomp) enriched this iterative process. These levels
    are classified from the most remote to the most local one, relatively to a production
    system hosting Docker containers: • Insecure production system configuration;
    • Vulnerabilities in the image distribution, verification, decompression, storage
    process; • Vulnerabilities inside the images; • Vulnerabilities directly linked
    to Docker or libcontainer; and, • Vulnerabilities of the Linux kernel. We discuss
    below these five categories, highlighting the limits of the protections offered
    by Docker. We assume minimal configuration (at least the default config) is applied.
    7.3. Insecure configuration Docker’s default configuration is relatively secure
    as it provides isolation between containers and restricts the containers’ access
    to the host. A container is placed in its own namespaces and own cgroup, and only
    owns the following capabilities: CAP_CHOWN, CAP_DAC_OVERRIDE, CAP_FSETID, CAP_FOWNER,
    CAP_MKNOD, CAP_NET_RAW, CAP_SETGID, CAP_SETUID, CAP_SETFCAP, CAP_SETPCAP, CAP_NET_BIND_SERVICE,
    CAP_SYS_CHROOT, CAP_KILL and CAP_AUDIT_WRITE. Vulnerabilities The use of some
    options, either given to the Docker daemon on startup, or given to the command
    launching a container, can provide containers an extended access to the host.
    We have, for instance: • Mounting of sensitive host directories into containers
    • TLS configuration of remote image registries • Permissions on the Docker control
    socket • Cgroups activation (disabled by default) • Options directly providing
    to containers an extended access to the host (–net=host, –uts=host, –privileged,
    additional capabilities) Exploitation For example, the option –uts=host allocates
    the container in the same UTS namespace of the host. This, in turn, allows the
    container to see and change the host’s name and domain. The option –cap-add=<CAP>
    gives to the container the specified capability, thus making it potentially more
    harmful to the host. With –cap-add=SYS_ADMIN, a container can, for instance, remount
    /proc and /sys sub-directories in read/write mode, and change the host’s kernel
    parameters, leading to potential vulnerabilities, such as data leakage or denial
    of service. Along with these runtime container options, several settings on the
    host can pave the way to attacks. Even basic properties can at least trigger denial
    of service. For instance, with some storage drivers (e.g., AUFS), Docker does
    not limit containers disk usage. A container with a storage volume can fill this
    volume and affect other containers on the same host, or even the host itself if
    the Docker storage, located at /var/lib/docker, is not mounted on a separate partition.
    Mitigation In order to limit these harmful options which can lead to have the
    host being accessed by the container, the Center for Internet Security realized
    a Docker Benchmark [62]. It provides two lists of options: the ones that should
    be used and the ones that should not be used when running containers as isolated
    applications with Docker. These options are sorted in six categories: Host configuration,
    Docker daemon configuration, Docker daemon configuration files, Container images
    and build file, Container runtime, and Docker security operations. They include
    the ones mentioned above, along with other host configuration, hardening configuration,
    file permissions on the host, and TLS configuration for Docker registries and
    the control socket. A Docker host running containers that fulfill both the good
    practices [49] and the CIS recommendations [62], and that further embed a single
    userspace application, should not need any of the options cited beforehand. Using
    these options one can break the isolation property. Hence, these options should
    be used only with trusted containers, reducing the container to an application
    packaging tool on the production host, and not to an isolation tool. 7.4. Vulnerabilities
    in the image distribution process The distribution of images through the Docker
    Hub and other registries is a source of vulnerabilities in Docker, since the code
    within these images will be executed on the host. We first discuss vulnerabilities
    and attacks on the Docker Hub and other registries. Then, we study vulnerabilities
    that are more specific to Docker, such as vulnerabilities in the extraction process,
    and related to the automated build chain. 7.4.1. Docker as a package manager Vulnerabilities
    The architecture of the Docker Hub is similar to a package repository, with the
    Docker daemon acting as a package manager on the host. Therefore it is vulnerable
    to the same vulnerabilities of package managers. These vulnerabilities include
    processing, storage and uncompression of potentially untrusted code, performed
    by the Docker daemon with root privileges. This code can be either tampered at
    the source (malicious image) or during the transfer (for instance as a consequence
    of the –insecure-registry option given to the Docker daemon, that makes possible
    a Man-in-the-Middle attack between the registry and the host). Exploitation Attacks
    on package managers are possible [39] if an attacker controls a part of the network
    between the Docker host and the repository. A successful attack would allow her
    to make her image downloaded on docker hosts. This leads to compromised images
    that can exploit vulnerabilities in the extraction process. First, since images
    are compressed, a specifically crafted image containing a huge file filled with
    gibberish data (e.g., zeros) would at least fill the host storage device causing
    denial of service (zipbomb-like attack). Then, since images are extracted on the
    host file-system, path traversals have been possible in the past (CVE-2014-9356
    [63], fixed in Docker 1.3.3). Exploitation of this vulnerability made the uncompression
    of the images (which is performed as root) follow absolute symlinks on the host,
    making possible to replace binaries on the host with binaries from the image.
    Other possible attacks include code injection in images or replay of old images
    containing known vulnerabilities. Mitigation Before release 1.8, the only protection
    was the use of TLS on the connection to the registry, which could be disabled.
    With version 1.8, Docker introduced Content Trust [46], an architecture for signing
    images the same way packages are signed with package managers. This raises two
    issues. First, Content Trust —and so image signature check— can be disabled passing
    the –disable-content-trust option to the Docker daemon. This looks convenient
    for a private registry on a local network, but constitutes a security vulnerability.
    Then, the image signature process requires to trust the developers, which is only
    possible by unifying the image signature. Moreover, this solution does not scale
    with thousands of developers signing their repositories with their own key. Beyond
    the technical challenge, this is a problem of trust and scale. 7.4.2. Automated
    deployment pipeline vulnerabilities Vulnerabilities Automated builds and webhooks
    proposed by the Docker hub are a key element in this distribution process. They
    lead to a pipeline where each element has full access to the code that will end
    up in production. Each element is also increasingly hosted in the cloud. For instance,
    to automate this deployment, Docker proposes automated builds on the Docker Hub,
    triggered by an event from an external code repository (GitHub, BitBucket, etc.)
    — Fig. 4, step 2. Docker then proposes to send an HTTP request to a Docker host
    reachable on the Internet to notify it that a new image is available; this event
    triggers an image pull and the container restarts on the new image (Docker hooks
    [12]) — Fig. 4, steps 9 and 10. With this deployment pipeline, a commit on GitHub
    (Fig. 4, step 1) will trigger a build of a new image (Fig. 4, step 2); this image
    will be automatically launched it in production (Fig. 4, steps 9 and 10). Optional
    test steps can be added before production, themselves potentially being hosted
    at another provider. In this last case, the Docker Hub makes a first call to a
    test machine (Fig. 4, steps 3 and 5), that will then pull the image, run the tests,
    and send the results to the Docker Hub using a callback URL (Fig. 4, steps 4 and
    6). The build process itself often downloads dependencies from other third-parties
    repositories (Fig. 4, steps 7 and 8), sometimes over an unsecure channel (that
    could be tampered with). The whole code pipeline is exposed in Fig. 4. Download
    : Download high-res image (505KB) Download : Download full-size image Fig. 4.
    Automated deployment setup in the public cloud using GitHub, the Docker Hub, external
    test machines and repositories from where code is downloaded during build process.
    Exploitation In this architecture, compromise approaches include account hijacking,
    tampering with network communications (depending on the use of TLS), and insider
    attacks. This setup adds several external intermediary steps to the code path,
    each of them having its own authentication and attack surface, overall increasing
    the global attack surface. For instance, we had the intuition that a compromised
    GitHub account could lead to the execution of malicious code on a large set of
    production machines within minutes. We therefore tested a scenario including a
    Docker Hub account, a GitHub account, a development machine and a production machine.
    The assumption was that the adversary will use the Docker ecosystem to put in
    production a backdoored Docker container. More precisely, we assumed that the
    adversary had successfully compromised some code on the code repository (for instance,
    via a successful phishing attack). Due to network restrictions (corporate proxy)
    our servers could not be reached by webhooks, so we wrote a script to monitor
    our repository on the Docker Hub as well as downloads of new images (Fig. 5).
    Our initial intuition was confirmed: adversary’s code was put in production 5
    minutes and 30s after the adversary’s commit on GitHub. This attack is particularly
    dreadful, since it scales to an arbitrary number of machines watching the same
    Docker Hub repository. Download : Download high-res image (228KB) Download : Download
    full-size image Fig. 5. Script monitoring the remote repository and updating the
    container if a new version is available. Note that while compromising a code repository
    is independent of Docker, automatically pushing it in production dramatically
    increases the number of compromised machines, even if the malicious code is removed
    within minutes. Compromise could also happen at the Docker Hub account level,
    with the same consequences. Account hijacking is not a new problem, but it should
    be an increasing concern with the multiplication of accounts at different providers.
    With source code tampered at the victim machine, TLS is useless. Actually, the
    tampered code is “securely” distributed over TLS to the various repositories.
    Furthermore, with TLS, IDS and IPS monitoring solutions are blind — unless legitimate
    man-in-the-middle setups are done. Moreover, while code path is usually (and always
    with Docker) secured using TLS communications, it is not the case of API calls
    that trigger builds and callbacks. Tampering with these data can lead to erroneous
    test results, unwanted restarts of containers, etc.. Additionally, such a setup
    is not compatible with the Content Trust scheme, since code is processed by external
    entities between the developer and the production environment. Content Trust provides
    an environment in which a single entity is trusted (the person or organization
    that signed the images) while in the present case trust is split over several
    external entities, each of them being capable of compromising the images. 7.5.
    Vulnerabilities inside the images The code available for download on the Docker
    Hub (used to build images) is directly exposed to attackers when in production.
    Vulnerabilities in [27] it has been showed from crawling the Docker Hub that 36%
    of official images contain high-priority CVE vulnerabilities, and 64% contain
    medium or high-priority vulnerabilities. These figures lower to 23% and 47% respectively
    for images tagged “latest”. Although these images are the most downloaded on the
    Docker Hub, they contain a significantly high amount of vulnerabilities, including
    some recent well known vulnerabilities (e.g., shellshock and heartbleed). The
    DevOps movement, promoted by Docker, lets developers package themselves their
    applications, thus mixing the development and production environments hence possibly
    introducing vulnerabilities. Development versions of packages or dev tools can
    remain in the final version of the Docker image, increasing its attack surface
    (e.g., a debugging tool installed in the image). Then, the provided images often
    contain outdated versions of packages, either because their base image (e.g.,
    Ubuntu or Debian) is old or because their build process pulls outdated code from
    some remote repository. The multiplication of image builds —virtually one for
    each commit in a project— leads to a persistence of outdated images still available
    on the repositories, while fast development cycles generally focus on latest versions.
    Exploitation Exploitation of such vulnerabilities is relevant in a context where
    the attack comes from outside (i.e., not a malicious image). Classical application
    vulnerabilities exploitation methods are possible, provided that the container
    exposes an entry point (network port, input data, etc.). Additionally, images
    built from external code repositories (i.e., images that pull data from some repository
    during the build process —Online code on Fig. 4— as stated in their Dockerfile)
    are dependent on this repository and on the (in)security of the connection used
    to fetch these data. These repositories, not always official, are another entry
    point for code injection. Mitigation Both Docker Hub and Docker Cloud make use
    of the Docker Security Scanning. Users can scan images in private repositories
    to verify that they are free from kwown security vulnerabilities. This feature
    is available as a free preview for private repository subscribers for a limited
    time. The scan traverses each layer of the image, identifying the software components
    and indexing their SHAs. These SHAs are compared against the CVE database in order
    to obtain information about the well-known security vulnerabilities [64]. The
    whole scan phase takes from 1 to 24 hours, depending on the size of the evaluating
    images. The support provided is limited due to both the availability only for
    private repositories and the cost of the service. Moreover, if a vulnerability
    is not part of the database the scan cannot reveal it, making the service unresponsive
    to new attacks. 7.6. Vulnerabilities directly linked to Docker or libcontainer
    Vulnerabilities Vulnerabilities found into Docker and libcontainer [61] mostly
    concern file-system isolation: chroot escapes (CVE-2014-9357, CVE-2015-3627),
    path traversals (CVE-2014-6407, CVE-2014-9356, CVE-2014-9358), access to special
    file systems on the host (CVE-2015-3630, CVE-2016-8867, CVE-2016-8887), container
    escalation (CVE-2014-6408), and privilege escalation (CVE-2016-3697). Most of
    these specific vulnerabilities are all patched as of Docker version 1.6.1. Further,
    CVE-2016-3697 is patched as of Docker version 1.11.0 while CVE-2016-3697 and CVE-2016-8867
    are patched with Docker version 1.12.3. Since container processes often run with
    PID 0 (i.e., root privileges), they have read and write access on the whole host
    file-system when they escape. Thus, they are allowed to overwrite host binaries,
    which leads to a delayed arbitrary code execution with root privileges. Exploitation
    Beyond the kernel namespaces, cgroups, Docker dropping capabilities and mount
    restrictions, Mandatory Access Control (MAC) can enforce constraints in case the
    normal execution flow is not respected. This approach is visible in the docker-default
    Apparmor policy. However, there is room for improvements in the MAC profiles for
    containers. In particular, Apparmor profiles normally behave as whitelists [65],
    explicitly declaring the resources any process can access, while denying any other
    access when the profile is in enforce mode. However, the docker-default profile
    installed with the docker.io package gives containers full access to network devices,
    file-systems along with a full set of capabilities, and contains just a small
    list of deny directives, consisting de facto in a blacklist. 7.7. Vulnerabilities
    of the Linux kernel Since containers run on the same kernel of the host, they
    are vulnerable to kernel exploits. An exploit giving full root privileges into
    a container can allow an attacker to break out this container and compromise the
    host (triggering for instance isolation and integrity breach, as well as data
    exposure). 8. Discussion 8.1. Vulnerability assessment We conducted an assessment
    of the severity of the explained vulnerabilities (Table 3) according to each use-case.
    This is coherent with the choice we made to analyze the Docker ecosystem and typical
    use-cases rather than focusing on a specific use-case with a specific application.
    This is also consistent with the NIST methodology that defines the context as
    a dimension that has to be considered when performing a vulnerability assessment
    [60]. Therefore, we defined three use-cases to base our comparison upon and we
    also proved experimentally some of the highlighted vulnerabilities. Table 3. Relative
    assessment scale of vulnerabilities severity on the three developed usages. Vulnerability
    categories Docker recommended use-case Wide-spread use-case (e.g., casting containers
    as VM) Cloud Provider CaaS use-case Insecure configuration Moderate. Docker’s
    default configuration on local systems is relatively secure — see Section 7.3.
    Lowering of security configuration possible by the sysadmin or containers placement.
    Very high Very likely insecure configuration. High CIS benchmark on EC2 by default
    score 62% of compliance. Containers in pods sharing the same NET namespace with
    Kubernetes. Vulnerabilities in the image distribution, verification, decompression
    and storage process Very high Usage promoted extensively be the DevOps approach.
    Automation at all layers to bring shorter development cycles and continuous delivery.
    Moderate Containers used as VMs, followed by less continuous delivery High Automation
    at all layers to bring shorter development cycles and continuous delivery. Vulnerabilities
    inside the images Moderate By default exposing a limited attack surface. Very
    high Very likely usage of heavyweight Linux distribution images with an attack
    surface bigger than micro-services- oriented images. Moderate Depending on what
    images and where they are retrieved from: both micro-server and VM like usages
    are possibly used here. Vulnerabilities directly linked to Docker or libcontainer
    Similar level across the use-cases N. A. N. A. Vulnerabilities in the kernel Similar
    level across the use-cases N. A. N. A. Our assessment is exclusively focused on
    the difference between use-cases, all other dimensions (i.e., threats and remediations)
    being equal. As a general comment, the wide-spread use-case (i.e., casting containers
    as VM) exposes vulnerabilities more than the other use-cases. Moreover, we can
    see that, whatever the considered use-cases, the kernel vulnerabilities are of
    a similar severity level, as well as the vulnerabilities directly linked to Docker.
    We believe that this analysis of the different layers of the Docker ecosystem
    provides a valuable vulnerability assessment state of the art and a sound basis
    for the research community upcoming contributions. 8.2. Multi-tenant implementation
    Container execution in PaaS providers data-centers is becoming a standard for
    cloud applications [1], [3]. Ensuring isolation is of paramount importance in
    such multi-tenant installations where users can run their own code in containers.
    We showed in Section 6.3 that main public cloud providers run these containers
    inside VM, and allocate a VM to a single user, creating a vertical multi-tenancy.
    These VMs are sometimes themselves running in containers (e.g., Google Container
    Engine [3]) — Fig. 6. Download : Download high-res image (443KB) Download : Download
    full-size image Fig. 6. Containers integration in a multi-tenant cloud system.
    There can be up to three different tenants vertically, and for each vertical layer
    an arbitrary number of tenants horizontally. Users may also delegate accounts
    to other users in their cluster, creating an horizontal multi-tenancy, where containers
    belonging to different users run concurrently in the same VM. For instance, Kubernetes
    allows creating user accounts within a cluster, each user being able to run containers.
    As illustrated in Fig. 6, the security profiles (e.g., SELinux and Apparmor profiles)
    are stacked at multiple layers. Therefore, the configuration of these security
    profiles may be challenging. In particular, enforcing cumulative and independently
    managed security profiles could lead to a situation where legitimate actions could
    be prevented from executing. This can become a complex task, since even with basic
    Apparmor profile, we observed unexpected behaviours, as described below. A script
    (Fig. 7) was run in a Docker container trying to perform actions prohibited by
    the apparmor-default profile while it was enforced. This container was run with
    the –cap-add=SYS_ADMIN option, so that it could perform the mount command. We
    ran our tests on two machines: a Debian 8 (kernel 3.16) with the Docker package
    from the distribution (version 1.6.2) and an Ubuntu 14.04 (kernel 3.19) with Docker
    installed from get.docker.com (version 1.8.3). The default Apparmor profiles for
    both distributions forbid mounts and write access to /proc/sys/kernel/sysrq and
    /proc/sysrq-trigger, so that even with CAP_SYS_ADMIN none of these commands should
    succeed. While on Ubuntu the commands were blocked, the mounts were successfully
    performed on the Debian 8, even though the profile contained en explicit “deny
    mount”. Download : Download high-res image (91KB) Download : Download full-size
    image Fig. 7. Script assessing effective application of Apparmor restrictions.
    This example shows that, even with a regular installation from repositories and
    a rudimentary script, the Apparmor default profile for Docker may be applied differently.
    This situation can only worsen when several layers of containers and hardening
    are nested, as illustrated in Fig. 6. Additionally, this architecture does not
    replace VM, so the rapidity of execution of containers is not exploited as it
    could be. Code execution paths of containers (left arrow on Fig. 6) are even longer
    than with a single VM, since they add two Docker processes, with their libraries
    and the associated hardening. 8.3. Synoptic survey According to the categories
    introduced above, we map in Table 4 the contributions of the most representative
    work on containers, as well as our own. Table 4. Synoptic survey of the major
    current contributions in the container domain. Empty Cell Container and virtualization
    comparison Security aspects of containers Defense against DoS attacks Defense
    against Escape attacks Vulnerability analysis Use-cases driven vulnerability analysis
    [21] [6] [22] [8] [9] [23] [24] [26] [25] [27] [28] [29] [30] This work The topic
    of containers has been addressed in the literature from different perspectives.
    Some work make a comparison with virtual machines, either from a performance point
    of view [6], [21], [22] or from a security one [29]. Other work evaluate the security
    aspects of containers [8], [9], [23], [24], [30] or propose defenses related to
    specific attacks [25], [26]. The analysis of Docker vulnerabilities (generic or
    specific) are dealt with in various work [26], [27], [28], [29], [30] but in this
    paper we introduce new elements (e.g., the analysis and exploitation of the vulnerabilities
    arisen from the automated construction of images triggered by events from external
    code repository, such as Docker Hub and BitBucket), as well as a taxonomy that
    can be used by other scientists and practitioners to evolve the mapping between
    solutions and the related vulnerabilities contributing to the identification of
    possible countermeasures. 8.4. Alternatives to Docker In parallel to containers,
    Unikernels have been around for a few years. Although they are not used in production
    yet, they address the isolation issue by embedding their own kernel, specifically
    optimized for one application. They achieve performance close to or even better
    than containers, and their very fast boot could allow launching them on the fly
    to serve a specific request. Latest technology developments promote unikernels
    to be a serious concurrent to containers [4], [20] for the coming years. 9. Conclusion
    In this paper, we performed a review of the components of the whole Docker ecosystem—the
    containers, the code delivery, and related orchestration. Leveraging an extensive
    literature review, coupled with a security-driven classification, we have identified
    key elements to assess the security of each component. In particular, we highlighted
    some new classes of vulnerabilities. To substantiate our findings, we performed
    some experiments to demonstrate how tangible were the security risks. We showed
    that the usual comparison “VM vs. containers” is inappropriate since the use cases
    are different and so is the underlying architecture. We also showed that from
    a local point of view, many vulnerabilities of Docker are due to casting containers
    as VM. From the point of view of the ecosystem, the multiplication of external
    intermediaries, providing code that will end up in production, widely increases
    the attack surface. Further, we discussed the current usage of orchestrators,
    and pointed out that these orchestrators could be a means of limiting misuse of
    Docker — leading to host and containers weak isolation. In particular, enforcing
    isolation could be achieved introducing higher levels of abstraction (tasks, replication
    controllers, remote persistent storage, etc.) that completely remove host dependence,
    enabling better isolation. We are currently working on the analysis of the orchestrators
    security. Acknowledgments The authors would like to thank the anonymous reviewers
    for their comments, that helped to improve quality and organization of the paper.
    References [1] Amazon ec2 container service reference, 2018, http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html.
    Google Scholar [2] Google compute engine reference, 2018, https://cloud.google.com/compute/docs/.
    Google Scholar [3] Intel, Linux* containers streamline virtualization and complement
    hypervisor-based virtual machines. Google Scholar [4] A. Madhavapeddy, R. Mortier,
    C. Rotsos, D. Scott, B. Singh, T. Gazagnaire, S. Smith, S. Hand, J. Crowcroft
    Unikernels: Library operating systems for the cloud 48, ACM (2013), pp. 461-472
    Google Scholar [5] M.G. Xavier, M.V. Neves, F.D. Rossi, T.C. Ferreto, T. Lange,
    C.A.F. De Rose Performance evaluation of container-based virtualization for high
    performance computing environments Proceedings of the 2013 21st Euromicro International
    Conference on Parallel, Distributed, and Network-Based Processing, PDP ’13, IEEE
    Computer Society, Washington, DC, USA (2013), pp. 233-240, 10.1109/PDP.2013.41
    View in ScopusGoogle Scholar [6] R. Morabito, J. Kjallman, M. Komu Hypervisors
    vs. lightweight virtualization: A performance comparison Proceedings of the 2015
    IEEE International Conference on Cloud Engineering (2015), pp. 386-393 CrossRefView
    in ScopusGoogle Scholar [7] S. Soltesz, H. Pötzl, M.E. Fiuczynski, A. Bavier,
    L. Peterson Container-based operating system virtualization: A scalable, high-performance
    alternative to hypervisors Proceedings of the 2Nd ACM SIGOPS/EuroSys European
    Conference on Computer Systems 2007, EuroSys ’07, ACM, New York, NY, USA (2007),
    pp. 275-287, 10.1145/1272996.1273025 View in ScopusGoogle Scholar [8] T. Bui,
    Analysis of Docker Security, 2015, ArXiv:1501.02967v1. Google Scholar [9] E. Reshetova,
    J. Karhunen, T. Nyman, N. Asokan Security of os-level virtualization technologies:
    Technical report CoRR, abs/1407.4245 (2014) Google Scholar [10] F. Lombardi, R.
    Di Pietro Virtualization and cloud security: Benefits, caveats, and future developments
    Z. Mahmood (Ed.), Cloud Computing, Springer International Publishing (2014), pp.
    237-255, 10.1007/978-3-319-10530-7_10 Google Scholar [11] R. Di Pietro, F. Lombardi
    Security for Cloud Computing Artec House, Boston (2015) Google Scholar ISBN 978-1-60807-989-6.
    [12] Docker hub: Automated builds and webhooks, 2018, https://docs.docker.com/docker-hub/builds.
    Google Scholar [13] Docker overview, 2018, https://www.docker.com/company. Google
    Scholar [14] Containers: Real adoption and use cases in 2017, http://en.community.dell.com/techcenter/cloud/m/dell_cloud_resources/20443801.
    Google Scholar [15] F. Lombardi, R. Di Pietro Secure virtualization for cloud
    computing Journal of Network and Computer Applications, 34 (4) (2011), pp. 1113-1122
    View PDFView articleView in ScopusGoogle Scholar [16] P.-H. Kamp, R.N.M. Watson
    Jails: Confining the omnipotent root Proceedings of the 2nd International System
    Administration and Networking Conference (SANE) (2010) Google Scholar http://therbelot.free.fr/Install_FreeBSD/jail/jail.pdf.
    [17] C. Zheng, D. Thain Integrating containers into workflows: A case study using
    makeflow, work queue, and docker Proceedings of the 8th International Workshop
    on Virtualization Technologies in Distributed Computing, VTDC ’15, ACM, New York,
    NY, USA (2015), pp. 31-38, 10.1145/2755979.2755984 View in ScopusGoogle Scholar
    [18] Xen wiki page about unikernels, 2018, http://wiki.xenproject.org/wiki/Unikernels.
    Google Scholar [19] I. Briggs, M. Day, Y. Guo, P. Marheine, E. Eide A performance
    evaluation of unikernels Technical Report (2014) Google Scholar [20] A. Madhavapeddy,
    T. Leonard, M. Skjegstad, T. Gazagnaire, D. Sheets, D. Scott, R. Mortier, A. Chaudhry,
    B. Singh, J. Ludlam, J. Crowcroft, I. Leslie Jitsu: Just-in-time summoning of
    unikernels Proceedings of the 12th USENIX Conference on Networked Systems Design
    and Implementation, NSDI’15, USENIX Association, Berkeley, CA, USA (2015), pp.
    559-573 View in ScopusGoogle Scholar [21] R. Dua, A. Raja, D. Kakadia Virtualization
    vs containerization to support paas Proceedings of the 2014 IEEE International
    Conference on Cloud Engineering (IC2E) (2014), pp. 610-614, 10.1109/IC2E.2014.41
    View in ScopusGoogle Scholar [22] W. Felter, A. Ferreira, R. Rajamony, J. Rubio
    An Updated Performance Comparison of Virtual Machines and Linux Containers Technical
    Report, IBM Research Report (2014) Google Scholar http://www.cs.nyu.edu/courses/fall14/CSCI-GA.3033-010/vmVcontainers.pdf.
    [23] E. Bacis, S. Mutti, S. Capelli, S. Paraboschi Dockerpolicymodules: mandatory
    access control for docker containers Communications and Network Security (CNS),
    2015 IEEE Conference on, IEEE (2015), pp. 749-750 CrossRefView in ScopusGoogle
    Scholar [24] A. Miller, L. Chen Securing your containers - an exercise in secure
    high performance virtual containers Proceedings of the International Conference
    on Security and Management (SAM), The Steering Committee of The World Congress
    in Computer Science, Computer Engineering and Applied Computing (WorldComp) (2012),
    p. 1 View PDFView articleGoogle Scholar http://worldcomp-proceedings.com/proc/p2012/SAM9702.pdf.
    [25] Z. Jian, L. Chen A defense method against docker escape attack Proceedings
    of the 2017 International Conference on Cryptography, Security and Privacy, ACM
    (2017), pp. 142-146 CrossRefView in ScopusGoogle Scholar [26] J. Chelladhurai,
    P.R. Chelliah, S.A. Kumar Securing docker containers from denial of service (dos)
    attacks Services Computing (SCC), 2016 IEEE International Conference on, IEEE
    (2016), pp. 856-859 CrossRefView in ScopusGoogle Scholar [27] J. Gummaraju, T.
    Desikan, Y. Turner Over 30% of Official Images in Docker Hub Contain High Priority
    Security Vulnerabilities Technical Report, BanyanOps (2015) Google Scholar [28]
    R. Shu, X. Gu, W. Enck A study of security vulnerabilities on docker hub Proceedings
    of the Seventh ACM on Conference on Data and Application Security and Privacy,
    ACM (2017), pp. 269-280 CrossRefView in ScopusGoogle Scholar [29] T. Lu, J. Chen,
    Research of penetration testing technology in docker environment (2017). Google
    Scholar [30] A. Mouat, Docker security using containers safely in production,
    2015. Google Scholar [31] T. Combe, A. Martin, R. Di Pietro To docker or not to
    docker: A security perspective IEEE Cloud Computing, 3 (5) (2016), pp. 54-62,
    10.1109/MCC.2016.100 View in ScopusGoogle Scholar [32] C.P. Wright, E. Zadok Kernel
    korner: Unionfs: Bringing filesystems together Linux J., 2004 (128) (2004) Google
    Scholar 8. [33] Docker image specification, 2018, https://github.com/docker/docker/blob/master/image/spec/v1.md.
    Google Scholar [34] Dockerfile reference, 2018, https://docs.docker.com/engine/reference/builder.
    Google Scholar [35] E. Biederman Multiple instances of the global linux namespaces
    Proceedings of the 2006 Linux Symposium (2006) Google Scholar https://www.kernel.org/doc/ols/2006/ols2006v1-pages-101-112.pdf.
    [36] Notes from a container, 2007, https://lwn.net/Articles/256389. Google Scholar
    [37] Linux kernel namespaces man page, 2018, http://man7.org/linux/man-pages/man7/namespaces.7.html.
    Google Scholar [38] Docker hub official repositories, 2018, https://docs.docker.com/docker-hub/official_repos.
    Google Scholar [39] J. Cappos, J. Samuel, S.M. Baker, J.H. Hartman, A look in
    the mirror: attacks on package managers, in: Proceedings of the 2008 ACM Conference
    on Computer and Communications Security, CCS 2008, Alexandria, Virginia, USA,
    October 27-31, 2008, pp. 565–574. doi:10.1145/1455770.1455841. Google Scholar
    [40] Docker store, 2018, https://docs.docker.com/docker-store. Google Scholar
    [41] Boot2docker project, 2018, http://boot2docker.io/. Google Scholar [42] Coreos
    project, 2018, https://coreos.com/docs/. Google Scholar [43] Kubernetes orchestrator,
    2018, http://kubernetes.io/. Google Scholar [44] Rancheros project, 2018, http://rancher.com/docs/os/v1.1/en/.
    Google Scholar [45] docker-default apparmor profile, 2014, https://wikitech.wikimedia.org/wiki/Docker/apparmor.
    Google Scholar [46] Docker content trust, official documentation, 2018, https://docs.docker.com/engine/security/trust/content_trust.
    Google Scholar [47] J. Samuel, N. Mathewson, J. Cappos, R. Dingledine Survivable
    key compromise in software update systems Proceedings of the 17th ACM Conference
    on Computer and Communications Security, CCS ’10, ACM, New York, NY, USA (2010),
    pp. 61-72, 10.1145/1866307.1866315 View in ScopusGoogle Scholar [48] Docker and
    ssh, 2014, https://blog.docker.com/2014/06/why-you-dont-need-to-run-sshd-in-docker.
    Google Scholar [49] Docker best practices, 2018, https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices.
    Google Scholar [50] T. Killalea The hidden dividends of microservices Communications
    of the ACM, 59 (8) (2016), pp. 42-45 CrossRefView in ScopusGoogle Scholar [51]
    Docker use-cases, 2018, https://www.airpair.com/docker/posts/8-proven-real-world-ways-to-use-docker.
    Google Scholar [52] Docker hub images, 2018, https://hub.docker.com/explore. Google
    Scholar [53] Containers are not vms, 2016, https://blog.docker.com/2016/03/containers-are-not-vms/.
    Google Scholar [54] Kubernetes installation advices, 2018, https://kubernetes.io/docs/setup/pick-right-solution/.
    Google Scholar [55] Azure container instances, 2018, https://docs.microsoft.com/en-us/azure/container-instances/.
    Google Scholar [56] Azure container services, 2018, https://docs.microsoft.com/en-us/azure/container-service/.
    Google Scholar [57] Swarm mode, 2018, https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/.
    Google Scholar [58] Use compose with swarm, 2018, https://docs.docker.com/compose/swarm/.
    Google Scholar [59] Overview of docker compose, 2018, https://docs.docker.com/compose/overview/.
    Google Scholar [60] R.S. Ross Guide for conducting risk assessments (nist sp-800-30rev1)
    The National Institute of Standards and Technology (NIST), Gaithersburg (2012)
    Google Scholar [61] Cve vulnerability statistics on docker, 2018, http://www.cvedetails.com/product/28125/Docker-Docker.html.
    Google Scholar [62] CIS Docker Benchmark, Technical Report, Center for Internet
    Security, 2018. Google Scholar [63] Cve-2014-9356, 2018, https://security-tracker.debian.org/tracker/CVE-2014-9356.
    Google Scholar [64] Docker security scanning, 2018, https://docs.docker.com/docker-cloud/builds/image-scan.
    Google Scholar [65] Novell apparmor administration guide, 2007, https://www.suse.com/documentation/apparmor/pdfdoc/book_apparmor21_admin/book_apparmor21_admin.pdf.
    Google Scholar Cited by (120) Container security: Precaution levels, mitigation
    strategies, and research perspectives 2023, Computers and Security Show abstract
    Flow based containerized honeypot approach for network traffic analysis: An empirical
    study 2023, Computer Science Review Show abstract Full-stack vulnerability analysis
    of the cloud-native platform 2023, Computers and Security Show abstract Research
    communities in cyber security vulnerability assessments: A comprehensive literature
    review 2023, Computer Science Review Show abstract On the Security of Containers:
    Threat Modeling, Attack Analysis, and Mitigation Strategies 2023, Computers and
    Security Show abstract SEAF: A Scalable, Efficient, and Application-independent
    Framework for container security detection 2022, Journal of Information Security
    and Applications Show abstract View all citing articles on Scopus View Abstract
    © 2018 Elsevier B.V. All rights reserved. Recommended articles Enriching sparse
    mobility information in Call Detail Records Computer Communications, Volume 122,
    2018, pp. 44-58 Guangshuo Chen, …, Carlos Sarraute View PDF Addressing the security
    challenges of using containers Network Security, Volume 2016, Issue 12, 2016,
    pp. 5-8 Mike Pittenger Extensible persistence as a service for containers Future
    Generation Computer Systems, Volume 97, 2019, pp. 10-20 Mohamed Mohamed, …, Heiko
    Ludwig View PDF Show 3 more articles Article Metrics Citations Citation Indexes:
    106 Patent Family Citations: 1 Captures Readers: 218 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Computer communications
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Docker ecosystem – Vulnerability Analysis
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ccwc.2019.8666479
  analysis: '>'
  authors:
  - Jay Shah
  - Dushyant Dubaria
  citation_count: 72
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2019
    IEEE 9th Annual Computin... Building Modern Clouds: Using Docker, Kubernetes &
    Google Cloud Platform Publisher: IEEE Cite This PDF Jay Shah; Dushyant Dubaria
    All Authors 66 Cites in Papers 5725 Full Text Views Abstract Document Sections
    I. Introduction II. DOCKER III. Kubernetes IV. Google Cloud Platform (GCP) V.
    Kubernetes, Docker or Both Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: To develop and build a modern cloud infrastructure
    or DevOps implementation than both Docker and Kubernetes have revolutionized the
    era of software development and operations. Although both are different, they
    unify the process of development and integration, it is now possible to build
    any architecture by using these technologies. Docker is used to build, ship and
    run any application anywhere. Docker allows the use of the same available resources.
    These containers can be used to make deployments much faster. Containers use less
    space, are reliable and are very fast. Docker Swarm helps to manage the docker
    container. Kubernetes is an automated container management, deployment and scaling
    platform. Using Google Cloud Platform to deploy containers on Kubernetes Engine
    enabling rapid application development and management. Kubernetes provides key
    features like deployment, easy ways to scale, and monitoring. Published in: 2019
    IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC) Date
    of Conference: 07-09 January 2019 Date Added to IEEE Xplore: 14 March 2019 ISBN
    Information: DOI: 10.1109/CCWC.2019.8666479 Publisher: IEEE Conference Location:
    Las Vegas, NV, USA I. Introduction Docker and Kubernetes have revolutionized the
    way of DevOps consulting [1] and both are leading container orchestration tools
    today [2]. There is always a challenge to control an increase in the demand of
    scaling and auto-healing of the network and virtual instances. Managing the containers
    is always as task for any company because microservices which are running on the
    containers do not communicate with each other. They work independently as a separate
    entity. This is where kubernetes steps in [13]. Kubernetes is nothing but a platform
    to manage containers. These containers can be docker containers or any other alternative
    containers. Kubernetes orchestrates, manages and forms a line of communication
    between these containers. This paper is divided into three sections. The first
    Section covers Docker: Containerization Using Docker, Docker for networking [4],
    The Docker File and hosting a WordPress website using Docker. In the Second Section,
    we have covered Kubernetes: The Role and Architecture of Kubernetes, basic concepts,
    features, Kubernetes clusters, scaling and deploying applications and hosting
    a Web Server Using Helm [10]. In the Final Section, we have discussed Google Cloud
    Platform, its compatibility, services, features and how Kubernetes and GCP go
    hand in hand [9]. To summarize we have mentioned the difference between the two-orchestration
    tool [24]. Sign in to Continue Reading Authors Figures References Citations Keywords
    Metrics More Like This Real-time monitoring system for containers in highway freight
    based on cloud computing and compressed sensing 2017 IEEE 17th International Conference
    on Communication Technology (ICCT) Published: 2017 The Relevance of Container
    Monitoring Towards Container Intelligence 2018 9th International Conference on
    Computing, Communication and Networking Technologies (ICCCNT) Published: 2018
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Building Modern Clouds: Using Docker, Kubernetes &amp; Google Cloud Platform'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/jlt.2018.2800660
  analysis: '>'
  authors:
  - Raül Muñoz
  - Ricard Vilalta
  - Noboru Yoshikane
  - Ramon Casellas
  - Ricardo Martínez
  - Takehiro Tsuritani
  - Ramon Casellas
  citation_count: 117
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Journals & Magazines >Journal of Lightwave Technology >Volume: 36 Issue:
    7 Integration of IoT, Transport SDN, and Edge/Cloud Computing for Dynamic Distribution
    of IoT Analytics and Efficient Use of Network Resources Publisher: IEEE Cite This
    PDF Raül Muñoz; Ricard Vilalta; Noboru Yoshikane; Ramon Casellas; Ricardo Martínez;
    Takehiro Tsuritani; Itsuro Morita All Authors 114 Cites in Papers 4939 Full Text
    Views Abstract Document Sections I. Introduction II. Related Work III. Requirements
    for IoT Services and Possible Solutions IV. SDN-Enabled Container-Based Edge Node
    V. Proposed IoT-Aware SDN and Cloud Orchestration Architecture Show Full Outline
    Authors Figures References Citations Keywords Metrics Abstract: Internet of Things
    (IoT) requires cloud infrastructures for data analysis (e.g., temperature monitoring,
    energy consumption measurement, etc.). Traditionally, cloud services have been
    implemented in large datacenters in the core network. Core cloud offers high-computational
    capacity with moderate response time, meeting the requirements of centralized
    services with low-delay demands. However, collecting information and bringing
    it into one core cloud infrastructure is not a long-term scalable solution, particularly
    as the volume of IoT devices and data is forecasted to explode. A scalable and
    efficient solution, both at the network and cloud level, is to distribute the
    IoT analytics between the core cloud and the edge of the network (e.g., first
    analytics on the edge cloud and the big data analytics on the core cloud). For
    an efficient distribution of IoT analytics and use of network resources, it requires
    to integrate the control of the transport networks (packet and optical) with the
    distributed edge and cloud resources in order to deploy dynamic and efficient
    IoT services. This paper presents and experimentally validates the first IoT-aware
    multilayer (packet/optical) transport software defined networking and edge/cloud
    orchestration architecture that deploys an IoT-traffic control and congestion
    avoidance mechanism for dynamic distribution of IoT processing to the edge of
    the network (i.e., edge computing) based on the actual network resource state.
    Published in: Journal of Lightwave Technology ( Volume: 36, Issue: 7, 01 April
    2018) Page(s): 1420 - 1428 Date of Publication: 31 January 2018 ISSN Information:
    DOI: 10.1109/JLT.2018.2800660 Publisher: IEEE Funding Agency: I. Introduction
    IT is envisioned that the Internet of Things (IoT) will connect billions of heterogeneous
    devices (ranging from complex interactive systems to tiny sensors) to the transport
    network using widely deployed wireless access technologies (e.g., Bluetooth, ZigBee,
    Wi-Fi, LoRa), mobile technologies (e.g., GPRS/2G or eMTC and NB-IoT) or fixed
    access technologies (e.g., PLC, ADSL, optical Access, Ethernet). It will facilitate
    a wide variety of use cases from different vertical industries, such as automotive
    and mobility, healthcare, energy, media and entertainment, factories of the future,
    or energy. Sign in to Continue Reading Authors Figures References Citations Keywords
    Metrics More Like This On-Demand Provisioning of Wearable Sensors Data Processing
    Services in Edge Computing 2023 IEEE SENSORS Published: 2023 Container-based edge
    computing data processing mechanism for digital grid 2021 IEEE 3rd International
    Conference on Frontiers Technology of Information and Computer (ICFTIC) Published:
    2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: Journal of lightwave technology
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Integration of IoT, Transport SDN, and Edge/Cloud Computing for Dynamic Distribution
    of IoT Analytics and Efficient Use of Network Resources
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/mnet.2016.1500244rp
  analysis: '>'
  authors:
  - Tarik Taleb
  - Adlen Ksentini
  - Riku Jäntti
  citation_count: 85
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Network >Volume: 30 Issue: 6 "Anything as a Service" for 5G Mobile Systems
    Publisher: IEEE Cite This PDF Tarik Taleb; Adlen Ksentini; Riku Jantti All Authors
    84 Cites in Papers 2338 Full Text Views Abstract Document Sections Introduction
    Anything as a Service: Concept Description ANYaas: TOFaaS and CDNaas Use Case
    ANYaaS: VNF Performance Challenges and Solutions Implementation Issues Conclusion
    Authors Figures References Citations Keywords Metrics Footnotes Abstract: 5G network
    architecture and its functions are yet to be defined. However, it is generally
    agreed that cloud computing, network function virtualization (NFV), and software
    defined networking (SDN) will be key enabling technologies for 5G. Indeed, putting
    all these technologies together ensures several advantages in terms of network
    configuration flexibility, scalability, and elasticity, which are highly needed
    to fulfill the numerous requirements of 5G. Furthermore, 5G network management
    procedures should be as simple as possible, allowing network operators to orchestrate
    and manage the lifecycle of their virtual network infrastructures (VNIs) and the
    corresponding virtual network functions (VNFs), in a cognitive and programmable
    fashion. To this end, we introduce the concept of “Anything as a Service” (ANYaaS),
    which allows a network operator to create and orchestrate 5G services on demand
    and in a dynamic way. ANYaaS relies on the reference ETSI NFV architecture to
    orchestrate and manage important services such as mobile Content Delivery Network
    as a Service (CDNaaS), Traffic Offload as a Service (TOFaaS), and Machine Type
    Communications as a Service (MTCaaS). Ultimately, ANYaaS aims for enabling dynamic
    creation and management of mobile services through agile approaches that handle
    5G network resources and services. Published in: IEEE Network ( Volume: 30, Issue:
    6, November-December 2016) Page(s): 84 - 91 Date of Publication: 19 October 2016
    ISSN Information: DOI: 10.1109/MNET.2016.1500244RP Publisher: IEEE Introduction
    5th generation mobile networks (5G), also referred to as beyond 2020 mobile communications
    systems, represent the next major phase of the mobile telecom industry, going
    beyond the current Long Term Evolution (LTE) and IMT-advanced systems. In addition
    to increased peak bit rates, higher spectrum spectral efficiency, better coverage,
    and the support of potential numbers of diverse connectable devices, including
    machine type communications (MTC) devices, 5G systems are required to be cost-efficient,
    flexibly deployable, elastic, and above all programmable. The need to lower mobile
    infrastructure costs and render their deployment flexible and elastic has become
    critical for the sustainability of mobile operators worldwide, mainly in light
    of the ever growing mobile data traffic on one hand and the stagnant (rather falling)
    average revenue per user (ARPU) on the other hand. With current mobile network
    designs, such required flexibility and elasticity are all but impossible to realize,
    particularly due to the traditional usage of specific-purpose networking equipment
    that can neither dynamically scale with mobile traffic nor be easily upgraded
    with new functions. Sign in to Continue Reading Authors Figures References Citations
    Keywords Metrics Footnotes More Like This A Comprehensive Survey of RAN Architectures
    Toward 5G Mobile Communication System IEEE Access Published: 2019 MonPaaS: An
    Adaptive Monitoring Platformas a Service for Cloud Computing Infrastructures and
    Services IEEE Transactions on Services Computing Published: 2015 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE network
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: '"Anything as a Service" for 5G Mobile Systems'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
