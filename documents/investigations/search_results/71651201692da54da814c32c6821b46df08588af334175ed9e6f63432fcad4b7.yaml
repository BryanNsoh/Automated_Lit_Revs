- analysis: '>'
  authors:
  - Walia G.K.
  - Kumar M.
  - Gill S.S.
  citation_count: '6'
  description: The proliferation of ubiquitous Internet of Things (IoT) sensors and
    smart devices in several domains embracing healthcare, Industry 4.0, transportation
    and agriculture are giving rise to a prodigious amount of data requiring ever-increasing
    computations and services from cloud to the edge of the network. Fog/Edge computing
    is a promising and distributed computing paradigm that has drawn extensive attention
    from both industry and academia. The infrastructural efficiency of these computing
    paradigms necessitates adaptive resource management mechanisms for offloading
    decisions and efficient scheduling. Resource Management (RM) is a non-trivial
    issue whose complexity is the result of heterogeneous resources, incoming transactional
    workload, edge node discovery, and Quality of Service (QoS) parameters at the
    same time, which makes the efficacy of resources even more challenging. Hence,
    the researchers have adopted Artificial Intelligence (AI)-based techniques to
    resolve the above-mentioned issues. This paper offers a comprehensive review of
    resource management issues and challenges in Fog/Edge paradigm by categorizing
    them into provisioning of computing resources, task offloading, resource scheduling,
    service placement, and load balancing. In addition, existing AI and non-AI based
    state-of-the-art solutions have been discussed, along with their QoS metrics,
    datasets analysed, limitations and challenges. The survey provides mathematical
    formulation corresponding to each categorized resource management issue. Our work
    sheds light on promising research directions on cutting-edge technologies such
    as Serverless computing, 5G, Industrial IoT (IIoT), blockchain, digital twins,
    quantum computing, and Software-Defined Networking (SDN), which can be integrated
    with the existing frameworks of fog/edge-of-things paradigms to improve business
    intelligence and analytics amongst IoT-based applications.
  doi: 10.1109/COMST.2023.3338015
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Communications Surveys
    &... >Volume: 26 Issue: 1 AI-Empowered Fog/Edge Resource Management for IoT Applications:
    A Comprehensive Review, Research Challenges, and Future Perspectives Publisher:
    IEEE Cite This PDF Guneet Kaur Walia; Mohit Kumar; Sukhpal Singh Gill All Authors
    7 Cites in Papers 747 Full Text Views Abstract Document Sections I. Introduction
    II. AI-Driven Resource Management in Fog/Edge III. Review Methodology IV. Exploring
    Future Research Directions: Demystifying Thrust Technology in Integration With
    Fog/Edge V. Results and Analysis Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: The proliferation of ubiquitous Internet
    of Things (IoT) sensors and smart devices in several domains embracing healthcare,
    Industry 4.0, transportation and agriculture are giving rise to a prodigious amount
    of data requiring ever-increasing computations and services from cloud to the
    edge of the network. Fog/Edge computing is a promising and distributed computing
    paradigm that has drawn extensive attention from both industry and academia. The
    infrastructural efficiency of these computing paradigms necessitates adaptive
    resource management mechanisms for offloading decisions and efficient scheduling.
    Resource Management (RM) is a non-trivial issue whose complexity is the result
    of heterogeneous resources, incoming transactional workload, edge node discovery,
    and Quality of Service (QoS) parameters at the same time, which makes the efficacy
    of resources even more challenging. Hence, the researchers have adopted Artificial
    Intelligence (AI)-based techniques to resolve the above-mentioned issues. This
    paper offers a comprehensive review of resource management issues and challenges
    in Fog/Edge paradigm by categorizing them into provisioning of computing resources,
    task offloading, resource scheduling, service placement, and load balancing. In
    addition, existing AI and non-AI based state-of-the-art solutions have been discussed,
    along with their QoS metrics, datasets analysed, limitations and challenges. The
    survey provides mathematical formulation corresponding to each categorized resource
    management issue. Our work sheds light on promising research directions on cutting-edge
    technologies such as Serverless computing, 5G, Industrial IoT (IIoT), blockchain,
    digital twins, quantum computing, and Software-Defined Networking (SDN), which
    can be integrated with the existing frameworks of fog/edge-of-things paradigms
    to improve business intelligence and analytics amongst IoT-based applications.
    Published in: IEEE Communications Surveys & Tutorials ( Volume: 26, Issue: 1,
    Firstquarter 2024) Page(s): 619 - 669 Date of Publication: 30 November 2023 ISSN
    Information: DOI: 10.1109/COMST.2023.3338015 Publisher: IEEE SECTION I. Introduction
    Over the past three decades, applications characterized by varying workloads and
    substantial datasets have been the driving force behind transformative developments
    in distributed computing. This computing paradigm gained prominence due to its
    capability to cater to both compute and data-intensive tasks, driven by its inherent
    characteristics such as fault tolerance, resource sharing, load balancing, robustness,
    scalability etc. However, there are challenges, including comprising data movement
    overhead, synchronization and the complexity involved in handling data distribution
    and communication amongst nodes, which makes it unsuitable for high-performance
    scientific and engineering applications. After that, High-Performance Computing
    (HPC) has been introduced to address the mentioned issues, and it plays a vital
    role in power systems by optimizing grid control, cost minimization, reducing
    losses and transmission investment planning [1]. This paradigm provides high-quality
    solutions within reasonable time, primarily due to its ability to deliver high
    computational performance. Market-driven advanced computing systems have strategically
    shifted from HPC to High Throughput Computing (HTC). This transition aims to enhance
    not only processing speed but also to address critical issues like cost efficiency,
    energy savings, system reliability, and security [2]. Nonetheless, the escalating
    trend of data sharing across networks makes it imperative to employ programming
    models for executing programs across multiple distributed infrastructures. In
    order to manage the same, leveraging Virtual Machines (VMs) and virtualization
    has proven to be crucial in effectively handling the growing data quantities within
    distributed infrastructure. Advances in virtualization have paved the way for
    the emergence of Internet clouds as a novel paradigm. Significantly, the advancement
    of technologies like Radio Frequency Identification (RFIDs), Global Positioning
    System, and various sensors has catalyzed the emergence of the Internet of Things
    (IoT). A. Unfolding Emerging Computing Paradigm With digitization revolutionizing
    the world at an expeditious rate, IoT is emerging as a broad and multifaceted
    term encompassing several components and protocols, leading to a dominant technological
    shift. This rapid evolution is being utilized in domains such as smart factories,
    structural healthcare, smart cities, smart transportation, supply chain control,
    intelligent shopping applications, smart agriculture and many more in the form
    of autonomous IoT applications [3]. This paradigm shift has empowered self-driving
    drones to carry out home deliveries of groceries, enabled healthcare experts to
    conduct continuous health monitoring through wearable sensors, facilitated real-time
    monitoring of equipment and processes in supply chain management, and many more.
    The predictions regarding the potential influence of IoT are indeed remarkable.
    The term “IoT” is defined as a network of physical objects which comprises embedded
    technologies to communicate, sense and perceive data from their external environment.
    The IoT applications mainly consist of three components: things also known as
    devices, insights and actions. The devices are embedded with sensors or actuators.
    The sensors detect changes in the ambient conditions or in the state of the system
    and forward this information to the designated destination. In other words, this
    paradigm aims at building a smart environment by employing smart devices that
    autonomously generate data and transmit it via the Internet to facilitate decision-making
    [4]. Finally, Business Intelligence (BI) is utilized to draw appropriate actions
    from the insights. However, this paradigm imposes a few challenges such as: (1)
    Security; (2) Energy-efficiency; (3) Data storage and analytics and (4) Resource-constrained.
    To illustrate these challenges, consider smart devices such as RFID tags used
    for asset identification and tracking in the Industry 4.0 scenario [5]. Smart
    manufacturing aims at predicting future conditions of manufacturing to ameliorate
    asset management and quality control of manufactured equipment. For example, NIROTech
    is a manufacturing company specializing in access and safety control devices for
    homes. Such devices include surveillance cameras, biometric and facial recognition
    door locks, fire alarms etc. These devices become susceptible to attacks by malicious
    users as smart home services are provided over a wireless network. Hence, it becomes
    important to preserve the integrity (prevent the insertion of malignant software
    applications into the IoT network, which might change the service purpose), availability
    (prevention against injecting fabricated data which has the ability to overload
    device causing financial losses) and authentication (which implies safeguarding
    the service environment from Denial of Service (DoS) Attack, Distributed DoS,
    and personnel information leakage etc.) aspects of a safe home [6]. The security
    aspect of Autonomous Vehicles (AVs) in Intelligent Transportation Systems (ITS)
    is considered, which interact with other vehicles and Road Infrastructure Units
    (RSUs) using telecommunication technology. AVs are now being embraced due to their
    driverless nature, fuel efficiency (AVs are capable of travelling at high speed
    because of Intelligence and quick sensors), adaptive behavior, remote monitoring
    and control, optimal path planning etc. The connected AVs contain sensors such
    as Light Detection and Ranging (LiDAR), Inertial Measurement Unit (IMU), Radio
    Detection and Ranging (Radar), GPS, cameras, thermal imaging etc., along with
    connection mechanisms (cellular connections, Wi-Fi, Bluetooth etc.). These components
    enable AVs to navigate efficiently in an environment by identifying obstacles.
    The information shared from vehicle sensors to their peer vehicles, or RSUs is
    vulnerable to being exploited by illegitimate users, raising concerns about data
    security and privacy in connected ITS. These unauthorized users have the potential
    to access an AV through various entry points, including USB connections, Bluetooth
    technology, in-car navigation systems, and other monitoring components. Hence,
    it becomes challenging to manage such a huge amount of data from potential attacks.
    Generally, such users conduct two types of attacks: spoofing and jamming [7].
    Spoofing attacks include radar spoofing and GPS spoofing, in which counterfeit
    data is fed to AVs, aiming to gain significant control over the system’s behavior.
    It might drag the AV in the wrong direction. Sometimes attackers send blocking
    signals which prevent AVs from receiving authentic information from their counterparts,
    which constitutes jamming. Therefore, it becomes important for developers and
    manufacturers to develop strategies for mitigating the dangers associated with
    such attacks to reduce such cyberattacks. To understand the energy perspective
    of IoT devices, consider the scenario of Industrial IoT (IIoT), which comprises
    devices such as Computational RFID (CRFID) tags, ZigBee or LoRa-based sensors.
    These devices generate a tremendous amount of data and signals, which are used
    for controlling, sensing, predictive maintenance, and data analysis [8]. However,
    the communication and computation tasks of these “things” consume a substantial
    amount of energy, leading to a carbon footprint. For example, smart grids incorporate
    a large number of sensors that autonomously report their information to grid infrastructure.
    The sensing and communication tasks consume a lot of energy, that ultimately impacts
    the lifetime of smart grids [9]. In addition, executing task on optimal destination
    becomes significant as it incurs a different amount of computation and communication
    costs [10]. Another challenge is that IoT devices are generally energy-constrained
    since they are powered by batteries. To increase the lifetime of IoT devices,
    battery replacements and recharging don’t serve as an optimal solution due to
    increased cost, and in some situations, the location of IoT devices might be inaccessible
    [11]. Last but not least, the substantial mobility exhibited by dynamic autonomous
    vehicles leads to increased fluctuations in the Received Signal Strength Indicator
    (RSSI) at the base station for wireless connections, which results in increased
    energy dissipation [12]. Therefore, addressing energy efficiency concerns becomes
    essential for achieving long-term sustainability in real-life deployments of IoT
    use cases. IoT devices generate both structured (numbers and values) and unstructured
    (text, video and audio files) types of data in massive amounts. As per an estimation
    by Cisco, 500 billion devices will fall under the expansive IoT paradigm, accumulating
    as much as 79.4 ZB of data by the year 2030 [13]. This necessitates finding suitable
    solutions for data storage and processing since the physical resources of these
    devices are limited in terms of power, memory, and computational resources [14].
    However, IoT devices execute one task at a time, and in some scenarios, the resource
    requirements of incoming applications cannot be merely served by IoT paradigm.
    For instance, meeting user demands while viewing video content of emergency patients
    in vehicles (such as ambulances) is challenging considering the resource-constrained
    nature of IoT devices. Therefore, considering all these challenges, it becomes
    evident that a single computing paradigm is not sufficient to address all the
    needs of consumers or IoT devices. It further necessitates the incorporation of
    appropriate resource management strategies via Artificial Intelligence (AI) leveraging
    the computation capabilities of other emerging paradigms to make it worthwhile
    in real-world scenarios. The processing and analysis of compute-intensive IoT
    devices’ data requires high-end storage, network, and computational capabilities,
    which can be accomplished by utilizing resource-rich cloud infrastructure [15].
    Cloud computing is possibly the most impoverished paradigm, which evolves from
    the capability to harness utility computing, enabling pay-as-you-go models, parallel
    computation, load balancing, and the data-intensive nature of tasks [16]. Its
    ability to endorse a Service-Oriented Architecture (SOA) empowers it to create,
    incorporate, and generate new services by seamlessly integrating with existing
    ones. This results in the provisioning of services at three distinct levels: Platform-as-a-Service
    (PaaS), Infrastructure-as-a-Service (IaaS), and Software-as-a-Service (SaaS) [17],
    leading towards a terminology called Everything-as-a-service (XaaS) [18]. A new
    computing paradigm has emerged, referred to as the Cloud of Things (CoT), which
    transforms ubiquitous computing and allows the utilization of cloud architecture
    in the processing and analysis of extensive IoT data [19], [20]. The combination
    of both technologies will provide robust, seamless, and agile services for Next-Generation
    Networks (NGNs) [21]. In CoT, dynamic provisioning of underlying resources and
    the creation of VMs on demand are the key solutions for managing physical machine
    resources [22]. Nonetheless, it results in engaging the limited resources of the
    host machine. CoT envisions device management, including brokering messages between
    devices and the cloud. Message Queuing Telemetry Transport (MQTT) works as a broker
    that enables machine-to-machine (M2M) communication by enabling publish/subscribe
    services [23]. Data generated by devices is published, and data in the cloud-IoT
    will subscribe to it. Containers, on the other hand, are gaining prominence in
    multi-cloud platforms to manage and orchestrate applications into portable containers,
    especially in PaaS models [24]. Within no time, CoT has started harnessing Containers-as-a-Service
    (CaaS) to provide services in the fields of transportation and Next-Generation
    Sequencing (NGS) bioinformatics [25]. Furthermore, it motivates the computation
    and processing of big data residing in cloud environments by efficiently processing
    spark jobs and hence, improving workload makespan as compared to traditional virtualization
    [26]. An integral approach involving the synergy of both technologies is going
    to be embraced by future cloud architectures. Despite the predominance of the
    CoT paradigm, it faces numerous challenges in hosting real-time IoT applications.
    Processing IoT requests at the cloud layer results in high latency due to bandwidth
    constraints, making it inadequate to cater the demands of real-time applications.
    This issue has been resolved by fog computing, which brings the computational
    capability of its underlying resources within close proximity of the end-user.
    It leverages cloud infrastructure in a decentralized manner, placing storage,
    computing, and processing components at the edge of the network [27]. Instead
    of being a replacement for the cloud, this computing paradigm introduced by Cisco
    acts as a complement to the existing framework. Fog/Edge computing emphasizes
    processing data in close proximity to the service-consumer, bringing new advantages
    such as greater context-awareness amongst nodes, real-time data processing, lower
    bandwidth consumption, and so on [28]. This computing paradigm makes the cloud
    truly distributed. However, IoT devices are battery-driven and resource-constrained,
    which elicits the need to incorporate other emerging paradigms such as fog and/or
    edge, serverless, and quantum computing to enrich their resource capabilities
    and make them capable of incorporating business logic, which is responsible for
    running lightweight computation. Resource management in the emerging computing
    paradigm for real-time applications is a critical issue due to the incoming transactional
    workload (of a complex and diverse nature), scalability across data centers, and
    last but not least, fluctuating interactions and managing Service Level Agreements
    (SLAs) along with QoS parameters [29]. In contrast to the cloud, resource allocation
    in fog computing is more complicated because workloads are distributed among fog
    nodes due to its de-centralized architecture. Furthermore, determining the suitable
    destination node for executing incoming IoT workloads is also a critical consideration.
    For example, the latency-sensitive data collected is analyzed at the edge of the
    network, whereas if the application thrives for high computation service and storage,
    then the data is offloaded to the cloud for processing and analysis. Hence, the
    problem of resource management in such a collaborative environment thrives for
    leveraging compute, storage, networking, and intelligence capabilities from resource-enriched
    cloud and fog layers. This eventually calls for integrating the intelligence tier,
    which runs machine learning and deep learning models at the cloud layer while
    inferencing is carried out at the intermediate fog layer. This approach has now
    been envisioned as a potential platform for ameliorating services in smart cities,
    smart industries, connected vehicles, UAVs, Wireless Sensors, and Actuators Networks
    (WSANs), further boosting the development of advanced applications. Figure 1 depicts
    the evolution of various prominent computing paradigms with a timeline along with
    their objectives, limitations, and research challenges. The subsequent pointers
    highlight various emerging paradigms such as Fog of Things (FoT), Edge of Things
    (EoT) and most importantly, the hybrid computing paradigm, which provides a collaborative
    framework for various layers to work in a harmonized manner. Fig. 1. A Timeline
    of the Evolution of Computing Paradigms. Show All 1) Fog of Things: Fog computing
    exploits its virtualization characteristics in order to deliver network, storage,
    and computing resources to end-users at the network edge. It offers services to
    latency-sensitive IoT applications with tolerable delay. Fog computing, also known
    as fogging, is an architectural framework that utilizes edge resources usually,
    but not entirely situated at the edge of the underlying network [27]. These nodes
    are designed to carry out a substantial number of local computing and storage
    tasks while also managing data routing over the network. Delay-sensitive applications,
    for instance, Mobile Augmented Reality (MAR), thrive on up-to-date data processing
    and computational requirements, which cannot be fulfilled with a cloud scenario
    [30]. Hence, to ensure seamless transmission of real-time gaming data, the fog
    paradigm provides a competent solution. Furthermore, offloading IoT data over
    fog optimizes Quality of Service (QoS) parameters such as latency, energy consumption,
    reliability, throughput, and bandwidth utilization. Furthermore, if the edge device
    sends latency-sensitive data to the cloud for analysis and waits for appropriate
    action, then it might result in unwanted delays due to its geographically distant
    nature as comparison with the fog/edge node. To overcome the mentioned limitations,
    fog computing provides limited processing, computation capability and storage
    services closer to the end user. Appropriate placement of services under IoT applications
    for execution in a fog or cloud environment is challenging, which often leads
    to inappropriate distribution of workload amongst VMs. To ensure optimal QoS and
    Quality-of-Experience (QoE), efficient distribution and tuning of IoT application
    workloads amongst Fog Nodes (FN), is complicated because of the distributed and
    heterogeneous architecture of resources in FNs [31]. 2) Edge of Things: As data
    is increasingly generated at the network edge, the most efficient processing of
    the data can be done on the edge device itself. The downstream flow of data is
    triggered by cloud datacenters, whereas the upstream data flow is managed on behalf
    of IoT devices [32]. In addition, moving massive amounts of potentially useless
    raw data to the cloud is expensive to transmit and store, and can even disable
    a network [33]. It introduces a heavy load on network transmission bandwidth,
    leading to data latency and backhaul, which makes it unfit for real-time applications,
    especially in the automotive industry. To exemplify, edge computing plays a vital
    role in the domain of vehicular networks by enabling a smooth exchange of data
    amongst vehicles and coordinating uniform flow to enrich user proficiency [34].
    The rationale of edge computing lies in the fact that computation will happen
    in proximity to the data source. Edge computing is an unprecedented term that
    planted its seed in enhancing the speech recognition process in mobile devices
    under finite resources by computational offloading to a proximate server [35].
    In the edge paradigm, content is pushed out geographically so that the data is
    more readily available to network clients with low latency. The compute capacity
    amongst edge servers and devices is introduced in distinctive environments, including
    warehouses, retail stores, banks, etc., where communication is enabled via the
    5G network. The edge nodes prompt the 5G network by ensuring latency (one millisecond
    or less). Despite improved services in comparison to CoT, this paradigm witnesses
    a few drawbacks, like seamless migration of workloads, limited storage, heat dissipation,
    battery life and the limited computational power of mobile devices in an IoT environment.
    To deal with it, a cyber-foraging framework has been proposed [36]. This technique
    empowers communication by offloading the data to powerful datacenters residing
    in the cloud or to edge nodes lying in proximity. 3) Fog vs Edge: Edge and fog
    are frequently used interchangeably by most researchers; however, it’s important
    to note that edge processes the task at the device itself whereas fog processes
    the request on near-end devices such as smart routers, gateways and network switches.
    The decision to incorporate a fog layer within a particular SOA rests entirely
    with the service provider, which is influenced by factors like application type,
    network architecture, data characteristics, and the location of essential network
    tools and resources. Although both fog and edge work towards leveraging storage
    and computational capabilities closer to the end user instead of pushing them
    to cloud datacenters, they still differ from one another in the following context:
    (1) Where does data processing take place? It happens at the network edge or device
    itself from the point of data generation in edge computing, but in the case of
    fog computing, the processed data is relocated to processors connected to a Local
    Area Network (LAN) relatively farther from sensors, gateways, and actuators. (2)
    possession of processing and storage capabilities? Fog nodes are comparatively
    more powerful than edge nodes. (3) On which layer do they work? Where edge computing
    emphasizes edge devices, fog computing sways at the infrastructure level. 4) Hybrid
    Computing Paradigm: Modern research is trending in the direction of exploiting
    the collaborative layered architecture of cloud-fog/edge computing in order to
    cater to delay-sensitive and compute-intensive tasks, as depicted in Figure 2.
    Some prominent application areas include AR/VR gaming, 24x7 video surveillance,
    maritime engineering, electronic health and activity tracking, autonomous vehicle
    management and Wireless Sensors and Actuator [37], [38]. This collaborative framework
    fits well for such applications where each layer performs distinctive functions,
    as explained below: Fig. 2. Hybrid Computing Paradigm. Show All Collaborative
    Cloud-Fog/Edge-IoT: It constitutes the following layers: Perception Layer: The
    IoT ecosystem encompasses a wide range of components, such as sensors, and mobile
    IoT devices like smartphones, smartwatches and consumer electronics, as well as
    household appliances like refrigerators, televisions, microwaves, and ovens. These
    devices gather and transmit data from their operational surroundings. This data
    generation spawns some tasks that require a timely response. For instance, these
    tasks can be categorized into hard and soft deadline-based tasks Fog Layer: This
    layer is characterized by features such as location awareness, low latency, support
    for mobility, heterogeneity and interoperability. Apart from providing a decentralized
    and ubiquitous form of computing, this layer acts as a platform to harmonize coordination
    amongst heterogeneous fog nodes and the programmability of networking resources,
    to name a few [40]. The fog nodes utilize the capabilities of cellular base stations,
    network routers, Wi-Fi gateways, etc., to perform operations such as managing
    and analyzing data and other time-sensitive actions in close proximity to the
    device user. In addition to enhancing the service response time, its capability
    to process data saves network bandwidth by reducing the need to upload it to the
    cloud every time. Cloud Layer: This layer enables ubiquitous, on-demand access
    to a shared pool of configurable computing resources that can be provisioned and
    scaled as per application needs. Despite this, IoT applications require genuine
    response services, which cannot be fulfilled by the cloud. However, its limitless
    computational capabilities make it an irreplaceable choice for catering to compute-sensitive
    applications such as social networking, video conferencing, and so forth. Therefore,
    we consider this framework to explore various issues relating to resource management
    in the resource-constrained fog layer and the computationally equipped cloud layer.
    Serverless Computing: The cloud layer is considered the optimal destination for
    hosting serverless computing due to its predominant characteristics which include
    centralized architecture, elasticity and scalability etc. However, fog nodes can
    communicate with serverless functions deployed on the cloud, thus acting as event
    sources or consumers [41]. Quantum Computing (QC): It is a fascinating and powerful
    technology that promises to revolutionize a considerable range of intellectual
    and economic factors in our society. In contrast to classical computing, this
    paradigm is based on quantum mechanics, which uses Qubits that can be in superpositions
    of both states at the same time [42]. Although the cloud layer is considered best
    for QC, quantum computations and insights can be leveraged by fog nodes and IoT
    devices. The in-depth incorporation of QC is depicted in Section IV. B. Google
    Trends The Google trends depict the pattern of emerging computing paradigms over
    the past six years (2018 to the present), which is visually presented in Figure
    3. It has been observed that Fog, Edge, and AI/Machine Learning are in vogue for
    integration with the cloud computing paradigm. Our study investigates recent research
    trends based on the state-of-the-art for newly fangled thrust technology, including
    blockchain, 5G, quantum computing, and many more. It is observed that the usage
    and practical applicability of the term quantum computing are still at an infancy
    stage. Only a few works have presented its usage, which identifies it as one of
    the future potential research directions. However, terminology such as blockchain,
    5G, and the IoT is taking the lead as compared to other emerging technologies.
    Also, the escalating trend amongst the cutting-edge technologies of fog and edge
    showcases the integration of these technologies in the era of modern computing
    paradigms. Fig. 3. Google Trends of Emerging Paradigms during last 6 years. Show
    All C. Motivation for Conducting the Survey With the advancements in the sphere
    of technology, new frontiers harnessing cloud computing have come up, encompassing
    fog and edge computing, whose intent resides in extending the cloud’s compute,
    storage, and ubiquitous characteristics nearer to IoT devices or mobile users.
    The existing literature lacks a clearly defined vision and concept of the emerging
    computing paradigm, resulting in a restricted comprehension of the role of AI
    in this domain. AI includes techniques such as metaheuristics, Machine Learning
    (ML), Deep Learning (DL), and Reinforcement Learning (RL) under its umbrella,
    which can automate and reshape the existing traditional resource management capabilities
    to reach new heights. For instance, the decision to map and manage the execution
    of tasks on fog nodes is challenging as multiple incoming tasks might compete
    for the same resources, and the time slot desired by one IoT application task
    might be occupied by another application. Hence, to resolve such issues, AI provides
    an adaptive system that is programmed intelligently to handle the dynamic workload
    requirements of incoming applications. Keeping all these factors in mind, this
    paper explores the architectural framework of emerging paradigms and provides
    insights into incorporating AI components with computing paradigms such as Fog-of-Things
    and Edge-of-Things. This review tries to familiarize its readers with the various
    state-of-the-art integrating technologies (AI-employed and non-AI-employed). This
    includes an exploration of research objectives, advantages, disadvantages, and
    a comparative analysis of QoS metrics. Despite the significance of utilizing AI
    techniques in fog/edge computing, we have found few surveys that emphasize the
    efforts made in the management of resources. One of the considerable studies has
    been done by Ghobaei-Arani et al. [43]. This survey presents resource management
    problems in taxonomical form, which is divided into six categories. It includes
    application placement at the appropriate destination node, task scheduling, provisioning
    of resources, allocation, load balancing, and task offloading. In-depth studies
    have been done in all categorized areas, along with discussions of open issues.
    However, this survey doesn’t provide research trends for integrating thrust technologies
    like serverless computing, blockchain, quantum computing and Software-Defined
    Networking (SDN). Abdulkareem et al. [44], have studied the role of ML approaches
    in addressing the problems of resource management pertaining to fog computing.
    The paper highlights the application areas, challenges, and open issues covering
    the security aspect. Nevertheless, the work discussed ignores the classification
    of resource management techniques. Other survey parameters like taxonomy and QoS-based
    comparison are not mentioned. Also, a comprehensive study done by Hong and Varghese
    [45] presents an architectural classification for effective management of resources
    in Fog/Edge. It discusses future research perspectives to address various challenges.
    However, they have ignored some topics such as resource provisioning, resource
    scheduling, allocation, etc. QoS parameters regarding algorithmic categorization
    of resource management have also been ignored. Another review by Deng et al. [46]
    addresses the convergence of edge computing with AI. The work presents two-faced
    objectives: using AI for edge computing and on the other side, using AI on edge
    computing environments. However, it only discusses the concept of task offloading
    along with mobility management using AI techniques. Reviewing the resource provisioning
    and allocation schemes in lieu of studying their efficacy to reinforce both static
    and dynamic IoT applications in fog computing has been done by Martinez et al.
    [47]. They presented four distinct phases for the implementation of fog infrastructure
    and highlighted some current challenges and future directions. Although it provides
    an in-depth study of the necessary steps vital for the practical implementation
    of infrastructure, it ignores important resource management concepts such as scheduling
    underlying resources, offloading IoT-based tasks and load balancing. Furthermore,
    Nayeri et al. [48] have surveyed the role of AI-based solutions for placing application
    workload in fog computing in the form of a taxonomy of AI algorithms (ML, evolutionary
    and combinational). However, they have studied only service placement techniques
    without categorizing them, and other important aspects of resource management
    such as resource provisioning, resource allocation, service placement and load
    balancing have not been scrutinized. Also, no discussion has been done regarding
    new-fangled technologies like SDN, serverless computing, quantum computing, blockchain
    etc. A study by Shakarami et al. [49] presents a systematic review by proposing
    a classification for resource provisioning. However, the work has been divided
    into five classes based on framework, heuristic or meta-heuristic, model-based,
    ML and game theory. Still, other important prospects for resource management have
    not been discussed. In another study done by Bushra et al. [50], QoS-based comparisons
    are evaluated in fog and IoE environments. The author describes the role of heuristic
    and metaheuristic algorithms in resource allocation and task scheduling. Nevertheless,
    it has not considered all aspects of resource management. Considering some recent
    research, Zhang and Tao [51] endow the concept of Artificial Intelligence of Things
    (AIoT) in a cloud-fog-edge environment. However, the importance of resource management
    in resource-constrained IoT applications is not discussed. Another study by Lin
    and Zhao [52] highlights the issue of Resource Management (RM) in wireless communication
    networks. But it doesn’t discuss the implications of the proposed work as real-life
    case studies relating to self-driving cars, smart logistics, the role of Collaborative
    Robots (COBOTS) in Industry 5.0 etc. AI-augmented technology integration and its
    impact are discussed in the Fog/Edge paradigm in context with resource management.
    But some important aspects of resource management, such as task offloading, which
    leverages IoT applications by incorporating orchestrators into edge devices, are
    ignored [53]. The work done by Gill et al. [42] provides the concept of quantum
    computation in lieu of managing large amounts of data and describes how quantum
    computing solutions will take over the existing world’s economy. But nevertheless,
    other cutting-edge technologies such as blockchain, Digital Twins, Federated Learning
    (FL), and Industrial IoT (IIoT) have not been discussed. In another study by Su
    et al. [54] the author emphasizes training and inference at the edge, utilizing
    FL to train models. The study remains confined to training the models via FL,
    ignoring possible solutions for edge intelligence such as security, consumer privacy,
    scheduling, etc., which can be accomplished by incorporating thrust technology.
    Another study brings forth a practical AIoT approach in a real-life scenario,
    discussing training and inferencing at the edge [55]. After extensively surveying
    all the related state-of-the-art works, the author realizes that none of the works
    have considered resource management in depth in the form of taxonomical bifurcation,
    its related challenges, or its proposed solutions. Therefore, to the best of the
    author’s knowledge, this is the first comprehensive review in this domain, encompassing
    all facets of resource management. It delves into suggested integrations, their
    complexities, upcoming trends, pros and cons, and underscores their incorporation
    with cutting-edge technologies. Hence, our work outlines the mentioned incorporation
    in addition to highlighting an in-depth survey of research challenges and recognizing
    various issues in resource management. Our study emphasizes the integration of
    thrust technologies like Serverless computing, 5G, SDN, Blockchain, QC, FL, Digital
    twins, the IIoT as a prospect for future research directions which the author
    believes is of paramount importance. This survey will make an impactful impression
    on readers by articulating needs and recommending solutions to various problems,
    including the social and ethical impacts of IoT-enabled computing paradigms in
    a real-world scenario. We also present a formulation of existing resource management
    problems in mathematical form, which aims to equip the researchers by articulating
    needs and further devising solutions to this problem. Table I summarizes the comparison
    study of thrust technology that can be integrated with the Fog/Edge-of-Things
    paradigm. TABLE I Comparison of our Survey With Existing Surveys Based Upon Integration
    With Thrust Technology D. Our Contributions The main contributions to this work
    are as mentioned below: Presents a comprehensive overview of state-of-the-art
    driving dynamic resource management, encompassing controlling over and under-provisioning,
    offloading IoT-based tasks, scheduling, and service placement, including allocation
    of resources and load balancing. This work classifies and analyses existing AI-based
    solutions, emphasizing machine learning, metaheuristics, and combinational techniques
    for fog/edge resource management. Mathematically formulate the problem of provisioning
    adequate resources, offloading task requests at optimal destinations, scheduling
    the task, service placement and finally balancing workload amongst nodes, considering
    latency and cost optimization models. We have analyzed the existing solution in
    the form of QoS metrics along with their limitations and considered all aspects
    of resource management with classification, description, and limitations. The
    article emphasizes fruitful discussions of recently published articles incorporating
    AI-enriched techniques, which can work as a stepping stone for potential future
    researchers. This survey represents and evaluates the integration of the computing
    paradigm with thrust technologies such as Serverless computing, FL, IIoT, Digital
    Twin, Industry 4.0, SDN, Blockchain and Quantum Computing in the form of frameworks
    and applications. Endows solutions based upon thrust technology corresponding
    to futuristic challenges in resource management in collaborative Cloud-Fog-IoT
    paradigm. E. Social and Ethical Implications of IoT Applications The escalating
    use of sensors and corresponding enhancement of smart environments have driven
    the world to integrate sensors within the system, eventually resulting in data
    being exchanged between environments, humans, and various objects [57]. IoT has
    brought about a transformative wave in the world of technology, promising unparalleled
    convenience and connectivity in our daily lives. As IoT applications continue
    to proliferate, they extend their influence far beyond the realm of technology,
    touching upon the very fabric of our society, ethics, and the ways we interact
    with the world. This exploration delves into the multifaceted landscape of the
    social and ethical implications arising from the widespread adoption of IoT applications.
    This implies having a significant impact on citizens, society, and government
    organizations by imparting round-the-clock medical assistance by eminent doctors
    and healthcare experts, improving quality of life by cutting the carbon footprint,
    enabling access to education in remote, unsupervised areas, and the list goes
    on and on. The utilization of automation and analytics within IoT devices enhances
    customer service and refines business management strategies by facilitating the
    tracking and monitoring of both employees and products [57]. Nonetheless, the
    adoption of IoT gives rise to certain non-technical consequences, encompassing
    social and legal risks as well as ethical considerations. The ethical aspect comprises
    several factors such as transparency, accountability, sustainability, consumer
    safety etc. [55]. The transparency principle ensures that decision-making and
    underlying techniques work in a transparent and understandable manner. Accountability
    guarantees that the developer or manufacturer bears the responsibility for the
    consequences and effects of AI-powered IoT applications. To demonstrate, consider
    the scenario of an autonomous driverless car that accelerates, gains momentum,
    applies brakes, slows down etc. in response to other vehicles on the road (heavy
    vehicles, pedestrians, cyclists etc.) and other traffic-related parameters. Despite
    the functionalities of IoT-integrated paradigms, any malfunction with the driverless
    vehicle might potentially harm the safety of other vehicles on the road [58].
    Henceforth, it becomes ethically foremost to mitigate such harms in the working
    environment of IoT, protecting the physical space [59]. Hence, accountability
    fosters trust between service provider and consumer, ensuring that the service
    provider will be held liable for their actions. In a similar manner, the safety
    principle assures consumer data safety from potential breaches and malicious attacks.
    To exemplify, recent advancements delve into the realm of robot ethics in the
    context of COBOTS in Industry 5.0 and autonomous vehicles. This aspect underscores
    the importance of upholding the safety aspects of physical spaces, ensuring that
    sensors and actuators operate appropriately to accurately interpret environmental
    conditions, thereby mitigating potential accidents [60]. These parameters collectively
    contribute towards an ethical, IoT-driven smart world. Nevertheless, the legal
    and ethical implications of this technology in the practical world encourage the
    government to enforce meticulous strategies, ensuring safety standards are met
    by IoT-driven devices. These devices should undergo periodic safety assessments
    to detect potential hazards and system failures. F. Article Organization The paper
    is structured as follows: Section II conceptualizes the concept of resource management
    in depth and presents the significance of incorporating AI-empowered techniques
    in the Fog/Edge of Things paradigm via state-of-the-art advantages and open issues.
    Also, a few non-AI approaches in the existing literature have been reviewed. Section
    III presents the review methodology. Section IV provides integration of emerging
    computing paradigms with thrust technology as a future research scope. Section
    V depicts an analytical study of selected articles in the form of a graphical
    representation, based upon categorization and comparative evaluation. Section
    VI includes the research gaps and challenges identified. Finally, we summarize
    the work in the form of a conclusion in Section VII. The detailed organization
    of this comprehensive review is illustrated in Figure 4. Table II lists various
    acronyms used in this comprehensive review. TABLE II List of Acronyms Used in
    the Article Fig. 4. The organization of this Systematic Literature Review (SLR).
    Show All SECTION II. AI-Driven Resource Management in Fog/Edge The stochastic
    nature of the fog environment is influenced by multiple factors, including the
    rate of job arrivals, delay-focused data, interdependencies among incoming requests,
    dynamicity, status of resources (busy or idle), the number of tasks within IoT
    applications, varying resource requirements of real-time applications, and the
    accessibility of computational resources. Thus, conventional or heuristic methods
    do not work well in a dynamic fog computing environment due to their inability
    to adapt to constant changes. Moreover, the implications of fog computing in physical
    world applications necessitate fault-tolerant and adaptive resource management
    mechanisms which can be achieved through efficient and optimized task or workload
    scheduling. Hence, to acquire efficiency at the infrastructural level, optimal
    management of resources can’t be compromised. Presently, research is trending
    in the direction of inculcating AI with computing paradigms, hence making the
    system autonomous. AI is increasingly ingrained in our daily lives, contributing
    to informed decision-making through the utilization of meta-heuristics, ML and
    DL approaches. It is being utilized by recommendation systems for companies such
    as Facebook, Instagram, Amazon, and Google and in use cases such as healthcare
    [61], earthquake prediction [62], Industry 4.0, etc., which require the efficient
    handling of gigantic amounts of data generated from sensors. This data needs to
    be efficiently analyzed in order to extract certain features for accurate training
    of AI models. The process of training a fully equipped model can be complex, especially
    in context to the time required for training a machine learning-based model. In
    contrast, deep learning offers a key advantage over classical machine learning
    by delivering superior performance, especially when dealing with extensive datasets.
    Since many IoT applications generate vast amounts of data, deep learning methods
    are particularly well-suited for such systems [63]. For example, Deep Reinforcement
    Learning (DRL) methods are gaining insight in various forms, including convolutional
    Deep Neural Networks (DNN), deep belief networks, Recurrent Neural Networks (RNN)
    etc., for enhancing the computational intelligence of systems. It also provides
    a solution for predicting extensive workloads and aids the system even where ML
    techniques fail. Nevertheless, metaheuristic solutions provide promising results
    for scheduling tasks to appropriate nodes in the distributed architecture of Fog/Edge
    of Things [64]. Hence, our subsequent section highlights an exhaustive study implicating
    non-AI and AI-based (ML, DL, metaheuristics, and hybrid methods) for dynamic resource
    management. A. Conceptualization of Resource Management in Fog/Edge Computing
    Paradigm Resource Management (RM) is a major challenge in the emerging computing
    paradigm that includes device heterogeneity, resource-constrained nature, large-scale
    geographical distribution, edge node discovery, dynamic workload, unpredictable
    demand, and diversity [29]. In contrast to the cloud, allocating resources for
    upcoming requests is quite tough in the case of fog computing, where the load
    has to be distributed amongst fog nodes due to its decentralized architecture
    [65]. In addition to this, it is the most preferred platform for performing complex
    computing for scientific workflows, where even one small wrong decision in resource
    allocation can lead to substantial monetary loss [65]. Moreover, the increasing
    number and complexity of IoT applications make the process even more challenging.
    Hence, in order to orchestrate IoT applications, it becomes important to optimally
    provision the resources without compromising the application’s performance and
    user satisfaction level. B. Taxonomy of Resource Management The trundle of fog
    RM begins with provisioning resources, scheduling, service placement, allocating,
    and load balancing [43]. It becomes noteworthy to define the term “resource” as
    a collection of hardware (processor, storage, network, power, memory and communication
    media) and software components (VMs, instances, and containers) [66]. The RM module
    comprises components that are responsible for resource allocation among incoming
    tasks, followed by scheduling. The task offloading mechanism decides where to
    offload the IoT requests for performance enhancement by determining the algorithm
    and trade-offs to be taken into consideration [67]. The trade-off comprises fault
    monitoring and incentives to be paid in accordance with the device type. The process
    of Resource Allocation (RA) reserves and defines resources for a particular end
    user, whereas the resource provisioning module provisions and deprovisions the
    resources as and when required by the service consumer. Resource provisioning
    comprises the effective allocation of IaaS resources among applications running
    over distributed platforms. Such applications require orchestration and fencing,
    which traditional methods can’t support. It intends to minimize SLA violations
    by implying efficient server allocation strategies amid interactive and real-time
    jobs. The resource allocation module is also responsible for managing the allocation,
    re-allocation, and de-allocation of all the dependent and independent tasks. This
    process is followed by the scheduling component, which ensures energy efficiency,
    optimal resource usage, and minimize the operational costs of task execution.
    All these components reside in the complex architecture of the fog computing environment,
    as depicted in Figure 5. The bottommost layer, comprising clusters of physical
    and virtual sensors residing in intelligent homes, devices, CCTVs, smart cities,
    grids, automated driving vehicles and so on, is networked to fog devices. The
    monitoring level keeps track of system performance, resource utilization, utility,
    and feedback [68]. Data pre-processing and reduction represent promising concepts
    that enhance the efficiency of IoT data processing and analysis. Implementing
    data reduction at the edge layer can effectively decrease network bandwidth and
    latency at the gateway, thereby mitigating I/O bottlenecks in the broader network
    connection [69]. For instance, consider the case of managing IoT-based industrial
    data in smart manufacturing. This IIoT data caters to prognosis and predictive
    maintenance tasks, which calls for unnecessary data to be trimmed due to limited
    transmission, computational, storage and processing capabilities. The trimming
    procedure guarantees the exclusion of faulty, incomplete, and redundant data [70].
    Finally, the provision of services to real-time applications must maintain a high
    level of security through the incorporation of encryption. In addition, it is
    imperative to acknowledge that while encryption plays a pivotal role in safeguarding
    data confidentiality, it is not a comprehensive solution for addressing all security
    considerations. Encryption primarily focuses on data confidentiality, ensuring
    that unauthorized access to data is prevented [71]. However, several other crucial
    aspects of security warrant attention. Firstly, in applications with real-time
    constraints, relying solely on encryption can introduce unacceptable latency,
    compromising the immediate responsiveness required in critical systems. Secondly,
    the concept of data integrity is central to security, ensuring that data remains
    unaltered during transmission or storage [72]. Encryption, in isolation, does
    not address the challenge of data tampering, making additional integrity checks
    essential, especially in the context of thwarting attacks involving false data
    injection. Authentication is another vital component, as it establishes trust
    between communicating entities. Encryption, on its own, does not verify the identities
    of participants, potentially leaving room for attackers to impersonate legitimate
    entities. Certain applications necessitate non-repudiation, where senders cannot
    deny their actions. Encryption, while valuable, does not inherently provide non-repudiation
    features. Furthermore, specific security threats, such as false data injection
    attacks, fall beyond the scope of encryption. These attacks require supplementary
    security measures like data validation and intrusion detection to detect and prevent
    potential harm [73], [74]. Lastly, effective key management is critical for encryption’s
    success. Inadequate key management can compromise the entire security infrastructure.
    Thus, a holistic security approach combines encryption with these additional measures
    to ensure comprehensive protection against a wide array of security challenges.
    Fig. 5. Resource Management in Fog Computing Environments. Show All In order to
    effectively manage resource-constrained fog layer devices, the author has proposed
    this problem in the form of a taxonomical representation, as depicted in Figure
    6, by articulating the needs and research challenges and recommending possible
    AI and non-AI-based solutions. The subsequent content presents a brief overview
    of existing non-AI and AI-based solutions in context to resource management in
    a Fog/edge-enabled IoT scenario. Existing Non-AI-Based Solutions: These solutions
    have been categorized as static approaches, heuristics, mathematical models (Euclidean
    formulation), mathematical optimization-based methods such as Integer Linear Programming
    (ILP), Mixed Linear Programming (MLP), etc. The static algorithms are characterized
    by pre-information schedule creation for the incoming jobs, such as time required
    to complete the job, resources required etc. [49]. In other words, a general resource
    schedule is generated beforehand, which, however, leads to resource waste if reserved
    instances are not utilized in that particular time period. It includes First Come
    First Serve (FCFS), in which the task that arrives first, gets the resources first
    [59], [60]. Another static approach is Min-Min, which works by determining the
    Minimum Completion Time (MCT) for each job, and then, based on the MCT value,
    the right resources are allocated to the respective jobs. On the contrary, the
    Max-Min approach selects the job with the maximum execution time and accordingly
    allocates resources. However, in a real-world scenario, tasks can appear at runtime,
    and resources can be dynamically introduced or removed at runtime. To work effectively
    in such an environment, resource management strategies often rely on optimization
    methods, which can be divided into two categories: exact and heuristic [76]. Although
    exact techniques such as Branch and Bound find optimal solutions, however they
    are not well-suited for extensive problems, particularly those associated with
    IoT applications. Hence, heuristic algorithms perform comparatively better at
    finding a near-optimal solution in minimal time. Mathematical optimization methods
    such as Integer programming are categorized as ILP in cases where both the optimization
    problem and constraints are linear, whereas when continuous decision variables
    are introduced, the problem is characterized as MLP. Existing AI-Based Solutions:
    In order to resolve the limitations of traditional IoT systems, such as enabling
    real-time response, poor Internet connectivity and data gravity which evolves
    a better way to find insights than shipping all the data to the cloud. AI heralds’
    momentum in IoT-enabled smart applications, which is further boosted with the
    emergence of 5G with the aim of addressing such issues. Recent research trends
    are gravitating towards incorporating machine learning, DL, DRL, and other hybrid
    techniques at the network edge for IoT applications. ML-based techniques utilized
    in the context of optimizing resource utilization include k-means clustering [77],
    Decision Tree Regression (DTR) [78], Multiple Linear Regression [79], Naïve Bayes
    [80]. However, the latest works are approaching RL-based techniques due to the
    need to dynamically adapt and fix parameters changing in IoT-based scenarios [81].
    The main benefit of this technique lies in the fact that RL-based techniques such
    as Q-Learning and State-Action-Reward-State-Action (SARSA) do not require any
    dataset for training the model. Furthermore, such techniques operate iteratively
    with the goal of maximizing rewards for the agent, which takes actions based on
    the environmental state. The drawbacks such as the inability to handle high-dimensional
    state information regarding incoming IoT tasks, can be resolved by utilizing the
    competence of Deep learning in RL. Such techniques include DRL, Deep Q-Learning
    (DQN) and DNN. DRL works well with larger data sets by predicting appropriate
    action from action space [82]. On the other hand, DQN utilizes deep neural nets
    typically Convolutional Neural Networks (CNN) for calculating the Q-values. Fig.
    6. Taxonomy of Resource Management in Fog/Edge Computing. Show All C. Resource
    Provisioning In order to facilitate energy-efficient and low-latency solutions
    for real-time IoT applications, a thorough analysis of the forecasting network,
    storage, and computational resource needs for these applications is essential.
    These applications are accompanied by fluctuating workloads, so static allocation
    methodologies won’t simply serve the purpose. In straightforward terminology,
    Resource Provisioning (RP) is described as a process that controls resource allocation
    and de-allocation in a manner such that QoS parameters should not be compromised
    even in a fluctuating incoming workload environment. This stipulates the need
    for an automated mechanism that can effectively manage the over-provisioned and
    under-provisioned system state issues in the fog ecosystem. In under-provisioning,
    the number of allocated resources is less in comparison to the actual task execution
    needs, which further leads to SLA violations. Therefore, the basic idea behind
    RP resides in the detection and selection of judicious resources for the user
    in accordance with incoming application requests [83]. Alongside, it maps the
    incoming requests to the VMs to deliver the services with the minimum cost and
    time. 1) Problem Formulation for Resource Provisioning: The collaborative cloud-fog-IoT
    scenario constitutes collection of n IoT devices D={ d 1 , d 2 … d d … d n } which
    spawn m tasks J={ J 1 , J 2 ,…, J j , J m } . Set of x fog nodes which serve as
    micro data centers is represented as FN={F N 1 ,F N 2 …F N x } . In addition,
    the author considers the cloud data center (DS) for offering highly computational-oriented
    services. The objective of resource provisioning is to search for optimal resources
    for end users within defined constraints. This module of RM accounts for the validation
    of various constraints, which acts as a preliminary before actual resource allocation
    and task service take place. The problem of resource provisioning is formulated
    as follows [84]: IoT device constraint: 1≤d≤n Task count constraint: 1≤J≤m FN
    constraint: 1≤FN≤x (1) (2) (3) View Source Deadline Constraint: It specifies that
    each task must not surpass its maximum latency, which corresponds to the task’s
    deadline, D d,j . ∑ j=1 m T d,j Compute ≤ D d,j (4) View Source Here, T d,j Compute
    denotes the compute time of task. Bandwidth Constraint: This constraint ensures
    the avoidance of network congestion while performing various resource-centric
    tasks. It further states that the allocated bandwidth of an IoT device, fog node
    or cloud data center must not exceed the maximum bandwidth. ∑ j=1 m P j ≤ P Max
    (5) View Source Computational Constraint: The required CPU cycles to accomplish
    the task must not be greater than the available processing capacity of the resource
    ( R− ). R− can be an IoT device, fog node or cloud data center. It can be illustrated
    as follows: ∑ j=1 m J j req ≤ R− max_cpu (6) View Source where J j req depicts
    about the required cpu cycle and R− max_cpu represents about the max cpu capacity
    of resource. Non-Preemption: It states that once a task is allocated to a computational
    node, that task must be completed first before starting the execution of another
    task at the same resource. 2) Resource Provisioning Challenges: The following
    are the main challenges of resource provisioning: Benchmark Resource Performance:
    It comprises some critical aspects such as resource utilization, predictability,
    and fault tolerance. Predictability defines the system’s ability to predict its
    future behavior. Complementing the existing infrastructure with futuristic knowledge
    enhances the system’s ability to predict future behavior. Nevertheless, fault
    tolerance underlines the system’s capability to withstand the failover without
    compromising service delivery. Uncertainties: The inherent characteristic of dynamism
    in fog environment leads to uncertainties. A powerful resource provisioning mechanism
    is capable of mapping incoming requests arriving in a stochastic manner to an
    available set of resources. Furthermore, due to the significant computational
    complexities involved in this decision-making process, it is considered an NP-Hard
    problem. Hence, to solve such a complex issue, new AI-based approaches are being
    proposed based on the problem and underlying environment. The subsequent sub-section
    discusses the framework for evaluating QoS study for the work, as shown in Table
    III, which concludes that the most targeted parameters include cost, delay, and
    utilization. Whereas, few studies have focused on failover ratio, response time,
    SLA violation etc. Furthermore, most recent state-of-the-art based on non AI-based
    and AI-based (Metaheuristics, ML, and DL) along with hybrid techniques is presented,
    which is summarized in Table IV. Concerning Resource Provisioning, non-AI models
    are based upon Markovian-Decisions, mathematical-based mechanisms, and various
    types of linear programming models. Alongside a Noteworthy, Table V depicts various
    datasets that have been used to evaluate the performance of AI approaches in the
    domain of resource management. In addition, it aims to deliver dataset-related
    aspects such as accessibility in the form of links, descriptions and types, facilitating
    researchers in accessing the dataset and gaining valuable information for their
    future investigations. TABLE III Comparison of Performance Metrics for Resource
    Provisioning in Fog/Edge Computing TABLE IV State-of-the-Art Solutions for Resource
    Provisioning in Fog/Edge Computing TABLE V Dataset Availability and Description
    3) Existing Solutions for Non AI-Based Resource Provisioning: The reviewed articles
    address the problem of over- and under-provisioning of resources in fog/edge computing
    with various heuristic and mathematical-based models. Yao and Ansari [85] depict
    the challenge of provisioning multiple VMs for incoming tasks as a multi-objective
    problem in a heterogeneous IoT-enabled environment. It aims at minimizing the
    total cost involved in renting VMs while nevertheless maximizing reliability to
    confront the issue of VM failovers. The objective has been accomplished by using
    a modified version of the Best Fit Decreasing (BFD) algorithm; where the incoming
    tasks are arranged in decreasing order of corresponding length and then provisioned
    to VMs as per weight capacity (price). The simulation results demonstrate the
    improvement in results in terms of cost and reliability as compared to state-of-the-art
    algorithms. The authors propose a software-based approach for workload partitioning
    among computing layers [81]. Partitioning the workloads provides a prime requisite
    for optimal provisioning of resources, especially bandwidth consumption to link
    factories with cloud data centers. Moreover, it assists the existing framework
    in determining the minimum and maximum number of locally situated servers to be
    integrated with available FNs in order to provide exquisite resources to real-time
    applications subjected to time and memory constraints. It becomes noteworthy to
    mention the problem of predicting and provisioning resources for serving the enormous
    number of devices in existing cellular networks. For instance, cellular relay
    networks thrive on strategies enabling ubiquitous coverage and providing a fair
    share to respective users, enabling optimal resource provisioning [86]. To maintain
    the QoS parameters, the service providers are working towards expanding their
    Base Stations (BS). BS consistently allocates a fixed number of resources, while
    mobile users access services at discrete intervals, often leading to inefficient
    resource allocation. In the context of vehicular networks and aerial units, a
    dynamic resource provisioning approach has been suggested in [92]. Additionally,
    a two-stage algorithm has been introduced to optimize the management of RSU workloads.
    Briefly, the vehicular computing framework manages incoming workload spikes via
    flying fog units, hence the introduction of RSUs as a computing paradigm. The
    overloaded RSU can provision the required resources from nearby base stations
    based on the calculated lease period, utilizing local and global workloads. The
    work done is capable of deadline and capacity-aware offloading, handling peak
    loads, and supporting dynamic resources in the form of vehicles or UAVs using
    various mathematical models. The results illustrate reduced energy consumption,
    waiting time and computational time in comparison to the state-of-the-art. 4)
    Existing Solutions for AI-Based Resource Provisioning: Due to the prevalence of
    AI, we decided to explore the existing work done in this challenging domain using
    metaheuristics, ML, DL, and hybrid approaches. Hatti and Sutagundar [94] implemented
    distributed provisioning using Multi-objective Particle Swarm Optimization (MOPSO).
    Multiple jobs have been categorized into different swarms based upon similarity
    in response time requirements. Further, each swarm is mapped to a single or multiple
    FNs taking into consideration the resource capacity, response time, and distance.
    It works by computing the task fitness value corresponding to a particular swarm.
    PSO is a nature-inspired evolutionary and stochastic technique for the optimization
    of computationally hard problems. It is inspired by the swarms in nature, which
    include bird flocking, fish schooling etc., and holds an edge over other optimization
    techniques as it utilizes a combination of both local and global search [107].
    Considering the ML approaches, the author in [89] proposed an IoT service autonomous
    mechanism possessing self-sustainability in fog nodes using Bayesian learning
    along with the incorporation of IBM’s MAPE-k model for better decisions regarding
    resource planning. This technique of Bayesian inference is based upon the Bayes
    theorem, which considers the probability distribution of each quantity, hence
    providing effective decision-making [108]. The proposed model withstands different
    workloads with a minimum error rate. The authors highlight the importance of resource
    provisioning in multimedia applications [87]. The work signifies the role of edge
    networks in satisfying the resource-hungry nature of modern applications. Sophisticated
    algorithms based on ML techniques have been discussed, which focus on the predictor
    module for the completion time of rendering jobs in context with the underlying
    available fog resources. Furthermore, the prediction accuracy of job completion
    time is improved using the multi-fold cross-validation method. It uses various
    algorithms for prediction problems, such as Random Forest (RF), Support Vector
    Machine (SVM), DL and Gradient Boosting Tree (GBT), and finally R-square scores,
    to shortlist the best-performing approach. Finally, the proposed predictor algorithm
    is compared with various state-of-the-art algorithms. The work done by the authors
    [78] brings forward a predictive autoscaling technique using ML for containerized
    Microdata Center (MDC) in fog environments. A workload forecasting mechanism has
    been used to determine the number of containers required to serve incoming IoT
    workloads without compromising Service-level Objectives (SLOs). The proposed framework
    has been evaluated under synthetic and realistic workloads. The work presents
    potential application areas comprising real-time traffic monitoring, smart healthcare,
    self-driving cars, and the IoE. The task classification is done on the basis of
    task priority and scheduling class. Sham and Vidyarthi [95] proposed a fuzzy-based
    admission control and resource provisioning system that places request analysis
    parameters such as CPU, memory, storage, job priority and time sensitivity. The
    presented methodology considers request parameters in the form of crisp values,
    which are then passed through the Fuzzy Inference System (FIS), and then aggregation
    is performed to select the best computing node. Deep Learning-Based Solutions:
    In contrast to classical machine learning techniques, RL-based approaches have
    the ability to perform linear and non-linear approximations. These approaches
    dynamically adapt to the changes in the environment and possess the capability
    to learn the system without prior knowledge. These capabilities make this technique
    well-suited for resource management problems. An AI-enabled resource provisioning
    architecture has been proposed for IoE services in 6G networks. It uses DRL along
    with the Markov Decision Process (MDP) for resource provisioning [93]. The MDP
    framework is employed for addressing challenges that are resolved through RL.
    In the context of a multi-application scaling solution, the author explores scenarios
    where each application consists of a collection of services. MDP considers a tuple
    (S , A , P , R , γ ), where S is the finite set of states, A represents the finite
    state of actions, P depicts the probability of state transition, R represents
    the immediate reward and γ is the discount factor. The state space ‘ S ’ is formed
    by the dynamic user demands and resource availability for each host at timestamp
    ‘t.’ The action space ‘ A ’ is of fixed size, comprises a pair of elements that
    determine the CPU and memory scaling decisions. The probability transition matrix
    ‘ P ’ quantifies the likelihood of transitioning to the next state ‘s’ when a
    specific action ‘a’ is taken. Usually, this value is not given in advance, hence
    model-free RL technique is utilized to predict this value. Finally, the immediate
    reward or cost function R , aims at selecting the best action which results in
    the minimum cost. For instance, minimization of application load and optimizing
    the available resources are a few examples of cost functions. However, the ultimate
    objective of an RL agent is to acquire knowledge about the probability distribution
    governing transitions from a given state to all possible subsequent states and
    to determine the optimal policy π ∗ . The following equations outline how to update
    the Q-value for a specific state-action pair Q(s,a) , by taking into account the
    immediate reward R and the minimum Q-value. This value is computed for the next
    state, s ′ which is weighted by the discount factor γ . The equation below iteratively
    improves the agent’s Q-value estimates. Q(s,a):=Q(s,a)+α[R+γQ min α ′ ( s ′ ,
    a ′ )] (7) View Source where Q(s,a) : represents the current assessment of the
    expected cumulative reward for taking action a min α ′ Q( s ′ , a ′ ) : signifies
    the lowest expected cumulative reward across all feasible actions a ′ in the next
    state s ′ . s ′ : final state after taking transition from state s . a ′ : action
    chosen by agent in the next state s ′ . All these Q-values are stored in a tabular
    manner, as a result of which the state space grows with the increasing number
    of containers/hosts. Therefore, it becomes computationally very expensive to store
    and further maintain the updated Q-values. Hence, Optimal Q-values can be derived
    from adjustable weights (θ) . Gradient descent can be employed to adjust weights
    in the right direction. Consequently, Q becomes close to optimal Q ∗ which is
    represented as: Q ∗ (s,a)=Q(s,a,θ) (8) View Source But linear approximations suffer
    from the following drawbacks: Inability to capture complex, non-linear relationships
    between states and optimal actions. Approximation errors accumulate over time,
    which leads to unstable policies. Not suitable for continuous state space, where
    states can be similar. May struggle to adapt to the dynamics of the environment.
    Linear approximations can help with large state spaces, but suffer from the curse
    of dimensionality, which increases with an increase in the number of features.
    Consequently, non-linear approximations such as DNNs are being utilized which
    empowers agents to employ deep learning for weight updates and customized learning
    adjustments. Etemadi et al. [90] work states an automatic and scalable resource
    provisioning framework for fog architecture that stores the incoming workload
    parameters (response time, resource requirements) in a shared database. An autonomous
    learning-based provisioning model has been proposed utilizing Nonlinear AutoRegressive
    (NAR) neural networks. In another study done by Li et al. [91], a provisioning
    strategy in a cloud-enabled fog computing scenario has been proposed, considering
    tenanted and overhead costs. The exquisite decision-making capability regarding
    whether to add more instances or release confers the data migration strategy.
    This is implemented using the hybrid Autoregressive Integrated Moving Average
    (ARIMA) model and a Back Propagation (BP) neural network. ARIMA is a univariate
    time-series forecasting model used to predict the number of resources needed to
    support the incoming workload of smart applications [109]. The hosts located at
    the edge layer are equipped with hybrid ARIMA and BP neural network algorithms.
    This forecasted workload acts as a basis for determining whether the request will
    be processed at the edge tier or rented from cloud data centers. A module known
    as the workload analyzer is supplied with historical workload data, offering workload
    estimates with a one-time interval forecast. This interval should be long enough
    in order to provision an appropriate number of VMs in advance [109]. ARIMA blends
    autoregressive (AR) and moving average (MA) elements in conjunction with differencing
    to achieve stationarity in the time series. Despite the capabilities of ARIMA
    for time series forecasting, it may not capture complex, non-linear patterns or
    sudden changes in workload behavior. Nevertheless, its univariate nature limits
    its capabilities to analyze and forecast single-time series variables. Henceforth,
    it is combined with ML-based models such as BP-NN to improve prediction accuracy.
    During the training phase of BP-NN, the input patterns are processed in two stages.
    In the first stage, a predicted value corresponding to each input pattern is provided.
    Any prediction-related error is propagated backward to update the weights of the
    hidden and output layers. This update process aims to minimize the prediction
    error that is associated with it [110]. Finally, the effectiveness of the work
    done has been validated using extensive experimentation on real-world datasets.
    The hybrid approaches provide an amalgamation of any two (or even more) of the
    above-mentioned solution types. The implementation of RP using Bayesian networks
    and workload clustering using Biogeography Based Optimization (BBO) in integration
    with K-means clustering has been proposed by Ghobaei-Arani [77]. The author has
    emphasized the significance of classification and analysis of the workload of
    incoming user requests, which acts as a prerequisite for effective resource provisioning.
    The workflow of the proposed work comprises three phases: preprocessing of incoming
    workload, clustering, and finally provisioning the resources. The pre-processing
    stage eliminates noise and filters out workloads with incomplete attributes, as
    well as artificial users like robots that may propagandize and desolate actual
    purposes. In addition, it also includes making SLAs. A hybrid approach is followed
    for workload clustering which works in four phases defined as: (1) Random initialization
    of habitats comprising the population, (2) Using k-means for habitat evaluation,
    (3) Immigration and emigration rate calculation and (4) Best habitat selection.
    Lastly, Bayesian learning serves as a classifier for categorizing RP decisions
    based on clustered workloads. It utilizes a state table containing SLA cost, response
    time, workload attributes, and resource scaling decisions to derive Bayesian-based
    rules. D. Task Offloading The best possible task offloading strategy is characterized
    by its capability to choose an optimal offloading decision, subjected to the incoming
    IoT workload. In cases of ample resource availability, the job is processed; otherwise,
    the architecture is scaled up and allocations are updated. In case the fog nodes
    are incompetent to execute the incoming application request, it is then offloaded
    to the cloud layer. Task offloading from edge devices to nearby FNs not only reduces
    latency but also significantly reduces energy consumption [111]. Figure 7 demonstrates
    the complexity of making the optimal offloading decision due to various factors
    such as offloading fraction, offloading constraints, and offloading objectives.
    The objectives focus on improving QoS parameters like cost, throughput, resource
    utilization rate, energy, etc. On the contrary, offloading constraints comprise
    bounds on bandwidth utilization, offloading fraction, and other task-specific
    criteria [112]. In certain scenarios, only a portion of the task is offloaded,
    requiring the processed outcome to be subsequently transmitted back along the
    same route or an alternative one, based on the most favorable option. Fig. 7.
    Task offloading Problem in Fog Computing. Show All 1) Problem Formulation for
    Task Offloading: This section highlights the significance of mathematical analysis
    in managing task offloading, particularly in relation to reducing delays for applications
    that require instantaneous responses. It aims to jointly minimize task delay/latency
    and simultaneously economizing energy consumption. Task offloading not only helps
    to enrich the consumer experience but also accelerates job execution. The authors
    introduce the offloading latency models to investigate the delay parameter, computational
    demand and communication demand, corresponding to each incoming request. Moreover,
    deciding how to offload tasks is intricate due to the diverse wireless networks
    (including WLAN and MAN) involved, as well as the necessity to choose the most
    efficient offloading strategy from among multiple edge/fog nodes, and cloud data
    centers. The problem for offloading tasks at optimal destinations is as follows
    [10], [113]. Local Computation at Device: It considers the task processing at
    local edge device itself, where the latency corresponding to local computational
    L d,j IoT is computed as follows: L d,j IoT = J d,j size ψ d_mips (9) View Source
    J d,j size : Task size corresponding to device d and j th task. ψ d_mips : Computational
    capacity of IoT device. Offloading to Fog Node: The task offloading at local fog
    delivers ultra-low latency services due to avoidance of network backhaul delay.
    The latency L d,j FN can be computed as: L d,j FN = T d,j FN_Uptime + T d,j FN_Compute
    + T d,j FN_Downtime (10) View Source where, T k Offloading represents the time
    to offload request to edge node. This time is also known as task uptime, which
    is represented as T d,j FN_Uptime T d,j FN_Uptime = J d,j size P FN (11) View
    Source Here: J d,j size : Task size corresponding to device d and j th task P
    FN : Bandwidth of underlying link (WLAN, MAN) T d,j FN_Compute = J d,j size ψ
    FS_mips (12) View Source where: ψ FS_mips denotes the computational capacity of
    Fog server node, and further the authors have considered that the downtime is
    equivalent to the uptime. Offloading to Cloud Data Centers: Noteworthy, our work
    considers full offloading scheme, where task is either on fog or cloud deployed
    node. Latency in full task offloading case L d,j(Full) DS , which transmits whole
    task to cloud for processing is computed as: L d,j(Full) DS = T d,j DS_Uptime
    + T d,j DS_Compute + T d,j DS_Downtime (13) View Source where, T d,j DS_Uptime
    represents the uptime of cloud data center, which can be also referred as offloading
    time. T d,j DS_Uptime = J d,j size P DS (14) View Source where P DS : depicts
    the bandwidth of network (WAN). T d,j DS_Compute represents the computational
    time by cloud datacenter node, which can be calculated as: T d,j DS_Compute =
    J d,j size ψ DS_mips (15) View Source where ψ DS_mips signifies computational
    capacity of cloud datacenter node. Noteworthy, the offloading latency model functions
    in accordance with offloading decision variable Θ d,j is defined as follows [114]:
    Θ d,j = ⎧ ⎩ ⎨ 0; −1; 1;  task computed by d   task offloaded to FN  task offloaded
    to DS (16) View Source This equation represents the offloading decision corresponding
    to j th task generated by IoT device. The value of Θ d,j =0 ; signifies that the
    task J j is processed at IoT device itself, whereas the value Θ d,j =−1 ; indicates
    that the task is offloaded to local fog server. Finally, the task is being offloaded
    to central cloud server in case Θ d,j =1 . 2) Task Offloading Challenges: Changing
    Networking Conditions: edge/fog computing environment is characterized by dynamic
    network and traffic conditions. The presence of different noise and interference
    levels can significantly impact the overall efficiency and latency of wireless
    transmission. It demands having an analysis and prediction mechanism for underlying
    networking conditions in order to estimate the right time for task offloading
    decisions. Such a scenario works in an architecture comprising mobile devices,
    Mobile Edge Servers (MES) and Fixed Edge Servers (FES), in which the mobile device
    layer includes various sensors and devices embedded in vehicles, whereas autonomous
    vehicles serve as MES, and RSUs serve as FES [115]. In this situation, the presence
    of mobility amongst vehicles and MES, as well as factors such as increased traffic
    during peak hours and complex road networks, inject a dynamic element into the
    network conditions. This, in turn, has a direct influence on the decision-making
    process for optimizing task offloading and route planning for MES operations.
    To handle this issue, DRL-based techniques are gaining prominence due to their
    proficiency to self-learn from the environment and incorporate the updated parameters
    in subsequent iterations [81]. Dynamic User Behavior: The randomized behavior
    of mobile users adds another level of complexity to task offloading decisions.
    Currently, research trends are being diverted to machine learning and data analytics
    techniques in contemplation of prediction and forecasting user behavior [117].
    For illustration, consider the urban transportation scenario which comprises latency-sensitive
    tasks spawned from self-driving vehicles, remote fleet monitoring, etc. Such tasks
    need to be cautiously handled in order to incorporate appropriate environmental
    perception, vehicle motion control, opportune decision-making and action, and
    collaborative Simultaneous Localization And Mapping (SLAM), to name a few. Such
    versatility among tasks makes the task-offloading decision even more complex.
    The majority of work done to address this challenge has utilized DRL approaches
    [115], [118]–[120]. Apart from DRL approaches, some authors have implemented meta-learning
    to handle dynamic IoT application requirements in the form of incorporating diverse
    task types [91], [92]. Highly Latency-Sensitive Requests: Certain applications
    cannot endure even milliseconds of delay and require immediate access to computational
    resources for processing upcoming requests. Such types of requests are either
    processed by IoT devices or offloaded over the fog/edge nodes. Nowadays, prominent
    video applications, particularly those involving VR/AR, are becoming increasingly
    significant in areas like the gaming industry, education, medicine, and more,
    owing to their immersive visual features [123], [124]. For instance, AR application
    tasks are effectively handled by utilizing DRL, MES and the capabilities of the
    5G network in order to cater to the highly sensitive latency requirements of the
    task. This is carried out by splitting the incoming task into a Directed Acyclic
    Graph (DAG) [124]. 3) Existing Solutions for Non AI-Based Task Offloading: This
    section reviews the state-of-the-art works in detail based on task offloading
    and optimization objectives. Muñoz et al. [125] proposed a framework for the optimization
    of computational resources in a cellular network. It utilizes the suboptimal approach
    and the optimal statistical approach. In the former class, the complete dataset
    is divided into smaller subsets, and an offloading decision is taken corresponding
    to each subset. In this suboptimal approach, a global decision is taken in conjunction,
    although in practice it is challenging to predict the channel state in advance.
    The latter category makes statistical offloading task decisions that adapt based
    on the real-time updates of channel statistics. Another work by Han et al. [126]
    proposed an online job dispatching and task scheduling problem in an edge-cloud
    environment. It assumes that incoming jobs are released in a random order and
    at irregular time intervals, without any initial statistical information about
    the jobs. It aims to minimize Weighted Response Time (WRT). As far as the offloading
    decision is concerned, OnDisc heuristically dispatches the job to cloud server
    with a minimum WRT. Extensive simulation results utilizing real-world Google Cluster
    signify better WRT results. Also, incorporating a fairness knob ensures uniform
    WRT amongst all jobs. Table VI consolidates QoS comparison study of surveyed work.
    Whereas Table VII summarizes the non AI-based and AI-based solutions to task offloading
    problem in collaborative cloud-fog-IoT paradigm. TABLE VI Comparison of Performance
    Metrics for Task Offloading in Fog/Edge Computing TABLE VII State-of-the-Art Solutions
    for Task Offloading in Fog/Edge Computing 4) Existing Solutions for AI-Based Task
    Offloading: The author emphasizes that the process of choosing the most suitable
    offloading destination must prioritize the judicious utilization of resources
    along with meeting the QoS requirements of the incoming request. It proposes a
    Classification and Regression Tree Algorithm (CART)-based solution for module
    placement [127]. The proposed approach begins by evaluating the power usage of
    all mobile devices and, if the power consumption surpasses that of Wi-Fi, it offloads
    the incoming request to the fog node. The traditional decision trees have limitations
    when dealing with input spaces. To address this, the CART method is employed,
    as it accommodates real-valued parameters and helps identify the best feature
    conditions. After executing Module Placement by Classification and regression
    tree Algorithm (MPCA), Poisson process is used to calculate the module arrival
    rate. The Poisson process aligns with Markov chain properties, providing systematic
    manageability. In addition, the Markov chain is memoryless, indicating that its
    probability distribution relies solely on the present state and does not take
    into account past events. It can be used to analyze arrival rates of fog devices
    by modeling the arrival process of data or tasks at these devices as a stochastic
    process with specific states and transition probabilities. Finally, this probability
    matrix is utilized to place the incoming modules at their best destinations for
    execution. The presented work is compared with First Fit (FF) and local mobile
    processing models. Vemireddy and Rout [98] have highlighted the problem of efficient
    offloading of IoT-based tasks in Vehicular Fog Computing (VFC) due to heterogeneity
    and mobility amongst vehicles. An RL-based agent explores all the possible actions
    in a greedy manner until it exploits the best action for maximizing long-term
    reward. The author interprets states as time slots and defines actions as representations
    of fog vehicles. The reward is determined by a function that combines service
    time and the energy consumption (EC) of RSUs. The work aims to reduce the response
    time and computational overhead of RSUs in VFC by utilizing a fuzzy-based RL technique.
    The presented incorporation of fuzzy with RL addresses the issue of high dimensionality,
    which occurs due to the increasing vehicle count under RSUs. In addition, the
    fuzzy if-then rule base is applied to calculate the vehicle weight (Very low,
    low, low medium, high medium and very high). This is inferred on the basis of
    input parameters such as process rate {low, medium, high}, staying period (Dwell
    time) {low, medium, high}, and distance to RSU {far, middle, near}. The suggested
    method not only expedites the learning process but also enhances long-term rewards
    compared to prominent Q-Learning, by incorporating greedy heuristics. A secure
    task offloading mechanism has been proposed by Alli and Alam [128]. The author
    utilizes machine learning techniques for implementing secure offloading in a cloud-fog
    environment. In the system model under consideration, IoT gateways function as
    a bridge, facilitating communication between IoT devices and the upper layers
    (Fog and cloud). To accommodate the resource-constrained nature of IoT devices,
    a Neuro-fuzzy model is deployed on smart gateways. This hybrid computational model
    is a fusion of ANN and fuzzy logic systems, which brings out the best useful traits
    from both models. The capabilities of NNs such as adaptability, learning and generalization
    can be applied to fuzzy logic, which further imparts transparency to the hybrid
    model. Adaptability: It deals with tuning the parameters of a fuzzy logic system
    via learning or training. Training: The NN learns from the incoming data and adjusts
    the membership functions and rule parameters of the Fuzzy Inference System (FIS).
    Generalizability: It refers to the system’s ability to make accurate predictions
    or decisions on new, unseen data that was not part of the training dataset. Transparency:
    The fuzzy logic component provides interpretability through linguistic variables
    and fuzzy rules, making it easier to understand and modify the system. Hence,
    this model harnesses the strengths of both approaches to improve modeling and
    control uncertain and complex use cases. The work employs a Neuro-fuzzy model
    which considers two factors: (1) Sensor value, and (2) Time. For security evaluations,
    the predicted value is derived correspondingly. A value exceeding 1.00 indicates
    a valid reading, while any value below this threshold is deemed invalid. As a
    result, the neuro-fuzzy logic knowledge base is trained to adjust to incoming
    IoT workloads. By categorizing the predicted values as valid/invalid, only data
    from trusted devices is retained. Afterwards, Q-Learning is used for dynamic offloading
    decisions. Finally, the resource allocation decision is carried out by the PSO
    algorithm. Another competent online IoT-based task offloading technique has been
    discussed by Zhu et al. [129], which aims at minimizing the cost corresponding
    to a specific node along with the latency incurred and energy consumption at the
    time of task computation. The problem considered is framed as a stochastic programming
    model in order to handle dynamic system-related parameters. The proposed Bandit
    Learning-based Offloading of Tasks (BLOT) is based on the Upper Confidence Bound
    (UCB) which assists in selecting the optimal fog node to offload tasks. This technique
    holds an edge over the other techniques in the context that it doesn’t consider
    prior knowledge about the system parameters, highlighting the fact that some system
    values called as bandit feedback appear only at the time of querying the nodes.
    After offloading the tasks to resource-rich fog nodes, the First In First Out
    (FIFO) strategy is used to schedule unfinished tasks. The numerical experimentation
    demonstrates that BLOT serves as an optimal task offloading strategy in dynamic
    online mode situations. Another work by Adhikari and Srirama [97], proposed an
    Accelerated PSO for performing real-time task offloading to apt computing devices
    as per resource requirements in hierarchical fog-cloud environments. Multi-Objective
    Offloading (MOO) based on Adaptive PSO (APSO) categorizes the real-time incoming
    tasks into resource-insensitive and delay-sensitive category. The former ones
    are allocated High Computing Fog (HCF) nodes, whereas the later ones concerning
    latency and cost are executed on Low Computing Fog (LCF) for faster response.
    The principal reason for using APSO is to reduce error rates and maximize accuracy,
    along with optimizing other QoS parameters. Further, advanced machine learning
    strategies can be employed to improve the performance and precision of offloading
    methods. A smart city-based scenario is considered by Hussein and Mousa [130]
    in which a framework has been presented which aims at enabling synchronized actions
    by efficiently processing voluminous data produced by IoT sensors. Task offloading
    is a problem of NP-hard complexity, which means that as the number of IoT devices
    and fog nodes grows, the complexity level increases exponentially. Therefore,
    the author presents two nature-inspired metaheuristic techniques, Ant Colony Optimization
    (ACO) and PSO to assure low-latency services. For instance, in PSO, each particle
    depicts a potential solution to the task offloading problem. Every iteration moves
    closer to the global best for the entire population and to its own local best.
    Typically, a large number of particles are used, making it an ideal choice for
    such problem scenarios. The results demonstrate significant improvement by ACO
    task offloading in terms of response time. E. Resource Scheduling In general,
    scheduling is the process of sequencing incoming tasks in some order, which is
    carried out by a special program known as a scheduler. Scheduling in cloud computing
    is defined at two levels: physical host to VMs and tasks to VMs. Scheduling in
    the fog landscape involves complexity because of heterogeneous devices and the
    resource-constraint nature of end devices. This heterogeneity makes resource scheduling
    an NP-hard optimization problem [131]. It can also be described as the optimal
    placement of different IoT-based tasks on fog nodes in order to meet real-time
    QoS requirements by minimizing task execution time and abiding by user SLAs [132].
    1) Problem Formulation for Resource Scheduling: An optimal scheduling solution
    intends to schedule a set of input tasks J={ J 1 , J 2 ,…, J j , J m } to improve
    various QoS requirements considering constraints (latency, deadline, SLA, cost).
    The distributed fog nodes FN={F N 1 ,F N 2 …F N x } accomplished the demands of
    IoT requests in the form of computational capability, memory and network usage.
    Figure 8 depicts resource scheduling in fog environment. In order to schedule
    the incoming task to its optimal destination, the overall communication cost of
    executing the task is computed, based on which task scheduling and allocation
    occur. Fig. 8. Resource Scheduling in Fog/Edge computing Environment. Show All
    Processing Cost Analysis at Local Device: Considering the latency L d,j IoT computed
    from Eq. (17), now energy consumption of processing task E C d,j IoT = J d,j size
    ψ d_mips ∗ ℘ j IoT (17) View Source where, ℘ j IoT denotes the per unit power
    consumption of j th task. Now, computing the total processing cost at IoT device
    can be depicted as: C d,j IoT = ω 1 ∗ L d,j IoT + ω 2 ∗E C d,j IoT (18) View Source
    Here, ω 1 and ω 2 are weight parameters such that ω 1 + ω 2 =1 . Processing Cost
    Analysis at Fog Node: When an IoT device doesn’t possess the capacity to handle
    high-end, latency-critical tasks, it is offloaded to a fog node. The execution
    delay for FN is computed from Eq. (8) and Eq. (9), where the uptime refers to
    the offloading time. D d,j FN = T d,j FN_Uptime + T d,j FN_Compute (19) View Source
    Now let ℘ j FN denote the power consumption to process the task at fog node and
    correspondingly ℘ j IoT→FN denote the power consumed during transferring task
    from IoT device to fog node. Energy consumption of execution request at this layer
    can be computed as [133]: E C d,j FN = ( T d,j FN_Uptime ∗ ℘ j IoT→FN ) +( T d,j
    FN_Compute ∗ ℘ j FN ) (20) View Source Hence, the total processing cost at this
    layer is computed by combining Eq. (19) and (20). C d,j FN = ω 1 ∗ D d,j FN +
    ω 2 ∗E C d,j FN (21) View Source Processing Cost Analysis at Cloud Data Center:
    It considers offloading compute-intensive tasks to a cloud data center. Computing
    the execution delay for FN from Eq. (13) and Eq. (14), where the uptime refers
    to the offloading time. D d,j DS = T d,j DS_Uptime + T d,j DS_Compute (22) View
    Source Now let ℘ j DS denote the power consumption to process the task at cloud
    datacenter and correspondingly ℘ j IoT→DS denote the power consumed during transferring
    task from IoT device to cloud data center. Hence, the energy consumption of execution
    requests at this layer can be computed as follows: E C d,j DS = ( T d,j DS_Uptime
    ∗ ℘ j IoT→DS ) +( T d,j DS_Compute ∗ ℘ j DS ) (23) View Source Finally, total
    processing cost, C d,j DS = ω 1 ∗ D d,j DS + ω 2 ∗E C d,j DS (24) View Source
    The processing cost is evaluated at all the possible task offloading destinations
    in order to schedule it to the optimal destination. 2) Resource Scheduling Challenges:
    The following are the main challenges of resource scheduling: Proficient Workload
    Deployment: In scenarios involving transactional workloads with unpredictable
    job arrivals, such as in e-commerce traffic, resource scheduling becomes increasingly
    challenging, particularly when there is no prior information available to make
    optimal decisions [134]. Such situations required AI-based solutions, particularly
    DRL, in which agents are supplemented with historical information on incoming
    jobs for effective training. Based on the value of reward, agents improve their
    decision strategies by updating the model parameters. Optimal Task Mapping: The
    goal of optimal task mapping is to find the most suitable allocation of tasks
    to resources in order to optimize various performance metrics, like minimizing
    execution time, energy consumption, or enhancing resource utilization. This is
    a complex problem because it often involves dynamic workloads, varying resource
    capabilities, and changing task requirements. However, finding the truly optimal
    task mapping is often computationally expensive and may require considering a
    large number of variables and constraints. As a result, various algorithms and
    heuristics are employed to address this challenge and provide effective solutions
    for resource scheduling in dynamic computing environments. There are many kinds
    of task scheduling algorithms, mainly categorized as Static and Dynamic. The static
    algorithm required advance information about incoming requests along with available
    resources, including memory, processing capability, bandwidth etc. This category
    covers FCFS, Shortest Job First (SJF), Round Robin (RR), Minimum Completion Time
    (MCT), Minimum Execution Time (MET), and many more, which are usually preferred
    when workloads have small variation. However, when dealing with real-time applications
    and multicore processors, achieving optimal resource utilization using these deterministic
    algorithms becomes a challenging task. On the contrary, dynamic task scheduling
    doesn’t require any advance information about the tasks or available resources.
    3) Existing Solutions for Non AI-Based Resource Scheduling: This section discusses
    the work that uses static scheduling algorithms along with mathematical models
    for resource scheduling problems. Li et al. [135] proposed a hybrid computing
    system for smart factories and Industry 4.0 by proposing a four-level architecture
    that integrates the historical heritage of computational resources. Furthermore,
    a two-phase resource scheduling strategy is introduced; the selection of edge
    computing servers is done by taking into consideration different factors corresponding
    to low real-time constraints in phase 1. Whereas phase 2 manages cooperation amongst
    multiple edge servers in order to construct an Edge Server Cluster (ESC), which
    further comprises the Manufacturing Edge Layer (MEL) cloud. Selecting Algorithms
    are utilized for ESC known as SAE and CEC (cooperation of edge computing clusters),
    which works at accomplishing real-time requirements. A deadline-based framework
    has been proposed by Fizza et al. [100], in which the incoming tasks are ramified
    into hard, firm, and soft real-time tasks and then assigned to the most suitable
    processor for their successful execution. The work has been carried out in an
    environment consisting of local embedded fog and cloud datacenters. Earliest Deadline
    First (EDF) is used for task scheduling on the appropriate processor, which works
    by sorting all the tasks in ascending order of their deadlines. The hard real-time
    tasks are allocated to run on embedded systems, the firm tasks are executed on
    fog nodes, and, finally, the soft real-time tasks are processed on cloud-based
    processors. The proposed architecture demonstrates a 62% improvement in Success
    Ratio (SR) and a 35% reduction in response time in comparison to scheduling tasks
    exclusively on the cloud infrastructure. This work focuses on its implications
    for autonomous cars. Another work by Boveiri et al. [136] proposed a robust solution
    based on the Max-Min Ant System (MMAS) to solve the multiprocessor task-graph
    scheduling problem. The algorithm determines the optimal task sequence from the
    provided task graph and subsequently assigns tasks to available processors in
    accordance with their sequence. The proposed algorithm outshined traditional methods
    in terms of makespan time. 4) Existing Solutions for AI-Based Resource Scheduling:
    The process of feature engineering involves the extraction of features, which
    are characteristics, properties, and attributes, from raw data through domain
    knowledge [137]. However, the machine learning techniques are based upon manual
    feature extraction. Afterwards, the model is selected based on chosen features
    for carrying out variable categorization. Overall, this process becomes time-consuming
    as the model created depends entirely on the designer’s discretionary knowledge.
    On the contrary, the complex architecture and multiple layers in deep learning
    models are capable of automatically learning relevant features directly from raw
    data. This ability to perform feature extraction as part of the learning process
    has made deep learning particularly powerful in the IoT domain. The potential
    feature of DL resides in its capability of self-improvement and expansion with
    increasing data, which is not the same in the case of machine learning. Henceforth,
    DL is promising for quick feature extraction from voluminous amounts of IoT sensor
    data. The DL methods have been implemented in the form of different architectures,
    which are as follows: Convolutional Deep Neural Networks: These models belong
    to a family inspired by the way the human brain’s visual cortex recognizes objects.
    Traditional machine learning models depend on input features that can be provided
    by domain experts or generated through computational feature extraction techniques.
    In contrast, neural networks automate the feature extraction process. For example,
    multilayer neural networks create a feature hierarchy by progressively combining
    low-level features to build high-level features in a layer-wise manner. This approach
    is well-suited for processing images, where initial layers extract low-level features
    that are then aggregated to create high-level features. A standard Convolutional
    Neural Network (CNN) consists of multiple convolutional and pooling layers, followed
    by one or more fully connected (FC) layers toward the end [138]. For example,
    surveillance systems employed at various crowded indoor and outdoor locations
    aim at recognizing abnormal human behavior in society. Carrying out stream analysis
    manually becomes time-consuming. Hence, CNNs are being utilized to identify objects,
    people, or specific patterns within video feeds, which is valuable in ensuring
    safety amongst people (elderly, patients), decreasing harassment at public places,
    and safeguarding government assets [139]. Apart from this, CNNs can be optimized
    and deployed at the edge (on IoT devices) to perform local data processing without
    the need for constant communication with the cloud. Recurrent Neural Networks
    (RNNs): These are particularly suitable for tasks that involve sequential data
    and dependencies over time. Predictive models built with RNNs can help optimize
    resource usage by anticipating demand fluctuations and adjusting resources accordingly.
    They allow IoT systems to make informed resource allocation decisions, improve
    operational efficiency, and enhance overall resource management in various domains.
    For example, these models excel at precisely recognizing complex, non-linear patterns
    within the input data. They are also effective in capturing temporal workload
    and node patterns, with their layers facilitating faster learning. Leveraging
    these models can lead to the optimization of stringent QoS metrics tailored to
    the application type by employing an adaptive loss function [140]. Autoencoder
    Neural Networks: These networks operate in an unsupervised manner and are employed
    for feature extraction and reduction. They have the capacity to identify intrinsic
    patterns within a dataset and then assign labels to these discovered patterns.
    Hence, autoencoders reconstruct the datasets, discovering their inherent structure
    and eventually carrying out dimensionality reduction. This technique can also
    be utilized for time-series forecasting in IoT applications. Shadroo et al. [141]
    proposed a framework to improve performance in terms of response time in an IoT
    environment. The tasks are clustered based on clustering methods using three Self-Organizing
    Maps (SOM) methods. In the first method, the primary feature is divided into multiple
    clusters by SOM. In the second approach, the initial features are transmitted
    to the Hierarchical Self-Organizing Map (H-SOM) cluster. Subsequently, a deep
    learning technique, the Autoencoder, which falls under unsupervised learning,
    is employed for feature extraction. Various features such as task type, priority,
    task arrival time, data privacy, data heterogeneity, and more are taken into account
    and then grouped based on these features. The simulation results highlight the
    superior cost-effectiveness and reduced number of missed tasks achieved by the
    deep learning method. In recent years, metaheuristic methods have become increasingly
    popular for their ability to discover optimal or nearly optimal solutions to task
    scheduling challenges in the context of fog computing [64]. The majority of the
    real-life IoT application scenarios consider task dependencies, which are depicted
    in the form of DAGs. In such cases, task scheduling is carried out in two phases:
    (1) Ordering the incoming tasks in some valid sequence; (2) Mapping the tasks
    to available computing resources. Hence, metaheuristics or evolutionary algorithms
    are preferred in place of traditional heuristics due to their capability to discover
    an optimized solution. In lieu of the same, Hosseinioun et al. [142] emphasized
    a power-aware solution utilizing Dynamic Voltage Frequency Scaling (DVFS) and
    a hybrid metaheuristic-algorithm enabled processor in a fog computing scenario.
    The purpose of DVFS is to provide appropriate voltage and frequency to the servers,
    which enables a sustainable solution for scheduling resources. The incoming tasks
    in the applications are ordered using a hybrid version of Invasive Weed optimization
    (IWO) and the Cultural Evolution Algorithm (CEA) to retain the precedence constraints
    for mapping them to an optimum number of resources. The work done not only maximizes
    task utilization but also provides an energy-efficient approach to task scheduling.
    Abdel-Basset et al. [143] highlighted the issue of energy efficiency while offloading
    tasks in fog computing using a metaheuristic technique called the Marine Predators
    Algorithm (MPA). The original algorithm is modified to propose a modified version
    (MMPA), which remediates the exploitation capability by utilizing the most recently
    updated position instead of considering the best one. The problem of resource
    scheduling is solved using certain steps, such as initialization, evaluation,
    normalization and scaling, and finally the application of the proposed MMPA technique.
    The initialization step establishes the predator’s vector size, which corresponds
    to the number of tasks to be scheduled by the fog nodes. The evaluation phase
    evaluates four parameters: makespan, energy, flow time, and CO2 emission rate.
    Then a bi-objective function is formulized, considering makespan and energy-efficiency
    as significant parameters. Normalization updates the continuous space into discrete
    values ranging from 0 to 1, and finally, the proposed technique is implicated.
    Furthermore, to enhance the MMPA, half of the population is reinitialized and
    subjected to mutation in the direction of the current best solution, while the
    remaining half is randomly generated to avoid potential local optima. The algorithm
    demonstrates improved results in comparison to other existing metaheuristic algorithms
    in terms of the QoS metrics considered. Hosseinzadeh et al. [145] use an improved
    version of the Butterfly Optimization Algorithm (BOA) for effective workflow scheduling
    in the Mobile Edge Computing (MEC) landscape. In addition to enhancing the convergence
    speed, the proposed technique also solves the problem of local optima by incorporating
    the Levy flight method. A diverse range of chaotic maps have been applied in the
    Discrete Version of the Butterfly Optimization (DBOA) algorithm, which results
    in discrete and randomization in the initial population. In addition, DVFS is
    incorporated into workflow scheduling to ensure optimal processor frequency and
    voltage for MEC virtual resources. A fog task scheduler utilizing the PSO algorithm
    with fuzzy logic incorporated into the fitness function is mentioned in work done
    by Javanmardi et al. [144]. The aim of the work is to optimally utilize fog resources
    to minimize application loop delays. The work refines the scheduling process by
    considering fog node characteristics (CPU compute, RAM and bandwidth) and task
    attributes (CPU need, memory required) for meeting the QoS requirements of delay-sensitive
    and delay-tolerant incoming IoT applications. The fuzzy logic assures that the
    task scheduler does not get stuck in local minima. Vijayasekaran and Duraipandian
    [101] proposed a two-phase solution. In the first phase, they incorporated the
    spectral clustering algorithm, an efficient clustering approach, to reduce data
    overlap and computational complexities within the edge computing framework. In
    the second phase, they employed deep learning-based resource scheduling to enhance
    resource utilization and decrease latency in processing user IoT requests. The
    comparison of performance matrix for resource scheduling problems for various
    state-of-the-art works is depicted in Table VIII. Along with this, all the investigated
    studies are depicted in Table IX. TABLE VIII Comparison of Performance Metrics
    for Resource Scheduling in Fog/Edge Computing TABLE IX Existing Solutions for
    Resource Scheduling in Fog/Edge Computing F. Service Placement Once the incoming
    requests are mapped by the resource manager, the main challenge is to place the
    incoming application on a specific fog node in a collaborative cloud-fog IoT computing
    environment. To resolve this issue, the fog broker, comprising task schedulers
    and other components as depicted in Figure 9, places them on fog nodes in clusters.
    Each cluster is governed by a fog master node, responsible for managing slave
    fog nodes. An optimal Service Placement (SP) solution ensure resource availability
    to minimize latency and, most importantly, meet deadlines for time-sensitive applications
    [146]. The task scheduler and SP module collaboratively ensures a fair share of
    resources for all requesting applications, as in a real-life scenario, multiple
    applications might compete for the same set of resources within the same time
    interval. Fig. 9. Service Placement Problem in Fog Computing Environment. Show
    All 1) Problem Formulation for Service Placement of Task at Optimal Destination:
    This phase computes the overall cost and minimizing the cost function. The overall
    cost associated during decision making phase of service placement C d,j Total
    can be calculated from Eq. (18), (21) and (24) as follows [133]: C d,j Total =
    ∑ j=1 m (1− Θ 2 d,j )∗ C d,j IoT + ∑ j=1 m Θ d,j ( Θ d,j −1) 2 ∗ C d,j FN + ∑
    j=1 m Θ d,j ( Θ d,j +1) 2 ∗ C d,j DS (25) View Source In similar manner, the total
    energy consumed E C d,j Total from Eq. (17), (20) and (23) E C d,j Total = ∑ j=1
    m (1− Θ 2 d,j )∗E C d,j IoT + ∑ j=1 m Θ d,j ( Θ d,j −1) 2 ∗E C d,j FN + ∑ j=1
    m Θ d,j ( Θ d,j +1) 2 ∗E C d,j DS (26) View Source Finally, total cost ϑ d,j can
    be computed combining Eq. (25) and (26), which is computed as follows: ϑ d,j =
    Ψ 1 ∗ C d,j Total + Ψ 2 ∗E C d,j Total (27) View Source where, Ψ 1 and Ψ 2 are
    weight factors. The optimization function can be formulated as: min ϑ d,j ;subjected
    to constraints Θ d,j ∈{0,−1,1} (28) View Source And, the additional constraints
    are depicted in Eq. (4), (5), (6) and (16). 2) Service Placement Challenges: The
    following are research challenges in service placement: Task Dependencies: The
    majority of IoT applications, like augmented reality and image recognition, are
    modelled as Directed Acyclic Graphs (DAGs) in diverse topologies. The nodes depict
    the tasks, whereas links delineate the data communication pathway. Consequently,
    such applications sustain intricate dependencies and constraints when it comes
    to the decision of service placement. Doubtlessly, SP is a combinational optimization
    problem [147]. Device Heterogeneity: The device heterogeneity in IoT is result
    of varying configurations (hardware and software) along with vendor-specific product-related
    specifications [148]. Moreover, apart from the collection of sensors and actuators,
    IoT technology thrives for high-end computing tasks such as routing and switching
    for heavy-duty tasks. Due to the same reason, this aspect is challenging and will
    be considered during the solution design and implementation phases. 3) Existing
    Solutions for Non AI-Based Service Placement: This section presents some state-of-the-art
    works based on polynomial and mathematical models. A study conducted by Yu et
    al. [149] provided a solution to the above-stated problem from a network perspective.
    In the case of real-time IoT processing, the underlying infrastructure determines
    the best FN residing on the host, along with the channels where the application
    data stream will be transmitted. The author presents the provisioning problem
    in the context of single and multiple applications. A Fully Polynomial-Time Approximation
    Scheme (FPTAS) is used in the case of single applications, whereas multiple applications
    can be parallelized amongst various instances. In the case of non-parallel applications,
    the author proposed a randomized algorithm, where the incoming application is
    assigned to a specific host only. 4) Existing Solutions for AI-Based Service Placement:
    Most of the existing work considers centralized DRL agents, that lack generalizability
    and quick adaptability in heterogeneous and stochastic fog computing environments.
    DRL agents excel at obtaining optimal policies and long-term rewards with no prior
    knowledge of the operational environment. However, fog computing operates within
    a stochastic environment characterized by an extensive state space. In order to
    learn about the environment, an exploration phase is carried out, which involves
    trial and error, and correspondingly, the experiences are recorded in the form
    of a sequence of states, actions and rewards. But this leads to increased exploration
    costs and time due to the large number of interactions required for optimal agent
    training. Therefore, centralized DRL is not considered suitable for highly distributed
    fog computing scenarios. To deal with the same, Goudarzi et al. [102] presents
    an Experience sharing Distributed-DRL is called the X-DDRL SP technique, in which
    different sets of experience trajectories are produced in parallel fashion, which
    further assists the agent in training and learning optimal policies. Multiple
    agents interact with the environment simultaneously, and the resulting trajectories
    are regularly sent to the learning system for developing an optimal policy. The
    actors synchronize their parameters with the learner’s during each policy update,
    and the actors independently conduct their exploration. Hence, this collaborative
    and distributed learning approach enables experience sharing, which reduces exploration
    costs and improves the reuse of experience trajectories. In addition, the proposed
    distributed Actor-Critic-based technique enables the capture of the temporal behavior
    of incoming data via interconnected layers, and the replay buffer enhances the
    overall efficiency. The goal of using experience replay is to improve sample efficiency,
    which refers to how effectively the model learns from the data it collects. By
    breaking the correlation between experiences, the learning process becomes more
    efficient and reliable. Finally, RNNs are employed to accurately identify temporal
    patterns within the data. The efficacy of the proposed model has been demonstrated
    by extensive simulation and testbed experimentation. The results reveal an 8–16
    times faster performance gain in contrast to other DRL-based techniques, along
    with improved application execution time, cost, and energy consumption. Another
    work done by Sami et al. [150] provides dynamic solutions inspired by MDP to formulate
    proactive fog selection and placement solutions. The proposed work utilizes DRL
    agents to make placement decisions before actual demand occurs. An energy-efficient
    dynamic service migration scheme for higher QoE in edge computing is presented
    by Chen et al. [151]. The study integrates cognitive learning, which encompasses
    self-learning technologies involving pattern recognition, data mining, and Natural
    Language Processing (NLP) to simulate human intelligence. Within the suggested
    Edge Cognitive Computing (ECC) platform, dynamic service migration occurs, guided
    by the behavioral analysis of mobile user-generated traffic data and network resource
    conditions. In contrast to the commonly deployed models of cognitive learning,
    which involve training the machine learning-based models on the cloud and carrying
    the analytics and inference parts on the edge. The proposed work emphasizes incorporating
    training and inferencing ML models on the edge itself to further improve latency.
    ECC not only resolves the problem of computation; furthermore, it aids in knowing
    “what” to compute and “where” to compute. The author has introduced a framework
    that comprises both the edge network and edge cognition. The former involves interconnected
    heterogeneous edge devices, while the latter is divided into two components: (1)
    the Data Cognitive Engine and (2) the Resource Cognitive Engine. The Data Cognitive
    Engine is responsible for real-time IoT data analysis and provides network processing
    capabilities at the network edge. For example, it utilizes Deep Convolutional
    Networks (DCN) for facial emotion recognition and Hidden Markov Models (HMM) for
    user mobility prediction. The insights derived from this analysis are then fed
    to the Resource Cognitive Engine, which reinterprets the data to generate new
    information. This new information is subsequently utilized by the Data Cognitive
    Engine. Additionally, the Resource Cognitive Engine receives the analyzed results
    from the data engine to perform functions such as admission control, resource
    scheduling, and traffic monitoring. Its caching capability ensures the availability
    of predicted content at the edge layer in advance, reducing the load and latency
    on the underlying core network. The application of metaheuristic techniques in
    service allocation has been highlighted by the work of Mishra et al. [152], which
    implements different nature-inspired algorithms like PSO, Binary PSO (BPSO), and
    BAT. The problem of assigning an appropriate target VM to an incoming user request
    is depicted as a bi-objective minimization problem. The author addresses the issue
    of optimal scheduling of incoming task requests to VMs (execution units in a fog
    environment). The results were simulated by an in-house simulator and MATLAB,
    and the above-stated techniques have been compared in terms of energy efficiency
    and makespan. The study conducted shows that BAT outperforms other Swarm intelligence-based
    metaheuristics in the mentioned performance matrix. Ghobaei-Arani and Shahidinejad
    [104] depicted the SP as an autonomous placement approach for diverse IoT applications
    onto the fog/edge infrastructure. The author has worked towards ensuring QoS requirements
    for requesting IoT devices in the form of predicting potential resources for fog
    using Whale Optimization Algorithms (WOA). This is a stochastic approach inspired
    by the humpback whale hunting strategy, employed for optimizing computationally
    challenging problems to achieve efficient resource allocation. Humpback whales
    belong to a unique category of whales known for using bubble-net feeding techniques
    in their prey hunting. This technique operates in three distinct phases: encirclement
    of the target, exploitation, and exploration to cover the entire search space.
    The effectiveness of this proposed method has been demonstrated through improvements
    in resource utilization, acceptance ratio, and energy efficiency. The baseline
    algorithms referred to are Genetic Algorithm (GA), PSO, BAT Algorithm (BAT) and
    Simulated Annealing (SA)-Fog Service Placement (FSP). The authors have proposed
    the implications of a hybrid model consisting of the amalgamation of Grey Wolf
    Optimizer (GWO) and SA for further amelioration of results. Another conceptual
    framework for IoT service placement in fog environments is proposed by Liu et
    al. [153], which comprises of Cloud Fog Control Middleware (CFCM). This module
    is responsible for managing incoming service requests and satisfying constraints.
    CFCM does all decision-making based upon MADE-k automatic control loops (M-Monitoring,
    A-Analysis, D-Decision making, E- Execution and k-shared knowledge base). The
    Fog Service Placement Problem (FSPP) is modelled as a multi-objective dynamic
    optimization problem based on the Cuckoo Search Algorithm (CSA). This prominent
    metaheuristic algorithm works with the behavior of cuckoo birds, which comprises
    adult cuckoos laying eggs and migrating as per environmental factors in a search
    for an optimal destination that suits their lives and laying eggs. Considering
    the implications of this metaheuristic in the context of service placement, the
    best environment would refer to achieving the global optima of objective function.
    Nevertheless, apart from improving QoS parameters, the work addressed prioritizing
    the incoming requests based on task deadlines. Table X depicts the comparison
    of various QoS parameters used to analyze existing studies in the domain of service
    placement. The majority of the works have been demonstrated via simulations using
    a VM-based fog nodes framework in a two-tier fog architecture for optimal service
    placement in IIoT applications. The authors address the problem of resource provisioning
    and IoT service placement as a multi-objective optimization problem that aims
    at minimizing service cost, energy usage, and service time (the sum of processing
    and communication time) for context-aware decisions in Industry 4.0. A hybrid
    version of Modified GA and PSO (MGAPSO) has been proposed and implemented in a
    testbed comprising 20 fog nodes harnessing the master-worker model. The result
    obtained signifies improvement in terms of the parameters stated above in comparison
    to First-Fit (FF), Branch & Bound (BB), Double Matching System (DMS), GA, PSO,
    etc. Finally, The key points of all surveyed work in the domain of service placement
    have been illustrated in Table XI. TABLE X Comparison of Performance Metrics for
    Service Placement in Fog/Edge Computing TABLE XI State-of-the-Art Solutions for
    Service Placement in Fog/Edge Computing G. Resource Allocation and Load Balancing
    The fog computing landscape comprises a load balancer module that manages the
    distribution of incoming IoT workloads evenly amongst available resources. The
    dynamism that occurs in fog infrastructure might lead to an increase or decrease
    in the number of active fog nodes due to variant workloads. Henceforth, the heterogeneity
    in fog nodes in terms of computational capability is the principal cause for the
    non-applicability of cloud load balancing solutions in fog computing environments.
    For a similar reason, from a resource management point of view, load balancing
    becomes a tough task. A specialized component called a load balancer, after receiving
    requests from users, runs load-balancing algorithms and then further selects and
    distributes the requests among VMs available on the underlying fog node. It ascertains
    that no node is overloaded or underloaded. It is the last step in the resource
    management lifecycle, which is represented by Figure 10. Fig. 10. Load balancing
    and Resource Allocation in Fog/Edge Computing. Show All 1) Problem Formulation
    for Load Balancing: The Load Balancing Degree (LBD) serves the purpose of distributing
    workloads among numerous fog nodes to enhance resource efficiency and performance.
    Its primary objective is to address the challenges associated with resource over-utilization
    and under-utilization. Assigning workloads to each fog server or cloud data center
    is crucial, as it is not an ideal solution to have some nodes overloaded while
    others remain idle. Therefore, it plays a crucial role in distributing and balancing
    the workload among nodes [156]. It is computed from the makespan time MS T j,x
    , which is as follows: MS T j,x = DLB= ∑ j=1 m ∑ i=1 n+x+1 ( T d,j IoT_Compute
    + T d,j FN_Compute + T d,j DS_Compute ) ∑ x i=1 max(MS T j,x )− ∑ x i=1 min(MS
    T j,x ) mean(MS T j,x ) (29) (30) View Source 2) Resource Allocation and Load
    Balancing Challenges: Interoperability: It’s important to provide consumers with
    the choice to migrate from one fog/edge-based platform to another in a customized
    way with load balancing, keeping in mind factors like cost and functionality [157].
    Fog/Cloud Dynamism: Although cloud and fog/edge work in harmony with one another,
    both dominant technologies rest upon different dynamics. Where the cloud rests
    on centralized datacenters, in contrast, edge/fog servers are distributed by nature.
    In addition to this, the capacity of edge devices to dynamically reconfigure themselves
    for various applications by offloading a variety of tasks introduces an additional
    layer of dynamism to the edge ecosystem. 3) Existing Solutions for Non AI-Based
    Resource Allocation and Load Balancing: This section provides an overview of currently
    employed approaches for allocating resources. Sthapit et al. [105] have employed
    a sensor network utilizing a network of queues for implementing computational
    load balancing in the absence of cloud and fog layers. The scheduling decisions
    are based on a linear programming model. On executing 100 Monte-Carlo simulations,
    the results reinforce the fact that the proposed model can compute the incoming
    jobs efficiently when the total job rate is less than the total computational
    capability of Node State Information (NSI). Another study by Ahmad et al. [158]
    describes effective resource utilization in Smart Grids (SGs). The authors have
    considered a scenario comprising a smart building with multiple apartments acquiring
    smart gadgets and devices. The request is fulfilled by the nearest grid, which
    is further connected to the centralized cloud. The closest datacenter service
    broker policy is used for selecting the fog nearest to the cluster of apartments.
    Conventional legacy approaches to allocating resources and load balancing are
    restricted by their static nature (fixed resource pool), which fail to find the
    optimal solution in a dynamic and heterogeneous environment. Hence, there is a
    need to explore dynamic resource allocation approaches that can predict workload
    changes automatically and adjust in accordance with resource needs. Hence, the
    subsequent section discusses AI-based solutions to the resource allocation problem.
    4) Existing Solutions for AI-Based Resource Allocation and Load Balancing: As
    per Forbes 2022 trends, the complexity of devices has been increasing due to the
    emergence of high-tech gadgets enabling VR, AR, and MR [162]. Extended Reality
    (XR) and smart applications thrive on the exquisite allocation of distributed
    resources in the close computing paradigm. This section highlights the existing
    work on the applicability of AI techniques, including ML, fuzzy logic, and metaheuristics,
    in this domain. Singh et al. [160] presented a fuzzy-based load balancer that
    handles overloading, underloading and disparity in resource utilization. The fuzzy-based
    three-tier framework is based on software-defined IoT task distribution. The rule-based
    fuzzy technique enables the handling of diversified incoming traffic patterns
    among IoT devices. This includes the type of incoming load (video, audio, Web,
    sensor data, etc.), which is generally unstructured in nature, traffic arrival
    time, etc., in a collaborative cloud-fog environment. The experimentation demonstrates
    improvements in resource utilization and cost reduction. In addition, the author
    inferred the superiority of 3-level fuzzy model design in comparison to 5-level
    and 7-level traffic controllers, supporting the fact that increasing the number
    of layers results in an overwhelming fuzzy system with overlapping, inconsistent
    fuzzy rules and nevertheless increases the complexity in terms of the if-then
    rule base. Talaat [106] presented a resource allocation technique based on effective
    predictions for ensuring QoS parameters. DRL helps in achieving load balancing
    amongst fog nodes whereas Probabilistic Neural Networks (PNN) are trained for
    providing predictions. The authors focus on real-time resource allocation being
    utilized in the smart healthcare sector to diagnose the probability of heart attack
    occurrence. It works in three distinct phases: data processing, resource allocation,
    and making effective predictions. Here, PNN assists in making predictions regarding
    the occurrence of heart attacks by training the model. To achieve optimized dynamic
    and real-time allocation of resources, load balancing in fog nodes has been explored
    using metaheuristic algorithms by Baburao et al. [161]. The study utilizes the
    concept of containerization in order to create microservices applications, which
    provide a lightweight solution in comparison to VMs. The Load Balancing (LB) algorithm
    bifurcates the incoming workload for assigning it to optimal fog node based upon
    computational resource availability utilizing PSO. Each incoming request from
    an IoT device is mapped to a particle, and the shortest path is calculated corresponding
    to the nearest available fog node for mobile IoT users. Lastly, the authors in
    [133] optimized the resource allocation decision, via autonomic workload prediction
    of incoming IIoT requests using metaheuristic techniques. This work predicts the
    incoming workload using the proposed autoencoder deep learning model. Then, after
    making predictions about the resources, the incoming job is allocated to an appropriate
    resource using the Crow Search Algorithm (CSA) by optimizing multiple objectives.
    As fog nodes are battery-driven, the allocation of incoming applications to a
    particular node shall consider the same to avoid application execution failover.
    In such cases, migrating the application to the backup fog node will further increase
    the response time, failing to satisfy time-sensitive requirements. For the confrontation
    of the same, Naha et al. [79] present an energy-aware AI-based technique for allocating
    resources. The fog nodes are fed with information related to CPU utilization and
    energy usage patterns from datasets, which is later processed using the proposed
    Multiple Linear Regression (MLR). The proposed approach makes predictions as per
    constraints such as task deadlines and energy in order to make optimal decisions.
    The author considers an experimental scenario which comprises independent and
    dependent variables. Here, the time it takes for the application to complete its
    execution serves as the primary independent variable, influenced by four other
    independent variables: CPU usage, node mobility, network communication, and response
    time. The following MLR equation depicts the calculation of execution completion
    time: E T A x = β 0 + β 1 CP U U d r + β 2 D M d r + β 3 N C d r + β 4 R T d r
    +∈ (31) View Source where, E T A x : Execution time of Application x CP U U d
    x : CPU utilization of device r D M d r : Device mobility N C d r : Network communication
    of device r R T d r : Response time of device r ∈ : symbolizes the error rate,
    denoting the variance between the predicted values from the multiple linear regression
    (MLR) model and the real observations. β 0 , β 1 , β 2 , β 3 and β 4 are the coefficients
    that represent the regression line slope. Noteworthy, the purpose of using MLR
    lies in the fact that the predictor variable depends on multiple quantitatively
    independent variables; hence, simple regression cannot be used. Hence, based on
    the energy usage of the fog devices, the best device is selected for energy-aware
    application processing. The work has been simulated using the extended CloudSim
    version along with the Planet Lab workload dataset and compared with the Fog Computing
    Architecture Network (FOCAN) [163]. The deadline constraint has been accompanied
    by an energy component using a hybrid approach that minimizes processing delay,
    time, and SLA violations. Table XII provides a list of various datasets along
    with QoS comparisons of existing works, which will serve as a benchmark for evaluating
    new techniques. Li et al. [80] introduced intermediary nodes between the edge
    and cloud layers. TABLE XII Comparison of Performance Metrics for Allocating Resources
    and Load Balancing in Fog/Edge Computing These nodes have the task of gathering
    real-time data regarding the attributes of fog nodes. This global information
    monitoring establishes the foundation for classifying edge nodes into categories
    of light, normal, and heavy workloads. The task assignment module selects the
    node with the lightest workload, while the other nodes temporarily remain unassigned
    to tasks, promoting dynamic load balancing. In addition to obtaining the state
    information of nodes, the intermediary nodes also diminish the pressure on edge
    nodes. Where most of the works discussed are based upon unilateral computing,
    implying job deployment either on the edge or on the cloud, Dong et al. [159]
    proposed a joint Cloud-Edge datacenters approach for resource allocation, which
    is based on the pruning algorithm and DRL. Firstly, a joint host set is formed
    by combining the physical hosts from cloud data centers and edge computing centers.
    In other words, the monitor module receives this “cloud-edge” physical host information
    along with the task set. Further, the pruning algorithm eliminates fewer promising
    hosts to create a non-dominated host set, which then serves as the starting point
    for the DRL process. The work fuses the idea of the Deep Deterministic Policy
    Gradient (DDPG) of DRL. This technique undergoes behavioral modifications after
    several iterations of learning from the environment. The main advantage of using
    pruning as a pre-processing technique is that it reduces the state space, which
    ultimately reduces the complexity of RL. Hence, making the algorithm more robust
    during its running phase via effective task deployment and load balancing. The
    environment is continuously explored for effective computational ability and load
    balancing. Table XIII depicts the discussed work. TABLE XIII State-of-the-Art
    Solutions for Resource Allocation and Load Balancing in Fog/Edge Computing SECTION
    III. Review Methodology Now a days, Systematic Literature Reviews (SLR) are becoming
    paramount, supplementing the minds of researchers with ingenious knowledge by
    supplying a repository of existing literature in a systematic manner. The SLR
    encompasses three distinct stages: planning, conducting and reporting the review.
    In the planning phase, we define the research objectives, pinpoint the research
    domains, and apply inclusion and exclusion criteria to select the specific research
    areas of interest. A. Planning the Review The process of conducting the review
    involves identifying primary studies, implementing inclusion and exclusion criteria,
    and finally generating the results. The electronic databases have been searched
    extensively, and the respective studies have been reported. In addition to this,
    some of the leading journals in fog/edge computing that did not appear in electronic
    searches have been searched manually. The study selection procedure is shown in
    Figure 11. This survey demystifies the emerging computing paradigms with their
    architectural framework and integration of various thrust technologies to enhance
    the QoS parameters in integrated next-generation computing paradigms. This survey
    discusses the following research questions and the sub-questions identified, including
    motivation for work. Fig. 11. Selection Criteria used in this SLR. Show All B.
    Research Questions The identification of research questions, as summarized in
    Table XIV, channels the flow of various processes, hence systematizing the reviewing
    methodology. TABLE XIV Research Questions and Motivation C. Sources of Information
    The efficacious content of our work has been collected from several sources. The
    keywords delved into were Fog/Edge computing, AI, Resource Management, IoT, Software
    Defined Networking (SDN), Industrial IoT (IIoT), Digital twins, Quantum Computing,
    Federated Learning, Serverless Computing, 5G and Blockchain. In addition to this,
    the survey has been emphasized by searching for the role of thrust technology
    like 5G, blockchain, SDN, Digital Twin, Industry 4.0, IIoT, and Federated Learning
    (FL). The databases searched are as follows: ACM Digital Library (https://dl.acm.org/journals)
    IEEE Xplore (https://ieeexplore.ieee.org) Web of Sciences (https://wos-journal.com/)
    Science Direct (https://www.sciencedirect.com/) Taylor and Francis Journal (https://www.tandfonline.com/)
    Elesvier (https://www.elsevier.com) Emerland (https://www.emerald.com) and other
    resources Additional Sources E-Scientific research databases. Books and Technical
    Reports National Digital Library D. Search Criteria The majority of the searches
    comprise the keyword “Fog/Edge” and “Resource Management” is included in the abstract.
    The following strings of words are applied using the Boolean operators AND and
    OR for combining the keywords, which are as follows: {(“Resource Management” OR
    “Resource Provisioning” OR “Task Offloading” OR “Resource Scheduling” OR “Task
    Scheduling” OR “Service Placement” OR “Resource Allocation” OR “Load Balancing”)
    AND (“Artificial Intelligence” OR “Machine Learning”) AND (“Fog computing” OR
    “Edge Computing”)} E. Inclusion and Exclusion Criteria The implications of AI-based
    approaches in Fog/Edge computing are relatively new research areas. In addition
    to this, a major chunk of our referred works lies in the past 6 years, which will
    assist researchers in enhancing their skills with the latest AI techniques and
    making an impact in the arena of IoT-assisted fog computing. To refine our work,
    inclusion and exclusion criteria have been applied to filter out insignificant
    papers. The above-mentioned search keywords and combinations were framed to narrow
    down the available academic databases to the most relevant articles. Due to the
    high potential of Web of Science (WoS) journals, we have taken the research work
    (journals, transactions, and conferences) indexed in WoS with peer-reviewed methods
    into consideration for AI-enabled resource management in fog/edge. Initially,
    490+ papers were considered at the start of the process. But to find eminent publications,
    an extensive screening process was carried out to filter out non-peer-reviewed
    articles, conferences, and book chapters that were not capable of contributing
    to our research domain. Henceforth, our subsequent steps shortlisted 223 potentials.
    F. Quality Assessment To compile the best available research on this topic, we
    used a systematic review approach in accordance with the “Centre for Reviews and
    Dissemination (CRD) guidelines” provided by Kitchenham [160]. Additionally, there
    are a number of academic articles and conference proceedings on AI for fog/edge
    computing. After implementing the criteria for exclusion and inclusion, we performed
    a quality assessment of the articles that fulfilled the standards to determine
    which were most deserving for further review. We used the criteria set by the
    CRD to assess the research’s overall quality, including its fairness, internal
    cohesion, and neutrality. SECTION IV. Exploring Future Research Directions: Demystifying
    Thrust Technology in Integration With Fog/Edge In this section, we spotlight the
    integration of emerging paradigms with thrust technology, which is evolving as
    a potential research trend. This integration is taking the existing functionality
    of fog computing to the next level by inculcating advanced and sophisticated techniques
    such as providing a hyper-personalized space for privacy preservation of IoT data,
    containerization, high-end telecommunication networking capabilities, and many
    more. A. Integration of Fog/Edge With Serverless Computing The ideology of serverless
    computing can be considered a pay-as-you-go model in the cloud paradigm, along
    with leading-edge technologies such as microservices, containerization, event-driven
    modelling, Function-as-a-Service (FaaS), and Backend-as-a-Service (BaaS) [41].
    The Cloud subscribers bear the consequences of paying for the resources allocated,
    not for the resources utilized, and the scalability drawback, wherein the configuration
    of auto-scalars is preordained based upon the load profile and application characteristics.
    Furthermore, the rapid shift from monolithic systems to SOA and then the final
    paradigm shift to microservices applications, recognized the possibility of running
    small pieces of code as functions, well-known as FaaS, hence leading to the emergence
    of serverless computing [164]. Apart from easing out the process of server management,
    it executes different events that simplify the backend code. For instance, the
    generation of an IoT event that originated from a home security sensor might invoke
    a lambda function that further notifies the user on the end device. Another example
    might include creating a database event that triggers a serverless function and
    then finally prompting the customer with an email. Other events might include
    HTTP requests, file uploads, or database analytics. Therefore, it can be called
    the next promising shift in the cloud era, with the full realization of cloud
    capabilities comprising disjointed functions called Lambda functions. Although
    the term was originally coined for the cloud, serverless is locating itself in
    the fog/edge landscape, with IoT as its prime partner. Seamless integration of
    serverless computing with fog/edge is complex due to the significantly larger
    count of fog nodes and their distributed nature [165]. Most of the IoT devices
    are connected through various communication mediums such as Wi-Fi, Bluetooth,
    ZigBee [166], and 2/3/4/5G/LTE networks. These devices create incoming tasks using
    messaging protocols like HTTP and Application Programming Interfaces (APIs), following
    event-driven principles. However, due to the distributed nature of IoT devices,
    such as the temperature sensors spread throughout a smart city’s manufacturing
    plant, relying solely on HTTP may not be sufficient [94]. Hence, the Serverless
    edge enables the capability to push/publish via communication protocols comprising
    Message Queuing Telemetry Transport (MQTT), MQTT-SN and Data Distribution Service
    (DSS) [96]. B. Integration of Fog/Edge With 5G The emergence of 5G has ushered
    in a wide range of unprecedented applications, offering enhanced mobile broadband,
    ultra-reliable connections, improved data rates, low latency, and extensive device
    connectivity. 5G serves as the fundamental pillar for enabling the new AIoT economy.
    Corporations are looking at the IoT as the next industrial revolution with its
    capability of imparting intelligence into their operations, building smart factories,
    hospitals, campuses, cities, businesses etc. These application domains necessitate
    advanced communication services ensuring data security, real-time communication,
    and widespread device connectivity. But at the same time, it becomes challenging
    to manage and serve the enormous number of devices with the existing cellular
    network architecture. To maintain the QoS parameters, the service providers are
    working towards expanding their Base Stations (BS). BS provides a consistent number
    of resources all the time, whereas mobile users utilize services only at discrete
    intervals, which results in futile resource management. Resource management has
    always been a perplexing problem in cellular networks because of the underlying
    heterogeneous resources and dynamic workload requirements. To help with the same,
    AI is one of the most dominant technologies and plays a significant role in almost
    all spheres of industrial domain applications. Concomitantly, it requires extra
    compute, memory resources, and training time for its decision-making. In order
    to transfer big data seamlessly across Baseband Units (BBUs) and Remote Radio
    Head (RRH), edge computing-enabled 5G networks are bringing cloud capabilities
    in close proximity to the end user. This strategic placement effectively mitigates
    the inherent challenges associated with high latency and security gaps found in
    conventional architectures [168]. This integration will not only decrease the
    transmission of useless data but also solve bottleneck issues like congestion.
    The edge devices (IoT gateways, routing switches) will assist the functionality
    of AI by filtering out useless data beforehand, which will result in a reduction
    in transmission backhaul. The incorporation of the Edge layer will assist BBU
    in autonomously configuring its underlying resources under real-time changes in
    its environment. However, the swift expansion of tech-savvy devices along with
    IoT devices is eventually overburdening the existing portable remote sensor networks.
    Even after the incorporation of AI with 5G & beyond and Industry 4.0, real-life
    application domains like medical service management and transportation frameworks
    suffer from some key issues like radio resource management optimization and interference
    management [169]. C. Integration of Fog/Edge With IIoT It refers to the utilization
    of selected IoT sensors and actuators in the industrial arena to enhance manufacturing
    and industrial processing capabilities without human intervention [170]. It focuses
    on digitizing and integrating all essential physical processes across the entire
    organization [171]. The IIoT, also called the Industrial Internet, is a metamorphic
    change after the industrial and Internet revolutions that drastically impacts
    the way industries function. It is a significant paradigm shift from traditional
    centrally controlled machines to more decentralized functioning capabilities.
    It utilizes a new software-defined machine framework that virtualizes the machine’s
    functionality in software, enabling seamless monitoring and management of industrial
    assets via remote access. With the advent of Industry 4.0, a prodigious amount
    of time- and delay-sensitive data is being generated by machines. Moreover, the
    sensors embedded in industrial equipment are resource-constrained and battery-driven.
    Henceforth, executing all the computational workload on these devices might drain
    them quickly. To realize the system from a resource perspective, the workload
    compute capability of IoT devices can be boosted by introducing a fog layer. As
    discussed by Sengupta et al. [172] the IoT devices transmit raw data to the closest
    fog node through Wi-Fi access points. For data with strict time sensitivity, the
    fog node analyzes the incoming request and sends a control command back to the
    respective device. In contrast, data with lower time constraints is sent to the
    cloud for extended storage and large-scale data analytics. The integration of
    the fog layer enhances resource performance by extending the battery life of sensing
    devices by offloading computation-intensive tasks to the cloud. This integration
    also diminishes the trust dependency on the cloud by utilizing it for storage
    and archival purposes. The decentralized nature of FNs imposes several challenges,
    such as global monitoring and controlling the computing states of FNs. Furthermore,
    the diversified nature of tasks in fog-enabled IIoT introduces a misalignment
    between the anticipated computational efficiency and the allocated resources on
    fog nodes. The problem has been addressed in the form of service popularity-based
    smart partitioning of resources for fog-enabled industrial IoT [173]. Still, fog-enabled
    IIoT suffers from trust establishment problems. To resolve these problems, the
    same blockchain-based security service architecture is proposed by Hewa et al.
    [174]. The proposed work facilitates cloud manufacturing equipment authentication,
    channel privacy protection, and the unlikability of transactional data over blockchain
    records. Use Cases: This subsection discusses the practical implications of Fog/Edge
    in IIoT. Smart Pump: An IIoT-enabled smart pump equipped with predictive analysis,
    maintenance, and machine learning is capable of imagining a potential failover.
    Such systems prepare in advance for the downtime instead of encountering a whole
    system failover. Smart Metering: This capability can be exploited in the arenas
    of measuring energy, natural gas, water consumption, and many more. Fleet Management:
    It holds applications in smart transportation systems where, for instance, the
    most efficient routes are calculated for waste management collection vehicles.
    This system is further strengthened by real-time traffic feeds and efficiency
    algorithms. Jet Engines: A fleet of locomotives, particularly airplanes equipped
    with IIoT sensors, can foresee fuel requirements or any other type of failover.
    This technology works with insight to impart zero unplanned downtime. With its
    advent, the maintenance course of the plane can be predicted even before the plane
    lands, thereby preventing unscheduled maintenance events. D. Integration of Fog/Edge
    With Blockchain IoT, in conjunction with the fast-propelling Industry 4.0, requires
    the gathering, analysis and sharing of raw data from which valuable information
    is extracted. Nevertheless, IoT security and data privacy remain major issues
    as the accumulated data is exposed to vulnerabilities. Any malicious user can
    track devices and continuously listen to conversations between IoT devices, which
    ultimately leads to a breach of privacy. To add to that, upcoming researchers
    have proposed the integration of blockchain in order to revolutionize the business
    process, which anticipates greater data integrity in fog-enabled IoT networks
    [175]. The integration of blockchain in fog-enabled IoT networks is revolutionizing
    broad spectrum of fields such as industries, retail, finance, the public sector,
    and above all, technological aspects, as depicted in Figure 12. Fig. 12. Applications
    of Blockchain-enabled Fog/Edge computing. Show All Blockchain is prominent for
    trusted transactions based on the concept of a distributed, shared, replicated
    and permissioned ledger, where the transactions are provably endorsed by relevant
    participants. It constitutes independent computers called nodes, which enable
    the sharing and synchronization of transactions in their corresponding electronic
    ledgers instead of maintaining a centralized ledger. The shared ledger contains:
    (1) immutable blocks containing a set of transactions that are chained together
    in append-only mode. The distributed ledger acts as the building block of the
    “Internet of Value,” where value is transferred from peer to peer. Here, value
    could be any identity, health information, personal data, and many more. (2) World
    State: stores the current state of assets, which includes an ordinary database
    (key/value store). The architecture flow of integration of blockchain with IIoT,
    Industry 4.0 in fog, and edge-enabled IoT networks is demonstrated in Figure 13,
    which depicts how digital trust can be implemented in the healthcare domain via
    blockchain. Initially, all the active actors, including healthcare workers (doctors,
    nurses) and patients, are registered with the main healthcare service provider.
    The parameters (like ID, name, age and gender) are stored as Ethereum addresses.
    The patients authenticate themselves in the proposed framework and are authorized
    to consult their own data at any time. Granting the right permissions is a vital
    ingredient for the efficient implementation of the proposed system. Afterwards,
    data processing is done via specific sensors relating to certain diseases, for
    instance, electrocardiogram (ECG) and blood-oxygen saturation (SpO2) sensors for
    heart patients, sugar levels for diabetic patients and so on. This phase involves
    data acquisition by various sensors and then storing the same in a blockchain.
    Storing such sensitive information directly on the cloud is not recommended as
    it can be deleted or tampered with by malicious actions which can be life-threatening
    for the patient. Noteworthy, such data storage requires resources on the cloud
    and blockchain; hence, periodic data storage like acquiring temperature sensor
    readings every 1 or 2 minutes is not a good choice. Hence, only vital values exceeding
    the threshold value are stored in the blockchain for further analysis by the application
    of AI algorithms. Afterwards, data is transferred via gateway to the edge/fog
    devices. This is where data processing takes place for real-time data, such as
    storing data in blockchain format and recovering patients’ parameters from blockchain.
    Blockchain-enabled fog paradigms are cooperative rather than competitive in nature.
    At this stage, smart contracts perform the verification of credentials at the
    fog layer. Finally, processed results can be accessed via a Web application or
    mobile application. The real-time parameters are accessed, and the reports are
    uploaded in Interplanetary File System (IPFS) format to the blockchain, which
    can further be stored on the cloud for future monitoring of the patient’s parameters
    [176], [177]. Fig. 13. Integration of IIoT, Industry 4.0, Blockchain and 5G with
    Fog/Edge computing. Show All E. Integration of Fog/Edge With Digital Twin With
    the inception of Industry 4.0, the journey towards automating traditional industrial
    practices emerged. Gartner estimates that by 2027, over 40 percent of the major
    industrial companies will utilize Digital Twins, in order to improve their revenue
    and operational effectiveness [205]. With a digital twin as its counterpart, which
    aims at replicating elements, processes, functions, and dynamics of the physical
    world come into a digital counterpart. The integration of the digital twin eases
    out processes such as monitoring, testing, and evaluation, along with predictive
    analytics of complex, which otherwise would have been out of question using traditional
    simulations [177]. The first pragmatic model was built in the form of a virtual
    spacecraft by NASA in 2012, which received real-time inputs from sensors [178].
    With the latest developments in ML, AI, VR, AR, next-generation mobile communications
    (beyond 5G), Transfer learning and many more along with emerging computing paradigms,
    the digital twin has been reshaped with its enhanced capabilities, covering a
    wide range of domains including logistics, smart cities, smart manufacturing,
    healthcare, and robotics under its umbrella. Talking about its legitimate applicability,
    Digital Twins technology is incorporated into IoT devices that are confined to
    a particular region. For such low-powered devices, the cloud alone cannot assure
    optimal QoS services for latency and real-time devices. Hence, Fog/Edge becomes
    indispensable for the inculcation of the framework of the Digital Twin by reducing
    connectivity and latency issues in networks, thereby empowering the system with
    robustness. F. Integration of Fog/Edge With Quantum Computing The idea emerged
    from transistor computing when Richard Feynman, a Nobel Prize-winning physicist,
    realized that atoms comprising transistors can exist in both high and low states
    simultaneously. In contrast to the classical bit, which holds only two values,
    zero and one, a quantum bit, or qubit, holds a complex coefficient value. The
    term envisioned for this behavior was coined Quantum Superposition State. Thus,
    quantum bits, or qubits (which hold a complex coefficient value describing a particular
    state), came into the picture, which are the fundamental building blocks of quantum
    computers [179]. Quantum Computing as a technology is still in its infancy, but
    it can be utilized to calculate intractable things around us. The fidelity of
    this computing paradigm is going to touch many applications and industry verticals.
    Currently, serverless computing-enabled fog/edge frameworks, driving function
    as a service, necessitate quantum computing for processing massive computations
    for dynamic provisioning and load balancing of underlying resources [180]. G.
    Integration of Fog/Edge With Federated Learning Assuring optimal resource management
    with known execution times in a classical edge computing landscape is a tedious
    task [181]. It becomes practically infeasible to estimate the execution time due
    to the complex framework of the edge server. Furthermore, processing colossal
    amount of data aggregated by cameras, GPS, sonar, and IMU within the existing
    framework of fog/edge is challenging without the integration of intelligent paradigms
    [182]. With AI almost influencing all aspects of our lives, the traditional AI
    methods involve training the models on data aggregated from several IoT/edge devices
    on a centralized cloud server. For instance, consider the smart city scenario
    where AI and ML techniques consolidate the results for better predictions and
    enhance the user experience. Nevertheless, data is stored at a centralized location,
    and the AI techniques rely on this training data in order to forecast trends and
    patterns [183]. Nonetheless, this method encounters various challenges including
    privacy concerns, data security, regulatory compliance etc. The solution to this
    problem lies in training the model on the device itself instead of centralized
    server. To serve the same, Federated Learning (FL) comes into play, which provides
    hyper-personalized space, low cloud infrastructure overhead, and prominent privacy
    preservation while minimizing latency. FL can be treated as a decentralized form
    of machine learning, which creates a shared model in place of a central data model.
    The new models are being trained collaboratively on the edge, where the data never
    leaves the personalized device. Although the devices and machines train several
    models at distributed locations in parallel and send their collaborative results
    to a centralized server to create a machine learning model [184]. Therefore, FL
    leverages both the distribution of data and computational resources while safeguarding
    data privacy [183]. For instance, Saha et al. [185] illustrated the implications
    of FL via an irrigation scheduling application where the deployed IoT sensors
    such as humidity, temperature, moisture, air flow, and so on forward, the parameters
    via edge devices. The demographic versatility across fields enables each edge
    device to update its local model utilizing on-device local data. Global aggregation
    of data on centralized servers results in inefficient training models that are
    even more susceptible to malicious attacks. Hence, fog nodes act as local aggregators,
    which send the global aggregators to centralized cloud servers. The implications
    of an integrated framework are shown in Figure 14. Fig. 14. Fog-Assisted Federated
    Learning Framework for Agriculture. Show All H. Integration of Fog/Edge With Software
    Defined Network (SDN) SDN is a new architecture wherein the control plane is separated
    from the data plane and consists of two primary components: the SDN switch and
    the controller. Each SDN switch comprises a flow table, which defines the actions
    to be applied to the packets that enter it. A match criterion for each entry in
    the flow table is defined over the IP source, IP destination, and protocol fields.
    The actions are implicated once the match criteria are satisfied. Whereas the
    SDN controller brings forth full control over the network, communication flow,
    enabling remote control, elevated flexibility, and programming capability [186].
    The integration of SDN with IoT boosts network performance by providing the management
    of ephemeral network states in a centralized control model. Besides, the SDN controller
    acts as a centralized controller for IoT networks, assisting in the monitoring
    and management of heterogeneous IoT devices [187]. The research is trending towards
    the usage of SDN for optimized IoT management in alliance with blockchain to improve
    the security aspect. Finally, Table XV presents an overview of various studies
    that delve into the incorporation of thrust technology in fog/edge computing.
    TABLE XV Demystifying Thrust Technology With Fog/Edge Computing SECTION V. Results
    and Analysis Our work emphasizes the systematic assessment and discussion of various
    articles based on the prevailing status of resource management issues in fog/edge
    computing, along with the identification of its collaborative thrust technology.
    Our study includes numerous driving forces that are leaving a remarkable impact
    on emerging computing paradigms, presented as open challenges and from a future
    research perspective. Throughout, 490+ articles were collected, out of which 223
    have been shortlisted after extensive selection. The articles highlight the existing
    state-of-the-art work carried out in the resource management domain based upon
    non-AI and AI-based technologies. Most of the selected articles cover the period
    from 2016 to 2023. As illustrated in Figure 15, a major chunk of our referred
    articles are from the past 6 years. The organization and methodology of the article
    are motivated by the systematic literature review procedure [188]. It can be ascertained
    that the formulated research questions serve as a principal solution for elucidating
    various RM-related issues, hence heading the flow of the review methodology. Fig.
    15. Year-wise Publications based on Resource Management in Fog/Edge Computing.
    Show All Furthermore, we meticulously reviewed each article and bifurcated it
    into five fields: review, SLR, Implementation platform (Real/Testbed or Simulation)
    and book chapters, as represented in Figure 16. In addition to this, Figure 17
    states comparison statistics in the context of evaluation tools, filtering out
    the papers based on real implementation. This indicates that a significant portion
    of the papers examined did not specify the simulator tool they used and, as a
    result, have been categorized as part of the “other” group. Besides, the majority
    of them utilized iFogSim, CloudSim, and Python-based implementations with 17%,
    12%, and 8%, respectively. It has been observed that few infrastructure platforms
    exist for pursuing real-time fog computing research work. Hence, there is a need
    to expedite research towards developing realistic Fog testbeds for evaluating
    the results of deployed AI models. Fig. 16. Type of Studies in the Selected Papers.
    Show All Fig. 17. Comparison of Performance Evaluation Tools for Resource Management
    in Fog/Edge Computing. Show All The search criteria play a significant role in
    the review methodology. In reference to the same context, Figure 18 represents
    a bifurcation of selected papers based upon prominent publishers comprising IEEE,
    Elsevier, ACM, Springer, and other sources such as Web of Sciences, Science Direct,
    Taylor and Francis Journal, Scopus, Google Scholar, Research Gate, Springer Link,
    Emerland, and other resources like scientific electronic research databases. It
    is concluded that the majority of the selected articles are published by IEEE
    journals, transactions, and conferences, in comparison to other publishers. Lastly,
    we have also categorized reviewed papers based on parameters including IoT, QoS
    parameters, energy efficiency, application-based (healthcare, vehicular network
    etc.) and papers integrating thrust technology, as shown in Figure 19. A major
    chunk of our surveyed papers is based on resource-related aspects. Fig. 18. Bifurcation
    of Selected publications based on different Publishers. Show All Fig. 19. Categorization
    of Publications based on Resource Oriented Factors relating Fog/Edge Computing
    Paradigms. Show All SECTION VI. Open Issues and Research Directions The Fog/Edge-Cloud
    computing model heralds unprecedented expansions in the building of IoT solutions.
    However, the real implications of this model for multi-layer computing pose a
    huge number of challenges. The previous section discussed futuristic trends in
    the form of key-enabler thrust technology, which are making room for their integration
    with the aforementioned model. In this section, we highlight some long-standing
    challenges that lay ahead, along with research perspectives, which are discussed
    as follows: A. Security and Privacy Preservation in Public FNs The IoT-enabled
    fog/edge framework strives to improve users’ experiences and the resilience of
    services in case of failovers. To achieve the same, retaining the security, authorization,
    integrity, and confidentiality of the application along with the underlying network
    becomes the prime factor that must be addressed. The IoT-sensing data is exposed
    to various risks, such as unauthorized access, the risk of intrusion and a wide
    range of security attacks [133], [161]. The authors had discussed a wide umbrella
    of attacks under the Network layer of the IoT architecture, which includes DoS,
    Spoofing, sinkholes, wormholes, man-in-the-middle attacks, and Sybil attacks.
    As most IoT devices are connected via wireless communication links, most of the
    security challenges in IoT are related to the wireless network [193]. Also, ensuring
    the secure and successful execution of applications on resource-constrained edge
    devices necessitates strong and robust light-weight encryption methods, security
    mechanisms and advanced and efficient cryptographic schemes [194]. B. Real-Time
    Analytics for Smart Applications Nowadays, many researchers are working towards
    creating efficient algorithms to train ML models over the network edge. For the
    same reason, sharing and communication of computational results are important
    for the deployment of this distributed computing paradigm. New computation-aware
    network models are gaining insight for developing distributed data-sharing systems.
    For instance, in smart cities, the deployment of large-scale sensing networks,
    anomalous and hazardous event identification, and enabling real time responses
    are pre-requisites. The fog computing paradigm must be equipped with learning
    algorithms to analyze real-time data for smart pipeline monitoring to timely detect
    any event threatening pipeline safety [195]. This opens avenues for researchers
    to incorporate learning algorithms in the real-time application area, as the majority
    of the state-of-the-art covers only theoretical aspects, where real implementation
    is still missing. C. Self-Adaptive Scheduling in FNs Most of the scheduling algorithms
    lack the learning capability of self-adaptiveness, which makes resource scheduling
    a challenging task in Fog/Edge nodes [203]. Although research is trending towards
    the deployment of self-adaptive scheduling, all these works are considered at
    the simulation level only [204]. Therefore, there is a dire need for resource
    scheduling techniques to equip themselves with the capability to generate optimal
    task schedules in a dynamic workload environment. D. Management of Geographically
    Distributed Resources Cloud virtualization solution would not be completely suitable
    for fog/edge. The varied hardware and OS configurations of Fog architecture call
    for the need for infrastructure virtualization. It calls for Container Orchestration
    tools, which ease out the scaling up and down of fog infrastructure nodes, henceforth
    meeting the requirements of real-time IoT applications and the constraints imposed
    on FNs [196]. Some of the prominent Container Orchestration tools include Kubernetes,
    Docker Swarm, and Apache Mesos-Marathon. E. FN Mobilization Management In a VFC
    environment, Unmanned Aerial Vehicles (UAVs), smartwatches and phones are configured
    to fully utilize the computational resources of mobile vehicles. However, the
    management of resources in such a scenario becomes challenging in terms of task
    offloading due to the varying distance between the FN and the end user [203].
    In contrast to static FNs, the decision about task offloading and service placement
    is not straightforward in the case of mobile FNs. It occurs due to the short and
    intermittent connection between service provider and user, as well as because
    the multi-hop forwarding of tasks amongst vehicles is time-consuming and susceptible
    to packet loss [197]. This ultimately raises a new challenge for task offloading
    in VFC. F. Programmability of FNs The cloud computing paradigm allows users to
    deploy their code with zero or no knowledge of the underlying platform where the
    code is being executed. However, the situation is different in Fog/Edge computing,
    which comprises heterogeneous FNs. The programmer faces huge difficulty in writing
    an application to run on the fog platform because of the varying runtime environments
    of FNs. Hence, it has been observed that few works have addressed the issue of
    the programmability of edge computing. One of the works by Shi et al. [32] proposed
    the concept of a computing stream: a software-defined computing flow of data,
    where computing can occur anywhere along with a propagation path in a distributed
    manner. It ensures data computation in closest proximity to the data source, which
    optimizes the energy, cost, Total Cost of Ownership (TCO), and latency parameters
    of applications [32]. All of these challenges have been as depicted in Figure
    20. Fig. 20. Open Research Challenges in Fog/Edge Computing. Show All Proposed
    Solutions: The implementation and management of smart applications thrive on real-time-based
    solutions, which can be accomplished by exploiting cutting-edge technologies,
    including the prediction and decision-making capabilities of AI. The fast-networking
    capabilities of 5G, the enhanced the security of blockchain etc. The emergence
    of 5G boosted a broad spectrum of unprecedented applications, along with enhanced
    mobile broadband with ultra-reliability, improved data rates, low latency, massive
    device connectivity, and support for a diverse range of IoT and mobile applications.
    The approaching era of hyper-connectivity with 5G as the fundamental pillar enabling
    the new AIoT economy, that will eventually bring more intelligence into operational
    work. Hence, catering to smart manufacturing, e-healthcare, smart campuses, smart
    stadiums, and smart businesses. 5G supports various types of communications services;
    ranging from high- speed LAN, WAN guarantee data security, real-time operations,
    and wide devices, connectivity, thereby enabling effective communication amongst
    geographically distributed resources in a collaborative environment [198]. In
    addition, the computational distribution of the incoming IoT workload by MEC can
    be boosted by utilizing beyond 5G networks [199]. Figure 21 illustrates the taxonomical
    representation of proposed solutions corresponding to challenges in a collaborative
    cloud-fog-IoT environment. Fig. 21. A Taxonomy of Proposed Solutions for Challenges
    in a Collaborative Cloud-Fog-IoT Environment. Show All Apart from this, a wide
    spectrum of critical applications, such as Autonomous vehicles, video surveillance,
    and AR/VR gaming possess challenges such as mobility amongst vehicles, UAVs, which
    cannot be handled by a single technique. Hence, hybrid approaches are needed to
    address these issues. For instance, the drawbacks of existing deep learning techniques,
    such as slow learning rates, large training data requirements and slow adaptability
    to dynamic IoT environments have been addressed by proposing a hybridized approach
    incorporating meta-learning capabilities into existing DRL solutions [200]. This
    approach improvises the resource management model to learn faster and quickly
    adapt to rapidly changing environments. Meta learning is characterized by “learning
    to learn” and possessing the capability to adapt quickly, requiring only a few
    training examples [201]. Despite the significant benefits of collaborative cloud-fog
    computing frameworks, the data produced by IoT-enabled applications remains a
    prime target for attackers, presenting potential privacy and security risks. Therefore,
    safeguarding the security of the extensive data generated at the IoT layer is
    of utmost importance, necessitating the integration of suitable security measures.
    Hence, the latest work proposes incorporating blockchain technology as a potential
    solution. For illustration, consider the healthcare domain, which contains patients’
    data being streamed from various sensors, smart watches etc. Such sensitive information
    cannot be sent to the cloud [202]. Consider a heart patient who needs continuous
    monitoring of their heart rate for the potential risk of a heart attack. In such
    a situation, the patients and medical staff are registered, followed by the blockchain
    monitoring agent granting access to registered devices. Then, smart contracts
    validated by protocols are responsible for proofing a transaction, which, if authenticated,
    is added as a block to a blockchain-enabled fog server [150]. Nevertheless, the
    authors envisions addressing the issue of workload prediction, which can facilitate
    better management of resources by providing a better knowledge of fluctuating
    incoming workloads [133]. SECTION VII. Conclusion The last decade has witnessed
    a massive drift in the form of emerging computing paradigms with the widespread
    prominence of IoT devices. A collaborative cloud-fog/edge paradigm is becoming
    immensely popular because of its capacity to facilitate real-time IoT applications,
    ensuring low latency and instant responsiveness. But management of underlying
    resources becomes more complex and demanding because of its large-scale geographical
    distribution, heterogeneous and resource-constrained nature, and, above all, the
    workload is too divergent in fog/edge computational nodes. Hence, our work efficiently
    presents a comprehensive literature review covering last the 6 year (2018–2023)
    which includes all aspects of resource management covering existing work to date
    on AI, non-AI based solutions and hybrid approaches to effectively manage resources
    in collaborative cloud-fog/edge-IoT environment. Throughout, 490+ articles were
    collected, out of which 223 have been shortlisted after extensive selection. The
    articles highlight the existing state-of-the-art work carried out in the resource
    management domain. Our study outlines various AI-based techniques under a wide
    umbrella of resource management which covers the provision of computing resources,
    offloading IoT-based tasks to the cloud, resource scheduling, placement of incoming
    IoT services, allocating resources and load balancing. Lots of significant efforts
    have been made to utilize advanced AI-empowered techniques such as metaheuristics,
    ANNs, Cognitive learning and DRL-based algorithms for optimizing QoS parameters.
    Moreover, the authors have formulated the mathematical model for each aspect of
    resource management with the mentioned objective functions like latency, service
    costs and energy consumption. We have efficiently recognized different challenges
    which arise at each and every phase of resource management. Our paper highlights
    the social and ethical impacts of the implications of AI in IoT-driven application
    areas. We also observed that the future research perspective resides in integrating
    thrust technology such as Serverless computing, 5G, IIoT, SDN and Federated Learning
    with edge/fog computing. This comprehensive review will be useful for practitioners,
    researchers, and academicians in digging into thrust technologies and how its
    potential key features can be exploited by integrating it with fog/edge. The work
    endows different proposed solutions in taxonomical form by illustrating IoT-based
    applications and correspondingly incorporating AI/thrust technology to overcome
    various challenges. The efforts expended on categorization within the field of
    resource management will assist researchers in recognizing and choosing the most
    suitable AI-based techniques for effective resource management in dynamic settings.
    The future perspective of our work aims to explore and analyze the capabilities
    and potential of Explainable-AI (XAI) to address the intricacies of existing AI
    techniques in real-world IoT applications. ACKNOWLEDGMENT The authors would like
    to thank the Editor-in-Chief, area editor and anonymous reviewers for their valuable
    comments, useful suggestions to improve the quality of the paper. Authors Figures
    References Citations Keywords Metrics More Like This IoT Farm: A Robust Methodology
    Design to Support Smart Agricultural System Using Internet of Things with Intelligent
    Sensors Association 2023 7th International Conference on Electronics, Communication
    and Aerospace Technology (ICECA) Published: 2023 A Survey on Emerging Integration
    of Cloud Computing and Internet of Things 2019 International Conference on Information
    Science and Communication Technology (ICISCT) Published: 2019 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Communications Surveys and Tutorials
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'AI-Empowered Fog/Edge Resource Management for IoT Applications: A Comprehensive
    Review, Research Challenges, and Future Perspectives'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Tarek Z.
  - Elhoseny M.
  - Alghamdi M.I.
  - EL-Hasnony I.M.
  citation_count: '0'
  description: The world's population is expected to exceed 9 billion people by 2050,
    necessitating a 70% increase in agricultural output and food production to meet
    the demand. Due to resource shortages, climate change, the COVID-19 pandemic,
    and highly harsh socioeconomic predictions, such a demand is challenging to complete
    without using computation and forecasting methods. Machine learning has grown
    with big data and high-performance computers technologies to open up new data-intensive
    scientific opportunities in the multidisciplinary agri-technology area. Throughout
    the plant's developmental period, diseases and pests are natural disasters, from
    seed production to seedling growth. This paper introduces an early diagnosis framework
    for plant diseases based on fog computing and edge environment by IoT sensors
    measurements and communication technologies. The effectiveness of employing pre-trained
    CNN architectures as feature extractors in identifying plant illnesses has been
    studied. As feature extractors, standard pre-trained CNN models, AlexNet are employed.
    The obtained in-depth features are eliminated by proposing a revised version of
    the grey wolf optimization (GWO) algorithm that approved its efficiency through
    experiments. The features subset selected were used to train the SVM classifier.
    Ten datasets for different plants are utilized to assess the proposed model. According
    to the findings, the proposed model achieved better outcomes for all used datasets.
    As an average for all datasets, the accuracy of the proposed model is 93.84 compared
    to 85.49, 87.89, 87.04 for AlexNet, GoogleNet, and the SVM, respectively.
  doi: 10.1038/s41598-023-43465-4
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    09 November 2023 Leveraging three-tier deep learning model for environmental cleaner
    plants production Zahraa Tarek, Mohamed Elhoseny, Mohamemd I. Alghamdi & Ibrahim
    M. EL-Hasnony  Scientific Reports  13, Article number: 19499 (2023) Cite this
    article 568 Accesses 1 Citations Metrics Abstract The world''s population is expected
    to exceed 9 billion people by 2050, necessitating a 70% increase in agricultural
    output and food production to meet the demand. Due to resource shortages, climate
    change, the COVID-19 pandemic, and highly harsh socioeconomic predictions, such
    a demand is challenging to complete without using computation and forecasting
    methods. Machine learning has grown with big data and high-performance computers
    technologies to open up new data-intensive scientific opportunities in the multidisciplinary
    agri-technology area. Throughout the plant''s developmental period, diseases and
    pests are natural disasters, from seed production to seedling growth. This paper
    introduces an early diagnosis framework for plant diseases based on fog computing
    and edge environment by IoT sensors measurements and communication technologies.
    The effectiveness of employing pre-trained CNN architectures as feature extractors
    in identifying plant illnesses has been studied. As feature extractors, standard
    pre-trained CNN models, AlexNet are employed. The obtained in-depth features are
    eliminated by proposing a revised version of the grey wolf optimization (GWO)
    algorithm that approved its efficiency through experiments. The features subset
    selected were used to train the SVM classifier. Ten datasets for different plants
    are utilized to assess the proposed model. According to the findings, the proposed
    model achieved better outcomes for all used datasets. As an average for all datasets,
    the accuracy of the proposed model is 93.84 compared to 85.49, 87.89, 87.04 for
    AlexNet, GoogleNet, and the SVM, respectively. Similar content being viewed by
    others Advanced deep learning techniques for early disease prediction in cauliflower
    plants Article Open access 27 October 2023 A novel smartphone application for
    early detection of habanero disease Article Open access 16 January 2024 Image-based
    crop disease detection with federated learning Article Open access 06 November
    2023 Introduction In the global economy, agriculture is crucial. The agriculture
    system will face more strain as the human population grows and the COVID-19 pandemic
    takes hold. Agri-technology has developed as a novel scientific area that employs
    data-intensive techniques to boost agricultural output while reducing the environmental
    impact. In contemporary agricultural activities, data is created by a variety
    of sensors that give a better understanding of the operational surroundings (weather
    conditions, dynamic crop, and soil) as well as the operation itself (data from
    machines), resulting in more precise and quicker decision-making1. Conservation
    agriculture has long been regarded as an effective and ecologically beneficial
    management strategy to boost agricultural yields. In addition, measuring the total
    impact of conservation agriculture on crop output amelioration by taking the average
    of the entire dataset is not unfamiliar. Nevertheless, the influence of conservation
    agriculture on yielding cleaner output should be examined2. Plant disease and
    pest identification are critical research areas in the realm of machine learning.
    To determine whether or not a plant image contains diseases or pests, machine
    vision equipment is used3. Are these types of detection systems needed? A vital
    food security hazard is plant disease. Agricultural and population growth are
    affected by plant diseases, as is the economy. Disease control, food safety, and
    anticipated loss of income need an automated and exact estimation of plant disease
    gravity. Plant diseases must thus be identified and treated at an early stage.
    Non-expert farmers, on the other hand, are frequently oblivious to non-native
    illnesses, necessitating consultation with experts to determine whether there
    are any strange symptoms or appearances on their crops. A farmer may have to travel
    vast distances to consult with an expert, which is costly and time-consuming.
    To automate the process of identifying plant illnesses, these difficulties motivate
    research and expansion in this area. The essential requirement for a plant disease
    diagnosis model that can operate in an Internet of Things (IoT) environment with
    minimal processing capabilities is very important4. Already, machine vision is
    being used in agriculture to detect plant diseases and pests. While artificial
    intelligence (AI) is still a long way from being widely deployed, the technology
    has tremendous development potential and application value5. Plant diseases have
    been classified and identified using machine learning (ML) models. Nonetheless,
    with improvements in deep learning (DL), this field of research looks to offer
    enormous promise for greater accuracy. When it comes to planting disease detection,
    multiple DL structures have been developed or modified, as have many visualization
    methodologies. Various performance measurements are also used to evaluate these
    architectures and techniques6. As a result, several researchers have sought to
    build robust plant disease detection systems that require a high number of disease-infected
    specimens to be successful. In the past, collecting such a vast number of samples
    was difficult. Thanks to the Internet of Things, we can now gather and diagnose
    diseases within the human body! As part of the Plantvillage datasets, there are
    a lot of photos of corpses with various diseases. Because it is well-labeled and
    extensively utilized, this dataset has been used in several plant disease detection
    studies. To maximize their harvests, the farmers also want an easy-to-use detection
    system that they can use on their phones to identify plant diseases and remove
    them early. Using image processing methods, Plant disease farmers and researchers
    may be able to diagnose plant ailments more precisely. Image processing techniques
    for detecting sickness can also yield satisfactory results, but they require human
    intervention for other detection and analysis7,8,9. According to these challenges,
    we aim to improve the quality of the product and arrive at cleaner production.
    For plant disease and pest identification using machine vision, the emphasis has
    switched from standard machine learning and image processing approaches to deep
    learning techniques in fog environments, which have handled complex previously
    unsolvable issues. The paper contribution handles four-folds. The first fold is
    using IoT sensors to generate data and images; there are many sensors used in
    this field, such as soil moisture, humidity, and temperature, light-dependent
    resistors, water level, relay module, analog extender, and buzzer ESP 8266. These
    images were preserved for ten economically and environmentally beneficial plants.
    Leaf pictures of these plants in ideal and dire circumstances have been collected
    and dispersed across two categories. In this paper, datasets that focus on plants
    with significant ecological and economic benefits to their ecosystems are examined.
    As a result, ten plants, popularly known as Arjun, Mango, Guava, Saptaparni, Jamun,
    Bael, Sukh Chain, Jatropha, Pomegranate, Basil, Chinar, and Lemon, have been picked.
    To name just a few, some of these plants have high medicinal value; others are
    popular for their fruits, and the vast majority are environmentally and economically
    significant. Algorithms and models based on deep learning must be successfully
    integrated with agricultural and plant protection experience to fully exploit
    AL and ML''s potential in the second fold. Three deep learning models applied
    for plant disease detection are AlexNet, one of the most widely used neural network
    designs nowadays, GoogleNet, one of the most significant advances in the domain
    of Neural Networks, notably for CNNs the support vector machine (SVM). These models
    are utilized to the deep extracted features generated by the pre-trained CNN layers,
    AexNet, and we extracted the (fc7) layer as our feature extracting layer. After
    feeding images into that layer, we can receive features of the images from it.
    After having all the features, we could use them for training the classifiers.
    Then, a comparative study among the three models is conducted to show the accuracy
    of the three models. The third fold is using a fog environment for computing all
    necessary tasks of image preprocessing, visualization, monitoring, and local decision
    support systems for detection and prediction tasks. As a new way of extending
    and assisting cloud computing, Fog Computing is a rapidly evolving technology.
    Its proximity to edge users, openness, and mobility, make fog computing platforms
    ideal for providing services to users quickly and improving the QoS (Quality of
    Service) of Internet of Things devices. A customer application based on IoT involving
    real-time activities in agriculture is increasingly reliant on this method10.
    Lastly, developing a novel version of the grey wolf optimization algorithm (GWO)
    for selecting the important features to feed to the classifiers. This process
    is very important to select the relevant features to accelerate the prediction
    models with fair accuracy. The selected features are fed to the SVM and compared
    to the standard model, which used all the features from AlexNet. The remainder
    of the paper is structured as follows: “Related work” provides some studies about
    the recent work. An overview of the basic concepts and methods utilized in this
    paper is presented in “Methods and overviews”. “Proposed methodology” provides
    the suggested methodology in detail. The experiment setting and results are shown
    in “Experimental results and discussion”. “Conclusion and future work” concludes
    with a look at what''s next. Related work Abbas et al.11, presented a technique
    based on deep learning for tomato disease diagnosis. To categorize tomato leaf
    pictures into ten disease categories, the DenseNet121 approach was trained on
    real and synthetic images using transfer learning. The suggested approach attained
    an accuracy of 97.11%, 98.65%, and 99.51% for the classification of leaf images
    into 10 classes, 7 classes, and 5 classes, respectively. Thenmozhi and Reddy12,
    proposed a powerful CNN approach, and transfer learning is being applied to achieve
    the best or a desired performance of the pre-training model. Three public insect
    datasets were used to classify insect species, with accuracy rates of 96.75 percent,
    97.47 percent, and 95.97 percent, respectively. Wiesner-Hanks et al.13, utilized
    community data for training a CNN, and nutrition the output into a conditional
    random field (CRF) to divide pictures into non-lesion and lesion areas with an
    accuracy of 0.9979 and F1 score of 0.7153. Too et al.14, utilized DenseNets, which
    have a propensity to always progress in accuracy as the number of iterations increases,
    with no evidence of performance decay or overfitting. For the classification of
    plant disease, an accuracy score of 99.75% was achieved. Chen et al.15, presented
    CNN architecture depended on a gliding window to construct a structure for location
    regression calculation and recognition of pests’ species and plant diseases, feature
    fusion, characteristics automatic learning, and the identification rate of 38
    frequent symptoms was 50–90%. Zhou et al.16, demonstrated a rapid approach for
    the detection of rice diseases founded on the combination of Faster R-CNN and
    FCM-KM. The sheath blight, bacterial blight, and detection accuracy, and rice
    blast time were 98.26 percent/0.53 s, 97.53 percent/0.82 s, and 96.71 percent/0.65
    s respectively, based on the application results of 3010 images. Sethy et al.17
    presented 5932 on-field pictures of 4 different kinds of rice leaf illnesses:
    brown spot, bacterial blight, tungro, and blast. Furthermore, the effectiveness
    of eleven CNN architectures in the deep feature with SVM and the transfer learning
    approach was assessed. According to the experimental findings, the deep feature
    of ResNet50 with SVM outperforms transfer learning equivalent in classification.
    Deep learning-based methods for identifying illnesses and pests in rice plant
    pictures have been developed by Rahman et al.18. A two-stage tiny CNN design was
    developed, and it was compared to SqueezeNet, NasNet Mobile, and MobileNet. The
    simulation findings demonstrated that the suggested framework could attain the
    necessary accuracy of 93.3%. Guo et al.19 presented a mathematical model based
    on deep learning for the recognition of plant disease and detection. The model
    was tested for illnesses such as rust diseases, black rot, and bacterial plaque.
    The results indicated that the accuracy of the model is 83.57%, which is greater
    than the previous technique, decreasing the impact of illness on agricultural
    productivity and being beneficial to agriculture''s long-term improvement. Atila
    et al.20, presented the EfficientNet model for the plant leaf disease classification,
    and the performance of the model was compared to existing previous deep learning
    techniques. The experimental findings revealed that the B4 and B5 approaches of
    the EfficientNet attained the greatest rates in the original and enhanced datasets,
    with the accuracy of 99.91% and 99.97%, and precision of 98.42% and 99.39% respectively.
    There are many studies for plan disease prediction as in Refs.7,8,9,21. Table
    1 summarizes the role of ML/DL in agriculture for plant diseases classification
    using accuracy measurement as mentioned by many authors. It is observable that
    most of the recent works use the PlantVillage dataset and deploying a set of pre-trained
    CNN models. In this paper, new datasets have been used for testing our proposed
    architecture for plant disease classification. Table 1 ML/DL for plant disease
    detection. Full size table The next section handles an overview of the problem
    statement and the used methods in this paper. Methods and overviews Overvies Climate
    change, population expansion, and food security concerns have pushed the sector
    to explore more creative ways to agricultural yield protection and improvement.
    As a result, artificial intelligence (AI) is progressively developing as a component
    of the industry''s technical growth31. Popular applications of traditional machine
    learning algorithms in agriculture are: Recognition/harvesting of vegetables and
    fruits. Plant disease classification/pest detection. Crop/weed discernment and
    classification. Plant/leaves recognition and classification. Land cover classification.
    In comparison to the defined segmentation, detection, and classification tasks
    in computer vision, the criteria for detecting pests and plant diseases are quite
    broad. Its needs may be classified into three categories: what, where, and how.
    Even though, the fact that the function needs and aims of the 3 phases of plant
    disease and pest detection are distinct, the three stages are mutually inclusive32.
    The classification job in computer vision is represented by \"what\" in the first
    step. Classification defines the image globally using feature expression and then
    decides if the image contains a certain type of object using the classification
    process. While structure learning is the primary research path in object detection,
    feature expression is the primary research path in classification tasks. Machine
    learning (ML) has developed alongside high-performance computation and big data
    technologies to open up new avenues for unraveling, quantifying, and comprehending
    data-intensive processes in agricultural operational contexts. ML offers machines
    the capacity to learn without being precisely programmed33. Convolutional neural
    networks (CNNs) are more complex to construct than traditional neural networks,
    but they are simpler to utilize. It is not required to extract picture characteristics
    independently in the case of this sort of neural network. In image classification
    problems, complex and pre-trained CNNs with millions of parameters are frequently
    utilized. Their complete training is difficult since it is a time-consuming and
    labor-intensive procedure34. With developments in machine learning (ML) principles,
    significant gains in agricultural activities have been noticed. The capacity to
    extract features automatically generates an adaptable nature in deep learning
    (DL), especially CNNs, which achieves human-level accuracy in a variety of agricultural
    applications, prominent among which are crop/plant recognition, fruit counting,
    land cover classification, weed/crop discrimination, and plant disease detection
    and classification35. Methods Transfer learning. Data Transfer Learning (DTL)
    is a strategy in which knowledge derived from the data is transferred to solve
    various but associated assignments to train the CNN, including new data that often
    comes from a lower population36. To initialize the models and pre-train two profound
    convolutionary neuro-network models, transfer learning was used: AlexNet and GoogleNet.
    AlexNet is expected to be the first recommended deep CNN technology due to its
    remarkable outcomes for the identification and classification functions on image
    data37. In an attempt to improve hardware constraints and obtain the total functionality
    of deep CNN, AlexNet was trained on two parallel GPUs. In AlexNet, the CNN depth
    was widened from only five layers in the LetNet CNN to eight layers in the way
    to produce CNN appropriate to different data sets of images. Dropout, ReLU, and
    pre-processing are major attributes to attain significant improvement in computer
    vision applications. The common 8 layers are five convolutional layers, two fully
    connected hidden layers, and one fully connected output layer, as shown in Fig.
    138. Figure 1 An illustration of AlexNet layers. Full size image In this study,
    we replaced the 1000 classes that the original AlexNet had, with only 2 classes
    which we evaluated in this paper, healthy images and diseased images of 10 different
    plants as illustrated in the dataset description. GoogleNet consists of 22 layers
    deep CNN that is a version of the inception network established by Google researchers.
    The design of the GoogleNet structure resolved many constraints that appeared
    for large networks, primarily out of the use of the Inception module. The structure
    diagram of the GoogleNet network is shown in Fig. 239. Figure 2 GoogleNet network
    structure diagram. Full size image GoogleNet consists of inception modules, so
    its architecture is complex. GoogleNet is looked like one of the initial CNN architectures
    to resist successively accumulating convolutions and pooling layers. In addition,
    GoogleNet plays a vital role in consideration of storage and power, since accumulating
    all tiers and combining different restrictions would take time for computation
    and will result in higher costs of memory40. Support vector machine The deep feature
    extraction technique necessitates the training of a classifier method with the
    extracted features. Vapnik''s SVM was utilized as a classifier in this study41.
    It has been found that the SVM classifier outperforms others in several agricultural
    image categorization tasks. The support vector machine is a classifier with a
    linear or non-linear relationships that is capable of distinguishing between two
    different types of objects. SVMs are machine learning approaches focused on cambered
    improvement that operate as stated by the concept of structural risk reduction.
    These approaches are separate of distribution, as it does not need any details
    on the common distribution functions42. SVM training can be illustrated with algorithm
    143. While a hyperplane classifier can distinguish between 2 classes, certain
    categories surpass the highest distance set as the most effective separation hyperplane.
    The objective of SVM is to construct an ideal hyperplane space by utilizing training
    sets40. The main idea behind using SVM to solve a classification issue is to find
    a hyperplane that best separates data from two groups. The formula for a linear
    SVM''s output is presented in Eq. (1), where \\(\\overrightarrow{w}\\) is the
    hyperplane''s normal vector and \\(\\overrightarrow{x}\\) is the input vector.
    Margin maximization may be thought of as an optimization issue: reduce Eq. (2)
    subject to Eq. (3) where yi and \\(\\overrightarrow{x}\\) are the SVM''s correct
    output and input vector for the ith training sample, respectively44. $$ u{ } =
    { }\\vec{w}.{ }\\vec{x} - {\\text{b}} $$ (1) $$ 1/2\\left\\| {\\vec{w}} \\right\\|2
    $$ (2) $$ yi{{(\\vec{w}}}{{.\\vec{x} - b)}} \\ge {1,}\\forall i $$ (3) SVM is
    a binary classifier that can only distinguish between two classes and does not
    handle multi-class classification issues. One approach to classification of multi-class
    using SVMs is to build a one-to-one group of classifiers and forecast the class
    picked by the majority of classifiers45. While this allows for the creation of
    K (K + 1)/2 classifier for the classification issue with K classes, the classifiers
    training time may be decreased because the training data set for every classifier
    is lower. In this article, SVM is used to analyze data in addition to CNN techniques
    such as AlexNet and GoogleNet. Grey wolf optimization (GWO) algorithm Mirjalili
    et al. proposed the grey wolf optimizer (GWO) as a novel swarm intelligence method46.
    The GWO method has been successfully utilized and applied in a variety of research.
    The primary inspiration for the GWO algorithm came from the social pursuit of
    grey wolves in nature. Figure 3 depicts the social hierarchy as well as an instance
    of the position update process47. Figure 3 GWO’s social structure and position
    update method. Full size image In the GWO algorithm, the first, second, and third
    best-recommended solutions are alpha (α), beta (β), and delta (δ). Omega is projected
    to be the outstanding solution. The wolves can be presented in a form that is
    representable mathematically in Eqs. (4–8) during the hunting process: $$\\overrightarrow{D}=\\left|\\overrightarrow{C}.
    {\\overrightarrow{X}}_{P}-{\\overrightarrow{X}}_{\\left(t\\right)}\\right|$$ (4)
    $${\\overrightarrow{X}}_{\\left(t+1\\right)}={\\overrightarrow{X}}_{P\\left(t\\right)}-\\overrightarrow{A}.\\overrightarrow{D}$$
    (5) $$\\overrightarrow{A}=2 \\overrightarrow{a}. \\overrightarrow{rand1}-\\overrightarrow{a}$$
    (6) $$a=2\\times (1-\\frac{t}{{t}_{max}})$$ (7) $$\\overrightarrow{C}=2. \\overrightarrow{rand2}$$
    (8) where x is the grey wolf''s vector position, xp is the prey''s vector position,
    D is the distance between x and xp, t is the current iteration number, and A and
    C correspond to component-wise multiplication. The \"A'' parameter is given in
    a [− a, a] random value according to the \"a\" value. Whether a gray wolf attacks
    or not, the value of A can be determined. As a result of the calculation, the
    gray wolf is exceptionally close to the hunt and can attack at any time if |A < 1|
    status is available. The gray wolf leaves a beast in the case of |A > 1|, hoping
    for a new beast. Another critical parameter of control, C, is recognized as the
    exploration component of the algorithm and may include random values within the
    range [0, 2]. This variable leads to a random behavior of the algorithm that prevents
    an optimization at optimum local values. This condition happens if the random
    conduct is minimized by |C < 1| or else |C > 1|47. To mimic grey wolf hunting
    behavior, Eqs. (9–14) show how grey wolves are positions updating of α, β, and
    δ wolves. It is accepted that the wolves of α, β, and δ are closest to the prey
    and attract the rest of the wolves to the prey area. The grey wolf population
    can use the following formulae to determine prey position: $${\\overrightarrow{D}}_{\\alpha
    }=\\left| {\\overrightarrow{C}}_{1} . {\\overrightarrow{X}}_{\\alpha }-\\overrightarrow{X}\\right|$$
    (9) $${\\overrightarrow{D}}_{\\beta }=\\left| {\\overrightarrow{C}}_{2} . {\\overrightarrow{X}}_{\\beta
    }-\\overrightarrow{X}\\right|$$ (10) $${\\overrightarrow{D}}_{\\delta }=\\left|
    {\\overrightarrow{C}}_{3} . {\\overrightarrow{X}}_{\\delta }-\\overrightarrow{X}\\right|$$
    (11) $${\\overrightarrow{X}}_{1}={\\overrightarrow{X}}_{\\alpha }(t)-{\\overrightarrow{A}}_{1}.({\\overrightarrow{D}}_{\\alpha
    })$$ (12) $${\\overrightarrow{X}}_{2}={\\overrightarrow{X}}_{\\beta }(t)-{\\overrightarrow{A}}_{2}.({\\overrightarrow{D}}_{\\beta
    })$$ (13) $${\\overrightarrow{X}}_{3}={\\overrightarrow{X}}_{\\delta }(t)-{\\overrightarrow{A}}_{3}.({\\overrightarrow{D}}_{\\delta
    })$$ (14) The locations are determined from Eqs. (12–14) is utilized to modify
    the next location of the wolves by Eq. (15): $${\\overrightarrow{X}}_{\\left(t+1\\right)}=\\frac{\\left({\\overrightarrow{X}}_{1}+{\\overrightarrow{X}}_{2}+{\\overrightarrow{X}}_{3}\\right)}{3}$$
    (15) where xt + 1 is the location of the next iteration. Using Eq. (15) to find
    a new location for the leading wolves drives the Omega wolves to change their
    locations to converge with prey. The GWO algorithm sequence consists of three
    steps: initialization, fitness calculation, swarm individual position updates,
    and the best result generation. The optimization process starts with the starting
    value for all control parameters, and all gray wolves are altered in regular intervals.
    The fitness function is then calculated based on the initial data, and the best
    solutions are identified as wolves of alpha, beta, and delta. The next step is
    to upgrade all gray wolves other than delta, beta, and alpha wolves. The next
    step is to renew all gray wolves'' positions and controller parameter values,
    followed by Alpha, Beta, and delta wolves. Finally, the alpha wolf returns its
    optimal position value. Fog computing and IOT The providers of cloud computing
    frequently utilize data centers that consider a variety of factors, including
    energy consumption and user proximity. Thus, the cloud layer, the top layer, often
    comprises a cloud infrastructure made up of data centers that provide resources
    and amenities that are dynamically assigned according to the demands of the users.
    These services could include networking, storage, and server (rendering tools,
    computational power, and so on) capabilities48. Fog Computing attempts to bring
    processing capabilities closer to end-users, preventing overuse of Cloud resources,
    further lowering computational burdens, enhancing load balancing, and shortening
    wait times49,50. The Internet of Things (IoT), which represents the future of
    communications and computers, is a breakthrough technology. IoT is now used in
    almost every sector, including intelligent cities, intelligent traffic control,
    and intelligent homes. The deployment of IoT is wide and may be applied in any
    field. IoT aids in better resource and crop management, crop monitoring, cost-effective
    agriculture, and increased quantity and quality. Air temperature sensors, soil
    moisture, soil pH, water volume, humidity, and other IoT sensors are employed47.
    Figure 4 shows IoT in agriculture using edge computing, fog computing, and cloud
    computing. Figure 4 Smart agriculture IoT with edge, fog, and cloud computing.
    Full size image The key benefits of IoT in agriculture are discovered in these
    points51: Community agriculture in rural and urban regions, utilizing software
    and hardware resources as well as vast amounts of data. Quality and logistical
    traceability of food security that allows reduced costs via real-time decision-making
    data. Business strategies established in the agricultural setting that enable
    direct consumer contact. Crop surveillance allows cost savings and machine robbery
    avoidance. Systems of automatic irrigation that operate based on soil moisture
    levels, and temperature measured by sensors. Environmental characteristics are
    automatically collected via sensor networks for subsequent analysis and processing.
    Large quantities of data are analyzed by decision support systems to increase
    production and operational efficiency. At the end of this section, we can summarize
    this paper in 3 folds; the first is applying DL models (AlexNet, GoogleNet) to
    extract features from plants. Secondly is using an optimization algorithm called
    the modified grey wolf optimization algorithm for eliminating the redundant features.
    The third is the classification of the output images using the support vector
    machine. The above techniques are divided to be used some processes in Fog and
    some processes in cloud computing. The next section introduces the architecture
    of the proposed solution using the deep learning techniques referred to above.
    Proposed methodology As technology advances, smart agricultural solutions are
    becoming more prevalent. Since then, technology has returned to agriculture with
    the latest trends and techniques it has produced. A significant advantage of smart
    agriculture is connecting to existing 3G and 4G networks using existing hardware
    and software. For smart agriculture solutions, it speeds up setting up hardware,
    resulting in the various successful implementation of IoT in agriculture that
    can run in a fog or cloud environment. There will be an evolution from the existing
    standard mobile computing scenario of smartphones and their apps to the connection
    of gadgets around us to help solve a real-world problem52. We’ll discuss in this
    section the proposed methodology based on the mentioned transfer learning, pretraining
    methods, and the optimization algorithm on fog and cloud computing using IoT sensors
    common in the problem statement of this paper. Figure 5 shows the block diagram
    of the proposed IoT smart agriculture network architecture which consists of three
    layers. The first layer contains the IOT devices that are used for different purposes
    in agriculture. Many technologies are being used in IoT agricultural solutions
    which have an important role in modernizing the services of IoT agriculture. Examples
    of these technologies are cloud and edge computing, machine learning and big data
    analytics, communication networks and protocols, and robotics. The second layer
    presents the sequence of work in this paper from collecting the images from IoT
    sensors then preprocessing these images if they need to resize, or normalize,
    or removing noise according to the recommended DL algorithms in this paper (CNN,
    SVM). All processes happened on the images from collecting it till detection of
    plant diseases are applied on fog environment to facilitate the function of scalability
    and stability that are advantages of fog computing. The third layer is connecting
    with cloud computing for henting resources for further and large processing. The
    other proposed models don’t suitable for cloud or fog computing, so we proposed
    a new model for plant disease detection using machine learning techniques by the
    Internet of Things (IoT) sensors that can run on fog or cloud environments. Figure
    5 Block diagram of the proposed IoT smart agriculture network architecture. Full
    size image The proposed model depends on deep learning, transfer learning, and
    shallow machine learning. In deep learning, multi-hidden layers are stacked for
    learning objects significantly. These layers require a training process including
    “fine-tuning” for adjusting the weights slightly of DNN discovered during the
    procedure of backpropagation. In turn, following an efficient training procedure,
    DL nets can categorize, extract characteristics, and give a decision effectively
    and accurately. In the proposed model, we use transfer learning to optimize different
    pre-innovated CNNs architectures to the datasets. As seen in Fig. 6, the proposed
    model starts by a data acquisition layer in which images are collected for different
    plants. This acquisition procedure was entirely wi-fi enabled, which means that
    the camera and the computer were linked with each other via the internet. In the
    preprocessing phase, the images are reconstructed and resized since the images
    are taken from various sources and their dimensions vary. In addition, the model
    layer of each of these products needs separate image dimensions to be managed.
    Therefore, the input image size is adjusted to fit the templates that are used
    in this analysis. Figure 6 Dataflow diagram of the proposed methodology. Full
    size image The feature extraction layer comes after image enhancements that represent
    the layer in which most of the calculations are carried out. The calculations
    include extracting image data set features and preserve the spatial relationship
    between image pixels. A pre-trained CNN, AlexNet, was used as feature extraction
    and we extracted the ’fc7’ layer as our feature extracting layer. After extracting
    the features, it is the role of feature subset selection to reduce the features
    and eliminate the irrelevant features. The proposed model makes use of a modified
    version of the grey wolf optimization algorithm. The details of the modified grey
    wolf optimization algorithm (MGWO) are explained in the next subsection. After
    that, the generated features set were utilized to train the SVM. Once we get the
    baseline SVM, we use a validated data set to adjust SVM parameters. Modified grey
    wolf optimization algorithm (MGWO) Mirjalili showed that The GWO algorithm tends
    to become stuck at optimal local values because of the small number of control
    parameters utilized in its simplest form. Because of this, researchers modified
    GWO by adding additional controls and changing control parameter values. According
    to their findings, the alpha-wolf was more powerful than the delta—and beta-wolf
    when searching for food. So, it''s possible to acquire better outcomes in tests
    in this manner. For this reason, there is much research in the literature that
    has adapted and developed the grey wolf algorithm in various sectors. As a result
    of this, it produces superior outcomes in tests47. The parameter adjusted equation
    for the \"a\" parameter was used in this study to improve the method50 significantly.
    However, instead of using the usual GWO Eq. (7), this study uses Eq. (16) to derive
    the parameter \"a\" instead. $$a=2\\times ({e }^{\\frac{-t\\times s}{tmax}})$$
    (16) \"s\" is only added in Eq. (16) to \"a\", and it reflects the total number
    of individuals in the swarm, as seen in Eq. (16). Standard GWO has a linearly
    decreasing \"a\" parameter, which prevents the algorithm from settling on local
    minimum values. Researchers found that as the \"a\" attribute approaches 0, it
    not only keeps the algorithm from reaching a locally minimal value but also considerably
    enhances its strength. Therefore, the method converges on the optimal values faster
    when this parameter is reduced from 2 to 0. So, the program has sped up and parabolically
    slowed down from 2 to 0. Moreover, it can be seen from the governing Eq. (15)
    that the dominants perform a similar function in the searching procedure; each
    of the grey wolves converges or flees away from the surroundings with an average
    weight of the beta, delta, and alpha. Even if the alpha is closest to the victim
    at first, it may be distant from the eventual result. Only the alpha position
    should be considered in Eq. (15) at the beginning of the search operation, or
    its weight should be substantially more significant than that of other dominants.
    Equation (15)''s average weight, on the other hand, contradicts the grey wolf
    social hierarchy idea. If the pack''s social hierarchy is strictly observed, the
    alpha is in charge, which could mean that he/she will always be the closest to
    the prey. This indicates that the alpha wolf''s weight in Eq. (15) should never
    be smaller than that of the delta and beta wolves. As a result, the beta''s weight
    should always be more than the delta''s. In light of these concerns, the authors53
    further hypothesize the following: (1) The dominants surround a supposed prey
    when it is being searched for, but they do not surround an actual prey when being
    hunted. As their social hierarchy dictates, the dominant grey wolves encircle
    the prey in order of their dominance. The alpha is the closest wolf in the pack,
    followed by the beta and the delta. Omega wolves play a role in this process,
    passing on their superior positions to the dominants. (2) Alpha controls the search
    and hunting process, while beta has a minor role, and delta has an even smaller
    one. A wolf''s position changes if an alpha wins out over his/her peers. Equation
    (15) should not use the same updating procedure for the positions as the previous
    hypotheses. Thus, the alpha weight should be near 1.0 at the beginning, whereas
    delta and beta weights could be close to 0. According to Eq. (15), the delta,
    beta, and alpha wolves should surround the victim at the final stage. During the
    entire process of searching, the alpha is always found by the beta, and the beta
    always finds the delta because he/she is always ranked third. As a result, the
    beta and delta weights are determined by the total number of iterations. Alpha''s
    weight should be reduced, and beta and delta''s weights should rise. In mathematics,
    the above ideas could be stated. When adding up the weights, ensure that they''re
    all varying and that the aggregate is capped at 1.0. As a result, Eq. (15) is
    altered to the following: $${\\overrightarrow{X}}_{\\left(t+1\\right)}={w}_{1}{\\overrightarrow{X}}_{1}+{w}_{2}{\\overrightarrow{X}}_{2}+{w}_{3}{\\overrightarrow{X}}_{3}$$
    (17) $${w}_{1}+{w}_{2}+{w}_{3}=1$$ As a second rule, when calculating the alpha
    w1, beta w2, and delta w3 weights, they should always satisfy w1 >  > w2 > w3.
    Along with the search technique, the weight of the alpha would be adjusted from
    1.0 to 1/3. While doing so, we''ll boost beta and delta''s weights, increasing
    them from 0.0 to 1/3 in the process. W1 can be described using a cosine function
    if we limit the angle range to be between [0, arccos (1/3)]. The weights should
    be adjusted based on the total iterations or \"it\" as a third point. And we recognize
    that w2⋯w3 ⟶ 0 if it = 0 and w1, w2, w3 ⟶ 1/3 when it ⟶ ∞. As a result, we present
    an arc-tangent function that changes from 0.0 to π/2. And then, somehow, cos (π/4) = sin
    (π/4) = − 2 √ /2, so different angular parameter φ was organized as seen below53:
    $$\\mathrm{\\varphi }=\\frac{1}{2}\\mathrm{ arctan}(it)$$ (18) Given that w2 would
    be maximized from 0.0 to 1/3 alongside it, we assume that it includes cos φ and
    sin θ and θ ⟶ arccos (1/3) when it ⟶ ∞; hence, $$\\uptheta =\\frac{2}{\\uppi }\\mathrm{arccos}\\frac{1}{3}.\\mathrm{arctan}(it)$$
    (19) when it ⟶ ∞, θ ⟶ arccos (1/3), w2 = 1/3, we can then formulate w2 in detail.
    The following is a new updating method for positions with variable weights that
    are based on these principles: $${w}_{1}=\\mathrm{cos\\theta },$$ (20) $${w}_{2}=\\frac{1}{2}\\mathrm{sin\\theta
    }.\\mathrm{ cos \\varphi },$$ (21) $${w}_{3}=1-{w}_{1}-{w}_{2}$$ (22) The flowchart
    of the Modified Gray Wolf Optimization (MGWO) algorithm is shown in Fig. 7. Figure
    7 Flowchart of the grey wolf optimization algorithm. Full size image The pseudocode
    of the MGWO is presented in algorithm 2. Experimental results and discussion Performance
    measures True positives, true negatives, false negatives, and false positives,
    and are displayed separately in the table, in two rows and two columns, accordingly
    (sometimes also referred to as a confusion matrix). In this way, the classification
    proportion can be studied in greater detail (accuracy). An unbalanced data collection
    (i.e., when the number of observations in different classes changes dramatically)
    will lead to inaccurate conclusions. Sensitivity and specificity are also valuable
    traits to have. As shown in Table 254, the most widely used measures are used
    to create the confusion matrix (Data science, 2019). Five measurements are utilized
    in this article to gauge our work performance, these measurements are shown in
    Table 3. Table 2 The confusion matrix. Full size table Table 3 Performance measures.
    Full size table Experiment 1 We provided the results as two experiments. For the
    first experiment, a modified grey wolf optimization method (MGWO) for feature
    selection is being evaluated. When developing a machine-learning model, feature
    selection is becoming increasingly important. The feature selection process involves
    deleting irrelevant or redundant characteristics and picking the ideal subset
    of features that better categorize patterns that belong to different plants. The
    evaluation is made by using fifteen standard feature selection datasets. The overall
    properties of these datasets are given in Table 455. Table 4 Datasets used for
    evaluating the MGWO. Full size table Using a random seeding strategy, a random
    population of n wolves or search agents is formed in the first part of the procedure.
    An ideal solution is found when the number of features \"d\" equals that of the
    original dataset features set. When selecting features for purity classification,
    make sure they enhance accuracy. Identify the appropriate characteristics (one
    value) and discard the rest (zero). Initially, the binary values (0 and 1) were
    set in each solution. A large part of GWO''s success depends on the development
    of initial populations. We use chaotic initialization of maps to increase the
    global convergence speed of the MGWO optimization process. Instead of a standard
    map, a chaotic map serves to improve the balance of search-and-exploitation skills.
    The logistics map is one of the most effective chaos-based approaches. It is represented
    as follows in Eq. (28) 56. $$ {\\text{X}}_{{{\\text{n}} + {1}}} = \\, \\phi \\left(
    {{\\text{X}}_{{\\text{n}}} , \\, \\mu } \\right) = \\, \\mu \\times {\\text{ X}}_{{\\text{n}}}
    \\left( {{1} - {\\text{ X}}_{{\\text{n}}} } \\right) $$ (28) where μ is set to
    4, the bifurcation coefficient is also defined. xn means the nth chaotic variable,
    in other words, xn ∈ (0, 1) in favor of limitations that the initial x0 ∈ (0,
    1) of severely static periods (0, 0.25, 0.5, 0.75,1). Figure 8 of the logistic
    map shows a consistently divided sequence, which prevents it from effectively
    immersing into minor regular cycles. Figure 8 Flowchart of logistic map for initialization.
    Full size image Because the problem has more than one objective, it is understood
    to be a multi-objective problem57. Following steps must be taken to solve the
    multi-objective issue of selecting optimal features. The first is to produce the
    highest accuracy rate, and the second is to eliminate the features to the lowest
    range. Taking this into consideration, the fitness function of the resulting solution
    evaluation is configured to balance the aims as follows: $$\\mathrm{fitness}=\\mathrm{\\alpha
    }{\\upgamma }_{\\mathrm{R}}\\left(\\mathrm{D}\\right)+\\upbeta \\frac{\\left|\\mathrm{S}\\right|}{\\left|\\mathrm{D}\\right|}$$
    (29) Given that \\(\\left|S\\right|\\) for the length of the selected subset feature
    cardinality, \\(\\alpha \\) and \\(\\beta \\) are generated as parameters for
    expressing a weight for the percentage of classification accuracy and the total
    number of selected features respectively, α ϵ [0,1] and β = 1 − α and have been
    selected concerning the evaluation function, \\({\\gamma }_{R}\\left(D\\right)\\)
    denotes the classification error rate. \\(\\left|D\\right|\\) represents dataset
    cardinality. So, to find the K neighbors for the KNN classifier, the Euclidean
    distance58 must be calculated as follows: $${\\mathrm{EUC}}_{\\mathrm{d}}\\left(\\mathrm{P},\\mathrm{Q}\\right)=\\sqrt{\\sum_{\\mathrm{i}=1}^{\\mathrm{d}}{\\left({\\mathrm{Q}}_{\\mathrm{i}}-{\\mathrm{P}}_{\\mathrm{i}}\\right)}^{2}}$$
    (30) Qi and Pi relate to a specific feature in the sample, \"i\" refer to the
    number of features in the sample, and d refers to the overall number of features
    used in the analysis. To reduce overfitting, cross-validation is a popular strategy.
    Cross-validation with K = 10 is utilized in this paper. In contrast to binary
    values, continuous values represent the positions of the search agents formed
    by the algorithm. A straight application to our situation would be impossible
    because it is incompatible with the standard binary format for feature selections.
    Features are selected depending on the problem of feature selection to increase
    the performance and accuracy of the classification system (0 or 1 in the case
    of binary features). A transformation function is used to convert a binary search
    space. The following equations can convert any continuous value into binary with
    the sigmoid function57: $$ {\\text{x}}_{{{\\text{s}}_{{\\text{i}}} }} = \\frac{1}{{1
    + {\\text{e}}^{{ - 10({\\text{x}}_{{\\text{i}}} - 0.5)}} }} $$ (31) $$ {\\text{x}}_{{{\\text{binary}}}}
    = \\left\\{ {\\begin{array}{*{20}c} 0 & {{\\text{if}}\\quad {\\text{R < x}}_{{{\\text{s}}_{{\\text{i}}}
    }} } \\\\ 1 & {{\\text{if}}\\quad {\\text{R}} \\ge {\\text{x}}_{{{\\text{s}}_{{\\text{i}}}
    }} } \\\\ \\end{array} } \\right. $$ (32) where i = 1, …, d, and \\({x}_{binary}\\)
    parameter identified as 0 or 1 by randomly selected value in range: R ϵ [0,1]
    value compared to \\({x}_{{s}_{i}}\\), the value of the parameter \\({x}_{{s}_{i}}\\)
    which defined in the S-shaped search agent is the value identified by the algorithm
    calculations (continuous), All trials were conducted on a Windows 10 Pro 64-bit
    operating system with a Core(TM) i7-8550U CPU running at 1.80 GHz and 1.99 GHz,
    respectively. To implement the algorithms, we use MATLAB (2018a). The selected
    values of the algorithms to be its parameters were collected from the literature
    to make sure that the algorithms are compared on an equal basis59. Although the
    KNN classification unit for feature selection is a frequent wrapper, it can also
    be thought of as a learning algorithm that is monitored and characterized by simple
    and quick learning. There are twenty different runs for each algorithm with a
    random seed. The maximum number of iterations for all subsequent experiments using
    the standard k-fold cross-validation is 20. Multiple observational experiments
    were conducted on a variety of datasets to determine the best literature values
    for α and β. Therefore, it has the value of 0.9 for α and has the value of 0.1
    for β. The parameters settings of our experiments are shown in Table 5. Table
    5 Parameters settings. Full size table Tables 6 and 7 show the resulted feature
    and the accuracy respectively. The experimental results are conducted for the
    standard grey wolf optimization (GWO), the Ant Colony Optimization (ACO), the
    Butterfly Optimization Algorithm (BOA), the Particle Swarm Optimization (PSO),
    and the Modified Grey Wolf Optimization (MGWO) algorithms. The experimental results
    show the superiority of the proposed MGWO in both achieving the least set of features
    in all the datasets while producing a fair accuracy in most of the utilized datasets.
    These results are graphically displayed in Figs. 9 and 10. Table 6 The features
    reduction for different algorithms. Full size table Table 7 The classification
    accuracy for different algorithms. Full size table Figure 9 The features reduction
    for different algorithms. Full size image Figure 10 The classification accuracy
    for different algorithm. Full size image According to the conclusion of these
    results, we can say that the MGWO can be used for our plant disease problem. Experiment
    2 According to the experimental result in the first experiment, the modified grey
    wolf optimization algorithm (MGWO) can be effective as a wrapper feature selection
    algorithm. In experiment 2, the core problem of the plant disease classification
    and prediction is introduced. As discussed in the previous section, the first
    stage of the proposed model is the feature extraction process where the pre-trained
    AlexNet CNN is used. This process is performed for ten datasets. The second stage
    is the feature selection process, in which the MGWO is used as the wrapper feature
    section method. Lastly, the generated reduced features set were used SVM training.
    The datasets’ details are discussed in the next subsection. Datasets description
    Plants play a crucial role in climate regulation and erosion reduction. To preserve
    the environment, ecosystem, and living beings, they are both equally necessary
    to consider. Deciduous and coniferous trees are the most common types. Compared
    to conifers, deciduous trees have broader and bigger leaves. During the fall,
    their leaves fall off. This is due to the giant leaf, which allows for more photosynthesis
    to occur. Trees of this type are famous for their high wood production. There
    is a coniferous tree or evergreen tree green throughout the year. Leaves have
    a triangular form and grow upwards in most cases. Even though they have softer
    wood, they are pretty durable and resistant to various weather conditions60. The
    data on https://data.mendeley.com/datasets/hb74ynkjcn/1 focuses on plants that
    contribute both ecologically and economically. As a result, ten different plants,
    such as Jamun, Lemon, Sukh Chain, Arjun, Pomegranate, Jatropha, Mango, Saptaparni,
    Guava, and Chinar, have been selected, as shown in Table 8. Images have been divided
    into two categories: healthy and diseased images. Table 9 shows the dataset description.
    Table 8 Sample of healthy and diseased leaf images of the plant’s disease dataset.
    Full size table Table 9 Plants diseases datasets description. Full size table
    Results The proposed model (AlexNet as feature extraction, MGWO as a feature selection,
    and the SVM as a classifier) has achieved better results compared to Alexnet,
    GoogleNet, and the SVM. The results showed in Table 10 give a comparison among
    the AlexNet, GoogleNet, SVM, and the proposed model through different metrics
    such as sensitivity, specificity, precision, F1-score, and accuracy. The proposed
    model achieved the highest accuracy in all datasets except the dataset named p2
    in which the GoogleNet achieved the best accuracy. Figure 11 display the comparison
    among the different model concerning the accuracy metric. A comparison between
    the SVM which trained for the extracted features directly without feature selection
    and the SVM which trained to the selected features by the MGWO that extracted
    by AlexNet showed in Fig. 12. The ROC curve on the test set for the proposed model
    SVM is introduced in Fig. 13. Table 10 Classification results for the four models.
    Full size table Figure 11 Classification accuracy for the four models. Full size
    image Figure 12 Classification accuracy of the standard SVM Vs the proposed model.
    Full size image Figure 13 The ROC curve on the test set for the proposed model
    SVM. Full size image Conclusion and future work We present in this paper a paradigm
    for the identification of plant diseases. Initially, a comparison is undertaken
    using the SVM, AlexNet, and Google Net-based transfer training method, which will
    be used on the edge servers with increased computational capability, to detect
    plant diseases. Then, with the AlexNet feature extraction and support vector machines
    for plant detection and classification diseases, we proposed a hybrid approach
    based on the modified gray wolf optimization algorithm for eliminating the resulted
    features from the AlexNet. The proposed model can operate on Internet of Things
    (IoT) devices that use a framework that integrates fog and cloud computing with
    limited resources. Experimental evidence shows that the suggested models can detect
    plant diseases accurately using the minimum computational resources from real-world
    datasets. The proposed model worked better on most data sets. In the future, using
    blockchain technology, we hope to improve the fog environment without impacting
    the efficiency of features map extraction. We will also develop apps to detect
    plant diseases to support smart agriculture with deep learning support. Data availability
    The datasets analyzed for this study available in “https://data.mendeley.com/datasets/hb74ynkjcn/1”
    focus on plants that contribute both ecologically and economically. All datasets
    used are open access data, and we didn’t use any private data. Our research complies
    with institutional, national, and international guidelines and legislation. We
    have permissions from our institutional committee for scientific research ethics
    to do this study and to collect plants data from the open access dataset. Throughout
    the entirety of the process of generating all the plots and statistics, Microsoft
    Excel 2016 and MATLAB were utilized. On the other hand, figures were generated
    using Microsoft PowerPoint and Adobe Photoshop. All plant samples used in the
    proposed model and tables are samples from the utilized dataset, which is open
    access data. Abbreviations AI: Artificial intelligence ML: Machine learning DL:
    Deep learning GWO: Grey wolf optimization IoT: Internet of things QoS: Quality
    of service MGWO: Modified grey wolf optimization DTL: Data transfer learning SSD:
    Single shot detector RCNN: Recurrent convolutional neural network MLP: Multi-layer
    perceptron FCN: Fully convolutional neural network KNN: K-nearest neighbor VGG:
    Visual geometry group RF: Random forest SVM: Support vector machines CNN: Convolutional
    neural network DCNN: Deep convolutional neural network DAML: Deep adversarial
    metric learning CAE: Convolutional autoencoder GLDD: Grape leaf disease dataset
    GPDCNN: Global pooling extended convolutional neural network EPA: Environmental
    Protection Agency TN: True negatives FN: False negatives FP: False positives TP:
    True positives ACO: Ant colony optimization PSO: Particle swarm optimization BOA:
    Butterfly optimization algorithm References Liakos, K. G., Busato, P., Moshou,
    D., Pearson, S. & Bochtis, D. Machine learning in agriculture: A review. Sensors
    18(8), 2674 (2018). Article   ADS   PubMed   PubMed Central   Google Scholar   Xiao,
    L., Zhao, R. & Zhang, X. Crop cleaner production improvement potential under conservation
    agriculture in China: A meta-analysis. J. Clean. Prod. 269, 122262 (2020). Article   Google
    Scholar   Lee, S. H., Chan, C. S., Mayo, S. J. & Remagnino, P. How deep learning
    extracts and learns leaf features for plant classification. Pattern Recognit.
    71, 1–13 (2017). Article   ADS   Google Scholar   Ale, L., Sheta, A., Li, L.,
    Wang, Y., Zhang, N. Deep learning based plant disease detection for smart agriculture.
    In 2019 IEEE Globecom Workshops (GC Wkshps), 1–6 (2019). Liu, J. & Wang, X. Plant
    diseases and pests detection based on deep learning: A review. Plant Methods 17(1),
    22 (2021). Article   PubMed   PubMed Central   Google Scholar   Saleem, M. H.,
    Potgieter, J. & Arif, K. M. Plant disease detection and classification by deep
    learning. Plants 8(11), 468 (2019). Article   PubMed   PubMed Central   Google
    Scholar   Singh, V. & Misra, A. K. Detection of plant leaf diseases using image
    segmentation and soft computing techniques. Inf. Process. Agric. 4(1), 41–49 (2017).
    Google Scholar   Mahum, R. et al. A novel framework for potato leaf disease detection
    using an efficient deep learning model. Hum. Ecol. Risk Assess. Int. J. 29(2),
    303–326 (2023). Article   CAS   Google Scholar   Gouse, S., Dulhare, U. N. Automation
    of Rice Leaf Diseases Prediction Using Deep Learning Hybrid Model VVIR. In Advancements
    in Smart Computing and Information Security: First International Conference, ASCIS
    2022, Rajkot, India, November 24–26, 2022, Revised Selected Papers, Part I, 133–143
    (2023). Sarhan, A. Fog computing as solution for IoT-based agricultural applications.
    In Smart Agricultural Services Using Deep Learning, Big Data, and IoT, 46–68 (IGI
    Global, 2021). Abbas, A., Jain, S., Gour, M. & Vankudothu, S. Tomato plant disease
    detection using transfer learning with C-GAN synthetic images. Comput. Electron.
    Agric. 187, 106279 (2021). Article   Google Scholar   Thenmozhi, K. & Reddy, U.
    S. Crop pest classification based on deep convolutional neural network and transfer
    learning. Comput. Electron. Agric. 164, 104906 (2019). Article   Google Scholar   Wiesner-Hanks,
    T. et al. Millimeter-level plant disease detection from aerial photographs via
    deep learning and crowdsourced data. Front. Plant Sci. 10, 1550 (2019). Article   PubMed   PubMed
    Central   Google Scholar   Too, E. C., Yujian, L., Njuki, S. & Yingchun, L. A
    comparative study of fine-tuning deep learning models for plant disease identification.
    Comput. Electron. Agric. 161, 272–279 (2019). Article   Google Scholar   Chen,
    T. et al. Intelligent identification system of disease and insect pests based
    on deep learning. China Plant Prot. 39(04), 26–34 (2019). Google Scholar   Zhou,
    G., Zhang, W., Chen, A., He, M. & Ma, X. Rapid detection of rice disease based
    on FCM-KM and faster R-CNN fusion. IEEE Access 7, 143190–143206 (2019). Article   Google
    Scholar   Sethy, P. K., Barpanda, N. K., Rath, A. K. & Behera, S. K. Deep feature
    based rice leaf disease identification using support vector machine. Comput. Electron.
    Agric. 175, 105527 (2020). Article   Google Scholar   Rahman, C. R. et al. Identification
    and recognition of rice diseases and pests using convolutional neural networks.
    Biosyst. Eng. 194, 112–120 (2020). Article   Google Scholar   Guo, Y. et al. Plant
    disease identification based on deep learning algorithm in smart farming. Discret.
    Dyn. Nat. Soc. 2020, 1–11 (2020). Google Scholar   Atila, Ü., Uçar, M., Akyol,
    K. & Uçar, E. Plant leaf disease classification using EfficientNet deep learning
    model. Ecol. Inform. 61, 101182 (2021). Article   Google Scholar   Gadekallu,
    T. R. et al. A novel PCA—whale optimization-based deep neural network model for
    classification of tomato plant diseases using GPU. J. Real-Time Image Process.
    18, 1383–1396 (2021). Article   Google Scholar   Sanga, S., Mero, V., Machuve,
    D., Mwanganda, D. Mobile-based deep learning models for banana diseases detection.
    arXiv Prepr. arXiv2004.03718 (2020). Chohan, M., Khan, A., Chohan, R., Hassan,
    S. & Mahar, M. Plant disease detection using deep learning. Int. J. Recent Technol.
    Eng. 9(1), 909–914 (2020). Google Scholar   Guo, X. Q., Fan, T. J. & Shu, X. Tomato
    leaf diseases recognition based on improved multi-scale AlexNet. Trans. Chin.
    Soc. Agric. Eng. 35(13), 162–169 (2019). Google Scholar   Tan, L., Lu, J. & Jiang,
    H. Tomato leaf diseases classification based on leaf images: A comparison between
    classical machine learning and deep learning methods. AgriEngineering 3(3), 542–558
    (2021). Article   Google Scholar   Agarwal, M., Singh, A., Arjaria, S., Sinha,
    A. & Gupta, S. ToLeD: Tomato leaf disease detection using convolution neural network.
    Proc. Comput. Sci. 167, 293–301 (2020). Article   Google Scholar   Kundu, N. et
    al. IoT and interpretable machine learning based framework for disease prediction
    in pearl millet. Sensors 21(16), 5386 (2021). Article   ADS   PubMed   PubMed
    Central   Google Scholar   Zhang, S., Zhang, S., Zhang, C., Wang, X. & Shi, Y.
    Cucumber leaf disease identification with global pooling dilated convolutional
    neural network. Comput. Electron. Agric. 162, 422–430 (2019). Article   Google
    Scholar   Khamparia, A. et al. Seasonal crops disease prediction and classification
    using deep convolutional encoder network. Circuits Syst. Signal Process. 39(2),
    818–836 (2020). Article   Google Scholar   Bedi, P. & Gole, P. Plant disease detection
    using hybrid model based on convolutional autoencoder and convolutional neural
    network. Artif. Intell. Agric. 5, 90–101 (2021). Google Scholar   Faggella, D.
    Ai in agriculture—present applications and impact. Emerj Artif. Intell. Res. Insight.
    18, 2020 (2020). Google Scholar   Boulent, J., Foucher, S., Théau, J. & St-Charles,
    P.-L. Convolutional neural networks for the automatic identification of plant
    diseases. Front. Plant Sci. 10, 941 (2019). Article   PubMed   PubMed Central   Google
    Scholar   Samuel, A. L. Some studies in machine learning using the game of checkers.
    IBM J. Res. Dev. 3(3), 210–229 (1959). Article   MathSciNet   Google Scholar   Kujawa,
    S., Mazurkiewicz, J. & Czekała, W. Using convolutional neural networks to classify
    the maturity of compost based on sewage sludge and rapeseed straw. J. Clean. Prod.
    258, 120814 (2020). Article   Google Scholar   Saleem, M. H., Potgieter, J. &
    Arif, K. M. Automation in agriculture by machine and deep learning techniques:
    A review of recent developments. Precis. Agric. 22, 1–39 (2021). Google Scholar   Weiss,
    K., Khoshgoftaar, T. M. & Wang, D. A survey of transfer learning. J. Big Data
    3(1), 1–40 (2016). Article   Google Scholar   Krizhevsky, A., Sutskever, I. &
    Hinton, G. E. Imagenet classification with deep convolutional neural networks.
    Adv. Neural Inf. Process. Syst. 25, 1097–1105 (2012). Google Scholar   Hemmer,
    M., Van Khang, H., Robbersmyr, K. G., Waag, T. I. & Meyer, T. J. J. Fault classification
    of axial and radial roller bearings using transfer learning through a pretrained
    convolutional neural network. Designs 2(4), 56 (2018). Article   Google Scholar   Dhillon,
    A. & Verma, G. K. Convolutional neural network: A review of models, methodologies
    and applications to object detection. Prog. Artif. Intell. 9(2), 85–112 (2020).
    Article   Google Scholar   Özyurt, F. Efficient deep feature selection for remote
    sensing image recognition with fused deep learning architectures. J. Supercomput.
    76, 1–19 (2019). Google Scholar   Vapnik, V. The Nature of Statistical Learning
    Theory (Springer Science & Business Media, 2013). MATH   Google Scholar   Soman,
    K. P., Loganathan, R. & Ajay, V. Machine Learning with SVM and Other Kernel Methods
    (PHI Learning Pvt. Ltd., 2009). Google Scholar   Pedersen, R., Schoeberl, M. An
    embedded support vector machine. In 2006 International Workshop on Intelligent
    Solutions in Embedded Systems, 1–11 (2006). Altuntacs, Y. & Kocamaz, F. Deep feature
    extraction for detection of tomato plant diseases and pests based on leaf images.
    Celal Bayar Univ. J. Sci. 17(2), 145–157 (2021). Google Scholar   Bishop, C. M.
    Pattern recognition. Mach. Learn. 128(9) (2006). Mirjalili, S., Mirjalili, S.
    M. & Lewis, A. Grey wolf optimizer. Adv. Eng. Softw. 69, 46–61 (2014). Article   Google
    Scholar   Dereli, S. A new modified grey wolf optimization algorithm proposal
    for a fundamental engineering problem in robotics. Neural Comput. Appl. 33, 1–13
    (2021). Article   Google Scholar   Tsipis, A. et al. An alertness-adjustable cloud/fog
    IoT solution for timely environmental monitoring based on wildfire risk forecasting.
    Energies 13(14), 3693 (2020). Article   Google Scholar   Guardo, E., Di Stefano,
    A., La Corte, A., Sapienza, M. & Scatà, M. A fog computing-based iot framework
    for precision agriculture. J. Internet Technol. 19(5), 1401–1411 (2018). Google
    Scholar   Goundar, S., Bhushan, S. B. & Rayani, P. K. Architecture and Security
    Issues in Fog Computing Applications (IGI Global, 2019). Google Scholar   Gómez-Chabla,
    R., Real-Avilés, K., Morán, C., Grijalva, P., Recalde, T. IoT applications in
    agriculture: A systematic literature review. In 2nd International Conference on
    ICTs in Agronomy and Environment, 68–76 (2019). Srinivasan, G., Vishnu Kumar,
    N., Shafeer Ahamed, Y. & Jagadeesan, S. Providing smart agricultural solution
    to farmers for better yielding using IoT. Int. J. Adv. Sci. Eng. Res 2(1), 2017
    (2017). Google Scholar   Gao, Z.-M. & Zhao, J. An improved grey wolf optimization
    algorithm with variable weights. Comput. Intell. Neurosci. 2019, 1–18 (2019).
    Article   CAS   Google Scholar   What is Confusion Matrix and Advanced Classification
    Metrics? Data Science and Machine Learning-blogger. manisha-sirsat.blogspot.com
    (2019). Dheeru, E. D., Taniskidou, K. {UCI} Machine Learning Repository. (2017).
    Dwivedi, S., Vardhan, M. & Tripathi, S. An effect of chaos grasshopper optimization
    algorithm for protection of network infrastructure. Comput. Netw. 176, 107251
    (2020). Article   Google Scholar   Abdel-Basset, M., El-Shahat, D., El-henawy,
    I., de Albuquerque, V. H. C. & Mirjalili, S. A new fusion of grey wolf optimizer
    algorithm with a two-phase mutation for feature selection. Expert Syst. Appl.
    139, 112824 (2020). Article   Google Scholar   Gou, J. et al. A generalized mean
    distance-based k-nearest neighbor classifier. Expert Syst. Appl. 115, 356–372
    (2019). Article   Google Scholar   El-Hasnony, I. M., Elhoseny, M. & Tarek, Z.
    A hybrid feature selection model based on butterfly optimization algorithm: COVID-19
    as a case study. Expert Syst. 39, e12786 (2022). Article   PubMed   Google Scholar   Chouhan,
    S. S., Singh, U. P., Kaul, A., Jain, S. A data repository of leaf images: Practice
    towards plant conservation with plant pathology. In 2019 4th International Conference
    on Information Systems and Computer Networks (ISCON), 700–707 (2019). Download
    references Author information Authors and Affiliations Faculty of Computers and
    Information Science, Mansoura University, Mansoura, Egypt Zahraa Tarek, Mohamed
    Elhoseny & Ibrahim M. EL-Hasnony College of Computing and Informatics, University
    of Sharjah, Sharjah, United Arab Emirates Mohamed Elhoseny Department of Computer
    Science, Al-Baha University, Al Bahah, Kingdom of Saudi Arabia Mohamemd I. Alghamdi
    Contributions All authors have equal contributions. Corresponding author Correspondence
    to Ibrahim M. EL-Hasnony. Ethics declarations Competing interests The authors
    declare no competing interests. Additional information Publisher''s note Springer
    Nature remains neutral with regard to jurisdictional claims in published maps
    and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article''s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article''s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Tarek, Z., Elhoseny,
    M., Alghamdi, M.I. et al. Leveraging three-tier deep learning model for environmental
    cleaner plants production. Sci Rep 13, 19499 (2023). https://doi.org/10.1038/s41598-023-43465-4
    Download citation Received 20 March 2022 Accepted 24 September 2023 Published
    09 November 2023 DOI https://doi.org/10.1038/s41598-023-43465-4 Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Subjects Energy infrastructure Engineering Comments By submitting a comment you
    agree to abide by our Terms and Community Guidelines. If you find something abusive
    or that does not comply with our terms or guidelines please flag it as inappropriate.
    Download PDF Sections Figures References Abstract Introduction Related work Methods
    and overviews Proposed methodology Experimental results and discussion Conclusion
    and future work Data availability Abbreviations References Author information
    Ethics declarations Additional information Rights and permissions About this article
    Comments Advertisement Scientific Reports (Sci Rep) ISSN 2045-2322 (online) About
    Nature Portfolio About us Press releases Press office Contact us Discover content
    Journals A-Z Articles by subject protocols.io Nature Index Publishing policies
    Nature portfolio policies Open access Author & Researcher services Reprints &
    permissions Research data Language editing Scientific editing Nature Masterclasses
    Research Solutions Libraries & institutions Librarian service & tools Librarian
    portal Open research Recommend to library Advertising & partnerships Advertising
    Partnerships & Services Media kits Branded content Professional development Nature
    Careers Nature Conferences Regional websites Nature Africa Nature China Nature
    India Nature Italy Nature Japan Nature Middle East Privacy Policy Use of cookies
    Your privacy choices/Manage cookies Legal notice Accessibility statement Terms
    & Conditions Your US state privacy rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Leveraging three-tier deep learning model for environmental cleaner plants
    production
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lin Y.B.
  - Chen W.E.
  - Chang T.C.Y.
  citation_count: '3'
  description: Integrating cloud with fog/edge is a main trend in networking. Many
    cloud computing applications have been shifted to the edge/fog domain. Such paradigm
    shift offers new opportunities for pervasive computing. An example is AgriTalk,
    an Internet of Things (IoT) application development platform for smart agriculture.
    By integrating cloud with edge/fog, this article describes how AgriTalk addresses
    six issues for developing edge/fog agriculture applications. These issues include
    device domain development, application generation and bug detection, sensor failure
    detection and calibration, big data management, Artificial Intelligence (AI) provisioning,
    and data privacy. We show how AgriTalk integrates fog/edge applications and use
    rice blast detection and piglet crushing mitigation as two examples to demonstrate
    that fog/edge computing is a better solution than cloud computing. Compared with
    cloud computing, fog/edge computing reduces the delays by 50 percent in AgriTalk.
    Through the low-code no-code approach, AgriTalk allows the farmers to create and
    maintain fog/edge agriculture applications by themselves.
  doi: 10.1109/MCOM.001.2200633
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Communications Magazine
    >Volume: 61 Issue: 12 Moving from Cloud to Fog/Edge: The Smart Agriculture Experience
    Publisher: IEEE Cite This PDF Yi-Bing Lin; Whai-En Chen; Ted C.-Y. Chang All Authors
    3 Cites in Papers 251 Full Text Views Abstract Document Sections Introduction
    AgriTalk-FE: the AgriTalk Architecture for IoT-FE Special FE-Talks AgriTalk-C:
    Integration of Cloud and Fog/Edge Domains Integrated Operations Center (IOC):
    MapTalk Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: Integrating cloud with fog/edge is a main trend in networking. Many
    cloud computing applications have been shifted to the edge/fog domain. Such paradigm
    shift offers new opportunities for pervasive computing. An example is AgriTalk,
    an Internet of Things (IoT) application development platform for smart agriculture.
    By integrating cloud with edge/fog, this article describes how AgriTalk addresses
    six issues for developing edge/fog agriculture applications. These issues include
    device domain development, application generation and bug detection, sensor failure
    detection and calibration, big data management, Artificial Intelligence (AI) provisioning,
    and data privacy. We show how AgriTalk integrates fog/edge applications and use
    rice blast detection and piglet crushing mitigation as two examples to demonstrate
    that fog/edge computing is a better solution than cloud computing. Compared with
    cloud computing, fog/edge computing reduces the delays by 50 percent in AgriTalk.
    Through the low-code no-code approach, AgriTalk allows the farmers to create and
    maintain fog/edge agriculture applications by themselves. Published in: IEEE Communications
    Magazine ( Volume: 61, Issue: 12, December 2023) Page(s): 86 - 92 Date of Publication:
    08 May 2023 ISSN Information: DOI: 10.1109/MCOM.001.2200633 Publisher: IEEE Funding
    Agency: Introduction In the advancement of the Internet of Things (IoT) technologies,
    one of the main trends in networking is the paradigm shift from cloud based IoT
    (IoT-C; that is, connecting the IoT device directly to a cloud server in Fig.
    1(1)) to fog/edge based IoT (IoT-FE; that is, connecting the IoT devices to the
    fog/edge nodes in Fig. 1 (2). We define three types of IoT-FE: monitoring-IoT,
    controlling-IoT, and automating-IoT. Monitoring-IoT allows the users to remotely
    read the sensor data ((3) → (2) in Fig. 1). Controlling-IoT allows the users to
    remotely control actuators ((2) → (4) in Fig. 1). Automating-IoT controls the
    actuators according to the sensor data without manual operation ((3) → (2) → (4)
    in Fig. 1). To deploy IoT-FE applications rapidly and reliably, an IoT development
    platform should provide functions to speed up application creation and management.
    Based on our experience with smart agriculture deployment in the past 8 years,
    this article describes the IoT-FE application development focusing on six issues.
    Issue 1–Device Domain Development Most commercial IoT applications use off-the-shelf
    sensors/actuators. To accommodate these devices in fog/edge computing, we need
    to integrate them with the microcontroller units (MCUs) with significant effort
    [1]. An MCU is an integrated circuit that consists of a processor unit, memory
    modules, communication interfaces (to connect the IoT servers) and peripheral
    pins (to connect sensors and actuators). To speed up the accommodation of an IoT
    device in fog/ edge computing, it is essential to provide a development environment
    in the IoT device domain. Examples of the MCU solutions include Ardublock, S4A,
    Webduino and ArduTalk [1]. Issue 2–Application Generation and Bug Detection IoT-FE
    application development consists of two phases [2]. In Phase 1, a proof-of-concept
    is built through simulation. In Phase 2, the real IoT application is implemented
    and experimented on a testbed. However, the code developed in Phase 2 may not
    be consistent with the code developed in Phase 1. In fog/edge computing, it is
    essential to automatically or semi-automatically generate the application codes
    and conduct automatic bug detection to speed up the development process [3]. Issue
    3–Sensor Failure Detection and Calibration Many IoT-FE applications fail due to
    inaccurate data produced by the sensors. Traditionally, sensor failure detection
    and calibration are conducted manually in an offline process. In fog/edge computing,
    there are many agricultural sensors spread across different locations. It is important
    to detect sensor failures and conduct calibration automatically to guarantee normal
    operations of the sensors [4], [5]. Issue 4–Big Data Management The data produced
    by the agricultural sensors from a large number of fog/edge nodes are typically
    handled by big data tools in an offline process. It is more beneficial to integrate
    the database system (Fig. 1 (5)) with the IoT-FEs and the IoT-C to support automatic
    real-time data analysis in fog/edge computing [6], [7]. Issue 5–Artificial Intelligence
    (AI) Provisioning Some data from the IoT-FE applications are manipulated by AI
    tools through an offline process [8]. Significant programming effort is required
    to integrate AI modeling into existing IoT-FE applications. Therefore, it is critical
    to provide transparent AI inferencing at IoT-FE (Fig. 1 (6)) and online AI training
    at IoT-C (Fig. 1 (7)) [6]. Issue 6–Data Privacy Since IoT-FE applications may
    support video monitoring, access rights must be enforced for data privacy [9].
    Also, the controlling-IoT and automating-IoT applications will trigger the actuators,
    and data security is required to protect these IoT-FE applications from illegal
    access. This article uses AgriTalk [10] as an example to address the above 6 issues
    for IoT-FE application development. A fog/edge node of AgriTalk is called an AgirTalk-FE.
    AgriTalk is an IoT-based smart agriculture platform for developing various AgriTalk-FEs,
    and the applications developed in an AgriTalk-FE are called FE-Talks. Examples
    are MapTalk for integrated operations center (IOC) and HouseTalk for greenhouse
    [10]. This article shows that fog/edge computing can be effectively achieved in
    these sustainable FE-Talk examples. We first describe how to organize the agriculture
    applications as FE-Talks hosted in the AgriTalk platform through a modular approach.
    Then we integrate every FE-Talk Graphical User Interface (GUI) with the MapTalk
    GUI to enhance the user experience. AgriTalk is an interesting fog/edge computing
    example, which conducts non-toxic organic farming at more than twenty soil farms
    and 4 greenhouses located in Taiwan, Japan, Philippe, Thailand and Armenia. AgriTalk
    eliminates the requirement of physical presence for some critical farming tasks
    through fog/edge computing to achieve distributed farm management. The article
    is organized as follows: The next section proposes the AgriTalk IoT-FE approach.
    Following that, we show several special FE-Talks, and then describe the AgriTalk
    mechanism on the cloud domain. We then elaborate on MapTalk, the IOC for AgriTalk,
    and illustrate PigTalk, a fog/ edge intelligent system for piglet crushing mitigation.
    The final section gives conclusions. AgriTalk-FE: the AgriTalk Architecture for
    IoT-FE Figure 2 illustrates the fog/edge AgriTalk architecture based on Fig. 1.
    The original AgriTalk was designed with a cloud-based server called AgriTalk-C
    in the network domain (Fig. 2 (3)), which directly connects the agricultural sensors/actuators
    in the device domain (Fig. 2 (1)) [10]. Figure 1. Fog/edge computing for IoT.
    Show All To offer fog/edge computing, the network domain is further partitioned
    into the fog/edge domain and the cloud domain, where the AgriTalk-FEs (individual
    farms) are deployed in the fog/edge domain (Fig. 2 (2)), and AgriTalk-C is located
    in the cloud domain. In this architecture, the low-layer communications among
    the three domains ((1) <-> (8), (1) <-> (5), (5) <-> (8) and (6) <-> (22)) can
    be Ethernet, Bluetooth, WiFi, 4G, and 5G. The high-layer APIs for these domains
    can be Restful or Message Queuing Telemetry Transport (MQTT), except that connections
    (5) <-> (8) and (6) <-> (22) use Restful API. The AgriTalk Database (DB) System
    (Fig. 2 (4)) is accessed through Object Relational Mapping (ORM). The AgriTalk-FEs
    interact with each other in two ways. They can exchange their data through the
    AgriTalk DB System or through AgriTalk-C. AgriTalk-C resides in the cloud as a
    docker container image. As an edge node in the current implementation, an AgriTalk-FE
    is installed in an industry version of Raspberry Pi 4. This section elaborates
    on AgriTalk-FE architecture for a fog/edge node. Multiple FE-Talks ((9), (10),
    (11), (13), (15), and (17) in Fig. 2) can be developed through the AgriTalk-FE
    GUI (Fig. 2 (6)), and be manipulated (e.g., parameter setups) through the FE-Talk
    GUIs (Fig. 2 (7)). Through connecting icons in the AgriTalk-FE GUI window, the
    program for an FE-Talk application is automatically created without any programming
    effort. This no-code approach allows the farmers to quickly deploy their farming
    applications. In Fig. 2, the system FE-Talks are marked green ((10), (11), (13),
    (15), and (17)) and the application FE-Talks are marked blue ((9)). Both the AgriTalk-FE
    and the FE-Talk GUIs are web based, which can be remotely accessed through any
    computing device with a browser (Fig. 3 (1)). In this GUI, a project window (Fig.
    3 (2)) allows the farmer to develop an FE-Talk. We use Figs. 2 and 3 to describe
    this development procedure. The IoT-FE device software binds an IoT device (Fig.
    2 (1)) to the AgriTalk-FE engine (Fig. 2 (8)) through several simple operations
    in the AgriTalk-FE GUI (Fig. 2 (6)), and an example of its layout is the “Project”
    window illustrated in Fig. 3 (2). AgriTalk defines a device model for real IoT
    devices with the same properties. For example, a smartphone model is mapped to
    real phones such as Apple iPhones, Samsung smart phones and so on. In the project
    window, an IoT device model is graphically represented by an icon, for example,
    “Sensors” (Fig. 3 (6)). This icon includes one or more smaller icons representing
    the “features” (individual sensors) of the device. For example, the features of
    a micro weather station (Fig. 3 (3)) include the sensors for temperature, humidity,
    electrical conductivity (EC), CO2 and so on. When we click the gear icon in the
    upper-left corner of a device icon, the physical device is bound to AgriTalk-FE
    (e.g., the (3) → (6) connection in Fig. 3). The binding mechanism is similar to
    the one for Bluetooth. We enable a sensor device to control an actuator device
    by dragging a “Join” link between them (see Joins 1–5 in Fig. 3). There is a circle
    in the middle of the link. When the circle is clicked, a function management window
    pops up (Fig. 3 (22)). We write functions in this window to manipulate the data
    passing through the link. For example, the soil EC values are collected in real
    time, and are used to train a fertilizer regulatory model for the farm. The model
    provides appropriate fertilizer solution, for example, the Nitrogen (N) to optimize
    the yield of the plant cultivation. The relationship between EC ( χ E ) and N
    ( f N ( χ E )) for a specific farm in the Bao Mountain is derived as [10] f N
    ( χ E )=63.2526 χ 2 E +14.2131 χ E +0.1797 (1) View Source Figure 2. The AgriTalk
    architecture for edge/fog computing. Show All Through this function, the values
    measured from the EC sensor are transformed into the amount of Nitrogen to drive
    the pump of the fertilizer solution through the path (3) → (6) → (8) → (7) in
    Fig. 3. This function is implemented as a Python program in the function management
    window; see Lines 3 and 4 in Fig. 3 (22). Note that AgriTalk-FE automatically
    integrates the Join functions to constitute the network program of the IoT-FE
    application, and this “Join” mechanism partially addresses Issue 2 in the fog/edge
    domain. FE-Talk is defined by the device models that can be accessed from the
    model pull-down list (Fig. 3 (5)). For example, the items in the AgriTalk model
    list include the micro weather station with the soil sensors (Fig. 3 (3)), the
    fertilizer and irrigation drippers (Fig. 3 (7)), the biopesticide sprayers and
    so on [10]. Besides the AgriTalk-FE GUI, we can design a specific GUI for FE-Talk,
    which is typically a browser that can be shown in a mobile device (Fig. 3 (4)).
    Based on the setups in this GUI, the AgriTalk-FE engine (Fig. 2 (8)) executes
    the FE-talk code (Fig. 2 (9)) to interact with the IoT devices (Fig. 2 (1)). The
    FE-Talk code is automatically created when the device connection configuration
    is complete in the Project window (Fig. 3 (2)), which addresses the automatic
    application generation part of Issue 2. Figure 3. AgriTalk-FE GUI. Show All In
    the device domain, many IoT devices are connected to the MCUs. Programming an
    MCU is a tedious task. The AgriTalk-FE engine automatically generates software
    module codes for the MCU boards including Arduino, Raspberry pi, MediaTek LinkIt,
    ROHM IoT kit and ESP8266 ESP-12F [1]. After we have deployed the standard IoT
    device software into an MCU board, a developer can easily create new applications
    with the AgriTalk-FE GUI (see the Project 1 window in Fig. 4) without the need
    of re-programming the MCU. Specifically, an MCU board (Fig. 4 (1)) can be represented
    by a device icon (Fig. 4 (3)), and its pins can be represented by the feature
    icons within the device icon (e.g., A0 in Fig. 4 (3)). If a device has both inputs
    (to send the data to the AgriTalk-FE engine; see Fig. 4 (6) in Arduino1) and outputs
    (to receive the data from the AgriTalk-FE engine; see Fig. 4 (7) and (8) in Arduino1),
    then the inputs are represented as the feature icons within an “input” device
    icon (Fig. 4 (3)), and the outputs are represented as the feature icons with-in
    an “output” device icon (Fig. 4 (4)). The input device icons are placed on the
    left side of the window, and the output device icons are placed on the right side
    of the window. In Project 1, we also use a second MCU board (Arduion2; see Fig.
    4 (2) and (5)). We build the Arduino network programs for the greenhouse fan control
    (Fig. 4 (7) → (11)) and the energy screen control (Fig. 4 (8) → (12)) as follows:
    We use drag-and-drop operations to draw lines between analog and digital pin icons
    in the AgriTalk-FE GUI, which allows quick agriculture application deployment.
    Furthermore, we can easily re-link sensors to actuators to modify Arduino applications
    without re-burning the MCU. Therefore, our solution has nicely addressed Issue
    1 in the device domain and the application generation part of Issue 2. In Project
    1, both the fan and the energy screen are automatically controlled by the heat
    sensor (Fig. 4 (6)). They are also manually controlled by the switches (Fig. 4
    (9) and (10)). With the pre-built agriculture functions, the low-code no-code
    approach of AgriTalk allows the farmers to create and maintain the fog/edge agriculture
    applications by themselves. Special FE-Talks We have developed several special
    FE-Talks to resolve Issues 2–6 for agriculture fog/edge computing. To address
    the bug detection part of Issue 2, we proposed BigraphTalk to detect any forbidden
    Join connections by using Bigraph, a universal mathematical model for representing
    the spatial configuration of physical or virtual entities and their interactions
    [11]. Our operation experience in commercial farms indicated that the farmers
    at a fog/edge site may inappropriately connect the IoT devices in the GUI. For
    example, the farmer may connect a motor to a temperature and a humidity sensors.
    The temperature and the humidity conditions may conflict with each other, which
    causes the motor to oscillate between “on” and “off” states, and eventually burn
    out. No existing solutions solved this problem automatically until we developed
    BigraphTalk. To check correctness of connections, we use bigraph to specify what
    an invalid configuration of entities looks like and check these against a given
    input model. Bigraph-Talk automates this verification process without requiring
    the developer to specify any bigraph. After static Join connection is proved correct,
    we may verify if the execution of the developed FE-Talk is correct by SimTalk
    [3] through the path (11)-(8)-(9) in Fig. 2. Specifically, after an FE-Talk is
    created, we click the “simulation” toggle button (Fig. 3 (11)). Then the SimTalk
    GUI (Fig. 2 (12)) pops up for specifying the traffic characteristics. Before physical
    sensors/controls are actually connected to the FE-Talk, SimTalk binds the feature
    icons to the simulated software modules corresponding to the real sensors/actuators,
    and set up specific traffic patterns to simulate or emulate the application. The
    simulated software modules are automatically created by SimTalk. The reader is
    referred to Fig. 5 in [3] for more details. We proposed SensorTalk (Fig. 2 (10))
    to address Issue 3 [4], [12]. The farmers often need to replace or calibrate failed/aged
    sensors. Such maintenance is labor-intensive, and the user experience is poor
    in the fog/edge environments. To address this issue, SensorTalk developed a Dash-Board
    output device model (Display-O; Fig. 3 (9)) that shows the real-time sensor values
    in a display (Fig. 3 (10)). DashBoard has a built-in calibration table to correct
    aging sensors [4]. Traditionally, aged sensors were calibrated in the laboratories
    manually or semi-automatically. Conversely, SensorTalk automatically calibrates
    the sensors under test by the standard sensors in fog/edge farming nodes directly.
    The reader is referred to Fig. 2 in [4] for more details. To address Issue 4,
    we developed DataTalk (Fig. 2 (13)) by creating the DataBank device model (with
    the features Data-O in (12) and Data-1 in (13) in Fig. 3). In this way, all AgriTalk-FEs
    can access the AgriTalk DB System (Fig. 2 (4)) and manage the database as an IoT
    device. Data-O of DataBank receives data from the sensors through (6) → (12) in
    Fig. 3. DataBank also receives data from open datasets stored in the AgriTalk
    DB system through (4) → (20) in Fig. 2. The received data may be pre-processed
    and then stored in a DataBank device for future usage. We use the rice blast detection
    [13] as an example to show how DataTalk works. The commercial farm operation indicated
    that image detection of rice blast is not practical (it is too late when you detect
    rice blast in an image). Therefore, we utilized non-image IoT devices to detect
    conditions leading to rice blast. The non-image data are the weather data (including
    the barometric pressure, the temperature and the relative humidity) obtained from
    the database of Central Weather Bureau in Taiwan and the real-time data of the
    AgriTalk sensors from the micro weather stations we established at 4 farm locations.
    We found that high spore germination rate for various fungi may cause rice blast
    disease. Therefore, it is also important to obtain information of spore germination
    rate that is affected by the temperature χ T and the relative humidity χ H . We
    designed a specific spore germination rate function for every farm field computed
    locally at its fog/edge node (an AgriTalk-FE). For example, in a Bao Mountain
    farm, the function is f T ( χ T , χ H )=0.1143×(0.27−0.0078 χ 3 T +0.28 χ 2 T
    + 1.67 χ T ) e 6.6 χ H (2) View Source Figure 4. Arduino MCU mapping for a greenhouse.
    Show All This function is derived from a bio regression model [13] and was implemented
    in a DataBank device through the DataTalk GUI (Fig. 2 (14)). In the rice blast
    AI model, three features – χ T ,  χ H , and f T ( χ T ,  χ H ) - are sent from
    DataBank (Fig. 3 (13)) to Almodule (Fig. 3 (15)). The subtle way we manipulate
    χ T , χ H and f T ( χ T ,  χ H ) significantly improves the accuracy of the rice
    blast prediction, which is 89.4 percent, the world record for non-image sensing
    [13]. Thanks to the “plug-in” module approach of AgriTalk-FE, any FE-Talk can
    be independently developed without any data preprocessing capability, and can
    easily connect to DataTalk for online database access and operation later. The
    rice blast detection was conducted at fourteen counties in Taiwan through fog/edge
    computing, where rice blast detection of a county is conducted by an AgriTalk-FE.
    To address Issue 5, we develop Altalk [6] (Fig. 2 (15)) by creating the Almodule
    device model (with the features Label-O in (14), Feature-O in (15) and Prediction-1
    in (16) in Fig. 3). Like the DataTalk approach, Almodule is manipulated as an
    IoT device. Therefore Altalk can rapidly extend existing IoT applications into
    AI-based smart applications [10]. Upon receiving the data from DataBank, Almodule
    performs feature extraction. The extraction method is selected through the Altalk
    GUI (Fig. 2 (16)), which extracts the data characteristics to form a feature vector.
    The feature vectors as well as the labels obtained from, for example, the remote
    control (Fig. 3 (17)) are used for training. We have ported scikit-learn, Tensor
    Flow, Flux, and other AI tools to Altalk, and the developer can select an appropriate
    tool for AI modeling. Through an appropriate ensemble method, the best prediction
    result is used to activate the actuators. The results are also used to improve
    the accuracy of prediction by conducting validation that provides better hyper-parameter
    setups for Almodule. Through the path (16) → (9) → (10) in Fig. 3, useful statistics
    are displayed for the developer to adjust the machine learning model. Like DataTalk,
    every FE-Talk can be independently developed without AI, and can easily connect
    to Altalk for online AI training and inference later. Figure 5. Maptalk and the
    fog/edge applications. Show All AgriTalk-C: Integration of Cloud and Fog/Edge
    Domains If fog/edge computing involves multiple countries, then Issue 6 (data
    privacy) is a major concern. For example, the current AgriTalk operation includes
    the sites (AgriTalk-FEs) in several countries. These AgriTalk-FEs are managed
    under AgriTalk-C in Chunghwa Telecom''s cloud in Taiwan. Our operation faces the
    problem that the General Data Protection Regulations (GDPRs) are not the same
    for every country. To fit an IoT-FE application to different GDPRs, existing approaches
    require significant efforts to modify the IoT-FE application. To address Issue
    6, the data privacy requirement of an FE-Talk in AgriTalk-FE is set in the Authentication,
    Authorization and Accounting (AAA) subsystem (Fig. 2 (17)) without changing the
    code of the FE-Talk application. Specifically, every IoT message delivered in
    a Join link has a privacy tag to indicate the privacy level of the delivered data.
    A smart application may need to set different privacy levels according to different
    users'' authorization for hiding information in IoT messages. In smart farming,
    a control message sent from a smartphone with a low privacy level cannot trigger
    the irrigation pump. According to the GDPRs of different countries, Personally
    Identifiable Information (PII) and user behavior history should be protected,
    which can be achieved by the AAA subsystem. The details of the tag mechanism are
    out of the scope of this article and the details can be found in [10]. The AgriTalk-C
    server has the same structure as an AgriTalk-FE. All farmer accounts can be managed
    in the AgriTalk-C AAA (Fig. 2 (18)) with single sign-on, where the farmer''s access
    right to a specific AgriTalk-FE is granted through the path (18) → (5) → (8) →
    (17) in Fig. 2. Therefore, the farms in different countries are supported by their
    AgriTalk-FEs with different GDPRs, and these AgriTalk-FEs are monitored by AgriTalk-C
    at the same time. The AgriTalk-C engine controls an AgriTalk-FE through the path
    (5) → (8) in real-time following the same protocol as that for the control path
    to an IoT device ((5) → (1) in Fig. 2). Before an edge/fog node (a new AgriTalk-FE)
    is allowed to join AgriTalk, SimTalk of AgriTalk-C conducts validation and performance
    tests on the AgriTalk-FE through the path (19) → (5) → (8) → (9) in Fig. 2. The
    DataTalk subsystems of individual AgriTalk-FEs (Fig. 2 (13)) and AgriTalk-C (Fig.
    2 (20)) interact with each other indirectly through the database operations on
    the AgriTalk DB System (Fig. 2 (4)). If an AI model is used in an FE-Talk, the
    AI model can be trained in the cloud (AgriTalk-C) and AI inference is performed
    in the edge/fog node (AgriTalk-FE). AgriTalk-C sends model parameters to an AI
    execution engine already installed on the AgriTalk-FEs. The training datasets
    may be generated by the FE-Talk and saved in the AgriTalk DB System. The data
    path for AI training is (4) → (20) → (5) → (21) in Fig. 2. The trained model is
    sent to the AgriTalk-FE through the path (21) → (5) → (8) → (15). Then AI inference
    is performed at the edge/fog node through the path (9) → (8) → (13) → (8) → (15)
    → (8) → (9). Integrated Operations Center (IOC): MapTalk The fog/edge nodes in
    a distributed farming system should be managed in an organized way so that the
    farm owners can conveniently access the applications. In AgriTalk-C, we have developed
    an IOC called MapTalk (Fig. 2 (22) and (23)) based on Google Maps. MapTalk is
    an application FE-Talk. Compared with a traditional IOC [14], the MapTalk user
    can transparently add the IoT devices to the IOC without any programming effort
    while a traditional IOC requires a professional system integrator to modify the
    IOC software. MapTalk shows the location and the status of every farming device
    and allows a farmer to interact with these devices through their cyber representations
    on the map. MapTalk can be displayed in a big screen like traditional IOCs. Furthermore,
    one can also access MapTalk from everywhere through any computing device with
    a browser. The GUIs of all AgriTalk-FEs are managed by MapTalk and can be accessed
    through the “App” pull-down list in the MapTalk GUI, where the layout is illustrated
    in Fig. 5 (1). When the user selects an application item, for example, “CO2” of
    the farms managed as fog/edge nodes (Fig. 5 (2)), the status of every CO2 sensor
    is shown on the map. The IoT devices in MapTalk can be stationary or movable.
    The icon of a stationary IoT device is placed at the map according to its GPS
    coordinates. The stationary examples are “PM2.5” (Fig. 5 (4)), “Germany Power
    Plants” (Fig. 5 (5)) and “Taiwan Farms” (Fig. 5 (6)). The icon of a movable IoT
    device is dynamically placed at its current position of the map. An example is
    the “Bus” application (Fig. 5 (7)) that shows the status of an individual bus
    or all buses. Fig. 5 (8) is “Bus 2” with a pink tail representing its one-hour
    trajectory. The “Bus” application can be transparently reused by AgriTalk to manage
    the cultivators and the tractors in the farms. There are two types of stationary
    applications: map-type and hybrid-type. “PM2.5” and “Germany Power Plants” are
    map-type applications. When we select “Germany Power Plants” from the App list,
    a map for this application pops up with a dedicated function bar to select various
    sensors, such as generated power, wind speed, temperature, and humidity of a power
    plant (Fig. 5 (5)). We are reusing “Germany Power Plants” to implement AgriTalk-FEs
    for agro-photovoltaics applications. “Taiwan Farms” is a hybrid-type application.
    When this application is selected from the App list, all farm icons are shown
    on the map. When we click a farm icon (Fig. 5 (9)), a dialog box (Fig. 5 (10))
    pops up. From this dialog box, we may select specific hyperlinks to see the sensor
    dashboard and the farm control board (including video; see Fig. 5 (11)). Examples
    of dashboard/control board are given in Fig. 3 (10), (18), (21) and Fig. 6. The
    name of the person who maintains this application is also listed (in Chinese;
    see Fig. 5 (12)). Adding an IoT-FE application to MapTalk is achieved through
    the MapTalk project created from the GUI of AgriTalk-C (Fig. 2 (24)), which shows
    the locations of the IoT devices in the digital map (through Display-O in Fig.
    3 (9)). To add a map-type application, we create a “Sensors” device (for example,
    “Germany power plants”; see Fig. 3 (6)) in the MapTalk project, and connect them
    through the path (23) → (22) → (5) → (8) → (9) in Fig. 2. We use the “Routing-with-obstacles”
    application to illustrate how multiple IoT-FE applications (path routing, PM2.5
    and CO2 applications) can interact with each other to create new functions through
    MapTalk. When we select the “Routing” application, MapTalk suggests a driving
    route from the starting point to the endpoint (Fig. 5 (3)). If we select “PM2.5”
    as obstacle, then MapTalk suggests another route to avoid passing through the
    PM2.5 pollution areas (Fig. 5 (4)). We are reusing this routing application to
    implement an AgriTalk-FE that guides automatic tractor movement in irregular-shaped
    farm fields with obstacles. MapTalk was originally designed for managing general
    smart applications. Some of these applications such as “Bus,” “Germany Power Plants”
    and “Routing-with-obstacles” are being reused in AgriTalk-FE without any code
    modification. MapTalk provides a low-code no-code approach to manage the fog/edge
    nodes in an IOC. Pigtalk as a Fog/Edge Computing Example Through the AgriTalk-FE
    platform, we have built more than 20 FE-Talks applications. As a controlling-IoT
    example of fog/edge computing, this section describes PigTalk, an intelligent
    system for piglet crushing mitigation. Figure 6. Pig talk control board accessed
    through maptalk. Show All Many piglets die on pig farms because they are crushed
    when sows roll sideways or lie down. PigTalk was proposed to resolve the piglet
    mortality issue [15], which uses a DataBank device to transform the voice data
    into audio clips. These audio clips serve as the input of the Convolutional Neural
    Network (CNN) model. Through real-time analysis of the voice data collected in
    a farrowing house from a directional microphone (Fig. 6 (1)), PigTalk detects
    if any piglet screaming occurs, and automatically activates sow-alert actuators
    such as heating light (Fig. 6 (2)) for emergency handling of the crushing event
    (light heating effectively forces the sow to stand up). The whole process is remotely
    monitored by the hog farmer through the camera in the farrowing house (Fig. 6
    (3)). The PigTalk configuration can reuse the “Project” configuration in Fig.
    3 where the micro weather station (Fig. 3 (3)) is replaced by the microphone,
    the irrigation system (Fig. 3 (7)) is replaced by the heating light, and the farm
    camera (Fig. 3 (20)) is replaced by the camera in the farrowing house. The raw
    voice data received from the microphone (Sensor-I in Fig. 3 (6)) are sent to DataBank
    (Fig. 3 (12)) to produce the audio clips. These audio clips are further modified
    by min-max scaling in feature extraction. Then the CNN model is used as the machine
    learning algorithm (Fig. 3 (15)). Finally, the k-fold cross validation is conducted
    to validate the model, and the predicted result is used to determine if the heating
    light (Fig. 3 (8)) should be activated. Through fog/edge computing, PigTalk can
    save piglets from being crushed within 0.05 seconds with a 99.93 percent success
    rate. The PigTalk system consists of several farrowing houses, each of which is
    an AgriTalk-FE (called PigTalk-FE). In a PigTalk-FE, the control path for piglet
    crushing avoidance is (6) → (12) → (13) → (15) → (16) → (8) in Fig. 3, where the
    control message is delivered locally through Asymmetric Digital Subscriber Line
    (ADSL) with the delay tA. A PigTalk-FE communicates with the PigTalk AgriTalk-C
    (called PigTalk-C) through the 4G technology. With the AgriTalk-FE GUIs accessed
    from MapTalk of PigTalk-C, we remotely observe pig activities of all farrowing
    houses within the delay tG. The delay tG in the cloud domain is about twice the
    delay t A in the fog/edge domain for other AgriTalk applications. In cloud computing,
    t A = t G . It is clear that t G is much larger than t A , and piglet crushing
    avoidance should be implemented locally as fog/edge computing instead of cloud
    computing. We enhanced the scream detection accuracy of the existing best solutions
    (up to 92.8 percent). With data pre-processing and subtle parameter setups of
    the CNN model, the piglet scream detection accuracy of PigTalk is up to 99.4 percent.
    Through fog/edge computing, PigTalk can save piglets from being crushed within
    0.05 seconds with a 99.93 percent success rate. Conclusion This article proposed
    the fog/edge approach for AgriTalk, an IoT application development platform for
    smart agriculture. By integrating cloud with edge/fog, we described how AgriTalk
    addresses six issues including device domain development, application generation
    and verification, sensor failure detection and calibration, big data management,
    AI provisioning, and data privacy. We showed how AgriTalk integrates its fog/edge
    applications called FE-Talks, and used rice blast detection and piglet crushing
    avoidance as two examples to indicate that fog/edge computing is a much better
    solution than cloud computing, where the delays can be reduced by 50 percent.
    In our solution, the non-agriculture applications in MapTalk can be transparently
    translated into smart agriculture applications. The examples include the cultivators
    and the tractors management, and agro-photovoltaics applications. Through the
    low-code no-code approach, AgriTalk and MapTalk allow the farmers to create and
    maintain FE-Talks by themselves in the fog/edge computing environment. ACKNOWLEDGMENT
    This work was supported in part by the National Science and Technology Council
    (NSTC) 112-2221-E-033-023, 112-2221-E-468 -005 -MY2, 111-2221-E-468 -012, 110-2622-8-A49-022,
    NSTC112-2221-E-A49-049, NCKU Miin Wu School of Computing, Research Center for
    Information Technology Innovation, Academia Sinica. Authors Figures References
    Citations Keywords Metrics More Like This Artificial Intelligence and Internet
    of Things for Sustainable Farming and Smart Agriculture IEEE Access Published:
    2023 Analysis of Internet of Things based Artificial Intelligence in Agriculture
    Fertilizer Process Management 2023 2nd International Conference on Automation,
    Computing and Renewable Systems (ICACRS) Published: 2023 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Communications Magazine
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Moving from Cloud to Fog/Edge: The Smart Agriculture Experience'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sharma M.K.
  - Mehta R.
  - Shekhawat R.S.
  citation_count: '1'
  description: We are living in the age of fast-evolving technology and it is significantly
    influencing our society, work culture, etc. Now we are witnessing 5G technology
    for fast and reliable internet. At the same time, we are increasing the use of
    IoT in our daily use. As we know, IoT enhances the communication among different
    digital assets (i.e., sensors, devices) and helps to capture required data and
    transmit it to fog or cloud. However, IoT devices generate a volume of data at
    the end-user level and it needs to be processed in a short span of time on the
    cloud. However, collection of data and transmission to the cloudbased processing
    is not as efficient as needed. Sending the data to the cloud for processing has
    a lot of overhead which degrades the quality of service and leaves a negative
    impact on IoT applications and their network performance. To eliminate such negative
    influence different cloud computing techniques (i.e., edge computing, fog computing,
    etc.) are in practice. Edge computing is playing a crucial role with IoT paradigms,
    especially in local networks, where we have to take decisions instantly instead
    of sending information to centralized systems for decision-making. Edge computing
    is a perfect example of a processing system. Edge computing is very useful in
    such applications where instant or local decision-making can enhance the productivity
    and quality of service (i.e., supply chain management, agriculture, resource utilization
    decisions, energy consumption, etc.). In the chapter, we are presenting a comprehensive
    exploration of IoT applications with edge computing and their architectures.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Recent Trends and Best Practices in Industry 4.0
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: IoT enabled by edge computing for telecomms and industry
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sajid J.
  - Hayawi K.
  - Malik A.W.
  - Anwar Z.
  - Trabelsi Z.
  citation_count: '2'
  description: Precision agriculture and smart farming have received significant attention
    due to the advancements made in remote sensing technology to support agricultural
    efficiency. In large-scale agriculture, the role of unmanned aerial vehicles (UAVs)
    has increased in remote monitoring and collecting farm data at regular intervals.
    However, due to an open environment, UAVs can be hacked to malfunction and report
    false data. Due to limited battery life and flight times requiring frequent recharging,
    a compromised UAV wastes precious energy when performing unnecessary functions.
    Furthermore, it impacts other UAVs competing for charging times at the station,
    thus disrupting the entire data collection mechanism. In this paper, a fog computing-based
    smart farming framework is proposed that utilizes UAVs to gather data from IoT
    sensors deployed in farms and offloads it at fog sites deployed at the network
    edge. The framework adopts the concept of a charging token, where upon completing
    a trip, UAVs receive tokens from the fog node. These tokens can later be redeemed
    to charge the UAVs for their subsequent trips. An intrusion detection system is
    deployed at the fog nodes that utilize machine learning models to classify UAV
    behavior as malicious or benign. In the case of malicious classification, the
    fog node reduces the tokens, resulting in the UAV not being able to charge fully
    for the duration of the trip. Thus, such UAVs are automatically eliminated from
    the UAV pool. The results show a 99.7% accuracy in detecting intrusions. Moreover,
    due to token-based elimination, the system is able to conserve energy. The evaluation
    of CPU and memory usage benchmarks indicates that the system is capable of efficiently
    collecting smart-farm data, even in the presence of attacks.
  doi: 10.3390/app13063857
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Applied Sciences All Article Types
    Advanced   Journals Applied Sciences Volume 13 Issue 6 10.3390/app13063857 Submit
    to this Journal Review for this Journal Propose a Special Issue Article Menu Academic
    Editors Romano Lottering Kabir Peerbhay Samuel Adelabu Subscribe SciFeed Recommended
    Articles Related Info Link More by Authors Links Article Views 1851 Citations
    2 Table of Contents Abstract Introduction Related Work System Model Methodology
    Evaluation Discussion Conclusions Author Contributions Funding Institutional Review
    Board Statement Informed Consent Statement Data Availability Statement Conflicts
    of Interest References share Share announcement Help format_quote Cite question_answer
    Discuss in SciProfiles thumb_up Endorse textsms Comment first_page settings Order
    Article Reprints Open AccessArticle A Fog Computing Framework for Intrusion Detection
    of Energy-Based Attacks on UAV-Assisted Smart Farming by Junaid Sajid 1, Kadhim
    Hayawi 2, Asad Waqar Malik 1,*, Zahid Anwar 3,* and Zouheir Trabelsi 4 1 School
    of Electrical Engineering and Computer Science (SEECS), National University of
    Sciences and Technology (NUST), Islamabad 44000, Pakistan 2 College of Interdisciplinary
    Studies, Computational Systems, Zayed University, Abu Dhabi P.O. Box 144534, United
    Arab Emirates 3 Department of Computer Science, The Sheila and Robert Challey
    Institute for Global Innovation and Growth, North Dakota State University, Fargo,
    ND 58105, USA 4 College of Information Technology, United Arab Emirates University,
    Abu Dhabi P.O. Box 15551, United Arab Emirates * Authors to whom correspondence
    should be addressed. Appl. Sci. 2023, 13(6), 3857; https://doi.org/10.3390/app13063857
    Submission received: 1 February 2023 / Revised: 7 March 2023 / Accepted: 8 March
    2023 / Published: 17 March 2023 (This article belongs to the Special Issue Remote
    Sensing Applications in Agricultural, Earth and Environmental Sciences) Download
    keyboard_arrow_down     Browse Figures Versions Notes Abstract Precision agriculture
    and smart farming have received significant attention due to the advancements
    made in remote sensing technology to support agricultural efficiency. In large-scale
    agriculture, the role of unmanned aerial vehicles (UAVs) has increased in remote
    monitoring and collecting farm data at regular intervals. However, due to an open
    environment, UAVs can be hacked to malfunction and report false data. Due to limited
    battery life and flight times requiring frequent recharging, a compromised UAV
    wastes precious energy when performing unnecessary functions. Furthermore, it
    impacts other UAVs competing for charging times at the station, thus disrupting
    the entire data collection mechanism. In this paper, a fog computing-based smart
    farming framework is proposed that utilizes UAVs to gather data from IoT sensors
    deployed in farms and offloads it at fog sites deployed at the network edge. The
    framework adopts the concept of a charging token, where upon completing a trip,
    UAVs receive tokens from the fog node. These tokens can later be redeemed to charge
    the UAVs for their subsequent trips. An intrusion detection system is deployed
    at the fog nodes that utilize machine learning models to classify UAV behavior
    as malicious or benign. In the case of malicious classification, the fog node
    reduces the tokens, resulting in the UAV not being able to charge fully for the
    duration of the trip. Thus, such UAVs are automatically eliminated from the UAV
    pool. The results show a 99.7% accuracy in detecting intrusions. Moreover, due
    to token-based elimination, the system is able to conserve energy. The evaluation
    of CPU and memory usage benchmarks indicates that the system is capable of efficiently
    collecting smart-farm data, even in the presence of attacks. Keywords: precision
    agriculture; unmanned aerial vehicles; smart farming; intrusion detection; fog
    nodes 1. Introduction With rapid advancements being made in remote sensing technology,
    the role of the Internet of Things (IoT) is playing a significant role in precision
    agriculture. Many techniques are being used to collect and analyze crop data to
    improve farm productivity and maximize revenue [1]. In most cases, agriculture
    information is generated with the help of deployed sensors, collected through
    UAVs. The role of UAVs in data gathering and field monitoring has increased. The
    affordability and simplicity of operating UAVs for precision agriculture are significant
    factors in their adoption [2]. In smart farming, wireless sensors are deployed
    to obtain field information to increase productivity and convenience [3]. Sensors
    are readily available at relatively low costs and can be used to develop applications
    to facilitate production management, crop security, irrigation control, and scheduling
    [4]. However, UAV integration with smart farming creates new security challenges
    [5]. As UAVs navigate in an open environment, they are increasingly susceptible
    to attacks that can cause them to be misguided, disrupted, or even physically
    taken over. Due to the omnipresence of UAV technology, several issues with UAV
    networks need to be taken into account, including communication, data collection,
    data security, storage, and supervision [6]. In precision agriculture, several
    UAVs work together to tackle the task of large-scale data gathering [7]. Due to
    the wireless nature of devices, it is important to consider the security aspect
    of the entire system before deployment. In many cases, the deployed devices are
    interconnected through short-range communication and are also able to connect
    through the internet [8]. A consequence of this increased connectivity is that
    such platforms become susceptible to vulnerabilities associated with GPS spoofing,
    GPS jamming, radio frequency interference, malware infiltration, man-in-the-middle,
    denial of service [9], denial of sleep [10], message replay and data feed interception
    [11]. If successfully exploited, these vulnerabilities can cause these systems
    to become a target of cyberattacks, where an intruder can penetrate and take control
    of the UAVs, interrupt activities, or modify the data gathered [12]. Besides vulnerabilities
    in wireless connectivity, UAVs are also susceptible to physical attacks whereby
    the attacker is physically adjacent and uses the close proximity to take control
    of the devices [13]. In the context of UAV-assisted precision agriculture where
    UAVs assist in a variety of tasks across several farms as illustrated in Figure
    1, such attacks can have devastating impact. This impact includes but is not limited
    to completely damaging cropland, flooding of farm fields, and malicious spraying
    of pesticides, which might result in a significant consumption of chemicals [14].
    These threats are classified as agricultural terrorism. Figure 1. UAV-assisted
    precision agriculture scenario. The Federal Bureau of Investigation (FBI) is alerting
    the agricultural sector that attackers may target farms more frequently during
    crucial planting and harvesting seasons and interrupt business activities, which
    would have a detrimental effect on the food supply chain. In a recently released
    alert, the FBI warned that ransomware attacks (https://www.beefmagazine.com/news/fbi-warns-cyberattacks-during-critical-ag-seasons,
    accessed on 14 December 2022) on six agricultural organizations during the autumn
    2021 harvest and well as two strikes in early 2022 might affect the planting season
    due to the sabotage of seeds and fertilizer delivery. One of the largest meat
    processing companies, JBS, spent about USD 11 million in ransom to end a cyberattack
    last year (https://www.bbc.com/news/science-environment-61336659, accessed on
    14 December 2022). A leading US agricultural company, AGCO, was the target of
    a ransomware attack in May 2022 that majorly impacted their output. A consortium
    of authorized governmental cyber security experts from the United Kingdom, United
    States, and Australia issued a warning in April alerting that supply chains, a
    crucial component of the Western national infrastructure, might be targeted by
    state-sponsored Russian attackers. The cybersecurity of technology that enables
    precision agriculture is a significant obstacle to its widespread adoption. To
    address this challenge, it is essential to develop a comprehensive framework that
    enhances the security of precision agriculture [15]. This research focuses on
    a subset of attacks that target the disruption of UAV data communication through
    techniques such as distributed denial-of-service (DDoS), unauthorized access,
    brute force, and infiltration. Compromised UAVs can significantly reduce the efficiency
    of the data collection system by reporting false data and consuming excessive
    energy for flight and hovering activities. They also compete for charge times
    on shared charging stations, thereby negatively impacting uncompromised UAVs.
    This work proposes a smart farming framework that allows securing the use of UAVs
    for data collection through deployed sensors. Comprehensive models for sensors
    and UAV energy consumption and threat vectors have been developed. Furthermore,
    algorithms have been developed for farm data transmission and collection utilizing
    a fog computing architecture. A fog broker, a key central element that manages
    interactions between the UAVs and sensors, is utilized for deploying an intrusion
    detection system (IDS). The IDS that utilizes machine learning classification
    is developed to detect and flag compromised UAVs based on their behaviors. Flagged
    UAVs are then penalized through a coin-based system where the greater number of
    coins collected allows for a greater amount of charge. UAVs lose coins and ultimately
    charge proportionally to the degree of malicious behavior to minimize the level
    of disruption to the overall system. The main contributions of the proposed work
    are listed as follows: A novel toolkit was developed that supports UAV-based data
    collection mechanisms in smart farming. The data were collected from sensors deployed
    in the fields. A fog broker was used to manage all of the responsibilities and
    interactions of the UAVs, sensors, data transmission, and data collection. A machine
    learning model was trained at the edge station based on the UAV-to-UAV communication
    logged and shared after every round. The trained model was then deployed to the
    UAVs and the model outcomes were shared with the fog node for identifying malicious
    behavior. A coin-based recharge system was proposed to prune malicious UAVs. The
    UAV charging was based on the coins it received in lieu of the activities it performed,
    such as data transmission and recording during data gathering. UAVs exhibiting
    suspicious behaviors received fewer coins in that cycle. UAVs consistently exhibiting
    this behavior in multiple cycles were automatically removed from the system because
    of a lack of coins and, thereby, charge. The innovation of this study lies in
    a framework that combines UAVs, IoT devices, and an IDS to enhance data collection
    in smart farming. Machine learning algorithms are used to detect and prevent attacks,
    and UAVs and IoT devices enable efficient and timely data collection. The IDS
    component addresses potential intrusion threats, and the XGBoost algorithm provides
    the best results for intrusion detection accuracy. The proposed framework has
    the potential to advance smart farming technology, benefiting the agriculture
    industry and society. The rest of the article is organized as follows: Section
    2 presents the related work. Section 3 covers the system model. The proposed methodology
    is presented in Section 4. The evaluations and results are given in Section 5,
    and the discussion and conclusion are provided in Section 6 and Section 7, respectively.
    2. Related Work We are interested in related research that considers security
    issues in precision agriculture and smart farming assisted by UAVs. UAV performances
    can be negatively impacted by malicious attacks because UAVs collect sensitive
    data that intruders may attempt to intercept in data transmission. Intrusion detection
    systems can help detect cyber attacks on UAVs and the data collected. We found
    relatively limited research available in the use of machine learning techniques
    for intrusion detection in UAV-assisted precision agriculture. However, we did
    find related work in the general areas of smart farming, UAV-assisted smart farming,
    and intrusion detection systems that benefited our research. This section summarizes
    the latest research works in these areas and highlights the research gaps. 2.1.
    Smart Farming Smart farming is described as a farming system in which innovative
    and cutting-edge technologies are used with conventional farming practices to
    increase farmland production quantity and quality while dramatically reducing
    manufacturing costs [16]. Smart farming and traditional farming are completely
    different from one another. Traditional farming practices include employing outdated
    equipment for labor and cultivating seasonal crops without first determining market
    demands and prices, or taking into account weather data from the weather service,
    among other things. Technological advances are used in smart farming, including
    smart devices, IoT sensors, cloud/fog computing, UAV data collection, and periodic
    assessments of various aspects. This makes farming simple and inexpensive with
    minimum labor costs. and results in improved crop yields and increased productivity
    [17]. The primary emphasis is on the integrated and coordinated implementation
    of new technologies into smart farms while offering sophisticated agriculture
    management that draws on collective expertise and judgment. Some noteworthy attempts
    include the Soil Scout [18] and Thoreau [19] projects, which monitor soil properties
    and farming conditions [20] using wireless sensor nodes. While traditional farming
    devices are designed to have a strong connection and low power utilization, they
    are inadequate for carrying out sophisticated operations. IoT devices, in contrast,
    are linked and have the potential to offload operations to the cloud or spread
    them across several devices. The advantages of multiple farms linked with sensors
    and actuators connected across an IoT access point that deliver smart agricultural
    systems for end users are explored in [21]. The IoT devices are also linked to
    a server that keeps track of all the interconnected farms. IoT gateway-based surveillance
    software in [22] evaluates the leaf area index. Since the Internet of Things-based
    data analytics operates with a range of sensors, solutions can only accommodate
    homogeneous sensors. Despite these efforts, a significant obstacle to widespread
    farmer adoption of such networked solutions remains, which is the availability
    of configurable hardware that can be programmed for customized data collection.
    A system employing open hardware to allow the creation of a smart farming framework
    is presented in [23] to address this difficulty. Furthermore, a proposed methodology
    in [24] introduces a smartphone-based smart agricultural system. Users of the
    system can operate the installed sensor modules or gather and evaluate agricultural
    data from the environment. The overlying infrastructure is adaptable and enables
    dispersed service deployment. These frameworks allow for the creation of reproducible
    IoT-based farming applications. 2.2. UAV-Assisted Smart Farming Smart farming
    has become a reality with the growth of the IoT and unmanned aerial vehicles.
    IoT increases the value of gathered data through perceptual computation, automated
    data collection, and access by facilitating data flow between various sensors
    and other IoT devices. As a result, smart farms may employ productivity and managerial
    methods that are more timely and affordable [25]. Agricultural UAVs are notable
    practical developments in smart farming and are frequently employed by farmers
    [26,27] for regulating and supervising farming operations. Several UAVs are often
    intended to effectively pour water and various pesticides into territory where
    personal mobility is difficult and the farms have varying altitudes. Recognizing
    the importance of this, the Massachusetts Institute of Technology designated UAVs
    as a green-tech tool for smart farming in 2014 [28]. Clusters of UAVs with diverse
    sensors and 3D cameras can cooperate with recent developments in swarm technologies,
    including mission-based administration, to provide farmers with a complete suite
    of soil management tools. UAVs designed for providing farming assistance are making
    it feasible for farmers to easily capture a bird’s eye perspective of their fields
    in order to maintain and govern the farms properly. This dramatically lowers operating
    time, leading to greater steadiness in farm production, as well as precision.
    Various applications have helped several aspects of agriculture [26,28], including
    searching and applying fertilizer and pesticides, finding and eliminating weeds,
    sowing seeds, determining productivity and mapping out the land. Researchers have
    proposed a system [29] that utilizes UAV pictures to identify weeds prematurely
    in an Australian chili crop. The yield elevation of maize and sorghum crops in
    a farming field was also measured using UAV pictures [30]. Researchers have also
    [31] suggested a unique approach to taking UAV photos of farming fields and rebuilding
    three-dimensional images to observe the growth characteristics of the crops. UAVs
    are already being equipped with smart devices to conduct a variety of tasks in
    smart farming, including monitoring field conditions [32], collecting meteorological
    data, including temperature, humidity, wind speed, and air movement, among others
    [33], and improving crop yield [28]. In order to demonstrate real-time measurement
    impacting the amount and quality of grape yield, researchers have linked wireless
    sensor networks with a smart UAV system [34]. A UAV equipped with smart aerial
    sensors was created by Hernandez et al. [35] to record the grain volume within
    a trailer while forage is being harvested. Shamshiri et al. [36] have demonstrated
    the use of UAVs in solving a variety of challenges relating to palm oil plantation,
    including yield prediction, disease detection, and pest monitoring. Despite all
    of these UAV developments, numerous problems remain that must be addressed for
    improved deployment, including energy drain of UAVs, security hardening against
    cyber attacks, managing long communication distances, and carrying payloads [27,37].
    For instance, since energy is a limited resource for UAVs, Islam et al. have concentrated
    on lowering energy usage in UAVs [38] but security against cyber attacks to prevent
    intrusion is still a critical challenge. 2.3. Intrusion Detection Systems Network
    IDS: Machine learning-based approaches and, in particular, deep learning (DL)
    methods [39,40,41,42], are being used to produce cybersecurity solutions for securing
    communication for the Internet of Things and cyber-physical systems in the face
    of intrusions. To detect GPS spoofing, Manesh et al. [43] used supervised learning
    and artificial neural networks and Min et al. [44] developed a semi-supervised
    and unsupervised framework for intrusion detection. To increase the accuracy of
    classification, essential characteristics, including Doppler shift and SNRratio,
    have been chosen using feature engineering on the well-known KDD Cup 99 dataset
    and its variations [45,46]. To give an IDS improved generalization capabilities,
    Wang et al. [45] combined group convolution networks with snapshot ensemble learning.
    In another work, an IDS for UAV networks was created by Wang et al. [47] using
    the LSTM recurrent neural network. A further advanced strategy that combines XGBoost
    and DNN was proposed by Devan et al. [46]. While the deep neural network is used
    to create the classification, XGBoost is useful for extracting features and dimensionality
    reductions. A hybrid optimization-driven ensemble classification was created in
    [48] using a fog computing environment that combines multiple individual classifiers
    in order to improve overall prediction accuracy. A hybrid approach proposed in
    [49] combines association rule mining and classification methods to enhance the
    privacy and security levels of the network environment and improve the accuracy
    of intrusion detection. The aim of the study in [50] is to examine and assess
    intrusion detection systems designed for Agriculture 4.0 cybersecurity. The focus
    is on discussing the cybersecurity threats that these systems face and the metrics
    used to evaluate their performance. UAV IDS: In critical situations, UAV systems
    are frequently utilized to communicate crucial data. The limits of the UAV infrastructure’s
    computing and communication capacities, meanwhile, make them vulnerable to intrusions
    and attacks. Intrusion detection systems for UAVs or UAV IDS are often designed
    to recognize a variety of anomalies and vulnerability types, including malware
    such as ransomware, signals, route modification, tracking attacks, and GPS spoofing
    [51] in UAV systems. Bithas et al. [52] conducted a thorough study of machine
    learning algorithms for dealing with different UAV challenges, including channel
    modeling and resources [53], and employed LSTM and convolutional neural network
    techniques to create spatial–temporal deep learning on communication graphs. Abu
    et al. [41] studied an unsupervised learning-based technique to detect persistent
    spoofing in UAV interconnected networks. In contrasting to typical machine learning
    frameworks, the use of deep learning has greatly improved the detection accuracy
    for recently created intrusions that are challenging to identify using conventional
    machine learning methods [54]. Automated hyperparameter optimization (HPO) has
    grown in popularity in both research and commercial applications as a way to remove
    the impediments for average consumers [55]. Intrusion detection based on hyperparameter
    optimization has been previously investigated [56] and has shown to be very promising.
    The research in [57] discusses an agricultural information security framework
    that incorporates UAV technology and machine learning for data collection. The
    integration of these technologies promotes the development of the Internet and
    information security. The paper also proposes the use of a Double Deep Q-network
    algorithm to efficiently optimize the deployment of UAVs and a smart agricultural
    information management system for intrusion detection. We extended traditional
    hyperparameter optimization methods and used an advanced hyperparameter optimization
    technique for intrusion detection in this research. 2.4. Summary of Existing Literature
    Based on a thorough examination of the literature, Table 1 compares the features
    of the most relevant research works to this proposed effort focusing on the mathematical
    modeling of smart farming systems as well as intrusion detection. It may be noted
    from the table that several studies address sensor-based smart farming frameworks,
    but most frameworks lack UAV integration to collect farming data. Similarly, there
    is limited research on intrusion detection in smart farming systems. The proposed
    framework is novel in that it combines aspects of smart farming, UAV based data
    collection, as well as proposes an advanced hyperparameter optimization-based
    intrusion detection system. Table 1. Comparison of Related Work in Precision Agriculture
    and Intrusion Detection. 3. System Model Consider an agricultural farm denoted
    as ℱ having a width (w) and a length (l). The farms are sub-labeled as 𝑓 1 , 𝑓
    2 … 𝑓 𝑚 ∈ ℱ , where m represents the total number of farms. There exist a total
    of n sensors 𝒮 deployed to obtain field data. The framework can accommodate diversified
    sensors with different transmission times ( 𝑇 𝑠 ) and energy requirements ( 𝐸
    𝑠 ). The sensor location is defined as 𝑆 𝑥𝑦 . The sensors also serve as gateways,
    forwarding data across the farm in addition to generating their own data. The
    UAVs are used for data collection from sensors and offloading to the back-end
    server for further analysis and decision-making. Each UAV has an energy denoted
    as ( 𝐸 𝑢 ) and a communication range ( 𝑅 𝑢 ). 3.1. Ground Sensors Energy Model
    It is assumed that 𝒮 has two states namely active and passive. 𝐸 𝑠𝑙 and 𝐸 𝑎𝑐 are
    the energies consumed in the passive and active states, respectively. Therefore,
    the total energy consumed 𝐸 𝑡 is given in Equation (1) [64]: 𝐸 𝑡 = 𝐸 𝑠𝑙 + 𝐸 𝑎𝑐
    (1) 𝐸 𝑠𝑙 = 𝑃 𝑠𝑙 × 𝑇 𝑠𝑙 (2) Here, 𝑃 𝑠𝑙 is the power usage, and 𝑇 𝑠𝑙 is the amount
    of time spent in sleep mode. Likewise, the total energy used when in the active
    state 𝐸 𝑎𝑐 is computed in Equation (3). 𝐸 𝑎𝑐 = 𝐸 𝑚 + 𝐸 𝑑 + 𝐸 𝑝 + 𝐸 𝑡 + 𝐸 𝑟 (3)
    In this case, 𝐸 𝑚 represents the energy consumed due to mode change, and 𝐸 𝑑 represents
    the energy consumed during data collection. 𝐸 𝑝 represents data processing energy
    and 𝐸 𝑡 and 𝐸 𝑟 represent the data transmitting and receiving energy, respectively.
    3.2. UAVs Energy Consumption Model The model supports two basic modes of operation
    for UAVs, i.e., flying and hovering. The power consumption in the flying mode
    is represented as 𝑃 𝜂 , and that consumed in the hover mode as 𝑃 𝜁 . Here, it
    is assumed that the UAV’s flight paths are uniform, requiring no modification
    in terms of acceleration and deceleration. It is further assumed that rotary wing
    UAVs are used in this scenario, flying at a speed V. Thus, the energy consumption
    of the UAVs during flying, represented as 𝐸 𝜂 is defined as: 𝐸 𝜂 = 𝑃 𝜂 𝑉 (4) Here,
    the V is the speed of the UAV and is defined as: 𝑃 𝜂 = 𝑃 1 (1+ 𝑃 2 𝑉 2 )+ 𝑃 3
    ⎛ ⎝ ⎜ ⎜ ⎜ 1+ 𝑉 4 𝑃 2 4 − − − − − − √ − 𝑉 2 𝑃 4 ⎞ ⎠ ⎟ ⎟ ⎟ 1/2 + 𝑃 5 𝑉 2 (5) where
    𝑃 𝑖 , i = 1, …, 5, are the parameters for the energy model specified in [65] and
    𝑃 1 (1+ 𝑃 2 𝑉 2 ) is the blade profile power. 𝑃 3 ( 1+ 𝑉 4 𝑃 2 4 − − − − − − √
    − 𝑉 2 𝑃 4 ) 1/2 is the induced power, and 𝑃 5 𝑉 2 is the parasite power. The parasite
    power is the element needed to counter the parasite friction drag caused by the
    aircraft flying through the air, and the induced power is the element needed to
    counteract the induced drag produced during lift force to keep the aircraft in
    the air. The power consumption for rotary-wing UAVs is provided by the finite
    value 𝑃 1 + 𝑃 3 for the exceptional case when the speed of the UAV is zero. Such
    power use is consistent with the rotary-wing UAV hovering in one place. However,
    if the UAV’s flight speed is not zero, then parasite power emerges. While the
    induced power reduces with UAV speed V, both the blade profile power and the parasite
    power increase with UAV speed. Hovering: The UAV interacts with each sensor only
    when it is hovering at one of the optimum hovering locations. The energy consumption
    when UAVs hover is represented by 𝐸 𝜁 and is defined as: 𝐸 𝜁 =( 𝑃 ℎ + 𝑃 𝑐 ) 𝑇
    𝜁 (6) where 𝑃 ℎ is the power consumed in the hovering, 𝑃 𝑐 is the power consumed
    by the UAV communicating with the ground node and 𝑇 𝜁 is the time of hovering.
    These parameters are expressed in [65]. 3.3. UAV Data Collection Notably, data
    are gathered from the deployed sensor by a number of N UAVs. It is considered
    that the UAVs and deployed sensors communicate via uplink-based orthogonal frequency
    division multiple access (OFDMA). K continuous links are supported by the UAVs
    for data collection. Additionally, we provide a distributed system where UAVs
    share data with surrounding fog sites. Since the inserted sensors may be portable,
    there may be sporadic communication between the sensors and UAVs with significant
    packet loss. Therefore, the line of sight (LoS) must be maintained for communication
    to be successful. The LoS probability is calculated as in [64]. 𝑝 𝐿𝑜𝑆 = 1 1+𝑜×𝑒𝑥𝑝(𝛾[𝜓−𝜈])
    (7) where 𝜓 is the elevation angle between the sensor 𝑆 𝑥𝑦 and the UAV 𝑈 𝑥𝑦 ,
    and o and 𝛾 are constants and are determined by the communication frequency and
    range. The probability increases with increasing UAV height; therefore, deployed
    sensors may only be allocated to the 𝑗 𝑡ℎ UAV if the chance of LoS is near to
    1. Consequently, the condition of the connection between the sensor and the UAV
    is: 𝑑 𝑖𝑗 = 𝑎 𝑗 𝑠𝑖𝑛( 𝑝 𝐿𝑜𝑆 ) (8) where 𝑑 𝑖𝑗 is the distance between the 𝑖 𝑡ℎ ground
    node and the 𝑗 𝑡ℎ UAV and 𝑎 𝑗 is the height of the UAV from the ground node. Considering
    there are a × b sensors spread out over the farm, the coverage time 𝑡 𝑐𝑜𝑣 of the
    UAV is calculated as: 𝑡 𝑐𝑜𝑣 = Σ 𝑎 Σ 𝑏 𝑇 𝑎𝑏 + 2 𝑁 Σ 𝑖 Σ 𝑗 𝑆 𝑖𝑗 (9) where 𝑇 𝑎𝑏 is
    the time required for the UAVs to travel from one ground node to the other node,
    and N is the total number of UAVs that were utilized to collect the data. We employed
    a set of brokers B to control the fog sites to analyze the collected data. A fog
    broker was in charge of the effective use of fog services near the target consumers.
    The broker distributed the resources to other brokers on the site. This supported
    latency-sensitive IoT applications by reducing the communication time. 𝑁 𝑑 = 𝑑
    𝑚 + 𝑑 𝑛 , where 𝑑 𝑚 is the network delay between the sensors and UAVs, and 𝑑 𝑛
    is the network delay between the UAV and the designated fog site and is dependent
    upon the distance between ground nodes. The network cost C, on the other hand,
    is a linear function of distance and is given as: 𝐶=𝜗( 𝑑 𝑚,𝑛 + Σ 𝑘∈𝐵&𝑛≠𝑜 𝑑 𝑛,𝑜
    ) (10) where 𝜗 is constant, 𝑑 𝑚,𝑛 is the distance between the 𝑚 𝑡ℎ UAV and its
    local broker n and 𝑑 𝑛,𝑜 is the distance of the local broker renting the computation
    resource 𝑛∈𝐵 . 3.4. Threat Model In order to detect cyberattacks, the proposed
    intrusion detection system is explained in Section 4.4.2 and the attacker goals
    and capabilities are detailed here. The attacker’s primary goal is to hamper the
    data collection process so that the farmer has an erroneous or inaccurate picture
    of the state of the farm. An example of this is that the UAVs report readings
    that cause the system to indicate that soil moisture is sufficient, while in reality
    it is dry. To accomplish this, the attacker locates UAVs, gains access to the
    control system, and tampers with the data communication. Compromised UAVs end
    up flying over the farmland needlessly consuming energy, providing incorrect data
    to the fog broker, and stealing valuable charging time from other legitimate UAVs
    at the charging station. The attacker achieves these goals by executing one or
    more of the following attacks: Distributed denial of service attack: In a DDoS
    attack, the attacker attempts to hamper the UAV’s data collection process by sending
    a large number of messages to needlessly engage the UAV in processing useless
    packets. As a result, either the UAV is unable to communicate with neighboring
    UAVs or it suffers delays in UAV-to-UAV and UAV-to-sensor communication. Such
    attacks consume UAV energy significantly. Heart bleed: In this attack, the attacker
    attempts to gain unauthorized access to the UAVs by scanning the UAV-to-UAV communication
    to identify vulnerabilities. For scanning, the attacker can use specialized devices
    installed nearby; moreover, the UAVs can also be used for initiating the scans.
    After compromising a UAV, the attacker injects malware and fake data into other
    connected UAVs; thus, trying to disrupt the data collection process. Brute force:
    In this attack, the attacker attempts to gain unauthorized access to the fog portal
    through UAVs. It is assumed that the admin portal is accessible by UAVs through
    a wireless link after providing the appropriate credentials. This interface is
    normally used by the administrator from the fog node to connect to multiple UAVs
    and update software, configure data collection routes, and perform other administrative
    tasks. After compromising a UAV the attacker injects malware and fake data into
    other connected UAVs; thus, trying to disrupt the data collection process. Infiltration
    attack: The goal of the infiltration attack is to compromise the UAV network and
    gain control over the UAVs themselves. Once access is achieved, the UAVs allow
    for passive reconnaissance, allowing the attacker to gather information about
    the network and its devices. This information can be used to plan further attacks
    by identifying and exploiting vulnerabilities that can be used for privilege escalation.
    The CICIDS2017 dataset [66] closely reflects actual real-world data on the following
    attacks: brute force FTP, brute force SSH, dos, ddos, heart bleed, web attacks,
    infiltration attacks, and botnets. Here, the observations pertinent to ddos, heart
    bleed, brute force, and infiltration attacks are used. Additionally, this dataset
    contains network traffic evaluation performed through CIC FlowMeter, which involved
    processes labeled according to the timestamp, source, and destination IP addresses,
    source, and destination port numbers, protocol, and attacks. The dataset is split
    into training and testing. The training models are deployed on UAVs in a simulation
    framework to efficiently identify the attacks. Further, the dataset is integrated
    into the simulation framework in the form of an attack module so that as the simulation
    progresses the data collected by the UAVs is affected and the proposed intrusion
    detection system attempts to detect this behavior and weed out the compromised
    UAVs. 4. Methodology The proposed system consists of farms, unmanned aerial vehicles,
    deployed sensors, fog brokers, and fog nodes, as shown in Figure 2. The framework
    allows the simulation of large-scale farms with sensors deployed to monitor the
    farms, and UAVs are used to gather information periodically. The information is
    offloaded at the connected fog node through the fog broker. Moreover, the system
    detects malicious UAV behavior through a machine-learning model deployed at the
    UAVs. The main modules are elaborated on below. Figure 2. Fog-based layered architecture.
    4.1. Smart Farms The framework allows the user to define farms, deploy sensors
    and configure the data collection process through UAVs. Here, we assumed that
    the farms are large-scale and comprise flat and mountainous terrain. Therefore,
    due to the uneven terrain, ground-to-ground communication is not a suitable option
    [67]. Therefore, data collection through UAVs are more suitable for uneven farming
    lands. 4.2. Broker The broker functionality is shown in Figure 3. A broker is
    a static node placed near a fog site to manage the site’s fog resources as well
    as control management aspects of the UAVs such as their scheduling. The broker
    node also collaborates with other brokers for UAV sharing. Each fog node owns
    a few UAVs but to cover the large-scale area, they may need additional resources
    from nearby fog sites. Therefore, a collaborative broker-based design is proposed.
    The broker can lease resources from neighboring fog sides which are later settled
    through a bartering mechanism. The brokers maintain a history of the resources
    leased and allocated. Figure 3. Detailed Architecture Diagram of Proposed Framework.
    4.3. Sensors The proposed framework facilitates the deployment of both stationary
    and mobile sensors to collect data to increase agricultural production. The transmission,
    storage, and battery life of the sensors vary and the framework provisions the
    behavioral simulation of heterogeneous sensors. Additionally, the sensors facilitate
    the ad hoc method of information transmission to nearby gateway nodes. The architecture
    of a typical sensor node is shown in Figure 3. In the proposed work, UAVs serve
    as mobile gateway nodes to collect data from both fixed and mobile sensors. In
    the traditional case of a deployed wireless sensor network, the data are sent
    to a cluster head of the gateway node. Furthermore, such methods consume a great
    deal of energy in the leader selection and route-finding processes. Therefore,
    a mobile gateway is employed to collect the data through UAVs and maximize the
    usage of sensors and lengthen their lifetime. 4.4. Unmanned Ariel Vehicles (UAVs)
    The data collection process is executed with the help of unmanned aerial vehicles,
    which collect data from the sensors deployed at the fog site. The path of the
    UAVs is pre-configured. A fleet of UAVs flies through this pre-defined path to
    collect the data. On cycle completion, UAVs return to the charging station to
    recharge their batteries for the next trip. 4.4.1. UAV Behavior-Based Charging
    A major limitation of UAV-assisted data collection is effectively managing UAVs’
    rapidly depleting energy banks during operation. Therefore, in the proposed work,
    we have integrated this aspect into the UAV behavior. The only way for the UAV
    to acquire a charge is by transmitting data and recording transactions at the
    fog node. The algorithm for UAV Charging is provided in Algorithm 1. The UAVs
    are allowed to use the charging spot only when they have the desired coin which
    they acquire by completing transactions with the fog node. The number of charging
    coins in this framework is assumed to be an integer for simplicity. The forwarding
    and recording costs for a single message are referred to as 𝑐 𝑓 and 𝑐 𝑟 , respectively.
    The forwarding and recording of messages is a prerequisite for obtaining coins
    to participate in the charging process. The charging coin ( 𝑃 𝑐 ) is assigned
    based on the following equation. 𝑃 𝑐 = ⎧ ⎩ ⎨     𝑖𝑓 𝑐 𝑟 == 𝑐 𝑓 𝑖𝑓 𝑐 𝑟 > 𝑐
    𝑓 𝑖𝑓 𝑐 𝑓 > 𝑐 𝑟 𝑐 𝑟 / 𝑐 𝑓 𝑐 𝑓 / 𝑐 𝑟 𝑐 𝑟 / 𝑐 𝑓 (11) Algorithm 1 UAVCharging UAV:
    list of UAVs U.Coin: reward value U.E: reward value U.P: Path U.CList: Received
    coin list S: list of deployed sensors procedure UAVDataCollection    while true
    do        𝑈 𝑖 ← MoveOnPath(U.P)                ▷ Moving UAVs on predefined paths         if  𝑈
    𝑖 is Connected to 𝑆 𝑗 then                     ▷ comm range with 𝑆 𝑗            𝑈
    𝑖 .data ← 𝑆 𝑗                            ▷ get data from sensor         end if         𝑈
    𝑖 .Communicate(UAV)                    ▷ Send msgs to UAVs in swarm         𝑈
    𝑖 .Rec(msg from 𝑈 𝑘 )                    ▷ Comm msgs from other UAVs         B
    ← 𝑈 𝑖 .ModelTest(msg)                     ▷ identify abnormal behavior         if
    B is NORMAL then                           ▷ Normal behavior                𝑈
    𝑖 .send(U.Coin, 1)                       ▷ send complete coin value         else                𝑈
    𝑖 .send(U.Coin, 0.5)                     ▷ Otherwise reduce coin value         end
    if         𝑈 𝑖 .CList = Rec(C from 𝑈𝐴𝑉 )                      ▷ Collect coin values         𝑈
    𝑖 .Coin = Accumulate( 𝑈 𝑖 .CList)         if  𝑈 𝑖 path Completed then            𝑈
    𝑖 .Upload(B)                          ▷ upload data to broker            𝑈 𝑖 .E
    ← Recharge( 𝑈 𝑖 .Coin)                  ▷ get recharge on coins value         end
    if     end while end procedure In the above equation, the UAV’s record and forward
    attributes are used to generate the charging coin. In case the UAV recollect matches
    with the forwarding parameter, one complete charging coin is issued. However,
    on any malicious behavior, the charging coin value is determined based on the
    ratio of collection and forwarding. In other words, a UAV is allowed to acquire
    a charge using the coin before the next trip. The flight route of each UAV is
    already available with the broker node which helps determine the energy needed
    to complete the trip. Therefore, if the UAV is unable to obtain the complete energy
    for the next trip, the broker tags it as malicious requiring a log inspection.
    Apart from the energy-based elimination from the UAV pool, the malicious activity
    is also monitored in the UAV network, which is explained in the next section.
    4.4.2. UAV-to-UAV Based Intrusion Detection Mechanism During data collection,
    the UAVs communicate with each other and exchange trajectory information along
    with other parameters, such as residual energy. This information is shared periodically,
    assuming that UAVs have sufficient energy to bear the communication cost. The
    UAVs move in the form of a fleet; where each UAV is covering a different path
    but is connected with other UAVs through a wireless connection. The entire UAV
    communication is logged for subsequent use in model training. The proposed intrusion
    detection model is trained on the collected data at the fog broker and deployed
    at UAVs and attempts to identify these attacks. The proposed architecture consists
    of multiple stages, i.e., data prepossessing, feature engineering, and intrusion
    detection. First, a dataset is gathered to evaluate the performance of the system.
    A k-means-based cluster sampling technique is used to create a highly representative
    subset of the data while preventing class imbalance. The dataset is treated throughout
    the feature engineering process to eliminate redundant and unnecessary features
    using information gain-based and correlation-based feature selection approaches,
    and the kernel principal component analysis model is used to further decrease
    the dimensions and noisy features. It is suggested that a hyperparameter-based
    optimization IDS be used to effectively identify both known and unidentified intrusions
    as discussed in [56]. The system is composed of several tiers, where the first
    tier consists of four tree-based machine learners namely decision tree, random
    forest, extra tree, and extreme gradient boost, which are used to identify known
    attacks. By integrating the output of the four base learners from the first tier
    and optimizing the learners, the second tier uses a stacking ensemble model and
    the Bayesian optimization-tree-structured Parzen estimator approach to further
    increase the intrusion detection accuracy. An anomaly-based intrusion detection
    system is built in the next stage to identify unknown attacks. The cluster-labeling
    k-means model is used as the 3rd tier of the intrusion detection system to successfully
    differentiate attack data from regular samples. The Bayesian optimization-Gaussian
    process technique and two biased classifiers make up the fourth tier of the IDS,
    which is utilized to improve the model and decrease classification errors in the
    CL-k-means. Each test sample’s detection outcome is ultimately reported, and it
    may be a known attack with a type, an unidentified attack, or typical benign traffic.
    5. Evaluation The proposed framework is evaluated for measuring the effectiveness
    of the hyperparameter optimization-based intrusion detection as well as the efficiency
    of the algorithm executing on the UAVs. 5.1. Machine Learning Framework For the
    effectiveness study, the proposed methodology is evaluated in terms of accuracy,
    precision, recall, F1 score, root mean square error, and R-squared score as evaluation
    metrics. The generated dataset is provided to the intrusion detection system that
    utilizes XGBoost random forest (RF), decision tree (DT), extra tree (ET), stacking,
    and k-means algorithms. The detailed results of the intrusion detection effectiveness
    evaluation are given in Figure 4. It can be seen that all algorithms perform well
    due to hyperparameter optimization with XGBoost showing the best results with
    99.77% accuracy, 0.1055 root mean square error, and 99.81% R-Squared score. These
    results indicate that algorithms perform well and fit the data model. Further,
    XGBoost can handle large datasets. Therefore, performing classification or regression
    using XGBoost typically begins with an estimate, determines the similarities value,
    and obtains a tree for each potential threshold [68]. Figure 4. Effectiveness
    of the Intrusion detection system using the selected machine learning algorithms.
    A comparison of the effectiveness with respect to related works is shown in Table
    2. The proposed model has improved accuracy, precision, recall, and F1 score as
    compared to other state-of-the-art related works. Table 2. Comparison with other
    models. 5.2. Simulation Framework The proposed UAV-assisted data collection framework
    is evaluated using a simulation model developed in AnyLogic (www.anylogic.com,
    accessed on 15 December 2022)—a multi-method simulation software that allows the
    integration of multiple modeling paradigms including discrete event, agent-based,
    and system dynamics modeling. The simulation model is designed to represent the
    behavior of the UAVs, fog nodes, and broker nodes, as well as their interactions
    in the data collection process. The simulation model includes several components
    including the fog sites, brokers, sensors and UAVs, the parameters for which are
    provided in Table 3. The UAVs are modeled as autonomous agents that can move around
    in a 2D space, collect data from different locations, and transmit data to the
    nearest fog node. The fog nodes are modeled as stationary nodes that receive UAV
    data and store them temporarily before transmitting them to the broker node. The
    broker node collects data from the fog nodes, processes them, and stores them
    in a database. Table 3. Simulation and System Specifications. The simulation model
    is parameterized with realistic values based on existing literature and real-world
    data. The energy consumption of the UAVs is modeled based on the type of UAV,
    speed and the distance traveled, while the transmission range of the UAVs and
    the fog nodes is modeled based on the signal strength and interference in the
    environment. The charging rate of the UAVs is also modeled based on the transaction
    costs and the amount of data transmitted. The simulation model is used to evaluate
    the performance of the proposed framework under different scenarios and conditions.
    The impact of the number of UAVs and fog nodes, the data collection rate, and
    the charging rate on the overall performance of the system is evaluated. The simulation
    model is also used to evaluate the robustness of the system under different types
    of attacks, such as denial of service attacks, spoofing attacks, and jamming attacks.
    The simulation results are analyzed and compared to existing literature to validate
    the effectiveness of the proposed framework. The simulation results show that
    the proposed framework can achieve higher data collection rates and better energy
    efficiency compared to existing methods. The simulation results also show that
    the proposed framework is robust against various types of attacks, demonstrating
    the effectiveness of the proposed intrusion detection system. In our proposed
    framework, we used the CICIDS2017 dataset to evaluate the performance of our UAV-assisted
    intrusion detection system. CICIDS2017 is a publicly available dataset for evaluating
    intrusion detection systems. It is a labeled dataset with various types of network
    traffic, including normal traffic and several types of attacks. The dataset contains
    a total of 80 features, including flow duration, protocol, source, and destination
    IP addresses, source and destination port numbers, and packet and byte counts.
    Some of the features are calculated from the network flow, such as the flow duration,
    packet count, and byte count, while others are extracted from the packet header,
    such as the protocol and port numbers. We used the flow duration, protocol, and
    packet count parameters from the dataset to train and test our machine learning
    models. We also used the source and destination IP addresses and port numbers
    to identify the type of attack. The AnyLogic simulator was used to generate synthetic
    network traffic data for testing our intrusion detection system. The simulator
    also allowed us to visualize the behavior of the system and test different scenarios
    to evaluate the performance. UAV energy consumption: Conserving energy is one
    of the critical challenges for UAV-based systems. In the proposed UAV model, the
    energy is consumed when a UAV moves, communicates, collects data, and hovers.
    The energy consumption in terms of speed is shown in Figure 5 and can be seen
    to be relatively high when the speed of the UAV is either too high or too low.
    The figure shows the energy consumption of three subsets of the energy, i.e.,
    parasite, induced power, and blade profile. The figure illustrates that approximately
    14 m/s is the ideal speed for achieving the most efficient energy consumption.
    Figure 5. UAV’s Consumed Energy with Respect to Speed. In the proposed work, the
    broker initiates the data collection process on multiple routes; therefore, Figure
    6, shows the UAV residual energy with simulation time. Each route has a variable
    length causing UAVs to take different times to complete the task. The first path
    is the longest as it covers more area to collect data from deployed sensors; thus,
    UAVs consumed more energy to collect the data. The third path is the shortest
    because of the small coverage area of the fog site where the UAVs consumed less
    energy. In the first route, UAVs took more time and consumed around 80% of their
    energy. In the case of a benign scenario, the UAVs have full charge after every
    cycle. On the second route, it takes around 60% of the residual energy and takes
    relatively less time to complete the path. The same is the case with the third
    route. In case of malicious behavior, the UAV acquires a lower charge after every
    trip and is eventually removed from the group. Moreover, the deployed model detects
    the malicious behavior using the energy and communication parameters; thus, helping
    to send the UAV for inspection before offloading the data at the fog node. Figure
    7 shows the residual energy when UAVs become malicious with simulation time. It
    is assumed that a UAV is 80% malicious because it participates in only 20% of
    the required recording and transmission. In this case, the UAV will only have
    20% more charging. In the first route, the UAV does not have enough charging to
    complete its route and eventually is removed from the system. Similarly, in the
    second and third routes, malicious UAVs are automatically removed from the system
    after completing two rounds due to zero charging. Figure 6. UAV’s residual energy
    with respect to time. Figure 7. UAV’s remaining energy when the UAV becomes malicious
    with respect to time. Resource utilization—the proposed framework is benchmarked
    in terms of resource usage, i.e., memory and CPU. These resources are used effectively
    by a well-constructed system. The memory consumption is shown in Figure 8 with
    varying nodes, i.e., UAVs/sensors. UAVs, brokers, farms, and sensors that have
    been deployed are included in this list of nodes. With 200 UAVs, only 4.5% of
    memory is utilized, whereas with 400 UAVs, 6.4% of memory is used. When the number
    of UAVs increases to 600, only 9.1% of memory is occupied, indicating a linear
    increase in memory usage with the number of UAVs. Notably, no memory leaks are
    noticed during the simulation run. The CPU consumption is shown in Figure 9. CPU
    usage escalates as the number of sensors increases. In this scenario, with 200
    UAVs, 30% of CPU is used, whereas with 400 UAVs, 35% of CPU is utilized. When
    the number of UAVs increases to 600, the CPU usage reaches 39% only, which includes,
    computational algorithms, mobility models, energy modules, and communication links
    between the UAVs, installed sensors, brokers, and fog sites. Figure 8. Memory
    utilization with increasing sensors. Figure 9. CPU Utilization with Increasing
    UAVs. UAV2UAV communication delay—the communication delay between UAVs is seen
    in Figure 10. The average delay is calculated by increasing the number of deployed
    sensors and different UAVs. Here, the UAVs gathered information from the sensors.
    According to availability, either the nearby sensor or the remote sensor communicates
    with the UAV for the transmission of the data. However, it has been noted that
    as the number of UAVs increases, the communication delay between the UAVs also
    increases. Figure 10. Average UAV-to-UAV communication delay with different numbers
    of sensors and UAVs. Transmission delay—the amount of time needed to send or receive
    a packet is called the transmission time or delay. When a UAV communicates with
    the sensor, it takes some time to receive the packets. The typical communication
    delay among UAVs and deployed sensors is seen in Figure 11. It is important to
    note that the delay grows as more sensors are placed. Due to the overlapping transmission
    ranges, several sensors attempt to interact with the UAVs, which causes increased
    channel congestion. Figure 11. Transmission delay between sensors and UAVs. 6.
    Discussion The results clearly indicate the increased security and the data-collection
    efficiency of the UAV-assisted smart farming framework that utilizes energy constraints
    and an intrusion detection system. The UAVs have a limited battery and are only
    allowed to charge by transmitting data and recording transactions at the fog node.
    This approach incentivizes the UAVs to identify malicious activities that may
    result in excluding the UAVs from the trusted network. The proposed work is evaluated
    with various machine learning models as well as the other network parameters.
    In most of the existing works, authors are only focused on utilizing limited features
    for the machine learning model; thus, overlooking critical parameters such as
    UAV energy, transmission delay, and the impact of cyber-attacks on UAV’s energy.
    This work presents an extensive framework that covers the data collection, sensors
    integrated environment, and role of fog nodes for recharging the UAVs. The CPU
    and memory are the important parameters to gauge the scalability of devices inside
    the framework. With the increased number of UAVs, a linear relation is observed
    in terms of CPU and memory usage. The utilization of resources reported with 600
    UAVs in a simulation framework require only 9.1% memory, and corresponding CPU
    utilization is 39%. Further, we observed that the speed of UAVs has a direct impact
    on their energy and in order to cover large-scale areas, a 14 m/s speed needs
    to be maintained for maximum utilization of UAVs. Moreover, amongst the machine
    learning models, adopted, XGBoost showed the best performance with 99.77% accuracy.
    7. Conclusions In recent years, smart farming technology has rapidly advanced
    and has significantly contributed to the improvement of crop yields. To further
    improve the efficiency of data collection in smart farming, this research proposes
    a framework that utilizes unmanned aerial vehicles (UAVs) and Internet of Things
    (IoT) devices. However, this open environment is vulnerable to intrusions, which
    can hinder the data collection process and ultimately reduce agricultural productivity.
    To address this potential threat, the research proposes an intrusion detection
    system (IDS) integrated into a fog-based UAV-IoT farm data collection system.
    The IDS utilizes machine learning algorithms that are trained on the CICIDS2017
    dataset, which is publicly available, to detect and prevent intrusions. A layer-based
    intrusion detection approach is considered to detect both known and zero-day attacks.
    For known attacks, a signature-based IDS is used and uses XGBoost, extra tree,
    random forest, and decision tree algorithms. For zero-day attacks, clustering
    techniques are used, which include the K-means algorithm. The evaluation results
    show that XGBoost provides the best results, as it can detect intrusions with
    99.77% accuracy, with an F1 score of 0.1055 RMSE and a 99.81% R-squared score.
    The modular design is developed to implement and benchmark the proposed work in
    terms of UAV energy, transmission, and communication delays. The proposed IDS
    integrated with the fog-based UAV-IoT farm data collection system can improve
    the security and efficiency of smart farming, which can ultimately lead to increased
    crop yield and improved agricultural productivity. In the future, the research
    will aim to improve the intrusion detection system using machine vision and extensive
    deep learning techniques. Author Contributions Conceptualization, J.S., Z.A. and
    A.W.M.; methodology, Z.A. and A.W.M.; software, J.S. and A.W.M.; validation, A.W.M.;
    formal analysis, Z.A.; investigation, J.S., K.H, A.W.M. and Z.A.; resources, Z.A.
    and Z.T.; data curation, J.S., A.W.M. and Z.A.; writing original draft preparation,
    J.S., K.H., A.W.M., Z.A. and Z.T.; writing review and editing, A.W.M. and Z.A.;
    visualization, J.S, A.W.M. and Z.A.; supervision, K.H., A.W.M., Z.A. and Z.T.;
    project administration, K.H., A.W.M., Z.A. and Z.T.; funding acquisition, K.H.,
    Z.A. and Z.T. All authors have read and agreed to the published version of the
    manuscript. Funding This work was supported by the Sheila and Robert Challey Institute
    for Global Innovation & Growth at North Dakota State University, USA, and Zayed
    University under the Cluster Research Grant R20140, UAE. Institutional Review
    Board Statement Not applicable. Informed Consent Statement Not applicable. Data
    Availability Statement The data may be requested by reaching out to authors through
    email. Conflicts of Interest The authors declare no conflict of interest. References
    Kiani, F.; Seyyedabbasi, A. Wireless sensor network and Internet of Things in
    precision agriculture. Int. J. Adv. Comput. Sci. Appl. 2018, 9, 99–103. [Google
    Scholar] [CrossRef] [Green Version] Maddikunta, P.K.R.; Hakak, S.; Alazab, M.;
    Bhattacharya, S.; Gadekallu, T.R.; Khan, W.Z.; Pham, Q.V. Unmanned aerial vehicles
    in smart agriculture: Applications, requirements, and challenges. IEEE Sens. J.
    2021, 21, 17608–17619. [Google Scholar] [CrossRef] Mendez, G.R.; Yunus, M.A.M.;
    Mukhopadhyay, S.C. A WiFi based smart wireless sensor network for monitoring an
    agricultural environment. In Proceedings of the 2012 IEEE International Instrumentation
    and Measurement Technology Conference Proceedings, Graz, Austria, 13–16 May 2012;
    IEEE: New York, NY, USA, 2012; pp. 2640–2645. [Google Scholar] Maes, W.H.; Steppe,
    K. Perspectives for remote sensing with unmanned aerial vehicles in precision
    agriculture. Trends Plant Sci. 2019, 24, 152–164. [Google Scholar] [CrossRef]
    Nguyen, M.T.; Nguyen, C.V.; Do, H.T.; Hua, H.T.; Tran, T.A.; Nguyen, A.D.; Ala,
    G.; Viola, F. Uav-assisted data collection in wireless sensor networks: A comprehensive
    survey. Electronics 2021, 10, 2603. [Google Scholar] [CrossRef] Alladi, T.; Chamola,
    V.; Sahu, N.; Guizani, M. Applications of blockchain in unmanned aerial vehicles:
    A review. Veh. Commun. 2020, 23, 100249. [Google Scholar] [CrossRef] Ju, C.; Son,
    H.I. Multiple UAV systems for agricultural applications: Control, implementation,
    and evaluation. Electronics 2018, 7, 162. [Google Scholar] [CrossRef] [Green Version]
    Faraci, G.; Grasso, C.; Schembra, G. Fog in the clouds: UAVs to provide edge computing
    to IoT devices. ACM Trans. Internet Technol. (TOIT) 2020, 20, 1–26. [Google Scholar]
    [CrossRef] Elrawy, M.F.; Awad, A.I.; Hamed, H.F. Intrusion detection systems for
    IoT-based smart environments: A survey. J. Cloud Comput. 2018, 7, 1–20. [Google
    Scholar] [CrossRef] [Green Version] Mothukuri, V.; Parizi, R.M.; Pouriyeh, S.;
    Huang, Y.; Dehghantanha, A.; Srivastava, G. A survey on security and privacy of
    federated learning. Future Gener. Comput. Syst. 2021, 115, 619–640. [Google Scholar]
    [CrossRef] Krishna, C.L.; Murphy, R.R. A review on cybersecurity vulnerabilities
    for unmanned aerial vehicles. In Proceedings of the 2017 IEEE International Symposium
    on Safety, Security and Rescue Robotics (SSRR), Shanghai, China, 11–13 October
    2017; IEEE: New York, NY, USA, 2017; pp. 194–199. [Google Scholar] Challita, U.;
    Ferdowsi, A.; Chen, M.; Saad, W. Machine learning for wireless connectivity and
    security of cellular-connected UAVs. IEEE Wirel. Commun. 2019, 26, 28–35. [Google
    Scholar] [CrossRef] [Green Version] Kumar, P.; Kumar, R.; Gupta, G.P.; Tripathi,
    R. A Distributed framework for detecting DDoS attacks in smart contract-based
    Blockchain-IoT Systems by leveraging Fog computing. Trans. Emerg. Telecommun.
    Technol. 2021, 32, e4112. [Google Scholar] [CrossRef] Bodkhe, U.; Tanwar, S.;
    Bhattacharya, P.; Kumar, N. Blockchain for precision irrigation: Opportunities
    and challenges. Trans. Emerg. Telecommun. Technol. 2020, 33, e4059. [Google Scholar]
    [CrossRef] Delavarpour, N.; Koparan, C.; Nowatzki, J.; Bajwa, S.; Sun, X. A technical
    study on UAV characteristics for precision agriculture applications and associated
    practical challenges. Remote Sens. 2021, 13, 1204. [Google Scholar] [CrossRef]
    Panchasara, H.; Samrat, N.H.; Islam, N. Greenhouse gas emissions trends and mitigation
    measures in Australian agriculture sector—A review. Agriculture 2021, 11, 85.
    [Google Scholar] [CrossRef] Dagar, R.; Som, S.; Khatri, S.K. Smart farming–IoT
    in agriculture. In Proceedings of the 2018 International Conference on Inventive
    Research in Computing Applications (ICIRCA), Coimbatore, India, 11–12 July 2018;
    IEEE: New York, NY, USA, 2018; pp. 1052–1056. [Google Scholar] Tiusanen, M.J.
    Soil scouts: Description and performance of single hop wireless underground sensor
    nodes. Ad Hoc Netw. 2013, 11, 1610–1618. [Google Scholar] [CrossRef] Zhang, X.;
    Andreyev, A.; Zumpf, C.; Negri, M.C.; Guha, S.; Ghosh, M. Thoreau: A subterranean
    wireless sensing network for agriculture and the environment. In Proceedings of
    the 2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),
    Atlanta, GA, USA, 1–4 May 2017; IEEE: New York, NY, USA, 2017; pp. 78–84. [Google
    Scholar] Doshi, J.; Patel, T.; Kumar Bharti, S. Smart Farming using IoT, a solution
    for optimally monitoring farming conditions. Procedia Comput. Sci. 2019, 160,
    746–751. [Google Scholar] [CrossRef] Ryu, M.; Yun, J.; Miao, T.; Ahn, I.Y.; Choi,
    S.C.; Kim, J. Design and implementation of a connected farm for smart farming
    system. In Proceedings of the 2015 IEEE SENSORS, Busan, Republic of Korea, 1–4
    November 2015; IEEE: New York, NY, USA, 2015; pp. 1–4. [Google Scholar] Bauer,
    J.; Aschenbruck, N. Design and implementation of an agricultural monitoring system
    for smart farming. In Proceedings of the 2018 IoT Vertical and Topical Summit
    on Agriculture-Tuscany (IOT Tuscany), Tuscany, Italy, 8–9 May 2018; IEEE: New
    York, NY, USA, 2018; pp. 1–6. [Google Scholar] Trilles, S.; González-Pérez, A.;
    Huerta, J. A comprehensive IoT node proposal using open hardware. A smart farming
    use case to monitor vineyards. Electronics 2018, 7, 419. [Google Scholar] [CrossRef]
    [Green Version] Xue-Fen, W.; Yi, Y.; Tao, Z.; Jing-Wen, Z.; Sardar, M.S. Design
    of distributed agricultural service node with smartphone in-field access supporting
    for smart farming in Beijing-Tianjin-Hebei region. Sens. Mater. 2018, 30, 2281–2293.
    [Google Scholar] [CrossRef] [Green Version] Glaroudis, D.; Iossifides, A.; Chatzimisios,
    P. Survey, comparison and research challenges of IoT application protocols for
    smart farming. Comput. Netw. 2020, 168, 107037. [Google Scholar] [CrossRef] Sharma,
    R. Review on Application of Drone Systems in Precision Agriculture. J. Adv. Res.
    Electron. Eng. Technol. 2021, 7, 5–7. [Google Scholar] Muchiri, G.; Kimathi, S.
    A review of applications and potential applications of UAV. In Proceedings of
    the Sustainable Research and Innovation Conference, Rovinj, Croatia, 4 April 2022;
    pp. 280–283. [Google Scholar] Boursianis, A.D.; Papadopoulou, M.S.; Diamantoulakis,
    P.; Liopa-Tsakalidi, A.; Barouchas, P.; Salahas, G.; Karagiannidis, G.; Wan, S.;
    Goudos, S.K. Internet of Things (IoT) and agricultural unmanned aerial vehicles
    (UAVs) in smart farming: A comprehensive review. Internet Things 2022, 18, 100187.
    [Google Scholar] [CrossRef] Islam, N.; Rashid, M.M.; Wibowo, S.; Wasimi, S.; Morshed,
    A.; Xu, C.; Moore, S. Machine learning based approach for Weed Detection in Chilli
    field using RGB images. In Proceedings of the International Conference on Natural
    Computation, Fuzzy Systems and Knowledge Discovery, Fuzhou, China, 30 July–1 August
    2020; Springer: New York, NY, USA, 2020; pp. 1097–1105. [Google Scholar] Malambo,
    L.; Popescu, S.C.; Murray, S.C.; Putman, E.; Pugh, N.A.; Horne, D.W.; Richardson,
    G.; Sheridan, R.; Rooney, W.L.; Avant, R.; et al. Multitemporal field-based plant
    height estimation using 3D point clouds generated from small unmanned aerial systems
    high-resolution imagery. Int. J. Appl. Earth Obs. Geoinf. 2018, 64, 31–42. [Google
    Scholar] [CrossRef] Chebrolu, N.; Läbe, T.; Stachniss, C. Robust long-term registration
    of UAV images of crop fields for precision agriculture. IEEE Robot. Autom. Lett.
    2018, 3, 3097–3104. [Google Scholar] [CrossRef] Popescu, D.; Stoican, F.; Stamatescu,
    G.; Ichim, L.; Dragana, C. Advanced UAV–WSN system for intelligent monitoring
    in precision agriculture. Sensors 2020, 20, 817. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Raja, L.; Vyas, S. The study of technological development in the
    field of smart farming. In Smart Farming Technologies for Sustainable Agricultural
    Development; IGI Global: Hershey, PA, USA, 2019; pp. 1–24. [Google Scholar] Spachos,
    P.; Gregori, S. Integration of wireless sensor networks and smart uavs for precision
    viticulture. IEEE Internet Comput. 2019, 23, 8–16. [Google Scholar] [CrossRef]
    Hernandez, A.; Murcia, H.; Copot, C.; De Keyser, R. Towards the development of
    a smart flying sensor: Illustration in the field of precision agriculture. Sensors
    2015, 15, 16688–16709. [Google Scholar] [CrossRef] [Green Version] Shamshiri,
    R.R.; Hameed, I.A.; Balasundram, S.K.; Ahmad, D.; Weltzien, C.; Yamin, M. Fundamental
    research on unmanned aerial vehicles to support precision agriculture in oil palm
    plantations. In Agricultural Robots-Fundamentals and Application; Intechopen:
    London, UK, 2018; pp. 91–116. [Google Scholar] Von Bueren, S.K.; Burkart, A.;
    Hueni, A.; Rascher, U.; Tuohy, M.P.; Yule, I.J. Deploying four optical UAV-based
    sensors over grassland: Challenges and limitations. Biogeosciences 2015, 12, 163–175.
    [Google Scholar] [CrossRef] [Green Version] Islam, N.; Sithamparanathan, K.; Chavez,
    K.G.; Scott, J.; Eltom, H. Energy efficient and delay aware ternary-state transceivers
    for aerial base stations. Digit. Commun. Netw. 2019, 5, 40–50. [Google Scholar]
    [CrossRef] Choudhary, G.; Sharma, V.; You, I.; Yim, K.; Chen, R.; Cho, J.H. Intrusion
    detection systems for networked unmanned aerial vehicles: A survey. In Proceedings
    of the 2018 14th International Wireless Communications & Mobile Computing Conference
    (IWCMC), Limassol, Cyprus, 25–28 June 2018; IEEE: New York, NY, USA, 2018; pp.
    560–565. [Google Scholar] Gao, X.; Shan, C.; Hu, C.; Niu, Z.; Liu, Z. An adaptive
    ensemble machine learning model for intrusion detection. IEEE Access 2019, 7,
    82512–82521. [Google Scholar] [CrossRef] Abu Al-Haija, Q.; Zein-Sabatto, S. An
    efficient deep-learning-based detection and classification system for cyber-attacks
    in IoT communication networks. Electronics 2020, 9, 2152. [Google Scholar] [CrossRef]
    Abu Al-Haija, Q.; Al Badawi, A. High-performance intrusion detection system for
    networked UAVs via deep learning. Neural Comput. Appl. 2022, 34, 10885–10900.
    [Google Scholar] [CrossRef] Manesh, M.R.; Kenney, J.; Hu, W.C.; Devabhaktuni,
    V.K.; Kaabouch, N. Detection of GPS spoofing attacks on unmanned aerial systems.
    In Proceedings of the 2019 16th IEEE Annual Consumer Communications & Networking
    Conference (CCNC), Vegas, NV, USA, 11–14 January 2019; IEEE: New York, NY, USA,
    2019; pp. 1–6. [Google Scholar] Min, E.; Long, J.; Liu, Q.; Cui, J.; Cai, Z.;
    Ma, J. Su-ids: A semi-supervised and unsupervised framework for network intrusion
    detection. In Proceedings of the International Conference on Cloud Computing and
    Security, Haikou, China, 8–10 June 2018; Springer: New York, NY, USA, 2018; pp.
    322–334. [Google Scholar] Wang, A.; Wang, W.; Zhou, H.; Zhang, J. Network intrusion
    detection algorithm combined with group convolution network and snapshot ensemble.
    Symmetry 2021, 13, 1814. [Google Scholar] [CrossRef] Devan, P.; Khare, N. An efficient
    XGBoost–DNN-based classification model for network intrusion detection system.
    Neural Comput. Appl. 2020, 32, 12499–12514. [Google Scholar] [CrossRef] Wang,
    B.; Wang, Z.; Liu, L.; Liu, D.; Peng, X. Data-driven anomaly detection for UAV
    sensor data based on deep learning prediction model. In Proceedings of the 2019
    Prognostics and System Health Management Conference (PHM-Paris), Paris, France,
    2–5 May 2019; IEEE: New York, NY, USA, 2019; pp. 286–290. [Google Scholar] MP,
    R.; Daniya, T.; Mano Paul, P.; Rajakumar, S. Intrusion detection using optimized
    ensemble classification in fog computing paradigm. Knowl.-Based Syst. 2022, 252,
    109364. [Google Scholar] Safara, F.; Souri, A.; Serrizadeh, M. Improved intrusion
    detection method for communication networks using association rule mining and
    artificial neural networks. IET Commun. 2020, 14, 1192–1197. [Google Scholar]
    [CrossRef] Ferrag, M.A.; Shu, L.; Friha, O.; Yang, X. Cyber security intrusion
    detection for agriculture 4.0: Machine learning-based solutions, datasets, and
    future directions. IEEE/CAA J. Autom. Sin. 2021, 9, 407–436. [Google Scholar]
    [CrossRef] Zhao, L.; Alipour-Fanid, A.; Slawski, M.; Zeng, K. Prediction-time
    efficient classification using feature computational dependencies. In Proceedings
    of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining, London, UK, 19–23 August 2018; pp. 2787–2796. [Google Scholar] Bithas,
    P.S.; Michailidis, E.T.; Nomikos, N.; Vouyioukas, D.; Kanatas, A.G. A survey on
    machine-learning techniques for UAV-based communications. Sensors 2019, 19, 5170.
    [Google Scholar] [CrossRef] [PubMed] [Green Version] Yao, Y.; Su, L.; Lu, Z.;
    Liu, B. Stdeepgraph: Spatial-temporal deep learning on communication graphs for
    long-term network attack detection. In Proceedings of the 2019 18th IEEE International
    Conference on Trust, Security and Privacy in Computing and Communications/13th
    IEEE International Conference on Big Data Science and Engineering (TrustCom/BigDataSE),
    Rotorua, New Zealand, 5–8 August 2019; IEEE: New York, NY, USA, 2019; pp. 120–127.
    [Google Scholar] Chowdhury, M.M.U.; Hammond, F.; Konowicz, G.; Xin, C.; Wu, H.;
    Li, J. A few-shot deep learning approach for improved intrusion detection. In
    Proceedings of the 2017 IEEE 8th Annual Ubiquitous Computing, Electronics and
    Mobile Communication Conference (UEMCON), New York, NY, USA, 19–21 October 2017;
    IEEE: New York, NY, USA, 2017; pp. 456–462. [Google Scholar] Yu, T.; Zhu, H. Hyper-parameter
    optimization: A review of algorithms and applications. arXiv 2020, arXiv:2003.05689.
    [Google Scholar] Kunang, Y.N.; Nurmaini, S.; Stiawan, D.; Suprapto, B.Y. Attack
    classification of an intrusion detection system using deep learning and hyperparameter
    optimization. J. Inf. Secur. Appl. 2021, 58, 102804. [Google Scholar] [CrossRef]
    Fu, R.; Ren, X.; Li, Y.; Wu, Y.; Sun, H.; Al-Absi, M.A. Machine Learning-Based
    UAV Assisted Agricultural Information Security Architecture and Intrusion Detection.
    IEEE Internet Things J. 2023. [Google Scholar] [CrossRef] Mitchell, R.; Chen,
    R. Adaptive intrusion detection of malicious unmanned air vehicles using behavior
    rule specifications. IEEE Trans. Syst. Man Cybern. Syst. 2013, 44, 593–604. [Google
    Scholar] [CrossRef] Raghuvanshi, A.; Singh, U.K.; Sajja, G.S.; Pallathadka, H.;
    Asenso, E.; Kamal, M.; Singh, A.; Phasinam, K. Intrusion detection using machine
    learning for risk mitigation in IoT-enabled smart irrigation in smart farming.
    J. Food Qual. 2022, 2022, 3955514. [Google Scholar] [CrossRef] Malik, A.W.; Rahman,
    A.U.; Qayyum, T.; Ravana, S.D. Leveraging fog computing for sustainable smart
    farming using distributed simulation. IEEE Internet Things J. 2020, 7, 3300–3309.
    [Google Scholar] [CrossRef] Kanimozhi, V.; Jacob, T.P. Artificial intelligence
    based network intrusion detection with hyper-parameter optimization tuning on
    the realistic cyber dataset CSE-CIC-IDS2018 using cloud computing. In Proceedings
    of the 2019 International Conference on Communication and Signal Processing (ICCSP),
    Melmaruvathur, India, 4–6 April 2019; IEEE: New York, NY, USA, 2019; pp. 0033–0036.
    [Google Scholar] Kumar, R.; Kumar, P.; Tripathi, R.; Gupta, G.P.; Gadekallu, T.R.;
    Srivastava, G. SP2F: A secured privacy-preserving framework for smart agricultural
    Unmanned Aerial Vehicles. Comput. Netw. 2021, 187, 107819. [Google Scholar] [CrossRef]
    Rajadurai, H.; Gandhi, U.D. A stacked ensemble learning model for intrusion detection
    in wireless network. Neural Comput. Appl. 2020, 34, 15387–15395. [Google Scholar]
    [CrossRef] Mozaffari, M.; Saad, W.; Bennis, M.; Debbah, M. Mobile Internet of
    Things: Can UAVs provide an energy-efficient mobile architecture? In Proceedings
    of the 2016 IEEE Global Communications Conference (GLOBECOM), Washington, DC,
    USA, 4–8 December 2016; IEEE: New York, NY, USA, 2016; pp. 1–6. [Google Scholar]
    Zeng, Y.; Xu, J.; Zhang, R. Energy minimization for wireless communication with
    rotary-wing UAV. IEEE Trans. Wirel. Commun. 2019, 18, 2329–2345. [Google Scholar]
    [CrossRef] [Green Version] Sharafaldin, I.; Lashkari, A.H.; Ghorbani, A.A. Toward
    generating a new intrusion detection dataset and intrusion traffic characterization.
    ICISSp 2018, 1, 108–116. [Google Scholar] Olasupo, T.O. Propagation modeling of
    IoT devices for deployment in multi-level hilly urban environments. In Proceedings
    of the 2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication
    Conference (IEMCON), Vancouver, BC, Canada, 1–3 November 2018; IEEE: New York,
    NY, USA, 2018; pp. 346–352. [Google Scholar] Mitchell, R.; Adinets, A.; Rao, T.;
    Frank, E. Xgboost: Scalable GPU accelerated learning. arXiv 2018, arXiv:1806.11248.
    [Google Scholar] Elmasry, W.; Akbulut, A.; Zaim, A.H. Evolving deep learning architectures
    for network intrusion detection using a double PSO metaheuristic. Comput. Netw.
    2020, 168, 107042. [Google Scholar] [CrossRef] Vijayanand, R.; Devaraj, D.; Kannapiran,
    B. Intrusion detection system for wireless mesh network using multiple support
    vector machine classifiers with genetic-algorithm-based feature selection. Comput.
    Secur. 2018, 77, 304–314. [Google Scholar] [CrossRef] Abdulhammed, R.; Faezipour,
    M.; Musafer, H.; Abuzneid, A. Efficient network intrusion detection using pca-based
    dimensionality reduction of features. In Proceedings of the 2019 International
    Symposium on Networks, Computers and Communications (ISNCC), Istanbul, Turkey,
    18–20 June 2019; IEEE: New York, NY, USA, 2019; pp. 1–6. [Google Scholar]        Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2023 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Sajid, J.; Hayawi, K.; Malik, A.W.; Anwar, Z.;
    Trabelsi, Z. A Fog Computing Framework for Intrusion Detection of Energy-Based
    Attacks on UAV-Assisted Smart Farming. Appl. Sci. 2023, 13, 3857. https://doi.org/10.3390/app13063857
    AMA Style Sajid J, Hayawi K, Malik AW, Anwar Z, Trabelsi Z. A Fog Computing Framework
    for Intrusion Detection of Energy-Based Attacks on UAV-Assisted Smart Farming.
    Applied Sciences. 2023; 13(6):3857. https://doi.org/10.3390/app13063857 Chicago/Turabian
    Style Sajid, Junaid, Kadhim Hayawi, Asad Waqar Malik, Zahid Anwar, and Zouheir
    Trabelsi. 2023. \"A Fog Computing Framework for Intrusion Detection of Energy-Based
    Attacks on UAV-Assisted Smart Farming\" Applied Sciences 13, no. 6: 3857. https://doi.org/10.3390/app13063857
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   1
    Scopus   2 Web of Science   1 Google Scholar   [click to view] Article Access
    Statistics Article access statistics Article Views 14. Jan 24. Jan 3. Feb 13.
    Feb 23. Feb 4. Mar 14. Mar 24. Mar 3. Apr 0 500 1000 1500 2000 2500 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Appl. Sci., EISSN 2076-3417, Published by MDPI RSS
    Content Alert Further Information Article Processing Charges Pay an Invoice Open
    Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For
    Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Applied Sciences (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Fog Computing Framework for Intrusion Detection of Energy-Based Attacks
    on UAV-Assisted Smart Farming
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Raikar M.M.
  - S M M.
  citation_count: '0'
  description: Internet of Things (IoT) has evolved and penetrated in different sectors
    such as healthcare, agriculture, manufacturing, transportation, and logistics.
    The IoT services are deployed in the cloud for accessing it virtually independent
    of location. The data-intensive IoT applications when deployed in the cloud have
    high response time. To overcome this challenge, the fog computing techniques is
    proposed to access the data from the sensors in real time. In this paper, the
    Fog-enabled IoT architecture for precision agriculture is presented. Precision
    agriculture is the usage of information technology to increase yield of crops
    and improve health of plants by ensuring accurate and controlled supply of nutrients.
    The sensors are deployed in the fields, and data is sent to the edge devices used
    for improving the produce. The different services available are automated irrigation
    and fertigation, recommendation by experts, decision support system, and protection
    from rodents. The farmers can subscribe to the service and make the best use.
    The advantage of Fog-enabled IoT is the reduction in round-trip time (RTT) compared
    to Cloud IoT architecture. The result demonstrates 40% reduction in RTT when Fog-enabled
    IoT architecture is used for different services.
  doi: 10.1007/978-3-031-15175-0_27
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Machine Learning and Big
    Data Analytics Conference paper Fog Computing-Enabled Internet of Things for Resource
    Optimization Conference paper First Online: 07 June 2023 pp 329–339 Cite this
    conference paper Access provided by University of Nebraska-Lincoln Download book
    PDF Download book EPUB Machine Learning and Big Data Analytics (ICMLBDA 2022)
    Meenaxi M. Raikar & Meena S M   Part of the book series: Springer Proceedings
    in Mathematics & Statistics ((PROMS,volume 401)) Included in the following conference
    series: International Conference on Machine Learning and Big Data Analytics 420
    Accesses Abstract Internet of Things (IoT) has evolved and penetrated in different
    sectors such as healthcare, agriculture, manufacturing, transportation, and logistics.
    The IoT services are deployed in the cloud for accessing it virtually independent
    of location. The data-intensive IoT applications when deployed in the cloud have
    high response time. To overcome this challenge, the fog computing techniques is
    proposed to access the data from the sensors in real time. In this paper, the
    Fog-enabled IoT architecture for precision agriculture is presented. Precision
    agriculture is the usage of information technology to increase yield of crops
    and improve health of plants by ensuring accurate and controlled supply of nutrients.
    The sensors are deployed in the fields, and data is sent to the edge devices used
    for improving the produce. The different services available are automated irrigation
    and fertigation, recommendation by experts, decision support system, and protection
    from rodents. The farmers can subscribe to the service and make the best use.
    The advantage of Fog-enabled IoT is the reduction in round-trip time (RTT) compared
    to Cloud IoT architecture. The result demonstrates 40% reduction in RTT when Fog-enabled
    IoT architecture is used for different services. Access provided by University
    of Nebraska-Lincoln. Download conference paper PDF Similar content being viewed
    by others Role of IoT in Smart Precision Agriculture Chapter © 2022 Role of IoT
    in Smart Precision Agriculture Chapter © 2023 Smart Farming: Application of Internet
    of Things (IoT) Systems Chapter © 2021 Keywords MQTT Raspberry Pi Publish Subscribe
    Edge device 1 Introduction The Fog-enabled Internet of Things (IoT) architecture
    is used for resource optimization. The agriculture sector use case is considered
    for providing service to the farmers. The farmer can enable the services using
    the mobile application. The lightweight protocol is chosen for communication between
    the devices for energy-efficient transmission of the data. The network topology
    consists of heterogeneous nodes such as things and edge devices. The nodes in
    the network are power constraint devices; hence, the lightweight protocol is opted
    for the communication. The Message Queue Telemetry Transport (MQTT) is the lightweight
    protocol that is used for communication between the edge devices such as Raspberry
    Pi. The MQTT protocol uses publish-subscribe communication model. It is assumed
    that the service provider will have the infrastructure for facilitating the different
    services mentioned. The recommendation by experts regarding which crop to be grown
    in the field based on region, season, and climate conditions is available to the
    farmers. Advice related to the appropriate time for sowing the seed to get better
    yield is facilitated on request through mobile. This paper outlines the use of
    IoT devices such as rain sensor, soil moisture sensor, and temperature sensor
    to sense the agricultural data and store into edge device database. The stored
    data is monitored through big data analytics, and prediction is performed using
    data mining techniques. The energy efficiency is computed in terms of the packet
    size and the round trip time for the transmission of the data between the devices.
    The organization of the paper: In Sect. 2, related work on Fog IoT services is
    presented. In Sect. 3, the architecture for Fog IoT taking precision agriculture
    as a use case is described. The energy model for the Fog IoT devices is discussed
    in Sect. 4. The distance between the Fog IoT nodes, a factor to reduce latency,
    is presented in Sect. 5. The result analysis and conclusion is presented in Sects.
    6, and 7, respectively. 2 State of the Art in Fog-Enabled IoT Services In [1],
    the system that collects soil properties and stores it in cloud for further analysis
    is mentioned. It also speaks about building a scalable sensor data analysis for
    smart farming with commercially available IoT devices which reduces maintenance
    cost and provides data analytics resulting in increased crop yield. In [2], cloud-based
    IoT application for precision agriculture with three-layer architecture is described.
    The top layer deals with collecting information and applying necessary agricultural
    actions. The next layer connects the top layer to IoT. The last layer deals with
    data storage and processing. AgroTick, a novel hybrid system for smart agriculture,
    is discussed in [3]. AgroTick is an Internet of Things (IoT) system with a mobile
    interface that was created using technological modules such as cloud computing,
    embedded firmware, hardware, and big data analytics. AgroTick is built to increase
    agricultural efficiency, develop a well-connected farming network, and provide
    a knowledge-sharing platform for farmers. In [4,5,6,7], the lightweight protocol,
    Message Queue Telemetry Transport (MQTT), is the communication model used. It
    has opened its way in many sectors since its invention in 1999. The different
    cloud deployments and services available for the Internet users are presented
    in [8]. The various applications of IoT such as smart parking, waste management,
    and home automation are described in [9]. The power efficiency for IoT devices
    is enhanced using the software-defined networks (SDN) architecture [10]. The energy
    efficiency of the IoT devices is improved using load balancing and fault tolerance
    techniques [11]. The IoT services of the smart city are described in [12]. The
    6LoWPAN and CoAP protocol in development of the applications are presented in
    [13]. The security issues are concerns in the IoT network presented in [14, 15].
    3 Fog Computing-Enabled IoT Architecture The Fog computing-enabled IoT architecture
    for precision agriculture using lightweight protocol is shown in Fig. 1. The objective
    is to provide different service for farmers that aid in precision agriculture
    such as: 1. Irrigation and fertigation as a service 2. Crop-related decision support
    system as a service 3. Recommendation as a service 4. Pest control as a service
    The farmers have to subscribe for the services by registration process. The metering
    and billing module at the service provider (SP) side keeps track of the unit amount
    of resource utilized by the farmers. Irrigation and Fertigation As a Service:
    This service is used by farmers to irrigate and fertigate the land. The controlled
    amount of water and fertilizers are supplied to the plants based on the different
    parameters such as soil moisture content, temperature, and rainfall in the region.
    Crop-Related Decision Support System As a Service This service aids the farmer
    in making decisions related to the type of crop to be sowed or the suitable time
    for sowing in the region. To facilitate this service at the SP side, big data
    analytics technique are applied as shown in Fig. 1. Recommendation As a Service
    The farmers who have subscribed to this service get recommendation related to
    the crop. The data related to each farmers land is collected using sensors and
    stored at the data center for analysis. The data mining techniques are applied
    for recommendation to the farmers. Pest Control As a Service This service enables
    the farmers to combat the loss in yield due to pests and rodents. The automated
    fertigation unit is installed at the service provider location. The metering unit
    aids in determining the amount of unit consumed by the farmers for accounting
    and billing. The virtualization technique is applied for efficient utilization
    of resources at the data centers. The virtual private network (VPN) component
    enables the user to have private network for security reasons. These services
    are deployed at the edge device closer to the sensors to minimize the latency.
    In the Fig. 1, end devices could be the mobile, laptop, or the tablet. The services
    could be accessed using a browser or an application. The service response time
    could be greatly enhanced by deploying these on the edge devices closer to the
    sensors. In the next section, the energy efficiency model for fog IoT architecture
    is presented. Fig. 1 Fog-enabled IoT architecture using lightweight protocol Full
    size image 4 Energy Model for the Fog IoT Devices The evolution of Fog computing-enabled
    Internet of Things is at its peak as represented in Gartner’s predictions in the
    recent days. Here, the things connect to the Internet anytime, anywhere providing
    the ubiquity to the users. These things are fueled by energy sources. Hence, energy
    efficient utilization of the IoT resources is an important research domain. With
    the invention of Industrial IoT, the need for low power technology is increasing
    tremendously. Thus, the power consumption model for IoT applications plays an
    important role in network resource optimization. A trade-off is to be achieved
    between cost incurred, network lifetime, and the power consumption of the devices.
    The power efficiency model for IoT devices is presented in [16]. The effect of
    power consumption based on the traffic pattern in IoT devices of smart city applications
    is described in [17]. In an IoT application, energy is consumed during data acquisition,
    processing, and communication. The alternate to save energy is to use energy harvesting
    technologies, which is out of the scope of this paper. The analysis of energy
    in the life cycle of an IoT application is presented here. The objective is to
    minimize the power consumption of the IoT applications. In the literature, system
    level conservation of energy is described vastly. The focus of this paper is mainly
    from the perspective of energy consumption during communication between the devices.
    In an IoT application, the pattern observed is data acquisition by the sensors/things,
    processing performed by the controller, and the transmission of the data. Based
    on this pattern, the power consumption of the IoT application is broken into four
    different blocks. The first block being the power consumed at the system/device
    level (PSYS) to perform the operations at the operating system level. The second
    block being the power consumption during the data acquisition by the sensors/things
    (PDACQ). The third block is power consumed for processing (PPROC) and finally
    the power utilized for communicating between the networking devices (PCOMMN).
    The mathematical representation of the observed pattern in IoT application is
    given in Eq. (1). $$ {P}_{NDEV}={P}_{SYS}+{P}_{DACQ}+{P}_{PROC}+{P}_{COMMN} $$
    (1) With the main objective being the power conservation during communication,
    the lightweight protocol is used in the development of the IoT applications. The
    payload in case of lightweight protocol such as MQTT for IoT applications is 2
    bytes. Since the IoT devices are battery powered, the objective is to minimize
    the power consumed during transmission for increasing the network lifetime. 4.1
    Power Consumption Model for Data Communication The average power consumed during
    communication is represented as energy required to send a message (EM) and the
    time interval between consecutive messages (TM) as given in Eq. (2). $$ {P}_{COMMN}=\\sum
    \\limits_{k=1}^n\\frac{E_M^{(k)}}{T_M^{(k)}} $$ (2) where n is the number of messages
    sent during the experimental period and k varies from 1 to n. The factors that
    influence EM are based on whether it is a wired medium or wireless medium for
    transmission, embedded chip type on the sensor/thing and the period for every
    transmission. The IoT applications can be categorized into two types such as periodic
    and event trigger-based reporting. In case of periodic reporting such as temperature
    monitoring, the value for \\( {T}_{\\textrm{M}}^{(k)} \\) is constant. Therefore,
    Eq. (2) is rewritten as: $$ {P}_{COMMN}=\\frac{1}{T_M^0}\\sum \\limits_{k=1}^n{E}_M^{(k)}
    $$ (3) Here, \\( {T}_M^0 \\) is constant time interval between consecutive reporting
    instances. 4.2 Power Consumption Model for Data Acquisition The IoT applications
    are classified into two types such as periodic reporting and nonperiodic or event
    trigger-based reporting. The power consumption for the data acquisition is given
    by Eq. (4): $$ {P}_{DACQ}=\\left\\{\\begin{array}{c}{P}_{DACQ-1- SAMPLE}\\cdot
    {SN}_1.\\dots (periodic)\\\\ {}{P}_{DACQ-1- SAMPLE}\\cdot {SN}_2\\cdot \\Pr ob(E)\\end{array}\\right.
    $$ (4) where SN1 and SN2 are the number of samples for periodic and event trigger-based
    reporting, respectively. In case of the event trigger-based reporting, the probability
    of occurrence of an event (E) is taken into consideration. 4.3 Power Consumption
    Model of a System/Device The different states of a networking device are active,
    idle/sleep, transmit, and receive. The power consumption of a networking device
    (PSYS) is given as: $$ {P}_{SYS}={P}_{active}+{P}_{sleep/ idle}+{P}_{transmit}+{P}_{receive}
    $$ (5) The power consumption during each of these states is presented in this
    section. Data Transmission The power consumption for transmission of data in an
    IoT environment, considering periodic transmission, is represented as: $$ {E}_{DataTx}={P}_{DataTx}(mW)\\times
    {T}_M^0(ms)\\vspace*{-1.5pc} $$ (6) Data Reception Similarly, the power consumed
    to receive the data is given as: $$ {E}_{DataRx}={P}_{DataRx}(mW)\\times {T}_{Rx}(ms)
    $$ (7) where TRx(ms) is the duration for data reception. Active/Sleep Modes Based
    on the active and sleep modes of the networking device, power consumption is modelled
    as: $$ {P}_{active+ sleep}={P}_s\\times {T}_s+\\left({T}_{total}-{T}_s\\right)\\ast
    {P}_{active} $$ (8) where Ttotal and Ts are the total time spent and time spent
    in sleep mode, respectively. The power consumption of an IoT device can be reduced
    by increasing the sleep modes. Based on the deviation of the sensed data, the
    sleep modes can be increased for prolonged network lifetime. The deviation of
    the sensed data is computed as: $$ Deviation\\mbox{\\_ }of\\mbox{\\_ }sensed\\mbox{\\_}data=\\frac{\\sqrt{\\sum
    \\left({y}_i-\\mu \\right)}}{\\mu } $$ (9) where yi is the sensed data and μ is
    the mean computed for “n” sensed data values. μ is computed as \\( \\sum \\limits_{i=1}^n{y}_i/n
    \\). The data reduction is performed by monitoring the deviation in the data sensed.
    The voluminous data, variety of sensors, and velocity of data generated are the
    aspects to be considered in Fog computing. 4.4 Power Consumption Model for Data
    Processing The power consumed for data processing depends on the number of operations
    performed such as arithmetic operations. It depends on the hardware architecture
    chosen for the deployment of the IoT applications. In this section, the different
    units of power consumption in the IoT applications are outlined. With the proposed
    architecture, the latency for the services is reduced which aids in decreasing
    the power consumed. 5 Distance Between Nodes in Fog IoT Assuming there exists
    “M” routers between the source and destination, the end-to-end delay (dE-to-E)
    is given by Eq. (10), when the queuing delay is negligible: $$ {d}_{E- to-E}=M\\ast
    \\left({d}_{proc}+{d}_{trans}+{d}_{prop}\\ \\right) $$ (10) where dproc is the
    processing delay, dtrans is the transmission delay, and dprop is the propagation
    delay [18]. If the link length is doubled from d to 2d, the propagation delay
    is dprop = 2 * dprop. The propagation delay is proportional to the distance between
    the nodes. Hence, the Fog computing reduces the end-to-end delay when the distance
    between the data gathering nodes and edge devices is reduced. 6 Result Analysis
    The Raspberry Pi 3 is used as the end node at the fog computing layer for aggregation,
    computation, and processing. The lightweight protocol is used for data transmission
    between the nodes. MQTT protocol is chosen as the lightweight protocol for transmission
    of the sensor-captured data. The “publish-subscribe” communication model is applied
    in MQTT. The mosquito broker is utilized to send the sensor-captured data between
    the subscriber (farmers) and publisher (SP). The mosquito broker is configured
    to function as a broker on Raspberry Pi system. An instance is created on broker
    with the port number. The topic with name as “IrrigationFertigation” is created.
    The subscribers connect to the broker using this topic. The message to “ON” the
    irrigation/fertigation unit is sent using this topic. The Wireshark tool is used
    to capture the packet information. The filter is applied to compare the packet
    length during transmission using “http” and “mqtt” protocol. The area covered
    with blue represents the packet length for HTTP, and the red color presents the
    packet length for MQTT as shown in Fig. 2. The other lightweight protocol is CoAP
    (Constrained Application Protocol) used for power-constrained devices. In Fig.
    3, the average, minimum, and maximum packet length of non-lightweight protocol
    – Hyper Text Transfer Protocol (HTTP) – and lightweight protocol (MQTT) is represented.
    In Fig. 4, the round-trip time comparison for Cloud IoT and Fog IoT architecture
    is presented. Fig. 2 Comparison of packet length for HTTP and MQTT protocol Full
    size image Fig. 3 Packet length for lightweight protocol and non-lightweight protocol
    Full size image Fig. 4 RTT comparison of Fog IoT and Cloud IoT architecture Full
    size image 7 Conclusion The Fog-enabled IoT architecture proposed reduces the
    round-trip time as the edge devices are closer to the sensor devices. The case
    study presented benefits the farmer to deploy precision agriculture techniques
    in the farm for increasing the crop yield. This model could be scaled to the entire
    rural area. The lightweight protocol is selected for service providing, because
    it aids in lowering the latency and achieving higher throughput. References S.
    Rajeswari and et al., “A smart agricultural model by integrating IoT, mobile and
    cloud-based big data analytics”, 2017 International Conference on Intelligent
    Computing and Control (I2C2), Coimbatore, 2017, pp. 1–5. Google Scholar   A. Khattab
    and et al., “Design and implementation of a cloud-based IoT scheme for precision
    agriculture,” 2016 28th International Conference on Microelectronics (ICM), Giza,
    2016, pp. 201–204. Google Scholar   S. Roy et al., “IoT, big data science & analytics,
    cloud computing and mobile app based hybrid system for smart agriculture,” 2017
    8th Annual Industrial Automation and Electromechanical Engineering Conference
    (IEMECON), 2017, pp. 303–304, https://doi.org/10.1109/IEMECON.2017.8079610. W.
    R. Heinzelman and et al., “Energy-efficient communication protocol for wireless
    microsensor networks,” Proceedings of the 33rd Annual Hawaii International Conference
    on System Sciences, Maui, HI, USA, 2000, pp. 10 pp. vol.2. Google Scholar   P.
    R. Deshmukh and D. Bhalerao, “An implementation of MQTT through the application
    of warehouse management system for climacteric fruits and vegetables”, 2nd International
    Conference on Communication and Electronics Systems (ICCES), Coimbatore, 2017,
    pp. 844–849. Google Scholar   J. Velez and et al., “IEEE 1451-1-6: Providing common
    network services over MQTT,” 2018 IEEE Sensors Applications Symposium (SAS), Seoul,
    2018, pp. 1–6. Google Scholar   M. M. Raikar and et al., “Blend of Cloud and Internet
    of Things (IoT) in agriculture sector using light weight protocol”, 2018 IEEE
    ICACCI, Bangalore. Google Scholar   M. M. Raikar, P. Desai, M. Vijayalakshmi and
    P. Narayankar, “Augmenting Cloud concepts learning with Open source software environment,”
    2018 International Conference on Advances in Computing, Communications and Informatics
    (ICACCI), Bangalore, 2018, pp. 1405–1411. https://doi.org/10.1109/ICACCI.2018.8554826
    M. M. Raikar, P. Desai, V. M and P. Narayankar, “Upsurge of IoT (Internet of Things)
    in engineering education: A case study,” 2018 International Conference on Advances
    in Computing, Communications and Informatics (ICACCI), Bangalore, 2018, pp. 191–197.
    https://doi.org/10.1109/ICACCI.2018.8554546 N. Kaur and S. K. Sood, “An Energy-Efficient
    Architecture for the Internet of Things (IoT),” in IEEE Systems Journal, vol.
    11, no. 2, pp. 796–805, June 2017. https://doi.org/10.1109/JSYST.2015.2469676.
    Farzad Kiani, “A Survey on Management Frameworks and Open Challenges in IoT,”
    Wireless Communications and Mobile Computing, vol. 2018, Article ID 9857026, 33
    pages, 2018. https://doi.org/10.1155/2018/9857026. A. Zanella, N. Bui, A. Castellani,
    L. Vangelista and M. Zorzi, “Internet of Things for Smart Cities,” in IEEE Internet
    of Things Journal, vol. 1, no. 1, pp. 22–32, Feb. 2014. https://doi.org/10.1109/JIOT.2014.2306328
    Izal, Mikel et al. “Computation of Traffic Time Series for Large Populations of
    IoT Devices” Sensors (Basel, Switzerland) vol. 19, 1 78. 26 Dec. 2018, https://doi.org/10.3390/s19010078.
    M. M. Raikar and S. M. Meena, “SSH brute force attack mitigation in Internet of
    Things (IoT) network: An edge device security measure,” 2021 2nd International
    Conference on Secure Cyber Computing and Communications (ICSCCC), 2021, pp. 72–77,
    https://doi.org/10.1109/ICSCCC51823.2021.9478131. M. M. Raikar and M. S M, “Vulnerability
    assessment of MQTT protocol in Internet of Things (IoT),” 2021 2nd International
    Conference on Secure Cyber Computing and Communications (ICSCCC), 2021, pp. 535–540,
    https://doi.org/10.1109/ICSCCC51823.2021.9478156. B. Martinez, M. Montón, I. Vilajosana
    and J. D. Prades, “The Power of Models: Modeling Power Consumption for IoT Devices,”
    in IEEE Sensors Journal, vol. 15, no. 10, pp. 57775789, Oct. 2015. https://doi.org/10.1109/JSEN.2015.2445094.
    A. Ikpehai, B. Adebisi and K. Anoh, “Effects of Traffic Characteristics on Energy
    Consumption of IoT End Devices in Smart City,” 2018 Global Information Infrastructure
    and Networking Symposium (GIIS), Thessaloniki, Greece, 2018, pp. 1–6. https://doi.org/10.1109/GIIS.2018.8635744.
    Kurose, J. F., & Ross, K. W. (2001). Computer networking: A top-down approach
    featuring the Internet. Boston: Addison-Wesley. Google Scholar   Download references
    Author information Authors and Affiliations K. L. E. Technological University,
    Hubballi, India Meenaxi M. Raikar & Meena S M Corresponding author Correspondence
    to Meenaxi M. Raikar . Editor information Editors and Affiliations Dept. of Computer
    Science & Engineering, Indian Institute of Technology Patna, Patna, Bihar, India
    Rajiv Misra Cardiff University, Cardiff, UK Rana Omer Dept. of EE Engineering,
    University of London, London, UK Muttukrishnan Rajarajan Dept. of ECE, National
    University of Singapore, Singapore, Singapore Bharadwaj Veeravalli Dept. of Computer
    Science, Central University of Rajasthan, Tehsil Kishangarh, Rajasthan, India
    Nishtha Kesswani Dept. of CSE, Indian Institute of Information Technology, Kota,
    Jawahar Lal Nehru Marg, Rajasthan, India Priyanka Mishra Rights and permissions
    Reprints and permissions Copyright information © 2023 The Author(s), under exclusive
    license to Springer Nature Switzerland AG About this paper Cite this paper Raikar,
    M.M., S M, M. (2023). Fog Computing-Enabled Internet of Things for Resource Optimization.
    In: Misra, R., Omer, R., Rajarajan, M., Veeravalli, B., Kesswani, N., Mishra,
    P. (eds) Machine Learning and Big Data Analytics. ICMLBDA 2022. Springer Proceedings
    in Mathematics & Statistics, vol 401. Springer, Cham. https://doi.org/10.1007/978-3-031-15175-0_27
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-15175-0_27
    Published 07 June 2023 Publisher Name Springer, Cham Print ISBN 978-3-031-15174-3
    Online ISBN 978-3-031-15175-0 eBook Packages Mathematics and Statistics Mathematics
    and Statistics (R0) Share this paper Anyone you share the following link with
    will be able to read this content: Get shareable link Provided by the Springer
    Nature SharedIt content-sharing initiative Publish with us Policies and ethics
    Sections Figures References Abstract Introduction State of the Art in Fog-Enabled
    IoT Services Fog Computing-Enabled IoT Architecture Energy Model for the Fog IoT
    Devices Distance Between Nodes in Fog IoT Result Analysis Conclusion References
    Author information Editor information Rights and permissions Copyright information
    About this paper Publish with us Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Springer Proceedings in Mathematics and Statistics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Fog Computing-Enabled Internet of Things for Resource Optimization
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Verma A.
  - Bodade R.
  citation_count: '0'
  description: Agriculture is an essential part of developing countries’ economies.
    Internet of Things (IoT) sensors can provide data about agricultural fields and
    then operate based on human input. The IoT is cloud-based technology that allows
    for scalable management, data management, data security, data analysis, and more.
    The present IoT solutions are not suitable for the Indian agriculture sector due
    to it being very complex and not being cost-effective, which means it is not applicable
    to the reality on the ground. The existing IoT solutions employ paid network GSM/NB-IoT/4G/5G
    for sensor networks. In this chapter, a holistic affordable IoT framework focused
    on the agriculture Indian scenario is proposed. The IoT end nodes are battery-operated,
    and power consumption should be low and adopt low-power wide-area network technology.
    Cloud services included the weather department, agriculture scientists of Indian
    Council of Agricultural Research, e-mandi, the Indian market, the nearest point
    for selling, and so on. It also includes the monitoring of temperature, humidity,
    soil moisture, irrigation and insect and pest detection, actuator intervention,
    message notification for disastrous weather warnings, and expert advice for farmers.
    Crop monitoring via by using machine learning and deep learning algorithms. In
    future work, this framework can adopt edge and fog computing to improve more secure
    data management.
  doi: 10.1201/9781003298335-16
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeComputer ScienceAlgorithms & ComplexityBig Data, Cloud Computing and
    IoTComplete Low-Cost IoT Framework for the Indian Agriculture Sector Chapter Complete
    Low-Cost IoT Framework for the Indian Agriculture Sector ByAshish Verma, Rajesh
    Bodade Book Big Data, Cloud Computing and IoT Edition 1st Edition First Published
    2023 Imprint Chapman and Hall/CRC Pages 14 eBook ISBN 9781003298335 Share ABSTRACT
    Agriculture is an essential part of developing countries’ economies. Internet
    of Things (IoT) sensors can provide data about agricultural fields and then operate
    based on human input. The IoT is cloud-based technology that allows for scalable
    management, data management, data security, data analysis, and more. The present
    IoT solutions are not suitable for the Indian agriculture sector due to it being
    very complex and not being cost-effective, which means it is not applicable to
    the reality on the ground. The existing IoT solutions employ paid network GSM/NB-IoT/4G/5G
    for sensor networks. In this chapter, a holistic affordable IoT framework focused
    on the agriculture Indian scenario is proposed. The IoT end nodes are battery-operated,
    and power consumption should be low and adopt low-power wide-area network technology.
    Cloud services included the weather department, agriculture scientists of Indian
    Council of Agricultural Research, e-mandi, the Indian market, the nearest point
    for selling, and so on. It also includes the monitoring of temperature, humidity,
    soil moisture, irrigation and insect and pest detection, actuator intervention,
    message notification for disastrous weather warnings, and expert advice for farmers.
    Crop monitoring via by using machine learning and deep learning algorithms. In
    future work, this framework can adopt edge and fog computing to improve more secure
    data management. Previous Chapter Your institution has not purchased this content.
    Please get in touch with your librarian to recommend this.  To purchase a print
    version of this book for personal use or request an inspection copy  GO TO ROUTLEDGE.COM  Policies
    Privacy Policy Terms & Conditions Cookie Policy Journals Taylor & Francis Online
    Corporate Taylor & Francis Group Help & Contact Students/Researchers Librarians/Institutions
    Connect with us Registered in England & Wales No. 3099067 5 Howick Place | London
    | SW1P 1WG © 2024 Informa UK Limited"'
  inline_citation: '>'
  journal: 'Big Data, Cloud Computing and IoT: Tools and Applications'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Complete Low-Cost IoT Framework for the Indian Agriculture Sector
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sakthi U.
  - Thangaraj K.
  - Poongothai T.
  - Kirubakaran M.K.
  citation_count: '1'
  description: With the development of the Internet of Things (IoT) and machine learning
    technology, a smart agriculture environment produces more agricultural land and
    crop-associated data for knowledge discovery systems. Machine learning decision-making
    algorithm is applied to discover hidden knowledge patterns from the agricultural
    data stored in the distributed database. Big data analytics extract useful information
    from the large, distributed, and complex datasets, which helps the farmer to increase
    crop yield and quality of the production. The edge computing node collects crop
    data and land environment data from the agricultural lands using a different kind
    of IoT sensors. The predicted smart agricultural knowledge pattern can provide
    needed information to the farmers and other users like an agent, agriculture officers,
    researchers, and producers to get more profit. Cloud and fog computing provides
    efficient distributed data storage for big data and execute dynamic operations
    to predict business intelligence facts to increase production and minimize natural
    resource utilization. We have compared traditional data mining techniques with
    the business analytical tool hybrid association rule-based decision tree (HDAT)
    MapReduce approach for implementing decision tree algorithm to predict and forecast
    the future needs of the farmer to increase the profit and reduce the resource
    wastage.
  doi: 10.1007/978-981-16-9967-2_63
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Smart Trends in Computing
    and Communications Conference paper Big Data Analytics and Machine Learning Approach
    for Smart Agriculture System Using Edge Computing Conference paper First Online:
    06 July 2022 pp 675–682 Cite this conference paper Access provided by University
    of Nebraska-Lincoln Download book PDF Download book EPUB Smart Trends in Computing
    and Communications U. Sakthi , K. Thangaraj , T. Poongothai & M. K. Kirubakaran   Part
    of the book series: Lecture Notes in Networks and Systems ((LNNS,volume 396))
    694 Accesses 1 Citations Abstract With the development of the Internet of Things
    (IoT) and machine learning technology, a smart agriculture environment produces
    more agricultural land and crop-associated data for knowledge discovery systems.
    Machine learning decision-making algorithm is applied to discover hidden knowledge
    patterns from the agricultural data stored in the distributed database. Big data
    analytics extract useful information from the large, distributed, and complex
    datasets, which helps the farmer to increase crop yield and quality of the production.
    The edge computing node collects crop data and land environment data from the
    agricultural lands using a different kind of IoT sensors. The predicted smart
    agricultural knowledge pattern can provide needed information to the farmers and
    other users like an agent, agriculture officers, researchers, and producers to
    get more profit. Cloud and fog computing provides efficient distributed data storage
    for big data and execute dynamic operations to predict business intelligence facts
    to increase production and minimize natural resource utilization. We have compared
    traditional data mining techniques with the business analytical tool hybrid association
    rule-based decision tree (HDAT) MapReduce approach for implementing decision tree
    algorithm to predict and forecast the future needs of the farmer to increase the
    profit and reduce the resource wastage. Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others An Edge-IoT
    Architecture and Regression Techniques Applied to an Agriculture Industry Scenario
    Chapter © 2022 Architecture development with measurement index for agriculture
    decision-making system using internet of things and machine learning Article 15
    March 2023 A Review on Advent of IoT, Cloud, and Machine Learning in Agriculture
    Chapter © 2021 Keywords Precision agriculture Machine learning Edge computing
    1 Introduction A computer-based smart agriculture system collects data from the
    farmer device, IoT sensors, and embedded devices and applies pattern prediction
    algorithm and deliver required information to the farmers and users. The size
    of agriculture data collected by Open Government Data (OGD) Platform India, and
    India Agriculture and Climate Dataset is in the order of gigabytes. The datasets
    are captured and gathered continuously, so the database size is increased by approximately
    92 KB/min in the OGD data repository. The Global Positioning System (GPS) and
    IoT sensors agriculture dataset consists of crop images, weather images, soil
    and productivity data, which are stored in a cloud data repository for data analysis.
    To perform data analysis on large datasets, there is a need for high storage and
    a powerful computing system to provide efficient and effective data patterns to
    the users [1]. Big data analytics and cloud computing have been applied to store
    huge data and perform data analysis in the smart agricultural system [2, 3]. Edge
    computing is integrated with cloud computing and IoT to provide decisions with
    low latency time and increase the cloud network bandwidth [4]. Figure 1 shows
    the smart agricultural model based on IoT and cloud computing technology. Smart
    farming is the integration of IoT and machine learning process to reduce excessive
    use of natural resources and reduce the damage of crops. The proposed system provides
    user-friendly Web-based application to the farmers for acquiring knowledge about
    the farms and the + environment. Big data are applied everywhere to increase the
    profit in the business and to analyze the historical data with the following four
    parameters: volume, velocity, variety, and veracity. Fig. 1 Smart agricultural
    decision-making model Full size image The rest of the paper is organized as follows.
    Section 2 presents related work in a smart agricultural system using machine learning
    and edge computing. The architecture and functionalities of the different layers
    are discussed in Sect. 3. In Sect. 3, the workflow of the proposed system is summarized.
    Section 4 presents the machine learning algorithm used for data classification.
    Section 5 describes the experimental setup, execution method, and analyzes the
    performance of the proposed system. Section 6 concludes the result of the precision
    agricultural system and future scope. 2 Related Works Big data analytics and machine
    learning approach play a significant role in large dataset analysis and efficient
    decision-making process, which makes the smart agricultural system more profitable,
    safer, and efficient [5]. Many applications use big data analytics for achieving
    better profit and make better decisions with less time and cost [6]. Many research
    works have focused on the knowledge intelligent system for monitoring and controlling
    the agricultural land using IoT and fog computing with the cloud environment.
    Edge computing performs data analysis locally without transforming data to the
    cloud environment, which makes the precision agricultural system provide dynamic
    knowledge to the farmers and reduce the network latency. The computational load
    and storage process of the cloud environment are reduced by introducing the edge
    node in the IoT environment [7]. For example, the farmer can get the knowledge
    about the humidity of the soil, and he can decide the level of water irrigation
    to the crops so that the natural resource water can be utilized properly without
    wastage [8]. Intelligent services are provided to the farmers by applying data
    classification and prediction algorithms on the large datasets stored in the data
    repository in the edge node and the decentralized database server [9]. Precision
    agriculture system includes big data analytics, machine learning classification
    algorithm, edge computing, and IoT to increase the profit and reduce the wastage.
    The need for an edge computing nodes in cloud-based environments is analyzed in
    many research works [10]. The intelligent system does not move all dataset to
    the cloud server, which transfers partial data to the cloud server, which reduces
    the network latency and increases the network bandwidth [11]. An effective and
    efficient smart system is developed to overcome the challenges in the traditional
    agricultural land management systems. Many researchers and business applications
    use big data analytics to accomplish great success [12]. A smart agricultural
    system is a modern application technology to provide, compute, and analyze multisource
    agricultural data for different management as shown in Fig. 2. It supports water
    irrigation management, crop management, soil analysis, and fertilizer analysis.
    The Hadoop distributed file system (HDFS) is used to work with a large amount
    of data in distributed methods in the cloud network [13]. Google proposed MapReduce
    programming model for parallel and distributed data analysis approach for large
    datasets in a cloud environment. The important classification algorithm called
    decision tree is implemented using the MapReduce processing model to perform data
    analysis in a distributed manner. Fig. 2 Functionalities of a smart agricultural
    system Full size image 3 Big Data Analytics in Smart Knowledge-Based Agricultural
    System A smart agricultural system includes electronic IoT sensors, edge computing
    transmission technology, big data analytics, and machine learning. The purpose
    of this proposed system is to provide required agricultural land data to the farmers
    and other users without any delay in a precision agricultural environment as shown
    in Fig. 3. A smart agricultural system consists of four layers, which are the
    agricultural environment data collection layer, edge computing layer, big data
    analytics layer, and application layer. All layers are interconnected by the information
    or data flow. The functionalities provided by each layer are given below. Fig.
    3 Layered architecture of the smart agricultural system using big data Full size
    image 3.1 Agricultural Environment Data Collection Layer This layer is responsible
    for collecting data from sources like the sensors deployed in the agricultural
    land, Global Positioning System (GPS), and Roadside Unit (RSU) and transfers those
    data to the next level edge computing layer and cloud server. The data will be
    in the form of photos, text, images, videos, and pictures. 3.2 Edge Computing
    Layer The cloud network connected with the edge computing node via edge gateway
    using the protocols like Zigbee, Bluetooth, Wi-Fi, NFC, and Ethernet protocol.
    The edge computing node performs data analysis on the data collected from the
    data source and sends useful required information to the users. 3.3 Big Data Analytics
    Layer The association rule-based decision tree algorithm is executed to generate
    knowledge patterns for the farmers. The large volume of data is collected from
    the data source and stored in the database server for data analysis. The high
    performance and distributed rule mining algorithm is executed and performs data
    management. 3.4 Application Layer The farmer, agent, researcher, scientist, producer,
    and agent can access the smart agricultural application software using mobile
    devices connected with the Internet. The user can send a query to the knowledge
    base server and can receive information about the crop yield, fertilizer availability,
    pest control, soil details, and water irrigation status. 4 Machine Learning Approach
    for the Precision Agriculture In the big data ecosystem, machine learning technology
    provides powerful agricultural knowledge prediction algorithm to execute on large
    datasets. In Fig. 4, we have explained the steps involved in the big data analysis.
    In this paper, we propose the parallel and distributed association rule mining
    algorithm and tested with large datasets. The hybrid association rule-based decision
    tree algorithm is designed to analyze the distributed datasets available in the
    fog node and cloud computing node. In the supervised machine learning approach,
    decision trees are widely used prediction algorithm for large datasets. The most
    widely used supervised algorithm is tree-based algorithm, which gives greater
    accuracy, easy implementation, and consistency. Fig. 4 System framework for big
    data analytics Full size image 5 Performance Analysis The performance of the proposed
    has been evaluated by recording the farmer query response time for 6 days. The
    query processing time is calculated as the sum of user query request time and
    query response time. Figure 5 shows the various mobile user query response time
    sent on different days to the cloud environment. QID number represents query identification
    number, and it is calculated as the time taken by the cloud application service
    to compute the query and send a response to the farmer user. Fig. 5 User query
    response time Full size image The response can vary based on the type and nature
    of the query sent by the end user. In Fig. 6, the performance of the various approaches
    for generating the knowledge from the large datasets is explained. The proposed
    hybrid association rule-based decision tree-based MapReduce approach gives more
    accuracy and less error than the other data classification approach. The query
    response time can vary based on the user request type and nature of the query.
    Fig. 6 Performance comparisons of various approaches Full size image 6 Conclusion
    and Future Work Integration of cloud computing technologies and machine learning
    approaches swiftly moves traditional agricultural systems to smart agricultural
    systems. Big data analytics system has been applied in a smart agricultural system
    to provide knowledge patterns to the farmers to make real-time decisions about
    farm management. The proposed precision agricultural system is used in the field
    of science, which leads to maximum crop yield by reducing the usage of resources.
    The farm management process is optimized with the use of big data and machine
    learning methods by increasing the crop yield and reducing the resources like
    fertilizer, water, and pesticide. This system provides the required information
    to the farmers at the right time and right place. In future, the system can be
    extended for finding particular crop disease analysis. References S. Wolfert,
    L. Ge, C. Verdouw, Big data in smart farming-a review. Agric. Syst. 153, 69–80
    (2017) Article   Google Scholar   G.B. Kumar, An encyclopedic overview of ‘“big
    data”’ analytics. Int. J. Appl. Eng. Res. 10(3), 5681–5705 (2015) Google Scholar   M.
    Carolan, Publicising food: big data, precision agriculture, and co-experimental
    techniques of addition. Sociol Ruralis 57(2), 135–154 (2017) Article   Google
    Scholar   H.R. Zhang, Z. Li, T. Zou, Overview of agriculture big data research.
    Comput. Sci. 41(S2), 387–392 (2014) Google Scholar   M. Chunqiao, P. Xiaoning,
    M. Yunlong, Research status and development trend of agriculture big data technology.
    J. Anhui Agric. Sci. 44(34), 235–237 (2016) Google Scholar   D. Waga, K. Rabah,
    Environmental conditions’ big data management and cloud computing analytics for
    sustainable agriculture. World J. Comput. Appl. Technol. 2(3), 73–81 (2014) Article   Google
    Scholar   Z.F. Sun, K.M. Du, F.X. Zheng, Perspectives of research and application
    of big data on smart agriculture. J. Agric. Sci. Technol. 15(6), 63–71 (2013)
    Google Scholar   C.S. Nandyala, H.K. Kim, Green IoT agriculture and healthcare
    application (GAHA). Int. J. Smart Home 10(4), 289–300 (2016) Article   Google
    Scholar   A. Kamilaris, A. Kartakoullis, F.X. Prenafeta-Boldu, A review on the
    practice of big data analysis in agriculture. Comput. Electron. Agric. 143, 23–37
    (2017) Article   Google Scholar   A.Z. Abbasi, N. Islam, Z.A. Shaikh, A review
    of wireless sensors and networks’ applications in agriculture. Comput. Stand Interface
    36(2), 263–270 (2014) Article   Google Scholar   J. Lee, S.H. Kim, S.B. Lee, A
    study on the necessity and construction plan of the Internet of things platform
    for smart agriculture. J. Phys. IV 17(11), 1313–1324 (2014) Google Scholar   H.
    Tian, W. Zheng, H. Li, Application status and developing trend of open field water-saving
    internet of things technology. Trans. Chin. Soc. Agric. Eng. 32(21), 1–12 (2016)
    Google Scholar   I. Protopop, A. Shanoyan, Big data and smallholder farmers: big
    data applications in the agri-food supply chain in developing countries. Int.
    Food Agribus. Manage. Rev. 19, 173–190 (2016) Google Scholar   Download references
    Author information Authors and Affiliations Department of Computer Science and
    Engineering, Saveetha School of Engineering, SIMATS, Chennai, Tamil Nadu, 602105,
    India U. Sakthi Department of Information Technology, Sona College of Technology,
    Junction Main Road, Salem, Tamil Nadu, 636005, India K. Thangaraj Department of
    Computer Science and Engineering, St. Martin’s Engineering College, Secunderabad,
    Telangana, 500100, India T. Poongothai Department of Information Technology, St.
    Joseph’s Institute of Technology, Chennai, Tamil Nadu, 600119, India M. K. Kirubakaran
    Corresponding author Correspondence to U. Sakthi . Editor information Editors
    and Affiliations University of Leicester, Leicester, UK Yu-Dong Zhang Faculty
    of Engineering, University of the Ryukyus, Nishihara, Okinawa, Japan Tomonobu
    Senjyu Department of Computer Science, Khon Kaen University, Khon Kaen, Thailand
    Chakchai So-In Global Knowledge Research Foundation, Ahmedabad, Gujarat, India
    Amit Joshi Rights and permissions Reprints and permissions Copyright information
    © 2023 The Author(s), under exclusive license to Springer Nature Singapore Pte
    Ltd. About this paper Cite this paper Sakthi, U., Thangaraj, K., Poongothai, T.,
    Kirubakaran, M.K. (2023). Big Data Analytics and Machine Learning Approach for
    Smart Agriculture System Using Edge Computing. In: Zhang, YD., Senjyu, T., So-In,
    C., Joshi, A. (eds) Smart Trends in Computing and Communications. Lecture Notes
    in Networks and Systems, vol 396. Springer, Singapore. https://doi.org/10.1007/978-981-16-9967-2_63
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-16-9967-2_63
    Published 06 July 2022 Publisher Name Springer, Singapore Print ISBN 978-981-16-9966-5
    Online ISBN 978-981-16-9967-2 eBook Packages Engineering Engineering (R0) Share
    this paper Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Sections Figures References Abstract
    Introduction Related Works Big Data Analytics in Smart Knowledge-Based Agricultural
    System Machine Learning Approach for the Precision Agriculture Performance Analysis
    Conclusion and Future Work References Author information Editor information Rights
    and permissions Copyright information About this paper Publish with us Discover
    content Journals A-Z Books A-Z Publish with us Publish your research Open access
    publishing Products and services Our products Librarians Societies Partners and
    advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress
    Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Big Data Analytics and Machine Learning Approach for Smart Agriculture System
    Using Edge Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ribeiro Junior F.M.
  - Bianchi R.A.C.
  - Prati R.C.
  - Kolehmainen K.
  - Soininen J.P.
  - Kamienski C.A.
  citation_count: '21'
  description: Smart agriculture applications that analyse and manage agricultural
    yield using IoT systems may suffer from intermittent operation due to cloud disconnections
    commonly occurring in rural areas. A fog computing solution enables the IoT system
    to process data faster and deal with intermittent connectivity. However, the fog
    needs to send a high volume of data to the cloud and this can cause link congestion
    with unusable data traffic. Here we propose an approach to collect and store data
    in a fog-based smart agriculture environment and different data reduction methods.
    Sixteen techniques for data reduction are investigated; eight machine learning
    (ML) methods combined with run-length encoding, and eight combined with Huffman
    encoding. Our experiment uses two real data sets, where the first contains air
    temperature and humidity values, and the second has soil moisture and temperature
    conditions. The fog filters cluster the unlabelled data using unsupervised machine
    learning algorithms that group data into categories according to their value ranges
    in all experiments. Supervised learning classification methods are also used to
    predict the class of data samples from these categories. After that, the fog filter
    compresses the identified categories using two data compression techniques, run-length
    encoding (RLE) and the Huffman encoding, preserving the data time series nature.
    Our results reveal that a k-means combined with RLE method achieved the highest
    reduction, where the fog needed to store and transmit only 3%–6% of the original
    data generated by sensors.
  doi: 10.1016/j.biosystemseng.2021.12.021
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Keywords 1. Introduction 2. Theoretical background 3. Related
    work 4. Fog data filter 5. Materials and methods 6. Results 7. Discussion 8. Conclusion
    Funding Declaration of competing interest References Show full outline Cited by
    (21) Figures (17) Show 11 more figures Tables (7) Table 1 Table 2 Table 3 Table
    4 Table 5 Table 6 Show all tables Biosystems Engineering Volume 223, Part B, November
    2022, Pages 142-158 Special Issue: Biosystems and Metrology Research Paper Data
    reduction based on machine learning algorithms for fog computing in IoT smart
    agriculture Author links open overlay panel Franklin M. Ribeiro Junior a b, Reinaldo
    A.C. Bianchi c, Ronaldo C. Prati a, Kari Kolehmainen d, Juha-Pekka Soininen d,
    Carlos A. Kamienski a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.biosystemseng.2021.12.021
    Get rights and content Highlights • It is a challenge to manage a massive amount
    of data generated by sensors in IoT. • Combining machine learning with data compression
    results in a larger data reduction. • Depending on the context, the fog needs
    to decide which classifier should use. Smart agriculture applications that analyse
    and manage agricultural yield using IoT systems may suffer from intermittent operation
    due to cloud disconnections commonly occurring in rural areas. A fog computing
    solution enables the IoT system to process data faster and deal with intermittent
    connectivity. However, the fog needs to send a high volume of data to the cloud
    and this can cause link congestion with unusable data traffic. Here we propose
    an approach to collect and store data in a fog-based smart agriculture environment
    and different data reduction methods. Sixteen techniques for data reduction are
    investigated; eight machine learning (ML) methods combined with run-length encoding,
    and eight combined with Huffman encoding. Our experiment uses two real data sets,
    where the first contains air temperature and humidity values, and the second has
    soil moisture and temperature conditions. The fog filters cluster the unlabelled
    data using unsupervised machine learning algorithms that group data into categories
    according to their value ranges in all experiments. Supervised learning classification
    methods are also used to predict the class of data samples from these categories.
    After that, the fog filter compresses the identified categories using two data
    compression techniques, run-length encoding (RLE) and the Huffman encoding, preserving
    the data time series nature. Our results reveal that a k-means combined with RLE
    method achieved the highest reduction, where the fog needed to store and transmit
    only 3%–6% of the original data generated by sensors. Previous article in issue
    Next article in issue Keywords Smart agricultureInternet of Things (IoT)Data reductionMachine
    learning (ML) 1. Introduction There is an increasing trend for using technological
    advances in artificial intelligence and the Internet of Things (IoT) in agriculture
    to increase production, reduce wastage and costs, and decrease the environmental
    footprint (Togneri et al., 2019). An IoT smart agriculture application can optimise
    production costs or manage the water supply of a farm. For example, an irrigation
    application can make decisions, perceiving the environment by employing sensors
    to collect weather and field-related data, such as rain, air temperature, solar
    radiation, and soil moisture (García et al., 2020; Kamienski et al., 2019). IoT
    systems comprise a distributed infrastructure and handle enormous quantities of
    data that is captured by physical devices (sensors and actuators) (Atzori et al.,
    2017) and transported to a processing place far from the field, usually at a cloud
    data centre (Zyrianoff et al., 2020). Fog computing allows processing and analysing
    data received from sensors locally at the edge of an IoT network (Mouradian et
    al., 2018), thus having local and real-time decision-making and reducing the volume
    of data sent to a cloud server. However, the cloud still needed to centralise
    and analyse historical data since the fog has constrained resources. Therefore,
    an IoT system can overload the communication channel between fog and the cloud
    with an overbearing amount of irrelevant data. A fog-based IoT system can make
    decisions faster in the edge, decrease the amount of data transferred between
    fog and cloud, and decrease the data volume to be processed by the cloud (Gia
    et al., 2015; Li et al., 2015; Mouradian et al., 2018). Despite these benefits,
    it is still challenging to manage a fog data flow concerning data storage, scalability
    and processing power constraints (Yousefpour et al., 2019). This paper proposes
    a fog data reduction approach based on machine learning clustering algorithms
    to collect, filter, store, and transmit data in an IoT smart farming application.
    Five well-known unsupervised learning algorithms, namely, K-Means, Hierarchical
    Tree, Gaussian Mixture Model (GMM), Spectral Clustering, and DBSCAN, were tested
    and combined with two data compression techniques: run-length encoding (RLE) and
    Huffman. Combinations of these algorithms working were analysed as a fog data
    filter to reduce the amount of data stored and transmitted by the fog to the cloud.
    The reason to use an unsupervised machine learning algorithm before manipulating
    the data is that the compression methods can work more efficiently with the clustered
    data. The root causes of the considerable data savings are twofold. First, by
    clustering the data in groups, instead of manipulating raw data composed of two
    or more floating-point variables, the fog manipulates a data stream composed of
    only one short integer, which is the group identification number. Secondly, as
    agricultural-related data tends to be homogeneous and change slowly over time,
    it is likely that the data remain within the same cluster for some time, making
    algorithms that encode groups of similar data - such as the RLE - more efficient.
    Our results reveal that the fog data filter can achieve at least a tenfold reduction
    in the volume of data sent by the fog to the cloud. The primary contributions
    of this paper are twofold. First, by empirical experiments two relevant theoretical
    advantages that fog computing provides to IoT systems are investigated. Secondly,
    it is demonstrated that unsupervised machine learning methods can facilitate the
    achievement of these benefits. This paper is organised as follows: section 2 describes
    the theoretical foundations of this work, and in section 3 related work. Section
    4 details our fog data filter for smart agriculture, and section 5 describes our
    methodology. Section 6 reports on results and they are discussed in section 7.
    Finally, section 8 presents our conclusions and suggestions for future work. 2.
    Theoretical background This section describes the foundations of this study, presenting
    the concepts of fog computing, unsupervised and supervised learning algorithms,
    and data compression techniques. 2.1. Fog computing Fog computing improves latency-sensitive
    systems (Arivazhagan & Natarajan, 2020) because it responds to the actuators faster
    than the cloud and it analyses IoT data in real-time on the network edge (Verma
    et al., 2017; Yousefpour et al., 2019). A fog-based IoT architecture includes
    at least three stages: thing, fog, and cloud stage (Fig. 1). The thing stage contains
    devices, such as sensors and actuators. The sensors perceive the environment and
    send data to the fog, but the actuators receive data (decisions) from the fog
    stage. The fog stage consists of virtualised machines (fog nodes) that store and
    process decisions locally, without an Internet connection, which means the fog
    keeps the IoT system working even during a cloud network disconnection. The cloud
    stage is connected to the fog stage by a gateway over the Internet. This stage
    contains the cloud server and has no hardware constraints like the fog nodes,
    which means the cloud has more processing power than the fog (Chiang & Zhang,
    2016; Mukherjee et al., 2018). Download : Download high-res image (172KB) Download
    : Download full-size image Fig. 1. IoT stages. 2.2. Unsupervised clustering methods
    The clustering problem can be described as the task of dividing a set of N data
    samples into a number of disjoint clusters, without previous knowledge of the
    group structure, i.e., without knowing to which data cluster each sample belongs.
    Because the data used for training the algorithms have not been labelled, classified,
    or categorised by a human teacher beforehand, they are called unsupervised learning
    methods (Hastie et al., 2009). Clustering algorithms work by identifying similarities
    in the original space by maximising some objective function or minimizing a loss
    function. Several approaches are used to capture the structure of the problem:
    using the distance between samples, defining clusters by merging or splitting
    them successively, density-based clustering, or by aggregating points by some
    measure of affinity between them. This section presents five well-known clustering
    algorithms chosen for this work because of their widespread use in machine learning
    problems and also because each one represents a different approach for the grouping
    problem. As four of the algorithms presented here require the number of clusters
    to be specified; two metrics for evaluating clustering algorithms are also presented.
    2.2.1. K-Means The K-Means algorithm (Jain, 2010; MacQueen, 1967) is a distance-based
    method that clusters data by grouping samples in k groups of equal variances,
    which are then represented by the centroid of the group for the prediction of
    new samples. The k-means algorithm, sometimes referred to as Lloyd''s algorithm,
    performs clustering in three steps: First, randomly choose K points in the data
    space, that will be referred to as centroids; then, for each sample, compute the
    distance to each centroid and assign each sample to the centroid that minimises
    the error criterion. Finally, recompute the centroids using the samples assigned
    to each cluster. Repeat steps 2 and 3 until a minimisation criterion is satisfied.
    The pairwise distance between points can be computed in several ways. The most
    common one is the squared Euclidean distance or the Manhattan city-block distance.
    Others such as the Mahalanobis (1936) distance or the Hamming (1950) distance
    for binary data, can be used. The most common criterion to finish the learning
    cycle of this algorithm is the pairwise distance of data points within the same
    cluster, which should be minimised. This criterion, also known as the within-cluster
    sum-of-squares (WCSS), corresponds to the variance of the clusters. Formally,
    given X samples, K clusters C1, …, Ck whose centroid is μ, and the WCSS refers
    (1). (1) The k-means algorithm has many advantages, including that it is simple
    to implement, can be used in a small or large dataset, can be updated incrementally
    and is fast when making a prediction. The main problems of k-means are its dependency
    on the initial values of the centroid. To overcome this, the algorithms are usually
    run several times with different initial centroids and the clusters should be
    convex. 2.2.2. Hierarchical clustering Hierarchical clustering (Johnson, 1967)
    is a connectivity-based method, in which an algorithm groups samples by building
    a tree that defines a cluster hierarchy. The tree is built by merging (agglomerating)
    or splitting (dividing) groups of samples successively, finding a tree in which
    the leaves represent samples of the same cluster. While k-means minimise the variance
    of the clusters, hierarchical clustering aims at finding the best step at each
    cluster fusion or division. The agglomerative hierarchical clustering technique
    first computes the distance between every pair of samples in the data set, using
    any valid measurement. Then, pairs of most similar samples (samples that have
    the smaller distance between them) are linked, creating binary clusters that will
    continue to merge, creating larger clusters that will form a binary, hierarchical
    cluster tree. The last step of the algorithm is pruning: as the number of classes
    is predefined, the bottom of the tree must be pruned, so data points below a threshold
    are all grouped in a single cluster. The main advantage of this approach is that
    any method to compute the pairwise distance between data points can be used. Its
    main problem is that it should be used when the data has a hierarchical structure,
    and due to its high time complexity, it should not be used with large datasets.
    2.2.3. Spectral clustering Spectral clustering (Ng et al., 2001) is a graph-based
    method that uses the spectrum of the data similarity matrix to construct a graph
    that clusters the data. The data spectrum is usually computed by finding the Laplacian
    matrix of the data and then computing its Eigenvectors. The first k Eigenvectors
    are used to define the features of k graph nodes, splitting the graph in k ways,
    used to cluster the data. The advantages of this method are that it performs dimensionality
    reduction before clustering data, making it more general, being able to cluster
    non-convex data, and fast. The main problem is that the algorithm is not advised
    for many clusters or noisy datasets. Also, it clusters one set of data and cannot
    be used to predict the class of a new sample. 2.2.4. Gaussian Mixture Model The
    Gaussian Mixture Model (Reynolds, 2009) (GMM) method is a distribution-based clustering
    that defines groups as sets of samples that most likely belong to the same Gaussian
    distribution. The GMM algorithm works by repeating two steps until it converges.
    First, it finds the probability of membership in each cluster for each point of
    the dataset. Second, it updates the Gaussian function that models each cluster
    using the probability of membership of all data points. The first step is called
    expectation, and the second is called maximisation, being an expectation-maximisation
    based algorithm. The GMM algorithm can be seen as an improvement of the k-means
    clustering that incorporates the covariance structure of the data, and not only
    uses the position of the centroids of the data. In this way, it allows for elliptic
    clusters and not only circular ones. The GMM method is the fastest algorithm for
    learning mixture models, but its drawback is that it does not work well with a
    small dataset, or when there are insufficient points for each Gaussian mixture.
    2.2.5. DBSCAN The density-based spatial clustering of applications with noise
    (Ester et al., 1996) (DBSCAN) is a density-based clustering method that groups
    data by separating areas of higher density separated low-density areas. DBSCAN
    works by grouping together samples that are closely packed together. Samples in
    sparse areas - required to separate clusters - are usually considered noise and
    border points, respectively. DBSCAN uses two parameters: ɛ, which defines the
    vicinity of a point, and the minimum number of points required to form a dense
    region. The algorithm starts with a sample point that has not been visited, and
    its vicinity is analysed: if this vicinity has more than a predefined number of
    points, it is considered as dense and a cluster with all the points in the vicinity
    is defined; otherwise, this point is classified as noise. This process continues
    until the dense cluster is completely found. Then, a new point (that does not
    belong to any cluster) is retrieved, and the process repeats until no points are
    left without a label. The main advantage of this method is that it does not need
    the number of classes to be defined prior to clustering a new set of data. The
    main problems are that the dependency on the ɛ parameter, on the distance function,
    and that it cannot cluster data sets well with very small or very large differences
    in densities. 2.3. Evaluating clustering Given that the knowledge of the ground
    truth class assignments is unknown, unsupervised clustering algorithms cannot
    be evaluated by traditional classification measures that compute the quantity
    of correct and incorrect classification, such as accuracy, precision or recall.
    Therefore, the evaluation of the clustering quality must be done using the model
    itself, where the model results from a clustering process. The most widely known
    method to define the number of classes to be used is called the elbow method (Thorndike,
    1953). In this method, for each number of classes, the algorithm is used to cluster
    the data. Then, the percentage of variance explained by the clusters is computed.
    By plotting the graph of the number of clusters against the explained variance,
    an elbow can be defined, where adding more classes will result in a marginal gain.
    By finding the elbow, one can define the number of classes needed for the clustering.
    Another widely used metric to evaluate a result is the Calinski-Harabasz (CH)
    index (Calinski & Harabasz, 1974), which is defined as variance ratio, i.e., the
    ratio between the overall between-cluster variance and the inter-cluster variance
    for all clusters. By computing the Calinski-Harabasz index, one can verify the
    quality of a clusterisation, where a higher value indicates that a model has better-defined
    clusters. Other methods for clustering evaluation include plotting the silhouette
    of a model (Rousseeuw, 1987) or computing the Davies-Bouldin index (Davies & Bouldin,
    1979). 2.4. Supervised classification algorithms Due to the nature of some clustering
    algorithms, they can only cluster a group of data, and cannot predict to which
    group a new data sample belongs. Both spectral and hierarchical clustering and
    DBSCAN do not perform prediction by themselves: they are used to cluster one dataset,
    creating a clustered dataset (one with the data and the clusters to which they
    belong). To be able to predict to which cluster a new sample belongs, a second
    algorithm is needed. This is a supervised classification algorithm that is used
    to learn, from the clustered training data, the model that will allow to predict
    the class of a new sample. Any classification algorithm can be used, but two algorithms
    are usually selected to perform the prediction stage: the k-NN and the decision
    tree, presented below. 2.4.1. K-nearest neighbours (k-NN) The k-nearest-neighbours
    (k-NN) (Altman, 1992) algorithm is a machine learning algorithm used to classify
    new data samples into categories. It computes the Euclidean distance between the
    values of a new data sample with other previously classified samples to find the
    k samples closer to the new data sample. Thus, the algorithm classifies the new
    data samples, according to the most often k nearest neighbours. The underlying
    assumption is that objects close to each other have a higher probability of belonging
    to the same category. Therefore, this estimation allows us to identify the category
    to which the new data sample belongs. 2.4.2. Decision trees Decision trees (Murthy,
    1998; Quinlan, 1986) can be used to predict the class to which a data sample belongs
    by following the decisions in the tree from the root node down to a leaf node,
    which contains the response. A binary decision tree is a tree where each decision
    creates 2 sub-nodes. To create a decision tree, the algorithms usually work in
    steps, where at each step they select a variable value that best splits the data.
    Several metrics can be used to define the best splitting variable These metrics
    are usually based on information theory, such as information gain and entropy.
    There are several advantages for using decision trees: they are straightforward
    to build, they are understandable because they are close to the way humans make
    decisions, they require little data preparation, and they perform well in large
    datasets. One drawback is that learning the optimal decision tree is known to
    be an NP-complete problem. 2.5. Data compression techniques As in smart agriculture,
    the values collected by sensors can be redundant. Using a compression algorithm
    will allow the fog to maintain a smaller amount of data in the fog database. 2.5.1.
    Run-length encoding (RLE) Run-length encoding (RLE) (Salomon & Motta, 2009) is
    a data compression technique that preserves the order of data values, which makes
    it possible to recover the data in the original order. In other words, when RLE
    is applied in a data time series, it is possible to recover the order of the events
    that occurred in the time series. RLE is a lossless technique that creates, from
    an original dataset, a new dataset in which a sequence of samples with the same
    value is replaced by the value and the number of times the value appears in the
    sequence. 2.5.2. Huffman encoding The Huffman encoding (Huffman, 1952; Salomon
    & Motta, 2009) is an algorithm that creates a variable-length code table to encode
    the symbols of a data source. It used the probability of occurrence of each symbol
    in the source to define codes with fewer bits for the symbols that happen the
    most in the dataset. In this way, more used symbols will be coded with fewer bits
    than symbols that are used less. As the codes used are uniquely decodable, this
    method can be used to create lossless compression that also enables to recover
    of data in the original order they are generated. This kind of encoding is known
    to be optimal for symbol coding if the probability distribution of the symbols
    in the original dataset is known. 3. Related work The use of machine learning
    (ML) in IoT, fog and edge is a challenging problem. The ML algorithms usually
    focus on IoT security (Ahmad & Alsmadi, 2021; Tahsien et al., 2020) and detection
    of intrusion and attacks (Saravanan et al., 2021) or focus on improving the network
    structure, as surveyed by Cui et al. (2018). Some studies reporting benefits of
    fog computing in IoT: e.g. Li et al. (2015) showed that an IoT system''s time
    to respond drops by 73% when fog analyses data locally against a cloud. The fog
    can also reduce the cloud sending data by 93%, as shown in research by (Gia et
    al., 2015). The TW-IoT Framework provides trustworthiness at the data level for
    mist and fog-based IoT systems, proposing a mechanism to deal with data reduction
    using machine learning (Junior and Kamienski, 2021). Li et al. (2018) introduces
    the use of ML for IoT into the edge computing environment to improve learning
    performance as well as to reduce network traffic. Some studies evaluated lossless
    compression techniques to an edge/fog-based IoT system. Gia et al. (2019) assessed
    four lossless algorithms for different edge devices and perceived the Zstandard
    algorithm has a higher compression rate. Spiegel et al. (2018) propose a variation
    of the RLE algorithm to reduce the data and saves energy for IoT devices. Routray
    et al. (2020) presented a comparison between different compression algorithms
    and mentioned which algorithm meets concerns network constraints in an edge-based
    IoT system. However, these studies do not consider real scenarios and device scalability
    since, in real applications, IoT deals with thousands of data. The use of intelligent
    systems for environmental applications is a growing trend, which is receiving
    attention due to the rise of new artificial intelligence (AI) techniques in recent
    years, being the focus of a special issue of the Biosystems Engineering Journal
    recently (Soriano-Disla & Munoz, 2019), with a number of articles related to this
    work: Reza et al. (2019) used the k-means algorithm to estimate rice crop yields
    from low altitude RGB images collected using a rotary-wing type UAV. The k-means
    clustering was combined with a graph-cut algorithm to segment the rice grain areas
    based on colour information. The results from this method can segment the grain
    areas with a relative error of 6% to −33%, a 30% improvement from the previous
    method. Senent-Aparicio et al. (2019) used machine learning for prediction of
    instantaneous peak flow, combining the soil and water assessment tool (SWAT) simulation
    and four time-series AI techniques: artificial neural network (ANN), adaptive
    neuro-fuzzy inference system (ANFIS), support vector machine (SVM) and extreme
    learning machine (ELM). Results showed that the ELM was the best technique for
    this estimation. González García et al. (2019) also used the ELM approach, to
    predict the absorption of pharmaceuticals in reclaimed water-irrigated lettuces
    by analysing effluent. González Perea et al. (2019) also addressed a water related
    problem, using a combination of ANN, Bayesian reasoning and Genetic algorithms
    to forecast daily irrigation water demand with limited data. Finally, Wang et
    al. (2019) proposed an approach based on multivariate timing-random deep belief
    networks for algal bloom prediction accuracy and Supriyanto et al. (2019) made
    use of a multilayer backpropagation neural network for estimating growth of polyculture
    microalgae using 553 datasets collected from semi-continuous open raceway pond.
    Because of the low computational power available in IoT devices, many studies
    have focussed on the efficiency and energy consumption of these algorithms. For
    example, Azar et al. (2019) proposed an energy-efficient approach for IoT data
    collection that made use of an error-bounded lossy compressor on the collected
    data prior to transmission from the device. Then, the transmitted data is rebuilt
    on an edge node and processed using supervised machine learning techniques. Shafique
    et al. (2018) give an overview of designing efficient, reliable, secure and scalable
    machine learning architectures for IoT devices. The role of IoT in agriculture
    is beyond the scope of this literature review, as it has grown enormously in recent
    years, and several reviews are available (Farooq et al., 2019; Sarkar & Chanagala,
    2016; Zhao et al., 2010). Zamora-Izquierdo et al. (2019) developed a low-cost
    hardware and software platform for soilless culture in full recirculation greenhouses.
    The proposed system consists of a three-tier open-source software platform. At
    the local level, it interacts with IoT devices to gather data and perform real-time
    control actions. At the edge level, it monitors precision agriculture tasks to
    increase reliability against network failures. In the cloud, it collects records
    and performs data analytics. IoT technologies such as FIWARE and MQTT are employed,
    and a prototype was tested, allowing farmers to control a hydroponic system through
    the platform. However, the use of IoT combined with ML techniques for agricultural
    problems is a more recent trend. Shamshiri et al. (2020) present an application
    of IoT for model-based evaluation of microclimate parameters inside greenhouse
    crop production systems. The proposed system uses a custom-built wireless sensor
    for data fusion to evaluate comfort ratio values associated with different growth
    stages that uses raw data, including air temperature, relative humidity, vapor
    pressure deficit, and solar radiation. This work also presents a survey on the
    application of IoT-based greenhouse monitoring. Patil and Thorat (2016) developed
    a monitoring system that identifies grape diseases by using a hidden Markov model,
    with temperature, relative humidity, moisture, leaf wetness IoT sensors. As it
    can be seen, ML is not used on the device to reduce the data transmission, as
    in the case of our approach. Despite fog-based data filters can obtain gains,
    it is still challenging to find a solution that can cover different IoT scenarios,
    such as a farm irrigation scenario. Therefore, dealing with fog data-level communication
    is a research challenge (Atlam et al., 2018). 4. Fog data filter Our approach
    uses fog computing in IoT smart agriculture, where sensors collect data and fog
    nodes store, analyse, filter, and finally transmit it to the cloud, as depicted
    by Fig. 2 (Junior and Kamienski, 2021). Our fog data filter focuses on data filtering
    in the fog, reducing data size and traffic between fog and cloud. Download : Download
    high-res image (287KB) Download : Download full-size image Fig. 2. Conceptual
    fog data flow to smart agriculture. The data flow (Fig. 2) works as follows: the
    sensors collect data from a farm parcel and send it to the fog. The data arrives
    in the fog through a network server, transferring the raw data to the data storage
    module and, in turn, to the data analytics module. Next, the data analytics module
    analyses the raw data to make decisions. Then, it stores the decisions in fog
    memory and sends the decisions to actuators to irrigate a crop. Finally, the data
    storage module keeps data in memory, providing access to past decisions and filtered
    data to the analytics module. Furthermore, the data storage module provides the
    raw data periodically to the fog data filter. After a specific period, the fog
    filters the (old) raw data stored, classifies it, deletes this (old) raw data
    from memory and sends the classified data to the cloud via the Message Queuing
    Telemetry Transport (MQTT protocol). Thus, the cloud may need (in the future)
    to perform an analysis with historical classified data. Therefore, the system
    operates processing the data locally, with no required cloud action. In a real
    farm setting, the sensors send data via LoRa packets to the fog, containing air
    temperature, air humidity, and soil moisture. On the other hand, the fog typically
    hosts different software modules, such as data analytics, data storage, data filtering,
    and the Long-Range Wide Area Network (LoRaWAN) server, such as ChirpStack1 (Fig.
    2). The use of LoRaWAN in smart agriculture has many advantages, such as providing
    security to the data communication channel, extending sensor battery lifetime,
    and providing a long-range distance in the transmission between the sensors and
    the fog (Elijah et al., 2018; Mekala & Viswanathan, 2017). The fog data filter
    uses an unsupervised machine learning data clustering algorithm and a data compression
    technique to compress each classified data register in chronological order to
    preserve the time-series nature of the data categories. The choice between the
    machine learning algorithm and the compression algorithm is the subject of this
    paper. The fog filter runs at time intervals initially defined by the application
    developer. Still, the fog may automatically configure this interval depending
    on the context or abrupt variations in the data patterns. With this approach,
    it could be expected that the amount of raw data transmission required would reduce.
    Our fog-based data filter approach has three advantages: (i) creating a new dataset
    for future decision-making based on the characteristics of the categories; (ii)
    decreasing the data traffic between fog and cloud; and (iii) reducing the data
    storage requirements in fog and cloud. The first advantage means the system can
    use a smaller dataset containing only the classes generated by the clustering
    algorithm to, for example, decide the precise amount of water to be spread in
    a crop, depending on the data classification. 5. Materials and methods This section
    presents the experiments conducted to evaluate clustering algorithms and their
    combination with data compression algorithms in three sections: firstly, the experiments
    with clustering algorithms are described; secondly, the algorithms are evaluated;
    thirdly, the data compression methods used in the previously clustered data are
    presented. The fog experiments were carried out using MATLAB release 2021a, executed
    in a MacBook Pro computer with a 2,8 GHz Intel Core i7 Quad-Core processor and
    16 GB 2133 MHz LPDDR3 RAM memory. 5.1. Datasets Two agricultural datasets collected
    in Spain were used in our experiments. • Soria Dataset: Our first dataset has
    30 days of air humidity and temperature collected in Cuerda del Pozo, Soria, Spain
    (Aguilar, 2014), in the Summer of 2013. Each weather record refers to 1 min of
    collected data, which means 1440 samples every day. These data were collected
    using a Vaisala weather transmitter WXT-520 (Aguilar, 2017). • Cartagena Dataset:
    Our second dataset was acquired by us in Cartagena, Spain (Kamienski et al., 2018),
    and consists of 1561 samples acquired with a 5-min interval in the Spring of 2020.
    Samples consist of three measurements of soil temperature and moisture at three
    different depths. Our choice for the Soria and Cartagena datasets aimed at validating
    that our approach independently of weather, location, farm, and metrics. While
    the Soria dataset has air temperature and air humidity, the Cartagena dataset
    had soil temperature and moisture in three soil depths. Also, both datasets cover
    different periods of the year and based in distinct locations. 5.2. Machine learning
    techniques Using the datasets introduced in previous subsection, we executed different
    machine learning classification methods described in Section 2: k-means, hierarchical
    classification, spectral classification, GMM, DBSCAN. As described in section
    2, since some clustering algorithms can only group existing samples of a dataset
    into a cluster and need a second algorithm to learn the model for predicting the
    group of new data samples. In theory, the prediction algorithm can be any supervised
    learning classification method, trained with the dataset resulting from the unsupervised
    clustering algorithm. Out of our five algorithms, spectral, hierarchical and DBSCAN
    clustering do not perform prediction by themselves. We tested the most used classification
    algorithms, namely the binary decision tree, k-NN, linear discriminant (Fisher,
    1936), quadratic Discriminant (Hastie et al., 2009), Naïve Bayes (Lewis, 1998),
    Linear SVM (Cortes & Vapnik, 1995), ensemble of boosted trees (Hastie et al.,
    2009) and an ANN (Haykin, 2009) with 1 hidden layer, 10 hidden units using Rectified
    Linear Unit (ReLU) activation function. The classification algorithms used data
    generated by the tree clustering algorithms. 5.3. Experiments The Soria dataset
    has a training phase for each algorithm covering 20 days of data (i.e., 28,800
    samples) and a testing phase with 10 days (14,400 samples). Samples are normalised
    before clustering, and relevant parameters include: • Replications: each experiment
    is replicated 50 times to ensure the convergence to the optimal clustering, avoiding
    any influence from the initial parameters. • Iterations: the maximum number of
    iterations for the k-means and GMM algorithms is 1000. • Distance: the Squared
    Euclidean method defines the distance between points. After the experiments with
    the Soria Dataset, we applied our fog-based data filter to the Cartagena Dataset
    for verifying the generality of our results, i.e., if the proposed techniques
    apply to different contexts. The Cartagena dataset differs from the previous one
    in two aspects. First, it has a reduced number of samples, and second, it has
    six dimensions for three measurements of soil moisture and temperature in three
    different depths reached by the soil sensor. In these experiments, we used k-means,
    GMM, Hierarchical and spectral clustering for data compression. For Cartagena,
    we do not present results for DBSCAN because the nature and pattern of the data
    made it classify all samples into only one class, regardless of the parameters
    used. 6. Results In this section, the results of the experiments with the Soria
    and Cartagena datasets are presented. 6.1. Clustering experiments: Soria dataset
    For the DBSCAN algorithm, an empirical evaluation was performed to define the
    vicinity of a point ɛ and the minimum number of the neighbours’ points. To estimate
    the value of these parameters, we plot a graph of distance to the kth nearest
    point versus sorted points against this distance. This chart has an elbow, which
    corresponds to the region where points begin to be classified as outliers, as
    depicted by Fig. 3. We can observe that the elbow corresponds approximately to
    ɛ is between 0.1 and 0.2, and the minimum number of neighbours points as 200.
    After some testing, ɛ = 0.1 produced the best results, while ɛ = 0.2 produced
    only one class. Download : Download high-res image (178KB) Download : Download
    full-size image Fig. 3. Elbow method to decide the DBSCAN ɛ parameter. Table 1
    shows the accuracy of classification algorithms for learning the model for spectral,
    hierarchical and DBSCAN clustering (section 5.2). From this evaluation, it was
    decided to use the k-NN and binary decision tree, with an accuracy higher than
    99% and speedy prediction times. Therefore, three clustering algorithms (spectral,
    hierarchical and DBSCAN) were tested using k-NN with k = 1 and Euclidean distance
    and binary tree, giving very similar results. Table 1. Accuracy of classification
    algorithms for learning the model for Spectral and Hierarchical and DBSCAN clustering.
    Trained on the result of clustering of 20 days of data. Results for 10 classes
    - except for DBSCAN that uses 9 classes. Classification Hierarchical Spectral
    DBSCAN Decision Tree 99.3 99.0 97.1 k-Nearest Neighbour 99.8 99.6 97.1 Linear
    Discriminant 89.4 95.3 96.1 Quadratic Discriminant 92.8 97.3 97.6 Naïve Bayes
    94.7 96.3 96.5 Support Vector Machine 93.7 98.3 99.8 Ensemble 94.4 94.8 94.3 Artificial
    Neural Network 95.5 99.0 99.9 Fig. 4, Fig. 5, Fig. 6, Fig. 7 show the result of
    the classification, i.e., the plot of the test dataset labelled with classes (in
    several colours), for 5, 10 and 15 clusters. Figure 8 shows the classification
    results for the DBSCAN algorithm, which is not dependent on a predefined number
    of classes. Download : Download high-res image (935KB) Download : Download full-size
    image Fig. 4. k-means classification for 5, 10 and 15 classes. Download : Download
    high-res image (734KB) Download : Download full-size image Fig. 5. Hierarchical
    Tree classification for 5, 10 and 15 classes. Download : Download high-res image
    (641KB) Download : Download full-size image Fig. 6. Spectral classification for
    5, 10 and 15 classes. Download : Download high-res image (935KB) Download : Download
    full-size image Fig. 7. Gaussian Mixture Models classification for 5, 10 and 15
    classes. Download : Download high-res image (304KB) Download : Download full-size
    image Fig. 8. The result of DBSCAN classification. As it does not use as input
    parameter the number of classes, but this number is decided by the algorithm,
    there is only one figure, with 9 classes, plus red points that were classified
    as outliers. (For interpretation of the references to colour in this figure legend,
    the reader is referred to the Web version of this article.) From Fig. 4, Fig.
    5, Fig. 6 it is possible to see that k-means, hierarchical, and spectral clustering,
    although using different clustering approaches, produce very similar results.
    The main difference is that for k-means, the boundary of the classes is straight
    lines, forming a Voronoi diagram of the points, calculated using the centroids.
    The boundaries are irregular for the hierarchical and spectral clustering, although
    the borders of the spectral clustering tend to be curved. From Fig. 7, Fig. 8,
    it can be seen that neither the GMM nor DBSCAN creates a uniform clustering. 6.1.1.
    Evaluating clustering solutions The clusters created by the five algorithms were
    further analysed using the Elbow Method and the Calinski-Harabasz Index (section
    2). Figure 9 shows the result of the elbow method, where it can be observed that
    the optimal number of classes is near k = 5. Figure 10 shows the results for the
    Calinski-Harabasz index (CH), revealing that for k-means and hierarchical clustering,
    the optimal number of classes is 4, while for spectral clustering, it is around
    10. The results for the GMM show that it is not an adequate model for our fog
    data filter, as the CH values are lower than the values for the other algorithms,
    and it varies, having several peaks and valleys. Download : Download high-res
    image (201KB) Download : Download full-size image Fig. 9. Elbow method to decide
    the number of clusters. Download : Download high-res image (272KB) Download :
    Download full-size image Fig. 10. CH method to decide the number of clusters.
    Another important measure that must be computed to verify the quality of the clustering
    solution, is the percentage error that using one clustering solution introduces
    in the data, after transmission. This measure reflects the fidelity of the transmitted
    data, and it can be computed measuring the distance between one sample point to
    the centre of the cluster that represents it, divided by the value of the sample
    (2). (2) Figure 11 presents the result of computing this error for the two variables
    of the Soria dataset testing phase within 10 days (14,400 samples dataset), using
    the clusters created by the K-means algorithm. As it can be seen, the larger the
    number of clusters, the smaller the error. But it also can be seen that after
    a certain number, the gain in having more clusters is not relevant, and just like
    in Fig. 10, one can see that 10 clusters are a good choice. This figure was made
    using the K-means algorithm, but one can see that the hierarchical tree and spectral
    analysis algorithms will perform in a similar way. DBSCAN and Gaussian Mixtures
    will perform worst, as clusters are less homogeneous. Download : Download high-res
    image (155KB) Download : Download full-size image Fig. 11. Percentage error for
    the K-means algorithm. 6.1.2. Evaluating compression solutions To evaluate the
    two compression algorithms described in section 2, a file was created with 10
    days of clustered data for each clustering algorithm. These files were then split
    into 10 individual files, each for one day''s worth of data and processed by the
    RLE and the Huffman encoding algorithms, resulting in encoded files with reduced
    size. Results presented in Table 2, Table 3 show the mean and standard deviation
    value of 10 codifications. The values show the generated file size compared to
    the original file size, in percentage: that means, as each original data file
    has 11,520 bytes (1440 samples of two 4-byte floating-point values), if the final
    mean has 1152 bytes, the table presents a value of 10%. It is worth noting that
    while the original files had two floating-point values (four bytes) for each sample,
    the compressed file uses two short integers (two bytes) for storing the resulting
    code. Table 2. Result of the use of classifiers and RLE algorithm. The values
    show the size of the generated file compared to the original file size, in percentage
    (%). Results are the mean and standard deviation values of 10 classifications
    (For the DBSCAN algorithm there is only one result for 9 classes + outliers).
    Classifier k = 5 k = 10 k = 15 k-means (2.4 ± 0.7) (3.9 ± 0.9) (5.3 ± 1.6) GMM
    (4.3 ± 2.6) (7.1 ± 3.3) (7.3 ± 2.7) Hierarchical + k-NN (2.5 ± 1.0) (4.8 ± 1.6)
    (6.1 ± 1.9) Hierarchical + Tree (2.4 ± 1.0) (4.7 ± 1.5) (6.1 ± 1.7) Spectral +
    k-NN (2.2 ± 0.6) (4.7 ± 0.9) (6.1 ± 1.5) Spectral + Tree (2.2 ± 0.5) (4.8 ± 1.0)
    (5.8 ± 1.5) DBSCAN + k-NN (5.1 ± 2.13) DBSCAN + Tree (5.5 ± 2.48) Table 3. Result
    of the use of classifiers and Huffman algorithm. The values show the size of the
    generated file compared to the original file size, in percentage (%). Results
    are the mean and standard deviation value of 10 classifications. For the DBSCAN
    algorithm there is only one result for 9 classes + outliers). Classifier k = 5
    k = 10 k = 15 k-means (3.6 ± 0.2) (5.2 ± 0.2) (6.1 ± 0.2) Gaussian MM (3.3 ± 0.2)
    (4.2 ± 0.4) (5.2 ± 0.3) Hierarchical + k-NN (3.6 ± 0.3) (5.1 ± 0.3) (6.0 ± 0.4)
    Hierarchical + Tree (3.6 ± 0.3) (5.2 ± 0.3) (6.0 ± 0.4) Spectral + k-NN (3.4 ±
    0.2) (5.0 ± 0.3) (6.0 ± 0.4) Spectral + Tree (3.4 ± 0.2) (5.0 ± 0.3) (6.0 ± 0.4)
    DBSCAN + k-NN (3.2 ± 0.61) DBSCAN + Tree (3.3 ± 0.65) From Table 2, Table 3, it
    can be seen that a combination of k-means and RLE renders the higher compression
    rate. For k = 5, it produces results similar to Hierarchical and Spectral clustering,
    both using k-NN or Decision Trees. However, it is better for a larger number of
    classes. An ANOVA (Fisher, 1925; Freedman, 2005) statistical test was performed
    on the results in Table 2, and it showed that there were algorithms that have
    significant differences among them for k = 5 and k = 10. For k = 15, the p-value
    was 0.1515, meaning that there was no statistically significant difference among
    them. To complete the statistical analysis, a pairwise Student t-test was performed
    (Freedman, 2005; Gosset, 1908) between all algorithms in Table 2, which have the
    results for the RLE encoding. (For simplicity, only hierarchical, spectral and
    DBSCAN algorithms + Decision Tree, were chosen as the results using k-NN and decision
    tree were identical). The results of this pairwise comparison is presented in
    Table 4, showing the p-value for all the comparisons. A p-value higher than 0.05
    indicates that there is a statistically significant difference between the two
    algorithms. As it can be observed, for k = 5 and k = 10, k-means performs better
    than GMM and DBSCAN, and have the same performance as hierarchical and spectral
    clustering algorithms. For k = 15 there is no statistically significant difference
    among them. Table 4. Student t test pairwise of the use of classifiers and RLE
    algorithm. Classifier k = 5 k = 10 k = 15 k-means vs GMM 0.1172 0.0096 0.2098
    k-means vs Hierarchical 1.0000 0.9068 0.9071 k-means vs Spectral 0.9990 0.8640
    0.9823 k-means vs DBSCAN 0.0089 0.6907 0.9995 GMM vs Hierarchical 0.1172 0.0862
    0.6914 GMM vs Spectral 0.0660 0.1092 0.4883 GMM vs DBSCAN 0.8365 0.2090 0.1377
    Hierarchical vs Spectral 0.9990 1.0000 0.9975 Hierarchical vs DBSCAN 0.0089 0.9923
    0.8133 Spectral vs DBSCAN 0.0042 0.9975 0.9408 Another conclusion deduced from
    both tables is that the RLE encoding performs better than Huffman encoding for
    all algorithms with uniform data distribution. The Huffman encoding produces a
    better result for the GMM and DBSCAN, mainly because the classes have different
    sizes. The Student T-test performed on the results is presented in both tables,
    and for every comparison between RLE and Huffman, the null hypotheses were rejected,
    meaning that results are statistically different. Table 5 shows the time needed
    to predict the data package in milliseconds (ms). This result is for a data package
    with 2880 bytes in a MacBook Pro computer with a 2,8 GHz Intel Core i7 Quad-Core
    processor and 16 GB 2133 MHz LPDDR3 RAM. Even though these values may be lower
    than those obtained using constrained resource hardware in the field, they are
    still helpful to compare the time spent by each algorithm. As expected, the k-means
    algorithm is the fastest one for all numbers of classes. Results are the mean
    value of 10 classifications. Table 5. Prediction time for each classifier (in
    ms). Results are the mean value of 10 classifications. Classifier k = 5 k = 10
    k = 15 k-means 0.07 0.13 0.16 Gaussian MM 0.27 0.38 0.54 Hierarchical + k-NN 78.6
    70.9 68.5 Hierarchical + Tree 0.38 1.54 0.59 Spectral + k-NN 72.8 74.3 68.7 Spectral
    + Tree 0.54 0.34 0.61 DBSCAN + k-NN k = 9 75.5 DBSCAN + Tree k = 9 0.04 6.1.3.
    Evaluating the scalability of the proposed approach The Soria dataset was augmented
    to evaluate the scalability of the proposed approach. Augmentation (van Dyk &
    Meng, 2001) is a common method in data science, in which new data is synthesised
    based on existing real data. In our problem, a new dataset was created with 1000
    sensors, using the Soria dataset as the baseline, and creating new data for the
    sensors by generating 1000 data points every minute in the Soria dataset, following
    a fitted Gaussian distribution, using a random noise with variance equal to 0.1,
    similar as (Ribeiro Junior & Kamienski, 2021). Figure 12 presents the new dataset,
    with 1000 data records per minute. Download : Download high-res image (164KB)
    Download : Download full-size image Fig. 12. The Augmented Soria Dataset, with
    1000 sensors. Using this dataset, the scalability of the approach was verified,
    by measuring the computational costs of learning the clusters, of predicting to
    which cluster one sample belongs and of encoding the clustered results with the
    RLE algorithm. Figure 13 shows the computational cost, i.e., the time the algorithm
    takes to learn how to cluster one set of data using the K-means algorithm, with
    K = 5. The same data described in section 5.3 was used, that is, the training
    phase covers 20 days of data (i.e., 28,800 samples) and a testing phase with 10
    days (14,400 samples). The number of sensors was varied from 100 to 1000. This
    figure presents two lines, one for the learning cost when we do not allow for
    any parallelisation optimisation, which is how it might operate in a low power
    IoT node, and another one that allows the code to make use of Intel processors
    advanced vector extensions (AVE), which exist on every Intel or AMD CPU since
    2011, which are the computers that are likely collect data prior to sending to
    the cloud. By using this vector optimisation, data from the sensors are treated
    simultaneously, leading to a lower computational cost. It can be seen in these
    figures that the time is linear, and as learning is not done frequently, the times
    presented in the figure allows for the learning of data situations of thousands
    of sensors. Download : Download high-res image (147KB) Download : Download full-size
    image Fig. 13. The learning time for the Augmented Soria Dataset, with 1000 sensors.
    Figure 14 shows the prediction time for the same situation: the K-means algorithm
    with 5 classes. The prediction time is the time needed to decide to which cluster
    one sample belongs. As it can be seen, the prediction can also be done in a sequential
    manner and in parallel, using vector instructions. It is possible to see that
    the time also grows in a linear manner with the number of sensors, and that 1000
    sensors can be easily treated during the 60 s that exists between two samples.
    Download : Download high-res image (158KB) Download : Download full-size image
    Fig. 14. Prediction time for the Augmented Soria Dataset, with 1000 sensors. Figure
    15 shows the encoding time using the run-length encoding (RLE) algorithm. It can
    be seen that the encoding time also raises in a linear manner, and that it is
    very low. The time complexity of the RLE solution is O(n), where n is the length
    of the input string. Download : Download high-res image (124KB) Download : Download
    full-size image Fig. 15. The encoding time for the Augmented Soria Dataset, with
    1000 sensors. The use of memory by the K-means algorithm also grows linearly with
    the number of sensors, as to be able to create the clusters, this algorithm uses
    only the data set and a vector of distances to the cluster centre. It is well
    known in literature that this algorithm depends linearly on the number of samples,
    the number of dimensions of the samples and the number of clusters (Hartigan &
    Wong, 1979). Finally, the running time of the K-means algorithm for different
    values of K was established. These values are shown in Fig. 16. As it can be seen,
    the growing number of clusters does not increase the learning time (as it should
    be, because the learning time does not depend on the number of classes). Download
    : Download high-res image (112KB) Download : Download full-size image Fig. 16.
    The learning time for the Augmented Soria Dataset, with 1000 sensors, for different
    number of classes. Finally, in this section only the results using the K-means
    algorithm are presented, as it has been shown in previous results that it is the
    most promising algorithm to be used in the fog filter. But it must be noted that
    the same procedure was carried out with all other algorithms, with very similar
    results. 6.2. Clustering experiments: Cartagena Dataset Figure 17 shows that the
    clusters created by k-means are similar for the three levels of soil depth, clustering
    data into six dimensions. Results for data compression achieved by the algorithms
    are presented in Table 6, Table 7. Download : Download high-res image (418KB)
    Download : Download full-size image Fig. 17. k-means classification for Higher,
    Middle and Lower sensor in probes of Cartagena Dataset. Table 6. Result of the
    use of classifiers and RLE algorithm for the Cartagena Dataset. The values show
    the size of the generated file compared to the original file size, in percentage.
    Results are the mean and standard deviation values of 10 classifications. Classifier
    k = 5 k = 10 k = 15 k-means (3.1 ± 3.0) (2.6 ± 1.5) (3.3 ± 3.0) Gaussian MM (3.6
    ± 2.2) (3.3 ± 2.5) (6.1 ± 5.3) Hierarchical + k-NN (2.8 ± 2.2) (3.1 ± 2.0) (4.2
    ± 3.3) Hierarchical + Tree (3.1 ± 1.7) (6.1 ± 6.0) (3.6 ± 3.7) Spectral + k-NN
    (1.9 ± 1.7) (2.9 ± 2.1) (4.0 ± 3.0) Spectral + Tree (1.4 ± 0.01) (4.4 ± 4.9) (4.3
    ± 3.4) Table 7. Result of the use of classifiers and Huffman algorithm for the
    Cartagena Dataset. The values show the size of the generated file compared to
    the original file, in percentage. Results are the mean and standard deviation
    values of 10 classifications. Classifier k = 5 k = 10 k = 15 k-means (3.3 ± 0.6)
    (5.1 ± 0.6) (6.0 ± 0.7) Gaussian MM (3.2 ± 0.1) (4.8 ± 0.3) (7.2 ± 0.6) Hierarchical
    + k-NN (3.3 ± 0.4) (5.1 ± 0.12) (6.4 ± 0.3) Hierarchical + Tree (3.4 ± 0.4) (5.1
    ± 0.9) (6.4 ± 0.3) Spectral + k-NN (1.6 ± 0.3) (3.8 ± 0.6) (5.2 ± 0.7) Spectral
    + Tree (1.6 ± 0.01) (4.1 ± 0.7) (5.7 ± 0.8) In Fig. 17, it was observed that the
    Cartagena dataset was not as well-defined as the Soria dataset groups because
    the Cartagena dataset had only 1562 records (against 44 thousand records in the
    Soria dataset). Also, Cartagena created different groups for the higher, middle,
    and lower sensor probes. Cartagena has six data dimensions (three for soil moisture
    and three for soil temperature) so that each soil depth can refer to one category.
    In order to facilitate replication of these results and encourage the development
    of similar approaches, the software and datasets used in this paper are available
    for download at: https://github.com/reinaldobianchi/SWAMP. 7. Discussion In this
    study, evidence is presented that with the dataset of Soria, RLE presented a better
    compression than Huffman in all but one situation. It was perceived that for the
    Soria dataset, the data reduction for Huffman and RLE encoding present a statistical
    tie when k = 15 for all classifiers. It was observed that for the Cartagena dataset,
    RLE and Huffman encoding were statistically tied in most cases. However, this
    may happen because the Cartagena dataset contains fewer data records than the
    Soria dataset for the same period, which means RLE joins less similar data. It
    may be evidence that RLE is most suitable for an IoT smart agriculture scenario
    with thousands of sensors. Based on our results, we perceive the K-means algorithm
    combined with RLE as the best filtering method, where the fog needs to store and
    transfer to the fog between 3% and 6% of the data on average. Also, this combination
    executes faster for all numbers of classes. However, it was observed that k-means,
    Hierarchical + k-NN, Hierarchical + Tree, Spectral + k-NN, and Spectral + Tree
    classifiers presented a statistical tie. Also, the Hierarchical Trees and Spectral
    clustering yielded positive results, but with prediction times at least three
    times higher than k-means. GMM and DBSCAN are not suitable for this application
    because the clusters created do not represent a homogeneous division of the data
    or a grouping representing any characteristic from the real world. DBSCAN has
    a more severe problem, as it classifies as outliers a large portion of the dataset.
    Based on the fact that GMM rely on clusters being described as Gaussians, which
    is not the case of our data, it is not a surprise that it did not provide a reasonable
    classification. An important finding was that DBSCAN also did not result in an
    adequate model: it is believed that this density-based algorithm would detect
    areas with more points and thus separate the results into more real-world, meaningful
    classes. It was noted that despite RLE having better data reduction, the different
    classifiers present statistical ties, indicating that the use of a classifier
    as a fog data filter may depend on input data. Therefore, depending on the context,
    the fog needs to analyse which classifier should combine with RLE autonomously.
    Our work reveals the feasibility of reducing the amount of data in agriculture
    using machine learning in fog computing. However, our approach is a lossy data
    reduction approach and thus cannot recover the original data, if required, in
    some scenarios. The cloud also can execute our data reduction approach because
    fog computing has computational constraints to train data models. However, executing
    it on fog allows decreasing network traffic to the cloud and bandwidth costs.
    In an agriculture scenario, many sensors transmit similar air temperature, soil
    moisture, and humidity values. Our experiments show that our fog data filter reduces
    these redundant values into a category that represents them. Therefore, our fog
    data filter solution for agriculture addresses the challenges of managing a massive
    amount of data generated by sensors in IoT. Finally, our approach should reduce
    data for any agricultural dataset because it converges sensor measurements into
    categories. 8. Conclusion This paper proposes and evaluates a data reduction approach
    for fog computing in a smart agriculture context. A fog data filter solution was
    introduced to reduce the amount of data that the fog stores and transfers to the
    cloud. Our solution was evaluated using eight machine learning methods combined
    with run-length-encoding and Huffman encoding in two datasets. This revealed that
    k-means combined with RLE achieves the highest reduction, where the fog needs
    to store and transmit 3%–6% of the original data generated by sensors. As future
    work, we intend to evaluate these data reduction methods in a mist-fog-based IoT
    system, considering the mist and fog computational resources constraints. It is
    also planned to perform these experiments in a large-scale scenario where the
    fog receives data from thousands of sensors every hour, using a greenhouse context
    and different weather conditions. Funding This work was jointly funded by the
    European Commission in Europe and MCTIC/RNP in Brazil [grant number EUB-02-2017
    IoT Pilots call]; by the São Paulo Research Foundation - FAPESP [grant numbers
    2018/25225-9, 2019/07665-4]; by the Federal Institute of Education, Science and
    Technology of Maranhão (IFMA). Declaration of competing interest The authors declare
    that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. References
    Aguilar, 2014 F. Aguilar Press, temperature and humidity august 2013 Dryad, dataset
    (2014), 10.15146/R3730 Google Scholar Aguilar, 2017 F. Aguilar Data management
    in a cloud Framework: Application to the LifeWatch ESFRI PhD Thesis (2017), pp.
    16-20 available in: https://digital.csic.es/handle/10261/157765 View in ScopusGoogle
    Scholar Ahmad and Alsmadi, 2021 R. Ahmad, I. Alsmadi Machine learning approaches
    to IoT security: A systematic literature review Internet of Things, 14 (2021),
    p. 100365, 10.1016/j.iot.2021.100365 View PDFView articleView in ScopusGoogle
    Scholar Altman, 1992 N.S. Altman An introduction to kernel and nearest-neighbor
    nonparametric regression The American Statistician, 46 (1992), pp. 175-185, 10.1080/00031305.1992.10475879
    View in ScopusGoogle Scholar Arivazhagan and Natarajan, 2020 C. Arivazhagan, V.
    Natarajan A survey on fog computing paradigms, challenges and opportunities in
    iot 2020 international conference on communication and signal processing, ICCSP)
    (2020), pp. 385-389, 10.1109/ICCSP48568.2020.9182229 View in ScopusGoogle Scholar
    Atlam et al., 2018 H.F. Atlam, R.J. Walters, G.B. Wills Fog computing and the
    internet of things: A review Big Data and Cognitive Computing, 2 (2018), 10.3390/bdcc2020010
    Google Scholar Atzori et al., 2017 L. Atzori, A. Iera, G. Morabito Understanding
    the internet of things: Definition, potentials, and societal role of a fast evolving
    paradigm Ad Hoc Networks, 56 (2017), pp. 122-140, 10.1016/j.adhoc.2016.12.004
    View PDFView articleView in ScopusGoogle Scholar Azar et al., 2019 J. Azar, A.
    Makhoul, M. Barhamgi, R. Couturier An energy efficient IoT data compression approach
    for edge machine learning Future Generation Computer Systems, 96 (2019), pp. 168-175,
    10.1016/j.future.2019.02.005 View PDFView articleView in ScopusGoogle Scholar
    Calinski and Harabasz, 1974 T. Calinski, J. Harabasz A dendrite method for cluster
    analysis Communications in Statistics, 3 (1974), pp. 1-27, 10.1080/03610927408827101
    View in ScopusGoogle Scholar Chiang and Zhang, 2016 M. Chiang, T. Zhang Fog and
    iot: An overview of research opportunities IEEE Internet of Things Journal, 3
    (2016), pp. 854-864, 10.1109/JIOT.2016.2584538 View in ScopusGoogle Scholar Cortes
    and Vapnik, 1995 C. Cortes, V. Vapnik Support vector networks Machine Learning,
    20 (1995), pp. 273-297 View in ScopusGoogle Scholar Cui et al., 2018 L. Cui, S.
    Yang, F. Chen, Z. Ming, N. Lu, J. Qin A survey on application of machine learning
    for Internet of Things International Journal Machineries Learning Cybernetics,
    9 (2018), pp. 1399-1417, 10.1007/s13042-018-0834-5 View in ScopusGoogle Scholar
    Davies and Bouldin, 1979 D.L. Davies, D.W. Bouldin A cluster separation measure
    IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-, 1 (1979),
    pp. 224-227, 10.1109/TPAMI.1979.4766909 View in ScopusGoogle Scholar Elijah et
    al., 2018 O. Elijah, T.A. Rahman, I. Orikumhi, C.Y. Leow, M.N. Hindia An overview
    of internet of things (iot) and data analytics in agriculture: Benefits and challenges
    IEEE Internet of Things Journal, 5 (2018), pp. 3758-3773, 10.1109/JIOT.2018.2844296
    View in ScopusGoogle Scholar Ester et al., 1996 M. Ester, H.P. Kriegel, J. Sander,
    X. Xu A density-based algorithm for discovering clusters in large spatial databases
    with noise Proceedings of the second international conference on knowledge discovery
    and data mining, AAAI Press (1996), pp. 226-231 Google Scholar Farooq et al.,
    2019 M.S. Farooq, S. Riaz, A. Abid, K. Abid, M.A. Naeem A survey on the role of
    iot in agriculture for the implementation of smart farming IEEE Access, 7 (2019),
    pp. 156237-156271, 10.1109/ACCESS.2019.2949703 View in ScopusGoogle Scholar Fisher,
    1925 R. Fisher Statistical methods for research workers Edinburgh Oliver & Boyd
    (1925) Google Scholar Fisher, 1936 R.A. Fisher The use of multiple measurements
    in taxonomic problems Annals of Eugenics, 7 (1936), pp. 179-188, 10.1111/j.1469-1809.1936.tb02137.x
    Google Scholar Freedman, 2005 D. Freedman Statistical models: Theory and practice
    Cambridge University Press (2005) Google Scholar García et al., 2020 L. García,
    L. Parra, J.M. Jimenez, J. Lloret, P. Lorenz Iot-based smart irrigation systems:
    An overview on the recent trends on sensors and IoT systems for irrigation in
    precision agriculture Sensors, 20 (2020), 10.3390/s20041042 Google Scholar Gia
    et al., 2015 T.N. Gia, M. Jiang, A. Rahmani, T. Westerlund, P. Liljeberg, H. Tenhunen
    Fog computing in healthcare internet of things: A case study on ecg feature extraction
    2015 IEEE international conference on computer and information Technology; ubiquitous
    computing and communications; dependable, autonomic and secure computing, Pervasive
    Intelligence and Computing (2015), pp. 356-363, 10.1109/CIT/IUCC/DASC/PICOM.2015.51
    View in ScopusGoogle Scholar Gia et al., 2019 T.N. Gia, L. Qingqing, J.P. Queralta,
    H. Tenhunen, Z. Zou, T. Westerlund Lossless compression techniques in edge computing
    for mission-critical applications in the iot 2019 twelfth international conference
    on mobile computing and ubiquitous network, ICMU (2019), pp. 1-2, 10.23919/ICMU48249.2019.9006647
    Google Scholar González García et al., 2019 M. González García, C. Fernandez-López,
    A. Bueno-Crespo, R. Martínez-España Extreme learning machine-based prediction
    of uptake of pharmaceuticals in reclaimed water-irrigated lettuces in the region
    of murcia, Spain Biosystems Engineering, 177 (2019), pp. 78-89, 10.1016/j.biosystemseng.2018.09.006
    View PDFView articleView in ScopusGoogle Scholar González Perea et al., 2019 R.
    González Perea, E. Camacho Poyato, P. Montesinos, J.A. Rodríguez Díaz Optimisation
    of water demand forecasting by artificial intelligence with short data sets Biosystems
    Engineering, 177 (2019), pp. 59-66, 10.1016/j.biosystemseng.2018.03.011 View PDFView
    articleView in ScopusGoogle Scholar Gosset, 1908 W.S. Gosset The probable error
    of a mean Biometrika, 6 (1908), pp. 1-25, 10.2307/2331554 (originally published
    under the pseudonym “Student”) Google Scholar Hamming, 1950 R.W. Hamming Error
    detecting and error correcting codes The Bell System Technical Journal, 29 (1950),
    pp. 147-160, 10.1002/j.1538-7305.1950.tb00463.x Google Scholar Hartigan and Wong,
    1979 J.A. Hartigan, M.A. Wong Algorithm as 136: A k-means clustering algorithm
    Journal of the Royal Statistical Society: Series A C., 28 (1) (1979), pp. 100-108
    JSTOR 2346830 CrossRefView in ScopusGoogle Scholar Hastie et al., 2009 T. Hastie,
    R. Tibshirani, J. Friedman The elements of statistical learning: Data mining,
    inference, and prediction. Springer series in statistics Springer (2009) Google
    Scholar Haykin, 2009 S.S. Haykin Neural networks and learning machines (3rd ed.),
    Pearson Education, Upper Saddle River, NJ (2009) Google Scholar Huffman, 1952
    D.A. Huffman A method for the construction of minimum-redundancy codes Proceedings
    of the IRE, 40 (1952), pp. 1098-1101, 10.1109/JRPROC.1952.273898 View in ScopusGoogle
    Scholar Jain, 2010 A.K. Jain Data clustering: 50 years beyond k-means Pattern
    Recognition Letters, 31 (2010), pp. 651-666, 10.1016/j.patrec.2009.09.011 View
    PDFView articleView in ScopusGoogle Scholar Johnson, 1967 S.C. Johnson Hierarchical
    clustering schemes Psychometrika, 32 (1967), pp. 241-254 View in ScopusGoogle
    Scholar Junior and Kamienski, 2021 F.M.R. Junior, C.A. Kamienski A survey on trustworthiness
    for the internet of things IEEE Access, 9 (2021), pp. 42493-42514, 10.1109/ACCESS.2021.3066457
    View in ScopusGoogle Scholar Kamienski et al., 2019 C. Kamienski, J.P. Soininen,
    M. Taumberger, R. Dantas, A. Toscano, T. Salmon Cinotti, R. Filev Maia, A. Torre
    Neto Smart water management platform: Iot-based precision irrigation for agriculture
    Sensors, 19 (2019), p. 276 CrossRefView in ScopusGoogle Scholar Kamienski et al.,
    2018 C. Kamienski, J. Soininen, M. Taumberger, S. Fernandes, A. Toscano, T.S.
    Cinotti, R.F. Maia, A.T. Neto Swamp: An iot-based smart water management platform
    for precision irrigation in agriculture 2018 global Internet of things summit,
    GIoTS (2018), pp. 1-6, 10.1109/GIOTS.2018.8534541 View in ScopusGoogle Scholar
    Lewis, 1998 D.D. Lewis Naive bayes at forty: The independence assumption in information
    retrieval European conference on machine learning, Springer (1998), pp. 4-15 View
    in ScopusGoogle Scholar Li et al., 2015 J. Li, J. Jin, D. Yuan, M. Palaniswami,
    K. Moessner Ehopes: Data-centered fog platform for smart living 2015 international
    telecommunication networks and applications conference, ITNAC (2015), pp. 308-313,
    10.1109/ATNAC.2015.7366831 View in ScopusGoogle Scholar Li et al., 2018 H. Li,
    K. Ota, M. Dong Learning IoT in edge: Deep learning for the internet of things
    with edge computing IEEE Network, 32 (2018), pp. 96-101, 10.1109/MNET.2018.1700202
    Google Scholar MacQueen, 1967 J.B. MacQueen Some methods for classification and
    analysis of multi-variate observations L.M.L. Cam, J. Neyman (Eds.), Proc. Of
    the fifth berkeley symposium on mathematical statistics and probability, University
    of California Press (1967), pp. 281-297 Google Scholar Mahalanobis, 1936 P.C.
    Mahalanobis On the generalised distance in statistics Proceedings of the National
    Institute of Sciences of India, 2 (1936), p. 49—55 URL: http://ir.isical.ac.in/dspace/handle/1/1268
    Google Scholar Mekala and Viswanathan, 2017 M.S. Mekala, P. Viswanathan A survey:
    Smart agriculture iot with cloud computing 2017 international conference on microelectronic
    devices, circuits and systems, ICMDCS (2017), pp. 1-7, 10.1109/ICMDCS.2017.821155
    View in ScopusGoogle Scholar Mouradian et al., 2018 C. Mouradian, D. Naboulsi,
    S. Yangui, R.H. Glitho, M.J. Morrow, P.A. Polakos A comprehensive survey on fog
    computing: State-of-the-art and research challenges IEEE Communications Surveys
    Tutorials, 20 (2018), pp. 416-464, 10.1109/COMST.2017.2771153 View in ScopusGoogle
    Scholar Mukherjee et al., 2018 M. Mukherjee, L. Shu, D. Wang Survey of fog computing:
    Fundamental, network applications, and research challenges IEEE Communications
    Surveys Tutorials, 20 (2018), pp. 1826-1857, 10.1109/COMST.2018.2814571 View in
    ScopusGoogle Scholar Murthy, 1998 S.K. Murthy Automatic construction of decision
    trees from data: A multi-disciplinary survey Data Mining and Knowledge Discovery,
    2 (1998), pp. 345-389, 10.1023/A:1009744630224 URL: View in ScopusGoogle Scholar
    Ng et al., 2001 A. Ng, M. Jordan, Y. Weiss On spectral clustering: Analysis and
    an algorithm Advances in Neural Information Processing Systems, 14 (2001), pp.
    849-856 Google Scholar Patil and Thorat, 2016 S.S. Patil, S.A. Thorat Early detection
    of grapes diseases using machine learning and iot 2016 second international conference
    on cognitive computing and information processing, CCIP (2016), pp. 1-5, 10.1109/CCIP.2016.7802887
    Google Scholar Quinlan, 1986 J.R. Quinlan Induction of decision trees Machine
    Learning, 1 (1986), pp. 81-106, 10.1023/A:1022643204877 URL: View in ScopusGoogle
    Scholar Reynolds, 2009 D.A. Reynolds Gaussian mixture models Encyclopedia of Biometrics,
    741 (2009), pp. 659-663 CrossRefGoogle Scholar Reza et al., 2019 M.N. Reza, I.S.
    Na, S.W. Baek, K.H. Lee Rice yield estimation based on k-means clustering with
    graph-cut segmentation using low-altitude uav images Biosystems Engineering, 177
    (2019), pp. 109-121, 10.1016/j.biosystemseng.2018.09.014 View PDFView articleView
    in ScopusGoogle Scholar Ribeiro Junior and Kamienski, 2021 F.M. Ribeiro Junior,
    C.A. Kamienski Data resilience system for fog computing Computer Networks, 195
    (2021), p. 108218, 10.1016/j.comnet.2021.108218 ISSN 1389-1286 View PDFView articleView
    in ScopusGoogle Scholar Rousseeuw, 1987 P. Rousseeuw Silhouettes: A graphical
    aid to the interpretation and validation of cluster analysis Journal of Computational
    and Applied Mathematics, 20 (1987), pp. 53-65 View PDFView articleView in ScopusGoogle
    Scholar Routray et al., 2020 S.K. Routray, A. Javali, A. Sahoo, W. Semunigus,
    M. Pappa Lossless compression techniques for low bandwidth io ts 2020 Fourth international
    conference on I-SMAC (IoT in social, mobile, analytics and cloud) (I-SMAC) (2020),
    pp. 177-181, 10.1109/I-SMAC49090.2020.924345 View in ScopusGoogle Scholar Salomon
    and Motta, 2009 D. Salomon, G. Motta Handbook of data compression (5th ed.), Springer
    Publishing Company, Incorporated (2009) Google Scholar Saravanan et al., 2021
    L. Saravanan, H. Sharma, K. Sreenivasulu, M. Deivakani Detection of software intrusion
    based on machine learning techniques for iot systems Materials Today Proceedings
    (2021), 10.1016/j.matpr.2021.03.138 Google Scholar Sarkar and Chanagala, 2016
    P.J. Sarkar, S. Chanagala A survey on iot based digital agriculture monitoring
    system and their impact on optimal utilization of resources Journal of Electronics
    and Communication Engineering (IOSR-JECE), 11 (2016), pp. 1-4 CrossRefGoogle Scholar
    Senent-Aparicio et al., 2019 J. Senent-Aparicio, P. Jimeno-Sáez, A. Bueno-Crespo,
    J. Pérez-Sánchez, D. Pulido-Velázquez Coupling machine-learning techniques with
    swat model for instantaneous peak flow prediction Biosystems Engineering, 177
    (2019), pp. 67-77, 10.1016/j.biosystemseng.2018.04.022 View PDFView articleView
    in ScopusGoogle Scholar Shafique et al., 2018 M. Shafique, T. Theocharides, C.S.
    Bouganis, M.A. Hanif, F. Khalid, R. Hafız, S. Rehman An overview of next-generation
    architectures for machine learning: Roadmap, opportunities and challenges in the
    iot era 2018 design, automation test in Europe conference exhibition (DATE) (2018),
    pp. 827-832, 10.23919/DATE.2018.8342120 View in ScopusGoogle Scholar Shamshiri
    et al., 2020 R.R. Shamshiri, I. Bojic, E. van Henten, S.K. Balasundram, V. Dworak,
    M. Sultan, C. Weltzien Model-based evaluation of greenhouse microclimate using
    iot-sensor data fusion for energy efficient crop production Journal of Cleaner
    Production, 263 (2020), p. 121303, 10.1016/j.jclepro.2020.121303 View PDFView
    articleView in ScopusGoogle Scholar Soriano-Disla and Munoz, 2019 J.M. Soriano-Disla,
    A. Munoz Special issue: Intelligent systems for environmental applications Biosystems
    Engineering, 177 (2019), pp. 1-3, 10.1016/j.biosystemseng.2018.11.016 View PDFView
    articleView in ScopusGoogle Scholar Spiegel et al., 2018 J. Spiegel, P. Wira,
    G. Hermann A comparative experimental study of lossless compression algorithms
    for enhancing energy efficiency in smart meters 2018 IEEE 16th international conference
    on industrial informatics, INDIN (2018), pp. 447-452, 10.1109/INDIN.2018.8471921
    View in ScopusGoogle Scholar Supriyanto et al., 2019 Supriyanto, R. Noguchi, T.
    Ahamed, D.S. Rani, K. Sakurai, M.A. Nasution, D.S. Wibawa, M. Demura, M.M. Watanabe
    Artificial neural networks model for estimating growth of polyculture microalgae
    in an open raceway pond Biosystems Engineering, 177 (2019), pp. 122-129, 10.1016/j.biosystemseng.2018.10.002
    View PDFView articleView in ScopusGoogle Scholar Tahsien et al., 2020 S.M. Tahsien,
    H. Karimipour, P. Spachos Machine learning based solutions for security of internet
    of things (iot): A survey Journal of Network and Computer Applications, 161 (2020),
    p. 102630, 10.1016/j.jnca.2020.102630 View PDFView articleView in ScopusGoogle
    Scholar Thorndike, 1953 R.L. Thorndike Who belongs in the family Psychometrika
    (1953), pp. 267-276 View in ScopusGoogle Scholar Togneri et al., 2019 R. Togneri,
    C. Kamienski, R. Dantas, R. Prati, A. Toscano, J.P. Soininen, T.S. Conic Advancing
    iot-based smart irrigation IEEE Internet of Things Magazine, 2 (2019), pp. 20-25
    CrossRefGoogle Scholar van Dyk and Meng, 2001 D.A. van Dyk, X.-L. Meng The art
    of data augmentation Journal of Computational & Graphical Statistics, 10 (1) (2001),
    pp. 1-50 Google Scholar Verma et al., 2017 S. Verma, Y. Kawamoto, Z.M. Fadlullah,
    H. Nishiyama, N. Kato A survey on network methodologies for real-time analytics
    of massive iot data and open research issues IEEE Communications Surveys Tutorials,
    19 (2017), pp. 1457-1477, 10.1109/COMST.2017.2694469 View in ScopusGoogle Scholar
    Wang et al., 2019 L. Wang, T. Zhang, X. Wang, X. Jin, J. Xu, J. Yu, H. Zhang,
    Z. Zhao An approach of improved multivariate timing-random deep belief net modelling
    for algal bloom prediction Biosystems Engineering, 177 (2019), pp. 130-138, 10.1016/j.biosystemseng.2018.09.005
    View PDFView articleView in ScopusGoogle Scholar Yousefpour et al., 2019 A. Yousefpour,
    C. Fung, T. Nguyen, K. Kadiyala, F. Jalali, A. Niakanlahiji, J. Kong, J.P. Jue
    All one needs to know about fog computing and related edge computing paradigms:
    A complete survey Journal of Systems Architecture, 98 (2019), pp. 289-330, 10.1016/j.sysarc.2019.02.009
    View PDFView articleView in ScopusGoogle Scholar Zamora-Izquierdo et al., 2019
    M.A. Zamora-Izquierdo, J. Santa, J.A. Martinez, V. Martinez, A.F. Skarmeta Smart
    farming iot platform based on edge and cloud computing Biosystems Engineering,
    177 (2019), pp. 4-17, 10.1016/j.biosystemseng.2018.10.014 View PDFView articleView
    in ScopusGoogle Scholar Zhao et al., 2010 J.C. Zhao, J.F. Zhang, Y. Feng, J.X.
    Guo The study and application of the iot technology in agriculture 2010 3rd international
    conference on computer science and information Technology (2010), pp. 462-465,
    10.1109/ICCSIT.2010.5565120 View in ScopusGoogle Scholar Zyrianoff et al., 2020
    I. Zyrianoff, A. Heideker, D. Silva, J. Kleinschmidt, J.P. Soininen, T. Salmon
    Cinotti, C. Kamienski Architecting and deploying iot smart applications: A performance–oriented
    approach Sensors, 20 (2020), p. 84 View in ScopusGoogle Scholar Cited by (21)
    Federated learning for performance behavior detection in a fog-IoT system 2024,
    Internet of Things (Netherlands) Show abstract Review of technology advances to
    assess rice quality traits and consumer perception 2023, Food Research International
    Show abstract Internet of agriculture: Analyzing and predicting tractor ride comfort
    through supervised machine learning 2023, Engineering Applications of Artificial
    Intelligence Show abstract Data reduction in fog computing and internet of things:
    A systematic literature survey 2022, Internet of Things (Netherlands) Citation
    Excerpt : The Lack of implementation and evaluation details of the proposed idea
    is one of the shortcomings of this research. Junior et al. [30] presented a fog-based
    filtering scheme to reduce IoT data transmission from fog to cloud. In the provided
    model, the authors have combined the machine learning clustering methods with
    compression algorithms to reduce the amount of collected raw data from Smart Agriculture
    IoT sensors. Show abstract Technological revolutions in smart farming: Current
    trends, challenges &amp; future directions 2022, Computers and Electronics in
    Agriculture Show abstract Transient Data Caching Based on Maximum Entropy Actor–Critic
    in Internet-of-Things Networks 2024, International Journal of Computational Intelligence
    Systems View all citing articles on Scopus 1 https://www.chirpstack.io/. View
    Abstract © 2022 IAgrE. Published by Elsevier Ltd. All rights reserved. Part of
    special issue New advances in measurement and data processing techniques for Agriculture,
    Food and Environment. Edited by Annachiara Berardinelli, Davide Brunelli, Chiara
    Cevoli View special issue Recommended articles Article Metrics Citations Citation
    Indexes: 17 Captures Readers: 81 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Biosystems Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Data reduction based on machine learning algorithms for fog computing in
    IoT smart agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Tripathy N.
  - Tripathy S.S.
  - Rath M.
  - Swain J.
  citation_count: '0'
  description: The Internet of Things (IoT) has transformed smart agriculture by increasing
    efficiency and lowering production costs in addition to boosting productivity
    and optimizing resource use. This article outlines the future of automation and
    emphasizes the possibilities of sensors and IoT in the field of greenhouse farming.
    Through a variety of sensors, the various parameters like humidity, pH and EC
    value, temperature, UV light intensity, and CO2 level are tracked in order to
    provide valuable insight into early fault detection and diagnosis. A growing computing
    technique to enhance and support cloud computing is called fog computing. Fog
    computing platforms include a number of features that enable delivery services
    to users more quickly and improve the Quality of Service (QoS) of IoT devices,
    such as being close to edge users, being an open platform, and supporting mobility.
    As a result, it is turning into a crucial strategy for user-centered IoT-based
    applications. The core operating system that directs and oversees all of the operations
    is a Decision Support System (DSS) in the Fog layer described in this article.
    The paper also discusses the various difficulties associated with greenhouse farming
    and spotlights a novel, smart, and sustainable IoT-Fog solution. This article's
    model is highly suited to the changing environment, redefining sustainability
    in the process.
  doi: 10.1109/CINE56307.2022.10037339
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 5th International Confer...
    An IoT Assisted Fog Enabled Framework for Smart Green House Publisher: IEEE Cite
    This PDF Niva Tripathy; Subhranshu Sekhar Tripathy; Mamata Rath; Jhum Swain All
    Authors 81 Full Text Views Abstract Document Sections I. Introduction II. Literature
    Survey III. Proposed Model IV. Proposed Algorithm V. Result and Discussion Show
    Full Outline Authors Figures References Keywords Metrics Abstract: The Internet
    of Things (IoT) has transformed smart agriculture by increasing efficiency and
    lowering production costs in addition to boosting productivity and optimizing
    resource use. This article outlines the future of automation and emphasizes the
    possibilities of sensors and IoT in the field of greenhouse farming. Through a
    variety of sensors, the various parameters like humidity, pH and EC value, temperature,
    UV light intensity, and CO2 level are tracked in order to provide valuable insight
    into early fault detection and diagnosis. A growing computing technique to enhance
    and support cloud computing is called fog computing. Fog computing platforms include
    a number of features that enable delivery services to users more quickly and improve
    the Quality of Service (QoS) of IoT devices, such as being close to edge users,
    being an open platform, and supporting mobility. As a result, it is turning into
    a crucial strategy for user-centered IoT-based applications. The core operating
    system that directs and oversees all of the operations is a Decision Support System
    (DSS) in the Fog layer described in this article. The paper also discusses the
    various difficulties associated with greenhouse farming and spotlights a novel,
    smart, and sustainable IoT-Fog solution. This article''s model is highly suited
    to the changing environment, redefining sustainability in the process. Published
    in: 2022 5th International Conference on Computational Intelligence and Networks
    (CINE) Date of Conference: 01-03 December 2022 Date Added to IEEE Xplore: 09 February
    2023 ISBN Information: DOI: 10.1109/CINE56307.2022.10037339 Publisher: IEEE Conference
    Location: Bhubaneswar, India SECTION I. Introduction Agriculture has undergone
    numerous revolutions over the years, whether it be an expansion of modern agriculture
    or an upgrade in farming methods. The green revolution movement has been fueled
    by technological advancements in agriculture. The Green Revolution has given farmers
    the chance to learn about and implement scientific farming practices, reducing
    the need for human labor and embracing technology. Due to its reliance on data,
    greenhouse farming has emerged as the secret to smart and sustainable agriculture.
    Agriculture has been altered by data-centric farming by being more exact and accurate,
    which has consolidated the entire farming process. The development and feeding
    of the plants are threatened by a number of trivial agricultural practices. The
    obstacles that farmers face throughout production include the availability of
    labor, accuracy in the percentage of disease diagnosis, decision-making on irrigation
    intervals, and feeding the plants the right amount of nutrients and pesticides.
    Environmental factors like humidity, soil moisture, C02 level, etc. greatly influence
    plant growth. In an open setting, it is impossible to simultaneously monitor and
    manipulate all the factors. However, an excellent substitute method that increases
    crop output and balances the variables is greenhouse farming. The procedure of
    this protected cultivation begins with seedlings, saplings, plantations, etc.
    It may be noticed that plants grow roughly twice as quickly under greenhouse culture
    than they do in the fields. Plants grown in greenhouses encounter a number of
    difficulties, such as the right balance of nutrients accessible to the plant,
    fluctuating temperatures and humidity, proper soil and moisture monitoring, disease
    identification and prevention, and periodic data collection. Despite the fact
    that these issues are covered in the literature [1], [2], it involves human resources
    and lacks accurate data With the aid of distributed architectures, loT offers
    a substantial way to connect a sizable number of smart and embedded devices, creating
    new options to create new applications without constraints. Presenting farmers
    with the right information in a timely manner so they may decide on their investments
    more wisely and quickly is one of the issues that are well-known in the agriculture
    industry. Fog computing is a distributed platform that provides loT devices and
    the Cloud computing storage area with execution, storage, and communication services.
    Since it offers numerous supporting and beneficial qualities including edge position,
    position awareness, and tiny response delay, the fog layer is currently regarded
    as a significant extension of the cloud. The primary purpose of the Fog was to
    provide services for end-user devices, like real-time applications. An loT Fog-based
    infrastructure is suggested in this article as a potential solution to the issues
    raised. The solution attempts to reduce the need for labor and increase the automation
    of farming. Our goal in designing a fully automated smart agriculture system for
    greenhouse farming is to maximize productivity while minimizing the need for physical
    labor. Our model is designed to be less expensive, have a successful risk management
    strategy, and be more precision-driven in its decision-making. Our model is intelligent,
    environmentally friendly, and data-centric, which means it relies on the data
    and continuously learns from it to provide a better user experience. In summary,
    our strategy will unquestionably support current farming practices and promote
    better growth and development. The suggested loT-based system is in charge of
    resource allocation, resource optimization, activity coordination, time management,
    human effort management, information gathering, and decision-making utilizing
    data analytics. The following is how the rest of the work is structured: It has
    discussed some previous works done in this regard under Section 2. Section 3 will
    describe the proposed model for the greenhouse. The proposed algorithm has been
    given in section 4. Section 5 includes performance analyses along with comparisons
    of outcomes Section 6 wraps up the project with a summary of results and suggestions
    for the following research SECTION II. Literature Survey IoT is making the physical
    and digital worlds interconnected, making our surroundings smarter and more adaptable
    [3]–[6]. Agriculture will gain the most from loT monitoring and automation because
    it can be done remotely from anywhere else in the world, eliminating the need
    for humans. By 2050, it is expected that farming would employ loT to enhance food
    output by 70 percent and feed up to 9.6 billion people, with 2 billion sensors
    being used on 525 million farms. In the field of smart farming, various investigations
    and extensive studies have been carried out [4]. Numerous options exist for loT
    to benefit agriculture. Farm owners may obtain access to a wealth of analytical
    data, including temperature, fertilizer, and water usage, by deploying sensors
    and actuators throughout farms and equipment. loT is the engine that drives loT
    - based smart agriculture. It is focused on sensors and statistics [1 of base
    paper]. Mostly in the field of intelligent farming, numerous studies and extensive
    studies have been carried out. loT is the engine that drives automated smart farming;
    it is focused on sensors and statistics [1]. Gutierrez et al. [14] suggested an
    loT-based irrigation system made up of a WSN interconnected to a remote server
    by a gateway. That work also distributed soil moisture and temperature sensors
    in addition to effective water use. Smart sensor it was suggested on the Web [16]
    to use the implanted sensors to assess soil moisture and operate them via a Web-based
    system should keep an eye on the humidity, temp, and pH. A video-sensing-based
    fertilizer control system for agricultural productivity was suggested by Cambra
    et al. [15]. [7] Suggests an autonomous approach for detecting plant diseases.
    Equivalent work is done by [8], which suggests an interconnected farm as a service
    (FaaS) platform and uses loT to monitor meteorological and strawberry plant growth
    data. [17] Develops an intelligent security and monitoring system built on the
    Internet of Things. In order to supervise and control surveillance systems in
    farmlands from a distance, cold storage and rainstorms are maintained there. In
    [13], a system for assessing soil moisture was created using an energy-efficient
    approach. Precision agriculture is another highly developed kind of agriculture
    that is used in modern agriculture. A crop yield is improved by utilizing loT,
    cloud computing, and edge computing. Cyber-Physical System is used to communicate
    with and control the crops in real-time (CPS) [18]. Most approaches, however,
    primarily take into account short-range technologies installed in a location with
    local Internet connectivity. Agriculture problems are still difficult to solve
    because farms and agriculture are mainly found in rural areas, making interconnecting
    the short-range WSN from a distance difficult. Furthermore, it is crucial to provide
    QoS for such an application in order to satisfy various network needs. from the
    literature survey, it is found that the traditional agriculture system has several
    problems associated with them. Adopting an automated system of smart agriculture
    wherein the involvement of manual labor is minimized and productivity is maximized.
    SECTION III. Proposed Model According to the literature review, the main components
    of plant and flower growth in a greenhouse are proper control of environmental
    temperature, humidity, soil and environmental wetness, environment C02 level,
    water level, and UV light density. Along with these factors, the right delivery
    of nutrients'' fertilizers, and medications as well as the visual assessment of
    the health of the leaf and flower are equally important because they ensure the
    plant''s well-being and productivity. The proper calculation of temperature and
    humidity values, the accurate monitoring of the soil and its moisture, the information
    gathering at regular intervals, etc., are some of the issues associated with monitoring
    and maintaining these parameters. These processes need to be automated as much
    as feasible in order to tackle the aforementioned issues. We propose an loT fog-based
    architecture as a technique of automated greenhouse farming to accomplish this.
    The recommended framework can promote plant growth in a comfortable environment
    by enabling soil monitoring, automatic environment optimization, optimal watering,
    in-the-moment problem diagnosis, decreased harvest failure, remote controlling
    and management, and a decrease in operating costs. The proposed framework is divided
    into three elements conceptually: IoT layer: In this layer, millions of data are
    gathered from various sources. loT is to allow humans and objects to connect altogether
    beyond time and place. Greenhouse farming data are collected by using different
    sensors in the loT layer. Later these data are shared to the next layer i.e Fog
    layer for efficient data processing and recourse management. Fog layer: This layer
    consists of three different systems 1) Data Acquisition system (DAS): collect
    real-time data from the sensor deployed in loT layer 2) Decision Support System
    (DSS): A decision support system is used to monitor and manage the resources,
    and3) Central Actuator Manager (CAM): A set of equipment for performing the different
    task of the greenhouse and this equipment is handled by a set of actuators. Cloud
    layer: In this layer, final computed information is shared from the fog layer
    for more analysis and storage purposes for future processing. Data Acquisition
    system (DAS) Fig. 1. Architecture of lot fog based smart greenhouse farming Show
    All In the DAS in case of any variation, an automation system is required to handle
    it immediately. For example, if the C02 level is above the upper limit (900 ppm)
    then it needs to open the ventilators and if the C02 level is below the lower
    limit (800 ppm) it needs to switch on the C02 cylinder. Similarly, actions can
    be taken for other parameters, when the light intensity is not within the range
    of 440lx - 670lx then the rooftop cover of the greenhouse can be uncovered or
    covered respectively, and the dehumidifier can be set on / off when the ground
    humidity is not in, the range of 55% - 65%, the warm air is activated when the
    temperature reaches below 170C and the air cooler may be activated when the temperature
    reaches above 28oC. In any case of any irregularity detected in the greenhouse
    plant, appropriate actions will be taken and resolved by applying different machine
    learning methods. A Support Vector Machine (SVM) based classifier is used for
    any deviation detection in the greenhouse plant. From the data set 70 % of the
    data is used for training and 30 percent the data is used for testing. An overall
    classification accuracy of 91% is obtained on training data and 85% classification
    accuracy on test data. This will lead to the activation of different equipment
    in greenhouse farming at the appropriate time. Upon receiving the data captured
    through the sensors and applying the machine learning algorithms, appropriate
    action is suggested by the DSS. The DSS will take proper action. Upon receiving
    the data, the DAS sends it to the DSS. Further, the DSS analyses the data and
    takes appropriate action through an actuator management system. Decision Support
    System (DSS) The DSS acts as the main operating system of our proposed loT Fog-based
    smart greenhouse farming model. Broadly, the DSS governs six major components
    of the proposed model, namely: (i) a Rule-based designed engine, ii) Central Actuator
    Manager (CAM), iii) machine learning models; (iv) experts or agronomists; (v)
    greenhouse workers; and (vi) Data data repository. Here the DSS not only acts
    as a resource allocator but also as a resource optimizer. Data fetched periodically
    from the data server are given as input to the rule-based engine. The data received
    may be regular data for which the rules are predefined by the DSS and accordingly,
    the instructions are sent from the DSS to the CAM. For example, starting the water
    pump when the water level is below the defined range, and starting of the dehumidifier
    in case the humidity level is more than the thresh hold. However, the data received
    may not be regular, like changes in the climate condition, flowers/leaves affected
    by the disease, or any other abnormal conditions for which no clear predefined
    rules exist. These data are passed to a machine learning module to obtain the
    required solution. But, in some instances, it might happen that the machine learning
    module fails to find a solution (for a new disease or for a change in climate).
    In this case, the information is automatically passed to an expert or agronomist,
    who uses the information received from the DSS and data from the repository to
    suggest the correct decision. So, the DSS acts here as a service provider. The
    CAM after getting the instruction activates the required actuator to take action.
    In addition to this, based on the information available through visual sensors,
    the DSS analyzes and prescribes the appropriate amount and time interval for supplying
    water, nutrients, and/or pesticides to the greenhouse plants. The correct proportion
    of the nutrients is decided by the expert/agronomist by looking at the plant''s
    health, climatic conditions, soil parameters, temperature, humidity, and plant
    disease (if any). The values can be set manually through the dashboard in the
    greenhouse based on user requirements. It is to be noted that the medicines will
    not be sprayed over all the plants; instead, they will only be sprayed over the
    plant diagnosed with the disease and the plants which fall within a radial distance
    of 1m. The following system eases the traditional problem of covering all plants
    and helps feed the plants with medicines at the prescribed time intervals. The
    DSS also provides an interface for communication and interaction with human actors
    such as greenhouse workers, supervisors, and experts through app notifications
    and SMS as and when required. The supervisor also observes the current status
    of a plant or the nearby plants using the optical CMOS magnifier. The magnifier
    takes the values of the plant''s row and column as input and helps in monitoring
    the plant''s status by picturing it. The supervisor monitors the status of all
    sensors and actuators through a dashboard on a screen in real-time using an interactive
    interface. Decisions such as changing the status of some of the actuators or selecting
    the number of insecticides or pesticides to be applied need to be taken by the
    supervisor. Although the automation of greenhouse farming requires less human
    labor, some groundwork, such as planting, weeding of grass, bending, cleaning,
    de-budding, loosening the soil, etc. requires human involvement. These are carried
    out by greenhouse workers. Some of these jobs need to be done periodically, and
    some are event-driven (such as unwanted grass is observed by a visual sensor and
    grass cutting task needs to be activated). The jobs of the greenhouse workers
    are controlled, scheduled, and actively monitored by the DSS. The remainder of
    the same is given by the DSS and the supervisor of the greenhouse periodically
    updates this status to the DSS. The DSS also provides an interface for communication
    and interaction with human actors such as greenhouse workers, supervisors, and
    experts. Actuator Management System To automate the equipment, parameters received,
    such as water level, C02, mister, humidity, temperature, and UV light, are associated
    to separate actuators for every action. Further, all actuators are connected to
    a Central Actuator Manager (CAM) (Refer Fig. 3). The CAM takes instructions from
    the DSS and act accordingly by activating or deactivating, or controlling the
    actuators. The action is taken immediately. This action may be a i) Rule based
    action defined earlier or ii) Action suggested by machine learning models, and
    iii) Expert advice (expert advice are recorded in the repository). In case of
    rule-based action, the CAM automatically activates / deactivates the actuator
    based on the rules decided by the DSS or by the agronomist a priori. However,
    in case of expert advice, the decision on actuators to be activated is made by
    considering the suggestions from DSS, previous outcomes in similar situations,
    and the experience of the manager. The mister and dehumidifier can also be controlled
    (start / stop) through this dashboard only. The dashboard also shows the amount
    of nutrients and water as per the suggestion of DSS. It also helps in observing
    the current status of a plant or the nearby plants using the optical CMOS magnifier.
    The magnifier takes the values of the plant''s row and column as input and help
    in monitoring the plant''s status by picturing it. The dashboard takes numerous
    inputs and upon clicking on the submit button, it makes the necessary changes
    and sends the data to the repository for further use. Once, there is enough labeled
    data available, the agronomist will use a machine-learning approach to improve
    the accuracy of the results. SECTION IV. Proposed Algorithm The major benefits
    that can be gained from the proposed loT-enabled fog-assisted green farming are
    a significant reduction in the cost of production; a drop-in death rate of plants,
    the accuracy of detecting diseases in plants, a quick response time to take remedies;
    and the effective use of resources like minerals fertilizers by minimizing the
    over or under dose. Among the above, cost and precision of accuracy for detecting
    diseases in plants are two major advantages, as they highly affect the business.
    We have proposed an SVM machine learning algorithm to give a comparative analysis.
    The proposed algorithm runs SVM to calculate the accuracy, error rate, and cost
    of loT-enabled fog-assisted green farming. Fig. 2. Flow chat of lot fog-based
    smart greenhouse farming Show All SECTION Algorithm 1: Proposed Algorithm Input:
    Xtrain, Xtest Output: Accuracy, ErrorRate,Cost 1 Select the optimal value of cost
    gamma for SVM 2 while(stopping condition is not met) 3 do 4 Implement SVM train
    step for each data point 5 Implement SVM classification for testing data points
    6 end while 7 Return Accuracy, Error rate, Cost SECTION V. Result and Discussion
    The prototyping of the proposed automation model was implemented in a sub-part
    of a 1000sqft area of the original greenhouse of 2.5 Acres. The recorded result
    for a year is analyzed. The various costs involved in a greenhouse plantation
    can be categorized as the cost for labor, installation of hardware for automation,
    regular periodic check-up, maintenance, and recurrent cost. From this we can conclude
    that: i) The cost involved in labor using the proposed IoT-based fog computing
    framework is greatly reduced because of the automation for various operational
    units. ii) The cost involved in installing the hardware for the proposed loT-based
    fog computing framework is quite large but it is almost zero in the trivial method.
    However, this cost is only a one-time investment. iii) For the rest three costs
    regular periodic check-ups, maintenance, and recurrent cost, the proposed loT-based
    fog computing method incur quite less as compared to its counterpart. Fig. 3.
    Accuracy comparison graph Show All Fig 3 represents the accuracy comparison of
    trivial methods and proposed loT-based fog computing framework. It has been observed
    that the proposed loT-based fog computing framework gives better accuracy with
    an increase in the number of samples. This is because with the increase in samples,
    the data set becomes rich and the learning rate increases, yielding better accuracy
    and prediction for detecting diseases in green house plants. Fig. 4. Estimated
    cost graph Show All Fig 4 represents the estimated cost comparison between trivial
    methods and the proposed loT-based fog computing framework. over time, the cost
    involved in the proposed 1oT-based fog computing framework is quite less as compared
    to the cost involved in the trivial method. Although the 1oT-based fog computing
    proposed approach incurs some cost in maintenance from time to time when required
    but still the total cost is significantly less than the trivial method involving
    human labor. SECTION VI. Conclusion The loT-enabled fog computing automated system
    described in the paper helps in increasing the productivity of plants inside the
    greenhouse along with cutting unnecessary costs. The investment done in these
    loT devices is also not very large as compared to the expenditure through the
    manual process. The loT-enabled fog computing process not only gives accurate
    pieces of information but also lowers the burden of manual work by automation.
    It mitigates the traditional load of data gathering, refining, and determining
    the accuracy percentage. With technology in hand, the new methods of making a
    field or greenhouse sustainable are far better and a lot more accessible. MyGreen
    can be integrated with other sustainable loT solutions to build a stronger backbone
    for smart cities and smart villages [6]. Authors Figures References Keywords Metrics
    More Like This Quality of Service (QoS) in Internet of Things 2018 3rd International
    Conference On Internet of Things: Smart Innovation and Usages (IoT-SIU) Published:
    2018 A Fog Computing Framework for Quality of Service Optimisation in the Internet
    of Things (IoT) Ecosystem 2020 2nd International Multidisciplinary Information
    Technology and Engineering Conference (IMITEC) Published: 2020 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings - International Conference on Computational Intelligence and
    Networks
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An IoT Assisted Fog Enabled Framework for Smart Green House
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nayak O.
  - Lachure J.
  - Doriya R.
  citation_count: '3'
  description: A cyber-physical system (CPS) is a system that can make decisions independently;
    this system is controlled or monitored by a computer-based algorithm. In CPS,
    devices are susceptible points for attackers, which makes the systems vulnerable
    to attacks. CPS can remotely control the network sensors, actuators and gates,
    and other physical devices. As a result, these systems are also used in critical
    infrastructures such as water distribution systems (WDS), power grids, and other
    similar systems. WDS is the large infrastructure that can deliver an adequate
    amount of water to industry, home, and agriculture areas. However, in recent years
    several cyber-attacks have been increased over WDS. In this paper, we proposed,
    that fog computing and fuzzy logic-based methods (Intuitionistic Fuzzy System)
    for feature selection and voting classifier are applied over the selected features
    to detect anomalies over WDS. The proposed system is tested with the (WADI) Water
    Distribution dataset, and the results are compared the state-of-the-art methods.
  doi: 10.1109/STPES54845.2022.10006594
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 1st International Confer...
    Fog Enabled Cyber-Physical Attack detection using Ensemble Machine Learning Publisher:
    IEEE Cite This PDF Omprakash Nayak; Jaykumar Lachure; Rajesh Doriya All Authors
    2 Cites in Papers 68 Full Text Views Abstract Document Sections I. Introduction
    II. Literature Review III. Case Study on the Wadi IV. Proposed Model V. Result
    and Discussion Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: A cyber-physical system (CPS) is a system that can make decisions
    independently; this system is controlled or monitored by a computer-based algorithm.
    In CPS, devices are susceptible points for attackers, which makes the systems
    vulnerable to attacks. CPS can remotely control the network sensors, actuators
    and gates, and other physical devices. As a result, these systems are also used
    in critical infrastructures such as water distribution systems (WDS), power grids,
    and other similar systems. WDS is the large infrastructure that can deliver an
    adequate amount of water to industry, home, and agriculture areas. However, in
    recent years several cyber-attacks have been increased over WDS. In this paper,
    we proposed, that fog computing and fuzzy logic-based methods (Intuitionistic
    Fuzzy System) for feature selection and voting classifier are applied over the
    selected features to detect anomalies over WDS. The proposed system is tested
    with the (WADI) Water Distribution dataset, and the results are compared the state-of-the-art
    methods. Published in: 2022 1st International Conference on Sustainable Technology
    for Power and Energy Systems (STPES) Date of Conference: 04-06 July 2022 Date
    Added to IEEE Xplore: 16 January 2023 ISBN Information: DOI: 10.1109/STPES54845.2022.10006594
    Publisher: IEEE Conference Location: SRINAGAR, India SECTION I. Introduction Water
    plays a vital role in the development and economy of a country. The economy and
    development of any country depend on its agriculture, business, and industries,
    in which water is used the most. But it can also be used as a weapon. By controlling
    the WDS of a country, we can destroy its core. As time progresses, the importance
    of WDS is increasing. With the growing importance of WDS, its security is also
    becoming a matter of concern. Water Distribution System (WDS) is the main urban
    component and is a critical infrastructure. It supplies water of desired quality
    and quantity to consumers. Water distribution systems include different components
    such as storage tank, valve, pump, and hydrant. It also uses the CPS to control
    all its activity. A CPS is an integrated system of the software system and physical
    infrastructure, which includes both physical devices and software (intrusion detection
    system). A cyber-physical system is an intelligent computer system that uses various
    computer-based algorithms to monitor and control different components of any critical
    infrastructure. Therefore, the popularity of CPS is rapidly growing in critical
    infrastructure systems (e.g., water distribution systems). CPS consists of actuators,
    sensors, controllers, and physical processes. CPS comprises many controllers,
    e.g., programmable logic control (PLCs) and Remote terminal unit (RTUs). These
    controllers relate to control software for calculating control action. The CPS
    comprises several numbers of PLCs to control the entire process of the Water distribution
    center. Figure 1. Key component of CPS adapted from [1] Show All A. A Cyber-Physical
    Attack on Wds WDS has also embraced the flexibility of CPS, where digital computation
    and networking are an integrated part of the physical process control system.
    WDS being a complex system, it is likely to have many vulnerabilities, using which
    the attacker can easily attack the system and take all the processes. Improved
    water distribution systems will be of great benefit to the development of metropolitan
    areas in the future. Consequently, the focus of this study is on the detection
    of cyber-physical attacks on water distribution centers as well as the connection
    between the water distribution Centre system and the cyber layer Thus, turning
    the WDS into a CPS. The Cyber layer remotely controls and monitors the physical
    layer of WDS. In case of anomalies, it enables predictive control and early-warning
    systems to be implemented. Center on computer networks is the root cause of this
    trend, which enables attackers to easily enter this network using traditional
    techniques and take control of Supervisory Control and Data Acquisition (SCADA)
    systems [4]. Once attackers have control over SCADA, they can access PLCs and
    can change the way pumps and valves function, chlorine concentration in water,
    functioning points, and corrupt the data in SCADA systems [5]. There have been
    various cyber-attacks reported on WDS in the last several years. For example,
    the Kemuri Water Network company attack. In this attack, attackers understood
    KWC''s internal network architecture and took advantage of inherent vulnerabilities
    in KWC''s controls related to system authentication and network segmentation to
    gain access to the AS400''s login credentials. Accessing the AS400 servers gave
    attackers the ability to manipulate approximately 2.5 million customer records,
    SCADA controls (valves, chemical mixtures, and water flows), additional password
    files, back-office system configuration settings, and other sensitive data. In
    Israel, the water supply system recently witnessed three attacks. In 2019, The
    attackers were able to modify the chlorine proportion, which affected the system''s
    water quality. In the year 2020, the attacks on Agriculture water supply pumps
    changed the pumping operations [5]. When analyzing all the attacks, we found that
    all attacks were performed remotely, and some were used for cryptocurrency mining.
    All of these systems were highly secure, yet the attackers found a way to enter
    these systems [6]. The researcher team organized an international workshop to
    maintain the reliability of the cyber-physical system and make it more secure.
    B. Goals The study''s purpose is to (a) To gain a better understanding of the
    Architecture and functionality of WDS and find the vulnerable and susceptible
    components of WDS. (b)Analyze the impact of cyber-attack on WDS. (c) Studying
    different cyber-attack detection methods and comparing their performance (d) Develop
    cyber-attack detection algorithms that detect the anomalies in the system within
    a short duration. The rest of the paper structure comprises different sections.
    Section 3 covers the literature survey where algorithms proposed by other researchers
    to detect cyber-attack on WDS are studied. SECTION II. Literature Review In this
    section, we have studied various research papers proposed by different authors
    on cyber-physical system-based attacks, and we got several data analysis and computational
    modeling techniques, detection methods, algorithms, and methodologies that are
    used to develop early warning systems for intrusion detection in WDS that are
    described below: In [7], the author proposed a classification approach for detecting
    intrusion and anomalies in WDS using Support Vector Machine (SVM)s. This approach
    is applied to a genuine dataset obtained from a WDS in France. Hidden Markov chains
    are used in [1] to analyze and diagnose anomalies in a WDS''s SCADA system. Attacks
    in PLCs are designed in [8] to better understand the impacts on the produced water.
    The author in [9] proposed two distinct attack detection methods. The first method
    is used to measure the consistency of SCADA data. And this method also checks
    the integrity of SCADA data and actuator regulations (e.g., pump/valve operation).
    And the second method is called the optimization method. The purpose of this method
    is to show how anomalies or cyber-attacks can be detected. Also, this method separates
    hydraulic time series into abnormal and normal data using principal component
    analysis (PCA). The author proposed a two-stage strategy that is based on vector
    extraction and classification in [10] Vector extraction technique was functional
    to multidimensional hydraulic data. For safety classification, random forests
    were used. Following the two-stage detection method, a three-stage detection model
    was presented in [11]. All three stages had specific work; the first stage detects
    outliers in the data. The second stage detects SCADA data unconventionality to
    normal operations. The third stage searches for anomalies that influence several
    sensors. The author in [12] used various machine learning-based approaches to
    develop an intrusion detection approach for the effective detection of intrusion
    on the water distribution system with the best detection performance. K-Nearest
    Neighbour (KNN), Support Vector Machines (SVM), Artificial Neural Networks (ANN),
    and ELM are some common machine learning-based methods for detecting cyber-attacks.
    The attack detection accuracy of all the algorithms is more than 80 percent, which
    is quite high. However, the K-nearest neighbor (KNN) has a lower TPR value among
    all ML algorithms because it''s a lazy learner algorithm; it only detects cyber-attacks
    10 times (T.P.). For the Artificial neural network (ANN), except STTD value, most
    of the performance indices are similar to or less than the value of the SVM algorithm.
    Moreover, ANNs have greater values of False Positive (F.P.) and False-negative
    (F.N.) than the SVM, with 81 and 218, respectively, among the total events (under
    attack conditions: 407, normal conditions: 1682). The ANNs have a high rate of
    false detection. However, the STTD value in ANNs was 0.789, which was greater
    than the SVM value. Although ANN outperformed SVM in detecting early cyber-attack
    events, the overall detection capability of ANN was weak. The Materials and methods
    and the obtained results are presented in next section 4. Then a section is devoted
    to the conclusion. The paper closes with the Conclusions section. SECTION III.
    Case Study on the Wadi The WADI is a working testbed that can deliver 10 gallons
    of filtered water per minute. It is a small and measured unit of the larger WDS
    of a city. The WADI testbed has three phases (shown in Figure 3): (1). Primary
    grid, (2). Secondary grid, and (3). Return water grid. These three phases are
    controlled by three equipment, Compact, Rio PLC. A multilayer communications network,
    PLC, RTU, HMI, and SCADA workstations are all part of WADI. The WADI testbed uses
    103 network sensors and actuators to monitor water level, water quality, water
    flow, pump pressure, actuators such as pumps, and motorized valves. There are
    two levels in the WADI network the communication between PLCs and their network
    sensors is done at level 0 and this communication is based on Modbus RS485. Level-luses
    National Instrument''s Publish-Subscribe Protocol (NI-PSP) for communication,
    while SCADA PACK speaks RTU Modbus TCP. Stage-a and Stage-c PLCs are connected
    through Modbus serial speaking. Level-l also covers communication between the
    Human-machine interface (HMI) and the plant control network. HMI, workstation
    and PLC are interconnected which provides us with a remote monitoring facility
    and analysis of system behavior [19]. Three stages in the WADI testbed are shown
    in figure 3. Here, solid arrows represent the water flow and the order of processes.
    ‘S’ represents sets of network sensors, and ‘A’ represents sets of actuators.
    l-LT-001is level sensor, 1-FS-001 is flow meter 1,1-T-001 is Tank 1 in stage-a,
    and 2-MV-001 is a motorized valve 2-MCV-101 is motorized consumer valve 1 in stage-b,
    and 3-P-004is water pump at the stage -c. Figure. 2 Three stage on WADI Show All
    There are different types of attacks detected on Wadi that are listed in figure
    3. Figure. 3 Attacks detected on WADI [19] Show All SECTION IV. Proposed Model
    In this section, we discuss our proposed intrusion detection model. This model
    comprises three layers. The first is a cloud control server, the second is fog
    enabled control room (SCADA), and the third is a water distribution system. The
    main cloud control server consists of only cyber devices and runs over cloud space,
    and it operates all fog-enabled control rooms (SCADA), which is a scaled unit
    of the main cloud] control server which includes both physical and cyber devices
    that are used to control water distribution networks. We know that the water distribution
    system is a critical infrastructure because it consists of various types of physical
    devices like sensors, actuators, controllers, and PLCs. These devices are very
    susceptible to attacks even though these are intrusion points for attackers where
    an attacker can attack easily and get control over WDS and perform different malicious
    activities. In the following step, we will use various detection techniques to
    detect partial intrusion and anomalies that happen on WDS. Apart from this anomaly
    and partial intrusion detection, our proposed model provides better security and
    real-time security, so that5 IIOT, IFS, and WDS perform well and utilize the resources.
    Figure 4. Water distribution system attack model Show All A. Fuzzy Logic “Fuzzy
    logic is like the Human decision-making procedure. It deals with vague and uncertain
    information, and it''s based on the degree of truth in which the truth value of
    all functions and variables map to any real number between 0 and 1.” The fuzzy
    system comprises four processes which are shown in figure 8. Fuzzification is
    a process of converting the crisp input (all elements are coded in 0 or 1) into
    fuzzy (only membership value for fuzzy logic). Fuzzy inference engines apply fuzzy
    rules over fuzzy (converted crisp input). A fuzzy rule base consists of rules
    defined for mathematical calculation. Defuzzification is a process of converting
    the fuzzy into crisp output [20]. Figure 5. Fuzzy logic system [14] Show All B.
    Intuitionistic Fuzzy Classifier for Intrusion Detectiontify the Headings We face
    various types of uncertainty during the detection of intrusion and anomaly. The
    primary worry at the moment is how to deal with this uncertainty. Our major purpose
    in this study is to use patterns of known assaults to classify intrusions or anomalies
    into three categories: normal, aberrant, and indeterministic. We use the Fuzzy
    logic system to solve this classification problem using intuitionistic fuzzy rules.
    C. Intuitionistic Fuzzy Logic Atanasso introduced this notation of the IFS (intuitionistic
    fuzzy set) in [15]. An IFS can be thought of as a fuzzy set, but a Fuzzy set can''t
    be IFS. Many authors said that IFS is a better tool than fuzzy set theory for
    discovering a solution to a wide range of real-world issues. When a fuzzy variable
    expresses a problem as a membership function, IFS can help solve the problem [16].
    In IFS theory, an object''s membership value also defines its non-membership value
    via a mathematical relationship, and this is the significant improvement of intuitionistic
    fuzzy logic over Fuzzy logic. However, in the former, an object''s membership
    value(MV) and non-membership value(NMV) are separated by a mathematical equation:
    Non-membership value = 1- Membership value But in reality, it does not exist because
    Human is not able to represent the NMV value as 1- MV and also there are some
    indeterministic parts while deciding the degree of membership of any function
    or object and this is due to the lack of knowledge in defining the membership
    function, as well as the presence of uncertainty or hesitancy. And this uncertainty
    is called the hesitation index or intuitionistic fuzzy index. A fuzzy set can
    be considered a particular case of an intuitionistic fuzzy set if there is no
    indeterminate state concerning any object in the universe of discourse at the
    primary level for evaluating the membership value. Let A is an object of IFS defined
    in-universe X: A={<x, μ Λ (x), v Λ (x)>|x∈X} (1) View Source Here: μ A :X→[0,1],
    v A :X→[0,1] Defined the degree of membership and non-membership for every element
    of the x∈ X. And for every x∈X of object A holds the condition: 0< μ A (x)+ v
    A (x)<1 View Source And we can also write as: {<x, μ A (x),1− v A (x)>|x∈X} View
    Source Recently, the third important parameter, which is called the intuitionistic
    index represented, (x)originates due to a lack of understanding or a “personal
    error” in computing the distances between two fuzzy sets [24]. As a result, the
    sum of the three degrees, MV, NMV, and hesitation degree, is equal to 1. For each
    x∈X , it is self-evident that 0⩽A(x)⩽1 . As a result, IFS set A in-universe X
    may be represented as A={<x, μ A (x), v A (x), π A (x)>|x∈X} View Source Were
    the condition being μ A (x}+ v A (x)+ π A (x}−1 View Source D. Intuition Is Tic
    Fuzzy Triangular Function IFS follows a triangular function(iftrif) which consists
    basically of three parameters, a, b, and c, where a is the lower limit, c is the
    upper limit, and b is the value. Where a⩽b⩽cThe , all three parameters a, b, and
    c form a triangle and determine the precise aspect of the function. The parameters
    a and c are used to locate the triangle''s foot, while the parameter b is used
    to locate the peak. Intuitionistic fuzzy triangular membership function: μ A −
    ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ 0 ( x−a b−a )−ϵ ( c−x c−b )−ϵ 0 ; ; ; ; x≤a a<x≤b
    b≤x<c x>c View Source Intuitionistic fuzzy triangular non-membership function:
    v A (x)= ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ 1ϵ 1−( x−a ba ) 1−( cx c−b ) 1−ϵ ; ; ; ; x≤a
    a<x≤b b≤x<c x≥c . View Source E. Ensemble Stacking Here we will discuss the flow
    of our proposed model. It''s divided into two sides: the cloud and the fog sides.
    In cloud-server redundant data, organize the raw data. After that, this clean
    dataset is used for the machine learning model. First, we will acquire the raw
    dataset of WADI attacks that are observed over the WADI testbed, and then it is
    pre-processed to clean and format the dataset. Data pre-processing is important
    to creating a machine learning model because the data observed over any system
    may be incomplete, Inconsistent, contain a null value, Inaccurate, or missing
    attribute. So, Data pre-processing helps to clean, format, delete, and then remove
    the null values present in the dataset. After that, in the obtained dataset, we
    will do feature selection using the IFS method; then, we split the dataset for
    training and testing purposes of our model. After that, using different ML classifiers,
    we are trying to increase the accuracy of our train model. The selected feature
    is saved for further use in the form of a pickle. We apply the saved pickle to
    our model on the fog server and then use different ML classifiers and voting.
    We will select the feature which gives much more accuracy to other features to
    detect the intrusion or anomalies. The ensembling stacking of our model is shown
    in figure 6. Figure 6. Feature selection using IFS &attack model Show All F. The
    Steps for The Method for Feature Selection 1. Computing Relative Frequencies of
    Each Feature''s attributes In this step, for each input and attribute, the quantity
    of appearance within the Positive and Negative classes is defined. 2. Mass Assignment
    for Each Attribute''s Value for Each feature and Building IFS: In the second step,
    Firstly, we compute pos and neg for each attribute value using this formula. pos
    i |ne g i =|F|i⋅pi+ ∑ ni=1 (|Fk|−|F|k+1|)⋅pk View Source where: |F|i=|{x∈Λ|P(x)≥pi}|,0≤pi+1<pi≤1
    So, for each feature attribute from the table of relative frequencies defined
    above, there are four values: two values for positive and negative class. These
    values can be defined as p+0(i),p+1 (i), same - for negative class, where i -
    feature''s value and 0,1 are feature''s values (attributes). For the mass assignment
    (computing pos and neg), they have to be sorted first. After that, the least frequency
    is multiplied by two as |Fi|=2 because two values of frequency probabilities are
    greater or equal to it, and the other is assigned to one by the mass assignment
    property. That step is computed in the voting function. Example: temp =[((0,0),0.725),((0,1),0.275),((1,0),0.55),((1
    , 1),0.45)] neg =voting''s (tem [2:]) =votings(((1,0),0.55),((1,1),0.45))={1:0.9,0:1}
    {1:0.9,0:1}:0.9=0.45∗2;0:1} View Source Secondly, based on obtained pos and neg
    values, an IFS triplet is obtained, and the corresponding values of the dataset
    are replaced with them for further averaging in step three. In General, the triplet''s
    values are computed as follows: π(i) - pos (i)+ ncg (i) - 1; μ=pos(i)−π(i) ; v=neg(i)−π(i);
    That part of computations is provided in the computing function. Computing Feature
    importance: feature importance of the feature k can be estimated by the following
    function. f( Λ k )−|1− π ¯ ¯ ¯ k |⋅| μ ¯ k − v ¯ k | View Source The output of
    the following steps is illustrated in below table 2. Table 1. Feature importance
    in IFS Based on the Feature importance, we can provide appropriate ranks for these
    features, which are shown below figure 10. SECTION V. Result and Discussion The
    method may look fine, but the results obtained differ, that is due to the following
    mass assignment''s property: for each attribute firstly pos and neg are computed,
    then, the hesitation margin is computed by its definition: π(x)− pos x +ne g x
    −1 View Source But take a deeper look at that definition: pos x (i)=2 p x or 1,
    where i-is the feature''s index, and x - attribute''s value. Same for ne g x (i)
    , and the value 2 p x (i)+2 p y (i) can be, of course, less than one. Thus, the
    hesitation margin can be below zero, and the authors did not define the resolution
    of that problem and how to deal with it, so the hesitation margin can be negative.
    In the next subsection, we are going to compare all performance parameters of
    different methods. A. Comparison of Result With Another Method 1. Random Forest
    Method Random Forest is a frequently used supervised ML technique for solving
    regression and classification issues. It employs ensemble learning techniques,
    which integrate several classifiers to provide better solutions to complex problems.
    It uses a majority vote for classification and an average vote for regression.
    When we are using the Random Forest algorithm, we have gained the training accuracy
    of 0.92 and test accuracy of 0.77 After this, we estimate the feature importance
    of all features and compare it with the feature importance of the IFS method.
    Comparison of feature importance of Random Forest with feature importance of IFS
    in table 3 Table 2. Comparison of feature importance 2. Decision Tree Classifier
    It''s a supervised ML method. Its most common method is used to solve both classification
    and regression issues, but it''s mostly used for classification problems. A decision
    tree classifier is a tree-structured classifier, where the method outcome is represented
    by leaf nodes, features or columns of a dataset is represented by internal nodes,
    and branches indicate the decision rules. When we are using the Decision Tree
    classifier, we have gained a training accuracy of 0.93 and a test accuracy of
    0.679 3. Logistic Regression Another supervised machine learning method is logistic
    regression. The variable takes only discrete values in this classification approach
    for the given set of feature samples. Logistic regression becomes a classification
    method when a decision threshold is brought into the picture. Choosing the threshold
    value is very important for the classification of features because the feature''s
    classification is purely dependent on the threshold value. The decision of threshold
    value is mostly affected by the value of recall and precision. We want both to
    be ideally 1, but in the case of logistic regression, we use some extra arguments
    to decide the threshold value; those are: Low precision (L.P.)/High recall (H.R.)
    High Precision (H.P.)/Low Recall (L.R.). When we are using a Logistic regression
    algorithm, we got the training accuracy of 0.83 and test accuracy of 0.7 Figure
    7. Performance comparison graph Show All Table 3 Calculating average metrics for
    selection (AMS) Conclusion The security threat and anomaly issue over water distribution
    centers may become a topic of concern in the future. The WDS is a typical and
    complex cyber-physical system consisting of different components (such as sensors,
    gates, and microcontrollers), these components are very susceptible to cyberattacks
    and can be controlled remotely. The presence of these components increases the
    possibility of attacks on WDS. We can detect these attacks or anomalies with the
    help of machine learning methods. We studied the different machine learning techniques
    to detect cyber-physical attacks and anomaly detection over water distribution
    systems. From the different machine learning methods, So our model performs best
    over the WADI dataset and its attack detection accuracy is 0.93 which is the highest
    among all other ML methods. Authors Figures References Citations Keywords Metrics
    More Like This Network-Based Machine Learning Detection of Covert Channel Attacks
    on Cyber-Physical Systems 2022 IEEE 20th International Conference on Industrial
    Informatics (INDIN) Published: 2022 An adaptive Medical Cyber-Physical System
    for post diagnosis patient care using cloud computing and machine learning approach
    2022 3rd International Conference for Emerging Technology (INCET) Published: 2022
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2022 1st International Conference on Sustainable Technology for Power and
    Energy Systems, STPES 2022
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Fog Enabled Cyber-Physical Attack detection using Ensemble Machine Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Saba Raoof S.
  - Durai M.A.S.
  citation_count: '9'
  description: Growth and advancement of the Deep Learning (DL) and the Internet of
    Things (IoT) are figuring out their way over the modern contemporary world through
    integrating various technologies in distinct fields viz, agriculture, manufacturing,
    energy, transportation, supply chains, cities, healthcare, and so on. Researchers
    had identified the feasibility of integrating deep learning, cloud, and IoT to
    enhance the overall automation, where IoT may prolong its application area through
    utilizing cloud services and the cloud can even prolong its applications through
    data acquired by IoT devices like sensors and deep learning for disease detection
    and diagnosis. This study explains a summary of various techniques utilized in
    smart healthcare, i.e., deep learning, cloud-based-IoT applications in smart healthcare,
    fog computing in smart healthcare, and challenges and issues faced by smart healthcare
    and it presents a wider scope as it is not intended for a particular application
    such aspatient monitoring, disease detection, and diagnosing and the technologies
    used for developing this smart systems are outlined. Smart health bestows the
    quality of life. Convenient and comfortable living is made possible by the services
    provided by smart healthcare systems (SHSs). Since healthcare is a massive area
    with enormous data and a broad spectrum of diseases associated with different
    organs, immense research can be done to overcome the drawbacks of traditional
    healthcare methods. Deep learning with IoT can effectively be applied in the healthcare
    sector to automate the diagnosing and treatment process even in rural areas remotely.
    Applications may include disease prevention and diagnosis, fitness and patient
    monitoring, food monitoring, mobile health, telemedicine, emergency systems, assisted
    living, self-management of chronic diseases, and so on.
  doi: 10.1155/2022/4822235
  full_citation: '>'
  full_text: '>

    "Journals Publish with us Publishing partnerships About us Blog Contrast Media
    & Molecular Imaging Journal overview For authors For reviewers For editors Table
    of Contents Special Issues Contrast Media & Molecular Imaging/ 2022/ Article On
    this page Abstract Introduction Related Work Conclusion Data Availability Conflicts
    of Interest Authors’ Contributions References Copyright Related Articles Special
    Issue Automated Interpretable and Lightweight Deep Learning Models for Molecular
    Images View this Special Issue Review Article | Open Access Volume 2022 | Article
    ID 4822235 | https://doi.org/10.1155/2022/4822235 Show citation A Comprehensive
    Review on Smart Health Care: Applications, Paradigms, and Challenges with Case
    Studies Syed Saba Raoof 1and M. A. Saleem Durai 1 Show more Academic Editor: Shailendra
    Tiwari Received 05 Aug 2022 Accepted 02 Sept 2022 Published 29 Sept 2022 Abstract
    Growth and advancement of the Deep Learning (DL) and the Internet of Things (IoT)
    are figuring out their way over the modern contemporary world through integrating
    various technologies in distinct fields viz, agriculture, manufacturing, energy,
    transportation, supply chains, cities, healthcare, and so on. Researchers had
    identified the feasibility of integrating deep learning, cloud, and IoT to enhance
    the overall automation, where IoT may prolong its application area through utilizing
    cloud services and the cloud can even prolong its applications through data acquired
    by IoT devices like sensors and deep learning for disease detection and diagnosis.
    This study explains a summary of various techniques utilized in smart healthcare,
    i.e., deep learning, cloud-based-IoT applications in smart healthcare, fog computing
    in smart healthcare, and challenges and issues faced by smart healthcare and it
    presents a wider scope as it is not intended for a particular application such
    aspatient monitoring, disease detection, and diagnosing and the technologies used
    for developing this smart systems are outlined. Smart health bestows the quality
    of life. Convenient and comfortable living is made possible by the services provided
    by smart healthcare systems (SHSs). Since healthcare is a massive area with enormous
    data and a broad spectrum of diseases associated with different organs, immense
    research can be done to overcome the drawbacks of traditional healthcare methods.
    Deep learning with IoT can effectively be applied in the healthcare sector to
    automate the diagnosing and treatment process even in rural areas remotely. Applications
    may include disease prevention and diagnosis, fitness and patient monitoring,
    food monitoring, mobile health, telemedicine, emergency systems, assisted living,
    self-management of chronic diseases, and so on. 1. Introduction The most indispensable
    aspect of life is undeniably health. In recent years, advanced healthcare systems
    have gained immense popularity due to demographic growth, and an increase in diseases
    which, in turn, requires enormous clinical assets and even hospital staff. Therefore,
    it challenges tremendous, computerized health systems providing excellent services
    to both patients and the hospital staff as traditional healthcare systems are
    incompetent to accomplish the necessity of all humans as it is not affordable
    and accessible to everyone. Advancement in the medical area was initiated in the
    early 1991s and it was considered a completely advanced area for treatment. Since
    then, healthcare systems had been revolutionized in several ways, for example,
    agile treatment, appropriate early patient serving, delivering, and monitoring
    healthcare services remotely, and quick action towards emergency cases. The key
    challenge encountered during the advancement of the medical area was its demand
    for emerging efficient types of equipment to deliver the best services to patients
    [ 1]. This challenge can be addressed by employing IoT and evolutionary technologies.
    Implementation of IoT in the medical area gained popularity, following modern
    techniques like smart cities, smart regions, and smart devices. IoT achieved immense
    popularity because of its data acquiring and visualizing property through sensing
    the objects and communication with devices through wireless networks. IoT devices
    are capable of sensing, visualizing, collecting, and sharing data, and communication
    among the devices can be done by wireless IoT protocols like Bluetooth, ZigBee,
    Z-Wave, WiFi, and RFID [ 2, 3]. These protocols play a vital role in the healthcare
    sector since they ensure ease and flexibility for data communication and data
    monitoring among employed devices. Data collected from these devices are used
    for different tasks like disease classification, designing, patient monitoring,
    and so on. In remote areas where one cannot access the hospital services suffers
    a lot, and this leads to worsening the patient condition; this situation can be
    handled by new emerging technology, i.e., tiny wireless chips or sensors connected
    with IoT devices remotely monitors patient’s health [ 4]. Wired or wireless sensors
    [ 5] connected to the patient’s body acquire securely patient data and at the
    same time physicians have access to data; thus, it provides ease for decision
    making. IoT applications in the healthcare sector benefit everyone to access medical
    help remotely, enhance the duration of therapy, and affordable cost. Many researchers
    had implemented various smart health devices like health monitoring and controlling,
    smart thermometers, automated insulin delivery (AID) systems [ 6], remote care
    biometric scanners, sleep monitoring, monitoring drug interaction, and so on.
    Regardless of this progress (i.e., IoT in healthcare), there are still challenges
    to overcome like deploying of IoT system which can manage a large amount of data
    along with proving all security concerns like data confidentiality, integrity
    control, authorization, and authentication. Cloud concepts also play a vital role
    in smart healthcare systems. It provides firm and efficient access to data; storage
    can be done effectively, and the above-mentioned challenges can be overcome by
    employing cloud services [ 7]. Deep learning (DL) techniques like Convolutional
    Neural Networks (CNN), Autoencoders (AE), Deep Belief Networks (DBNs), Long Short-Term
    Memory (LSTM), Recurrent Neural Networks (RNNs), etc. [ 8, 9] are being employed
    to analyze large data and to effectively detect and diagnose various diseases.
    DL applications in medical imaging and medical signals assist both physicians
    and patients effectively. Integrating IoT technologies along with cloud services,
    machine learning, and deep learning techniques enhances smart healthcare services
    immensely [ 10]. The key objective of this research paper is to provide a comprehensive
    survey related to IoT-cloud-Artificial Intelligent-Machine Learning-Deep learning
    healthcare systems (i.e., smart healthcare). The rest of the study is organized
    as follows: Section 2 discusses related work. Section 3 explains background study.
    Section 4 discusses smart healthcare applications. Section 5 challenges in smart
    healthcare systems. Section 6 explains conclusion and Table 1 describes the abbrevations.
    2. Related Work This section provides a review of some existing smart healthcare
    papers and Table 2 demonstrates the summary survey carried out during our research
    work. Table 1  list of abbreviations. Ali et al. [ 11] developed a Smart Healthcare
    Monitoring System (SHMS) to predict heart disease by employing deep ensemble learning,
    and feature fusion methods. For experimental analysis, both sensor data and EMR
    data had been utilized and the system obtained 98.5% accuracy in predicting heart
    disease. The entire process is carried out in four layers, i.e., data collection
    layer, data fusion and feature extraction layer, data preprocessing layer, and
    prediction layer. Ismail et al. [ 12] proposed an efficient solution “a smart
    speech recognition system” for elderly and disabled patients. The main components
    of the proposed system are smartphones, controlling devices, and household appliances/devices.
    A combination of support vector machine (SVM) and dynamic time wrapping (DTW)
    approaches had been used for speech recognition and controlling the system Raspberry
    Pi board, which recorded 97% accuracy. Rizwan et al. [ 13] introduced an integrated
    smart system (i.e., deep neural network (DNN) + IoT) namely Grey Filter Bayesian
    Convolutional Neural Network (GFB-CNN) depending on real-time analytics for analyzing
    heart signals. Cloud services had been also used for data storing and sharing.
    Mobile HEALTH (MHEALTH) dataset had been utilized and obtained 88% of accuracy.
    Subasi et al. [ 14] implemented efficient and automated Human Action Recognition
    (HAR) system by employing various ML algorithms like SVM, K-NN, ANN, Naive Bayes,
    RF, CART, C4.5, REP Tree, and LAD Tree, and compared the results of all these
    algorithms and recorded accuracy between 84% and 99%. Body sensor data REALDISP
    and Smartphone sensor data are used for experimental analysis of the system. Pereira
    et al. [ 15] implemented a 3D-UNet model for brain disease classification and
    the BRATS dataset is used and achieved 89% accuracy. Further interpretability
    techniques were employed for analyzing prediction which enhanced preprocessing
    step. Manogaran et al. [ 16] proposed MF-R and GC architecture for monitoring
    patients’ health. The proposed architecture is composed of three phases, i.e.,
    data collection, transfer, and storage phase. The system monitors blood pressure,
    heart rate, sugar level, respiratory rate, and body temperature. Stochastic gradient
    descent algorithm and logistic regression techniques are employed in the proposed
    smart system for detecting heart diseases and the CHDD dataset is used for this
    purpose. Vikas and Ananthula [ 17] designed a network, namely Body Sensor Network
    (BSN) for monitoring the health condition of patients and LabView software is
    employed on the patient side. The proposed model helps physicians to view and
    monitor the patient’s health remotely. Arduino board is used for connecting the
    sensors to acquire patient health data. Luca et al. [ 18] developed IoT-based
    and Smart Health Care System (SHS) architecture for monitoring patient health,
    tracking hospitals’ biomedical devices, and nursing staff in nursing institutes.
    CoAP, REST, and 6LoWPAN, IoT paradigms are employed to allow intercommunication
    and interoperation between SHS devices (i.e., UHF, WSN, and phones). The proposed
    system is divided into two subsystems: one to monitor patients and the other to
    deal with emergencies. Loubet et al. [ 19] developed a Cyber-physical system for
    structural patient health monitoring. Smart mesh WSN consisting of sensing and
    communication nodes and a far-field Wireless Power Transmission system (a battery
    and wire-free system) is used to develop the proposed architecture. Autonomous
    control of sensing nodes is made possible because of Wireless Power Transmission.
    Swati et al. [ 20] proposed a block-wise fine-tuning method using a pretrained
    CNN model for detecting brain tumors. CE-MRI benchmark dataset is used for experimental
    analysis and recorded 94.82% accuracy. Rahman et al. [ 21] proposed a Smart E-Health
    care system by employing Fog Computing thus making geo disseminated intermediate
    intelligence layer among sensors and cloud. Even implemented a smart health gateway
    called UT GATE and Early Warning Score (EWS) health monitoring system to meet
    the challenges viz. energy efficiency, security, interoperability, reliability,
    mobility, and enhanced system intelligence. Verma et al. [ 22] proposed an IoT
    fog cloud-based Cyber-physical system to diagnose and classify ulcerative colitis.
    For detecting and classifying cancer deep neural networks (DNN) and Naive Bayes
    classifiers are utilized. The key concept of the proposed system is to generate
    real-time alarms in emergency conditions (registered patients of the proposed
    system), and the cloud is employed to store the patient data. Mohamad et al. [
    23] developed an android system to pronounce and write Hijaiyah letters using
    a dynamic time-wrapping approach. Voice analogous technique is used for extracting
    data, inputs, and voice, whereas principal component analysis (PCA) is used for
    image extraction and MFCC to extract voices. The system recorded 92.85%. De Brouwer
    et al. [ 24] proposed a framework for healthcare systems to communicate data among
    smart home sensors and smart hospital sensors. The system is used to monitor constantly
    for clinically diagnosed patients and emergency alarms are responded to accordingly.
    The key benefit of the system is it eradicates the data latency to enhance the
    responsiveness of the system. Modu et al. [ 25] developed a warning system and
    an android application to analyze malaria outbreaks. For experimental analysis,
    data are acquired from Climate Forecast System Reanalysis (CFSR). Hypothetical
    SEM and ellipse shapes are utilized for detecting the ecological factor relationship.
    SVM, KNN, decision trees, and Naive Bayes machine learning algorithms are utilized
    for predicting the disease and recorded accuracy between 80.06% and 99.0%. Li
    et al. [ 26] developed a system to detect human emotions and a hybrid deep neural
    network, i.e., CNN and LSTM are employed for this purpose. For experimental analysis,
    EEG signal data were used. EEG spatial and temporal characteristics were integrated
    and then converted the outcome into a 2D image. The proposed system recorded 75.2%
    accuracy. According to the literature done, Table 2 summarizes deep learning and
    machine learning techniques incorporated with IoT can be applied for healthcare
    applications for monitoring, classification, and diagnosis of various diseases
    like cancer, Alzheimer’s, cardiovascular diseases, brain tumor, elderly patient
    monitoring, and so on. It is observed that the machine learning algorithms performed
    well for classification purposes and whereas deep learning algorithms worked well
    for segmentation, detection, and monitoring. SVM, ANN, and CNN have mostly used
    algorithms, and where they demonstrated that they typically had high evaluation
    performances. 3. Background Study 3.1. Machine Learning (ML) ML is a branch of
    AI which aims on automating machines with minimum human involvement. It is broadly
    categorized into three types: supervised learning, unsupervised learning, reinforcement
    learning, and semi-supervised learning. Supervised and unsupervised algorithms
    implement mathematical models and are designed to provide proficiency to systems
    to analyze and learn how to address different tasks. Dataset and corresponding
    labels (i.e., values, or classes) are used to train the model in supervised learning.
    Input dataset and its relevant outputs are fed to the algorithm, then it earns
    through comparing actual output by correct output to identify errors, then accordingly
    model is modified [ 41]. Further, it is divided into different methods such as
    classification, prediction, regression, and gradient boosting. Examples of supervised
    learning are support vector machine (SVM), linear regression (LR), random forest
    (RF), and decision trees (DT). Unsupervised learning trains the model only on
    the given input values; corresponding output variables are not fed to the model
    along with input data unlike supervised learning, and it is further classified
    into clustering and association problems which means models are not supervised
    based on training data, rather it identifies hidden patterns from input data like
    the human brain. K-means, k-nearest neighbors (KNN), principal component analysis
    (PCA), independent component analysis (ICA), hierarchal clustering, and the apriori
    algorithm are some of the examples of unsupervised algorithms [ 42]. Whereas semi-supervised
    learning utilizes both labeled and unlabelled data for training and sometimes,
    it employs both supervised and unsupervised methods. ML is advancing promptly
    in every field due to its advanced algorithms that detect and classify objects.
    ML in smart healthcare plays a significant role as it advances services provided
    by healthcare systems such as disease analysis, precise findings, accurate diagnosis,
    and early-stage prediction. Since medical data are in digital form, to exploit
    these data in smart healthcare systems, they are numerous challenges to conquer.
    In recent years, there has been enormous growth in healthcare data which needs
    to be managed and classified accordingly for accurate diagnosis. ML algorithms
    can be employed to provide solutions to different healthcare problems. Research
    surveys had demonstrated that ML had achieved enhanced performance for disease
    prediction, diagnosis, and classification tasks. SVM, KNN, RF, and CNN are some
    of the ML algorithms which fit best for disease classification and identification
    [ 43, 44]. Some of the applications of ML in smart healthcare are disease identification,
    diagnosis, medical image diagnosis, personalized medication, robotic surgeries,
    cancer detection, oral disease detection, etc. In [ 45], the author developed
    a model (ML based, i.e., ANN and RF) to diagnose and identify bacteria intoxication
    among serious patients and recorded 90.8% accuracy. In [ 46, 47], and [ 48], a
    system was proposed by utilizing data mining techniques, ensemble learning approach,
    and SVM respectively to identify readmitting of ICU patients. In [ 49], a DL model
    was proposed by employing a CNN algorithm to diagnose glaucoma and obtained 81.6%
    accuracy on data acquired from Beijing Tongren Hospital. In [ 50], the authors
    developed a model by integrating different ML algorithms, i.e., Linear Regression,
    Lasso Regression, Ridge Regression, and Elastic Net to identify Alzheimer’s disease
    (AD) MMSE scores (Mini-Mental State Examination). A cataract detection model was
    proposed [ 51], by employing SVM, and a skating algorithm and obtained around
    80% to 84% accuracy. SVM and RF algorithms were utilized for analyzing EHR data
    [ 52]. The disease diagnosis model was developed [ 53] by implementing SVM and
    deep belief network. 3.2. Deep Learning (DL) DL is a subdomain of ML engrossed
    by algorithms, functioning like the human brain, i.e., neural networks. The key
    asset of DL over ML is that it removes the preprocessing, feature extraction,
    and feature selection processes that are engaged in traditional machine learning.
    DL algorithms work well for unstructured data such as images, records, text, sensor
    data, geospatial data, etc., and feature extraction, and feature selection processes
    are automated. Artificial neural networks or deep neural networks simulate the
    working of the human brain by employing various elements like inputs, weights,
    bias, and fusing data, where all these elements function simultaneously to accomplish
    various tasks in a particular classification, and recognition [ 54]. Deep learning
    algorithms consist of three layers input, output, and hidden layer. Input and
    output layers are known as visible layers, input layers acquire and process the
    data, whereas the output layer is used to classify or predict the result. The
    main computations of the network are carried out in the hidden layers. In DL algorithms
    multiple layers and nodes are interlinked together to optimize and enhance the
    task of either classification or recognition. Such computation successions over
    a network are known as “Forward Propagation”. Layers or algorithms where errors
    are computed through backward traversal by adjusting nodal weights are known as
    “Backward Propagation”. These two forward and backward propagations enable the
    algorithm for claiming the predictions accurately. Since DL algorithms analyze
    data according to the functioning of the brain, it is feasible to be implemented
    it in different real-world tasks. It is mostly applied for image, text, and speech
    recognition, disease detection [ 55], prediction, and diagnosis, drug discovery,
    business and management, manufacturing, bioinformatics, and natural language processing.
    It has been discovered that these applications are unified as a range of services
    and products such that clients are strangers to the intricate functioning of the
    model. 3.2.1. Deep Learning Architectures Deep learning algorithms give the best
    performance for all types of data; the key requirements of this algorithm are
    an enormous amount of data and a high processing unit. Deep learning algorithms
    are majorly classified into two types of supervised deep learning, i.e., artificial
    neural networks (ANNs), convolutional neural networks (CNNs), recurrent neural
    networks (RNNs) and unsupervised deep learning autoencoder, Boltzmann machine
    and Self Organizing Maps (SOM). These algorithms consist of layers and numerous
    nodes. Layers can be specified based on the task and different kinds of models
    had been designed for different tasks like CNNs for recognition tasks, RNNs for
    time sequence and prediction problems, U-Nets for medical images, and in deep
    learning it is specified that there is no specific algorithm for a specific task
    and the algorithm can be structured based on various factors. The following section
    briefly explains various deep learning algorithms and table 4 describes the summary
    of various deep learning algorithms and Table 3 describes the summary of various
    deep learning algorithm. The term “deep” applies to learning/analyzing succeeding
    layers from progressive important input images. The depth of the network is determined
    by the number of layers employed. At present deep learning, architectures include
    nearly tens to hundreds of layers. The traditional machine learning techniques
    usually emphasize learning from a few layers, i.e., one or two; these techniques
    are considered shallow learning. (i) Convolutional Neural Networks (CNNs). The
    most prominent deep learning algorithm is CNN, inclusive of several receptive
    layers enabling robust and fast training to process the input images. It is mainly
    applied for various computer vision tasks and to enhance the performance of recognition
    problems [ 37, 38]. CNNs are neural networks that use both mathematical convolutions
    and matrices. CNN layers are organized in overlapping forms across the input layers
    to procure high-resolution outcomes like the original input and every layer in
    the network follows the above process. The major aim of CNNs is to analyze data-specified
    kernels in preference to predefined kernels. It employs a sequence of convolutions
    followed by pooling and activation functions to extract features automatically
    and activate the neurons, whereas a fully connected layer is employed to classify
    the extracted features; all CNN operations are depicted in Figure 1. Inputs of
    the CNN layer are placed in a three-dimensional matrix, i.e., hwd where h is the
    height, is the width, and d is the depth of the network. Several filters of size
    m × n (m > d and n ≥ d) are present in each layer. As stated previously, filters
    are the kernels of network internal connection which are convolved through input
    and sharing the same parameters, i.e., weight (WK) and bias (bk) to create feature
    maps k(bk) of size h−m−1. All the convolutional layers perform the dot product
    among input and weight. The next activation operation is applied to the convolutional
    layer outcome, i.e., as follows: Next subsampling is done, where all the feature
    maps are down-sampled for dimensionality reduction thus training is enhanced and
    regulates overfitting problems, later the pooling function is employed. At last,
    the fully connected layer acquires the preceding layers’ outcome and generates
    higher-level output, where generally, the softmax function is applied to perform
    a classification task. (ii)   Recurrent Neural Network (RNN). It is the feed-forward
    neural network comprising recurrent memory units which acquire and store preceding
    layer outcomes and feed them to the succeeding layer as input to predict the specific
    layer output, Figure 2 represents the simple recurrent unit. In Figure 2, x represents
    the input, y represents the output, and h is the hidden layer. Parameters of the
    network are represented by P, Q, and R which enhances the output efficiency. Consider
    the time “t” as an instance then the input is depicted as x(t) and x(t − 1). At
    any given time, t, the current input is a combination of input at x(t) and x(t − 1).
    The output at any given time is fetched back to the network to improve the output.
    Where P, Q, and R are parameters of the network, x is the input and y is the output,
    as shown in Figure 3. As seen in the figure x-input layer acquires and passes
    the input to the next layer h-middle layer; in this layer several weights, bias,
    and activation functions can be deployed. RNN will regularize the weights, bias,
    and activation functions to ensure that all the hidden layers share similar parameters
    and, therefore, the RNN originates a single hidden layer and employs a loop to
    it. (iii) Autoencoder (AE).Autoencoders are a specific variant of FFNN holding
    the same input and output. AEs are trained NNs that replace the data from input
    to output layers. AE includes three parts viz, encoder, decoder, and code unit.
    The working rule is according to encoding the input data to a smaller dimension
    “code”, followed by activation, and then it is decoded out of the representation,
    i.e., the output is generated. The abstract or compacted input data are known
    as code or depiction of latent space as depicted in Figure 4. It is a dimensionality
    reduction algorithm, and the applications are prediction, medical domain, image
    processing, and drug discovery. It can be constructed with an encoder, decoder,
    and loss function operator. Input is passed to the encoder unit a fully connected
    neural network and generates an encoded code, whereas the decoder is also an artificial
    neural network that generates the output by decoding the encoded code. The key
    objective of AE is to generate an output similar to input, i.e., the decoder unit
    is a reflection of the encoder unit. During this process, AE is optimized and
    enhanced by diminishing the error rate. Deep AE is generally trained through distinct
    backpropagation techniques like the conjugate gradient approach. Different variants
    of autoencoders are also present like; sparse autoencoder, denoising autoencoder,
    contractive autoencoder, saturating autoencoder, zero-bias autoencoder, and convolutional
    autoencoder. (iv) Generative Adversarial Networks (GANs). GAN is a semi-supervised
    or unsupervised model, and they are generative models which generate new data
    samples like the input. Adversarial GAN term implies that there exists competition
    among generator and discriminator. It is most broadly implemented in CV and NLP
    applications. It comprises two NN parts, i.e., the generator and the discriminator
    presented in Figure 5. (a) Generator: It is an NN, efficient to generate new feasible
    data instances like original data, and these instances are considered as adverse
    training instances by the discriminator. (b) Discriminator. This is reliable for
    classification, i.e., to classify the actual instances from those generated by
    GANs generator. The discriminator penalizes the generator for generating improbable
    data.    Figure 1  Architecture of CNN, representing all the operations of CNN,
    i.e., convolution, pooling, feature extraction, and classification with “n” number
    of convolution, pooling, and fully connected layers.    Figure 2  Simple recurrent
    unit representation of RNN.    Figure 3  The architecture of a recurrent neural
    network, where , Q, and R are parameters of the network, x is the input and y
    is the output.    Figure 4  Autoencoder architecture.    Figure 5  Generative
    adversarial networks (GAN). The generator generates the false instances, and the
    discriminator promptly learns the generated data and declares it as false data.
    With the progression of training, it generates an instance that may deceive the
    discriminator. At last, the discriminator becomes inferior at identifying the
    actual and the false data and classifies the false data as actual data, thus reducing
    the network efficiency. Based on the literature, it is observed that the CNN,
    variants of CNN, CNN-based algorithms, and combination of CNN with other algorithms
    had achieved better results when compared to other deep learning and traditional
    algorithms as shown in Figure 6.    Figure 6  Comparison of deep learning algorithms,
    i.e., CNN, RNN, AE, and GAN performance. 3.3. IoT IoT is a system that comprises
    devices connected which possess the ability to acquire data and can transfer/exchange
    data through a wireless network automatically. It permits automated data collection
    and exchange among a wide variety of industrial sectors. IoT had already been
    implemented in various fields like industrial automation, agriculture, transportation,
    construction, supply chain, retail, smart home applications, smart cities, smart
    grids, and smart parking. Apart from all these fields, it can even be applied
    to healthcare to perform different tasks ranging from patient data collection
    to monitoring patients’ health [ 56, 57]. The survey done demonstrates that the
    existing IoT models work well for data gathering and object controlling/monitoring
    [ 32, 58, 59] and, therefore, IoT models can be implemented for health monitoring
    and reporting the patient condition to appropriate persons like doctors, guardians,
    medical centers, and to emergency crews [ 60]. The general IoT architecture comprises
    different layers such as the data management layer, application layer, middleware
    layer, network layer, and perception layer. Figure 7 shows the IoT architecture
    with different layers and the devices used in each layer.    Figure 7  IoT layered
    architecture. A survey done during the research demonstrates that patient remote
    health monitoring (RHM) [ 61] systems are considerable and the key challenge in
    these systems is the services that it delivers in various environments. RHM systems
    are implemented to examine and monitor patients’ conditions remotely benefiting
    patients, hospital staff, and resources [ 62]. The main aim of such systems is
    to provide the best healthcare services to rural area patients. Even though this
    system provides diverse services some challenges that need to be overcome are
    data privacy and data storage which can be solved by introducing cloud-based IoT
    healthcare systems. Many researchers had developed such systems as described in
    the literature study since advancement persists the IoT-based RHM systems growing
    its popularity and it had become a permanent solution to healthcare systems providing
    services to rural areas and elderly patients [ 63]. RHM system can be implemented
    to provide a diverse variety of services like BP checks, heart rate monitoring,
    temperature check, diabetes checks, rehabilitation, cancer detection, brain-related
    diseases, health monitoring, and so on. Therefore, these systems can be developed
    for different tasks, and they are closely related to the applications of equivalent-enabled
    technologies. The major requirements of RHM systems are a variety of sensors and
    communication devices [ 64]. Widely used sensors are wearable sensors (wireless
    or external wearable sensors), vision-based sensors, body sensor networks (BSN),
    and communication devices are categorized into two types, i.e., short-range communication
    devices and long-range communication devices [ 65– 71] as described in Tables
    4 and 3, demonstrating various sensors used in healthcare applications for acquiring
    real-time data, and the complete range of all communication devices is also described
    in Table 5. Table 2  Summary of deep learning-based algorithms applied for various
    disease detection and classification. Table 3  Summary of various deep learning
    algorithms and their advantages and disadvantages. 3.4. Cloud Computing CC is
    computing service provision through the Internet ranging from servers to software
    viz. storage space, networking, data communication service, data security challenges
    (privacy, security, interoperability, and reliability), data analytics, and software.
    Cloud can be used in any domain differing from business to healthcare for storing,
    managing, analyzing, and processing data, and it also provides services to manage
    real-time software and applications. Cloud service deployment is categorized into
    three types, i.e., private cloud, public cloud, and hybrid cloud [ 72]. A private
    cloud is owned by an individual organization or company, and it is placed at an
    organization’s data center [ 73]. Private cloud resources can be supplied externally
    or internally at the organization itself and in this cloud maintenance of infrastructures
    and services are done on a private network. Public clouds are owned and managed
    by third-party vendors, for example, Microsoft Azure, Amazon Elastic Compute Cloud
    (EC2), IBM’s Cloud, etc. all the services and resources in the public cloud are
    maintained by vendors (cloud providers) the user can access these services and
    resources through Internet browsers [ 74]. Private and public clouds are integrated
    to form a hybrid cloud; the hybrid cloud provides the facility to access public
    cloud services from the private cloud which is known as cloud bursting technology.
    As shown in Figure 8, cloud renders three service layers viz, Infrastructure as
    a Service (IaaS), Software as a Service (SaaS), and Platform as a Service (PaaS).
    (a) IaaS (Infrastructure as a Service): according to membership clients are serviced
    by a virtual environment comprising of virtual, networking, and storage devices.
    The billing system is also provided to clients where one can demand and pay for
    the required infrastructure. (b) SaaS (Software as a Service): this provides data
    center and software accessibility based on membership. It manages software installation,
    update, and recovery; this software can be accessed remotely through the Internet.
    (c) PaaS (Platform as a Service): this provides a platform and interfaces where
    the clients can create, develop, manage, test, and deploy applications by utilizing
    the interface provided. It also provides a framework where various users can remotely
    perform collaborative work.    Figure 8  Cloud services—IaaS, PaaS, and SaaS.
    Since the cloud is abundant in services like data storage, data maintenance, and
    processing, it can be employed in the healthcare sector for storing patients’
    data and it is also helpful to provide different remote services to both the patients
    and hospital staff [ 75]. CC in healthcare advances the organization’s efficiency
    and simultaneously lowers the cost. It benefits the healthcare system in various
    terms like ease in medical data storage, sharing, and maintenance, and provides
    an automatic backend. Since machine learning and deep learning work well and give
    fine results on large datasets and whenever needed physicians need to access patients’
    health information; these two challenges of the healthcare system can be met by
    cloud infrastructure integrated with IoT devices [ 76]. The two key concerns of
    a cloud-based IoT system are security, and privacy of data, and it should endorse
    the deployment of ML and DL models on it. Figure 9 describes the advantages of
    the cloud in the healthcare system. Characteristics of cloud-based IoT systems
    are as follows: (1) Cloud data storage: it is a CC model which stores data on
    the Internet. To diagnose and analyze any disease, data play a major role; consequently,
    storing data in a repository (database) are necessary to conduct research. It
    assists the hospitals and clinics to store a massive number of patient and staff
    data in a database that can be recovered in case of any disaster. Durability,
    agility, and anywhere-anytime access to data. Object storage, block storage, and
    file storage are three types of cloud storage methods. The advantages of cloud
    storage are information maintenance and management, deployment time, and affordable
    ownership cost [ 77]. (2) Data processing and analysis: the cloud offers different
    data processing techniques such as computer offloading, online data processing,
    electronic data processing, real-time processing, and machine learning; among
    all these, the most applicable methods are machine learning, specified data mining,
    and offloading processing. Complex data can be processed by the computational
    offloading method; it transmits the raw data to the cloud where computational
    resources are applied for further processing. Data analysis can be performed by
    employing machine and deep learning techniques such as SVM, KNN, decision trees,
    random forests, CNN, LSTM, and autoencoders, whereas data mining techniques are
    utilized for extracting adequate information from repositories. The beneficial
    impact of deploying high-potential computing integrated with processing on smartphones
    is the execution of sophisticated algorithms with ease producing accurate and
    robust outcomes, and the smartphone service life is extended because of fewer
    internal computations. (3) Data cleaning: noise is a significant modification
    in image pixel value; while data acquisition and transmission noise are initiated
    in sensed data which decreases the diagnosis performance, thus there is a need
    for data cleaning. Filters are used for this purpose; an appropriate filter like
    a mean filter, median filter, Gaussian filter, and wiener filers can be applied
    for denoising. Image filtering replaces the pixel value with the average neighbor
    pixel value and thus filtering smooths out the image. (4) Emergency alert: the
    sudden downfall of health endangers patients’ lives peculiarly in case of dreadful
    diseases; thus, the emergency warning system is important and becomes a protective
    safeguard for patients’ lives. This challenge can be overcome by the cloud-based
    IoT system which identifies serious situations by data analysis process and alerts
    the physicians or guardians eventually.    Figure 9  Advantageous of cloud in
    healthcare. 3.5. Fog Computing Fog computing is a novel approach that extends
    CC to its networking edge. It is a decentralized computing environment where some
    devices and services are managed by the smart device at the networking edge and
    rests are managed at remote data hubs like the cloud. It has been determined that
    fog computing can be considered a platform/framework to support IoT. Fog is considered
    an additional layer of a decentralized network and it is intricately connected
    to CC and IoT. The benefit of adopting fog computing in organizations is that
    it provides various alternatives for data processing which is the major task in
    all organizations as shown in Figure 10. For example, in various fields (like
    hospitals, manufacturing, and business) data should be processed at the initial
    stage to respond quickly [ 78]. It is implemented to enhance the system efficiency
    and to decrease data quantity transferring from physical components to the cloud.
    Since traditional CC approaches transfer complete data to data centers resulting
    in latency problems.    Figure 10  Fog computing architecture. Nodes in fog networks
    are computing, storage devices, and networking devices. The working of fog computing
    depends on sensors, controllers, and actuators where sensors are utilized to acquire
    data, controllers transfer data to actuators or any other devices [ 79]. Then,
    data are analyzed, optimal patterns are defined by actuators and this information
    is transmitted to end-users through smartphones. The fog layer in IoT architecture
    works on data instantly and the delay-sensitive data generated by the user at
    the edge is analyzed promptly. This instant action minimizes time, and cost, unlike
    cloud operations. Diverse domain applications can be implemented on fog platforms
    viz, enterprise, healthcare, transportation, electricity, smart cities, smart
    buildings, etc. Therefore, fog computing surpasses CC by building up a distributed
    platform for IoT to address embedded devices and sensors’ demands for data storage,
    analysis, and data processing. 4. Smart Healthcare Applications and Challenges
    4.1. Applications Smart healthcare applications are software’s implemented to
    generate, gather, maintain, and data related to both patients and health organizations.
    This data is utilized for performing different tasks like remote patient health
    monitoring [ 80], generating patient records, planning treatment, disease detection,
    sensing patient conditions, and so on. Smart healthcare systems benefit patients,
    physicians, guardians, healthcare centers, and insurance organizations. Smart
    Healthcare System (SHS) is a healthcare service and maintenance system that works
    with integrated technologies like IoT, wearable devices, Internet for the exchange
    of information, and it binds the patients, medical staff, healthcare institutions,
    guardians, and data intelligently into a common platform to assist remotely. It
    is an aggregate of various areas viz, patient monitoring and management, detection
    of diseases, prevention, and diagnosis, decision-making systems, virtual assistance,
    drug discovery, health center management, and assisting drug and medical research.
    A package of distinct technologies viz, IoT, artificial intelligence, cloud computing,
    fog computing, edge computing [ 81, 82], big data, the Internet, sensors, wearable
    devices, applied sciences, and nanotechnology collectively embodies “smart healthcare”.
    The SHS will facilitate connection among all concerned bodies and acknowledge
    that all the associates receive the necessary services. Concisely SHS can be defined
    as the medical information structure of the top level. Prospects of doctors and
    patients in healthcare systems are discussed as follows: (a) Patients: wearable
    devices or sensors can be used to control/manage and monitor patient health, remote
    services, elderly people monitoring, and access to data like records and test
    reports. (b) Doctors: various diagnosis and decision-support systems can be utilized,
    and can access patient data, and monitor patients. 4.2. Challenges Integration
    of cloud-based IoT and ML techniques in the medical field set out the signature
    outcome in everyday life which benefits both the patients and the hospital staff.
    Despite its benefits for the patients, it also leads to some risky challenges
    which demand viable solutions. Certain significant challenges are listed as follows
    [ 83– 87]: (1) Security and privacy: since cloud-based IoT systems are ubiquitous,
    enhancing fundamental issues like security, privacy and integrity play a key role
    in the integration of different technologies. To meet various data security challenges
    like data availability, integrity, and authenticity certain measures should be
    considered to preserve data stored in the cloud. Personal data theft may occur
    when these security challenges are not addressed and when data becomes vulnerable
    to third parties it gets complicated as they can perform unauthorized activities
    on data. Therefore, a profound security system should be developed for decentralized
    communication devices among IoT devices and the cloud [ 88]. (2) Quality of services
    (QoS): this is the foremost paradigm while addressing the overall performance
    outcome of integrated networks. In the case of enormous data storage and data
    exchange among IoT devices and cloud/data banks, QoS service maintenance is the
    paramount challenge. Client demands to cloud platform an effective and robust
    data management as they might be delay sensitive. Therefore, deploying QoS service
    in applications prevents data loss. The optimal solution for cloud-based IoT applications
    is using IPv6 protocol as it provides significant quality features along with
    QoS assurance. (3) Standard protocol support: as there is no common standard IoT
    architecture various protocols are used to communicate and transfer data in the
    network. Despite deploying homogenous nodes’ primary protocols like Zigbee, CoAp,
    6LOWPAN, and Z-wave remaining heterogeneous, it rises to inconsistent conflicts.
    This may lead to a major challenge when IoT and cloud are combined. Therefore,
    there is a need to build standard protocols enabling Cloud-based IoT architectures.
    (4) Delay and bandwidth-limited: decentralized cloud platforms provide unlimited
    resources and various services, but it does not assure service access through
    delay and latency. The solution for an optimum efficiency system is using high
    bandwidth to transmit data. To eradicate delay and latency issues, a fog computing
    approach can be implemented as an intermediate layer between IoT and the cloud.
    Apart from the above-mentioned challenges, some of them are flexibility and technological
    advancement, proficiency in the healthcare sector, efficient energy and power
    consumption, wearable device knowledge, and issues related to hardware during
    data storing and exchanging such as controlling memory, and system performance.
    5. Case Studies 5.1. Brain Cancer Uncontrolled division and growth of abnormal
    cells in any part of the brain is known as Brain cancer. Symptoms and diagnosis
    of a brain tumor depend on the type of tumor, location, and size. Thus, detecting
    which part is affected by the tumor is an extremely challenging task in segmenting
    the cancerous part from the noncancerous part. Several challenges had been addressed
    by many researchers by using various datasets like BRATS, BTI, and BMI [ 89, 90].
    Some of such works are presented in this section. Guo et al [ 91] proposed two
    deep learning algorithms, i.e., 2D-CNN and 3D-CNN for 2D and 3D image datasets.
    The model was optimized by combing two models’ outcomes. Zhao et al. [ 92] developed
    a 3D model called as Voxel Classification Model. Fully CNN (FCNN) and Conditional
    Random Fields (CRF) algorithms were used for training the model and three datasets
    were used for performance analysis, i.e., BRATS2013, BRATS 2015, and BRATS 2016;
    the segmentation model was built with T1, T2, T1c, and flair information. Heba
    et al. [ 93] proposed a model for classifying brain tumors as per DNN and DWT
    [ 94] and PCA was used for feature extraction. Three types of malignant brain
    tumors were classified, glioblastoma, sarcoma, and metastatic bronchogenic carcinoma;
    66 MRI real images were collected from Harvard Medical School website (https://www.med.harvard.edu/AANLIB/home.html).
    In [ 95] deep transfer learning models were adopted to build a brain classification
    model and for feature extraction, GoogleNet was implemented. Three types of brain
    cancer were classified as meningioma glioma and pituitary tumors. The proposed
    model addressed the challenge of training the system with higher accuracy using
    fewer samples. DenseNet201 and deep transfer learning algorithms [ 96] for multiclass
    brain tumor classification. Feature selection was done using Entropy–Kurtosis-based
    High Feature Values (EKbHFV) and Modified Genetic Algorithm (MGA) techniques from
    BRATS2018 and BRATS2019 datasets [ 97]. 5.2. Breast Cancer A profusion of research
    has taken place in the last research in the field of breast cancer detection and
    diagnosis; some of this research are specified in the present section. Spanhol
    et al. [ 98] developed a system using CNN variant ALexNet for classifying malignant
    from benign using histopathological images. In [ 99] Ertosun and Rubin implemented
    a model to detect the subsistence of mass in mammography (input) images and later
    it localizes the mass from those images. A deep learning model was designed in
    [ 100] to detect mitosis, feature extraction techniques, i.e., CNN was used, and
    then the output of this was fed to the SVM classifier to detect the tumor in the
    breast. A deep contour-aware network was developed for mitosis detection, using
    breast histopathology scans [ 101]. Xu et al. [ 102] proposed a classification
    model using Stacked Sparse Autoencoder (SSAE) for classifying nuclei from histopathology
    breast images. A greedy algorithm was used for optimizing the SSAE. Furthermore,
    breast cancer was detected using a mammography image dataset. In [ 103, 104] a
    hybrid model (i.e., CNN + SVM) was developed to detect mass from mammographic
    scans. CNN was trained using mammogram blemishes and the fully connected layer
    was used to extract high-level features then these features were used for training
    the SVM classifier. Kim et al [ 105] proposed a 3D multi-view using CNN to analyze
    bilateral features using Digital Breast Tomosynthesis (DBT). Volume of Interest
    (VOI) was obtained from the input volume and to extract features from VOI two
    different CNN algorithms were implemented. Yu et al. [ 106] proposed a deep learning-enabled
    breast cancer diagnosis system for remote healthcare via the 5 G mechanism. The
    proposed technique is carried out in three steps: first the input is acquired
    from hospitals through 5 G technology, then a transfer learning algorithm is applied
    to procure a diagnostic system. At last, this model is deployed on an edge server
    for remote diagnosis. 5.3. Diabetics Detection Diabetes is an incurable and long-lasting
    illness caused due to increase in glucose in the blood and it has become increasingly
    common among people irrespective of sexual orientation, race, age, habits, etc.
    The primary energy means is glucose and the pancreas secreted hormone, i.e., insulin
    regulates the metabolism of the body and controls the glucose level in the blood.
    The body converts the sugar into energy with the help of insulin and stores the
    excess. If ample insulin is not produced in the body diabetes is caused as blood
    glucose cannot be regulated and remains in the blood. If the blood glucose level
    exceeds a minimal value, it leads to various diseases. According to medical research,
    diabetes is not completely curable, but it is treatable and if it is left untreated
    it emerges into various diseases viz., heart stroke, brain stroke, kidney failure,
    blindness, and even may lead to death. Thus, it is requisite to detect diabetes
    at its early stage, thereby diagnosing and treating it to prevent disease advancement
    as it aids to death and further diseases. Immense research is going on detecting
    and diagnosing diabetes at its early stage. Vast medical data can be found through
    various resources like lab reports, medical images, Electronic Health Record (EHR),
    and clinical reports but the significant challenge is data understanding and interception.
    Several methods and techniques are proposed for diabetic detection. In [ 107]
    different machine learning techniques had been applied viz, SVM, decision tree,
    and Naive Bayes, and in [ 108] a hybrid method is used, i.e., Principal Component
    Analysis and Adaptive Neuro-Fuzzy Interface System (PCA and ANFIS). ResNet is
    developed in [ 109] to address the vanishing gradient problem. ANN architecture
    is proposed to predict diabetics in [ 110]; the proposed model is used to minimize
    the error rate of the function during training and recorded 87% accuracy in prediction,
    and the estimated error rate is 0.01%. In [ 111], a computer-aided system had
    been developed to detect diabetic retinopathy by using digital signals from retinal
    images. The major aim of the work is to classify nonproliferative diabetic retinopathy
    from retinal images. [ 112] CNN approach is developed for segmenting blood vessels
    of diabetic patients followed by the classification, and then extraction of discriminant
    patterns is done. [ 113] A shallow CNN framework is developed. In [ 114], residual
    network is developed to classify the retinopathy images automatically. 6. Conclusion
    Advancement and increasing superiority of cloud-based IoT and DL techniques had
    become the key elements of healthcare applications. Cloud and IoT integration
    in the healthcare sector is mostly recommended due to its effective services like
    immense data repositories, data availability, optimum performance, network balancing,
    and computing infrastructures. Most of the research had been done on cloud-based
    IoT healthcare systems emphasizing background technologies, and system architectures.
    This study presents an intense review of cloud and IoT integration with machine
    learning and deep learning techniques in healthcare. We reviewed the literature
    thoroughly and preferred significant and contemporary studies to determine the
    techniques and research gaps. This study also presents cloud services, and advantages
    in healthcare, IoT architecture, various communication protocols, sensors, ML,
    and DL techniques, and challenges. In addition, a thorough analysis of their advantages
    and disadvantages, and challenges are discussed. This study gives an insight to
    the researchers and can exert them the opportunity to begin their research by
    choosing an application/domain from the available methods discussed. With adequate
    adherence to security and privacy measures, the cloud-based IoT and ML healthcare
    systems are accurate and of complete assistance to patients, guardians, and hospital
    staff. Table 4  Various sensors are utilized in healthcare to sense and acquire
    data. Table 5  Short- and long-range communication devices. Data Availability
    Data set will be provided upon request. Conflicts of Interest The authors have
    no conflicts of interest to declare. Authors’ Contributions All authors contributed
    to the design, analysis, writing, and revising of this manuscript. All authors
    approved the submitted version. References D. J. Cook, A. S. Crandall, B. L. Thomas,
    and N. C. Krishnan, “CASAS: a smart home in a box,” Computer, vol. 46, no. 7,
    pp. 62–69, 2013. View at: Publisher Site | Google Scholar T. D. Rajeeve, A. J.
    P. Antony, and T. Prathiba, “Wireless sensor based healthcare monitoring system
    using cloud,” in Proceedings of the International Conference on Inventive Systems
    and Control (ICISC), Coimbatore, India, January 2017. View at: Google Scholar
    G. B. Mohammad, S. Shitharth, S. A. Syed et al., “Mechanism of internet of things
    (IoT) integrated with radio frequency identification (RFID) technology for healthcare
    system,” Mathematical Problems in Engineering, vol. 2022, Article ID 4167700,
    8 pages, 2022. View at: Publisher Site | Google Scholar D. H. Prasad and M. Srikanth,
    “Mobile healthcare monitoring system in mobile cloud computing,” Int J Comput
    Technol Appl, vol. 6, no. 1, pp. 43–46, 2015. View at: Google Scholar F. Serpush,
    M. B. Menhaj, B. Masoumi, and B. Karasfi, “Wearable sensor-based human activity
    recognition in the smart healthcare system,” Computational Intelligence and Neuroscience,
    vol. 2022, Article ID 1391906, 31 pages, 2022. View at: Publisher Site | Google
    Scholar U. Ahmad, H. Song, A. Bilal, S. Saleem, and A. Ullah, “Securing insulin
    pump system using deep learning and gesture recognition,” in Proceedings of the
    IEEE International Conference On Trust, Security And Privacy In Computing And
    Communications/12th IEEE International Conference On Big Data Science And Engineering
    (TrustCom/BigDataSE), New York, NY, USA, August 2018. View at: Google Scholar
    A. E. Eshratifar, M. S. Abrishami, and M. Pedram, “JointDNN: an efficient training
    and inference engine for intelligent mobile cloud computing services,” IEEE Transactions
    on Mobile Computing, vol. 20, no. 2, pp. 565–576, 2021. View at: Publisher Site
    | Google Scholar S. Tuli, N. Basumatary, S. S. Gill et al., “HealthFog: an ensemble
    deep learning based smart healthcare system for automatic diagnosis of heart diseases
    in integrated IoT and fog computing environments,” Future Generation Computer
    Systems, vol. 104, pp. 187–200, 2020. View at: Publisher Site | Google Scholar
    T. Ahmad and H. Chen, “A review on machine learning forecasting growth trends
    and their real-time applications in different energy systems,” Sustainable Cities
    and Society, vol. 54, Article ID 102010, 2020. View at: Publisher Site | Google
    Scholar S. Shitharth, G. B. Mohammad, and K. Sangeetha, “Predicting epidemic outbreaks
    using IOT, artificial intelligence and cloud,” The Fusion of Internet of Things,
    Artificial Intelligence, and Cloud Computing in Health Care, Springer, Berlin,
    Germany, pp. 197–222, 2021. View at: Google Scholar F. Ali, S. El-Sappagh, S.
    R. Islam et al., “A smart healthcare monitoring system for heart disease prediction
    based on ensemble deep learning and feature fusion,” Information Fusion, vol.
    63, pp. 208–222, 2020. View at: Publisher Site | Google Scholar A. Ismail, S.
    Abdlerazek, and I. M. El-Henawy, “Development of smart healthcare system based
    on speech recognition using support vector machine and dynamic time warping,”
    Sustainability, vol. 12, no. 6, p. 2403, 2020. View at: Publisher Site | Google
    Scholar P. Rizwan, G. S. Pradeep Ghantasala, R. Sekaran, D. Gupta, and M. Ramachandran,
    “Smart healthcare and quality of service in IoT using grey filter convolutional
    based cyber physical system,” Sustainable Cities and Society, vol. 59, Article
    ID 102141, 2020. View at: Publisher Site | Google Scholar A. Subasi, K. Khateeb,
    T. Brahimi, and A. Sarirete, “Human activity recognition using machine learning
    methods in a smart healthcare environment,” Innovation in Health Informatics,
    Springer, Berlin, Germany, pp. 123–144, 2020. View at: Google Scholar S. Pereira,
    R. Meier, V. Alves, M. Reyes, and C. A. Silva, “Automatic brain tumor grading
    from MRI data using convolutional neural networks and quality assessment,” in
    Proceedings of the In Understanding and interpreting machine learning in medical
    image computing applications, pp. 106–114, Granada, Spain, September 2018. View
    at: Google Scholar G. Manogaran, R. Varatharajan, D. Lopez, P. M. Kumar, R. Sundarasekar,
    and C. Thota, “A new architecture of Internet of Things and big data ecosystem
    for secured smart healthcare monitoring and alerting system,” Future Generation
    Computer Systems, vol. 82, pp. 375–387, 2018. View at: Publisher Site | Google
    Scholar V. Vikas and S. Ananthula, “Internet of things (IoT) based smart health
    care system,” in Proceedings of the International Conference on signal processing,
    communication, power and embedded system (SCOPES), Odisha, India, October 2016.
    View at: Google Scholar C. Luca, D. D. Donno, L. Mainetti et al., An IoT-aware
    architecture for smart healthcare systems, vol. 2, no. 6, pp. 515–526, 2015. G.
    Loubet, A. Takacs, and D. Dragomirescu, “Implementation of a battery-free wireless
    sensor for cyber-physical systems dedicated to structural health monitoring applications,”
    IEEE Access, vol. 7, pp. 24679–24690, 2019. View at: Publisher Site | Google Scholar
    Z. N. K. Swati, Q. Zhao, M. Kabir et al., “Brain tumor classification for MR images
    using transfer learning and fine-tuning,” Computerized Medical Imaging and Graphics,
    vol. 75, pp. 34–46, 2019. View at: Publisher Site | Google Scholar M. M. Rahman,
    D. R. Dipta, and M. M. Hasan, “Dynamic time warping assisted SVM classifier for
    Bangla speech recognition,” in Proceedings of the International Conference on
    Computer, Communication, Chemical, Material and Electronic Engineering (IC4ME2),
    Rajshahi, Bangladesh, November 2018. View at: Google Scholar P. Verma, S. K. Sood,
    and H. Kaur, “A fog-cloud based cyber physical system for ulcerative colitis diagnosis
    and stage classification and management,” Microprocessors and Microsystems, vol.
    72, Article ID 102929, 2020. View at: Publisher Site | Google Scholar I. Mohamad,
    I. Z. Mutaqin, and R. G. Utomo, “Implementation of Dynamic Time Warping algorithm
    on an Android based application to write and pronounce Hijaiyah letters,” in Proceedings
    of the International Conference on Cyber and IT Service Management, Chengdu ,
    China, August 2016. View at: Google Scholar M. De Brouwer, F. Ongenae, P. Bonte,
    and F. De Turck, “Towards a cascading reasoning framework to support responsive
    ambient-intelligent healthcare interventions,” Sensors, vol. 18, no. 10, p. 3514,
    2018. View at: Publisher Site | Google Scholar B. Modu, N. Polovina, Y. Lan, S.
    Konur, A. T. Asyhari, and Y. Peng, “Towards a predictive analytics-based intelligent
    malaria outbreak warning system,” Applied Sciences, vol. 7, no. 8, p. 836, 2017.
    View at: Publisher Site | Google Scholar Y. Li, J. Huang, H. Zhou, and N. Zhong,
    “Human emotion recognition with electroencephalographic multidimensional features
    by hybrid deep neural networks,” Applied Sciences, vol. 7, no. 10, p. 1060, 2017.
    View at: Publisher Site | Google Scholar L. Yue, X. Gong, J. Li, H. Ji, M. Li,
    and A. K. Nandi, “Hierarchical feature extraction for early Alzheimer’s disease
    diagnosis,” IEEE Access, vol. 7, pp. 93752–93760, 2019. View at: Publisher Site
    | Google Scholar M. M. Islam, F. Karray, R. Alhajj, and J. Zeng, “A review on
    deep learning techniques for the diagnosis of novel coronavirus (COVID-19),” IEEE
    Access, vol. 9, pp. 30551–30572, 2021. View at: Publisher Site | Google Scholar
    S. Wang, Y. Zha, W. Li et al., “A fully automatic deep learning system for COVID-19
    diagnostic and prognostic analysis,” European Respiratory Journal, vol. 56, no.
    2, Article ID 2000775, 2020. View at: Publisher Site | Google Scholar P. R. Jeyaraj
    and E. R. S. Nadar, “Deep Boltzmann machine algorithm for accurate medical image
    analysis for classification of cancerous region,” Cognitive Computation and Systems,
    vol. 1, no. 3, pp. 85–90, 2019. View at: Publisher Site | Google Scholar A. S.
    Al-Waisy, M. Abed Mohammed, S. Al-Fahdawi et al., “COVID-DeepNet: hybrid multimodal
    deep learning system for improving COVID-19 pneumonia detection in chest X-ray
    images,” Computers, Materials & Continua, vol. 67, no. 2, pp. 2409–2429, 2021.
    View at: Publisher Site | Google Scholar R. Aznar-Gimeno, G. Labata-Lezaun, A.
    Adell-Lamora, D. Abadía-Gallego, R. del-Hoyo-Alonso, and C. González-Muñoz, “Deep
    learning for walking behaviour detection in elderly people using smart footwear,”
    Entropy, vol. 23, no. 6, p. 777, 2021. View at: Publisher Site | Google Scholar
    G. Wang, Q. Li, L. Wang, Y. Zhang, and Z. Liu, “Elderly fall detection with an
    accelerometer using lightweight neural networks,” Electronics, vol. 8, no. 11,
    p. 1354, 2019. View at: Publisher Site | Google Scholar B. Zhou, K. Wu, P. Lv
    et al., “A new remote health-care system based on moving robot intended for the
    elderly at home,” Journal of healthcare engineering, pp. 1–11, 2018. View at:
    Publisher Site | Google Scholar A. O. Aseeri, “Uncertainty-aware deep learning-based
    cardiac arrhythmias classification model of electrocardiogram signals,” Computers,
    vol. 10, no. 6, p. 82, 2021. View at: Publisher Site | Google Scholar P. Agrahari,
    A. Agrawal, and N. Subhashini, “Skin cancer detection using deep learning,” Futuristic
    Communication and Network Technologies, Springer, Berlin, Germany, pp. 179–190,
    2022. View at: Google Scholar M. S. Ali, M. S. Miah, J. Haque, M. M. Rahman, and
    M. K. Islam, “An enhanced technique of skin cancer classification using deep convolutional
    neural network with transfer learning models,” Machine Learning with Applications,
    vol. 5, Article ID 100036, 2021. View at: Publisher Site | Google Scholar P. Afshar,
    K. N. Plataniotis, and A. Mohammadi, “Capsule networks’ interpretability for brain
    tumor classification via radiomics analyses,” in Proceedings of the IEEE International
    Conference on Image Processing (ICIP), Taipei, Taiwan, September 2019. View at:
    Google Scholar A. Saber, M. Sakr, O. M. Abo-Seida, A. Keshk, and H. Chen, “A novel
    deep-learning model for automatic detection and classification of breast cancer
    using the transfer-learning technique,” IEEE Access, vol. 9, pp. 71194–71209,
    2021. View at: Publisher Site | Google Scholar K. Jabeen, M. A. Khan, M. Alhaisoni
    et al., “Breast cancer classification from ultrasound images using probability-based
    optimal deep learning feature fusion,” Sensors, vol. 22, no. 3, p. 807, 2022.
    View at: Publisher Site | Google Scholar A. I. Newaz, A. K. Sikder, M. A. Rahman,
    and A. S. Uluagac, “Healthguard: a machine learning-based security framework for
    smart healthcare systems,” 2019, https://arxiv.org/abs/1909.10565. View at: Google
    Scholar S. Ramasamy Ramamurthy and N. Roy, “Recent trends in machine learning
    for human activity recognition—a survey,” WIREs Data Mining and Knowledge Discovery,
    vol. 8, no. 4, p. 1254, 2018. View at: Publisher Site | Google Scholar X. Wu,
    V. Kumar, J. Ross Quinlan et al., “Top 10 algorithms in data mining,” Knowledge
    and Information Systems, vol. 14, no. 1, pp. 1–37, 2008. View at: Publisher Site
    | Google Scholar G. Holmes, B. Pfahringer, R. Kirkby, E. Frank, and a. M. Hall,
    “Multiclass alternating decision trees,” in Proceedings of the European Conference
    on Machine Learning, pp. 161–172, Helsinki, Finland, August 2002. View at: Google
    Scholar Y. Liu and K.-S. Choi, “Using machine learning to diagnose bacterial sepsis
    in the critically ill patients,” in Proceedings of the International Conference
    on Smart Health, pp. 223–233, Hong Kong, China, June 2017. View at: Google Scholar
    A. S. Fialho, F. Cismondi, S. M. Vieira, S. R. Reti, J. M. Sousa, and S. N. Finkelstein,
    “Data mining using clinical physiology at discharge to predict ICU readmissions,”
    Expert Systems with Applications, vol. 39, no. 18, pp. 13158–13165, 2012. View
    at: Publisher Site | Google Scholar R. Viegas, C. M. Salgado, S. Curto, J. P.
    Carvalho, S. M. Vieira, and S. N. Finkelstein, “Daily prediction of ICU readmissions
    using feature engineering and ensemble fuzzy modeling,” Expert Systems with Applications,
    vol. 79, pp. 244–253, 2017. View at: Publisher Site | Google Scholar B. Zheng,
    J. Zhang, S. W. Yoon, S. S. Lam, M. Khasawneh, and S. Poranki, “Predictive modeling
    of hospital readmissions using metaheuristics and data mining,” Expert Systems
    with Applications, vol. 42, no. 20, pp. 7110–7120, 2015. View at: Publisher Site
    | Google Scholar Y. Chai, L. He, Q. Mei, and H. Li, “Deep learning through two-branch
    convolutional neuron network for glaucoma diagnosis,” in Proceedings of the International
    Conference on Smart Health, pp. 191–201, Hong Kong, China, June 2017. View at:
    Google Scholar J. Zhang, Y. Luo, Z. Jiang, and X. Tang, “Regression analysis and
    prediction of mini-mental state examination score in Alzheimer’s disease using
    multi-granularity whole-brain segmentations,” in Proceedings of the International
    Conference on Smart Health, Hong Kong, China, June 2017. View at: Google Scholar
    Y. Dong, Q. Wang, Q. Zhang, and J. Yang, “Classification of cataract fundus image
    based on retinal vascular information,” in Proceedings of the International Conference
    on Smart Health, Hong Kong, China, June 2016. View at: Google Scholar K. P. Murphy,
    Machine Learning: A Probabilistic Perspective, MIT press, Cambridge, MA, USA,
    2012. Z. Liang, G. Zhang, J. X. Huang, and Q. V. Hu, “Deep learning for healthcare
    decision making with EMRs,” in Proceedings of the IEEE International Conference
    on Bioinformatics and Biomedicine (BIBM), Madrid, Spain, December 2014. View at:
    Google Scholar S. Selvarajan, H. Manoharan, T. Hasanin et al., “Biomedical signals
    for healthcare using Hadoop infrastructure with artificial intelligence and fuzzy
    logic interpretation,” Applied Sciences, vol. 12, no. 10, p. 5097, 2022. View
    at: Publisher Site | Google Scholar A. Kumar, Z. J. Zhang, and H. Lyu, “Object
    detection in real time based on improved single shot multi-box detector algorithm,”
    EURASIP Journal on Wireless Communications and Networking, vol. 2020, pp. 204–218,
    2020. View at: Publisher Site | Google Scholar J. Ženko, M. Kos, and I. Kramberger,
    “Pulse rate variability and blood oxidation content identification using miniature
    wearable wrist device,” in Proceedings of the International Conference on Systems,
    Signals and Image Processing (IWSSIP), Sofia, Bulgaria, June 2016. View at: Google
    Scholar A. O. Khadidos, S. Shitharth, A. O. Khadidos, K. Sangeetha, and K. H.
    Alyoubi, “Healthcare data security using IoT sensors based on random hashing mechanism,”
    Journal of Sensors, vol. 2022, Article ID 8457116, 17 pages, 2022. View at: Publisher
    Site | Google Scholar R. Udendhran and G. Yamini, “The pivotal role of internet
    of things and ubiquitous computing in healthcare,” Applications in Ubiquitous
    Computing, Springer, Berlin, Germany, pp. 3–12, 2021. View at: Google Scholar
    H. Wang, S. Chen, F. Xu, and Y.-Q. Jin, “Application of deep-learning algorithms
    to MSTAR data,” in Proceedings of the IEEE International Geoscience and Remote
    Sensing Symposium (IGARSS), Milan, Italy, July 2015. View at: Google Scholar P.
    Rajan Jeyaraj and E. R. S. Nadar, “Smart-monitor: patient monitoring system for
    IoT-based healthcare system using deep learning,” IETE Journal of Research, vol.
    68, no. 2, pp. 1435–1442, 2022. View at: Publisher Site | Google Scholar Y. Shu,
    C. Li, Z. Wang, W. Mi, Y. Li, and T.-L. Ren, “A pressure sensing system for heart
    rate monitoring with polymer-based pressure sensors and an anti-interference post
    processing circuit,” Sensors, vol. 15, no. 2, pp. 3224–3235, 2015. View at: Publisher
    Site | Google Scholar P. D. Singh, G. Dhiman, and R. Sharma, “Internet of things
    for sustaining a smart and secure healthcare system,” Sustainable computing: Informatics
    and Systems, vol. 33, Article ID 100622, 2022. View at: Publisher Site | Google
    Scholar Y. Zhang, M. Berthelot, and B. Lo, “Wireless Wearable Photoplethysmography
    Sensors for Continuous Blood Pressure Monitoring,” in Proceedings of the IEEE
    Wireless Health (WH), pp. 1–8, Bethesda, MD, USA, October 2016. View at: Google
    Scholar B. Alhayani, A. S. Kwekha-Rashid, H. B. Mahajan et al., “5G standards
    for the Industry 4.0 enabled communication systems using artificial intelligence:
    perspective of smart healthcare system,” Applied Nanoscience, pp. 1–11, 2022.
    View at: Publisher Site | Google Scholar D. Griggs, M. Sharma, A. Naghibi et al.,
    “Design and development of continuous cuff-less blood pressure monitoring devices,”
    in Proceedings of the IEEE sensors, pp. 1–3, Orlando, FL, USA, October 2016. View
    at: Google Scholar H. Lin, W. Xu, N. Guan, D. Ji, Y. Wei, and W. Yi, “Noninvasive
    and continuous blood pressure monitoring using wearable body sensor networks,”
    IEEE Intelligent Systems, vol. 30, no. 6, pp. 38–48, 2015. View at: Publisher
    Site | Google Scholar S.-H. Chang, R.-D. Chiang, S.-J. Wu, and W.-T. Chang, “A
    context-aware, interactive M-health system for diabetics,” IT professional, vol.
    18, no. 3, pp. 14–22, 2016. View at: Publisher Site | Google Scholar C. F. Pasluosta,
    H. Gassner, J. Winkler, J. Klucken, and B. M. Eskofier, “An emerging era in the
    management of Parkinson’s disease: wearable technologies and the internet of things,”
    IEEE journal of biomedical and health informatics, vol. 19, no. 6, pp. 1873–1881,
    2015. View at: Publisher Site | Google Scholar G. Wolgast, C. Ehrenborg, A. Israelsson,
    J. Helander, E. Johansson, and H. Manefjord, “Wireless body area network for heart
    attack detection [Education Corner],” IEEE Antennas and Propagation Magazine,
    vol. 58, no. 5, pp. 84–92, 2016. View at: Publisher Site | Google Scholar S. B.
    Baker, W. Xiang, and I. Atkinson, “Internet of things for smart healthcare: technologies,
    challenges, and opportunities,” IEEE Access, vol. 5, pp. 26521–26544, 2017. View
    at: Publisher Site | Google Scholar J. S. K. P. G. B. M. D. S. N. M. S. Divakaran,
    S. K. Prashanth, G. B. Mohammad et al., “Improved handover authentication in fifth-generation
    communication networks using fuzzy evolutionary optimisation with nanocore elements
    in mobile healthcare applications,” Journal of Healthcare Engineering, vol. 2022,
    Article ID 2500377, 8 pages, 2022. View at: Publisher Site | Google Scholar M.
    Aazam, E.-N. Huh, M. St-Hilaire, C.-H. Lung, and I. Lambadaris, “Cloud of things:
    integration of IoT with cloud computing,” Robots and Sensor Clouds, Springer,
    Berlin, Germany, pp. 77–94, 2016. View at: Google Scholar A. Kumar, “Design of
    secure image fusion technique using cloud for privacy-preserving and copyright
    protection,” International Journal of Cloud Applications and Computing, vol. 9,
    no. 3, pp. 22–36, 2019. View at: Publisher Site | Google Scholar N. Fernando,
    S. W. Loke, and W. Rahayu, “Mobile cloud computing: a survey,” Future Generation
    Computer Systems, vol. 29, no. 1, pp. 84–106, 2013. View at: Publisher Site |
    Google Scholar M. Aazam, P. P. Hung, and E.-N. Huh, “Smart gateway based communication
    for cloud of things,” in Proceedings of the international conference on intelligent
    sensors, sensor networks and information processing (ISSNIP), Singapore, April
    2014. View at: Google Scholar S. Aguzzi, D. Bradshaw, M. Canning et al., “Definition
    of a research and innovation policy leveraging cloud computing and IoT combination,”
    Final Report, European Commission, SMART, vol. 37, 2013. View at: Google Scholar
    P. Siarry, M. A. Jabbar, R. Aluvalu, A. Abraham, and A. Madureira, The Fusion
    of Internet of Things, Artificial Intelligence, and Cloud Computing in Health
    Care, Springer, Berlin, Germany, 2021. M. Aazam and E.-N. Huh, “Fog computing:
    the cloud-iot\\/ioe middleware paradigm,” IEEE Potentials, vol. 35, no. 3, pp.
    40–44, 2016. View at: Publisher Site | Google Scholar M. Aazam and E.-N. Huh,
    “Fog computing and smart gateway based communication for cloud of things,” Fog
    Computing and Smart Gateway Based Communication for Cloud of Things, Springer,
    Berlin, Germany, 2014. View at: Google Scholar A. V. L. N. Sujith, G. S. Sajja,
    V. Mahalakshmi, S. Nuhmani, and B. Prasanalakshmi, “Systematic review of smart
    health monitoring using deep learning and Artificial intelligence,” Neuroscience
    Informatics, vol. 2, no. 3, Article ID 100028, 2022. View at: Publisher Site |
    Google Scholar M. Hartmann, U. S. Hashmi, and A. Imran, “Edge computing in smart
    health care systems: review, challenges, and research directions,” Transactions
    on Emerging Telecommunications Technologies, vol. 33, no. 3, p. 3710, 2022. View
    at: Publisher Site | Google Scholar R. Rajavel, S. K. Ravichandran, K. Harimoorthy,
    P. Nagappan, and K. R. Gobichettipalayam, “IoT-based smart healthcare video surveillance
    system using edge computing,” Journal of Ambient Intelligence and Humanized Computing,
    vol. 13, no. 6, pp. 3195–3207, 2022. View at: Publisher Site | Google Scholar
    A. Sultan and A. Mohd, “Security & privacy challenges in IoT-based health cloud,”
    in Proceedings of the International Conference on Computational Science and Computational
    Intelligence (CSCI), Las Vegas, NV, USA, December 2016. View at: Google Scholar
    M. R. Allan Cook, M. A. Ferrag, L. A. Maglaras, Y. He, K. Jones, and H. Janicke,
    “Internet of cloud: security and privacy issues,” Cloud Computing for Optimization:
    Foundations, Applications, and Challenges, Springer, Berlin, Germany, pp. 271–301,
    2018. View at: Google Scholar M. Díaz, C. Martín, and B. Rubio, “State-of-the-art,
    challenges, and open issues in the integration of Internet of things and cloud
    computing,” Journal of Network and Computer Applications, vol. 67, pp. 99–117,
    2016. View at: Publisher Site | Google Scholar J. Dizdarević, F. Carpio, A. Jukan,
    and X. Masip-Bruin, “A survey of communication protocols for internet of things
    and related challenges of fog and cloud computing integration,” ACM Computing
    Surveys, vol. 51, no. 6, pp. 1–29, 2019. View at: Publisher Site | Google Scholar
    A. Ali, M. F. Pasha, J. Ali et al., “Deep learning based homomorphic secure search-able
    encryption for keyword search in blockchain healthcare system: a novel approach
    to cryptography,” Sensors, vol. 22, no. 2, p. 528, 2022. View at: Publisher Site
    | Google Scholar A. Kumar, “A cloud-based buyer-seller watermarking protocol (CB-BSWP)
    using semi-trusted third party for copy deterrence and privacy preserving,” Multimedia
    Tools and Applications, vol. 81, no. 15, pp. 21417–21448, 2022. View at: Publisher
    Site | Google Scholar D. H. Hubel and T. N. Wiesel, “Binocular interaction in
    striate cortex of kittens reared with artificial squint,” Journal of Neurophysiology,
    vol. 28, no. 6, pp. 1041–1059, 1965. View at: Publisher Site | Google Scholar
    M. Puttagunta and S. Ravi, “Medical image analysis based on deep learning approach,”
    Multimedia Tools and Applications, vol. 80, no. 16, pp. 24365–24398, 2021. View
    at: Publisher Site | Google Scholar Y. Guo, Y. Gao, and D. Shen, “Deformable MR
    prostate segmentation via deep feature learning and sparse patch matching,” IEEE
    Transactions on Medical Imaging, vol. 35, no. 4, pp. 1077–1089, 2016. View at:
    Publisher Site | Google Scholar X. Zhao, Y. Wu, G. Song, Z. Li, Y. Zhang, and
    Y. Fan, “A deep learning model integrating FCNNs and CRFs for brain tumor segmentation,”
    Medical Image Analysis, vol. 43, pp. 98–111, 2018. View at: Publisher Site | Google
    Scholar M. Heba, E.-S. A. El-Dahshan, E.-S. M. El-Horbaty, and A.-B. M. Salem,
    “Classification using deep learning neural networks for brain tumors,” Future
    Computing and Informatics Journal, vol. 3, no. 1, pp. 68–71, 2018. View at: Publisher
    Site | Google Scholar A. Kumar, “A review on implementation of digital image watermarking
    techniques using LSB and DWT,” Information and Communication Technology for Sustainable
    Development, Springer, Berlin, Germany, pp. 595–602, 2020. View at: Google Scholar
    S. a. P. M. A. Deepak and P. Ameer, “Brain tumor classification using deep CNN
    features via transfer learning,” Computers in Biology and Medicine, vol. 111,
    Article ID 103345, 2019. View at: Publisher Site | Google Scholar T. Ganji, M.
    S. Velpuru, and R. Dugyala, “Multi variant handwritten Telugu character recognition
    using transfer learning,” IOP Conference Series: Materials Science and Engineering,
    vol. 1042, no. 1, Article ID 012026, 2021. View at: Publisher Site | Google Scholar
    M. I. Sharif, M. A. Khan, M. Alhussein, K. Aurangzeb, and M. Raza, “A decision
    support system for multimodal brain tumor classification using deep learning,”
    Complex & Intelligent Systems, vol. 8, pp. 1–14, 2021. View at: Google Scholar
    F. A. Spanhol, L. S. Oliveira, C. Petitjean, and L. Heutte, “Breast cancer histopathological
    image classification using convolutional neural networks,” in Proceedings of the
    international joint conference on neural networks (IJCNN), Vancouver, British
    Columbia, Canada, July 2016. View at: Google Scholar M. G. Ertosun and D. L. Rubin,
    “Probabilistic visual search for masses within mammography images using deep learning,”
    in Proceedings of the IEEE International Conference on Bioinformatics and Biomedicine
    (BIBM), Washington, DC, USA, July 2015. View at: Google Scholar A. Albayrak and
    G. Bilgin, “Mitosis detection using convolutional neural network based features,”
    in Proceedings of the IEEE 17th International symposium on computational intelligence
    and informatics (CINTI), Budapest, Hungary, November 2016. View at: Google Scholar
    H. Chen, X. Qi, L. Yu, and P.-A. Heng, “DCAN: deep contour-aware networks for
    accurate gland segmentation,” in Proceedings of the IEEE conference on Computer
    Vision and Pattern Recognition, Las Vegas, Nevada, USA, June 2016. View at: Google
    Scholar B. Xu, N. Wang, T. Chen, and M. Li, “Empirical evaluation of rectified
    activations in convolutional network,” 2015, https://arxiv.org/abs/1505.00853.
    View at: Google Scholar I. Wichakam and P. Vateekul, “Combining deep convolutional
    networks and SVMs for mass detection on digital mammograms,” in Proceedings of
    the international conference on knowledge and smart technology (KST), Chon buri,
    Thailand, January 2016. View at: Google Scholar V. R. Allugunti, “Breast cancer
    detection based on thermographic images using machine learning and deep learning
    algorithms,” International Journal of Engineering in Computer Science, vol. 4,
    no. 1, pp. 49–56, 2022. View at: Google Scholar D. H. Kim, S. T. Kim, and Y. M.
    Ro, “Latent feature representation with 3-D multi-view deep convolutional neural
    network for bilateral analysis in digital breast tomosynthesis,” in Proceedings
    of the IEEE international conference on acoustics, speech and signal processing
    (ICASSP), Shanghai, China, March 2016. View at: Google Scholar K. Yu, L. Tan,
    L. Lin, X. Cheng, Z. Yi, and T. Sato, “Deep-learning-empowered breast cancer auxiliary
    diagnosis for 5GB remote E-health,” IEEE Wireless Communications, vol. 28, no.
    3, pp. 54–61, 2021. View at: Publisher Site | Google Scholar D. Sisodia and D.
    S. Sisodia, “Prediction of diabetes using classification algorithms,” Procedia
    Computer Science, vol. 132, pp. 1578–1585, 2018. View at: Publisher Site | Google
    Scholar K. Polat and S. Güneş, “An expert system approach based on principal component
    analysis and adaptive neuro-fuzzy inference system to diagnosis of diabetes disease,”
    Digital Signal Processing, vol. 17, no. 4, pp. 702–710, 2007. View at: Publisher
    Site | Google Scholar K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
    for image recognition,” in Proceedings of the IEEE conference on computer vision
    and pattern recognition, Las Vegas, Nevada, USA, June 2016. View at: Google Scholar
    N. S. El_Jerjawi and S. S. Abu-Naser, “Diabetes prediction using artificial neural
    network,” International Journal of Advanced Science and Technology, vol. 121,
    2018. View at: Google Scholar E. V. Carrera, A. González, and R. Carrera, “Automated
    detection of diabetic retinopathy using SVM,” in Proceedings of the IEEE XXIV
    international conference on electronics, electrical engineering and computing
    (INTERCON), Cusco, Peru, June 2017. View at: Google Scholar F. Girard, C. Kavalec,
    and F. Cheriet, “Joint segmentation and classification of retinal arteries/veins
    from fundus images,” Artificial Intelligence in Medicine, vol. 94, pp. 96–109,
    2019. View at: Publisher Site | Google Scholar H. Pratt, F. Coenen, D. M. Broadbent,
    S. P. Harding, and Y. Zheng, “Convolutional neural networks for diabetic retinopathy,”
    Procedia Computer Science, vol. 90, pp. 200–205, 2016. View at: Publisher Site
    | Google Scholar D. Zhang, W. Bu, and X. Wu, “Diabetic retinopathy classification
    using deeply supervised ResNet,” in Proceedings of the 2017 IEEE SmartWorld, Ubiquitous
    Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications,
    Cloud & Big Data Computing, pp. 1–6, San Francisco, CA, USA, August 2017. View
    at: Google Scholar Copyright Copyright © 2022 Syed Saba Raoof and M. A. Saleem
    Durai. This is an open access article distributed under the Creative Commons Attribution
    License, which permits unrestricted use, distribution, and reproduction in any
    medium, provided the original work is properly cited. PDF Download Citation Download
    other formats Order printed copies Views 1521 Downloads 950 Citations 11 About
    Us Contact us Partnerships Blog Journals Article Processing Charges Print editions
    Authors Editors Reviewers Partnerships Hindawi XML Corpus Open Archives Initiative
    Fraud prevention Follow us: Privacy PolicyTerms of ServiceResponsible Disclosure
    PolicyCookie PolicyCopyrightModern slavery statementCookie Preferences"'
  inline_citation: '>'
  journal: Contrast Media and Molecular Imaging
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Comprehensive Review on Smart Health Care: Applications, Paradigms, and
    Challenges with Case Studies'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Basharat A.
  - Mohamad M.M.B.
  citation_count: '4'
  description: The Internet of Things (IoT) is a massive and evolving technology that
    has the potential to revolutionize our world. It enables devices to connect to
    the internet via sensors, drones, satellites, and so on. Agriculture is critical
    to the economy of any country. With the world's population growing, traditional
    agricultural methods are finding it hard to fulfill the world's food needs. It
    is the need of the hour to incorporate IoT in agriculture to digitalize the farming
    methods to increase the production level. IoT-based modernization in agriculture
    helps the farmers to check soil moisture, humidity, water level, climate conditions,
    crop production, etc. IoT applications also promise to maximize the comfort level
    and efficiency and offer automation to the farmers. With rising cyber-Attacks
    on IoT, smart devices need better security, confidentiality, integrity, and recovery
    from attacks. In this review, our main focus is on the security challenges faced
    by resource constraints, low storage, and low power devices. The basic security
    requirements for IoT devices in smart agriculture are briefly reviewed. Some state-of-The-Art
    security technologies are explored, including machine learning, fog computing,
    edge computing, SDN, and Blockchains. This analysis of security concerns and security
    measures will give future scholars a research direction.
  doi: 10.1109/ICSSA54161.2022.9870979
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 4th International Confer...
    Security Challenges and Solutions for Internet of Things based Smart Agriculture:
    A Review Publisher: IEEE Cite This PDF Asma Basharat; Mohd. Murtadha Bin Mohamad
    All Authors 3 Cites in Papers 444 Full Text Views Abstract Document Sections I.
    Introduction II. Organization of the Paper III. Architecture of Smart Agriculture
    IV. Security Challenges In Smart Agriculture V. Existing Solutions To Improve
    Security Authors Figures References Citations Keywords Metrics Abstract: The Internet
    of Things (IoT) is a massive and evolving technology that has the potential to
    revolutionize our world. It enables devices to connect to the internet via sensors,
    drones, satellites, and so on. Agriculture is critical to the economy of any country.
    With the world’s population growing, traditional agricultural methods are finding
    it hard to fulfill the world’s food needs. It is the need of the hour to incorporate
    IoT in agriculture to digitalize the farming methods to increase the production
    level. IoT-based modernization in agriculture helps the farmers to check soil
    moisture, humidity, water level, climate conditions, crop production, etc. IoT
    applications also promise to maximize the comfort level and efficiency and offer
    automation to the farmers. With rising cyber-attacks on IoT, smart devices need
    better security, confidentiality, integrity, and recovery from attacks. In this
    review, our main focus is on the security challenges faced by resource constraints,
    low storage, and low power devices. The basic security requirements for IoT devices
    in smart agriculture are briefly reviewed. Some state-of-the-art security technologies
    are explored, including machine learning, fog computing, edge computing, SDN,
    and Blockchains. This analysis of security concerns and security measures will
    give future scholars a research direction. Published in: 2022 4th International
    Conference on Smart Sensors and Application (ICSSA) Date of Conference: 26-28
    July 2022 Date Added to IEEE Xplore: 05 September 2022 ISBN Information: DOI:
    10.1109/ICSSA54161.2022.9870979 Publisher: IEEE Conference Location: Kuala Lumpur,
    Malaysia SECTION I. Introduction The Internet of Things (IoT) is a device-connecting
    technology. As technology advances, there are more gadgets that use sensors, embedded
    systems, actuators, and cloud computing. The use of these devices and computing
    systems improves the communication between devices. IoT simply involves the internet
    for coordination among devices wirelessly which automates things and reduces human
    efforts [1] [2]. IoT is playing a significant role in monitoring vital information
    regarding crop health. It is a huge challenge to cope with food demands due to
    the increasing population worldwide. As a climatic condition, low rain and other
    conditions cause low productivity. IoT provides an atomizing solution to these
    problems by connecting sensors through the internet using protocols. All these
    may be connected to a cloud server which saves data like current temperature,
    humidity in the air, the PH level of the land, and soil moisture and accordingly
    updates the farmer on necessary steps he should take for better productivity.
    Smart farming is effective in enhancing productivity which is directly proportional
    to growth in a country’s economy. The use of IoT devices in smart irrigation support
    farmers in meticulously managing the agricultural fields. This modernization help
    farmers to decide which fertilizer or fungicide will be most effective to enhance
    crop production [3]. Automatic farming machines are also designed which are using
    Artificial intelligence to facilitate the farmer in seed spreading and pesticide
    spray machines [4]. Some applications of IoT are depicted in Fig. 1. Fig. 1. Applications
    of IoT Show All There is evidence of a boost in production by using IoT-based
    applications in agriculture [5]. Smart irrigation is one such example that involves
    components like decision support tools to manage check soil moisture, water level
    fertilization, automated farming systems, frost protection, and remote monitoring.
    These actions are performed with the help of IoT devices and intelligent software
    [6] –[9]. An example is Malthouse, which is an intelligent system used in precision
    agriculture [11]. In this study, we will discuss the various challenges faced
    by IoT in smart agriculture and analyze the existing solutions to mitigate these
    issues. A. Abbreviations and Acronyms Some abbreviations used throughout the study
    are explained in Table I. SECTION II. Organization of the Paper The paper is divided
    into the sections listed below. Section 3 will go into the architecture of IoT-based
    smart agriculture systems. Section 4 delves into the security risks and obstacles
    associated with installing IoT infrastructure in agricultural 4.0. Section 5 discussed
    several state-of-the-art methods intended to reduce and tackle existing security
    concerns. Fig. 2 depicts the paper’s organization. SECTION III. Architecture of
    Smart Agriculture Fig. 3 depicts the three layers of IoT-based smart agriculture
    architecture: perception layer, network layer, and application layer. Table I
    List of Abbreviations Fig. 2. Organization of the paper Show All Fig. 3. Architecture
    of IoT based Smart agriculture Show All A. Perception Layer It is the most significant
    layer in gathering data physically through detectors, drones, as well as other
    hardware components. These components are linked to a microcontroller. It facilitates
    the collecting of real-time data. The data includes moisture, warmth, soil quality,
    water level, and soil PH level, among other things. Before going to the cloud,
    the acquired data is temporarily kept locally and thoroughly reviewed after precise
    error extraction. B. Network Layer This layer’s primary duty is to securely transport
    data to a cloud server. It encompasses both long- and short-range wireless networks,
    such as 4G, Wi-Fi, Bluetooth, LTE, ZigBee, and others. C. Application Layer It
    involves protocols and host communication networks such as AMQP, CoAP, and MQTT.
    Data was sent to this layer from the network layer. This information is used to
    demonstrate the current pattern. The program analyses data and makes intelligent
    recommendations to steadily increase the system’s throughput [13] [14]. SECTION
    IV. Security Challenges In Smart Agriculture IoT based smart agriculture systems
    face many security challenges and issues. We have discussed some of the challenges
    that adversely affect the implementation and thus the productivity of the farm.
    The challenges explored in this paper are shown in Fig. 4. A. Integrity Data integrity
    assures that it has not been altered by an unauthorized entity. The goal is to
    jeopardize the data’s integrity. The attacker intends to obtain unauthorized access
    to IoT devices and alter their data and configurations. This could lead to data-generating
    issues and mistakes. Misinformation attacks that distribute incorrect information
    about crops and the environment endanger farm data integrity. The attacker can
    send out real-looking bogus farm statistical reports. They may claim crop disease
    outbreaks or system malfunctions. Finally, analyzing data and proving the initial
    result incorrect would take time and farm money. Forgery attacks, Trojan horse
    attacks, fabrication, and biometric template attacks are other types of integrity
    attacks [19]. Fig. 4. Security Challenges Show All B. Availability It verifies
    that the system’s services are available to genuine users. The goal of this approach
    is to render agricultural IoT equipment inoperable. This sort of Denial of Service,
    also known as Distributed Denial of Service, occurs when an attacker floods a
    server with illegal data or wirelessly induces fake data into IoT software. Botnet
    attack is an example of an availability attack [20]. C. Authentication Authentication
    guarantees that only authorized and authentic users have access to data. It safeguards
    data and devices from unwanted modification. Hackers use identity forgery to get
    authorized access to devices and nodes. To obtain access to agricultural devices
    and data, the attacker acts as a legitimate node. Identity-based attacks include
    masquerading, spoofing, and impersonation attacks [21]. Fake nodes are presented
    as real nodes in a masquerade attack. The security of agricultural devices and
    data can be exploited in a replay attack by intercepting agriculture data packets
    connecting IoT devices with an entry point and transmitting them to their targets.
    Agriculture sensory and other farm RFID systems are the main targets of spoofing.
    By collecting information from the farm network and inserting false information
    on nodes and RFID systems, suspicious activity is gained to farm IoT devices and
    networks by the invader. D. Confidentiality It ensures that the attacker is unable
    to understand information even if he has been able to intercept it. Unregistered
    intruders can eavesdrop on IoT-based farm devices and networks using access points
    at the sensor layer to misdirect and undermine smart agriculture systems. This
    may be a simple passive attack, where the attacker may steal the data without
    causing any disruption or it can lead to advanced attacks where the intruder manipulates
    into making wrong decisions and performing harmful activities. An adversary uses
    three attack techniques to steal the identity of a computer in an identity-based
    attack. First, the intruder uses eavesdropping to obtain data from farm devices.
    Second, the attacker uses identifying information gathered by eavesdropping to
    track and trace the farm unit. Finally, he uses a redundant password to undermine
    the security of farm devices and data [22]. E. Privacy It ensures that only the
    information that has been allowed is revealed. Any information regarding identity
    or behavior should not be intercepted or traceable. Intruders may capture and
    manipulate smart devices at the agro perception layer in order to acquire illegal
    data access and violate consumer privacy. Accessing agricultural data acquired
    at the IoT sensory layer, such as crop growth, soil quality, humidity, rainfall,
    soil type, and pH settings, can provide insight into a farmer’s actions. Farm
    data must be protected from unauthorized access because it can lead to a variety
    of attacks. The principles of farm privacy are confidentiality, anonymity, and
    autonomy [24]. F. Non-repudiation It assures that the text’s provider cannot later
    deny sending a signal. Any system element, even storage servers, cannot reject
    receiving data or control commands in the interim [25]. SECTION V. Existing Solutions
    To Improve Security One of the current research fields is developing solutions
    and approaches to mitigate and regulate security concerns in various IoT applications.
    There have been many solutions and techniques proposed, as shown in table II.
    IoT has several security problems, making its widespread implementation a difficult
    challenge. Emerging techniques, such as blockchain and Software-Defined Networking,
    can provide IoT security and privacy, along with flexibility and scalability [32].
    The researchers of [33] employed Blockchain technology to safeguard data. To remove
    security difficulties such as DDoS and other large assaults, they used the DolevYao
    (DY) threat model as well as Canetti and Krawczyk’s (CK-adversary model). Data
    acquisition, reliability, and safety are all issues that must be addressed for
    an effective system. The intrusion detection system (IDS) detects data invaders
    and gateways in order to prevent hostile agents from getting unauthorized remote
    access and control. In [34], signature wrapping attacks were prevented using XPath
    and FastXPath. They proposed using dark web technology to secure the secrecy of
    blockchains and servers in order to address protection, verification, and integrity
    concerns. Table II Related Work SDN is used by the SerIoT (Safe and Secure IoT)
    project to securely send IoT data from sensors to cloud servers [29]. The Cognitive
    Packet Network (CPN) and its Random Neural Network have both employed reinforcement
    learning (RNN). They presented an IoT-driven farm architecture that integrates
    the LoRa and Fog Computing paradigms [29]. Authors have proposed Blockchain and
    edge computing a hybrid approach to mitigate the Scalability, security, privacy,
    transparency, and data protection issues [36]. Use of big data to manage and save
    data on weather, soil, humidity and other, etc. Massive data in smart agriculture
    causes loss of data so to avoid this situation they used big data [37]. Edge computing
    is used by researchers to avoid authentication, authorization, and security risks
    [38]. Fog computing was also implemented by researchers to mitigate the security
    challenges that are flexibility, Access control, and Client protection [39]. IoT
    applications hold a lot of promise in terms of improving user comfort, performance,
    and automation. We also require high levels of protection, safety, authentication,
    and attack recovery. [40]. SDN/NFV can be used to optimize resource use in Edge-IoT
    networks [30]. To tackle IoT stability, dependability, and privacy, the authors
    employed a Double Deep-Q Learning technique for controlling virtual data flows
    in SDN/NFV utilizing an Edge-IoT architecture. Black SDN, a highly secure SDN
    that improves network performance, protection, and privacy while also securing
    metadata and payload within each layer has been proposed in [30]. Authors Figures
    References Citations Keywords Metrics More Like This Satellite Internet of Things
    for Smart Agriculture Applications: A Case Study of Computer Vision 2023 20th
    Annual IEEE International Conference on Sensing, Communication, and Networking
    (SECON) Published: 2023 Age-Critical and Secure Blockchain Sharding Scheme for
    Satellite-Based Internet of Things IEEE Transactions on Wireless Communications
    Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: '4th International Conference on Smart Sensors and Application: Digitalization
    for Societal Well-Being, ICSSA 2022'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Security Challenges and Solutions for Internet of Things based Smart Agriculture:
    A Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nicolas C.
  - Naila B.
  - Amar R.C.
  citation_count: '20'
  description: Smart agriculture researchers bring numerous tools and prospects to
    the farm ecosystem to improve its productivity and, mainly, its sustainability.
    Artificial Intelligence (AI) is widely used in precision agriculture as Internet
    of Things (IoT) technologies have brought a huge volume of data to exploit to
    provide useful insights for farmers such as weather prediction, pest development
    detection, or harvest time estimation. AI algorithms are mostly executed in the
    cloud due to their inherent computing constraints, thus requiring the different
    sensors to offload their data to the appropriate server. Depending on the amount
    and volume of data exchanged, the need for computer offloading may induce privacy,
    security, and latency issues in addition to weighting on the sensor's battery
    consumption as wireless transmission methods have a high-energy demand. To overcome
    this difficulty, recent research has tried to bring AI computation closer to the
    end device with edge or fog computing and more recently with the Tiny Machine
    Learning (TinyML) paradigm that aims to embed the AI algorithm directly into the
    sensor's microcontroller. In that context, this paper proposes a prototype of
    smart sensor capable of detecting fruits presence with TinyML. We then study the
    energy consumption of our system in different IoT scenarios.
  doi: 10.1109/ICUFN55119.2022.9829675
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 Thirteenth International...
    TinyML Smart Sensor for Energy Saving in Internet of Things Precision Agriculture
    platform Publisher: IEEE Cite This PDF Chollet Nicolas; Bouchemal Naila; Ramdane-Cherif
    Amar All Authors 13 Cites in Papers 1320 Full Text Views Abstract Document Sections
    I. Introduction II. State of the Art IV. Experimentation V. Conclusion Authors
    Figures References Citations Keywords Metrics Abstract: Smart agriculture researchers
    bring numerous tools and prospects to the farm ecosystem to improve its productivity
    and, mainly, its sustainability. Artificial Intelligence (AI) is widely used in
    precision agriculture as Internet of Things (IoT) technologies have brought a
    huge volume of data to exploit to provide useful insights for farmers such as
    weather prediction, pest development detection, or harvest time estimation. AI
    algorithms are mostly executed in the cloud due to their inherent computing constraints,
    thus requiring the different sensors to offload their data to the appropriate
    server. Depending on the amount and volume of data exchanged, the need for computer
    offloading may induce privacy, security, and latency issues in addition to weighting
    on the sensor’s battery consumption as wireless transmission methods have a high-energy
    demand. To overcome this difficulty, recent research has tried to bring AI computation
    closer to the end device with edge or fog computing and more recently with the
    Tiny Machine Learning (TinyML) paradigm that aims to embed the AI algorithm directly
    into the sensor’s microcontroller. In that context, this paper proposes a prototype
    of smart sensor capable of detecting fruits presence with TinyML. We then study
    the energy consumption of our system in different IoT scenarios. Published in:
    2022 Thirteenth International Conference on Ubiquitous and Future Networks (ICUFN)
    Date of Conference: 05-08 July 2022 Date Added to IEEE Xplore: 20 July 2022 ISBN
    Information: ISSN Information: DOI: 10.1109/ICUFN55119.2022.9829675 Publisher:
    IEEE Conference Location: Barcelona, Spain SECTION I. Introduction It is estimated
    that the world population will increase to reach approximately 9 billion by 2050.
    The Food and Agriculture Organization (FAO) of the united state nation, therefore,
    estimates that by then, food production must increase by around 60 % if we want
    to ensure global food security [1]. To answer this raising concern, Smart Farming
    (SF) technologies, also called Precision Agriculture (PA) are reshaping the agricultural
    practices and industry to make them more productive and sustainable [2]. SF tools
    gather information and communication technologies (ICT) such as Artificial Intelligence,
    IoT (Internet of Things) Platforms, and Robotics to provide sustainable modern
    solutions. One of the fields that could bring significant progress to agriculture
    is the use of AI-powered computer vision analysis to evaluate the crop’s growth
    process or detect unwanted situations such as pest development or weed multiplication.
    Such AI algorithms rely heavily on the cloud due to their need for heavy computing
    capacity. This asks for adapted network architectures and raise concern in term
    of privacy, security, and latency but moreover increase the need for energy as
    heavy data offloading call for a larger data throughput [3]. However, battery
    lifetime is a crucial parameter for agricultural purposes as sensors can be spread
    over long distances without access to electricity [4]. To overcome those issues,
    researchers have looked into bringing the computation process closer to the end
    device that collects the data with fog or edge computing methods to avoid unnecessary
    communications. However, more recent research on embedded AI has shown that the
    Tiny ML paradigm now allows micro-controller with small computing capabilities
    to perform AI directly on the device [5]. In this context future agricultural
    sensors could perform AI inference directly on the device and only communicates
    the result as small messages through energy-efficient Low power networks (LPWAN)
    such as LoRaWAN. We propose in this paper a prototype of a smart intelligent sensor
    for fruit presence detection in the context of the smart farm. This proposal is
    based on the development of embedded vision AI algorithms with Tiny ML. This will
    allow the farmer to know when and where fruits need attention (for example when
    to harvest them or when to apply fertilizer) thanks to communication between the
    AI sensor and the Smart Farm environment using LoRaWAN. An energy analysis of
    the system in different network scenarios is then conducted to validate our hypothesis
    that TinyML usage can improve battery lifetime. Fig. 1. Overall Architecture Show
    All SECTION II. State of the Art Researches on Tiny ML are relatively new and
    could be the answer to a wide number of applications. In the Agricultural domain,
    the potential applications are vast such as livestock management or insect detection
    [6], but SF implementations are still limited and we assume that they will be
    rising in the future. Most research concentrate on the industrial domain, a good
    survey on recent advancements in Tiny ML has been made by authors in [7]. For
    other practical implementations, TinyML has been used in multiple scenarios such
    as adaptive traffic Control [8] or wildlife conservation [9]. TinyML also offers
    multiple advantages over Fog, Edge, or Cloud computing including improvement in
    terms of privacy, security, latency, and energy as addressed by the authors in
    [10]. A. Scenario We propose a battery-powered sensor with a camera placed in
    front of a fruit field. The sensor uses TinyML algorithm to infer the number of
    fruits. Then, the sensor sends the number to a decision platform through a LoRaWAN
    network to extend the battery lifetime. This number is then analyzed and a decision
    is made regarding the necessary action to take, for example, harvesting if enough
    fruits are present or fertilizer application if fruits are too small or absent.
    The necessary actions are then communicated to the performers as seen in Figure
    1. Our architecture is made up of four main components: 1) Smart sensor In charge
    of performing the TinyML, it is a microcontroller equipped with a low-resolution
    camera and a LoRaWan communication module. We choose LoRaWAN as it is a widely
    used LPWAN protocol in agricultural Wireless Sensor Network (WSN) as discussed
    by the authors in [4]. 2) Gateway The gateway is in charge of the link between
    the local LoRaWAN network and the Internet. 3) Cloud decision platform The information
    received from the sensor is stored in a database. The system then decides regarding
    other parameters such as time, weather, or farmer’s occupation, to ask for an
    action. 4) Performer The decided action is then communicated to the performer
    for example a farmer or a robot. B. Environment development In order to implement
    our proposal, we choose the following modules: For the smart sensor, an Arduino
    Portenta H7 microcontroller with a Lora Vision shield is used. Its 32-bit architecture
    and low power consumption abilities make it an adapted choice for TinyML algorithms.
    Moreover, the Arduino ecosystem facilitates the implementation of complex algorithms
    into microcontrollers that should help the replicability and comparison of our
    work to other researchers. To create the TinyML algorithm, we used Edge Impulse
    [11], the development platform based on tensor flow lite. Finally, to communicate
    with Lora we use a Laird RG1868 gateway and The Thing Network (TTN) environment
    to store our data online. TTN is a LoRaWAN network server, built on an open-source
    core that allows users to build and manage LoRaWAN networks easily. C. Phases
    In this section, we describe the end-to-end phases implemented in our prototype.
    1) Phase 1: Data collection Collecting data from real devices is the first step
    to train the model. Recent TinyML development is limited to performing on-device
    inference and cannot do on-device learning. The creation process of a TinyML model
    is presented in Figure 2. It consists of first gathering data and then pre-train
    the model independently from the device and afterward deploying it to the hardware
    and finally testing. To collect the data we use the Edge impulse tool to directly
    gather pictures from the Arduino Portenta 7. For our experimentation, we trained
    our model to detect strawberries by collecting 100 pictures containing 0 to 10
    instances of the fruit. Fig. 2. TinyML algorithm building process with Edge Impulse
    Show All 2) Phase 2:TinyML model training To perform our fruit detection we use
    Edge Impulse FOMO (Faster Objects, More Objects) method [11]. It is a novel machine-learning
    algorithm that brings object detection to highly constrained devices. It allows
    the device to count objects, find the location of objects in an image, and track
    multiple objects in real-time using up to 30x less processing power and memory
    than other similar dedicated algorithms such as MobileNet SSD or YOLOv5 [12].
    3) Phase 3: On-device inferring Once the model is deployed on the Hardware, it
    can detect how many fruits are in front of it. To save energy, the inference process
    should be performed only for the minimum amount of time so that the microcontroller
    can stay in sleep mode otherwise. 4) Phase 4: Results transmission After we get
    the fruit number, we use LoRaWAN protocol to communicate it to a cloud decision
    platform. This result could be later processed by the hypothetical decision system
    of our scenario to ask for harvesting. SECTION IV. Experimentation A. Model performance
    Evaluation In order to test our solution, the algorithm is implemented directly
    on the device. For our dataset of 100 images, the accuracy of the FOMO algorithm
    reaches 90.2% according to Edge Impulse testing tools and similar accuracy was
    obtained in real-life tests. The results show that the estimated peak RAM usage
    is 243,9 kb and the firmware size is 77,5 kb, respectively 24.39 % and 3.88 %
    of Arduino’s capacity, leaving space for some improvement opportunities. The average
    inference time that will be later used to calculate the energy consumption of
    the system is 148 ms. You can find the sources on GitHub to reproduce our experiment
    [13]. B. Energy consumption evaluation To show the energy effectiveness of our
    proposal, several simulations were performed using OMNET++ simulator and INET
    Framework [14] on a PC running Ubuntu 20.04 with 16GB RAM and an Intel I7 8565U.
    Three scenarios were considered. The first one, where the TinyML algorithm runs
    directly on the microcontroller and the inference result is communicated through
    LoRaWAN. The second one, where the TinyML algorithm also runs directly on the
    microcontroller, and the result is communicated through WiFi. Finally, the third
    one, where only the taken picture is sent through WiFi to represent data offload
    and to validate the impact of TinyML on energy consumption. IoT device energy
    consumption eval-uation is a complex process as discussed in [15]. For each scenario,
    the device only wakes up from sleep mode once a day. When awake, the device is
    in full power mode during the image capture, the model inference process, and
    the data communication. During the wireless communication process, we also add
    the energy consumption of our network interface regarding the volume of data to
    be transmitted. Either the short message representing the number of fruits detected
    in scenarios 1 and 2 (50 bytes in our platform) or the image size to transmit
    in scenario 3 (20kb). Indeed the volume of data to transmit increase the energy
    consumption of the system and TinyML allows us to send the minimum amount of data
    required. In order to perform the simulation, we need to collect the energy consumption
    characteristics of our hardware. In table I, we gather the values found for the
    different running modes in the hardware datasheet. Thanks to those data the simulator
    can estimate the approximate energy needed and therefore the impact of each transmission
    on the battery lifetime. TABLE I Power consumption of the system Finally, the
    simulator computes the sensor lifetime expectations for a 2000 mA battery until
    every scenario runs out of power. The results for each one are presented in Figure
    3 where the evaluation of the battery level over time can be observed. It appears
    that Scenario 1 where TinyML and Lora are used, is the most energy-efficient one
    as it can last up to 105 days, this is 3 times longer than Scenario 3. Scenario
    2 also show that TinyML can save battery in the case of full WiFi usage as the
    battery last 1,5 time longer than Scenario 3. The battery lifetime expectation
    results are regrouped in Figure 4. . The results validate the hypothesis that
    TinyML sensors could increase battery life in such context since it allows us
    to diminished the need for power-hungry data communication by minimizing the size
    of the messages to transmit. Fig. 3. Evolution of Battery level over time Show
    All Fig. 4. Battery Lifetime of the system in days Show All SECTION V. Conclusion
    In this paper, TinyML and LoRaWAN were used to propose an energy-efficient model
    capable of fruit detection to show the capabilities of such technologies in the
    agricultural domain. Experimentation showed that our model had a 90 % accuracy
    level and was three times more energy-efficient than a cloud-based model for the
    same application, opening the way for a new range of computer vision applications
    in Smart Farming based on battery-powered sensors. Despite promising results,
    the TinyML paradigm presents some limitations regarding on-device learning capabilities,
    as the neural network needs to be pre-trained before being embedded into the microcontroller
    unit. Therefore, the TinyML sensor can not adapt itself to the specific environment
    it will be deployed in. Later research should be conducted on how to update the
    model once the sensors are deployed to increase the accuracy after the aquisition,
    which means performing firmware updates over the air with LPWAN networks for TinyML
    applications. Authors Figures References Citations Keywords Metrics More Like
    This Towards Integration of Wireless Sensor Networks and Cloud Computing 2015
    IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)
    Published: 2015 Providing Information Services for Wireless Sensor Networks through
    Cloud Computing 2012 IEEE Asia-Pacific Services Computing Conference Published:
    2012 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: International Conference on Ubiquitous and Future Networks, ICUFN
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: TinyML Smart Sensor for Energy Saving in Internet of Things Precision Agriculture
    platform
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sharma R.
  - Kumar N.
  - Sharma B.B.
  citation_count: '13'
  description: Today IoT and artificial intelligence are taking over control of industries,
    from healthcare to smart solutions. The applications of artificial intelligence
    is making its impact on almost every sector. The use of AI has been evident in
    the field of agriculture. The agriculture field provides numerous challenges in
    front of researchers regarding soil properties, pest management, irrigation, post-harvest
    management, knowledge gap in farmers, and recent technology. To overcome these
    challenges, AI can play an important role. This review paper provides detailed
    literature on artificial intelligence with advantages and disadvantages addressing
    disease management, post-harvest crop management, soil properties, and crop growth
    management. Moreover, various models and frameworks with their accuracies have
    been discussed.
  doi: 10.1007/978-981-16-8248-3_11
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Recent Innovations in Computing
    pp 135–142Cite as Home Recent Innovations in Computing Conference paper Applications
    of Artificial Intelligence in Smart Agriculture: A Review Ruchika Sharma , Nagesh
    Kumar & Brij Bhushan Sharma   Conference paper First Online: 10 March 2022 692
    Accesses 7 Citations Part of the book series: Lecture Notes in Electrical Engineering
    ((LNEE,volume 832)) Abstract Today IoT and artificial intelligence are taking
    over control of industries, from healthcare to smart solutions. The applications
    of artificial intelligence is making its impact on almost every sector. The use
    of AI has been evident in the field of agriculture. The agriculture field provides
    numerous challenges in front of researchers regarding soil properties, pest management,
    irrigation, post-harvest management, knowledge gap in farmers, and recent technology.
    To overcome these challenges, AI can play an important role. This review paper
    provides detailed literature on artificial intelligence with advantages and disadvantages
    addressing disease management, post-harvest crop management, soil properties,
    and crop growth management. Moreover, various models and frameworks with their
    accuracies have been discussed. Keywords Smart applications Sensors AI Fog computing
    Agriculture Agribusiness Internet of thing Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF 1 Introduction In today’s world of technology, where
    everything is digital. Our thinking process has also changed, and innovations
    are now being done not only with the normal brain but also with the artificial
    one also [1]. This is how artificial intelligence came into the existence. Artificial
    intelligence is a new field with lots of opportunities to explore for researchers
    as it is the process in which man-made machines can be made more intelligent.
    Artificial intelligence should be able to work on deep neural networks, deep learning,
    machine learning, ANN, and CNN. These domains can be used to enhance the working
    capabilities of a particular machine and develop new advance technologies [2].
    Artificial intelligence has already penetrated various major field like agriculture,
    medical services, education, security, finance, and the industrial sector. For
    the implementation of artificial intelligence in any of the above-mentioned fields
    or sectors, you need to understand the process of any machine [3], where the role
    of machine learning becomes very important. Machine learning provides the data
    in a combination of past experiences and statistical data, so that the machine
    can appropriately perform its task to achieve or solve the existing problem presented
    in front of a machine [1] (Fig. 1). Fig. 1 Artificial intelligence taxonomy Full
    size image The perfect example for the above-mentioned dialog is speech-recognition,
    medical services, face recognition, and weather predictions [3], where we can
    feed the machine based on past experiences and statistical data. Machine learning
    is a mathematical approach toward solving a problem where for solutions past data
    of a particular machine is evaluated based on mathematical approaches and intelligent
    machines can be built accordingly. Fuzzy logic, expert systems, artificial neural
    networks, and neuro-fuzzy logic are the methods and logics which were invented
    to make problem-solving simpler [4]. Agriculture is an essential part not only
    for the growth of the country but also to fulfill the food requirements and needs
    of the citizens of any country. Embedded systems and artificial intelligence can
    play a vital role in the development and growth of a particular sector. With the
    increase in population, the demand for food products has also been increased,
    and agricultural land is decreasing day by day [2]. So, traditional farming techniques
    are not capable enough to compete with the required amount of productivity. In
    India, agriculture is one of the most sensitive sectors concerning its economic
    growth. So, the development of new technologies and precision farming techniques
    is the demand of today [3]. This review article discusses the various application
    of artificial intelligence in agriculture. 2 Role of AI in Agriculture Agriculture
    industries need to grow as it is the necessity of the society; various IoT based
    platforms have already been implemented for the different sectors of the agriculture
    industry [5]. Artificial intelligence technologies can also play a crucial role
    in the further development of the industry helping farmers in yielding of healthier
    crops, pest controlling, soil parameters monitoring, atmospheric conditions, organization
    of data, reducing workload of farmers, and improving so tasks related to the farming
    (Fig. 2). Fig. 2 Artificial intelligence in agriculture Full size image 3 Study
    Process The papers are included from last six years. On the basis of these papers,
    some questions are raised. 1. Research Questions Some important questions are:
    Q1. What is role of AI in agriculture? Q2. What are various applications of AI
    in agriculture? Q3. What are various models proposed in last six years? Q4. What
    are the main algorithms used in paper? 2. Sources Papers are taken from various
    databases. These are mentioned below: 1. IEEE 2. Science Direct (Elsevier) 3.
    Springer Nature The papers taken in this paper are from high impact journals.
    These papers are indexed in Scopus, SCI, and Web of Sciences databases. The keywords
    used for search were AI, smart agriculture, precision agriculture, irrigation
    control, crop management, soil management, etc. 4 Literature Survey This literature
    survey includes model proposed for smart agriculture and application of AI in
    agriculture. These categories are summarized in the below section. Eli-Chukwu
    [1] the application of AI has been evident within the rural division as of now.
    Numerous challenges are faced by the sector, in order to maximize the output focus
    needs to maintain on the agricultural procedures to maintain the growth of the
    crops which includes proper knowledge of the crop disease, pest controls, improper
    soil treatment. Moreover, the lack of knowledge in farmers is the biggest issue.
    Authors have presented a literature review in this article which covers the role
    of Artificial Intelligence in the field of agriculture. Ghadiyali et al. [2] a
    new terminology has been introduced by the authors for agribusiness and agricultural
    intelligence. Moreover, authors have also proposed agricultural intelligence architectural
    model which is capable of capturing data from the available data warehouses and
    after implementing a proper data mining techniques to it can transform the data
    and resultants can be shown on dashboard. Jha et al. [3] completed and review
    and concluded that day by day request for nourishment is coming to its tall top
    and without execution of the advanced strategies in farming it is exceptionally
    difficult to realize the expanding request. Agriculture monitoring is the prime
    concern because it makes a difference to diminish work and increment generation.
    Artificial intelligence has been implemented in edit choice and to assist the
    agriculturist within the choice of fertilizers. With the assistance of the database
    which the client has accumulated and indicated to the system, the machine communicates
    among themselves to choose which crop is reasonable for gathering additionally
    the fertilizers which advance the most extreme development. Profound learning
    has a wide reach and its application in the industry has gotten colossal progression.
    Utilizing profound learning is an included advantage over machine learning, and
    it includes profundity to machine learning. Numerous noteworthy strategies can
    guarantee the ranchers with way better crops and legitimate field administration.
    Barenkamp [5] the combination of IoT and artificial intelligence can be very useful
    in agricultural business. A new IoT based gateway for the artificial intelligence
    in agricultural industry have been proposed by the author. IoT-AI platform proposed
    by author can offer a wide range of applications to the users. Author also claimed
    that the proposed system is the first of its kind which can systematically manage
    the diverse problem sets in the production of agricultural products. Dolci [6]
    presented a paper in conference and stated that the agricultural sector has been
    lagged than other business sectors in case of adoption of new technologies biotechnologist
    have enhanced the seeds capacity and they require lesser care, lesser water requirements
    and lesser nutrients. The only issue now available is to use the IoT and artificial
    intelligence in appropriate manner. Very high-end machineries are being used to
    maximize the outcome. With the help of satellite-operated machines, use of sensor
    can enhance the agribusiness output. Bannerjee et al. [7] have segregated enhancements
    achieved by the artificial intelligence in agriculture and provided an overview
    on the various artificial intelligence techniques that can be used in agricultural
    sector. They have provided a literature survey and stated that artificial intelligence
    and IoT techniques have penetrated this field since 1983 onward. Since at that
    point, there have been numerous suggestions and proposed frameworks for improvement
    in agribusiness from the database to decision-making processes. Furthermore, they
    have filtered out each process and stated that the artificial intelligence-based
    systems have proved them self-most reliable, efficient, and effective one. The
    AI based method does not generalize the issue and gives a particular solution
    to a specific characterized complex issue. The literature survey covers major
    breakthroughs within the space of horticulture from early 1980s to 2018. The paper
    examines more than fifty progression in technologies within the sub-space of agribusiness.
    To begin with, it examines penetration of fake neural systems and master frameworks
    to unravel over said issues, then machine learning and fluffy rationale system.
    Lastly, it covers mechanization and IOT within the agribusiness. Ravichandran
    et al. [8] authors have developed a framework based on MATLAB and the reason of
    the framework was to create it helpful for the farmers, it is created on APK stage.
    The source code was written in Eclipse with Java codes within the backhand, and
    the algorithm was developed using MATLAB and ANN tool stash. The full record was
    at that point extracted on the Android stage, so that it can be utilized by smartphones.
    Besides suggesting the trim to the rancher, the framework too has the additional
    advantage of prompting the rancher for the fertilizer to be utilized if the agriculturist
    wishes to utilize the edit of his choice. In Table 1 provided, a comparison of
    various available or developed algorithms for the agricultural sector is given.
    Table 1 Various model proposed for agriculture using artificial intelligence Full
    size table 5 Discussion Artificial intelligence is a crucial technology for each
    and every sector. It has created an impact from industry to healthcare and from
    healthcare to agricultural business. With the combination of IoT devices, fuzzy
    systems artificial intelligence can improve further to fulfill the user requirement
    according to the need of particular application. Furthermore, from the above literature
    survey, it has been clear that in agribusiness sector it can be very helpful to
    the farmers who are still using the traditional farming techniques. Although Artificial
    Intelligence may face several issues while implementing it to the agribusiness.
    These issues can be continuously changing environment which can make testing,
    validation, and successful rollout of such technologies much more laborious than
    in most other industries. Another issue that may occur while implementing artificial
    intelligent to the agriculture sector is the applications developed using the
    artificial intelligence which may be on the costlier side as compared to the other
    technologies offered for the growth of the particular sector. But it also offers
    numerous benefits that other technologies lack off. Like with the proper stored
    data and information according to the environment, it will be easy to maintain
    the various parameters like disease management, post-harvest crop management,
    soil properties, and crop growth management using artificial intelligence. 6 Conclusion
    In countries like India where approximately 17–18% population depend upon the
    agriculture, a proper modernization of the sector is required. To fulfill the
    requirements of food products and dairy products, traditional techniques are not
    enough now. So, from the study that have been completed, this survey can provide
    a literature contribution to the society of research to explore further the uses
    of artificial intelligence in agriculture business. So that growth of this sector
    may take place, and alongside it the country’s GDP growth will also improve. References
    N.C. Eli-Chukwu, Applications of artificial intelligence in agriculture: a review.
    Eng. Technol. Appl. Sci. Res. 9(4), 4377–4383 (2019) Article   Google Scholar   T.
    Ghadiyali, K. Lad, B. Patel, Agriculture intelligence: an emerging technology
    for farmer community, in Proceedings of 2nd International Conference on Emerging
    Applications of Information Technology EAIT 2011, pp. 313–316 (2011) Google Scholar   K.
    Jha, A. Doshi, P. Patel, M. Shah, A comprehensive review on automation in agriculture
    using artificial intelligence. Artif. Intell. Agric. 2, 1–12 (2019) Google Scholar   K.
    Anand, C. Jayakumar, M. Muthu, S. Amirneni, Automatic drip irrigation system using
    fuzzy logic and mobile technology, in Proceedings of 2015 IEEE International Conference
    on Technological Innovation in ICT for Agricultural and Rural Development TIAR
    2015, no. Tiar, pp. 54–58 (2015) Google Scholar   M. Barenkamp, A new IoT gateway
    for artificial intelligence in agriculture, in 2nd International Conference on
    Electrical, Communication and Computer Engineering ICECCE 2020, no. June, pp.
    12–13 (2020) Google Scholar   R. Dolci, IoT solutions for precision farming and
    food manufacturing: artificial intelligence applications in digital food. Proc.
    Int. Comput. Softw. Appl. Conf. 2, 384–385 (2017) Google Scholar   G. Banerjee,
    U. Sarkar, I. Ghosh, A radial basis function network based classifier for detection
    of selected tea pests. Int. J. Adv. Res. Comput. Sci. Softw. Eng. 7(5), 665–669
    (2017) Article   Google Scholar   G. Ravichandran, R.S. Koteeshwari, Agricultural
    crop predictor and advisor using ANN for smartphones, in Proceeding of 1st International
    Conference on Emerging Trends in Engineering Technology and Science ICETETS 2016,
    no. February 2016 (2016) Google Scholar   W.L. Chen, Y.B. Lin, F.L. Ng, C.Y. Liu,
    Y.W. Lin, ricetalk: rice blast detection using internet of things and artificial
    intelligence technologies. IEEE Internet Things J. 7(2), 1001–1010 (2020) Article   Google
    Scholar   D. Shadrin, A. Menshchikov, A. Somov, G. Bornemann, J. Hauslage, M.
    Fedorov, Enabling precision agriculture through embedded sensing with artificial
    intelligence. IEEE Trans. Instrum. Meas. 69(7), 4103–4113 (2020) Article   Google
    Scholar   G. Arvind, V.G. Athira, H. Haripriya, R.A. Rani, S. Aravind, Automated
    irrigation with advanced seed germination and pest control, in Proceeding of 2017
    IEEE Technological Innovation in ICT for Agricultural and Rural Development TIAR
    2017, vol. 2018-January, no. Tiar, pp. 64–67 (2018) Google Scholar   J.R.D. Cruz,
    R.G. Baldovino, A.A. Bandala, E.P. Dadios, Water usage optimization of smart farm
    automated irrigation system using artificial neural network, in 2017 5th International
    Conference on Information and Communication Technology ICoIC7 2017, vol. 0, no.
    c, (2017) Google Scholar   A.R. Al-Ali, M. Qasaimeh, M. Al-Mardinia, S. Radder,
    I.A. Zualkernan, ZigBee-based irrigation system for home gardens, in 2015 International
    Conference on Communication Signal Processing and Their Application ICCSPA 2015,
    pp. 0–4 (2015) Google Scholar   S. Choudhary, V. Gaurav, A. Singh, S. Agarwal,
    Autonomous crop irrigation system using artificial intelligence. Int. J. Eng.
    Adv. Technol. 8(5 Special Issue), 46–51 (2019) Google Scholar   M.K. Nema, D.
    Khare, S.K. Chandniha, Application of artificial intelligence to estimate the
    reference evapotranspiration in sub-humid Doon valley. Appl. Water Sci. 7(7),
    3903–3910 (2017) Article   Google Scholar   R. Shahzadi, J. Ferzund, M. Tausif,
    M. Asif, Internet of Things based expert system for smart agriculture. Int. J.
    Adv. Comput. Sci. Appl. 7(9) (2016) Google Scholar   S.S. Patil, S.A. Thorat,
    Early detection of grapes diseases using machine learning and IoT, in Proceedings
    of 2016 2nd International Conference on Cognitive Computing and Information Processing
    CCIP 2016 (2016) Google Scholar   Download references Author information Authors
    and Affiliations Yogananda School of AI, Computer and Data Science, Shoolini University
    of Biotechnology and Management Sciences Solan, Bajhol, HP, India Ruchika Sharma,
    Nagesh Kumar & Brij Bhushan Sharma Corresponding author Correspondence to Ruchika
    Sharma . Editor information Editors and Affiliations KIET Group of Institutions,
    Ghaziabad, India Pradeep Kumar Singh Department of CSE, Central University of
    Jammu, Jammu and Kashmir, India Yashwant Singh Department of Electrical Engineering,
    IIT Patna, Patna, India Maheshkumar H. Kolekar Department of Management Studies
    and School of Artificial Intelligence, Indian Institute of Technology Delhi, New
    Delhi, Delhi, India Arpan Kumar Kar School of Technology, Polytechnic Institute
    of Castelo Branco, Castelo Branco, Portugal Paulo J. S. Gonçalves Rights and permissions
    Reprints and permissions Copyright information © 2022 The Author(s), under exclusive
    license to Springer Nature Singapore Pte Ltd. About this paper Cite this paper
    Sharma, R., Kumar, N., Sharma, B.B. (2022). Applications of Artificial Intelligence
    in Smart Agriculture: A Review. In: Singh, P.K., Singh, Y., Kolekar, M.H., Kar,
    A.K., Gonçalves, P.J.S. (eds) Recent Innovations in Computing. Lecture Notes in
    Electrical Engineering, vol 832. Springer, Singapore. https://doi.org/10.1007/978-981-16-8248-3_11
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-16-8248-3_11
    Published 10 March 2022 Publisher Name Springer, Singapore Print ISBN 978-981-16-8247-6
    Online ISBN 978-981-16-8248-3 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Download book PDF Download book EPUB Sections Figures References Abstract
    Introduction Role of AI in Agriculture Study Process Literature Survey Discussion
    Conclusion References Author information Editor information Rights and permissions
    Copyright information About this paper Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Electrical Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Applications of Artificial Intelligence in Smart Agriculture: A Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Tomar A.
  - Gupta G.
  - Salehi W.
  - Vanipriya C.
  - Kumar N.
  - Sharma B.
  citation_count: '6'
  description: Plant-based diseases such as Cork Spot, mildew, Black Rot, Apple Scab,
    pear blight, Frog Eye Leaf Spot, Crown Rot and so on are the most common leaf
    diseases. It gives a hand over the spread of diseases between the infected leaves.
    These diseases can be classified and detected earlier and can help in the production
    of healthy agriculture. Food production depends on the economy as well as on the
    yield of agricultural producers and is typically influenced by diseases that occur
    during the life of the growth of a given fruit-bearing plant. The plant disease
    detection systems are economically efficient, non-harmful, and have fewer results,
    however, the norm of leaves characterizes the level of greatness or a condition
    of being liberated from imperfections, shortages, and significant varieties. The
    plant disease detection and classification are the motivation of the proposed
    work. The present works reflect the comparison and the factual evaluation of several
    techniques in this field and the classification of the simplest fusion of algorithms
    that will help us establish a highly precise arrangement based on their leaves
    for plant diseases.
  doi: 10.1007/978-981-16-8248-3_24
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Recent Innovations in Computing
    pp 297–303Cite as Home Recent Innovations in Computing Conference paper A Review
    on Leaf-Based Plant Disease Detection Systems Using Machine Learning Abhishek
    Tomar , Gaurav Gupta , Waleed Salehi , CH. Vanipriya , Nagesh Kumar & Brijbhushan
    Sharma   Conference paper First Online: 10 March 2022 541 Accesses 2 Citations
    Part of the book series: Lecture Notes in Electrical Engineering ((LNEE,volume
    832)) Abstract Plant-based diseases such as Cork Spot, mildew, Black Rot, Apple
    Scab, pear blight, Frog Eye Leaf Spot, Crown Rot and so on are the most common
    leaf diseases. It gives a hand over the spread of diseases between the infected
    leaves. These diseases can be classified and detected earlier and can help in
    the production of healthy agriculture. Food production depends on the economy
    as well as on the yield of agricultural producers and is typically influenced
    by diseases that occur during the life of the growth of a given fruit-bearing
    plant. The plant disease detection systems are economically efficient, non-harmful,
    and have fewer results, however, the norm of leaves characterizes the level of
    greatness or a condition of being liberated from imperfections, shortages, and
    significant varieties. The plant disease detection and classification are the
    motivation of the proposed work. The present works reflect the comparison and
    the factual evaluation of several techniques in this field and the classification
    of the simplest fusion of algorithms that will help us establish a highly precise
    arrangement based on their leaves for plant diseases. Access provided by University
    of Nebraska-Lincoln. Download conference paper PDF 1 Introduction The basic idea
    of recognition of plants disease is often summed up into various classes, all
    of which are specified in one specific commonly comprehensive undertaking. Artificial
    intelligence with profound learning models has assisted with spotting plant illnesses
    by the plant''s presence and graphical manifestation that incorporates the human
    conduct should be taken into consideration [1]. Smartphone-based AI applications
    can be built for the interaction with the farmers and determine and detect diseases
    further diagnosis can be done on the same, thus curing the possible outrage of
    pests and diseases on an early stage, so that many farmers of developing countries
    do not have access to those advanced tools, internet insinuation and smart-phone
    dissemination present new outfits for in field crop disease diagnosis [2]. The
    criteria withhold going before the layer which is given to the resultant layer
    so that it increases the efficiency and speed of the primary stages. This is the
    reason why each layer should be built cautiously to make up the desired system.
    In the testing purpose, the researchers from various fields have made prototypes
    considering the estimations and structures that can help in the clustering of
    the diseases based on their classes, structures, appearance, etc. Various technologies
    are developed to detect leaf-based disease systems. Some of these technologies
    include as follows: Initially, the images of leaves have been stored as RGB count
    using a tool dependent color space MATLAB. To compare and differentiate the same,
    they are transferred to a comparable, device-independent color-space (CIRLAB).
    Secondly, the infected areas are extracted with the help of segmentation techniques
    such as region-based segmentation. Later, the K-means clustering took part as
    a key to be implemented on the segmented part. Thirdly, the color extraction shape
    determination and texture-based features to be considered and are used for the
    region description to implement these the wavelet and gray level co-occurrence
    matrix techniques were considered. The most important concept in this was SVR
    (Support Vector Regression) technique to determine the apple leaf disease. The
    other features that are drawn out are Principal Component Analysis (PCA) some
    of the classifiers used in this are backpropagation networks, radial basis function,
    generalized regression networks, and probabilistic neural networks. 2 Review of
    Literature This paper includes different machine learning classification techniques
    [3, 4] and feature extraction of the diseased pants based on their leaves and
    some of the studies that are done to classify the diseases and techniques which
    are used to classify and detect the diseased leaves. Some of the most relevant
    papers are reviewed below. In [5] author did a research in finding the unhealthy
    area of the leaf detection using the texture characteristics of the plant. In
    this research, they changed the picture into various RGB formats afterward the
    green pixels were masked to detect the unhealthy area of the leaf. Shape and surface
    of the plants act as the attributes and later are extracted to analyze. In [6]
    used HIS and K-means clustering together in detecting the plant leaf diseases.
    In addition to plant leaf disease detection, they also did detection of stem diseases
    were considered as well as during this the images at ground level were sectioned
    for the disease detection. In [7] author did research on cotton leaf spot diseases.
    The skew-divergence method was used for the feature-selection and in this paper,
    the author has proposed the use of skew divergence method. Some of the highlighted
    features in this paper are exact EDGE, CYMK, genetic algorithm, variances in relation
    to color and texture. In [8] gave some of the insights about the use of the wireless
    sensor networks for identifying the attacks on protocols used in different domains
    of the wireless networks. In [9] did some of the enhancements using simpler versions
    of the Convolutional Neural Networks (CNN). A vice approach toward Google Net
    and Cifar10 models are used for better performances. They also proposed that these
    measures require a proposed implementation of ReLu function so that the CNN activates
    effectively, and its efficiency can be enhanced. The accuracy obtained by this
    method was about 98%. In [10] author proposed some easier methods of classification
    such as Eccentricity, Convex Hull, Centroid, symbolic logic, Solidity, ANN, SVM,
    KNN. The common diseases of the plants/fruit leaf were taken as the dataset. While
    the SVM performs these two methods significantly. In [11] researcher did another
    research and increased the grouping exactness for the detection of diseases by
    75%. This was achieved by increasing the number of pictures which was about 25
    K. So, the accuracy can be determined accordingly. In [12], author implemented
    an automated irrigation with the monitoring system and Internet of Things. The
    author introduced the irrigation method which is automatic, and it helps farmer
    in monitoring the system. In [13], increased the statistical performance evaluation
    of proposed methods for the detection of diseases. It proposes a brief idea about
    the different techniques used in Plant’s leaf disease detection it also defines
    compares and evaluate the performance of each method. In [14] has implemented
    Wireless Sensor Network (WSN) and Internet of Thing in crops like saffron and
    wheat for a smart agriculture perspective. The author also presented the Smart
    farming techniques with real-time checking of the whole agribusiness. The use
    of Wi-Fi, Zigbee modules were considered for controlling the crops. In [15] proposed
    the CNN model for classification of Alzheimer Disease. It can also be used in
    determining the diseases conditions in plants. The research machine learning algorithms
    that were applied in their work for detection of Alzheimer, Deep learning approach
    is the better approach over traditional machine learning approaches. In [16] author
    proposed their work on Internet of things based on smart agriculture its hardware
    and software. This gives a brief idea about the devices and protocols and explains
    the issues one can face, while implementing IoT in agriculture. In [17], the author
    purposed works on smart agriculture which consist of manufacturing devices and
    sensors to study the climatic conditions. The use of sensors in smart agriculture
    specifies different applications like irrigation control, precision agriculture
    and soil quality checking. In [18], authors give insights about the machine learning
    algorithms in various fields like disease prediction and diagnosis. The accuracy
    comes out to be 97% which is very good. Same pipeline can be used for leaf disease
    prediction as they are image. In [19], the author presented the development and
    execution of a hydroponic structure equipped with intelligent agents for internet
    enabled monitoring, data collection, and storage. Data set accuracy is low because
    the size of datasets is smaller, and it is not possible to find the accuracy on
    such a less amount of data. So, we have to test all these algorithms and methods
    on large datasets for better performance and accurate detection of diseases. Further,
    there is a need for large and quality datasets to improve the experimental results.
    Statistical analysis is done on different plants like tomato, cotton, maize, mango,
    etc. is taken as a study with various algorithms and techniques (see Fig. 1).
    The accuracy of the different algorithms based on plants leaf disease detection
    algorithms. The X-axis comprises of the methods and algorithms, and Y-axis consists
    of the percentage accuracy of the respected algorithm. Fig. 1 Various algorithms
    with their depth of research and accuracies Full size image 3 Analysis of Various
    Algorithms In Table 1, the analysis is done based on goals and future perspective
    of the various papers which give a vast description of the work done in the field
    of agriculture practices to the automated world of disease. Some of the goals
    can be achieved with the previous studies based on the papers and further these
    are described in Table 1. Table 1 Analysis of various algorithms Full size table
    These algorithms process certain advantages some of them are mentioned: 1. For
    the automatic starting of cluster centers, the use of estimators is considered,
    this helps in the automation of the user inputs at the time of segmentation. 2.
    With the introduction of the proposed algorithm, the detection accuracy is intensified.
    3. These proposed algorithms were automated, while other methods require the user
    to input the best image after the segmentation process. 4. It in return also offers
    an ecological recovery measure for the detected diseases. 4 Conclusion and Future
    Scope Based on the above-cited literature and result from the analysis, it’s concluded
    that best algorithms for the detection of leaf-based disease were recorded for
    the CNN+ Relu with an accuracy of 98%, followed by CNN+ alexnet with an accuracy
    of 96% and SVM with an accuracy of 95% in comparison with other techniques like
    CCD-based image acquisition, color-based pre-processing (noise reduction), k-Means
    clustering and segmentation expectation intensification, in-built features so
    that these methods would be useful in leaf disease photographic identification,
    and are useful to advance the general performance of the studies under investigation.
    Designing SVM for image accusation, neural network, BPN, early stage disease selection
    and classification of edges via Fuzzy. CNN algorithms for clustering and detection
    of diseases on early stages Mobile phone application for ease in the diagnosis
    of diseases. Maintaining the health of plants at early stages. Working on the
    datasets for CNN for further checking the accuracy of the data so that the algorithms
    are best suited can be found and implemented. References A. Camargo, J.S. Smith,
    An image-processing based algorithm to automatically identify plant disease visual
    symptoms. Biosys. Eng. 102(1), 9–21 (2009) Article   Google Scholar   S.P. Mohanty,
    D.P. Hughes, M. Salathé, Using deep learning for image-based plant disease detection.
    Front. Plant Sci. 7, 1419 (2016) Article   Google Scholar   A. Bojamma, C. Shastry,
    A study on the machine learning techniques for automated plant species identification:
    current trends and challenges. Int. J. Inf. Technol., 1–7 (2019) Google Scholar   S.B.
    Jadhav, V.R. Udupi, S.B. Patil, Identification of plant diseases using convolutional
    neural networks. Int. J. Inf. Technol., 1–10 (2020) Google Scholar   S. Arivazhagan,
    R.N. Shebiah, S. Ananthi, S.V. Varthini, Detection of unhealthy region of plant
    leaves and classification of plant leaf diseases using texture features, vol.
    15, no. 1, pp. 211–217 (2013) Google Scholar   K. Khairnar, R. Dagade, Disease
    detection and diagnosis on plant using image processing—a review. Int. J. Comput.
    Appl. 108(13), 36–38 (2014) Google Scholar   P. Revathi, M. Hemalatha, Cotton
    leaf spot diseases detection utilizing feature selection with skew divergence
    method. Int. J. Sci. Eng. Technol. 3(1), 22–30 (2014) Google Scholar   P. Sharma,
    G. Gupta, Proficient techniques and protocols for the identification of attacks
    in WSN: a review. Indian J. Sci. Technol. 9, 42 (2016) Google Scholar   X. Zhang,
    Y. Qiao, F. Meng, C. Fan, M. Zhang, Identification of maize leaf diseases using
    improved deep convolutional neural networks. IEEE Access 6, 30370–30377 (2018)
    Article   Google Scholar   Q. Wang, F. Qi, M. Sun, J. Qu, J. Xue, Identification
    of tomato disease types and detection of infected areas based on deep convolutional
    neural networks and object detection techniques. Comput. Intell. Neurosci. 2019
    (2019) Google Scholar   P. Jiang, Y. Chen, B. Liu, D. He, C. Liang, Real-time
    detection of apple leaf diseases using deep learning approach based on improved
    convolutional neural networks. IEEE Access 7, 59069–59080 (2019) Article   Google
    Scholar   D. Rani, N. Kumar, B. Bhushan, Implementation of an automated irrigation
    system for agriculture monitoring using IoT communication, in 2019 5th International
    Conference on Signal Processing, Computing and Control (ISPCC) (IEEE, 2019), pp.
    138–143 Google Scholar   R. Jogekar, N. Tiwari, Summary of leaf-based plant disease
    detection systems: a compilation of systematic study findings to classify the
    leaf disease classification schemes, in 2020 Fourth World Conference on Smart
    Trends in Systems, Security and Sustainability (WorldS4) (IEEE, 2020), pp. 745–750
    Google Scholar   M.W. Rasooli, B. Bhushan, N. Kumar, Applicability of wireless
    sensor networks & IoT in saffron & wheat crops: a smart agriculture perspective.
    Int. J. Sci. Technol. Res. 9(2), 2456–2461 (2020) Google Scholar   A.W. Salehi,
    P. Baglat, B.B. Sharma, G. Gupta, A. Upadhya, A CNN model: earlier diagnosis and
    classification of Alzheimer disease using MRI, in 2020 International Conference
    on Smart Electronics and Communication (ICOSEC) (IEEE, 2020), pp. 156–161 Google
    Scholar   B.B. Sharma, N. Kumar, Internet of things-based hardware and software
    for smart agriculture: a review, pp. 151–157 (2020) Google Scholar   N. Kumar,
    B. Sharma, Opportunities and challenges with WSN’s in smart technologies: a smart
    agriculture perspective, in Handbook of Wireless Sensor Networks: Issues and Challenges
    in Current Scenario’s (Springer, 2020), pp. 441–463 Google Scholar   P. Baglat,
    A.W. Salehi, A. Gupta, G. Gupta, Multiple machine learning models for detection
    of Alzheimer’s disease using OASIS dataset, in International Working Conference
    on Transfer and Diffusion of IT (Springer, 2020), pp. 614–622 Google Scholar   CH.
    Vanipriya, S. Malladi, G. Gupta, Artificial intelligence enabled plant emotion
    xpresser in the development hydroponics system. Mater Today Proc (2021) Google
    Scholar   M. Maheswari, P. Daniel, R. Srinivash, N. Radha, Detection of diseased
    plants by using convolutional neural network, in Evolutionary Computing and Mobile
    Sustainable Networks (Springer, 2021), pp. 659–671 Google Scholar   S.B. Dhaygude,
    N.P. Kumbhar, Agricultural plant leaf disease detection using image processing,
    vol. 2, no. 1, pp. 599–602 (2013) Google Scholar   V. Singh, A.K. Misra, Detection
    of plant leaf diseases using image segmentation and soft computing techniques.
    Inf. Process. Agric. 4(1), 41–49 (2017) Google Scholar   K.R. Gavhale, U. Gawande,
    An overview of the research on plant leaves disease detection using image processing
    techniques. IOSR J. Comput. Eng. (IOSR-JCE) 16(1), 10–16 (2014) Google Scholar   S.
    Bashir, N. Sharma, Remote area plant disease detection using image processing.
    IOSR J. Electron. Commun. Eng. 2(6), 31–34 (2012) Article   Google Scholar   S.
    Sladojevic, M. Arsenovic, A. Anderla, D. Culibrk, D. Stefanovic, Deep neural networks
    based recognition of plant diseases by leaf image classification. Comput. Intell.
    Neurosci. 2016 (2016) Google Scholar   S.B. Patil, S.K. Bodhe, Leaf disease severity
    measurement using image processing. Int. J. Eng. Technol. 3(5), 297–301 (2011)
    Google Scholar   M.A. Chandra, S. Bedi, Survey on SVM and their application in
    image classification. Int. J. Inf. Technol., 1–11 (2018) Google Scholar   Download
    references Author information Authors and Affiliations Yogananda School of Artificial
    Intelligence, Computers and Data Science, Shoolini University of Biotechnology
    and Management Sciences, Solan, HP, India Abhishek Tomar, Gaurav Gupta, Waleed
    Salehi, Nagesh Kumar & Brijbhushan Sharma SIR MVIT, Bangalore, India CH. Vanipriya
    Editor information Editors and Affiliations KIET Group of Institutions, Ghaziabad,
    India Pradeep Kumar Singh Department of CSE, Central University of Jammu, Jammu
    and Kashmir, India Yashwant Singh Department of Electrical Engineering, IIT Patna,
    Patna, India Maheshkumar H. Kolekar Department of Management Studies and School
    of Artificial Intelligence, Indian Institute of Technology Delhi, New Delhi, Delhi,
    India Arpan Kumar Kar School of Technology, Polytechnic Institute of Castelo Branco,
    Castelo Branco, Portugal Paulo J. S. Gonçalves Rights and permissions Reprints
    and permissions Copyright information © 2022 The Author(s), under exclusive license
    to Springer Nature Singapore Pte Ltd. About this paper Cite this paper Tomar,
    A., Gupta, G., Salehi, W., Vanipriya, C., Kumar, N., Sharma, B. (2022). A Review
    on Leaf-Based Plant Disease Detection Systems Using Machine Learning. In: Singh,
    P.K., Singh, Y., Kolekar, M.H., Kar, A.K., Gonçalves, P.J.S. (eds) Recent Innovations
    in Computing. Lecture Notes in Electrical Engineering, vol 832. Springer, Singapore.
    https://doi.org/10.1007/978-981-16-8248-3_24 Download citation .RIS.ENW.BIB DOI
    https://doi.org/10.1007/978-981-16-8248-3_24 Published 10 March 2022 Publisher
    Name Springer, Singapore Print ISBN 978-981-16-8247-6 Online ISBN 978-981-16-8248-3
    eBook Packages Intelligent Technologies and Robotics Intelligent Technologies
    and Robotics (R0) Share this paper Anyone you share the following link with will
    be able to read this content: Get shareable link Provided by the Springer Nature
    SharedIt content-sharing initiative Publish with us Policies and ethics Download
    book PDF Download book EPUB Sections Figures References Abstract Introduction
    Review of Literature Analysis of Various Algorithms Conclusion and Future Scope
    References Author information Editor information Rights and permissions Copyright
    information About this paper Publish with us Discover content Journals A-Z Books
    A-Z Publish with us Publish your research Open access publishing Products and
    services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Electrical Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Review on Leaf-Based Plant Disease Detection Systems Using Machine Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lalithadevi B.
  - Krishnaveni S.
  citation_count: '1'
  description: In recent years, Internet of things (IoT) are being applied in several
    fields like smart healthcare, smart cities and smart agriculture. IoT-based applications
    are growing day by day. In healthcare industry, wearable sensor devices are widely
    used to track patient’s health status and their mobility. In this paper, IoT-based
    framework for healthcare us ing a suitable machine learning algorithm have been
    analysed intensely. Transmission of data using various standards are reviewed.
    Secure storage and retrieval of medical data using various ways are discussed.
    Machine learning techniques and storage mechanisms are analysed to ensure the
    quality of service to the patient care.
  doi: 10.1007/978-981-16-7610-9_16
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Intelligent Data Communication
    Technologies and Internet of Things pp 219–237Cite as Home Intelligent Data Communication
    Technologies and Internet of Things Conference paper Analysis of (IoT)-Based Healthcare
    Framework System Using Machine Learning B. Lalithadevi & S. Krishnaveni  Conference
    paper First Online: 28 February 2022 890 Accesses 1 Citations Part of the book
    series: Lecture Notes on Data Engineering and Communications Technologies ((LNDECT,volume
    101)) Abstract In recent years, Internet of things (IoT) are being applied in
    several fields like smart healthcare, smart cities and smart agriculture. IoT-based
    applications are growing day by day. In healthcare industry, wearable sensor devices
    are widely used to track patient’s health status and their mobility. In this paper,
    IoT-based framework for healthcare us ing a suitable machine learning algorithm
    have been analysed intensely. Transmission of data using various standards are
    reviewed. Secure storage and retrieval of medical data using various ways are
    discussed. Machine learning techniques and storage mechanisms are analysed to
    ensure the quality of service to the patient care. Access provided by University
    of Nebraska-Lincoln. Download conference paper PDF 1 Introduction An Internet
    of Things technology is a process, which is utilized to provide the interface
    and brilliant gadgets to make human life easier. It gathers the sensor data, process
    it and send through the internet. Many organizations predict the expansion of
    IoT over the years and Cisco Systems is one of them. A specific report of Cisco
    Systems states that IoT will be an operational domain of over 50 billion devices
    by 2023 [1]. Moreover, IoT has its list of providing numerous advantages in daily
    life. It can efficiently monitor the working and functionality of devices and
    gadgets with limited resources in a very smooth way. Smart healthcare industry
    can do the following things such as, Telehealth, Assisted living, Hospital asset
    management, Drug management, Hygiene compliance, Disease management and rehabilitation.
    These variety of management services makes the patient life easier and assist
    the medical experts as well as hospital to manage and delivering the service within
    a short time intervals [2]. IoT is a dynamic community infrastructure that interconnects
    different sensor networks via the Internet, accumulates sensor data that transmits
    and receives records/facts for further processing. According to structure and
    strategies followed in IoT environment, security challenges are there in terms
    of maintain the person’s clinical records and hospital records confidentially.
    1.1 Contributions The main contribution of this paper are as follows, Presented
    the deep analysis of smart healthcare framework based on Internet of Things Discussed
    about the architectural design of cloud computing as well as fog computing in
    healthcare field for data storage and retrieval. In addition, the impact of block
    chain for data security particularly security challenges to manage the medical
    data records. Presented the influence of Internet of things in medical care industry
    that makes it as smart environment Presented the existing framework for healthcare
    industry and focused on cloud computing in general and also sensors for the generalization
    of health parameters. Discussed the machine learning algorithms that used for
    diseases prediction. This paper is organized as follows: in Sect. 2, discuss the
    background of an IoT in healthcare field that address the related technologies
    and monitoring the remote patients through wearable sensors. Section 3 listed
    out different IoT architecture, various healthcare services which are available
    nowadays and various applications toward patient, hospital and doctors. Section
    4 describes the machine learning techniques and various algorithm. Section 5 discuss
    the data communication standards used for deploying the healthcare environment
    based on Internet of things. Section 6 describes enabling cloud computing in IoT
    for data transmission. Section 7 gives the ideas about fog computing in smart
    environment. Section 8 describes various security challenges in IoT due to global
    data access. Section 9 discusses about the research challenges and limitations
    in IoT-based healthcare framework design. Finally, in Sect. 10, we present the
    concluding remarks and reviews. 2 Background IoT has included a lot of research
    ideas over the years and the possible areas of success of it being implemented
    are studied strictly. Elder people can track down their health levels through
    the use of IoT is reduces the stress and dependency of people on the healthcare
    system [2]. 2.1 m-Health Things (m-IoT) Mobile—IoT (m-IoT) has come up in recent
    days following the fusion of contemporary digital fitness standards with LTE connectivity.
    It has been summed up with high speed 4G networks to boost up the standard further.
    The 6LoWPAN is summed up with the 4G protocols. 2.2 Wearable Medical Sensors The
    health-related dataset includes the following attributes such as, lung disease,
    severe headache, kidney disorder, liver disorder, LDL, TC, DBP, HDL, TC, obesity,
    BG, and HR [3]. Any one of these attributes can be the major cause of hypertension
    disease [4]. Figure 1 represents the overview of healthcare framework model. According
    to healthcare industry sector, the basic building blocks of an IoT are various
    wearable sensors that collects the data from patients remotely and relevant data
    is collected from sensors. Fig. 1 Overview of healthcare framework Full size image
    These data is transmitted into cloud server and stored on it. In this, artificial
    intelligence or machine learning based prediction and detection model is incorporated
    to predict the risk values. Alert or warning message is send to medical experts
    via the cloud server. In turn, the respective physician can prescribe the appropriate
    medicine and take necessary action to protect the persons in critical situation.
    Role of Datasets and Acquisition methods Dataset is a collection of information
    used to train our model through machine learning or deep learning techniques there
    are four different ways to acquire the data from end nodes such as data collection,
    data conversion and data sharing. Table 1 demonstrates the role of patient dataset
    and acquisition method of wearable body sensors. Table 1 Patient medical report
    Full size table Pulse Sensors The accurate measurement of the pulse depends on
    a variety of factors but it mostly depends on the body part from where it is measured.
    The chest and wrist are common areas. It is measured from fingertips and earlobes
    as well [1]. Respiratory Rate Sensors It is based on the fact that the exhaled
    air is warmer than the intrinsic temperature. The number of breaths taken is also
    considered into account. Often advanced signals are used to make it more precise.
    Body Temperature Sensors The importance of body temperature sensors in wearable
    devices is noteworthy as well. Fever, high body temperature, and other ailments
    can be measured. Figure 2 shows the collection of various healthcare sensors based
    on IoT. Terrible temperature coefficient (NTC) and pressure temperature coefficient
    (PTC) are temperature sensors [1]. Heart strokes can be detected through this
    sensor [4]. Hypothermia can be identified based on these body temperature sensor
    which is embedded in a wearable device. Fig. 2 Representation of various IoT healthcare
    sensors Full size image Blood Pressure Sensor Hypertension is a significant danger
    component for cardiovascular sickness, comprising of a coronary respiratory failure.
    Heartbeat travel time (PTT) defined as the time taken among beat at the heart
    and heartbeat at another area, which incorporates the ear cartilage or outspread
    artery [4]. Glucose Sensor A sensible device for monitoring the blood-glucose
    degrees in diabetic sufferers have been proposed earlier. This gadget calls for
    sufferers to manually check blood-sugar levels at ordinary time intervals [5].
    ECG sensor ECG sensors are based on electrocardiogram. Their function is to monitor
    cardiac activity. Smart sensors and microcontrollers are used in these systems.
    There is a smart device involved in a smartphone that is connected through Bluetooth
    with the clinical data [6]. Pulse Oximetry Sensors It estimates the oxygen level
    in the blood. The degrees of oxygen in the blood are resolved. This is not a much
    essential thing to consider in designing a medical wearable device but can certainly
    provide an edge in certain cases [1]. Accuracy and precision of sensed data from
    wearable devices may be corrupted by the malicious intruders and change it as
    erroneous data. So it leads to misguide the end user in terms of decision support
    system in treatment. If we want to make a smart healthcare environment based on
    IoT, then need to provide a highly secured framework and efficient model to maintain
    the privacy and confidentiality of patient medical data. 3 IoT Architecture, Healthcare
    Services and Application Iot architecture describes the flow of data transmission
    from edge devices to cloud server through interconnected network for data analysis,
    storage and retrieval. In that, all sensed data will be processed further through
    our prediction model. Figure 3 represents the evolution of IoT architecture. About
    IoT Healthcare Services and their Applications, different fields assume a significant
    part in the administration of private wellbeing and wellness, care for pediatric,
    management of persistent sicknesses, elder patients among others [2]. Fig. 3 Evolution
    of IoT architectures Full size image 3.1 Real-Time Significance of Proposed Model
    The proposed model has the significant ability to transmit the data from wearable
    medical sensors into cloud server via data transmission and communication protocol
    standards as shown in Fig. 1. Smart phone acts like as an intermediate agent between
    sensors and web apps. Request and response protocol (XMPP) is applied to send
    the amount of data payload and data length code from sensors to android listening
    port finally web interface layer activate the physician, patients and hospital
    to respond the action based on request parallel. Figure 4 represents the list
    of Healthcare Services in medical field. A robotic supporter is used for tracking
    senior citizen status. This robot is utilized for the ZigBee sensor gadget to
    extraordinarily distinguish individuals that it is tracking [7]. Fig. 4 List of
    healthcare services Full size image 3.2 IoT for Patients Doctors cannot attend
    to all patients at all the time, so patients can check the progress in their health
    by themselves through the wearable IoT devices. This is made by the healthcare
    system in a very efficient way for doctors. Basic research challenge involved
    in this, monitor multiple reports of various patients at a time through their
    mobile phones and give appropriate medications on time without any delay. 3.3
    IoT for Physicians The various fitness rate are useful to monitor the overall
    condition of the patient. One of the research challenges is associated with the
    doctor. They should give preventive medications and the patients also have to
    suffer less if the disease is diagnosed in the early stage. 3.4 IoT for Hospitals
    IoT contraptions labeled with sensors are utilized for observing the real-time
    district of clinical bits of gear like wheelchairs, nebulizers, oxygen siphons,
    and so on. Appointment fees and travel costs can be reduced by IoT devices [8].
    Azure Web application is used to store the data to perform analysis and predict
    the health conditions within the expected time [9]. The most important advantages
    and challenges of IoT in healthcare consist of: Cost and error reduction Improved
    and proactive Treatment Faster Disease Diagnosis Drugs and Equipment Management.
    4 Machine Learning Techniques for Disease Prediction The medical field has experienced
    innovation in disease diagnosis and medical data analysis since it has collaborated
    its research work with machine learning [10]. The data of patient details which
    are generated every day is huge and cannot be surveyed by simple methods. This
    statistical data is given to the model which has been trained. The model might
    not achieve higher accuracy but near to it. 4.1 Naive Bayes To understand the
    naive Bayes theorem, let us first understand the Bayes theorem. It is based on
    conditional probability. There are two types of events namely dependent and independent
    events. $$P\\left( {X/Y} \\right) = P\\left( {X \\cap Y} \\right)/P\\left( Y \\right),\\quad
    {\\text{if}}\\quad P\\left( Y \\right) \\ne 0$$ (1) $$P\\left( {Y \\cap X} \\right)
    = P\\left( {X \\cap Y} \\right)$$ (2) $$P\\left( {X \\cap Y} \\right) = P\\left(
    {X/Y} \\right)P\\left( Y \\right) = P\\left( {Y/X} \\right)P\\left( X \\right)$$
    (3) $$P(X/Y) = (P(Y/X)P(X))/(P(Y)),\\quad if\\quad P(Y) \\ne 0)$$ (4) P(X/Y)—conditional
    probability, the probability of event X occurring given that Y is true. P(Y/X)—likelihood
    probability, the probability of event Y occurring given that X is true. This idea
    is applied on classification datasets where there is a categorical column such
    as YES and NO or TRUE and FALSE. The categorical data can be binary data or multi-classified
    data [11]. 4.2 Artificial Neural Networks Machine learning has found its use in
    various fields and medical science is one of them. Machine learning or deep learning
    models need very little human assistance to solve any problem. Figure 5 shows
    the overview of artificial neural network model. A machine learn ing model uses
    feature selection techniques to find out the probable outcome. Certain limitations
    hinder the performance of a machine learning model. Machine learning algorithm
    is only bound to the features that are given to it as input, if any foreign feature
    comes in, it might predict wrongly. Fig. 5 Overview of artificial neural network
    Full size image 4.3 Support Vector Machine There are three kinds of learning techniques
    in Artificial Intelligence such as regulated, unaided, and support learning. SVM
    lies under supervised learning which is used for classification and regression
    analysis. Figure 6 shows the support vector machine classifier. Classification
    datasets contain categorical data as target variables and regression datasets
    contain continuous variable as a target [12]. SVM is a supervised learning technique
    that is working on labeled data [12]. If there is a dataset consisting of circles
    and quadrilaterals, it can predict whether the new data is a circle or a quadrilateral.
    SVM creates a boundary between the two classes. This boundary plane is the decision
    boundary which helps the model to predict. Fig. 6 Representation of support vector
    machine Full size image 4.4 Random Forest Ensemble techniques in machine learning
    are methods that combine multiple models into one model. There are two types of
    ensemble techniques, bagging and boosting. Random forest is a bagging technique.
    In bagging, many base models can be created for feature extraction [13]. A new
    random set of data will be given as a sample to the models. This method is also
    known as Row Sampling with replacement. Figure 7 represents the random forest
    classifier model. For a particular test data, the output of different models are
    observed. The output of all models may not be the same so we used a voting classifier.
    All the votes are combined and the output which has the highest frequency or majority
    votes are considered [5]. Multiple decision trees are used in a random forest.
    Decision trees have low bias and high variance. The various decision trees of
    different models are aggregated and the final output achieved after majority voting
    has low variance [5]. Fig. 7 Overview of random forest classifier mode Full size
    image 5 Data Communication Standards To comprehend information correspondence
    in remote gadgets, we need to comprehend the body zone organization. EEG sensors
    [14], body temperature sensors, heart rate sensors, etc., are interconnected with
    the cloud through some handheld devices. 5.1 Short-Range Communication Techniques
    The short-range communication techniques present in this paper are ZigBee and
    Bluetooth. Both are come under the home network. Zigbee is a technology that was
    created to control and sense a network. The various layers in Zig-Bee [15] are
    the application layer, security layer, networking layer, media access control
    (MAC) layer, and physical layer. As demonstrated in Table 2, a few communication
    standards are mentioned for short distance coverage. Table 2 Short-range communication
    standards Full size table 5.2 Long-Range Communication Techniques A network that
    allows long distance data communication or transmission is known as a wide area
    network. This transmission is suitable for large geographical areas. The range
    of WAN is beyond 100 km. Tables 3 demonstrates the communication standards for
    long-range distance coverage. The major challenge of data communication standard
    is selecting the range of data coverage based on dataset collection feasibility
    and problem statement for further process. Table 3 Long range communication standards
    Full size table 6 Cloud Computing in Healthcare Cloud computing is a technology
    that emerged when the size of data generated daily became impossible to store
    and handle. The patient data needs security and privacy. The resources that are
    available on our computers like storage and computational power are managed by
    the cloud services according to the data [15]. Figure 8 shows an impact of cloud
    computing in healthcare field for instant data transmission. Fig. 8 Representation
    of cloud computing for healthcare Full size image Storage Access Control Layer
    is the spine of the cloud enabled environment, which access medical services by
    utilizing sensors along with BG and sphygmo manometers in every day’s exercises
    [16]. Data Annotation Layer resolves hetero geneity trouble normally occurs throughout
    statistics processing. Data testing layer analyzes the medical records saved inside
    the cloud platform. Portability and integrity level of data transfer from end
    devices to cloud server is a challenging task in IoT healthcare platform. 7 Influence
    of Fog Computing in Healthcare Previously, the ability for medical services in
    2017 was actualized by utilizing Fog Computing [16]. A healthcare system was launched
    in 2016, called health fog [15]. Figure 9 illustrates the recent fog computing
    studies in healthcare industry using machine learning and deep learning techniques.
    Next, to improve system reliability, cloud-based security functionality was included
    into health fog [15]. Fig. 9 Representation of recent fog based healthcare studies
    Full size image The benefits of edge computing for home and hospital control systems
    have been exploited by the current architecture. Health technologies have been
    moving from cloud computing to fog computing in recent years [17]. Similar work
    has been performed by authors [18] as a four-layered healthcare model comprising
    of sensation layer, classification layer, mining layer, and application layer.
    Sensation layer obtained the data from various sensors which are located in the
    office room. Classification is done based on five different categories such as,
    Data about health, Data about Environment, Data about Meal, Data about Physical
    posture, Data about behavior in classification layer. The mining layer is used
    for extract the information from a cloud database. Finally, the application layer
    provides various services like personal health recommender system, remote medical
    care monitoring system and personal health maintenance system to the end user
    [19]. 7.1 Fog Computing Architecture In cloud computing science, Fog Computing
    Architecture shows potential challenges and security. The basic architecture of
    fog computing is shown. It is separated into three main layers as shown in Fig.
    10. Device layer is the nearest layer to the end users or devices are the application
    layer. It comprises of many hardware such as mobile devices and sensors. These
    devices are spread globally. They are responsible for detecting and communicating
    the knowledge about the physical object for analysis and storing to the upper
    layer. Fog layer is the second layer at the edge of the network is the mist layer,
    which incorporates a tremendous measure of haze hubs [20]. Cloud layer is responsible
    for permanent management and the comprehensive computational processing of data
    [21]. Fog Nodes and cloud platform must consider the following challenges, such
    as, Fig. 10 Overview of Fog computing architecture Full size image Retrieve data
    from IoT devices based on any protocol. Check-up IoT-enabled applications for
    control and analysis of real-time applications, with minimum response time. Send
    cyclic data to the cloud for further process. Data are aggregated which are received
    from many fog nodes. Analysis can be done on the IoT data to get business insight.
    8 Security Challenges in IoT The primary goal of IoT security is to safeguard
    the consumer privacy, data confidentiality, availability, transportation infrastructure
    by an IoT platform. Block chain innovation improves responsibility among patients
    and doctor [23]. DDoS attacks perhaps one of the great examples of the issues
    that include shipping gadgets with default passwords and no longer telling customers
    to exchange them as soon as they obtain them [16]. As the wide variety of IoT
    related gadgets proceeds to upward push before very long, a wide assortment of
    malware and ransomware are utilized to misuse them [24]. An IP camera is suitable
    for capturing sensitive statistics on the usage of a huge variety of places, such
    as your private home, paintings office, or maybe the nearby gasoline station [25].
    There are various security problems in IOT. Mostly the password of IoT devices
    is weak or hard coded [26]. Various security challenges in IoT are, Insufficient
    checking out and updating Brute-forcing and the issue of default passwords IoT
    malware and ransomware Data security, privacy and untrustworthy communication.
    Investigators applied the various machine learning and deep learning models in
    healthcare framework for different disease detection. Table 4 summarize the various
    methodology used for disease prediction and detection. Table 4 Summary of Data
    analysis using Machine learning and Deep learning algorithms in healthcare Full
    size table 9 Discussion In this section, discuss about the research limitations
    and challenges of IoT-based healthcare framework. flexible wearable sensors are
    required to monitor the patient’s health status. Design a secured framework for
    data transmission from edge device to control device then cloud server. In that,
    various intruders may be involved to modify the data and break the confidentiality.
    Analysis of signals should be done in ECG and EEG monitoring using ML. Energy
    efficient optimization algorithm is needed to protect the consumption and reduce
    the amount of usage level. Data privacy is more important especially in healthcare
    domain. It can be achieved through cryptographic model and standards. 10 Conclusion
    IoT-based healthcare technologies offer different architectures and platforms
    for healthcare networks that support the connectivity to IoT backbone and enable
    the transmission and reception of medical data. Studies about various fields of
    healthcare in IoT, cloud computing and fog computing, this research review is
    valuable for researchers. The Internet of Things have developed the healthcare
    sector, enhancing the performance, reducing costs and concentrating on quality
    care for patients. It provides a complete healthcare network for IoT which associated
    with cloud and fog computing, also serve as backbone for cloud computing applications
    and provides a framework for sharing Medical data between medical devices and
    remote servers. References Baker SB, Xiang W, Atkinson I (2017) Internet of things
    for smart healthcare: technologies, challenges, and opportunities. Institute of
    Electrical and Electronics Engineers Inc., vol 5, pp 26521–26544, Nov. 29, 2017.
    IEEE Access. https://doi.org/10.1109/ACCESS.2017.2775180 Carnaz GJF, Nogueira
    V (2019) An overview of IoT and health- care question answering systems in medical
    and healthcare domain view project NanoSen AQM view project Vitor Nogueira Universidade
    de E´vora An Overview of IoT and Healthcare. Available: https://www.researchgate.net/publication/330933788
    Hussain S, Huh E, Kang BH, Lee S (2015) GUDM: automatic generation of unified
    datasets for learning and reasoning in healthcare, pp 15772–15798. https://doi.org/10.3390/s150715772
    Majumder AJA, Elsaadany YA, Young R, Ucci DR (2019) An energy efficient wearable
    smart IoT system to predict cardiac arrest. Adv Hum-Comput Interact vol 2019.
    https://doi.org/10.1155/2019/1507465 Ani R, Krishna S, Anju N, Sona AM, Deepa
    OS (2017) IoT based patient monitoring and diagnostic prediction tool using ensemble
    classifier. In: 2017 International Conference on Advanced Computing and Communication
    Informatics, ICACCI 2017, vol 2017-January, pp 1588–1593. https://doi.org/10.1109/ICACCI.2017.8126068
    Joyia GJ, Liaqat RM, Farooq A, Rehman S (2017) Internet of Medical Things (IOMT):
    applications, benefits and future challenges in healthcare domain, May 2018. https://doi.org/10.12720/jcm.12.4.240-247
    Konstantinidis EI, Antoniou PE, Bamparopoulos G, Bamidis PD (2015) A lightweight
    framework for transparent cross platform communication of controller data in ambient
    assisted living environments. Inf Sci (NY) 300(1):124–139. https://doi.org/10.1016/j.ins.2014.10.070
    Article   Google Scholar   Saba T, Haseeb K, Ahmed I, Rehman A (2020) Journal
    of Infection and Public Health Secure and energy-efficient framework using Internet
    of Medical Things for e-healthcare. J Infect Public Health 13(10):1567–1575. https://doi.org/10.1016/j.jiph.2020.06.027
    Article   Google Scholar   Krishnaveni S, Prabakaran S, Sivamohan S (2016) Automated
    vulnerability detection and prediction by security testing for cloud SAAS. Indian
    J Sci Technol 9(S1). https://doi.org/10.17485/ijst/2016/v9is1/112288 Yang X, Wang
    X, Li X, Gu D, Liang C, Li K (2020) Exploring emerging IoT technologies in smart
    health research: a knowledge graph analysis 9:1–12 Google Scholar   Nashif S,
    Raihan MR, Islam MR, Imam MH (2018) Heart disease detection by using machine learning
    algorithms and a real-time cardiovascular health monitoring system. World J Eng
    Technol 06(04):854–873. https://doi.org/10.4236/wjet.2018.64057 Krishnaveni S,
    Vigneshwar P, Kishore S, Jothi B, Sivamohan S (2020) Anomaly-based intrusion detection
    system using support vector machine. In: Artificial intelligence and evolutionary
    computations in engineering systems, pp 723–731 Google Scholar   Ram SS, Apduhan
    B, Shiratori N (2019) A machine learning framework for edge computing to improve
    prediction accuracy in mobile health monitoring. In: Lecture notes in computer
    science (including subseries lecture notes in artificial intelligence and lecture
    notes in bioinformatics), July 2019, vol 11621 LNCS, pp 417–431. https://doi.org/10.1007/978-3-030-24302-930
    Umar S, Alsulaiman M, Muhammad G (2019) Deep learning for EEG motor imagery classification
    based on multi-layer CNNs feature fusion. Futur Gener Comput Syst 101:542–554.
    https://doi.org/10.1016/j.future.2019.06.027 Minh Dang L, Piran MJ, Han D, Min
    K, Moon H (2019) A survey on internet of things and cloud computing for healthcare.
    Electronics 8(7). https://doi.org/10.3390/electronics8070768 Dewangan K, Mishra
    M (2018) Internet of things for healthcare: a review. Researchgate.Net 8(Iii):526–534.
    Available: http://ijamtes.org/ Sood SK, Mahajan I (2019) IoT-Fog-based healthcare
    framework to identify and control hypertension attack. IEEE Internet Things J
    6(2):1920–1927. https://doi.org/10.1109/JIOT.2018.2871630 Bhatia M, Sood SK (2019)
    Exploring temporal analytics in fog-cloud architecture for smart office healthcare.
    Mob Networks Appl 24(4):1392–1410. https://doi.org/10.1007/s11036-018-0991-5 Raj
    JS (2021) Security enhanced blockchain based unmanned aerial vehicle health monitoring
    system. J ISMAC 3(02):121–131 Google Scholar   Nandyala CS, Kim HK (2016) From
    cloud to fog and IoT-based real-time U- healthcare monitoring for smart homes
    and hospitals. Int J Smart Home 10(2):187–196. https://doi.org/10.14257/ijsh.2016.10.2.18
    Article   Google Scholar   Dubey H, Yang J, Constant N, Amiri AM, Yang Q, Makodiya
    K (2016) Fog data: enhancing Telehealth big data through fog computing. In: ACM
    international conference on proceeding series, vol 07–09-October-2015. May 2016.
    https://doi.org/10.1145/2818869.2818889 He W, Yan G, Da Xu L, Member S (2017)
    Developing vehicular data cloud services in the IoT environment. https://doi.org/10.1109/TII.2014.2299233
    Suma V (2021) Wearable IoT based distributed framework for ubiquitous computing.
    J Ubiquitous Comput Commun Technol (UCCT) 3(01):23–32 Google Scholar   Hariharakrishnan
    J, Bhalaji N (2021) Adaptability analysis of 6LoWPAN and RPL for healthcare applications
    of internet-of-things. J ISMAC 3(02):69–81 Google Scholar   Pazienza A, Polimeno
    G, Vitulano F (2019) Towards a digital future: an innovative semantic IoT integrated
    platform for Industry 4.0. In: Healthcare, and territorial control Google Scholar   Aceto
    G, Persico V, Pescap´e A (2018) The role of Information and Communication Technologies
    in healthcare: taxonomies, perspectives, and challenges. J Netw Comput Appl 107:125–154.
    https://doi.org/10.1016/j.jnca.2018.02.008 Tahmassebi A et al (2019) Impact of
    machine learning with multiparametric magnetic resonance imaging of the breast
    for early prediction of response to neo adjuvant chemotherapy and survival outcomes
    in breast cancer patients. Invest Radiol 54(2):110–117. https://doi.org/10.1097/RLI.0000000000000518
    Article   Google Scholar   Kayal CK, Bagchi S, Dhar D, Maitra T, Chatterjee S
    (2019) Hepatocellular carcinoma survival prediction using deep neural network.
    In: Proceedings of international ethical hacking conference 2018, pp 349–358 Google
    Scholar   Zheng T et al (2017) A machine learning-based framework to identify
    type 2 diabetes through electronic health records. Int J Med Inform 97:120–127.
    https://doi.org/10.1016/j.ijmedinf.2016.09.014 Article   Google Scholar   Rahimian
    F et al (2018) Predicting the risk of emergency admission with machine learning:
    development and validation using linked electronic health records. PLoS Med 15(11):e1002695.
    https://doi.org/10.1371/journal.pmed.1002695 Zacharaki EI et al (2009) Classification
    of brain tumor type and grade using MRI texture and shape in a machine learning
    scheme. Magn Reson Med 62(6):1609–1618. https://doi.org/10.1002/mrm.22147 Article   Google
    Scholar   Goodfellow D, Zhi R, Funke R, Pulido JC, Mataric M, Smith BA (2018)
    Predicting infant motor development status using day long movement data from wearable
    sensors. Available: http://arxiv.org/abs/1807.02617 Hassan MM, Huda S, Uddin MZ,
    Almogren A, Alrubaian M (2018) Human activity recognition from body sensor data
    using deep learning. J Med Syst 42(6):99. https://doi.org/10.1007/s10916-018-0948-z
    Lonini L et al (2018) Wearable sensors for Parkinson’s disease: which data are
    worth collecting for training symptom detection models. npj Digit Med 1(1). https://doi.org/10.1038/s41746-018-0071-z
    Kanjo E, Younis EMG, Ang CS (2019) Deep learning analysis of mobile physiological,
    environmental and location sensor data for emotion detection. Inf Fusion 49:46–56.
    https://doi.org/10.1016/j.inffus.2018.09.001 Liang CA, Chen L, Wahed A, Nguyen
    AND (2019) Proteomics analysis of FLT3-ITD mutation in acute myeloid leukemia
    using deep learning neural network. Ann Clin Lab Sci 49(1):119–126. https://doi.org/10.1093/ajcp/aqx121.148
    Article   Google Scholar   Download references Author information Authors and
    Affiliations Department of Computer Science and Engineering, SRM Institute of
    Science and Technology, Chennai, India B. Lalithadevi Department of Software Engineering,
    SRM Institute of Science and Technology, Chennai, India S. Krishnaveni Editor
    information Editors and Affiliations Department of Electronics and Communication
    Engineering, Karunya Institute of Technology and Sciences, Coimbatore, India D.
    Jude Hemanth Faculty of Communication Sciences, University of Teramo, Teramo,
    Italy Danilo Pelusi Department of Computer Engineering, San Jose State University,
    San Jose, CA, USA Chandrasekar Vuppalapati Rights and permissions Reprints and
    permissions Copyright information © 2022 The Author(s), under exclusive license
    to Springer Nature Singapore Pte Ltd. About this paper Cite this paper Lalithadevi,
    B., Krishnaveni, S. (2022). Analysis of (IoT)-Based Healthcare Framework System
    Using Machine Learning. In: Hemanth, D.J., Pelusi, D., Vuppalapati, C. (eds) Intelligent
    Data Communication Technologies and Internet of Things. Lecture Notes on Data
    Engineering and Communications Technologies, vol 101. Springer, Singapore. https://doi.org/10.1007/978-981-16-7610-9_16
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-16-7610-9_16
    Published 28 February 2022 Publisher Name Springer, Singapore Print ISBN 978-981-16-7609-3
    Online ISBN 978-981-16-7610-9 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Download book PDF Download book EPUB Sections Figures References Abstract
    Introduction Background IoT Architecture, Healthcare Services and Application
    Machine Learning Techniques for Disease Prediction Data Communication Standards
    Cloud Computing in Healthcare Influence of Fog Computing in Healthcare Security
    Challenges in IoT Discussion Conclusion References Author information Editor information
    Rights and permissions Copyright information About this paper Publish with us
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes on Data Engineering and Communications Technologies
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Analysis of (IoT)-Based Healthcare Framework System Using Machine Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhang J.
  - Tao D.
  citation_count: '315'
  description: 'In the Internet-of-Things (IoT) era, billions of sensors and devices
    collect and process data from the environment, transmit them to cloud centers,
    and receive feedback via the Internet for connectivity and perception. However,
    transmitting massive amounts of heterogeneous data, perceiving complex environments
    from these data, and then making smart decisions in a timely manner are difficult.
    Artificial intelligence (AI), especially deep learning, is now a proven success
    in various areas, including computer vision, speech recognition, and natural language
    processing. AI introduced into the IoT heralds the era of AI of things (AIoT).
    This article presents a comprehensive survey on AIoT to show how AI can empower
    the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly
    present the AIoT architecture in the context of cloud computing, fog computing,
    and edge computing. Then, we present progress in AI research for IoT from four
    perspectives: 1) perceiving; 2) learning; 3) reasoning; and 4) behaving. Next,
    we summarize some promising applications of AIoT that are likely to profoundly
    reshape our world. Finally, we highlight the challenges facing AIoT and some potential
    research opportunities.'
  doi: 10.1109/JIOT.2020.3039359
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things
    Journal >Volume: 8 Issue: 10 Empowering Things With Intelligence: A Survey of
    the Progress, Challenges, and Opportunities in Artificial Intelligence of Things
    Publisher: IEEE Cite This PDF Jing Zhang; Dacheng Tao All Authors 286 Cites in
    Papers 7190 Full Text Views Abstract Document Sections I. Introduction II. Architecture
    III. Progress Review of AI for IoT IV. AIoT Applications V. Challenges and Opportunities
    Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: In the Internet-of-Things (IoT) era, billions of sensors and devices
    collect and process data from the environment, transmit them to cloud centers,
    and receive feedback via the Internet for connectivity and perception. However,
    transmitting massive amounts of heterogeneous data, perceiving complex environments
    from these data, and then making smart decisions in a timely manner are difficult.
    Artificial intelligence (AI), especially deep learning, is now a proven success
    in various areas, including computer vision, speech recognition, and natural language
    processing. AI introduced into the IoT heralds the era of AI of things (AIoT).
    This article presents a comprehensive survey on AIoT to show how AI can empower
    the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly
    present the AIoT architecture in the context of cloud computing, fog computing,
    and edge computing. Then, we present progress in AI research for IoT from four
    perspectives: 1) perceiving; 2) learning; 3) reasoning; and 4) behaving. Next,
    we summarize some promising applications of AIoT that are likely to profoundly
    reshape our world. Finally, we highlight the challenges facing AIoT and some potential
    research opportunities. Published in: IEEE Internet of Things Journal ( Volume:
    8, Issue: 10, 15 May 2021) Page(s): 7789 - 7817 Date of Publication: 19 November
    2020 ISSN Information: DOI: 10.1109/JIOT.2020.3039359 Publisher: IEEE Funding
    Agency: SECTION I. Introduction The Internet of Things (IoT), a term originally
    coined by Kevin Ashton at MIT’s Auto-ID Center [1], refers to a global intelligent
    network that enables cyber–physical interactions by connecting numerous things
    with the capacity to perceive, compute, execute, and communicate with the Internet;
    process and exchange information between things, data centers, and users; and
    deliver various smart services [2], [3]. From the radio-frequency identification
    (RFID) devices developed in the late 1990s to modern smart things, including cameras,
    lights, bicycles, electricity meters, and wearable devices, the IoT has developed
    rapidly over the last twenty years in parallel with advances in networking technologies,
    including Bluetooth, Wi-Fi, and long-term evolution (LTE). The IoT represents
    a key infrastructure for supporting various applications [4], e.g., smart homes
    [5], [6], smart transportation [7], [8], smart grids [9], and smart healthcare
    [10], [11]. According to McKinsey’s report [12], the IoT sector will contribute
    $ 2.7 to $ 6.2 trillion to the global economy by 2025. A typical IoT architecture
    has three layers [13]: 1) a perception layer; 2) a network layer; and 3) an application
    layer. The perception layer lies at the bottom of the IoT architecture and consists
    of various sensors, actuators, and devices that function to collect data and transmit
    them to the upper layers. The network layer lies at the center of the IoT architecture
    and comprises different networks [e.g., local area networks (LANs), cellular networks,
    and the Internet] and devices (e.g., hubs, routers, and gateways) enabled by various
    communication technologies, such as Bluetooth, Wi-Fi, LTE, and fifth-generation
    mobile networks (5G). The application layer is the top IoT layer and it is powered
    by cloud computing platforms, offering customized services to users, e.g., data
    storage and analysis. In conventional IoT solutions, data collected from sensors
    are transmitted to the cloud computing platform through the networks for further
    processing and analysis before delivering the results/commands to end devices/actuators.
    However, this centralized architecture faces significant challenges in the context
    of the massive numbers of sensors used across various applications. Based on reports
    from Cisco [14] and IDC [15], 50 billion devices will be IoT connected by 2025,
    generating 79.4 ZB of data. Transmitting this huge amount of data requires massive
    bandwidth, and cloud processing and sending the results back to end devices lead
    to high latency. To address this issue, “fog computing,” coined by Cisco [16],
    aims to bring storage, computation, and networking capacity to the edge of the
    network (e.g., to distributed fog nodes, such as routers) in proximity to the
    devices. Fog computing offers the advantages of low latency and high computational
    capacity for IoT applications [17], [18]. “Edge computing” has also recently been
    proposed by further deploying computing capacity on edge devices in proximity
    to sensors and actuators [19], [20]. Note that the terms fog computing and edge
    computing are interchangeable in some literature [19], [21] or the fog is treated
    as a part of the broader concept of edge computing [22]. For clarity, here we
    treat them as different concepts, i.e., fog computing at the network side and
    edge computing at the thing side. Edge computing can process and analyze data
    on premises and make decisions instantly, thereby benefitting latency-sensitive
    IoT applications. The processed data from different devices can then be aggregated
    at the fog node or cloud center for further analysis to enable various services.
    In addition to these challenges created by massive numbers of sensors, another
    challenge arises through their heterogeneous nature [23], including scalar sensors,
    vector sensors, and multimedia sensors as summarized in Table I. Perceiving and
    understanding dynamic and complex environments from sensor data are fundamental
    to IoT applications providing useful services to users. As a result, various intelligent
    algorithms have been proposed for certain applications with scalar and vector
    sensors, e.g., decision rules-based methods and data-driven methods. Typically,
    these methods use handcrafted features extracted from data for further prediction,
    classification, or decision [Fig. 1(a)]. However, this paradigm of using handcrafted
    features and shallow models is unsuited to modern IoT applications with multimedia
    sensors. First, multimedia sensor data are high dimensional and unstructured (semantics
    are unavailable without additional processing), so it is difficult to design handcrafted
    features for them without domain knowledge. Second, handcrafted features are usually
    vulnerable to noise and different types of variance (e.g., illumination and viewpoint)
    in data, limiting their representation and discrimination capacity. Third, feature
    design and model training are separate, without joint optimization. TABLE I Summary
    of Exemplar AIoT Sensors. A: Agriculture, C: Cities/Homes/Buildings, E: Education,
    G: Grids, H: Healthcare, I: Industry, S: Security, and T: Transportation Fig.
    1. Schematic paradigm of (a) classical machine learning methods and (b) deep learning.
    Show All The last few years has witnessed a renaissance in artificial intelligence
    (AI) assisted by deep learning. Deep neural networks (DNNs) have been widely used
    in many areas and have achieved excellent performance in many applications, including
    speech recognition [24], face recognition [25], image classification [26], object
    detection [27], semantic segmentation [28], and natural language processing [29],
    benefitting from their powerful capacity to feature learn and end-to-end model
    [Fig. 1(b)]. Moreover, with modern computational devices, e.g., graphics processing
    units (GPUs) and tensor processing units (TPUs), DNNs can efficiently and automatically
    discover discriminative feature representations from large-scale labeled or unlabeled
    data sets in a supervised or unsupervised manner [30]. Deploying DNNs into cloud
    platforms, fog nodes, and edge devices in IoT systems enables the construction
    of an intelligent hybrid computing architecture capable of leveraging the power
    of deep learning to process massive quantities of data and extract structured
    semantic information with low latency. Therefore, advances in deep learning have
    paved a clear way for improving the perceiving ability of IoT systems with large
    numbers of heterogeneous sensors. Although an IoT’s perception system is a critical
    component of the architecture, simply adapting to and interacting with the dynamic
    and complex world are insufficient. For example, edge cases exist in the real
    world that may not be seen in the training set nor defined in the label set, resulting
    in the degeneration of a pretrained model. Another example is in industry, where
    the operating modes of machines may drift or change due to fatigue or wear and
    tear. Consequently, models trained for the initial mode cannot adapt to this variation,
    leading to a performance loss. These issues are related to some well-known machine
    learning research topics, including few-shot learning (FSL) [31], zero-shot learning
    (ZSL) [32], metalearning [33], unsupervised learning (USL) [34], semisupervised
    learning (SSL) [35], transfer learning (TL) [36], and domain adaptation (DA) [37],
    [38]. Deep learning has facilitated progress in these areas, suggesting that deep
    learning can be similarly leveraged to improve IoT system learning. Furthermore,
    to interact with the environment and humans, an IoT system should be able to reason
    and behave. For example, a man parks his car in a parking lot every morning and
    leaves regularly on these days. Therefore, a smart parking system may infer that
    he probably works nearby. Then, it can recommend and introduce some parking offers,
    car maintenance, and nearby restaurants to him via an AI chatbot. These application
    scenarios could benefit from recent advances in causal inference and discovery
    [39], graph-based reasoning [40], reinforcement learning (RL) [41], and speech
    recognition and synthesis [24], [42]. According to Cisco’s white paper [43], 99.4%
    of physical objects are still unconnected. Advanced communication technologies,
    such as Wi-Fi 6 (IEEE 802.11ax standard) and 5G and AI technologies will enable
    mass connection. This heralds the era of the AI of things (AIoT), where AI encounters
    IoT. Both academia and industry have invested heavily in AIoT, and various AIoT
    applications have now been developed, providing services and creating value. Therefore,
    here we performed a survey of this emerging area to demonstrate how AI technologies
    empower things with intelligence and enhance applications. A. Contributions of
    This Survey There are several excellent existing surveys on IoT covering different
    perspectives, a detailed discussion and comparison of which is provided below.
    Here, we specifically focus on AIoT and provide an overview of research advances,
    potential challenges, and future research directions through a comprehensive literature
    review and detailed discussion. The contributions of this survey can be summarized
    as follows. We discuss AIoT system architecture in the context of cloud computing,
    fog computing, and edge computing. We present progress in AI research for IoT,
    applying a new taxonomy: perceiving, learning, reasoning, and behaving. We summarize
    some promising applications of AIoT and discuss enabling AI technologies. We highlight
    challenges in AIoT and some potential research opportunities. B. Relationship
    to Related Surveys We first review existing surveys related to IoT and contrast
    them with our work. Since the IoT is related to many topics, such as computing
    architectures, networking technologies, applications, security, and privacy, surveys
    have tended to focus on one or some of these topics. For example, Atzori et al.
    [44] described the IoT paradigm from three perspectives: 1) “things” oriented;
    2) “Internet” oriented; and 3) “semantic” oriented, corresponding to sensors and
    devices, networks, and data processing and analysis, respectively. They reviewed
    enabling technologies and IoT applications in different domains and also analyzed
    some remaining challenges with respect to security and privacy. Whitmore et al.
    [45] presented a comprehensive survey on IoT and identified recent trends and
    challenges. We review the other surveys according to the specific topic covered.
    1) Architecture: In [46], several typical IoT architectures were reviewed, including
    software-defined network-based architectures, the MobilityFirst architecture,
    and the CloudThings architecture. They argued that future IoT architectures should
    be scalable, flexible, interoperable, energy efficient, and secure such that the
    IoT system can integrate and handle huge numbers of connected devices. Lin et
    al. [13] discussed two typical architectures: 1) the three-layer architecture
    (i.e., with a perception layer, network layer, and application layer) and 2) the
    service-oriented architecture. For the IoT computing architecture, integrating
    cloud computing [47] with fog/edge computing [13] has attracted increasing attention.
    Chiang and Zhang [17] and Pan and McElhannon [20] provided a detailed review of
    fog computing and edge computing for IoT. Since we focus on AI-empowered IoT,
    we are also interested in the cloud/fog/edge computing architectures of IoT systems,
    especially those tailored for deep learning. More detail will be presented in
    Section II. 2) Networking Technologies: Connecting massive numbers of things to
    data centers and transmitting data at scale relies on various networking technologies.
    Verma et al. [48] presented a comprehensive survey of network methodologies, including
    data center networks, hyperconverged networks, massively parallel mining networks,
    and edge analytics networks, which support real-time analytics of massive IoT
    data. Wireless sensor networks have also been widely used in IoT to monitor physical
    or environmental conditions [49]. The recently developed 5G mobile networks can
    provide very high data rates at extremely low latency and a manifold increase
    in base station capacity. 5G is expected to boost the number of connected things
    and drive the growth of IoT applications [50]. Due to the massive numbers of sensors
    and network traffic, resource management in IoT networks has become a topic of
    interest, with advanced deep learning technologies showing promising results [51].
    Although we also focus on deep learning for IoT, we are more interested in its
    role in IoT data processing rather than networking, which is, therefore, beyond
    the scope of this survey. 3) Data Processing: Massive sensor data must be processed
    to extract useful information before being used for further analysis and decision
    making. Data mining and machine learning approaches have been used for IoT data
    processing and analysis [52], [53]. Moreover, the context of IoT sensors can provide
    auxiliary information to help understand sensor data. Therefore, various context-aware
    computing methods have been proposed for IoT [54]. There has recently been rapid
    progress in deep learning, with these positive effects also impacting IoT data
    processing, e.g., streaming data analysis [55], mobile multimedia processing [56],
    manufacturing inspection [57], and health monitoring. In contrast, we conduct
    this survey on deep learning for IoT data processing using a new taxonomy, i.e.,
    how deep learning improves the ability of IoT systems to perceive, learn, reason,
    and behave. Since deep learning is itself a rapidly developing area, our survey
    covers the latest progress in deep learning in various IoT application domains.
    4) Security and Privacy: Massive user data are collected via ubiquitous connected
    sensors, which may be transmitted and stored in the cloud through IoT networks.
    These data may contain some biometric information, such as faces, voice, or fingerprints.
    Cyberattacks on IoT systems may result in data leakage, so data security and privacy
    have become a critical concern in IoT applications [18]. Recently, access control
    [58] and trust management [59] approaches have been reviewed to protect the security
    and privacy of IoT. We also analyze this issue and review progress advanced by
    AI, such as federated learning (FL) [60]. 5) Applications: Almost all surveys
    refer to various IoT application domains, including smart cities [61], smart homes
    [6], smart healthcare [62], smart agriculture [63], and smart industry [4]. Furthermore,
    IoT applications based on specific things, e.g., the Internet of Vehicles (IoV)
    [7] and Internet of Video Things (IoVT) [23], have also been rapidly developed.
    We also summarize some promising applications of AIoT and demonstrate how AI enables
    them to be faster, smarter, greener, and safer. C. Organization The organization
    of this article is shown in Fig. 2. We first discuss AIoT computing architecture
    in Section II. Then, we present a comprehensive survey of enabling AI technologies
    for AIoT in Section III, followed by a summary of AIoT applications in Section
    IV. The challenges faced by AIoT and research opportunities are discussed in Section
    V, followed by conclusions in Section VI. Fig. 2. Diagram of the organization
    of this article. Show All SECTION II. Architecture In this section, we discuss
    the architecture for AIoT applications. Similar to [13] and [23], we also adopt
    a tri-tier architecture but from the perspective of computing. For simplicity,
    we term the three layers as the cloud/fog/edge computing layer, as shown in Fig.
    3. The edge computing layer may function like the perception layer in [13] and
    smart visual sensing block in [23]. It also supports control and execution over
    sensors and actuators. Thereby, this layer aims to empower AIoT systems with the
    ability to perceive and behave. The fog computing layer is embodied in the fog
    nodes within the networks, such as hubs, routers, and gateways. The cloud computing
    layer supports various application services, functioning similarly to the application
    layer [13] and intelligent integration block in [23]. The fog and cloud computing
    layers mainly aim to empower AIoT systems with the ability of learning and reasoning
    since they can access massive amounts of data and have vast computation resources.
    It is noteworthy that the edge things and fog nodes are always distributed while
    the cloud is centralized in the AIoT network topology. Fig. 3. Diagram of the
    tri-tier computing architecture of AIoT. Show All A. Tri-Tier Computing Architecture
    1) Cloud Computing Layer: The cloud enables AIoT enterprises to use computing
    resources virtually via the Internet instead of building their physical infrastructure
    on premises. It can provide flexible, scalable, and reliable resources, including
    computation, storage, and network for enabling various AIoT applications. Typically,
    real-time data streams from massive distributed sensors and devices are transmitted
    to the remote cloud center through the Internet, where they are further integrated,
    processed, and stored. With the off-the-shelf deep learning tools and scalable
    computing hardware, it is easy to set up the production environment on the cloud,
    where DNNs are trained and deployed to process the massive amounts of data. An
    important feature of cloud computing is that it provides elastic computing resources
    in the pay-as-you-go way, which is useful for the AIoT services with fluctuant
    traffic loads. Another feature is that it can leverage all the data from the registered
    devices in an AIoT application, which is useful for training deep models with
    better representation and generalization ability. 2) Fog Computing Layer: Fog
    computing brings storage, computation, and networking capacity to the edge of
    the network that is in the proximity of devices. The facilities or infrastructures
    that provide fog computing service are called fog nodes, e.g., routers, switches,
    gateways, and wireless access points. Although functioning similarly to cloud
    computing, fog computing offers a key advantage, i.e., low latency, since it is
    closer to devices. Besides, fog computing can provide continuity of service without
    the need for the Internet, which is important for specific AIoT applications with
    an unstable Internet connection, e.g., in agriculture, mining, and shipping domains.
    The other advantage of fog computing is the protection of data security and privacy
    since data can be held within the LAN. Fog nodes are better suited for deploying
    DNNs rather than training since they are designed to store data from local devices,
    which are incomplete compared with those on the cloud. Nevertheless, model training
    can still be scheduled on fog nodes by leveraging FL [60]. 3) Edge Computing Layer:
    The term of edge computing is interchangeable with fog computing in some literature
    [19], [21] or denotes a broader concept that the fog can be treated as a part
    of it [22]. Nevertheless, we treat them as different concepts for clarity in this
    article. Specifically, we distinguish them based on their locations within the
    LAN, i.e., fog computing at the network side and edge computing at the thing side.
    In this sense, edge computing refers to deploying computing capacity on edge devices
    in proximity to sensors and actuators. A great advantage of edge computing over
    fog and cloud computing is the reduction of latency and network bandwidth since
    it can process data into compact structured information on-site before transmission,
    which is especially useful for AIoT applications using multimedia sensors. However,
    due to its limited computation capacity, only lightweight DNNs can run on edge
    devices. Therefore, research topics, including neural network architecture design
    or search for mobile setting and network pruning/compression/quantization, have
    attracted increasing attention recently. In practice, it is common to deploy multiple
    different models into cloud platforms, fog nodes, and edge devices in an AIoT
    system to build an intelligent hybrid computing architecture. By intelligently
    offloading part of the computation workload from edge devices to the fog nodes
    and cloud, it is expected to achieve low latency while leveraging deep learning
    capacities for processing massive amounts of data. For example, a lightweight
    model can be deployed on edge devices to detect cars in a video stream. It can
    act as a trigger to transmit keyframes to fog nodes or the cloud for further processing.
    B. Hardware and Software 1) Hardware: While GPU is initially developed for accelerating
    image rendering on display devices, the general-purpose GPU turns the massive
    computational power of its shader pipeline into general-purpose computing power
    (e.g., for massive vector operations), which has sparked the deep learning revolution
    along with DNN and big data. Lots of operations in the neural network, such as
    convolution can be computed in parallel on GPU, significantly reducing the training
    and inference time. Recently, an application-specific integrated circuit (ASIC)
    named TPU is designed by Google specifically for neural network machine learning.
    Besides, field-programmable gate arrays (FPGAs) have also been used for DNN acceleration
    due to their low-power consumption and high throughput. Several machine learning
    processors have also been developed for fog and edge computing, e.g., Google Edge
    TPU and NVIDIA Jetson Nano. 2) Software: Researchers and engineers must design,
    implement, train, and deploy DNNs easily and quickly. To this end, different open-source
    deep learning frameworks have been developed, from the beginners, such as Caffe1
    and MatConvNet2 to the popular TensorFlow3 and PyTorch.4 MatConvNet is a MATLAB
    toolbox for implementing convolutional neural networks (CNNs). Caffe is implemented
    in C++ with Python and MATLAB interfaces and well known for its speed but does
    not support distributed computation and mobile deployment. Caffe2 improves it
    accordingly, which has been later merged into PyTorch. The features, such as dynamic
    computation graphs and automatic computation of gradients in TensorFlow and PyTorch,
    have made them easy to use and popular. They also support for deploying models
    into mobile devices by enabling model compression/quantization and hardware acceleration.
    Porting models among different frameworks is necessary and useful. The open neural
    network exchange (ONNX)5 offers this feature by defining an open format built
    to represent machine learning models, which has been supported by TensorFlow and
    Pytorch. There are other deep learning frameworks, such as MXNet,6 Theano,7 PaddlePaddle,8
    and neural network inference computing framework for mobile devices such as ncnn.9
    SECTION III. Progress Review of AI for IoT In this section, we comprehensively
    review the progress of enabling AI technologies for AIoT applications, especially
    deep learning. We conduct the survey by applying a new taxonomy, i.e., how deep
    learning improves the ability of AIoT systems for perceiving, learning, reasoning,
    and behaving. To prevent it from being a survey on deep learning, we carefully
    select the topics and technologies that are closely related to and useful for
    various AIoT applications. Moreover, we only outline the trend of the research
    progress and highlight state-of-the-art technologies rather than diving into the
    details. We specifically discuss their potentials for AIoT applications. We hope
    this survey can draw an overall picture of AI technologies for AIoT and provide
    insights into their utility. A. Perceiving Empowering things with the perceiving
    ability, i.e., understanding the environment using various sensors, is fundamental
    for AIoT systems. In this part, we will focus on several related topics as diagrammed
    in Fig. 4. Fig. 4. Diagram of the perceiving-related topics in AIoT. Show All
    First, we present a review of the progress in generic scene understanding, including
    image classification, object detection, and tracking, semantic segmentation, and
    text spotting. 1) Image Classification: Image classification refers to recognizing
    the category of an image. Classical machine learning methods based on hand-crafted
    features have been surpassed by DNNs [26] on large-scale benchmark data sets such
    as ImageNet [30], sparking a wave of research on the architecture of DNNs. From
    AlexNet [26] to ResNet [64], more and more advanced network architectures have
    been devised by leveraging stacked 3 × 3 convolutional layer for reducing network
    parameters and increasing network depth, 1×1 convolutional layer for feature dimension
    reduction, residual connections for preventing gradient vanishing and increasing
    network capacity, and dense connections for reusing features from previous layers
    as shown in Fig. 5. A brief summary of representative deep CNNs is listed in Table
    II. As can be seen, with the increase of network depth and the number of parameters,
    the representation capacity also increases, leading to lower top1 classification
    error on the ImageNet data set. Besides, the architecture of the network matters.
    Even with fewer model parameters and computational complexity, the recently proposed
    networks, such as ResNet and DenseNet, outperform previous ones such as VGGNet.
    Lightweight networks are appealing to AIoT applications where DNNs are deployed
    on edge devices. Recently, some computationally efficient networks such as MobileNet
    have been proposed by leveraging depthwise convolutions [65], pointwise convolutions
    [66], or binary operations [67]. Besides, network compression, such as pruning
    and quantization can be used to obtain lightweight models from heavy models, which
    will be reviewed in Section III-A17. Image recognition can be very useful in many
    AIoT applications, such as smart education tools or toys, which help and teach
    children to explore the world with cameras. Besides, some popular applications
    in smartphones also benefit from the advances in this area for recognizing flowers
    and birds, and food items and calories. TABLE II Summary of Representative Deep
    CNNs. Param.: Number of Parameters; Comp.: Computational Complexity (MACs) Fig.
    5. Basic blocks of representative deep CNNs. Show All 2) Object Detection: Generic
    object detection refers to recognizing the category and location of an object,
    which is used as a prepositive step for many downstream tasks, including face
    recognition, person reidentification, pose estimation, behavior analysis, and
    human–machine interaction (HMI). The methods for object detection from images
    have been revolutionized by DNNs. State-of-the-art methods can be categorized
    into two groups: 1) two-stage methods and 2) one-stage methods. The former follows
    a typical “proposal → detection” paradigm [27], while the latter directly evaluates
    all the potential object candidates and outputs the detection results [68]. Recently,
    one-stage anchor-free detectors have been proposed by representing object location
    using points or regions rather than anchors [69], achieving a better tradeoff
    between speed and accuracy, which is appealing to AIoT applications that require
    onboard detection. Detection of specific category of objects, such as pedestrian,
    car, traffic sign, and the license plate has been widely studied, which are useful
    for improving the perceiving ability of AIoT systems for traffic and public safety
    surveillance and autonomous driving [70]. Besides, object detection is a crucial
    technique for video data structuring in many AIoT systems using visual sensors,
    which aims to extract and organize compact structured semantic information from
    video data for further retrieval, verification, statistics, and analysis at low
    transmission, storage, and computation cost. 3) Object Tracking: Classical object
    tracking methods include generative and discriminative methods, where the former
    ones try to search the most similar regions to the target and the latter ones
    leverage both foreground target and background context information to train an
    online discriminative classifier [71]. Later, different deep learning methods
    have been proposed to improve the classical methods by learning multiresolution
    deep features [72], end-to-end representation learning [73], and leveraging siamese
    networks [74]. Object trackers usually run much faster than object detectors,
    which can be deployed on edge devices of AIoT applications, such as video surveillance
    and autonomous driving for object trajectory generation and motion prediction.
    One possible solution is to leverage the hybrid computation architecture (see
    Section II-A) by deploying object tracker on edge devices while deploying object
    detectors on the fog nodes or cloud, i.e., tracking across all frames while detecting
    only on keyframes. In this way, only keyframes and the compact structured detection
    results should be transmitted via the network, thereby reducing the network bandwidth
    and processing latency. 4) Semantic Segmentation: Semantic segmentation refers
    to predicting pixel-level category label for an image. The fully CNN with an encoder–decoder
    structure has become the de facto paradigm for semantic segmentation [75], [76],
    since it can learn discriminative and multiresolution features through cascaded
    convolution blocks while preserving spatial correspondence. Many deep models have
    been proposed to improve the representation capacity and prediction accuracy from
    the following three aspects: 1) context embedding; 2) resolution enlarging; and
    3) boundary refinement. Efficient modules are proposed to exploit context information
    and learn more representative feature representations, such as global context
    pooling module in the ParseNet [77], atrous spatial pyramid pooling in the DeepLab
    models [28], and the pyramid pooling module in the PSPNet [78]. Enlarging the
    resolution of feature maps is beneficial for improving prediction accuracy, especially
    for small objects. Typical techniques include using the deconvolutional layer,
    unpooling layer, and dilated convolutional layer. Boundary refinement aims to
    obtain sharp boundaries between different categories in the segmentation map,
    which can be achieved by using conditional random field as the postprocessing
    technique on the predicted probability maps [28]. There are two research topics
    related to semantic segmentation, i.e., instance segmentation and panoptic segmentation.
    Instance segmentation refers to detecting foreground objects as well as obtaining
    their masks. A well-known baseline model is Mask R-CNN, which adopts an extra
    branch for object mask prediction in parallel with the existing one for bounding
    box regression [79]. Its performance can be improved further by exploiting and
    enhancing the feature hierarchy of deep convolutional networks [80], employing
    nonlocal attention [81], and leveraging the reciprocal relationship between detection
    and segmentation via hybrid task cascade [82]. Panoptic segmentation refers to
    simultaneously segmenting the masks of foreground objects as well as background
    stuff [83], i.e., unifying both the semantic segmentation and instance segmentation
    tasks. A simple but strong baseline model is proposed in [84], which adds a semantic
    segmentation branch into the Mask R-CNN framework and uses a shared feature pyramid
    network backbone [80]. Semantic segmentation in many subareas, such as medical
    image segmentation [75], road detection [85], and human parsing [86], is useful
    in various AIoT applications. For example, it can be used to recognize the dense
    pixel-level drivable area and traffic participants, such as cars and pedestrians,
    which can be further combined with 3-D measure information to get a comprehensive
    understanding of the driving context and make smart driving decisions accordingly.
    Moreover, obtaining the foreground mask or body parts matters for many AIoT applications,
    e.g., video editing for entertainment and computational advertising, virtual try-on,
    and augmented/virtual reality (AR/VR). Besides, the structured semantic mask is
    also useful for semantic-aware efficient and adaptive video coding. 5) Text Spotting:
    Text spotting is a composite task, including text detection and recognition. Although
    text detection is related to generic object detection, it is a different and challenging
    problem: 1) while generic objects have regular shapes, text may be in variable
    length and shape depending on the number of characters and their orientation and
    2) the appearance of the same text may change significantly due to fonts, styles,
    as well as background context. Deep learning has advanced this area by learning
    more representative feature [87], devising better representation of text proposals
    [88], and using large-scale synthetic data set [89]. Recently, end-to-end modeling
    of text detection and recognition has achieved impressive performance [90], [91].
    Each subtask can benefit from the other by leveraging more supervisory signals
    and learning a shared feature representation. Moreover, rather than recognizing
    text at the character level, recognizing text at the word or sentence level can
    benefit from the word dictionary and language model. Specifically, the idea of
    sequence-to-sequence modeling and connectionist temporal classification (CTC)
    [92] from the areas of speech recognition and machine translation (MT) has also
    been explored. Since the text is very common in real-world scenes, e.g., traffic
    sign, nameplate, and information board, text spotting can serve as a useful tool
    in many AIoT applications for “reading” text information from scene images, e.g.,
    live camera translator for education, reading assistant for the visually impaired
    [93], optical character recognition (OCR) for automatic document analysis, and
    store nameplate recognition for self-localization and navigation. The representative
    benchmark data sets in the areas related to generic scene understanding are summarized
    in Table III. TABLE III Representative Benchmark Data Sets in Generic Scene Understanding.
    BBox: Bounding Box; Mask: Pixel-Level Semantic Mask Next, we present a review
    of the progress in human-centric perceiving, including biometric recognition,
    such as face/fingerprint/iris recognition, person reidentification, pose/gesture/action
    estimation, and crowd density estimation. 6) Biometric Recognition: Biometric
    recognition, based on face, fingerprint, or iris, is a long-standing research
    topic. We first review the progress in face recognition. There are usually four
    key stages in a face recognition system, i.e., face detection, face alignment,
    face representation, and face classification/verification. Face detection, as
    a specific subarea of object detection, benefits from the recent success of deep
    learning in generic object detection. Nevertheless, special effort should be made
    to address the following challenges, i.e., vast scale variance, severe imbalance
    of positive and negative proposals, profile and front face, occlusion, and motion
    blur. One of the most famous classical methods is the Viola–Jones algorithm, which
    sets up the fundamental face detection framework [94]. The idea of using cascade
    classifiers inspires many deep learning methods, such as cascade CNN [95]. Recently,
    jointly modeling face detection with other auxiliary tasks, including face alignment,
    pose estimation, and gender classification, can achieve improved performance,
    owing to the extra abundant supervisory signals for learning a shared discriminative
    feature representation [96], [97]. Note that the all-in-one model is appealing
    to some AIoT application where multiple structured facial information could be
    extracted. Face alignment, also known as (a.k.a.), facial landmark detection aims
    to detect facial landmarks from a face image, which is useful for front face alignment
    and face recognition. Typically, face facial landmark detectors are trained and
    deployed in a cascade manner that a shape increment is learned and used to update
    the current estimate at each level [98]. Face landmark detectors are usually lightweight
    and run very fast, which are very useful for latency-sensitive AIoT applications.
    For face recognition, significant progress has been achieved in the last decade,
    mainly owing to deep representation learning and metric learning. The milestone
    work in [25] proposes to learn discriminative deep bottleneck features using classification
    and verification losses. Nevertheless, they face a challenge to scale to orders
    of magnitude larger data sets with more identities. To address this issue, a representation
    learning method using triplet loss is proposed to directly learn discriminative
    and compact face embedding [99]. Face recognition is one of the most widely used
    perceiving techniques for identity verification and access control in various
    AIoT applications, e.g., smart cities and smart homes. Associating the facial
    identity with one’s accounts can create vast business value, e.g., mobile payment,
    membership development and promotion, and fast track in smart retail. Regarding
    the number of people to be recognized and privacy concerns, either offline or
    online solutions can be used, where models are deployed on edge devices, fog nodes,
    or cloud centers [100], [101]. A research topic related to practical face recognition
    applications is liveness detection and spoof detection. Different methods have
    been proposed based on action imitation, speech collaboration, and multimodal
    sensors [102]. In addition to face recognition, iris, fingerprint, and palmprint
    recognition have also been studied for a long period and are widely used in practical
    AIoT applications. Compared with fingerprint, palmprint has abundant features
    and can be captured using common built-in cameras of mobile phones rather than
    sensitive sensors. Typically, a palmprint recognition system is composed of a
    palmprint image acquisition system, a palmprint region of interest (ROI) extraction
    module, a feature extraction module, and a feature matching module for recognition
    or verification. Both hand-crafted features, such as line features, orientation-based
    features, and the orthogonal line ordinal feature and deep learning-based feature
    representation have been studied in the literature [103], [104]. For example,
    Zhang et al. [105] proposed a novel device to capture palmprint images in a contactless
    way, which can be used for access control, aviation security, and e-banking. It
    uses blockwise statistics of competitive code as features and the collaborative
    representation-based framework for classification. Besides, a DCNN-based palmprint
    verification system named DeepMPV is proposed for mobile payment in [104]. It
    first extracts the palmprint ROI by using pretrained detectors and then trains
    a siamese network to match palmprints. Recently, Amazon has announced its new
    payment system called Amazon One, which is a fast, convenient, and contactless
    way for people to use their palms to make payments based on palmprint recognition.
    In practice, the choice of a specific biometric recognition solution depends on
    sensors, usage scenarios, latency, and power consumption. Although biometric recognition
    offers great utility, the concerns about data security and privacy have to be
    carefully addressed in practical AIoT systems. 7) Person Reidentification: Person
    reidentification, as a subarea of image retrieval, refers to recognizing an individual
    captured in disjoint camera views. In contrast to face recognition in a controlled
    environment, person reidentification is more challenging due to the variations
    in the uncontrolled environment, e.g., viewpoint, resolution, clothing, and background
    context. To address these challenges, different methods have been proposed [110],
    including deep metric learning based on various losses, integration of local features
    and context, multitask learning based on extra attribute annotations, and using
    human pose and parsing mask as guidance. Recently, generative adversarial networks
    (GANs) have been used to generate style-transferred images for bridging the domain
    gap between different data sets [111]. Person reidentification has vast potential
    for AIoT applications, such as smart security in an uncontrolled and noncontact
    environment, where other biometric recognition techniques are not applicable.
    Although extra efforts are needed to build practical person reidentification systems,
    one can leverage the idea of human-in-the-loop AI to achieve high performance
    with low labor effort. For example, the person reidentification model can be used
    for initial proposal ranking and filtering, then human experts are involved to
    make final decisions. 8) Human Pose Estimation and Gesture/Action Recognition:
    Human pose estimation, a.k.a. human keypoint detection refers to detecting body
    joints from a single image. There are two groups of human pose estimation methods,
    i.e., top-down methods and bottom-up methods. The former consists of two stages,
    including person detection and keypoint detection, while the latter directly detects
    all keypoints from the image and associates them with corresponding person instances.
    Although top-down methods still dominate the leaderboard of public benchmark data
    sets such as MS COCO,10 they are usually slower than bottom-up methods [112].
    Recent progress in this area can be summarized in the following aspects. Learning
    better feature representation from stronger backbone network, multiscale feature
    fusion, or context modeling [113]. Effective training strategy, including online
    hard keypoint mining, hard negative person detection mining, and harvesting extra
    data [107]. Subpixel representation or postprocessing techniques [107], [114].
    Recently, dealing with pose estimation in crowd scenes with severe occlusions
    also attracts much attention. The other related topic is 3-D human pose estimation
    from a single image or multiview images [115], aiming to estimate the 3-D coordinate
    of each keypoint rather than the 2-D coordinate on the image plane. Once we detect
    the human keypoints for each frame given a video clip, the skeleton sequence for
    each person instance can be obtained, from which we can recognize the action.
    This process is known as skeleton-based action recognition. To model the long-term
    temporal dependencies and dynamics, as well as spatial structures within the skeleton
    sequence, different neural networks have been exploited for action recognition,
    such as the deep recurrent neural network (RNN) [116], CNN [117], and deep graph
    convolutional networks (GCNs) [118]. Besides, since some joints may be more relevant
    to specific actions than others, attention mechanism has been used to automatically
    discover informative joints and emphasize their importance for action recognition
    [119]. Estimation of human pose and recognition of action can be very useful in
    many real-world AIoT scenarios, such as rehabilitation exercises monitoring and
    assessment [120], dangerous behavior monitoring [121], and HMI. Hand gesture recognition
    is also a hot research topic and has many practical applications, such as HMI
    and sign language recognition. Different sensors can be used in AIoT systems for
    gesture recognition, such as millimeter-wave radar and visual sensors, such as
    RGB camera, depth camera, and event camera [122]–[124]. Nevertheless, due to the
    prevalence of cameras and great progress in deep learning and computer vision,
    visual hand gesture recognition has the vast potential, which can be categorized
    into two groups, i.e., static ones and dynamic ones. The former aims to match
    the gesture in a single image to some predefined gestures, while the latter tries
    to recognize the dynamic gesture from an image sequence, which is more useful.
    Usually, there are three phases in dynamic hand gesture recognition, i.e., hand
    detection, hand tracking, and gesture recognition. While hand detection and tracking
    can benefit from recent progress in generic object detection and tracking as described
    in Sections III-A2 and III-A3, hand gesture recognition can also borrow useful
    ideas from the area of action recognition, e.g., exploiting RNN and 3-D CNN to
    capture the gesture dynamics from image sequences. Hand gesture recognition can
    be very useful for interactions with things in AIoT systems, e.g., noncontact
    control of television and car infotainment system, and communication with the
    speech and hearing impaired [125]. 9) Crowd Counting: In the video surveillance
    scenario, it is necessary to count the crowd in both indoor and outdoor areas
    and prevent crowd congestion and accident. For practical AIoT applications with
    crowd counting ability, WI-FI, Bluetooth, and camera-based solutions have been
    proposed by estimating the connections between smartphones and WI-FI access points
    or Bluetooth beacons [126] or estimating the crowd density of a crowd image [127].
    Although counting the detected faces or heads in a crowd image can be used for
    crowd counting intuitively, the person instance in a crowd image is always in
    relatively low resolution and blurry, which limits the performance of the detection
    model. Besides, detecting a vast amount of persons in a single shot is computationally
    inefficient. Therefore, most CNN-based methods directly regress the crowd density
    map, in which the ground truth is constructed by placing Gaussian density maps
    at the head regions. Since it is costly to collect and annotate crowd images,
    synthetic data sets can be used and have demonstrated its value for this task,
    i.e., either being used in the pretraining-finetuning scheme or by DA [128]. Despite
    the progress in this area, more efforts are needed to address real-world challenges
    for practical AIoT applications, e.g., designing lightweight and computational
    efficient crowd counting models, simultaneous crowd counting and crowd flow estimation,
    and integration of multimodal sensors for more accurate crowd counting. The representative
    benchmark data sets in the aforementioned research areas related to human-centric
    perceiving are summarized in Table IV. TABLE IV Representative Benchmark Data
    Sets in Human-Centric Perceiving. ID: Identity; BBox: Bounding Box In the following,
    we review several topics related to 3-D perceiving, including depth estimation,
    localization, and simultaneous localization and mapping (SLAM). 10) Depth Estimation/Localization/SLAM:
    Estimating depth using cameras is a long-standing research topic [109], [129]–[131].
    In real-world AIoT applications, there can be several configurations, such as
    the monocular camera, stereo camera, and multiview camera system. Recently, depth
    estimation from monocular video together with camera pose estimation has attracted
    a lot of attention. In contrast to traditional matching and optimization-based
    methods, current research on this topic mainly focuses on deep learning in an
    unsupervised or self-supervised way [132]. Nevertheless, they construct the self-supervisory
    signals based on the reprojection photometric loss with respect to depth and camera
    pose derived from the well-defined multiview geometry, which is similar to the
    matching error or photometric error terms in the traditional optimization objective.
    Although CNN has powerful representation capacity, special effort has to be made
    to address the challenges, including occlusions and dynamic objects, as well as
    the scale issue (per-frame ambiguity and temporally inconsistent). The aforementioned
    camera pose estimation is also related to visual odometry (VO) and visual-inertial
    odometry (VIO) [133], [134], which aim to calculate sequential camera poses of
    an agent based on the camera and inertial measurement unit (IMU) sensors. VO and
    VIO are always used at the front end in a SLAM system, where the back end refers
    to the nonlinear optimization of the pose graph, aiming to obtain globally consistent
    and drift-free pose estimation results. In traditional methods such as ORB-SLAM
    [135], the front end and back end are two separate modules. Recently, a differentiable
    architecture named neural graph optimizer is proposed for global pose graph optimization
    [136]. Together with a local pose estimation model, it achieves a complete end-to-end
    neural network solution for SLAM. Depth estimation, pose estimation, VO/VIO, and
    SLAM constitute the important 3-D perceiving ability of AIoT, which could be very
    useful in smart transportation [137], smart industry [138], smart agriculture
    [139]–[141], smart cities, and homes [142]–[144]. For example, deploying multiple
    cameras at different viewpoints, one can construct a multiview visual system for
    depth estimation and object or scene 3-D reconstruction. In the autonomous driving
    scenario, depth estimation can be integrated into the object detection module
    and road detection module for forward collision warning. Besides, SLAM can be
    used for lane departure warning, lane keeping, high-precision map construction,
    and update [137]. Other use cases may include self-localization and navigation
    for the agricultural robot, sweeper robot, service robot, and unmanned aerial
    vehicle (UAV) [138]–[140]. The representative benchmark data sets in the areas
    of 3-D perceiving are summarized in Table V. Fig. 6 presents an example of using
    AI techniques for generic scene understanding, human-centric perceiving, and 3-D
    perceiving. TABLE V Representative Benchmark Data Sets in 3-D Perceiving Fig.
    6. of AI techniques for generic scene understanding, human-centric perceiving,
    and 3-D perceiving. (a) Frame from the video “Walking Next to People”11. (b) Processed
    result by using different perceiving methods, i.e., semantic segmentation [85];
    object detection [106]; text spotting [91]; human parsing [86]; human pose estimation
    [107]; face detection, alignment, and facial attribute analysis [96], [108]; and
    depth estimation [109]. Show All Due to sensor quality and imaging conditions,
    the captured image may need to be preprocessed to enhance illumination, increase
    contrast, and rectify distortions before being used in the aforementioned visual
    perception tasks. In the following, we briefly review the recent progress in the
    area of image enhancement as well as image rectification and stitching. 11) Image
    Enhancement: Image enhancement is a task-oriented task that refers to enhancing
    the specific property of a given image, such as illumination, contrast, and sharpness.
    Images captured in a low-light environment are in low visibility and difficult
    to see details due to insufficient incident light or underexposure. An image can
    be decomposed into the reflectance map and illumination map based on the Retinex
    theory [145]. Then, the illumination map can be enhanced, thereby balancing the
    overall illumination of the original low-light image. However, it is a typical
    ill-posed problem to obtain the reflectance and illumination from a single image.
    To address this issue, different prior-based or learning-based low-light enhancement
    methods have been proposed in recent literature. For example, LIME leverages a
    structure prior of the illumination map to refine the initial estimation [146]
    while a piecewise smoothness constraint is used in [147]. Since low-light images
    usually contain noises that will be amplified after enhancement, some robust Retinex
    models have been proposed to account for noise and estimate reflectance, illumination,
    and noise simultaneously [147], [148]. Images captured in a haze environment are
    in low contrast due to the haze attenuation and scattering effects. Recovering
    the clear image from a single hazy input is also an ill-posed problem, which can
    be addressed by both prior-based and learning-based methods [149]–[151]. For example,
    He et al. propose a dark channel prior to estimate the haze transmission efficiently
    [149]. Cai et al. [150] proposed the first deep CNN model for image dehazing,
    which outperforms traditional prior-based methods by leveraging the powerful representation
    capacity of CNNs. Recently, Zhao et al. [152] proposed a real-world benchmark
    to evaluate dehazing methods according to visibility and realness. When images
    are captured in the low-light and haze environment, it comes to the more challenging
    case, i.e., nighttime image dehazing. Similarly, some methods have been proposed
    based on either statistical priors or deep learning, e.g., maximum reflectance
    prior [153], glow separation [154], and ND-Net [155]. 12) Image Rectification
    and Stitching: Wide field-of-view (FOV) cameras, such as fisheye cameras, have
    been widely used in different AIoT applications, e.g., video surveillance and
    autonomous driving since they can capture a larger scene area than narrow FOV
    cameras. However, the captured images contain distortions since they break the
    perspective transformation assumption. To facilitate downstream tasks, the distorted
    image should be rectified beforehand. The rectification methods can be categorized
    as camera calibration-based methods and distortion model-based methods. The former
    calibrate the intrinsic and extrinsic parameters of cameras and then rectify the
    distorted image by following perspective transformation. The widely used calibration
    method is proposed by Zhang [156] based on planar patterns at a few different
    orientations, where radial lens distortion is modeled. The latter directly estimate
    the distortion parameters of a distortion model and map the distorted image to
    the rectified image based on it accordingly. Different geometric cues have been
    exploited for formulating the optimization constraints in optimization-based methods
    or loss functions in learning-based methods [157], such as lines and vanishing
    points. Given two or more fisheye cameras with calibrated parameters, a panorama
    image can be obtained from their images by image stitching. For example, Liu et
    al. [158] proposed an online camera pose optimization method for the surround
    view system, which is composed of several fisheye cameras around the vehicle.
    The surround view system can capture a 360° view around the vehicle, which is
    useful in IoV for the advanced driver assistant system and crowd-sourcing high-precision
    map update. In addition to the above reviewed visual perception methods, we then
    present a brief review of auditory perception, specifically speech perception.
    We include two topics in the following part, i.e., speech recognition and speaker
    verification. 13) Speech Recognition: Speech recognition, a.k.a. automatic speech
    recognition (ASR), is a subfield of computational linguistics that aims to recognizing
    and translating spoken language into text automatically. Traditional ASR models
    are based on hand-crafted features like cepstral coefficient and hidden Markov
    model (HMM) [159], which have been revolutionized by the DNN for end-to-end modeling
    without the need of domain knowledge for feature engineering, HMM design, as well
    as explicit dependency assumption. For example, RNN, especially long short-term
    memory (LSTM), is used to model the long-range dependencies in the speech sequence
    and decode the text sequentially [24]. However, for one thing, extra effort is
    needed to presegment training sequences so that the classification loss can be
    calculated at each point in the sequence independently, for another, RNN processes
    data in a sequential manner, which is parallel-unfriendly. To address the first
    issue, the CTC is proposed by directly maximizing the probabilities of the correct
    label sequence in a differentiable way [92]. To mitigate the other issue, the
    transformer architecture is devised using scaled dot-product attention and multihead
    attention [160]. Recently, real-time ASR systems have been developed either using
    on-device computing or in a cloud-assisted manner [161], [162]. ASR is very useful
    in many AIoT applications since speech is one of the most important noncontact
    interaction modes. For example, ASR can be used in the smart input system [163],
    automatic transcription system, smart voice assistant [164], [165], and computer-assisted
    speech rehabilitation and language teaching [166]. The computing paradigm could
    be on-device edge computing (e.g., offline mode of smart voice assistant), fog
    computing with a powerful computing device and sound pickup system (e.g., automatic
    transcription system for conferences), as well as the cloud computing with acceptable
    latency (e.g., online mode of smart voice assistant). Besides, some related techniques
    for music and humming recognition and birdsong recognition could be useful to
    empower AIoT systems for music retrieval and recommendation and wild bird conservation.
    14) Speaker Recognition: While face recognition aims to recognize an individual
    through one’s unique facial patterns, speaker recognition achieves the same goal
    using one’s voice characteristics. A speaker recognition system is composed of
    three modules, i.e., speech acquisition and production, feature representation
    and selection, and pattern matching and classification [167]. Previously, speaker
    recognition methods are dominated by the i -vector representation and probabilistic
    linear discriminant analysis framework [168], where i -vector refers to extracting
    low-dimensional speaker embeddings from sufficient statistics. Recently, several
    end-to-end deep speaker recognition models have been devised [169], achieving
    better performance than i -vector baselines. Similar to the techniques in face
    recognition, speaker recognition also benefits from the advances in deep metric
    learning, i.e., leveraging the contrastive loss or triplet loss to learn discriminative
    speaker embeddings from large-scale data sets. Speaker recognition is one of the
    important means for identity identification, which has many applications in various
    AIoT domains, for example, automatic transcription system for multiperson meetings,
    personalized recommendation by smart voice assistants [170], and audio forensics
    [171]. Besides, speaker recognition can be integrated with face recognition for
    access control. The representative benchmark data sets in the areas related to
    auditory perception are summarized in Table VI. TABLE VI Representative Benchmark
    Data Sets in Auditory Perception. ID: Identity; BBox: Bounding Box of Speakers
    Next, we present a review of the progress in natural language processing (taking
    MT as an example) and multimedia and multimodal analysis. 15) Machine Translation:
    MT is also a subfield of computational linguistics that aims to translate text
    from one language to another automatically. Neural MT (NMT) based on deep learning
    has made rapid progress in recent years, outperforming the traditional statistical
    MT methods or example-based MT methods by leveraging the powerful representation
    capacity and large-scale training data. The prevalent architecture for NMT is
    the encoder–decoder [172]. Later, attention mechanism is used to attend to all
    source words (i.e., global attention) or only part of them (i.e., local attention)
    when decoding at each step of RNN [173]–[175]. Attention can be useful for learning
    context features related to the target and achieve joint alignment and translation,
    showing better performance for long sentences. Unsupervised representation learning
    has shown promising performance for many downstream language tasks by learning
    context-aware and informative embeddings, e.g., BERT [29]. Recently, unsupervised
    NMT has also been studied, which could be trained on monolingual corpora. For
    example, leveraging BERT as contextual embedding has been proved useful for NMT
    by borrowing informative context from the pretrained model [176]. Together with
    speech recognition and speech synthesis, MT can be extended to translation speech
    from one language to another, which is very useful in many AIoT applications,
    such as language education [166], automatic translation and transcription, and
    multilingual customer service (e.g., subway broadcast). 16) Multimedia and Multimodal
    Analysis: With the rapid growth of multimedia content (e.g., text, audio, image,
    and video) created in various Internet platforms, understanding the content becomes
    a hot research topic. Recent studies on cross-media matching and retrieval try
    to align both domains semantically by leveraging deep learning, especially adversarial
    learning [177]. However, the modality-exclusive information impedes representation
    learning. To address this issue, disentangled representation learning has been
    proposed [178], which tries to maximize the mutual information between feature
    embeddings from different modalities and separate modality-exclusive features
    from them. Image/video captioning and text-to-image generation are two generative
    tasks related to cross-modal matching, where captioning refers to generating a
    piece of text description for a given image or video [179] while text-to-image
    generation aims to generate a realistic image that matches the given text description
    [180]. In addition to the aforementioned multimedia content, there are other modalities
    of data that are also useful for scene understanding, e.g., depth image, Lidar
    point cloud, thermal infrared image. By using them with RGB images as input, cross-modal
    perceiving has attracted increasing attention in real-world applications, e.g.,
    scene parsing for autonomous driving [85], [181], object detection and tracking
    in low-light scenarios [182], [183], and action recognition [184]. There are three
    ways of fusing multimodal data, i.e., at the input level [181], at the feature
    level [85], [182], [183], [185], [186], and at the output level [184], respectively.
    Among them, fusing multimodal data at the feature level is most prevalent, which
    can be further categorized into three groups, i.e., early fusion [186], late fusion
    [185], and fusion at multiple levels [85], [182]. For example, a multibranch group
    fusion module is proposed to fuse features from RGB and thermal infrared images
    at different levels in [182], since the semantic information and visual details
    differ at different levels. Besides, Chen et al. [85] leveraged the residual learning
    idea to fuse the multilevel RGB image features and Lidar features via a residual
    structure in a cascaded manner. Multimedia generation and cross-modal analysis
    are useful in some AIoT applications, e.g., television program retrieval/recommendation
    based on speech description [165], automatic (personalized) item description generation
    in e-commerce, a teaching assistant in education, multimedia content understanding
    and responding in a chatbot, nighttime object detection and tracking for smart
    security, and action recognition for rehabilitation monitoring and assessment.
    Another research topic that is close to AIoT is multimedia coding, which has also
    been advanced by deep learning [187]. It is noteworthy that a novel idea named
    video coding for machines is proposed recently [188], which attempts to bridge
    the gap between feature coding for machine vision and video coding for human vision.
    It can facilitate downstream tasks given the compact coded features as well as
    support human-in-the-loop inspection and intervention, therefore, having vast
    potential for supporting many AIoT applications. The representative benchmark
    data sets in the areas related to natural language processing and multimedia analysis
    are summarized in Table VII. TABLE VII Representative Benchmark Data Sets in Natural
    Image Processing and Multimedia Analysis. I: Image; T: Text; V: Video; and A:
    Audio In the end, we briefly review the progress in network compression and neural
    architecture search (NAS). 17) Network Compression and NAS: Network compression
    is an effective technique to improve the efficiency of DNNs for AIoT applications
    with limited computational budgets. It mainly included four kinds of techniques,
    i.e., network pruning, network quantization, low-rank factorization, and knowledge
    distillation. Typically, network pruning consists of three stages: 1) training
    a large network; 2) pruning the network according to a certain criterion; and
    3) retraining the pruned network. Network pruning can be carried out at different
    levels of granularity, e.g., weight pruning, neuron pruning, filter pruning, and
    channel pruning based on the magnitude of weights or responses calculated by L
    1 / L 2 norm [189]. Network quantization compresses the original network by reducing
    the number of bits required for each weight, which significantly reduces memory
    use and float point operations with a slight loss of accuracy. Usually, uniform
    precision quantization is adopted inside the whole network, where all layers share
    the same bit width. Recently, a mixed-precision model quantization method has
    been proposed by leveraging the power of NAS [190], where different bit widths
    are assigned to different layers/channels. For other techniques, we recommend
    the comprehensive review in [191]. Instead of manually designing the network,
    NAS aims to automatically search the architecture from a predefined search space
    [192]. Most NAS methods fall into three categories, i.e., evolutionary methods,
    RL-based methods, and gradient-based methods. Evolutionary methods need to train
    a population of neural network architectures, which are then evolved with recombination
    and mutation operations. RL-based methods model the architecture generation process
    as a Markov decision process, treat the validation accuracy of the sampled network
    architecture as the reward, and update the architecture generation model (e.g.,
    RNN controller) via RL algorithms. The above two kinds of methods require rewards/fitness
    from the sampled neural architecture, which usually leads to a prohibitive computational
    cost. In contrast, gradient-based methods adopt a continuous relaxation of the
    architecture representation. Therefore, the optimization of neural architecture
    can be conducted in a continuous space using gradient descent, which is orders
    of magnitude faster. B. Learning Since the real world is dynamic and complex,
    using a fixed model in AIoT systems cannot adapt to the variations, probably leading
    to a performance loss. Thereby, empowering things with learning ability is important
    for AIoT so that it can update and evolve in response to the variations. Here,
    we briefly review the progress in several subareas of machine learning as diagrammed
    in Fig. 7. Fig. 7. Diagram of the learning-related topics in AIoT. Show All First,
    we review some research topics in machine learning, where none or few data/annotations
    from the target task are available, i.e., USL, SSL, TL, DA, FSL, and ZSL. 1) Unsupervised/Semisupervised
    Learning: Deep USL refers to learning from data without annotations based on DNNs,
    e.g., deep autoencoders, deep belief networks, and GAN, which can model the probability
    distribution of data. Recently, various GAN models have been proposed, which can
    generate high resolution and visually realistic images from random vectors. Accordingly,
    the models are expected to have learned a high-level understanding of the semantics
    of training data. For example, the recent BigBiGAN model can learn discriminative
    visual representation with good transferring performance on downstream tasks,
    by devising an encoder to learn an inverse mapping from data to the latent space
    [193]. Another hot research subarea is self-supervised learning, which learns
    discriminative visual representation by solving predefined pretext tasks [194].
    For example, the recently proposed SimCLR method defines a context-based contrasting
    task for self-supervised learning [34], obtaining comparable performance as fully
    supervised models. SSL refers to learning from both labeled and unlabeled data
    [195]. Usually, the amount of unlabeled data is much larger than that of labeled
    data. Recent studies adopt a teacher–student training paradigm, i.e., pseudolabels
    are generated by the teacher model on the unlabeled data set, which is then combined
    with the labeled data and used to train or finetune the student model. For example,
    an iterative training scheme is proposed in [35], where the trained student model
    is used as the teacher model at the subsequent training round. The method outperforms
    the fully supervised counterpart on ImageNet by a large margin. Since annotating
    large-scale data can be prohibitively expensive and time consuming, USL and SSL
    can be useful for continually improving models in AIoT systems by harvesting the
    large-scale unlabeled data collected by massive numbers of sensors [196]. Besides,
    the multimodal data from heterogeneous sensors (e.g., RGB/infrared/depth camera,
    IMU, Lidar, and microphone) can be used to design cross modal-based pretext tasks
    (e.g., by leveraging audio–visual correspondence and ego-motion) and free semantic
    label-based pretext task (e.g., by leveraging depth estimation and semantic segmentation)
    for self-supervised learning [197]. 2) Transfer Learning and Domain Adaptation:
    TL is a subfield of machine learning, aiming to address the learning problem of
    a target task without sufficient training data by transferring the learned knowledge
    from a source-related task [198]. Note that different from the aforementioned
    SSL where labeled and unlabeled data are usually drawn from the same distribution,
    TL does not require the data distributions of the source and the target domains
    to be identical. For example, it has been almost the de facto practice to fine-tune
    the models pretrained on ImageNet in different downstream tasks, e.g., object
    detection and semantic segmentation, for faster convergence and better generalization.
    In a recent study [36], a computational taxonomic map is discovered for TL between
    26 visual tasks, providing valuable empirical insights, e.g., what tasks transfer
    well to other target tasks, and how to reuse supervision among related tasks to
    reduce the demand for labeled data while achieving same performance. DA is also
    a long-standing research topic related to TL, which aims to learn a model from
    one or multiple source domains that performs well on the target domain for the
    same task (Fig. 8). When there are no annotations available in the target domain,
    this problem is a.k.a. unsupervised DA (UDA). Visual DA methods try to learn domain-invariant
    representations by matching the distributions between source and target domains
    at the appearance level, feature level, or output level, thereby reducing the
    domain shift. DA has been used in many computer vision tasks, including classification,
    object detection, and especially semantic segmentation [37], where obtaining the
    dense pixel-level annotations in the target domain is costly and time consuming.
    Recently, a mobile DA framework is proposed for edge computing in AIoT [38] by
    knowledge distillation from the teacher model on the server to the student model
    on the edge device. Fig. 8. Illustration of DA. Show All In real-world AIoT systems,
    there are always many related tasks involved, e.g., object detection and tracking,
    and semantic segmentation in video surveillance. Therefore, finding the TL dependencies
    across these tasks and leveraging such prior knowledge to learn better models
    are of practical value for AIoT [199]–[202]. DA could be useful for AIoT applications
    when deploying models to new scenarios or new working modes of machines [128],
    [203]–[206], e.g., “synthetic → real,” “daytime → nighttime,” or “clear → rainy.”
    3) Few-/Zero-Shot Learning: FSL, as an application of meta-learning (i.e., learning
    to learn), aims to learn from only a few samples with annotations [31]. Prior
    knowledge can be leveraged to facilitate addressing the unreliable empirical risk
    minimizer issue in FSL due to the small few-shot training set. For example, prior
    knowledge can be used to augment training data by transforming samples from the
    training set, or an extra weakly labeled/unlabeled data set, or extra similar
    data sets. Besides, it can also be used to constrain hypothesis space and alter
    the search strategy in hypothesis space. In real-world AIoT applications, there
    are always some rare cases that need to be recognized by AI models, e.g., a car
    collision, cyber attack, and machine fault. However, the collection and annotation
    of such large-scale cases are usually very difficult. Thereby, FSL can be used
    to learn suitable models in these scenarios [207]. ZSL refers to learning a model
    with good generalization ability that can recognize unseen samples, whose classes
    have not been seen previously. Usually, auxiliary semantic information is provided
    to describe both seen and unseen classes, e.g., attributes-based description and
    text-based description. Thereby, each category can be represented as a feature
    vector in the attribute space or lexical space (a.k.a. semantic space). In some
    cases, the semantic space is a given learned semantic space, e.g., label embedding
    space or text-embedding space, where semantically similar classes are embedded
    as nearby vectors. Therefore, ZSL can be formulated as learning a mapping from
    the data space to the semantic space [32]. ZSL can be useful in some AIoT application
    scenarios. For example, in fault diagnosis [208], some specific types of faults
    may occur in the future and should be recognized, but no training instances belonging
    to them have ever been collected previously. In other cases, classes may change
    over time or new classes may emerge, e.g., electric bicycle and tricar, where
    ZSL can be leveraged to adapt to the variations. Then, we review another two learning
    topics related to AIoT, i.e., RL and FL. 4) Reinforcement Learning: As one of
    the three basic learning paradigms along with supervised learning and USL, RL
    aims to learn a policy model for an agent through interactions with the environment
    such that it can maximize the cumulative reward. Recently, rapid progress has
    been made by incorporating deep learning in RL (i.e., DRL) [209]. DNN has a strong
    representation capacity for learning compact and discriminative feature representation
    from the high-dimensional image and video data, which enables RL to deal with
    previously intractable problems by learning better policy and value function.
    There are two main groups of DRL methods, including deep Q -network [210] and
    policy gradient-based methods, such as asynchronous advantage actor–critic [211].
    While actor–critic-based methods directly optimize the cumulative reward, a surrogate
    objective function can be used to address the distribution shift problem in policy
    gradient-based RL methods. DRL can empower things with the ability to interact
    with and adapt to the dynamic world, making it useful in many AIoT applications
    (Fig. 9), such as autonomous driving in smart transportation [199], [203], 3-D-landmark
    detection of CT scans [212] and robot control [213] in smart healthcare, course
    recommendation in smart education [41], real-time scheduling (RTS) for smart factory
    [214], load scheduling in smart grids [215], [216], plant growth control in smart
    agriculture [217], [218], and network management in smart cities [51], [219].
    Moreover, DRL can be integrated into the FL framework for privacy-preserving learning
    [199], [216]. Fig. 9. Illustration of RL in AIoT. Show All 5) Federated Learning:
    FL was initially proposed to address the learning problem in which data sets are
    distributed across multiple devices and not allowed to be leaked to others [220].
    Different data owners collaboratively train a model, whose performance is expected
    to match the performance of the model directly trained on the union of all data
    [60]. The architecture of FL usually consists of a central server (or collaborator)
    and many distributed client devices. Gradients are computed in each client using
    their own data and aggregated (or concatenated) in the server, and then sent back
    to clients to update their models (Fig. 10). FL offers a general learning framework
    to use massive distributed and isolated data while preserving data security and
    privacy, which is particularly appealing to AIoT applications in the context of
    edge computing [197], [221]. For example, FL can be used to improve the perceiving
    and learning ability of AIoT devices, such as connected vehicles for autonomous
    driving [199] and wearable devices for health monitoring [197], [222]. Fig. 10.
    Illustration of FL in AIoT. Some edge devices offload gradient computation to
    fog nodes. Show All C. Reasoning In our real world, there is much knowledge carried
    by Web information, medical records, financial transactions, etc., which can be
    used to reason the answer to a question or infer patient cohorts. We humans have
    the ability of causal reasoning, such as causal inference and causal discovery.
    Empowering AIoT with such reasoning abilities is important for making smart and
    explainable decisions. In this part, we present the review on two related topics,
    i.e., knowledge graph (KG) and reasoning, as well as causal reasoning. 1) Knowledge
    Graph and Reasoning: KG is an efficient structured way to represent knowledge
    in a graph, where nodes represent entities, and edges represent relations (a.k.a.
    facts) in the forms of triples, i.e., (head entity, relation, and tail entity).
    Some well-known KGs, such as WordNet, Freebase, YAGO, and NELL have been constructed
    and used in many applications via knowledge reasoning. Knowledge reasoning refers
    to inferring new knowledge based on the existing knowledge, e.g., identification
    and removal of erroneous knowledge, adding missing knowledge, answering questions,
    and drawing conclusions. Classical knowledge reasoning methods are based on rules,
    including first-order predicate logic rules, probability rules, ontology languages,
    and path rules. Recently, KG embedding-based methods have attracted significant
    attention, aiming to embed the entities and relations in a KG into continuous
    vector spaces such that the reasoning can be done by leveraging translational
    distance models and semantic matching models [223]. Knowledge reasoning is useful
    for many downstream tasks, which can be categorized into in-KG and out-of-KG.
    In-KG applications include graph refinement (e.g., completion and error detection),
    triple/entity classification, and entity resolution. Out-of-KG applications include
    relation extraction, question answering, and recommendation, which are related
    to specific AIoT scenarios, such as network forensics analysis in smart security
    [224], smart assistants in smart healthcare and agriculture [39], [225], explainable
    recommendation in smart education and e-commerce [40], [226], digital twins in
    smart factory [163], [227], and fault diagnosis in smart grids [228]. Note that
    one crucial feature of knowledge reasoning is explainable, which has the potential
    to be incorporated with deep learning and mitigate the interpretability issue
    in AIoT applications. 2) Causal Reasoning: Causality refers to the generic relationship
    between an effect and the cause, where the cause partly gives rise to the effect
    and the effect partly depends on the cause. Causal reasoning includes the topics
    of causal inference that aims to estimate the causal effect and causal discovery
    that aims to find causal relations. One classical way of reasoning causality is
    via randomized controlled trial by evaluating the outcomes from the treatment
    group and control group, which is, however, costly and time consuming. Recently,
    learning causality from observational data has attracted much attention [229].
    There are two well-known causal models used for learning causality, i.e., the
    structural causal models and the potential outcome framework (a.k.a. Rubin Causal
    Model). Different representation learning, multitask learning, and metalearning
    methods have been proposed for causal inference under the potential outcome framework.
    Causal reasoning is useful in many AIoT applications, such as online recommendation
    in smart e-learning [226], fault analysis in smart grids [230], and driving safety
    in smart transportation [231]. Moreover, causal inference can be used to unfold
    the “black-box” decision process of DNNs to address the interpretability issue,
    including model-based interpretation methods [232] and example-based interpretation
    methods [233]. They are crucial for building explainable AIoT systems-based DNNs,
    which have been widely used in many perceiving tasks as described in Section III-A.
    D. Behaving The behaving ability is also very important for AIoT systems passively
    responding to environment variations and raised requests, or actively exploring
    the unknown. Thereby, in this part, we present a brief review of two topics related
    to behaving in AIoT, i.e., control and interaction. 1) Control: The term of control
    here refers to controlling sensors and actuators in an AIoT system to transform
    the current system state to the target. The system state can be measured by sensors
    or calculated based on the perceiving methods described in Section III-A. The
    target state can be calculated based on predefined rules or determined by a decision
    model. The control algorithms are task oriented and specifically designed in different
    AIoT systems regarding their physical structures and sensors. For example, there
    are two kinds of control in autonomous driving systems, i.e., lateral control
    (steering) for autonomic turning and lane keeping, and longitudinal control (brake
    and throttle) for autonomic braking and forward collision avoidance. Multirobot
    systems have been used in many AIoT applications, such as smart logistics and
    precision agriculture, where decentralized control methods and coordination strategies
    are proposed for achieving and maintaining formations. Recently, deep RL has been
    used for autonomous robot control, e.g., self-driving vehicle [199], [203], medical
    robot [213], and mobile service robot. In the public safety or traffic video surveillance
    scenarios, the active cameras can change its orientation and focal length by pan-tilt-zoom
    control to track and focus on specific targets [234], [235]. 2) Interaction: Real-world
    AIoT systems may interact with humans and the environment in different ways, which
    can be categorized into three groups: 1) using input and output devices for communication,
    e.g., keyboard, mouse, touch screen, microphone, and headset; 2) using mechanical
    arms for gripping and moving items, e.g., humanoid robots and industrial robots;
    and 3) using gearing for moving and transportation, e.g., wheels of mobile robots.
    In the first category, AIoT systems may communicate with humans via multimedia,
    such as text, speech, image, and video. For example, AIoT systems can send a message
    to user end or edge devices and display it on the screen as a reminder or a response
    to the user query. The message may contain text, audio, and video, which can be
    prerecorded or searched from the database. Besides, the content can also be automatically
    created according to the user request. For example, in the dialogue system of
    a smart home assistant, the user may send: 1) a picture or video to the assistant
    for generating a text description about it, which is related to image and video
    captioning [179]; 2) a keyword to the assistant for writing a poem [236]; 3) a
    piece of text description or a sketch to the assistant for creating an image [180],
    [237]; and 4) lyrics and score to the assistant for composing a piece of music
    [238]. Speech is another way for AIoT systems communicating with humans, which
    is enabled by speech synthesis [42], e.g., in question-answering system [163]
    and multilingual translator [166]. Moreover, AIoT systems can create virtual 3-D
    objects or scenes and render them on the AR/VR glasses or headsets. Human can
    interact with the virtual objects in the virtual environment or real objects in
    the augmented real environment. In the second group, a service robot in the smart
    home scenario may use its arms with flexible joints to grip a cup and fill it
    with water for elderly people. Besides, robot arms in the scenario of the Industrial
    IoT (IIoT) can be used for assembling in industrial product lines [138]. Note
    that the human–robot interaction is a long-standing research topic that special
    efforts have been made in task planning and programming [239], safe interaction
    [138], and imitation learning [240]. Recently, the idea of a digital twin has
    been proposed for the smart factory, various human interactions are enabled, such
    as question-answering and voice control [163], [227]. In the third group, due
    to the simple kinematics, wheel-based mobile robots are widely used for SLAM and
    navigation to explore and interact with the unknown environment [137], [139].
    Note that there may be several interactive ways in an AIoT system upon the usage
    scenarios and devices. SECTION IV. AIoT Applications As reviewed in Section III,
    the progress of AI shows a great potential to empower the connected things in
    AIoT systems with the ability for perceiving, learning, reasoning, and behaving.
    The resulting AIoT systems will have a huge impact on the economic sectors and
    our living environments, such as security, transportation, healthcare, education,
    industry, energy, agriculture, as well as our homes and cities. In this part,
    we showcase some promising applications of AIoT in these areas (Fig. 11) and demonstrate
    how will AI enable the AIoT systems to be faster, smarter, greener, and safer.
    Fig. 11. AI empowers things with the ability of perceiving (P), learning (L),
    reasoning (R), and behaving (B) in many AIoT domains. Show All A. Smart Security
    The goal of smart security is to ensure the security of our physical world and
    cyberspace, which can be achieved with the help of various AIoT systems. One of
    the most important features of them is human-centric perceiving, which can recognize
    the identities of individuals and analyze their behaviors to prevent illegal activities.
    For example, face recognition systems have been deployed in building entrance,
    railway station, and airport, enabled by cloud/fog computing [100] or edge computing
    [101]. Despite their utility, one major concern is data security and privacy preservation.
    Recently, a lightweight solution is proposed in [241] by block-based logic transformation,
    which not only reduces feature size but also preserves the original feature, therefore
    appealing to resource-limited AIoT devices and resistant to potential attacks.
    Beyond the biometric features from the face, fingerprint, and iris for recognition,
    spatial and temporal human body features (e.g., shape and gait) are leveraged
    for person reidentification, which aims to recognize individuals and trace their
    trajectories in multiple cameras. However, deploying the technique in a real-world
    scenario faces the domain shift challenge arisen from camera view differences
    (e.g., viewpoint, illumination, and resolution). To address it, Zhang et al. [206]
    proposed a style translation-based method for cross-domain person reidentification
    in camera sensor networks, which can reduce domain shift and learn domain-invariant
    features. Besides, recognizing identities of human-related objects, e.g., vehicle
    license plate recognition [242], is also useful for tracing human trajectory.
    Moreover, to capture salient and high-resolution humans and vehicles, active cameras
    can be used in AIoT systems by adjusting the orientation and focal length according
    to target location [234], [235]. Beyond the aforementioned techniques for individual
    analysis, estimating the crowd density and monitoring the crowd flow is also very
    important for public safety [128], e.g., avoid deadly accidents of trampling and
    crushing of pedestrians. Some AIoT applications in different domains are summarized
    in Table VIII according to the enabling AI technologies. TABLE VIII AIoT Applications
    Empowered by Different AI Technologies B. Smart Transportation Smart transportation
    enabled by AIoT covers traffic participants (e.g., smart IoV [256]), traffic infrastructures
    [257], and industry applications (e.g., smart connected logistics [258]). Among
    them, the self-driving car is a typical example empowered by AI, which integrates
    various perceiving, learning, reasoning, and behaving abilities together. The
    self-driving system should perceive the driving environment, such as detecting
    road [85], traffic sign [259], pedestrian [260], and car [261], estimating the
    intention of cars and pedestrians and predicting their trajectories [244], [245].
    Besides, it should also measure the pose and location of landmarks (e.g., traffic-signs)
    for SLAM [137]. Based on them, the self-driving system can determine its driving
    policy and interact with other traffic participants. Recently, deep RL is leveraged
    to learn driving policy directly from visual input (e.g., front-view images).
    However, carrying out the training in the real world is unaffordable. To address
    the issue, both DA [203] and TL methods [199] are proposed by leveraging virtual
    3-D game engines. AIoT can also enable in-car driver monitoring and interactions
    with the infotainment system. Monitoring dangerous driver behaviors is crucial
    for preventing traffic accidents. To this end, a mobile application is developed
    in [121] for detecting dangerous driving behaviors, e.g., distraction and drowsiness.
    It leverages multimodel data for behavior detection, including images for face
    state detection (e.g., eyes openness and head yaw angles), motion data from IMU
    sensors for estimating vehicle location and speed, light level for estimating
    lighting conditions, and speech data for speech state detection (e.g., speech
    rate and loudness). Moreover, the research in [231] shows that identifying the
    causal factors of driving fatigue can be used for triggering corresponding countermeasures.
    For example, sleep-related fatigue is resistant to most interventions while task-related
    fatigue (e.g., distraction) can be counteracted effectively. Recently, AI interaction
    techniques, such as gesture and speech recognition, have been used for noncontact
    control in the in-car infotainment system [165]. The study in [164] shows that
    voice control allows drivers to keep watching out-car environment and demands
    lower mental load. In contrast, handheld controls may distract driver’s attention
    and require higher mental demand, probably leading to dangerous driving behaviors.
    C. Smart Healthcare AIoT systems for smart healthcare cover several phases, including
    monitoring, examination, surgery, and rehabilitation. For monitoring, both wearable
    devices with motion sensors [262] and cameras [263] can be used for human activity
    recognition. Using motion sensors such as accelerometer, human activities can
    be recognized from time-series motion data based on a CNN model. Recently, a mobile
    robot with a camera is used for human activity recognition, where the control
    policy is obtained using deep RL by maximizing the recognition accuracy while
    minimizing its energy consumption. Note that the recognition module can be deployed
    in the edge devices or in the fog node depending on the model size and computational
    demand, which can be connected to the smart healthcare systems in community healthcare
    centers or hospitals. For examination, deep learning has been used for medical
    image understanding, such as 3-D-landmark detection in CT scans [212] and semantic
    segmentation [75]. Usually, these models are deployed on the private cloud of
    the hospital due to their high computational cost and the privacy concern. Recently,
    deep RL has been used to control the surgical robot [213] for multilateral cutting
    in 2D orthotropic gauze. Furthermore, AIoT systems can be useful for various rehabilitation
    monitoring and assessment [120], e.g., stroke rehabilitation and ankles rehabilitation.
    Via the connected 3-D AR/VR devices, therapists can assess the rehabilitation
    and make better treatments accordingly, which is helpful for patients in rural
    or distant areas. Recently, online healthcare services or medical assistant robots
    can offer convenient information-query and auxiliary diagnosis services. For example,
    a hierarchical attention network is proposed to make explainable and accurate
    answers by exploiting the structural, linguistics, and visual information within
    a multimodal medical KG [39]. D. Smart Education AI technologies can empower AIoT
    things to help children and students recognize new species, learn native or foreign
    languages, choose personalized learning resources, and help those with visual
    impairments learn through interactions. For example, a weight imprinting-based
    FSL method is proposed in [207], which can be used in edge devices to recognize
    new bird and animal species with only a few labeled samples. In [246], Raspberry
    Pi is used to build a smart classroom by leveraging hand gesture recognition and
    text recognition technologies. This AIoT system enables to control Raspberry Pi
    to capture the lecture notes in blackboard/whiteboard via static hand gesture,
    recognize the text in the image (e.g., numbers, characters, and symbols), and
    convert them into an editable format, which is then saved in private cloud and
    shared to students for further editing or collaboration via the desktop application.
    A speech-to-speech multilingual translation system on mobile devices is proposed
    in [166], which includes three modules, i.e., speech recognition, language translation,
    and text-to-speech synthesis. It can work in offline mode and offers grammatical
    information that is useful for language learners. Recently, many mobile translator
    products have been released in CES 2020, which can translate tens of languages,
    benefiting from the advances of AI technologies such as deep learning. As a complement
    of on-campus learning, online learning via massive open online course (MOOC) platforms
    (e.g., Coursera) has become very popular. Learners watch the courses via their
    end devices connected to the cloud. Recommending courses from massive numbers
    of candidates could be helpful for personalized learning. To this end, deep RL-based
    [41] and rule-based [226] recommendation methods have been proposed. Besides,
    helping children with visual impairments in education also matters. For example,
    multisensory interactive maps are designed [247], which can help children to acquire
    skills via interactions, e.g., touching, listening, tasting, and scenting. E.
    Smart Industry Digital twin, i.e., a mirror digital representation of a physical
    system, has demonstrated great value for smart factories in Industry 4.0, e.g.,
    monitoring the manufacturing process, diagnosing the fault, and preventing downtime.
    AIoT can be a critical part of implementing digital twins where the connected
    sensors and actuators can collect real-time data from production lines and send
    them to the digital twin running in the cloud (Fig. 12). Moreover, AI technologies
    can enable an intelligent analysis of data and help to make smart decisions. Recently,
    a service-oriented digital twin model is proposed in [163], which uses an ontology-oriented
    knowledge structure to represent the knowledge about the manufacturing system
    from the sensing data. It also designs a vocal interaction system for knowledge
    retrieval based on speech recognition and text-to-speech synthesis. In [227],
    a KG-based digital twin model is introduced which is composed of four parts, i.e.,
    feature extraction, ontology creation, KG generation, and semantic relation extraction.
    It can extract and infer knowledge from large-scale production line data and enhance
    manufacturing process management via semantic relation reasoning. RTS in the smart
    factory is another hot research topic. In [214], a RL-based RTS model is proposed,
    which can incrementally update and maintain the knowledge base in RTS during operations
    to respond to shop floor environment change. Fig. 12. Diagram of the digital twin
    system in a smart factory. Show All A typical example of AIoT application in the
    smart industry is the printed circuit board (PCB) manufacturing. There are three
    scenarios that are related to AIoT systems with different sensors and devices,
    i.e., manufacturing, visual defect inspection, and machine fault diagnosis. First,
    industrial robots have been widely used in the production line of smart factories,
    e.g., for drilling and grasping. AI technologies can be used to improve their
    functionalities. For example, Bousmalis et al. [204] proposed a deep robotic grasping
    model named GraspGAN, which bridges the domain gap between synthetic images and
    real-world ones via the pixel-level image translation and a feature-level domain
    classifier. To increase the safety, speed, and accuracy of autonomous picking
    and palletizing, Krug et al. [138] proposed a novel grasp representation scheme
    allowing redundancy in the gripper pose placement. Second, PCB defect inspection
    carried out by workers manually is laborious and time consuming. Recently, deep
    learning-based methods have been proposed for automatic real-time visual defect
    inspection [264]. Third, it is important to predict and diagnose machine faults
    from sensor data to reduce PCB defects, thereby increasing production efficiency
    and reducing losses. Although the digital twin system provides a useful mirror
    virtual environment for creating and testing new equipment and models, it is still
    challenging to fast adapt the trained model or control policy to the physical
    world. Thereby, more efforts should be made in the areas of DA, TL, and metalearning.
    Besides, since it is difficult to collect and annotate edge samples in the industrial
    context, zero-/FSL is also worth further study. In addition, causal analysis of
    the product defects based on data and knowledge is also of practical importance.
    F. Smart Grids AIoT in the smart grids can be used for grid fault diagnosis, load
    monitoring and scheduling, and cyber attack detection. For example, UAVs connected
    to the control center via the cellular network are used for damage classification
    and estimation of power distribution poles [248]. The captured image by UAVs is
    sent back to the cloud and processed by a CNN model to predict falling damage
    extent and fire damage extent. Besides, some “industrial stethoscopes” are designed
    to recognize and localize fault sound sources in visual scenes via cameras and
    multiple microphones, where the algorithm can run on edge devices for real-time
    monitoring. Recently, a two-stream network with an attention mechanism is proposed
    to directly localize a sound source in images [249]. AI technologies have also
    advanced fault diagnosis. For example, a convolutional sparse autoencoder-based
    USL method has been proposed for power transmission line fault diagnosis based
    on voltage and current signals [250]. It can detect faults within 7-ms latency,
    which is appealing to real-world applications. Besides, knowledge representation
    [228] and causal relationship discovery [230] are also explored for power grid
    fault diagnosis and impact causal analysis. For load monitoring and electric vehicle
    charging scheduling, TL [200] and deep RL [215] methods are proposed. Furthermore,
    to preserve the privacy of the households while intelligently managing their load
    scheduling, a distributed deep RL method is proposed inspired by the idea of FL
    [216], where the action networks are located at distributed households and the
    critic network is located at an aggregator from a trusted third party. The security
    concern about the smart grids under cyber attacks has attracted much attention.
    Recently, a semisupervised deep learning method is proposed based on autoencoder
    and GANs, which can effectively detect false data injection attacks in smart grids.
    G. Smart Agriculture Recently, the concept of precision agriculture becomes popular,
    referring to observing, measuring, and responding to crop variability through
    sensors, autonomous agricultural machines, and geographic information systems,
    which can be achieved by AIoT in smart agriculture. For example, crop counting
    and yield estimation is an important topic of precision agriculture [141], [202].
    UVAs are used for capturing images of crops and fruits and sending them to the
    cloud for further counting [202]. Note that accumulating the counting results
    across image frames does not lead to the total yield since fruits are double-counted
    in adjacent frames. To address this issue, a detection-tracking-counting-based
    method is proposed [141], which can reject outliers and double-counted fruits.
    UAVs can also be used for continuous crop monitoring by capturing crop field images
    over time and temporally aligning them [251]. The path planning [140] and self-localization
    and navigation [139] abilities of UAVs are critical to complete the above tasks,
    which have been studied in the agricultural scenario. For plant grow control,
    an AIoT system is set up in a tomato greenhouse [217], which is composed of a
    wireless sensor network and cloud computing center empowered by AI technologies.
    Deep RL is used for obtaining the optimal control policy on the illumination,
    nutrition, and ventilation conditions in the greenhouse. For crop management and
    pest/disease control, agricultural knowledge retrieval and answering system will
    be helpful. To this end, an agricultural KG, namely, AgriKG [225], is automatically
    constructed by leveraging NLP and deep learning techniques. H. Smart Cities/Homes/Buildings
    Smart cities/homes/buildings AIoT is related to those in the aforementioned smart
    sectors and can be empowered by similar AI technologies. The examples are as follows.
    Continuous speaker authentication [170] in smart home voice assistants by integrating
    body-surface vibration signals with speech signal is related to smart security
    (Section IV-A). HMI systems (e.g., control television) based on hand gesture recognition
    [122], [124] in smart homes share the similar techniques for traffic sign language
    recognition in smart transportation (Section IV-B) and sign language recognition
    in smart education (Section IV-D). Recently, an event-based gesture recognition
    system is proposed by using dynamic vision sensors and a neurosynaptic event-based
    processor [124], which can identify gestures with a low latency energy consumption.
    Visual-navigation systems for the visually impaired [142]–[144], share some basic
    features, such as localization, obstacle recognition, and path planning of SLAM
    for autonomous driving in smart transportation and smart agriculture (Sections
    IV-B and IV-G), but also have some differences. For example, these systems usually
    contain some feedback modules (e.g., via vibration or speech) specifically designed
    for the visually impaired. Sound visualization systems for the hearing impaired
    are related to the industrial stethoscopes for localizing fault sound sources
    in the grid (Section IV-F), but also offer different features. For example, the
    gender of speakers, sound types, loudness, and sound directions is displayed as
    indicator icons [252], specifically designed for the hearing impaired. The energy
    management and optimization systems in smart buildings [253], [255] can be regarded
    as the extension of smart grids AIoT (Section IV-F). SECTION V. Challenges and
    Opportunities A. Challenges 1) Multimodal Heterogeneous Data Processing, Transmission,
    and Storage: AIoT systems contain massive numbers of heterogeneous sensors that
    generate a large data stream of different formats, sizes, and timestamps, thereby
    significantly challenging further processing, transmission, and storage. An efficient
    coding scheme can be used to reduce network bandwidth and transmission latency.
    For example, the video coding scheme for machines [188] is promising for facilitating
    downstream computer vision tasks and requires further study. The AI perceiving
    technologies mentioned in Section III-A could be used to extract compact and structural
    representations from the data, which would be transmission and storage friendly.
    However, the structural representation is task oriented and should be calculated
    at the network edge to minimize bandwidth and latency, which represents another
    challenge. 2) Deep Learning on Edge Devices: Deploying deep CNN models on edge
    devices is crucial for real-time data stream processing and low latency in AIoT
    systems. However, edge devices are limited by their limited computational and
    storage resources. Thereby, how to design or automatically search lightweight,
    computationally efficient, and hardware-friendly DNN architectures is of practical
    value but still remains challenging. Moreover, network pruning, compression, and
    quantization are also worth further exploring. 3) Computational Scheduling in
    the AIoT Architecture: As described in Section II-A, a typical AIoT architecture
    contains heterogeneous computing resources, including cloud centers, fog nodes,
    and edge devices. In real-world AIoT systems, some intense computation may be
    needed to offload to the fog node or cloud center from the edge devices, thereby
    creating a computational scheduling challenge. Specifically, when scheduling computation
    across different resources, the following factors should be taken into account:
    data type and volume, network bandwidth, processing latency, performance accuracy,
    energy consumption, and data security and privacy in each specific application
    scenario. Moreover, a dynamic adaptive scheduling strategy would deal with unbalanced
    data flow and user demands over time. 4) Big and Small Data for Deep Learning
    in AIoT: Big data generated from massive numbers of sensors are ubiquitous in
    AIoT systems, with huge potential for deep learning. As reviewed in Section III-A,
    deep supervised learning methods have achieved remarkable success for perceiving
    in different areas due to large-scale labeled data. However, most AIoT data are
    unlabeled, and labeling them would be both time and financially expensive. Although
    there has been rapid progress in USL, especially self-supervised learning [34],
    [194], [265], future efforts are expected to further leverage AIoT data, especially
    multimodal data. Furthermore, given the small scale of labeled data, TL, SSL,
    and FSL in the AIoT context might provide solutions to challenges from new classes,
    rare cases, and state drifting of devices. 5) Data Monopoly: In the AI era, data
    provide a valuable resource for creating new products and improving services.
    AIoT companies collect and exploit massive data, thereby leading to new opportunities
    for data collection and exploitation. This positive loop could lead to a data
    monopoly, i.e., vast proprietary data protected by established interests that
    cannot be accessed by other entities. Consequently, new competitors face a de
    facto barrier to market entry, and a data monopoly becomes a real threat to free-market
    competition. 6) Data Security and Privacy: Due to sensors being ubiquitous in
    smart homes, hospitals, and cities, vast biometric data (e.g., face image, voice,
    action, pulse, imaging data, etc.) of AIoT users or informed and uninformed participants
    may be collected. This raises important concerns with regards to data security
    and privacy. Who owns these data? How long will these data be retained? How will
    these data be used? Legislation is important in response to these concerns, a
    good example being the general data protection regulation (GDPR)12 enforced by
    the European Union, which gives individuals control over their personal data.
    Controllers and processors of personal data must take appropriate measures to
    protect data security and privacy. 7) Growing Energy Consumption in Data Centers:
    According to [266], electricity use by communication technologies is expected
    to account for 21% of global total usage, with data centers contributing more
    than 1/3 to this. Therefore, enhancing energy efficiency in data centers is required
    for a sustainable future. For example, some data centers are located in cold climates
    to take advantage of air cooling. Other solutions include water cooling and immersing
    servers in a nonconductive oil or mineral bath. Workload analysis, task scheduling,
    and virtual machine consolidation have also been studied to improve power efficiency
    in data centers [267], [268]. The rapidly growing number of cloud centers mirrors
    the rapid growth in AIoT applications. Consequently, more efforts should continue
    to be made to address energy consumption in data centers. B. Opportunities 1)
    Built-in Neural Processing Capacity for Edge Devices: Many edge devices are equipped
    with specialized chips (e.g., GPUs in smartphones and intelligent cameras) to
    accelerate neural network processing. Consequently, building neural processing
    capacity into edge devices is very useful for AIoT applications. First, it reduces
    processing latency and network bandwidth consumption. Since the sensing data can
    be processed on-site, only a small amount of processed data need to be transmitted.
    Second, it can protect data security and privacy. For example, for biometric verification,
    registered user biometric data could be stored with encryption on local hardware,
    with only the built-in verification capacity on the edge devices exposed to the
    applications, thereby reducing the risk of data leakage. Third, it enables distributed
    and asymmetric model training. An FL framework can be used to train models on
    distributed edge devices by leveraging their local sensor data. Moreover, some
    groups of devices may choose different model updating policies than others depending
    on their usage scenarios. 2) Event-Based Sensors and Neuromorphic Processors:
    Traditional camera sensors continually generate dense data once opened, which
    are further fed into deep CNNs for further GPU processing. Generally, all the
    pixels are used in the calculation, resulting in high computational costs. Recently,
    event-based sensors and neuromorphic processors have been proposed [124]. For
    example, event-based cameras only record pixels that change in brightness, thereby
    reducing redundant data generation and transmission. Event-based neuromorphic
    processors can operate on sparse and asynchronous event streams directly, avoiding
    dense and redundant computations on regular sensing data such as GPUs. These can
    be used in many AIoT applications, such as gesture/action recognition with low-power
    consumption and latency. 3) Deep Learning From the Virtual to the Real: In embodied
    AI, it is difficult and/or costly to train models in the real world, e.g., autonomous
    driving, robot arm control, and robot navigation. 3-D virtual platforms that mimic
    real-world scenarios have been proposed, such as Voyage Deepdrive,13 OpenAI gym,14
    and Habitat,15 which are very useful for cost effectively training deep learning
    models, especially for deep RL. Nevertheless, the critical issue of domain shift
    between the virtual and physical environments must be addressed before deploying
    the trained model into real-world scenarios. Recently, TL and DA have attracted
    significant attention to address this issue in the setting of both USL and RL.
    4) Data and Knowledge Integration for Perceiving, Learning, Reasoning, and Behaving:
    Deep learning model performance is largely determined by large-scale training
    data. However, humans learn new concepts-based not only on data but on prior knowledge.
    Likewise, prior knowledge can be very useful for training deep learning models
    in a data-efficient way. For example, attribute-based class description enables
    ZSL for new concepts via attribute transfer. Another example is KGs, which represent
    structural relationships between entities. Knowledge can be extracted from unstructured
    data to build KGs, learn knowledge-embedding representations, and for reasoning.
    Integrated with deep learning (e.g., graph neural networks), this is a useful
    approach in many areas, such as question and answer systems and fault/disease
    diagnosis, opening up promising research avenues toward human-level cognition
    intelligence. Therefore, data and knowledge integration are important for improving
    the perceiving, learning, reasoning, and behavior of AIoT. 5) Privacy-Preserving
    Deep Learning: Deep learning requires large-scale data generated by different
    things from different users in the context of AIoT. Individuals may worry about
    data security and privacy if data are transmitted to and stored in the cloud.
    To alleviate these concerns, privacy-preserving deep learning has attracted attention
    from both the deep learning and information security communities. The recently
    proposed FL framework (refer to Section III-B5) is a representative and promising
    solution that allows data to be stored locally in distributed devices. Homomorphic
    encryption [269] has been used in FL to prevent data leakage to the server. All
    abbreviations in this article are listed in Table IX. TABLE IX List of Abbreviations
    SECTION VI. Conclusion Here, we present a comprehensive survey of AIoT, covering
    AIoT computing architectures; AI technologies for empowering IoT with perceiving,
    learning, reasoning, and behaving abilities; promising AIoT applications; and
    the challenges and opportunities facing AIoT research. The three-tier computing
    architecture of AIoT provides different computing resources for deep learning
    whilst also posing new challenges, e.g., in the design and search of lightweight
    models and computation scheduling within the three-tier architecture. Deep learning
    has rapidly progressed in many perceiving areas and enables many AIoT applications.
    Nevertheless, more effort should be made to improve edge intelligence. In the
    context of USL and other machine learning topics such as RL, deep learning has
    attracted an increasing amount of attention and is useful for further improving
    the intelligence of AIoT systems to handle dynamic and complex environments. Moreover,
    reasoning based on KGs and causal analysis is a challenging but active research
    area, having the potential to enable AIoT systems to approach human-level cognitive
    intelligence. To respond to the dynamic environment, AIoT behaves via control
    and interaction, where deep learning has demonstrated its value in improving control
    accuracy and enabling multimodal interactions. In the future, empowered by rapidly
    developing AI technologies, many fast, smart, green, and safe AIoT applications
    are expected to deeply reshape our world. Authors Figures References Citations
    Keywords Metrics Footnotes More Like This Edge-Cloud Computing for Internet of
    Things Data Analytics: Embedding Intelligence in the Edge With Deep Learning IEEE
    Transactions on Industrial Informatics Published: 2021 Fog Computing: A Review
    on Integration of Cloud Computing and Internet of Things 2018 IEEE International
    Students'' Conference on Electrical, Electronics and Computer Science (SCEECS)
    Published: 2018 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Empowering Things with Intelligence: A Survey of the Progress, Challenges,
    and Opportunities in Artificial Intelligence of Things'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Alharbi H.A.
  - Aldossary M.
  citation_count: '52'
  description: The current agriculture systems compete to take advantage of industry
    advanced technologies, including the internet of things (IoT), cloud/fog/edge
    computing, artificial intelligence, and agricultural robots to monitor, track,
    analyze and process various functions and services in real-time. Additionally,
    these technologies can make the agricultural processes smarter and more cost-efficient
    by using automated systems and eliminating any human interventions, hence enhancing
    agricultural production to meet future expectations. Although the current agriculture
    systems that adopt the traditional cloud-based architecture have provided powerful
    computing infrastructure to distributed IoT sensors. However, the cost of energy
    consumption associated with transferring heterogeneous data over the multiple
    network tiers to process, analyze and store the sensor's information in the cloud
    has created a huge load on information and communication infrastructure. Besides,
    the energy consumed by cloud data centers has an environmental impact associated
    with using non-clean fuels, which usually release carbon emissions (CO2) to produce
    electricity. Thus, to tackle these issues, we propose a new integrated edge-fog-cloud
    architectural paradigm that promises to enhance the energy-efficient of smart
    agriculture systems and corresponding carbon emissions. This architecture allows
    data collection from several sensors to process and analyze the agriculture data
    that require real-time operation (e.g., weather temperature, soil moisture, soil
    acidity, irrigation, etc.) in several layers (edge, fog, and cloud). Thus, the
    real-time processing could be held by the edge and fog layers to reduce the load
    on the cloud layer, which will help to enhance the overall energy consumption
    and process the agriculture applications/services efficiently. Mathematical modeling
    is conducted using mixed-integer linear programming (MILP) for a smart agriculture
    environment, where the proposed architecture is implemented, and results are analyzed
    and compared to the traditional implementation. According to the results of thousands
    of agriculture sensors, the proposed architecture outperforms the traditional
    cloud-based architecture in terms of reducing the overall energy consumption by
    36% and the carbon emissions by 43%. In addition to these achievements, the results
    show that our proposed architecture can reduce network traffic by up to 86%, which
    can reduce network congestion. Finally, we develop a heuristic algorithm to validate
    and mimic the presented approach, and it shows comparable results to the MILP
    model.
  doi: 10.1109/ACCESS.2021.3101397
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 9
    Energy-Efficient Edge-Fog-Cloud Architecture for IoT-Based Smart Agriculture Environment
    Publisher: IEEE Cite This PDF Hatem A. Alharbi; Mohammad Aldossary All Authors
    52 Cites in Papers 5287 Full Text Views Open Access Comment(s) Under a Creative
    Commons License Abstract Document Sections I. Introduction II. Proposed Architecture
    for Smart Agriculture System III. MILP Model IV. MILP Model Design V. Results
    and Discussion Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: The current agriculture systems compete to take advantage of
    industry advanced technologies, including the internet of things (IoT), cloud/fog/edge
    computing, artificial intelligence, and agricultural robots to monitor, track,
    analyze and process various functions and services in real-time. Additionally,
    these technologies can make the agricultural processes smarter and more cost-efficient
    by using automated systems and eliminating any human interventions, hence enhancing
    agricultural production to meet future expectations. Although the current agriculture
    systems that adopt the traditional cloud-based architecture have provided powerful
    computing infrastructure to distributed IoT sensors. However, the cost of energy
    consumption associated with transferring heterogeneous data over the multiple
    network tiers to process, analyze and store the sensor''s information in the cloud
    has created a huge load on information and communication infrastructure. Besides,
    the energy consumed by cloud data centers has an environmental impact associated
    with using non-clean fuels, which usually release carbon emissions (CO 2 ) to
    produce electricity. Thus, to tackle these issues, we propose a new integrated
    edge-fog-cloud architectural paradigm that promises to enhance the energy-efficient
    of smart agriculture systems and corresponding carbon emissions. This architecture
    allows data collection from several sensors to process and analyze the agriculture
    data that require real-time operation (e.g., weather temperature, soil moisture,
    soil acidity, irrigation, etc.) in several layers (edge, fog, and cloud). Thus,
    the real-time processing could be held by the edge and fog layers to reduce the
    load on the cloud layer, which will help to enhance the overall energy consumption
    and process the agriculture applications/services efficiently. Mathematical modeling
    is conducted using mixed-integer linear programming (MILP) for a smart agriculture
    environment, where the proposed architecture is imp... (Show More) An IoT-based
    edge-fog-cloud architecture for smart agriculture system. Published in: IEEE Access
    ( Volume: 9) Page(s): 110480 - 110492 Date of Publication: 30 July 2021 Electronic
    ISSN: 2169-3536 DOI: 10.1109/ACCESS.2021.3101397 Publisher: IEEE Funding Agency:
    CCBY - IEEE is not the copyright holder of this material. Please follow the instructions
    via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles
    and stipulations in the API documentation. SECTION I. Introduction The Internet
    of Things (IoT) is one of the emerging technologies that promise to transform
    the way on how people work and live. The term IoT refers to a network of physical
    objects “things” that contain embedded systems with connectivity and computing
    power to exchange data with other devices and systems over the Internet. By 2025,
    the number of IoT devices connected to the Internet is projected to be 100 billion,
    with an economic impact of more than $ 11 trillion [1]. The recent development
    of IoT devices presents a new dimension in the agriculture field, where the IoT
    has become an ideal choice for smart agriculture due to its highly scalable and
    ubiquitous architecture. Moreover, the IoT-based smart agriculture value is estimated
    to reach $ 18.45 billion in 2022, and 75 million IoT devices are used for the
    agricultural sector in 2020 [2]. Furthermore, smart farms are projected to have
    12 million IoT points by 2023 [3]. Smart agriculture has started incorporating
    IoT solutions to improve operational efficiency, maximize yield, and minimize
    wastage through real-time field data collection, data analysis, and deployment
    of control mechanisms. Also, the diverse of IoT-based applications such as precision
    farming and smart irrigation is very helpful to the enhancement of agricultural
    processes. Thus, the IoT is considered as one of the promising solutions for embracing
    connected farms to address agriculture-based issues and increase the quality and
    quantity of agricultural production. IoT solutions are highly associated with
    cloud computing to process the huge amount of heterogeneous data sent or received
    by agriculture sensors/actuators [4]. Although cloud computing can handle smart
    agriculture applications, some of the applications and services produce a large
    amount of data and need to be processed in a real-time manner, which may cause
    a heavy load on the network, long response time, and poor quality of service,
    due to limited bandwidth [5]. Therefore, using the traditional cloud-based architecture
    may not be efficient to support these applications, which may also result in high
    energy consumption due to the transfer of agriculture data to and from the cloud.
    The Information and Communication Technology (ICT) industry is projected to account
    for 20% of the global electricity demand by 2025 [5]–[8], [9]. Usually, consuming
    electricity is accompanied by carbon emissions (CO2). fossil fuel usage is the
    primary source of CO2 [10]. Consequently, this causes the growth of carbon dioxide
    emissions. According to [11], ICT uses 730 Million ton (Mt) CO2 equivalents (CO2e)
    or 1.4% of worldwide carbon emissions. To overcome the above shortcomings, edge
    and fog computing architecture are introduced to process the real-time IoT applications
    and services at the proximity of data sources in an efficient way, which have
    several benefits (e.g., reduce energy consumption, network traffic and improve
    quality of service) compared to traditional cloud-based architecture, that does
    not exploit the latest paradigms such as fog and edge in the agriculture system
    [4], [6]. However, edge and fog computing are not a replacement for cloud computing,
    as cloud computing will still be preferable and suitable for analyzing and processing
    heavy tasks, as well as storing data in a long term. The collaboration between
    edge, fog, and cloud computing is the best practice to achieve smart agriculture
    solutions. Several related works in the literature, (e.g. in [12], [13]), have
    discussed various architectures, techniques, and methods applied for smart agriculture
    systems considering different technologies such as IoT, big data analytics, and
    cloud computing. However, none of the existing works focused on the edge-fog-cloud
    architecture intending to reduce the energy consumption, CO2 emission, and network
    traffic as considering the three computing layers (edge, fog, and cloud). Therefore,
    this paper presents a new approach for smart agriculture systems to develop an
    energy-efficient offloading of IoT agriculture applications over an edge-fog-cloud
    computing architecture, according to the resource requirements of each agriculture
    task. Also, this approach could help to enhance the solutions of many traditional
    agriculture issues by taking the advantage of edge and fog computing, which will
    improve the overall energy efficiency and reduce CO2 emission, network traffic
    of smart agriculture systems. The major contributions of this paper are summarized
    as follows: ∙ Develop an energy-efficient architecture based on mathematical modeling
    and heuristic algorithm to study the offloading of IoT applications from agriculture
    sensors to edge, fog, and geo-distributed cloud, while considering minimization
    of the overall power consumption of networking and processing of the IoT agriculture
    services. ∙ Optimize the offloading of IoT agriculture applications over an edge-fog-cloud
    architecture, which connected to the access network, metro area network, and wide
    area network, respectively, thus eliminating the associated power consumption
    and telecommunication network traffic. ∙ Evaluate the usability and the capability
    of the proposed architecture and its models, using the mixed-integer linear programming
    (MILP) model, and compared the results to the traditional approach. The remainder
    of this paper is organized as follows: Section II introduces the edge-fog-cloud
    system architecture and its interaction layers. Section III presents the mixed-integer
    linear programming (MILP) model for optimizing the offloading of IoT agriculture
    applications in the edge-fog-cloud architecture. The model’s design, scenarios,
    and the input parameters of the models are presented in Section IV. This is followed
    by discussing the optimization model results and analysis in Section V. In Section
    VI, we introduce energy-efficient agriculture IoT applications distribution heuristic
    over the edge-fog-cloud architecture (EEAIOT-EFC). Finally, Section VII concludes
    the paper and discusses future work. SECTION II. Proposed Architecture for Smart
    Agriculture System Today, the traditional cloud-based architecture for agriculture
    systems is inefficient to satisfy all the requirements of the current scenarios
    [4], [5], [7], [8], as it lacks the essential efficiency prerequisites such as
    energy consumption, CO2 emission, network traffic, and so on [6]. Consequently,
    there is a need to develop an energy-efficient architecture for a smart agriculture
    system to fulfill these requirements. This section provides an outline of the
    proposed edge-fog-cloud architecture and its role in providing dynamicity and
    efficiency based on different IoT agriculture applications. The proposed architecture
    of the smart agriculture system is shown in Fig. 1; and it consists of four essential
    layers, namely, IoT sensor layer, edge layer, fog layer, and cloud layer. The
    description of each layer of the proposed architecture is presented as follow:
    FIGURE 1. An IoT-based edge-fog-cloud architecture for smart agriculture system.
    Show All A. IoT Sensor Layer IoT sensors generate massive heterogeneous data to
    the gateways by using various sensors deployed in different areas of the agriculture
    field. Also, this layer can receive decisions from to control actuators (e.g.,
    turning on/off irrigation system) [14]. In smart agriculture, there is a range
    of IoT sensor nodes used to identify several phenomena over the urban areas including
    but not limited to soil pH, soil temperature, soil moisture, soil electrical conductivity,
    and ambient temperature [15]. In the IoT agriculture system, low power wide area
    (LPWA) technologies have paved the way due to their low power consumption and
    wide area coverage. Long range (LoRa) is proved its efficiency, as a transmission
    protocol for IoT sensors. Besides its low power consumption, it ensures an extent
    of 10 kilometers coverage or more. In addition to LoRa, multiple wireless technologies
    can be used for smart agriculture urban areas such as narrowband (NB)-IoT, WiFi,
    Zigbee, and the 5G. The Zigbee technology has been successfully used in the field
    of agriculture at a low power cost. However, the limited distance coverage for
    wireless data transferring (about 20 meters) is reducing its efficiency. A comprehensive
    comparison of different IoT wireless network technologies (Zigbee, LoRa, NB-IoT,
    and 5G), is presented in Table 1. TABLE 1 IoT Wireless Network Technologies Comparison
    B. Edge Layer Edge computing refers to a new computing model that implements the
    computation of sensors/actuators data at the edge of the network. With this concept,
    some applications and services that do not require a lot of computing resources
    can be processed in the edge layer (close to the data source) and no longer need
    to traverse the network to be processed by the fog or the cloud. Thus, edge computing
    can improve data transmission performance, ensure real-time processing, and reduce
    the computational load as well as the amount of data transmitted to and from the
    fog or cloud data centers [8]. However, in case of unavailability/unsuitability
    of the resources in the edge layer, the sensors will automatically request to
    process their data in the fog or the cloud, and this will be done hierarchically.
    C. Fog Layer The fog computing concept was initially proposed by Cisco in 2014
    to expand the resources of cloud computing to the edge of the telecommunications
    network. In this context, the fog layer has the responsibility to process and
    analyze data sent from IoT sensors, which helps to minimize the latency for agriculture
    applications and services. Also, the fog layer has the ability to process and
    analyze complex data more than the edge layer. Both fog and edge can provide computation,
    networking, and storage services in between the sensor layer and the cloud layer.
    It means that instead of executing all processing at the cloud layer, the fog
    and edge layers can process and analyze agricultural data locally and close to
    the sensor layer (based on their ability) to reduce latency and cost [5], [7].
    D. Cloud Layer At the same level of importance as edge and fog, cloud computing
    is a vital enabler for the growth of IoT agriculture applications. It offers on-demand
    computing resources and services (e.g., storage, networking, and processing) in
    a scalable way. The cloud layer handles the agriculture data received from the
    sensor layer or the fog layer to process, analyze and store them into the cloud.
    Cloud computing can process and analyze heavy data, that requires more complex
    operations (e.g., big data processing and predictive analysis like weather forecasting,
    fire warning, and soil droughting), which exceeds the fog computing capability
    [6]. Also, it could provide a large-scale secure platform and cheap data storage
    services for the IoT agriculture applications [5], [8]. E. Telecommunication Networks
    The traditional telecommunication network architecture consists of three layers
    [16]: the core layer, the metro layer, and the access network layer. The wide
    area network (WAN) is the key network infrastructure that provides interconnection
    between different regions and cities. The Internet protocol (IP) over wavelength
    division multiplexing (WDM) is widely implemented in the core network as it can
    provide high scalability, large capacity, and fast communication network transfer
    speeds. Based on the reference hierarchy in Fig. 1, every core network has a direct
    connection with a metro area network (MAN), which covers a metropolitan area.
    Metro Ethernet is the technology commonly used in the metro network. It offers
    connectivity between the core network and users located in the access network.
    The local area network (LAN) supports Internet access to numerous user premises.
    We adopted the passive optical networks (PONs) which considered as the leading
    networking in the LAN network. SECTION III. MILP Model In this section, a new
    approach is developed based on mathematical mixed-integer linear programming (MILP)
    optimization model to study the energy-efficiency of offloading IoT agriculture
    applications over an edge-fog-cloud architecture, considering the three telecom
    network layers: LAN equipped with an edge layer, MAN equipped with a fog layer
    and the WAN equipped with a cloud layer. In the following, we introduce the parameters
    and variables of our proposed architecture. The architecture consists of the IoT
    sensor, edge, fog, and cloud layers. Then, we provide the mathematical model to
    find the optimum distribution of IoT agriculture applications to serve the offloaded
    requests from the IoT sensor layer based on their energy consumption over an edge-fog-cloud
    architecture. A. IoT Sensor Layer The parameters and variables that represent
    the IoT sensor layer, are shown in Tables 2 and 3. TABLE 2 IoT Parameters TABLE
    3 IoT Variables IoT sensor layer power consumption (IoT) is composed of: ( ∑ s∈i
    IoT (number) s IoT (power) ) +( ∑ s∈i GW (number) s IoT (power) ) (1) View Source
    Equation (1) calculates the total power consumption of the IoT sensor layer, including
    IoT sensors and gateway devices. B. Edge, Fog, and Cloud Layers The following
    parameters and variables (in Tables 4 and 5) represent the IoT agriculture applications
    that will be placed in the edge, fog, or cloud layers, as well as the resulted
    traffic and power consumption. TABLE 4 Cloud, Fog, and Edge Networking and Processing
    Parameters TABLE 5 Cloud, Fog and Edge Networking and Processing Variables The
    power consumption of cloud/fog/edge nodes consist of: Cloud layer power consumption
    (Cloud): PUE (cloud) ( ∑ s∈N MIPS iot i,s PPMIPS (cloud) + ∑ s∈N PPbits (cloud)
    TU s,d )∀s=c (2) View Source Power consumption of fog layer (Fog): PUE (fog) (
    ∑ s∈N MIPS iot i,s PPMIPS (fog) + ∑ s∈N PPbits (fog) TU s,d )∀s=f (3) View Source
    Power consumption of edge layer (Edge): PUE (edge) ( ∑ s∈N MIPS iot i,s PPMIPS
    (edge) + ∑ s∈N PPbits (edge) TU s,d )∀s=e (4) View Source Equations (2, 3, and
    4) calculate cloud, fog, and edge computing layers total power consumption, including
    processing, and networking devices, taking into consideration the power usage
    effectiveness (PUE) of cloud, fog, and edge layers, respectively. C. Communication
    Networks As described in Section II-E, a typical telecom network is considered
    including WAN, MAN, and LAN networks. The traffic traverse through these layers
    as well as the corresponding power consumption are represented by the parameters
    and variables described below. 1) Local Area Network (LAN) The parameters and
    variables that define the LAN network are shown in Tables 6 and 7. TABLE 6 LAN
    Network Parameters TABLE 7 LAN Network Variables Local area networks power consumption
    (LAN) consists of: Total power consumption of LAN network: PUE (network) ( ∑ s∈N
    ONU (number) s ONU (power) ) +( ∑ s∈N OLT (number) s OLT (power) ) (5) View Source
    Equation (5) calculates the total power consumption of the LAN network, including
    Optical Network Units (ONU) and Optical Line Terminals (OLTs) devices, taking
    into consideration the network PUE. 2) Metro Area Network (MAN) The parameters
    and variables introduced to define the MAN are shown in Tables 8 and 9. TABLE
    8 MAN Parameters TABLE 9 MAN Variables The metro area network power consumption
    (MAN) consists of: PUE (network) (( MR (number) s MR (power) s ) +( MS (number)
    s MS (power) s ))∀s=N (6) View Source Equation (6) calculates the total power
    consumption of the MAN network, including router ports and switch devices, taking
    into consideration the network PUE. 3) Wide Area Network (WAN) The parameters
    and variables introduced to define WAN network are shown in Tables 10 and 11.
    TABLE 10 WAN Network Parameters TABLE 11 WAN Network Variables The wide area network
    ( WAN ) [17] power consumption consists of: PUE (network) ( ∑ d∈N r (power) r
    d + ∑ m∈N ∑ n∈ Nm m :n≠m ∑ s∈N ∑ d∈N:s≠d r s,d m,n t (power) + ∑ m∈N ∑ n∈ Nm m
    :n≠m E (power) F m,n A m,n + ∑ d∈N S (power) d ) (7) View Source Equation (7)
    calculates the total power consumption of the WAN network, including core router
    ports, transponders, amplifiers, and switch devices, taking into consideration
    the network PUE. The MILP model, considering the equations from (1-7), represented
    by the following: The objective: Minimize total power consumption: WAN+MAN+LAN+IoT+Cloud+Fog+Edge
    (8) View Source Expression (8) calculates the power consumption of our proposed
    architecture as the sum of the power consumption of the WAN network, the MAN network,
    the LAN network, IoT, cloud, fog, and edge. Subject to the following constraints:
    IoT offloading constraints: ∑ s,d∈N UI i,s,d = ∑ s,d∈N T iot i,s,d ∀i∈I (9) View
    Source Constraint (9) guarantees that all the IoT offloaded traffic is processed
    at a cloud, fog, or edge destination node. IoT application in edge/fog/cloud constraints:
    ∑ s∈N T iot i,s,d ≥ Ψ i,d ∀d∈N,i∈I ∑ s∈N T iot i,s,d ≤ω Ψ i,d ∀d∈N, i∈I (10) (11)
    View Source Constraints (10) and (11) make sure that the binary variable Ψ i,d
    =1 if processing node d∈N is powered on to place the IoT application i∈I , otherwise
    Ψ i,d =0 . Physical link-activated: L s,d m,n ≥ L s,d m,n ≤ r s,d m,n ∀s,d, m,
    n∈N r s,d m,n ∀s,d, m, n∈N (12) (13) View Source Constraints (12) and (13) ensure
    that the physical link m,n∈c is activated if there is a traffic flow between the
    nodes s,d∈c transmitting through the physical links m,n∈c. Edge, fog, and cloud
    processing requirements: MIPS iot i,d = Ψ i,d MIPS iot i,d ∀d∈N,i∈I MIPS iot d
    = ∑ i∈I MIPS iot i,d ∀d∈N (14) (15) View Source Constraints (14) gives the processing
    requirements of IoT application i∈I in a cloud, a fog, and an edge layer. Constraint
    (15) gives the total processing of a cloud, a fog, and an edge layer d∈N . Traffic
    demand on WAN network: TU s,d = ∑ i∈I T iot i,s,d ∀s,d∈c (16) View Source Constraint
    (16) calculates the demand between WAN nodes due to the IoT applications placed
    in the clouds. Flow conservation constraint: ∑ m∈N:m≠n L s,d m,n − ∑ n∈N:m≠n L
    s,d m,n = ⎧ ⎩ ⎨ L s,d − L s,d 0 i=s i=d otherwise ∀s, d∈N:s≠d (17) View Source
    Constraint (17) define the flow conservation of WAN network. It ensures that the
    total inbound / outbound traffic in all WAN nodes is identical; apart from the
    source/sink nodes. Physical link capacity: ∑ s∈N ∑ d∈N:i≠j L s,d m,n ≤WB F m,n
    ∀m, n∈N (18) View Source Constraints (18) gives the physical link capacity by
    ensuring that the traffic in a link does not exceed the maximum capacity of fibers.
    Total number of router ports in a WAN network node: r d ≥ ∑ s∈c TU s,d B ∀d∈c
    (19) View Source Constraint (19) gives the router ports count at every WAN node.
    Total number of IoT gateways: GW (number) s ≥ IoT (number) s GW (users) ∀s∈i (20)
    View Source Constraint (20) gives the number of used gateways in each farm. Total
    number of ONU terminals: ONU (number) s ≥ ∑ i∈i ∑ d∈N UI i,s,d ONU (bitrate) ∀s∈N
    (21) View Source Constraint (21) gives the number of used ONU terminals in each
    farm. Total number of OLT: OLT (number) s ≥ ∑ i∈I ∑ d∈N UI i,s,d OLT (bitrate)
    ∀s∈N (22) View Source Constraint (22) gives the number of used OLT in node s .
    Total number of MAN routers: MR (number) s ≥2 ∑ i∈i ∑ d∈(f∩c) UI i,s,d MR (bitrate)
    ∀s∈N (23) View Source Constraint (21) gives the number of used routers in each
    MAN network s . Total number of MAN switches: MS (number) s ≥ ∑ i∈i ∑ d∈(f∩c)
    UI i,s,d MS (bitrate) ∀s∈N (24) View Source Constraint (22) gives the number of
    used switches in each MAN network s . Total Traffic in communication network:
    T d = ∑ i∈i ∑ d∈N UI i,s,d ∀s∈N (25) View Source Constraint (23) gives the total
    traffic in each node s . 4) Carbon Emissions (CO2) of IoT-Edge-FOG-Cloud Layers
    Carbon emissions [18] can be defined as the carbon emission intensity per an energy
    consumption and the unit of carbon emission intensity is kgCO2e / kWh. The research
    found that using solar, wind or nuclear plants creates a low carbon footprint
    compared with fossil fuels [19]. However, there are multiple limitations to the
    usage of low carbon sources including but not limited to the cost of installing
    these clean plants. Thus, in this work, we assume that only the IoT sensor layer
    and edge layer are powered by low carbon sources (i.e., solar plants panels) to
    reduce the power consumption of the proposed architecture. In the following Tables
    12 and 13, we define parameters and variables related to carbon emissions. TABLE
    12 Emission Parameters TABLE 13 Emission Variables Total carbon emission (CO)
    is composed of: (WAN O)+(MAN O)+(LAN O)+(IoT S) +(Cloud O)+(Fog O)+(Edge S) (26)
    View Source Considering that IoT sensors, gateway, and edge processing layers
    are powered by solar energy sources. While others are powered by oil energy sources.
    SECTION IV. MILP Model Design In this section we explain the scenarios and the
    design of the model conducted in order to evaluate the proposed architecture.
    A. Scenarios As shown in Fig. 1, different scenarios can be implemented with this
    proposed architecture to show its effectiveness. In this work, the following scenarios
    are considered in a hierarchical order based on the edge, fog, and cloud ability.
    Edge/fog layers can be deployed in the proposed architecture according to the
    resources required by the agriculture tasks. Essentially, all tasks from heterogeneous
    IoT devices/sensors in the agriculture field, using different IoT wireless network
    technologies will be offloaded to the network gateways and then directed to edge/fog
    or cloud layer. Each layer has pros and cons. For example, processing the tasks
    within the edge layer will save the power and traffic cost of request transmission
    from/to the fog or cloud layer. However, handling all types of tasks within the
    edge layer is not possible, as it has limited capacity. Therefore, fog and cloud
    layers can be the choice for processing heavy tasks (e.g., resource-intensive
    applications). In our model, we assume that a scheduler in the gateway of the
    IoT layer checks if the edge node has available resources and can handle the request
    of IoT applications (e.g., CPU capability - the number of million instructions
    per second (MIPS)), the tasks will then pass to the edge layer to process them.
    In case of insufficient/unavailability of processing the tasks in the edge layer,
    the request will be transferred to the fog layer and check if there is enough
    capacity. Otherwise, the tasks will be forwarded to the cloud layer for processing,
    which supports resource-intensive applications. Also, we have assumed that the
    cloud has enough resources and capability to handle all kinds of tasks. B. Input
    Parameters of the Models In the MILP model, we have configured four layers in
    a smart agriculture system, which is composed of the IoT sensor layer, edge layer,
    fog layer, and cloud layer. The configuration of edge, fog, and cloud layers depend
    on the type of tasks (e.g., number of MIPS) requested by each IoT sensor/device
    at the IoT sensor layer. The model input parameters of different layers (IoT sensor,
    edge, fog, and cloud layers), in addition to networks and carbon emissions parameters,
    are shown in Tables 14, 15, 16, 17, and 18, respectively. TABLE 14 IoT Sensor
    Layer Input Parameters TABLE 15 Cloud, Fog, and Edge Input Parameters TABLE 16
    LAN, MAN, WAN Network Input Parameters TABLE 17 Carbon Emission Inputs for Each
    Fuel Type [17], [22] In our model, we assume that there are 100,000 sensors distributed
    in each farm The sensors task requirements are divided into three types (sensing
    60%, processing 30%, heavy processing 10%). The sensing processing task is usually
    limited to handling offloaded reading data sent by the sensors (e.g., temperature
    reading or send control commands for the irrigation system). The processing task
    is the requirement of light processing (e.g., soil analytics and event detection).
    The heavy processing task is the requisite of higher processing power and resources
    (e.g., weather prediction and analysis). SECTION V. Results and Discussion In
    this section, we discuss the proposed energy-efficient edge-fog-cloud architecture.
    In addition to its energy efficiency, we evaluate our model to find the consequence
    CO2 emission, and network traffic compared to the traditional cloud-based architecture.
    We have evaluated the proposed architecture and models using MILP optimizer based
    on the AT&T network topology, as shown in Fig. 2. To solve the MILP model, we
    use the CPLEX solver over a laptop with an Intel Core i7–7660U CPU, running at
    2.50 GHz, with 16 GB RAM. FIGURE 2. AT&T WAN network topology. Show All As shown
    in Table 1, we have categorized all IoT wireless network technologies used in
    this work based on their data rate, range, number of devices, power consumption
    of both gateway/base-station, and sensors. This work has identified that the power
    consumption of different IoT wireless network technologies almost the same, as
    shown in Fig. 3. The Zigbee technology delivers connectivity with low power consumption
    compared to other technologies. However, using Zigbee in the urban area is not
    the best choice as it only covers 20 meters, thus, hundreds or thousands of gateways
    are required to cover a large area. FIGURE 3. Comparison of different IoT wireless
    network technologies based on their energy consumption. Show All Since we aim
    to use a technology that covers a large area with the least amount of energy consumption.
    Therefore, LoRa has been chosen as an IoT wireless communication technology between
    the IoT sensors and the gateway, that covers long-distance communication, with
    low power consumption, and considers one of the most suitable technology for IoT
    agriculture applications, as shown in Fig. 3. A. Energy Consumption Fig. 4 illustrates
    the power consumptions of different tasks in the proposed edge-fog-cloud architecture
    versus the traditional cloud-based architecture. Also, it shows the placement
    location of each task/application in edge-fog-cloud architecture, as well as the
    power consumption values of each task individually. FIGURE 4. The energy consumption
    of the proposed architecture vs. the traditional cloud-based architecture, considering
    the IoT LoRa technology. Show All The results showed that the sensing tasks are
    offloaded to the edge layer, as it has enough capacity (i.e., sensing requires
    500 MIPS, and the edge layer has the capability to process up to 1800 MIPS). The
    normal processing tasks are offloaded to the fog respectively, as the fog layer
    has sufficient resources (i.e., 4000 MIPS) to process the tasks (i.e., 2000 MIPS).
    All remaining requests are offloaded to the cloud layer as there is no capacity
    in edge neither fog layers to accommodate heavy processing tasks. Fig. 5 shows
    the power consumption of our proposed architecture compared to the traditional
    cloud-based architecture, considering different IoT wireless technologies. Also,
    the figure displays power saving achieved by the proposed architecture. It is
    clearly shown that our proposed architecture outperforms the traditional cloud-based
    architecture by up to 36% of the total power consumption. However, the power savings
    have slightly ranged between 33.6% and 35.6% based on the different IoT wireless
    technologies. The Zigbee shows a higher power saving as it capable of offloading
    sensor data with lower power consumption. FIGURE 5. The energy saving of the proposed
    architecture vs. the traditional cloud-based architecture, using different IoT
    wireless technologies. Show All B. CO2 Emission Fig. 6 illustrates the total carbon
    emissions of the proposed edge-fog-cloud architecture versus the traditional cloud-based
    architecture, considering powering the IoT sensor layer and edge layer by a solar
    power source. FIGURE 6. The total carbon footprint emission of the proposed architecture
    vs. the traditional cloud-based architecture. Show All It shows that our proposed
    architecture can reduce up to 42% of CO2 emission, for real-time IoT applications
    in agriculture systems. The results also show a comparable carbon emission using
    different IoT wireless technologies. The power consumption of these technologies
    has been eliminated, as all IoT sensors and gateways are power by a solar plant,
    that emits very low carbon footprints (solar plants emit only 0.048 kgCO2/kWh).
    C. Network Traffic Fig. 7 shows the total traffic in each network tier in our
    proposed architecture versus the traditional cloud-based architecture. The results
    showed that our proposed architecture is capable to reduce the total traffic by
    14% and 86% in MAN and WAN tiers, respectively, compared to a cloud-based approach.
    FIGURE 7. The network traffic of the proposed architecture vs. the traditional
    cloud-based architecture. Show All In the traditional cloud-based architecture,
    the process of sending/retrieving the data to/from the cloud in real-time requires
    high-capacity bandwidth, which may cause a burden on the three network tiers.
    Thus, employing edge and fog computing has allowed processing most requests locally
    in edge or fog layers, which significantly decreases the flow of data traverse
    to the cloud and reduces network traffic, as shown in Fig. 7. SECTION VI. Energy
    Efficient Agriculture IoT Edge/Fog/Cloud Architecture Heuristic The problem over
    energy-efficient offloading of IoT applications in edge-fog-cloud architecture
    for smart agriculture environment is a non-deterministic polynomial (NP)-hard
    problem. For instance, if i is IoT applications count and n is the count of locations
    in edge-fog-cloud architecture, then we will have ( ∑ z=1 n n! (n−z)! ) combinations
    of possible applications locations to find the optimum locations that result in
    optimal power consumption. Thus, applying MILP to large-size problems is not feasible.
    Therefore, heuristic provides a simple and fast real-time implementation. Also,
    the optimal solution provided by the heuristic can provide validation to results
    obtained from MILP. To provide that, a heuristic algorithm was developed, referred
    to as energy-efficient agriculture IoT applications distribution heuristic over
    the edge-fog-cloud architecture (EEAIOT-EFC). In the EEAIOT-EFC heuristic, IoT
    application i is checked based on their total MIPS processing requirements. Firstly,
    the algorithm tries to place and run the application on the edge layer. If there
    is not enough MIPS capacity at the edge layer, then, agriculture IoT application
    is placed in the fog layer, if it has enough capacity. In case of unavailability
    resources in both edge and fog layers, the cloud layer will host the IoT application,
    as it has enough processing capability to handle all types IoT applications. After
    distributing all IoT applications, the power consumption of EEAIOT-EFC is determined.
    The heuristic flowchart process is shown in Fig. 8. FIGURE 8. Flowchart of EEAIOT-EFC
    heuristic. Show All The heuristic is assessed using a PC with an Intel Core i7–7660U
    CPU, running at 2.50 GHz, with 16 GB RAM. Similar to MILP, the AT&T network is
    considered a WAN network example. The heuristic took 5 seconds to evaluate the
    EEAIOT-EFC, and the MILP and EEAIOT-EFC show a comparable result, as shown in
    Fig. 9. The gaps between them are limited to 0.7% and 4.7% of the total power
    consumption under the proposed and the traditional cloud-based, respectively.
    FIGURE 9. Difference between the MILP model vs. EEAIOT-EFC heuristic. Show All
    SECTION VII. Conclusion and Future Works In this paper, the concept of an edge-fog-cloud
    architecture is introduced in the smart agriculture system, which solved existing
    real-time processing issues in terms of reducing energy consumption, CO2 emission,
    and network traffic, compared to the traditional cloud-based architecture. The
    proposed architecture employed the edge and fog layers, which are placed close
    to the agriculture fields to collect heterogeneous data from various kinds of
    IoT agriculture sensors and process them at these layers. Although the proposed
    architecture was significantly reduced the computational load and the amount of
    transmitted data to and from the cloud due to the use of edge and fog layers,
    however, the cloud layer is inevitably used to process the heavy and complex data/task
    requested by IoT agriculture devices/sensors. Most of the processing tasks are
    completed on the edge and fog layers, while few tasks are offloaded to the cloud
    layer for processing. In the paper, different metrics have been taken into consideration,
    including energy consumption, CO2 emission, and network traffic to study the performance
    and the outcomes of the proposed architecture. Using mathematical modeling, the
    proposed architecture is compared with the traditional cloud-based architecture.
    The model results showed that our proposed architecture can reduce the overall
    power consumption, carbon footprints, and network traffic by up to 43%, 36%, 86%,
    respectively. Moreover, we developed energy-efficient agriculture IoT applications
    distribution heuristic over the edge-fog-cloud architecture (EEAIOT-EFC) algorithm,
    which showed comparable results to the MILP model. Though this proposed solution
    is based on the idea of smart agriculture, it also can be suitable for other IoT
    applications and sectors, such as e-healthcare, smart city, and smart home. In
    the future, we intend to extend the proposed approach in a distributed real agricultural
    environment, considering machine-learning and decision-making algorithms to further
    understand the capability of the proposed work. ACKNOWLEDGMENT The authors would
    like to acknowledge the Deanship of Scientific Research, Taibah University, Medina,
    Saudi Arabia, for providing research resources and equipment. This work was supported
    by the Deanship of Scientific Research, Prince Sattam Bin Abdulaziz University,
    Al-Kharj, Saudi Arabia. Authors Figures References Citations Keywords Metrics
    More Like This IoT Farm: A Robust Methodology Design to Support Smart Agricultural
    System Using Internet of Things with Intelligent Sensors Association 2023 7th
    International Conference on Electronics, Communication and Aerospace Technology
    (ICECA) Published: 2023 Smart Agriculture Wireless Sensor Routing Protocol and
    Node Location Algorithm Based on Internet of Things Technology IEEE Sensors Journal
    Published: 2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Energy-Efficient Edge-Fog-Cloud Architecture for IoT-Based Smart Agriculture
    Environment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Vishwanath Y.
  - Upendra R.S.
  - Ahmed M.R.
  citation_count: '5'
  description: Agriculture is the backbone of India, and its produce must be optimized.
    The technology trio of IoT, cloud, and machine learning can have a trending impact
    on the agriculture domain. Each one of these techniques can contribute to a great
    extent in increasing agriculture productivity. Most of the research carried out
    in this direction has implemented a layered architecture. The lowermost layer
    is the sensor layer, followed by the data collection layer, and finally the data
    analytics layer. The sensor layer consists of various kinds of sensors like moisture,
    light, temperature, etc. and along with the IoT provides an ambiance to understand
    the feature of nomenclature of the plant and generates the data. The second layer
    is the data collection layer which uses cloud or fog computing basically to structure
    the enormous amount of data generated from the sensors and finally analysis layer
    where machine learning or any other rule framework is used to analyze the data
    to provide the best possible solution for better crop yield. The various types
    of sensors used, the type of IoT support that is developed, and the kind of data
    analytics used in research in recent times have been discussed in this review
    paper.
  doi: 10.1007/978-3-030-49795-8_57
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home International Conference
    on Mobile Computing and Sustainable Informatics Conference paper A Review on Advent
    of IoT, Cloud, and Machine Learning in Agriculture Conference paper First Online:
    01 December 2020 pp 595–603 Cite this conference paper Access provided by University
    of Nebraska-Lincoln Download book PDF Download book EPUB International Conference
    on Mobile Computing and Sustainable Informatics (ICMCSI 2020) Y. Vishwanath, Raje
    Siddiraju Upendra & Mohammed Riyaz Ahmed  Part of the book series: EAI/Springer
    Innovations in Communication and Computing ((EAISICC)) Included in the following
    conference series: International Conference on Mobile Computing and Sustainable
    Informatics 699 Accesses 5 Citations Abstract Agriculture is the backbone of India,
    and its produce must be optimized. The technology trio of IoT, cloud, and machine
    learning can have a trending impact on the agriculture domain. Each one of these
    techniques can contribute to a great extent in increasing agriculture productivity.
    Most of the research carried out in this direction has implemented a layered architecture.
    The lowermost layer is the sensor layer, followed by the data collection layer,
    and finally the data analytics layer. The sensor layer consists of various kinds
    of sensors like moisture, light, temperature, etc. and along with the IoT provides
    an ambiance to understand the feature of nomenclature of the plant and generates
    the data. The second layer is the data collection layer which uses cloud or fog
    computing basically to structure the enormous amount of data generated from the
    sensors and finally analysis layer where machine learning or any other rule framework
    is used to analyze the data to provide the best possible solution for better crop
    yield. The various types of sensors used, the type of IoT support that is developed,
    and the kind of data analytics used in research in recent times have been discussed
    in this review paper. Access provided by University of Nebraska-Lincoln. Download
    conference paper PDF Similar content being viewed by others Role of Machine Learning
    and Cloud-Driven Platform in IoT-Based Smart Farming Chapter © 2022 Spade to Spoon:
    An IoT-Based End to End Solution for Farmer Using Machine Learning in Precision
    Agriculture Chapter © 2021 Role of IoT in Smart Precision Agriculture Chapter
    © 2022 Keywords Wireless sensor network (WSN) Cluster Fog computing Data mining
    LoRa and Zigbee 57.1 Introduction The agriculture yield must be enhanced by 2050
    to meet the demand of the increasing population globally as per the UN report
    [14]. By this time the population is expected to reach 9.7 billion with 34% growth.
    Agriculture forms the basis for food security, and hence it is important. Hence,
    correspondingly the food yield or production must be increased to meet the population
    demand. It is possible to achieve this by either increasing the agricultural land
    or increasing the crop yield. But increasing the agriculture land by means of
    deforestation is not an appropriate solution, so it needs to go by the approach
    of increasing the crop yield which can be achieved by implementing smart or precision
    agriculture. In smart/precision agriculture, every parameter is monitored such
    as soil fertility, moisture content of the soil, light intensity, and balance
    between demand and supply (usually farmers incur a huge loss because of more produce)
    which supports productive and profitable agriculture practice. The agriculture
    practice shall be a profitable venture in support of the technology trio which
    provides a conducive ambiance that results in healthier growth of agriculture
    yield. This can be made possible by effectively utilizing current technology trends.
    In India, a majority of the population, i.e., above 55%, is dependent on agriculture
    as per the recent information. Agriculture is the field that enables the farmers
    to grow ideal crops in accordance with the environmental balance. In India, wheat
    and rice are the major grown crops along with sugarcane, potatoes, oil seeds,
    etc. Farmers also grow non-food items like rubber, cotton, jute, etc. More than
    70% of the household in the rural area depends on agriculture. This domain provides
    employment to more than 60% of the total population and has a contribution to
    GDP (about 17%) [1]. The current technology trio of IoT, cloud, and machine learning
    can contribute greatly and effectively to agriculture sector. The technology trends
    are gradually making their ways in various forms like smart agriculture, smart
    farming, precision agriculture, etc. In the present review, paper attempts were
    made to showcase the advantages and the impacts of current technology trio, that
    is, IoT, cloud, and machine learning, on optimizing and enhancing the agriculture
    productivity and also about the contemporary interdisciplinary research in the
    field of agriculture that uses technology trio. 57.1.1 Internet of Things (IoT)
    IoT is a concept of connecting any communicable device to the Internet to share
    the information and can be perceived as a huge network of various sizes and kinds
    of devices right from a nanosensor to a space vehicle. All these connected devices
    collect and share data about its functionality and the ambiance. In IoT, most
    of the objects are embedded with sensors that are connected to the platform. These
    objects collect data from various related objects in a given context and analysis
    of the relevant data and are applied to solve a particular problem without the
    need for human-to-human or human-to-computer interaction [15]. Consider a scenario
    where an agriculture farm is watered every day in the morning at 7 am and then
    watering is stopped when the moisture level reaches a certain level. This works
    fine until something goes wrong like there is no water in the tank. If this is
    IoT enabled, then before 7 am, the water level measuring sensor in the tank provides
    the water quantity which then triggers the actuators. Now the water must be transported;
    hence, the system calculates the time to travel and time to fill the tanker and
    then we finally see that water is available before 7 am and can be utilized. 57.1.2
    Cloud Cloud computing which is simply referred to as “cloud” technology provides
    an on-demand computing service. The services range from application to storage
    of data on pay-and-use basis. The three major services provided by the cloud are
    (1) software as a service (SaaS), (2) infrastructure as a service (IaaS), and
    (3) platform as a service (PaaS). There are thousands of sensors available today
    which primarily work based on an IoT platform to support agriculture practice,
    each of these sensors generates a huge amount of data that can be considered as
    agriculture-based big data, which can be stored in a cloud for effective and efficient
    usage. 57.1.3 Machine Learning Machine learning is about programming the computer
    to learn from the given input automatically and improve through the experience
    without being explicitly programmed. It has been evolving consistently and has
    gained importance because of the ability to predict and also plays a crucial role
    in decision-making and real-time actions. Machine learning algorithms consume
    a large voluminous amount of data from which learning happens by considering different
    entities, drawing relationships, and correlating them. The learning is better
    when there is large and diverse data integrated from various sources. 57.2 Literature
    Survey The greenhouse technology has made a greater impact on agriculture worldwide,
    and the current technology trend of IoT has complemented it and has provided a
    new paradigm to agriculture practice. A research group has designed greenhouse
    agriculture with IoT. The main objective of the study is to create an ambiance
    by monitoring and controlling the essential parameters that are necessary for
    healthy crop yield. It is a three-layered architecture, which consists of perception,
    network, and application layer. These layers have wireless sensor nodes, Zigbee
    and gateway, respectively. The wireless sensors include temperature, pressure,
    light, humidity, and CO2 measuring devices equipped with CC2530 processing chip.
    The network layer uses Zigbee that collects the data from the WSN and forwards
    it to gateways which in turn sends to display devices or to smartphones using
    GPRS. The paper concludes by sharing the information that they were able to monitor
    and control the greenhouse more effectively and efficiently [2]. Regarding improving
    the farm productivity, Jayaraman et al. [3] define smart farming as an application
    of ICT, IoT, and big data analytics to solve the problems of electronic monitoring
    of crops and environmental-related properties like humidity, temperature, etc.
    The Australian multidisciplinary team has introduced an IoT platform SmartFarmNet
    for smart farm applications and is considered to be the largest system that gives
    very specific information about crop performance analysis and recommendations.
    The developed system uses four-layered architecture, and layer 1 provides the
    information about the feature of deploying any IoT device (sensors, smartphone,
    etc.) while collecting the primary raw data from sensor nodes and delivering it
    to the gateway nodes. In layer 2 the gateways help in processing the data near
    the source by improving communication bandwidth. The gateway uses Open IoT X-GSN
    component data ingestion. It then sends the data to layer 3 the cloud storage
    which can provide a huge amount of data generated through various devices and
    real-time data analysis is performed. It provides a query access performance latency
    that has the ability to store high-velocity data and at the same time provide
    sub-second query response. One such response evaluation has been discussed in
    Fig. 57.1. Layer 4 is a visualization platform. Hence this framework developed
    is expected to be flexible in terms of deploying new sensors, fast and high data
    rate collection of data as well as procession the query and responding and better
    visualization for decision-making [3]. Fig. 57.1 Query time multiple sensor stream
    with summarization (Source: [3]) Full size image Cambra et al. [4] demonstrate
    the potential of IoT and context-based information on precision agriculture. In
    this background, the research group defines a layered architecture called PLATEM
    PA that provides service to the farmers through multimedia communication. Layer
    1 consists of a WSN node, aerial sensors (drones), and irrigation controllers.
    Basically, the mesh network of sensors is created using an operating frequency
    band of 868 MHz LoRa or Sigfox using the drones. Layer 2 is the middleware and
    is responsible to create the best routes and identify the sensors and also gather
    data from them and further store it in the database. In layer 3 of the discussed
    system, the research group adapted SOA architecture in which data from sensors
    is collected, and it’s up to the farmers to get the required information for smart
    farming based on their needs. Therefore with respect to this, the server is identified
    with three main functionalities: (1) collecting data from the sensor, (2) applying
    context-based knowledge using drools governing based on Rete pattern matching
    algorithm, and (3) producing results in the form of multimedia. This setup was
    used to experimentally detect the potential fungal attack. Finally, for optimization
    purpose, the study also tested the bandwidth utilization at various phases as
    presented in Fig. 57.2 [4]. Fig. 57.2 Bandwidth consumption (Source: [4]) Full
    size image In a recent study, a research group developed an IoT model blended
    with machine learning to analyze agriculture data for smart farming. In the developed
    model, the study used the soil moisture sensors to measure the soil humidity and
    to auto control the functioning of a water sprinkler, solenoid valve to control
    the flow of water, DHT22 sensor to control the humidity of mushroom farm, and
    an ultrasonic sensor to control the water level in a chicken farm. A web application
    is developed to store and maintain the data obtained from these sensors. The stored
    data is further processed using the data mining approach to define the data format
    and to cleanse the data to get effective data, and then the study applied the
    knowledge extraction tools such as classifier, clustering, and association rules
    (Apriori algorithm) and processed the data effectively to correlate with the input
    parameters, i.e., temperature, humidity, and moisture. The study also used a linear
    regression model to assess the relationship between input parameters and output
    parameters of the lemon and vegetable cultivation agricultural practice. Study
    claimed that through the data modeling, the knowledge results showed that if the
    yield of vegetables is high (more than 4 kg/day) and lime cultivation is high
    (more than 6 kg/day), then the temperature lies between 29 °C and 32 °C, and the
    humidity will be between 72% and 81% [5]. From this, it is evident that the optimization
    of the essential factors and conditions of the agricultural practice applying
    ML and sensor-based IoT plays a pivotal role in reporting the enhanced crop yield.
    Araby et al. [6] developed precision-based agriculture smart system based on the
    integration between IoT and machine learning tool to predict the late blight disease
    in potatoes and tomatoes in Egypt, where 15% of the crop cultivation is potatoes.
    The prediction is done before the first occurrence of the disease based on the
    temperature, humidity, leaf wetness, and soil moisture conditions, and the developed
    technology will certainly save and reduce the costs of the agriculture practice.
    The technology utilized shall send the warning message to the farmer during the
    specific time of infection season. The farmer user interface system helps them
    to take precautionary measures and apply the protective pesticides in order to
    save the crop yield and reduce the usage of the needless pesticides. The design
    aspect of the project implementation has used cloud-based IoT platform architecture,
    and it has three layers of the system: the perception layer, network, and gateway
    layer, and application layer. The perception layer consists of air temperature,
    air moisture, and soil moisture sensors along with Node MicroController Unit (NodeMCU).
    The perception layer connects to the next layer, the gateway using a protocol
    called MQTT along with the Wi-Fi module. The gateway layer is implemented using
    R-Pi 3 microcontroller. The gateway layer ensures that all the captured data from
    various sensors is relayed to the cloud server for further analysis. The data
    set is a real data set retrieved from CALC and GDU (growing daily data) data source.
    SVM (support vector machine) learning algorithm is applied to find the disease
    severity (DS) based on which the action can be initiated. The application layer
    consists of a website designed that displays the results of the analysis and also
    the actual data [6]. In the context of IoT applications in precision agriculture,
    Grimblatt et al. [7] have intensely looked into effective parameters that are
    essential and also influence a plant growth such as soil and environment parameters.
    The soil parameters are moisture, nutrients, pH, temperature, and texture, and
    the environment parameters are light, temperature, and weather. The study concludes
    that the IoT can be used effectively in monitoring the agriculture practice, and
    the prototype designed in the study was still in its progressive development stage;
    this can be meticulously scaled up to the broader benefit to the farming activity
    [7]. A four-layer architecture called “my sense environment” was developed by
    Morais et al. [8] to enhance the crop yield productivity using precision agriculture.
    The major goal is to provide low-cost data acquisition support, use standardized
    communication protocols, and also provide better application interface for easy
    access to the data. Various types of sensors were deployed, and level 2 has gateways
    to collect data from sensors, level 3 edge computing (fogs) was installed to process
    the collected data locally, and at level 4, finally, the data is passed on to
    cloud for applying big data analytics, machine learning, and user interface systems.
    The software components include RPi operating system that drives a number of modules
    that are implemented using python which include (1) database module, (2) real-time
    alert module, (3) WSN manager, (4) local application, and (5) mySense agent. The
    database module stores data from every node in the environment in a table created
    for every device. The Real-Time Alert System (RTAS) module uses database data
    from all sensors, applies an algebraic expression, and can raise an alarm to find
    whether there is any possibility of disease because of a change in moisture or
    air temperature. The WSN manager is dealing with communication and is essential
    to exchange data between the SPGATE’18 and WSN nodes and basically does the job
    of pushing WSN nodes data into the local database. The primary aspect of this
    paper is to provide and ascertain that the data can be collected from different
    types of sensors and manage this huge data at low cost and use standardized communication
    protocols [8]. The data comparison in Table 57.1 is collected from Google Scholar
    papers having maximum citations in this domain and in that particular year. Table
    57.1 The technology used in developing precision agriculture over a period of
    10 years from 2010 to 2019 Full size table 57.3 Conclusion In smart/precision
    agriculture, the technology trio of IoT, cloud, and machine learning has been
    extensively used, and still it is in an infant state. The researchers have developed
    extensively IoT using various kinds of sensors and communication protocols to
    gather data from various plants. The generated raw data can be structured and
    stored in different forms using edge computing or fog or cloud computing. Finally,
    an analysis is performed on data collected, and decisions can be taken to improve
    the crop yield. This approach of increasing the agricultural crop yield can be
    the solution to meet the food needs of the future population. References Ramesh,
    D., Vishnu Vardhan, B.: Data mining techniques and applications to agricultural
    yield data. Int. J. Adv. Res. Comput. Commun. Eng. 2(9), 3477–3480 (2013) Google
    Scholar   Dan, L.I.U., Xin, C., Chongwei, H., Ji, L.: Intelligent agriculture
    greenhouse environment monitoring system based on IoT technology. In: International
    Conference on Intelligent Transportation, Big Data and Smart City, pp. 487–490.
    IEEE (2015) Google Scholar   Jayaraman, P., Yavari, A., Georgakopoulos, D., Morshed,
    A., Zaslavsky, A.: Internet of things platform for smart farming: experiences
    and lessons learnt. Sensors. 16(11), 1884 (2016) Article   Google Scholar   Cambra,
    C., Sendra, S., Lloret, J., Garcia, L.: An IoT service-oriented system for agriculture
    monitoring. In: International Conference on Communications, pp. 1–6. IEEE (2017)
    Google Scholar   Muangprathub, J., Boonnam, N., Kajornkasirat, S., Lekbangpong,
    N., Wanichsombat, A., Nillaor, P.: IoT and agriculture data analysis for smart
    farm. Comput. Electron. Agric. 156, 467–474 (2019) Article   Google Scholar   Araby,
    A.A., Abd Elhameed, M.M., Magdy, N.M., Abdelaal, N., Abd Allah, Y.T., Saeed Darweesh,
    M., Ali Fahim, M., Mostafa, H.: Smart IoT monitoring system for agriculture with
    predictive analysis. In: 8th International Conference on Modern Circuits and Systems
    Technologies (MOCAST), pp. 1–4. IEEE (2018) Google Scholar   Grimblatt, V., Ferré,
    G., Rivet, F., Jego, C., Vergara, N.: Precision agriculture for small to medium
    size farmers—an IoT approach. In: International Symposium on Circuits and Systems,
    pp. 1–5. IEEE (2019) Google Scholar   Morais, R., Silva, N., Mendes, J., Adão,
    T., Pádua, L., López-Riquelme, J.A., Peres, E.: mySense: a comprehensive data
    management environment to improve precision agriculture practices. Comput. Electron.
    Agric. 162, 882–894 (2019) Article   Google Scholar   Kassim, M.R.M., Mat, I.,
    Harun, A.N.: Wireless sensor network in precision agriculture application. In:
    International Conference on Computer, Information and Telecommunication Systems,
    pp. 1–5. IEEE (2014) Google Scholar   TongKe, F.: Smart agriculture based on cloud
    computing and IoT. J. Converg. Inf. Technol. 8(2), 1–7 (2013) Google Scholar   Li,
    S.: Application of the internet of things technology in precision agriculture
    irrigation systems. In: International Conference on Computer Science and Service
    System, pp. 1009–1013. IEEE (2012) Google Scholar   Bo, Y., Wang, H.: The application
    of cloud computing and the internet of things in agriculture and forestry. In:
    International Joint Conference on Service Sciences, pp. 168–172. IEEE (2011) Google
    Scholar   Zhao, J.Y.: The Study and Application of the loT Technology in Agriculture,
    pp. 462–465 (2011) Google Scholar   Grafton, R.Q., Daugbjerg, C., Qureshi, M.E.:
    Towards food security by 2050. Food Secur. 7(2), 179–183 (2015) Article   Google
    Scholar   Kelly, S.D.T., Suryadevara, N.K., Mukhopadhyay, S.C.: Towards the implementation
    of IoT for environmental condition monitoring in homes. IEEE Sensors J. 13(10),
    3846–3853 (2013) Article   Google Scholar   Download references Author information
    Authors and Affiliations School of Computing & Information Technology, REVA University,
    Bengaluru, India Y. Vishwanath Department of Biotechnology, School of Applied
    Science, REVA University, Bengaluru, India Raje Siddiraju Upendra School of Electronics
    & Communication, REVA University, Bengaluru, India Mohammed Riyaz Ahmed Corresponding
    author Correspondence to Y. Vishwanath . Editor information Editors and Affiliations
    Professor, ECE, Gnanamani College of Engineering and Technology, Namakkal, India
    Jennifer S. Raj Rights and permissions Reprints and permissions Copyright information
    © 2021 Springer Nature Switzerland AG About this paper Cite this paper Vishwanath,
    Y., Upendra, R.S., Ahmed, M.R. (2021). A Review on Advent of IoT, Cloud, and Machine
    Learning in Agriculture. In: Raj, J.S. (eds) International Conference on Mobile
    Computing and Sustainable Informatics . ICMCSI 2020. EAI/Springer Innovations
    in Communication and Computing. Springer, Cham. https://doi.org/10.1007/978-3-030-49795-8_57
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-030-49795-8_57
    Published 01 December 2020 Publisher Name Springer, Cham Print ISBN 978-3-030-49794-1
    Online ISBN 978-3-030-49795-8 eBook Packages Engineering Engineering (R0) Share
    this paper Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Sections Figures References Abstract
    Introduction Literature Survey Conclusion References Author information Editor
    information Rights and permissions Copyright information About this paper Publish
    with us Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: EAI/Springer Innovations in Communication and Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Review on Advent of IoT, Cloud, and Machine Learning in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bansal S.
  - Kumar D.
  citation_count: '130'
  description: In this era of research and technology, Internet of things (IoT) takes
    a prominent part in the evolution of applications of the various field like health,
    education, smart cities, homes, agriculture etc. This paper provides a survey
    of the IoT ecosystem. All the components of IoT and their significance has been
    elaborated. The smart sensors collaborate through wireless communication and internet,
    with zero human activity, to deliver automated intelligent applications. In this
    internet world, machine-to-machine (M2M) technologies are the first phase of the
    IoT. As IoT is expanding, it is bringing together vast technologies as in Big
    Data, Artificial Intelligent, Machine Learning to tackle the huge data and devices.
    This paper starts by providing an overview of the taxonomy of the IoT ecosystem.
    Then, it provides a technical overview of IoT enabling architectures, devices,
    gateways, operating systems (OS), middleware, platforms, data storage, security,
    communication protocols and interfaces for the data flow in an ecosystem. This
    paper also discusses the key hurdles that need to be tackled for expanding IoT.
    A relation between IoT and new technologies like big data, cloud and fog computing
    has been briefed. Finally, it presents the growing applications that IoT delivers.
  doi: 10.1007/s10776-020-00483-7
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home International Journal of Wireless
    Information Networks Article IoT Ecosystem: A Survey on Devices, Gateways, Operating
    Systems, Middleware and Communication Published: 13 February 2020 Volume 27, pages
    340–364, (2020) Cite this article Download PDF Access provided by University of
    Nebraska-Lincoln International Journal of Wireless Information Networks Aims and
    scope Submit manuscript Sharu Bansal & Dilip Kumar  4989 Accesses 115 Citations
    3 Altmetric Explore all metrics Abstract In this era of research and technology,
    Internet of things (IoT) takes a prominent part in the evolution of applications
    of the various field like health, education, smart cities, homes, agriculture
    etc. This paper provides a survey of the IoT ecosystem. All the components of
    IoT and their significance has been elaborated. The smart sensors collaborate
    through wireless communication and internet, with zero human activity, to deliver
    automated intelligent applications. In this internet world, machine-to-machine
    (M2M) technologies are the first phase of the IoT. As IoT is expanding, it is
    bringing together vast technologies as in Big Data, Artificial Intelligent, Machine
    Learning to tackle the huge data and devices. This paper starts by providing an
    overview of the taxonomy of the IoT ecosystem. Then, it provides a technical overview
    of IoT enabling architectures, devices, gateways, operating systems (OS), middleware,
    platforms, data storage, security, communication protocols and interfaces for
    the data flow in an ecosystem. This paper also discusses the key hurdles that
    need to be tackled for expanding IoT. A relation between IoT and new technologies
    like big data, cloud and fog computing has been briefed. Finally, it presents
    the growing applications that IoT delivers. Similar content being viewed by others
    RETRACTED ARTICLE: A Review and State of Art of Internet of Things (IoT) Article
    14 July 2021 Data Science and Analytics: An Overview from Data-Driven Smart Computing,
    Decision-Making and Applications Perspective Article 12 July 2021 The impact of
    5G on the evolution of intelligent automation and industry digitization Article
    21 February 2021 1 Introduction With the advancement of technology, people across
    the globe want to become more connected over the internet [1]. As technology is
    growing, IoT has become one of the most eminent technologies. In this technology,
    billions of devices such as sensors, actuators, gateways and controller are connected
    to one another in an efficient manner and provide the countless application in
    every domain [2, 3]. IoT is a paradigm where the real world is connected to the
    virtual world through the internet. IoT has wide scope in each trade of the world
    starting from engineering, medical, finance, food, energy and agriculture. New
    devices, OS, architectures, platforms, security and communication protocol are
    emerging in terms of IoT. As research is done in IoT, many new challenges are
    coming like massive connectivity, coverage, security, scalability. To achieve
    these goals that create problems a few years back now seems easy due to the interconnection
    of things. Both advancement and challenges are growing hand in hand. IoT adds
    a new taxonomy every day to its definition. It will take time to make IoT’s boundary
    stable [4]. IoT applications can be broadly categorized as consumer-based, industrial-based
    and infrastructure-based application. Some of the consumer-based applications
    are home automation, self-driven vehicles, smart wearable, automated healthcare,
    smart homes etc. Major industrial application is smart manufacturing process and
    control system, smart retail and supply chain, internet industry, industrial automation
    etc. Infrastructure based application are smart city, habitat monitoring, smart
    environment, smart grid etc. [5, 6]. IoT ecosystem is a system which brings together
    all the heterogeneous components of IoT in a managed way to build an efficient
    system. It is the integration of devices, operating system, controllers, gateways,
    middleware and platform. All these elements are connected through communication
    protocol and interfaces like Zigbee, low power Wi-Fi, Message Queuing Telemetry
    Transport (MQTT), Low Power Personal Area Network for IPv6 (6LoWPAN), Near Field
    Communication (NFC), Bluetooth low energy (BLE) etc. [7]. IoT ecosystem connects
    a large number of physical devices in a single system. These connected devices
    have been increasing exponentially. In 2020 the count will increase to 50–100
    billion according to various Information and Communication Technology (ICT) reports
    [8]. The block diagram in Fig. 1 represents the IoT ecosystem. The sensor and
    actuators communicate with the gateway through various communication protocols.
    The sensors which are also called preceptors gathers all the information in the
    form of environmental parameters and gives all data to the gateway. The actuator
    acts on the surroundings and receives instruction from the gateway. The gateway
    can manage hundreds to thousands of sensors and actuators. It also manages the
    flow of data between devices. It performs filtering and formatting on data which
    is received from the sensors. The controller handles hundreds of gateways and
    perform high-level processing of data like classification, computation on data
    and converted the data into meaningful information [9]. Then the controller communicates
    with middleware. The middleware performs the function like save all the data into
    the database server, analysis the data, generates the graphs and reports, performs
    security and privacy, controls and manages the whole system which is present below
    it, all the data can be supported by the cloud which is present in the middleware.
    All the application like smart city, smart homes need to consume services and
    analytical information provided by middleware. These services can consume through
    Application Programming Interface (API). Then the application gives the complete
    IoT view to the user [10]. Fig. 1 Block diagram of IoT ecosystem Full size image
    The motivation of this paper is to identify the IoT as an ecosystem by finding
    the diverse research present on IoT and fitting all the elements like devices,
    OS, middleware, communication, gateways in an ecosystem. This research has combined
    efforts in various sectors and has enabled all sectors to be connected. After
    recognizing the advancement in IoT, this paper explores what are the components
    that build and successfully enables IoT as an ecosystem. In past many surveys
    have been conducted for IoT. As compared to other IoT survey papers, the paper
    provides a detailed thorough summary of all elements that comes up together to
    enable the IoT ecosystem. This would help new researchers and developers to get
    a quick overview of how the different elements fit together to deliver functionalities
    without digging into RFCs and the standards specifications. It gives an overview
    of varied technologies that come together as a phenomenon in IoT, thus revolutionizing
    the world and solving challenges, in the form of real-world applications like
    smart city, smart health care etc. 1.1 Related Work In this survey, IoT has been
    reviewed from different aspects. The architectures of IoT gives the illustration
    that decouples layers and allows frameworks, infrastructures, protocols and technologies
    to fit in the ecosystem. This architecture also helps IoT systems to scale and
    interoperate with better abstraction and in a modularized way. There is a comparative
    analysis of various devices, OS, platforms, communication interfaces which enables
    the correct choice according to the requirement. After reviewing all recent studies,
    the major open issues in the IoT have been explained. The priority concerns for
    IoT applications in terms of security and privacy has been explained. The discussion
    on the significance and need of big data, cloud, fog computing in IoT as devices
    and data increasing has been done for real-time data analytics, performance, availability
    and mobility. In last this survey presented how these elements are chosen and
    bring together everything to deliver services of the IoT ecosystem. The survey
    on this paper is different from the existing surveys as it gives an idea on IoT
    as a complete ecosystem in terms of devices, gateways, OS, middleware, platforms
    and communication technologies. All these components work together to give an
    IoT application. A recent survey [18] cover IoT technologies and various application
    of IoT. Various design aspects of OS and challenges are discussed in paper [45].
    There is a survey on enabling technologies of IoT which also explains research
    issues on IoT [2]. The paper [16] gives the literature survey on IoT architectures,
    protocols and applications. The survey on communication technologies and protocols
    is discussed in papers [7, 24, 62]. An extensive study on the capabilities of
    IoT middleware [73] and components of platforms [80] is explained in recent surveys.
    But there is no comprehensive survey on complete IoT ecosystem which cover all
    IoT components and give various application as a complete IoT ecosystem. Table
    1 gives an overview of the comparison of existing surveys. Table 1 Comparative
    overview of existing reviews Full size table Relative to the existing literature
    on IoT survey, the paper contribution is summarized below: A survey has been shared
    on the key hurdles in the expansion of IoT and how IoT and emerging technologies
    help in tacking some problems. This survey paper compares and contrasts various
    IoT devices and OS which would help in understanding and choosing them based on
    the requirement. This paper also illustrates the available gateways, middleware,
    platforms and various communication technologies present at all layers of IoT.
    The paper contributes towards a survey on the significance of security and privacy
    in IoT and describes the linkage between IoT and new technologies like big data,
    cloud and fog computing. The paper is organized as follows: In Sect. 2 various
    architectures of IoT are presented. The paper represents the proposed taxonomy
    in Sect. 3. Then the elements for IoT ecosystem is presented in Sect. 4 in which
    IoT devices, IoT gateways, IoT operating system, IoT communication, middleware,
    IoT platform, data storage, security and privacy are explained in detail. Section
    5 explains the various challenges of IoT ecosystem. In Sect. 6, various applications
    in IoT Ecosystem can be explained and the conclusion is there in Sect. 7. 2 IoT
    Architecture The architecture is an outline that specifies major physical components,
    their functional orientation and the underlined principles [11]. Different researchers
    give different architectures based on the IoT application. The first and basic
    architecture of IoT is three-layer architecture. It basically consists of three
    layers-perception layer, network layer and application layer. Perception layer
    basically deals with sensing and actuation. The network layer is responsible for
    transmitting and processing the information. The application layer gives a specific
    application to the user [12]. In five-layer architecture, two more layers are
    there to give more abstraction to the IoT architecture. The five layers are perception,
    transport, processing, middleware and application layer. The transport layer is
    responsible to transfer the data for the processing which is obtained from the
    sensors. The processing layer processes the data which is obtained from the transport
    layer and then analyses that data [13]. In middleware architecture, middleware
    layer plays a big role. This layer is the heart and brain of the ecosystem. Middleware
    layer is not only responsible for managing the complete system but also controls
    the flow of data in the system. In this architecture perception layer, access
    layer and edge layer come under physical plane where sensor and actuators are
    present. Backbone Network Layer and Middleware layer are present in a virtualised
    plane which consists of clouds and servers. The Co-ordination and Application
    Layer is present in application plane which gives a final view of IoT to the user
    [14]. In service-oriented based architecture, the ecosystem functionalities are
    abstracted and exposed through interfaces. Objects and applications use these
    functionalities through services. The benefit of this architecture is that API
    doesn’t change even if the inner technology and code are changed. If new functionality
    is to be introduced, it simply used through a new service without impacting the
    existing system [15]. In fog-based architecture, four layers are present between
    the physical layer and transport layer which are monitoring, pre-processing, storage
    and security. The monitoring layer observes and checks the data obtained from
    the sensors. The pre-processing layer performs operations on the sensed data.
    The storage layer gathers all the processed data. The security layer is responsible
    for the integrity and privacy of the data [16]. All these architectures for IoT
    ecosystem are explained in Fig. 2. The detailed explanation of five-layered architecture
    is as follows [17,18,19]: Fig. 2 IoT architectures, a layered architecture—three
    and five-layer; b middleware based; c service oriented based; d fog based Full
    size image a. Perception layer—This is the layer which is present at ground level
    and mostly deals with sensors and actuators. Sensors in this layer senses and
    gathers information in the form of environmental parameters and physical parameters
    like X–Y–Z coordinates. Sensors transmit the gathered information further. Additionally,
    this layer performs the pre-processing on the data to remove unwanted data. Using
    this layer, a user could take action and sends a signal back to the actuators
    in the physical layer to perform actions accordingly. b. Transport layer—This
    layer is responsible to carry the pre-processed data to the processing layer and
    vice versa through the communication/networking channel. This layer is also responsible
    for node to node communication. It uses various communication protocol like Zigbee,
    Sigfox, BLE, Radio Frequency Identification (RFID) and NFC. c. Processing layer—This
    layer is responsible for filtering and formatting the data, organizing, managing
    and storing the data received from various sensors and nodes through communication
    protocols. d. Middleware layer—This layer is responsible to perform various logical
    and analytical operations on the stored data and using the facts and figures it
    process the data to meaningful information. This meaningful information is then
    to use to produce various analytics by use of communication interfaces. This layer
    makes use of platforms for databases, cloud computing, and big data processing.
    e. Application layer—Using the analytics and observation from the processed data,
    this layer is responsible to deliver an application to provide various services
    to the users by communication protocols like MQTT, Constrained Application Protocol
    (CoAP), Data Distribution Services (DDS) etc. There is a large number of applications
    in which the IoT as a technology can be used to give a complete ecosystem like
    smart city, smart buildings and smart healthcare. 3 IoT Taxonomy Taxonomy of IoT
    is a process of describing the way in which all devices or areas are impacted
    and related by putting them together in a group. This taxonomy would define most
    generalized layers that would always be part of the IoT ecosystem. A taxonomy
    for research in IoT has been proposed in the diagram based on the elements and
    architecture. At perception layer sensors and actuators are important IoT enabling
    devices which can be categorised as low-end, middle-end and high-end devices.
    Sensors collect data and actuators performs actions. Diverse sensors are available
    for location, motion, video and audio detection and capturing, light detection,
    proximation, sensing environmental parameters (temperature, pressure, humidity),
    chemical identification etc. are used in IoT applications. Sensors, actuators,
    motes are low-end devices with constrained OS, memory and battery. At the data
    pre-processing layer there are technologies with limited storage, a small processing
    unit and some security features to filter and summarize data before sending it
    on the middleware. Pre-processors are microprocessors and microcontrollers enabled
    with the scaled OS, higher memory and battery than low-end devices. The various
    OS with their capabilities used in the devices has been classified as low-end
    and high-end based on the resources. Various communication technologies like NFC,
    low-power technologies, WSN and Internet Protocols that are available starting
    from constrained to unconstrained features have been classified for IoT. Devices
    communicate over the wireless network using a diverse protocol. Short-range low
    power communication protocols like RFID and NFC and non-constrained Bluetooth,
    Zigbee, and Wi-Fi are few of the communication technologies that are briefed in
    this paper. This is like a constrained WSN that communicate over the network with
    various protocols and interfaces. A major component, middleware that acts as an
    abstraction layer for the programmer with a lot of complex interconnected systems
    which abstracts the functionality of the system are walked through. Middleware
    can be classified as service-based, actor-based and cloud-based. IoT platforms,
    storage and security are the integrated part of middleware. Cloud computing and
    big data analytics are major functions in middleware that manages data and enables
    complex computational capabilities to enhances interoperability, scalability,
    mobility, availability like features. Security and Privacy are the key components
    to the success of IoT and at the same time a great challenge in a remote location.
    IoT platforms are installed to work along middleware to achieve abstraction and
    program of the whole ecosystem. Application in terms of end-user apps, web and
    API that allows the exchange of data. These applications that have been possible
    because of IoT and have proved to be helpful as in smart home, smart health, smart
    traffic, smart cities, smart agriculture, smart grids etc. Therefore, a large
    number of algorithms and mechanisms have been used by the researchers and implemented
    for each layer of the OSI/TCP model. This section of the paper proposes the taxonomy
    of IoT starting from low-end devices to the IoT application featuring IoT devices,
    operating system, communication interfaces and networks, middleware, platform
    defining their capabilities that give a significant impact on end to end the flow
    of IoT ecosystem. The proposed taxonomy of IoT ecosystem is shown in Fig. 3. Fig.
    3 IoT taxonomy Full size image 4 Elements of IoT Ecosystem This section of the
    paper highlights the major components of the IoT ecosystem. The whole ecosystem
    is divided into six parts which are (i) IoT devices (ii) IoT gateways (iii) IoT
    operating system (iv) IoT communication (v) IoT middleware 4.1 IoT Devices IoT
    devices are the basic building block of the IoT system. These devices are present
    in all the layers of IoT architecture. IoT devices can be classified as open-source
    or proprietary. Open-source IoT devices are those which are publicly available.
    Everything regarding open-source devices can be studied, modified and reproduced
    according to the requirement whereas proprietary IoT devices are licensed and
    details of these devices are not available [20]. IoT devices have limited functionality
    due to low internal storage, memory, computational capability and limited power.
    IoT devices use different types of memory depending on the limited resources,
    cost and working of the device. Major types of RAM (Random Access Memory) are
    Static RAM (SRAM), Dynamic RAM (DRAM), Synchronous DRAM (SDRAM), Double Data Rate
    RAM (DDR), Embedded Multimedia Card (eMMC) etc. SRAM retains the data in static
    form until power is supplied. The memory speed is quite fast and is expensive.
    This type of memory is integrated with microcontrollers and is also used as L1/L2
    cache in the microprocessor. SDRAM is a type of dynamic memory. It stores the
    binary information in the form of electric charges. This type of RAM is directly
    synchronized with the clock of microprocessors and microcontrollers. This type
    of memory is used in devices where the processing of the number of instructions
    per seconds is high. DDR is an advanced SDRAM whose speed is almost double of
    SDRAM. The data is transferred in both the cycles of the clock in DDR as compared
    to SDRAM. The cost is high and it is used in devices where synchronisation with
    the controller clock is required along with higher data transfer rate. eMMC is
    a special type of memory that acts as internal storage which is integrated on
    eMMC chips along with controllers. It is used in smartphones, tablets etc. The
    IoT environment constitutes of various service architectures, protocols and network
    design to deal with millions of IoT devices for exchanging data. IoT should support
    heterogeneous devices based on variant interfaces with a large number of available
    resources. IoT devices can be broadly classified into three categories—Class 0,
    Class 1 and Class 2 which are shown in Fig. 4 [21, 22]. Fig. 4 Categorization
    of IoT devices Full size image Class 0 or low-end IoT devices are devices with
    limited resources like memory, power, computational capability, architecture etc.
    These devices are mostly present at first layer of IoT ecosystem. They have the
    functionality of sensing and actuating. They use lightweight communication protocols.
    They have very basic operating system. The RAM varies from 1 to 50 KB and flash
    memory varies from 10 to 50 KB. These devices are very much vulnerable to threats
    and security is the biggest concern in these low-end devices. These devices are
    closer to environmental parameters like temperature, humidity, pressure etc. Class
    1 or middle-end IoT devices are devices which have more resources as compared
    to low end devices. They provide more functionality than low end IoT devices but
    the computational capability is not enough to handle very complex requirements.
    These are basic microcontrollers. These devices sit over low-end devices in IoT
    ecosystem to manage and improvise the capabilities of low end IoT devices. These
    devices have capabilities like image processing, data filtering etc. These devices
    can have multiple communication technologies installed on them [23]. These devices
    have higher clock speed as compared to low-end devices where clock speed ranges
    from 100 MHz to 1.5 GHz. The RAM varies from 100 KB to 100 MB and flash memory
    varies from 10 KB to 100 MB. Due to more functionalities, these devices can be
    partially secured using encryption of data. Some of middle end IoT devices are
    Arduino, Netduino. They are also present either at first or second layer of IoT
    architecture. These devices have better peripheral interfaces like USB, microUSB.
    These can be considered as basic gateways in the IoT ecosystem [24, 25]. Class
    2 or high-end IoT devices are single-board computers that have a high number of
    resources in terms of CPU, RAM, flash memory etc. These devices support traditional
    operating system such as LINUX, UNIX [26]. These devices support growing technologies
    like artificial intelligence, machine learning, deep learning, nature language
    processing etc. These devices also provide a graphical user interface for development
    purpose. These devices are available up to 64-bit architectures. These devices
    have everything on a single board and support almost all communication protocols.
    The devices have many connectivity interfaces such as HDMI, Wi-Fi, Ethernet, USB,
    Bluetooth etc. and all are present on a single board. These devices support high
    graphics along with multimedia and data analytics. These are highly intelligent
    devices used as complex gateways and controllers in the IoT ecosystem. These have
    relatively less security concern due to the high number of resources [27]. All
    the devices can be compared in terms of platform, clock frequency, architecture,
    memory requirement, the operating system on which the devices run and battery
    required which is presented in Table 2 and the comparison in terms of communication
    networks and interfaces is shown in Table 3. Table 2 Comparison of devices in
    terms of platform, clock, memory, OS Full size table Table 3 Comparison of devices
    in terms of communication and interfaces Full size table 4.2 IoT Gateways IoT
    gateway is a middle-end device that acts as an intermediate between various sensing
    networks and high-end IoT devices or to the cloud platform through the internet.
    Its major task is to manage the heterogeneity due to various kind of data collected
    from different sensors and sends data to the higher controller or platform. The
    data collected by the gateway should be filtered and processed. High-level processing
    of data is done based on the requirement, kind of data and commands. Gateways
    act as a major bridge between different layers of IoT architecture that lies in
    different networks. Gateways manage itself and the terminal nodes attached to
    it in an efficient way to improve performance and form a robust layer in the IoT
    ecosystem. In terms of data transfer gateways act as a proxy between the data
    centers or cloud data source and the low-end IoT devices like sensors. Gateways
    sometimes act as a high-end device when the IoT ecosystem is very large and more
    often acts as a middle-end device when the IoT ecosystem had a small number of
    low-end-devices. Gateways are designed in a way that they are robust to environmental
    conditions in terms of the hardware. They also have the ability to overcome failure
    and tries to solve the communication gap between low-end devices and controllers
    [28]. Gateways also have a small operating system with limited functionalities
    and resources in terms of memory, computational capacity and power consumption.
    So, the gateway performs tiny operations like filtering data, converting data
    of different format into a unique format and should be able to handle the power
    break [29]. In case of power break, the gateway should have a small power backup
    to save system status into the permanent memory and put the system into hibernation
    mode. And when power is back, the gateway should start from the same state. Gateways
    should also have the ability to troubleshoot and reboot, in case any component
    is down and if still issue is not resolved then should be able to communicate
    to IoT platform for user-visible information [30, 31]. In large IoT ecosystem,
    gateways have a high-performance controller on top of it. So, the controller manages
    a large number of gateways. So, a gateway in this system supports following kind
    of communication (i) IoT low-end device to Gateway (ii) Gateway to IoT Platform
    (iii) Gateway to Controller (iv) Gateway to Gateway. Generally, gateways communicate
    through GPS, Wi-Fi, Bluetooth, Ethernet etc. In large IoT system, there exists
    local intranet at the gateway and controller level. Finally, data collected at
    this small intranet level is communicated to the higher device, platform or cloud
    through the internet [32]. Some gateways are designed in a way to support real-time
    device monitoring like auto identification, addition and removal of IoT devices.
    Moreover, it monitors the data and process limited data to produce relevant information
    to be passed onto the ecosystem. This leads to the supply of appropriate data
    to the operating system, database and other IoT devices to improve the real-time
    performance. Like CoAP protocol can be used in constrained IoT network to support
    the communication between the IoT devices [33]. IoT gateways works in three modes:
    Passive, Semi-Automated and Fully Automated [34]. i. Passive: In this mode, it
    is required to add new devices or delete the existing devices manually. The user
    gives permission to the gateway for adding and deletion of the devices. It can
    handle and optimize network operation. The gateway can access to each node or
    sensor. It also accesses each network without changing the communication protocol
    or source code. They are not flexible in nature and not customizable. ii. Semi-Automated:
    In this mode, there is a link between the devices which is added and the gateway
    for the connection. It improves the real-time performance, reliability in terms
    of transferring the data and manages the data in the network. It is based on a
    plug-configurable architecture which shows that the devices can be plugged according
    to the network requirement. It is more flexible than passive gateways because
    of the external interface [35]. iii. Fully Automated: In this mode, the devices
    have the capability for self-configuration. There is no need to take the permission,
    IoT gateways can add or delete the devices itself. This kind of gateways able
    to communicate with various communication protocol and interfaces like Wi-Fi,
    Zigbee, Bluetooth, CoAP etc. [36]. These gateways can easily work with heterogeneous
    networks. It also supports real-time device monitoring so that it can easily modify
    and review the devices by addition or deletion of devices [37]. 4.3 IoT OS OS
    is a set of programs that act as a bridge between application or user and the
    devices. Programs are designed and run over the called operating system. OS is
    installed over IoT devices to make it capable enough for programs to run on the
    IoT device and also manage the device. So, OS can be defined as a component what
    makes hardware a complete IoT device. So, it can be said that OS is responsible
    to manage and monitor power consumption, enable instructions to run, to program
    the IoT device for its attributes and enable devices for to and fro communication.
    OS plays a vital role in the deployment of a scalable and reliable IoT ecosystem.
    Due to the heterogeneous nature of devices in the IoT ecosystem, OS should be
    customizable to meet the requirement of each sub-IoT system. IoT devices have
    important characteristics are computation, capacity, energy capacity and memory
    capacity. In future, IoT devices should exist with these three properties as customizable.
    OS are best suitable if it supports the feature of controlling, connecting and
    communicating that would change with the change in properties of IoT devices [39].
    In addition, there is a requirement of standard development tools that could support
    portability of applications across various architectures and are easy to develop,
    extend and maintain. IoT OS can be classified as: a. High-End IoT OS—which mostly
    is linux based. This operating system runs over high end or middle-end IoT devices
    which are single-board computers with high resources, memory and power e.g. Raspberry
    pi [40]. b. Low-End IoT OS—which may be of linux based or non-linux based as shown
    in Fig. 5. This operating system runs over low and middle-end IoT devices which
    are small board with limited resources, computational power and capacity e.g.
    Arduino [41]. Fig. 5 Various types of IoT operating system Full size image OS
    play a major role in IoT for developing reliable, efficient, scalable and interoperable
    application. Both high and low end IoT devices require OS. OS is described based
    on many aspects that are architecture and kernel, programming model, scheduling,
    memory management, networking protocols, simulator, security, power consumption
    and support for multimedia. Following are key points kept in mind while designing
    an operating system. 1. Architecture—Architecture of an operating system composes
    of the kernel and specifies the services to the user [24]. Based on these parameters,
    architecture can be categorized as follows: a. Monolithic—Used for multilayer
    application system. These are capable of handling higher order complex computation
    and have higher processing speed thus a better throughput. All processes run in
    kernel space. b. Microkernel—This architecture provides only the major functionalities
    like scheduling, inter process communication and synchronization that run on kernel
    space. Other functionalities of OS run in threads. Here processes run both in
    kernel space and user space. They provide high flexibility in processing due to
    plugin availability and moreover allows an OS to be designed over the base system.
    c. VM architecture—This system is a virtual system over the actual running system.
    Due to virtualization, this architecture is slower but they provide a high level
    of portability, flexibility and extensibility. d. Modular architecture—This architecture
    enables adding and replacing components into the kernel at runtime dynamically.
    Each module represents a separate functionality. So, modules can be plugged in
    and out as per the requirement. e. Layered architecture—Multilayered architecture
    is designed for a specific requirement and is not flexible in nature when it comes
    to functionality. But this kind of operating system is easy to operate and handle.
    2. Development model—It defines the specification in which an application or program
    will be modelled. The factors that influences are multithreading, event-driven,
    concurrency handlers and memory design. Now it depends on the IoT device in which
    this OS will be installed so that they could make it flexible, extensible and
    achieving the purpose of the device [42]. OS should be programmed in a way that
    it could be redesigned and upgraded. A Software Development Kit (SDK) present
    in the OS helps in modelling libraries, improvise OS interface, memory model and
    power model. 3. Scheduling—The scheduling strategy is directly proportional to
    the capabilities of an OS. The scheduling algorithms are priority and non-priority
    schedulers accompanied by preemptive and non-preemptive nature. Preemptive performs
    the highest priority task stopping all running task whereas non-preemptive starts
    new task post completion of the current task [43]. 4. Memory management—It refers
    to memory allocation and deallocation. The memory is categorized based on the
    flexibility requirement. If memory is fixed it is static memory and when it is
    required to have a flexible nature then it is called dynamic memory. Moreover,
    it is a major concern to have a different level of memory based on memory speed
    and performance. Performance and memory form a major concern while designed an
    OS. In addition, another important memory for OS is cache memory that is small
    memory which very close to OS-level operations. This memory contains data and
    metadata required by OS [44]. 5. Interfaces and communication protocols—In IoT,
    OS is to be designed to provide maximum interfacing with the devices. Interfacing
    is possible through hardware interfaces and communication protocols. Now it depends
    on the kind of IoT device, location of installation, ecosystem requirement, speed
    and security of data transfer that helps in finalizing the protocols in OS. Various
    communication protocols are CoAP, Zigbee, Wi-Fi, Bluetooth and interfaces are
    like USB, microUSB, HDMI etc. which not only helps in inter-device communication
    but helps in connecting to other devices present in higher and lower hierarchy
    of IoT ecosystem or the internet. This also takes into consideration the heterogeneous
    nature of massive IoT ecosystem. 6. Simulation ability—Simulation is the process
    of predicting the performance in the real world. Simulators are external processes
    that plug into the OS to extend its functionality by analyzing its behavior [45].
    This helps in scalability and cost optimization in an existing system. 7. Security—When
    it comes to the IoT ecosystem, security is a major concern. OS at various level
    in this ecosystem itself needs to have a high level of security to make it a robust
    system. Security not only enables encryption of user data but also enables reliable
    communication in heterogenous devices present at different layers of this ecosystem.
    Security can be in terms of data restriction, user control to access the system
    in terms of authorization and authentication along with physical security in case
    of foreign attacks [46]. The device is not only concerned with security but also
    a recovery system is required that manages the device in case it is in unsafe
    hand to ensure the integrity and avoid loss of data. Another security concern
    comes into play at the communication level so as to avoid data leak during inter-device
    or device internet communication. According to resources, the different OS has
    security systems to overcome attacks. TinyOS has TinySec library to provide message
    authentication, integrity and confidentiality semantic security. Contiki has ContikiSec
    Transport Layer Security (TLS) in 3 modes authentication, confidentiality, integrity.
    RIOT has Cyber-Physical Ecosystem (CPS) to interact, monitor and control smart
    objects. LiteOS has support for the embedded chip to provide security. FreeRTOS
    as has WolfSSL for security, authentication, integrity, confidentiality. Mynewt
    has secured radio communication, secured bootloader and secure updates. uClinux
    uses shepherd process for security along with encrypted storage security. Raspbian
    supports authentication, authorization and encryption for multimedia. Android
    Thing uses verified secure boot and signed over-the-air updates. It provides full
    disk encryption. 8. Power management—This is one of the biggest constraints in
    the IoT ecosystem. IoT devices are constrained devices. They are installed at
    locations starting from the environment to the internet servers. Power management
    needs to be robust to conserve energy, regenerate energy, longing the battery
    life and mitigate physical attacks. It should also be capable enough to save the
    energy, to route power in a new direction in case a device goes down. OS should
    be powerful enough to handle power failure to save the current state. 9. Multimedia
    feature—OS should be designed based on the requirement of multimedia. Multimedia
    processes are power and memory intense. It requires highly complex operations.
    It supports audio, videos in terms of the virtual environment [47]. The various
    operating system which are linux and non-linux based can be compared according
    to design aspects in Table 4 and according to memory requirement and operational
    support is presented in Table 5. Table 4 Comparison of operating system in terms
    of design aspects Full size table Table 5 Comparison of Operating System in terms
    of Memory and Operational Support Full size table 4.4 IoT Communication Iot devices
    are resource-constrained. A large number of heterogeneous devices need to be connected
    through communication protocol and interfaces to the internet [48]. Due to resource-constrained
    nature, there are many communication challenges like the requirement of low power
    communication, less memory-intensive routing protocol, deploy enhanced Wi-Fi,
    addressing and identification and high speed [49]. If IoT devices connect through
    the internet directly, it would require IP stack. IP stack is highly power and
    memory intensive. So, IoT devices should be connected using low power technologies
    like Wi-Fi, Zigbee, Bluetooth, NFC, low power technologies, WSN. Below are some
    major communication technologies [50]. 1. Near field communication (NFC)—It enables
    at very short range (up to 10 cm) of wireless communication. NFC’s underlined
    technology is RFID and uses the magnetic field for data transfer. NFC communication
    is of two types—active and passive [51]. In active type, the magnetic field is
    generated by both communicated devices for data transfer and in passive, energy
    is optimized where one device generates a magnetic field and another device depends
    upon modulation for data transfer [52]. 2. Wireless Sensor Networks (WSN)—It is
    a network of hundreds and thousands of sensors connected through IP. The communication
    is based on IEEE 802.15.4. Inter-network communication may be direct or through
    multi-hop. WSN supports star, mesh and hybrid topology [53, 54]. 3. Internet Protocol
    Stack—Various protocol has been defined at the various layer of communication.
    At the physical layer, IEEE.802.15.4 supports low power and constrained resources.
    The energy consumed is very less in comparison to the existing Wi-Fi network.
    It features robust communication by detecting data loss and re-transfer of loss
    data. In pre-processing layer 6LoWPAN solve the large addressing problem. This
    protocol helps bring stability and scalability in large IoT ecosystem. In the
    network layer, RPL is present [55]. RPL creates a directed graph for all the devices
    in the network which defines how the data would be routed from one device to another
    device and to the gateway. In the application layer, CoAP and extended MQTT protocols
    are present for low resource devices. CoAP protocol has a feature like a resource
    detection, congestion control, data compression etc. [56, 57]. Extended MQTT features
    publish/subscribe message for lightweight communication. 4. Low power technologies—For
    IoT, many low power technologies have been developed to support constrained environment
    and resources. Some of them are Bluetooth low energy (BLE), low power Wi-Fi, Zigbee,
    Low Power Wide-Area Network (LPWAN) etc. The comparative analysis of all these
    technologies are explained in Table 6. BLE transfers a small packet of data at
    1 Mbps [58, 59]. In BLE, various peripheral devices are connected to a central
    device. The peripheral devices are by default is in sleep mode and wakes up when
    a packet is received from the central device. Low power Wi-Fi is an extension
    of old Wi-Fi and is based on IEEE 802.11ah [60]. It consumes low power and has
    a higher range of nearly 1 km. Zigbee is based on IEEE 802.15.4 and communicates
    in a range of 100 m. It provides multi-hop routing, maintenance of routes, identification
    of add/remove of IoT device. LPWAN is used for long-range communication at a low
    data transfer rate [61]. LoRA and NB-IoT (Narrowband-IoT) are LPWAN (Low Power
    Wide Area Network) communication technologies which work on constrained devices
    like IoT and requires low cost and low power usage [62]. Table 6 Comparison of
    low power technologies Full size table i. LoRA—As the name specifies ‘Long Range’.
    LoRA is a technology which operates on a frequency less than 1 GHz. It is given
    by LoRA Alliance in 2015. It is the first low-cost commercial communication technology.
    It acts as low power modulation technology for LoRAWAN. It uses CSS (chirp spread
    spectrum) modulation which is bidirectional in nature. The rate of data transmission
    lies between 300 bps and 50 kbps. The communication protocol which is used by
    LoRA is LoRAWAN [50]. LoRAWAN uses three classes of end devices for IoT applications.
    All these end devices perform two-way communication. Class A: End devices perform
    transmission according to the need of communication. These are the low power device.
    The window which receives the signal open randomly. Most of the time end devices
    are in the sleeping mode. Class B: Receiving windows is greater than Class A windows
    and it opens at the particular scheduled time. Class C: It has the maximum receiving
    windows which is almost open at all time [63]. ii. NB-IoT—It is the technology
    which is given by 3GPP (Third Generation Partnership Project). NB-IoT technology
    is a combination of GSM and LTE. The modulation technique used by NB-IoT technology
    is QPSK and BPSK. In this technology, there is three kind of operations- a stand-alone
    operation which uses only GSM frequencies, guard band operation which uses LTE
    carrier’s guard band, in-band operation which uses LTE carrier resource block
    [64]. It has better performance due to the fast modulation rate with high cost
    and high-power consumption. Apart from these IoT communication technologies, there
    are IoT interfaces that are used by IoT devices like GPIO, ADC, DAC, I2C, SPI,
    UART etc. [65, 66]. GPIO (General Purpose Input Output) performs direct low level
    I/O operations with processors. This is used as an input port for communication
    with CPU where when acting as an output port it can drive an output operation
    based on CPU instructions. It has features like flexibility, easy to implement
    and high portability [67]. Analog to digital convertor/digital to analog converter
    (ADC/DAC) acts as voltage converter where ADC converts analog voltage to digital
    and DAC converts digital to analog. Now it depends on the current signal form
    and the form of input and output signal required if ADC is required or DAC is
    required. But ADC helps in the transmission of signal with accuracy and DAC ensures
    high-speed transmission of the signal. Serial peripheral interface (SPI) [68,
    69]. This type of interfaces enables the serial exchange of data. SPI operates
    in the full-duplex mode which enables the two-way transfer of data at the same
    time. This is used in processing units with peripherals. SPI supports short-range
    communication. These types of interfaces are mostly used in a master–slave scenario
    between devices [50, 70]. Inter IC (I2C) is a bidirectional two-wire serial bus
    which enables the communication between integrated circuits. It is used in devices
    where each chip can become master for initiating the communication. It supports
    serial communication in master–slave scenarios. Universal asynchronous receiver
    transmitter (UART) is either an IC or part of IC for Asynchronous serial communication.
    Such interfaces are used for managing the serial ports. Here the speed of data
    flow is configurable [71, 72]. 4.5 IoT Middleware IoT middleware is a component
    of IoT ecosystem that provides the raw data to the user in form of web application.
    The information gathered from IoT devices must be manipulated in a form such that
    it can support all types of applications related to the IoT ecosystem. The above
    challenges are solved by the concept of IoT middleware. But it has limited functionality
    in terms of interpreting and integration of data [73]. This presents a big challenge
    in terms of inter-device communication, collecting and accessing the data, and
    merge the data of multiple devices to form the flexible composition. IoT middleware
    should be designed in such a way that it should be scalable, adaptable, flexible,
    secure and open source. This kind of middleware would enable researchers and developers
    to configure and compose new applications and also introduce new IoT devices into
    the ecosystem and would avoid low-level reprogramming of complete IoT ecosystem.
    IoT middleware could be broadly categorized into three types based on usability,
    flexibility and adaptive nature [74]. i. Service-Oriented Middleware: This kind
    of middleware enables end-users and developers to add and modify IoT devices into
    IoT ecosystem as services [75]. It provides services like access control, storage
    management, event processing engine. Security models are costly in terms of time
    and memory so they provide limited support in terms of security, privacy and trust.
    In such a system, the control of user data is limited. The security techniques
    in SOA architecture are not designed to support constrained resources [76]. ii.
    Cloud-Oriented Middleware: This middleware enables the collection and interpretation
    of data with ease. However, it limits the growth of the ecosystem in terms of
    types of IoT devices. The security model in cloud-oriented architecture is defined
    by the cloud which is being used [77]. So, privacy and security are defined by
    the cloud system and is not configurable by users. The major concerns in such
    a system are the control of sensitive data and such system are also not designed
    to support constrained resources [78]. iii. Actor-Oriented Middleware: This kind
    of middleware is open source and is designed on a plug and play model. IoT devices
    can be added to the IoT ecosystem as a plugin and when an IoT device is not required,
    it can be easily removed without impacting IoT ecosystem. The security model here
    is configurable by users through plug and play mechanism. Actor based architecture
    is designed to support constrained resources [79]. 4.5.1 IoT Platforms IoT platforms
    are the part of IoT middleware. IoT platforms are software’s specially designed
    to manage the complete IoT system. They are the backbone of the IoT system. IoT
    platform connects devices, gateways, networks to the cloud, server and applications.
    The platform helps the recognition of all devices. They provide a Software Development
    Kit for all IoT system devices [80]. This system enables virtual visualization
    of IoT devices from the perspective of managing, designing and enabling the IoT.
    They manage security. This is the place where rules of IoT ecosystem are defined
    for message evaluation. These platforms are used to define the business of the
    IoT system. They are like a bridge to the cloud and the hardware IoT ecosystem.
    The complete control of the ecosystem in terms of speed, cost, complexity, business,
    data flow is handled here [81]. IoT platforms are the one which enable the cross-device
    compatibility, automation of connected devices. In IoT architecture, IoT platforms
    may act as middleware which helps in versatility, flexibility, reliability and
    scalability of an IoT ecosystem. So, IoT platform as a middleware can be considered
    as a bridge between the hardware and the software. The major tasks are to identify
    devices and to understand heterogeneous inputs of different devices over various
    protocols and keep the system updated. It supports integration among the entirely
    different devices using its artificial intelligence to make the devices communicate
    and take very complex actions [82]. The platform can be described in form of layers.
    It is based on the features of IoT platform that adds to IoT ecosystem forming
    itself a backbone of the system. IoT platforms are responsible for identification,
    communication, computation and the responsiveness of the components [83]. i. Infra
    layer—This layer enables and manages the functioning of the IoT platform. This
    layer is responsible for messaging, device-intercommunication, orchestration and
    management of the ecosystem. ii. Communication layer—This is a layer which enables
    the communication between the hardware and the cloud system in IoT that enables
    data transfer to the data analytics processes. iii. Core layer—This enables the
    diverse and major features of the system which are data collection, device identification
    and management, disaster recovery, system configuration, system software update.
    iv. Visualization, Reporting and Processing layer—This is the layer majorly related
    to processing activity, generating reports and visualizing the analytical outcomes
    of big data that are user understandable. Moreover, this layer enables the user
    to define rules for the way data is processed, report and visualizations are generated.
    The IoT platform connects the devices like gateways, network to the cloud or with
    the application. The whole system is controlled by the central management system
    that acts as the processing unit [84]. IoT consists of devices, sensors, software
    and networks that simultaneously work together and use the IoT platforms to implement
    the application. The factors which enable the requirement of IoT platform are
    network connectivity, optimizing the operation, managing the data, analytics,
    static processing and connection to the application. The features that an IoT
    platform requires stability in terms of portability of carrying the operation
    and availability, flexibility in terms of performing any task according to the
    application, scalable in terms of work with any kind of devices, and affordable
    [85]. The major concerns and research area in IoT platforms are scalability, customizability
    and security. It enables load balancing, flexibility, encryption, enabling comprehensive
    identity, flexible development and manage data and user accessibility [86]. There
    are many platforms which support IoT applications which are shown in Table 7.
    Table 7 Comparison of platforms Full size table 4.5.2 Data Storage Storing the
    data in various form is the one of the abilities of middleware. The data traffic
    volume in IoT is huge and thus require new calibration and analytical techniques
    to support big data. Data mining techniques as in AI, ML and decision-making algorithms
    enable the complex computational processes of useful information for increasing
    raw data of IoT [87]. Cloud technology can manage and meet the complex requirement.
    Cloud has capabilities as in storage and complex computation, support heterogeneous
    application protocols. But still, there are road blockers to be emphasized. Transferring
    edge devices (e.g. sensors, smartphones, etc.) data to the cloud causes issues
    viz. network performances (in terms of delays, bandwidth, congestion, reliability,
    availability, etc.), costs of internet, security and storage cost on cloud, secure
    transmission and privacy, different cloud interoperability, device-cloud security
    and reliability [88]. i. Big Data—Bulk data is generated as a result of interconnection
    between the sensors and a large number of objects. Data generated by IoT devices
    is increasing rapidly. This massive data is called ‘big data’ which represents
    the size of the data set. It is a process of searching and analyse the data and
    finally give the patterns and correlation of the data. Four fundamentals of big
    data are storage, data security issues, big data analysis, impact on day to day
    living. There are various sizes of data and various types of data like numerical,
    text, audio, video, figures etc. Data can be of structured, unstructured or semi-structured.
    Big data is the only way to make the data in a structured form. There is a large
    amount of data even after filtering the redundant data [89]. It is very necessary
    to manage this kind of data. Bigdata technique has the ability to handle a large
    amount of data and deliver that data to the analytics tool. Then this data is
    transfer to the data centre. Big data storage faces a lot of security issues like
    secure transactions, secure data mining, secure filtering and secure computations.
    Big data analytics categorized into three factors volume, variety and velocity.
    Volume deals with the size of data streams. Variety of bigdata means diversity
    of data in the form of audio, video, text etc. Velocity represents the pace at
    which the data can be processed. There is a large number of big data and IoT applications
    which has a direct impact on our daily life like smartwatches which continuously
    measure the health of a person by collecting the data and give the message to
    the doctor if a person falls sick or faces other problems. Big data analytics
    examines a large amount of data to give relevant and efficient decisions [90].
    The combination of IoT and big data is used to improve decision making. The data
    obtained from the ‘connected devices’ is in a large amount. So, there is a need
    for big data analytics which manages the data by using reports, query, training
    data and analytical tools. To perform analytical and logical operations the data
    must be stored in a smart and efficient way. Single processor and limited storage
    are not capable to perform an action on big data. Big data platforms for analytics
    like Apache Hadoop are not sufficient to support the huge data of big IoT system.
    The processing of real-time data in IoT is also a necessity for efficiency. IoT
    needs a common huge analytical platform that delivers to all IoT applications.
    Such analytical unit shouldn’t be an overhead on the overall IoT ecosystem. A
    solution for IoT big data is to keep track of just required and important data
    only. ii. Cloud Computing—Cloud computing is an evolving technological model to
    tackle the on-demand network of shared resources and has ability to configure/manage
    sources such as networks, servers, applications, and services. Cloud computing
    enables remote usage and maintenance of resources in a reliable and cost optimized
    way. Cloud’s storage and computational resources enables IoT to manage, store
    and process this huge data [91]. Some of the major used cloud for IoT are ThingWorx,
    OpenIoT, Google Cloud, Amazon, GENI, etc. [87]. Cloud computing manages big data
    enabling the processing of data and extracting valuable information. Deploying
    cloud computing in IoT is a challenging task. Synchronizing and Standardizing
    heterogenous cloud vendors for real-time services due to interoperability. A balance
    between cloud environments and IoT requirements due to the infrastructure results
    in a challenge. Security is a big concern for IoT cloud due to the different security
    mechanisms involved in all layers of IoT. Cloud services need to be validated
    to ensure quality services. Cloud computing is not a necessity in local infra
    resources as data is limited [92]. The processing and operational cost would grow
    if the cloud is induced. Raw data should be processed on local nodes to reduce
    the reduce internet transferred data thus reducing congestion, latency, costs.
    This would also optimize performances [93]. Cloud here can be replaced with new
    computing techniques as in cloudlet and fog computing that is designed for the
    need for local infrastructure and can act as an extension to or a bridge to cloud
    computing. iii. Fog Computing—Fog is a synonym for “mobile” cloud. Fog Computing
    is also known as cloudlets or edge computing. It comes into play for large-scale
    cloud computing and storage. This layer acts as a bridge between smart devices
    layer and cloud storage layer. It enables extending of cloud computing services
    to the edge devices. Fog computing availability is high to end-users as compared
    to the cloud and hence the performance is improved in terms of delivery, delay
    and availability. It can be referred to as small scale cloud. The fog computing
    overcomes the problems present in cloud computing like mobility, reliability,
    availability etc. The fog is a mini cloud that is close to the ground. It provides
    limited capabilities viz. data storage, processing, filtering, and analysing at
    the edge of the network and is then sent to cloud through expensive communication.
    Fog computing features are low latency, location awareness, distributed nodes,
    mobility, real-time response, interactivity with the cloud. The fog and cloud
    go together in larger IoT and helps achieve optimal performance. Smart gateway
    is employed between underlying networks and the cloud to enable fog computing
    [54]. Fog computing enables smart devices for data pre-processing and improving
    availability. This is not a must due to limited capabilities for complex analysis
    and huge storage. Some basic computations are done at the edge layer. To overcome
    limited resource at local layer pre-processed data is sent to Cloud. But this
    is a solution to the mobility challenges of a cloud. At gateway level fog enables
    smoother data flows. 4.5.3 Security IoT is a real-world network with a real-time
    system. There are major operations without human intervention for large durations
    through wireless communications. Although it results in improved social efficiency,
    it creates a list of new concerns leading to privacy and information security
    breach. Security is the degree of the special considerations to protect the individual’s
    information from exposure to IoT environment, in which almost any physical or
    logical entity can be given a unique identification and the autonomous ability
    to communicate over all kinds of network bidirectionally. Middleware provides
    the security and privacy to the whole ecosystem. The network then broadcasts all
    the gathered information and events to the server for the logical and analytical
    process. The privacy should not be compromised and present at all stages like
    the device, the communication, storing, processing and the applications. The user’s
    privacy and protection of data are one of the largest challenges to be addressed
    in the IoT. Security in the device—There should be no unauthorized data manipulation,
    hardware and software handling for the devices. Sensing devices gather sensitive
    data which should be robust and tamper resistance. Trusted computing technologies
    should be used for device validations, and authorized environments. For devices
    privacy, location privacy of device, identification, protecting from theft or
    loss and protection from side-channel attacks. Barcodes can help in Personal Identifiable
    Information (PII). Blind values calculations could be used for side-channel attacks.
    Security in communication—is achieved through encryption. Encryption can be done
    for the sequence number, IPsec- Security Parameter Index, etc. Securing Communication
    Protocol helps in achieving privacy. Pseudonyms can be an alternative to encryption
    in case the device’s identity is not feasible to reduce vulnerability. Devices
    communication should occur if and only if needed. Security in storage—Only the
    required information should be stored. Mandatory PI should be retained. Information
    browsing should be on the basis of “need-to-know”. Pseudonymization and Anonymization
    could be used for privacy. Statistical data is only made accessible. Differential
    privacy technique could be used. Security in processing data—Personal data must
    be treated as per the intended purpose. Personal data should be disclosed or retained
    to third parties on the consent of the owner. Digital Rights Management systems
    are suitable to control data and defend illegal re-distribution. Trusted and secure
    devices are used by DRM. User’s permit and awareness for distribution of PI data
    are necessary. Devices privacy concerns are raised due to inefficient authentication
    and authorization, no encryption in transport, loopholes in the security of web
    interface, insecure software and firmware, etc. [94]. The concerns of security
    are different in each layer of IoT. In the perception layer, the data flow is
    through built-in sensors. Then data transmission is using modules or machine to
    machine (M2M) device resulting in networking services of multiple sensors. Here
    security of machines is involved in consideration of implementation and node connectivity.
    Perception nodes are distributed but are not monitored. Access to these devices
    is quite easy for others. This could lead to damage or illegal actions on these
    nodes. As analysed and categorized these threats are unauthorized access, the
    Internet and denial of service attack. In the network layer, the network provides
    comprehensive capability to interconnect, effectual and economical connection,
    along with the authentic quality of service in IoTs. Excess of machines sending
    data to a large number of IoT nodes could lead to congestion and thus denial of
    service attacks. In the back-end layer, the gateway, middleware, require effective
    and efficient security. The data gathering, examining real-time sensor data and
    increasing business intelligence are highly critical. The security of IoT system
    should have seven major controls all the time viz; privacy, accessibility, authentication,
    communication layer security, data integrity, data confidentiality and availability.
    In IoT systems, reliability and secured communication protocols are required at
    all layers [95]. Security risk in IoT system is a concern due to the use of wireless
    communications technologies and due to open system, accessibility to components
    like sensors. There should be mechanisms to detect external attacking activities
    and self-healing. Encryption is a key element for information security but encrypting
    big volume of real-time data is the big challenge. Efficient encryption algorithm
    should be used. Implementing complex schemes for securing the environment face
    a challenge of constrained energy. Beside mechanisms, appropriate policies are
    required to protect the privacy and make sure all users are comfortable using
    these IoT solutions. 5 IoT Challenges As the IoT ecosystem grows, more requirements
    of memory, power, computational capability and constrained resources are needed.
    The major challenges in key areas of IoT ecosystem in terms of security, scalability,
    heterogeneity and interoperability are required to implement at different layers
    of the ecosystem [96, 97]. Exchanging and managing personal data is a big threat
    to the IoT ecosystem. IoT quality is directly proportional to resolving future
    complexities and the challenges. IoT vision cannot be analysed due to critical
    challenges. A few of the challenges could be discussed as follow: 1. Heterogeneity—To
    create a large IoT ecosystem there is a need of thousands to millions of devices
    and these devices are completely different from each other. All IoT devices are
    heterogeneous in nature based on the input and output format of information and
    the communication protocols. For all these heterogeneous devices to work together
    it is important that the devices understand the information which is captured
    or transferred and be able to compute, create meaningful data after analysis and
    observations on obtained data [98]. The heterogeneity is also present in the communication
    protocol, communication format, the complexity of computation etc. So, inter-device
    communication, interoperable devices and analysis on the data from all heterogeneous
    device to make reliable IoT ecosystem is a big challenge (Fig. 6). Fig. 6 IoT
    challenges Full size image 2. Security and Privacy—When it comes to heterogeneity
    and a scalable system, security is always a major concern. IoT devices record
    huge data about people daily life that collectively could produce personal information
    when analysed in depth so security is a big concern when it comes to heterogeneous
    devices and devices inter-communication So, encryption of data and protecting
    movement of data in a secure manner is a big challenge in a large ecosystem [99,
    100]. A problem in IoT security is the lack of standards. Privacy is the profile
    access in IoT network. Exchanges of data securely is a necessity without compromising
    privacy. All devices communication required transparent and secured access. An
    effort and standardization are necessary to mature the approach of access control
    in the application. Research is going on to design security and privacy in all
    IoT components. There are many open issues as follow: i. Lightweight security
    on constrained resources—IoT ecosystem operates on constrained resources such
    as low energy, small memory and low-end processors. These directly impact the
    security and privacy of the ecosystem. So, it is difficult to design lightweight
    security for constrained resources. ii. Denial of service attack—As IoT ecosystem
    is with limited resources, the useless calls from a hacking system could exhaust
    memory, processor and can bring down the ecosystem. These calls may be occurring
    at all the layers of the ecosystem. iii. End to end security—Although small securities
    are being implemented at all the layers but when we take the complete IoT ecosystem,
    there are many loopholes. Issues like inter-device communication due to different
    protocols, data format support by middleware for heterogeneous devices, securing
    all communication protocols, information flow at various level, classification
    of information for privacy when data is very large are big challenges for an end
    to end security. 3. Scalability—is the ability to add devices, services and functions
    without impacting the performance of the existing system. Diverse heterogeneous
    hardware and communications is a challenge. Designing to enable extensible services
    and operations is required. Interoperability and scalability go hand in hand in
    IoT environment to deliver scalable services. Scalable mechanisms should support
    new device registration, look-up and discovery and interoperability between objects.
    For a large ecosystem containing thousands of devices is a big challenge in terms
    of power, recovery, data flow, security, managing the network, processing data
    etc. These all impact the IoT system when the IoT devices number grow [91]. 4.
    Interoperability—is another challenge for IoT to handle heterogeneous things across
    platforms. Interoperability needs to consider for application developers and device
    manufacturers. Adding new functionality with zero or negligible impact on the
    integrity of the existing system is a significant criterion in designing IoT.
    To process data to form a fruitful analysis, the IoT devices not only themselves
    have to be intelligent but need to operate along with the neighboring devices.
    They need to understand the format of data and also need to work based on another
    device analysis. So, this is also a great challenge to the IoT ecosystem with
    millions of devices. 5. Management and Architecture—billions or trillions of devices
    causes issues in terms of fault, configuration, accounting, performance, architecture
    and security. This management effort incurs extra cost. Managing is a major factor
    in growing IoT. A robust platform to facilitate the management of IoT assets in
    real-time remotely is required. Maintaining compatibility across all IoT layers
    is required to enhance connectivity and ensure delivery. The architecture plays
    a role in the IoT ecosystem where the same device could become a recovery device
    at one time and at other time is used for measuring a physical property. IoT architecture
    should be robust enough to support the ecosystem in terms of complexity, flexibility,
    artificial intelligence, smartness, device count and heterogeneous nature of IoT
    devices. To cope with these problems in creating a flexible ecosystem architecture
    plays a unique role [101]. 6. Availability—in terms of hardware and software to
    achieve anytime and anywhere services for users is a challenge. This is referred
    to as devices or software compatibility with the IoT functionalities and protocols.
    There are studies and research to assess and evaluate the availability of IoT
    applications at the design stage to maximize availability. 7. Reliability—ensures
    the proper working of the system and aims towards the success of service delivery.
    Availability and reliability complement each other towards success. In an application
    that works on emergency response achieved through efficient communication, reliability
    is critical. Reliability should be present at all layers of IoT. 8. Mobility—Most
    services are to be continuously delivered to mobile users. Transfer of device
    causes service interruption as communication transfers one gateway to another.
    Tunnelling supports service continuity resulting in data accessibility while resource
    unavailability. Distributed service also enables mobility. Performance depends
    on many components’ performance. Continuous improvement in services is necessary
    for performance optimization. IoT devices are monitored and evaluated for best
    performance. Performance of the IoT is evaluated on processing and communication
    speed, device form factor, and cost. All protocols and technologies contribute
    to the performance factor and QoS. Due to the IoT revolution, the number of IoT
    devices and functionality of each device is expanding which requires an intelligent
    controller to monitor several inputs from the devices. Creating an OS that satisfies
    all the requirements of all low- and high-end devices are nearly impossible to
    achieve but there is a choice of open-source OS because of detailed information
    about the functionality and requirement which could help to merge simulator to
    extend functionality. Developing applications that are platform-independent and
    to create a middleware in a system and cross interoperability is one of the solutions
    for IoT challenges. 6 Applications in IoT Ecosystem A diverse set of intelligent
    application has been developed. Many of them are readily available and there is
    still a big scope of research in these applications thus enhancing the quality
    of life like smart home, fitness tracker, smart health monitor, smart city etc.
    i. Smart Home—The popularity of this IoT application is that it is growing globally
    due to improving the comfort of life. The smart devices as in sensors and actuator
    along with wireless networks are maturing at a very good pace. This is not only
    improving life’s quality but also providing better security in homes. Sensors
    are deployed for automation and intelligence services that helps to automate the
    day to day activities. On one end there is a cost involved in the installation
    and operation of smart devices but in the long run, they help in energy conservation.
    A few examples are turning lights off and regularizing electronic appliances.
    The sensors collect environmental data like light, temperature, humidity, gas,
    and fire events. This data from sensors is sent to middleware services and accordingly
    takes actions through actuators [102]. For example, when a human is in home or
    arriving home, the application can automatically turn on the AC. From a security
    point in case of a gas leak, home electricity is cut down and an alarm is sent
    to the owner. The smart home can keep a track of the health of family members
    starting from children to old people and alarms for emergencies. Smartphone applications
    are available, which detect emergency, alarm and handle home security and functionalities.
    But there is security and privacy concern as data being captured is floating on
    the internet. An intruder may attack the insecure system and system could fail.
    ii. Smart Cities—It involves sub smart applications like smart transport, smart
    parking, smart traffic fines, smart driving, smart water management etc. Smart
    traffic manages daily traffic using sensors and AI-enabled middleware systems.
    They track the events and activities leading to traffic congestion and take actions
    to ensure smooth traffic. It also enables smart parking; this helps to avoid accidents
    by the proper routing of traffic. Improper and illegal driving is identified and
    automate fines are sent to their register mobile. Vehicles are uniquely identified
    and the cloud is used here to manage the huge data and observe useful information.
    Traffic patterns are observed and future controls are implemented. All these applications
    are connected to smartphones and keep you posted with the update. The congestion
    is also captured by their accelerometer and gyroscope of mobile for sending your
    movement to middleware and GPS is tracked through google maps and thus analysing
    the congestion. At the same time applications can educate drivers on wrong practice
    taken on the road. IoT applications are available to help drivers in becoming
    safer drivers. Sensors are being installed in smart cars to capture driver behaviour
    like drowsy or sleepy and also detect alcohol consumed by the driver. Such sensors
    avoid starting of cars. Sensors like face detection, eye movement detection, alcohol
    detectors and pressure detectors are installed on steering for smart driving.
    Smart parking enables hassle-free parking through the Internet to find an empty
    parking lot. Smart traffic also gives priority to the ambulance by providing a
    clear path by managing traffic lights. Smart water management manages water resources
    efficiently. The water supply lines are managed accordingly and provide an auto-cut
    option. There are also storm drains available in the city. The leaks are identified
    by water inflow and outflow quantity. Similarly, sensors are installed to track
    storm drains, water storage tanks, and water supply lines and helps in the centralized
    water planning strategy [103]. iii. Smart healthcare—It is the system to monitor
    the health condition of individuals, old people, small children and patients etc.
    through many wearable sensors. They continuously monitor an individual health
    condition. In case of emergency or abnormal sensing, the concerned person is alarmed.
    For minor problems, patients are indicated for health check-ups. Smart health
    devices are regularly used for recording allergies, measuring blood sugar, blood
    pressure and stress recognition. Health care systems keep monitoring GPS location
    during the whole day physical activity, also the sleep and rest information [104].
    Fitness sensors keep a monitor on fitness based on our daily activity level measuring
    the number of steps taken and the exercise done. Pressure sensors, accelerometer,
    gyroscope etc. analyse the patterns of the workout and informs for better options.
    The long health record can help the doctor in case of an emergency and take quick
    actions. The various sub-system in smart healthcare are: Patient Monitoring System
    This use-case targets collecting vital sign of patients and sending them to connected
    nursing stations. Assuming private rooms for patients, light and door sensors
    are deployed to monitor patient’s activity and identification of depression patients.
    Relevant Smart sensors can be used. ZigBee or Z-wave are used for communication
    protocols are used to collect and transfer collected data to the nursing stations.
    Using sensors along with microcontroller or microprocessor helps in providing
    efficient integration with hardware and software. These can use Wi-Fi or IEEE
    802.15.4 to communicate. The sensors, microcontroller or microprocessor act as
    clients to the servers installed at nursing stations. For inter-sensor collaboration,
    routing protocol like RPL is used for enabling the multi-hop communication. Doctors
    can remotely access the processed data at servers by mobile application [105].
    Monitoring and improving Eating Disorders To improve eating habits or eating without
    spilling food, a glove equipped with tiny vibrating MEMS motors could be used.
    These counteract unstable hand movement measured by accelerometer like sensors.
    The accelerometer and vibrating motors communicate with a minimum delay to achieve
    the required functionality. The DDS protocol is chosen for this scenario for quick
    direct communication. The collected information is collected and sent to the nursing
    stations [106]. A proxy is required here to communicate between DDS and the gateway
    for translating and sending it to the station. The eating habits which analysed
    can be shared to the patients over the mobile app as a tutorial (Fig. 7). Fig.
    7 Smart healthcare system Full size image Navigation System for Visually Impaired
    People The application is required to provide a real-time location to the users.
    User held stick could have sensors as in proximity sensor, accelerometer, a real-time
    location sensor and other sensors. These would be connected to a local server
    as well. These would estimate the current location of the user. The local server
    overlays the position on a floorplan and is connected to the Internet. This can
    guide a user with navigation information allowing them to avoid obstacles and
    movement constraints earlier captured by other users on the system [107, 108].
    Sensors in Organ IoT health care still has a big scope of research. It will not
    only affect people life but also boom the healthcare industry. Various applications
    in the medical field are there. UV sensors monitor the level of radiation areas
    wise and notify about the high radiation areas to people. Freezer sensors monitor
    the observational objects and adjust the required temperature accordingly [109].
    The activity of disabled or independently living old age people are observed through
    the installed sensors. Medical sensors monitor the sportsman activity and keep
    the doctors notified on the medical health. Sensors for organ are microdevices
    that are implanted in organs. They precisely replicate organ-level functions by
    the development of the primary function parts and provide real-time measurements
    of the organ and their endurance. These devices are successful even in the field
    of drug testing as well as defence research projects. Digital and 3D Printed Pills
    are the latest medical application that is revolutionizing healthcare. These built-in
    chips implanted in pill records the effects of a drug on individuals and transmits
    the data which are analysed. Some wearable sensors pills sense your body and dose
    your body as required. Smart Wheelchairs eliminate the dependency on handicap
    and older people. The aim is to reduce the effort required to navigate the wheelchair.
    The intelligence used with the collaboration of sensors, AI and IoT which helps
    in ease the usability and interactivity. Smart Wearables are the healthcare monitoring
    system in integration with IoT. These constantly monitors the real-time well-being
    of individuals. The data is sent to the servers which can be analysed by the doctor
    in the annual body check-up or in case of an emergency [110]. iv. Smart Agriculture—Environmental
    parameters such as temperature, humidity, soil concentration, the water level
    is measured through agricultural sensors and keep farmers notified about the condition.
    Adequate manual and automated steps are taken by smart agriculture actuators that
    results in high-quality production. Automated irrigation depending on weather
    conditions, greenhouses are some of the implementations of smart agriculture [111].
    The information is gathered by sensors and sent to middleware through gateways.
    The analytical operations are performed and the required actions result in yield
    quality. Pesticide presence is analysed and the land under the strong influence
    is guided not to be used and that part of the land is improved before using. v.
    Smart Air pollution controlling—Industries, Vehicles, fire etc. cause a lot of
    air pollution. Air pollution monitors have been installed in various part of the
    city through electrochemical toxic gas sensors. Vehicles causing pollution are
    identified by RFID readers placed on roads. Smart traffic helps to avoid traffic
    congestion and thus avoid air pollution. vi. Smart Logistics—In supply chain real-time
    tracking of goods displacement are done using sensor technologies like RFID and
    NFC. Handling information can be identified using the smart sensors and RFID tags
    on the product. In case of loss and damage of the product, an analysis report
    could help to highlight the lag in handling and thus adequate actions are taken
    in future that improves the performance of supply chain systems in long run. viii.
    Smart Energy Conservation—The smart grid is technology-enabled electricity management
    system that optimizes generation, transmission, distribution, and consumption
    by adding AI at each step thus resulting in two-way power flow (between consumer
    and supplier). This saves a lot of energy and controls power flow and also provides
    dynamic pricing to consumers. The sensors are deployed to monitor and enables
    the actuator to switch the supply and surplus energy back to the central grid.
    Two-way power flow benefits consumers that have energy generation systems installed
    in the building like solar or wind power. The surplus power is transmitted and
    is reused. This smart system also keeps a monitor on the health of transmission
    lines for disaster prevention and efficient use. In case of emergency, the alarms
    ensure maintainability, availability and also avoid accidents. Smart meters analyse
    consumption patterns and suggest for optimal options and the user are benefitted
    by these smart applications [112]. 7 Conclusion The rapid and emerging paradigm
    of connecting billions of physical objects, empowering human interaction and quality
    of life both physically and virtually thus make everything automatic around us
    is named as IoT. The proliferation of the IoT is dramatically increasing and already
    covers many prominent domains and disciplines. In addition to existing M2M, many
    empowering emerging technologies are there with capabilities and functionalities.
    This survey intends to describe borders of elements to fit correct elements in
    massive heterogeneous IoT ecosystem. A survey on all the elements of the IoT ecosystem
    helps in understanding the concept of IoT, architecture, devices, operating system,
    middleware and communication interfaces. This paper explains about the various
    architectures for IoT ecosystem based on requirements like multilayer, middleware-based,
    service-oriented etc. The significance of low end, middle-end and high-end devices
    in IoT devices are explained in the paper. All smart devices at ground level have
    been compared based on capabilities like architecture, computation, memory, communication
    interfaces have been discussed. OS facilitates development and subsistence of
    IoT. According to the requirement of the hardware, various IoT OS based on the
    resource constraint is discussed in the paper. A comparative overview of open-source
    IoT OS on aspects like kernel, scheduler, memory management, performance, simulator,
    security, power has been done. IoT platforms and middleware act as a bridge between
    devices and application to support heterogeneity, scalability, security and highly
    complex computational capability. IoT middleware has been analysed ranging from
    consumer-centric cloud-based, light-weight actor-based, and heavyweight service-based.
    Basic communication technologies to support IoT has been described. For data to
    flow in a secured way, it discusses low power communication networks and protocols
    for all the layers starting from the physical layer to the application layer.
    Security and privacy issues grew significantly in direct proportion in advancing
    networking and communicating sectors. This open several issues arising due to
    increasing devices, technological integration, increased traffic, data storage
    and processing, privacy and security etc. that become the key areas of research.
    Cloud computing as a base technology in order to operate and integrate with recent
    technologies such as big data. The technology of cloud computing refers to the
    processing power of the data at the “edge” of a network. Additionally, we could
    say that cloud computing operates in “Fog” environment. The interplay between
    the IoT, big data analytics, cloud and fog computing making it an IoT ecosystem
    resolving the problems like mobility, availability, storage, computational capability
    etc. for real-time scenarios has been discussed. The challenges and issues that
    are of significance for design and deployment of expanding IoT ecosystem have
    been displayed. As technology grows and expands but in parallel challenges also
    comes that becomes the key areas of research and thus paper also illustrates IoT
    challenges. In the last this paper also mentions lifesaver smart healthcare and
    other applications in IoT ecosystem focusing on all the component of the ecosystem.
    The dynamic environment of IoT introduces unseen opportunities for communication,
    which are going to change the perception of computing and networking. Abbreviations
    ADC: Analog to digital convertor BLE: Bluetooth low energy CSI camera port: Camera
    Serial Interface camera port DAC: Digital to analog computer DSI display port:
    Display Serial Interface display port FIFO: First In First Out GPS: Global Positioning
    System HDMI: High Definition Multimedia Interface HTTP: Hyper Text Transfer Protocol
    I2C: Inter IC (Integrated Circuit) IETF: Internet Engineering Task Force LAN:
    Local Area Network LCD: Liquid Crystal Display LoRAWAN: Long Range Wireless Area
    Network LTE: Long Term Evolution lwIP: Lightweight Internet Protocol MCU: Microcontroller
    Unit microUSB: Micro Universal Serial Bus NB-IoT: Narrow Band IoT RFID: Radio
    Frequency Identification RISC: Reduced Instruction Set Computer RPL: Routing protocol
    RTOS: Real Time Operating System SPI: Serial peripheral interface UART: Universal
    Asynchronous Receiver/Transmitter UDP: User Datagram Protocol uIP: Micro Internet
    Protocol USART: Universal Synchronous/Asynchronous Receiver/Transmitter WLAN:
    Wireless Local Area Network References X. Wang, X. Zha, W. Ni, R. P. Liu, Y. J.
    Guo, X. Niu and K. Zheng, Survey on blockchain for Internet of Things, Computer
    Communications, Vol. 36, pp. 10–29, 2019. Google Scholar   A. Čolaković and M.
    Hadžialić, Internet of Things (IoT): a review of enabling technologies, challenges,
    and open research issues, Computer Networks, Vol. 144, pp. 17–39, 2018. Google
    Scholar   I. Mashal, O. Alsaryrah, T. Y. Chung, C. Z. Yang, W. H. Kuo and D. P.
    Agrawal, Choices for interaction with things on Internet and underlying issues,
    Ad Hoc Networks, Vol. 28, pp. 68–90, 2015. Google Scholar   I. Lee and K. Lee,
    The Internet of Things (IoT): applications, investments, and challenges for enterprises,
    Business Horizons, Vol. 58, No. 4, pp. 431–440, 2015. Google Scholar   A. Whitmore,
    A. Agarwal and L. Da Xu, The Internet of Things—a survey of topics and trends,
    Information Systems Frontiers, Vol. 17, No. 2, pp. 261–274, 2015. Google Scholar   S.
    D. T. Kelly, N. K. Suryadevara and S. C. Mukhopadhyay, Towards the implementation
    of IoT for environmental condition monitoring in homes, IEEE Sensors Journal,
    Vol. 13, No. 10, pp. 3846–3853, 2013. Google Scholar   A. Al-Fuqaha, M. Guizani,
    M. Mohammadi, M. Aledhari and M. Ayyash, Internet of things: a survey on enabling
    technologies, protocols, and applications, IEEE Communications Surveys & Tutorials,
    Vol. 17, No. 4, pp. 2347–2376, 2015. Google Scholar   E. Ahmed, I. Yaqoob, A.
    Gani, M. Imran and M. Guizani, Internet-of-things-based smart environments: state
    of the art, taxonomy, and open research challenges, IEEE Wireless Communications,
    Vol. 23, No. 5, pp. 10–16, 2016. Google Scholar   F. Al-Turjman, Artificial Intelligence
    in IoT, SpringerCham, 2019. Google Scholar   A. A.Osuwa, E. B. Ekhoragbon, and
    L. T. Fat, Application of artificial intelligence in Internet of Things. In 2017
    9th international conference on computational intelligence and communication networks
    (CICN), IEEE, New York, pp. 169–173, 2017. S. Krco, B. Pokric, and F. Carrez,
    Designing IoT architecture (s): a European perspective. In 2014 IEEE world forum
    on Internet of Things (WF-IoT), IEEE, New York, pp. 79–84, 2014. O. Novo, Blockchain
    meets IoT: an architecture for scalable access management in IoT, IEEE Internet
    of Things Journal, Vol. 5, No. 2, pp. 1184–1195, 2018. Google Scholar   H. Guo,
    J. Ren, D. Zhang, Y. Zhang and J. Hu, A scalable and manageable IoT architecture
    based on transparent computing, Journal of Parallel and Distributed Computing,
    Vol. 118, pp. 5–13, 2018. Google Scholar   J. Gubbi, R. Buyya, S. Marusic and
    M. Palaniswami, Internet of Things (IoT): a vision, architectural elements, and
    future directions, Future Generation Computer Systems, Vol. 29, No. 7, pp. 1645–1660,
    2013. Google Scholar   R. Chen, J. Guo and F. Bao, Trust management for SOA-based
    IoT and its application to service composition, IEEE Transactions on Services
    Computing, Vol. 9, No. 3, pp. 482–495, 2016. Google Scholar   P. Sethi and S.
    R. Sarangi, Internet of things: architectures, protocols, and applications, Journal
    of Electrical and Computer Engineering, 2017. https://doi.org/10.1155/2017/9324035.
    Article   Google Scholar   J. Ren, H. Guo, C. Xu and Y. Zhang, Serving at the
    edge: a scalable iot architecture based on transparent computing, IEEE Network,
    Vol. 31, No. 5, pp. 96–105, 2017. Google Scholar   S. Balaji, K. Nathani and R.
    Santhakumar, IoT technology, applications and challenges a contemporary survey,
    Wireless Personal Communications, Vol. 108, pp. 1–26, 2019. Google Scholar   S.
    Kraijak, and P. Tuwanut, A survey on IoT architectures, protocols, applications,
    security, privacy, real-world implementation and future trends. In 11th international
    conference on wireless communications, networking and mobile computing (WiCOM
    2015), 2015. N. Singh and M. Vardhan, Distributed ledger technology-based property
    transaction system with support for IoT devices, International Journal of Cloud
    Applications and Computing (IJCAC), Vol. 9, No. 2, pp. 60–78, 2019. Google Scholar   V.
    Vujović and M. Maksimović, Raspberry Pi as a Sensor Web node for home automation,
    Computers & Electrical Engineering, Vol. 44, pp. 153–171, 2015. Google Scholar   T.
    Käfer, S. R. Bader, L. Heling, R. Manke, and A. Harth, Exposing Internet of Things
    devices via REST and linked data interfaces. In Proc. 2nd workshop semantic web
    technol. Internet Things, pp. 1–14, 2017. S. Huh, S. Cho, and S. Kim, Managing
    IoT devices using blockchain platform. In 2017 19th international conference on
    advanced communication technology (ICACT), IEEE, New York, pp. 464–467, 2017.
    E. Baccelli, C. Gündoğan, O. Hahm, P. Kietzmann, M. S. Lenders, H. Petersen, K.
    Schleiser, T. C. Schmidt and M. Wählisch, RIOT: an open-source operating system
    for low-end embedded devices in the IoT, IEEE Internet of Things Journal, Vol.
    5, No. 6, pp. 4428–4440, 2018. Google Scholar   D. Zhai, R. Zhang, L. Cai, B.
    Li and Y. Jiang, Energy-efficient user scheduling and power allocation for NOMA-based
    wireless networks with massive IoT devices, IEEE Internet of Things Journal, Vol.
    5, No. 3, pp. 1857–1868, 2018. Google Scholar   F. Shaikh, E. Bou-Harb, N. Neshenko,
    A. P. Wright and N. Ghani, Internet of malicious things: correlating active and
    passive measurements for inferring and characterizing internet-scale unsolicited
    IoT devices, IEEE Communications Magazine, Vol. 56, No. 9, pp. 170–177, 2018.
    Google Scholar   M. A. R. Shuman, A. Goel, S. Sharma, B. Gupta, A. Aggarwal, I.
    D. Guedalia, R. P. Chandhok, and J. Guedalia, Qualcomm Inc, Establishing groups
    of internet of things (IOT) devices and enabling communication among the groups
    of IOT devices. U.S. Patent 9853826, 2017. P. Desai, A. Sheth, and P. Anantharam,
    Semantic gateway as a service architecture for iot interoperability. In 2015 IEEE
    International Conference on Mobile Services, IEEE, New York, pp. 313–319, 2015.
    G. Aloi, G. Caliciuri, G. Fortino, R. Gravina, P. Pace, W. Russo and C. Savaglio,
    Enabling IoT interoperability through opportunistic smartphone-based mobile gateways,
    Journal of Network and Computer Applications, Vol. 81, pp. 74–84, 2017. Google
    Scholar   B. Kang and H. Choo, An experimental study of a reliable IoT gateway,
    ICT Express, Vol. 4, No. 3, pp. 130–133, 2018. Google Scholar   Q. Zhu, R. Wang,
    Q. Chen, Y. Liu, and W. Qin, Iot gateway: bridging wireless sensor networks into
    internet of things. In 2010 IEEE/IFIP international conference on embedded and
    ubiquitous computing, IEEE, New York, pp. 347–352, 2010. B. Kang, D. Kim and H.
    Choo, Internet of everything: a large-scale autonomic IoT gateway, IEEE Transactions
    on Multi-Scale Computing Systems, Vol. 3, No. 3, pp. 206–214, 2017. Google Scholar   S.
    K. Datta, C. Bonnet, and N. Nikaein, An IoT gateway centric architecture to provide
    novel M2M services. In 2014 IEEE World Forum on Internet of Things (WF-IoT), IEEE,
    New York, pp. 514–519, 2014. H. Chen, X. Jia, and H. Li, October. A brief introduction
    to IoT gateway. In IET international conference on communication technology and
    application (ICCTA 2011), IET, London, pp. 610–613, 2011. S. Guoqiang, C. Yanming,
    Z. Chao, and Z. Yanxu, Design and implementation of a smart IoT gateway. In 2013
    IEEE international conference on green computing and communications and IEEE internet
    of things and IEEE cyber, physical and social computing, IEEE, New York, pp. 720–723,
    2013. F. C. Cheng, Automatic and secure Wi-Fi connection mechanisms for IoT end-devices
    and gateways. In International conference for emerging technologies in computing,
    Springer, Cham, pp. 98–106, 2018. P. Gaur, and M. P. Tahiliani, Operating systems
    for IoT devices: a critical survey. In 2015 IEEE region 10 symposium, IEEE, New
    York, pp. 33–36, 2015. O. Hahm, E. Baccelli, H. Petersen and N. Tsiftes, Operating
    systems for low-end devices in the internet of things: a survey, IEEE Internet
    of Things Journal, Vol. 3, No. 5, pp. 720–734, 2016. Google Scholar   A. Musaddiq,
    Y. B. Zikria, O. Hahm, H. Yu, A. K. Bashir and S. W. Kim, A survey on resource
    management in IoT operating systems, IEEE Access, Vol. 6, pp. 8459–8482, 2018.
    Google Scholar   D. Balsamo, A. Elboreini, B. M. Al-Hashimi, and G. V. Merrett,
    Exploring ARM mbed support for transient computing in energy harvesting IoT systems.
    In 2017 7th IEEE international workshop on advances in sensors and interfaces
    (IWASI), IEEE, New York, pp. 115–120, 2017. A. Dunkels, O. Schmidt, N. Finne,
    J. Eriksson, F. Österlind, and N. T. M. Durvy, The Contiki os: the operating system
    for the internet of things, 2011, http://www.contikios.org. V. J. P. Amorim, S.
    Delabrida, and R. A. R. Oliveira, A constraint-driven assessment of operating
    systems for wearable devices. In 2016 VI Brazilian symposium on computing systems
    engineering (SBESC), IEEE, New York, pp. 150–155, 2016. P. Dutta and A. Dunkels,
    Operating systems and network protocols for wireless sensor networks, Philosophical
    Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,
    Vol. 370, No. 1958, pp. 68–84, 2012. MathSciNet   MATH   Google Scholar   A. Kazmi,
    M. Serrano and J. Soldatos, Vital-os: an open-source iot operating system for
    smart cities, IEEE Communications Standards Magazine, Vol. 2, No. 2, pp. 71–77,
    2018. Google Scholar   F. Javed, M. K. Afzal, M. Sharif and B. S. Kim, Internet
    of things (IoT) operating systems support, networking technologies, applications,
    and challenges: a comparative review, IEEE Communications Surveys & Tutorials,
    Vol. 20, No. 3, pp. 2062–2100, 2018. Google Scholar   D. Zhang, C. C. Chan and
    G. Y. Zhou, Enabling Industrial Internet of Things (IIoT) towards an emerging
    smart energy system, Global Energy Interconnection, Vol. 1, No. 1, pp. 39–47,
    2018. Google Scholar   A. C. G. Anadiotis, S. Milardo, G. Morabito and S. Palazzo,
    Toward unified control of networks of switches and sensors through a network operating
    system, IEEE Internet of Things Journal, Vol. 5, No. 2, pp. 895–904, 2018. Google
    Scholar   R. Morabito, V. Cozzolino, A. Y. Ding, N. Beijar and J. Ott, Consolidate
    IoT edge computing with lightweight virtualization, IEEE Network, Vol. 32, No.
    1, pp. 102–111, 2018. Google Scholar   S. Iraji, P. Mogensen and R. Ratasuk, Recent
    advances in M2M communications and internet of things (IoT), International Journal
    of Wireless Information Networks, Vol. 24, pp. 240–242, 2017. https://doi.org/10.1007/s10776-017-0362-3.
    Article   Google Scholar   S. Persia, C. Carciofi, and M. Faccioli, NB-IoT and
    LoRA connectivity analysis for M2M/IoT smart grids applications. In 2017 AEIT
    international annual conference, IEEE, New York, pp. 1–6, 2017. E. S. Lohan, M.
    Koivisto, O. Galinina, S. Andreev, A. Tolli, G. Destino, M. Costa, K. Leppanen,
    Y. Koucheryavy and M. Valkama, Benefits of positioning-aided communication technology
    in high-frequency industrial IoT, IEEE Communications Magazine, Vol. 56, No. 12,
    pp. 142–148, 2018. Google Scholar   M. Siekkinen, M. Hiienkari, J. K. Nurminen,
    and J. Nieminen, How low energy is Bluetooth low energy? Comparative measurements
    with zigbee/802.15. 4. In 2012 IEEE wireless communications and networking conference
    workshops (WCNCW), IEEE, New York, pp. 232–237, 2012. M. M. Alam, H. Malik, M.
    I. Khan, T. Pardy, A. Kuusik and Y. Le Moullec, A survey on the roles of communication
    technologies in IoT-based personalized healthcare applications, IEEE Access, Vol.
    6, pp. 36611–36631, 2018. Google Scholar   A. A. Mutlag, M. K. A. Ghani, N. A.
    Arunkumar, M. A. Mohamed and O. Mohd, Enabling technologies for fog computing
    in healthcare IoT systems, Future Generation Computer Systems, Vol. 90, pp. 62–78,
    2019. Google Scholar   P. Pongle, and G. Chavan, A survey: attacks on RPL and
    6LoWPAN in IoT. In 2015 international conference on pervasive computing (ICPC),
    IEEE, New York, pp. 1–6, 2015. C. Bormann, K. Hartke, and Z. Shelby, The constrained
    application protocol (CoAP), RFC 7252, 2015. S. Bansal, and D. Kumar, IoT application
    layer protocols: performance analysis and significance in smart city. In 2019
    10th international conference on computing, communication and networking technologies
    (ICCCNT), IEEE, New York, pp. 1–6, 2019. J. Wan, B. Chen, M. Imran, F. Tao, D.
    Li, C. Liu and S. Ahmad, Toward dynamic resources management for IoT-based manufacturing,
    IEEE Communications Magazine, Vol. 56, No. 2, pp. 52–59, 2018. Google Scholar   A.
    A. Zaidan, B. B. Zaidan, M. Y. Qahtan, O. S. Albahri, A. S. Albahri, M. Alaa,
    F. M. Jumaah, M. Talal, K. L. Tan, W. L. Shir and C. K. Lim, A survey on communication
    components for IoT-based technologies in smart homes, Telecommunication Systems,
    Vol. 69, No. 1, pp. 1–25, 2018. Google Scholar   R. Bonetto, N. Bui, V. Lakkundi,
    A. Olivereau, A. Serbanati, and M. Rossi, Secure communication for smart IoT object
    sacks, use cases and practical examples. In 2012 IEEE international symposium
    on a world of wireless, mobile and multimedia networks (WoWMoM), IEEE, New York,
    pp. 1–7, 2012. S. Al-Sarawi, M. Anbar, K. Alieyan, and M. Alzubaidi, Internet
    of Things (IoT) communication protocols. In 2017 8th international conference
    on information technology (ICIT), IEEE, New York, pp. 685–690, 2017. K. Ponnusamy
    and N. Rajagopalan, Internet of things: a survey on IoT protocol standards. Progress
    in Advanced Computing and Intelligent Engineering, SpringerSingapore, 2018. pp.
    651–663. Google Scholar   B. Vejlgaard, M. Lauridsen, H. Nguyen, I. Z. Kovács,
    P. Mogensen, and M. Sorensen, Coverage and capacity analysis of sigfox, Lora,
    GPRS, and nb-iot. In 2017 IEEE 85th vehicular technology conference (VTC Spring),
    IEEE, New York, pp. 1–5, 2017. M. Lauridsen, H. Nguyen, B. Vejlgaard, I. Z. Kovács,
    P. Mogensen, and M. Sorensen, Coverage comparison of GPRS, NB-IoT, LoRa, and SigFox
    in a 7800 km2 area. In 2017 IEEE 85th vehicular technology conference (VTC Spring),
    IEEE, New York, pp. 1–5, 2017. R. S. Sinha, Y. Wei and S. H. Hwang, A survey on
    LPWA technology: LoRa and NB-IoT, Ict Express, Vol. 3, No. 1, pp. 14–21, 2017.
    Google Scholar   J. Dizdarević, F. Carpio, A. Jukan and X. Masip-Bruin, A survey
    of communication protocols for internet of things and related challenges of fog
    and cloud computing integration, ACM Computing Surveys (CSUR), Vol. 51, No. 6,
    p. 116, 2019. Google Scholar   Y. Chen, and T. Kunz, Performance evaluation of
    IoT protocols under a constrained wireless access network. In 2016 international
    conference on selected topics in mobile & wireless networking (MoWNeT), IEEE,
    New York, pp. 1–7, 2016. U. D. Ulusar, F. Al-Turjman, and G. Celik, An overview
    of Internet of things and wireless communications. In 2017 international conference
    on computer science and engineering (UBMK), IEEE, New York, pp. 506–509, 2017.
    K. Hartke, Observing resources in the constrained application protocol (CoAP)
    (No. RFC 7641), 2015. K. Mekki, E. Bajic, F. Chaxel and F. Meyer, A comparative
    study of LPWAN technologies for large-scale IoT deployment, ICT Express, Vol.
    5, No. 1, pp. 1–7, 2019. Google Scholar   W. Ayoub, A. E. Samhat, F. Nouvel, M.
    Mroue and J. C. Prévotet, Internet of mobile things: overview of LoRaWAN, DASH7,
    and NB-IoT in LPWANs standards and supported mobility, IEEE Communications Surveys
    & Tutorials, Vol. 21, No. 2, pp. 1561–1581, 2018. Google Scholar   M. Centenaro
    and L. Vangelista, Time-power multiplexing for LoRa-based IoT networks: an effective
    way to boost LoRaWAN network capacity, International Journal of Wireless Information
    Networks, Vol. 26, pp. 1–11, 2019. Google Scholar   A. H. Ngu, M. Gutierrez, V.
    Metsis, S. Nepal and Q. Z. Sheng, IoT middleware: a survey on issues and enabling
    technologies, IEEE Internet of Things Journal, Vol. 4, No. 1, pp. 1–20, 2017.
    Google Scholar   M. A. da Cruz, J. J. Rodrigues, A. K. Sangaiah, J. Al-Muhtadi
    and V. Korotaev, Performance evaluation of IoT middleware, Journal of Network
    and Computer Applications, Vol. 109, pp. 53–65, 2018. Google Scholar   S. Bandyopadhyay,
    M. Sengupta, S. Maiti and S. Dutta, Role of middleware for internet of things:
    a study, International Journal of Computer Science and Engineering Survey, Vol.
    2, No. 3, pp. 94–105, 2011. Google Scholar   A. Palade, C. Cabrera, F. Li, G.
    White, M. A. Razzaque and S. Clarke, Middleware for internet of things: an evaluation
    in a small-scale IoT environment, Journal of Reliable Intelligent Environments,
    Vol. 4, No. 1, pp. 3–23, 2018. Google Scholar   C. Pereira, J. Cardoso, A. Aguiar
    and R. Morla, Benchmarking Pub/Sub IoT middleware platforms for smart services,
    Journal of Reliable Intelligent Environments, Vol. 4, No. 1, pp. 25–37, 2018.
    Google Scholar   A. Ranganathan, J. Al-Muhtadi, S. Chetan, R. Campbell and M.
    D. Mickunas, Middleware: a middleware for location awareness in ubiquitous computing
    applications. ACM/IFIP/USENIX International Conference on Distributed Systems
    Platforms and Open Distributed Processing, SpringerBerlin, 2004. pp. 397–416.
    Google Scholar   G. Kokkonis, A. Chatzimparmpas, and S. Kontogiannis, Middleware
    IoT protocols performance evaluation for carrying out clustered data. In 2018
    South-Eastern European design automation, computer engineering, computer networks
    and society media conference (SEEDA_CECNSM), IEEE, New York, pp. 1–5, 2018. H.
    Hejazi, H. Rajab, T. Cinkler, and L. Lengyel, Survey of platforms for massive
    IoT. In 2018 IEEE international conference on future IoT technologies (future
    IoT), IEEE, New York, pp. 1–8, 2018. J. Kim, J. Yun, S. C. Choi, D. N. Seed, G.
    Lu, M. Bauer, A. Al-Hezmi, K. Campowsky and J. Song, Standard-based IoT platforms
    interworking: implementation, experiences, and lessons learned, IEEE Communications
    Magazine, Vol. 54, No. 7, pp. 48–54, 2016. Google Scholar   A. Bröring, S. Schmid,
    C. K. Schindhelm, A. Khelil, S. Käbisch, D. Kramer, D. Le Phuoc, J. Mitic, D.
    Anicic and E. Teniente, Enabling IoT ecosystems through platform interoperability,
    IEEE Software, Vol. 34, No. 1, pp. 54–61, 2017. Google Scholar   G. Keramidas,
    N. Voros and M. Hübner, Components and Services for IoT Platforms, Springer International
    PuCham, 2016. Google Scholar   G. Fortino, C. Savaglio, C. E. Palau, J. S. de
    Puga, M. Ganzha, M. Paprzycki, M. Montesinos, A. Liotta, and M. Llop, Towards
    multi-layer interoperability of heterogeneous IoT platforms: the INTER-IoT approach.
    In Integration, interconnection, and interoperability of IoT systems, Springer,
    Cham, pp. 199–232, 2018. F. Y. Okay and S. Ozdemir, Routing in fog-enabled IoT
    platforms: a survey and an SDN-based solution, IEEE Internet of Things Journal,
    Vol. 5, No. 6, pp. 4871–4889, 2018. Google Scholar   T. Jell, C. Baumgartner,
    A. Bröring, J. Mitic and B. I. G. IoT, interconnecting IoT platforms from different
    domains—first success story. Information Technology-New Generations, SpringerCham,
    2018. pp. 721–724. Google Scholar   B. B. Gupta and D. P. Agrawal, editors., Handbook
    of Research on Cloud Computing and Big Data Applications in IoT, IGI GlobalHershey,
    2019. Google Scholar   A. Ghosh, D. Chakraborty and A. Law, Artificial intelligence
    in Internet of things, CAAI Transactions on Intelligence Technology, Vol. 3, No.
    4, pp. 208–218, 2018. Google Scholar   A. Oussous, F. Z. Benjelloun, A. A. Lahcen
    and S. Belfkih, Big Data technologies: a survey, Journal of King Saud University-Computer
    and Information Sciences, Vol. 30, No. 4, pp. 431–448, 2018. Google Scholar   G.
    Sun, V. Chang, S. Guan, M. Ramachandran, J. Li and D. Liao, Big Data and Internet
    of Things—fusion for different services and its impacts, Future Generation Computer
    Systems, Vol. 86, pp. 1368–1370, 2018. Google Scholar   C. Stergiou, K. E. Psannis,
    B. G. Kim and B. Gupta, Secure integration of IoT and cloud computing, Future
    Generation Computer Systems, Vol. 78, pp. 964–975, 2018. Google Scholar   K. Hossain,
    M. Rahman and S. Roy, IoT data compression and optimization techniques in cloud
    storage: current prospects and future directions, International Journal of Cloud
    Applications and Computing (IJCAC), Vol. 9, No. 2, pp. 43–59, 2019. Google Scholar   Y.
    A. Mo, Data security storage method for IoT under Hadoop cloud computing platform,
    International Journal of Wireless Information Networks, Vol. 26, pp. 152–157,
    2019. https://doi.org/10.1007/s10776-019-00434-x. Article   Google Scholar   B.
    B. Gupta, Computer and Cybersecurity: Principles, Algorithm, Applications, and
    Perspectives, CRC PressBoca Raton, 2018. Google Scholar   C. Stergiou, K. E. Psannis,
    B. B. Gupta and Y. Ishibashi, Security, privacy & efficiency of sustainable cloud
    computing for big data & IoT, Sustainable Computing: Informatics and Systems,
    Vol. 19, pp. 174–184, 2018. Google Scholar   A. Sehgal, V. Perelman, S. Kuryla
    and J. Schonwalder, Management of resource-constrained devices in the internet
    of things, IEEE Communications Magazine, Vol. 50, No. 12, pp. 144–149, 2012. Google
    Scholar   Z. K. Zhang, M. C. Y. Cho, C. W. Wang, C. W. Hsu, C. K. Chen, and S.
    Shieh, IoT security: ongoing challenges and research opportunities. In 2014 IEEE
    7th international conference on service-oriented computing and applications, IEEE,
    New York, pp. 230–234, 2014. A. Dorri, S. S. Kanhere, R. Jurdak, and P. Gauravaram,
    Blockchain for IoT security and privacy: the case study of a smart home. In 2017
    IEEE international conference on pervasive computing and communications workshops
    (PerCom workshops), IEEE, New York, pp. 618–623, 2017. B. Guo, D. Zhang, Z. Wang,
    Z. Yu and X. Zhou, Opportunistic IoT: exploring the harmonious interaction between
    human and the internet of things, Journal of Network and Computer Applications,
    Vol. 36, No. 6, pp. 1531–1539, 2013. Google Scholar   C. Chang, S. N. Srirama
    and R. Buyya, Internet of things (IoT) and new computing paradigms, Fog and Edge
    Computing: Principles and Paradigms, Vol. 6, pp. 1–23, 2019. Google Scholar   I.
    Yaqoob, I. A. T. Hashem, A. Ahmed, S. A. Kazmi and C. S. Hong, Internet of things
    forensics: recent advances, taxonomy, requirements, and open challenges, Future
    Generation Computer Systems, Vol. 92, pp. 265–275, 2019. Google Scholar   M. Chen,
    J. Yang, X. Zhu, X. Wang, M. Liu and J. Song, Smart home 2.0: innovative smart
    home system powered by botanical IoT and emotion detection, Mobile Networks and
    Applications, Vol. 22, No. 6, pp. 1159–1169, 2017. Google Scholar   A. Sharif,
    J. P. Li and M. A. Saleem, Internet of things enabled vehicular and ad hoc networks
    for smart city traffic monitoring and controlling: a review, International Journal
    of Advanced Networking and Applications, Vol. 10, No. 3, pp. 3833–3842, 2018.
    Google Scholar   L. Catarinucci, D. De Donno, L. Mainetti, L. Palano, L. Patrono,
    M. L. Stefanizzi and L. Tarricone, An IoT-aware architecture for smart healthcare
    systems, IEEE Internet of Things Journal, Vol. 2, No. 6, pp. 515–526, 2015. Google
    Scholar   F. Fernandez, and G. C. Pallis, Opportunities and challenges of the
    Internet of Things for healthcare: systems engineering perspective. In 2014 4th
    international conference on wireless mobile communication and healthcare-transforming
    healthcare through innovations in mobile and wireless technologies (MOBIHEALTH),
    IEEE, New York, pp. 263–266, 2014. A. Iyengar, A. Kundu and G. Pallis, Healthcare
    informatics and privacy, IEEE Internet Computing, Vol. 22, No. 2, pp. 29–31, 2018.
    Google Scholar   R. Zgheib, E. Conchon and R. Bastide, Semantic middleware architectures
    for IoT healthcare applications. Enhanced Living Environments, SpringerCham, 2019.
    pp. 263–294. Google Scholar   S. B. Baker, W. Xiang and I. Atkinson, Internet
    of things for smart healthcare: technologies, challenges, and opportunities, IEEE
    Access, Vol. 5, pp. 26521–26544, 2017. Google Scholar   D. A. Gandhi, and M. Ghosal,
    Intelligent healthcare using IoT: a extensive survey. In 2018 second international
    conference on inventive communication and computational technologies (ICICCT),
    IEEE, New York, pp. 800–802, 2018. N. Deshai, S. Venkataramana, B. V. D. S. Sekhar,
    K. Srinivas and G. P. S. Varma, A study on IOT tools, protocols, applications,
    opportunities and challenges. Information Systems Design and Intelligent Applications,
    SpringerSingapore, 2019. pp. 367–380. Google Scholar   K. Lakhwani, H. Gianey,
    N. Agarwal and S. Gupta, Development of IoT for smart agriculture a review. Emerging
    Trends in Expert Applications and Security, SpringerSingapore, 2019. pp. 425–432.
    Google Scholar   J. P. Lemayian and F. Al-Turjman, Intelligent IoT communication
    in smart environments: an overview. Artificial Intelligence in IoT, SpringerCham,
    2019. pp. 207–221. Google Scholar   Download references Author information Authors
    and Affiliations ECE Department, Sant Longowal Institute of Engineering and Technology,
    Longowal, Sangrur, India Sharu Bansal & Dilip Kumar Corresponding author Correspondence
    to Sharu Bansal. Additional information Publisher''s Note Springer Nature remains
    neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Reprints and permissions About this article
    Cite this article Bansal, S., Kumar, D. IoT Ecosystem: A Survey on Devices, Gateways,
    Operating Systems, Middleware and Communication. Int J Wireless Inf Networks 27,
    340–364 (2020). https://doi.org/10.1007/s10776-020-00483-7 Download citation Received
    09 October 2019 Revised 08 January 2020 Accepted 05 February 2020 Published 13
    February 2020 Issue Date September 2020 DOI https://doi.org/10.1007/s10776-020-00483-7
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords IoT devices OS Middleware Communication Gateways Security
    Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections
    Figures References Abstract Introduction IoT Architecture IoT Taxonomy Elements
    of IoT Ecosystem IoT Challenges Applications in IoT Ecosystem Conclusion Abbreviations
    References Author information Additional information Rights and permissions About
    this article Advertisement Discover content Journals A-Z Books A-Z Publish with
    us Publish your research Open access publishing Products and services Our products
    Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio
    BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state
    privacy rights Accessibility statement Terms and conditions Privacy policy Help
    and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University
    of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: International Journal of Wireless Information Networks
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'IoT Ecosystem: A Survey on Devices, Gateways, Operating Systems, Middleware
    and Communication'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Taneja M.
  - Byabazaire J.
  - Jalodia N.
  - Davy A.
  - Olariu C.
  - Malone P.
  citation_count: '82'
  description: Timely lameness detection is one of the major and costliest health
    problems in dairy cattle that farmers and practitioners haven't yet solved adequately.
    The primary reason behind this is the high initial setup costs, complex equipment
    and lack of multi-vendor interoperability in currently available solutions. On
    the other hand, human observation based solutions relying on visual inspections
    are prone to late detection with possible human error, and are not scalable. This
    poses a concern with increasing herd sizes, as prolonged or undetected lameness
    severely compromises cows' health and welfare, and ultimately affects the milk
    productivity of the farm. To tackle this, we have developed an end-to-end IoT
    application that leverages advanced machine learning and data analytics techniques
    to monitor the cattle in real-time and identify lame cattle at an early stage.
    The proposed approach has been validated on a real world smart dairy farm setup
    consisting of a dairy herd of 150 cows in Waterford, Ireland. Using long-range
    pedometers specifically designed for use in dairy cattle, we monitor the activity
    of each cow in the herd. The accelerometric data from these sensors is aggregated
    at the fog node to form a time series of behavioral activities, which are further
    analyzed in the cloud. Our hybrid clustering and classification model identifies
    each cow as either Active, Normal or Dormant, and further, Lame or Non-Lame. The
    detected lameness anomalies are further sent to farmer's mobile device by way
    of push notifications. The results indicate that we can detect lameness 3 days
    before it can be visually captured by the farmer with an overall accuracy of 87%.
    This means that the animal can either be isolated or treated immediately to avoid
    any further effects of lameness. Moreover, with fog based computational assistance
    in the setup, we see an 84% reduction in amount of data transferred to the cloud
    as compared to the conventional cloud based approach.
  doi: 10.1016/j.compag.2020.105286
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Literature Review, background
    and motivation 3. Experimental setup – smart dairy farm setup: real world test-bed
    deployment 4. Materials, methods and machine learning model description 5. Results,
    evaluation and discussion 6. Conclusion 7. Ongoing and future work CRediT authorship
    contribution statement Declaration of Competing Interest Acknowledgment References
    Vitae Show full outline Figures (16) Show 10 more figures Tables (5) Table 1 Table
    2 Table 3 Table 4 Table 5 Computers and Electronics in Agriculture Volume 171,
    April 2020, 105286 Machine learning based fog computing assisted data-driven approach
    for early lameness detection in dairy cattle Author links open overlay panel Mohit
    Taneja a b, John Byabazaire a b, Nikita Jalodia a b, Alan Davy a b, Cristian Olariu
    c, Paul Malone a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.compag.2020.105286
    Get rights and content Under a Creative Commons license open access Highlights
    • Design and development of an IoT application leveraging the fog computing paradigm.
    • Validation of the application in IoT enabled smart dairy farming scenario. •
    A real-world smart farm setup with a full dairy herd of 150 cows. • Blended clustering
    & classification ML model for early lameness detection in cattle. • Early lameness
    detection window of 3 days before visual signs, with 87% accuracy. Abstract Timely
    lameness detection is one of the major and costliest health problems in dairy
    cattle that farmers and practitioners haven''t yet solved adequately. The primary
    reason behind this is the high initial setup costs, complex equipment and lack
    of multi-vendor interoperability in currently available solutions. On the other
    hand, human observation based solutions relying on visual inspections are prone
    to late detection with possible human error, and are not scalable. This poses
    a concern with increasing herd sizes, as prolonged or undetected lameness severely
    compromises cows'' health and welfare, and ultimately affects the milk productivity
    of the farm. To tackle this, we have developed an end-to-end IoT application that
    leverages advanced machine learning and data analytics techniques to monitor the
    cattle in real-time and identify lame cattle at an early stage. The proposed approach
    has been validated on a real world smart dairy farm setup consisting of a dairy
    herd of 150 cows in Waterford, Ireland. Using long-range pedometers specifically
    designed for use in dairy cattle, we monitor the activity of each cow in the herd.
    The accelerometric data from these sensors is aggregated at the fog node to form
    a time series of behavioral activities, which are further analyzed in the cloud.
    Our hybrid clustering and classification model identifies each cow as either Active,
    Normal or Dormant, and further, Lame or Non-Lame. The detected lameness anomalies
    are further sent to farmer''s mobile device by way of push notifications. The
    results indicate that we can detect lameness 3 days before it can be visually
    captured by the farmer with an overall accuracy of 87%. This means that the animal
    can either be isolated or treated immediately to avoid any further effects of
    lameness. Moreover, with fog based computational assistance in the setup, we see
    an 84% reduction in amount of data transferred to the cloud as compared to the
    conventional cloud based approach. Previous article in issue Next article in issue
    Keywords Smart dairy farmingFog computingInternet of Things (IoT)Cloud computingSmart
    farmData analyticsMicroservicesMachine learningClusteringClassificationData-driven
    1. Introduction Internet of things (IoT), fog computing, cloud computing and data
    driven techniques together offer a great opportunity for verticals such as the
    dairy industry to increase productivity by getting actionable insights to improve
    farming practices, thereby increasing efficiency and yield. There has been active
    initiation and movement in the agricultural domain to move towards tech-enabled
    smart solutions to improve farming practices, and increase productivity and yield.
    The concept of Smart Dairy Farming is no longer just a futuristic concept, and
    has started to materialize as different fields such as machine learning have found
    a practical applications in this domain. Timely detection of lameness is a big
    problem in the dairy industry which farmers are not yet able to adequately solve.
    It is one of the factors for reduced performance on many dairy farms, at least
    through reduced reproductive efficiency, milk production and increased culling
    (Chapinal et al., 2009). Lameness is considered to be the third disease of economic
    importance in dairy cows after reduced fertility and mastitis (Van Nuffel et al.,
    2015). An all-encompassing definition of lameness includes any abnormality which
    causes a cow to change the way that she walks, and can be caused by a range of
    foot and leg conditions, themselves caused by disease, management or environmental
    factors (AHDB, 2016). Prevention, early detection and treatment of lameness is
    therefore important to reduce these negative effects of lameness in dairy cows
    (Alsaaod et al., 2015, Poursaberi et al., 2011). Early detection of disease allows
    farmers to intervene earlier, leading to prevention of antibiotic administration
    and improvement in the milk yield, and savings on veterinary treatment for their
    herd. With the increasing global demands of agricultural and dairy produce, the
    scale of farming and livestock management is only set to increase. To increase
    the productivity on a dairy farm, farmers generally look towards the following
    two: 1. Control welfare related issues like lameness 2. Increase the number of
    accurate estrus detection, so as to increase the size of the herd for further
    profitability. The latter has been adequately addressed. In fact, there are major
    industry partnerships like Fujitsu partnering with Microsoft to provide a SaaS
    (Software as a Service) for accurate estrus detection powered by Microsoft''s
    Azure and Analytics (Meet the connected cow, xxxx, “Connected Cows”, 2015). So,
    the next major problem to be solved in smart dairy farming is the ability to accurately
    and timely detection of lameness. Our work is motivated by the following facts:
    • Manual human observation based solutions for lameness detection are susceptible
    to human error and are not scalable as the size of the farm increases. • Few of
    the existing solutions (discussed in the next section) require the equipment in
    use for detection to be placed in a controlled position, and the cows need to
    be constrained to walk through them. Guiding the cows to walk in a controlled
    manner and presence of a human being leads to bias in the measurement because
    of stoic nature of cows; as they will try to hide their weakness and pain compared
    to measurements made during normal routine without the presence of a human. Although
    there are existing wearable sensor based cloud centric and isolated offline solutions,
    they suffer from the issue of multi-vendor interoperability and vendor-lock in,
    which has been detailed in the next section. Therefore, there is still a need
    for further automation and advanced machine learning based solution that: • Monitors
    the animals everywhere they are – either in the fields grazing, during milking,
    or lying down in the shed. • Is Population Agnostic – takes into account individual
    animal behaviour. • Is Environment Agnostic – takes into account variations in
    weather and environment. In this article, we present an end-to-end IoT application
    that leverages threshold based clustering and machine learning classification
    to detect lameness in dairy cattle. The application automatically measures and
    gathers activity data (lying time, step count and swaps per hour) continuously,
    so that cows can be monitored daily. Furthermore, the clustering technique employed
    ensures that the models dynamically adjust depending on farm and weather conditions,
    and automatically selects a custom learning model for that cluster. Our contribution
    and core novelty of the work presented is summarized as follows: 1. Although the
    existence of clusters in herds have been used before in cattle behaviours (Stephenson
    and Bailey, 2017), this study is the first to use cluster specific model for lameness
    detection as opposed to a one-size-fits-all solution. 2. The technique used to
    form animal profiles eliminates the effects of external factors like weather,
    location and farm conditions. This study is the first to offer an early lameness
    detection service that is population and environment agnostic. 3. The study is
    the first to propose a feedback based re-training based on the inputs of the human
    in the loop, who could be an agricultural expert or farmer. 4. The study is amongst
    one of the few to apply modern AI (Artificial Intelligence) techniques to detect
    lameness in dairy cattle. 5. The methods are deployed in practice and real data
    has been collected. The proposed approach has been validated in a real-world IoT
    deployment in a Smart Dairy farm setup with a full herd of 150 dairy cows. The
    paper has been further structured as follows: 2 presents the literature review,
    background, state-of-the-art and motivation, 3 presents the experimental setup,
    system architecture and application design, 4 presents materials, methods and
    machine learning model developed, 5 presents discussion of the results, and finally
    6 and 7 present the conclusion, Ongoing and future work respectively. 2. Literature
    Review, background and motivation 2.1. Lameness in dairy cattle: geographical
    variance and associated costs The prevalence of lameness has been reported differently
    in different regions and states. Ger reported that on an average Irish farm, 20
    in every 100 cows will be affected by lameness in a given year. In the United
    States authors in (Cook, 2003) and (Espejo et al., 2006) reported a mean lameness
    prevalence of 25%, whereas in California and the northeastern United States, overall
    lameness prevalence was estimated to be 34% and 63%, respectively (von Keyserlingk
    et al., 2012). British and German studies reported a lameness prevalence of 37%
    and 48% (Whay et al., 2003, Barker et al., 2010), whereas a prevalence of 16%
    was reported in the Netherlands (Amory et al., 2006). In our experimental deployment
    on a herd of 150 cows in Ireland, 26 cases of lameness were recorded during July
    to December 2017. Further up by the end of the experiment in April 2018, a total
    of 32 cases were recorded overall. Lameness can be classified into three main
    categories: solar ulcers, digital disease (white line abscess, foreign bodies
    in the sole, and pricked or punctured sole), and inter digital disease (lesions
    of the skin between claws and heel including foul in the foot, inter digital fibroma
    and dermatitis). More than 65% of cases of lameness are said to be caused by diseases
    (Foditsch et al., 2016). Other causes include injuries to the upper skeleton or
    major muscles, septic joints and injection site lesions. Lameness has many negative
    effects, including reduction in feed intake, reduction in milk production (mainly
    due to withdrawal due to antibiotics usage) and weight loss. It therefore has
    a drastic effect on the performance of a dairy farm. Lameness is mostly detected
    at advanced stage and thus requires immediate and often costly treatment. Once
    an animal becomes lame, it can take several weeks to recover. Lameness thus represents
    a significant cost to dairy farmers in terms of time, financial expenditure for
    veterinary calls, medication and treatment, and also for loss in production. Table
    1 (Ger) summarizes the costs estimates for each type of lameness. Table 1. Costs
    associated with each type of lameness. Type of lameness Digital Inter digital
    Solar ulcer Average case Prevalence (%) 45 35 20 Total cost of a single case (in
    €) 282.85 136.12 504.58 275.26 2.2. Existing approaches for lameness detection
    2.2.1. Pressure plate/load cell In these solutions, the main aim is to investigate
    how the weight is distributed across the legs of the animal as it walks through
    a marked area. Neveux et al. (2006) studied the use of a platform outside the
    automatic milking system to measure the weight distribution of cows while standing
    on different surfaces. Chapinal et al., 2010, Pastell et al., 2010 later adjusted
    the experimental setup to measure lameness and hoof lesions. The drawbacks of
    such solutions may not be only the costs of new and complex equipment but also
    other technical concerns. For example, Pastell et al. (2010) suggested that a
    cow may suffer pain when walking, which is not as obvious when the cow is standing
    still. In this setup, the cow must be guided or must be standing in controlled
    position. Because cows are stoic in nature, this will affect the measurements
    and alter the results. 2.2.2. Image processing techniques This category studies
    the use of image processing techniques to analyse the posture of the animal as
    it walks through a milking parlour. Poursaberi et al. (2010) proposed a method
    based on detecting the arc of back posture and fitting a circle through selected
    points on the spine line of a cow as it walks. Viazzi et al. (2013) further studied
    the idea and an algorithm based on Body Movement Pattern was tested under farm
    conditions. Further study on this method shows that there remain challenges on
    real farm conditions. For example, changing lighting conditions cause noise and
    shadows in the images that impede extraction of the back posture, or continuous
    background changes that interfere with cow segmentation from the images. Some
    of these challenges were explored by Poursaberi et al., 2009, Van Hertem et al.,
    2013, Viazzi et al., 2014. 2.2.3. Activity based techniques Here, techniques use
    accelerometers (2D and 3D) and pedometers to record movement patterns of the animal.
    This data is then used to build the daily activities of the cow, e.g. walking,
    lying down. Munksgaard et al. (2006) proposed the use of sensors that measure
    acceleration in different dimensions to automatically monitor activity (standing
    and lying behaviour) of cows. Their results indicate excellent accuracy between
    the sensor data attached to the legs of the cows and observations for lying and
    standing (0.99), activity (0.89), and number of steps (0.84). Chapinal et al.
    (2011) used five 3D accelerometers on cows, one on each limb, and concluded a
    single device attached to one of the legs appeared to be sufficient to measure
    the walking speed of cows, which was associated with locomotion scores. In other
    studies accelerometers were mounted on a hind leg of 348 cows in 401 lactations
    on four commercial farms (Thorup et al., 2015). Since then, a vast number of studies
    have used accelerometers to measure dairy cow activity and behaviour (Alsaaod
    et al., 2012, Yunta et al., 2012, O''Driscoll et al., 2008, Blackie et al., 2011).
    2.3. IoT, fog computing and data analytics in agriculture domain There have been
    proposed systems in industry (Ag, 2017, Ireland, 2017, Boumatic, 2017) as well
    in academia (Taylor et al., 2013, Chen et al., 2014, Wark et al., 2009, Brewster
    et al., 2017) for animal health management in dairy farms. Authors in (Al-Fuqaha
    et al., 2015) provide a detailed survey of IoT enabling technologies that can
    offer automation, data aggregation and protocol adaptation in the wide field of
    IoT. They also present the required integration of IoT with emerging technologies
    such as data analytics and fog computing. Another survey in (Rutten et al., 2013)
    identifies a serious lack of analytics and intelligence in existing smart dairy
    farming systems, thus leading to gaps between the desired requirement of the system
    and proposed solutions. It articulates the pressing requirement of intelligence
    to be present on the premises, in the on-farm systems. As a consequence, attention
    is being drawn towards designing systems with intelligence and data analytics
    capability being present on premises (Shi et al., 2016), and utilizing fog computing
    comes to shore with those objectives in mind. Authors in (Muangprathub et al.,
    2019) present the need of data driven movement in agriculture in order to improve
    crop yields, improve quality, and reduce costs. Another recent survey by authors
    in (Jukan et al., 2017) identifies the lack of interoperability provided by such
    systems, and the need of developing an integrated system combining edge, fog and
    cloud to provide application and services. The authors here also identify that
    technology solutions with no consideration of interoperability results in vendor
    lock-in, which not only hinders innovation, but also results in higher costs for
    the farmer/user. Authors in (Caria et al., 2017) present the use of Raspberry
    Pis as edge devices which are further connected with the cloud to demonstrate
    a smart farm computing systems for animal welfare monitoring. Authors demonstrated
    that a low-cost and open computing and sensing system can effectively monitor
    multiple parameters related to animal welfare. While animal welfare remains a
    broad concept, their paper shows that many parameters relevant to various stakeholders
    can be measured, collected, evaluated and shared, opening up new possibilities
    to improve animal welfare and foster high-tech innovations in this sector. The
    authors in (Hsu et al., 2018) propose the use of fog computing for innovative
    service creations for existing cloud based agriculture system. By means of simulation,
    the authors demonstrate that fog computing presents a unique capability for a
    creative IoT platform adoption in agriculture with existing cloud support. Quite
    recently, authors in (Zamora-Izquierdo and Santa, 2019) describe the design, development
    and evaluation of a system that covers extreme precision agriculture requirements
    by using automation, IoT technologies, and edge and cloud computing through virtualisation.
    2.4. How is the proposed system novel as compared to the existing ones? Although
    most farms are equipped with some kind of estrus detection system (Miiaková et
    al., 2018) which is based on accelerometers, lameness detection systems based
    on the same have not been successful. This is because of vendor lock-in. Each
    of the system would require its own hardware. Worth mentioning is the insight
    in the review by Van De Gucht et al. (2017) that farmers who already had an estrus
    detection system were willing to have an add-on for lameness detection. All the
    current systems lack this kind of integration. Another assumption made by all
    the current solutions is that all the animals will get lame the same way. To put
    this into context, consider a farm in Ireland – there are two main seasons, summer
    and winter. During summer, the animals stay in the field and graze freely, while
    during the winter the animals are kept in house. In both cases the activity levels
    of the animals are different. Interestingly, as the activity levels are low during
    winter considering the animals stay indoors rather than having much outdoor activity,
    the entire herd''s activity pattern will be closer to that of a lame animal during
    the summer. Therefore, a learning model should be able to consider such external
    factors. In a review by authors in (Van Nuffel et al., 2015) about automatic lameness
    detection, some suggestions were made. One of these was the need for automatic
    and continuous measurement of the parameters. This is because most solutions available
    require the animals to be guided one way or another. The other suggestion was
    the need for custom solutions, systems that need less space or those that can
    be included in the existing farm infrastructure (Van Nuffel et al., 2015). The
    presented work differs from the existing sensor based system solutions by offering
    following advantages: • Sensor agnostic: The model is built to take in activity
    data from any kind of sensor used to monitor activity of the animal. This among
    other thing will reduce the initial installation costs if a farm already has a
    system in place. • Avoids vendor lock-in: Design, creation and development of
    services following a microservices based application design principles to tackle
    the problem of vendor lock-in and to support multi-vendor interoperability. •
    Multiple end-users: Since our system is designed as a service, this makes it easy
    to integrate with the existing systems. The end user therefore could be a farmer
    with an existing system or even an agri-tech service provider who wants to provide
    more services to their clients. One of the primary limitations of the previously
    proposed systems is that they follow the technique to process and analyse previously
    collected data and perform only cloud based analytics without leveraging and efficiently
    utilizing the resources (Taneja and Davy, 2016) available on the farm along the
    things-to-cloud continuum (Taneja and Davy, 2017), moreover such techniques are
    not always suitable for real-time tracking and monitoring of dynamic entities
    such as dairy cows. The gaps with the existing research is that either it has
    been developed out of the agricultural context, or addresses the issue of analytics
    and control in isolation; this has also been identified as key limitations by
    authors in (Zheleva et al., 2017). While there has been a movement towards data-driven
    agriculture in recent times for sustainable and productive growth, there is still
    a void when it comes to leveraging emerging paradigms such as fog computing, and
    applying innovating machine learning models to solve a specific problem in the
    dairy sector. Most of the articles in literature present results based on simulated
    experiments, and those which come from real world deployment are mostly agriculture
    based, and rarely based on dairy farming. Further, only some of them have a machine
    learning element to automate their approach. However, to the best of our knowledge,
    no prior work focuses on providing an end-to-end IoT solution integrating edge,
    fog and cloud intelligence specifically in case of smart dairy farming IoT settings.
    We position our work as an answer to the issues mentioned above, thus bridging
    the gap, and providing an innovative way that integrates edge, fog, cloud computing
    and machine learning to provide a solution specifically in case of smart dairy
    farming in an IoT setup. The novelty of the proposed model comes from the standpoint
    that it has been specifically designed and developed to address a specific vertical
    of the IoT ecosystem i.e., dairy farming, and within that to address a specific
    problem related to animal welfare i.e., detecting lameness at an early stage before
    the clinical signs of it appear, with a microservices oriented design making it
    multi-vendor interoperable. 3. Experimental setup – smart dairy farm setup: real
    world test-bed deployment As part of the experiment, the trial1 was conducted
    on a local dairy farm with a full dairy herd of 150 cows in Waterford, Ireland.
    Amongst the available options for the sensors/wearables available for livestock
    monitoring, we used commercially available radio based Long Range Pedometer (433
    MHz, ISM band, LRP --- ENGS Systems©®, Israel) in our deployment. These pedometers
    were attached to the front leg of cows in the herd, as shown in Fig. 1. Download
    : Download high-res image (549KB) Download : Download full-size image Fig. 1.
    Long Range Pedometer (LRP) attached as a part of the experiment to the front leg
    of the cows. 3.1. Architecture design and system overview The overall architecture
    of the test-bed is shown in Fig. 2. As shown in Fig. 2, the Receiver is the master
    unit which sends the received data to the communication unit (RS485 to USB) through
    wired connection, which in turn then sends it to the gateway (a PC form factor
    device in our case, which acts as controller and fog node. The configuration2
    used is Intel® Core™ 3rd Generation i7-3540 M CPU @ 3.00 GHz, 16.0 GB RAM, 500
    GB Local Storage) through wired connection via USB interface. The fog node consists
    of a local database which stores all the data from the sensors before it is preprocessed.
    The collected raw data is then preprocessed and aggregated at fog node to form
    behavioural activities, and summed to form daily time series. In this study, we
    used three behavioural activities (step count, lying time, swaps) for the analysis
    with their description below: 1. Step count: This is the number of steps an animal
    makes. 2. Lying time: The number of hours an animal spends lying down. 3. Swaps:
    This is the number of times an animal moves from lying down to standing up. Download
    : Download high-res image (316KB) Download : Download full-size image Fig. 2.
    Overall architecture of the test-bed and system overview. We used Message Queue
    Telemetry Transport (MQTT) (MQTT, 2017) as the connectivity protocol between fog
    node and cloud (service instances running on IBM Cloud) in our deployment setting.
    MQTT is an open-source protocol originally invented and developed by IBM (Getting
    to know MQTT, 2017). It is a lightweight publish-subscriber model based protocol
    designed on top of the TCP/IP stack. It is specifically targeted for remote location
    connectivity with characteristically unreliable network environments such as high
    delays and low bandwidth (Lee and Kim, 2013), which is one of the issues in remote
    farm based deployments such as ours. Hence, we chose MQTT as the connectivity
    protocol in our deployment. The MQTT architecture comprises of two functional
    components, namely MQTT clients (such as publishers and subscribers) and MQTT
    broker (for mediating messages between publishers and subscribers). In our setup
    these components are as follows: • MQTT Publisher: Script running on fog node
    (i.e., local PC at farm) • MQTT Broker: IBM Watson IoT Platform (as a service
    on IBM Cloud) • MQTT Subscriber: Application designed and hosted on IBM Cloud
    Thus, the data from fog node after pre-processing, aggregation and classification
    as described above and shown in Figs. 2 and 3 is streamed to IBM Waston IoT platform
    using MQTT, the IBM Watson IoT platform receives all these messages, and the MQTT
    subscriber listening to the events of this broker picks up all the data and stores
    it in Cloudant NoSQL JSON Database at IBM Cloud. Download : Download high-res
    image (265KB) Download : Download full-size image Fig. 3. Work flow and data flow
    in the test-bed deployment. 3.2. Designing and developing an IoT based software
    system: objectives and challenges Building an IoT application is an intricate
    process involving end-to-end components, each of which is adapted to the use case
    being addressed. Generally speaking, an end-to-end IoT solution towards a smart
    scenario involves the following steps: • Connecting the Unconnected: This step
    involves the installation of sensors on physical entities such as objects (both
    static or in motion), remote infrastructure or living entities towards achieving
    a specified objective such as monitoring. • Data Acquisition: This involves attaining
    the sensor data and transferring it to the data analytics platform(s) to achieve
    actionable insights for better decision making. This becomes a critical problem
    in scenarios such as ours, wherein a farm location has little or no Internet connectivity.
    • Architecting, Integrating and Management: This crucial phase involves key decisions
    on the software architecture and design principles to be used during development
    of the system. Once finalized, the next step is to integrate, optimize and manage
    the computing system thus built, which is usually an ongoing process. • Data Analytics:
    Once the data is at the desired platform (be it fog or cloud), this part involves
    figuring out how to analyze the data to get the desired information to achieve
    the specified objective, given the constraints. We also had the same objectives
    in mind while developing the application in our scenario, the first three of these
    objectives have been explained above and further below in this section, and the
    data analytics objective has been explained in greater detail in the next section.
    The end-to-end data and work flow of the developed application has been presented
    in Fig. 3. The primary challenge is to design an end to end IoT solution to meet
    the specified objective given the highly variable, harsh and resource constrained
    environment in a smart dairy farming setting. This includes making the system
    resilient and fault tolerant to cope up with the variable farm environments, including
    weather-based network outages and connectivity issues because of remote location
    of the farm. A detailed discussion on test-bed deployment challenges, technical
    challenges faced during deployment and development, and critical decisions made,
    was presented in Taneja et al. (2019b). The use of fog computing brings efficiency
    and sustainability to the overall IoT solution being proposed. In most cases,
    farms are located in remote locations and can suffer from phases of low or no
    Internet/network connectivity. In such adverse connectivity scenarios it becomes
    ideal to process the data locally as much as possible and send the aggregated
    or partial outputs over the internet to the cloud for further enhanced analytical
    results. In view of this we design our solution utilizing fog computing which
    aims to bring computation capabilities closer to the source of data. The fog computing
    based approach leads to effective utilization of limited available resources (Taneja
    and Davy, 2017) and also leads to significant reduction in the amount of the data
    being transferred to the cloud. 3.3. Offline-first model for mobile application
    design Farms are usually located in geographically remote locations facing constrained
    network connectivity. Most of the IoT deployments in such settings are faced with
    limited cellular coverage. Existing solutions are mostly cloud based or completely
    offline. This limits the farmers'' ability to interact with the application anytime
    and anywhere. The system developed in this study has an offline enabled strategy
    via the mobile application and cloud dashboard. Fig. 4 shows the data flow of
    the offline enabled design approach. Download : Download high-res image (318KB)
    Download : Download full-size image Fig. 4. Mobile application developed specifically
    considering the needs of the farmer, including an offline first strategy. The
    figure presents data and notification flow in the developed mobile application.
    Once the model produces notifications, these are sent to the farmer''s mobile
    device as push notifications. On board the application is a PouchDB (Pouch, 2019)
    database which synchronizes with the cloudant database in IBM cloud using a REST
    API whenever a connection is established. The application in general helps to
    achieve the following tasks: • Push notifications: Whether on WiFi or limited
    cellular network, or whether the application is open or not, these will go through
    each time the status of the farm changes. • Data annotation: During the training
    process, this feature was used by the human operator to annotate the data. In
    our case, this was done weekly by an agricultural science student. • Feedback
    to improve model learning: When a notification is generated, the farmer has the
    option of confirming if the identified cow is actually lame, or tag it as a false
    alarm or even report a missed alert. All this information is sent back to the
    model to improve its accuracy. 3.4. Microservices based application flow for multi-vendor
    interoperability Unlike the existing systems that are based on a monolithic design
    approach, the application designed in this study follows a microservices (Balalaie
    et al., 2016) based approach for design, creation and deployment. The aim is to
    make the developed system as Application/Software as a Service'', which can be
    used by the service providers to integrate with their existing systems. For example,
    an agri-tech company could be a service provider for any other solution such as
    mastitis detection, who wants to expand their system or integrate any of the services
    such as lameness or heat detection into their system. A visual representation
    of such a possible integration is presented in the Fig. 5. Feature engineering
    layer as shown in the Fig. 5 ensures that data is transformed to output only the
    required features and also reject those that cannot be engineered to form the
    required features for a desired service; for example Lameness Detection and Heat
    Detection Service expects lying time, step count and swaps but a service provider
    might have activity counter instead of step count and (Stand up + Liedown) instead
    of swaps. Download : Download high-res image (159KB) Download : Download full-size
    image Fig. 5. Microservices based application flow for integration of services
    from different service providers. It is important to note that this layer will
    be different for each service provider since the underlying sensor technology
    might be different. This in turn makes the developed system sensor agnostic. The
    output from the feature engineering layer is then passed to the access layer,
    which includes both mobile and web components. This then goes through a REST API
    which in turn calls the desired service. 4. Materials, methods and machine learning
    model description 4.1. Data The data from the sensors is sent via the receiver
    to the fog node, where it is pre-processed and aggregated into three behavioural
    activities—(1) Step count, (2) Lying time, and (3) Swaps. The choice of these
    3 features is guided by literature study, which indicates that they are among
    the best predictors of a lame cow, or one transitioning to lameness (Thorup et
    al., 2015). The data is then summed to form daily time series. Out of 150 cows
    used in the trial, only 146 cows were used in the analysis. Only data from July
    to December 2017 was included in this analysis. During this period, 26 animals
    were reported as lame (cows were checked for lameness by either the agricultural
    science student or by the farmer). Because the number of lame animals was small,
    splitting the data into training and testing folds was made in a such a way that
    atleast 75% of the lame animals was put in the training fold and the rest in the
    testing fold. This was a challenge as the dataset was imbalanced, but because
    this was a live experiment, we hoped to re-train the models after sometime. The
    performance on both the training and testing are reported in a later section.
    Fig. 6 gives a quick overview of the end-to-end pipeline of the developed solution
    illustrating: (1) data collection from sensors, (2) observation of the herd by
    an animal expert for locomotion scoring, (3) translating the human observer''s
    expertise into a machine learning based system leading to early detection of lameness
    in dairy cattle. Table 2 presents the locomotion scoring scale system used by
    the agricultural science student during animal observation. Download : Download
    high-res image (198KB) Download : Download full-size image Fig. 6. A diagrammatic
    representation of the end-to-end pipeline of the developed solution illustrating:
    (1) data collection from sensors, (2) observation of animals by an animal expert
    to give locomotion score, (3) translating the human observer''s expertise into
    a machine learning based system leading to early detection of lameness in cattle.
    Table 2. Locomotion scoring scale system used by the agricultural science student
    while observing cows. 4.2. Machine learning model and data analytics 4.2.1. Cow
    profiling In order to build robust profiles that are distinguishable by the learning
    model, it is important to understand how each test profile (lame and non-lame)
    relates to the rest of the herd. The most common approach would be to compare
    the activity level of lame and non-lame animals and investigate how these deviate
    from the mean of the entire herd. However, the mean can be affected by a single
    value being too high or low compared to the rest of the sample. This is why a
    median is sometimes taken as a better measure. Fig. 7 compares the mean and median
    of the herd. The results show that these almost trace out each other for all the
    three activities; lying time, step count and swaps. This is one of the features
    of a normal distribution, and therefore it would not matter whether the mean or
    median is used. Thus, we decided to use herd mean in our analysis. Download :
    Download high-res image (272KB) Download : Download full-size image Fig. 7. Comparing
    the Mean and Median of the various Animal Activities. Authors in (Stephenson and
    Bailey, 2017) have argued that animals grazing within the same pasture can influence
    the movement, grazing locations, and activities of other animals randomly, with
    attraction, or with avoidance; therefore most of the animals will have their activity
    levels equal to the herd mean. For this reason and the one discussed above, the
    herd mean was used as the baseline and any deviation from such behaviour due to
    lameness will be classified as an anomaly. It is also important to note that this
    will eliminate the effects of external factors as these will be affecting the
    whole herd and only leave the individual effects of lameness on the cow. We further
    define the Lame Activity Region (LAR) and the Normal Activity Region (NAR) as
    shown in Fig. 8. Once a cow is identified as lame, we compare the herd mean for
    all the activities to that cow''s activities and define a region , where is the
    day the activity starts to deviate from the herd activity mean, is the day that
    cow is identified as lame. As lameness is a transition, we ascertain that the
    cow will remain lame after that until it''s out of its lameness cycle. is the
    entire duration between and the days after until the cow is out of its lameness
    cycle. It''s the whole duration between to the last day when the cow was still
    lame. The values of , , and will vary for each cow as some may have longer lameness
    cycles than others, and also depending on when the cow is identified as lame.
    This is motivated by the fact that lameness is a transition from normal behaviour
    to lameness and back, it will probably start before it is seen and even continue
    after treatment until the cow becomes normal again. Once we define the LAR, the
    rest of the graph is treated as the NAR. Download : Download high-res image (172KB)
    Download : Download full-size image Fig. 8. Relationship between herd mean and
    cow activity for cow 2346. Normal Profile, To form the normal profile, we define
    a small window within NAR for each of the normal cows and calculate mean absolute
    deviation for a given period of time. (1) Here is the herd mean, is the cow activity
    and is the window size of NAR. Lame Profile To form the lame profile, we define
    a small window within LAR for each of the lame cows and calculate mean absolute
    deviation for a given period of time. (2) Here is the herd mean, is the cow activity
    and is the window size of LAR. The process is repeated for all the lame and non-lame
    cows. The results of this are plotted in a density distribution plot as shown
    in Fig. 9. Download : Download high-res image (82KB) Download : Download full-size
    image Fig. 9. Density distribution plot comparing the Normal and Lame profiles.
    Relationship between individual cows, Normal (Non-Lame) and Lame profiles To test
    the viability of the profiles, randomly chosen cows that have at least been identified
    as lame at some point during the experiment were used (these were not used to
    form the profiles above). The goal was to go back in time and see how these relate
    to the constructed profiles before they get lame, when they are lame and when
    they transition back to normal behaviour. Using Eq. (3), we define a window slice
    (the optimal number of days were chosen after a repetitive process) starting at
    the beginning of the experiment when the cow is not lame. We then slide through
    time as we calculate the Average Deviation (AD) within each for each of the cows,
    and for each of the activity. (3) Here is the herd mean within and is the cow
    activity. This is repeated as we slide the window. The results of this are plotted
    as density distribution and compared with Fig. 9. The three graphs (a, b and c)
    for both Fig. 10, Fig. 11 show periods of transition from normal to lameness for
    the two cows 2463 and 1469. In Figs. 10(a) and 11(a), both animals are non-lame
    and the distributions relate to the Normal cows profile distribution. In Figs.
    10(b) and 11(b) the distribution starts to shift to the right. Fig. 10(b) has
    two peaks. One relates more to the normal profile and the other to the lame profile.
    Fig. 11(b) on the other hand has one peak and this is mid way between both the
    normal and the lame profile. This kind of behaviour is justifiable because lameness
    is a transition. Perhaps this could be the best stage for the system to identify
    early lameness. Finally in Fig. 10(c) and 11(c), the distributions overlap with
    the lame profile distribution. It is important to note, even at this stage lameness
    is not yet visually detectable by the farmer for both cows. Download : Download
    high-res image (108KB) Download : Download full-size image Fig. 10. Comparing
    the distribution of cow 2463 against the normal and lame profile at three different
    stages. Download : Download high-res image (108KB) Download : Download full-size
    image Fig. 11. Comparing the distribution of cow 1469 against the normal and lame
    profile at three different stages. 4.2.2. Clustering From the above, it was discovered
    that not all animals behaved the same way. For example, some animals had their
    activity levels (step count, lying time and swaps) tracing out the herd mean,
    others with activity levels always higher than the herd mean and, the other category
    always lower than the herd mean. It''s also important to note that even when they
    became lame they had different activity levels depending on which category they
    belonged to. Therefore the clustering model is based on this observation. We set
    thresholds, and based on this we form three clusters. To define a cluster, we
    define a window of size days, and calculate MAD (Mean Absolute Deviation) between
    the cow activity and the herd mean for all the three activities. (4) Here is the
    herd mean within a defined window, is the cow activity for activity and is the
    window size. We varied the values of while testing the accuracy of the classification
    model and concluded that 14 days would be the optimal number of days to define
    a cluster. Based on MAD, we defined a threshold . Now based on this threshold,
    and the following criterion, we define three clusters. If any two of the activity
    levels are below a certain threshold, then that animal is assigned into one of
    the below clusters: Active These are animals in the herd that have activity levels
    always higher than the herd mean. These have the mean deviation of any two of
    the activities is greater than threshold . Normal These are animals in the herd
    that have activity levels always tracing out the herd mean. These have the mean
    deviation of any two of the activities is less than but great or equal to zero.
    Dormant These are animals in the herd that have activity levels always lower than
    the herd mean. These have the mean deviation of any two of the activities less
    than zero. The threshold was carefully chosen by a repetitive evaluation process,
    and was set to 1.7. The results presented in next section have been derived with
    value equals to 1.7. It’s also important to note that these clusters are dynamic,
    i.e., the animals keep changing the clusters they belong to. This can be caused
    by many factors like age and weather. So it is the role of the clustering model
    to keep regrouping the animals before selecting the appropriate classification
    model for that cluster (the best amount of time to re-cluster was found to be
    two weeks i.e., 14 days). Table 3 shows the distribution of the clusters as of
    writing of this article. The total number used to build clusters was 146 as three
    of the animals were eliminated due other health related issues and one animal
    lost the tag during the experiment. Table 3. Distribution of the clusters. Active
    Normal Dormant 25 109 12 4.2.3. Classification Classification algorithms are a
    family of machine learning algorithms that output a discrete value. The output
    variables are sometimes called labels or categories. These kind of problems always
    require the examples be classified into two or more classes. Classification problems
    with two labels are called binary classification problems while those with more
    than two are called multi-class. We formulated our problem as a binary class problem
    with Lame being the positive class and Non-lame as the negative class. In general,
    to solve these kind of tasks, the learning model is usually tasked to produce
    a function , where is the number of labels. For example, let denote the data set
    (feature, label), and the parameters, where: When , an input depicted by vector
    will be assigned to a class label identified by . It is important to note that
    there are other variants of functions . For example might output a probability
    distribution as opposed to a class label. At the time of writing, the feature
    matrix was made up 3 columns. Each of the columns is a feature. These were chosen
    because most literature suggests that they more representative of an animal transitioning
    to lameness or one that is already lame. The vector consists of labels and , where
    0 indicates non-lame and 1 otherwise. Fig. 12 shows the overall work flow and
    data flow of training and testing of the designed model. Download : Download high-res
    image (338KB) Download : Download full-size image Fig. 12. Designed hybrid machine
    learning model and work flow illustrating the steps in process of data collection,
    clustering, transformation, classification, training, evaluation and production
    mode. 5. Results, evaluation and discussion A brief demo-video of the developed
    system is available at Taneja et al. (2018a). Our initial work on age-based clustering
    of cows combined with data analytics to detect anomalies in their behaviour, and
    microservices based application design for integration of different services has
    been presented in Taneja et al., 2019a, Byabazaire et al., 2019, Taneja et al.,
    2018b respectively. 5.1. Rationale behind clustering In a study about association
    patterns of visually observed cattle, Stephenson et al. (2016) concluded that
    herds with 40 or less cows did not exhibit preferential or avoidance associations.
    This means that they lived together as a single group. In contrast, larger herd
    sizes (53–240 cows) tended to form associations with other cows stronger than
    what you would expect by chance. Therefore, the clustering step is only relevant
    to large herd sizes. Needless to mention, automated lameness solutions are meant
    for large herd sizes as it is assumed that for small ones, the farmer can visually
    inspect and keep track of the cows'' welfare easily. We compared the results of
    a one-size-fits-all model and a cluster specific models. Overall, cluster specific
    models reduced the classification error by 8% as compared to a one-size-fits-all
    model without clustering. For example, Fig. 13 shows an animal that was confirmed
    as lame from 03/12/2017 to 15/12/2017. The activity clustering based normal cluster
    model could correctly identify all the days the animal was lame, which has been
    illustrated in Fig. 13 using the highlighted red box. However, the one-size-fits-all
    model could only pick up some days as shown by the red points within the highlighted
    red box in Fig. 13. Download : Download high-res image (178KB) Download : Download
    full-size image Fig. 13. Animal confirmed as lame between 03/12/2017 and 15/12/2017
    but could not be correctly identified by a one-size-fits-all model. The highlighted
    red box shows that using activity clustering based normal cluster in the designed
    machine learning model, the system was able to detect animal as lame on 03/12/2017
    i.e., starting of the highlighted red box; whereas the one-size-fits-all system
    was able to pick only some days, shown as red points in the figure. (For interpretation
    of the references to colour in this figure legend, the reader is referred to the
    web version of this article.) 5.2. Early lameness detection assessment The problem
    was formulated as a binary classification problem with Lame as being the positive
    class and Non-lame as the negative class. These are denoted as ( ) for the negative
    class and ( ) for the positive class in the model diagram presented in Fig. 12.
    The training process reported in this study is unique from the previous approaches
    because it has a feedback loop added. After model validation, an agricultural
    expert or farmer re-annotates training data to improve the model accuracy. Please
    note that once the activity based clustering (Section 4.2.2) is done, and LAR
    and NAR have been defined (cow profiling – Section 4.2.1), we then calculate the
    euclidean distance between all the points on the herd-mean curve and cow activity
    curve within each of these regions. This gives us two sets and , where are the
    values from NAR and are the values from LAR, and these form the two classes that
    are used in machine learning element of the developed system. These two sets (
    and ) are fed into a K-NN machine learning classification model. We experimented
    on a number of sklearn (Pedregosa et al., 2011) classification algorithms ranging
    from Support Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbors (K-NN)
    and Decision Trees. We selected K-NN classification algorithm, as it was best
    balanced in terms of accuracy and early lameness detection window as shown in
    Table 4, Table 5. Table 4. Lameness detection accuracy of the developed system.
    Classification Model Accuracy (in %) Number of days before the visual signs of
    lameness appears Random Forest 91 1 K-Nearest Neighbors (K-NN) 87 3 Table 5. Different
    K-values, accuracy of the developed system and early detection window size. K-value
    Accuracy (in %) Number of days before the visual signs of lameness appear 2 91
    1 3 89 2 4 87 3 5 81 1 It is also important to note that although a different
    model was trained and built for each of the three clusters (i.e., three classification
    models – one for each cluster), results reported (performance and accuracy) in
    this study are only for the normal cluster. This is because it was not possible
    to efficiently evaluate the other two clusters as testing data in these was very
    small (i.e., imbalanced for a proper evaluation). K-NN This has a number of parameters
    that should be fine-tuned in order to achieve the desired results. Among these,
    we evaluated different K-values (2–5), which is the number of neighbours to consider
    while assigning the nearest class. We set the distance metric to minkowski. The
    highest accuracy was obtained with although this was over fitting the data. Table
    5 presents the accuracy of detection with different K-values, and also the length
    of the early detection window. Optimal results were obtained at which gave an
    accuracy of 87% with 3 days before the visual signs could be seen. In all, the
    normal cluster model had a sensitivity of 89.7% and specificity of 72.5%. Fig.
    15 shows some of the correct detections. One particular cow was confirmed as lame
    between 16/10/2017 and 25/10/2017 and the model could correctly classify all the
    days as shown by the red points in Fig. 15. Fig. 14 shows two cases where the
    model was able to detect a cow being lame 3 days before its visual clues were
    available to the farmer. The highlighted blue box shows the day when it was visually
    detected by the farmer or animal expert, and the start of the red points shows
    when the model detected the cow to be lame, and highlighted box shows the number
    of days for which the visual sign didn''t appear to be seen by the farmer or animal
    expert. Download : Download high-res image (1MB) Download : Download full-size
    image Fig. 14. Early detection of lameness by the developed model and late observation
    by farmer. Download : Download high-res image (170KB) Download : Download full-size
    image Fig. 15. Red points indicating lameness anomalies identified by the normal
    cluster model for cow ID 1988. (For interpretation of the references to colour
    in this figure legend, the reader is referred to the web version of this article.)
    5.3. Reduction in data transfer: fog-cloud data reduction Among the downsides
    of the existing approaches is that they are either fully cloud-centric in nature,
    i.e., all the data is sent to the cloud for processing and analysis; or have just
    farm premises based system which limits the accuracy and intelligence (Rutten
    et al., 2013) of such systems as there are no dynamic and frequent updates. In
    this work, we focused on the reduction of data exchanged between fog node and
    the cloud. We leveraged and utilized fog architecture in our work and were able
    to reduce data exchange between fog and cloud node from 10.1 MB to 1.61 MB on
    daily basis. Fig. 16 shows an 84% reduction in the amount of data that would otherwise
    have streamed to the cloud throughout the day. This aspect of data reduction becomes
    even more crucial while scaling up the farm and the herd, as the amount of data
    collected and streamed would then rapidly increase. Download : Download high-res
    image (85KB) Download : Download full-size image Fig. 16. Daily reduction in the
    amount of data between the fog node and the cloud. 6. Conclusion Our results suggest
    that building custom models for small groups of animals in the herd that share
    similar features within the herd improves the accuracy of the lameness detection
    as opposed to a one-size fits all approach. This approach becomes more important
    and practically viable with increase in size of the herd. Insights from our real
    world deployment suggest that activity based cluster specific models reduce the
    classification error of lameness detection by 8% as opposed to a one-size-fits-all
    approach. Using these clusters to then identify the anomalies in animal behaviour
    gives a better early detection. In our case, feeding the resultant cluster in
    K-NN (K-Nearest Neighbours) based classification models gives an accuracy of 87%
    with an early detection of 3 days window before any visual or clinical sign of
    lameness appears. It is because of this carefully blended design of clustering
    and classification model that results in a hybrid model for early lameness detection
    in dairy cattle. Further, the fog-based computational assistance enables the intelligent
    processing of data closer to the source, thereby leading to an 84% reduction in
    the amount of data transfer to cloud. Another key lesson learned is that any of
    the edge/fog/cloud resources of the overall architecture if considered in isolation
    would not be able to manage the developed IoT application, without compromising
    on functionalities or performance. And thus a careful coordination of edge, fog
    and cloud components is required as presented in this work. 7. Ongoing and future
    work To further validate the proposed approach for early lameness detection, we
    are expanding the work undertaken to date through the execution of a use case
    in the IoF2020 project3 named MELD4. The MELD project is building and expanding
    upon this existing work, and integrating it into the IoF2020 dairy farming technology
    trials with deployments in Portugal, Israel and South Africa. It aims to leverage
    sensor technologies from two different vendors on a combined total of approximately
    1000 cattle, consisting of both beef and dairy.5 In future work we also intend
    to investigate a more robust clustering technique as the current one is only based
    on threshold. Also, we plan to evaluate the other cluster models. Further, in
    context of efficient in-network resource utilization and increasing system resilience,
    one of the possible directions of future work is to look into distributed learning
    (Konecný et al.) and distributed data analytics (Taneja et al., 2019c, Chang et
    al., 2017) based approaches in such real-world IoT based deployments. Once the
    developed technology has been validated on a number of farms with different geographical
    and environmental settings, the goal is to roll out the technology to the vendors''
    customer base as an added feature through licensing. CRediT authorship contribution
    statement Mohit Taneja: Investigation, Software, Methodology, Data curation, Writing
    - original draft, Formal analysis, Validation. John Byabazaire: Investigation,
    Software, Methodology, Data curation, Formal analysis, Validation. Nikita Jalodia:
    Methodology, Writing - review & editing, Visualization. Alan Davy: Supervision,
    Funding acquisition, Conceptualization. Cristian Olariu: Resources, Formal analysis,
    Supervision. Paul Malone: Writing - review & editing, Project administration.
    Declaration of Competing Interest The authors declare that they have no known
    competing financial interests or personal relationships that could have appeared
    to influence the work reported in this paper. Acknowledgment This work has emanated
    from research conducted with the financial support of Science Foundation Ireland
    (SFI) and is co-funded under the European Regional Development Fund under Grant
    Number 13/RC/2077. Mohit Taneja is also supported by CISCO Research Gift Fund.
    The future work (MELD) is funded through the IoF2020 which has received funding
    from the European Union''s Horizon 2020 research and innovation programme under
    grant agreement no. 731884. References “Connected Cows”, 2015 “Connected Cows?”
    - Joseph Sirosh (Strata + Hadoop 2015) – YouTube. https://www.youtube.com/watch?v=oY0mxwySaSo
    (accessed on 04/29/2019). Google Scholar Ag, 2017 Ag, Q., Cattle health management,
    tags, and software | quantified ag. http://quantifiedag.com/ (accessed on 12/07/2017).
    Google Scholar AHDB, 2016 Dairy Cow Mobility and Lameness - AHDB Dairy. Accessed
    on 1st Nov 2016. URL https://dairy.ahdb.org.uk/technical-information/animal-health-welfare/lameness.
    Google Scholar Al-Fuqaha et al., 2015 A. Al-Fuqaha, M. Guizani, M. Mohammadi,
    M. Aledhari, M. Ayyash Internet of things: A survey on enabling technologies,
    protocols, and applications IEEE Commun. Surv. Tutor., 17 (4) (2015), pp. 2347-2376,
    10.1109/COMST.2015.2444095 View in ScopusGoogle Scholar Alsaaod et al., 2012 M.
    Alsaaod, C. Römer, J. Kleinmanns, K. Hendriksen, S. Rose-Meierhöfer, L. Plümer,
    W. Büscher Electronic detection of lameness in dairy cows through measuring pedometric
    activity and lying behavior Appl. Animal Behav. Sci., 142 (3–4) (2012), pp. 134-141,
    10.1016/j.applanim.2012.10.001 View PDFView articleView in ScopusGoogle Scholar
    Alsaaod et al., 2015 M. Alsaaod, J. Niederhauser, G. Beer, N. Zehner, G. Schuepbach-Regula,
    A. Steiner Development and validation of a novel pedometer algorithm to quantify
    extended characteristics of the locomotor behavior of dairy cows J. Dairy Sci.,
    98 (9) (2015), pp. 6236-6242 http://linkinghub.elsevier.com/retrieve/pii/S0022030215004609,
    10.3168/jds.2015-9657 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Amory et al., 2006 J. Amory, P. Kloosterman, Z. Barker, J. Wright, R. Blowey,
    L. Green Risk factors for reduced locomotion in dairy cattle on nineteen farms
    in The Netherlands J. Dairy Sci., 89 (5) (2006), pp. 1509-1515 http://linkinghub.elsevier.com/retrieve/pii/S0022030206722184,
    10.3168/jds.S0022-0302(06)72218-4 View PDFView articleView in ScopusGoogle Scholar
    Balalaie et al., 2016 A. Balalaie, A. Heydarnoori, P. Jamshidi Microservices Architecture
    Enables DevOps: Migration to a Cloud-Native Architecture (2016), 10.1109/MS.2016.64
    Google Scholar Barker et al., 2010 Z.E. Barker, K.A. Leach, H.R. Whay, N.J. Bell,
    D.C.J. Main Assessment of lameness prevalence and associated risk factors in dairy
    herds in England and Wales J. Dairy Sci., 93 (3) (2010), pp. 932-941, 10.3168/jds.2009-2309
    https://doi.org/10.3168/jds.2009-2309 View PDFView articleView in ScopusGoogle
    Scholar Blackie et al., 2011 N. Blackie, E. Bleach, J. Amory, J. Scaife Impact
    of lameness on gait characteristics and lying behaviour of zero grazed dairy cattle
    in early lactation Appl. Animal Behav. Sci., 129 (2–4) (2011), pp. 67-73, 10.1016/j.applanim.2010.10.006
    View PDFView articleView in ScopusGoogle Scholar Boumatic, 2017 Boumatic. https://boumatic.com/us_en/
    (accessed on 12/07/2017). Google Scholar Brewster et al., 2017 C. Brewster, I.
    Roussaki, N. Kalatzis, K. Doolin, K. Ellis Iot in agriculture: Designing a europe-wide
    large-scale pilot IEEE Commun. Mag., 55 (9) (2017), pp. 26-33, 10.1109/MCOM.2017.1600528
    View in ScopusGoogle Scholar Byabazaire et al., 2019 Byabazaire, J., Olariu, C.,
    Taneja, M., Davy, A., 2019. Lameness detection as a service: Application of machine
    learning to an internet of cattle. In: 2019 16th IEEE Annual Consumer Communications
    Networking Conference (CCNC), pp. 1–6. https://doi.org/10.1109/CCNC.2019.8651681.
    Google Scholar Caria et al., 2017 Caria, M., Schudrowitz, J., Jukan, A., Kemper,
    N., 2017. Smart farm computing systems for animal welfare monitoring. In: 2017
    40th International Convention on Information and Communication Technology, Electronics
    and Microelectronics (MIPRO), pp. 152–157. https://doi.org/10.23919/MIPRO.2017.7973408.
    Google Scholar Chang et al., 2017 Chang, T.-C., Zheng, L., Gorlatova, M., Gitau,
    C., Huang, C.-Y., Chiang, M., 2017. Decomposing data analytics in fog networks.
    In: Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems,
    SenSys ''17, ACM, New York, NY, USA, pp. 35:1--35:2. https://doi.org/10.1145/3131672.3136962.
    Google Scholar Chapinal et al., 2009 N. Chapinal, A. de Passillé, D. Weary, M.
    von Keyserlingk, J. Rushen Using gait score, walking speed, and lying behavior
    to detect hoof lesions in dairy cows J. Dairy Sci., 92 (9) (2009), pp. 4365-4374
    http://linkinghub.elsevier.com/retrieve/pii/S002203020970760X, 10.3168/jds.2009-2115
    View PDFView articleCrossRefView in ScopusGoogle Scholar Chapinal et al., 2010
    N. Chapinal, A. de Passillé, J. Rushen, S. Wagner Automated methods for detecting
    lameness and measuring analgesia in dairy cattle J. Dairy Sci., 93 (5) (2010),
    pp. 2007-2013 http://linkinghub.elsevier.com/retrieve/pii/S002203021000189X, 10.3168/jds.2009-2803
    View PDFView articleCrossRefView in ScopusGoogle Scholar Chapinal et al., 2011
    N. Chapinal, A. de Passillé, M. Pastell, L. Hänninen, L. Munksgaard, J. Rushen
    Measurement of acceleration while walking as an automated method for gait assessment
    in dairy cattle J. Dairy Sci., 94 (6) (2011), pp. 2895-2901 http://linkinghub.elsevier.com/retrieve/pii/S0022030211002773,
    10.3168/jds.2010-3882 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Chen et al., 2014 M.-C. Chen, C.-H. Chen, C.-Y. Siang Design of information system
    for milking dairy cattle and detection of mastitis Math. Probl. Eng., 2014 (2014),
    10.1155/2014/759019 Google Scholar Cook, 2003 N.B. Cook Prevalence of lameness
    among dairy cattle in Wisconsin as a function of housing type and stall surface
    J. Am. Vet. Med. Assoc., 223 (9) (2003), pp. 1324-1328, 10.2460/javma.2003.223.1324
    View in ScopusGoogle Scholar Espejo et al., 2006 L. Espejo, M. Endres, J. Salfer
    Prevalence of lameness in high-producing holstein cows housed in Freestall Barns
    in Minnesota J. Dairy Sci., 89 (8) (2006), pp. 3052-3058 http://linkinghub.elsevier.com/retrieve/pii/S0022030206725796,
    10.3168/jds.S0022-0302(06)72579-6 View PDFView articleView in ScopusGoogle Scholar
    Foditsch et al., 2016 C. Foditsch, G. Oikonomou, V.S. Machado, M.L. Bicalho, E.K.
    Ganda, S.F. Lima, R. Rossi, B.L. Ribeiro, A. Kussler, R.C. Bicalho Lameness prevalence
    and risk factors in large dairy farms in Upstate New York. Model development for
    the prediction of claw horn disruption lesions PLoS ONE, 11 (1) (2016), p. e0146718,
    10.1371/journal.pone.0146718 View in ScopusGoogle Scholar Ger, xxxx C. C. V. Ger,
    C. of XLVets), Economic cost of lameness in Irish dairy herds, Forage Nutrit.
    https://www.xlvets.ie/sites/xlvets.ie/files/press-article-files/XLVets%2520Article%2520Forage%2520Guide%25202012.pdf.
    Google Scholar Getting to know MQTT, 2017 Getting to know MQTT. https://www.ibm.com/developerworks/library/iot-mqtt-why-good-for-iot/index.html
    (last accessed on August 03, 2017). Google Scholar Hsu et al., 2018 T.-C. Hsu,
    H. Yang, Y.-C. Chung, C.-H. Hsu A creative iot agriculture platform for cloud
    fog computing Sustain. Comput. Inform. Syst. (2018) http://www.sciencedirect.com/science/article/pii/S2210537918303275,
    10.1016/j.suscom.2018.10.006 Google Scholar Ireland, 2017 Ireland, D., Milking
    parlours & machines, heat detection, scrapers, feeders, milk cooling - dairymaster
    Ireland. http://www.dairymaster.ie/ (accessed on 12/07/2017). Google Scholar Jukan
    et al., 2017 A. Jukan, X. Masip-Bruin, N. Amla Smart computing and sensing technologies
    for animal welfare: A systematic review ACM Comput. Surv., 50 (1) (2017), pp.
    10:1-10:27 http://doi.acm.org/10.1145/3041960, 10.1145/3041960 Google Scholar
    Konecný et al., xxxx Konecný, J., McMahan, H.B., Ramage, D., Richtárik, P., Federated
    optimization: Distributed machine learning for on-device intelligence, CoRR abs/1610.02527.
    arXiv:1610.02527. URL http://arxiv.org/abs/1610.02527. Google Scholar Lee and
    Kim, 2013 Lee, S., Kim, H., Hong, D.K., Ju, H., 2013. Correlation analysis of
    mqtt loss and delay according to qos level, in: The International Conference on
    Information Networking 2013 (ICOIN), pp. 714–717. https://doi.org/10.1109/ICOIN.2013.6496715.
    Google Scholar Meet the connected cow, xxxx Meet the “connected cow” | Financial
    Times. https://www.ft.com/content/2db7e742-7204-11e7-93ff-99f383b09ff9 (accessed
    on 04/29/2019). Google Scholar Miiaková et al., 2018 M. Miiaková, P. Strapák,
    I. Szencziová, E. Strapáková, O. Hanušovský Several methods of Estrus detection
    in cattle dams: a review Acta Universitatis Agriculturae et Silviculturae Mendelianae
    Brunensis, 66 (2) (2018), pp. 619-625 https://www.researchgate.net/publication/324902783_Several_Methods_of_Estrus_Detection_in_Cattle_Dams_A_Review
    https://acta.mendelu.cz/66/2/0619/, 10.11118/actaun201866020619 Google Scholar
    MQTT, 2017 MQTT. http://mqtt.org/ (last accessed on August 03, 2017). Google Scholar
    Muangprathub et al., 2019 J. Muangprathub, N. Boonnam, S. Kajornkasirat, N. Lekbangpong,
    A. Wanichsombat, P. Nillaor Iot and agriculture data analysis for smart farm Comput.
    Electron. Agric., 156 (2019), pp. 467-474 http://www.sciencedirect.com/science/article/pii/S0168169918308913,
    10.1016/j.compag.2018.12.011 View PDFView articleView in ScopusGoogle Scholar
    Munksgaard et al., 2006 L. Munksgaard, C.G. van Reenen, R. Boyce Automatic monitoring
    of lying, standing and walking behavior in dairy cattle Anim. Sci., 84 (suppl)
    (2006), p. 304 Google Scholar Neveux et al., 2006 S. Neveux, D. Weary, J. Rushen,
    M. von Keyserlingk, A. de Passillé Hoof discomfort changes how dairy cattle distribute
    their body weight J. Dairy Sci., 89 (7) (2006), pp. 2503-2509 http://linkinghub.elsevier.com/retrieve/pii/S0022030206723256,
    10.3168/jds.S0022-0302(06)72325-6 View PDFView articleView in ScopusGoogle Scholar
    O''Driscoll et al., 2008 K. O''Driscoll, L. Boyle, A. Hanlon A brief note on the
    validation of a system for recording lying behaviour in dairy cows Appl. Animal
    Behav. Sci., 111 (1–2) (2008), pp. 195-200, 10.1016/j.applanim.2007.05.014 View
    PDFView articleView in ScopusGoogle Scholar Pastell et al., 2010 M. Pastell, L.
    Hänninen, A. de Passillé, J. Rushen Measures of weight distribution of dairy cows
    to detect lameness and the presence of hoof lesions J. Dairy Sci., 93 (3) (2010),
    pp. 954-960 http://linkinghub.elsevier.com/retrieve/pii/S0022030210000627, 10.3168/jds.2009-2385
    View PDFView articleCrossRefView in ScopusGoogle Scholar Pedregosa et al., 2011
    F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
    Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
    M. Brucher, M. Perrot, E. Duchesnay Scikit-learn: Machine learning in Python J.
    Mach. Learn. Res., 12 (2011), pp. 2825-2830 Google Scholar Pouch, 2019 PouchDB.
    The JavaScript Database that Syncs! https://pouchdb.com/ (accessed on 05/02/2019).
    Google Scholar Poursaberi et al., 2010 A. Poursaberi, C. Bahr, A. Pluk, A. Van
    Nuffel, D. Berckmans Real-time automatic lameness detection based on back posture
    extraction in dairy cattle: Shape analysis of cow with image processing techniques
    Comput. Electron. Agric., 74 (1) (2010), pp. 110-119, 10.1016/j.compag.2010.07.004
    View PDFView articleView in ScopusGoogle Scholar Poursaberi et al., 2009 Poursaberi,
    A., Pluk, A., Bahr, C., Martens, W., Veermäe, I., Kokin, E., Praks, J., Poikalainen,
    V., Pastell, M., Ahokas, J., Van Nuffel, A., Vangeyte, J., Sonck, B., Berckmans,
    D., 2009. Image based separation of dairy cows for automatic lameness detection
    with a real time vision system. In: American Society of Agricultural and Biological
    Engineers Annual International Meeting 2009, ASABE 2009, vol. 7, pp. 4763–4773.
    https://doi.org/10.13031/2013.27258. URL http://www.scopus.com/inward/record.url?eid=2-s2.0-77649156361
    partnerID=tZOtx3y1. Google Scholar Poursaberi et al., 2011 Poursaberi, A., Bahr,
    C., Pluk, A., Berckmans, D., VeermÃďe, I., Kokin, E., Pokalainen, V., 2011. Online
    lameness detection in dairy cattle using body movement pattern (bmp). In: 2011
    11th International Conference on Intelligent Systems Design and Applications,
    pp. 732–736. https://doi.org/10.1109/ISDA.2011.6121743. Google Scholar Rutten
    et al., 2013 C. Rutten, A. Velthuis, W. Steeneveld, H. Hogeveen Invited review:
    Sensors to support health management on dairy farms J. Dairy Sci., 96 (4) (2013),
    pp. 1928-1952 http://www.sciencedirect.com/science/article/pii/S0022030213001409,
    10.3168/jds.2012-6107 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Shi et al., 2016 W. Shi, J. Cao, Q. Zhang, Y. Li, L. Xu Edge computing: Vision
    and challenges IEEE Internet Things J., 3 (5) (2016), pp. 637-646, 10.1109/JIOT.2016.2579198
    View in ScopusGoogle Scholar Stephenson and Bailey, 2017 M.B. Stephenson, D.W.
    Bailey movement patterns of gps-tracked cattle on extensive rangelands suggest
    independence among individuals? Agriculture, 7 (7) (2017) http://www.mdpi.com/2077-0472/7/7/58,
    10.3390/agriculture7070058 Google Scholar Stephenson et al., 2016 M.B. Stephenson,
    D.W. Bailey, D. Jensen Association patterns of visually-observed cattle on Montana,
    USA foothill rangelands Appl. Animal Behav. Sci., 178 (2016), pp. 7-15 http://www.sciencedirect.com/science/article/pii/S0168159116300442,
    10.1016/j.applanim.2016.02.007 View PDFView articleView in ScopusGoogle Scholar
    Taneja and Davy, 2017 Taneja, M., Davy, A., 2017. Resource aware placement of
    iot application modules in fog-cloud computing paradigm. In: 2017 IFIP/IEEE Symposium
    on Integrated Network and Service Management (IM), pp. 1222–1228. https://doi.org/10.23919/INM.2017.7987464.
    Google Scholar Taneja and Davy, 2016 M. Taneja, A. Davy Resource aware placement
    of data analytics platform in fog computing Proc. Comput. Sci., 97 (2016), pp.
    153-156 http://www.sciencedirect.com/science/article/pii/S1877050916321111, 10.1016/j.procs.2016.08.295
    View PDFView articleView in ScopusGoogle Scholar Taneja et al., 2018a Taneja,
    M., Jalodia, N., Byabazaire, J., Davy, A., Olariu, C., Smartherd-connected_cows_demo.mp4
    - google drive. https://drive.google.com/file/d/1QIrKSp8SkAZRRAFQDQHdPXu1TVYaVyv3/view
    (accessed on 10/04/2018). Google Scholar Taneja et al., 2018b Taneja, M., Byabazaire,
    J., Davy, A., Olariu, C., 2018. Fog assisted application support for animal behaviour
    analysis and health monitoring in dairy farming. In: 2018 IEEE 4th World Forum
    on Internet of Things (WF-IoT), pp. 819–824. https://doi.org/10.1109/WF-IoT.2018.8355141.
    Google Scholar Taneja et al., 2019b M. Taneja, N. Jalodia, P. Malone, J. Byabazaire,
    A. Davy, C. Olariu Connected cows: Utilizing fog and cloud analytics toward data-driven
    decisions for smart dairy farming IEEE Internet of Things Mag., 2 (4) (2019),
    pp. 32-37, 10.1109/IOTM.0001.1900045 Google Scholar Taneja et al., 2019a M. Taneja,
    N. Jalodia, J. Byabazaire, A. Davy, C. Olariu SmartHerd management: A microservices-based
    fog computing–assisted IoT platform towards data-driven smart dairy farming Software:
    Pract. Exp., 49 (7) (2019), pp. 1055-1078, 10.1002/spe.2704 https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.2704.
    In this issue View in ScopusGoogle Scholar Taneja et al., 2019c M. Taneja, N.
    Jalodia, A. Davy Distributed decomposed data analytics in fog enabled iot deployments
    IEEE Access, 7 (2019), pp. 40969-40981, 10.1109/ACCESS.2019.2907808 View in ScopusGoogle
    Scholar Taylor et al., 2013 K. Taylor, C. Griffith, L. Lefort, R. Gaire, M. Compton,
    T. Wark, D. Lamb, G. Falzon, M. Trotter Farming the web of things IEEE Intell.
    Syst., 28 (6) (2013), pp. 12-19, 10.1109/MIS.2013.102 Google Scholar Thorup et
    al., 2015 V.M. Thorup, L. Munksgaard, P.E. Robert, H.W. Erhard, P.T. Thomsen,
    N.C. Friggens Lameness detection via leg-mounted accelerometers on dairy cows
    on four commercial farms Animal, 9 (10) (2015), pp. 1704-1712, 10.1017/S1751731115000890
    View PDFView articleView in ScopusGoogle Scholar Van De Gucht et al., 2017 T.
    Van De Gucht, W. Saeys, A. Van Nuffel, L. Pluym, K. Piccart, L. Lauwers, J. Vangeyte,
    S. Van Weyenberg Farmers'' preferences for automatic lameness-detection systems
    in dairy cattle J. Dairy Sci., 100 (7) (2017), pp. 5746-5757 http://linkinghub.elsevier.com/retrieve/pii/S0022030217305027,
    10.3168/jds.2016-12285 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Van Hertem et al., 2013 T. Van Hertem, E. Maltz, A. Antler, C. Romanini, S. Viazzi,
    C. Bahr, A. Schlageter-Tello, C. Lokhorst, D. Berckmans, I. Halachmi Lameness
    detection based on multivariate continuous sensing of milk yield, rumination,
    and neck activity J. Dairy Sci., 96 (7) (2013), pp. 4286-4298 http://linkinghub.elsevier.com/retrieve/pii/S0022030213003561,
    10.3168/jds.2012-6188 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Van Nuffel et al., 2015 A. Van Nuffel, I. Zwertvaegher, L. Pluym, S. Van Weyenberg,
    V.M. Thorup, M. Pastell, B. Sonck, W. Saeys Lameness detection in dairy cows:
    Part 1. How to distinguish between non-lame and lame cows based on differences
    in locomotion or behavior Animals, 5 (3) (2015), pp. 838-860, 10.3390/ani5030387
    View in ScopusGoogle Scholar Van Nuffel et al., 2015 A. Van Nuffel, I. Zwertvaegher,
    S. Van Weyenberg, M. Pastell, V.M. Thorup, C. Bahr, B. Sonck, W. Saeys Lameness
    detection in dairy cows: Part 2. Use of sensors to automatically register changes
    in locomotion or behavior Animals (2015), 10.3390/ani5030388 Google Scholar Viazzi
    et al., 2013 S. Viazzi, C. Bahr, A. Schlageter-Tello, T. Van Hertem, C. Romanini,
    A. Pluk, I. Halachmi, C. Lokhorst, D. Berckmans Analysis of individual classification
    of lameness using automatic measurement of back posture in dairy cattle J. Dairy
    Sci., 96 (1) (2013), pp. 257-266 https://linkinghub.elsevier.com/retrieve/pii/S0022030212008466,
    10.3168/jds.2012-5806 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Viazzi et al., 2014 S. Viazzi, C. Bahr, T. Van Hertem, A. Schlageter-Tello, C.E.B.
    Romanini, I. Halachmi, C. Lokhorst, D. Berckmans Comparison of a three-dimensional
    and two-dimensional camera system for automated measurement of back posture in
    dairy cows Comput. Electron. Agric., 100 (2014), pp. 139-147, 10.1016/j.compag.2013.11.005
    View PDFView articleView in ScopusGoogle Scholar von Keyserlingk et al., 2012
    M. von Keyserlingk, A. Barrientos, K. Ito, E. Galo, D. Weary Benchmarking cow
    comfort on North American freestall dairies: Lameness, leg injuries, lying time,
    facility design, and management for high-producing Holstein dairy cows J. Dairy
    Sci., 95 (12) (2012), pp. 7399-7408 http://linkinghub.elsevier.com/retrieve/pii/S0022030212007606,
    10.3168/jds.2012-5807 View PDFView articleCrossRefView in ScopusGoogle Scholar
    Wark et al., 2009 T. Wark, D. Swain, C. Crossman, P. Valencia, G. Bishop-Hurley,
    R. Handcock Sensor and actuator networks: Protecting environmentally sensitive
    areas IEEE Pervas. Comput., 8 (1) (2009), pp. 30-36, 10.1109/MPRV.2009.15 View
    in ScopusGoogle Scholar Whay et al., 2003 H.R. Whay, D.C.J. Main, L.E. Green,
    A.J.F. Webster Assessment of the welfare of dairy caftle using animal-based measurements:
    direct observations and investigation of farm records Vet. Rec., 153 (7) (2003),
    pp. 197-202 http://veterinaryrecord.bmj.com/cgi/doi/10.1136/vr.153.7.197, 10.1136/vr.153.7.197
    CrossRefView in ScopusGoogle Scholar Yunta et al., 2012 C. Yunta, I. Guasch, A.
    Bach Short communication: Lying behavior of lactating dairy cows is influenced
    by lameness especially around feeding time J. Dairy Sci., 95 (11) (2012), pp.
    6546-6549 http://linkinghub.elsevier.com/retrieve/pii/S0022030212006261, 10.3168/jds.2012-5670
    View PDFView articleCrossRefView in ScopusGoogle Scholar Zamora-Izquierdo et al.,
    2019 M.A. Zamora-Izquierdo, J. Santa, J.A. Martínez, V. Martínez, A.F. Skarmeta
    Smart farming iot platform based on edge and cloud computing Biosyst. Eng., 177
    (2019), pp. 4-17 http://www.sciencedirect.com/science/article/pii/S1537511018301211,
    10.1016/j.biosystemseng.2018.10.014 View PDFView articleView in ScopusGoogle Scholar
    Zheleva et al., 2017 Zheleva, M., Bogdanov, P., Zois, D.-S., Xiong, W., Chandra,
    R., Kimball, M., 2017. Smallholder agriculture in the information age: Limits
    and opportunities. In: Proceedings of the 2017 Workshop on Computing Within Limits,
    LIMITS ''17, ACM, New York, NY, USA, pp. 59–70. https://doi.org/10.1145/3080556.3080563.
    URL http://doi.acm.org/10.1145/3080556.3080563. Google Scholar Cited by (0) Mohit
    Taneja is currently working as a software research engineer, and also as a PhD
    researcher with Emerging Networks Laboratory at Telecommunications Software and
    Systems Group of Waterford Institute of Technology, Ireland. He joined them in
    2015 as a Masters Student as a part of the Science Foundation Ireland (SFI) funded
    CONNECT Research Centre. He was a part of the SFI-CONNECT-IBM project termed SmartHerd,
    which has now been extended with follow-on IoF2020 project termed MELD, which
    is a project to detect early stage lameness in cattle with the help of technology.
    His research focuses on fog computing support for Internet of Things applications.
    His current research interests include Fog and Cloud Computing, Internet of Things
    (IoT), Distributed Systems, and Distributed Data Analytics. He received his Bachelor''s
    Degree in Computer Science and Engineering from The LNM Institute of Information
    Technology, Jaipur, India in 2015. John Byabazaire is currently a PhD student
    in school of computer science, University College Dublin working on IoT systems
    for data collection in precision agriculture. Before that, John pursed a MSc in
    Computer Science from Waterford Institute of Technology and due to graduate. He
    received his BSc in Computer Science from Gulu University in 2013. Alongside his
    current PhD study, John continues to research e-learning for low bandwidth environments,
    Software Defined Networking, Network Function Virtualization, and Remote Sensing,
    IoT (Internet of Things) and Fog Analytics. Nikita Jalodia is a PhD Researcher
    in the Department of Computing and Mathematics at the Emerging Networks Lab Research
    Unit in Telecommunications Software and Systems Group, Waterford Institute of
    Technology, Ireland. She is working as a part of the Science Foundation Ireland
    funded CONNECT Research Centre for Future Networks and Communications, and her
    research is based in Deep Learning and Neural Networks, NFV, Fog Computing and
    IoT. She received her Bachelor''s Degree in Computer Science and Engineering from
    The LNM Institute of Information Technology, Jaipur, India in 2017, with an additional
    diploma specialization in Big Data and Analytics with IBM. Alan Davy received
    the B.Sc. (with Hons.) degree in applied computing and the Ph.D. degree from the
    Waterford Institute of Technology, Waterford, Ireland, in 2002 and 2008, respectively.
    Since 2002, he has been with the Telecommunications Software and Systems Group,
    originally as a student and then, since 2008, as a Post-Doctoral Researcher. In
    2010, he was with IIT Madras, India, as an Assistant Professor, lecturing in network
    management systems. He was a recipient of the Marie Curie International Mobility
    Fellowship in 2010, which brought him to work at the Universitat Politecnica de
    Catalunya for two years. He is currently Head of Department of Computing and Mathematics
    in School of Science and Computing at Waterford Institute of Technology (WIT),
    Waterford, Ireland. Previously, he was Research Unit Manager of the Emerging Networks
    Laboratory with the Telecommunications Software Systems Group of WIT. He is coordinator
    of a number of national and EU projects such as TERAPOD. His current research
    interests include Virtualised Telecom Networks, Fog and Cloud Computing, Molecular
    Communications and TeraHertz Communication. Cristian Olariu received his B.Eng.
    degree in 2008 from the Faculty of Electronics and Telecommunications, Politehnica
    University of Timisoara, Romania, and his Ph.D. degree in 2013 WIT in VoIP over
    wireless and cellular networks. He was a research engineer with the Innovation
    Exchange, IBM Ireland, and is currently a research scientist with the Huawei Research
    Centre, Ireland. His interests are in computer networks optimisation, applied
    predictive modelling, anomaly detection, root cause analysis, deep neural nets,
    machine learning and AI. Paul Malone graduated from Waterford Institute of Technology
    with a 1st class honours degree in 1998 and completed a research MSc in 2001.
    Paul has worked on researching technologies and techniques related to IT security
    in the areas of distributed trust and reputation management and privacy and data
    protection controls. Paul is currently coordinating an Agri-Tech use case sub-project
    of the H2020 IoF2020.eu project applying machine learning technologies in detecting
    early stage lameness in cattle. 1 The ethical approval for the experimentation
    was taken from Research Ethics Committee of Waterford Institute of Technology,
    Ireland prior to the deployment in July 2017. 2 Note that our results presented
    in Taneja et al. (2019a) suggest that the system is fully capable to run with
    fog node of lower computational power without compromising on quality of service.
    The results on resource utilization at fog node and discussion on platform performance
    on using fog node with low level computational power, and system resilience has
    been discussed in greater detail in our work available at Taneja et al. (2019a).
    3 Internet of Food & Farm 2020, https://www.iof2020.eu/https://www.iof2020.eu/.
    4 MELD stands for Machine Learning based Early Lameness Detection in Beef and
    Dairy Cattle. 5 The collected data from the existing real-world deployment is
    available to be shared with the academic research community upon request. © 2020
    The Authors. Published by Elsevier B.V. Recommended articles Variable segmentation
    and ensemble classifiers for predicting dairy cow behaviour Biosystems Engineering,
    Volume 178, 2019, pp. 156-167 Manod L. Williams, …, Michael T. Rose View PDF Single-stream
    long-term optical flow convolution network for action recognition of lameness
    dairy cow Computers and Electronics in Agriculture, Volume 175, 2020, Article
    105536 Bo Jiang, …, Huaibo Song View PDF Short communication: Investigation of
    the temporal relationships between milk mid-infrared predicted biomarkers and
    lameness events in later lactation Journal of Dairy Science, Volume 103, Issue
    5, 2020, pp. 4475-4482 Axelle Mineur, …, Nicolas Gengler View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 75 Captures Readers: 255
    View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Machine learning based fog computing assisted data-driven approach for early
    lameness detection in dairy cattle
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 26 papers. The special focus in this conference
    is on Future Data and Security Engineering. The topics include: A Comparative
    Study of Join Algorithms in Spark; blockchain-Based Forward and Reverse Supply
    Chains for E-waste Management; a Pragmatic Blockchain Based Solution for Managing
    Provenance and Characteristics in the Open Data Context; OAK: Ontology-Based Knowledge
    Map Model for Digital Agriculture; A Novel Approach to Diagnose ADHD Using Virtual
    Reality; a Three-Way Energy Efficient Authentication Protocol Using Bluetooth
    Low Energy; clustering-Based Deep Autoencoders for Network Anomaly Detection;
    flexible Platform for Integration, Collection, and Analysis of Social Media for
    Open Data Providers in Smart Cities; post-quantum Digital-Signature Algorithms
    on Finite 6-Dimensional Non-commutative Algebras; data Quality for Medical Data
    Lakelands; malicious-Traffic Classification Using Deep Learning with Packet Bytes
    and Arrival Time; detecting Malware Based on Dynamic Analysis Techniques Using
    Deep Graph Learning; understanding the Decision of Machine Learning Based Intrusion
    Detection Systems; combining Support Vector Machines for Classifying Fingerprint
    Images; toward an Ontology for Improving Process Flexibility; sentential Semantic
    Dependency Parsing for Vietnamese; An In-depth Analysis of OCR Errors for Unconstrained
    Vietnamese Handwriting; authorization Policy Extension for Graph Databases; A
    Model-Driven Approach for Enforcing Fine-Grained Access Control for SQL Queries;
    on Applying Graph Database Time Models for Security Log Analysis; integrating
    Web Services in Smart Devices Using Information Platform Based on Fog Computing
    Model; adaptive Contiguous Scheduling for Data Aggregation in Multichannel Wireless
    Sensor Networks; relating Network-Diameter and Network-Minimum-Degree for Distributed
    Function Computation; growing Self-Organizing Maps for Metagenomic Visualizations
    Supporting Disease Classification.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 7th International Conference on Future Data and Security Engineering, FDSE
    2020
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Baghrous M.
  - Ezzouhairi A.
  - Benamar N.
  citation_count: '5'
  description: Nowadays, the increased agricultural production at a lower cost is
    more and more driven by the Internet of Things (IoT) and the cloud computing paradigms.
    Many researches and projects have been elaborated so far in this context. It aims
    at reducing human efforts as well as resources and power consumption. Such projects
    are mainly based on collecting various data pertaining to the agricultural area
    and sending it to the cloud for further analysis. However, the long distance between
    sensors/actuators and the cloud leads to a significant increase in latency, which
    leads to a decrease of performances of the irrigation systems, pesticide monitoring,
    etc. This paper presents an alternative solution based on Fog Nodes and LoRa technology
    to optimize the number of nodes deployment in smart farms. Our proposed solution
    reduces the total latency induced during data transmission toward the cloud for
    processing.
  doi: 10.1007/978-981-15-0947-6_21
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Embedded Systems and Artificial
    Intelligence pp 217–225Cite as Home Embedded Systems and Artificial Intelligence
    Conference paper Smart Farming System Based on Fog Computing and LoRa Technology
    Mohamed Baghrous, Abdellatif Ezzouhairi & Nabil Benamar  Conference paper First
    Online: 08 April 2020 1489 Accesses 3 Citations Part of the book series: Advances
    in Intelligent Systems and Computing ((AISC,volume 1076)) Abstract Nowadays, the
    increased agricultural production at a lower cost is more and more driven by the
    Internet of Things (IoT) and the cloud computing paradigms. Many researches and
    projects have been elaborated so far in this context. It aims at reducing human
    efforts as well as resources and power consumption. Such projects are mainly based
    on collecting various data pertaining to the agricultural area and sending it
    to the cloud for further analysis. However, the long distance between sensors/actuators
    and the cloud leads to a significant increase in latency, which leads to a decrease
    of performances of the irrigation systems, pesticide monitoring, etc. This paper
    presents an alternative solution based on Fog Nodes and LoRa technology to optimize
    the number of nodes deployment in smart farms. Our proposed solution reduces the
    total latency induced during data transmission toward the cloud for processing.
    Access provided by University of Nebraska-Lincoln. Download conference paper PDF
    1 Introduction In Morocco, the agriculture is considered as a vital sector, employing
    about four million people and contributing about 14% of the GDP [1]. Nevertheless,
    this sector is facing tremendous challenges such as climate change and water scarcity.
    In these circumstances, the adoption of IoT in the Moroccan agricultural sector
    can certainly help to face these various challenges as well as improving the agricultural
    income. On the other hand, it is supposed to reduce considerably farmer’s physical
    effort, water and energy consumption. Actually, smart farming (SF) is widely adopted
    in USA, China and in many leading countries in this area. SF involves many tools
    (sensors, robots, etc), which makes farms as an important source of data. In general,
    SF functionalities are categorized into the following phases: (1) data collection
    by sensors; (2) processing, store and analyzing this data at cloud and (3) feedback
    and taking actions by actuators. Transferring data to the cloud for processing
    takes time and resources, and the delays are not always acceptable in latency-sensitive
    systems like autonomous drones/robots, fire detection system and livestock control.
    On the other hand, most of the wireless sensor communication technologies, such
    as those mentioned in Table 1, cover only a short range which demands additional
    equipments, especially in the vast farms. The rest of this paper is organized
    as follows: literature review, background, system architecture based on Fog and
    LoRa technology, system simulation and conclusion. Table 1 LoRa versus other wireless
    technologies Full size table 2 Literature Review In the last few years, many researchers
    have addressed the issues relevant to SF. Indeed, in [2], the authors proposed
    an intelligent farming (IF) system that uses the concepts of IoT and SF to help
    farmers to monitor and sense useful information from their farms in order to help
    in the quality improvement and product quantity. In [3], the proposed model is
    architecture of IoT sensors that collect data and send it over the WiFi network
    to the cloud server; their server can take actions depending on the treated information.
    The work published in [4] proposed a greenhouse monitoring system based on agriculture
    IoT with a cloud platform; the parameters that are collected by a network of sensors
    are being logged and stored online based on a cloud computing platform. In [5],
    authors proposed an animal behavior monitoring platform; based on IoT, it includes
    an IoT local network to gather data from animals and a cloud platform with processing
    and storage capabilities. All of these systems and others mentioned in a survey
    in [6] are mainly based on the cloud which is considered as a promising solution
    for IoT agricultural applications. However, it still has some limitations. The
    fundamental limitation is the connectivity between the cloud and the SF devices.
    The distance between the cloud and the end devices might be an issue for latency-sensitive
    applications such as autonomous robots used for agricultural purposes [7], fire
    detection, drones and smart surveillance system. This latency is unavoidable due
    to the large number of hops that a packet has to cross to reach the cloud. Furthermore,
    most of the aforementioned proposals are based on short-range communication using
    ZigBee or WiFi which lacks to ensure coverage of wide agriculture areas. To overcome
    these issues, we propose Fog Nodes for agricultural purposes based on Fog Computing
    paradigm and LoRa technology. 3 Background 3.1 Cloud Computing Cloud computing
    has been used as an efficient way to process data because of its high computation
    power and storage capability [8]. However, as cloud computing paradigm is a centralized
    computing model, most of the computations happen in the cloud. This means that
    all the data produced in smart farms needs to be transmitted to the cloud. Although
    the data processing speed has risen rapidly, the network bandwidth has not increased
    appreciably. This may result in long latency. 3.2 Fog Computing Fog Computing
    paradigm was introduced by Cisco Systems in 2012, and in its initial definition,
    it was considered as an extension of the cloud computing paradigm that provides
    computation, storage and networking services between sensors and traditional cloud
    servers [9]. Fog aims to bring networking resources near source of data (the nodes
    that are generating data). Because of their proximity to the source of the data
    compared to the cloud data centers, Fog Computing has the potential to offer services
    that deliver better delay performance. 3.3 Lora Technology As the name implies,
    long range (LoRa) is a long-range wireless communications system, promoted by
    the LoRa Alliance. It is capable of transmitting data over long distances. This
    system aims at being usable in long-lived battery-powered devices, where the energy
    consumption is very important [10]. The payload of each transmission can range
    from 2 to 255 octets, and the data rate can reach up to 50 Kbps when channel aggregation
    is employed. The LoRa modulation has been patented by Semtech Corporation [11].
    Table 1 compares some parameters including power consumption, transfer rate, transmission
    range and cost between some popular wireless technologies used in the above systems.
    Accordingly, LoRa has shown its superiority in many aspects. Its only weakness
    is the data rate. However, in wireless sensor network applications, this is not
    an issue. A. LoRa parameters In LoRa technology, there are four main types of
    parameters to consider [12]: RF Transmission Power (TxP): RF Txp is the maximum
    power that can be selected to transmit the packets on air. TxP on a LoRa radio
    can be adjusted from −4 to 20 dBm, but because of limited hardware implementation,
    the range is often limited to 10 dBm on 433 MHz and 14 dBm in 868 MHz. Carrier
    Frequency (CF): The frequency at which the LoRa module transmits at CF in different
    channels the LoRa module operates in two bands 863–870 MHz and 433.050–434.790
    MHz. Spreading Factor: In LoRa modulation, a chirp signal that continuously varies
    in frequency is used to spread the spectrum of the original data. In this case,
    the time and frequency offset between the transmitter and receiver are same. As
    well as the frequency bandwidth of this chirp and the spectral bandwidth of the
    signal are same. Bandwidth (BW): The radio bandwidth of the LoRa module takes
    any of the three values 125, 250 and 500 kHz. B. Time on the air of data packet
    For LoRa, the actual time on the air for a packet can be defined as [12]: $$ T_{\\text{packet}}
    = T_{\\text{preamble}} + T_{\\text{payload}} $$ (1) where \\( T_{\\text{preamble}}
    \\) is transmission time of preamble field and \\( T_{\\text{payload}} \\) is
    transmission time of the payload field. \\( T_{\\text{preamble}} \\) is defined
    as: $$ T_{\\text{preamble}} = \\left( {n_{\\text{preamble}} + 4.25} \\right)T_{\\text{sym}}
    $$ (2) The payload transmission time is: $$ T_{\\text{payload}} = n_{\\text{preamble}}
    *T_{\\text{sym}} $$ (3) where \\( n_{\\text{preamble}} \\) is: $$ n_{\\text{preamble}}
    = 8 + { \\hbox{max} }\\left( {A*\\left( {{\\text{CR}} + 4} \\right),0} \\right)
    $$ (4) And, A is defined as: $$ A = \\frac{{\\left( {8{\\text{PL}} - 4{\\text{SF}}
    + 28 + 16{\\text{CRC}} - 20{\\text{IH}}} \\right)}}{{4\\left( {{\\text{SF}} -
    2{\\text{DE}}} \\right)}} $$ (5) With the following dependencies: PL: number of
    bytes in payload field. IH: 0 when the implicit header mode is used and 1 for
    vice versa. DE: 1 for enabled low data rate optimization and 0 for opposite case.
    CR: code rate. So, using 1, 2 and 3, the total time on the air $$ T_{\\text{packet}}
    = (n_{\\text{preamble}} + n_{\\text{payload}} + 4.25) T_{\\text{sym}} $$ (6) 4
    Proposed System Architecture In this section, we introduce a SF System based on
    a Fog Nodes with LoRa technology. As shown in Fig. 1, these Fog Nodes are installed
    in a strategy place allowing it to cover a large number of sensors and actuators.
    The main role of the proposed nodes is not only to build a bridge between the
    low-level sensors and the network, but also it is considered as a local server
    equipped with LoRa module and located near the source of the data. It handles
    data collected from sensors they are connected to. As mentioned above, a range
    of time-sensitive tools and technologies (autonomous drones, robots, etc.) has
    been adopted in smart farms which need data processing in the real time. So, these
    proposed Fog Nodes are the ideal solution for providing less latency service to
    SF time-sensitive subsystems. On the other hand, each agricultural area may contain
    many smart farms and each farm has an area of hundreds of hectares. The proposed
    Fog Nodes cover a large area because of its LoRa unit, allowing it to handle a
    large number of sensors and actuators, thus reducing the investment in communications
    nodes. Fig. 1 Proposed system architecture based on fog and LoRa technology Full
    size image The following are some advantages of the proposed system: Low latency:
    In our proposed architecture, Fog Nodes acquire the data generated by sensors,
    process and store this data locally. It significantly reduces data movement across
    the Internet to the cloud. Therefore, it enables low latency and meets the demand
    of real-time processing, especially for latency-sensitive applications such as
    autonomous drones/robots for agricultural purposes. Save bandwidth: Some computation
    tasks, for example, data preprocessing, data filtering are performed locally at
    Fog Nodes near the source of the data. Fog Nodes transmitted only the useful data
    to the cloud. In this way, the proposed system will save the bandwidth effectively
    and reducing the cost. Low energy consumption: The proposed system is based on
    LoRa technology which combines long-range connectivity with a considerable increase
    of the battery lifetime at a low-cost using sub-GHz unlicensed spectrum. In addition,
    there are many researches that have proven that energy consumption for Fog Computing
    architecture is 40.48% less than the traditional cloud computing model [13]. So
    the combination of Fog Computing and LoRa technology in SF applications will lead
    to reduce power consumption and decrease the cost. Data security: Compared to
    [2], [3] and [4] which are mainly based on the cloud computing, data in our proposed
    system is transmitted in a limited range between sensors/actuators and Fog Nodes,
    it is sent to the cloud over the Internet only when necessary. Therefore, data
    is protected from attacks that may adversely affect the performance of various
    systems used in smart farms (Irrigation systems, surveillance systems, etc.).
    5 Simulation and Results We have performed the simulation with the iFogSim simulator
    based on the network topology that is shown in Fig. 2. It consists of three levels:
    Cloud level, fog level and sensors/actuators level. In fog level, Fog Nodes (Agri-Fog-Nodes
    and Agri-Areas) handle with data generated by sensors distributed in smart farms.
    These nodes configured to be servers close to sensors. Fig. 2 Simulation network
    topology Full size image Figure 3 presents the average latency in millisecond
    as a function of different configurations. The graph shows clearly that latency
    is much lower when application modules are executed on the fog. This will certainly
    enhance real-time processing. Therefore, it will improve the performance of time-sensitive
    applications in smart farms. Fig. 3 Average latency of control loop Full size
    image In any IoT application, more the network is used, more the overhead increases
    too. Figure 4 illustrates the behavior of network usage in both cloud and fog
    paradigms. The obtained results clearly show that network overhead in cloud architecture
    is considerably high compared to the fog. This result is be explained by the fact
    that data processing is performed locally at the fog nodes rather than the cloud
    located in the network. Fig. 4 Network usage Full size image 6 Conclusion Cloud
    computing is surely an interesting solution for many SF applications. However,
    it became clear that the present cloud architecture will not be able to handle
    a great amount of data collected through thousands of sensors across the network.
    Fog Computing came to mitigate the shortcomings of the cloud computing paradigm
    within the IoT environment by bringing network, processing and storage resources
    closer to the source of data. Hence, it paves the way to meet their hard constraints
    and offloading much of their data to the fog layer. The proposed architecture
    is considered as an efficient solution to save bandwidth and reduce latency. LoRa
    technology offers compelling features for SF application including long range,
    low power consumption and secure data transmission. This paper has presented SF
    System based on Fog Computing paradigm and LoRa technology. The results showed
    that the adoption of Fog Computing paradigm in the context of SF will improve
    the real-time processing, reduce time response and save bandwidth. Moreover, the
    use of LoRa technology in the communication between nodes will ensure efficient
    power management and large-scale coverage. References Department of Agriculture
    of Morocco. http://www.agriculture.gov.ma/. Accessed 03 May 2018 Putjaika, N.,
    Phusae, S., Chen, I.: A control system in an intelligent farming by using arduino.
    In: IEEE Fifth ICT International Conference, pp. 53–56 (2016) Google Scholar   Dagar,
    R., Som, S., Khatri, S.: Smart farming—IoT in agriculture. In: IEEE International
    Conference on Inventive Research in Computing Applications, pp. 1052–1056 (2018)
    Google Scholar   Keerthi, V., Kodandaramaiah, N.: Cloud IoT based greenhouse monitoring
    system. In: Int. J. Eng. Res. Appl. 35–41 (2015) Google Scholar   Nóbrega, L.,
    Tavare, A., Cardoso, A., Gonçalves, P.: Animal monitoring based on IoT technologies.
    In: IEEE IoT Vertical and Topica Summit on Agriculture, pp. 1–5 (2018) Google
    Scholar   Shareef, M., Viswanathan, P.: A Survey: smart agriculture IoT with cloud
    computing. In: IEEE International conference on Microelectronic Devices (ICMDCS),
    pp. 1–7 (2017) Google Scholar   Ball, D., Ross, P., English, A., Milani, P., Richards,
    D., Bate, A., Upcroft, B., Wyeth, G., Corke, P.: Farm workers of the future: vision-based
    robotics for broad-acre agriculture In: IEEE Robotics and Automation Magazine,
    pp. 97–107 (2017) Google Scholar   Mohiddin S., Babu, S., Sharmila, S.: A complete
    ontological survey of CLOUD forensic in the area of cloud computing. In: Springer-Sixth
    International Conference on Soft Computing, pp. 38–47 (2017) Google Scholar   Linthicim,
    S.: Connecting fog and cloud computing. IEEE J. Mag. 18–20 (2017) Google Scholar   Sasián,
    F., Gachet, D., Buenaga, M., Aparicio, F.: A dictionary based protocol over LoRa
    technology for applications in internet of things. In: International Conference
    on Ubiquitous Computing and Ambient Intelligence, pp. 140–148 (2017) Google Scholar   Semtech:
    LoRa Technology. https://www.semtech.com/. Accessed 15 Jan 2019 Semtech-Corporation.
    https://www.Lorasx1276/77/78/79datasheet/. Accessed 19 Jan 2019 Fatima, I., Javaid,
    N., Iqbal, M., Shafi, I., Anjum, A., Memon, U.: Integration of cloud and fog based
    environment for effective resource distribution in smart buildings. In: 14th IEEE
    International Wireless Communications and Mobile Computing Conference, pp. 60–64
    (2018) Google Scholar   Download references Author information Authors and Affiliations
    Renewable Energies and Intelligent Systems Laboratory, Faculty of Science and
    Technology, Sidi Mohammed Ben Abdellah University, Fez, Morocco Mohamed Baghrous
    National School of Applied Sciences, Sidi Mohamed Ben Abdellah University, Fez,
    Morocco Abdellatif Ezzouhairi School of Technology, University Moulay Ismaïl,
    Meknes, Morocco Nabil Benamar Corresponding author Correspondence to Mohamed Baghrous
    . Editor information Editors and Affiliations Department of Electronics and Communication
    Engineering, Shri Ramswaroop Memorial Group of Professional Colleges (SRMGPC),
    Lucknow, Uttar Pradesh, India Vikrant Bhateja School of Computer Engineering,
    Kalinga Institute of Industrial Technology (KIIT), Bhubaneswar, Odisha, India
    Suresh Chandra Satapathy Department of Computer Sciences, Faculty of Sciences
    Dhar Mahraz, Sidi Mohammed Ben Abbdallah University, Fez, Morocco Hassan Satori
    Rights and permissions Reprints and permissions Copyright information © 2020 Springer
    Nature Singapore Pte Ltd. About this paper Cite this paper Baghrous, M., Ezzouhairi,
    A., Benamar, N. (2020). Smart Farming System Based on Fog Computing and LoRa Technology.
    In: Bhateja, V., Satapathy, S., Satori, H. (eds) Embedded Systems and Artificial
    Intelligence. Advances in Intelligent Systems and Computing, vol 1076. Springer,
    Singapore. https://doi.org/10.1007/978-981-15-0947-6_21 Download citation .RIS.ENW.BIB
    DOI https://doi.org/10.1007/978-981-15-0947-6_21 Published 08 April 2020 Publisher
    Name Springer, Singapore Print ISBN 978-981-15-0946-9 Online ISBN 978-981-15-0947-6
    eBook Packages Intelligent Technologies and Robotics Intelligent Technologies
    and Robotics (R0) Share this paper Anyone you share the following link with will
    be able to read this content: Get shareable link Provided by the Springer Nature
    SharedIt content-sharing initiative Publish with us Policies and ethics Download
    book PDF Download book EPUB Sections Figures References Abstract Introduction
    Literature Review Background Proposed System Architecture Simulation and Results
    Conclusion References Author information Editor information Rights and permissions
    Copyright information About this paper Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Advances in Intelligent Systems and Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Smart Farming System Based on Fog Computing and LoRa Technology
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nguyen Gia T.
  - Qingqing L.
  - Pena Queralta J.
  - Zou Z.
  - Tenhunen H.
  - Westerlund T.
  citation_count: '58'
  description: The agricultural and farming industries have been widely influenced
    by the disruption of the Internet of Things. The impact of the IoT is more limited
    in countries with less penetration of mobile internet such as sub-Saharan countries,
    where agriculture commonly accounts for 10 to 50% of their GPD. The boom of low-power
    wide-area networks (LPWAN) in the last decade, with technologies such as LoRa
    or NB-IoT, has mitigated this providing a relatively cheap infrastructure that
    enables low-power and long-range transmissions. Nonetheless, the benefits that
    LPWAN technologies enable have the disadvantage of low-bandwidth transmissions.
    Therefore, the integration of Edge and Fog computing, moving data analytics and
    compression near end devices, is key in order to extend functionality. By integrating
    artificial intelligence at the local network layer, or Edge AI, we present a system
    architecture and implementation that expands the possibilities of smart agriculture
    and farming applications with Edge and Fog computing and LPWAN technology for
    large area coverage. We propose and implement a system consisting on a sensor
    node, an Edge gateway, LoRa repeaters, Fog gateway, cloud servers and end-user
    terminal application. At the Edge layer, we propose the implementation of a CNN-based
    image compression method in order to send in a single message information about
    hundreds or thousands of sensor nodes within the gateway's range. We use advanced
    compression techniques to reduce the size of data up to 67% with a decompression
    error below 5%, within a novel scheme for IoT data.
  doi: 10.1109/AFRICON46755.2019.9134049
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 IEEE AFRICON Edge AI in Smart
    Farming IoT: CNNs at the Edge and Fog Computing with LoRa Publisher: IEEE Cite
    This PDF T. Nguyen Gia; L. Qingqing; J. Peña Queralta; Z. Zou; H. Tenhunen; T.
    Westerlund All Authors 45 Cites in Papers 1354 Full Text Views Abstract Document
    Sections I. Introduction II. Related Work III. System Architecture IV. Implementation
    and Results V. Conclusion and Future Work Authors Figures References Citations
    Keywords Metrics Abstract: The agricultural and farming industries have been widely
    influenced by the disruption of the Internet of Things. The impact of the IoT
    is more limited in countries with less penetration of mobile internet such as
    sub-Saharan countries, where agriculture commonly accounts for 10 to 50% of their
    GPD. The boom of low-power wide-area networks (LPWAN) in the last decade, with
    technologies such as LoRa or NB-IoT, has mitigated this providing a relatively
    cheap infrastructure that enables low-power and long-range transmissions. Nonetheless,
    the benefits that LPWAN technologies enable have the disadvantage of low-bandwidth
    transmissions. Therefore, the integration of Edge and Fog computing, moving data
    analytics and compression near end devices, is key in order to extend functionality.
    By integrating artificial intelligence at the local network layer, or Edge AI,
    we present a system architecture and implementation that expands the possibilities
    of smart agriculture and farming applications with Edge and Fog computing and
    LPWAN technology for large area coverage. We propose and implement a system consisting
    on a sensor node, an Edge gateway, LoRa repeaters, Fog gateway, cloud servers
    and end-user terminal application. At the Edge layer, we propose the implementation
    of a CNN-based image compression method in order to send in a single message information
    about hundreds or thousands of sensor nodes within the gateway''s range. We use
    advanced compression techniques to reduce the size of data up to 67% with a decompression
    error below 5%, within a novel scheme for IoT data. Published in: 2019 IEEE AFRICON
    Date of Conference: 25-27 September 2019 Date Added to IEEE Xplore: 07 July 2020
    ISBN Information: ISSN Information: DOI: 10.1109/AFRICON46755.2019.9134049 Publisher:
    IEEE Conference Location: Accra, Ghana SECTION I. Introduction In order to perform
    activities such as watering or fertilizing, farmers need to visit their plants
    frequently (e.g., every day or every few days depending on the plant and trees).
    In some cases, farmers need to stay close to their remote farms in order to protect
    the crop and their resources. When the farmed areas are large, it becomes increasingly
    difficult and more human resources are required to perform these tasks. This can
    cause a significant increase in operational costs with a limited impact on productivity.
    In the era of the Internet of Things (IoT), a solution is to deploy remote monitoring
    and management systems for these remote farms. Farms adopting IoT technologies
    are often referred to as smart farms. However, remote farm monitoring and controlling
    have limited impact in areas with unstable or poor Internet connectivity. This
    occurs not only in developing but also in developed countries [1]. The Internet
    of Things (IoT) can be defined as a platform where virtual and physical objects
    are interconnected and communicate with each other. IoT systems consist of different
    technologies such as wireless sensor networks, cloud computing and embedded intelligence.
    These systems offer advanced services such as real-time remote monitoring, online
    analytics and remote management. IoT is applied in many remote monitoring applications
    in vast domains from healthcare to smart factories, and including smart homes,
    smart cities and smart farming, improving productivity and reducing costs [2]–[6].
    Some of the benefits of the IoT can be utilized to improve the quality of services
    for automated and remote farming systems. However, relying only on traditional
    cloud-centric IoT architectures for remote farm monitoring and management cannot
    guarantee that the systems work properly because the IoT still presents several
    challenges. For instance, cloud-centric IoT applications cannot be deployed in
    remote areas where the Internet is not stable or coverage is limited. In such
    cases, data cannot be real-time monitored and actions toward abnormalities might
    not be executed on time. For instance, if a sudden fire occurs or a group of wild
    animals raid the crops, the system cannot react on time. Edge and Fog computing
    can be illustrated as a mini cloud which is closer to the edge of the network.
    In other words, Edge and Fog computing represent the convergence of different
    network layers into interconnected smart gateways. Edge and Fog computing can
    help to overcome some limitations of the traditional could-centric IoT systems.
    For instance, Edge and Fog computing offer many advantages such as energy efficiency,
    distributed local storage, interoperability and enhanced security. In more detail,
    Edge and Fog computing can help to reduce the network load and the computational
    and storage burden of cloud servers. This is done by moving many computationally
    intensive processes from the cloud to the Edge and Fog layers and gateways, at
    the same time enabling more power-efficient sensor nodes as they rely more on
    local network smart gateways. Compared to traditional IoT applications which often
    rely on a 3-layer architecture (sensor-cloud-terminal), a Fog-assisted IoT application
    has extra layers between the sensor nodes and the cloud. Depending on the application
    and type of acquired data, a different number of Edge/Fog layers can be deployed
    [7]–[9]. Even though Edge and Fog computing can provide many advanced services,
    Fog-based systems still cannot work properly in remote areas where the Internet
    is not stable or covered, as they often rely on high-speed local networks for
    real-time processing and latency-critical applications. A solution in these scenarios
    is to deploy low-power wide-area network technologies, such as LoRa, which enable
    long-range transmissions with the drawback of reduced data rates. LoRa is one
    of the most popular LPWAN protocols for the physical layer [10], offering low-power
    and long-range communication up to 10 or 20 km in open and line-of-sight transmissions
    [11]. However, LoRa cannot be used to send data with high data rate due to local
    regulations and limitations to the transmission duty cycle in most regions of
    the world of 0.1%, 1% or 10%. Therefore, LoRa alone cannot help to solve the existing
    problems of IoT applications in remote areas. In this paper, we propose an advanced
    Edge/Fog assisted LoRa-based system for remote farming. The system targets remote
    areas in developing countries where the need is evident. We propose to leverage
    Edge and Fog computing, together with LoRa communication to back the limitations
    of each other while still providing smart services and more efficient computation
    distribution when compared to cloud-centric solutions. In addition, the integration
    of Edge and Fog computing with LoRa into IoT systems can help to achieve a high
    level of energy efficiency for sensor nodes. When some abnormalities occur, Fog-assisted
    IoT systems can provide support for latency-critical applications. As LoRa has
    limited transmission speed, and there are strict regulations controlling the duty
    cycle, we propose a novel data compression technique integrating the spatial distribution
    of sensor nodes, which can be densely deployed in smart farms, with image compression
    methods. We have simulated the output of thousands of sensor nodes and applied
    different image compression techniques. We compare our proposed method with standard
    JPEG compression and show how can we achieve up to 67% reduction of data size
    while keeping decompression error under 5%. This is a threshold often found in
    many inexpensive sensors. For instance, it is equivalent to having a 1 degree
    accuracy in a temperature sensor measuring around 20° C. The main contributions
    of the paper are: Advanced architecture for monitoring and controlling remote
    farms where Internet connectivity is not reliable. Integration of image compression
    techniques with convolutional neural networks to compress data from multiple sensor
    nodes at once exploiting their spatial distribution. Enhancement of edge gateway
    for pre-processing data and considerably reducing the amount of data to be transmitted
    over the LoRa link. The remainder of this paper is structured as follows: Section
    II presents related work in remote monitoring and management systems for smart
    farming. Section III describes the architecture of a Fog-assisted IoT system with
    the integration of Lora. Section IV includes the system implementation, edge layer
    analytics and discusses experimental results, while section VI concludes the work.
    SECTION II. Related Work Many efforts have been devoted to proposing smart and
    remote farm monitoring. Maia et al. [12] present a remote monitoring system for
    precision agriculture. The system consists of monitoring nodes, central nodes,
    and node in cloud. Data collected from sensors (e.g., soil temperature and humidity)
    integrated into a monitoring node is sent to a central node via ZigBee. The central
    node then forwards the received data to nodes in cloud via the Internet. Ragai
    et al. [13] present are a remote control and monitoring system for fish farms
    by using wireless sensor networks. The wireless sensor node collects different
    data such as temperature, pH, dissolved Oxygen and sends the collected data via
    ZigBee to gateways forwarding to cloud servers via the Internet. When cloud servers
    detect abnormalities such as too low or too high value of temperature and pH,
    they send an alarm to farmers. Saraf et al. [14] propose an IoT-based system for
    smart irrigation monitoring and controlling. The system consists of wireless sensor
    nodes, a center node connected to cloud and an Android application. The sensor
    node acquires temperature, humidity, soil moisture and water level of the tank
    and transmits the data to a center node via Zigbee. The center node then forwards
    the collected data to cloud which runs data processing algorithms and do actions
    e.g., sending commands to actuator nodes to control water. Shaout et al. [15]
    present an embedded system for remote agricultural areas monitoring. The system
    consists of sensor nodes which are based on Arduino board, Bluetooth and sensors.
    The sensor node can collect temperature, humidity, soil moisture and temperature,
    and transmit to Data Droid modem for further processing. Yang et al. [16] introduce
    a remote farm monitoring system which consists of client block, server block and
    end-user client. Client block is made from an LPC11C14 microcontroller, fan, light
    sensor, temperature and humidity sensor, buzzer, LED and OLED display screen.
    The data collected from a client block is sent to server block via ZigBee. A server
    block comprises of a camera, Wi-Fi, GPRS, and ZigBee module. The acquired data
    is forwarded by the server block to an end-user client via Wi-Fi. Ezhilazhahi
    et al. [17] propose an IoT-based system for plant soil moisture monitoring. Sensor
    nodes of the system collect soil moisture data and transmit the data via ZigBee
    to a gateway for data aggregation and transmission. The data then is sent to a
    remote server. End-users can use their mobile phone to access the server to achieve
    real-time soil moisture. Similarly, James et al. [18] present a remote monitoring
    system for plant growth. The system built from Rasberry Pi collects relative humidity,
    atmospheric temperature and soil moisture and sends the data via Bluetooth to
    a mobile application of a farmer. Gayatri et al. [19] introduce a remote monitoring
    system for agriculture by using a combination of IoT and cloud computing. The
    system relies on Wi-Fi/4G networks for making the interconnection between sensors,
    gateways and cloud servers. End-users such as farmers can use a farmer console
    to control or give an instruction to sensor devices in their farms remotely. Channe
    et al. [20] propose a smart system for remote monitoring agriculture. The system
    is based on IoT, cloud computing, mobile computing and bid-data analysis. The
    system senses soil and environmental properties, and then stores in cloud servers.
    In addition, the system processes and analyzes the data to extra useful information
    such as fertilizer requirements and best crop sequences. End-users such as farmers
    can access the raw data and extracted information stored in cloud servers remotely.
    Although the mentioned works show benefits of remote monitoring, they still have
    many disadvantages (e.g., energy inefficiency) and cannot be suitable for remote
    monitoring farms in remote areas where the Internet connectivity is not covered
    or stable. Therefore, we propose an enhanced architecture utilizing Edge and Fog
    computing, IoT, and LoRa to overcome the mentioned problems and offer many advantages
    services such as data processing at the edge and push notification. SECTION III.
    System Architecture The system architecture is illustrated in Fig. 1. The proposed
    architecture consists of 5 layers, namely sensor layer, Edge layer, Fog layer,
    cloud layer, and terminal layer. The sensor and edge layers are connected via
    nRF in our proposed system design, but other wireless solutions such as Wi-Fi
    or Bluetooth 5 can be utilized if they provide enough bandwidth and range. The
    Edge and Fog layer are connected via LoRa, which in open areas can enable transmissions
    of over 10 or even 20 km with a low data rate. The Fog gateways are in turn connected
    to cloud servers via wired Ethernet or wireless solutions with high throughput
    such as Wi-Fi or mobile 4G/5G. The final layer consists of the front-end user
    application and interfaces. A. Sensor Layer The sensor layer consists of several
    groups of sensor nodes. Sensor nodes and actuator nodes are part of this layer,
    and can be deployed to different areas of the farm. The main difference between
    these nodes is that the sensing node mainly collects and sends the data to Edge
    gateways in Edge layer while the actuating node primarily receives commands from
    Edge gateway to control actuators such as turning water system. A prototype sensor
    node is shown in Fig. 2, equipped with a micro-controller, nRF wireless module,
    and different sensors. Depending on the sensor node location, some of the sensor
    nodes are equipped with solar panels. Most of the hardware components of a sensor
    node are connected via an SPI wire communication protocol as SPI supports high
    data rate with low energy consumption. B. Edge Layer The Edge layer is formed
    by Edge gateways which are responsible for receiving data from sensor nodes via
    nRF, processing data and sending the processed and compressed data to Fog gateways
    via LoRa. This intermediate step helps to significantly reduce the amount of data
    transmitted over the LoRa links, and fulfill the strict requirements of LoRa''s
    duty cycle. An Edge gateway mainly consists of an embedded board with nRF and
    LoRa wireless modules. Some gateways can be equipped with sensors or cameras.
    These gateways are equipped with Linux because Linux offers many advantages such
    as free to use, lightweight, enhanced security, customization, reliability and
    high performance while Linux does not require powerful hardware. Edge gateway
    offers many advantages services such as push notification, channel categorization
    and security. For instance, when an edge gateway detects abnormalities such as
    hardware failure in sensor nodes, the gateway can send an instant message to end-users
    such as farmers or system administrators to inform about the problem. The possibilities
    of data processing in the Edge layer range from multi-robot mapping [9], to bio-signal
    analysis [2]. An nRF protocol supports more than 100 channels in which each channel
    has its own corresponding frequency (e.g.,). This creates a premise to utilize
    these channels in the most optimized way to reduce error rate or bandwidth limitations.
    Some channels are more interfered by noise surrounding or are busier due to a
    large amount of data transmitted by a vast number of sensor nodes. It is recommended
    to use a suitable channel or a group of channels in a specific moment. This service
    can be implemented in Edge gateways. In order to provide some levels of security,
    Edge gateway can implement some algorithms including encryption algorithms such
    as AES-128, AES-256, or cryptography algorithms such as ECDSA. Although running
    these algorithms can increase latency and an Edge gateway''s energy consumption,
    the overhead is not significant because an Edge gateway is often equipped with
    powerful hardware and uses power-line from a power socket. Edge gateway can run
    data compression or data processing methods to reduce a large amount of data transmitted
    over a LoRa network. Depending on data or the applications, lossless or lossy
    data compression can be applied at Edge gateways. Lossy compression can provide
    a high compression rate e.g., 50:1 while lossless compression often offers a low
    compression rate e.g., 10:1 For example, temperature data can be applied with
    lossy compression as temperature is often collected by several sensors in an area
    and the changing rate of temperature is not fast in terms of minutes. Therefore,
    losing or missing a sample or several samples cannot reduce a system''s reliability.
    Besides the mentioned services, Edge gateway can offer other advanced services
    such as interoperability, data fusion and mobility support. In this paper, we
    propose a novel integration of image compression techniques with accumulated data
    from multiple sensor nodes. Rather than using standard JPEG or other lossy compression
    techniques, we have implemented a compression solution based on deep learning.
    Convolutional neural networks have been increasingly used in recent years for
    lossy compression of images [21]–[23]. Convolutional neural networks provide robust
    methods for compression with less error after decompression when compared to traditional
    methods such as JPEG. We utilize a semantic perceptual image compression method
    proposed by Prakash et al. [24]. Their method is able to highlight semantically-salient
    regions, which are in turn encoded with higher accuracy. We have selected this
    method for IoT sensor data as it can increase the compression rate when the data
    is very homogeneous, while preserving regions of interest with proper accuracy.
    In order to use this method, edge gateways first gather data from multiple sensor
    nodes, and this data is encoded in the pixels of an image. For instance, the three
    channels of an RGB image can be used to encode three different variables, and
    the position of pixels can represent the spatial distribution of sensor nodes.
    Fig. 1. Proposed 5-layer sensor-edge-fog-cloud-terminal system architecture with
    LoRa wireless link used as the edge-fog bridge Show All C. Fog Layer There are
    two tiers in the Fog layer. The first tier consists of an optional group of repeaters
    which mainly receive the data transmitted from Edge gateways and forward the data
    to Fog gateways via LoRa. The use of this tier and the specific number of repeaters
    depend on the distance between the deployment location, such as a remote farm,
    and the nearest Fog gateway. For instance, according to a work from Petäjäjärvi
    et al., about 20 repeaters are required to maintain a stable LoRa communication
    link over a distance of 300 km, in near line-of-sign transmission [25]. In the
    same work, the authors warn that even though LoRa can provide long-range transmissions
    (e.g. 25 to 30 km), packet loss data increases dramatically after the 10–15 km
    threshold. The second tier in the Fog layer consists of a series of interconnected
    Fog gateways which are communicated with each other and share information about
    sensor nodes, whether they are in a near location or not. If fog gateways are
    not connected through a local area network, then VPNs or other virtual network
    solutions can be utilized. Fog gateways are located in areas where there exist
    stale and reliable Internet connectivity. Fog gateways are connected to cloud
    servers via Wi-Fi, Ethernet or 4G. Similar to Edge gateways, Fog gateways can
    offer advanced services such as push notification, distributed data storage, security,
    data fusion, and data processing. More detailed information of Fog services and
    are discussed in our previous articles [7], [8], [26]–[28]. Fig. 2. Sensor node
    prototype Show All D. Cloud Layer and End-User Terminal Layer Cloud layer consists
    of cloud servers and its services such as global data storage, big data analysis,
    push notification, data processing with complex algorithms. Depending on the application,
    specific cloud services can be implemented. End-user terminal layer consists of
    mobile applications and web browser which are used to access real-time data and
    input commands to control actuators in a remote farm. SECTION IV. Implementation
    and Results In this paper, a complete system for remote monitoring and controlling
    a farm in a remote area is implemented. In particular, a group of sensor nodes
    including sensing and actuating nodes, an Edge-gateway, a repeater, a Fog gateway,
    cloud servers and end-user terminal are implemented. A sensing node is implemented
    with an nRF module nRF24L01, a microcontroller AVR ATmega8, a group of sensors
    (i.e., DHT111 temperature and humidity sensor and soil sensor). Some of the sensor
    nodes can use the power socket while some can be equipped with solar panel and
    battery. Depending on the placement of the sensing nodes, one of the mentioned
    methods is applied for providing power for sensing nodes. The sensing node mainly
    senses and transmits the collected data to an Edge gateway. Therefore, it is programmed
    to sleep in most of the time to save power except for a moment when it senses
    and transmit data. When it is working, the radio receiving part is turned off
    to save power. An actuating node is also implemented with an nRF module nRF24L01,
    a microcontroller AVR ATmega8, a relay or a controlling circuit to turn on/off
    or control the actuator such as a water system. Different from a sensing node,
    an actuating node is powered by a power socket and its radio receiving part is
    turned on always to wait for the command from a farmer or a system administrator.
    The Edge gateway is implemented with a Rasberry Pi v3B, which has quad-core 1.4
    GHz CPU and 1G RAM. The Rasberry Pi is connected with nRF24L01+PA+LNA for receiving
    data from sensor nodes via nRF. Comparing with an nRF24L01 module having a printed
    onboard antenna, an nRF24L01+ module has a long antenna. This long antenna helps
    to increase receiving power at the receiver end although it also increases energy
    consumption of the Edge gateway. The high energy consumption of an edge gateway
    is not a problematic issue as edge gateways are often powered by a power socket.
    In addition, the Rasberry Pi is connected with a LoRa chip (i.e, Dragino hat including
    LoRa chip and GPS) for sending the processed data to repeaters. In this paper,
    a repeater is a simple LoRa gateway which receives data from several Edge-gateways
    and forwards the data to Fog gateways via LoRa. In future work, a repeater can
    be more investigated to perform some other services such as data compression,
    and data processing. Fog gateway is implemented with a LoRa gateway and an Intel
    UP gateway. Fog gateway is equipped with Ethernet, Wi-Fi and 4G module for connecting
    to the Internet and interconnecting between each other. In nominal operation,
    Ethernet is preferable when it is available. Otherwise, Wi-Fi and 4G are used.
    Fog gateway is powered by a power socket. Therefore, power consumption is not
    an issue. In this paper, we reuse and adapt fog services, cloud applications,
    and end-user terminals which have been implemented in our previous papers [7],
    [8], [26]–[28]. The user interface has been modified for data visualization. A.
    Sensor Node The implemented sensor node collects and transmits data (i.e., temperature,
    humidity, or soil moisture) with a data rate of 1 sample per minute to Edge gateways
    which are about 200 meters. This data rate is sufficient for farm applications
    as temperature, humidity, soil moisture and other similar data do not change frequently
    in terms of minutes. The sensor node is supplied with 3.3V and its energy consumption
    is measured with a power monitor tool MonSoon [29]. The energy consumption of
    the sensor node per minute is 138.92 mJ. In addition, we also test the transmission
    rate of the sensor node by sending more than 1000 samples with line-of-sight.
    The result shows that the success rate is larger than 99% and less than 5 samples
    are collapsed. If we use a 2000 mAh battery (5.6mm ⨯ 49.2mm ⨯ 68.8mm), a sensor
    node can work up to 863 hours. When a small solar panel (145mm ⨯ 145mm ⨯ 2mm)
    is used at noon with Finland sunlight, we can harvest around 1650 mJ per second.
    This number should be much larger if the solar panel is placed at Africa where
    sunlight is much more powerful at noon. With the solar panel and an average of
    4 hours per day in Africa, a sensor node''s battery can fully be charged. Lithium
    batteries often allow for 400 to 500 charging cycles. Therefore, a sensor node
    with the solar panel can be used for the same number of days. For verifying the
    functionality of the proposed system, we use a Web browser to access data including
    temperature, humidity and soil moisture. The results show that the system works
    properly. For example, temperature and relative humidity of the experimented land
    is shown (i.e., 20 degrees Celsius and 57%, respectively) in real-time. In addition,
    we also send some commands via a Web interface to control a water system of the
    experimented land. B. Data Compression In order to validate the proposed compression
    method, we have simulated the generation of 16384 samples, each with 3 8-bit channels.
    These channels can be used, for instance, for temperature, humidity and air quality
    data. The data is then saved as a 128⨯128 image, which is compressed using JPEG
    format. Data can be structured such that neighbor pixels in the image correspond
    to neighbor sensor nodes when taking into account their spatial distribution.
    Instead of sending information about individual sensor nodes, edge gateways gather
    data from all of them and then merge the data into an image where spatial information
    is encoded. The correspondence between sensor node identifiers and image pixels
    must be done either beforehand or through an initial message when the edge gateways
    are configured. Figure 3 shows an example of image compression using the semantic
    perceptual image compression for simulated random data with equal mean and variance
    through the image. A different method has been used to generate simulated data
    in Figure 4, where the image has been divided in four quadrants of 64⨯64 pixels
    each and different mean and variance have been applied. In a real scenario, the
    first approach simulates a field with only one kind of crops and homogeneous conditions,
    where the second approach simulates a field with multiple crops or different soil
    conditions. In both cases, sub-figure (a) represents the random data, while (b)
    illustrates the map generated by the method from [24], and (c) shows the overlay
    of the original image and the map. The compression error and reduction of data
    batch size is shown in Tables I and II, where we can see that for 57% compression
    rate the error amounts to only 2%, a reduction of over 40% when compared to standard
    JPEG compression. We can also see that error rates decrease when there is more
    information added to the image. This is particularly beneficial for detecting
    abnormalities with higher precision due to the semantic compression method utilized.
    Fig. 3. Illustration of simulated data with constant distribution. Show All Table
    I Compression comparison in random noise Fig. 4. Simulated data with four different
    distributions. Show All Table II Compression comparison in 4-quadrant noise SECTION
    V. Conclusion and Future Work We have presented a hybrid 5-layer architecture
    for IoT systems deployed in smart farms. The proposed architecture consists of
    sensor, Edge, Fog, cloud and terminal layers. This architecture can be used to
    multiply the options of IoT deployments with LoRa or other LPWAN technologies
    that provide low-power and long-range transmission but limited data rate. We propose
    different methods for compressing data and reducing the LPWAN link load, minimizing
    the amount of data transmitted and stored in cloud servers while maintaining a
    similar user experience. In particular, we propose a novel integration of image
    compression algorithms to be applied in IoT data from the agricultural and farming
    industries, such as temperature, humidity, air quality or soil properties. In
    future work, we will focus on deploying multiple sensor nodes in a real scenario
    and provide a wider set of compression and analytics algorithms for the edge layer.
    Authors Figures References Citations Keywords Metrics More Like This A Novel Fair
    and Scalable Relay Control Scheme for Internet of Things in LoRa-Based Low-Power
    Wide-Area Networks IEEE Internet of Things Journal Published: 2021 Low-Power Wide-Area
    Networks: Comparison of LoRaWAN and NB-IoT Performance IEEE Internet of Things
    Journal Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE AFRICON Conference
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Edge AI in Smart Farming IoT: CNNs at the Edge and Fog Computing with LoRa'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
