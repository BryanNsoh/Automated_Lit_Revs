- DOI: https://doi.org/10.1109/ic2e.2016.26
  analysis: '>'
  authors:
  - Hui Kang
  - Michael Le
  - Tao Shu
  citation_count: 139
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2016 IEEE International Confe... Container and Microservice
    Driven Design for Cloud Infrastructure DevOps Publisher: IEEE Cite This PDF Hui
    Kang; Michael Le; Shu Tao All Authors 126 Cites in Papers 15 Cites in Patents
    7243 Full Text Views Abstract Document Sections I. Introduction II. Cloud Infrastructure
    Devops III. Case Study: Applying Containerization and Microservice-Style Design
    to OpenStack IV. Evaluation V. Limitations and Discussion Show Full Outline Authors
    Figures References Citations Keywords Metrics Footnotes Abstract: Emerging container
    technologies, such as Docker, offer unprecedented agility in developing and running
    applications in cloud environment especially when combined with a microservice-style
    architecture. However, it is often difficult to use containers to manage the cloud
    infrastructure, without sacrificing many benefits container offers. This paper
    identifies the key challenges that impede realizing the full promise of containerizing
    infrastructure services. Using OpenStack as a case study, we explore solutions
    to these challenges. Specifically, we redesign OpenStack deployment architecture
    to enable dynamic service registration and discovery, explore different ways to
    manage service state in containers, and enable containers to access the host kernel
    and devices. We quantify the efficiency of the container-based microservice-style
    DevOps compared to the VM-based approach, and study the scalability of the stateless
    and stateful containerized components. We also discuss limitations in our current
    design, and highlight open research problems that, if solved, can lead to wider
    adoption of containers in cloud infrastructure management. Published in: 2016
    IEEE International Conference on Cloud Engineering (IC2E) Date of Conference:
    04-08 April 2016 Date Added to IEEE Xplore: 02 June 2016 Electronic ISBN:978-1-5090-1961-8
    DOI: 10.1109/IC2E.2016.26 Publisher: IEEE Conference Location: Berlin, Germany
    SECTION I. Introduction Infrastructure as a Service (IaaS) provides users with
    immediate access to scalable and seemingly “infinite” infrastructure resources
    without the expensive overhead and burden of having to maintain and manage their
    own datacenters. To offer IaaS, providers must deploy complex software stacks
    (aka., cloud infrastructure management software) to allocate, manage, monitor,
    and configure the underlying physical resources, e.g., CloudStack [1], Eucalyptus
    [2], and OpenStack. With IaaS quickly becoming commoditized and with businesses
    relying more on IaaS to serve as a foundation on which more lucrative managed
    services are built, the need to efficiently deploy and operate these infrastructure
    services grows in importance. Recent advancements in container technology have
    led to renewed interests in using containers in the cloud. PaaS systems, e.g.,
    CloudFoundry [3], Heroku [4], OpenShift [5], have long been the early adopters
    of containers. In these systems, containers are commonly used to host and allocate
    resources to applications. More recently, efforts to containerize cloud infrastructure
    services have emerged, i.e., running the infrastructure management software in
    containers [6], [7], [8]. Whether applied to applications or to cloud infrastructures,
    containers are attractive as they provide good isolation with low overhead and
    fast start-up time leading to highly agile solutions [9], [10]. This agility can
    be further amplified when combined with a microservice architecture where software
    is broken into small functional units; each unit can independently fail, scale,
    be maintained and reused [11]. DevOps is a set of techniques for streamlining
    and integrating the software development process with the deployment and operations
    of said software. Some of the DevOps challenges include, for example, how to easily
    deploy and upgrade cloud infrastructures, how to smoothly deliver code from development
    to production, how to scale the infrastructure with less human intervention, etc.
    Getting DevOps correct is one of the keys to the overall success of cloud-based
    software solutions. When dealing with cloud infrastructure software, besides the
    challenges of DevOps, being able to efficiently make use of the underlying physical
    resources is also critical, i.e., how to run more services with less resources.
    Efficient DevOps combined with efficient use of resources leads to high IT performance,
    which is a must for today''s highly competitive IaaS business. Being highly agile
    and lightweight, containers combined with the microservice architecture have shown
    great promises in achieving the desired level of efficiency. Although promising,
    significant challenges remain in containerizing and applying the microservice
    design principles to cloud infrastructure services. First, since infrastructure
    management software are broken into individual components with different roles,
    it is unclear what are the definitive interfaces between these separate components
    in the microservice architecture. Second, containerizing stateful components of
    the infrastructure layer (e.g., database, messaging server, in-memory key-value
    store) is difficult, due to limited support for data portability in containers
    [12]. Although such challenges also apply to the application layer, cloud applications
    can live with workarounds, such as running stateful components, e.g., databases,
    as external services [3] (therefore, obviating the need for containerization).
    However, this is a sub-optimal solution for infrastructure software, because service
    states are an integral part of the runtime environment. Third, infrastructure
    services often interact with physical resources such as compute, network, and
    storage. While physical resources are increasingly becoming “software-defined,”
    they still lack common, programmable interfaces. This fundamentally impedes portability
    as a containerized service built for one system may fail to interface with the
    physical resources on another. In this paper, we explore both the opportunities
    and challenges in using containers and applying the microservice design principles
    to operate and manage cloud infrastructure services. The goal of this work is
    to develop a more agile, reliable, and efficient DevOps approach for such infrastructure
    software. Specifically, to make the discussion more concrete, we present the design,
    implementation, and deployment of a microservice based Dockerized OpenStack instance.
    We use this instance to measure and quantify different aspects of operational
    efficiency discussed above. Lessons from our deployment along with limitations
    of our approach will also be discussed along with key open research problems,
    that if resolved, can lead to wider adoption of containers in the cloud. The remainder
    of this paper is organized as follows: in Section II we describe how containers
    and microservice-style design can be used to achieve operational efficiency. In
    Section III, we describe our prototype which is the application of microservice
    style DevOps to OpenStack using Docker containers. In Section IV, we present measurements
    quantifying operational efficiency of our prototype architecture to a reference
    OpenStack architecture. Section V discusses limitations of current implementation
    and related work is presented in Section VI. Section VII concludes the paper.
    SECTION II. Cloud Infrastructure Devops In this section, we discuss how containers
    and microservice-style design can be used to achieve high operational efficiency
    for cloud infrastructure software. We start with discussing the characteristics
    of cloud infrastructure software and their impact on DevOps (Section II-A), how
    container and microservice-style design can improve operational efficiency (Section
    II-B), and finally, the challenges of applying container and microservice-style
    design to cloud infrastructure software (Section II-C). A. Characteristics of
    Cloud Infrastructure Software Large Configuration Space Due to the need to support
    and anticipate a wide array of use cases, infrastructure software typically provides
    a myriad of configuration options. These configuration options allow the fine-grained
    tuning of software and hardware components of the infrastructure. Unfortunately,
    such abilities result in increased complexity which in turn can cause operational
    failure. Such failures are often caused by improper setting of certain parameters
    by operators who do not have the detailed knowledge or good understanding of the
    software as the development team. To reduce the chance of failures, only a minimum
    set of configurations should be exposed to the software. Loosely Coupled Components
    Cloud infrastructure software is often composed of many components (e.g, database,
    networking, computing, storage, etc.). These components collaborate with each
    other to provide the overall function of managing infrastructure resources. For
    the sake of performance and high availability (HA), each component is instantiated
    in more than one instances (e.g., multiple virtual routers). When new instance
    joins or existing one fails, the rest of the cluster must be informed of such
    event. Therefore, to make it easier to manage the cloud infrastructure, individual
    components should be loosely coupled. Short Release Cycles To adapt to constant
    changes in user de- mands and needs, features and fixes to infrastructure software
    are constantly being added. Updates to different components are done by different
    teams and with different release cycles. For example, OpenStack has a 6-month
    release cycle with milestone releases every one and a half months. Each release
    can contain hundreds of features and bug fixes. For instance, from OpenStack IceHouse
    to the Juno release, more than 300 new features were added and more than 3000
    bug fixes committed across all components and libraries [13]. To keep up with
    the rate of updates, operators need automation and continuous integration of updated
    software. Runtime Consistency Infrastructure software requires the runtime environment
    to be configured with appropriate OS version, kernel modules, and supporting libraries.
    These dependencies are more restrictive and numerous than application-layer software.
    For example, a successful deployment/upgrade in a development environment does
    not necessarily imply the same in the staging or production environment where
    the OS or library versions might differ. Hence, mechanisms to enable consistent
    execution environment is critical. Table I. Number of configuration points in
    deploying openstack: chef-based vs docker image-based approaches. B. Container
    and Microservice-Based Devops Software-defined systems have enabled developers
    to treat infrastructure as code. This has allowed for the possibility of infrastructure
    to be managed in an automated, flexible, and efficient manner. Unfortunately,
    due to the nature and complexity of the infrastructure software (see previous
    section), many of these potentials are not fully realized. By leveraging containers
    and microservice-based designs to achieve code portability, ease lifecycle management,
    and utilize resources more efficiently, developers come closer to unlocking the
    full potential of software-defined systems. Code Portability One of the most appealing
    features of con- tainer is its support for code portability. By packing software
    along with its dependencies into a single image [14], [15], [16], container enables
    image-based deployment process, which offers the freedom of “develop once, deploy
    everywhere.” It should be noted that while image-based management can also be
    achieved with VMs, VMs are not as portable nor lightweight as containers, thereby
    reducing the effectiveness of the approach. The image-based approach can alleviate
    many of the pain points discussed above with regards to configuration complexity
    and runtime consistency of infrastructure software. Conventional deployment of
    cloud management software often relies on automation tools (e.g., Chef, Puppet)
    that tend to create many configuration points, in order to codify the dependencies
    between software and the deployment environment. This makes the install/uninstall
    or update process cumbersome. If not handled carefully, these configuration points
    can turn into break points or errors. In contrast, running services in containers
    simplifies the deployment process, as most of the software dependencies are packaged
    in a single image, leaving few necessary parameters for operators to configure.
    To illustrate the above point, Table I compares the number of configuration points
    in OpenStack installation, when using Chef cookbooks [17] or Docker [14] container
    images developed for our own cloud operational environment. As can be seen from
    this example, the image-based approach eliminates much complexity in the configuration
    steps, thereby simplifying the entire deployment process. Lifecycle Management
    As discussed in the previous subsection, today''s cloud infrastructure software
    demands continuous delivery and integration. To satisfy this demand, operators
    prefer managing the lifecycle of the infrastructure code using image-based approaches
    (e.g., versioned Docker images), so as to avoid in-situ updates. Furthermore,
    they aspire to capabilities such as easy deployment and tracking of different
    releases simultaneously in different environments, online “A/B testing,” fast
    roll back, and so on. When problem occurs in a new release, operators do not need
    to debug and fix them in a production environment. Instead, they simply roll back
    to the previous container image (see Figure 1) and wait until new images with
    fixes become available. Image-based approaches combined with a microservice-style
    design amplify the benefits as components of the system are decoupled preventing
    interfering with one another during maintenance. Fig. 1. Lifecycle management
    of infrastructure code using container images. Show All Resource Utilization Container
    increases the utilization of computing resources from two aspects. First, like
    VM, multiple containers can be hosted on a single server to increase its resource
    utilization. Second, containers are more efficient than VMs in accessing memory
    and I/O devices, while still providing good isolation between applications in
    different containers [10]. Therefore, at single-host level, containers can run
    more cloud services than VMs, while achieving the same performance. Moreover,
    lightweight, portable and fast-booting container images facilitate the application
    of microservice-sty le design by running many cloud infrastructure components
    as “micro-services” that can be scaled out (or back) easily with workload changes.
    This leads to more efficient resource utilization at multi-host level. C. Design
    Challenges This subsection describes the three main challenges associated with
    applying containers and microservice-style design to cloud infrastructure management
    software. Minimize Cross-Configuration Cloud infrastructure software such as OpenStack
    often adopts a componentized and distributed architecture. Hence, different service
    components naturally become the units for containerization. How these individual
    components are managed and organized can greatly affect the portability and scalability
    of the design (see Subsection III-A). Some services are typically replicated for
    fault tolerance and high availability (HA) purposes. With a containerized approach,
    one of the main challenges is to allow multiple instances of the same service
    to be instantiated from the same container image while still allowing some level
    of customization and configuration of the service. Too many configuration points
    would greatly complicate an image-based deployment (see Subsection II-A) while
    too few can make the container image less portable. Another challenge is to minimize
    the amount of cross-configuration that occurs in the overall setup. Cross-configuration
    occurs when changes to one component will cause configuration updates on all other
    components. Hence, adding or removing a service can result in impacting all other
    existing services. This phenomenon can be found in conventional HA setups using
    cluster management tools such as Pacemaker [18] where each node must keep track
    of the IP addresses of all other nodes in the cluster. Such approach is not suitable
    for a microservice-style deployment. Maintain State The cloud infrastructure layer
    manages many software state. One type of state is the runtime configurations of
    the cloud infrastructure, e.g., service IP addresses and ports, master-slave relationship
    in HA setups, etc. These runtime configurations should be managed in a way that
    suits container-based deployment, i.e., 1) few configurations need to be managed
    outside of the containers, and 2) any configurations that have to be managed outside
    of the container can automatically be fed to service registration and discovery
    processes (see Subsection III-A). The other type of state comes from the running
    application in the container. Application state can either be on disk or in memory.
    For example, a database may store user profiles on disk, while messages between
    components may be stored in a messaging server''s memory. To be fault-tolerant
    and/or support the dynamic scaling out of services, mechanisms must be developed
    to allow the replication of application states. For on-disk state in the container,
    existing options include using shared storage, disk volume migration [19], and
    application-level data replication. Depending on the applications, these choices
    lead to different performance and scalability trade-offs. For in-memory state,
    however, there is no mechanism available yet for containers to transparently perform
    memory replication. In this case, the current practice is for containerized services
    to avoid storing state in memory altogether (e.g., caching), or rely on the clients
    to tolerate the errors caused by the loss or inconsistent state of failed services.
    We discuss some of our efforts in dealing with state in memory in Subsection III-C.
    Provide Host Resource Access The lack of a dedicated kernel in a container makes
    it difficult to access host resources needed by some infrastructure service components.
    For example, some services need to interact with the kernel through IOCTL or /proc.
    Others need to access the host''s hardware device to handle high performance tasks
    (e.g., a software router). One potential solution is to enhance the capability
    or privilege of these containers. However, this decreases the portability of such
    containers, and may result in a conflict when multiple privileged containers access
    the same host resource. SECTION III. Case Study: Applying Containerization and
    Microservice-Style Design to OpenStack OpenStack is a widely adopted, open source
    infrastructure cloud management software. Its distributed, service-oriented architecture
    makes it an ideal candidate for demonstrating the feasibility of containerizing
    cloud infrastructure software. In this section, we focus on addressing the challenges
    identified in Subsection II-C, while preserving the benefits of containerization
    discussed in Subsection II-B. A. Container-Based Deployment A typical OpenStack
    architecture is comprised of three types of nodes: control plane, compute, and
    network. The control plane nodes run management services, while compute nodes
    and network nodes are responsible for provisioning VMs and network resources.
    Services across the different nodes can be containerized at different levels of
    granularity, based on the DevOps requirements. In our current implementation,
    we opt for a relatively simple setup for the services in the control plane: one
    container hosts the database service (MySQL), another hosts messaging service
    (RabbitMQ [20]), and the third hosts the controller services (including Keystone,
    Glance, Nova, Neutron servers, etc.). To support HA, each control plane container
    is replicated on two additional hosts. On the compute nodes and network nodes,
    there is one container that runs the Nova and Neutron agent services. Figure 2(a)
    shows the Docker container-based OpenStack deployment, which we will refer to
    throughout the rest of the paper. Each service in OpenStack consists of its application
    code and configuration files. We have built the application code together with
    all its dependent libraries into container images, which can be downloaded and
    used to create container on any Docker host. Meanwhile, container images contain
    application-specific scripts to generate configuration file at runtime. B. Microservice
    Architecture Containerizing OpenStack is more than just running those management
    services in containers. For example, in the control plane the controller container
    should be informed where the database and the messaging services are, whenever
    these services are newly created, recovered from failure, or shutdown. Specifically,
    this means the services must have ways to register themselves, be discovered by
    other services, record their configuration/runtime state, and be generally orchestrated
    in their deployment and update processes. Therefore, we need an architecture that
    eases the operation of scaling-out, HA, and load balancing. Based on these requirements,
    the key components of the proposed architecture include: Service Proxy: all services
    are configured to be only accessible via a proxy (e.g., HAProxy [21]), such that
    changes to a service instance do not affect others in the HA setup. Meanwhile,
    the proxy must have the up-to-date configurations of each service, e.g., IP address
    and port number. These configurations are retrieved from the configuration state
    manager. Configuration State Manager (CSM): manages the configurations of all
    services in the control plane. We implemented the CSM as a distributed key-value
    store based on etcd [22]. To register the service in CSM, each container has its
    “sidekick” process. The sidekick process monitors the status of the service in
    the container periodically (e.g., through test API calls); depending on the status,
    the sidekick process will register or de-register the service in the CSM. Service
    Orchestrator: all services are managed by an orchestrator (built on fleet [23])
    that decides how many instances of a service need to be run and where to run,
    based on certain policies (e.g., co-location or anti-colocation). The orchestrator
    not only launches containers along with their sidekicks on the same host, but
    also tracks the status of the container and takes action accordingly. For example,
    if an existing container is terminated due to rebooting the host, the orchestrator
    will relaunch a new one automatically. Figure 2(b) illustrates how these three
    components cooperate in the proposed architecture. In step (1), the service orchestrator
    launches a controller container and its sidekick process on a host. In step (2),
    when the sidekick process finds that the service is up and running, it registers
    the well-known service name (e.g., /controller/ip) and the IP address (e.g., 172.16.1.101)
    as a key-value pair in the CSM. A confd [24] process running with the proxy watches
    the changes to the key-value pair in CSM, generates new configuration files, and
    reloads the HAProxy process to make use of the new configurations (step 3). Later,
    if the controller container or service in the container fails, the service orchestrator
    will be informed and start a new controller container. The same service registration
    process will then commence. C. Handling Stateful Services The architecture described
    above offers clear benefits to stateless services in OpenStack. However, handling
    stateful services requires careful considerations, so as not to obviate the benefits
    from containerization. The first type of service states are the application data
    residing on disk. The main challenge is how to efficiently replicate these states
    in an HA setup. Here we use MySQL database as an illustrative example. MySQL supports
    two HA modes: active-standby and active-active. In the active-standby mode, only
    the master node serves client requests, while the remaining nodes are slaves and
    replicate data from the master. This mode requires distinction between master
    and slaves roles and triggers cross-configuration during failover or scaling out.
    In the active-active mode, every node in the cluster are identical and actively
    serve client requests; the joining or loss of individual nodes does not impact
    the others. Therefore, the active-active setup is more suitable for the container-based
    architecture. Since every node in the active-active MySQL cluster can update its
    local state, consistency must be guaranteed by synchronizing data between instances.
    Common approaches for data synchronization are shared storage and application-level
    replication. In the former case, all MySQL servers access the same data volume
    via a distributed filesystem such as Global File System 2 (GFS2) or OCFS. In the
    latter case, the database process (e.g., Galera patched MySQL [25]) synchronously
    replicates data among all nodes in the cluster. In both setups, data on disk survive
    the loss of any database server. However, as presented below, they have different
    performance and scalability trade-offs. Table II compares the latency of accessing
    the database using shared storage and application-level data replication. In this
    experiment, two MySQL containers across two hosts form an HA cluster, where data
    synchronization is either achieved by GFS2, or by Galera replication. We use SysBench
    [26] to simulate various workload combinations and numbers of concurrent users
    accessing both servers. We observe that while the results for read-only workload
    are similar, the Galera setup significantly outperforms the GFS2 setup for the
    read-write workloads (100× and 15× faster). This is because with shared storage,
    a write request will lead to filesystem lock on the entire table file (aka., table-level
    locking). In contrast, Galera allows multiple MySQL servers to update different
    rows in a table simultaneously, as it uses row-level locking. This leads to the
    superior performance of the Galera setup. However, the benefit of Galera setup
    comes at the cost of state replication during scaling out or recovery. That is,
    when a new MySQL container is created to join an existing cluster, DB states need
    to be transferred to the new server. Even though there are ways of minimizing
    such state transfer, e.g., through incremental state transfer (IST) data transfer
    still incurs network overhead and delays in starting new service instances. Fig.
    2. A container-based microservice architecture of openstack. (a) Openstack deployment,
    (b) service registration and discovery in microservice architecture. Show All
    Table II. Latency of 95th percentile of requests: GFS2 shared storage vs. galera-based
    data replication. The other type of states are the in-memory application data,
    such as network sessions or cached values. To handle in-memory state, mechanisms
    for container memory snapshotting, replication, and migration are required. These
    mechanisms provide operators the ability to gracefully handle planned host maintenance
    owing to OS/hardware upgrade or zero-downtime load balancing and failover for
    HA. The abilities of snapshotting and restoring applications are available for
    both containers [27] and VMs [28]. With VMs, these mechanisms have been widely
    adopted by many modern VM management tools such as VMware''s vCenter, virt-manager,
    etc. Linux implementation of container memory snapshotting also exists for LXC
    [15] containers but unfortunately, not for Docker containers. We have worked with
    the open source community to integrate the abilities of snapshotting LXC containers
    through the CRIU [29] project to Docker containers [30]. In essence, snapshotting
    involves dumping all the runtime information (e.g., mapped memory page, content
    of CPU registers, pending signals) of the processes in a container along with
    the context of the container (e.g., namespaces, network interfaces, configuration)
    into a set of image files located locally on disk. The size of the image files
    are proportional to the amount of memory actively used by the container. The abilities
    to take a snapshot of the memory state of a container is the foundation for other
    services such as replication and migration, which are also under development by
    the community. Given the immaturity of this technology, we do not yet handle in-memory
    state of our deployed OpenStack services. We do, however, provide some evaluation
    of the performance of container memory snapshotting abilities in Section IV. D.
    Handling Services Accessing Host Resource In OpenStack, infrastructure services
    on compute, storage and network node are responsible for creating VM instances,
    provisioning storage and network resources, respectively. As these resources are
    tightly coupled with the host OS and require some degree of privilege to access,
    it is a challenge to run compute, storage or network functions inside containers
    in a safe and portable way as explained below. Containers that run services that
    need access to host resources require both privileged access to the host kernel
    and access to specific directories on the host. As a consequence, this can jeopardize
    the security of the host. For example, the /proc/modules directory must be shared
    so that the container can load kernel modules, such as KVM, to launch VM instances
    [31]. Access to this directory allows a container access to other modules on the
    host that it is not privy to. This lack of fine-grained control for privileged
    access is a limitation of current container technology that we will briefly discuss
    in Section V. Fortunately, not all services that require access to host resources
    need to run in privileged containers. For instance, containers requiring access
    to data volumes on the host for storing VM images do not require privileged access.
    SECTION IV. Evaluation This section evaluates and validates the design choices
    discussed in Section III. Specifically, we highlight the benefits of using containers
    and microservice-style architecture for DevOps of OpenStack over more conventional
    VM-based approaches. We do not present analysis of script-based DevOps mechanisms
    such as using Chef or custom shell scripts as these techniques lack the agility,
    flexibility, and speed necessary for efficient DevOps in the cloud. For example,
    script-based techniques are time consuming because they require downloading and
    installing software packages when new instances of services are instantiated.
    In addition, they are error prone due to the complex logic needed to handle installation
    and configuration failures thus requiring specialized knowledge from developers
    to debug (see results in Table I). Table III. Devops practices that improve IT
    performance [33] In subsection IV-A, we briefly describe common DevOps practices
    that are most critical to IT and the main differences between DevOps using VM-based
    vs. container-based approaches. Specifically, we show that OpenStack DevOps using
    containers benefit greatly from (1) fast deployment time, (2) ease of rolling
    upgrade, and (3) simplification of failure recovery and HA. In subsection IV-B,
    we assess how well the control plane scales under varying workload. Our experimental
    setup consists of machines connected by a 1Gb switch. Each machine has two Xeon
    E5649 processors with 64GB RAM and runs Linux 3.13. KVM is used as the hypervisor
    to instantiate VMs on the Linux hosts. For experiments using containers, we use
    Docker 1.6. Each Docker host also runs etcd and fleet agent to form the complete
    microservice architecture. These orchestration tools are available in most Linux
    distributions hence the selection of a particular Linux distribution (e.g, Ubuntu,
    CentOS, CoreOS [37] is not critical to our evaluation. For the OpenStack control
    plane, we use components in OpenStack Juno release, MySQL Galera, and RabbitMQ.
    We use the Rally benchmark tool [32] to generate OpenStack workload. For experiments
    involving container memory state snapshotting, we use CRIU 1.7. The architecture
    of the experimental testbed is shown in Figure 2. A. Devops Practices: Vm-Vs Container-Based
    Approaches OpenStack DevOps refer to a wide range of tasks that manipulate the
    lifecycle of individual OpenStack components. The most important ones are summarized
    in the first column of Table III. The latencies of performing these operations
    (deployment, rolling upgrade, and failure detection/recovery) are used to quantify
    the efficiency of container vs. VM-based approaches. Table IV compares the time
    to perform the three operations using VM-based and container-based approaches.
    The results show that container-based approach outperforms VM-based approach in
    terms of time to perform the DevOps tasks. In general, the major differences between
    between VMs and containers with respect to DevOps are the provisioning overhead
    and speed. Bringing up a container is not only faster than bringing up a VM, but
    uses less system resources. Besides the fast container creation time, container
    images are composed of different layers, a feature that avoids having to transfer
    the complete history of the image, resulting in faster image transfer time and
    more fine-grained version control. Detailed analysis of each DevOps task are provided
    in the following paragraphs. Table IV. Execution time of openstack devops tasks:
    vm or container-based approaches (in second) Deployment (Validating Section III-A)
    As shown in Figure 2, the OpenStack control plane consists of three types of nodes:
    messaging, database, and controller. The deployment of these nodes has two phases:
    initialization and scale out. In the initialization phase, we create one instance
    of each node type on each host (create a VM/container) to stand up a fully functional
    OpenStack control plane. We then scale out the services by adding another two
    instances of each type to form an HA cluster. As shown in row one of Table IV,
    container-based deployment is 1.5× faster than VM-based deployment. To explain
    why the container-based approach leads to faster deployment, we show the break
    down of the deployment time of each node type in Figure 3. Figure 3(a) shows the
    initialization phase of the first instance of each node type. The instantiation
    of database and messaging are in parallel (dark gray), while the instantiation
    of services in the controller instance (light gray) has to wait for the completion
    of the database and messaging components because services in the controller node
    need to register users and endpoints in the database and certain services (e.g.,
    Nova conductor) coordinates via the messaging service. As can be seen, the controller
    node takes the longest time to finish and its duration is almost the same between
    VM and container. Therefore, although launching a container is much faster than
    a VM (specifically, < 1s vs. 30s), the actual benefits of using containers in
    completing the initialization phase across the three node types is relatively
    small. In contrast, the scale out time when using containers is reduced by > 50%
    as shown in Figure 3(b). During the scale out phase, we start two additional VMs/containers
    of each node type. While it takes the same amount of time to spin up the containers
    as in the initialization phase, the startup time of individual VM is tripled (90s
    vs. 27s). This is the result of resource contention caused by VM creation (Figure
    3(c)). With either serial or parallel creation of the instances, VMs typically
    cost more system resources and incur more interferences than containers, leading
    to longer startup time. Therefore, for the entire deployment, the container-based
    deployment finishes more quickly than VM-based approach. Further, as initialization
    is a one-time process for the Open-Stack deployment, we expect more benefits from
    scaling out services using containers. Rolling Upgrade (Validating Section III-B)
    A rolling upgrade is an upgrade procedure that causes zero-downtime. This ability
    is essential to operators to deal with the high rate of bug fixes and new features
    typical in modern cloud deploy- ment. Figure 4 illustrates how rolling upgrade
    is performed in a container and microservice style DevOps. In step 1), a container
    containing upgraded software is created on the same host as the existing service.
    Then the active version of the service is updated either manually or automatically
    triggered by the upgrade event (step 2). By continuously watching the version
    key, the configuration file of the proxy is updated (step 3), followed by redirecting
    the traffic to the service running in the new container (step 4). This process
    is transparent to the external clients, causing minimal interruption to the other
    components of OpenStack. Fig. 3. Deployment comparison: containers vs. Vms. (a)
    Deploy one instance of each type on three hosts respectively; (b) scale out instances
    on three hosts; host CPU utilization. Show All Fig. 4. Rolling upgrade in a container-based
    microservice style architecture. Show All Image-based DevOps, regardless of VM-based
    or container-based simplifies rolling upgrade by eliminating the complexities
    during the conventional software upgrade process (such as uninstalling/installing
    the old/new binaries along with dependencies and resolving conflicts). In essence,
    the upgrade process is to replace the existing VM/container with a new instance
    while minimizing interruption of the provided service. Since the size of container
    image is typically smaller than that of a VM and starting a container is faster
    than a VM, the total time to upgrade a service using containers, is therefore,
    faster than using VMs. This can be seen in our results (Table IV, 2nd row). Replacing
    the three controller containers in the testbed is 1.6× faster than replacing three
    VMs running the same service. However, while not shown, the downtime experienced
    by the two approaches is not noticeably different. Failover/Recovery (Validating
    Section III-C) Service availability is critical in any production environment,
    but failure is inevitable. A service becomes unavailable due to either planned
    maintenance or unplanned outage. Depending on the cause of the downed service,
    maintaining service availability requires performing different operations. For
    planned maintenance, typical solutions include snapshotting the runtime state
    of the service and later restoring the service on the same host (after it has
    been fixed), or temporarily migrating the service to a different host. The solution
    to unplanned service outage is more complicated and relies on an HA framework
    with support for continuous redundancy. In the following paragraphs, we will compare
    the differences in VM and container-based DevOps with regards to failover and
    recovery. High Availability The design of a high available system aims at minimizing
    the number of failed transactions during service outage and quickly bringing system
    back online. Moreover, given the diversity of the services in OpenStack, from
    DevOps perspective, it is desirable that the HA framework can provide a unified
    and simplistic interface to manage these services. For existing VM-based approach,
    failure detection/recovery relies on clustering software such as Pacemaker (approach
    recommended by the OpenStack installation guide), while with containers we can
    leverage generic service discovery/registration mechanisms such as etcd and fleet.
    In Pacemaker, individual services must use a Pacemaker specific way to write their
    own custom resource agents that monitor service availability and run application-specific
    scripts to perform service recovery after failure. In our microservice architecture,
    a simpler and more generic mechanism is used to monitor availability (e.g., shell
    scripts used by sidekick for probing) as well as for recovery (kill failed service
    and instantiate new service). In the following paragraphs, we first evaluate and
    compare the recovery time and transaction error rate of failed OpenStack services
    deployed in VMs using Pacemaker for HA with the container-based microservice style
    HA explored in this work. We then describe the complexity in implementing HA using
    containers vs. Pacemaker. The first experiment compares the recovery time when
    failure occurs in the MySQL database component of Open-Stack. Recovery time is
    determined by the latency to detect the failure and the time to recover the failed
    service. In this work, the detection latency is configured to be the same for
    both approaches. Hence, the major difference between the two approaches is the
    time to recover a failed instance. To cause a failure, we explicitly shut down
    a VM and a container, respectively 1. As a consequence, the recovery action is
    to reboot the VM in the Pacemaker case and start a new container running the service
    in the container-based approach. As expected, the recovery of a failed container
    takes about half of the time rebooting a VM (row three in Table IV). Table V.
    Percentage of failed requests during service failover and the recovery time. 240
    requests in total. The second experiment examines the transaction error rate during
    node failure. In active-active HA mode, the traffic intended for the failed instance
    will be directed to the remaining nodes. The failed instance can still return
    error for those outstanding transactions before the proxy can redirect traffic.
    We run Rally to generate transactions of adding new users and querying existing
    users. Table V lists the percentage of failed requests under different failed
    node types. As can be seen, failure of the MySQL server led to 1.8% failed requests,
    while that of the controller led to 5.8% failed requests. This is because not
    all user requests trigger database transactions (controller node uses memcached
    to cache the returned value of frequently issued requests in order to avoid unnecessary
    database transactions). Therefore, when the controller container fails, all in-memory
    state containing ongoing sessions are lost, thus resulting in the observed higher
    error rate. The HA framework in the microservice architecture simplifies the process
    of managing, monitoring, and recovering OpenStack services. In a Pacemaker cluster,
    each service is considered as a resource and managed by an executable (aka., resource
    agent [34], [35]). The resource agent has to follow certain specifications such
    as Open Cluster Framework (OCF), a complexity that not only imposes additional
    burden on the developers, but is also known to be notoriously difficult for operators
    to debug. Further, since services are different from each other, operators has
    to manipulate resource agents for individual resources. In a microservice architecture,
    all resources are deployed in containers. Thus, the HA framework only requires
    implementing the mechanisms to monitor the status of corresponding services. Regardless
    of how the service fails, the HA framework will perform the same operations: kill
    the existing container and create a new one in its place. By following such “cattle
    vs. pet” principle, HA in microservice architecture achieves simplicity, flexibly,
    and efficiency, compared to conventional HA approach. Snapshot/migration Snapshot
    and migration mechanisms pro- vide the ability to recover the system from planned
    host maintenance owing to hardware upgrade, soft reboot, etc. However, snapshot
    and migration introduce overhead when persisting state to local disk or transferring
    state across hosts. In the following paragraphs, we measure such overhead with
    respect to time and size of state for snapshotting and migrating the VM/container
    holding the MySQL database service in OpenStack. Table VI. Snaopshot/migration
    comparison of vm and container (the service inside is mysql handling openstack
    keystone user creation requests To perform a VM snapshot, we use virsh save and
    for Docker containers, we use docker checkpoint. Given the immaturity of container
    memory migration mechanisms (see subsection III-C), we emulate migration by using
    scp to send the checkpointed state from the source to the destination host. Table
    VI shows the results of our experiments. For both VM and container, the majority
    of the snapshot operation involves dumping the memory pages into local disk. Thus,
    the duration is proportional to the resident set size (RSS) of the process. Since
    the VM process 2 typically consists of a large number of processes, the RSS of
    the VM is significantly larger than a container where there are a small number
    of microservice processes. Therefore, we observe that it takes about 7 seconds
    to snapshot the VM (RSS is 530MB), while the time is < 1 second for container
    (RSS is 107MB). As expected, the smaller RSS of the container leads to faster
    transfer time for the pre-copy round of migration. Overall, lightweight containers
    are faster to recover from failure and have less overhead in snapshot/migration.
    In addition, the simplicity of the HA model of the microservice architecture allows
    it to be more easily extended to support different services compared to existing
    HA model exemplified by Pacemaker. This subsection presents our evaluation of
    the scalability of the proposed architecture when handling increased number of
    requests. Workload requests are generated to stress the Keystone component (stateless)
    and the MySQL galera cluster (stateful), with a variety of concurrent connections.
    Since these workloads are neither CPU nor memory intensive, VMs and containers
    have comparable performances. Here, we report the results demonstrating the ability
    of containers in handling increasing workload by quickly instantiating new service
    containers into the system. Figure 5 plots the Keystone''s user authentication
    latency curves as a function of concurrent users under different number of controller
    containers. For each curve, the latency constantly increases as the number of
    concurrent uses varies from 4 to 512 - each Keystone process has to process more
    requests. As additional controller containers are added to the cluster, the load
    balancer can distribute less requests to each of the Keystone processes, leading
    to faster authentication time. Therefore, with higher number of concurrent users
    (from 128 to 512), the 3-controller case noticeably reduces the latency to more
    than half, compared to I-controller case. Since containers can often be instantiated
    in seconds, the container-based microservice style DevOps allows the OpenStack
    control plane to better respond to workload variations in a timely fashion. Fig.
    5. Average and 95th percentile latency of user authentication under different
    number of controller containers (lower is better)) Show All Fig. 6. Scale the
    database performance (higher is better) by increasing the number of mysql containers
    under increased workload with fixed number of controller containers (three controller
    containers). Show All Figure 6 plots the throughput of the MySQL database with
    different cluster sizes. Workloads are generated by varying the number of concurrent
    users (up to 512) and the transaction type (read-only and read-write). The results
    demonstrate the performance benefit of scaling up the database cluster. For both
    workload settings, the 3-node cluster outperforms the 1-node and 2-node cluster.
    This happens because like Keystone, more database node in the cluster implies
    less overhead on individual node. For the read-write transactions (Figure 6(b)),
    however, the performance improvement is less than the read-only transactions (Figure
    6(a)) since write operations can not be committed until the write transactions
    are certified by all the nodes in the cluster. The result is that adding additional
    nodes may decrease the write performance. We do not experiment with write-only
    workload, because in reality, OpenStack workloads are a mix of read and write
    transactions. We note that although stateful components like databases have replication
    overhead for write operations, the overheads are tolerable since the write operation
    does not dominate the workload. As a result, when a bottleneck is found for certain
    components, it is possible to scale up the performance by provisioning more containers
    of the corresponding node type. SECTION V. Limitations and Discussion This work
    reveals several open issues in current container technology, in the context of
    containerizing infrastructure cloud services, a non-typical usage scenario for
    containers. The infrastructure services need access to OS-level facilities, e.g.,
    kernel modules, in-memory state, and physical devices. Such requirements make
    it difficult to containerize these types of services without sacrificing portability
    and security, which we further elaborate below. Os for Containers Our current
    containerization approach for interacting with host OS is somewhat ad-hoc in that,
    in some cases, we run the containers in privileged mode and develop specific workarounds
    to circumvent restrictions imposed by the Linux kernel. This presents a barrier
    to containerization and exposes security risks. To address this issue, we argue
    that OS should expose more kernel functions to containers via special interfaces.
    For example, fine-grained access control can be defined for privileged OS services
    (e.g., loading kernel modules) without sacrificing safety. Atomic [36] and CoreOS
    [37] are Linux distributions for containers but have complex mechanisms for specifying
    access control. Potential solutions include micro-kernel and multikernel OS architectures
    that expose selected kernel functions at the user-level [38], [39]. Thereby, containers
    can manage kernel services through well-defined interfaces. Multikernel further
    allows kernel state to be migrated between containers, which provides additional
    benefits to the cloud infrastructure services such as upgrading compute/network
    node without the need to reboot. Container State Replication We believe that container
    state replication should be further improved. Things to consider include weighing
    the overhead of replication with the new opportunities made possible by this mechanism
    in allowing the deployment of critical services in containers. With state replication
    enabled, we expect to see reduced error rate during failover (see Table V). Container
    state replication can also be directly used for container live migration [40].
    While there are some active development efforts in this area [29], it is worth
    debating whether this capability is really beneficial in a container world. Direct
    I/o in Container Having access to I/O devices (e.g., NIC) is critical for some
    infrastructure services (e.g., OpenStack Neutron). However, existing container
    mechanisms for accessing host devices are either inefficient (via NAT) [10], or
    ad-hoc (e.g., via privileged containers). We believe enabling direct I/O access
    from container solves the performance issue, but introduces portability and security
    concerns, especially when migration of these containers needs to be supported.
    SECTION VI. Related Work Our work is enabled by advancement in modern container
    technology and draws inspiration from best practices in deploying and operating
    large-scale distributed systems. Specifically, our design is largely based on
    techniques for delivering and managing applications using Docker container. A
    large number of applications have been shipped by Docker, because Docker provides
    a portable execution environment including code, runtime, system library, etc.
    Examples found in Docker Hub [41] include well-known applications, from front-end
    web server (e.g., httpd, nginx) to backend data store (e.g., MySQL, redis, mongodb),
    from tiny OS (e.g., Cir-rOS) to full-fledged programming runtime (e.g., golang,
    ruby), from standalone analytic service (e.g, single Spark instance) to complex
    distributed applications (e.g., storm, hadoop). Our work extends the scope to
    cloud infrastructure management software such as OpenStack. One recent project
    that also explores containerizing Open-Stack is Kolla [8]. Kolla aims at providing
    production-ready containers that allow rapid OpenStack deployment with proper
    customization. However, Kolla lacks the support for either dynamic configuration
    or high availability. Our work is orthogonal to Kolla because we show that the
    missing DevOps tasks in Kolla - scale out and failure recovery - are difficult
    to achieve with human intervention. We fully automate DevOps by integrating containers
    into the microservice architecture, making DevOps more efficient and scalable.
    Microservice architecture requires an ecosystem of functionalities that allow
    container-based DevOps to obtain all the desired benefits. For example, like fleet,
    kubernetes [42] and Docker Swarm [43] are clustering tools that manage a cluster
    of containers across hosts as a single system. Consul [44], Eureka [45], SkyDNS
    [46], and ZooKeeper [47] are alternatives to etcd which provide key/value store
    to support dynamic service registration and discovery. Dynamic configuration can
    also be achieved by combining Registrator [48] and HttpRouter [49]. However, existing
    works built on these tools are limited to user-space applications. Instead of
    comparing different tools, this paper demonstrates that proper implementation
    of microservice architecture is suitable to accelerate the DevOps of infrastructure
    management software. SECTION VII. Conclusion We have discussed and shown, using
    OpenStack as a case study, that containerizing cloud infrastructure services and
    combining with a microservice style architecture greatly improve operational efficiency.
    When dealing with infrastructure services, we have shown that simply running the
    services in containers is not enough to reap the full benefits of containerization
    and microservice style design. We identified three main challenges to improving
    operational efficiency: (1) minimize cross-configuration of services, (2) maintain
    state of running services, and (3) provide safe access to host resources. We addressed
    most of these challenges and identified fundamental shortcomings of current container
    technologies that will need to be addressed before a complete solution can be
    had with using containers for infrastructure software. Our work have explored
    and evaluated different approaches for providing portable container images and
    handling stateful services. Combined with our proposed orchestration architecture,
    our prototype offers dynamic service registration and discovery which are essential
    for dealing with service scaling and failures. Based on our work, we identified
    areas for further research including: in-memory state replication, host kernel
    state management for containers, and efficient device access/sharing among containers.
    Authors Figures References Citations Keywords Metrics Footnotes More Like This
    Long Live The Image: Container-Native Data Persistence in Production 2021 IEEE
    18th International Conference on Software Architecture Companion (ICSA-C) Published:
    2021 A Novel Solution of Distributed Memory NoSQL Database for Cloud Computing
    2011 10th IEEE/ACIS International Conference on Computer and Information Science
    Published: 2011 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Container and Microservice Driven Design for Cloud Infrastructure DevOps
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.jnca.2018.07.003
  analysis: '>'
  authors:
  - Xili Wan
  - Xiaodong Guan
  - Tianjing Wang
  - Guangwei Bai
  - Baek‐Young Choi
  citation_count: 66
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Journal of Network and Computer Applications Volume 119, 1 October 2018, Pages
    97-109 Application deployment using Microservice and Docker containers: Framework
    and optimization Author links open overlay panel Xili Wan a, Xinjie Guan a, Tianjing
    Wang a, Guangwei Bai a, Baek-Yong Choi b Show more Share Cite https://doi.org/10.1016/j.jnca.2018.07.003
    Get rights and content Abstract To improve the scalability and elasticity of application
    deployment and operation in cloud computing environments, new architectures and
    techniques are developed and studied, e.g., microservice architecture, and Docker
    container. Especially, Docker container enables the sharing on operation system
    and supporting libraries, which is more lightweight, prompt and scalable than
    Hypervisor based virtualization. These features make it ideally suit for applications
    deployed in microservice architecture. However, existing models and schemes, which
    are mostly designed for Hypervisor based virtualization techniques, fall short
    to be efficiently used for Docker container based application deployment. To take
    the benefits of microservice architecture and Docker containers, we explore the
    optimization of application deployment in cloud data centers using microservice
    and Docker containers. Our goal is to minimize the application deployment cost
    as well as the operation cost while preserving service delay requirements for
    applications. In this paper, we first formulate the application deployment problem
    by examining the features of Docker, the requirements of microservice-based applications,
    and available resources in cloud data centers. We further propose a communication
    efficient framework and a suboptimal algorithm to determine the container placement
    and task assignment. The proposed algorithm works in a distributed and incremental
    manner, which makes it scalable to massive physical resources and diverse applications
    under the framework. We validate the efficiency of our solution through comparisons
    with three existing strategies in Docker Swarm using real traces from Google Cluster.
    The evaluation results show that the proposed framework and algorithm provide
    more flexibility and save more cost than existing strategies. Previous article
    in issue Next article in issue Keywords Application deploymentMicroservice architectureDocker
    container 1. Introduction Cloud computing is experiencing a rapid proliferation.
    To take its benefits, e.g., scalable, elastic, agile, cost efficient, more applications
    are migrated from private infrastructures to cloud data centers. By properly allocating
    the physical resources in clouds, e.g., CPU, memory, storage, and network resources,
    to various cloud applications, computing resources are efficiently utilized and
    the cost is reduced for application deployment and operation. Nevertheless, cost
    efficiency for application deployment in clouds is one of the biggest concerns
    for service providers and cloud operators. To achieve this goal, resource allocation
    and management have been widely studied for applications deployed under different
    architectures or paradigms. To support elastic computing in a cloud with multi-tier
    structure, resource can be scaled by adjusting the amount of physical resources
    assigned to each deployed application (Liu et al., 2015; Wei et al., 2010). There
    are two ways for resource scaling. One is to add more virtual machines (VMs) (Jiang
    et al., 2013) which is referred as the horizontal scaling. The other way is to
    allocate more resources to deployed VMs (Shi et al., 2016), which is referred
    as the vertical scaling. However, the horizontal scaling may take dozens of seconds
    to deploy VMs or wake up a physical machine (PM). For the vertical scaling, the
    supporting techniques, for example the Dynamic Voltage Frequency Scaling (DVFS),
    need additional supports from both the host operation system (OS) and guest OS,
    which will inevitably incur additional latency (Baccarelli et al., 2015; Shojafar
    et al., 2016). Various architecture, paradigms are studied to accelerate the procedures
    and optimize the cost of applications'' development and deployment over clouds.
    The emergent container based virtualization, Docker in particular, becomes an
    alternative for deploying applications in clouds. Containers share not only physical
    resources but also the operating system as well as supporting libraries, while
    traditional Hypervisor based VMs only offer an abstraction at the hardware level.
    Meanwhile, microservice architecture (Martin and James) decouples complicated
    applications into lightweight and loosely linked components. Each component independently
    performs a microservice, and could be replaced or updated without the involvement
    of other components. The advent of container and microservice architecture provide
    the possibility to improve the scalability and elasticity of application development.
    Nevertheless, due to the inherent differences, existing studies and mechanisms
    on resource allocation for Hypervisor based VM and monolithic architecture cannot
    be directly applied for resource allocation using Docker containers and microservice-based
    applications. Some work has been done towards utilizing containers to deploy microservice-based
    applications in practice. Zhou et al. (2018) studied the microservice scheduling
    problem aiming to maximum the revenue of deploying microservices. Guerrero et
    al. (2018) designed genetic approaches to determine the amount of resources assigned
    to each microservice, and how to efficiently scale while the workload changes.
    Fazio et al. (2016) summarized the difficulties in scheduling and resource management
    in deploying microservices with dynamic user requests and heterogeneous settings
    in clouds. However, most existing work did not consider the features of containers,
    e.g., dynamic resource scaling at fine granularity, image layering and libraries
    reuse, which would impact the resource usage efficiency and application deployment
    cost. We noticed the importance of considering the feature of Docker containers,
    and developed a preliminary approach to dynamically allocate resources for applications
    in data centers using docker container (Guan et al., 2017). Nevertheless (Guan
    et al., 2017), focused on the feature of Docker containers, but assumed that applications
    are deployed in monolithic architecture so that containers for a single application
    are homogeneous. The limitations of monolithic architecture, such as inefficiency
    in updating and shipping, drive us to explore resource allocation for microservice-based
    application deployment. We explore both the opportunities and challenges in deploying
    cloud applications using Docker containers under microservice architecture. Specifically,
    we consider the following unique features for application deployment with Docker
    containers under microservice architecture: ∙ Each application may consist of
    heterogeneous microservices, each of which requires different amount of resources
    and supporting libraries. ∙ The number and capacity of containers are adaptively
    determined based on not only applications'' requirements but also available resources
    on PMs. ∙ The containers deployment cost is related to available supporting libraries
    on PMs and required libraries of applications. ∙ Resource management and application
    execution functions are decoupled, and resource management is performed in a distributed
    manner. Considering the above-mentioned features, we target to optimize the deployment
    of microservice-based applications with Docker containers by minimizing the application
    deployment cost as well as operation cost. Through solving the application deployment
    problem, we aim to answer the following questions: 1) Where to place the container-based
    microservices for each application so that the deployment and operation cost could
    be minimized? 2) How much amount of resources, including computational resources
    and network resources, would be assigned to each container without conflicting
    with the resource constraints while satisfying users'' requirements? In addition,
    to be applicable in large scale data centers with fluctuated amount of workloads,
    the application deployment solution is expected to be scalable and adaptive to
    changes. In order to tackle the application deployment problem in practice, we
    first propose a novel framework named ADMD, in which the resource assigned to
    each application could be adaptively adjusted by the microservice controller for
    the application in a distributed manner. Then we develop a cost-efficient and
    scalable algorithm for each microservice controller to determine the deployment
    of execution containers and task assignment. The main contribution of this paper
    are summarized below: ∙ We mathematically model the microservice-based application
    deployment problem to minimize total cost, taking the features of Docker container
    into consideration. ∙ A scalable framework is presented for the problem which
    could dynamically adjust the amount of resources allocated to each application
    based on its requirements and status. ∙ Concerning for the difficulties of the
    application deployment problem, we decompose the origin problem as small-sized
    sub-problems, which could be independently solved in the proposed framework in
    a distributed manner. The rest of this paper is organized as follows. Section
    2 briefly introduces container-based virtualization and microservice architecture.
    Section 3 briefly overviews related technologies including related studies specifically
    within the context of resource allocation in cloud data centers. We formulate
    the cost for application deployment using microservice applications and Docker
    containers as an optimization problem in Section 4. Due to the hardness of this
    optimization problem, we decompose it into subproblems, and solve them with a
    scalable framework and a communication-efficient algorithm. Section 5 demonstrates
    the microservice-based application deployment framework, and Section 6 details
    the design about the resource allocation algorithms. Evaluations are shown in
    Section 7, and the conclude remarks are summarized in Section 8. 2. Background
    This section provides a brief background on the container-based virtualization
    and microservice architecture including definition and motivation in using these
    techniques for application deployment in data centers. 2.1. Container-based virtualization
    As an alternative of hypervisor based virtualization, container based virtualization
    has been proposed to meet the need for saving system resources (Soltesz et al.,
    2007), and attracts many attentions in recent years. Compared with traditional
    Hypervisor-based virtualization that provides isolation at hardware abstraction
    layer, container-based virtualization offers operation system level abstraction
    and isolation. It allows system resources and operation system to be shared among
    multiple containers. Google Cloud Platform and Microsoft Azure has started to
    support container based virtualization in their cloud services. Due to its light
    weight size, prompt deployment and migration (Felter et al., 2015; Dłaz et al.,
    2016), container could further improve the efficiency and flexibility of resource
    allocation in cloud data centers (Felter et al., 2015; Xavier et al., 2013). compared
    the capacity of container based virtualization with traditional Hypervisor based
    VMs, and validated that container outperforms VM in performance, lightweight and
    scalability. There are different implementations of container, e.g., Solaris 10,
    Linux Vserver, and Docker. Among these variations of container, Docker, which
    is an open source project, further enables the reuse of common supporting libraries.
    In addition, from Docker v1.11.1, the amount of resources assigned to a running
    container could be dynamically adjusted on the fly. Fig. 1 illustrates the differences
    among Hypervisor based VMs, Linux containers (LXC) (Soltesz et al., 2007), and
    Docker container. As shown in Fig. 1(a), each Hypervisor based VM (Type 2) needs
    to run a separate operating system and install all supporting libraries before
    deploying applications, while the kernel system could be shared by Linux containers
    as shown in Fig. 1(b). Docker further enables image layering that makes it possible
    to share the supporting libraries as shown in Fig. 1(c). This key feature makes
    containers, especially the Docker containers, much more lightweight, prompt and
    scalable than Hyperisor based VMs. Download : Download high-res image (299KB)
    Download : Download full-size image Fig. 1. Comparison for Hypervisor based VMs,
    Linux containers (LXCs) and Docker containers. 2.2. Microservice architecture
    Till now the most common architecture to deploy applications is monolithic. In
    the “monolithic” architecture, each deployment unit, i.e., a container or a VM,
    is an autonomous entity that handles all the functions. For example, a container
    for the HTTP server-side application would process everything for HTTP requests,
    including performing domain logic, executing database relate procedures, generating
    and sending HTML views. Despite that applications in the monolithic architecture
    are easy to deploy, it is difficult to update and less scalable. Any small modification
    would induce updating and re-deploying the entire system (Namiot and Sneps-Sneppe,
    2014), which inevitably delay application''s release cycle. To overcome this limitation
    of “monolithic” architecture, microservice architecture (Martin and James) is
    proposed that decouples complicated applications into lightweight and loosely
    coupled components. Each component independently performs a microservice with
    separate source code repository, and could be updated without the involving of
    the others. In addition, considering different resource requirements of different
    application components, microservice architecture supports independently scaling
    up of each component. Thus, microservice architecture provides the possibility
    to improve the scalability and elasticity to application development. Currently,
    microservice architecture is still in its initial phase with many challenges before
    being widely employed in application deployments. Container-based virtualization,
    due to its light-weight and prompt, is treated as a good match to accelerating
    the application of microservices architecture in practice. 3. Related work As
    an essential issue in cloud computing, application deployments have been widely
    studied for various applications. For the applications that are deployed in a
    multi-tier structure, such as Hadoop MapReduce (Dean and Ghemawat, 2008), a single
    computing job could be partitioned into multiple tasks by a central manager. Then,
    these tasks are assigned to a set of worker nodes placed on servers with limited
    physical resources. In order to fully utilize the limited resources while ensuring
    the quality of services, work (Alicherry and Lakshman, 2013; Xu and Tang, 2014)
    discussed the strategies to determine the number and placement of worker nodes.
    In addition, after placing a proper number of worker nodes, certain amount of
    tasks are assigned and executed on those worker nodes. The assignment of tasks
    and the scheduling of different tasks on worker nodes impact the performance of
    applications (Kwon et al., 2012; Le et al., 2014; Kc and Anyanwu, 2010; Sandholm
    and Lai, 2010). Kwon et al. (2012) and Le et al. (2014) suggested dividing tasks
    with a large amount of computing requirement into pieces so that all the tasks
    could be completed within a similar time. Kc and Anyanwu (2010) and Sandholm and
    Lai (2010) proposed tasks scheduling strategies to ensure the fairness among different
    jobs. It is worth to note that in the multi-tier structure, resources could be
    easily scaled and adapted to various application workloads by adjusting the number
    of worker nodes assigned to each task (Liu et al., 2015; Wei et al., 2010). Although
    the multi-tier structure supports resource allocation at fine granularity, it
    is mainly used for big data computation rather than complicated applications,
    e.g. game hosting, video conference. For those complicated applications, specific
    execution environments and functions are required, including operation systems,
    and various supporting libraries. These environments can hardly be built by using
    multi-tier structure. To satisfy the specific requirements of these complicated
    applications, VMs are employed to simulate the environments of physical servers
    while allowing resource sharing. Multiple VMs could be consolidated or isolated
    from each other in the same PM. The mapping from VMs to PMs, named VM placement
    problem have been studied towards different objectives, such as maximizing the
    number of embedded VMs (Tang et al., 2007; Wang et al., 2011), minimizing the
    cost and maximizing the revenue (Ardagna et al., 2012), minimizing the energy
    consumption (Aroca et al., 2016; Xiao et al., 2013; Beloglazov et al., 2012; Guan
    et al., 2014), and improving the reliability (Machida et al., 2010). In addition,
    resource management for applications with varied workload has been taken into
    consideration as well. Jiang et al. (2013) studied automatic scaling from horizontal
    direction, while (Shi et al., 2016; Lama et al., 2013) focused on adaptive scaling
    from vertical direction. However, in horizontal scaling, deploying or waking up
    a physical server may take up to dozens of seconds, while vertical scaling needs
    additional supports from both host operation system and guest operation system.
    Therefore, dynamic scaling is not quite efficient and practical for resource allocation
    using Hypervisor based VM. As an alternative of Hypervisor based VM, container
    is light weight and could be promptly deployed. Motivated by the benefits of containers,
    effort has been done towards deploying application using containers. Various objectives
    have been considered while satisfying resource constraints. Sureshkumar and Rajesh
    (2017) aimed to minimize the energy consumption in container based resource allocation
    through dynamically switches the status of containers between sleep and awake
    based on a predefined threshold. Ahmed et al. (2017) designed a process state
    synchronization mechanism to assist the migration of computation instance between
    different clouds with the minimized network interruption. In (Ahmed et al., 2015),
    they also summarized recent proposed approaches for deployment of delay sensitive
    applications in mobile cloud computing, with the emphasizing on the ways to reduce
    application response time and user interaction delay. Zhang et al. (2017) formulated
    the container placement problem as ILP to minimize the deployment cost including
    host energy cost and image transmission cost. Considering the hardness of the
    application deployment problem, artificial intelligent based algorithms have been
    employed to determine the placement of containers, e.g., ant colony optimization
    based algorithm in (Kaewkasi and Chuenmuneewong, 2017), artificial fish swarm
    based algorithm in (Li et al., 2016). Besides, evolutionary computation algorithm
    (Vigliotti and Batista, 2014) and game theory (Xu et al., 2014) were applied to
    solved the container placement problem as well (Vigliotti and Batista, 2014).
    utilized containers to retrieve resource usage status and develops Knapsack based
    algorithm and Evolutionary Computation algorithm to place containers (Xu et al.,
    2014). partitioned a job into a set of sub tasks and uses game theory to solve
    the resource allocation problem at container level (Tao et al., 2017). designed
    a fuzzy inference system node selection algorithm for container deployment. The
    above mentioned work introduced various mechanisms and approaches for adopting
    container in application deployment. However, most of them model the container
    placement problem as a knapsack problem, without taking the features of Docker
    container into consideration. Specifically, to make fully use of Docker containers''
    feature, there are two main differences compared with traditional knapsack problem:
    ∙ Docker container supports operation system and libraries reusing, so that the
    total deployment cost of a set of containers on the same PM, is not the sum of
    each of their deployment cost. ∙ The number of containers and their capacities
    could be dynamically adjusted on the fly based on time varying applications''
    workloads and available physical resources. Facing the differences in Docker container
    based application deployment, we proposed an initial solution in (Guan et al.,
    2017), where Application Oriented Docker Container (AODC) based resource allocation
    is developed. In AODC, resource management and application execution functions
    are decoupled and performed on different kinds of containers, and resource management
    is performed in a distributed manner. Despite presenting the possibility of improving
    cost efficiency and scalability in (Guan et al., 2017) by adapting Docker container
    than using traditional Hypervisor-based VMs, this work is based on monolithic
    architecture in which the containers for an application are assumed to be homogeneous
    and autonomous. To deploy complex applications, researchers partitioned the applications
    into subtasks. For each subtask, a container based replica would be deployed towards
    certain optimization goal. In (Singh and Peddoju, 2017), a container-based framework
    is designed to deploy applications in microservice architecture with minimum downtime
    for application integration and delivery. Zhou et al. (2018) aimed to maximize
    the revenue by properly scheduling jobs with and without deadline while considering
    the dependence relationship between subtasks (Guerrero et al., 2018). illustrated
    genetic approaches to determine the amount of resources assigned to each microservice,
    and studied how to efficiently scale while the workload changes. These studies
    discussed the tasks assignment and resource allocation for applications in microservice
    architecture. Even though containers are utilized as the basic units to execute
    microservices, most of these studies focused on the partition among multiple subtasks,
    but omitted the features of Docker containers. Different from existing work and
    our preliminary study (Guan et al., 2017), we utilize the unique features of Docker
    containers and microservice architecture for 1) improving resource usage efficiency,
    2) reducing the deployment cost, 3) building an accurate and practical model of
    application deployment. In addition, to deal with a large amount of applications,
    we design a framework for deploying applications using microservice and Docker
    in a distributed manner. With this framework, the resources assigned to each application
    would be monitored and dynamically adjusted independently by its controller. Therefore,
    the proposed solution is scalable while applications'' workloads or the number
    of applications change. 4. Problem formulation In this section, we model the applications
    deployment application using microservice and Docker as an optimization problem,
    aiming to minimize the deploying cost while satisfying the application''s QoS
    requirements. Here, we define the total deployment costs of an application as
    the summation of operation cost for each individual microservice in the application.
    The notations used in this section is listed as in Table 1. Table 1. Notations
    used. Notation Explanation Gp The physical network Np The set of physical nodes
    cp(i) Residual resources on PM i, i ∈ Np The set of supporting libraries sp(i)
    An indicator that if physical node i is in sleep or awake status Λ The set of
    applications needed to be deployed Ga The application Ga, Ga ∈ Λ Na The set of
    microservice Na for application Ga wa(u) Total number of requests for application
    Ga that need microservice u ta(u) The amount of unit time to finish an instance
    of microservice u with a unit amount of physical resources Ta(u) The maximum allowed
    service delay to complete an instance of microservice u Fa(u) Expected amount
    of internal traffic between controller and containers for microservice u in application
    Ga Ca(u) The minimum amount of computational resources demanded for guarantee
    the service delay bound I(a) The PM embedding the controller for application Ga
    Unode(i, a) The node cost for installing a microserive of application Ga on physical
    node i Ps The baseline cost of a PM Pl(k) The installation cost for setting up
    Library k, Po The operation cost of performing a unit amount of workload Ulink(i,
    a) The link cost for communication between the controller and the Docker container
    on PM i for application Ga D(i, j) The distance between PM i and PM j f(i, a,
    u) The amount of traffic passing through the path between PM i for microservice
    u and the controller for application Ga x(i, a, u) A variable indicating number
    of requests for microservice u'' that is embedded in physical node i s(i) Binary
    variable if any micro-service is embedded in physical node i or not lp(k, i) An
    indicator that if supporting Library k, available on physical node i la(k, u)
    An indicator that if supporting Library k, is required for microservice u in application
    Ga l(k, i) Binary variable that library k would be install on PM i 4.1. System
    model We consider a physical network Gp(Np) with a set of PMs. Each PM is equipped
    with limited physical resources. Here, we use computational resources as an example
    to model the resource allocation problem. We assume that these PMs are identical
    in capacity and price. However, the PMs may install different libraries in advance
    to support different microservices for applications. A set of applications are
    deployed with resources in the physical network. Each application can be represented
    as a set of loosely-coupled microservice. Each microservice u of an application
    takes responsibility to complete a specific subfunction for requests to this application.
    To provide fine-grained elasticity, each microservice is deployed and scaled independently
    without impacting the rest of the application. A microservice u of application
    Ga is further considered as a tuple (wa(u), ta(u), Ta(u), Fa(u), la(k, u)). Here,
    wa(u) denotes the expected amount of requests for microservice u. Each request
    for microservice u could be completed by a unit amount of computational resource
    in ta(u) unit amount of time. When more computational resources are assigned for
    serving this request, it could be finished within a less time. Ta(u) specifies
    the maximum allowed service delay to complete a request for microservice u of
    application Ga. Fa(u) specifies the expected total amount of internal traffic
    between the controller and all Docker containers for microservice u of application
    Ga. la(k, u) is a binary indicator, that equals to 1, if supporting library k
    is required for deploying microservice u. To coordinate the communications among
    microservices, each application has a controller to collects intermediate results
    from microservice and forwards them to the next microservices requiring the data.
    Assume that all the Docker containers serving microservice u are started and terminated
    at the same time, and at least unit amount of computational resources are demanded
    to guarantee the service delay bound Ta(u). 4.2. Cost-minimized application deployment
    using Microservice and Docker Given a physical network and a set of applications,
    each application consists of different microservices. We want to determine the
    number and placement of Docker containers, and the amount of requests assigned
    to each Docker container so that the total cost of applications is minimized.
    The cost of an application is defined as the summation of the cost for each individual
    microservice of this application. Fundamentally, the cost of each microservice
    of application Ga comes from the node cost Unode and link cost Ulink. Specifically,
    Unode includes deployment cost and execution cost of Docker containers, while
    Ulink quantifies the communication cost between Docker containers. We adapt the
    widely accepted linear power model (Krishnan et al., 2011; Chen et al., 2015;
    Guan et al., 2015) to estimate the cost of a PM i that consists of baseline cost
    and operation cost. Here, we assume that the operation cost is proportional to
    the workload assigned to the PM i. To save deployment cost, we utilize PMs that
    support the sleep/awake mode, and assume no baseline cost for a PM in sleep mode.
    Besides the baseline cost and operation cost, deploying an Docker container may
    bring additional cost for installing and configuring necessary supporting libraries
    of the application. Therefore, for the node cost, three factors are taken into
    consideration when counting, i.e., the cost for waking up an inactive PM, the
    cost for installing supporting libraries, and the cost for serving assigned requests.
    Thus, the node cost Unode(i, a) for installing microservices of application Ga
    on a PM i, is modeled as shown in Eq. (1). (1) Ps is the cost for wake-up a PM.
    The binary variable s(i) is set to 1, if a PM would be switched from off status
    to on, otherwise 0. Pl(k) is the cost for installing library k on a PM, and l(k,
    i) is a binary variable that library k would be install on PM i. Variable x(i,
    a, u) denotes the number of requests assigned to the Docker container on PM i
    for microservice u. Since a request is not allowed to be decomposed, x(i, a, u)
    is a non negative integer. Ca(u) is the amount of physical resources required
    to complete all requests for microservice u of application Ga before the deadline.
    wa(u) is the total amount of requests for microservice u. is the amount of physical
    resources needed to complete all assigned requests for microservice u on PM i
    before the deadline. Currently, the voltage and frequency of PMs are assumed to
    be fixed in our model. In the future, we would consider dynamic voltage and frequency
    for further cost saving as in (Baccarelli et al., 2015). The link cost Ulink describes
    the data exchange cost between the controller and Docker containers for a single
    application, which is highly related to the considered technology, e.g., virtual
    links using TCP/IP connections (Cordeschi et al., 2012). Here, we consider the
    general scenario without assumptions on the underlying technology as in (Chowdhury
    et al., 2009). We further assume that traffic between the controller and Docker
    containers is unsplitable, and always goes through the shortest path for simplicity.
    Then, the link cost is modeled as the product of the path length and expected
    traffic amount between the controller and containers. When the controller of an
    application Ga has been determined, the link cost for application Ga between PM
    i, and the selected PM to place controller I(a), Ulink(i, a), is modeled as: (2)
    D(I(a), i) is the length of the shortest path between PM I(a) and PM i. Note D(I(a),
    i) is known when PM I(a) is determined as the request of deploying an application
    Ga arrives in a data center. This process would be further discussed in Section
    5. We do not consider link failure and reconfiguration, so D(I(a), i) is fixed
    for application Ga and PM i, i ∈ Np. f(i, a, u) indicates the amount of traffic
    passing through this path for microservice u of application Ga. Assume that f(i,
    a, u) is proportional to the amount of requests assigned to the Docker container
    on PM i. Thus, we have . To save communication cost, Docker containers with heavier
    traffic are preferred to be embedded near the controller. As we consider the node
    cost as well as the communication cost, a coefficient α ∈ [0, 1] is employed to
    balance the influence of the node cost and the communication cost to satisfy various
    application and performance requirements. Then, by summing up the balanced cost
    for deploying a set of applications, the objective function of applications deployment
    using microservice with Docker could be formulated as follows: (3) In addition,
    this optimization problem is subject to the following constraints: ∙ The available
    physical resource is limited on each PM. We restrict that the total provisioned
    workloads would not exceed the maximum available physical resources on each PM.
    (4) ∙ Similar as existing work, it is guaranteed that each request for microservice
    u would be assigned to exactly one Docker container. (5) ∙ Before assigning workload
    to a PM i, the status of PM i should be checked. If it is in sleep mode, it will
    be switched to awake status. (6) ∙ Concerning on the possible differences between
    required supporting libraries and the installed libraries on PM i, all additional
    libraries should be installed to make sure a mircoservice u could be executed
    on a PM i. (7) ∙ The variable s(i) that indicates if a PM i should be switched
    from sleep to awake status, and the variable l(k, i) that indicates if a library
    k would be installed on PM i, are binary variables. Constraint is included to
    confine their range. (8) ∙ The variable x(i, a, u) denotes the number of requests
    for microservice u of application Ga that are assigned to PM i. We assume that
    the workload for a single request cannot be further decomposed. The variable x(i,
    a, u) is an integer variable. (9) As above presented, our aim is to optimize the
    weighted total cost of PMs and communications under a set of constraints by solving
    the optimization problem Eq. (3) subject to Eqs. (4), (5), (6), (7), (8), (9).
    We refer this optimization problem for Application Deployment using Microservice
    and Docker container as the ADMD problem. Note that this ADMD problem is NP-hard,
    since the well known bin packing problem can be reduced to the special case of
    this problem, when no common supporting libraries can be shared by any two mircoservices.
    5. A framework for deploying microservice-based applications with docker Considering
    the hardness of the ADMD problem, and scalability requirements of deploying cloud
    applications, we develop a framework for deploying microservice-based applications
    with Docker, as well as a resource allocation algorithm that runs independently
    on each controller in a distributed manner. This algorithm acquires the resources''
    status from a limited number of Docker Engines and makes decisions for the Docker
    container''s placement and task assignment for microservices of an application.
    Before presenting the resource allocation algorithm, we illustrate the framework
    for applications deployment using microservice and Docker (ADMD framework), which
    is the base of the algorithm described in Section 6. In the ADMD framework, application
    requests are processed as microservices on Execution Containers (ECs). The allocation
    and management of resources for applications are decentralized and performed on
    Microservice Controller (MSCs). Unlike the Hypervisor based VM placement, the
    number of ECs and their demands on physical resources are dynamically determined
    by not only the applications'' workload but also the available resources in the
    data center. The overview of the ADMD framework is illustrated in Fig. 2. Download
    : Download high-res image (566KB) Download : Download full-size image Fig. 2.
    Framework for deploying microservice-based applications with Docker. As presented
    in Fig. 2, when deploying an application, resources are allocated to the application
    as a set of containers distributed on multiple PMs. In particular, each application
    has a MSC and at least one EC. A MSC makes resource allocation decisions, requests
    resources for ECs, tracks the task status on ECs, and manages the life cycle of
    ECs. ECs complete the assigned tasks, and report to the MSC about their task execution
    status compared with the expected progress. In addition, the MSC collects and
    distributes the intermediate data generated by microservices for the application.
    Based on the execution status of ECs, the MSC would adjust allocated resources
    to ECs, balance ECs'' workloads, or migrate ECs. If the execution status of an
    EC is behind schedule, the MSC could 1) dynamically add more resources for this
    EC, 2) balance its workload to other ECs, or 3) migrate this EC to another PM
    with enough resources. Both MSCs and ECs are embedded on PMs. Fig. 3 depicts the
    main components in a PM. Here, each PM has a host OS, on which Docker runs an
    engine to maintain the operating environment for containers, assist embedding
    containers, and isolate containers running on the same PM. In addition, we introduce
    a scheduler on each PM to manage the life cycle of the MSCs and ECs running on
    this PM. When receiving an application deployment request, the gateway (as shown
    in Fig. 2) distributes the request to 1 p.m. according to predetermined policy,
    e.g., load balancing, location preference. The scheduler on the selected PM, e.g.,
    physical machine 1 in the Fig. 3, creates a MSC and assigns resources to it based
    on the application''s requirements and available resources on the PM. It may forward
    the application deployment request to other schedulers, if there is not enough
    physical resource on the selected PM. For the other schedulers on the PMs that
    have not been selected to place MSCs, e.g., physical machine 2 in the Fig. 3,
    they 1) report local available resources when receiving queries from a MSC, 2)
    approve or reject the MSCs'' requirements on creating new ECs, and 3) recycle
    the physical resources when ECs complete the assigned tasks. In addition, the
    Docker Engine enables the OS kernel and common supporting libraries to be shared
    by multiple ECs, so that aggregating applications that share libraries could further
    save costs by reducing redundancy. As demonstrated in Fig. 3, by placing the EC
    for application A (EC1-A) and the EC for application B (EC1-B) on the same physical
    machine 2, the common libraries (cycled in the Fig. 3) could be shared rather
    than be installed separately for each application. Download : Download high-res
    image (640KB) Download : Download full-size image Fig. 3. Components in a physical
    machine: physical machine 1 embeds microservice controllers, while physical machine
    2 embeds execution containers. We illustrate the workflow of deploying an application
    in a data center in Fig. 4. When an application request arrives, the gateway dispatches
    the request to a PM based on certain policy, e.g., load balancing, location preference
    (steps (1)). The scheduler on the selected PM (PM 1 in Fig. 4) then initiates
    a MSC and assigns physical resources for the MSC. It then replies with a message
    indicating the MSC has been successfully created (step (2)). The MSC queries a
    set of PMs about their available resources, supporting libraries installed (step
    (3)), and receives replies from the PMs (step (4)). Based on the application''s
    requirements, and available resources on PMs, the MSC makes resource allocation
    decisions using the proposed EPTA algorithm (Algorithm 1, discussed in Section
    6), and requests resources on selected PMs (PM 2 and PM 3 in Fig. 4) for creating
    ECs based on the decision (step (5)). If the request has been approved by the
    scheduler on the selected PM (PM 2 in Fig. 4), the PM would provision resources,
    create an EC for the microservice, and send a confirmation message to the MSC
    (steps (6)). After that, tasks are assigned to the EC according to the resource
    allocation decision. PM may reject EC creating requirement (step (7)) when it
    does not have enough resource. In this case, the MSC has to repeat the step (5)
    and request for creating the ECs on other PMs (step (8) and (9)). A MSC may repeat
    the steps (4–6) to create multiple ECs based on the resource allocation decision.
    When all ECs are created, the application has been successfully deployed (step
    (10)). Download : Download high-res image (397KB) Download : Download full-size
    image Fig. 4. An example of the communication sequence of deploying an application
    in a data center based on ADMD framework. The created ECs work on assigned tasks
    and update task execution status to the MSC. Based on the real time workload of
    the application, a MSC may dynamically adjust the number, location and assigned
    resources of ECs. When the application is deactivated, the MSC is terminated and
    its resources are collected by the scheduler. Under this framework, physical resources
    allocated for each application could dynamically shrink or expand based on the
    application''s requirements, real time workload and available resources in the
    data center. Since the resource allocation decision is made by each MSC independently,
    the framework is scalable in data centers with a large amount of physical resources
    and diversity applications. To further minimize the cost of applications deployment
    in the framework, we develop a scalable algorithm to solve this problem in Section
    6. 6. Algorithms Considering the hardness of the ADMD problem and scalability
    requirements of deploying cloud applications, we develop a scalable algorithm
    that runs independently on each MSC in a distributed manner to acquire the resources''
    status from a limited number of Docker Engines and to make resource allocation
    decisions. To reduce the computational complexity of origin ADMD problem, we decompose
    it to sub-problems ADMD-s. Every ADMD-s aims to minimize the deployment cost for
    a single application Ga (Eq. (10)) subjected to the constraints on available resources,
    total workloads of microservices, and existence of supporting libraries (Eqs.
    (11), (12), (13), (14), (15), (16)). (10) subject to: (11) (12) (13) (14) (15)
    (16) By solving the sub-problem ADMD-s, a MSC only need to determine for a single
    application where to place ECs and the amount of requests assigned to each EC.
    Compared to origin ADMD problem, the solution space for ADMD-s has been significantly
    reduced. In addition, the computation is decentralized to MSCs that are allocated
    on different PMs, so that the workload of the gateway server is alleviated. We
    name the EC Placement and Task Assignment algorithm for microservice-based application
    as EPTA, and present it in Algorithm 1. Algorithm 1. EC Placement and Task assignment
    Algorithm for microservice-based application i (EPTA). Input: Physical network
    in region(s) ; Resource allocation request GA(NA); A set of neighbor region IDs
    setn Output: Allocation decision for microservice u  1: Query PMs in the local
    region, e.g., a rack, for available resources  2: Solve the optimization problem
    (10) subject to (11), (12), (13), (14), (15), (16) using LP solver, e.g., CPLEX  3:
    If a feasible solution exist, send resource request to PM i for the amount of
    for each variable x(i, u) in the solution  4: Wait for the response from PM i  5:
    If the EC creation request has been approved by PM i, record x(i, u) into directoryu,
    and update the status of PM i  6: If the EC creation request has been rejected
    by PM i, record x(i, u) into setu, and update the status of PM i  7: When the
    responses for all requests have been received, but setu is not null  8: Update
    the resource allocation request by deducting allocated microserives  9: Extend
    searching area, e.g., a pod, and call the EPTA algorithm again As shown in Algorithm
    1, the MSC starts querying available physical resources within a small local region,
    e.g., a rack (Step (1)). Based on the available resources reported by the Docker
    Engine on each PM, the ADMD-s problem (Step (2)) could be solved by adopting Linear
    Programming (LP) solver, e.g., CPLEX. Then, the MSC requests physical resources
    based on the optimization problem''s solution (Steps (3)), and waits for the responses
    from PMs (Step (4)). If the request is approved, an EC is built on the selected
    PM, and the status of the PM, e.g., available resources, installed supporting
    libraries, is updated (Step (5)). The MSC records the deployed EC x(i, u) into
    directoryu, so that it could track each EC for dynamic adjustments. If the request
    is rejected, this PM is marked as infeasible and the assigned task is moved to
    setu (Step (6)). When there is no feasible solution for the optimization problem,
    or some tasks have not been successfully assigned, the MSC queries PMs in a larger
    scale, e.g., a pod, and solves the optimization problem again for not assigned
    tasks (Steps (7–9)). Algorithm 2. Microservice embedding procedures on ECs. 1:
    When receive a resource query from a MSC, share current status 2: When an EC creation
    request has been received, check if its resource demands could be satisfied with
    residual resources 3: If there is not enough resources, send a response 4: If
    there are enough resources, reserve the resources, create an EC and send a response
    5: For each embedded EC, build a channel between the EC and its MSC for control
    communications Corresponding to the EPTA algorithm running on MSC for searching
    available resources and deploying cloud applications, PM responses the queries
    and requests from MSC as described in Algorithm 2. When a PM receives a resource
    inquiry, the scheduler on this PM shares the current status of this PM including
    residual resources and supporting libraries installed with the MSC (Step (1)).
    When the scheduler receives an EC creation request, it checks if residual resources
    could satisfy the demands of the EC (Step (2)). If current available resource
    cannot meet demands, the scheduler replies a response to reject the request along
    with updated status (Step (3)). Otherwise, the scheduler reserves demanded resources,
    installs necessary supporting libraries, and downloads the EC image from repository.
    After that, an EC has been successfully created and the scheduler sends a confirmation
    message to the MSC (Step (4)). In addition, for each embedded EC, the scheduler
    on the PM maintains a secure channel for control communications and intermediate
    results updates (Step (5)). Note that in the EPTA algorithm, a MSC initially only
    queries a small area rather than the entire physical network, then incrementally
    expands the searching area when there are not enough available resources. Each
    PM would be queried at most once, and in most cases, only a portion of the physical
    network would be queried. Therefore, the EPTA algorithm is communication-efficient.
    Meanwhile, the EPTA algorithm executes in a distributed manner by the MSCs. Each
    MSC determines the placement of ECs and job assignments for an application. Therefore,
    the algorithm scales with the number of applications. In addition, for each application,
    the searching area is incrementally increased, and only a part of the physical
    network for most applications would be queried. Thus, the EPTA algorithm is scalable
    while the size of physical network grows. 7. Evaluations In this section, we evaluate
    the performance of our EPTA algorithm in different context through trace-driven
    simulation studies. All evaluations are based on real traces from Google Cluster
    Traces. To demonstrate the necessity of considering the Docker containers'' feature,
    we compare the EPTA algorithm with three strategies implemented in Docker Swarm
    Strategies, including Spread, Binpack and Random, with respect to different aspects.
    Docker Swarm is an orchestration framework that is currently integrated into Docker
    Engine for container swarm creation and management as well as application deployment.
    In addition, even through the performance differences between Hypervisor-based
    VM and container have been widely studied and compared, e.g., (Felter et al.,
    2015; Xavier et al., 2013), for readers'' convenience, we still include a Hypervisor
    based VM embedding algorithm in the comparison to show the differences between
    container and Hypervisor-based VM in application deployment. The three strategies
    of Docker Swarm and Hypervisor-based VM embedding are explained as follows: ∙
    Spread selects the PM with the least number of containers to place new containers.
    ∙ Binpack selects the PM that is the most packed to place new containers. ∙ Random
    selects PMs randomly regardless of PMs'' resource usage status. ∙ Optimal-VM selects
    PMs based on the optimal solution of Hypervisor-based VM embedding problem achieved
    by CPLEX For the sake of simplification and to simulate large-scale scenarios,
    we implement the five algorithms and base our simulation on real traces from Google
    Cluster Traces. These traces specify more than 1,000,000 tasks along with their
    arrival time and resource requirements during 7 h. Each task maps a set of Linux
    processes. Considering the large amount of tasks in these traces, only the tasks
    arrived in the first hour are used in our evaluation. In addition, due to different
    granularity between a task and a microservice, we group every 50 tasks as a single
    microservice, and use the arrival time for the first task as the arrival time
    for the microservice request. The physical network is randomly generated by Overview-NetworkX,
    and the maximum available computational resource of each PM is randomly chosen
    from (1,2,4,8,16) core(s). In addition, we randomly set the number of supporting
    libraries required by one microservice between (Liu et al., 2015; Baccarelli et
    al., 2015), while the supporting libraries are randomly selected from a pool of
    10 libraries. The size of each libraries is set between [0.03,0.15] GB. 7.1. On
    the number of PMs We first compare the total deployment cost of our EPTA algorithm
    with three strategies of Docker Swarm and the Optimal-VM while the number of available
    PMs varies from 50 to 150, then check the PM active rate in the network and the
    average number of deployed libraries on each PM. Through this set of evaluation,
    we want to demonstrate the performance of EPTA in physical networks with different
    scales. Here, PM active rate is defined as the ratio of the number of active PMs
    to the total number of PMs, while the average number of deployed libraries is
    the average number of supporting libraries installed on each PM for microservices
    deployed on this PM. The more active PMs induce more baseline cost, while a larger
    average number of deployed libraries indicates more library installation cost.
    For all the five algorithms, they are executed online, with no knowledge about
    next arriving applications. Thus, they make decisions based only on current status
    of the physical network and the application. We also take a look at the computation
    time of each algorithm. As shown in Fig. 5, EPTA outperforms other deployment
    algorithms in total deployment cost, while Optimal-VM spends the most deployment
    cost. The application deployment costs of the three Docker swarm strategies are
    between that of EPTA and Optimal-VM. As the number of PMs increases from 50 to
    150, the total deployment costs of EPTA slightly drop. This is because when more
    PMs are available, the potential to find a better PM to place a microservice increases.
    However, the total deployment costs of other strategies and algorithm increase,
    since they occupy more PMs as shown in Fig. 6. Download : Download high-res image
    (163KB) Download : Download full-size image Fig. 5. Average application deployment
    cost with varied number of PMs. Download : Download high-res image (157KB) Download
    : Download full-size image Fig. 6. Number of active PMs with varied number of
    PMs. In Fig. 6, for load balancing, Spread always utilize all the PMs to deploy
    the coming applications, while EPTA, Optimal and Binpack use a portion of the
    PMs when there is enough number of PMs. Random also utilizes a large portion of
    PMs, thus, its deployment cost grows as the number of PMs in the network grows
    as well. We further look into the average number of supporting libraries on each
    PM in Fig. 7. Based on Fig. 7, EPTA has the smallest number of libraries installed
    on each PM. This validates that EPTA eliminates redundancy by sharing supporting
    libraries and operation systems. Compared with EPTA, all the other strategies
    and algorithm do not consider libraries reuse when deploying the applications.
    Thus, more supporting libraries are installed on PMs. Furthermore, as the number
    of PMs in the network rises, containers are distributed on more PMs, which reduces
    the average number of libraries on each PM for EPTA and the three Docker swarm
    strategies. Note that, for Optimal-VM, every embedded VM includes an OS and all
    supporting libraries. The average number of libraries on each PM when using Optimal-VM
    is proportional to the number of VM on each PM and the number of libraries in
    this VM. Therefore, we did not include the number of libraries of Optimal-VM in
    Fig. 7. Download : Download high-res image (146KB) Download : Download full-size
    image Fig. 7. Average number of supporting libraries on each PM with varied number
    of PMs. In addition, the computation time for the five strategies and algorithms
    are compared in Fig. 8. As demonstrated in Fig. 8, the three Docker Swarm strategies
    are a bit faster than EPTA and Optimal-VM for the cases of 50, 70 and 90 PMs.
    This is because that both Optimal-VM and EPTA include linear problems and use
    LP solver. However, it is worth to note that the time complexity of EPTA does
    not grow exponentially as the network scales, since EPTA incrementally extends
    the searching area rather than using the entire physical network as an input.
    Download : Download high-res image (154KB) Download : Download full-size image
    Fig. 8. Average computation time with varied number of PMs. 7.2. On the number
    of microservices We examine the performance of EPTA for deploying applications
    with single or multiple microservices in a mid-sized physical network. Specifically,
    we vary the number of microservices in each application from 1 to 5 for the physical
    network with 90 PMs, and compare the total deployment costs, number of active
    PMs, average number of supporting libraries, and computation time of the five
    application deployment strategies and algorithms. Here, to exclude the impact
    of applications'' workload and number of supporting libraries, the total workload
    of an application and the total number of supporting libraries used in an application
    keep the same. In other words, when there are multiple microservices in an application,
    these microservices are evenly assigned a portion of the application''s total
    workload, and the union of their supporting libraries equals to the supporting
    libraries required by the application. It is worth to note that when an application
    only has one microservice, it could be considered as in monolithic architecture.
    The application deployment costs of the five strategies and algorithms are presented
    in Fig. 9. It can be observed in Fig. 9 that EPTA has the smallest application
    deployment cost, while that of Optimal-VM increases significantly. Among the three
    Docker Swarm strategies, Binpack consumes the least deployment cost, while Spread
    takes the most. This is mainly because that Binpack trends to put microservices
    from the same application on the same PM, while Spread and Random trend to distribute
    the microservices in the network for load balancing. Therefore, Binpack saves
    the communication costs among microservices. In addition, as illustrated in Fig.
    10 and Fig. 11, Spread and Random wake up more PMs, and install more libraries
    on each PM, which increase their deployment cost, compared with EPTA. Fig. 12
    indicates that as the number of microservices in an application increases, the
    computation time of EPTA and Optimal-VM rises, because of the utilization of LP
    solver. In the future, we will design an efficient solver tailored for our problem
    to replace the common LP solver used in the EPTA algorithm. Download : Download
    high-res image (163KB) Download : Download full-size image Fig. 9. Average application
    deployment cost with varied number of microservices. Download : Download high-res
    image (148KB) Download : Download full-size image Fig. 10. Number of active PMs
    with varied number of microservices. Download : Download high-res image (139KB)
    Download : Download full-size image Fig. 11. Average number of supporting libraries
    on each PM with varied number of microservices. Download : Download high-res image
    (156KB) Download : Download full-size image Fig. 12. Average computation time
    with varied number of microservices. 7.3. On the varied size of supporting libraries
    Finally, we check the impact of average size of supporting libraries on the performance
    of the five strategies and algorithms. The influence of the libraries sizes are
    examined in physical networks with different scales (90 PMs and 150 PMs). As shown
    in Fig. 13 and Fig. 17, the application deployment costs of the five strategies
    and algorithms increases as the average library size grows. It does not lead too
    much changes on the number of active PMs (shown in Fig. 14 and Fig. 18) and the
    average number of libraries (shown in Fig. 15 and Fig. 19) for Docker Swarm strategies
    and Optimal-VM. However, the active PMs used by EPTA is slightly increased, and
    the number of libraries of EPTA drops. This indicates that EPTA could find a good
    balance between the baseline cost and library installation cost. Furthermore,
    there is no obvious change on the computation time of all the strategies and algorithms
    when library size varies as presented in Fig. 16 and Fig. 20. Download : Download
    high-res image (170KB) Download : Download full-size image Fig. 13. Average application
    deployment cost with varied library size (90 PMs). Download : Download high-res
    image (163KB) Download : Download full-size image Fig. 14. Number of active PMs
    with varied library size (90 PMs). Download : Download high-res image (147KB)
    Download : Download full-size image Fig. 15. Average number of supporting libraries
    on each PM with varied library size (90 PMs). Download : Download high-res image
    (146KB) Download : Download full-size image Fig. 16. Average computation time
    with varied library size (90 PMs). Download : Download high-res image (181KB)
    Download : Download full-size image Fig. 17. Average application deployment cost
    with varied library size (150 PMs). Download : Download high-res image (174KB)
    Download : Download full-size image Fig. 18. Number of active PMs with varied
    library size (150 PMs). Download : Download high-res image (141KB) Download :
    Download full-size image Fig. 19. Average number of supporting libraries on each
    PM with varied library size (150 PMs). Download : Download high-res image (150KB)
    Download : Download full-size image Fig. 20. Average computation time with varied
    library size (150 PMs). Through the above three sets of simulations, it is observed
    that EPTA could improve the application deployment cost by balancing baseline
    cost and library installation cost. This is because 1) library reuse is fully
    utilized and redundancy has been alleviate; 2) the size of containers is not fixed
    but dynamically adopted based on application''s requirements and available resources.
    In addition, EPTA could be efficiently scaled while the size of physical network
    grows. However, its computation time is still high compared to Docker Swarm strategies,
    since we adopt the common LP solver in our algorithm. We will reduce its time
    complexity in our future work. 8. Conclusion and future work By allowing sharing
    on operation system as well as supporting libraries, container based virtualization
    offers a great opportunity for reducing application deployment cost and improving
    final users'' experience. Considering the features and benefits of emergent Docker
    container, we modeled the cost efficient resource allocation for microservice-based
    applications with docker container, and proposed a communication-efficient and
    scalable resource allocation algorithm, named EPTA, to minimize the application
    deployment cost constrained by capacity and the service delay bound. The presence
    of EPTA algorithm significantly improves the deployment cost by balancing the
    PM waking-up cost, supporting library installation cost and communication cost.
    By incrementally extend searching area, it could easily scale up as the size of
    the cloud data center grows. We validated the efficiency of the proposed schemes
    with the comparison with VM placement algorithm and three strategies implemented
    in Docker Swarm in evaluations based on real data traces. For future research
    directions, we will reduce the computation complexity in EPTA by solving the LP
    model in a more efficient way. In addition, we will consider other optimization
    goal, e.g., minimize applications'' response time, energy efficiency, in our framework,
    and build a prototype which integrates our framework and algorithm into Docker
    swarm for implementation in real cloud environments. Acknowledgement This work
    is supported in part by the National Nature Science Foundation of China (Grant
    No. 61602235, 61501224), and the Natural Science Foundation of Jiangsu Province
    of China (Grant No. BK20161007). References Ahmed et al., 2015 E. Ahmed, A. Gani,
    M.K. Khan, R. Buyya, S.U. Khan Seamless application execution in mobile cloud
    computing: motivation, taxonomy, and open challenges J. Netw. Comput. Appl., 52
    (2015), pp. 154-172 View PDFView articleView in ScopusGoogle Scholar Ahmed et
    al., 2017 E. Ahmed, A. Naveed, A. Gani, S.H.A. Hamid, M. Imran, M. Guizani Process
    state synchronization for mobility support in mobile cloud computing IEEE International
    Conference on Communications (ICC) (May 2017), pp. 1-6 View in Scopus Alicherry
    and Lakshman, 2013 M. Alicherry, T. Lakshman Optimizing data access latencies
    in cloud systems by intelligent virtual machine placement IEEE INFOCOM (2013),
    pp. 647-655 CrossRefView in ScopusGoogle Scholar Ardagna et al., 2012 D. Ardagna,
    B. Panicucci, M. Trubian, L. Zhang Energy-aware autonomic resource allocation
    in multitier virtualized environments IEEE Trans. Serv. Comput., 5 (1) (2012),
    pp. 2-19 View in ScopusGoogle Scholar Aroca et al., 2016 J.A. Aroca, A.F. Anta,
    M.A. Mosteiro, C. Thraves, L. Wang Power-efficient assignment of virtual machines
    to physical machines Future Generat. Comput. Syst., 54 (2016), pp. 82-94 Google
    Scholar Baccarelli et al., 2015 E. Baccarelli, D. Amendola, N. Cordeschi Minimum-energy
    bandwidth management for qos live migration of virtual machines Comput. Network.,
    93 (2015), pp. 1-22 View PDFView articleView in ScopusGoogle Scholar Beloglazov
    et al., 2012 A. Beloglazov, J. Abawajy, R. Buyya Energy-aware resource allocation
    heuristics for efficient management of data centers for cloud computing Future
    Generat. Comput. Syst., 28 (5) (2012), pp. 755-768 View PDFView articleView in
    ScopusGoogle Scholar Chen et al., 2015 X. Chen, C. Li, Y. Jiang Optimization model
    and algorithm for energy efficient virtual node embedding IEEE Commun. Lett.,
    19 (8) (Aug 2015), pp. 1327-1330 View in Scopus Chowdhury et al., 2009 N. Chowdhury,
    M. Rahman, R. Boutaba Virtual network embedding with coordinated node and link
    mapping IEEE INFOCOM (April 2009), pp. 783-791 Cordeschi et al., 2012 N. Cordeschi,
    T. Patriarca, E. Baccarelli Stochastic traffic engineering for real-time applications
    over wireless networks J. Netw. Comput. Appl., 35 (2) (2012), pp. 681-694 View
    PDFView articleView in ScopusGoogle Scholar Dean and Ghemawat, 2008 J. Dean, S.
    Ghemawat Mapreduce: simplified data processing on large clusters Commun. ACM,
    51 (1) (2008), pp. 107-113 CrossRefView in ScopusGoogle Scholar Docker Docker.
    https://www.docker.com/. Google Scholar Docker Swarm Strategies Docker Swarm Strategies.
    https://docs.docker.com/swarm/scheduler/strategy/. Google Scholar Dłaz et al.,
    2016 M. Dłaz, C. Martłn, B. Rubio State-of-the-art, challenges, and open issues
    in the integration of internet of things and cloud computing J. Netw. Comput.
    Appl., 67 (2016), pp. 99-117 Google Scholar Fazio et al., 2016 M. Fazio, A. Celesti,
    R. Ranjan, C. Liu, L. Chen, M. Villari Open issues in scheduling microservices
    in the cloud IEEE Cloud Comput., 3 (5) (2016), pp. 81-88 View in ScopusGoogle
    Scholar Felter et al., 2015 W. Felter, A. Ferreira, R. Rajamony, J. Rubio An updated
    performance comparison of virtual machines and linux containers 2015 IEEE International
    Symposium on Performance Analysis of Systems and Software (ISPASS) (March 2015),
    pp. 171-172 CrossRefView in Scopus Google Cloud Platform Google Cloud Platform.
    https://cloud.google.com/products/. Google Scholar Google Cluster Traces Google
    Cluster Traces. https://github.com/google/cluster-data/blob/master/TraceVersion1.md.
    Google Scholar Guan et al., 2014 X. Guan, B.Y. Choi, S. Song Topology and migration-aware
    energy efficient virtual network embedding for green dcs 23rd International Conference
    on Computer Communication and Networks (ICCCN), IEEE (2014), pp. 1-8 CrossRefGoogle
    Scholar Guan et al., 2015 X. Guan, B.Y. Choi, S. Song Energy efficient virtual
    network embedding for green dcs using dc topology and future migration Comput.
    Commun., 69 (2015), pp. 50-59 View PDFView articleView in ScopusGoogle Scholar
    Guan et al., 2017 X. Guan, X. Wan, B.Y. Choi, S. Song, J. Zhu Application oriented
    dynamic resource allocation for data centers using docker containers IEEE Commun.
    Lett., 21 (3) (March 2017), pp. 504-507 View in Scopus Guerrero et al., 2018 C.
    Guerrero, I. Lera, C. Juiz Genetic algorithm for multi-objective optimization
    of container allocation in cloud architecture Springer J. Grid Comput., 16 (1)
    (2018), pp. 113-135 CrossRefView in ScopusGoogle Scholar Jiang et al., 2013 J.
    Jiang, J. Lu, G. Zhang, G. Long Optimal cloud resource auto-scaling for web applications
    International Symposium on Cluster, Cloud and Grid Computing (CCGrid) (2013),
    pp. 58-65 View in ScopusGoogle Scholar Kaewkasi and Chuenmuneewong, 2017 C. Kaewkasi,
    K. Chuenmuneewong Improvement of container scheduling for docker using ant colony
    optimization 9th International Conference on Knowledge and Smart Technology (KST)
    (Feb 2017), pp. 254-259 CrossRefView in Scopus Kc and Anyanwu, 2010 K. Kc, K.
    Anyanwu Scheduling hadoop jobs to meet deadlines IEEE Second International Conference
    on Cloud Computing Technology and Science (CloudCom) (2010), pp. 388-392 CrossRefView
    in ScopusGoogle Scholar Krishnan et al., 2011 B. Krishnan, H. Amur, A. Gavrilovska,
    K. Schwan Vm power metering: feasibility and challenges Perform. Eval. Rev., 38
    (3) (2011), pp. 56-60 CrossRefGoogle Scholar Kwon et al., 2012 Y. Kwon, M. Balazinska,
    B. Howe, J. Rolia Skewtune: mitigating skew in mapreduce applications ACM SIGMOD
    International Conference on Management of Data (2012), pp. 25-36 CrossRefView
    in ScopusGoogle Scholar Lama et al., 2013 P. Lama, Y. Guo, X. Zhou Autonomic performance
    and power control for co-located web applications on virtualized servers IEEE/ACM
    21st International Symposium on Quality of Service (IWQoS) (2013), pp. 1-10 Google
    Scholar Le et al., 2014 Y. Le, J. Liu, F. Ergun, D. Wang Online load balancing
    for mapreduce with skewed data input IEEE INFOCOM (2014), pp. 2004-2012 View in
    ScopusGoogle Scholar Li et al., 2016 Y. Li, J. Zhang, W. Zhang, Q. Liu Cluster
    resource adjustment based on an improved artificial fish swarm algorithm in mesos
    IEEE 13th International Conference on Signal Processing (ICSP) (Nov 2016), pp.
    1843-1847 CrossRefView in Scopus Liu et al., 2015 Z. Liu, Q. Zhang, M. Zhani,
    R. Boutaba, Y. Liu, Z. Gong Dreams: dynamic resource allocation for mapreduce
    with data skew International Symposium on Integrated Network Management (2015),
    pp. 18-26 Google Scholar Machida et al., 2010 F. Machida, M. Kawato, Y. Maeno
    Redundant virtual machine placement for fault-tolerant consolidated server clusters
    IEEE Network Operations and Management Symposium (NOMS) (2010), pp. 32-39 CrossRefView
    in ScopusGoogle Scholar Martin and James F. Martin and L. James. Microservice.
    https://martinfowler.com/articles/microservices.html. Google Scholar Microsoft
    Azure Microsoft Azure: Cloud Computing Platform & Services. https://azure.microsoft.com/.
    Google Scholar Namiot and Sneps-Sneppe, 2014 D. Namiot, M. Sneps-Sneppe On micro-services
    architecture Int. J. Open Inf. Technol., 2 (9) (2014), pp. 24-27 Google Scholar
    Overview-NetworkX Overview-NetworkX. http://networkx.github.io/. Google Scholar
    Sandholm and Lai, 2010 T. Sandholm, K. Lai Dynamic proportional share scheduling
    in hadoop Job Scheduling Strategies for Parallel Processing, Springer (2010),
    pp. 110-131 CrossRefView in ScopusGoogle Scholar Shi et al., 2016 X. Shi, J. Dong,
    S. Djouadi, Y. Feng, X. Ma, Y. Wang Papmsc: power-aware performance management
    approach for virtualized web servers via stochastic control J. Grid Comput., 14
    (1) (2016), pp. 171-191 CrossRefView in ScopusGoogle Scholar Shojafar et al.,
    2016 M. Shojafar, N. Cordeschi, E. Baccarelli Energy-efficient adaptive resource
    management for real-time vehicular cloud services IEEE Trans. Cloud Comput., 99
    (Apr. 2016) 1–1 Singh and Peddoju, 2017 V. Singh, S.K. Peddoju Container-based
    microservice architecture for cloud applications 2017 International Conference
    on Computing, Communication and Automation (ICCCA) (May 2017), pp. 847-852 View
    in Scopus Soltesz et al., 2007 S. Soltesz, H. Pötzl, M. Fiuczynski, A. Bavier,
    L. Peterson Container-based operating system virtualization: a scalable, high-performance
    alternative to hypervisors ACM SIGOPS Operating System Review, vol. 41 (2007),
    pp. 275-287 View in ScopusGoogle Scholar Sureshkumar and Rajesh, 2017 M. Sureshkumar,
    P. Rajesh Optimizing the docker container usage based on load scheduling 2nd International
    Conference on Computing and Communications Technologies (ICCCT) (2017), pp. 165-168
    CrossRefView in ScopusGoogle Scholar Tang et al., 2007 C. Tang, M. Steinder, M.
    Spreitzer, G. Pacifici A scalable application placement controller for enterprise
    data centers Proc. 16th International Conference on World Wide Web, ACM (2007),
    pp. 331-340 CrossRefView in ScopusGoogle Scholar Tao et al., 2017 Y. Tao, X. Wang,
    X. Xu, Y. Chen Dynamic resource allocation algorithm for container-based service
    computing IEEE 13th International Symposium on Autonomous Decentralized System
    (ISADS) (March 2017), pp. 61-67 View in Scopus Vigliotti and Batista, 2014 A.
    Vigliotti, D. Batista Energy-efficient virtual machines placement Brazilian Symposium
    on Computer Networks and Distributed Systems (SBRC), IEEE (2014), pp. 1-8 CrossRefGoogle
    Scholar Wang et al., 2011 M. Wang, X. Meng, L. Zhang Consolidating virtual machines
    with dynamic bandwidth demand in data centers IEEE INFOCOM (2011), pp. 71-75 CrossRefGoogle
    Scholar Wei et al., 2010 G. Wei, A. Vasilakos, Y. Zheng, N. Xiong A game-theoretic
    method of fair resource allocation for cloud computing services J. Supercomput.,
    54 (2) (2010), pp. 252-269 CrossRefView in ScopusGoogle Scholar Xavier et al.,
    2013 M.G. Xavier, M.V. Neves, F.D. Rossi, T.C. Ferreto, T. Lange, C. De Rose Performance
    evaluation of container-based virtualization for high performance computing environments
    21st Euromicro International Conference on Parallel, Distributed and Network-based
    Processing (PDP), IEEE (2013), pp. 233-240 View in ScopusGoogle Scholar Xiao et
    al., 2013 Z. Xiao, W. Song, Q. Chen Dynamic resource allocation using virtual
    machines for cloud computing environment IEEE Trans. Parallel Distr. Syst., 24
    (6) (2013), pp. 1107-1117 View in ScopusGoogle Scholar Xu and Tang, 2014 X. Xu,
    M. Tang A more efficient and effective heuristic algorithm for the mapreduce placement
    problem in cloud computing IEEE 7th International Conference on Cloud Computing
    (CLOUD) (2014), pp. 264-271 View in ScopusGoogle Scholar Xu et al., 2014 X. Xu,
    H. Yu, X. Pei A novel resource scheduling approach in container based clouds 17th
    International Conference on Computational Science and Engineering (CSE) (2014),
    pp. 257-264 CrossRefView in ScopusGoogle Scholar Zhang et al., 2017 D. Zhang,
    B.H. Yan, Z. Feng, C. Zhang, Y.X. Wang Container oriented job scheduling using
    linear programming model 3rd International Conference on Information Management
    (ICIM) (April 2017), pp. 174-180 CrossRefView in Scopus Zhou et al., 2018 R. Zhou,
    Z. Li, C. Wu Scheduling frameworks for cloud container services IEEE/ACM Trans.
    Netw., 26 (1) (Feb. 2018), pp. 436-450 CrossRefView in Scopus Cited by (0) Xili
    Wan received the BE and ME degrees from Nanjing Tech University in China. He obtained
    PhD degree from University of Missouri-Kansas City. He is currently an assistant
    professor at the school of computer science and technology, Nanjing Tech University,
    China. His research interests include optimization in wireless sensor networks,
    design and analysis of approximation algorithms, artificial intelligence and cloud
    computing. Xinjie Guan received the BS degree in Computer Science from Southeast
    University, in 2006, and the Ph.D degree in Computer Science from University of
    Missouri-Kansas City, in 2015. Currently, she is an assistant professor at the
    School of Computer Science and Technology, Nanjing Tech University. Her research
    interests include network optimization, cloud computing, and mobile edge computing.
    Tianjing Wang received the BS degrees in Mathematics, from Nanjing Normal University
    in 2000, the MS degrees in Mathematics, from Nanjing University in 2005 and the
    Ph.D degree in Telecommunication, from Nanjing University of Posts & Telecommunications
    in 2009. She worked in the post-doctoral research center of Nanjing University
    of Posts & Telecommunications from 2010 to 2013. Currently, she is an associate
    professor in the College of Science, Nanjing Tech University. Her research interests
    include signal processing, cognitive radio, and network optimization. Guangwei
    Bai holds a B.Eng. (1983) and a M.Eng. (1986) from the Xi''an Jiaotong University
    in China, both in Computer Engineering, as well as a Ph.D. (1999) in Computer
    Science from the University of Hamburg in Germany. From 1999 to 2001, he worked
    at the German National Research Center for Information Technology, Germany, as
    a Research Scientist. In 2001, he joined the University of Calgary, Canada, as
    a Research Associate. Since 2005, he has been working at the Nanjing Tech University
    in China, as a Professor in Computer Science. His research interests are in location-based
    services, privacy and security, mobile crowd sensing and computing. He is a member
    of the ACM. Dr. Baek-Young Choi is an Associate Professor at the University of
    Missouri – Kansas City (UMKC), has been in teaching and research in the broad
    areas of computer networks and systems including Cloud Computing, Smart Device
    Technologies, Internet-of-Things, Network Algorithms and Protocols, Data Storage
    and Management Systems, and Measurement, Analysis and Modeling of Network Traffic,
    Performance and Security. Prior to joining the University of Missouri – Kansas
    City, Dr. Choi held positions at Sprint Advanced Technology Labs, and the University
    of Minnesota, Duluth, as a postdoctoral researcher, and as a 3M McKnight distinguished
    visiting assistant professor, respectively. She published three books on network
    monitoring, storage systems, and cloud computing.  She has been a faculty fellow
    of the National Aeronautics and Space Administration (NASA), U.S. Air Force Research
    Laboratory''s Visiting Faculty Research Program (AFRL-VFRP) and Korea Telecom''s
    - Advance Institute of Technology (KT-AIT). She is a senior member of ACM and
    IEEE, and a member of IEEE Women in Engineering. View Abstract © 2018 Elsevier
    Ltd. All rights reserved. Recommended articles A new open source platform for
    lowering the barrier for environmental web app development Environmental Modelling
    & Software, Volume 85, 2016, pp. 11-26 Nathan R. Swain, …, Steven J. Burian View
    PDF Deploying Docker Swarm cluster on hybrid clouds using Occopus Advances in
    Engineering Software, Volume 125, 2018, pp. 136-145 József Kovács, …, Márk Emődi
    View PDF Container-based load balancing for energy efficiency in software-defined
    edge computing environment Sustainable Computing: Informatics and Systems, Volume
    30, 2021, Article 100463 Amritpal Singh, …, Rasmeet Singh Bali View PDF Show 3
    more articles Article Metrics Citations Citation Indexes: 68 Patent Family Citations:
    1 Captures Readers: 247 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply.'
  inline_citation: '>'
  journal: Journal of network and computer applications
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: 'Application deployment using Microservice and Docker containers: Framework
    and optimization'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icdcs.2019.00017
  analysis: '>'
  authors:
  - Anthony Kwan
  - Jonathon Wong
  - Hans‐Arno Jacobsen
  - Vinod Muthusamy
  citation_count: 30
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2019 IEEE 39th International ... HyScale: Hybrid
    and Network Scaling of Dockerized Microservices in Cloud Data Centres Publisher:
    IEEE Cite This PDF Anthony Kwan; Jonathon Wong; Hans-Arno Jacobsen; Vinod Muthusamy
    All Authors 26 Cites in Papers 1684 Full Text Views Abstract Document Sections
    I. Introduction II. Related Work III. Horizontal and Vertical Scaling IV. Autoscaling
    Algorithms V. Autoscaler Architecture Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: When designing modern software, care must
    be taken to allow for applications to scale based on the demands of its users
    while still accommodating flexibility in development. Recently, microservices
    architectures have garnered the attention of many organizations-providing higher
    levels of scalability, availability, and fault isolation. Many organizations choose
    to host their microservices architectures in cloud data centres to offset costs.
    Incidentally, data centres become over-encumbered during peak usage hours and
    underutilized during off-peak hours. Traditional microservice scaling methods
    perform either horizontal or vertical scaling exclusively. When used in combination,
    however, these methods offer complementary benefits and compensate for each other''s
    deficiencies. To leverage the high availability of horizontal scaling and the
    fine-grained resource control of vertical scaling, we developed two novel hybrid
    autoscaling algorithms and a dedicated network scaling algorithm and benchmarked
    them against Google''s popular Kubernetes horizontal autoscaling algorithm. Results
    indicated up to 1.49x speedups in response times for our hybrid algorithms, and
    1.69x speedups for our network algorithm under high-burst network loads. Published
    in: 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)
    Date of Conference: 07-10 July 2019 Date Added to IEEE Xplore: 31 October 2019
    ISBN Information: ISSN Information: DOI: 10.1109/ICDCS.2019.00017 Publisher: IEEE
    Conference Location: Dallas, TX, USA SECTION I. Introduction Microservices architectures
    have gained widespread popularity in the software development community, quickly
    becoming a best practice for creating enterprise applications [1]. Under the microservices
    architecture model, a traditional monolithic application is dissociated into several
    smaller, self-contained component services or functions [2]. Three of the most
    notable benefits include an application''s enhanced deployability, fault-tolerance
    and scalability. Hosting one''s own microservices architecture, however, comes
    at a high price, including server-grade hardware acquisition costs, maintenance
    costs, power consumption costs, and housing costs. Instead of bearing these expenses
    themselves, software companies typically choose to pay cloud data centres to host
    their applications. Companies relinquish control of their microservices'' resource
    allocations and run the risk of performance degradation. Owners of these microservices
    architectures, known as tenants, negotiate a price for a specified level of quality
    of service, usually defined in terms of availability and response times. This
    information is encapsulated in a document referred to as a service-level agreement
    (SLA). The SLA stipulates the monetary penalty for each violation and impels the
    cloud data centre to provision more resources to the tenants. To reduce operating
    costs and improve user-perceived performance, it is paramount to cloud data centres
    to allocate sufficient resources to each tenant. Unfortunately, data centres are
    reaching their physical and financial limitations in terms of space, hardware
    resources, energy usage, and operating costs [3]. As such, it is not always possible
    to simply provision more resources as a buffer against SLA violations. In fact,
    this approach often results in higher costs to the data centres as the number
    of machines and power consumption increase [4]. Conversely, data centres can suffer
    from resource underutilization. During off-peak hours, tenants are typically overprovisioned
    resources [4]. These unused resources can be reclaimed to conserve power and be
    more readily allocated to another tenant when there is an immediate need. Most
    cloud clusters are also heterogeneous in nature, implying that machines can run
    at different speeds and have different memory limits. Increasing the efficiency
    of resource utilization on each machine, while minimizing the number of machines
    used, presents another way to lower the overall power consumption cost. If individual
    machine specifications are not taken into account, however, this can lead to overloaded
    machines. For example, exceeding memory limits forces the machine to swap to disk,
    resulting in significantly slower response times and poorer overall performance.
    Scaling resources efficiently for virtualized microservices should therefore be
    imperative for data centres as it can result in significant cost savings [5],
    [6]. Traditional methods for scaling can generally be categorized into vertical
    or horizontal scaling with the more popular approach being horizontal scaling
    [7]–[10]. This scaling technique involves replicating a microservice onto other
    machines to achieve high availability. By replicating a microservice onto another
    machine, its resource allocations are also copied over. This approach, however,
    is greedy and presumes there is no shortage of hardware resources [11]. Furthermore,
    horizontal scaling creates additional overhead on each replicated machine and
    is confronted with bandwidth limitations on network and disk I/O, and hardware
    limitations on socket connections. Vertical scaling, on the other hand, aims to
    maximize the utilization of resources on a single machine by providing the microservice
    with more resources (e.g., memory and CPU) [12]–[15]. Unfortunately, this method
    is also limited as a single machine does not necessarily possess enough resources.
    Upgrading the machines to meet demands quickly becomes more expensive than purchasing
    additional commodity machines. Fig. 1. Illustrates a hybrid autoscaling scenario.
    Container 1 and 2 (left) reside on the same machine. Container 2 is vertically
    scaled while container 3 is horizontally scaled. Show All Currently, most cloud
    data centres employ the use of popular tools and frameworks, such as Google''s
    Kubernetes, to automatically scale groups of microservices [9]. Many of these
    autoscaling algorithms are devised to achieve high availability within a cluster.
    These frameworks, however, usually consider only one aspect of resource scaling
    (e.g., CPU utilization, memory consumption, or SLA adherence) and use either vertical
    or horizontal scaling techniques, exclusively [13], [15], [16]. Moreover, an administrator
    must manually reconfigure the resource allocations within their own system when
    the framework''s algorithm does not output the optimal configuration. If allocations
    are left sub-optimal, higher costs are incurred leading to loss of profit [4].
    For the most part, frameworks such as Kubernetes have simple autoscaling algorithms
    that frequently lead to non-optimal resource allocations. We propose and investigate
    the possibility of hybrid scaling techniques for allocating resources within a
    data centre. Hybrid scaling techniques reap the benefits of both the fine-grained
    resource control of vertical scaling, and the high availability of horizontal
    scaling. This makes hybrid scaling a promising solution for effective autoscaling.
    Several challenges arise, however, when designing a hybrid scaling algorithm.
    Finding an optimal solution with hybrid scaling can be viewed as a complex multidimensional
    variant of the bin packing problem. When presented with a limited number of physical
    resources and a set of microservices with variable dimensions (e.g., CPU, memory,
    and network), finding the optimal configuration is an NP-complete problem [17].
    These resource allocations and reconfigurations must be determined in real-time,
    thus limiting the time spent searching the solution space. Moreover, when user
    load is unstable, an aggressive algorithm can induce successive conflicting decisions
    (i.e., thrashing), leading to scaling overhead. As the exact correlation between
    resource allocations and performance metrics is unclear, the definition of the
    optimal configuration is ambiguous making closed-form calculations of the optimal
    configuration difficult. Furthermore, resources intrinsically tied to other resources
    greatly obscure this relationship. Network I/O is one metric that is particularly
    complicated to scale in virtualized environments. Due to its dependence on CPU
    and socket resources, and independent ingress and egress filters, network bandwidth
    scaling is very convoluted and does not form the basis for many scaling algorithms.
    To address these issues, this paper makes the following contributions: Performance
    analysis of horizontal versus vertical scaling of Docker containers to quantitatively
    assess their trade-offs (Section III). Design and implementation of two novel
    hybrid scaling techniques, HYSCALECPU and HYSCALECPU+Mem, as well as a simple
    horizontal network scaling algorithm (Section IV). Design and implementation of
    an autoscaling platform prototype to evaluate and compare various scaling techniques
    (Section V). Validation of the performance and resource efficiency benefits gained
    through the use of hybrid and network scaling by benchmarking HYSCALE against
    Google''s Kubernetes horizontal autoscaling algorithm on microbenchmarks and Bitbrain''s
    VM workload (Section VI). SECTION II. Related Work Although container-based virtualization
    technology is relatively new, virtual machines (VMs) have been well researched
    and widely used for several decades [8]. Dynamic scaling of hardware resources
    has been approached from the VM perspective and is not a foreign concept in the
    world of virtualization [8], [12], [18], [19]. A. Vertical Scalers VMs typically
    benefit greatly from vertical scaling, as compared to horizontal scaling, since
    they have long start up times. Thus, several reactive vertical scaling solutions
    exist in the VM domain, such as Azure Automation, CloudVAMP, and AppRM. Azure
    Automation [12] supports vertical scaling of VMs based on observed resource utilization.
    Scaling, however, is relegated to tenants to perform manually. Azure bills tenants
    based on the amount of resources they have been allocated, thus encouraging tenants
    to scale downwards. From the cloud centre perspective, this model does little
    to manage resource efficiency and fragmentation across the cluster. Moreover,
    the cloud centre is very susceptible to overprovisioning of resources, since tenants
    are required to scale their services manually, which is slow for a reactive solution.
    CloudVAMP [13] is a platform that provides mechanisms for memory oversubscription
    of VMs. Memory oversubscription allows a VM to utilize more memory than the host
    has available (i.e., vertical scaling). This is made possible via the hypervisor
    retrieving unused memory from VMs co-located on the machine, and allocating it
    to VMs in need. CloudVAMP, however, does not support scaling through other types
    of resources, such as CPU or disk I/O, forfeiting the ability to achieve a more
    granular level of resource management. Similarly, AppRM vertically scales VMs
    based on whether current performance is meeting user-defined SLAs [15]. The system
    periodically monitors performance of each VM, comparing them to SLAs and vertically
    scaling them accordingly. Currently, this only supports CPU and memory scaling.
    Several other works on VM scaling exist, however, they perform vertical scaling
    and horizontal scaling, exclusively [18], [19]. There are far fewer vertical scaling
    solutions for containers, however, due to their propensity for replication. A
    notable example of exclusive vertical scaling for containers is ElasticDocker
    [16]. ElasticDocker employs the MAPE-K loop to monitor CPU and memory usage and
    autonomously scales Docker containers vertically. It also performs live migration
    of containers, when the host machine does not have sufficient resources. This
    approach was compared with the horizontally scaling Kubernetes, and shown to outperform
    Kubernetes by 37.63%. The main flaw with this solution is the difference in monitoring
    and scaling periods between ElasticDocker and Kubernetes. ElasticDocker polls
    resource usage and scales every 4 seconds, while Kubernetes scales every 30 seconds,
    giving ElasticDocker an unfair advantage to react to fluctuating workloads more
    quickly. Moreover, the cost of machines with sufficient hardware to support a
    container with high demands far exceeds the cost savings achieved. Another example
    of a container vertical scaler is Spyre [14]. This framework splits resources
    on a physical host into units called slices. These slices are allocated a variable
    amount of CPU and memory resources and house multiple containers (similar to Kubernetes
    pods). Vertical scaling is then performed on these slices to allocate or deallocate
    resources to a group of containers. Unfortunately, resources are shared amongst
    containers within a slice making fine-grained adjustments difficult. B. Horizontal
    Scalers Although VM scaling usually does not benefit from horizontal scaling as
    much as vertical scaling due to long start up times, there exist various VM horizontal
    scaling solutions. OpenStack [8] provides users with the ability to automate horizontal
    scaling of VMs via Heat templates. To achieve this, OpenStack provides two mechanisms
    that are user-defined: Ceilometer Alarms and Heat Scaling Policies. Ceilometer
    Alarms trigger based off observed resource usage (e.g., CPU usage exceeding a
    certain percentage), and invoke Heat Scaling Policies to instantiate or decommission
    VMs. Although containers are supported by OpenStack, users tend to use other container
    orchestrators in combination with OpenStack [20]. For container-based horizontal
    autoscaling, the most popular tools are Docker Swarm and Google''s Kubernetes.
    Docker Swarm [7] takes a cluster of Docker-enabled machines and manages them as
    if they were a single Docker host. This allows users to horizontally scale out
    or scale in containers running within the cluster. Scaling commands, however,
    must be input manually and is far too slow to react to sudden load variations.
    Kubernetes [9] offers a horizontal autoscaler that monitors average CPU utilization
    across a set of containers and horizontally scales out or scales in containers
    to match the user-specified target CPU or memory utilization. It also attempts
    to provide a beta API for autoscaling based on multiple observed metrics [21].
    This, however, does not actually perform scaling based on all given metrics. After
    evaluating each metric individually, the autoscaling controller only uses one
    of these metrics. Kubernetes has also added support for a vertical autoscaler,
    however, at the time of publication, it was still a conceptual beta with few details
    on implementation [22]. C. Hybrid Scalers To our knowledge, there are very few
    container-based hybrid scalers in comparison to VM-based hybrid scalers. SmartScale
    [23] uses a combination of vertical and horizontal scaling to ensure that the
    application is scaled in a manner that optimizes both resource usage and reconfiguration
    costs incurred due to scaling. Scaling is performed in two steps. First, the number
    of VM instances is estimated based on observed throughput. once the number of
    instances is determined, an optimal resource configuration is found using a binary
    search. Optimality is defined with respect to maximizing savings and minimizing
    performance impact. This approach assumes that each VM instance operates at maximum
    utilization. In the cost-aware approach of J. Yang et al. [18], [24], [25], an
    extension of R. Han et al.''s VM work is presented by including a horizontal scaling
    aspect to the algorithm. The scaling method is divided into three categories:
    self-healing scaling, resource-level scaling, and VM-level scaling. The first
    two methods are vertical scaling, while the last method is horizontal scaling.
    Self-healing allows complementary VMs to be merged, while resource-level scaling
    consumes unallocated resources on a machine. Finally, VM-level scaling is performed
    using threshold-based scaling. The self-adaptive approach [26] also attempts to
    perform hybrid scaling of VMs by first vertically scaling where possible, then
    allocating new VM instances when required. If a service within a VM instance requires
    more CPUs and there are CPUs available on the node, they are allocated to the
    VM and all services within that VM. If no VM instance with those resources available
    exist, a new VM is started. While this approach is interesting, the implementation
    limits the solution''s scaling granularity as only whole virtual CPUs can be allocated
    or deallocated to a VM at any given time. Moreover, since the resources are allocated
    to the VM itself, the resource distribution within the VM cannot be controlled
    for each service or is not discussed. Four-Fold Auto-Scaling [27] presents a hybrid
    autoscaling technique to reduce costs for containers within VMs. It models the
    scaling problem as a multi-objective optimization problem and minimizes cost using
    IBM''s CPLEX optimizer. To simplify the search space, their model discretizes
    VM and container sizes into distinct, enumerated resource types and abstracts
    physical resource types. This forces a tradeoff between the granularity of their
    scaling decisions and the complexity of their optimization problem. Furthermore,
    there are no guarantees on the optimality of the solutions generated. In their
    implementation, they consider CPU and memory, and have shown a 28% reduction in
    costs. There are, however, negligible improvements in performance and SLA adherence.
    Furthermore, this approach requires manual tuning and fails to expose the performance
    and resource utilization ramifications. Jelastic [28] monitors CPU, memory, network
    and disk usages to automatically trigger either vertical or horizontal scaling
    for a single application. Although Jelastic supports both types of scaling, it
    does not support them simultaneously. For vertical scaling, when user-specified
    thresholds are exceeded, Jelastic provisions set amounts of resources, known as
    Cloudlets, to the application. A cloudlet consists of a 400MHz CPU and 128MiB
    of memory. For horizontal scaling, the user must define the triggers and scaling
    actions for their application. For example, the user must specify the number of
    replicas to scale up by when exceeding their memory threshold. This approach lacks
    flexibility as users cannot use both simultaneously, and must manually tune their
    triggers and scaling actions, especially under varying and unstable loads. Although
    several hybrid scaling solutions do exist, none of them are designed specifically
    for container-based ecosystems. Most of these approaches use VMs which are inherently
    different from containers. As VMs have inherently higher resource overhead and
    scaling costs compared to containers, frequent and responsive scaling actions
    are incompatible with a VM dominated cloud environment. Containers, on the other
    hand, do not suffer from these same constraints and therefore are contingent on
    different concerns than VM machine scaling. Similar techniques from VM scaling
    could be leveraged, but must be modified to conform to the higher throughput input
    events and the lower latency nature of container ecosystems. D. Network Scalers
    NBWGuard [29] presents the first network bandwidth QoS solution for Kubernetes
    pods using tc commands. Egress traffic is drained by hierarchical token bucket
    filters allowing for varying network priorities amongst pods. Ingress traffic
    is redirected to a virtual interface using a kernel intermediate functional block,
    before applying tc. Their approach was then validated using iperf. Most other
    solutions aim to address the issue of network performance and scalability through
    machine placement. X. Meng et al. [30] attempt to alleviate network traffic congestion
    by using a network-focused heuristic placement algorithm. This algorithm optimizes
    placement of VMs by shortening the distance between communicating VMs. Other algorithms
    present various placement algorithms that optimize communication traffic of all
    VMs to designated nodes [31], [32]. Work for network scaling and QoS of containers
    is sparse due to its intricate dependencies on various other resources. Network
    scaling solutions generally have been tailored to VMs. Due to high scaling overheads,
    physical placement becomes critical. Contrarily, containers have negligible scaling
    overhead and are lightweight enough to be replicated very quickly. During bursty
    traffic, these attributes can be exploited to reduce network congestion through
    the use of vertical and horizontal scaling. SECTION III. Horizontal and Vertical
    Scaling To motivate the hybridization of scaling techniques, the effects of horizontal
    and vertical scaling must be understood. Conducive to this, we stressed CPU-bound
    and memory-bound microservices under a fixed client load and measured their response
    times. As a baseline, we first measured the response times of each microservice
    on its own with full access to a 4 core node. Subsequent runs entailed manually
    varying resource allocations to simulate equivalent vertical and horizontal scaling
    scenarios. The following experiments were run with 640 client requests on up to
    16 machines: A. CPU Scaling A container''s Docker CPU shares define its proportion
    of CPU cycles on a single machine [33]. By tuning the CPU shares allocated to
    each microservice, we effectively control their relative weights. For example,
    if two microservice containers run on a single machine with CPU shares of 1024
    and 2048, the containers will have 1/3 and 2/3 of the access time to the CPU,
    respectively. We utilized this sharing feature to induce a form of vertical scaling,
    as increasing or decreasing shares directly correlate with an increase or decrease
    in CPU resource allocation to a container. The baseline microservice is run on
    a single machine with no contention and is configured to consume CPU time per
    request. The latency of a request is measured as the microservice execution time.
    This simulates CPU load on the system from the request/response mechanic that
    is inherent in a microservices architecture. To create contention of CPU resources,
    the microservice is run alongside another container. This container runs progrium
    stress [34], which consumes CPU resources. In both the horizontal and vertical
    scaling scenarios, the microservice is given an equivalent amount of resources
    overall to isolate the effects of both. For example, in the vertical scaling emulation,
    we allocated 1024 CPU shares to both the microservice and the progrium stress
    container, splitting CPU access time equally between the two. The equivalent horizontally-scaled
    instance with 3 microservices running over 3 machines allocates 1024 and 5120
    CPU shares to the microservice and the progrium stress container, respectively.
    This results in 1/6 of the total CPU access time for each microservice, adding
    up to a total of 1/2 of the CPU access time. This is equivalent to the vertical
    scaled scenario. Fig. 2. Response times of horizontal scaling for the CPU tests.
    Show All Results of our studies on vertical versus horizontal scaling for CPU
    resources show a preference for vertical scaling. It provided negligible overhead
    in request processing times when compared to the equivalently horizontally scaled
    instances. Results also strongly indicate that more replicated instances decrease
    overall CPU performance (see Figure 2). Although Docker containers, themselves,
    have negligible overhead [35], when contention over shared CPU resources is introduced,
    significant overhead becomes apparent. In our experiments, this manifested itself
    as a 17% increase in response times. This would be further exacerbated by the
    presence of more co-located containers. Moreover, the applications within the
    Docker containers also incur measurable costs. When replicated several times,
    this performance overhead becomes much more significant, and can affect response
    times. For our experiments, this overhead resides mainly within the Java Virtual
    Machine. Significant overhead is also seen when replicas are distributed across
    several nodes resulting in a logarithmic increase with the number of replicas.
    Note that in practice, replicating a container in a cloud environment will inevitably
    force containers to co-locate with other containers further decreasing overall
    performance of every microservice. B. Memory Scaling Docker also allows users
    to impose memory limits on containers [33]. Once a container uses memory in excess
    of its limit, portions of its memory are swapped to disk. The effects of vertical
    and horizontal scaling on memory were tested analogously to the CPU tests, ensuring
    equivalent resources in each scenario (i.e., one 512 MB container is equivalent
    to two 256 MB containers). One difference, however, is that there is no contention
    for memory between Docker containers, and thus there was no need to run a progrium
    stress container. Results show negligible differences in request times between
    vertical and horizontal scaling scenarios. Also, increasing memory limits did
    not speed up processing times. Performance drastically degraded, however, when
    the number of incoming requests forced the microservice to swap. Moreover, horizontal
    scaling introduced slightly more memory overhead. This was due to the memory used
    by the application and the container image, itself. This overhead will vary from
    application to application. If high enough, horizontally scaled instances are
    much more likely to swap compared to a single vertically scaled instance, given
    the same amount of memory. Fig. 3. Response times of horizontal scaling for the
    network tests with a total bandwidth of 100mbps. Show All C. Network Scaling Although
    Docker supports container resizing for many resource types, network I/O is an
    exception. Limiting network bandwidth for containers in general must be done through
    third party tools, such as the traffic control (tc) command in conjunction with
    iptables routing. In this section, we explore the scaling of egress UDP and TCP
    traffic by using tc and iptables to scale network bandwidth in each scenario.
    Analogously to our CPU tests, a total bandwidth of 100Mbps was allocated to our
    microservice running iperf [36] and run alongside a custom stress container that
    attempts to hog all available CPU and network resources. Results were averaged
    over 30 runs for each scenario and showed negligible changes for vertical scaling
    of network due to the effective and fair distribution of traffic using tc and
    iptables. For horizontal scaling, however, a large decrease in execution time
    is achieved at a larger number of replicas, tapering off at around 8 replicas
    (see Figure 3). This is mainly attributed to the alleviated contention over the
    network tx queues when using more machines. In general, horizontally scaling outwards
    provides significant benefits for network scaling for our cluster. Although results
    for only 100Mbps are shown, there are several other factors that affect actual
    performance, such as network speeds, number of requests, and size of requests.
    From varying these parameters, we found the results followed the same general
    trends. SECTION IV. Autoscaling Algorithms To help understand our novel hybrid
    scaling algorithms, we first discuss the implementation details of the popular
    Kubernetes horizontal scaling algorithm. All variables in the following equations
    are measured as a percentage. A. Horizontal Scaling Algorithms 1) Kubernetes Algorithm
    The Kubernetes autoscaling algorithm utilizes horizontal scaling to adjust the
    number of available replica instances of services to meet the current incoming
    demand or load. The algorithm increases and decreases the number of replicated
    microservice instances based on current CPU utilization. If the average CPU utilization
    for a microservice and all its replicas exceed a certain target percentage threshold,
    then the system will scale up the number of replicas. Conversely, when the average
    CPU utilization falls below the threshold, the system will scale down the number
    of replicas. CPU utilization is calculated as the CPU usage over the requested
    CPU. In order to measure average CPU utilization, the Kubernetes algorithm periodically
    queries each of the nodes within the cluster for resource usage information. By
    default, the query period is set to 30s, however, for our experiments, we query
    every 5s. The Kubernetes autoscaling algorithm takes in 3 user- specified inputs:
    overall target CPU utilization, and minimum and maximum numbers of replicas. The
    system scales up and down towards the minimum and maximum whenever the overall
    CPU utilization is above or below the target, respectively. The algorithm calculates
    the target number of replicas to scale up or down for microservice m and replica
    r using the formula: utilizatio n r = usag e r requeste d r NumReplica s m =⌈
    sum(utilizatio n r ) Targe t m ⌉ View Source This, however, introduces a problem
    where thrashing can occur. To prevent thrashing between quickly scaling up and
    scaling down horizontally, the Kubernetes algorithm uses minimum scale up and
    scale down time intervals. Rescaling intervals are enforced when a scaling up
    or scaling down operation occurs, and notifies the system to halt any further
    scaling operations until the specified time interval has passed. Our experiments
    used 3s and 50s minimum scale up and scale down intervals, respectively. There
    is another Kubernetes feature that mitigates thrashing. It only performs rescaling
    if the following holds true: | average(usag e r ) Tarqe t m −1|>0.1 View Source
    Recently, Kubernetes has added support to use memory utilization or a custom metric
    instead of CPU utilization. Kubernetes has also attempted to provide support for
    multiple metrics, which is currently in beta. This support however is limited,
    as only the metric with the largest scale is chosen. 2) Network Scaling Algorithm
    There are several aspects of networking that contribute to the complexity of its
    scaling, such as number of sockets, protocols, shared network interface cards
    and switches, kernel scheduling, and filtering. As a result, there is no known
    generic implementation of network bandwidth scaling nor is it natively supported
    in Kubernetes. Therefore, we chose to design an exploratory horizontal algorithm
    based on the results of our network scaling experiments. This algorithm uses the
    same algorithm as Kubernetes, but replaces CPU usage for outgoing network bandwidth
    usage in its calculations. B. Hybrid Scaling Algorithms The main goal of our hybrid
    autoscaling algorithms is to dynamically reallocate and redistribute resources
    amongst microservices in a fashion that preserves high availability, low response
    times and high resource utilization per node. Some microservices tend to use a
    mix of different resources, and cannot be scaled effectively when using Kubernetes,
    leading to longer response times and more SLA violations. Horizontal scaling is
    not always the best solution as the addition of a new replica instance may be
    grossly more than required. Additionally, horizontally scaling microservices that
    need to preserve state is non-trivial as it introduces the need for a consistency
    model to maintain state amongst all replicas. Hence, in these scenarios, the best
    scaling decisions are those that bring forth more resources to a particular container
    (i.e., vertical scaling). Our hybrid autoscaling algorithm takes a similar approach
    to the Kubernetes autoscaling algorithm. As opposed to calculating only a coarse-grained
    target number of replicas for a microservice, the hybrid approach is to deterministically
    calculate the exact microservice''s needs. While still retaining the desired coarse-grained
    replication factor, this calculation also contains the required fine-grained adjustments.
    Two main distinctions separate our hybrid algorithms from Kubernetes: the use
    of vertical scaling, and the broadening of the measurement criteria to include
    both CPU and memory. These algorithms first ensure the minimum and maximum number
    of replicas are running for fault-tolerance benefits. They then attempt to vertically
    scale onto the same machines, granted enough available resources. If there are
    insufficient resources to meet demands, horizontal scaling is performed on another
    machine; one not hosting the same microservice, and advertising sufficient available
    resources. In the following sections, we present two such hybrid algorithms. 1)
    HyscaleCPU Algorithm This hybrid algorithm considers only CPU usage and calculates
    the number of missing CPUs for microservice m using the equation: MissingCPU s
    m = sum(usag e r )−(sum(requeste d r )∗Targe t m ) Targe t m View Source If the
    overall CPU usage is equal to that of the target utilization, then the equation
    will output 0 signifying that no changes are required. If the result is negative,
    then the resource allocation is greater than the usage and signals to the algorithm
    that there are unused CPU resources for this microservice. Similarly, a positive
    result signifies that there are insufficient resources allocated to the microservice
    overall. Once the number of missing resources has been calculated for every microservice,
    the algorithm enters the resource reclamation phase. For every microservice that
    indicated a negative value, downward vertical scaling (i.e., resource reclamation)
    is attempted on each of their replicas to move the instance towards the target
    utilization. If an instance has been vertically scaled downwards and its allocated
    resources drop below a minimum threshold (currently set to 0.1 CPUs), it is removed
    entirely. Moreover, any reclaimed resources contribute to increasing the number
    of missing resources back to 0. The amount of CPU resources reclaimable from each
    instance is calculated as follows: ReclaimableCPU s r =requeste d r − sum(usag
    e r ) Targe t m ∗0.9 View Source Once reclamation is complete, the second phase
    of the algorithm attempts to acquire unused or reclaimed resources for microservices
    that indicated a positive number of missing resources. In a similar manner, each
    microservice instance is vertically scaled upwards by the following amount: RequiredCPU
    s r = sum(usag e r ) Targe t m ∗0.9 −requeste d r AcquiredCPU s r =min(RequiredCPU
    s r , AvailableCPU s n ) View Source Each replica instance will claim as many
    resources as it needs, up to the amount available on the node. If vertical scaling
    on all replicas does not provide sufficient resources to provide for the microservice
    as a whole, horizontal scaling is performed onto other nodes that have free resources.
    Furthermore, a new replica can only be instantiated if the node advertises at
    least the baseline memory requirement of the microservice, as well as a minimum
    CPU threshold (currently set to 0.25 CPUs). This is to ensure that an instance
    is not spawned with resource allocations that are too small. Finally, similar
    to Kubernetes, the hybrid algorithm enforces rescaling intervals, whereby frequent
    horizontal rescaling is throttled to avoid thrashing. Vertical scaling, however,
    is exempt from this rule, as vertical scaling must perform fine- grained adjustments
    quickly and frequently. 2) HyScaleCPU+Mem Algorithm This hybrid algorithm extends
    from the previous algorithm by considering memory and swap usage. The algorithm
    and equations used are analogous to those used for CPU measurements and are shown
    below. MissingMe m m = sum(usag e r )−(sum(requeste a r )∗Targe t m ) Targe t
    m ReclaimableMe m r =requeste d r − sum(usag e τ ) Targe t m ∗0.9   RequiredMe
    m r = sum(usag e r ) Targe t m ∗0.9 −requeste d r AcquiredMe m r =min(RequiredMe
    m r , AvailableMe m n ) View Source With the consideration of a second variable,
    horizontal scaling becomes much less trivial. The algorithm can no longer indiscriminately
    remove a container that is consuming memory or CPU, if it falls below a certain
    CPU or memory threshold, respectively. Furthermore, new containers cannot be added
    with no allocated memory or CPU. This changes the conditions for container removal
    and addition by requiring the CPU and memory threshold conditions to be met mutually.
    Fig. 4. Full stack overview showing the resource management layer (second bottom)
    above the cloud resources (bottom). Docker microservices (second top) give rise
    to enterprise applications (top) that clients interact with. Show All SECTION
    V. Autoscaler Architecture To benchmark various scaling techniques on a common
    platform, we present an autoscaler architecture that supports vertical, horizontal
    and hybrid scaling. The autoscaler performs resource scaling on microservice containers,
    where a central arbiter autonomously manages all the resources within a cluster.
    Much like Apache YARN [37], this central entity is named the Monitor and is tasked
    with gathering resource usage statistics from each microservice running in the
    cluster. The Monitor interacts with each machine through the Node Managers (NMs).
    Each NM manages and reports the status of its machine and checks for microservice
    liveness. Additionally, distributed server-side Load Balancers (LBs) act as proxies
    for clients interacting with microservices. The different components in our autoscaling
    platform architecture are illustrated in Figures 4 and 5. Further details on each
    component are covered in following sections. A. Docker Containers/Microservices
    Docker containers provide a lightweight virtualization alternative, allowing developers
    to package their application and its corresponding dependencies into a single
    isolated container. Instead of including the entire operating system into the
    virtual image, Docker utilizes operating system virtualization to emulate the
    operating system kernel. This makes Docker images significantly lighter in size,
    and quick to deploy onto machines. This differs significantly from traditional
    hypervisor virtualization, whereby hardware resources (e.g., CPU, memory, and
    hard disk) are emulated by the hypervisor, and guest operating systems are installed
    directly above. Furthermore, Docker containers leverage a copy-on-write filesystem
    to make the container images lightweight and compact, increasing their efficiency
    in deployment. In industry, applications are deployed as groups of microservices
    (i.e., microservices architectures or Kubernetes pods) and each microservice component
    is housed within its own container. In our architecture, each Docker container
    houses a single microservice and is the unit of deployment for all microservices,
    including differing instances and replications. We consider each microservice
    to be an individual entity and not part of a group of microservices, to isolate
    the effects of the scaling techniques. B. Node Managers Each node runs a single
    NM (see Figure 5), in charge of monitoring the combined microservice resource
    usage of all microservices stationed on that node. NMs are also in charge of aggregating
    container failure information and request statistics, such as completion times,
    from all microservices. The NMs are written in Java and interface with the Docker
    daemon through docker-java, an open source Java library that provides a Java interface
    to Docker''s API. NMs also gather relevant resource usage information (i.e., CPU
    and memory usage) through the Docker API via ‘docker stats''. Additionally, the
    NM receives vertical scaling resource commands from the Monitor for specific containers.
    NMs perform these adjustments by invoking ‘docker update''. They have no control
    over vertical scaling decision-making for the node upon which they reside, as
    they only have sufficient information to make locally optimal configurations.
    This can result in suboptimal global configurations. For example, the NM being
    unaware of any horizontal scaling decisions made by the Monitor allows the NM
    to vertically scale the microservice at its own discretion. This creates situations
    where the NM and the Monitor simultaneously increase and decrease allocated resources
    to a microservice, which then result in large oscillations around the target resource
    allocation. Moreover, the NM can also act to negate or lessen the intended effects
    of the Monitor. Therefore, the decision-making logic for resource allocation resides
    solely with the Monitor and not the NMs. C. Monitor The Monitor is the central
    arbiter of the system. The Monitor''s centralized view puts it in the most suitable
    position for determining and administering resource scaling decisions across all
    microservices running within the cluster. The MONITOR‘ s goal is to reclaim unused
    resources from microservices, and provision them to microservices that require
    more. Scaling can be performed at the node level by the reallocation of resources
    to co-located containers (i.e., vertical scaling), and at the cluster level by
    the creation and removal of microservice replicas (i.e., horizontal scaling).
    The use of different scaling algorithms is also supported through communication
    to the Autoscaler module, and can be specified at initialization or through the
    command-line interface. Fig. 5. Architecture overview illustrating the various
    interactions between each component. Show All SECTION VI. Experimental Analysis
    Conducive to validating the benefits of hybrid scaling techniques, we evaluate
    and compare the Kubernetes CPU horizontal scaling algorithm with our HYSCALE and
    network scaling algorithms on our autoscaler platform. To evaluate the effectiveness
    of each algorithm, we look at metrics primarily regarding user-perceived performance.
    We chose to evaluate each algorithm under various types of loads in the form of
    microbenchmarks that would encapsulate typical scenarios most data centres would
    experience. For client load, we emulate peak and off-peak “hours” to analyze how
    the algorithms react under stable and unstable loads. For our experiments, the
    stable load consists of a low amplitude bursty traffic, labelled low-burst, and
    the unstable load forms a spiking pattern, labelled high-burst. This wave-like
    bursty pattern simulates repeated peaks and troughs in client activity. We also
    present the system with 4 different types of microservices: CPU-bound, memory-bound,
    network-bound, mixed CPU and memory. Microservices applications'' workloads are
    emulated using a custom Java microservice with configurable workload. Upon instantiation,
    our emulated microservices are specified an amount of resources to consume per
    incoming client request. Based on these inputs, we can create microservices that
    vary in resource consumption. Additional computing resource types, such as disk
    I/O, are also supported, however, they are not currently implemented and will
    be part of future works. Each experiment was performed using 15 different microservices
    for an hour on a cluster of 24 nodes with the Monitor on a separate machine. Each
    cluster node runs Ubuntu 14.04 LTS and the exact same computing hardware, with
    2 dual core Intel Xeon 5120 processors (4 cores in total), 8GB of DDR2 memory
    and 3Gbit/s SAS-1 hard drives. Five cluster nodes were designated as LBs and all
    other nodes hosted the NMs and Docker containers. All results were averaged over
    5 runs. Since the Kubernetes and HYSCALECPU algorithms are unable to handle memory-bound
    loads and crash, these results have been omitted from the following sections.
    In the following figures and tables, ‘hybrid’ refers to HYSCALE CPU and ‘hybridmem’
    refers to HYSCALE CPU+Mem . Fig. 6. Graphs depicting the percentage of requests
    failed and the average request response times for the CPU-bound experiments. Removal
    failures are requests that end prematurely due to container removals. Connection
    failures are requests that fail prematurely at the microservice. Show All A. User-Perceived
    Performance Metrics Average microservice response times and number of failed requests
    were analyzed to determine the effectiveness of the scaling algorithms. Faster
    user-perceived response times reflect well on the resource allocations, whereas
    slower times reflect the opposite. Figures 6 and 7 show the average response times
    and percentage of failed requests of each algorithm. In the CPU-bound experiments
    (Figures 6a and 6b), HYSCALE CPU+Mem has the fastest response times overall, while
    Kubernetes has the slowest response times. There are clear improvements in response
    times of HYSCALE as compared to Kubernetes resulting in 1.49x and 1.43x speedups
    for the low and high-burst workloads, respectively. Although availability is generally
    very high (at least 99.8% up-time), it is evident that HYSCALE drastically lowers
    the number of failed requests (up to 10 times fewer compared to Kubernetes). This
    shows our HYSCALE algorithms'' high availability and robust performance under
    stable and unstable CPU loads. In the mixed experiments (Figures 7a and 7b), Kubernetes
    and HYSCALE CPU showed significant percentages of failed requests, mainly due
    to the lack of consideration for memory usage. These numbers are positively offset
    by the partial CPU usage when forced to swap to disk. An interesting observation
    is made in Figure 7a, where Kubernetes appears to perform better than HYSCALE
    CPU . This is caused by HYSCALE CPU ''s preference to vertical scaling over horizontal
    scaling. As an unintentional side effect of Kubernetes'' aggressive horizontal
    scaling, more memory is allocated with each container scale out allowing it to
    perform better with memory requests. In Figure 7b, the response times are significantly
    skewed for Kubernetes and HYSCALE CPU , since the microservices are effectively
    handling a smaller portion of requests (up to 23.67% less requests). Fig. 7. Request
    statistics graphs for mixed resource experiments. Note the significant difference
    in failed requests between 7a and 7b. Show All In the network-bound experiments
    (Figures 8a and 8b), our network scaling algorithm outperformed the others overall
    with Kubernetes being the slowest. Despite the other algorithms'' lack of consideration
    for actual network usage, they still manage to stay competitive under low-burst
    stable workloads, due to the moderate use of CPU caused by networking system calls.
    This, however, does not hold for unstable workloads, where dedicated network scaling
    shows a clear advantage with response times dropping by up to 59.22%. The results
    for Kubernetes and HYSCALE stay consistent with previous experiments (Figures
    7a and 7b), further corroborating our HYSCALE algorithms'' ability to achieve
    better performances under CPU loads. B. Bitbrains Workload To emulate the stress
    that a microservices architecture would undergo in a realistic cloud data centre,
    we benchmarked the Kubernetes and HYSCALE algorithms using the Rnd dataset from
    the GWA-T-12 Bitbrains workload trace [38]. Bitbrains is a service provider that
    specializes in managed hosting and business computation for enterprises. Customers
    include many major banks (ING), credit card operators (ICS) and insurers (Aegon).
    The Rnd dataset consists of the resource usages of 500 VMs used in the application
    services hosted within the Bitbrains data centre. We re-purposed this dataset
    to be applicable to our microservices use case and scaled it to run on our cluster.
    This trace (see Figure 9) exhibits the same behaviour as the low-burst mix and
    high-burst mix workloads, and thus is expected to manifest the same result trends.
    Fig. 8. Graphs depicting the percentage of requests failed and the average request
    response times for the network-bound experiments. Show All Fig. 9. Graph of the
    bitbrains rnd workload trace for CPU and memory usage averaged over all microservices.
    Show All The performance results (see Figure 10) were similar to the mixed experiment
    results (see Figure 7). HYSCALECPU+Mem performs the best because of its ability
    to scale both CPU and memory. Kubernetes, however, outperformed the HYSCALECPU
    because of its preference to horizontally scale, whereas HYSCALECPU prefers to
    vertically scale. Kubernetes'' horizontal scaling actions inadvertently allocated
    more memory to each replica, which reduced the number of timed out requests, as
    well as, the amount of memory swapped to disk. Fig. 10. Request statistics graph
    for the bitbrains experiment. Show All SECTION VII. Conclusions The microservices
    architecture model lends itself well to the continuous delivery of robust and
    maintainable applications. To help cloud data centres keep up with increasing
    demand, innovative scaling techniques must be developed to support effective utilization
    of available resources and ensure service to clients. In this paper, we propose
    a dedicated network scaling algorithm and HYSCALE, two hybrid autoscaling algorithms
    that combine horizontal and vertical scaling techniques to achieve higher resource
    efficiencies. Using our autoscaler platform, we demonstrate the availability and
    performance benefits of each when benchmarked against Google''s Kubernetes horizontal
    autoscaling algorithm. The higher SLA adherence and faster response times attained
    will allow cloud data centres to save substantially on power consumption costs
    and SLA violation penalties. Furthermore, our algorithms will help data centres
    adhere to their maximum capacities as larger numbers of microservices can be packed
    more efficiently onto available hardware. As our results showed, HYSCALECPU+Mem
    predominantly outperformed HYSCALECPU and Kubernetes by considering multiple metrics
    simultaneously. Due to the complexity and obscurity of the network resource metrics,
    network scaling was considered separately and a preliminary horizontal scaling
    algorithm was presented. This was able to achieve significant performance benefits
    and shows huge potential. In future works, we aim to further explore network resource
    scaling and extend our hybrid autoscaling algorithms to incorporate a cost-based
    aspect, a machine learning aspect and various others. We also aim to support features
    such as the dynamic addition and removal of machines, and stateful microservices.
    We also intend to benchmark our solution against new and upcoming algorithms such
    as Kubernetes when it fully supports vertical scaling. ACKNOWLEDGEMENTS This research
    has in part been funded by NSERC and an IBM Canada CAS fellowship. We would also
    like to express our sincere gratitude to several individuals from IBM Research
    and IBM Canada who have supported this work with input and advice including Cong
    Xu, Karthick Rajamani, Allen Chan, and Suzette Samoojh. Finally, we would like
    to thank Geoffrey Elliott for managing the cluster for our experiments. Authors
    Figures References Citations Keywords Metrics More Like This MicroCloud: A Container-Based
    Solution for Efficient Resource Management in the Cloud 2016 IEEE International
    Conference on Smart Cloud (SmartCloud) Published: 2016 An Energy-Aware Host Resource
    Management Framework for Two-Tier Virtualized Cloud Data Centers IEEE Access Published:
    2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'HyScale: Hybrid and Network Scaling of Dockerized Microservices in Cloud
    Data Centres'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/cloudcom.2016.0051
  analysis: '>'
  authors:
  - Hamzeh Khazaei
  - Cornel Barna
  - Nasim Beigi-Mohammadi
  - Marin Litoiu
  citation_count: 47
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2016 IEEE International Confe... Efficiency Analysis
    of Provisioning Microservices Publisher: IEEE Cite This PDF Hamzeh Khazaei; Cornel
    Barna; Nasim Beigi-Mohammadi; Marin Litoiu All Authors 41 Cites in Papers 2128
    Full Text Views Abstract Document Sections I. Introduction II. Microservice Platforms
    III. System Description IV. Analytical Model V. Experimental Setup and Results
    Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: Microservice architecture has started a new trend for application development/deployment
    in cloud due to its flexibility, scalability, manageability and performance. Various
    microservice platforms have emerged to facilitate the whole software engineering
    cycle for cloud applications from design, development, test, deployment to maintenance.
    In this paper, we propose a performance analytical model and validate it by experiments
    to study the provisioning performance of microservice platforms. We design and
    develop a microservice platform on Amazon EC2 cloud using Docker technology family
    to identify important elements contributing to the performance of microservice
    platforms. We leverage the results and insights from experiments to build a tractable
    analytical performance model that can be used to perform what-if analysis and
    capacity planning in a systematic manner for large scale microservices with minimum
    amount of time and cost. Published in: 2016 IEEE International Conference on Cloud
    Computing Technology and Science (CloudCom) Date of Conference: 12-15 December
    2016 Date Added to IEEE Xplore: 26 January 2017 ISBN Information: Electronic ISSN:
    2330-2186 DOI: 10.1109/CloudCom.2016.0051 Publisher: IEEE Conference Location:
    Luxembourg, Luxembourg SECTION I. Introduction Infrastructure-as-a-Service (IaaS)
    cloud providers, such as Amazon EC2 and IBM Cloud deliver on-demand operating
    system (OS) instances in the form of virtual machines (VM). A virtual machine
    manager (VMM), or hypervisor, is usually used to manage all virtual machines on
    a physical machine. This virtualization technology is quite mature now and can
    provide good performance and security isolation among VM instances. An individual
    VM has no awareness of other VMs running on the same physical machine (PM). However,
    for applications that require higher flexibility at runtime and less isolation,
    hypervisor based virtualization might not satisfy the entire set of quality of
    service (QoS) requirements. A container runs directly on a Linux kernel with similar
    performance isolation and allocation characteristics as VMs but without the expensive
    VM runtime management overhead [1], [2]. Containerization of applications, that
    is deployment of application or its components in containers, has become popular
    in cloud service industry. For example, Google is providing many of its popular
    products through a container-based cloud. A Docker container [3] comes with all
    dependent software packages for an application, providing a fast and simple way
    to develop and deploy different versions of the applications [3]. Container based
    services are popularly known as Microservices and are being leveraged by many
    service providers for a number of reasons: (1) to reduce complexity when using
    tiny services; (2) to scale, remove and deploy parts of the system or application
    easily; (3) to improve flexibility by using different frameworks and tools; (4)
    to increase the overall scalability; and (5) to improve the resilience of the
    system. Containers have empowered the usage of microservices architectures by
    being lightweight, providing fast start-up times, and having a low overhead [4].
    A flexible computing model combines IaaS based clouds with container based PaaS
    (Platform-as-a-Service) cloud. Platforms such as Nirmata [5], Docker Cloud [6],
    previously known as Tutum, and Giant Swarm [7] offer platforms for managing virtual
    environments made of containers while relying on IaaS public/private clouds as
    the backend resource providers. Service availability and service response time
    are two important quality measures from cloud''s users perspective [8]. Quantifying
    and characterizing such performance measures requires accurate modeling and coverage
    of large parameter space while using tractable and solvable models in timely manner
    to assist with runtime decisions. This paper consider container based PaaS operating
    on top of VM based IaaS and introduces a performance model for microservice provisioning.
    The model supports microservice management use cases and incorporate the following
    contributions: Supports virtualization at both VM and container layers Captures
    different delays imposed by the microservice platform on users’ requests; Characterizes
    the service availability and elasticity; Provides capacity planning and what-if
    analysis for microservice platform providers; Provides insights in performance
    vs cost trade offs. The rest of the paper is organized as follows. Section II
    describes the new trend of emerging microservice platforms. Section III describes
    the details of the platform that we are going to model in this work. Section IV
    elaborate the performance analytical model. Section V and VI present the details
    of our experiments and the numerical results obtained from the analytical model.
    In section VII, we survey related work in cloud performance analysis, and finally,
    section VIII summarizes our findings and concludes the paper. SECTION II. Microservice
    Platforms Recently, a pattern has been adopted by many software-as-a-service providers
    in which both VMs and containers are leveraged to provide so called microservices.
    Microservices is an approach that allows more complex applications to be configured
    from basic building blocks, where each building block is deployed in a container
    and the constituent containers are linked together to form the cohesive application.
    The application''s functionality can then be scaled by deploying more containers
    of the appropriate building blocks rather than entire new iterations of the full
    application. Microservice platforms (MSP) such as Nirmata [5], Docker Cloud [6]
    and Giant Swarm [7] facilitate the management of such service paradigm. MSPs are
    automating deployment, scaling, and operations of application containers across
    clusters of physical machines in cloud. MSPs enable software-as-service providers
    to quickly and efficiently respond to customer demand by scaling the applications
    on demand, seamlessly rolling out new features and optimizing hardware usage by
    using only the resources that are needed. Fig. 1 shows the layered architecture
    in which both virtualization techniques are leveraged to deliver microservices
    on the cloud. Fig. 1: Leveraging both virtualization techniques, ie, VMs and containers,
    to offer microservices on IaaS cloud. Show All Fig. 2 depicts the high-level architecture
    of MSPs and the way they leverage the backend public or private cloud (ie, infrastructure-as-a-service
    clouds). Various microservice platform providers such as Nirmata, Docker Tutum
    and Giant Swarm implemented their platform based on this conceptual model. SECTION
    III. System Description In this section we describe the system under modeling
    with respect to Fig. 3 that shows the servicing steps of a request in microservice
    platforms. In microservice platforms (MSP), a request may originate form two sources:
    first, direct requests from users (ie, microservice users in Fig. 2) that want
    to deploy a new instance of an application or service; second type would be runtime
    requests from applications (eg, consider adaptive applications) by which applications
    adapt to the runtime conditions; for example, scaling up the number of containers
    to cope with a traffic peak. We model these two types of requests altogether as
    a Poisson process. Fig. 2: Conceptual model: Including both microservice platform
    and the backend public/private cloud. Show All The steps incurred in servicing
    a provisioning request in MSP are shown in the Fig. 3. User requests for containers
    are submitted to a global finite queue and then processed on a first-come, first-serve
    basis (FCFS). A request that finds the queue full, will be rejected immediately.
    Once the request is admitted to the queue, it must wait until the VM Assigning
    Module (VMAM) processes it. VMAM finds the most appropriate VM in the user''s
    cluster (based on the policy) and then send the request to that VM''s so that
    the Container Provisioning Module (CPM) initiates and deploys the container. when
    a request is processed in CPM, a pre-build or customized container image is used
    to create a container instance. These images can be loaded from a public repository
    like Docker Hub [3] or private repositories. If there are not enough resources
    (ie, VM) in the MSP to accommodate the new request, scheduler asks for a VM from
    the backend IaaS (see Fig. 3). The request will be rejected if the application
    (or the user) has already reached its capacity. Otherwise, a VM will be provisioned
    and the last request for container will be deployed on this new VM. In the IaaS
    cloud when a request is processed, a pre-built or customized disk image is used
    to create a VM instance [8]. In this work we assume that pre-built images fulfill
    all user requests. To model the behavior of this system, we design a provisioning
    performance model that includes all the servicing steps in fulfilling requests
    for containers. Then, we solve this model to compute the provisioning performance
    metrics: request rejection probability, mean response delay and cluster utilization
    as functions of variations in workload (request arrival rate), container lifetime
    and cluster size. We describe our analysis in detail in Section IV, using the
    symbols and acronyms listed in Table I. SECTION IV. Analytical Model The performance
    model of the microservice platform is shown in Fig. 4. The MSP performance model
    has been realized by a 3-dimensional Continues Time Markov Chain (CTMC) with states
    labeled as (i, j, k); state i indicates the number of requests in Microservice
    Global Queue, j denotes the number of running containers in the platform and finally
    k shows the number of active VMs in the user''s cluster. Each VM can accommodate
    up to M containers that is set by the user. The request arrival can be adequately
    modelled as a Poisson process [9] with the arrival rate of λ . Also let ϕ be the
    rate at which containers can be deployed on a VM and μ be the service rate of
    each container (note that 1/μ would be the container lifetime). Therefor, the
    total service rate for each VM is the product of number of running containers
    by μ . Assume α and β are the rates at which the MSP can obtain and release a
    VM from IaaS cloud respectively. The scheduler asks for a new VM from backend
    IaaS cloud when explicitly ordered by the MSP user (or application) or when the
    utilization of the host group is equal or greater than a predefined value. For
    state ( i , j, k), utilization u is defined as follows, u= i+j k×M (1) View Source
    in which M is the maximum number of containers that can be run on a single VM.
    On the other hand, if utilization drops lower than a predefined value, the MSP
    will release one VM to optimize the cost. A VM can be released if there is no
    running containers on it so that the VM should be fully decommissioned in advance.
    Also the MSP holds a minimum number of VMs (ie., s) in the cluster regardless
    of utilization, in order to maintain the availability of the microservices. The
    user may also set another value for its application(s) (ie, S) indicates the MSP
    can not request more than S VMs from IaaS cloud on behalf of the user. Thus the
    application scale up at most to S VMs in case of high traffic and scale down to
    s VMs in times of low utilization. We set the global queue size (ie, L q ) to
    the total number of containers that it can accommodate at its full capacity (ie,
    L q =S×M ). Note that requests will be blocked if the user reached its capacity,
    regardless of Global Queue state. Fig. 3: Servicing steps of a provisioning request
    in microservice platforms; derived form conceptual model in fig. 2. Show All Table
    I: Symbols and corresponding descriptions State (0, 0, s) indicates that there
    is no request in queue, no running container and the cluster consists of s VMs
    that is the minimum number of VMs that user maintain in its host group. Consider
    an arbitrary state such as (i, j, k), in which five transitions might happen:
    Upon arrival of a new request the system with rate of λ moves to state (i+1,j,k)
    if the user still has capacity (ie, i+j<S×M ), otherwise the request will be blocked
    and the system stays in the current state. A container will be instantiated with
    rate ϕ for the request in the head of Global Queue and moves to (i−1,j+1,k) .
    The service time (ie, lifetime) of a containers finishes with rate of kμ and the
    system moves to (i,j−1,k) . If the utilization gets higher than the threshold,
    the scheduler requests a new VM, and the system moves to state (i,j,k+1) with
    rate α . Or, the utilization dropped bellow a certain value, MSP decommission
    a VM, and the system releases the idle VM so that moves to (i, j, k - 1) with
    rate β . Fig. 4: Microservice platform model. Show All Suppose that π (i,j,k)
    is the steady-state probability for the model (Fig. 4) to be in the state (i,
    j, k). So the blocking probability in MSP can be calculated as follow, b p q =
    ∑ (i,j,k)∈S π (i,j,k)  if i+j= L q (2) View Source We are also interested in two
    probabilities by which the MSP requests ( p req ) or releases ( p rel ) a VM.
    P req = ∑ (i,j,k)∈S π (i,j,k)  if u≥high−util & k<S P rel = ∑ (i,j,k)∈S π (i,j,k)  if
    u≤low−util & k>s (3) (4) View Source Using these probabilities, the rate by which
    microservice platform requests (ie, λ c ) or releases (ie, η c ) a VM can be calculated.
    λ c =λ× p req η c =μ× p rel (5) (6) View Source In order to calculate the mean
    waiting time in queue, we first establish the probability generating function
    (PGF) for the number of requests in the queue [10], as Q(z)= ∑ i=0 L q ( π (i,j,k)
    ) z i (7) View Source The mean number of requests in queue is q ¯ ¯ = Q ′ (1)
    (8) View Source Applying Little''s law [10], the mean waiting time in the global
    queue is given by: w t q ¯ ¯ ¯ ¯ ¯ ¯ ¯ = q ¯ ¯ λ(1−b p q ) (9) View Source The
    total response time of a request would be summation of waiting time in queue,
    VM provisioning time from IaaS in case of need and container provisioning time
    at MSP. Thus, the response time can be calculated as: rt ¯ ¯ ¯ ¯ = w t q ¯ ¯ ¯
    ¯ ¯ ¯ ¯ +( p req × 1 α )+(1/ϕ) (10) View Source We set α and ϕ based on our experiment
    on Amazon EC2 and Docker ecosystem which will be described in section V. The mean
    number of running VMs, mean number of running containers and mean utilization
    in the system can be calculated as follow: v m no ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ = ∑ (i,j,k)∈S
    k×( π (i,j,k) ) con t no ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ = ∑ (i,j,k)∈S j×( π (i,j,k)
    ) util ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ = ∑ (i,j,k)∈S ( i+j k×M )×( π (i,j,k) ) (11) (12) (13)
    View Source SECTION V. Experimental Setup and Results In this section, we present
    our microservice platform and discuss experiments that we have performed on this
    platform. For experiments we couldn''t use available third party platforms such
    as Docker Cloud or Nirmata as we needed full control of the platform for monitoring,
    parameter setting, and performance measurement. As a result, we have created a
    microservice platform from scratch based on the conceptual architecture presented
    in Fig. 2. We employed Docker Swarm as the cluster management system, Docker as
    the container engine and Amazon EC2 as the backend public cloud. We developed
    a front-end in Java for the microservice platform that interacts with the cluster
    management system (ie, Swarm) through REST APIs. The microservice platform leverages
    three initial VMs, two configured in worker mode and another one in master mode
    to manage the Docker Swarm cluster. All VMs are of type rn3. rnedium (1 virtual
    CPU with 3.5 GB memory). In our deployment, we have used Consul as the discovery
    service, that has been installed on the Swarm Manager VM. For the containers,
    we have used Ubuntu 16.04 image available on Docker Hub. Each running container
    was restricted to use only 512 MB memory, thus making the capacity of a VM to
    be 7 containers. The Swarm manager strategy for distributing containers on worker
    nodes was binpack. The advantage of this strategy is that fewer VMs can be used,
    since Swarm will attempt to put as many containers as possible on the current
    VMs before using another VM. Table II presents the input values for two scenarios
    in our experiments. Table II: Range of parameters for experiments To trigger VM
    elasticity, we have defined two cluster utilization (based on Eq. 1) thresholds:
    an upper threshold that signifies cluster is overloaded, and a lower threshold
    to show that the cluster is underloaded. In order to avoid oscillation or ping-pong
    effect in provisioning/releasing VMs, we do not add/remove a VM immediately after
    crossing thresholds rather we employ Algorithm 1 as the elasticity mechanism.
    SECTION Algorithm 1: The Decision Making Algorithm for Adding and Removing VMs
    In order to control experiment''s costs, we have limited the cluster size to maximum
    of 10 running VMs for the application, which gives us a maximum capacity of 70
    running containers. For the same reason, we set the container lifetime as 1 and
    2 minutes for two scenarios. Under this configuration, our experiments take up
    to 1000 minutes combined (300 for the first scenario and 700 minutes for the second).
    The results of our experiments have been presented in Fig. 5. Note that, the X
    axis in Fig. 5 is experiment time in which we report the average values of performance
    indicators in every single minute; hereafter we call each minute of the experiments
    as iteration. In the first experiment scenario (Fig. 5(a)), we have set the average
    lifetime of a container to one minute; the lower and upper utilization thresholds
    are set to 50% and 80% respectively (shaded area in the third plot of Fig. 5(a)
    shows the areas where the cluster is underloaded or overloaded). The arrival rate
    has a Poisson distribution with mean value of 20 requests per minute shown in
    the second plot of Fig. 5(a) with blue line. In the first plot, red line shows
    the number of running VMs and the blue line enumerates the number of running containers.
    An interesting observation here is the behavior of the system at the beginning
    of the experiment. Because the workload is very high and the capacity of the cluster
    is low, the response becomes very long (ie, up to approximately 85 s) that is
    attributed to long waiting time in the queue for capacity to become available.
    Once enough VMs have been provisioned, the queue empties and the response time
    drops. Occasional spikes appear in the response time when there is no available
    capacity (and a new VM has to be added to the cluster), but they are less severe.
    All in all, in scenario 1 the system is operating well with desired utilization
    and response time with no rejected request. In the second experiment scenario,
    presented in Fig. 5(b), we have increased the lifetime of a container to two minutes
    and changed the cluster utilization thresholds to 70% and 90%; we also increased
    the arrival rate from 20 req/min to 40 req/min around iteration 400. The other
    parameters of the experiments remained the same. We noticed the same high response
    time at the beginning of the experiment; this time the cluster scaled up to 10
    VMs (maximum allowed number) while still the queue did not get cleared; this is
    attributed to longer lifetime of containers that makes resource releasing slower
    compared to the first scenario. At this moment, because maximum capacity of the
    cluster has been reached, we have witnessed a large number of rejected requests
    (around iteration 20, the red line in the second plot). After 50 iterations the
    behaviour of the cluster is similar to that of scenario 1. Around iteration 400,
    we started to increase the workload (blue line in the second plot, Fig. 5(b)).
    This resulted in eventually utilization of all allowed VMs and rejection of requests
    as there was no capacity available. SECTION VI. Numerical Validation and Analysis
    In this section, We first validate the analytical model with results of experiments
    presented in section V. Second, thanks to minimal cost and runtime associated
    with analytical performance model, we leverage it to study interesting scenarios
    at large scale with different configuration and parameter settings to shed some
    light on MSP provisioning performance. We use the same parameters, outlined in
    Table II, for both experiments and numerical analysis. The analytical model has
    been implemented and solved in Python using NumPy, SciPy, Sympy and Matplotlib
    libraries [11]. Table III shows the comparison between the results from analytical
    model and experiment for both scenarios. As can be seen, both analytical models
    and experimental results are well in tune with differences less than 10%. Note
    that in analytical model for the sake of equilibrium conditions (ie, steady state)
    we put a limit on queue size while we do not have such a limitation in the experiment.
    Also, in the experiments we employ a more sophisticated elasticity policy (ie,
    Algorithm 1) compared to simple threshold approach in analytical model. These
    two differences (ie, queue limit and elasticity policies) are the source of narrow
    divergence between analytical model and the experiments. Fig. 5: Experimental
    results; see table II for parameter settings. Show All Table III: Corresponding
    results from analytical model (AM) and experiment (Exp) for both scenarios Now
    that the analytical model captures the microservice platforms accurately enough,
    we leverage it to study interesting scenarios at large scale. Note that analytical
    model take less than a minute to conclude while in experiment we needed around
    1000 minutes to compute desired performance indicators; more importantly, analytical
    model cost is negligible compared to the cost of experiments. Table IV presents
    the range of individual parameter values that we either set or calculate from
    our experiments for numerical analysis of the analytical model. Microservice applications
    may span across large number of VMs and, as a result, incorporate a very large
    number of containers. Also, container may live longer depending on the applications;
    for example consider an email application, which has been deployed as microservices,
    that spin a container for each active user; this container serves the user while
    he/she is checking or composing emails and will be released when the user logs
    out of the system [12]. Motivated by these facts we employed analytical model
    to investigate the microservice provisioning performance under such an assumptions.
    In general, as the analytical model scale linearly1, with increasing input parameters
    such as number of VMs and the number of containers per VMs, it can be leveraged
    to study microservice platforms with large scales. We let the mean container lifetime
    to be in the range of [8..20] minutes; also we set various cluster size to include
    [28..44] VMs. Under this parameter setting we obtained the interested performance
    indicators. Fig. 6(a) shows the trend of total response time when changing above
    control variables. As can be seen, in order to fulfill a request under 5 seconds,
    the container lifetime should not exceed 12 minutes and the cluster should include
    at least 40 VMs. Also, it can be noticed that none of the clusters can maintain
    response time lower than 25 seconds when the mean lifetime of containers is 20
    minutes on average. Table IV: Range of parameters for the analytical model Fig.
    6(b) shows the probability by which a request may get rejected due to either lack
    of room in the global queue or lack of capacity in the microservice platform.
    A linear relationship can be noticed between the container lifetime and the rejection
    probability. Also, for keeping the rejection below 5%, the application should
    employ at least 40 VMs. We also characterize the utilization of the cluster under
    various cluster size and container lifetime. As can be seen in Fig. 6(c), we set
    the desired utilization between 70% and 90%. If the mean container lifetime is
    8 minute, regardless of the cluster size, the utilization would be less than 70%
    which economically is not desirable. On the other hand, for containers with average
    lifetime of 20 minutes, the utilization of the cluster would be more than 90%
    which is not desirable for the sake of performance and reliability. In addition
    to results presented here, we also characterized the response time, rejection
    probability, utilization, number of VMs and number of containers for different
    arrival rate of requests which have been omitted due to page limit. SECTION VII.
    Related Work Performance analysis of cloud computing services has attracted considerable
    research attention although most of the works considered hypervisor based virtualization
    in which VMs are the sole way of providing isolated environment for the users.
    However, recently, container based virtualization is getting momentum due to its
    advantages over VMs for providing microservices. Performance analysis of cloud
    services considering containers as a virtualization option is in its infancy.
    Much of the works have been focused on comparison between implementation of various
    applications deployed either as VMs or containers. In [14], authors showed that
    containers have outperformed VMs in terms of performance and scalability. Container
    deployment process 5x more requests compared to VM deployment and also containers
    outperformed VMs by 22x in terms of scalability. This work shows promising performance
    when using containers instead of VMs for service delivery to end users. Fig. 6:
    Analytical model results; see table IV for input parameters. Show All Another
    study has been carried out in [15] to compare the performance of three implementations
    of an application using native, Docker and KVM implementation. In general, Docker
    equals or exceeds KVM performance in every case. Results showed that both KVM
    and Docker introduce negligible overhead for CPU and memory performance. The authors
    in [16] performed a more comprehensive study on performance evaluation of containers
    under different deployments. They used various benchmarks to study the performance
    of native deployment, VM deployment, native Docker and VM Docker. All in all,
    they showed that in addition to the well-known security, isolation, and manageability
    advantages of virtualization, running an application in a Docker container in
    a vSphere VM adds very little performance overhead compared to running the application
    in a Docker container on a native OS. Furthermore, they found that a container
    in a VM delivers near native performance for Redis and most of the micro-benchmark
    tests. Amaral et al. [4] evaluated the performance impact of choosing between
    the two models for implementing related processes in containers: in the first
    approach, master-slave, all child containers are peers of each other and a parent
    container which serves to manage them; in the second approach, nested-container,
    involves the parent container being a privileged container and the child containers
    being in its namespace. Their results showed that the nested-containers approach
    is a suitable model, thanks to improved resource sharing and guaranteeing fate
    sharing among the containers in the same nested-container. These studies reveal
    a promising future for using both virtualization techniques in order to deliver
    secure, scalable and high performant services to the end user [17], [18]. The
    recent popularity of microservice platforms such as Docker Cloud, Nirmata and
    Giant Swarm are attributed to such advantages mentioned above. However, to the
    best of our knowledge, there is no comprehensive performance model that incorporates
    the details of provisioning performance of microservice platforms. In this work,
    we studied the performance of PaaS and IaaS collaborating with each other to leverage
    both virtualization techniques for providing fine-grained, secure, scalable and
    performant microservices. SECTION VIII. Conclusions In this paper, we presented
    our work on provisioning performance analysis of microservice platforms. We first
    designed and developed a microservice platform on Amazon cloud using Docker family
    technologies. We performed experiments to study the important performance indicators
    such as total request response time, request rejection probability, cluster size
    and utilization while capturing the details of provisioning at both container
    and VM layers. Then we developed a tractable analytical performance model that
    showed a high fidelity to experiments. Thanks to linear scalability of the analytical
    model and realistic parameters from experiments, we could study the provisioning
    performance of microservice platforms at large scale. As a result, by leveraging
    proposed model and experiments in this paper, what-if analysis and capacity planning
    for microservice platform can be carried out systematically with minimum amount
    of time and cost. ACKNOWLEDGMENTS We would like to thank Dr. Murray Woodside for
    his valuable technical comments and inputs. This research was supported by Fuseforward
    Solutions Group Ltd., the Natural Sciences and Engineering Council of Canada (NSERC),
    and the Ontario Research Fund for Research Excellence under the Connected Vehicles
    and Smart Transportation (CVST) project. Authors Figures References Citations
    Keywords Metrics Footnotes More Like This Lightweight Virtualization Approaches
    for Software-Defined Systems and Cloud Computing: An Evaluation of Unikernels
    and Containers 2019 Sixth International Conference on Software Defined Systems
    (SDS) Published: 2019 Build Trust in the Cloud Computing - Isolation in Container
    Based Virtualisation 2016 9th International Conference on Developments in eSystems
    Engineering (DeSE) Published: 2016 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Efficiency Analysis of Provisioning Microservices
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.future.2018.01.022
  analysis: '>'
  authors:
  - Alfonso García Pérez
  - Germán Moltó
  - Miguel Caballer
  - Amanda Calatrava
  citation_count: 87
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. The SCAR
    framework 4. Use cases: Results and discussion 5. Conclusions and future work
    Acknowledgments References Vitae Show full outline Figures (12) Show 6 more figures
    Future Generation Computer Systems Volume 83, June 2018, Pages 50-59 Serverless
    computing for container-based architectures Author links open overlay panel Alfonso
    Pérez, Germán Moltó, Miguel Caballer, Amanda Calatrava Show more Share Cite https://doi.org/10.1016/j.future.2018.01.022
    Get rights and content Highlights • A framework to run containerized applications
    in serverless computing is proposed. • Containers out of Docker images can now
    be run on AWS Lambda. • Highly-parallel event-driven file-processing serverless
    computing is introduced. • An analysis of the Freeze/Thaw cycle of AWS Lambda
    and caching is assessed. • Bursty workloads of short stateless jobs can benefit
    from serverless computing. Abstract New architectural patterns (e.g. microservices),
    the massive adoption of Linux containers (e.g. Docker containers), and improvements
    in key features of Cloud computing such as auto-scaling, have helped developers
    to decouple complex and monolithic systems into smaller stateless services. In
    turn, Cloud providers have introduced serverless computing, where applications
    can be defined as a workflow of event-triggered functions. However, serverless
    services, such as AWS Lambda, impose serious restrictions for these applications
    (e.g. using a predefined set of programming languages or difficulting the installation
    and deployment of external libraries). This paper addresses such issues by introducing
    a framework and a methodology to create Serverless Container-aware ARchitectures
    (SCAR). The SCAR framework can be used to create highly-parallel event-driven
    serverless applications that run on customized runtime environments defined as
    Docker images on top of AWS Lambda. This paper describes the architecture of SCAR
    together with the cache-based optimizations applied to minimize cost, exemplified
    on a massive image processing use case. The results show that, by means of SCAR,
    AWS Lambda becomes a convenient platform for High Throughput Computing, specially
    for highly-parallel bursty workloads of short stateless jobs. Previous article
    in issue Next article in issue Keywords Cloud computingServerlessDockerElasticityAWS
    lambda 1. Introduction Cloud computing introduced the ability to provision on-demand
    computational resources reducing the needs for on-premises resources. Indeed,
    Virtual Machines (VMs) have played a fundamental role to create customized and
    replicable execution environments for applications, in order to guarantee successful
    executions. Also, elasticity has been the cornerstone functionality of IaaS (Infrastructure
    as a Service) Cloud computing where new VMs can be provisioned in order to cope
    with increased workloads. Public Cloud providers such as Amazon Web Services (AWS)
    [1] have fostered the migration of complex application architectures to the Cloud
    in order to take advantage of the pay-per-use cost model. In parallel, the mainstream
    adoption of Linux containers, propelled by the popularity of Docker [2], enabled
    users to maintain customized execution environments, in the shape of lightweight
    Docker images instead of bulky Virtual Machine Images. This paved the way for
    the microservices architectural pattern to rise, in order to decouple complex
    applications into several small, independently deployed services that interact
    via REST interfaces [3]. Creating distributed applications based on microservices
    required the ability to manage a fleet of Docker container at scale, thus fostering
    the appearance of Container Management Platforms (CMPs) such as Kubernetes, Apache
    Mesos or Docker Swarm. Public Cloud providers also provided their CMP offerings
    as a service, as is the case of Amazon ECS [4]. The ability to run containers
    at scale was adopted by public Cloud providers to create serverless computing
    [5] in which applications are defined as a set of event-triggered functions that
    execute without requiring the user to explicitly manage servers. As an example,
    AWS Lambda supports functions defined in different programming languages. AWS
    Lambda executes those functions on specific runtime environments provided by containers
    specifically tailored for the execution of the function, depending on the language
    chosen. Serverless computing introduces large-scale parallelism and it was specifically
    designed for event-driven applications that require to carry out lightweight processing
    in response to an event (e.g. to do a minor image manipulation when a file is
    uploaded to an Amazon S3 bucket or to access a NoSQL back-end to retrieve data
    upon invocation of a REST API). However, the programming model that these services
    impose, as is the case AWS Lambda, hinder the adoption of this service for the
    general execution of applications. It is true that one can include a binary application
    in a deployment package for AWS Lambda but, still, managing the execution of generic
    applications in AWS Lambda is far from being a trivial task. Considering the popularity
    of Docker as a software distribution approach, it would be convenient to be able
    to run containers out of Docker images in Docker Hub on AWS Lambda to support
    generic execution of applications on such a serverless platform. To this aim,
    this paper introduces SCAR, a framework to transparently execute containers (e.g.
    Docker) in serverless platforms (e.g. AWS Lambda). The following benefits are
    obtained: First, the ability to run customized execution environments in such
    platforms opens up new approaches to adopt serverless computing in scientific
    scenarios that were previously unable to easily exploit the benefits of such computing
    platforms. Second, new programming languages can be used apart from those natively
    supported by the serverless platform. Third, a highly-parallel event-driven file-processing
    serverless execution model is defined. This has allowed to run on AWS Lambda High
    Throughput Computing applications on customized runtime environments provided
    by Docker containers. After the introduction, the remainder of the paper is structured
    as follows. First, Section 2 describes the related work in this area. Then, Section
    3 describes the SCAR framework together with the underlying technology employed
    and the programming model that it introduces for event-driven file-processing
    applications. Next, Section 4 describes different use cases of this framework
    together with execution results in order to evaluate the benefits and limitations
    of the framework. Finally, Section 5 summarizes the main achievements and points
    to future work. 2. Related work Serverless computing is a new execution model
    that is currently emerging to transform the design and development of modern scalable
    applications. Its evolution is reinforced by the continuous advances in container-based
    technology together with the consolidation of cloud computing platforms. In this
    way, new event-driven services have appeared in the last three years. AWS Lambda
    [6] was the first serverless computing service to appear offered by Amazon Web
    Services, followed by Google Cloud Functions [7], Microsoft Azure Functions [8],
    and the open source platform Apache OpenWhisk [9]. A discussion about all these
    services can be found in the work by McGrath et al. [10], where authors evaluate
    the usage of this new event-driven technology with two different case studies,
    outlining the potential of cloud event-based services. Although it is a relatively
    new area, there exists in the literature several works contributing to the evolution
    of serverless computing. For example, the initial developments of OpenLambda are
    presented in [11] as an open-source platform for building web services applications
    with the model of serverless computing. The work also includes a case study where
    performance of executions in AWS Lambda are compared with executions in AWS Elastic
    Beanstalk [12], which concludes with better performance results for AWS Lambda.
    The study by Villamizar et al. [13] presents a cost comparison of a web application
    developed and deployed using three different approaches: a monolithic architecture,
    a microservices architecture operated by the cloud customer, and a microservices
    architecture operated by AWS Lambda. Results show that AWS Lambda reduces infrastructure
    costs more than 70% and guarantees the same performance and response times as
    the number of users increases. Several tools related with serverless computing
    are emerging in the literature. First, Podilizer [14] is a tool that implements
    the pipeline specifically for Java source code as input and AWS Lambda as output.
    Second, Snafu [15] is a modular system to host, execute and manage language-level
    functions offered as stateless microservices to diverse external triggers. Finally,
    [16] presents the prototype implementation of PyWren, a seamless map primitive
    from Python on top of AWS Lambda that is able to reuse one registered Lambda function
    to execute different user-defined Python functions. Also, use cases of this emerging
    event-based programming model can be found in the literature, like the work by
    Yan et al. [17] where the authors present a prototype architecture of a chatbot
    using the OpenWhisk platform, or the experiments described in [18] about face
    recognition with LEON, a research prototype built with OpenWhisk, Node-RED [19]
    and Docker. Another use of serverless computing is data analytics, exemplified
    in [20], covering data processing with Spark and OpenWhisk. All the works mentioned
    above highlight the advantages of using these new event-driven services because
    they are more elastic and scalable than previous platforms. Moreover, they point
    out the challenges derived from the granular nature of serverless computing. The
    work by Baldini et al. [5] regarding the open problems of serverless computing
    identifies several unaddressed challenges which include: (i) the ability to run
    legacy code on serverless platforms, and (ii) the lack of patterns for building
    serverless solutions. Indeed, this paper addresses the aforementioned challenges
    by introducing an open-source framework that enables users to run generic applications
    on a serverless platform, AWS Lambda in this case. This introduces unprecedented
    flexibility for the adoption of serverless computing for different kind of applications.
    Previously, users were constrained to create functions on the specific programming
    languages supported by AWS Lambda. In addition, we introduce a High Throughput
    Computing programming model to create highly-parallel event-driven file-processing
    serverless applications that execute on customized runtime environments (provided
    by Docker images) on AWS Lambda. Therefore, the main scientific contribution of
    SCAR is to democratize serverless computing for a wide range of scientific applications
    and programming languages that were not previously able to easily exploit a serverless
    platform. As far as the authors’ knowledge, this is the first work in the literature
    that proposes, and provides an open-source implementation, a framework to run
    containers out of Docker images in a serverless platform such as AWS Lambda. Indeed,
    several examples of applications successfully ported to serverless computing on
    AWS Lambda with SCAR are already available in the GitHub repository [21]. These
    include, but are not limited to, deep learning frameworks, such as Theano [22]
    and Darknet [23], programming languages such as Erlang [24] and Elixir [25] and
    generic tools for image and video processing such as ImageMagick [26] and FFmpeg
    [27]. We believe that the integration of Docker for application delivery with
    a serverless platform such as AWS Lambda provides an appropriate platform for
    different computing scenarios that require fast elasticity and the ability to
    scale beyond the limits of current IaaS Clouds. 3. The SCAR framework Fig. 1 describes
    the architectural approach designed to support container-based applications on
    a serverless platform. The services typically made available by the Cloud provider
    are: (i) Serverless service, also known as Functions as a Service (FaaS), responsible
    for executing the Cloud functions in response to events; (ii) File storage service,
    which hosts the files uploaded by the user and triggers the events to the Serverless
    service so that the file can be processed by the invocation of the function. file
    uploads will trigger invocations of the function where each one processes exactly
    one file; (iii) Log service, where the information concerning the execution of
    the function is logged; (iv) Monitoring service, which provides metrics of the
    resources consumed by the function. In addition, a Container Image Service is
    required, in order to store the images that include the operating system together
    with the application and its dependencies. An invocation of the function involves
    the execution of the Supervisor, responsible for: (i) Data stage from the File
    storage service into the temporary data space allocated to that particular function
    invocation; (ii) Cache management in order to minimize the data movement from
    the Container image service to the data space available to the function; (iii)
    Log management, to retrieve the output of the execution of the container. The
    supervisor delegates on a Container runtime in order to instantiate a Container
    out of an image, on which either a script or an application is run on the customized
    runtime environment provided by the container. Download : Download high-res image
    (302KB) Download : Download full-size image Fig. 1. Architectural approach for
    supporting container-based applications on serverless platforms. 3.1. Underlying
    technology employed in SCAR The following subsection identifies and justifies
    the different technology choices made to develop SCAR. 3.1.1. Cloud provider services:
    AWS As described in the related work section, there are different services for
    serverless computing. We chose AWS Lambda [6], a serverless computing service
    to run code, in the shape of functions created in a programming language supported,
    in response to events so that no explicit management of servers is performed by
    the user. The most important features and limitations of AWS Lambda are: (i) Constrained
    computing capacity currently limited by a maximum of 3008 MB, where CPU performance
    is correlated with the amount of the memory chosen; (ii) Maximum execution time
    of 300 s (5 min); (iii) Read-only file system based on Amazon Linux; (iv) 512
    MB of disk space in , which may be shared across different invocations of the
    same Lambda function; (v) Default concurrent execution limit of 1000 invocations
    of the same function, which can be increased, (vi) Supported execution environments:
    Node.js v4 and 6, Java 8, Python 3.6 and 2.7, .NET Core 1.01 (C#), and (vii) No
    inbound connections are allowed for the Lambda invocations. For the file storage
    service, Amazon S3 (Simple Storage Service) was chosen, an object storage designed
    to provide durable and highly available access to files, stored in buckets, which
    are created in a specific AWS Region. S3 can publish event notifications when
    certain actions occur. For example, the s3:ObjectCreated event type is published
    whenever the S3 APIs such as PUT, POST or COPY are used to create an object in
    a bucket. These events can be published for different destinations (services)
    such as Amazon SNS (a push messaging service), Amazon SQS (a message queuing service)
    and AWS Lambda. Amazon CloudWatch [28] is the monitoring service of AWS. In particular,
    CloudWatch Logs is a service to monitor, store and access log files produced from
    different sources and services in AWS. Therefore, the standard output generated
    by the Lambda function invocations are sent to CloudWatch Logs so that different
    Log streams are obtained from which to obtain the information regarding the execution
    of a given invocation. 3.1.2. Containers: Docker, Docker Hub and udocker Among
    the different choices for Linux containers, such as OpenVZ [29] and LXC/LXD [30],
    we chose Docker due to its mainstream adoption for software delivery. This is
    exemplified by Docker Hub [31] a cloud-based registry service that hosts Docker
    images and can automatically create them by linking code repositories, thus providing
    a centralized place to distribute Docker images. Since external packages cannot
    be installed on a FaaS platform, i.e., no root privileges are available to install
    Docker, a mechanism to run a container out of a Docker image on user space without
    requiring prior installation is needed. This is precisely the ability of udocker
    [[32], [33]], a tool to execute containers in user space out of Docker images
    without requiring root privileges. This allows pulling images from Docker Hub
    and create containers by non-privileged users on systems where Docker cannot be
    installed. This tool has demonstrated to be useful to run jobs on customized execution
    environments in both Grid environments (such as the European Grid Infrastructure)
    and HPC (High Performance Computing) clusters of PCs in the context of the INDIGO-DataCloud
    European project [34]. Udocker provides several execution modes, described in
    the documentation [35]. However, due to the restrictions of the execution environment
    provided in AWS Lambda, only the execution mode properly works, which involves
    using Fakechroot [36] with direct loader invocation. Using this approach, it is
    possible to run a process in the execution environment provided by a Docker image
    without actually creating a Docker container. Notice that process isolation is
    automatically provided by the execution model of AWS Lambda, where different invocations
    of the same function are run on isolated runtime spaces. 3.2. Architecture of
    SCAR SCAR allows users to define Lambda functions, where each invocation will
    be responsible for executing a container from a Docker image stored in Docker
    Hub and, optionally, execute a shell-script inside the container for further versatility.
    Fig. 2 describes the architecture of SCAR. The framework architecture is divided
    in two parts: Download : Download high-res image (422KB) Download : Download full-size
    image Fig. 2. Architecture of SCAR. SCAR Client. The client is a Python script
    that provides a Command-Line Interface (CLI) that is responsible for: (i) validating
    the input information from the user; (ii) creating the deployment package, which
    includes udocker; (iii) creating the Lambda function containing the SCAR Supervisor;
    (iv) providing an easy access to the Logs generated by each invocation of the
    Lambda function; (v) providing means for the user to manage the lifecycle of the
    Lambda function (init, list, run, delete) and (vi) manage the configuration to
    trigger events from an S3 bucket to a Lambda function. The client heavily uses
    the Boto 3 library [37] to interact with the AWS services SCAR Supervisor. The
    supervisor represents the code of the Lambda function, which targets the Python
    3.6 runtime environment and is responsible for: (i) retrieving the Docker image
    from Docker Hub using udocker into , unless the Docker image is already available
    there; (ii) creating the container out of the Docker image and setting the appropriate
    execution mode for udocker; (iii) in case of being triggered from S3, manage the
    staging of the input data into the container and the stage out of the output results
    back into S3; (iv) passing down the environment variables to the container (those
    defined by the user and others of interest, such as the temporary credentials,
    so that the code running in the container has precisely the same privileges as
    the Lambda function itself; (v) merging the generated output from the script running
    in the container to the Lambda output in order to have consolidated logging available
    in CloudWatch Logs. The usage of SCAR in order to run a generic application on
    AWS Lambda is as follows, as described in Fig. 2. First, the user chooses a Docker
    image available in Docker Hub and creates (init) the Lambda function with the
    specific performance configuration provided by the user (in terms of memory).
    Second, the user can directly invoke (run) the Lambda function. This triggers
    the SCAR supervisor which, as described earlier, effectively ends up executing
    a container out of a Docker image and optionally run a user-defined shell script.
    Data staging from and to S3 is automatically managed by the SCAR supervisor, together
    with diverting the logs into CloudWatch. Notice that caching is a fundamental
    technique for the SCAR framework, especially considering the maximum execution
    time of 300 s. The first invocation of a Lambda function will pull the Docker
    image from Docker Hub into , which can take a considerable amount of time (in
    the order of seconds), depending on the size of the Docker image. Subsequent invocations
    of the Lambda function may already find that Docker image available in and, therefore,
    there is no need to retrieve the Docker image again from Docker Hub. However,
    caching does not restrict to the Docker image. In addition, the container created
    with udocker, is also shared among all the Lambda invocations that may find it
    already available in . The rationale behind this approach is that since Lambda
    functions are provided with a read-only file system, so are provided the scripts
    executed in the containers run on the Lambda functions. Notice that due to the
    stateless environment inherent to the Lambda functions, caching does not introduce
    side effects. It just reduces the invocation time whenever the cache is hit and
    the container is already available. Therefore, the duration of the Lambda function
    invocation using SCAR is greatly dependent on the ability for the Lambda functions
    to find a cached container file system in . This will be thoroughly assessed in
    Section 4. Concerning the overhead introduced by SCAR, users should be aware that
    it requires a reduced amount of memory and disk space to run ( 36 MB of RAM and
    16 MB of disk space). Indeed, it is the size of the container what really determines
    how much disk space is going to be available to be used by the applications. Empirical
    experimentations show that an image in Docker Hub larger than 220 MB will hardly
    fit inside the ephemeral storage allocated to the Lambda function, due to the
    storage requirements of both the Docker image and the container file system unpacked
    by udocker. Regarding the remaining time available for the execution of the application,
    once the first invocation is finished and the container is cached, SCAR takes
    a negligible time to check if the container exists, thus allowing the application
    to run during almost the maximum time provided by the function. 3.3. Event-Driven
    File-Processing serverless programming model SCAR introduces a programming model
    that can be effectively used to process files on demand, by triggering the execution
    of the Lambda function as soon as a file is available in an Amazon S3 bucket.
    This is a summary of the programming model, exemplified by an application to transform
    videos into a grayscale using the well-known ffmpeg application. This use case
    is available online for the reader to test it [38]. The following command creates
    a Lambda function that will be triggered for any video upload to the input folder
    in the scar-test bucket. Upon invocation of the Lambda function, a container out
    of the sameersbn/ffmpeg Docker image (available in Docker Hub) will be started
    and the process.sh shell-script will be executed inside. Download : Download high-res
    image (23KB) Download : Download full-size image The programming model results
    in the following workflow: Download : Download high-res image (189KB) Download
    : Download full-size image Fig. 3. Sample script to perform video transcoding.
    1. The user uploads the video into the input folder of the scar-test S3 bucket.
    2. The input video file is made available to the executed container in /tmp/$REQUEST_ID/input,
    as specified by the $SCAR_INPUT_FILE environment variable. 3. The script converts
    the video to grayscale and saves the output file into /tmp/$REQUEST_ID/output.
    4. The video file is automatically uploaded to the output folder of the Amazon
    S3 bucket and deleted from the underlying storage. The content of the script is
    included in Fig. 3. Notice the simplicity of the script that just invokes the
    command-line application in order to process the video using FFmpeg into an output
    folder. Since data stage is automatically managed by the SCAR Supervisor (running
    on the Lambda function), the user just focuses on how to specifically process
    one file. Then, multiple instances of this script can be run in parallel, each
    one running on its own Lambda invocation in order to simultaneously process different
    videos at scale, taking into account the limited storage space available. This
    approach also facilitates testing, which can be performed locally to process one
    file within a Docker container and then can be scaled out to thousands of concurrent
    invocations run on AWS Lambda. It is important to point out that there is currently
    no other work in the literature that proposes a High Throughput Computing Programming
    Model to create highly-parallel event-driven file-processing serverless applications
    that execute on customized runtime environments provided by Docker containers
    run on a serverless platform such as AWS Lambda. 4. Use cases: Results and discussion
    This section provides a thorough assessment of the capabilities and limitations
    of SCAR, which are greatly dependent on the underlying features of AWS Lambda.
    As stated in Section 3.1.1, Lambda functions have a maximum execution time of
    300 s and an allocated temporary space (in ) of 512 MB. Going beyond those thresholds
    make the Lambda function invocation fail. According to the documentation [39],
    when a Lambda function is invoked, AWS Lambda launches a container (i.e., an execution
    environment). After a Lambda function is executed, the service keeps the container
    for some time just in case the same Lambda function is invoked again. The service
    freezes the container after a Lambda function finishes, and thaws the container
    for reuse, in the case AWS Lambda decides to reuse the container when the Lambda
    function is invoked again, a process known as the freeze/thaw cycle. Therefore
    the content remains when the container is frozen, providing a transient cache
    that can be used for multiple invocations Notice that, when using SCAR, a container
    in user-space is run via udocker inside the container provided by the AWS Lambda
    invocation. Being able to cache the user-space container dramatically affects
    the execution time of a Lambda function created by SCAR. To this end, Section
    4.1 provides a comprehensive study of the reuse of the ephemeral disk space. Next,
    Section 4.2 introduces a realistic use case of the programming model presented
    in Section 3.3: A customized execution environment containing an open-source deep
    learning framework is used to recognize and classify a set of images stored in
    a Cloud provider making use of the massive scaling capabilities of the serverless
    architectures with the help of SCAR. 4.1. On the Freeze/Thaw cycle: Disk space
    reuse Due to the importance of the AWS Lambda Freeze/Thaw cycle for the SCAR framework,
    an study of the behavior of this feature is conducted in order to extract optimized
    invocation patterns to be used in SCAR. For all the tests in this study, only
    the time used to download the Docker image from Docker Hub and run actions (i.e.,
    creation of the container via udocker) are measured. The time spent by the script
    executed inside the container is negligible for this study. Also, the memory was
    set to 512 MB for all the functions. The first comparison is done between the
    two different invocation types that AWS Lambda offers (i.e. request–response and
    asynchronous) [40]. We analyze the time spent for each invocation. Each one involves
    creating a container out of a Docker image (in this case centos:7 [41]) stored
    in Docker Hub and executing a trivial shell-script inside. Ten different Lambda
    functions were created for each invocation type (i.e. a total of 20 different
    Lambda functions). Each function was invoked a hundred times (i.e. a total of
    2.000 invocations). Fig. 4 shows the average execution time for each invocation
    type. The req–resp line refers to the request–response invocation type and it
    shows that all the invocations performed after the first one take a negligible
    time to execute. This means that the first invocation spends 25 s on average downloading
    the container image from Docker Hub, creating the container and running a script
    inside it. However, the subsequent invocations execute very rapidly. This is a
    coherent behavior when using the request–response type, where subsequent invocations
    after the first one will find the container file system already available in the
    ephemeral space of the Lambda function invocation (i.e. cached by SCAR). Download
    : Download high-res image (151KB) Download : Download full-size image Fig. 4.
    Average execution time (in seconds) for each invocation type (i.e. request–response
    and asynchronous). Download : Download high-res image (219KB) Download : Download
    full-size image Fig. 5. Average execution time (in seconds) for different container
    sizes. All the invocations are asynchronous. The async line refers to the asynchronous
    invocation type and it shows an slightly erratic behavior in the execution time
    of the functions along the first 40 invocations. Indeed, the asynchronous model
    carries out all the invocations almost simultaneously, though there is a slight
    delay due to the SCAR invocation manager. Therefore, the Lambda functions that
    are invoked first do not find the container cached in and need to retrieve the
    Docker image from Docker Hub and create the container. This means that the execution
    is performed successfully, but requires additional time. After approximately 40
    invocations, the container is finally cached in the ephemeral disk space and,
    therefore, the execution time of the subsequent invocations decreases considerably,
    as expected. Therefore, the main conclusion of this first experiment is that a
    newly created Lambda function with SCAR should be executed at least once, in order
    to fully cache the container in the ephemeral disk space, before attempting to
    perform multiple asynchronous invocations. To further investigate how the container
    size affects the caching behavior when using the asynchronous invocation type,
    we launched different functions that use different container image sizes and we
    analyzed the duration of the invocations. To carry out this experiment we created
    three different function types: the minideb type, which uses the Docker image
    bitnami/minideb [42] and has an image size of 22 MB; the c7 type, which uses the
    Docker image centos:7 [41] and has an image size of 70 MB; and finally the ub14
    type, which uses the Docker image grycap/jenkins:ubuntu14.04-python [43] and has
    an image size of 153 MB. Each one of these types are used to create ten functions
    (i.e. a total of 30 different Lambda functions) and each different function is
    invoked a hundred times (i.e. 3.000 invocations in total). For the sake of clarity,
    the execution times of the functions belonging to the same type are presented
    as average values. Fig. 5 shows the results of these invocations. The minideb
    function, which represents the smallest container image, is the first to present
    a cached behavior. The c7 function, which has the medium size container image,
    is cached after approximately 30 invocations and the function with the largest
    container image, ub14, requires more invocations, approximately 80, before it
    is cached. In this figure, it can be clearly seen the relation between increasing
    the container image size and the time that takes the container image to be cached
    in the ephemeral disk space. As explained above, this is directly related to the
    asynchronous way of working of the Lambda functions, in which the system does
    not wait for the previous invocation to finish. Therefore, the container image
    can be cached or not depending on the time passed since the first execution and
    the size of such container image. As seen in the experiments, on the one hand
    there is the request–response invocation, where SCAR achieves a cached behavior
    starting from the second invocation, at the expense of waiting for that first
    invocation to end. On the other hand, there is the asynchronous invocation type,
    in which all the invocations can be carried out in parallel, but the container
    size affects the time until the containers are cached by SCAR in the ephemeral
    disk space. Download : Download high-res image (126KB) Download : Download full-size
    image Fig. 6. Average execution time (in seconds) for different container sizes.
    The first invocation type is request–response and subsequent are asynchronous.
    Download : Download high-res image (103KB) Download : Download full-size image
    Fig. 7. Execution time (in seconds) used by the function related to the time waited
    (in minutes) from a previous invocation of the same Lambda function. In the programming
    model proposed by SCAR, the executions benefit from both invocation types by performing
    the first invocation as request–response and the rest invocations as asynchronous.
    To assess the advantages of this approach, an experiment was carried out using
    the aforementioned approach. The same function types described in the previous
    test were used to carry out this experiment: minideb, c7 and ub14. These types
    were used to generate 30 different functions and each function was invoked a hundred
    times. Again, for the sake of clarity, the execution times of the functions belonging
    to the same type are presented as average values. Fig. 6 presents the results
    of this experiment. The erratic behavior of the caching system has disappeared
    and all the invocations present a cached performance starting from the second
    invocation and thereafter. This approach allows SCAR to reduce the overall execution
    time and, therefore, we adopt it for the event-driven file processing programming
    model. As such, a Lambda function created with SCAR is previously preheated, i.e.,
    invoked with a request–response type, so that subsequent parallel asynchronous
    invocations already find the container cached in the ephemeral disk space. The
    last experiment designed for this case study investigates the influence of the
    time between invocations on the ability for AWS Lambda to reuse the container,
    which ends up on SCAR being able to find a cached container, thus speeding up
    the Lambda function invocation. Therefore, since the benefits of preheating a
    Lambda function were identified, it is important to know the time before it cools
    down again, i.e., when the invocation of the function will not reuse the same
    Lambda container and, therefore, SCAR will have to retrieve the Docker image again
    from Docker Hub and create a new user-space container. To this end, this experiment
    invokes the Lambda function in predefined ranges of time (each one of them defined
    by increasing 15 min the previous waiting time). Eleven time ranges were defined,
    from 0 to 150 min of waiting time. A thousand different functions were created
    and invoked just once in each period, resulting in a total of 11.000 invocations
    launched. Fig. 7 shows the results of this experiment using a box plot representation
    of the time used by each invocation. The whiskers of the box plot extend for a
    range equal to 1.5 times the interquartile range. By analyzing the figure we can
    extract three conclusions: first, all the functions show that the invocations
    are being cached between 15 and 30 min; second, if we wait more than 60 min the
    cache is lost and all of our functions need to download again the container image,
    and finally, waiting 45 or 60 min do not ensure the correct working of the cache
    due to the randomness of the underlying system. Understanding the behavior of
    the Freeze/Thaw cycle and, therefore, when an invocation of a Lambda function
    created with SCAR will be cached, enables to adopt best practices when adopting
    serverless computing to execute generic applications. This knowledge is applied
    in the following section with a real example on image recognition using a deep
    learning framework. 4.2. Massive image processing In this section SCAR is used
    to deploy a customized execution environment in order to recognize different patterns
    in images using deep learning techniques. The framework used to recognize the
    patterns is Darknet [44], an open source neural network framework written in C,
    in combination with the You Only Look Once (YOLO) [45] library. The Docker image
    used for this case study can be found in the grycap/darknet [46] Docker Hub repository
    and the memory set for the function in this case study is 1024 MB, which is the
    minimum function size that allows to run the experiment due to memory and time
    constraints. The programming model presented in Section 3.3 has been extended
    with the ability to automatically perform Lambda invocations out a set of files
    already available in a Cloud storage service (Amazon S3, in this case). This feature
    allows the user to reuse an existing S3 bucket with data files in order to perform
    a High Throughput Computing analysis across all the files in that bucket. A Lambda
    invocation per file will be carried out (up to the 1.000 soft limit of concurrent
    Lambda invocations). Each Lambda invocation will execute a shell-script to process
    exactly one file. For this experiment we use: (i) a thousand images of animals
    and objects already stored in an AWS S3 bucket with a size of almost 500 MB; (ii)
    the Docker image stored in Docker Hub which contains all the libraries and dependencies
    needed to execute the Darknet software and (iii) a shell-script executed inside
    the container, in charge of processing the input image, using DarkNet and the
    YOLO library to obtain the output (the patterns recognized in the image). Download
    : Download high-res image (215KB) Download : Download full-size image Fig. 8.
    Script used to launch the YOLO object detection library of the Darknet framework.
    It also processes the inputs and outputs of the container. The script used to
    launch the Darknet object detection software is presented in Fig. 8. The SCAR_INPUT_FILE
    variable is created by SCAR to simplify the creation of the script and contains
    the name of the file received by the Lambda function invocation. Some output variables
    are created and the information about the script execution is written in the standard
    output. When the container execution finishes and the output files have been processed,
    SCAR writes all the standard output produced by the container in the Lambda function
    logs, thus easing the traceability of possible errors. The Darknet invocation
    command receives an image as an input file and stores the results in two separate
    files, the OUTPUT_IMAGE which will be the image with the recognized objects and
    the RESULT file which will contain the percentage of certainty for each recognized
    object. Sample input/output images are presented in Fig. 9, Fig. 10 respectively.
    Also the output file generated after processing the image is shown in Fig. 11.
    To finish, the image produced is moved next to the output file into the output
    folder, so that SCAR can automatically upload them into S3. Remember that the
    user does not have to write any code to manage the download and upload of the
    input and output files respectively. All the files are managed transparently by
    SCAR. After the execution of the case study, the following metrics were retrieved:
    2 min used to finish the experiment; 880 min as the total aggregated execution
    time across the multiple Lambda function invocations; 4.575 different objects
    and animals recognized. As summary, in only two minutes we have downloaded, processed
    and uploaded a thousand images without worrying about the deployment and the management
    of the architecture. Download : Download high-res image (1MB) Download : Download
    full-size image Fig. 9. Test image passed to the Darknet framework. Download :
    Download high-res image (1MB) Download : Download full-size image Fig. 10. Animals
    recognized after the execution of the YOLO library. This output image has been
    generated by the Darknet framework. Download : Download high-res image (86KB)
    Download : Download full-size image Fig. 11. Darknet output file produced for
    the test image. It contains the total execution time and the objects recognized
    (in this case animals) with a percentage of certainty. It is important to point
    out four main conclusions that arise from this case study. Firstly, without SCAR
    the user has no easy way of using specific libraries such as Darknet in serverless
    providers like AWS Lambda. Secondly, the user does not have to manage the deployment
    of computational infrastructure, auto-scaling, coordinating the execution of jobs,
    etc. Instead, the serverless computing platform introduced seamless elasticity
    by performing multiple concurrent executions. Thirdly, the simplicity of the programming
    model introduced by SCAR just requires the user to write a shell-script to process
    a file assuming that will be automatically delivered. This is probably the simplest,
    most convenient approach to perform a file-processing application on the Cloud.
    Fourthly, once the Lambda function has been created by SCAR, this turns into a
    reactive service that is left on the Cloud service at no cost unless it is triggered
    again by uploading a new file to the bucket. This will cause a new Lambda function
    invocation, resulting in the creation of the container and execution of the shell-script
    to process the file. This has an important economic factor, since real pay-per-use
    is enforced as opposed to the pay-per-deploy approach that happens when deploying
    a VM in a Cloud service, which has a cost regardless of the actual usage of the
    VM. Finally, notice that the ability to scale in the order of thousands of Lambda
    function invocations reduces the requirement for a job scheduler, in cases where
    the incoming workload can be seamlessly absorbed by the underlying computing platform.
    To finish, we present a cost of the case study execution. Based on the metrics
    extracted earlier, the average execution time of the invocations is 52.8 s. Each
    function used 1024 MB of memory, and one invocation per photo available in the
    bucket was carried out, 1.000 in total. The AWS Lambda pricing calculator [47]
    indicates a cost of $0.88. Since AWS Lambda offers a free usage tier that includes
    1 million requests and 400.000 GB-s of compute time per month, and this use case
    involved 1.000 requests and 52.000 GB-s, the real cost of classifying the images
    was $0. 5. Conclusions and future work This paper has introduced SCAR, a framework
    to execute container-based applications using serverless computing, exemplified
    using Docker as the technology for containers and AWS Lambda as the underlying
    serverless platform. This is a step forward contribution to the state of the art,
    implemented in an open-source framework, that opens new avenues for adopting serverless
    computing for a myriad of scientific applications distributed as Docker images.
    Using the proposed approach, customized execution environments can now be employed
    instead of being locked-in to programming functions in the programming languages
    supported by the serverless platform (in our case AWS Lambda). This has easily
    introduced the ability to run generic applications on specific runtime environments
    defined by Docker Images stored in Docker Hub, a functionality that is actually
    missing from the current serverless computing platforms. A High Throughput Computing
    programming model has been developed in order to create highly-parallel event-driven
    file-processing serverless applications that execute on customized runtime environments
    provided by Docker containers run on AWS Lambda. This has been exemplified by
    using a deep learning application to perform pattern recognition on an image dataset.
    SCAR not only provides means to deploy containers in AWS Lambda, it also manages
    the Lambda functions’ lifecycle and eases the execution of the serverless workflow
    by applying optimizations without the need of user intervention, such as caching
    the container’s underlying file system to minimize the execution time. However,
    the current limitations of AWS Lambda in terms of maximum execution time (5 min),
    maximum allocated memory (3008 MB) and, most important, ephemeral disk capacity
    (512 MB), impose serious restrictions for the applications that can benefit from
    SCAR. Bursty workloads of short stateless jobs are specially appropriate to benefit
    from the ultra-elastic capabilities of AWS Lambda, both in terms of the amount
    of concurrent Lambda function invocations (in the order of thousands) and the
    rapid elasticity (in the order of few seconds). Having said that, we expect these
    limits to be risen in future updates of the service, which will be greatly help
    expand the adoption of SCAR for applications that cannot be encapsulated in a
    Docker image fitting in such scarce amount of computing and storage resources.
    Future work of SCAR includes adapting the development to other serverless providers.
    In particular, our dependence on udocker, begin developed in Python, suggests
    using a provider supporting that language, such as Microsoft Azure Functions (Google
    Cloud Functions currently only supports Node.JS). Notice that the programming
    model of SCAR is agnostic to the provider. In addition, SCAR users could benefit
    from a mechanism that maintains the deployed Lambda functions ‘hot’, based on
    the knowledge extracted from the freeze/thaw cycle study by means of periodic
    invocations of the Lambda functions. Finally, we are currently researching on
    mechanisms to checkpoint applications so that new Lambda functions are spawn recursively
    in order to bypass the maximum execution time for iterative scientific applications.
    Acknowledgments The authors would like to thank the Spanish “Ministerio de Economía,
    Industria y Competitividad” for the project “BigCLOE” under grant reference TIN2016-79951-R.
    The authors would also like to thank Jorge Gomes from LIP for the development
    of the udocker tool. References [1] Amazon, Amazon Web Services (AWS). https://aws.amazon.com.
    (Online; accessed 25 July 2017). Google Scholar [2] Docker, Docker. https://www.docker.com.
    (Online; accessed 25 July 2017). Google Scholar [3] Dragoni Nicola, Giallorenzo
    Saverio, Lluch-lafuente Alberto, Mazzara Manuel, Montesi Fabrizio, Mustafin Ruslan,
    Safina Larisa Microservices: yesterday, today and tomorrow CoRR (2016) abs/1606.04036
    Google Scholar [4] Amazon, Amazon EC2 Container Service. https://aws.amazon.com/ecs/.
    (Online; accessed 25 July 2017). Google Scholar [5] Ioana Baldini, Paul Castro,
    Kerry Chang, Perry Cheng, Stephen Fink, Vatche Ishakian, Nick Mitchell, Vinod
    Muthusamy, Rodric Rabbah, Aleksander Slominski, Philippe Suter, Serverless computing:
    Current trends and open problems, Jun. 2017, pp. 1-20. Google Scholar [6] Amazon,
    Amazon Lambda (AWS Lambda). https://aws.amazon.com/lambda/. (Online; accessed
    25 July 2017). Google Scholar [7] Google, Google Cloud Functions. https://cloud.google.com/functions/.
    (Online; accessed 25 July 2017). Google Scholar [8] Microsoft. Microsoft Azure
    Functions. https://azure.microsoft.com/en-in/services/functions/ . (Online; accessed
    25 July 2017). Google Scholar [9] The Apache Software Foundation. Apache Openwhisk.
    http://openwhisk.org/. (Online; accessed 25 July 2017). Google Scholar [10] G.
    Mcgrath, J. Short, S. Ennis, B. Judson, P. Brenner, Cloud event programming paradigms:
    Applications and analysis, in: 2016 IEEE 9th International Conference on Cloud
    Computing, CLOUD, June 2016, pp. 400–406. Google Scholar [11] Hendrickson Scott,
    Sturdevant Stephen, Harter Tyler, Venkataramani Venkateshwaran, Arpaci-Dusseau
    Andrea C., Arpaci-Dusseau Remzi H. Serverless computation with openlambda 8th
    USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 16, USENIX Association,
    Denver, CO (2016) Google Scholar [12] Amazon, AWS Elastic Beanstalk. https://aws.amazon.com/elasticbeanstalk.
    (Online; accessed 25 July 2017). Google Scholar [13] Villamizar Mario, Garcés
    Oscar, Ochoa Lina, Castro Harold., Salamanca Lorena, Verano Mauricio, Casallas
    Rubby, Gil Santiago, Valencia Carlos, Zambrano Angee, Lang Mery Cost comparison
    of running web applications in the cloud using monolithic, microservice, and AWS
    lambda architectures Serv. Oriented Comput. Appl., 11 (2) (2017), pp. 233-247
    CrossRefView in ScopusGoogle Scholar [14] Spillner Josef, Dorodko Serhii Java
    code analysis and transformation into AWS lambda functions CoRR (2017) abs/1702.05510
    Google Scholar [15] Spillner Josef Snafu: Function-as-a-Service (FAAS) runtime
    design and implementation CoRR (2017) abs/1703.07562 Google Scholar [16] Jonas
    Eric, Venkataraman Shivaram, Stoica Ion, Recht Benjamin Occupy the cloud: Distributed
    computing for the 99% CoRR (2017) abs/1702.04024 Google Scholar [17] Yan Mengting,
    Castro Paul, Cheng Perry, Ishakian Vatche Building a chatbot with serverless computing
    Proceedings of the 1st International Workshop on Mashups of Things and Apis, MOTA
    ’16, ACM, New York, NY, USA (2016), pp. 1-5 Google Scholar [18] Glikson Alex,
    Nnastic Stefan, Dustdar Schahram Deviceless edge computing: Extending serverless
    computing to the edge of the network Proceedings of the 10th ACM International
    Systems and Storage Conference, SYSTOR ’17, ACM, New York, NY, USA (2017), p.
    28:1 Google Scholar [19] Node-RED, Node-RED. https://nodered.org/. (Online; accessed
    25 July 2017). Google Scholar [20] Alex Glikson, TRANSIT: Flexible pipeline for
    IoT data with Bluemix and Openwhisk. https://medium.com/openwhisk/transit-flexible-pipeline-for-iot-data-with-bluemix-and-openwhisk-4824cf20f1e0
    . (Online; accessed 25 July 2017). Google Scholar [21] GRyCAP, SCAR Github repository.
    https://github.com/grycap/scar. (Online; accessed 25 July 2017). Google Scholar
    [22] Theano, Theano. http://deeplearning.net/software/theano. (Online; accessed
    25 July 2017). Google Scholar [23] Joseph Redmon, Darknet. https://pjreddie.com/darknet/.
    (Online; accessed 25 July 2017). Google Scholar [24] Erlang, Erlang. https://www.erlang.org/.
    (Online; accessed 25 July 2017). Google Scholar [25] Jose Valim, Elixir. https://elixir-lang.org/.
    (Online; accessed 25 July 2017). Google Scholar [26] ImageMagick Studio, Image
    Magick. https://www.imagemagick.org. (Online; accessed 25 July 2017). Google Scholar
    [27] FFmpeg, FFmpeg. https://ffmpeg.org/. (Online; accessed 25 July 2017). Google
    Scholar [28] Amazon, Amazon CloudWatch. https://aws.amazon.com/cloudwatch. (Online;
    accessed 25 July 2017). Google Scholar [29] Virtuozzo, OpenVZ. https://openvz.org.
    (Online; accessed 25 July 2017). Google Scholar [30] Canonical, LXC. https://linuxcontainers.org/.
    (Online; accessed 25 July 2017). Google Scholar [31] Docker, Docker Hub. https://hub.docker.com/.
    (Online; accessed 25 July 2017). Google Scholar [32] Jorge Gomes, udocker. https://github.com/indigo-dc/udocker.
    (Online; accessed 25 July 2017). Google Scholar [33] Jorge Gomes, Isabel Campos,
    Emanuele Bagnaschi, Mario David, Luis Alves, Joao Martins, Joao Pina, Alvaro Lopez-Garcia,
    Pablo Orviz, Enabling rootless linux containers in multi-user environments: the
    udocker tool, 2017. arXiv preprint arXiv:1711.01758. Google Scholar [34] D. Salomoni,
    I. Campos, L. Gaido, G. Donvito, P. Fuhrman, J. Marco, A. Lopez-Garcia, P. Orviz,
    I. Blanquer, G. Molto, M. Plociennik, M. Owsiak, M. Urbaniak, M. Hardt, A. Ceccanti,
    B. Wegh, J. Gomes, M. David, C. Aiftimiei, L. Dutka, S. Fiore, G. Aloisio, R.
    Barbera, R. Bruno, M. Fargetta, E. Giorgio, S. Reynaud, L. Schwarz, INDIGO-Datacloud:
    foundations and architectural description of a Platform as a Service oriented
    to scientific computing. Technical report, INDIGO-DataCloud, Mar 2016. Google
    Scholar [35] Jorge Gomes, udocker documentation. https://github.com/indigo-dc/udocker/blob/udocker-fr/doc/user_manual.md.
    (Online; accessed 25 July 2017). Google Scholar [36] JPiotr Roszatycki, Fakechroot.
    https://github.com/dex4er/fakechroot. (Online; accessed 25 July 2017). Google
    Scholar [37] Amazon, Boto 3. http://boto3readthedocs.io. (Online; accessed 25
    July 2017). Google Scholar [38] GRyCAP, FFmpeg on AWS Lambda. https://github.com/grycap/scar/tree/master/examples/ffmpeg
    . (Online; accessed 25 July 2017). Google Scholar [39] Amazon, AWS Lambda. How
    it works. http://docs.aws.amazon.com/es_es/lambda/latest/dg/lambda-introduction.html.
    (Online; accessed 25 July 2017). Google Scholar [40] Amazon, Invoke. http://docs.aws.amazon.com/es_es/lambda/latest/dg/API_Invoke.html.
    (Online; accessed 25 July 2017). Google Scholar [41] CentOS, Docker Hub: centos:7.
    https://hub.docker.com/_/centos/. (Online; accessed 25 July 2017). Google Scholar
    [42] Bitnami, Docker Hub: bitnami/minideb. https://hub.docker.com/r/bitnami/minideb/.
    (Online; accessed 25 July 2017). Google Scholar [43] GRyCAP, Docker Hub: grycap/jenkins:ubuntu14.04-python.
    https://hub.docker.com/r/grycap/jenkins . (Online; accessed 25 July 2017). Google
    Scholar [44] Joseph Redmon, Darknet: Open source neural networks in C, 2013-2016.
    http://pjreddie.com/darknet/. Google Scholar [45] Joseph Redmon, Ali Farhadi,
    Yolo9000: Better, faster, stronger, 2016. arXiv preprint arXiv:1612.08242. Google
    Scholar [46] GRyCAP, Docker Hub: grycap/darknet. https://hub.docker.com/r/grycap/darknet/
    . (Online; accessed 25 July 2017). Google Scholar [47] Amazon, Amazon Lambda pricing
    calculator. https://s3amazonaws.com/lambda-tools/pricing-calculator.html . (Online;
    accessed 25 July 2017). Google Scholar Cited by (0) Alfonso Pérez received B.Sc.
    + M.Sc. degrees in Computer Science and M.Sc. degree in Computer Engineering from
    the Universitat Politècnica de València (UPV), Spain, in 2011 and 2014, respectively.
    In 2011 he initiated his research career working in medical informatics projects
    related with clinical prediction models and computer interpretable guidelines.
    He has been member of the Grid and High Performance Computing research group (GRyCAP)
    at the Institute for Molecular Imaging (I3M) since 2015 where he began doing research
    work related with automatic cluster deployment and elasticity. In 2016 he started
    his Ph.D. in the field of Big Data and High Throughput Computing on Container-based
    and Serverless Infrastructures. Germán Moltó received B.Sc. and Ph.D. degrees
    in Computer Science from the Universitat Politècnica de València (UPV), Spain,
    in 2002 and 2007, respectively. He has been a member of the Grid and High Performance
    Computing research group (GRyCAP) at the Institute for Molecular Imaging (I3M)
    since 2002. He is also an associate professor in the Department of Computer Systems
    and Computation (DSIC) at UPV. He has participated in several European projects
    and led national research projects in the area of cloud computing. His broad research
    interests are cloud computing and scientific computing. Miguel Caballer obtained
    B.Sc., M.Sc., and Ph.D. degrees in Computer Science from the Universitat Politècnica
    de València (UPV), Spain, in 2000, 2012, and 2014, respectively. He has been a
    member of the Grid and High Performance Computing research group at the Institute
    for Molecular Imaging (I3M) since 2001. He has participated in several European
    and national research projects related to the application of parallel, grid, and
    cloud computing techniques to various areas of engineering. His other fields of
    interest include green computing. Amanda Calatrava received the B.Sc., M.Sc. and
    Ph.D. degrees in computer science from the Universitat Politècnica de València
    (UPV) in 2010, 2012 and 2016, respectively. In 2011, she joined the Grid and High
    Performance Computing research group as a graduate under a collaboration fellowship
    while she worked on her Master’s Thesis. She obtained her Master’s Degree in February
    2012, in which she specialized in grid and cloud computing. In November 2016,
    she obtained the Ph.D. degree whose work was focused in the field of virtual elastic
    clusters over hybrid cloud infrastructures. Currently, her research interests
    are focused in Virtualization, Cloud Computing and Scientific Computing. View
    Abstract © 2018 Elsevier B.V. All rights reserved. Recommended articles Container-based
    virtual elastic clusters Journal of Systems and Software, Volume 127, 2017, pp.
    1-11 Carlos de Alfonso, …, Germán Moltó View PDF A performance modeling framework
    for lambda architecture based applications Future Generation Computer Systems,
    Volume 86, 2018, pp. 1032-1041 M. Gribaudo, …, M. Kiran View PDF Automation and
    consistency analysis of test cases written in natural language: An industrial
    context Science of Computer Programming, Volume 189, 2020, Article 102377 Filipe
    Arruda, …, Augusto Sampaio View PDF Show 3 more articles Article Metrics Citations
    Citation Indexes: 79 Captures Readers: 251 View details About ScienceDirect Remote
    access Shopping cart Advertise Contact and support Terms and conditions Privacy
    policy Cookies are used by this site. Cookie settings | Your Privacy Choices All
    content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Future generation computer systems
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Serverless computing for container-based architectures
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ccaa.2017.8229914
  analysis: '>'
  authors:
  - V.P. Singh
  - Sateesh K. Peddoju
  citation_count: 56
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Loading [MathJax]/extensions/MathMenu.js
    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2017 International Conference...
    Container-based microservice architecture for cloud applications Publisher: IEEE
    Cite This PDF Vindeep Singh; Sateesh K Peddoju All Authors 53 Cites in Papers
    2073 Full Text Views Abstract Authors Citations Keywords Metrics Abstract: Cloud
    Environment allows enterprises to scale their application on demand. Microservice
    design is a new paradigm for cloud application development which is gaining popularity
    due to its granular approach and loosely coupled services unlike monolithic design
    with single code base. Applications developed using microservice design results
    in better scaling and gives extended flexibility to the developers with minimum
    cost. In this paper, first, different challenges in deployment and continuous
    integration of microservices are analyzed. To overcome these challenges, later,
    an automated system is proposed and designed which helps in deployment and continuous
    integration of microservices. Containers are recently heavily used in deploying
    the applications as they are easy to manage and lightweight when compared to traditional
    Virtual Machines (VMs). We have deployed the proposed microservices architecture
    on the docker containers and tested using a social networking application as case
    study. Finally, the results are presented and the performance of monolithic and
    microservice approach is compared using various parameters such as response time,
    throughput, deployment time etc. Results show that application developed using
    microservice approach and deployed using the proposed design reduce the time and
    effort for deployment and continuous integration of the application. Results also
    shows that microservice based application outperform monolithic design because
    of its low response time and high throughput. Published in: 2017 International
    Conference on Computing, Communication and Automation (ICCCA) Date of Conference:
    05-06 May 2017 Date Added to IEEE Xplore: 21 December 2017 ISBN Information: DOI:
    10.1109/CCAA.2017.8229914 Publisher: IEEE Conference Location: Greater Noida,
    India Authors Citations Keywords Metrics More Like This Lightweight Virtualization
    Approaches for Software-Defined Systems and Cloud Computing: An Evaluation of
    Unikernels and Containers 2019 Sixth International Conference on Software Defined
    Systems (SDS) Published: 2019 Green cloud computing: A review on efficiency of
    data centres and virtualization of servers 2017 International Conference on Computing,
    Communication and Automation (ICCCA) Published: 2017 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Container-based microservice architecture for cloud applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/sose.2018.00011
  analysis: '>'
  authors:
  - Tetiana Yarygina
  - Anya Helene Bagge
  citation_count: 58
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2018 IEEE Symposium on Servic... Overcoming Security
    Challenges in Microservice Architectures Publisher: IEEE Cite This PDF Tetiana
    Yarygina; Anya Helene Bagge All Authors 45 Cites in Papers 5514 Full Text Views
    Abstract Document Sections I. Introduction II. What Microservices Really Are III.
    Layered Security for Microservices IV. Security Implications of Microservice Design
    V. Emerging Security Practices Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: The microservice architectural style is an emerging
    trend in software engineering that allows building highly scalable and flexible
    systems. However, current state of the art provides only limited insight into
    the particular security concerns of microservice system. With this paper, we seek
    to unravel some of the mysteries surrounding microservice security by: providing
    a taxonomy of microservices security; assessing the security implications of the
    microservice architecture; and surveying related contemporary solutions, among
    others Docker Swarm and Netflix security decisions. We offer two important insights.
    On one hand, microservice security is a multi-faceted problem that requires a
    layered security solution that is not available out of the box at the moment.
    On the other hand, if these security challenges are solved, microservice architectures
    can improve security; their inherent properties of loose coupling, isolation,
    diversity, and fail fast all contribute to the increased robustness of a system.
    To address the lack of security guidelines this paper describes the design and
    implementation of a simple security framework for microservices that can be leveraged
    by practitioners. Proof-of-concept evaluation results show that the performance
    overhead of the security mechanisms is around 11%. Published in: 2018 IEEE Symposium
    on Service-Oriented System Engineering (SOSE) Date of Conference: 26-29 March
    2018 Date Added to IEEE Xplore: 17 May 2018 ISBN Information: DOI: 10.1109/SOSE.2018.00011
    Publisher: IEEE Conference Location: Bamberg, Germany SECTION I. Introduction
    Microservices is an architectural style inspired by Service-Oriented Architecture
    in combination with the old Unix principle of “do one thing and do it well”. Microservices
    are intended to be lightweight, flexible and easy to get started with, fitting
    in with modern software engineering trends such as Agile development, Domain Driven
    Design (DDD), cloud, containerization, and virtualization. The most frequently
    listed benefits of microservice architecture [1], [2] are organizational alignment,
    faster and more frequent releases of software, independent scaling of components,
    and overall faster technology adoption. The disadvantages include the need for
    making multiple design choices, the difficulty of testing and monitoring’ and
    operational overhead when compared to typical non-distributed solutions. The modern
    use of the term microservice dates back to 2011 [3], but the community has so
    far not reached a full consensus on a formal definition of the style. The fundamental
    basis of microservices brings together design principles from distributed systems
    and services, and from classic programming principles of abstraction, modularity,
    separation of concerns and component-oriented design. Although the underlying
    principles are widely explored in the academic literature, research on microservices
    themselves is lagging behind the rapid adoption and development in the software
    industry. Understanding the distinctiveness of microservices is crucial. In particular,
    microservices bring new security challenges, and opportunities, that were not
    present in traditional monolithic applications. These challenges include establishing
    trust between individual microservices and distributed secret management; concerns
    that are of much less interest in traditional web services, or in highly modular
    software that only runs locally. Effective microservice security solutions need
    to be scalable, lightweight and easy to automate, in order to fit in with the
    overall approach and be adopted by industry users. For instance, manual security
    provisioning of hundreds or thousands of service instances is infeasible. As services
    are migrated from offline applications and monolithic web services to a microservice-style
    architecture, code that was never designed to be accessible from outside is now
    exposed through Web APIs, raising multiple major security concerns. The past years
    have seen rapid adoption of microservices in the industry, and yet there has been
    surprisingly little focus on security. There is a growing body of literature [4],
    [5] that recognizes the need for microservice security evaluation. Security is
    one of the greatest challenges when building realworld systems, hence there is
    an urgent need to address the security concerns in microservice architecture.
    Researchers have not treated microservice security in much detail; Fetzer [6]
    discusses how microservices can be used to build critical systems if the execution
    is warranted by secure containers and compiler extensions. Otterstad & Yarygina
    [7] suggests a combination of isolatable microservices and software diversity
    as a mitigation technique against low-level exploitation. Sun [8] explored the
    use of an Intrusion Detection System (IDS) for fine-grained virtual network monitoring
    of a cloud infrastructure based on Software Defined Network (SDN). While being
    presented as a solution for microservices, in reality it does not exploit any
    microservice-specific features. Although the studies by Fetzer [6] and Otterstad
    & Yary-gina [7] highlight the isolation benefits of microservice design, a systematic
    understanding of how the architecture affects security is still lacking. Moreover,
    microservice security is an overloaded term that requires proper clarification,
    and there is no overview of emerging industry security practices either. This
    paper identifies and unravels some of the mysteries surrounding microservice security,
    and is the first study to undertake a holistic approach to microservice security.
    In summary, we make five contributions to the understanding of microservice security:
    putting microservices and their security in the bigger context of SOA and distributed
    systems (Section II); decomposing the notion of microservice security into smaller
    and more familiar components, with a formalized attack model (Section III); analyzing
    the security implications of microservice design (Section IV); identifying several
    prominent microservice security trends in industry (Section V); presenting an
    open source prototype security framework for microservices (Section VI). SECTION
    II. What Microservices Really Are Before we proceed with the security challenges
    for microservices, it is important to understand how microservices relate to other
    software architectural styles as well as to agree on the definitions. A. Defining
    Microservices Numerous terms are used to describe microservices, the most common
    microservice definitions are presented below: Newman [1] defines microservices
    as small autonomous services build around the following principles: model [services]
    around business concepts, adopt a culture of automation, hide internal implementation
    details, decentralize all things, isolate failure, and make services independently
    deployable and highly observable. Lewis and Fowler [3] view microservices as “an
    approach to developing a single application as a suite of small services, each
    running in its own process and communicating with lightweight mechanisms, often
    an HTTP resource API. These services are built around business capabilities and
    are independently deployable by fully automated deployment machinery. There is
    a bare minimum of centralized management of these services, which may be written
    in different programming languages and use different data storage technologies.”
    The term has also been used in unrelated ways; for example, Kim [9] uses the term
    “microservice” when referring to the basic security primitives in context of formal
    methods: authentication microservice, integrity microservice, among others. This
    definition should not be confused with the modern definition of microservice architecture.
    B. Defining Service-Oriented Architecture (SOA) SOA can be seen as a predecessor
    to microservices; Josuttis [10] gives the following definition: “SOA is an architectural
    paradigm for dealing with business processes distributed over a large landscape
    of existing and new heterogeneous systems that are under the control of different
    owners”. Fig. 1. Microservice architecture in perspective. Microservices are an
    implementation approach to SOA. SOA is a subclass of distributed systems. Show
    All The core technical concepts of SOA are: Services. Service in the context of
    SOA is “an IT realization of some self-contained business functionality” [10].
    Based on multiple existing definitions of a service, additional situation-dependent
    attributes that services may have are: self-contained, coarse-grained, visi-ble/discoverable,
    stateless, idempotent, reusable, composable, vendor-diverse. Interoperability.
    SOA interoperability in most cases is achieved via the Enterprise Service Bus
    that enables service consumers to call the service providers. Loose coupling.
    Loose coupling is needed to fulfill the goals of flexibility, scalability, and
    fault tolerance. Loose coupling is a principle that aims to minimize dependencies
    so the change in one service does not require a change of another service. An
    important aspect of SOA, as stated in the SOA Manifesto, is that “SOA can be realized
    through a variety of technologies and standards”. C. Déjà Vu Majority of opinions
    on microservices fall into one of the two following categories: (1) microservices
    are a separate architectural style [3], [11]; (2) microservices are SOA [1], [12].
    Some sources [4], [13] take a middle ground by viewing microservices as a refined
    SOA. The frequently mentioned differences are the degree of centralization, size
    of services, decomposition based on bounded context, and level of independence.
    However, these criteria are too vague to be used in a rigorous scientific comparison.
    They are neither easily quantifiable-it is unclear how the service size should
    be measured, nor sufficient to serve as style constraints. Based on the above-listed
    definitions of SOA and microservices it is clear that these two approaches are
    similar. The definitions of SOA services and microservices are almost identical.
    This conclusion is important because it means that the security issues faced by
    practitioners who were migrating to SOA are now relevant for practitioners migrating
    to microservices. Figure 1 depicts this relation. Zimmermann [12] is the first
    to systematically compare microservices and SOA, by surveying the authoritative
    technical blog posts and online articles on microservice principles. After contrasting
    the distilled microservice principles and SOA characteristics, he concludes that
    “the differences between microservices and previous attempts to service-oriented
    computing do not concern the architectural style as such (i.e., its design intent/constraints
    and its platform-independent principles and patterns), but its concrete realization
    (e.g., development/deployment paradigms and technologies)” [12]. Hence, we may
    expect many of the concerns and solutions that apply to SOA to also be applicable
    to microservices. D. Distributed Systems When talking about microservices many
    sources repeat the same fallacy of contrasting microservices to monolithic applications.
    The real situation is not as binary. In practice, a “monolithic” application may
    be highly modular internally, being built from a large number of components and
    libraries that may have been supplied by different vendors, and some components
    (such as a database) may be also distributed across the network. The issues of
    decomposition, concerns separation, and designing and specifying APIs will be
    similar regardless of whether API calls are made locally or across the network.
    The essence of microservices is that they are (or compose to form) highly modular,
    distributed systems, reusable through a network-exposed API. This implies that
    microservices inherit advantages and disadvantages of both distributed systems
    and web services. While distributed systems bring multiple highly desirable benefits
    such as scalability on demand and embrace of heterogeneity, these systems also
    come with drawbacks. Distributed systems are more challenging to develop in the
    first place. Moreover, monitoring, testing, and debugging such systems is more
    difficult than for non-distributed systems. Other well-known problems include
    maintaining data consistency and replication among nodes, node naming and discovery
    due to constantly changing network topology, and dealing with unreliable networks.
    Important distinguishing characteristics of distributed systems over monolithic
    systems can be summarized as follows (adapted from Tanenbaum [14]): 1) The overall
    system state is unknown to individual nodes; 2) Individual nodes make decisions
    based on the locally available information; 3) Failure of one node should not
    affect other nodes. The microservice style enforces these requirements. E. Microservices
    in Context of Other Technologies A technology closely related to microservice
    philosophy is unikernels [15]. Unikernels are fixed-purpose OS kernels which run
    directly on a hypervisor. A full system would consist of multiple unikernels performing
    different tasks in a distributed fashion. The microservice design principles of
    small independent loosely coupled single-purpose components fit well into the
    unikernels world. Unikernels promise to provide high security and strong isolation
    of virtual machines while still being lightweight like containers. Some programming
    languages, such as Erlang and its successor Elixir, support the distributed computing
    model by design. Erlang is an actor-based programming language. In the Erlang
    world “everything is a process” and the only process interaction method is through
    message passing. Erlang language features encapsulation/process isolation, concurrency,
    fault detection, location transparency, and dynamic code upgrade [16], p.29. Erlang
    has advanced monitoring tools, and it is used for several high-load systems such
    as the messaging service WhatsApp. Systems built with Erlang are inherently microservice-like.
    As the aforementioned examples show, the core microservice principles can be rediscovered
    in many technologies. F. Summary: Essence of Microservices Compared to other software
    and service architectures, these are the distinctive characteristics of microservices:
    Distributed composition: Microservices build on other microservices and communicate
    across the network. (Compared to monolithic services.) Modularity: Microservices
    will tend to offer finer-grained APIs that lend themselves to flexible reuse.
    (Compared to big, single-purpose APIs/applications.) Encapsulation: Services are
    to a large degree encapsu-lated and isolated from others, and may even be written
    in different programming languages. (Compared to libraries and object-oriented
    encapsulation.) Network service: Services are network-accessible, and reuse happens
    by contacting the service rather than by installing and linking to a library.
    (Compared to a classic modular design.) SECTION III. Layered Security for Microservices
    Building secure systems is hard. The classic security objectives are data confidentiality
    and integrity, entity and message authentication, authorization, and system availability.
    Naturally, these abstract objectives can be realized in many different ways in
    practice. How these objectives are met and to what degree are the system-specific
    architectural decisions that depend on the particular threat model, budget, and
    expertise. An important issue is where in the system to place security mechanisms.
    We argue that microservice security is a multifaceted problem and it relies heavily
    on underlying technologies and the environment. To get to the bottom of it, we
    need to decompose microservice security into its components. A. Taxonomy of Microservice
    Security To illustrate the underlying complexity of the problem, we divided microservice
    security concerns into six categories or layers as shown in Table I: hardware,
    virtualization, cloud, communication, service, and orchestration. We do not claim
    the proposed layering is complete, but we do claim that it provides a good overview
    of the topic. Table I Proposed hierarchical decomposition of microservice security
    issues into layers This decomposition is based on the basic computer science principles
    and common sense. While it is inspired by the OSI model and its seven layers of
    communication, the proposed hierarchical decomposition also incorporates new trends
    in software engineering such as virtualization and orchestration. This decomposition
    is applicable to any modern distributed system and is not specific to microservices
    or SOA. However, it is crucial for the discussion of microservice security. The
    decomposition shows that multiple security choices should be made on each level.
    There are many places where security components reside, and, more importantly,
    where security can fail. The system should not be treated as a black box. Some
    levels bleed into each other, e.g. virtualization is a main enabling technology
    for cloud computing. The separation of security concerns is not always strict.
    The two bottom-most layers, hardware and virtualization, are at least partially
    accessible to an attacker with shell access on the host or virtual machines/containers
    correspondingly. A malicious hardware manufacturer that provides hardware with
    backdoors is a threat. On the Cloud level, a cloud vendor itself is a threat.
    Other tenants are also a potential threat in the cloud using various side-channel
    attacks, such as the FLUSH+RELOAD technique [28], or Meltdown and Spectre [29].
    For communication and orchestration levels, a network attacker inside the perimeter
    who can eavesdrop and manipulate traffic is a major concern. For the service/application
    level, the threat of an external attacker should be considered. The price of addressing
    the threats on different levels varies. For example, hardware concerns are extremely
    difficult to address, if at all practically possible. Most developers would be
    concerned with service/application and communication layers because addressing
    the security concerns on these levels is cost-feasible for them. B. Attack Model:
    Redefining Perimeter Security Until recently perimeter defense was the most common
    approach to security of microservice-based systems [1][p.173]. From the modern
    security perspective, perimeter security is in general considered insufficient-we
    should rather assume that the other services in the system may be compromised
    and hostile (“trust no one”). The rise of microservices, as well as advances in
    security automation, facilitate placement of additional security mechanisms inside
    the perimeter (see Figure 2). In other words, defense in depth as a concept of
    placing multiple and sometimes overlapping security mechanisms on different levels
    throughout the system becomes more feasible with microservices. We assume an adversary
    is able to compromise at least one service inside the perimeter and wants to move
    laterally through the system to gain full control. If internal services blindly
    trust whoever is calling them, then a single compromised microservice will allow
    an attacker to manipulate all the other nodes in the microservice network, for
    example by issuing arbitrary malicious requests that the nodes will fulfill. The
    latter is sometimes referred to as a confused deputy problem [1]. The adversary
    can attempt to eavesdrop on the inter-service communication, insert and modify
    data in transit. We also make a standard cryptographic assumption that the adversary
    is computationally bound. Fig. 2. Microservices redefine the notion of perimeter
    security and move towards defense in depth. The figure excludes infrastructure
    services, such as API gateway or monitoring service Show All Security is a trade-off
    between minimizing the budget and covering more attack vectors. For practical
    reasons, we assume the hardware and cloud providers are trusted, as well as the
    way microservices are deployed provides high degree of isolation. We believe that
    the given model is the most realistic approximation of the real world and reaches
    the limits of what software developers and security engineers will be willing
    to accept in practice. The following discussion is centered around this model.
    SECTION IV. Security Implications of Microservice Design Several important security
    properties emerge as side effects of microservice design. Herein, we describe
    and evaluate the properties distilled from conducting a careful literature review
    on the subject [1]–[4], [11]–[13], [27], as well as discussions with practitioners
    and personal experience. We also provide an interpretation of the implications.
    a): Do One Thing and do it WellThe properties of context boundary, design around
    business concepts, and Domain Driven Design (DDD) usually results in a smaller
    codebase per microservice. If we attempt to measure microservices size in lines
    of code (LOC), then the more LOC there are, the more bugs the service has. LOC
    and bugs are statistically correlated. More bugs in general means more exploitable
    bugs in particular. Less complex code is easier to maintain. A smaller codebase
    of individual microservices results in a smaller attack surface given the other
    conditions, such as the overall code quality, remain the same. This statement
    can be further supported by reduced cognitive complexity of the code that facilitates
    better code comprehension for individual developers. This is not necessarily true
    for system architects who still need to maintain a more global view of the system.
    b): Automated, Immutable DeploymentServices should be immutable: to introduce
    permanent changes to microservices, services should be rebuilt and redeployed.
    Microservices immutability improves overall system security since malicious changes
    introduced by an attacker to a specific microservice instance are unlikely to
    persist past redeployment. Automation should be leveraged in maintaining the security
    infrastructure. Immutability aids the security of microservices similarly to how
    immutability promotes correctness in programming languages [30]. c): Isolation
    Through Loose CouplingBoth SOA and microservices are built around the concept
    of loose coupling. Microservices take the concept even further by emphasizing
    the share nothing principle and strict data owning. This implies that each service
    can be isolated: only able to access the information it needs, and only able to
    access the particular services it needs. This limits the damage should an individual
    service be compromised. Better isolation as an inherent security benefit of microservice
    design has been discussed by Otterstad & Yarygina [7]. Fetzer argues [6] that
    microservices can be used to build critical systems where integrity, confidentiality,
    and correct execution inside microservices is warranted by secure containers and
    compiler extensions. Such secure containers were implemented as Docker containers
    using an Intel Software Guard Extensions (SGX) enclave that can protect the microservices
    running inside such secure containers from OS, hypervisor, and cloud provider
    level attacks-with a degree of isolation for each component that is much greater
    than what is achieved in typical monolithic applications. d): Diversity Through
    System HeterogeneityDistributed systems are often heterogeneous systems. Microservice
    architecture embraces this fact by allowing the individual components to be written
    in any programming language and/or technology given that they retain same interfaces
    (service contracts). This results in natural diversity of components in the microservice
    architecture. Otterstad & Yarygina [7] suggested diversification of microservices
    as a mitigation technique against low-level exploitation. Although system diversity
    as a security inducing property is not new, the application of it in microservice
    setting is. Diversity in computer systems can be achieved in many ways including
    the use of different programming languages, compilers, OS/basic images. Microservice
    design philosophy readily allows for such approaches to be taken. Even coexistence
    of older and newer versions of the same microservice adds to the heterogeneity
    of the system. Otterstad & Yarygina [7] has also suggested use of N-version programming
    to improve microservice security. e): Fail FastAlthough fault tolerance does not
    directly translate to security, we believe it contributes enough to be listed.
    If the main point is to disrupt the service, such as in case of Denial of Service
    (DoS) attacks, fault tolerance does directly translate to security. In contrast
    to monolithic systems where a failure is often total, distributed systems can
    be characterized with partial failures where only some of the nodes fail. A microservice
    network should tolerate the presence of partial failures and limit their propagation.
    The Circuit breaker pattern [26] prevents cascading failures and increases overall
    system resilience by adjusting the node behavior if the network interactions with
    its peers fail partially or completely. Following the fail fast principle will
    decrease the likelihood of attacks succeeding and minimize the possible damage.
    Fundamentals of fault tolerance in distributed systems can be found in a book
    by Tanenbaum [14]. SECTION V. Emerging Security Practices Although there are currently
    few industry practices for microservice security, some interesting trends present
    themselves. The first one is the use of Mutual Transport Layer Security (MTLS)
    with a self-hosted Public Key Infrastructure (PKI) as a method to protect all
    internal service-to-service communication. The second trend is use of tokens and
    local authentication. Both approaches are leaning towards the defense in depth
    approach to security and further support our attack model presented in Section
    III-B. A. Mutual Authentication of Services Using MTLS While sharing the same
    underlying concept, two different solutions for establishing trust between microservices
    were developed simultaneously by Docker and Netflix. 1): Docker Swarm CaseDocker
    Swarm is a container orchestration solution and clustering system for Docker that
    allows building distributed systems. Docker Swarm is particularly interesting
    because a) it is a popular platform for implementing microservices; b) it has
    a variety of built-in security features. MTLS is used by all the nodes in a swarm
    to authenticate each other, encrypt all the network traffic, and differentiate
    between worker and manager nodes [31], [32]. Docker Swarm automatically deploys
    its own PKI to provide identity to all the nodes. The first manager node generates
    a new self-signed root Certificate Authority (CA) along with a key pair. A hash
    of the root CA and a randomly generated secret are sealed into a token that is
    provided to all other nodes during (re)deployment. To join a swarm, a node verifies
    the identity of the remote manager based on the token, generates a preliminary
    certificate data and sends a certificate signing request (CSR) to the manager
    together with the token. After verifying the secret from the token, the manager
    issues a certificate to the node. When a node needs to connect to another node,
    both nodes will authenticate to each other and set up a TLS tunnel, using the
    certificates issued to them by the CA. See Figure 3. The default behavior for
    the nodes is to automatically renew their certificate every three months, but
    shorter intervals can also be used. The update does not happen simultaneously
    for all the nodes but instead takes place within given timeframe due to security
    reasons. Docker Swarm also supports rotation of the CA certificate and embedding
    into already existing PKI. Another popular container orchestration solution, Kubernetes,
    does not support MTLS with automated certificate provisioning at the moment, but
    the work on Kubelet TLS Bootstrap feature is ongoing. Fig. 3. A generic solution
    for microservice trust based on MTLS. 1) CA service generates a self-signed root
    certificate and a shared secret. 2) A cryptographic hash of the certificate and
    a shared secret are extracted from the CA service and provisioned into the new
    benign service either automatically or manually. 3) A benign service establishes
    a one-way TLS connection with the CA service, verifies CA identity using the provisioned
    hash of the CA certificate, and if successful submits a CSR and shared secret.
    4) upon successful verification of the received CSR and shared secret, CA service
    signs and sends back the newly issued certificate. 5) two benign services communicate
    over MTLS. Show All 2): Netflix CaseNetflix internal microservice network utilizes
    a PKI based on short-lived certificates for TLS with mutual authentication [33].
    The Netflix approach builds on the notions of short- and long-lived credentials.
    The long-lived credentials are provisioned into the service during a bootstrap
    procedure, stored either in Trusted Platform Module (TPM) or SGX, and are required
    to obtain and update short-lived credentials. While the Docker source code is
    publicly available. the Netflix PKI solution is not. The idea of issuing certificates
    with a short lifetime as a solution to the certificate revocation problem on the
    Web has been suggested before [34]. The short expiration time of certificates
    limits the utility for revocation mechanisms. TLS with mutual authentication addresses
    problems of service authentication and traffic encryption, but not service authorization.
    Moreover, user to service authentication and authorization are still left to the
    discretion of developers. B. Principal Propagation via Security Tokens After a
    user has been authenticated by the gateway, the microservices behind it will be
    processing user''s requests. Microservices should be aware of the user authentication
    state, i.e. whether the user was authenticated, and what the user''s role is in
    authorization context. The user needs to be identified multiple times in each
    service down the operation chain, as each service calls other services on the
    users behalf. 1): Security Tokens and Relevant StandardsToken-based authentication
    is a well known commonplace security mechanism that relies on cryptographic objects
    called security tokens containing authentication or authorization information.
    A security token is created on the server side upon the successful validation
    of the clients credentials and given to the client for subsequent use. Security
    tokens substitute the client''s credentials within limited time-frame. Token-based
    authentication via HTTP cookies is a prominent example. Fu et al. [35] gives a
    detailed security evaluation of the approach. Token-based authentication advanced
    even further with the widespread adoption of OpenID Connect [36], a security standard
    for single sign-on and identity provisioning on the Web. The same functionality
    is provided by the Security Token Service (STS). STS is a component of the WS-Trust
    standard [37] that extends WS-Security standard with methods for issuing, renewing,
    and validating security tokens. Other relevant standards are JSON security standards:
    JSON Web Signature (JWS), Encryption (JWE), and Token (JWT). 2): Reverse Security
    Token ServiceA noteworthy trend in the industry is the use or JWT tor principal
    propagation within the microservice network. Multiple informal records of the
    approach can be found [38]. Although no formal description of it exists in scientific
    literature, those familiar with the above-listed security standards will find
    the suggested approach closely related to the existing standards. Token-based
    user-to-service authentication where each service understands tokens allows transporting
    user identity and user session state through the system in a secure and decentralized
    manner. After the user is authenticated with the authentication service, a security
    token representing the user will be generated for internal use within the microservice
    network. In this paper, we refer to a separate service that is responsible for
    the token generation as a Reverse STS. Limited lifetime of the security token
    is achieved by including an expiration time in its body. For security reasons,
    a shorter token lifetime is desired. Information about the user and intended audience
    can be included if needed. The token will be passed to the microservices involved
    in processing the request. Before executing the received request each microservice
    will validate the adjacent token using a corresponding public key. Token validation
    is a mandatory first step of the request processing. See Figure 4. This approach
    fits well with the second design principle of distributed systems: individual
    nodes make decisions based on locally available information (see Section II-D).
    It facilitates loose coupling of the services and is highly scalable while having
    no overhead of a centralized solution. Moreover, there is good tool support for
    this approach. OAuth 2.0, a standard for delegated authorization, and OpenID Connect,
    an authentication layer on top of OAuth 2.0, can be tailored for inter-service
    security. There are three caveats with this approach. The first is an assumption
    that the clock synchronization problem is nonexistent. To validate tokens, both
    the token issuing node and the node performing validation must have their clocks
    synchronized. This is usually straightforward to handle, e.g., using NTP. The
    second one is that the tokens must be sent over a protected channel, i.e. TLS,
    otherwise the tokens can be intercepted and re-used within their validity period.
    Having short validity time is a partial solution to this problem. The third one
    is that the private key of the token issuing service must be kept safe at all
    times. If the private key is compromised, any user can be impersonated by the
    attacker. C. Fine-Grained Authorization Although no prominent trends for microservice
    authorization exist in industry at the moment, we will mention several promising
    approaches. Fig. 4. A generic token-based authentication scheme for microservices
    that enables a user-to-service authentication and identity propagation based on
    cryptographic tokens. 1) incoming user request hits API gateway. 2) API-gateway
    prompts user for authentication by redirecting to the dedicated user authentication
    service. 3) requesting a security token. If user is authenticated successfully,
    the authentication token service generates a token that represents the given user
    inside the system. 4) returning a security token. The token is passed alongside
    the user request to the downstream services. This token is designated for internal
    use only and is not given to the user. Show All 1): Security Tokens for User AuthorizationVarious
    access control mechanisms exist including Role Based Access Control (RBAC) and
    Attribute Based Access Control (ABAC). RBAC and its predecessors are user-centric
    access control models. Therefore, they do not account for relationship between
    the requesting entity and the resource. For fine-grained authorization on resources,
    such as access to a specific API call, ABAC should be used. Security tokens can
    include authorization information. For example, RBAC authorization roles can be
    incorporated into JWT tokens as an additional attribute. 2): Inter-Service Authorization
    Based on CertificatesUse of digital certificates for authorization rather than
    authentication was first suggested in 1999, e.g. Simple Distributed Security Infrastructure
    (SDSI) [39] and Simple Public Key Infrastructure (SPKI) [40]. Later, Marcon [41]
    proposed a certificate-based permission management system for SOA. Not all microservices
    are deployed equal: only microservices co-dependent by design should be able to
    call each other. If the microservice network already relies on MTLS and self-hosted
    PKI, the same PKI can provide the basic service to service authorization. A separate
    signing certificate should be created per microservice type. This certificate
    will be used to sign the certificates of all instances of the same type. Let''s
    assume there are three types of microservices: A, B, and C. There are multiple
    instances of each type. While B is connected to both A and C, no direct connection
    is allowed between A and C. The default rule is to trust no one. To allow access
    to B from A and C, all instances of B should be preconfigured to trust the certificates
    signed by certificate type A and C. To allow access to A and C from B, the certificate
    for type B should be added to A''s and C''s trust lists. SECTION VI. Microservice
    Security Framework As shown, no standard way to deal with microservice security
    concerns exists. The existing implementations are often closed source (Netfiix
    MTLS), not directly portable to other environments (Docker Swarm MTLS), and in
    general not well documented or understood. Moreover, a performance cost of using
    the existing security solutions in a microservice setting is unknown. To partially
    address this problem we implemented a microservice security framework, MiSSFire
    that provides a standard way to embed security mechanisms into microservices.
    Furthermore, we evaluated the performance of the framework against a toy microservice-based
    bank system of our design (MicroBank). A. Design and Implementation When designing
    the MiSSFire framework we tried to address the main microservice security challenge-the
    problem of establishing trust between individual microservices. The core design
    criteria were security, scalability, and automation. We followed the defense in
    depth principle and the attack model introduced in Section III-B. Security mechanisms
    that we implemented are heavily based on the emerging security practices from
    Section V, specifically mutual authentication of services using MTLS and principal
    propagation via JWT. The framework consists of a set of infrastructure services
    that need to be up and running within a system and a template for a regular functional
    service. Currently, the framework is bundled with two infrastructure services
    that expose relevant REST APIs: The CA service is a core part of the self-hosted
    PKI that enables MTLS between microservices. It generates a self-signed root certificate
    and signs CSR from other services. See Figure 3 for more details. The Reverse
    STS stays behind a user authentication service (not included) and generates security
    tokens in JWT format. A new JWT is generated per user request. See Figure 4 for
    more details. The template for a regular functional service simplifies integration
    with the infrastructure services by providing all the necessary functionality.
    Additionally, it forces use of MTLS for all connections and requires presence
    of JWT for all incoming requests. B. Experiment Although performance is not a
    security property, it has been an important deciding factor for adoption or rejection
    of security mechanisms in the real world. In this section we address the question
    of how the common security mechanisms that are bundled in our framework impact
    the performance of an actual microservice-based system. 1): MicroBankTo test the
    framework we needed an actual microservice application. For this purpose we developed
    our own fictitious microservice-based bank system that consists of the following
    microservices: API gateway: The main entry point to the system. Accounts: Manages
    user accounts. Fig. 5. Experiment setup: Payment operation Show All Payment: Provides
    payment functionality. Transactions: Handles transaction operations. Users: Manages
    users. The system was built using mainstream development techniques for microservices.
    The system is written in the Python v2.7 programming language. Although the microservice
    style does not dictate what communication protocols or styles should be used,
    in practice REST APIs and JSON format are the default choices. Each service in
    the system exposes a set of relevant REST APIs. This is done by using the web
    framework Flask and WSGI HTTP server Gunicorn. The number of workers per server
    is adjustable. The ‘shared nothing’ architecture is a central part of the loose
    coupling concept in microservice world. Therefore, Accounts, Transactions, and
    Users microservices maintain individual SQLite databases. For the simplicity of
    the experiment there is only one instance of each microservice type. The services
    of the bank model can be run as separate processes or in Docker containers. The
    whole bank model can also be run as a multi-container Docker application using
    the compose tool (though both the framework and the sample application can work
    independently of Docker). The tool automates configuration and creation of containers
    and allows to start the whole system with a single command. The source code has
    a partial unit-test coverage. 2): MethodologyThe test client registers two users
    and opens bank accounts for them. Then, the test client carries out a series of
    payment operations between the two users. In the experiment, 50 concurrent test
    clients are started simultaneously, where each test client performs 100 sequential
    payment operations. To measure the system performance, an average execution time
    of 5000 payment requests is calculated on the client side. The payment operation
    involves four microservices as shown in Figure 5. Several factors contribute to
    the time it takes to perform one payment operation. These factors are network
    delays and processing time inside microservices including database access time.
    In the given setup, payment operations always succeed. If the operation is invalid,
    such as an attempt of transferring a negative amount, or if the recipient bank
    account is closed in the middle of the payment operation, the implemented system
    will roll back to revert the partially made changes. This would adversely affect
    the payment operation execution time. Fig. 6. Performance of the bank model under
    load of 50 test clients making payments Show All The experiment consists of four
    parts: Baseline. Running the test client against the bank model with security
    features disabled. Tokens. Running the test client against the bank model with
    the Reverse STS service in place and JWT tokens validation. MTLS. Running the
    test client against the bank model with the CA service in place and all communication
    secured with MTLS. Tokens+M''TLS. Running the test client against the bank model
    with all security features enabled. The experiment was run on a MacBook Pro with
    a 2,6 GHz Intel Core i7 processor and 16 GiB of memory. The computing resources
    dedicated to Docker were 8 CPUs and 8 GiB of memory. Both the test clients and
    the bank model resided on the same machine, all communication was performed via
    localhost. 3): Results and DiscussionFigure 6 presents the results of our experiment.
    As expected the performance in the baseline case is higher than in the cases of
    tokens and MTLS. Closer inspection of the data shows that the tokens decrease
    performance by 7% on average, while the MTLS impact is around 4%. The most interesting
    aspect of this figure is that the difference between the baseline case and the
    case with all security mechanisms in place is relatively small, and accounts for
    around 11% in the given setup. Based on the fact that microservice solutions are
    slow in general, we believe this security overhead is still acceptable, especially
    for security critical applications. There is a relatively high cost to setting
    up HTTPS connections and validating tokens, which we reduce by pooling and reusing
    existing connections between services when possible. Ueda et.al. [42] showed that
    the performance of the microservice version can be around 80% lower than the monolithic
    version on the same hardware configuration. This is a significant overhead that
    industry is willing to take and in many cases has already embraced. This also
    means that the overhead of having a microservice-based solution in first place
    is so high, that the impact of security mechanisms becomes insignificant in comnarison.
    From the chart, it can be seen that the best throughput is achieved with 3 workers
    (except for the MTLS case). When more workers are added the throughput starts
    to deteriorate slowly. These results deviate from an expected behavior where the
    throughput scales linearly up to the number of cores available, assuming the parallelizable
    portion of the program completely dominates the execution time. This may be related
    to the clients sharing the same CPU; though it may also be due to specifics of
    Gunicorn. The relatively slow performance of our bank model can be attributed
    to the following: 1) Synchronous HTTP communication: sequential requests and no
    parallelization; 2) Slow database access. 3) Python is not as efficient as certain
    other programming languages; 4) Suboptimal configuration of the web servers. Also,
    the bank model is only a proof-of-concept and offers limited performance and scalability.
    C. Evaluation 1): Security ConsiderationsOur framework relies exclusively on well-known
    security mechanisms and standards such as SSL and JWT. No new cryptographic primitives
    or protocols are introduced. Although implementation flaws are always possible,
    we need to rely on security and trustworthiness of the building blocks. The CA
    service and the Reverse STS are security-critical. It makes sense to run these
    two services in a hardened environment. SGX-capable servers in the public cloud
    offered by Microsoft Azure [43] is one of possible solutions. 2): Performance
    EvaluationOur experiments show that a microservice network introduces latencies
    on the order of milliseconds; in this setting the performance hit of basic security
    features becomes negligible. 3): Framework LimitationsCurrently, the framework
    lacks multiple important security features. It is not a production-ready tool,
    and should mostly be seen as a proof of concept. The framework is overly simplified:
    it does not address authorization, there is no key rotation or key revocation
    mechanisms. Also, it is currently limited to Python. 4): Reproducibility and Future
    WorkTo support reproducible research and allow others to improve on our results
    we released our code open source under GNU GPLv3 license. The source code of both
    MiSSFire framework and the MicroBank are publicly available at GitHub (https://github.com/yarygina/
    MiSSFire and MicroBank), including setup and benchmarking scripts. In future investigations,
    we intend to improve on the aforementioned limitations of our framework. SECTION
    VII. Conclusion In this paper, we have examined the micro service architectural
    style, with a particular focus on its security implications. Microservices bring
    together concepts from both service-orientation, distributed systems and fundamental
    software engineering principles of abstraction, reuse and separation of concerns.
    This combination brings both new challenges that must be addressed, as well as
    old security challenges in a new wrapping. The microservice style of highly isolated,
    easily redeployable distributed components also implies new opportunities for
    better security, e.g., through increased diversity or restricting data access
    to only the services that need it. As a concrete example of microservice-specific
    security, we have developed a small, openly available prototype framework for
    establishing trust and securing microservice communication with MTLS, self-hosted
    PKI and security tokens. Our case study shows that there is little extra performance
    cost to securing microservice communication, likely due to the overall high overhead
    of the communication itself. With increased industry adoption of microservices,
    and the overall increasing threat level on the Internet, researching and developing
    secure microservices is crucial; and, with microservices seen as a lightweight,
    easy-to-use approach to SOA, we believe it is particularly important that security
    solutions are also lightweight, easy to use, and accessible to real-world developers.
    Authors Figures References Citations Keywords Metrics More Like This Providing
    security to a smart grid prosumer system based on a service oriented architecture
    in an office environment 2013 IEEE PES Innovative Smart Grid Technologies Conference
    (ISGT) Published: 2013 [WiP] A Workflow and Cloud Based Service-Oriented Architecture
    for Distributed Manufacturing in Industry 4.0 Context 2018 IEEE 11th Conference
    on Service-Oriented Computing and Applications (SOCA) Published: 2018 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Overcoming Security Challenges in Microservice Architectures
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/cloud.2017.67
  analysis: '>'
  authors:
  - Yahya Al-Dhuraibi
  - Fawaz Paraïso
  - Nabil Bachir Djarallah
  - Philippe Merle
  citation_count: 99
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2017 IEEE 10th International ... Autonomic Vertical
    Elasticity of Docker Containers with ELASTICDOCKER Publisher: IEEE Cite This PDF
    Yahya Al-Dhuraibi; Fawaz Paraiso; Nabil Djarallah; Philippe Merle All Authors
    90 Cites in Papers 1 Cites in Patent 2608 Full Text Views Abstract Document Sections
    I. Introduction II. Motivation III. Background IV. Elastic Docker Approach V.
    Experiments and Evaluation Show Full Outline Authors Figures References Citations
    Keywords Metrics Footnotes Abstract: Elasticity is the key feature of cloud computing
    to scale computing resources according to application workloads timely. In the
    literature as well as in industrial products, much attention was given to the
    elasticity of virtual machines, but much less to the elasticity of containers.
    However, containers are the new trend for packaging and deploying microservices-based
    applications. Moreover, most of approaches focus on horizontal elasticity, fewer
    works address vertical elasticity. In this paper, we propose ELASTICDOCKER, the
    first system powering vertical elasticity of Docker containers autonomously. Based
    on the well-known IBM''s autonomic computing MAPE-K principles, ELASTICDOCKER
    scales up and down both CPU and memory assigned to each container according to
    the application workload. As vertical elasticity is limited to the host machine
    capacity, ELASTICDOCKER does container live migration when there is no enough
    resources on the hosting machine. Our experiments show that ELASTICDOCKER helps
    to reduce expenses for container customers, make better resource utilization for
    container providers, and improve Quality of Experience for application end-users.
    In addition, based on the observed migration performance metrics, the experiments
    reveal a high efficient live migration technique. As compared to horizontal elasticity,
    ELASTICDOCKER outperforms Kubernetes elasticity by 37.63%. Published in: 2017
    IEEE 10th International Conference on Cloud Computing (CLOUD) Date of Conference:
    25-30 June 2017 Date Added to IEEE Xplore: 11 September 2017 ISBN Information:
    Electronic ISSN: 2159-6190 DOI: 10.1109/CLOUD.2017.67 Publisher: IEEE Conference
    Location: Honololu, HI, USA SECTION I. Introduction Elasticity is one of the key
    characteristics of cloud computing, which leads to its widespread adoption. Elasticity
    is defined as the ability to adaptively and timely scale computing resources in
    order to meet varying workload demands [1], [2]. There are two types of elasticity:
    horizontal and vertical [1], [3]. Horizontal elasticity consists in adding or
    removing instances of computing resources associated to an application. Horizontal
    elasticity is also known as replication of resources. Vertical elasticity consists
    in increasing or decreasing characteristics of computing resources, such as CPU
    time, cores, memory, and network bandwidth. Vertical elasticity is also known
    as resizing of resources. Both elasticities are driven by the variation of workload
    demands, such as the request response time or the number of end-users. In the
    scientific literature but also in industry practices, most of proposed approaches
    focus on horizontal elasticity but few addresses vertical elasticity. Virtualization
    techniques are the keystone of elasticity in cloud computing and consist to virtualize
    the actual physical resources-e.g., CPU, storage, network-as virtual resources
    such as virtual machines (VMs), virtual storages, virtual networks. Numerous works
    proposed various cloud elasticity handling mechanisms for VMs [3]–[6]. However,
    with the advent of Docker [7], containers are becoming the new trend for packaging
    and deploying microservices-based applications [8]. Since Docker provides more
    flexibility, scalability, and resource efficiency than VMs [9], [10], [11] it
    becomes popular to bundle applications and their libraries in lightweight Linux
    containers and offers them to the public via the cloud. Then, Docker containers
    have gained a widespread deployment in cloud infrastructures such as in Amazon
    Ec2 Container Service, Google Container Engine, Docker Datacenter, Rackspace.
    But compared to VMs, there are only few works that deal with elasticity of Docker
    containers: [12], [13], [14] focus on automatic horizontal elasticity, [15] address
    manual vertical elasticity, [16] supports migration. To the best of our knowledge,
    there is no related work that handles vertical elasticity of containers autonomously.
    The main contribution of this paper is to present ELAS-TICDOCKER: the first system
    powering vertical elasticity of Docker containers autonomously. Based on the well-known
    IBM''s autonomic computing MAPE-K principles [17], ELAS-TICDoCKER scales up and
    down both CPU and memory assigned to each container when the application workload
    grows up and down, respectively. This approach modifies resource limits directly
    in the Linux control groups (cgroups) associated to Docker containers. Vertical
    elasticity is limited to host machine capacity as it cannot provision more resources
    when all the host machine resources are already allocated to containers. Therefore,
    in this work, we use live migration to handle this limit. Live migration is the
    process of moving a container in its executing state from source to target host.
    Container migration takes place when resizing is no longer possible on the host
    machine. ELASTICDOCKER uses Check-point/Restore In Userspace (CRIU) [18] to implement
    the concept of container live migration. The approach then evaluated by running
    experiments using Graylog1 and RUBiS [19] applications. These experiments show
    that ELASTICDOCKER helps to improve performance and Quality of Experience (QoE)
    for application end-users, reduce expenses for container customers, and make better
    resource utilization for container providers. Our experiments also show that ELASTICDOCKER
    outperforms Kubernetes autoScaling [20] by 37.63%. We have also evaluated the
    efficiency of containers live migration for different Docker images, and the observed
    migration, checkup and restore times are small and negligible. The remainder of
    this paper is organized as follows. Section II describes the motivation for vertical
    elasticity of Docker containers. Section III provides the technical background
    on Docker. Section IV presents our ELASTICDOCKER approach to scale up/down and
    migrate Docker containers. Section V describes the experimental setup to evaluate
    ELASTICDOCKER and discusses obtained results. After that, we discuss this approach
    and its limits in Section VI. We present related works in Section VII. Section
    VIII concludes the paper and hightlights research perspectives. SECTION II. Motivation
    A. Resource Over-Provisioning and De-Provisioning The load of cloud applications
    varies along with time. Diverse applications have different requirements. Therefore
    as shown in Fig. 1, maintaining sufficient resources to meet workload burst and
    peak requirements can be costly. Conversely, maintaining minimum or medium computing
    resources can not meet workload''s peak requirements, and cause bad performance
    and Service Level Objective (SLO) violations. Autonomic cloud elasticity permits
    to adaptively and timely scale up/down resources according to the actual workload.
    Fig. 1. Resource over-lunder-provisioning Show All B. Vertical Elasticity Elasticity
    is defined as the ability to adapt resources on the fly to handle load variation.
    There are two types of elasticity: horizontal elasticity and vertical elasticity.
    Horizontal elasticity requires more support from the application, so that it can
    be decomposed into instances. Recently, most attention has been given to horizontal
    elasticity management. Vertical elasticity is limited due to the fact it can not
    scale outside the resources provided by a single physical machine and then introduces
    a single point of failures. However, vertical elasticity is better when there
    are enough available resources. Vertical elasticity has the following characteristics:
    Vertical elasticity is fine-grained scaling while it permits to add/remove real
    units of resources. Vertical elasticity is applicable to any application, it also
    eliminates the overhead caused by booting instances in horizontal elasticity while
    horizontal elasticity is only applicable to applications that can be replicated
    or decomposed. Vertical elasticity does not need running additional machines such
    as load balancers or replicated instances. Vertical elasticity guarantees that
    the sessions of the application are not interrupted when scaling. Some applications
    such as MATLAB, AutoCAD do not support horizontal elasticity. They are not replicable
    by design. These applications are composed of components and their interconnections.
    These components can not be elastic, which means that it is impossible to create
    several instances of the same component. Since horizontal elasticity consists
    in replicating the application on different machines, some applications such as
    vSphere and DataCore require additional licenses for each replica. These licenses
    could be very expensive, while only one license is required in vertical elasticity.
    Vertical elasticity maintains better performance, less SLO violations, and higher
    throughput. Vertical elasticity increases the performance because the elasticity
    controller just increases the capacity of the same instance. In horizontal scaling,
    the elasticity controller can add/remove instances, which impacts the application
    performance. This fact is verified by using the queuing theory [6], and the proof
    was demonstrated in [6]. Horizontal elasticity could result to have many small
    instances and modeled as MIM/1, while vertical elasticity controls one instance
    by varying its capacity and modeled as M/M/c in the queuing theory, where c is
    the number of CPUs. After solving the corresponding equations for each model,
    [6] founded that the response and waiting time in vertical elasticity is much
    less than that of horizontal elasticity for the same workload input. Due to the
    limit of space, we do not duplicate the equations and examples to show this fact.
    However in Section V-B5, we show experimentally that ELASTICDOCKER vertical elasticity
    outperforms Kubernetes horizontal elasticity by 37,63%. C. Containers Vs VMS Docker
    containers are a new lightweight virtualization technology. We outline this technology
    in details in Section III. Docker requires an autonomic elastic system in order
    to avoid the problems of over-provisioning and under-provisioning. We present
    here motivations towards Docker vertical elasticity. Containers consume low resource
    because they share resources with the hosting operating system, which makes them
    more efficient. Therefore, we can deploy more containers than VMs on a physical
    machine [9]. Containers result in equal or better performance than VMs [11]. Containers
    have small image size, therefore, time of generating, distributing and downloading
    images is short, in addition they require less storage space [9]. ELASTICDOCKER
    proposes an approach that manages autonomous vertical provisioning and deprovisioning
    of Docker containers on the host machine and migrates them if there is no enough
    resources. SECTION III. Background This section gives a brief introduction about
    Docker technologies in order to facilitate the understanding of our work. It also
    elaborates Docker image file system and CRIU, the concept behind ELASTICDOCKER
    container live migration. A. Docker Technology Docker is a lightweight virtualization
    technology that allows to package an application with all of its dependencies
    into one standardized unit for software deployment. Docker uses a client-server
    architecture. It consists of three main components, Docker client, Docker host
    and Docker registry. Docker host represents the hosting machine on which Docker
    daemon and containers run. Docker daemon is an essential part of Docker architecture,
    it is responsible for building, running, and distributing the containers. The
    interactions with Docker daemon are done through Docker client. Docker client
    is the user interface to Docker. Docker registry is the service responsible for
    storing and distributing images. B. Resource Management of Docker Docker containers
    use namespaces to isolate resources, and cgroups to manage and monitor resources.
    Runc is a lightweight tool that runs the containers (container runtime). Runc
    uses libcontainer and LXC drivers which are Linux container libraries that enable
    and abstract interactions with Linux kernel facilities such as cgroups and namespaces
    to create, control and manage containers. 1) Control Groups (cgroups) Docker container
    relies on cgroups to group processes running in the container. Cgroups allow to
    manage the resources of a container such as CPU, memory, and network. Cgroups
    not only track and manage groups of processes but also expose metrics about CPU,
    memory and I/O block usage. Cgroups or subsystems are exposed through pseudo-filesystems.
    The filesystem can be found under /sys/js/cgroups in recent Linux distributions.
    Under this directory, we can access multiple subsystems in which we can control
    and monitor memory, CPU cores, CPU time, I/O, etc. In these files, Docker resources
    can be configured to have hard or soft limits. When soft limit is configured,
    the container can use all resources on the host machine. However, there are other
    parameters that can be controlled here such as CP U shares that determine a relative
    proportional weight that the container can access the CPU. Hard limits are set
    to give the container a specified amount of resources, ELASTICDOCKER changes these
    limits dynamically according to the container workload. By default, Docker sets
    no limits, then a Docker container can use all the available resources on the
    host machine. It can use all the CPU cores as well as memory. The CPU access is
    scheduled either by using Completely Fair Scheduler (CFS) or by using Real-Time
    Scheduler (RTS) [21]. In CFS, CPU time is divided proportionately between Docker
    containers. On the other hand, RTS provides a way to specify hard limits on Docker
    containers or what is referred to as ceiling enforcement. Our elastic approach
    is integrated with RTS in order to make it elastic. Limits on Docker containers
    are set, our ELASTICDOCKER scales up or down resources according to demand. Once
    there is no limit set, it is hard to predict how much CPU time a Docker container
    will be allowed to utilize. In addition, as indicated, by default Docker can use
    all resources on the host machine, there is no control how much resources will
    be used by that container (customer) as many containers (customers) can coexist
    on the same hosting machine. A customer may not afford to pay for such uncontrolled
    amount of resource. Moreover, it will be complicated for the provider to manage
    the customer billing system, e.g., providers usually bill the customer by instance
    (VM) or according to the number of CPUs, not by a partial usage of many CPUs.
    C. Docker Image Filesystem Docker builds and stores images, these images are then
    used to create containers. Docker image consists of a list of read-only layers
    that represent filesystem differences. The layers are stacked on top of each other
    to form the base of a container''s root filesystem. When container is created,
    a new writable layer called container layer is added on top of the underlying
    layers or stack. Docker supports many storage drivers such as aufs, btrfs, overlay,
    etc. In our case, AUFS is used. AUFS is a unification filesystem, which means
    it takes many multiple directories (image layers), stacks them on top of each
    other, provides a single unified view through a single mount point. Docker hosts
    the filesystem as a directory under /var/lib/docker/. D. Checkpoint/restore in
    Userspace (CRIU) CRIU is a Linux functionality that allows to check-point/restore
    processes. It has the ability to save the state of a running application so that
    it can latter resume its execution from the time of the checkpoint. Our migration
    approach for Docker containers with CRIU can be divided in two steps, checkpoint
    and restore, in addition to copy process if the dumped files do not reside on
    a shared file system between the source and target host [22]. SECTION IV. Elastic
    Docker Approach A. System Design We designed ELASTICDOCKER to automatically scale
    up/down Docker containers to adjust resources to the varying workload. ELASTICDOCKER
    provisions and deprovisions Docker resources vertically on the host machine. As
    shown in Fig. 2, ELASTICDOCKER consists of monitoring system and elasticity controllers.
    Elasticity controllers can adjust memory, vCPU cores, and CPU fraction according
    to the workload demand. These components are presented in the below sections.
    ELASTICDOCKER adheres to use the well-known autonomic MAPE-K loop [17]. MAPE-K
    is an autonomic computing architecture introduced by IBM, it consists of four
    phases: Monitor, Analyze, Plan, Execute, and Knowledge. In our system, firstly,
    different Docker metrics are continuously monitored. In the second analysis phase,
    thresholds are calculated based on the monitored metrics, then the decision and
    plan to scale up/down is taken accordingly. Finally, we implement the decisions
    to adjust Docker resources according to the need. Fig. 2. ELASTICDOCKER architecture
    Show All B. Monitoring System Our monitoring system collects most resource utilization
    and limits of Docker containers by interrogating directly with Docker cgroup subsystems
    while it uses Docker RESTful API to check CPU usage. The system continuously monitors
    the memory subsystem in cgroup to check the memory current size assigned to each
    Docker container as well the current memory utilization. Similarly, we check the
    CPU parameters such as the number of vCores and time. In Section IV-C, we highlight
    how to control the CPU time, thus allowing us to control CPU percentage assigned
    to each Docker container. In the experimentation, we have noticed that the CPU
    and memory utilization values are sometimes fluctuating rapidly, which could be
    due to the nature of workload. Therefore, to avoid this oscillation, we measure
    CPU and memory utilization each 4 seconds on an interval of 16 seconds (as showoff
    in Table I), then we take the average value as the current utilization of CPU
    and memory. C. Elasticity Controller The elastic controller adjusts memory, CPU
    time, vCPU cores according to workloads. ELASTICDOCKER modifies directly the cgroups
    filesystem of Docker containers to implement scaling up/down decisions. The memory
    is monitored and then based on its usage and thresholds, ELASTICDOCKER increases
    or decreases its size. The upper threshold is set to 90%, and the lower threshold
    is set to 70%. The values shown in Table I are chosen following [6], [23] which
    are based on real-world best practices, in addition we tried different values,
    and selecting the best values that lead to less response time. Once the memory
    utilization is greater than the upper threshold, ELASTICDOCKER adds 256MB to its
    size. In the deprovisioning state, the memory size is decreased by 128MB. We decrease
    memory size by small amount in the scaling down process because the applications
    are sensitive to the memory resource, and this could lead to interrupt the functionality
    of the application. In addition, after each scaling decision, ELASTICDOCKER waits
    a specific period of time (breath duration). Breath duration is a period of time
    left to give the system a chance to reach a stable state after each scaling decision.
    As shown in Table I, we set two breath durations, breath-up and breath-down. Breath-up
    is time to wait after each scaling up decision. We chose these small values because
    the application adapts quickly to the container change, we have noticed that the
    application functions normally after these time periods. Breath-up is smaller
    than breath-down to allow the system to scale-up rapidly to cope with burst workload.
    Breath-down is larger than breath-up duration in order to avoid uncertain scaling
    down which could cause degradation in the performance of the system. Table I Elasticdocker
    parameters ELASTICDOCKER also controls CPU time (percentage) and number of vCPUs
    assigned to each Docker container. As we have seen in Section III, we can control
    CPU time by changing CFS parameters, namely cpu.cJs_period_us and cpu.cJs_quota_us,
    we refer to them simply as period and quota. For example if period is set to 100000
    and quota set to 10000, Docker can use 10% of CPU percentage (i.e, 0.01 second
    of each 0.1 second), if a Docker container has two vCPUs and period == 100000
    and quota == 200000, this means the Docker container can completely use the two
    vCPUs. ELASTICDOCKER increases quota or CPU percentage in function of CPU usage
    and dynamic thresholds. For example, if a container has 10% of CPU time, the threshold
    will be 9.5, 20% of CPU time, threshold will be 19% and so on. Once a container
    has used all the CPU time, new core will be added. Upper threshold to add a vCPU
    core is 90%. Lower threshold is set to 70%, if CPU usage is less than lower threshold,
    vcores will be removed. However let''s suppose that a Docker container has three
    vCPUs cores and quota/period == 250000/100000 and CPU usage is less than 70%,
    the scaling down decision is taken according to the following condition: cpu_usage
    < 70% and no_vC PU s > 1 and quota < period * (no_vCPUs −1), where no_vCPUs is
    the number of vCPUs allocated to the container. Similar to memory, breath durations
    are set for CPU resizing. It is worth noting that ELASTICDOCKER takes in consideration
    the available resources on the host machine, and the allocated resources of other
    Docker containers on the host upon each scaling up/down decision. D. Container
    Live Migration Many containers generally reside on the same host machine. Therefore,
    when one Docker container continues to ask for more resources, if there is no
    more resources on the host, live migration will take place for that Docker container.
    The container will be migrated to another host machine. The process of live migration
    consists of four main steps as shown in Fig. 3. Firstly, the filesystem differences
    of the container image layers in /aufs/diff/ will be transferred. There are many
    directories in /aufs/diff/ representing image layers, so we tar and send these
    layers to the destination host. Secondly, The container process will be pre-dumped.
    The container is still running after the pre-dump. The objective of the pre-dump
    is to minimize the migration downtime. The pre-dumped images are compressed using
    LZ4 compression in a TAR file and sent to the destination. We perform several
    pre-dump iterations, each pre-dump generates a set of pre-dump images, which contain
    memory changes after previous pre-dump. This reduces the amount of data dumped
    on the dump stage. Thirdly, we proceed to dump the container state, the dump process
    will be rapid because it only takes the memory that has changed after the last
    pre-dump. On the destination host, we will restore the container to the same memory
    state on the source host thank to CRIU. Fig. 3. Migration procedure based on criu
    Show All SECTION V. Experiments and Evaluation A. Experimental Setup We evaluated
    our approach with respect to the performance and end-user QoE, customer cost,
    resource utilization, migration efficiency and then we compare ELASTICDOCKER versus
    Kubernetes autoscaling. We performed all our experiments on Scalair2 infrastructure
    inside VMs. Scalair is a private cloud provider company. The VM on which containers
    run has 7vCPUs with 5GB RAM and Centos 7.2 OS. We use Graylog, a powerful log
    management and analysis platform. We chose this application because it consumes
    a lot of resources. Since Graylog centralizes and aggregates all log files from
    different sources, it can suddenly get overloaded, and that requires a lot of
    attention from the providers to adjust resources according to the need particularly
    at peak''s times. Graylog is widely implemented in the industry and it is based
    on four main components Graylog Server, Elasticsearch, MongoDB and Web Interface.
    Graylog Server is a worker that receives and processes messages and communicates
    with all other components. Its performance is CPU dependent. Elasticsearch is
    a search server to store all of the logs/messages. It depends on the RAM, disk
    and also CPU. MongoDB stores read-only configuration and metadata and does not
    experience much load. Web Interface is the user interface. We run three containers,
    the first one is for the Graylog server version 2.0.0 and Web interface 2.0.0
    while the second and third are for Elasticsearch version 2.3.3 and MongoDB version
    3.2.6 respectively. We use Docker version 1.9.1 and Docker Compose version 1.7.1.
    Docker Compose is used to define and run the different components of Graylog in
    the containers. We also installed Ubuntu 14.04 and Httperf3 version 0.9.0-2buildl
    on the second VM. It has 2vCPUs with 4GB RAM. We also set scripts to send log
    messages and overload Graylog server on this VM. The two VMs are on different
    VLANs. httperf generates requests to query the Graylog server. B. Evaluation and
    Results 1) Performance and End-Users Qoe First, we investigate the impact of our
    proposed approach on the performance and end-user QoE and compare the results
    between Docker and ELASTICDOCKER. Therefore, we run our experiments to evaluate
    the performance of the Graylog application in two cases: i) with Docker only,
    and ii) with ELASTICDOCKER system. We generate different workloads using httperf
    to query and search information from Graylog via its REST API. Fig. 4 shows the
    results of comparison experiments of average response time (RT) between Docker
    and ELASTICDOCKER. As shown in Fig. 4, we have different request rates 10 req./sec.,
    50 req/sec., etc. The more the number of requests are, the more the RT increases.
    When the number of requests are between 10 and 50 requests per second, there is
    no significant difference in average RT between the two cases. However, when the
    number of requests increases and requires more resources, ELASTICDOCKER reacts
    to provision resources accordingly, therefore the RT decreases and the performance
    increases. The blue and red bars in Fig. 4 indicate Docker and ElasticDocker performances,
    respectively. ELASTICDOCKER increases performance by 74,56%. Fig. 4. Graylog response
    time with docker vs elasticdocker Show All 2) Customers'' Expenses Reduction In
    this part of experiment, we study the impact of ELASTICDOCKER on the cost. To
    put stress on Graylog and ELASTICDOCKER containers, we generated workloads with
    a random rates that arrive to more than 1000 req./sec. by scripts on the second
    VM. The workloads are syslog and Graylog Extended Log Format (GELF) logs. The
    logs generated sent to be processed by Graylog server and stored in the Elasticsearch
    container. At the beginning, each Graylog, Elasticsearch and MongoDB Docker has
    1vCPU and 1130MB, 384MB and 128MB respectively. Fig. 5 shows the vCPU and memory
    size for each Docker over time. Graylog and Elasticsearch containers consume CPU
    and memory while MongoDB has only 1vCPU and 128MB of RAM because it just stores
    metadata. To facilitate the understanding of this experiment, let us consider
    the following simplified pricing model used by cloud providers: cost= ∑ n=1 n
    cpu( t n ,  t n−1 )∗( t n ,  t n−1 )∗p+ mem( t n ,  t n−1 )∗( t n ,  t n−1 )∗
    p ′ (1) View Source where cpu( t n ,  t n−1 ) is the number of vCPUs in a time
    period between ( t n ,  t n−1 ),p is the price for each vCPU in time period (
    t n ,  t n−1 ),mem( t n , t n−1 ) is the memory size in a time period ( t n ,  t
    n−1 ),p'' price for the memory. To ease the understanding of the customer costs,
    let us consider the vCPU consumption of Graylog containers, referring to Fig.
    5 and Table II, the cost according to Equation (1) is cost=1∗(t1, t0)∗p+2(t2,
    t1)∗p+3(t3, t2)∗p+4(t4, t3)∗p+3(t5, t4)∗p+2(t6, t5)∗p+1(t Without ELASTICDOCKER
    the customer will reserve fixed resources all the time, in our case, it could
    be 4vCPUs for graylog server and then the cost will be 4∗(t8, t0)∗p=66,04p , (from
    Fig. 5, the time periods ( t n ,  t n−1 ) are translated against a fixed interval).
    From these results, ELASTICDOCKER reduces cost by 56.65%. It is shown that ELASTICDOCKER
    significantly decreases the charge for the customers. Fig. 5. CPU and memory consumption
    of Graylog, Elasticsearch and MongoDB containers Show All Table II Vcpus vs. time
    for docker1 3) Optimal Utilization for Resources We evaluated the resource utilization
    in a single host. Fig. 6 shows that ELAS-TICDOCKER can maintain a better utilization
    of resources. Without ELASTICDOCKER the resources reserved while they are idle.
    For example in our experiment, to avoid services interruption in Graylog container,
    4vCPU must be reserved, however the need for these vcores is for small period
    only, after that they are idle and would not be possible to run three containers
    on the same host. ELASTICDOCKER reserves and frees resources according to the
    charge. Fig. 6. Resource utilization Show All 4) Docker Live Migration Efficiency
    In this section we migrate different Docker applications from one host to another.
    We used Docker version 1.9.0-dev and CRIU version 2.2 on both hosts. We evaluate
    ELASTICDOCKER live migration technique with respect to many parameters such as
    checkpoint, restore time, etc. We migrate different applications as shown in Table
    III. The simple workload generator tool (stress) is used. We have checked the
    application state, for example, we set a counter in the source container and we
    check the value of the counter once the container is migrated. It shows that the
    first value on the counter in the migrated container is the value following the
    last value in the source container. In addition, the container nginx with PHP-FPM
    pushes incremental counter to a web page, after migration, the operation continues
    except a suspension for few seconds. Table III shows different migration indicators.
    Pre-dump time is the time duration during the pre-dump process of the container.
    Dump time is the time duration during the final dump of the container. Migration
    and restore times are the periods during the whole process of migration and time
    to restore the application respectively. Downtime is the time of interruption
    when the container process is frozen during the final dump process. Table III
    Migration performance indicators Table III shows the different migration indicators
    and their values measured during migration of the application containers. The
    values are small especially the downtime which is the most important in the live
    migration. Downtime causes a negative impact particularly on stateful applications
    that are too sensitive for TCP sessions. It is worth noting that there are other
    factors that could impact the migration such as network bandwidth. 5) Vertical
    Elasticity Vs Horizontal Elasticity We compared ELASTICDOCKER with Kubernetes,
    i.e., vertical elasticity versus horizontal elasticity. Kubernetes is an open-source
    system for automating deployment, scaling, and management of containerized applications.
    To achieve the experiment, we use RUBiS and Kubernetes version v1.2.0. RUBiS is
    a well-known Web application benchmark that simulates an online auction site.
    Our deployment of RUBiS on Kubernetes uses three tiers: a loadbalancer (Kubernetes
    service performs this role), a scalable pool of JBoss application servers, and
    a MySQL database. Kubernetes platform is deployed on 4 nodes running CentOS Linux
    7.2. RUBiS is deployed in two containers, in addition to a loadbalancer. Then,
    we set the Kubernetes Horizontal Pod Autoscaling (HPA) to scale RUBiS containers
    based on rules-based threshold. We use the same thresholds used in ELASTICDOCKER.
    We generate three workloads (low, medium and high) using Apache HTTP server benchmarking
    tool (ab) and the total execution time is measured for each workload. We then
    generate the same workloads to ELASTICDOCKER (running RUBiS on the machine described
    in Section V-A) and the total execution time is measured as shown in Fig. 7. Fig.
    7. Comparison between kubernetes'' elasticity capabilities and elastic-docker
    Show All Based on the analysis of these results we concluded the following findings:
    (i) the average total execution time for the three workload in ELASTICDOCKER is
    132.6 seconds, while the average total execution time in Kubernetes is 212.62
    seconds. (ii) ELASTICDOCKER outperforms Kubernetes horizontal elasticity by 37.63%.
    (iii) This confirms the analysis in Section 11-B that vertical elasticity is more
    efficient than horizontal elasticity. Due to the limit of space, we do not include
    the resource usage, however, Kubernetes uses more resources, even when the workloads
    are terminated, the scaled containers takes more than 5 minutes to start to scale
    in. This could be due to the slow monitoring component in Kubernetes (Heapster).
    SECTION VI. Discussions Our system uses reactive approach based on the threshold-based
    rules to perform elastic actions. While threshold based rules are the most popular
    auto-scaling technique, even in commercial systems, setting-up suitable thresholds
    is very tricky, and may lead to instability of the system. Therefore, in the performance
    section of our experimentation, we have tried different thresholds, i.e., 90,
    85, 80, 70, 60 and different breath or cooling durations. After that, we have
    chosen the best values as shown in Table I, which yields to best performance (lower
    response time). The ideal will be to use machine learning. The improvement of
    QoE by ELASTICDOCKER is not without cost. In fact, ELASTICDOCKER allocates more
    resources to overcome the workload burst. The system proposed allows to control
    CPU and memory. Docker allows to control the numbers of operations per second
    (ops) or amount of data, bits per second (bps) on specific devices connected to
    Docker container. However, there is no direct method particularly in AUFS filesystem
    to adjust quota or amount of disk available to a specific container. So, it is
    difficult to resize the disk storage at runtime. Although it is true that Docker
    provides the option-storage-opt to resize storage, this option is applied to the
    daemon level not to a single container. For the migration, we simply migrate the
    container which requires more resources when there is no sufficient resources
    to reply its demand. The idea is to improve this mechanism in order to have more
    intelligent system that decides which container to migrate: the one which currently
    requires more resources, or the one which has less activity, etc. SECTION VII.
    Related Work Elasticity is a major research challenge in the field of cloud computing.
    Several different approaches have been proposed for the elasticity at VMs level
    such as [3], [5], [6]. However, with the appearance of Docker containers and their
    widespread popularity among cloud providers, some researches are dedicated to
    this field. Kukade et al. [12] proposed a design for developing and deploying
    micro services applications into Docker containers. The elasticity is achieved
    by constantly monitoring memory load and number of requests by an external master.
    Once certain thresholds are reached, the master node invokes scaling agent. The
    scaling agent permits to horizontally spin in or out the container instances.
    Haldy et al. [13] worked on container live migration technique and proposed a
    framework called MultiBox. MultiBox is a mean for creating and migrating containers
    among different cloud providers. It makes use of Linux cgroups to create containers
    and migrate the source containers to those newly created ones. Hoenisch et al.
    [16] proposed a control architecture that adjusts VMs and containers provisioning.
    DoCloud [14] is a horizontal elastic cloud platform based on Docker. It permits
    to add or remove Docker containers to adapt Web application resource requirements.
    In DoCloud, a hybrid elasticity controller is proposed that uses proactive and
    reactive model to scale out and proactive model to scale down. Monsalve et al.
    [24] proposed an approach that controls CPU shares of a container, this approach
    uses CFS scheduling mode. Nowadays, Docker can use all the CPU shares if there
    is not concurrency by other containers. Paraiso et al. [15] proposed a tool to
    ensure the deployability and the management of Docker containers. It allows synchronization
    between the designed containers and those deployed. In addition, it allows to
    manually decrease and increase the size of container resource. These works either
    handle horizontal elasticity or manual vertical elasticity. [23] proposed horizontal
    and vertical autoscaling technique based on a discrete-time feedback controller
    for VMs and containers. This approach is limited to Web applications. In addition,
    the application requirements and metadata must be precisely defined to enable
    the system to work. It also adds overhead by inserting agents for each container
    and VM. Kubernetes and Docker Swarm are orchestration tools that permit container
    horizontal elasticity, they allow also to set limit on containers during their
    initial creation. Our proposed approach supports automatic vertical elasticity
    for Docker containers and live migration if there is no enough resources. SECTION
    VIII. Conclusion & Perspectives ELASTICDOCKER is an elasticity controller to dynamically
    grow or shrink Docker resources according to workloads. If there is no more resources
    on the host machine, we migrate the container to another host. This migration
    technique is based on CRIU functionality in Linux systems. Through experimental
    evaluations we have shown that ELASTICDOCKER significantly increases end-user
    QoE and performance, reduces customer''s expenses and makes a better resource
    utilization. In addition, the migration downtime is very small and the application
    state is maintained. The experiments also demonstrate that fine-grained adaptation
    capabilities of ELASTICDOCKER greatly improve performance when compared to Kubernetes
    autoscaling. We envision extending this work in several ways: (i) coordinating
    container elasticity and hosting VM elasticity, (ii) enhancing ELASTICDOCKER with
    features to support predictive approaches in order to anticipate workloads and
    rapidly scale up resources, (iii) extending the proposed platform to complement
    vertical elasticity with horizontal elasticity (what we name “diagonal” elasticity).
    ACKNOWLEDGMENT This work is supported by the OCCI ware research and development
    project (http://www.occiware.org) funded by French Programme d''Investissements
    d'' Avenir (PIA). Likewise, this work is also funded by Scalair company (http://www.scalair.fr).
    Authors Figures References Citations Keywords Metrics Footnotes More Like This
    Monitoring and Billing of A Lightweight Cloud System Based on Linux Container
    2017 IEEE 37th International Conference on Distributed Computing Systems Workshops
    (ICDCSW) Published: 2017 Real-time monitoring system for containers in highway
    freight based on cloud computing and compressed sensing 2017 IEEE 17th International
    Conference on Communication Technology (ICCT) Published: 2017 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Autonomic Vertical Elasticity of Docker Containers with ELASTICDOCKER
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icc.2019.8762053
  analysis: '>'
  authors:
  - Isam Mashhour Al Jawarneh
  - Paolo Bellavista
  - F. Bosi
  - Luca Foschini
  - Giuseppe Martuscelli
  - Rebecca Montanari
  - Amedeo Palopoli
  citation_count: 59
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >ICC 2019 - 2019 IEEE Internat... Container Orchestration
    Engines: A Thorough Functional and Performance Comparison Publisher: IEEE Cite
    This PDF Isam Mashhour Al Jawarneh; Paolo Bellavista; Filippo Bosi; Luca Foschini;
    Giuseppe Martuscelli; Rebecca Montanari; Amedeo Palopoli All Authors 54 Cites
    in Papers 3201 Full Text Views Abstract Document Sections I. Introduction II.
    Background III. Performance metrics for container orchestration engine IV. Experimental
    Results V. Related Literature Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: In the last decade, novel software architectural patterns,
    such as microservices, have emerged to improve application modularity and to streamline
    their development, testing, scaling, and component replacement. To support these
    new trends, new practices as DevOps methodologies and tools, promoting better
    cooperation between software development and operations teams, have emerged to
    support automation and monitoring throughout the whole software construction lifecycle.
    That affected positively several IT companies, but also helped the transition
    to the softwarization of complex telco infrastructures in the last years. Container-based
    technologies played a crucial role by enabling microservice fast deployment and
    their scalability at low overhead; however, modern container-based applications
    may easily consist of hundreds of microservices services with complex interdependencies
    and call for advanced orchestration capabilities. While there are several emerging
    container orchestration engines, such as Docker Swarm, Kubernetes, Apache Mesos,
    and Cattle, a thorough functional and performance assessment to help IT managers
    in the selection of the most appropriate orchestration solution is still missing.
    This paper aims to fill that gap. Collected experimental results show that Kubernetes
    outperforms its counterparts for very complex application deployments, while other
    engines can be a better choice for simpler deployments. Published in: ICC 2019
    - 2019 IEEE International Conference on Communications (ICC) Date of Conference:
    20-24 May 2019 Date Added to IEEE Xplore: 15 July 2019 ISBN Information: ISSN
    Information: DOI: 10.1109/ICC.2019.8762053 Publisher: IEEE Conference Location:
    Shanghai, China SECTION I. Introduction The need for business agility has led
    to pressure for more frequent software delivery and software development techniques,
    known as "agile", and related DevOps methodologies and tools to make software
    development and delivery a continuous lifecycle [1]. Along these trends, the microservice
    architectural pattern has been adopted to decompose the monolithic structure in
    independent components that are easier to develop, manage, and (horizontally)
    scale. In brief, this pattern encapsulates individual core application functionalities
    in microservices and builds larger systems by composing microservices as building
    blocks. Each application consists of deployable independent services that perform
    specific business functions and communicate via Application Programming Interfaces
    (APIs). Containers provide an ideal means to realize those microservices due to
    their low overhead and speed of deployment. Furthermore, they are suitable for
    efficient horizontal scaling that can be obtained by deploying multiple identical
    containers. Therefore, modern complex applications can consist of hundreds or
    even thousands of services with several (potentially intricate) interdependencies.
    According to these new trends of design and deployment, the usage of container
    solutions for large applications can result difficult to be adopted; that justified
    the introduction of a higher containerization layer known as container orchestration.
    Container orchestrator engine (or container orchestrators) automate container
    provisioning and management including resource scheduling, coordination, and communication
    across microservices, and resource booking and accounting [2], [3]. Currently,
    after a decade of research and development in container technologies, there are
    several container orchestrators available on the market, such as Docker Swarm,
    Kubernetes and Mesos (just to cite some of the most diffused ones). In the last
    years, some related works in the literature addressed and benchmarked performances
    of traditional virtualization solutions vs container-based ones, in general, and
    in vertical domains [4], [5], [6]. However, to the best of our knowledge, focusing
    on container orchestrators there are still neither well-established frameworks
    to qualitatively compare them, nor thorough assessments of their performances,
    especially under heavy-load situations. To fill those voids, this paper presents
    a novel and original comparison of container orchestration engines that presents
    the following main features. First, we isolate the main functional elements to
    analyze existing solutions and we apply them to four main representative market
    players, namely, Docker Swarm, Kubernetes, Apache Mesos and Cattle. Second, we
    propose some performance metrics to benchmark them. Third, we show several experimental
    results to assess their performance behavior. Fourth, we provide some technology
    selection guidelines that we believe could help telco provider IT managers in
    the design and migration to 5G fully-software-based telco infrastructures. The
    remainder of the paper is divided as follows. Section II provides the background
    about functional analysis and the qualitative comparison. In Section III, we introduce
    main performance metrics, and in Section IV we use them for performance assessment.
    Section V and Section VI summarize recent related works and draw conclusions and
    future work. SECTION II. Background In this section, we introduce needed background
    about the container orchestrator model, and then we use it to qualitatively compare
    a selection of four widely diffused container orchestrators. A. Container Orchestration:
    Model and Functional Elements Container orchestration allows to define automated
    provisioning and change management workflows to operate so to always grant agreed
    policies and service levels. Fig. 1 depicts our reference layered orchestration
    engine architecture where a set of machines, through their kernel and container
    runtime realize the support substrate. The orchestration engine structure sits
    atop and consists of three layers: resource management, scheduling, and service
    management. In the following, due to space limitation, we provide a fast overview
    of main functional elements and then we use them for our qualitative analysis
    (see also Tables I, II, and III). The resource management layer manages low-level
    resources; in this case, functional elements are the resources that can be managed/composed
    and include: memory, CPU/GPU, disk space, volumes (i.e., possibility to interact
    with the file system of the local hosting machine), persistent volumes (i.e.,
    possibility to interact also with a remote cloud file system), port and IP (i.e.,
    configuration of UDP/TCP ports and IPs within the container virtual network).
    It aims at maximizing utilization and minimizing interference between containers
    competing for resources. Table I reports the resources supported by the compared
    solutions. The scheduling layer aims at using cluster resources efficiently. It
    typically receives user-supplied indications (e.g., placement constraints, replication
    degree, etc.) as input and then decides how to place all containers composing
    the applications. Most important capabilities (see Table II) include: placement,
    to directly control the scheduling decisions; replication/ scaling to express
    the number of microservice replicas; readiness checking to include a container
    only when it is ready to answer; resurrection to reenact fast long-lived processes
    whose job requires being always up and running; rescheduling to automatically
    restart and schedule crashed containers running on a failed node; rolling deployment
    to automatically up-/down-grades the application version; co-location to assert
    deployment constraints, such as to co-locate containers so to take advantage of
    local inter-process communication. Fig. 1 Container Orchestration Layers Show
    All Finally, the service management layer provides functional capabilities for
    building and deploying (complex) enterprise applications. It manages high-level
    aspects (see Table III) that include: labels to attach metadata to container objects;
    groups/namespaces to isolate containers and support multi-tenancy; dependencies
    to express dependencies between microservices; load-balancing to divide incoming
    load; and readiness checking to make the application available online only when
    it is ready to accept incoming traffic. B. Docker Swarm, Kubernetes, Apache Mesos,
    and Cattle Without any pretense of being exhaustive, in the remainder of the paper,
    we focus on four container orchestrators, namely, Docker Swarm, Kubernetes, Apache
    Mesos and Cattle. Docker Swarm and Kubernetes have been selected because they
    are the most diffused in the market, Mesos because it represents a very significant
    baseline seminal effort in the field, and finally Cattle was included since it
    is the reference orchestrator for Rancher. Rancher is not a container orchestrator,
    but rather a complete container management platform that we leveraged to deploy
    and run our experimental results. Docker Swarm is a clustering and scheduling
    tool for Docker containers [7]. It allows IT operators to manage a cluster of
    Docker nodes as a single system. This is an important aspect because it creates
    a cooperative group of machines that provide redundancy and enable the failover
    mechanism if one or more nodes experience an outage. The orchestrator is based
    on the master/slave model for which master dominate. The master (known as Manager)
    is the node that is responsible for scheduling containers, whereas a slave (commonly
    known as Agent) is responsible for launching received containers. Both redundancy
    and placement are managed by the scheduling layer (see Table II). To connect containers
    hosted on different nodes under the same network, Docker Swarm offers an overlay
    network which exploits the VXLAN tunnel for creating a virtual network among hosts.
    The Manager node keeps tracks of the state of all nodes in a cluster(in the context
    of Docker Swarm this service is called discovery and is based on heartbeat mechanism
    that overlay networking module uses in determining whether a Docker daemon on
    a remote host in a cluster is still functioning). In cloud environments, elasticity
    is an important feature, Docker Swarm allows the fine-grained scalability of part
    of the service scaling one or more replicated services either up or down to the
    desired number of replicas. Table I. Resource management layer comparison TABLE
    II. Scheduling Layer Comparison TABLE III. Service Management Layer Comparison
    Kubernetes is an open-source platform introduced by Google in 2014 for managing
    containerized applications across a cluster of machines [8]. By analogy, Kubernetes
    is also based on a master/slave architectural pattern for which a developer submits
    a list of applications to a master node and, subsequently, the platform deploys
    them across slave and master nodes. Master node represents a control plane of
    the cluster, and it can be replicated to guarantee high-availability and fault-tolerance
    exploiting the scheduling layer. Slave nodes (known as minions) are those nodes
    where application containers are executed. Kubernetes provides a containerized
    application as a set of containers, each of which is specific for a single microservice.
    Pod is a basic unit in Kubernetes and it represents a group of containers that
    are co-scheduled. All containers within a pod are controlled as a single application
    and thus share the same environment. There may be one or more containers within
    a pod. Since pods are co-scheduled and run in a shared context, containers within
    the pod can be scaled and scaled as a unique application. Pods replication is
    managed by Kubernetes component called Replication Controller, which is responsible
    for ensuring that a certain number of pods are currently providing a specific
    service. If current state departs from expectations such as in cases of an outage
    node the replication controller automatically starts the scheduling of a new instance
    on a different slave node. The heartbeat controller mechanism is not too aggressive
    and is designed with a subscriber notification system. Apache Mesos is a seminal
    open source project developed by the University of California, Berkeley. The architecture
    consists of a master/slave design pattern in which the execution of tasks is delegated
    to slave nodes. The master process, running on a manager node of the cluster is
    responsible for managing and monitoring the whole cluster architecture. Therefore,
    it communicates with frameworks that aim at scheduling jobs on slave nodes [9].
    Mesos solutions are often developed by installing on top of a Mesos cluster an
    application-level management called Marathon. Marathon interacts with the master
    component providing orchestration functionalities to the whole Mesos cluster.
    Thus, in the case of slave faults, Marathon starts a new instance to guarantee
    the fault-tolerance. Mesos offers high-availability replicating master nodes in
    order to provide failover mechanisms in case of master failures. To achieve this,
    it depends on Apache Zookeeper that consists of an election algorithm which elects
    a new node to play a master role. In the same vein, Cattle is an orchestration
    engine powered by Rancher which is extensively exploited by Rancher users for
    creating and managing applications based on Docker containers. One of the key
    reasons for its extensive adoption is its compatibility with standard Docker yaml
    file (also known as docker-compose) syntax and Docker commands. The architecture
    is based on a master/slave architectural pattern and the application deployment
    is based on the concept of "stack". Each stack is a composition of "services"
    which are primarily docker images, characterized by application requirements such
    as scaling, health checks, service discovery links and configuration parameters.
    SECTION III. Performance metrics for container orchestration engine This section
    defines the set of metrics that we used for comparing container orchestrator performances.
    First, we considered the time needed to complete the deployment of the container
    orchestration solution (Subsection III.A). Then, we focused on the performance
    analysis of the scheduling and service management layers (see Fig. 1) to evaluate
    the time to make the application ready (for different scenarios, see Subsection
    III.B and III.C) and to evaluate the time to reschedule and recover from container/node
    faults (Subsection III. D). A. Cluster provisioning time analysis Elastic scalability
    is one of the core properties to be granted by cloud computing that has been boosted
    and facilitated by the advent of container-based technologies that enable fast
    bootstrap. Along with that direction, the first performance metric evaluates the
    provisioning time required to provision a new cluster and to properly configure
    it with the required container orchestration support. Indirectly, this is also
    a measure of the container orchestrator complexity. B. Provisioning time of applications
    with different complexity with local image and Docker registry The second performance
    metric, aiming at evaluating the behavior of provisioning time of container orchestrator
    when deploying different applications with increasing complexities. We chose three
    applications: Jenkins, WordPress, and GitLab. Jenkins is a continuous integration
    server. It automatically runs software tests on a non-developer machine every
    time someone pushes new code into the source repository. This application consists
    of a single container. WordPress is a Content Management System (CMS) based on
    PHP and MySql. This supports the creation and modification of websites. This application
    consists of two containers. Gitlab is a web-based Git repository manager with
    wiki and issue tracking features. This is important for tracking changes in computer
    files and coordinating work on those files among multiple users and teams. This
    application consists of four containers. Moreover, we claim the importance of
    investigating two different scenarios. The first one exploits Docker images pre-installed
    on each Docker host. The second one, assuming no Docker image is available locally,
    downloads it dynamically from a private Docker registry running on a different
    physical server on the same cluster. C. Provisioning time of a web application
    with a high number of replicas using local images and Docker registry This third
    performance metric evaluates the behavior of the container orchestrator with respect
    to the provisioning a high number of replicas. We use the WordPress application
    that represents a medium complexity application with two containers hosting, respectively,
    the front-end and the database. The goal is to analyze how the provisioning time
    is influenced by the increase in the number of replicas of the front-end container
    component. In addition, as for the previous metric, we claim the relevance of
    repeating the experiments for local and remote scenarios (i.e., w/out and with
    local image). D. Failover Time Failover mechanisms increase the reliability and
    availability of IT resource typically using clustering technologies to provide
    redundant implementations. For the analyzed container orchestration solutions,
    failover was configured to automatically switch over to a redundant resource instance
    whenever the currently active IT resources become unavailable. The failure can
    involve a single container or the whole hosting cluster node; hence, in our analysis,
    we investigate both types of failures in terms of the time to fully recover from
    the fault and make the application usable again. SECTION IV. Experimental Results
    This section first introduces the experimental setup of the testbed used to collect
    the experimental results and then reports a through performance evaluation and
    discussion structured according to the four performance metrics introduced in
    the previous section. A. Experimental Setup Our testbed consists of 8 physical
    servers, namely, MicroServer Hewlett Packard Enterprise (HPE) Proliant Gen 8.
    They include 2x Intel (R) Celeron(R) CPU G16610T @ 2.30 GHz processors with 12
    GB of RAM. Each server is equipped with 2 hard drives of 750GB. Both devices work
    with 7200 rpm. Since customizing and setting up orchestrators is a daunting task,
    we used Rancher [10]. Rancher includes everything required for managing containers
    working at a staggering level compared to orchestration perspective. It automates
    the setup of container orchestrator environments by allowing to neglect orchestration-specific
    configurations and facilitates the update and (re)configuration of new stable
    releases. We have directly installed Rancher on top of our four physical servers,
    each operating on Ubuntu system. We have used Rancher UI tool and its monitoring
    internal system for measuring performance test’s times. To reduce error factor,
    each experiment has been repeated 33 times and we show average values; we are
    not reporting standard deviations that are typically rather limited (always below
    6% across all tests). B. Results and Discussion This section shows performance
    results collected for our four representative container orchestration engines:
    Docker Swarm, Kubernetes, Mesos, and Cattle. 1) Cluster provisioning time The
    first set of results inspects cluster provisioning time required for deploying
    a single orchestration tool via Rancher. As the Fig. 2 shows, Cattle requires
    the shortest time to deploy a single cluster since it is the Rancher native orchestration
    tool, and so, the entire architecture is optimized to deploy the container orchestrator
    that is provided by default. Kubernetes requires the highest provisioning time.
    We believe this is because its architecture is the most complex one. In fact,
    Kubernetes is the solution that provides the most capabilities to manage and deploy
    production-level services (see Tables I, II, III). Docker Swarm and Apache Mesos,
    instead, showed a shorter provisioning time according to their reduced complexity.
    2) Provisioning time with local image and Docker registry using applications with
    increasing complexity Fig. 3a and Fig. 3b shows our second set of results for
    the three applications with increasing complexities (i.e., Jenkins, Wordpress,
    and Gitlab) using local/remote images. Obtained results show that application
    complexity significantly impacts the provisioning time. In fact, passing from
    the simplest Jenkins application (with one container) to the most complex Gitlab
    application (4 containers) the provisioning time increases of more than 25% for
    all container orchestrator. About differences due to local/remote image download
    and between all compared solutions, instead, they are almost negligible because
    we do not have a high number of containers (replicas) to manage. To better study
    this scalability aspect, in our second set of results we stress orchestrators
    with an increasing number of replicas. Fig. 2. Provisioning time to deploy the
    orchestration tools Show All Fig. 3a. Provisioning time of applications with different
    using local image. Show All Fig. 3b. Provisioning time of applications with different
    complexity using Docker registry image. Show All 3) Provisioning time with local
    images and Docker registry using an application with a high number of replicas
    Fig. 4a and Fig. 4b shows the provisioning time of the medium-complexity WordPress
    application for an increasing number of replicas, and when Docker image is located
    on either each server node or downloaded from a local Docker private registry.
    Results are quite different because each of the two scenarios involves different
    operations. In the first local download scenario (see Fig. 4a), Kubernetes takes
    the shortest provisioning time, while in the remote one the longest. We believe
    the reason for this behavior is that the interaction between the Kubernetes agent
    and the Docker registry introduces a considerable overhead. Cattle exhibited the
    best running time (let us note that it is the Rancher-native orchestration tool),
    thus optimizing the interaction between orchestrator agent and Docker Registry.
    The other two orchestrators collocate in the middle. Finally, focusing on scalability,
    although we heavily stressed orchestrators with a very high number of replicas
    (up to 100), all solutions tend to scale linearly and showed a reasonably good
    behavior (always below 3 minutes even in the worst-case scenario). 4) Failover
    time Rescheduling allows restoring container services and it is natively supported
    by Kubernetes and Docker Swarm. However, Rancher introduces the possibility to
    define a minimum number of running containers for each microservice. Therefore,
    there is a dedicated Rancher functionality that provides this feature, even if
    the underlying container orchestrator does not offer this capability. Hence, for
    this set of experiments, we used the native feature for Kubernetes and Docker
    Swarm, whereas for Cattle and Apache Mesos we exploited the Rancher capability.
    Fig. 4a. Provisioning time of a single application using local image. Show All
    Fig. 4b. Provisioning time of a single application using Docker registry image.
    Show All As we can see in Fig. 5a, Kubernetes is the best solution to the failure
    of the single container. On the contrary, as shown in Fig. 5b, it is the worst
    in case of a node failure. This is not surprising since in the first case failure
    is detected by local Kubernetes agent that guarantees the availability of running
    containers. In the second case, instead, there is a replication controller that
    is responsible for guaranteeing the rescheduling of failed pods. This approach
    is event-based, and the Kubernetes object is notified by the Kubernetes controller
    when the number of pods changes. That chain effect is the reason for the highest
    failover time value with Kubernetes. Docker Swarm and Cattle, instead, exploit
    the heartbeat functionality of the Docker architecture and that allows them to
    provide the shortest failover time. SECTION V. Related Literature While there
    are already several works in the literature focusing on containerization, such
    as [4], [5], [6], we were able to find only a few works that aim at addressing
    container orchestration and its performance evaluation. [11] focuses on container
    networking in cloud virtualization architectures. They perform systematic experiments
    to study the performance of container networking technologies. [12] presents DockerSim,
    a cloud simulator which incorporates a full container deployment and its behavioral
    layer. It shows a set of experiments simulating behaviors of a containerized web
    application varying the number of executing containers. Authors in [13] focus
    on virtualization technologies in an Internet of Things (IoT) context. It presents
    a performance evaluation of container technologies on constrained devices such
    as Raspberry Pi. Their empirical investigation evaluates, by means of various
    benchmark tools, the performance of Docker when running on this single board computer.
    Fig. 5a. Failover Time - Container Failure. Show All Fig. 5b. Failover Time -
    Host Failure. Show All Focusing on works that, similar to ours, are addressing
    container orchestration, [14] investigates how container technology is employed
    to facilitate multi-tenancy and multi-cloud deployment of SaaS applications. Along
    the same line, [15] presents an easy-to-use and extensible workbench exemplar,
    named K8-Scalar, by implementing and evaluating different self-adaptive approaches
    based on Kubernetes for autoscaling container-orchestrated services. To the best
    of our knowledge, there is still no effort similar to our study in terms of coverage
    of both qualitative and performance aspects, as well as a comparison of multiple
    container engines. Presented functional analyses and performance assessment discussions
    will help IT managers in making informed choices, especially for complex softwarized
    5G telco infrastructures [16]. SECTION VI. Conclusions and future works In this
    paper, we have analyzed container orchestration structure comparing features and
    services offered by container orchestrators. We have also designed a cohesive
    set of metrics to compare their performances at scheduling and service management
    layers and we believe those metrics are reusable for all types of container orchestrators.
    Finally, we chose four representative container orchestrators, namely, Docker
    Swarm, Kubernetes, Apache Mesos, and Cattle, that we deeply inspected and assessed.
    In terms of functional comparison, we notice that Kubernetes is one of the most
    complete orchestrators nowadays on the market. That explains why practitioners
    gravitate toward preferring it to other ones. At the same time, its complex architecture
    introduces, in some cases, a significant overhead that may hinder its performances.
    The obtained results are stimulating our further research activities in the field.
    On the one hand, we are working to enable scalable monitoring of the Kubernetes
    coordination substrate by using and scaling the Istio service. On the other hand,
    as a longer-term goal, we are working to improve the Kubernetes coordination support
    to lower its overhead to cope with complex applications with stringent latency
    requirements. ACKNOWLEDGMENT This research was supported by the SACHER (Smart
    Architecture for Cultural Heritage in Emilia Romagna) project funded by the POR-FESR
    2014-20 (no. J32I16000120009) through CIRI. Authors Figures References Citations
    Keywords Metrics More Like This ContAv: A Tool to Assess Availability of Container-Based
    Systems 2018 IEEE 11th Conference on Service-Oriented Computing and Applications
    (SOCA) Published: 2018 Overview of Docker container orchestration tools 2020 18th
    International Conference on Emerging eLearning Technologies and Applications (ICETA)
    Published: 2020 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Container Orchestration Engines: A Thorough Functional and Performance Comparison'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icit.2017.7915594
  analysis: '>'
  authors:
  - João Rufino
  - Muhammad Alam
  - Joaquim Ferreira
  - Abdur Rehman
  - Kim Fung Tsang
  citation_count: 54
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2017 IEEE International Confe...
    Orchestration of containerized microservices for IIoT using Docker Publisher:
    IEEE Cite This PDF João Rufino; Muhammad Alam; Joaquim Ferreira; Abdur Rehman;
    Kim Fung Tsang All Authors 51 Cites in Papers 3059 Full Text Views Abstract Document
    Sections I. Introduction II. Related Work III. Proposed Architecture IV. Test
    Case Scenario V. Results and Discussion Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: Industrial Internet of things (IIoTs) relies
    on different devices working together, gathering and sharing data using multiple
    communication protocols. This heterogeneity becomes a hindrance in the development
    of architectures that can support applications operating independently of the
    underlying protocols. Therefore in this paper, we proposed a modular and scalable
    architecture based on lightweight virtualization. The modularity provided by the
    proposed architecture combined with lightweight virtualization orchestration supplied
    by Docker simplifies management and enables distributed deployments. Availability
    and fault-tolerance characteristics are ensured by distributing the application
    logic across different devices where a single microservice or even device failure
    can have no effect on system performance. The proposed architecture is instantiated
    and tested on a simple time-dependent use case. The obtained results validates
    that the proposed architecture can be used to deploy services on demand at different
    architecture layers. Published in: 2017 IEEE International Conference on Industrial
    Technology (ICIT) Date of Conference: 22-25 March 2017 Date Added to IEEE Xplore:
    04 May 2017 ISBN Information: DOI: 10.1109/ICIT.2017.7915594 Publisher: IEEE Conference
    Location: Toronto, ON, Canada SECTION I. Introduction Industry 4.0 was presented
    as the next high-tech initiative by the German Government and since then, various
    companies and research institutes have been working to contribute on this topic.
    Industry as always relied on Machine to Machine (M2M) systems where devices exchange
    information and perform actions without the manual assistance of human services.
    These devices called Cyber Physical Systems (CPS) have been evolving from identification
    technologies (like RFID tags) and sensors or actuators with limited functions
    to more complex systems. The Internet of Things (IoT) has brought new cards to
    the table by presenting a layered architecture that separates application layer
    from the sensing layer. It worked as an enabler for internet connected devices
    to cooperate and achieve common goals [1]. The generation of large volumes of
    data, which have to be processed, stored and presented, lead to the concept of
    Fog Computing. Thus, in order to attain better overall performance and reduce
    latency, a mediation layer between cloud and end-devices, capable of hosting small
    applications, was introduced by Cisco [2]. In fact, scalability was an high motivator
    for the application of IoT to Industry. It became intuitive that using a layered
    architecture and leveraging a cloud computing platform could improve asset performance
    and efficiency by providing a central virtual infrastructure for monitoring and
    analytic tools to be deployed. This results in an end-to-end service-based system
    that can be managed by businesses and accessed by end-users on demand. To achieve
    modularity, the micro-service architecture [3] has been explored. Micro-service
    is a relatively new term in software architecture patterns. Contrary to the modern
    monolithic architecture where an application self-contains all the components,
    micro-services present an approach to develop applications as a set of small independent
    services. Consequently, different parts of the same application can be deployed
    in their own processes, making it possible to decompose huge applications in single
    use cases or services with a specific objective. These different parts are centrally
    managed by a completely separate service. Since each micro-service is relatively
    small, it makes easier to deploy new service versions and dynamically adapt to
    changes. Container-based virtualization is not a new concept. In fact, Docker
    [4] defines a standard for a group of well-known Linux functions creating a lightweight
    virtualization technique. Moreover, Docker is an open platform for developers
    and system administrators to build, share, and run distributed applications. Docker
    containers have two different layers, a Read-Only called image and a Read/Write
    normally named container. An image can be composed of other images, for example,
    we can have a Linux based OS as a base image and install the required dependencies,
    this compilation creates our service image. The instantiation of an image starts
    a container. Therefore, it is possible to create the minimal environment to have
    a service running. With the infrastructure and service model defined the next
    step is orchestration. Having a centralized control over containers which enables
    starting, migrating and terminating services is of uttermost importance. Moreover,
    orchestration allows service policies and permissions to be defined and ensured
    on the different architecture layers. Hence, orchestration can be used by both
    developers and operations for an agile service development, integration and implementation,
    to improve security and even to schedule updates and tasks across layers. In this
    paper, we propose a way of combining Docker and micro-services strategies while
    using a distributed, modular and easily scalable architecture that satisfies key
    requirements for the execution of IIoT applications. Containerization enables
    service isolation, and different Docker tools allow scaling containerized services.
    Modularity and decentralization are achieved trough dividing applications in independent
    micro-services and deploy them across different components of the system. Moreover,
    interoperability between devices and machines can be abstracted using REST-based
    protocols and/or a distributed database at the gateway for communication mediation.
    Developing new services become easy as the architecture is decoupled from technical
    specifications and complexities. Additionally, combining these two features with
    proper testing provide a faster way of development and orchestration. Fig. 1.
    Proposed architecture. Show All The rest of this paper is organized as follows:
    Section II presents a review of the related work. Following that, section III
    describes the proposed architecture. Section IV depicts the implementation of
    the proposed architecture using SBC as both gateway and end-devices and section
    on a use case scenario. Furthermore, while section V presents the experimental
    tests and validation procedures, as well as the obtained results. Finally, section
    VI summarizes the conclusions and future work. SECTION II. Related Work In recent
    years, Docker has been emerging in the Cloud-Computing (CC) world where virtual-machines
    (VM) used to dominate. A number of research studies have been conducted to compare
    their overall performance. In [5] an in-depth comparison of different virtualization
    methods was made, it was concluded that Docker presents minimal overhead in terms
    of CPU, memory, storage and network performance. This has motivated researchers
    to study Docker suitability in different scenarios. For instance, Docker was tested
    as a Platform as a Service for CC [6]. Docker was also analysed in terms of deployment,
    management and fault tolerance of services [7]. Both authors concluded that Docker
    could be a suitable candidate for edge computing. Docker has also been used in
    [8] to create a Gateway for IoT. In this study, gateway functionalities are extended
    from the normal protocol conversion and traffic handling. In particular, this
    study shows how to efficiently use Docker containers to customize the IoT platform,
    by offering data processing services at the Edge. In [9] the authors have created
    a docker based CPS with real-time constraints. A Single Board Computer (SBC) running
    Docker was combined with a micro-controller to create an automated guided vehicle.
    In this work, only the end-devices were contemplated, although it became clear
    that docker could be used alongside physical systems. Recent micro-service based
    architecture has been tested in IoT scenarios. For example, [10] presents an architecture
    for Smart Buildings where sensors share data with micro-services running on SBC.
    Authors concluded that the flexibility and scalability enabled by the isolated
    services can benefit IoT deployments. From previous presented work, it is reasonable
    to conclude that Docker is a suitable candidate for an IIoT virtualization technique.
    Different architectures were presented but none of these provide a solution considering
    end-devices as CPS capable of running containerized services. It is reasonable
    to assume that with the evolution of CPS the sensing layer will also have enough
    computational resources to host lightweight virtualization. SECTION III. Proposed
    Architecture Figure 1 shows the proposed architecture. As can be seen in the figure,
    each component is embedded with a Containerization Software (Docker). Applications
    are divided in small services and implemented inside containers. Each service
    depicts a different use case and runs as a independent containerized micro-service.
    Containers can be grouped, managed and scaled by a manager running in the Enterprise
    System. The architecture is composed of three different layers, each growing in
    computational resources. The first layer can interact with the surrounding environment
    by sensing or monitoring physical systems and/or communicating with the end-user.
    All collected data is temporarily stored in the border gateway defining a new
    layer. And therefore, Gateways are having a strategic position that is ideal for
    local data processing and simple analytics. Moreover, they are used to mediate
    communication between layers, thus reducing network complexity by working as a
    single entry/exit point. Furthermore, the enterprise tier works as the union point
    for different local domains. In addition, this layer is for central storage and
    a controlling point for the subsequent layers. Fig. 2. Test scenario for orchestration.
    Show All A. Sensing Layer End-devices are presented as CPS capable of performing
    lightweight virtualization. This extra layer of computing power allows micro-services
    to be hosted inside containers. Since containers are managed by the gateway and
    orchestrated at the enterprise tier, a new level of adaptability is achieved.
    Services can be deployed on demand or scaled to ensure system performance. Although
    service monitoring take place on the mediation layer, the enforcement of sensor
    correct behaviour is made locally by the CPS. To achieve a most efficient response
    data transformation services have to be combined with behaviour monitoring and
    sensor management. Also, containerized software defined switches (SD-SW) can be
    deployed for packet forwarding optimization. B. Mediation Layer Exploiting the
    fog computing paradigm, the major role of the gateway is to reduce the gap between
    end-nodes and the cloud. Moreover, in data-driven systems, preventing data loss
    is a major concern. Thus, an effective network control and data-management system
    is required. In our proposal, GWs work has a P2P system, hosting three different
    distributed micro-services. An SDN controller, a database and a machine learning
    unit (MLU). SDN is a networking paradigm that enables network devices to be centrally
    controlled. An independent communication channel is used to separate the control
    layer from the data layer. In our topology, this separation is achieved by defining
    a specific port, or by having an independent physical connection. Moreover, an
    SDN controller will manage all containerized SD-SWs which are either deployed
    on the GWs or end-devices. For mediating the communication between different services,
    controllers translate enterprise system rules to network flows which are inserted
    into switches. Following the rules instantiated by the controller, SD-SWs perform
    packet switching for the different micro-services running on the host. These containers
    will manage both connectivity and packet forwarding enhancing network control.
    The MLU provides local intelligence to the gateway which can be used to meet and
    deploy real-time requirements. The MLU can communicate directly with the controller
    and the distributed database to perform behaviour analysis. This unit receives
    trained data from the enterprise layer and compares it to data on other components.
    It can be used to perceive local system erroneous behaviour, verify different
    components responsiveness and ensure that micro-services are complying with system
    politics. The micro-service architecture relies on fully independent services
    working together. In terms of scalability, using a database for communication
    between different services can be an hindrance. To allow simultaneously reading
    and writing by multiple micro-services a distributed database is used and deployed
    across different GWs. Consistency can be tuned for specific use-cases, but it
    can be eventually sacrificed to ensure minimal data loss and fast access to data.
    C. Enterprise Layer In comparison to gateways where the primary requirement is
    speed, enterprise systems work with enormous quantities of unstructured data and
    therefore consistency is a key requirement. Composed by several large computers,
    the enterprise layer is used for more CPU/memory demanding services. Furthermore,
    it has a centralized position on the architecture, and consequently a functional
    viewpoint. Hence, it has three main objectives: data warehousing, data treatment
    and business logistics. In this regard, by consuming large volumes of data stored;
    data modeling, system optimization, and behaviour prognosis can be achieved. In
    fact, through data-mining techniques, the behaviour of different system parts
    is modeled. Therefore, the results obtained can be shared with the MLUs for a
    supervised control of the running services. Enterprise Systems are also the main
    management and control entity. For a more granulated control enterprise politics
    are translated into orchestration directives, or/and SDN flows. This rules are
    defined, instantiated, disseminated and enforced across gateways. Thus, a manager
    instance ensures that business logistics and network policies are enforced. The
    MLU can tune the policies to increase performance. Fig. 3. Published and subscribed
    data per second Show All D. Key Characteristics The industry is always adapting,
    software is being improved and customization is an hindrance for monolithic platforms.
    In fact, adding new features would normally imply completely re-building the whole
    system. The modularity provided by the proposed architecture combined with lightweight
    virtualization orchestration supplied by Docker simplifies management and enables
    distributed deployments. In real-time scenarios where resilience is crucial and
    fault-tolerance essential, having independent services as base-blocks of applications
    help developers to meet these requirements. In fact, the application logic is
    distributed across different devices and therefore a single micro-service or even
    device failure can have no effect on system performance. Moreover, reliability
    can be achieved through the application of orchestration rules which can ensure
    service recovery in case of failure. Overall, resilience can be improved by providing
    redundancy at different layers. SECTION IV. Test Case Scenario Figure 2 depicts
    a test use case for the proposed architecture. The purpose of this example scenario
    is to demonstrate the capabilities of the architecture presented for IIoT deployments.
    More specifically, a time-sensitive application where end-devices are highly dependent
    on cloud input data. In this scenario, three Raspberry Pi B+ (RPi) were used with
    Hypriot Operating System [11]. RPis are SBCs with ARM CPUs and 1 GB RAM memory.
    Hypriot is a minimal operating system for RPi running Docker. Additionally, two
    SH5461AS 7-segment displays were used to display enterprise data. Each end-device
    is a RPi connected to one 7-segment display. End-devices are wired to the GW which
    communicates with the Cloud wirelessly using Wi-Fi. The Cloud is an Intel core
    i5 and 8 GB memory, laptop computer running Archlinux OS and Docker. Docker version
    1.12 was used to create this topology. Docker 1.12 has a embedded orchestration
    tool named Swarm which has two types of roles, workers and managers. A worker
    can be used for hosting containers while a manager can terminate, deploy and manage
    running containers. Fig. 4. Comparison between cloud and end-device time in milliseconds
    Show All The service created for this experiment is publish/subscribe based. The
    service was decomposed in three different micro-services, publisher, storage and
    subscriber. The publisher is a script written in python for publishing the current
    epoch time to the database every 2 ms. Storage is a Redis distributed database,
    with a REST interface for publishing data. The subscriber is also written in python
    and uses a Redis client for subscribing to data changes. This data is then translated
    and presented in the 7-segment displays. SECTION V. Results and Discussion To
    evaluate the proposed architecture, we present and discuss the results obtained
    while orchestrating a new containerized micro-service to the scenario depicted
    in the last section. For this experiment, all the docker images used were already
    stored in the local repository and a Redis database instance was running on the
    gateway. The test had a duration of 1 minute and begins with the deployment of
    publisher and a subscriber instance. Twenty seconds after starting, the micro-services
    running in the CPS are stopped and started in the remaining end device. In order
    to have both clocks displaying the current time, the first container is restarted
    after 15 seconds. To analyse the system behaviour, all data published to redis
    is also written to a file on the Enterprise System. Additionally after updating
    the 7-segment display, each subscriber writes the time displayed to a local file.
    By comparing the different files we can see the number of times data has been
    published, and the number of times the clock was updated. The results are depicted
    in figures 3 and 4. Figure 3 presents the number of times data is written to each
    local file. We can see that on average the publisher writes 362 times per second
    while the subscriber updates the clock an average of 220 per second. The letters
    A and B mark the time interval taken to start the two micro-services. This time
    includes both building and deploying of the container. In short, this means that
    the docker image had to be downloaded from the registry, compiled and used to
    create a new container. In this context, and with a better computing power, the
    publisher started in 3.89 seconds. In contrast, the SBC took almost 9 seconds,
    to start a service with similar size. The interval depicted as C is the migration
    of service between CPS. It takes almost 1.8 seconds to terminate the service and
    due to the fact that the image was already built the time to start a new instance
    decreased significantly. Thus, restarting a micro-service (D), takes only 876
    ms because both building and deployment were already made. Figure 4 presents the
    time gap between enterprise layer and the end-device layer. This interval is the
    difference between the data received by the publisher and the system time. It
    can be seen that, on average, the seven-segment displays were always delayed in
    281 milliseconds. Although significantly delayed, for this use case, where only
    hours, minutes and seconds are presented on the displays it didn''t affect the
    end-user experience. SECTION VI. Conclusions Going beyond the existing state-of-the-art
    work, in this paper, we proposed an architecture considering end-devices as CPS
    capable of running containerized services. In the proposed architecture, each
    component has embedded docker, applications (divided in small services) and implemented
    inside containers. The architecture is composed of three different layers named
    sensing, mediation and enterprise layer. Devices performing sensing and operations
    are represented as sensing layer, the intermediate layer representing GWs is characterized
    as the mediation layer while the enterprise layer is used for more CPU/memory
    demanding services. By adopting the proposed architecture, reliability can be
    achieved through the application of orchestration rules which can ensure service
    recovery in case of failure and system resilience can be improved by including
    redundancy at different layers. The proposed architecture was tested via a use
    case scenario in which a time-sensitive application was deployed where end-devices
    are highly dependent on cloud input data. The obtained results showed that the
    enterprise layer have management and control capabilities that ensure application
    deployment through orchestration tools. The results also prove that the implementation
    of the proposed architecture can deploy the time-dependent micro-services for
    IIoT. The presented architecture can also be extended to resource-constrained
    devices, therefore, it is crucial to reduce system overhead. This can be achieved
    by reducing the docker images used and adopting different protocols for the inter-microservice
    communication. In this work, we used Hypertext Transfer Protocol (HTTP) as a Representational
    State Transfer (REST) interface for the database microservice. We are currently
    working on changing it to CoAP (Constrained Application Protocol) hoping to improve
    the communication. In addition, the programming language used was Python which
    is also being changed to C in order to reduce the docker image size. ACKNOWLEDGMENTS
    This work is funded by National Funds through FCT - Fundação para a Ciência e
    a Tecnologia, Portugal, under the project UID/EEA/50008/2013. Authors Figures
    References Citations Keywords Metrics More Like This Lightweight Virtualization
    Approaches for Software-Defined Systems and Cloud Computing: An Evaluation of
    Unikernels and Containers 2019 Sixth International Conference on Software Defined
    Systems (SDS) Published: 2019 Virtualization on Internet of Things Edge Devices
    With Container Technologies: A Performance Evaluation IEEE Access Published: 2017
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Orchestration of containerized microservices for IIoT using Docker
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/3219104.3229280
  analysis: '>'
  authors:
  - Pankaj Saha
  - Angel Beltre
  - Piotr W. Uminski
  - Madhusudhan Govindaraju
  citation_count: 40
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesPEARCProceedingsPEARC
    ''18Evaluation of Docker Containers for Scientific Workloads in the Cloud RESEARCH-ARTICLE
    SHARE ON Evaluation of Docker Containers for Scientific Workloads in the Cloud
    Authors: Pankaj Saha , Angel Beltre , Piotr Uminski , + 1 Authors Info & Claims
    PEARC ''18: Proceedings of the Practice and Experience on Advanced Research ComputingJuly
    2018Article No.: 11Pages 1–8https://doi.org/10.1145/3219104.3229280 Published:22
    July 2018Publication History 38 citation 833 Downloads eReaderPDF PEARC ''18:
    Proceedings of the Practice and Experience on Advanced Research Computing Evaluation
    of Docker Containers for Scientific Workloads in the Cloud Pages 1–8 Previous
    Next ABSTRACT References Cited By Index Terms Recommendations Comments ABSTRACT
    The HPC community is actively researching and evaluating tools to support execution
    of scientific applications in cloud-based environments. Among the various technologies,
    containers have recently gained importance as they have significantly better performance
    compared to full-scale virtualization, support for microservices and DevOps, and
    work seamlessly with workflow and orchestration tools. Docker is currently the
    leader in containerization technology because it offers low overhead, flexibility,
    portability of applications, and reproducibility. Singularity is another container
    solution that is of interest as it is designed specifically for scientific applications.
    It is important to conduct performance and feature analysis of the container technologies
    to understand their applicability for each application and target execution environment.
    This paper presents a (1) performance evaluation of Docker and Singularity on
    bare metal nodes in the Chameleon cloud (2) mechanism by which Docker containers
    can be mapped with InfiniBand hardware with RDMA communication and (3) analysis
    of mapping elements of parallel workloads to the containers for optimal resource
    management with container-ready orchestration tools. Our experiments are targeted
    toward application developers so that they can make informed decisions on choosing
    the container technologies and approaches that are suitable for their HPC workloads
    on cloud infrastructure. Our performance analysis shows that scientific workloads
    for both Docker and Singularity based containers can achieve near-native performance.
    Singularity is designed specifically for HPC workloads. However, Docker still
    has advantages over Singularity for use in clouds as it provides overlay networking
    and an intuitive way to run MPI applications with one container per rank for fine-grained
    resources allocation. Both Docker and Singularity make it possible to directly
    use the underlying network fabric from the containers for coarsegrained resource
    allocation. References Abdulrahman Azab. 2017. Enabling Docker Containers for
    High-Performance and Many-Task Computing. In 2017 IEEE International Conference
    on Cloud Engineering (IC2E). IEEE, 279--285. Minh Thanh Chung, An Le, Nguyen Quang-Hung,
    Due-Dung Nguyen, and Nam Thoai. 2016. Provision of Docker and InfiniBand in High
    Performance Computing. In 2016 International Conference on Advanced Computing
    and Applications (ACOMP). IEEE, 127--134. Benjamin Hindman, Andy Konwinski, Matei
    Zaharia, Ali Ghodsi, Anthony D. Joseph, Randy Katz, Scott Shenker, and Ion Stoica.
    2011. Mesos: a platform for fine-grained resource sharing in the data center.,
    295--308 pages. http://dl.acm.org/citation.cfm?id=1972488 Show All References
    Cited By View all Wang H, Li C, Li Z, Zhang L and Chen Z. (2024). Research and
    implementation of unified access technology in the field of health and elderly
    care. Entertainment Computing. 10.1016/j.entcom.2024.100634. 49. (100634). Online
    publication date: 1-Mar-2024. https://linkinghub.elsevier.com/retrieve/pii/S1875952124000028
    Bonde B. (2024). Edge, Fog, and Cloud Against Disease: The Potential of High-Performance
    Cloud Computing for Pharma Drug Discovery. High Performance Computing for Drug
    Discovery and Biomedicine. 10.1007/978-1-0716-3449-3_8. (181-202). https://link.springer.com/10.1007/978-1-0716-3449-3_8
    Medeiros D, Wahlgren J, Schieffer G and Peng I. (2023). Kub: Enabling Elastic
    HPC Workloads on Containerized Environments 2023 IEEE 35th International Symposium
    on Computer Architecture and High Performance Computing (SBAC-PAD). 10.1109/SBAC-PAD59825.2023.00031.
    979-8-3503-0548-7. (219-229). https://ieeexplore.ieee.org/document/10306010/ Show
    All Cited By Index Terms Evaluation of Docker Containers for Scientific Workloads
    in the Cloud General and reference Cross-computing tools and techniques Performance
    Hardware Communication hardware, interfaces and storage Networking hardware Software
    and its engineering Software notations and tools Development frameworks and environments
    Application specific development environments Recommendations A performance comparison
    of linux containers and virtual machines using Docker and KVM Abstract Virtualization
    is a foundational element of cloud computing. Since cloud computing is slower
    than a native system, this study analyzes ways to improve performance. We compared
    the performance of Docker and Kernel-based virtual machine (KVM). KVM ... Read
    More Combining containers and virtual machines to enhance isolation and extend
    functionality on cloud computing Abstract Virtualization technology is the underlying
    element of cloud computing. Traditionally, cloud computing has employed virtual
    machines to distribute available resources and provide isolated environments among
    users. Multiple virtual ... Highlights We study the combination of virtual machines
    and containers in the cloud. Read More Performance comparison of multi-container
    deployment schemes for HPC workloads: an empirical study Abstract The high-performance
    computing (HPC) community has recently started to use containerization to obtain
    fast, customized, portable, flexible, and reproducible deployments of their workloads.
    Previous work showed that deploying an HPC workload into a ... Read More Comments
    12 References View Table Of Contents Footer Categories Journals Magazines Books
    Proceedings SIGs Conferences Collections People About About ACM Digital Library
    ACM Digital Library Board Subscription Information Author Guidelines Using ACM
    Digital Library All Holdings within the ACM Digital Library ACM Computing Classification
    System Digital Library Accessibility Join Join ACM Join SIGs Subscribe to Publications
    Institutions and Libraries Connect Contact Facebook Twitter Linkedin Feedback
    Bug Report The ACM Digital Library is published by the Association for Computing
    Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics
    Feedback'
  inline_citation: '>'
  journal: Proceedings of the Practice and Experience on Advanced Research Computing
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Evaluation of Docker Containers for Scientific Workloads in the Cloud
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ic2e.2019.00034
  analysis: '>'
  authors:
  - Joy Rahman
  - Palden Lama
  citation_count: 30
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 IEEE International Confe...
    Predicting the End-to-End Tail Latency of Containerized Microservices in the Cloud
    Publisher: IEEE Cite This PDF Joy Rahman; Palden Lama All Authors 26 Cites in
    Papers 1792 Full Text Views Abstract Document Sections I. Introduction II. Background
    on Microservice Architecture III. Related Work IV. Platform V. Performance Characterization
    Show Full Outline Authors Figures References Citations Keywords Metrics Abstract:
    Large-scale web services are increasingly adopting cloud-native principles of
    application design to better utilize the advantages of cloud computing. This involves
    building an application using many loosely coupled service-specific components
    (microservices) that communicate via lightweight APIs, and utilizing containerization
    technologies to deploy, update, and scale these microservices quickly and independently.
    However, managing the end-to-end tail latency of requests flowing through the
    microservices is challenging in the absence of accurate performance models that
    can capture the complex interplay of microservice workflows with cloudinduced
    performance variability and inter-service performance dependencies. In this paper,
    we present performance characterization and modeling of containerized microservices
    in the cloud. Our modeling approach aims at enabling cloud platforms to combine
    resource usage metrics collected from multiple layers of the cloud environment,
    and apply machine learning techniques to predict the end-to-end tail latency of
    microservice workflows. We implemented and evaluated our modeling approach on
    NSF Cloud''s Chameleon testbed using KVM for virtualization, Docker Engine for
    containerization and Kubernetes for container orchestration. Experimental results
    with an open-source microservices benchmark, Sock Shop, show that our modeling
    approach achieves high prediction accuracy even in the presence of multi-tenant
    performance interference. Published in: 2019 IEEE International Conference on
    Cloud Engineering (IC2E) Date of Conference: 24-27 June 2019 Date Added to IEEE
    Xplore: 08 August 2019 ISBN Information: DOI: 10.1109/IC2E.2019.00034 Publisher:
    IEEE Conference Location: Prague, Czech Republic SECTION I. Introduction Large-scale
    web services (e.g Netflix, Microsoft Bing, Uber, Spotify etc.) are increasingly
    adopting cloud-native principles and design patterns such as microservices and
    containers to better utilize the advantages of the cloud computing delivery model,
    which includes greater agility in software deployment, automated scalability,
    and portability across cloud environments [24], [30]. In a micro-services architecture,
    an application is built using a combination of loosely coupled and service-specific
    software containers that communicate using APIs, instead of using a single, tightly
    coupled monolith of code. This development methodology combined with recent advancements
    in containerization technologies makes an application easier to enhance, maintain,
    and scale. However, it is challenging to manage the end-to-end tail latency (e.g
    95thpercentile latency) of requests flowing through the microservice architecture,
    which could result in poor user experiences and loss of revenue [32], [46]. Containerized
    microservices deployed in a public cloud are scaled automatically based on user-specified
    static thresholds for per-microservice resource utilization [1], [2], [6]. However,
    this places a significant burden on application owners who are concerned about
    the end-to-end tail latency (e.g 95thpercentile latency) [28]. Setting appropriate
    resource utilization thresholds on various microservices to meet the end-to-end
    tail latency in such complex distributed system is difficult and error-prone in
    the absence of accurate performance models. There are many challenges in modeling
    the end-to-end tail latency of containerized microservices. First, a microservice
    architecture is characterized by complex request execution paths spanning many
    microservices forming a directed acyclic graph (DAG) with complex interactions
    across the service topology [28], [29], [39]. Second, the tail latency is highly
    sensitive to any variance in the system which could be related to application,
    OS or hardware [32]. Third, in a cloud environment where microservices run as
    containers hosted on a cluster of virtual machines (VMs), application performance
    can degrade often in unpredictable ways [18], [21], [24], [44]. Traditionally,
    analytical models based on queuing theory have been widely applied for performance
    prediction and resource provisioning of monolithic (3-tier) applications [40],
    [41]. However, such techniques can become intractable when dealing with the scale
    and complexity of microservice architecture, and the presence of cloud-induced
    performance variability. Furthermore, analytical modeling is a white-box approach
    that often requires intrusive instrumentation of application code for workload
    profiling and expert knowledge about the application structure and data flow between
    various components [25]. Such approach can be impractical from a cloud provider''s
    perspective since customer applications appear with limited visibility to the
    cloud providers. There are black-box modeling approaches that relate observable
    resource usage metrics [36], [42] or resource allocation metrics [43] with the
    performance of monolithic applications hosted in virtualized computing environments.
    More recent studies [19], [26] focused on runtime trace analysis tools and simulation
    based approaches to analyze the performance of microservice-based applications.
    However, none of these works study the impact of cloud induced performance interference
    on microservice-based applications, and the resulting inaccuracies in performance
    modeling. In this paper, we observe that the end-to-end tail latency of microservice
    workflows are highly sensitivity to performance interference in the cloud. Furthermore,
    we show that the tail latency of microservice workflows can be accurately predicted
    even in the presence of performance interference, with the help of machine learning
    and multi-layer data collected from the cloud environment. In particular, we make
    the following contributions. We quantify the impact of resource utilization and
    performance interference experienced by various microservices on the end-to-end
    tail latency of various request workflows in a web application. Since CPU is a
    major bottleneck for most web applications, we use CPU utilization as a resource
    metric in this paper, and focus on the performance interference caused by the
    contention in shared processor resources such as LLC (last level cache) and memory
    bandwidth. However, our approach can be easily extended to include other resource
    metrics. We propose a modeling approach that combines multilayer data including
    container-level, VM level and a hardware performance counter based metric, CPI
    (clock cycles per instruction), to accurately predict end-to-end tail latency
    in the presence of performance interference in the cloud. We apply several machine
    learning based modeling techniques, and compare their accuracy in predicting the
    end-to-end performance for containerized microservices. We demonstrate the feasibility
    of utilizing the proposed performance models in making efficient resource scaling
    decisions. For this purpose, we formulate resource scaling of microservices as
    a constrained nonlinear optimization problem, and solve it to calculate appropriate
    resource utilization thresholds on various microservices, so that they can be
    scaled efficiently to meet a performance SLO (service level objective) target.
    We implement and evaluate the proposed techniques using a representative microservices
    benchmark, Sock Shop [14], using the NSF Chameleon cloud [3] testbed. The Sock
    Shop benchmark is containerized with Docker [35] and deployed in a cluster of
    VMs managed by Kubernetes [8] an open-source container orchestration engine. The
    rest of this paper is organized as follows. Section II provides the background
    on microservice archiecture. Related work are discussed in Section III. Section
    IV describes the testbed setup and benchmarks used. Section V presents the performance
    characterization of containerized microservices. Section VI provides the performance
    modeling approach. Section VII discusses resource scaling optimization based on
    the proposed models. Section VIII concludes the paper. Figure 1: Monolithic vs
    microservice architecture. Show All SECTION II. Background on Microservice Architecture
    Microservice architecture aims to overcome various limitations of traditional
    monolithic architecture for software development [10], [22]. Figure 1 illustrates
    the difference between multi-tier monolithic architecture and microservice architecture
    in the context of an e-commerce application that takes orders from customers,
    verifies product catalogue, processes payment and ships orders. In monolithic
    architecture, the web application is divided into technology-specific tiers such
    as a frontend web tier for serving web contents, an application tier composed
    of numerous tightly coupled components for implementing the entire business logic,
    and a shared database tier for data persistence. A monolithic application is often
    simple to design. However, in order to update one component, the entire application
    has to be redeployed. Furthermore, each component within a tier cannot be scaled
    independently based on its resource requirements. On the other hand, microservice
    architecture splits the application into many smaller self-contained components,
    called microservices, that serve specific business functions and communicate with
    each other via lightweight language-agnostic APIs. Each microservice has its own
    code and database without any shared component with other services. This facilitates
    flexibility in application deployment and enhanced scalability since each component
    of an application can be updated and scaled independently. In essence, microservice
    architecture is a variant of the Service-Oriented Architecture (SOA) that emphasizes
    fine-grained services and lightweightness. SECTION III. Related Work Performance
    modeling and dynamic resource provisioning of Internet applications has been an
    important research topic for many years [31], [36], [37], [40], [41], [43], [45].
    There are traditional analytical modeling approaches based on queueing theory
    [40], [41], and hybrid approaches that combine queueing theory with machine learning
    techniques [38], [45]. Urgaonkar et al. [41] designed a dynamic server provisioning
    technique on multi-tier server clusters. The technique decomposes the per-tier
    average delay targets to be certain percentages of the end-to-end delay constraint.
    Singh et al. [38] applied k-means clustering algorithm and a G/G/1 queuing model
    to predict the server capacity for a given workload mix. Although these approaches
    were effective for multi-tier monolithic applications, they can become intractable
    when dealing with complex microservice architecture in a cloud environment. The
    complexity introduced by having many moving parts with complex interactions and
    the presence of cloud-induced performance variability [21], [44] pose significant
    challenges in modeling the system behavior, identifying critical resource bottlenecks
    and managing them effectively. Blackbox modeling techniques have been widely adopted
    in cluster resource allocation and management [31], [36], [42], [43]. Nguyen et
    al. [36] applied online profiling and polynomial curve fitting to provide a black-box
    performance model of the applications SLO violation rate for a given resource
    pressure. Wajahat et al. [42] presented an application-agnostic, neural network
    based auto-scaler for minimizing SLA violations of diverse applications. Wang
    et al. [43] applied fuzzy model predictive control and Lama et al. [31] proposed
    self-adaptive neural fuzzy control techniques for dynamic resource management
    of monolithic cloud applications. However, these studies do not address the modeling
    inaccuracies caused by the performance interference in the cloud, and the complexity
    introduced by microservice architecture. A few studies have focused on managing
    the end-to-end performance objectives of large-scale web services and analyzing
    their complex performance behavior [27], [28], [39]. Guo et al. [27] highlighted
    how the complex interactions between various components of large-scale web services
    not only lead to sharp degradation in performance, but also trigger cascading
    behaviors that result in wide-spread application outages. Jalaparti et al. [28]
    presented Kwiken, a framework that decomposes the problem of minimizing latency
    over a general processing DAG in a large web service into a manageable optimization
    over individual stages. Suresh et al. [28] presented Wisp, a resource management
    framework that applies a combination of techniques, including estimating local
    workload models based on measurements of immediate neighborhoods, distributed
    rate control and metadata propagation to achieve end-to-end throughput and latency
    objectives in Service-Oriented architectures. These approaches are complimentary
    to our work as they focus on solutions that need to be adopted at the application
    layer in the context of cloud computing stack, and requires expert knowledge about
    the application. On the other hand, our performance modeling approach does not
    require intrusive instrumentation of application code for profiling or expert
    knowledge about the data flow between various components. Figure 2: Workflow DAGs.
    Show All SECTION IV. Platform A. Experimental Testbed We set up a cloud prototype
    testbed, which closely resembles real-world cloud platforms such as Google Kubernetes
    Engine [6] and Amazon Elastic Container Services [2]. Our testbed consists of
    a physical layer of bare metal servers, a VM layer built on top of the physical
    layer and a container layer built on top of VM layer. Physical Servers We used
    four bare metal servers leased on NSF Chameleon Cloud[3] testbed. Each server
    was equipped with dual socket Intel Xeon E5-2670 v3 Haswell processors (each with
    12 cores @ 2.3GHz) and 128 GiB of RAM. Each server was connected to a Dell switch
    at 10Gbp, with 40Gbps of bandwidth to the core network from each switch. Vms We
    setup 16 VMs on top of the bare metal servers by using KVM for server virtualization.
    Each VM was configured with four vCPUs, 8GB Ram and 30GB disk space. Containers
    We setup a 16 VM Kubernetes cluster for container orchestration and management.
    Docker (version 18.03.1-ce) was used as the container run time engine on each
    VM. Kubernetes pod networking was set up using the Calico CNI (Container Network
    Interface) network plugin [11]. We use the term pod and container interchangeably
    in this paper, since we use a one-container-per-Pod model, which is the most common
    Kubernetes use case. B. Workloads For performance characterization, we used Sock
    Shop [14], an open-source microservices benchmark that is particularly tailored
    for container platforms. Sock Shop emulates an e-commerce website as shown in
    Figure 1 with the specific aim of aiding the demonstration and testing of existing
    microservice and cloud-native technologies. A recent study suggests that Sock
    shop closely reflects how typical microservices applications are currently being
    developed and delivered into production, as reported by practitioners and industry
    experts [17]. We used the Locust tool [9] to generate user traffic for the Sock
    Shop benchmark. The workload traffic is composed of a number of concurrent clients
    that generate HTTP-based REST API calls to Sock Shop. To create a controlled interference
    workload for our experiments, we used the STREAM Memory Bandwidth benchmark[33].
    STREAM is a synthetic benchmark program geared towards measuring memory bandwidth
    (in MB/s) corresponding to computation rate for simple vector kernels. We run
    the benchmark inside a docker container and deploy it as a batch job in Kubernetes.
    Figure 3: Impact of CPU utilization on the tail latency of various workflows.
    Show All Figure 4: Parallel coordinates plot showing the impact of performance
    interference on the multivariate relationship between CPU utilization and end-to-end
    tail latency of orders workflow. Show All SECTION V. Performance Characterization
    One of the challenges that complicate performance characterization of a microservice
    architecture is that request execution workflows can form directed acyclic graph
    (DAG) structures spanning across many microservices. As a result, the end-to-end
    latency of a workflow is impacted by the performance behavior of multiple microservices
    in a complex way. We use the term workflow to represent an application-specific
    group of requests that are associated with a particular API endpoint, which is
    usually in the form of an HTTP URI. For instance, in case of the Sock Shop benchmark
    shown in Figure 1, the HTTP URIs for workflows involved with processing orders
    are [base url: / GET / Orders] and [base url: / POST / Orders]. The exact structure
    of the DAG for request workflows is often unknown, since it depends on multiple
    factors such as the APIs invoked at each encountered microservice, the supplied
    arguments, the content of caches, as well as the use of load balancing along the
    service graph [39]. We used a visualization and monitoring tool, weavescope [16],
    to map the DAG structure of orders and cart workflows as shown in Figure 2. A.
    End-to-End Tail Latency First, we analyze the impact of CPU utilization of individual
    microservices on the end-to-end tail latency of two different workflows viz. orders
    and cart in the Sock Shop benchmark. For this purpose, we run experiments with
    various workload intensities by varying the number of concurrent clients in the
    workload generator from 5 to 50, while setting the total number of generated requests
    to be 50000. We also vary the number of pods allocated to cart, orders and frontend
    microservices to include various combination of scaling configurations. The CPU
    utilization of a particular microservice is measured as the average CPU utilization
    of all the pods allocated to that microservice. As shown in Figures 3 (a), (b)
    and (c) the end-to-end tail latency of various workflows have a non-linear relationship
    with the CPU utilization of individual microservices. We observe that the 95thpercentile
    latency of the two workflows increase significantly even at low CPU utilization
    values of the orders and cart microservices. On the other hand, only high CPU
    utilization values ( >70% ) of the frontend microservice has significant impact
    on the 95thpercentile latency. For example, the tail latency of the orders workflow
    reaches 200 ms at 49%, 57% and 106% CPU utilizations of the orders, cart and frontend
    microservices respectively. Figure 5: Parallel coordinates plot showing the impact
    of performance interference on multivariate relationship between CPU utilization
    and end-to-end tail latency of cart workflow. Show All Figure 6: Impact of performance
    interference on the end-to-end tail latency of various workflows. Show All B.
    Impact of Performance Interference Next, we analyze the impact of performance
    interference in a cloud environment on the multivariate relationship between CPU
    utilization of various microservices and the end-to-end tail latency of particular
    request workflows. For the sake of clarity, we present our analysis using top
    four microservices from the Sock Shop benchmark ranked according to their CPU
    utilization values. To induce performance interference, we colocate pods running
    the memory-intensive STREAM [33] benchmark on the VMs that host the pods running
    cart and frontend microservices respectively. The intensity of interference is
    fixed by running four pods for each interfering workload. The workload intensities
    and the scaling configurations for orders, cart and frontend microservices are
    varied similar to the previous experiment. As shown in Figures 4 (a), (b) and
    (c), the end-to-end tail latency of the orders workflow is influenced by the CPU
    utilization of multiple microservices. However, their multivariate relationship
    changes significantly depending on the performance interference experienced by
    various microservices. For example, in the case of no interference, the 95thpercentile
    latency of orders workflow is greater than 300 ms when the CPU utilization measured
    at cart, frontend, orders and user microservices are 67%, 110%, 55% and 41% respectively.
    However, similar tail latency of orders workflow was observed at much lower CPU
    utilization values when one of the microservices experienced performance interference.
    Similar results were obtained for the cart workflow as shown in Figures 5 (a),
    (b) and (c). This implies that the CPU utilization of microservices measured at
    the pod level are insufficient in accurately predicting the end-to-end tail latency
    of various workflows. Figure 6 shows the distribution of the 95thpercentile latency
    of various workflows under three different scenarios, i.e with interference on
    cart, interference on frontend and without interference. The variation in the
    latency observed within each case is mainly due to the varying workload intensities
    in these experiments. On average the performance degradation observed by orders
    and cart workflows due to interference on cart microservice are 22% and 79% respectively.
    On the other hand, the average performance degradation of the two workflows due
    to interference on frontend microservice are 6% and 18% respectively. These results
    demonstrate the complex interplay between performance interference, inter-service
    performance dependency and the end-to-end tail latency of various workflows. SECTION
    VI. Performance Modeling with Machine Learning In this section, we present our
    approach to address the challenges of predicting the end-to-end tail latency of
    complex workflows in a microservice architecture in the face of diverse performance
    interference patterns. Our approach combines the resource usage metrics at the
    container/pod level with VM level resource usage and hardware performance counter
    values to construct machine learning (ML) based performance models for individual
    workflows. Our modeling approach does not rely on any expert application knowledge.
    Hence, it can be easily extended to fit the need of diverse applications. A. Data
    Collection In this paper, we use CPU utilization as a resource metric for the
    microservices since CPU is a major resource bottleneck in most web applications.
    We use docker stats [4] to measure pod level CPU utilization. To capture the impact
    of performance interference due to the contention of processor resources, such
    as the last level cache (LLC) and memory bandwidth, we utilize the CPU utilization
    and CPI metric associated with the VMs that host the various microservices as
    pods. We use the virt top [15] tool to measure VM level CPU utilization. CPI is
    measured on a per cgroup basis by using the perf event [23] tool and each cgroup
    is mapped to a VM. For data collection, we conduct extensive experiments on our
    cloud prototype testbed by varying the number of concurrent clients, and the performance
    interference levels experienced by different microservices in the Sock Shop benchmark.
    We also vary the number of pods allocated to the microservices. For each experiment,
    we measure the end-to-end tail latency of various workflows as reported by the
    Locust [9] tool. The collected data is used to train our machine learning based
    performance models. B. Machine Learning Models We build performance models for
    predicting the end-to-end tail latency of each microservice workflow by applying
    various machine learning (ML) techniques including Linear Regression (LR), Support
    Vector Regression (SVR), Decision Tree (DT), Random Forrest (RF) and a deep Neural
    Network (NN) based regression (more specifically a multilayer perceptron with
    multiple hidden layers). The ML models are built and trained by using scikit-learn
    [12], a machine learning library in Python. Feature Selection The input features
    of our ML models include the number of concurrent clients, pod-level resource
    metrics and VM-level resource metrics. The pod-level metrics include the average
    CPU utilization of load-balanced pods for each microservice. The VM-level metrics
    include the CPU utilization or the CPI of VMs that host the pods. To reduce our
    feature space and avoid potential overfitting issues, we apply a popular feature
    selection technique called stability selection [34]. In particular, we use scikit-learn
    [12] library''s randomized lasso technique, which works by subsampling the training
    data and computing a Lasso estimate where the penalty of a random subset of coefficients
    has been scaled. By performing this operation several times, the method assigns
    high scores to features that are repeatedly selected across randomizations. The
    features selected for the orders workflow are the number of concurrent clients,
    pod-level CPU utilization of the microservices including frontend, orders, users,
    shipping, payment, cart, users-db, orders-db, cart-db, and the CPU utilization
    or CPI of the VMs that host these microservices. Similarly, the features selected
    for the cart workflow are the number of concurrent clients, the pod-level CPU
    utilization of the microservices including front-end, orders, cart, cart-db, and
    the CPU utilization or CPI of the VMs that host these microservices. Figure 7:
    Prediction accuracy of various ML models for orders. Show All Figure 8: Prediction
    accuracy of various ML models for cart. Show All Figure 9: Cross-validated predictions
    of tail latency in orders workflow. Show All Table I: Optimal number of neurons
    in the three hidden layers of NN models for orders and cart workflow Hyper-Parameters
    The hyper-parameters of each model is set to the default values provided by scikit-learn.
    We observe that the prediction accuracy of the deep NN model is highly sensitive
    to the number of hidden layers and the size (number of neurons) in each hidden
    layer. Hence, we tuned these parameters through an exhaustive search for various
    combinations of input feature space and the targeted workflow for the prediction
    of end-to-end tail latency. The optimal number of hidden layers for our NN model
    is three, and the optimal number of neurons in these three hidden layers is summarized
    in Table I. C. Prediction Accuracy In this section, we evaluate the prediction
    accuracy of various ML models (LR, SVR, DT, RF, NN) and three modeling approaches.
    First, the Pod_CPU approach includes pod-level CPU utilization metrics in the
    input feature space. Second, the Pod_CPU+VM_CPU approach includes both pod-level
    and VM-level CPU utilization metrics. Third, the Pod_CPU+VM_CPI approach includes
    pod-level CPU utilization and VM-level CPI metrics in the input feature space.
    The models are evaluated with 10-fold cross validation on the collected dataset.
    As a result, 90% of data is used for training, 10% of data is used for testing
    in each of the 10 iterations of cross-validation. We utilize commonly used metrics
    such as the mean absolute percentage error (MAPE) and the coefficient of determination,
    R 2 . MAPE is calculated as 1 n ∑ n i=1 ∣ ∣ y− y ^ y ∣ ∣ where y and y ^ are the
    measured and predicted values of the end-to-end tail latency respectively. R 2
    is a statistical measure of how well the regression predictions approximate the
    real data points. An R 2 of 1 indicates that the regression predictions perfectly
    fit the data. Figures 7 (a) and (b) show that, compared to the Pod_CPU based modeling
    approach, Pod_CPU+VM_CPU and Pod_CPU+VM_CPI approaches achieve significant improvement
    in the prediction accuracy of each ML model for the orders workflow. This is because
    VM-level CPU utilization can capture inter-pod CPU contention within a VM. Furthermore,
    VM-level CPI metric can capture the contention of shared processor resources between
    multiple pods within a VM as well as across VMs. Such inter-VM resource contention
    may arise when the concerned VMs are colocated in the same physical machine. The
    improvement in the prediction accuracy in terms of MAPE due to Pod_CPU+VM_CPU
    and Pod_CPU+VM_CPI approaches are up to 36% and 38% respectively. The largest
    improvement is observed in case of the NN model. We also observe that the NN model
    outperforms all other models in prediction accuracy since the Neural Network is
    a universal function approximator. On the other hand, the LR model shows the worst
    prediction accuracy. This is because a linear regression model can not capture
    the non-linearity of tail latency. Overall, we observed similar results in the
    latency prediction of cart workflow as shown in Figure 8. Figure 9 plots the cross-validated
    predictions vs. the measured values of end-to-end tail latency of the orders workflow
    in order to graphically illustrate the different R 2 values for the LR and NN
    models. Theoretically, if a model could explain 100% of the variance in the observed
    data, the predicted values would always equal the measured values and, therefore,
    all the data points would fall on the fitted regression line. The more variance
    that is accounted for by the regression model the closer the data points will
    fall to the fitted regression line. The proportion of variance accounted for by
    the LR model with Pod_CPU, LR model with Pod_CPU+VM_CPI, NN model with Pod_CPU
    and NN model with Pod_CPU+VM_CPI approaches are 42%, 66%, 71% and 89% respectively.
    SECTION VII. Optimization for Resource Scaling Although existing cloud platforms
    [1], [2], [5], [6] provide mechanisms for auto-scaling microservices, they expect
    application owners to specify thresholds for various microservice load metrics
    to enable auto-scaling features. For example, the auto-scaling feature [7] in
    Kubernetes determines the allocation of containers/pods to a microservice by using
    the formula: desiredReplicas=⌈currentReplicas∗ currentMetricValue desiredMetricValu
    ⌉ (1) View Source Table II: Notation used in resource scaling optimization problem
    If the desiredMetricValue (threshold) is specified as an average CPU utilization
    of 50% for a particular microservice, and the current average CPU utilization
    is 100%, then the number of pods allocated to that microservice will be doubled.
    Furthermore, any scaling is performed only if the ratio of currentMetricValue
    and desiredMetricValue drops below 0.9 or increases above 1.1 (10% tolerance by
    default). It is challenging and burdensome for application owners to determine
    the resource utilization thresholds for various microservices in order to meet
    the application''s end-to-end performance target. Setting inappropriate thresholds
    may lead to overprovisioning or underprovisioning of resources. We propose that
    cloud platforms should automatically determine these thresholds based on user-provided
    performance SLO targets. For this purpose, we study the feasibility of utilizing
    the proposed performance models in making efficient resource scaling decisions
    by formulating a constrained nonlinear optimization problem. A) Problem Formulation
    Consider that the performance SLO target in terms of the end-to-end tail latency
    for a workflow is specified. For a given workload condition, we aim to find the
    highest resource utilization values of the relevant microservices, at which the
    given SLO targets will not be violated. These optimal utilization values can be
    calculated periodically and set as the thresholds (desiredMetricValue) for making
    resource scaling decisions. These thresholds will help in determining which microservices
    should be scaled, and how many pods should be allocated to each microservice based
    on Equation 1. This approach aims to avoid resource overprovisioning while providing
    performance guarantee to the given workflow. We formulate the optimization problem
    as follows: max ∑ i∈ S j x i s.t. r j (x)≤SL O target j x=( x i ) i∈ S j (2) (3)
    (4) View Source where, the symbol notations are described in Table II. The objective
    function in Equation 2 aims to maximize the pod-level resource usage i.e the sum
    of average CPU utilization in the set of microservices that are relevant to the
    target workflow. The relevance of a microservice to a workflow can be determined
    either by analyzing the workflow DAG, or through machine learning based feature
    selection as described in Section VI-B. Consider that r j (x) is the tail latency
    predicted by machine learning model for workflow j . The inequality constraint
    in Equation 3 ensures that the SLO target of workflow j will not be violated.
    The optimization problem is nonlinear since the workflow tail latency r j (x)
    included in the constraint Equation 3 has a nonlinear relationship with the average
    CPU utilization of various microservices. In the formulation of the optimization
    problem, application-layer metrics (e.g number of concurrent clients), VM-level
    CPU utilization and CPI metrics are not included as variables, although the tail
    latency prediction r j (x) depends on these metrics as well. Instead, the values
    of these metrics are fixed according to their observed values at the time of solving
    the optimization problem, and are treated as constants for that instance of optimization.
    As a result, the solutions to the optimization problem will only include pod-level
    CPU utilization values, which can be directly used as thresholds for making resource
    scaling decisions. This allows the resource scaling mechanism to be practical
    and simple to implement. B) Solution We apply a non-linear optimization technique,
    trust-region interior point method [13], [20], to solve this problem. This optimization
    technique provides two main benefits. First, it is efficient for large scale problems.
    Second, the gradient of the constraint function which is required for optimization,
    can be approximated through finite difference methods in this optimization technique
    [13]. This property is desirable since the machine learning models for workflow
    tail latency are blackbox functions, whose gradient can not be directly calculated.
    C) Feasibility Study As a case study, we apply the optimization technique to calculate
    the desired CPU utilization (thresholds) for various relevant microservices, when
    a workload of 30 concurrent clients is applied to the SockShop benchmark, and
    a performance SLO target of 240 ms is specified for the 95th percentile latency
    of orders workflow. For this optimization, we utilize our Neural Network model
    for orders workflow with pod-level CPU utilization, VM-level CPI metrics and the
    number of concurrent clients as the input features. Figure 10 (a) compares the
    current (measured) CPU utilization of the microservices relevant to orders workflow
    and their desired CPU utilization values, when only one pod is allocated to each
    microservice. Based on Equation 1, the optimal resource scaling option is to allocate
    an additional pod to the frontend microservice. As shown in Figure 10 (b), we
    validate the optimality of this resource scaling option by comparing the tail
    latency of orders workflow for various possible resource scaling configurations.
    We observe that the resource scaling configuration suggested by our optimization
    technique is able to meet the performance SLO target while allocating minimum
    number of pods in total. Figure 10: Optimization of CPU utilization thresholds
    for efficient resource scaling with a workload of 30 concurrent clients, and SLO
    target 240 ms for 95th percentile latency of orders workflow. Show All SECTION
    VIII. Conclusions and Future Work We present the performance characterization
    and modeling of containerized microservices in the cloud. Our modeling approach
    utilizes machine learning and multi-layer data collected from the cloud environment
    to predict the end-to-end tail latency of microservice workflows even in the presence
    cloud induced performance interference. We also demonstrate the feasibility of
    utilizing the proposed models in making efficient resource scaling decisions.
    We envision that our performance modeling and resource scaling optimization approach
    can enable cloud platforms to automatically scale microservice-based applications
    based on user-provided performance SLO targets. This will remove the burden of
    determining resource utilization thresholds for numerous microservices from the
    cloud users, which is prevalent in existing cloud platforms. In future, we will
    extend our work to include diverse microservice-based applications with different
    resource bottlenecks. We will also evaluate the effectiveness of the proposed
    resource scaling system in the face of dynamic workloads. ACKNOWLEDGMENT Results
    presented in this paper were obtained using the Chameleon testbed supported by
    the National Science Foundation. The research is partially supported by NSF CREST
    Grant HRD-1736209. We thank the anonymous reviewers for their many suggestions
    for improving this paper. In particular we thank our shepherd, Prof. Maarten van
    Steen. Authors Figures References Citations Keywords Metrics More Like This Application
    interference analysis: Towards energy-efficient workload management on heterogeneous
    micro-server architectures 2017 IEEE Conference on Computer Communications Workshops
    (INFOCOM WKSHPS) Published: 2017 A Resilient Agent-Based Architecture for Efficient
    Usage of Transient Servers in Cloud Computing 2018 IEEE International Conference
    on Cloud Computing Technology and Science (CloudCom) Published: 2018 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Predicting the End-to-End Tail Latency of Containerized Microservices in
    the Cloud
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.robot.2018.10.001
  analysis: '>'
  authors:
  - Chongkun Xia
  - Yunzhou Zhang
  - Lei Wang
  - Sonya Coleman
  - Yanbo Liu
  citation_count: 30
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related work 3. Cloud robotics system
    architecture based on microservice used in intelligent space 4. Experiment and
    analysis 5. Conclusion and future work Acknowledgments Appendix A. Supplementary
    data References Vitae Show full outline Figures (20) Show 14 more figures Tables
    (6) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Extras (1) MMC S1 Robotics
    and Autonomous Systems Volume 110, December 2018, Pages 139-150 Microservice-based
    cloud robotics system for intelligent space☆ Author links open overlay panel Chongkun
    Xia a 1, Yunzhou Zhang a, Lei Wang a, Sonya Coleman b, Yanbo Liu a Show more Share
    Cite https://doi.org/10.1016/j.robot.2018.10.001 Get rights and content Abstract
    Cloud robotics (CR) is a red-hot branch of the burgeoning field of service robots
    that is centered on the benefits of integrating infrastructure and shared services
    via a cloud computing environment. Although it extends the computation power and
    information sharing capabilities of the network robots, the development and operations
    (DevOps) of the CR system are currently limited for enterprise-scale projects
    due to the heavy framework. In fact, current developed CR systems are typical
    distributed monomer architectures followed by a “top-down” design. As the scale
    of the applications gets larger, the operation and maintenance of CR systems will
    become a very difficult task. In this paper, a new architecture for a microservice-based
    cloud robotics system in intelligent space is proposed to solve the present dilemma.
    To enable this, we design a service management architecture based on a microservice
    to provide a highly efficient and flexible development/deployment mechanism. The
    container technology based on the docker engine is then used to functionally decompose
    the application into a set of collaborating services to ensure the software design
    methods, based on microservice, easy for implementation. Finally, a real experiment
    on SLAM (Simulation localization and mapping) in an intelligent space is implemented
    to verify the proposed architecture. Compared with traditional monomer architectures,
    the results show that the proposed framework is more productive, flexible and
    cost effective. Previous article in issue Next article in issue Keywords Cloud
    roboticsMicroserviceContainer technologyCloud computingIntelligent spaceVisual
    SLAM 1. Introduction Intelligent systems that mimic the behaviors and cognitive
    processes of human are rapidly being developed around the world. With the rapid
    development of sensor devices, the volume and type of information and data that
    need to be processed by the onboard processors of robots are growing rapidly.
    As a unique device, the robot carries out all the computation and storage processes
    on board, which significantly increases its computational burden and can become
    bloated and inefficient. To solve this problem, James J. Kuffner proposed the
    concept of “cloud robotics” [1]. This concept introduced a new scenario where
    robots were regarded as agents, relying on remote servers for most of their computational
    load and data storage, and creating a middleware where they can share information
    and knowledge. The typical structure diagram for a cloud robotics system is depicted
    in Fig. 1. The use of cloud computing for robotics and automation brings many
    potential benefits such as largely ameliorating the performance of robotic systems.
    Since the on-board processing capacity and storage capacity are very limited for
    physical robots, it often leads that robots have a long processing time and run
    slowly. Cloud robotics [2] can not only solve the inherent problems of traditional
    robotic systems, such as onboard computation and storage limitation, asynchronous
    communication and compatibility problem of multi-robot systems, but can also enhance
    the performance via concepts such as a remote brain, shared knowledge-base, collective
    learning and intelligent behavior. The standard CR system follows a systematic
    “top-down design”, which can be treated as a stepwise design or a synonym of decomposition.
    The approach in [2] describes an overview of the system, specifying, but not detailing
    any first-level subsystems. Moreover, as the scale of the applications expands,
    it may be very difficult to deploy and maintain the system for a long time [3].
    In fact, current research into the CR framework is based on a typical distributed
    monomer architecture [4], such as M2M/M2C (Machine-to-machine/machine-to-cloud)
    [5] or UNR-PF (Ubiquitous Network Robot Platform) [6]. It should be noted that
    the monomer architecture is a mainstream development framework in the current
    software development background due to wide popularity, friendly IDE (Integrated
    Development Environment) and facilitated resource-sharing functionality. But the
    monomer architecture is limited by a technology stack, which forces developers
    to use a unified programming language, even though that may be inappropriate.
    Besides, too much coupling between services worsens the problems caused by code
    duplication for the monomer architecture. Hence, even though M2M/M2C and UNR-PF
    are widely known in academia, they cannot get out of the laboratory and achieve
    popularity and recognition from the market. These existing problems indicate that
    current architectures are unsuitable and unfavorable for the long-term development
    of cloud robotics system. Given this, the microservice, as a new software design
    idea, provides a novel perspective for technology companies and developers. Download
    : Download high-res image (311KB) Download : Download full-size image Fig. 1.
    The cloud robotics system provides a management center and a data center. Every
    robot service can be registered on the cloud servers by a uniform interface standard
    and rule. The robot clients are the low-cost robot platforms with an embedded-class
    processor and a wireless connection. Robot clients can request these robot services
    from a service item which is stored in the management center. Therefore, inspired
    by the new design idea, in this paper we present a new cloud robotics system framework
    based on a microservice that tries to meet the requirements of designers and developers
    in an intelligent environment. Our idea is that all functions of the CR system
    can be modularized and regarded as a service, and some complicated tasks can be
    effectuated through the composition of available services. Finally, a real experiment
    “3D Visual Mapping and Localization” in an intelligent environment is adopted
    to verify the new architecture. We also design a comparative experiment to demonstrate
    the excellent performance of our proposed architecture. The results show the promise
    of our work. 2. Related work 2.1. Cloud-enabled robotics Cloud technology-based
    computing or simply Cloud Computing is one of the most active fields of Info-Communication
    Technologies (ICT) [7]. The principal structure of cloud computing is depicted
    in Fig. 2. From Fig. 2, we see that the cloud computing enables desktop-based
    computing to move towards full web-based computing where a web browser can be
    used to access, develop and configure various applications, hardware and data
    over the internet. It also indicates that the combination of cloud computing and
    robotics is an inevitable trend. Many major companies participate in cloud computing
    and establish all kinds of platforms such as Google App engine [8], Amazon Elastic
    Compute Cloud (EC2) [9], Microsoft Azure [10], GRIDS Lab Aneka [11] and Sun Grid
    [12]. The emergence of cloud computing and the corresponding platforms make it
    possible to conveniently use cheap computing resources in a similar manner to
    water or electricity in daily life. Download : Download high-res image (476KB)
    Download : Download full-size image Fig. 2. The cloud computing system consists
    of middleware module, background tasks module and control module. Every module
    can provide a diverse set of capabilities. The main advantages of cloud computing
    relate to dynamic scalable mechanism, parallelism computing and distributed structure.
    The concept of cloud robotics can be traced back at least two decades to network
    robots. Masayuki Inaba [13] proposed a robot control method based on remote computing,
    highlighting the advantages and the IEEE Robotics and Automation Society established
    a technical committee for Networked Robotics in 2001 [14]. Kamei et al., [15]
    proposed “cloud networked robotics” to fulfill various location-based tasks in
    a shopping mall for supporting daily activity, especially for the elderly and
    disabled. The ASORO lab in Singapore proposed DAvinCi based on Hadoop and ROS
    (Robot Operating System), which shows the scalability and parallelism advantages
    of cloud computing for service robots in large environments [16]. Tenorth [17]
    designed the UNR-PF to realize human–computer interaction in a convenience store
    and Gostainet [18] established an infrastructure using cloud robotics for speech
    recognition on the humanoid robot NAO. Carlos and Du Z et al. [19] present an
    architecture design of “robot cloud” to bridge the power of robotics and cloud
    computing. They use the SOA (Service-oriented architecture) to expand the capacities
    of physical robots. Nan Tian et al. [20] described Berkeley Robotics and Automation
    as a Service, which is a RAaaS prototype that allows robots to access a remote
    server that hosts a robust grasp planning system (Dex-Net 1.0). The above research
    mainly focuses on some practical application areas but does not present systematic
    architectures for cloud robotics which is the focus of this paper. In addition,
    the European Union also started a groundbreaking cloud robotics project “RoboEarth”
    [21] in 2009. This project attempts to build a giant network and database repository
    where robots can share information and knowledge and learn from each other about
    their behaviors and environments. The researchers have developed the famous cloud
    engine “Rapyuta” [22] and the knowledge processing system “KnowRob” [23] successfully.
    Furthermore, in 2014 scientists from institutes including Cornell, Stanford, Google,
    and Microsoft developed a new project “RoboBrain” [24] that allows robots to learn
    and share representations of knowledge. These developments indicated that cloud
    robotics can be used to effectively and efficiently expand the robots’ knowledge
    and skills. Undoubtedly, there is much more additional research being undertaken
    in the field of cloud robotics. However, from this brief review, we can determine
    the main cloud robot architectures can be divided into two subgroups: M2M/M2C
    and UNR-PF and the these are typical distributed monomer architectures. The deployment
    of a typical distributed monolithic architecture is depicted in Fig. 3. However,
    the traditional distributed monolithic architecture has also some inherent defects.
    From the development methodology, many companies want to deploy more applications
    to the cloud and they also need to innovate as fast as possible to avoid competition
    [25]. Therefore, continuous delivery is very important for many startups or large
    Internet corporates in recent years. Applications based on a typical monolithic
    architecture would have a single codebase shared among multiple developers and
    be developed using an MVC (Model View Controller) [26] web application framework
    such as JEE, .NET, Symfony, Rails, Grails and many others. If these developers
    want to add or change services, they must do more work to make sure that the new
    service is perfectly compatible with other services. Thus, as more services are
    added, the complexity of deployment will increase significantly and limit the
    ability of companies to innovate with new application versions and features in
    the monolithic applications. The above limitations and problems have become a
    great challenge for most Internet companies and SaaS providers which we aim to
    address in this paper. Download : Download high-res image (177KB) Download : Download
    full-size image Fig. 3. Deployment of the monolithic architecture. 2.2. Microservice
    in the cloud To solve the above problem of deployment, we propose a novel lightweight
    cloud robotics architecture based on microservice. Microservice [27] is a software
    architecture style in which complex applications are composed of small, independent
    processes communicating with each other using language-agnostic APIs. It should
    be noted that, although the design and philosophy behind each architecture approach
    share some traits, microservices and the SOA are fundamentally different in other
    key ways. With the new architecture, the companies can innovate quickly and reduce
    complexity by using computing resources efficiently. Therefore, the development
    teams can be enlarged in a controlled way. The brief deployment of the microservice
    architecture on a cloud solution is depicted in Fig. 4. In Fig. 4, S1, S2, S3
    and S4 all are microservices, each of which can be developed using different technological
    stacks as three tiers applications. The gateway is developed as a light web application
    that receives requests from end-users and gets or returns the results. It does
    not contain a persistence layer because no information needs to be stored. Moreover,
    it must use the services offered by the microservices ( S1, S2, S3 and S4) through
    REST (Representational State Transfer). JSON is used as the interchange message
    protocol between the display module and the gateway, and the gateway and each
    microservice. From Fig. 4, we can see that the gateway and each microservice can
    be developed and maintained by independent teams as self-managed applications,
    which facilitates the increase of the number of developers in a more scalable
    way than is currently available. Besides, we also find that each microservice
    may be developed with different programming languages such as Python, Java, .NET,
    PHP, Ruby, etc. The increasing adoption of microservices in the cloud is motivated
    by the ease of deploying and updating the applications, as well as the provisioned
    loose coupling provided by dynamic service discovery and binding. Furthermore,
    structuring the software deployed in the cloud environment using a collection
    of microservices allows cloud service providers to offer higher scalability guarantees
    through more efficient utilization of cloud resources, and to restructure the
    software to accommodate growing consumers’ demand dynamically and quickly [28].
    The microservice framework attempts to simplify the process of defining service
    descriptions to promote automatic service consumption in the semantic web. In
    the framework, the description task can be improved by enabling reusability across
    service descriptions. Recent advancements in the container technology [29] and
    its capability to overcome limitations in virtualization have shown the advantage
    of the utilization of containers in the cloud for software applications development
    and deployment. The container technology is very attractive because it completely
    enables isolation of independent software applications running in a shared environment.
    Docker is a representative product of the container technology [30]. The Docker
    provides a single and lightweight API to manage the execution of containers and
    allows developers to pre-package the software dependencies into a lightweight
    and portable file that requires less operation costs than a standard hypervisor.
    The diagram of microservice based on the container technology is depicted in Fig.
    5. Microservice supports the realization of small (sized) software applications
    that are fine-grained and loosely coupled via the REST communication. These applications
    are implemented using APIs provided by the infrastructure-as-a-service (IaaS)
    layer for provisioning data computing, storage and delivery capabilities. Besides,
    the microservice model also enables a simpler and faster migration of software
    component instances from one visual Machine to another to satisfy variable resource
    demands for cloud applications. Download : Download high-res image (296KB) Download
    : Download full-size image Fig. 4. Deployment of the microservice architecture.
    Download : Download high-res image (189KB) Download : Download full-size image
    Fig. 5. The publishing principle of container-based microservices. 3. Cloud robotics
    system architecture based on microservice used in intelligent space The motivation
    of this research is to design a new cloud robotics system framework based on microservice
    to implement the Cloud-based Assisted Living Project (CALP). From Section 2, we
    find that current research focusses on two main parts: cloud platforms and robots.
    However, various sensors and monitoring systems in the environment are changing
    our life substantially, and we cannot ignore the revolution brought about by the
    Internet of Things for intelligent robots. There is no doubt that the environment
    around the robots should be regarded as an integral part of the whole CR system
    and it also plays a significant role in the real application of the CR system.
    Therefore, in this paper, the CR system mainly includes three parts: cloud platform,
    robots and robots’ working environment. For the CALP, the robot’s working environment
    can be seen as an intelligent environment. The intelligent space (iSpace) [31],
    also called a smart space or intelligent environment, is a space with devices,
    multi-source information and communication technologies creating interactive environments
    that bring computation into the physical world and enhance the occupants experiences;
    the iSpace is depicted in Fig. 6. For cloud robotics system, most of services
    requested by local physical robots are apparently compute-intensive tasks in the
    cloud such as SLAM, navigation and scene recognition and fusion. The distributed
    monolithic architecture often deploys and runs an integrated development application
    and provides the robot services by the entire system. However, the change of one
    function may affect the others and cause more difficulties of redeployment and
    continuous integration due to the change and evolution of the system function.
    Additionally, since the monomer system adopts a unified technology stack and development
    standard, it will make the development process more limited and complicated. The
    discussion indicates that current distributed monomer architectures need to be
    improved and changed. Based on this, we replace the microservice architecture
    as a solution. The microservice, as a new software architecture design pattern,
    has shown the competitive strengths such as more productive, flexible and low
    development costs. Obviously, it is a very complex work to design a novel architecture
    based on microservice for the CR system. We need to consider many factors including
    reliability, scalability, modularity, interoperability, interface and QoS. The
    core designs of the proposed system are described briefly as follows. Download
    : Download high-res image (206KB) Download : Download full-size image Fig. 6.
    Intelligent space (iSpace). The iSpace can physically and mentally support people
    through robot and intelligent hardwires, thereby providing satisfaction for their
    needs. 3.1. Cloud robotics system based on microservice in iSpace The structure
    diagram of the proposed CR system based on microservice is depicted in Fig. 7.
    The fundamental idea is that the service architecture can be divided into smaller
    granularity services that run in an isolated environment. Fig. 7 clearly shows
    the basic components and system composition of cloud robotics that are migrated
    to the microservice architecture from a single distributed application. In Fig.
    7, the microservice application is released to the distributed environment via
    a continuous delivery platform after deployment and verification; then it will
    be registered. Besides, physical robots can upload the collected multisource data
    information and request “robot services” towards cloud management system via wireless
    WIFI and wired short-range network. The iSpace can share all sorts of environment
    information with the cloud platform. To implement the proposed microservice architecture
    successfully, these components will be put into a container and be managed by
    the Docker engine. In addition, the APIs’ service will be accessed by the users
    or external services via Service Gateway. The key components of the proposed architecture
    are described below in detail. (1) Service registry and discovery component Service
    registration and discovery is the core component of the proposed architecture.
    In the distributed environment, the service instance will be changed dynamically
    according to the default rule or policy in a dynamic environment. leading to a
    higher requirement for this component. The sketch of the service registry and
    discovery mechanism is depicted in Fig. 8. It should be noted that the proposed
    services refer to “robot services”, such as SLAM service, navigation service and
    vision recognition service. (a) Registration and identification service Since
    a microservice application can be deployed via the Continuous Delivery Platform
    (CDP), it will be registered as a service instance by the service registry automatically.
    Besides, the location of service instance will change when the health status and
    the network environment change, so the service registry needs to track and identify
    the service instance. (b) Locating and discovery service Download : Download high-res
    image (564KB) Download : Download full-size image Fig. 7. Microservice-based cloud
    robotics (MCR) system for intelligent space consists of cloud platform, robots
    and intelligent space. Each function of the MCR system will be seen as a microservice,
    especially for robot services. The data center shared in this system is built
    on the cloud servers. Ideally, when the user accesses directly from the client,
    the scheduling module will query the service registry to find the accessible service
    and send it to the corresponding service instance via the load balancing algorithms.
    Dynamic discovery means that calling components can locate microservice information
    as needed without closely integrating the service. However, an application often
    relies on the collaboration of several microservices in the real environment,
    especially for a robot application. For example, SLAM is a complex robot application,
    that contains tracking service, local mapping service and loop closing service.
    Therefore, it is very important to locate and discover services. If the caller
    accesses a service layer directly, it can query the service registration center
    and find the access service and the corresponding service instances, and then
    use the load balancing mechanism to invoke the service instance. (2) Sustainable
    delivery platform Download : Download high-res image (208KB) Download : Download
    full-size image Fig. 8. Service registry with non-center nodes and auto discovery
    mechanism. The main function of the sustainable delivery platform is the rapid
    and flexible deployment of the microservice application. In addition, the deployed
    microservice must be programmable, easier-to-maintain and scalable, which can
    run in a separate and isolated container as a process. This sustainable delivery
    process can bring more rapid feedback to the application. Furthermore, compared
    with the traditional “Waterfall” software development process, sustainable delivery
    will become more cooperative and more efficient on demand analysis, user experience,
    interactive design, testing and maintenance collaboration. For example, as an
    important part of the robot application “SLAM”, the loop closing often needs to
    be improved or updated by researchers. If we take full advantage of this platform
    to publish a new improved version, we believe that the development of SLAM will
    be easier and faster than what currently exists. The sustainable delivery (deployment)
    process is depicted in Fig. 9. (3) Service gateway Download : Download high-res
    image (159KB) Download : Download full-size image Fig. 9. The sustainable delivery
    (deployment) process. The service gateway (SG) is a unified call logic portal,
    which encapsulates the service information of a node in a distributed environment.
    The main functions of the service gateway are described as follows: (a) The services
    which are registered via an existing service registry are exposed to an external
    call directly; (b) The SG can satisfy the requirement that a client requests multiple
    services at one time; (c) Support cache storage for some services whose operating
    results are constant in certain time intervals. If the service request fails,
    the SG will provide the last correct cache execution or null response; (d) Provide
    request distribution routing, load balancing, security protection, protocol conversion
    and other functions. (4) Log service, application monitoring and RPC The log service
    component will accurately collect various pieces of information, such as operation
    log, SQL operation log, exception log, etc. Then, after standardization, filtering,
    merging and alarm analysis, it centralizes storage and management in a unified
    format, helping users to locate faults quickly, and providing objective basis
    for tracking and recovery by summarizing and analyzing all log information. The
    monitoring component provides the running status of the microservice, the JVM
    performance index, the system performance index and the monitoring function of
    the microservice call chain to facilitate real-time monitoring for users. The
    RPC (Remote procedure call) component provides a remote procedure call mechanism
    that is suitable for a distributed environment to ensure the performance and reliability
    of inter-service communication. (5) Communication protocol For the communication
    pattern, all components can contact each other, no matter how they communicate
    at the interface or protocol levels. In this paper, we adopt the REST (Representational
    state transfer) protocol [32] as the communication specification among different
    microservices and JSON (JavaScript Object Notation) [33] is used as the data format.
    Compared with the traditional protocol such as the SOAP [34] protocol and WSDL
    [35], REST and JSON are all lightweight protocols and communicate directly by
    HTTP requests. The REST is an architecture style that can allow web services to
    provide interoperability between computer systems via an Internet. The web services
    based on REST allow requesting systems to access and manipulate text descriptions
    of various web resources using a unified and predefined set of stateless operations.
    Through the stateless protocol and the standard operations, REST systems can provide
    high availability, fast performance, continuous growth capacity and reliability
    by reusing components that can be managed and updated without impacting the system,
    even while it is running. JSON is a lightweight data exchange format that uses
    human-readable and easy-to-edit texts to transmit the data objects containing
    attribute-value pairs. Moreover, the JSON is also the most common data format
    that is used for asynchronous browser/server communication. In addition, each
    service data will be clearly defined in two formats: normal and abnormal. To make
    the data processing and reading easier, the exception code and exception information
    of the abnormal format are necessary. The communication flow between different
    microservices is described in Fig. 10. When we adopt the instance independence
    patterns, the proposed architecture can support component-to-component communications
    by synchronous or asynchronous models. And it does not force the other components
    to be in any specific state before receiving the requests or messages. Thus, if
    our proposed deployment is appropriate, all of the services can respond to any
    requests from components asynchronously and retain or manage every state no matter
    what the sequence is. Download : Download high-res image (156KB) Download : Download
    full-size image Fig. 10. Communication flow between different microservices. (6)
    Security mechanism The data sharing mechanism of our proposed architecture supports
    the cross platform and the cross application. It is very important to ensure safe
    and reliable access while unauthorized access is denied. The flow chart of security
    mechanism based on OAuth2 protocol is depicted in Fig. 11. To enable the security
    mechanism, we will take advantage of the federated security system that can create
    trust between components, no matter whether the security model is local to the
    components. OAuth is an open protocol that provides a safe, open and simple standard
    API service for the authorization of user resources without providing the passwords
    or keys. Moreover, any third party can use the OAuth authentication service and
    any service providers can their own OAuth authorization service. The security
    mechanism has been widely used by many Internet companies such as Google, Facebook,
    Microsoft and Twitter to permit users to share their account information with
    third party applications or websites. Therefore, we use OAuth protocol as every
    microservice’s authorization standard in our proposed system. Download : Download
    high-res image (201KB) Download : Download full-size image Fig. 11. The flowchart
    of security mechanism. 3.2. Scheme demonstration and feasibility analysis The
    proposed scheme in this paper comes with some irreplaceable advantages, including
    the ability to reduce complexity by leveraging container abstractions which indicates
    that we can abstract the access to resources (such as storage) and make the application
    portable thus speeding up the refactoring of the applications by removing dependencies
    on the underlying infrastructure services. In the past, most of researches focusing
    on cloud robotics architecture, security and governance services have been platform
    specific, not application specific. It is obvious that the traditional on-premises
    applications have almost no security and governance functions. Therefore, the
    new proposed architecture can provide better portability and less complexity by
    placing security and governance services outside of the application domain. Moreover,
    in the proposed architecture, the applications can be distributed and optimized
    according to their utilization of the platform. When the proposed architecture
    is adopted, we can easily place an I/O-intensive portion of the application on
    the cloud thus providing better performance than non-cloud based approaches, placing
    a computationally-intensive portion of the application on a public cloud that
    can provide the proper scaling and load balancing. Additionally, an important
    prerequisite for the proposed framework is that all of these elements work together
    to form the application and the applications should be divided into components
    that can be optimized. It means that the application should be broken down to
    its functional primitives and built it up as component pieces to minimize the
    amount of code that needs to be revised. From an operational point-of-view, the
    nature of the operations in the proposed CR system based on microservice is cloud
    operations [36], or the operation of the application containers in the cloud.
    Before the applications are generated, developers should take advantage of the
    microservice architecture and container technology. They should manage the applications
    as distributed components that can be separately scaled. For example, the container
    that manages the user interface can be easily replicated in servers when the demand
    increases within a certain time period. This indicates that the operation is a
    very convenient way to achieve the scalability automatically around the application
    thus expanding the use of cloud resources as needs change. Although the proposed
    method forces the application developers to think about how to best redesign the
    applications to make them containerized and service-oriented, it is more productive,
    flexible and cost effective. Compared with the complexity of the cloud operations,
    the advantages of the follow-up use also verify the feasibility and necessity
    of the proposed architecture. Additionally, two important considerations regarding
    the proposed system are programmability and ease of use. Programmability can be
    regarded as a set of tools and best practices to add, deploy and manage applications’
    microservices. Although the learning process is necessary, compared with other
    traditional CR systems the proposed system required lower learning threshold and
    less learning cost. According to our research, if you have a general familiarity
    with new applications and docker technology, you can develop and deploy the applications
    in the proposed system. Ease of use is also a fundamental consideration factor
    for developers and users. For the proposed system, microservice allows to develop
    and maintain the applications in different programming languages, which is unimaginable
    for traditional CR systems. Moreover, microservice can make the deployed applications
    clearer and more convenient for users. These advantages make the proposed system
    architecture more competitive than other CR systems. 4. Experiment and analysis
    We use Kubernetes (commonly referred to as “K8s”) [37] to build a service cluster
    environment and adopt the Docker engine to implement service encapsulation. In
    addition, the cloud platform uses our own private cloud which is built by OpenStack
    [38]. 4.1. Experimental system deployment To complete the deployment of the cloud
    management development environment, we create four nodes: one master node, two
    slave nodes and one docker private node. The private node provides mirroring services
    for a cluster environment as a private warehouse server of Docker. The details
    of all nodes are shown in Table 1. Then, we need to install the Flannel service
    and configure the environment of Kubernetes. We also change the configuration
    file of Docker to install some components for the master node such as the API
    server, Scheduler and Controller Management, and install other components for
    slave nodes such as Kubelet and Kubernetes Proxy. Obviously, to ensure the components
    work, we should create the Service configuration file to manage and control the
    system [39]. In addition, we choose the new ORB-SLAM2 [40] as an application of
    FaaS (Function as a service) in a cloud management platform. The ORB-SLAM2 can
    work in real time on the standard CPUs (Central processing units) in a wide variety
    of environments from small hand-held indoors scene to drones flying in industrial
    environments and cars driving around a city for monocular, stereo and RGB-D cameras.
    Due to the complicated structure and intensity of technologies, SLAM is very difficult
    to modify or develop for most of scholars and researchers. Therefore, we split
    the ORB-SLAM2 process into three microservices: the tracking service, the local
    mapping service and the loop closing service, as is depicted in Fig. 12. These
    microservices are built in a shared database. Then we define three related Dockerfiles
    and use the command “$ sudo docker build ” to construct Docker images. Finally,
    we publish the docker images on a docker private mirror server. Besides, it should
    be noted that the docker mirror name must be prefixed with an IP number of the
    Docker private server, such as “19216859131”. When the experimental system environment
    has been deployed, we need to create a configuration file by YAML to build resources
    and start services. Therefore, we should create three Replication Controller (RC)
    files and the related Kubernetes Service files. The RC file and Service file in
    the pose map are shown below. Then, we can start the service using the command
    “$ kubectl ” on the master node. Moreover, it should be noted that every microservice
    is assigned a cluster-IP address, which is an access entry address defined by
    Kubernetes. Using the IP address, we can access the cluster instances consisted
    of Pod copies. (1) RC file: Table 1. The details of all nodes. Nodes Node IP Operation
    System Configuration Master node 192.168.59.128 CentOS 7 Memory: 2G; Storage:
    10G Slave node 1 192.168.59.129 CentOS 7 Memory: 16G; Storage: 20G Slave node
    2 192.168.59.130 CentOS 7 Memory: 16G; Storage: 20G Docker private server 192.168.59.132
    Ubuntu 14 Memory: 2G; Storage: 40G Download : Download high-res image (599KB)
    Download : Download full-size image Fig. 12. The ORB-SLAM2 is divided into three
    nodes (microservice): tracking, local mapping and loop closing. Each microservice
    can be assigned a independent IP address and deployed in a separate container.
    The case builds the communication between nodes by ROS. Download : Download high-res
    image (193KB) Download : Download full-size image (2) Service file: Download :
    Download high-res image (97KB) Download : Download full-size image 4.2. Simulation
    using the standard dataset To verify the proposed system, we use the standard
    dataset “freiburg2-desk” [41] from TUM for simulation testing. The tested dataset
    information is described in Table 2. The point cloud map and octomap built by
    the microservice-based ORB-SLAM2 are depicted in Fig. 13. The comparison of estimated
    trajectory and ground truth is depicted in Fig. 14. From Fig. 13, we can find
    that the microservice-based ORB SLAM2 (M-ORB-SLAM2) system runs well and the point
    cloud map and octomap are all sufficiently clear for the robot’s navigation and
    relocation. Fig. 14 shows that the error of the M-ORB-SLAM2 is relatively small
    and meets the necessary level of accuracy. Then we use the Root Mean Square Error
    (RMSE) of the Absolute Trajectory Error (ATE) and running time to accurately evaluate
    the system performance. Suppose the estimated robot pose is , and the real moving
    trail is . Then the RMSE of the ATE can be calculated as follows: (1) where trans
    is the translation vector. To strengthen the argument, we compared the system
    with the LSD-SLAM, RGBD-SLAM, ORB-SLAM2. Every method is run 10 times and the
    average results are used as an evaluation index for comparison and shown in Table
    3. From Table 3, we can see that the differences of the ATE and RMSE between ORB-SLAM2
    and the proposed method are all less than 0.001 m. The difference of running time
    is less than 2 s. The results indicate that the M-ORB-SLAM2 has similar performance
    to the ORB-SLAM2. Additionally, compared with the ORB-SLAM2, the M-ORB-SLAM2 system
    tends to be more loosely coupled, heterogeneous and physically dispersed and each
    component can be easy to be modified or developed by researchers and technicians.
    It indicates that the proposed method will encourage more people to participate
    in improving a particular component of SLAM without knowledge of other components.
    Moreover, this also indicates that the M-ORB-SLAM2 is extensible and can be reused
    by other teams, ultimately promoting standardization. Based on the above, the
    M-ORB-SLAM2 can perform better than LSD-SLAM and RGBD-SLAM. Table 2. The tested
    dataset basic information. Table 3. Comparison of SLAM system performance. Empty
    Cell LSD-SLAM RGBD-SLAM ORB-SLAM2 M-ORB-SLAM2 Average error /m 0.038 0.090 0.010
    0.009 RMSE /m 0.045 0.095 0.011 0.012 Running time /s 500 500 271.5 269.7 Download
    : Download high-res image (680KB) Download : Download full-size image Fig. 13.
    The simulation results of “freiburg2-desk” based on the proposed method: (a) dense
    point cloud map; (b) octomap map. Download : Download high-res image (256KB) Download
    : Download full-size image Fig. 14. Comparison of estimated trajectory and ground
    truth: (a) 3D visual angle; (b) 2D projection angle. 4.3. Experiment in a real-world
    scenario To further verify the proposed M-CR system, we design a SLAM experiment
    in a real scenario. We use the above M-ORB-SLAM2 as the mapping method in this
    experiment. The tested scene is an indoor study room of our college. To simulate
    the environment of the CALP, we created a simple smart space with some Ultra-Wideband
    (UWB) modules. These UWB modules can assist 3D map building and localization in
    the work space. The main experimental hardware equipment is a TurtleBot with mounted
    Kinect1.0 and the embedded board NXP.I.MX6Q. The experimental indoor floor plan
    is depicted in Fig. 15. Detailed information of all required hardware equipment
    is described in Table 4. In conclusion, the complete flow chart of the experiment
    is described in Fig. 16. It is important to note that the embedded processor in
    this experiment is low cost and unable to complete the whole visual SLAM process
    alone. In addition, the communication mechanism between cloud and robot adopts
    Socket. The communication method between physical robot and cloud mainly uses
    the wireless WIFI. From Fig. 16, we can also see that the cloud management platform
    is the key part for the proposed Microservice-CR system. To investigate the accuracy
    of the mapping and location service in smart space, we imitated the smart home
    and designed a model of a simple realistic home scenario in our laboratory. The
    labels “A” to “E” represent desks with sensors. The labels “G” to “I” represent
    chairs with sensors. The label “F” represents the door with a sensor. In this
    experiment, the robot needs to fulfill two tasks: build a 3D map of the tested
    space and locate these desks, chairs and the door. Task 1:3D visual SLAM task
    Table 4. Required hardware equipment. Equipment Detailed configuration No. Private
    cloud servers Intel Xeon E5-2620v4 CPU 2, 2.4 GHz, 64G DDR4ECC memory, 2T storage,
    GTX1080, 4xPCIE3.0 × 16 4 Robot TurtleBot platform 1 Embedded processor NXP(Freescale)
    I.MX6Q Cortex A9 CPU, 4cores@1.2 GHz, 2G RAM, 16G eMMC Flash 1 Visual sensor Kinect1.0
    from Microsoft 1 PC Intel Core i5, 8 GB DDR4 memory, 256 GB SSD storage, NVIDIA
    940MX monitor, 1 Other sensors UWB modules 9–10 Though the proposed architecture
    is very lightweight and easily expandable, it can also easily obtain huge computing
    resource and tackle all kinds of computation tasks due to cloud platform. The
    3D mapping result is described in Fig. 17. Since 3D visual mapping is a typical
    computationally intensive task, we use the running time as a performance index
    to evaluate the system. To make the experiments more convincing and appealing,
    we use “Running locally by laptop” and the monomer CR system as comparative schemes.
    For all schemes, the key frames processed by embedded-level robots will be sent
    to receivers (laptop or cloud servers) using wireless transmission technology
    such as WIFI. It should be noted that the laptop used for the first comparative
    scheme has the following general hardware configurations: Core i5@2.5 GHz, 4G
    memory and 1T Storage. Moreover, the monomer CR system is a non-microservice CR
    system based on Robot Operating System (ROS) [42], which implies that SLAM in
    a cloud platform is a whole package and the robot is just regarded as a simple
    image acquiring unit. The communication protocol of the monomer CR system based
    on ROS is Rosbridge. The detailed development or deployment information of the
    three schemes is described in Table 5. The comparison of the run-times of the
    whole 3D visual SLAM is depicted in Fig. 18. From Table 5, we can see that the
    proposed system has almost the similar deployment time with scheme 2. However,
    the proposed system has better scalability and computing capability than others.
    Besides, compared with other schemes the proposed system can offer more diverse
    and friendlier development languages support such as Java, Python, C# and Ruby.
    Undoubtedly, the advantage reduces the development threshold and allows more people
    to participate in the improvement of robot applications and services. Moreover,
    Table 5 indicates that the proposed system can offer easier code maintenance,
    which can reduce the cost of operation and maintenance. The low hardware cost
    of the proposed system allows more “Shortage of Funds” researchers to do some
    “expensive” robot research work. Since deployed applications need to be split
    into “microservices”, they have lower coupling degree and more flexible deployment.
    Moreover, the proposed system architecture can build and improve the SLAM process
    more flexibly than other existing architectures. For example, we can easily replace
    the closed-loop module with deep convolutional neural network to improve the mapping
    accuracy. Additionally, we expediently check and modify every microservice to
    remain in an optimal state. In Table 5, we make a detailed correlation among these
    schemes with respect to various aspects such as the deployment time and language,
    coupling, scalability and cost. Table 5 shows that the proposed system has relatively
    more superiority than other schemes. Though the deployment of the proposed system
    takes longer, it will be acceptable and worthwhile for sustainable development
    and research. Download : Download high-res image (214KB) Download : Download full-size
    image Fig. 15. The diagram of the tested smart space. Download : Download high-res
    image (724KB) Download : Download full-size image Fig. 16. An overview of the
    entire experimental setup. Download : Download high-res image (649KB) Download
    : Download full-size image Fig. 17. The 3D map of our workspace. From Fig. 17
    we see that the 3D mapping based on a cheap embedded board processor is successful
    and the reconstructed indoor scene of the laboratory is very clear. The result
    entirely meets the requirements of robot location and navigation in a real scenario.
    Fig. 18 shows that the difference in the average run time of each key frame among
    three schemes is less than 8 ms. From Fig. 18, we see that the proposed architecture
    has almost the same run time as the other two methods. It also indicates that
    splitting into microservices do not reduce the execution speed of SLAM process.
    This is because the communication mechanism among the microservices is like the
    memory reading and writing process, and these communication time can often be
    ignored. The results indicate that the proposed architecture is effective and
    the deployment of the microservice application is successful. Though it is not
    perfect as a prototype system, there is no doubt that the proposed architecture
    presents a better solution on cloud robotics than the past research work. Table
    5. Comparison of three schemes in detail . No. Deployment time(/h) Development
    language Coupling degree Scalability Computing capability Code maintenance difficulty
    Communication protocol (Robot-cloud) Hardware cost (robot controller) Scheme 1
    6 C++ High Bad + Hard None Like a laptop ($800) Scheme 2 12 C++ Medium Normal
    + + + Medium Rosbridge Embedded device ( $50) Scheme 3 12 No limit Very low Very
    good + + + Easy Socket Embedded device ( $50) Note: Scheme 1: Running locally
    using a laptop level controller; Scheme 2: The monomer CR system based on ROS;
    Scheme 3: The proposed Microservice-based CR system. Task 2: Location task Download
    : Download high-res image (191KB) Download : Download full-size image Fig. 18.
    Comparison of running time using different frameworks. When the 3D map has been
    built, a robot can determine the position of the labels by self-localization and
    UWB signal. Therefore, we regard the location as a microservice and package the
    third-party application using Docker to verify the scalability of the proposed
    architecture. Since different microservices share the same database, the communications
    consumption in the cloud platform can be ignored. However, it should be noted
    that the positioning effect depends on map accuracy and the robot’s self-localization
    accuracy. Thus, in this paper we run the location experiment 10 times and use
    the average as the final result. The location experiment’s result is depicted
    in Table 6; the location precision is 0.1 m. In Table 6, the location results
    show that the robot can judge the location of the desks and chairs effectively.
    From the experiment, we find that the third-party application can be developed
    independently and is easier to deploy for the proposed architecture than a traditional
    monomer architecture. 5. Conclusion and future work We have presented a new cloud
    robotics prototype system architecture based on microservice for use in an intelligent
    environment. We demonstrate the proposed system performance using both a simulation
    test and a real experiment and this highlights that both run-time and flexibility
    of the proposed approach are comparable with existing architectures. Additionally,
    third-party applications can be developed independently and are easy to deploy
    for the proposed architecture than a traditional monomer architecture. There is
    no doubt that the CR system based on microservice is a very important and prospective
    research for the development of intelligent robots. The microservice-based ORB-SLAM2
    also presents a meaningful exploration for standardizing the visual SLAM process.
    Future development of the research direction will enable proposed system architecture
    to show its superiority and bring more convenience and benefits. The proposed
    architecture is currently just a prototype system; we will continue with in-depth
    study to improve it further. In the future, we hope that the Microservice-CR system
    can play an important role in the field of home service robots, especially for
    the disable and the elderly. Table 6. The location experiment’s results . Empty
    Cell Label Desk A B C D E Accuracy 0.9 0.8 1.0 0.8 0.9 Chair G H I Accuracy 1.0
    0.9 0.9 Door F Accuracy 1.0 Acknowledgments We would like to thank Professor Sonya
    Coleman and other members from Intelligent System Research Centre (ISRC) of Ulster
    University for helping us to improve this work. Additionally, the authors would
    like to thank experienced anonymous reviewers for their constructive and valuable
    suggestions for improving the overall quality of this paper. Appendix A. Supplementary
    data The following is the Supplementary material related to this article. Download
    : Download zip file (346MB) MMC S1. Source codes and Experimental results. References
    [1] J.J. Kuffner, Cloud-enabled robots, in: EEE-RAS International Conference on
    Humanoid Robotics, Nashville, TN. 2010. Google Scholar [2] Kehoe B., Patil S.,
    Abbeel P., et al. A survey of research on cloud robotics and automation IEEE Trans.
    Autom. Sci. Eng., 12 (2) (2015), pp. 398-409 View in ScopusGoogle Scholar [3]
    Villamizar M., Garcés O., Castro H., et al. Evaluating the monolithic and the
    microservice architecture pattern to deploy web applications in the cloud 10th
    Computing Colombian Conference (10CCC), IEEE (2015), pp. 583-590 CrossRefView
    in ScopusGoogle Scholar [4] Lorido-Botran T., Miguel-Alonso J., Lozano J.A. A
    review of auto-scaling techniques for elastic applications in cloud environments
    J. Grid Comput., 12 (4) (2014), pp. 559-592 CrossRefView in ScopusGoogle Scholar
    [5] Hu G., Tay W.P., Wen Y. Cloud robotics: architecture, challenges and applications
    IEEE Netw., 26 (3) (2012), pp. 21-27 Google Scholar [6] Kamei K., Nishio S., Hagita
    N., et al. Cloud networked robotics IEEE Network, 26 (3) (2012), pp. 28-34 View
    in ScopusGoogle Scholar [7] Mell P., Grance T. The NIST definition of cloud computing
    Commun. Acm, 53 (6) (2011), pp. 1-7 Google Scholar [8] Ciurana E. Developing with
    Google App Engine Apress (2009) Google Scholar [9] Amazon Elastic Compute Cloud
    (EC2). http://www.amazon.com/ec2/. Google Scholar [10] Microsoft Azure. http://www.microsoft.com/windowsazure.
    Google Scholar [11] X. Chu, K. Nadiminti, C. Jin, et al. Aneka: Next-generation
    enterprise grid platform for e-science and e-business applications, in: IEEE International
    Conference on e-Science and Grid Computing, 2007, pp. 151-159. Google Scholar
    [12] Sun network.com (Sun Grid). http://www.network.com. Google Scholar [13] Inaba
    M. Remote-Brained robots Multisensor Fusion and Integration for Intelligent Systems,
    1994, IEEE (1997), pp. 747-754 Google Scholar [14] IEEE networked robots technical
    committee. [Online]. Available: http://www-users.cs.umn.edu/ isler/tc/. Google
    Scholar [15] Kamei K., Nishio S., Hagita N., et al. Cloud networked robotics IEEE
    Netw., 26 (3) (2012), pp. 28-34 View in ScopusGoogle Scholar [16] Arumugam R.,
    Enti V.R., Bingbing L., et al. DAvinCi: A cloud computing framework for service
    robots 2010 IEEE International Conference on Robotics and Automation (ICRA), IEEE
    (2010), pp. 3084-3089 CrossRefView in ScopusGoogle Scholar [17] Tenorth M., Kamei
    K., Satake S., et al. Building knowledge-enabled cloud robotics applications using
    the ubiquitous network robot platform IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS), IEEE (2013), pp. 5716-5721 CrossRefView in ScopusGoogle
    Scholar [18] Mester G. Cloud robotics model Interdisciplin. Descr. Complex Syst.,
    13 (1) (2015), pp. 1-8 CrossRefGoogle Scholar [19] Du Z., He L., Chen Y., et al.
    Robot Cloud: Bridging the power of robotics and cloud computing Future Gener.
    Comput. Syst., 21 (4) (2016), pp. 301-312 Google Scholar [20] Tian N., Matthew
    M., Mahler J., et al. A cloud robot system using the dexterity network and berkeley
    robotics and automation as a service (Brass) IEEE International Conference on
    Robotics and Automation (ICRA), IEEE (2017), pp. 1615-1622 CrossRefView in ScopusGoogle
    Scholar [21] Hunziker D., Gajamohan M., Waibel M., et al. Rapyuta: the roboearth
    cloud engine IEEE International Conference on Robotics and Automation (ICRA),
    IEEE (2013), pp. 438-444 CrossRefView in ScopusGoogle Scholar [22] Tenorth M.,
    Beetz M. KnowRob: a knowledge processing infrastructure for cognition-enabled
    robots Int. J. Robot. Res., 32 (5) (2013), pp. 566-590 CrossRefView in ScopusGoogle
    Scholar [23] Saxena A., Jain A., Sener O., et al. RoboBrain: large-scale knowledge
    engine for robots Comput. Sc. (2014) Google Scholar [24] Waibel M., Beetz M.,
    Civera J., et al. Roboearth IEEE Robot. Autom. Mag., 18 (2) (2011), pp. 69-82
    View in ScopusGoogle Scholar [25] Balalaie A., Heydarnoori A., Jamshidi P. Microservices
    architecture enables devops: migration to a cloud-native architecture IEEE Softw.,
    33 (3) (2016), pp. 42-52 View in ScopusGoogle Scholar [26] Pop D.P., Altar A.
    Designing an MVC model for rapid web application development Procedia Eng., 69
    (1) (2014), pp. 1172-1179 View PDFView articleView in ScopusGoogle Scholar [27]
    Esposito C., Castiglione A., Choo K.K.R. Challenges in delivering software in
    the cloud as microservices IEEE Cloud Comput., 3 (5) (2016), pp. 10-14 View in
    ScopusGoogle Scholar [28] Sill A. The design and architecture of microservices
    IEEE Cloud Comput., 3 (5) (2016), pp. 76-80 Google Scholar [29] Stubbs J., Moreira
    W., Dooley R. Distributed systems of microservices using docker and serfnode 2015
    7th International Workshop on Science Gateways (IWSG), IEEE (2015), pp. 34-39
    CrossRefView in ScopusGoogle Scholar [30] Bernstein D. Containers and cloud: From
    lxc to docker to kubernetes IEEE Cloud Comput., 1 (3) (2014), pp. 81-84 View in
    ScopusGoogle Scholar [31] Lee J.H., Morioka K., Ando N., et al. Cooperation of
    distributed intelligent sensors in intelligent environment IEEE/ASME Trans. Mechatronics,
    9 (3) (2004), pp. 535-543 View in ScopusGoogle Scholar [32] Richardson L., Amundsen
    M., Amundsen M., et al. RESTful Web APIs O’Reilly Media (2013) Google Scholar
    [33] Zaidi R. JavaScript object notation (JSON) JavaScript Essentials for SAP
    ABAP Developers, Apress (2017) Google Scholar [34] Dumusque X., Boisse I., Santos
    N.C. SOAP 2.0: A tool to estimate the photometric and radial velocity variations
    induced by stellar spots and plages Astrophys. J., 796 (2) (2014), p. 132 View
    in ScopusGoogle Scholar [35] Zheng Z., Zhang Y., Lyu M.R. Investigating QoS of
    real-world web services IEEE Trans. Serv. Comput., 7 (1) (2014), pp. 32-39 View
    in ScopusGoogle Scholar [36] Linthicum D.S. Practical use of microservices in
    moving workloads to the cloud IEEE Cloud Comput., 3 (5) (2016), pp. 6-9 Google
    Scholar [37] Brewer E.A. Kubernetes and the path to cloud native ACM Symposium
    on Cloud Computing, ACM (2015) 167-167 Google Scholar [38] Rosado T., Bernardino
    J. An overview of openstack architecture Proceedings of the 18th International
    Database Engineering & Applications Symposium, ACM (2014), pp. 366-367 CrossRefView
    in ScopusGoogle Scholar [39] Sefraoui O., Aissaoui M., Eleuldj M. OpenStack: toward
    an open-source solution for cloud computing Int. J. Comput. Appl., 55 (3) (2012),
    pp. 38-42 CrossRefGoogle Scholar [40] Mur-Artal R., Tardos J.D. ORB-SLAM2: an
    open-source SLAM system for monocular, stereo, and RGB-D cameras IEEE Trans. Robot.,
    33 (5) (2016), pp. 1255-1262 Google Scholar [41] Freiburg2_desk dataset. https://vision.in.tum.de/data/datasets/rgbd-dataset/download.
    Google Scholar [42] M. Quigley, K. Conley, B. Gerkey, et al. ROS: an open-source
    Robot Operating System, : inICRA workshop on open source software, Vol. 3(3.2),
    2009, p. 5. Google Scholar Cited by (0) Chongkun Xia received the B.S. degree
    from North China University of Water Resources and Electric Power, and the M.S.
    degree from Liaoning Shihua University. He is currently studying toward the Ph.D.
    degree with the College of Information Science and Engineering, Northeastern University,
    Shenyang, China. He has published several English research papers and conference
    papers. His research interests include cloud robotics, visual SLAM, object recognition
    and grasping. Yunzhou Zhang received B.S. and M.S. degree in Mechanical and Electronic
    engineering from National University of Defense Technology, Changsha, China in
    1997 and 2000, respectively. He received Ph.D. degree in pattern recognition and
    intelligent system from Northeastern University, Shenyang, China, in 2009. He
    is currently a professor with the Faculty of Robot Science and Engineering, Northeastern
    University, China. Now he leads the Cloud Robotics and Visual Perception Research
    Group. His research has been supported by funding from various sources such as
    National Natural Science Foundation of China, Ministry of science and technology
    of China, Ministry of Education of China and some famous high-tech companies.
    He has published many journal papers and conference papers in intelligent robots,
    computer vision and wireless sensor networks. His research interests include intelligent
    robot, computer vision, and sensor networks. Lei Wang received the B.S. degree
    from Northeastern University, Shenyang, China. He is currently working toward
    the Ph.D. degree with the Faculty of Robot Science and Engineering, Northeastern
    University, Shenyang, China. His research interests include cloud robotics, machine
    learning, reinforcement learning. Sonya Coleman received a BSc (Hons) in Mathematics,
    Statistics and Computing (first class) from the Ulster University, UK in 1999,
    and a PhD in Mathematics from the Ulster University, UK in 2003. She is a Professor
    and a leader in the Cognitive Robotics team of Intelligent Systems Research Centre.
    She is a Fellow of the Higher Education Academy. She has many publications in
    image processing, pattern recognition, computational intelligence and robotics.
    Her research has been supported by funding from various sources such as EPSRC
    , The Nuffield Foundation, The Leverhulme Trust and the European Commission. Additionally,
    she was co-investigator on the EU FP7 funded project RUBICON, the FP7 project
    VISUALISE and is currently co-investigator in the FP7 SLANDIAL project. She is
    also secretary of the Irish Pattern Recognition and Classification Society. Yanbo
    Liu received the B.S. degree from Harbin Engineering University. He is currently
    working toward the M.S. degree with the College of Information Science and Engineering,
    Northeastern University, Shenyang, China. His research interests include cloud
    robotics. ☆ Research supported by National Natural Science Foundation of China
    (No. 61471110, 61733003), National Key R&D Program of China (No. 2017YFC0805000/5005,
    2017YFB1301103), Fundamental Research Funds for the Central Universities, China
    (N160413002, N172608005), Natural Science Foundation of Liaoning Province (No.
    20180520040). 1 Ph.D. candidate. View Abstract © 2018 Elsevier B.V. All rights
    reserved. Recommended articles Context-aware 3D object anchoring for mobile robots
    Robotics and Autonomous Systems, Volume 110, 2018, pp. 12-32 Martin Günther, …,
    Joachim Hertzberg View PDF Curved path following control for planar eel robots
    Robotics and Autonomous Systems, Volume 108, 2018, pp. 129-139 Zhang AnFan, …,
    Wang MingHui View PDF A neuro-dynamic walking engine for humanoid robots Robotics
    and Autonomous Systems, Volume 110, 2018, pp. 124-138 Ghassan Atmeh, Kamesh Subbarao
    View PDF Show 3 more articles Article Metrics Citations Citation Indexes: 24 Captures
    Readers: 135 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: Robotics and autonomous systems (Print)
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: Microservice-based cloud robotics system for intelligent space
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1145/2949550.2949562
  analysis: '>'
  authors:
  - Spencer Julian
  - Michael Shuey
  - Seth Cook
  citation_count: 21
  full_citation: '>'
  full_text: '>

    This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesXSEDEProceedingsXSEDE16Containers
    in Research: Initial Experiences with Lightweight Infrastructure RESEARCH-ARTICLE
    SHARE ON Containers in Research: Initial Experiences with Lightweight Infrastructure
    Authors: Spencer Julian , Michael Shuey , Seth Cook Authors Info & Claims XSEDE16:
    Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at ScaleJuly
    2016Article No.: 25Pages 1–6https://doi.org/10.1145/2949550.2949562 Published:17
    July 2016Publication History 19 citation 937 Downloads eReaderPDF XSEDE16: Proceedings
    of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale Containers
    in Research: Initial Experiences with Lightweight Infrastructure Pages 1–6 Previous
    Next ABSTRACT References Cited By Recommendations Comments ABSTRACT HPC environments
    have traditionally existed installed directly on hardware or through virtual machine
    environments. Linux Containers, and Docker specifically, have gained extensive
    popularity; we believe this current trend toward containers and microservices
    can be applied to HPC to improve efficiency and quality of development and deployment.
    User interest in Docker is rising, with several communities planning production
    deployments. We describe some of our site''s experiences, along with an autoscaling
    web cluster and an autoscaling PBS-based computational cluster we have developed
    that are currently in a pre-production testing phase. Some basic performance tests
    are covered, comparing network and filesystem performance between a native Docker
    environment and a traditional Red Hat-based environment. In our tests, we noticed
    negligible differences in computational performance when run out of the box, approximately
    0.4%, but we required some minor tweaking in the form of additional docker plugins
    to achieve similar or better performance in the network and filesystem tests.
    While additional testing is needed for some aspects of computational clusters,
    particularly RDMA performance, we believe initial testing indicates Docker containers
    are ready for broader adoption at larger-scale production environments. References
    Matthew Heins. The Globalization of American Infrastructure: The Shipping Container
    and Freight Transportation. Routledge, 2016. Paul B Menage. Adding generic process
    containers to the linux kernel. In Proceedings of the Linux Symposium, volume
    2, pages 45--57. Citeseer, 2007. Geoffrey Fox, Judy Qiu, Shantenu Jha, Saliya
    Ekanayake, and Supun Kamburugamuve. Big data, simulations and hpc convergence.
    Show All References Cited By View all Recommendations State machine replication
    in containers managed by Kubernetes Computer virtualization brought fast resource
    provisioning to data centers and the deployment of pay-per-use cost models. The
    system virtualization provided by containers like Docker has improved this flexibility
    of resource provisioning. Applications ... Read More A performance comparison
    of linux containers and virtual machines using Docker and KVM Abstract Virtualization
    is a foundational element of cloud computing. Since cloud computing is slower
    than a native system, this study analyzes ways to improve performance. We compared
    the performance of Docker and Kernel-based virtual machine (KVM). KVM ... Read
    More Power consumption of visualization technologies: an empirical investigation
    UCC ''15: Proceedings of the 8th International Conference on Utility and Cloud
    Computing Virtualization is growing rapidly as a result of the increasing number
    of alternative solutions in this area, and of the wide range of application field.
    Until now, hypervisor-based virtualization has been the de facto solution to perform
    server ... Read More Comments Login options Check if you have access through your
    login credentials or your institution to get full access on this article. Sign
    in Full Access Get this Publication Information Contributors Published in XSEDE16:
    Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale
    July 2016405 pages ISBN: 9781450347556 DOI: 10.1145/2949550 General Chair: Kelly
    Gaither Copyright © 2016 ACM Permission to make digital or hard copies of all
    or part of this work for personal or classroom use is granted without fee provided
    that copies are not made or distributed for profit or commercial advantage and
    that copies bear this notice and the full citation on the first page. Copyrights
    for components of this work owned by others than the author(s) must be honored.
    Abstracting with credit is permitted. To copy otherwise, or republish, to post
    on servers or to redistribute to lists, requires prior specific permission and/or
    a fee. Request permissions from Permissions@acm.org. Publisher Association for
    Computing Machinery New York, NY, United States Publication History Published:
    17 July 2016 Permissions Request permissions about this article. Request Permissions
    Check for updates Author Tags DockerLXCcgroupscontainer Qualifiers Research-Article
    Research Refereed Limited Acceptance Rates Overall Acceptance Rate129of190submissions,68%
    Bibliometrics Citations19 Article Metrics 19 Total Citations View Citations 937
    Total Downloads Downloads (Last 12 months) 11 Downloads (Last 6 weeks) 0 Other
    Metrics View Author Metrics PDF Format View or Download as a PDF file. PDF eReader
    View online with eReader. eReader Matthew Heins. The Globalization of American
    Infrastructure: The Shipping Container and Freight Transportation. Routledge,
    2016. Paul B Menage. Adding generic process containers to the linux kernel. In
    Proceedings of the Linux Symposium, volume 2, pages 45--57. Citeseer, 2007. Geoffrey
    Fox, Judy Qiu, Shantenu Jha, Saliya Ekanayake, and Supun Kamburugamuve. Big data,
    simulations and hpc convergence. Tiffany Trader. Toward a converged exascale-big
    data software stack. 2016. http://hub.docker.com. Alan B. Craig. Science gateways
    for humanities, arts, and social science. In Proceedings of the 2015 XSEDE Conference:
    Scientific Advancements Enabled by Enhanced Cyberinfrastructure, XSEDE ''15, pages
    18:1--18:3, New York, NY, USA, 2015. ACM. https://github.com/jupyterhub/jupyterhub.
    Nicholas Davis. Exploring adverse drug effect data with apache spark, hadoop,
    and docker. 2015. P China Venkanna Varma, KV Kalyan Chakravarthy, V Valli Kumari,
    and S Viswanadha Raju. Analysis of network io performance in hadoop cluster environments
    based on docker containers. In Proceedings of Fifth International Conference on
    Soft Computing for Problem Solving, pages 227--237. Springer, 2016. Rui Zhang,
    Min Li, and Dean Hildebrand. Finding the big data sweet spot: Towards automatically
    recommending configurations for hadoop clusters on docker containers. In Cloud
    Engineering (IC2E), 2015 IEEE International Conference on, pages 365--368. IEEE,
    2015. Bukhary Ikhwan Ismail, Ehsan Mostajeran Goortani, Mohd Bazli Ab Karim, Wong
    Ming Tat, Sharipah Setapa, Jing Yuan Luke, and Ong Hong Hoe. Evaluation of docker
    as edge computing platform. In Open Systems (ICOS), 2015 IEEE Confernece on {sic},
    pages 130--135. IEEE, 2015. W. Felter, A. Ferreira, R. Rajamony, and J. Rubio.
    An updated performance comparison of virtual machines and linux containers. In
    Performance Analysis of Systems and Software (ISPASS), 2015 IEEE International
    Symposium on, pages 171--172, March 2015. https://github.com/jpetazzo/pipework.
    Victor Marmol, Rohit Jnagal, and Tim Hockin. Networking in containers and container
    clusters. In Proceedings of netdev 0.1, February 2015. Daniel Lezcano. LXC.CONTAINER.CONF(5)
    Man Page, May 2016. https://github.com/ContainX/docker-volume-netshare. Howto
    create docker container enabled with roce. https://community.mellanox.com/docs/DOC-1506,
    October 2014. Sage A Weil, Scott A Brandt, Ethan L Miller, Darrell DE Long, and
    Carlos Maltzahn. Ceph: A scalable, high-performance distributed file system. In
    Proceedings of the 7th symposium on Operating systems design and implementation,
    pages 307--320. USENIX Association, 2006. Craig A Stewart, Timothy M Cockerill,
    Ian Foster, David Hancock, Nirav Merchant, Edwin Skidmore, Daniel Stanzione, James
    Taylor, Steven Tuecke, George Turner, et al. Jetstream: a self-provisioned, scalable
    science and engineering cloud environment. In Proceedings of the 2015 XSEDE Conference:
    Scientific Advancements Enabled by Enhanced Cyberinfrastructure, page 29. ACM,
    2015. Nick Nystrom. Introduction to bridges: Connecting researchers, data and
    hpc. https://www.youtube.com/watch?v=hn3tPkZaY4U, January 2015. Richard L Moore,
    Chaitan Baru, Diane Baxter, Geoffrey C Fox, Amit Majumdar, Phillip Papadopoulos,
    Wayne Pfeiffer, Robert S Sinkovits, Shawn Strande, Mahidhar Tatineni, et al. Gateways
    to discovery: Cyberinfrastructure for the long tail of science. In Proceedings
    of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment,
    page 39. ACM, 2014. Douglas M Jacobsen and Richard Shane Canon. Contain this,
    unleashing docker for hpc. http://singularity.lbl.gov. Hsi-En Yu and Weicheng
    Huang. Building a virtual hpc cluster with auto scaling by the docker. arXiv preprint
    arXiv:1509.08231, 2015. E Mazzoni, S Arezzini, T Boccali, A Ciampa, S Coscetti,
    and D Bonacorsi. Docker experience at infn-pisa grid data center. In Journal of
    Physics: Conference Series, volume 664, page 022029. IOP Publishing, 2015. J Gomes,
    J Pina, G Borges, J Martins, N Dias, H Gomes, and C Manuel. Exploring containers
    for scientific computing. In 8th Iberian Grid Infrastructure Conference Proceedings,
    page 27. Bill McMillan and Chong Chen. High performance docking. Technical report,
    IBM, 2014. http://xcat-docs.readthedocs.io/en/stable/advanced/docker/index.html.
    Figures Other None Share this Publication link https://dl.acm.org/doi/10.1145/2949550.2949562
    Copy Link Share on Social Media Share on Twitter LinkedIn Reddit Facebook Email
    28 References View Table Of Contents Footer Categories Journals Magazines Books
    Proceedings SIGs Conferences Collections People About About ACM Digital Library
    ACM Digital Library Board Subscription Information Author Guidelines Using ACM
    Digital Library All Holdings within the ACM Digital Library ACM Computing Classification
    System Digital Library Accessibility Join Join ACM Join SIGs Subscribe to Publications
    Institutions and Libraries Connect Contact Facebook Twitter Linkedin Feedback
    Bug Report The ACM Digital Library is published by the Association for Computing
    Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Containers in Research
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.jss.2019.05.031
  analysis: '>'
  authors:
  - Minxian Xu
  - Rajkumar Buyya
  citation_count: 30
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. System
    architecture, design and implementation 4. Policies implemented in BrownoutCon
    5. Performance evaluation 6. Conclusions and future work Acknowledgments References
    Vitae Show full outline Cited by (35) Figures (13) Show 7 more figures Tables
    (2) Table 1 Table 2 Journal of Systems and Software Volume 155, September 2019,
    Pages 91-103 In Practice BrownoutCon: A software system based on brownout and
    containers for energy-efficient cloud computing Author links open overlay panel
    Minxian Xu a b, Rajkumar Buyya b Show more Share Cite https://doi.org/10.1016/j.jss.2019.05.031
    Get rights and content Highlights • Model that manages the containers and resources
    in a fine-grained manner. • Software system based on Docker Swarm to provide energy-efficient
    approaches. • Evaluations of proposed software system on French Grid’5000 infrastructure.
    Abstract VM consolidation and Dynamic Voltage Frequency Scaling approaches have
    been proved to be efficient to reduce energy consumption in cloud data centers.
    However, the existing approaches cannot function efficiently when the whole data
    center is overloaded. An approach called brownout has been proposed to solve the
    limitation, which dynamically deactivates or activates optional microservices
    or containers. In this paper, we propose a brownout-based software system for
    container-based clouds to handle overloads and reduce power consumption. We present
    its design and implementation based on Docker Swarm containers. The proposed system
    is integrated with existing Docker Swarm without the modification of their configurations.
    To demonstrate the potential of BrownoutCon software in offering energy-efficient
    services in brownout situation, we implemented several policies to manage containers
    and conducted experiments on French Grid’5000 cloud infrastructure. The results
    show the currently implemented policies in our software system can save about
    10%–40% energy than the existing baselines while ensuring quality of services.
    Previous article in issue Next article in issue Keywords Cloud data centersEnergy
    efficiencyQuality of serviceContainersMicroservicesBrownout 1. Introduction Cloud
    computing has been regarded as a new paradigm for resource and service provisioning,
    which has offered vital benefits for IT industry by lowering operational costs
    and human expenses. However, the huge amount of energy consumption and carbon
    emissions resulted from cloud data centers have become a significant concern of
    researchers. Nowadays, data centers contain thousands of servers and their sizes
    range from 300–4500 square meters, which can consume more than 27,000 kWh energy
    per day (Mastelic et al., 2015). It is estimated that, in 2010, the energy consumption
    of data centers consumed 1.1% to 1.5% of total electricity worldwide (Mastelic
    et al., 2015). Moreover, the excessive usage of brown energy to generate power
    increases the carbon emission. It is also reported that about 2% carbon emissions
    of total carbon amount released into the atmosphere worldwide are from data centers
    (Lavallée, 2014). Recently, some dominant service providers have established a
    community, called Green Grid, to promote energy-efficient techniques to minimize
    the environmental impact of data centers (Beloglazov et al., 2012). Unfortunately,
    reducing energy consumption is a challenging mission as applications and data
    are growing complex and consuming more computational resources (Liu et al., 2012).
    The applications and data are generally required to be processed within the required
    time, and to meet this requirement large and powerful servers are provisioned.
    To ensure the sustainability of future growth, cloud data centers are required
    to utilize the resource computing infrastructure efficiently and minimize energy
    consumption. To address this problem, the concept of green cloud was proposed,
    which aimed to reduce power consumption, energy cost, carbon emissions and also
    optimize renewable energy usage (Kong, Liu, 2015, Buyya, Srirama, Casale, Calheiros,
    Simmhan, Varghese, Gelenbe, Javadi, Vaquero, Netto, Toosi, Rodriguez, Llorente,
    Vimercati, Samarati, Milojicic, Varela, Bahsoon, Assuncao, Rana, Zhou, Jin, Gentzsch,
    Zomaya, Shen, 2018). Therefore, in addition to resource provisioning and Quality
    of Service (QoS) assurance, data centers are required to be energy-efficient.
    The dominant methods to improve resource utilization and reduce energy consumption
    are Virtual Machine (VM) consolidation (Beloglazov et al., 2012) and Dynamic Voltage
    Frequency Scaling (DVFS) (Kim et al., 2011). The VM consolidation method migrates
    VMs from underutilized hosts to minimize the number of active hosts, and the idle
    hosts are switched to low-power mode to save energy consumption. The DVFS method
    reduces energy usage by dynamically scaling voltage frequency. When the host is
    underutilized, the voltage frequency scales to a lower frequency to reduce power.
    These approaches have been proved to be efficient to save data center’s power
    consumption, however, when the whole data center is overloaded, both of them cannot
    function efficiently. For example, the VMs can not be migrated if all the hosts
    are overloaded. In data centers, another reason for high energy consumption lies
    in that computing resources are inefficiently utilized by applications. Thus,
    applications are currently built with microservice paradigm in order to utilize
    resources more efficiently. Microservice is referred as a set of self-contained
    application components. The components encapsulate their logic and expose their
    functionality via interfaces to enable flexible deployment and replacement. With
    microservices or components, developers and users can gain technological heterogeneity,
    resilience, scalability, ease of deployment, organizational alignment, composability
    and optimization for replaceability (Newman, 2015). In addition, microservices
    also brings the benefits of more fine-grained utilization control over the application
    resource. To overcome the limitations of VM consolidation and DVFS, as well as
    improve the utilization of applications, we take advantage of brownout, a paradigm
    inspired from voltage shutdown to cope with emergency cases, in which the light
    bulbs emit fewer lights to save power (Xu and Buyya, 2019). Brownout is also applied
    to cloud scenarios, especially for microservices or application components that
    are allowed to be shortly deactivated to enhance system robustness. In brownout-compliant
    microservices, a control knob called dimmer is used to show the probability that
    whether a microservice should be executed or not (Klein et al., 2014). When requests
    are bursting and the system becomes overloaded, the brownout is triggered to temporally
    degrade the user experience, so that relieving the overloaded situation as well
    as saving energy consumption. Microservices can be featured with brownout characteristic.
    An example of online shopping system with a recommendation engine is introduced
    in (Klein et al., 2014). The recommendation engine enhances the function of the
    system and increases profits via recommending products to users. However, because
    the engine is not the necessary component and it requires more resources in comparison
    to other components, it is not mandatory to keep running all the time, especially
    under the overloaded situation when requests have a long delay or even not served.
    Deactivating the engine enables service providers to serve more requests with
    essential requirements or QoS constraints. Apart from this example, brownout paradigm
    can also be applied to other systems that allow application components to be deactivated,
    especially for the container-based system that applications are built with microservice
    paradigm. With container technology, the microservices can be functionally isolated,
    thus the deactivation of some microservices will not influence other microservices.
    In addition, as microservices are light-weight, they can be deactivated/activated
    quickly to support the brownout approach. In this paper, we propose and develop
    a software system, called BrownoutCon, which is inspired by brownout-based approach
    to deliver energy-efficient resource scheduling. The implementation of BrownoutCon
    is based on Docker Swarm (Docker, 2017) that provides the management of container
    cluster. The software system is designed and implemented as an add-on for Docker
    Swarm, which has no necessity to modify the configurations of Docker Swarm. The
    system also applies the public APIs of Grid''5000 (2017), which is a real testbed
    that provides power measurement for hosts. The aims of BrownoutCon are twofold:
    (1) providing an open-source software system based on brownout and Docker Swarm
    to manage containers; (2) offering an extensible software system for conducting
    research on reducing energy consumption and handling overloads in cloud data centers.
    The BrownoutCon is designed and implemented by following the brownout enabled
    system model in our previous works (Xu, Dastjerdi, Buyya, 2016, Xu, Buyya, 2017).
    Mandatory containers and optional containers are introduced in the system model,
    which are identified according to whether the containers can be temporarily deactivated
    or not. The brownout controller is the key part of the system model to manage
    brownout, which also provides the scheduling policies for containers. The problem
    of designing the brownout controller splits into several sub-problems: 1. Predicting
    the future workloads, so that the system can avoid overloads to foster the system
    robustness. 2. Determining whether a host is overloaded or not, so that the brownout
    controller will be triggered to relieve the overloads. 3. Deciding when to disable
    the containers, so that the system can relieve overloads and reduce energy consumption
    while ensuring QoS constraints. 4. Selecting the containers to be disabled, so
    that a better trade-off can be achieved between the reduced energy and QoS constraints.
    5. Deciding when to turn the hosts on or into the low-power mode, so that the
    idle hosts can be switched into low-power mode to save power consumption. Compared
    with VM consolidation approaches, the software system based on brownout and containers
    has two advantages: (1) a container can be stopped or restarted in seconds, while
    VM migration may take minutes. Thus, scheduling with containers is more light-weight
    and flexible than VMs. (2) the brownout-based approach provides another optional
    energy-efficient approach apart from VM consolidation and DVFS, which is also
    available to be combined with VM consolidation to achieve better energy efficiency,
    especially for the situation when the whole data center is overloaded. To evaluate
    the proposed system in practice, we conduct our experiments on Grid''5000 (2017)
    real testbed. We also evaluate the performance of proposed system with real traces
    derived from Wikipedia1 workloads. The main contributions of our work are as follows:
    • Proposed an effective system model that enables brownout approach to manage
    the containers and resources in a fine-grained manner; • Designed and developed
    a software system based on Docker Swarm to provide energy-efficient approaches
    for cloud data centers; • Experimental evaluations of our proposed software system
    on French Grid’5000 infrastructure for service providers to deploy microservices
    in an energy-efficient manner while ensuring QoS constraints. The rest of the
    paper is organized as follows. Section 2 discusses the related work, followed
    by the system design and implementation in Section 3. Brownout-based policies
    implemented in BrownoutCon are presented in Section 4. In Section 5, we introduce
    our experiments setup and evaluate the performance of implemented policies under
    Grid’5000 testbed. Conclusions along with future work are presented in Section
    6. 2. Related work It is estimated that U.S. data centers will consume 140 billion
    kWh of electricity annually by the year 2020, which equals to the annual output
    of about 50 brown power plants that have high carbon emissions (Delforge, Bawden).
    To minimize the operational expenses and impacts on the environment, a variety
    of state-of-the-art works have been conducted to reduce data center energy consumption.
    There is a close relationship between resource utilization and energy consumption,
    as inefficient utilization of resource contributes to more power consumption (Kaur
    and Chana, 2015). Virtualization is an import technique in Clouds and it can improve
    resource utilization. Therefore, numerous energy-efficient resource scheduling
    approaches based on VM consolidation have been proposed. Consolidating VMs on
    fewer physical machines and turning the unused machines into the low-power mode
    reduce the number of active machines. Beloglazov et al. (2012) proposed several
    VM consolidation algorithms to save data center energy consumption. The VM consolidation
    process has been modeled as a bin-packing problem, where VMs are regarded as items
    and hosts are regarded as bins. The objective of these VM consolidation algorithms
    is mapping the VMs to hosts in an energy-efficient manner. Based on the VM consolidation
    approaches in this work, other works like (Belog1azov, Buyya, 2012, Chen, Chen,
    Zheng, Cui, Qian, 2015, Han, Tan, Chen, Wang, Chen, Lau, 2016) have extended them
    to improve algorithm performance. Zhang et al. (2019) proposed VM allocation algorithm
    based on evolution algorithm to achieve energy efficiency in cloud data centers
    for reserved services. Experiments under both simulation and realistic environments
    showed that the proposed approach can effectively reduce energy consumption for
    a set of reserved VMs. Li et al. (2017) developed a Bayesian network-based estimation
    model for VM consolidation and took nine data center factors into consideration.
    The proposed approach can reduce energy consumption while ensuring QoS by avoiding
    inefficient VM migrations. Another dominant approach to reduce energy consumption
    is Dynamic Voltage Frequency Scaling (DVFS). The DVFS approaches achieve energy
    reduction by adjusting frequencies of processors rather than using less active
    servers in VM consolidation. The DVFS approach investigates a trade-off between
    energy consumption and computing performance, where processors lower their frequency
    when they are lightly loaded and utilize full frequency when loads are heavy.
    Kim et al. (2011) modeled real-time service as real-time VM requests, and proposed
    several DVFS algorithms to reduce energy consumption for the DVFS-enabled cluster.
    Arroba et al. (2015) proposed an approach combines DVFS and VM consolidation techniques
    by considering energy consumption and performance degradation together. Teng et
    al. (2016) presented several heuristic algorithms combining DVFS and VM consolidation
    together for batch-oriented scenarios. Fan et al. (2017) presented an online energy
    management approach by dynamically configuring voltage frequencies to minimize
    the power consumption for single processor scheduling while ensuring reliability
    requirement. All these approaches are focusing on developing algorithms for energy
    efficiency purposes. Some research taking both energy consumption and QoS into
    account have been conducted, which is also the consideration of our proposed software
    prototype system. Dou et al. (2016) introduced an energy-aware dynamic VM scheduling
    approach for QoS enhancement in Clouds for big data, which aimed to benefit users
    with discount prices and reduce the execution time of tasks. Adhikary et al. (2017)
    developed a QoS-aware and energy-aware cloud resource management approach for
    multimedia applications, and proposed two distributed and localized resource management
    algorithms based on energy consumption and resource demands. VM consolidation
    and DVFS have been proven to be efficient to reduce energy consumption in both
    theory and practice, however, both of them cannot function well when the whole
    data center is overloaded. Thus, brownout is applied to handle data center overloads
    and reduce energy consumption. Klein et al. (2014) applied brownout to design
    more robust applications under the overloaded or unpredicted situation. In our
    previous work, brownout was applied to save energy consumption in data centers.
    In Xu et al. (2016), we presented the brownout enabled system model and proposed
    several heuristic policies to find the microservices or application components
    that should be deactivated for energy saving purpose. The results showed that
    a trade-off existed between energy consumption and discount, and in Xu and Buyya
    (2017), we adopted approximate Markov Decision Process to improve the trade-off.
    Compared to the existing energy-efficient approaches based on VMs, our software
    system is based on containers. Container technology is derived from the Linux
    LXC techniques (Bernstein, 2014), which provides mechanism to isolate processes
    on a shared operating system. Compared with VMs, containerization provides a fine-grained
    control on microservice resource usage and is more light-weight. Kozhirbayev and
    Sinnott (2017) compared several existing container-based technologies for Clouds
    and evaluated their strength and weakness. They concluded that containers can
    give almost the same performance of native systems. As the main reason of the
    energy consumption issue in clouds is due to the inefficient resource usage, and
    containers can provide a more fine-grained control on resources compared with
    VMs, we consider to apply container technology for energy efficiency purposes.
    Currently, container technology is mostly focused on the orchestration of construction
    and deployment for containers (Pahl, Brogi, Soldani, Jamshidi, 2017, Rodriguez,
    Buyya, 2018), and it has been applied for various purposes, such as scalability
    (Hightower et al., 2017), high availability (Naik, 2016), high utilization (Vavilapalli
    et al., 2013), high throughput (Schwarzkopf et al., 2013), and QoS (Boutin et
    al., 2014). For example, Liu et al. (2016) proposed a flexible container-based
    computing platform for scientific workflow. Baresi et al. (2016) introduced MicroCloud,
    which is a container-based solution for managing cloud resource efficiently. Santos
    et al. (2018) evaluated the energy consumption of different applications executed
    in Docker and bare metal. However, the energy efficient scheduling is not considered
    in these container-based work. In our previous work (Xu et al., 2018), we have
    proposed an approach for managing energy in container-based clouds while focusing
    on scheduling algorithms design. Whereas, in this paper, we focus on the design
    and development of a new software system supporting brownout-based energy-efficient
    management of clouds. Some of the software systems supported energy-efficient
    resource management in Clouds, including OpenStack Neat (Beloglazov and Buyya,
    2015), Parasol (Goiri et al., 2013) and vGreen (Dhiman et al., 2009). However,
    none of them support brownout. Table 1shows the comparison of related work. To
    the best of our knowledge, BrownoutCon is the first software system developed
    to reduce energy consumption with brownout based on containers, which also considers
    both energy consumption and QoS. Table 1. Comparison of related work Approach
    Key Technique Management Unit Optimization Objective Focus DVFS VM Consolidation
    Brownout Processor Host VM Container Energy QoS/SLA Algorithm Design Software
    System Beloglazov et al. (2012); Belog1azov and Buyya (2012) √ √ √ √ √ √ Chen
    et al. (2015) √ √ √ √ √ √ Han et al. (2016) √ √ √ √ √ √ Zhang et al. (2019) √
    √ √ √ √ √ Li et al. (2017) √ √ √ √ √ √ Kim et al. (2011) √ √ √ √ √ Arroba et al.
    (2015) √ √ √ √ √ √ √ √ Teng et al. (2016) √ √ √ √ √ √ √ √ Fan et al. (2017) √
    √ √ √ √ Dou et al. (2016) √ √ √ √ √ √ Adhikary et al. (2017) √ √ √ √ √ √ Klein
    et al. (2014) √ √ √ Xu et al. (2016) Xu and Buyya (2017) √ √ √ √ √ √ √ Liu et
    al. (2016) √ √ √ Baresi et al. (2016) √ √ √ Xu et al. (2018) √ √ √ √ √ √ Beloglazov
    and Buyya (2015) √ √ √ √ √ √ Goiri et al. (2013) √ √ √ BrownoutCon √ √ √ √ √ √
    3. System architecture, design and implementation The purpose of BrownoutCon is
    to provide a software system based on brownout and containers for energy-efficient
    cloud data centers. The system takes advantage of public APIs of Docker Swarm
    and is evaluated under Grid’5000 testbed. The system is designed to be extensible,
    which means new components can be added without the necessity to modify the original
    codes or configurations of Docker Swarm and Grid’5000. Our software system is
    deployed on Docker Swarm master and worker nodes. Docker Swarm provides a platform
    for managing container cluster, monitoring status of swarm master and worker nodes,
    deploying containers on nodes, collecting resource usage of containers, controlling
    the lifecycle of containers, sending messages and commands between the master
    and worker nodes. Docker Swarm needs to be deployed on physical machines or virtual
    machines. Therefore, we adopt Grid’5000, a real testbed that provides access to
    ample resources for Docker Swarm deployment. We also take advantage of the Grid’5000
    APIs to collect the energy consumption data of the machines. In the following
    sections, we discuss the system requirements, assumptions, system design and its
    implementation. 3.1. Requirements and assumptions The components of the proposed
    software prototype system are running on the Docker Swarm master and worker nodes.
    Our current implementation assumes that a single instance of each key components
    is invoked on the master node, such as components for controlling brownout, monitoring
    system status, managing deployment policies and managing models. On each worker
    node, a single instance of a component that collects node information is running.
    When new nodes are joining the Docker Swarm as worker nodes, the master node is
    responsible for deploying containers to the nodes. BrownoutCon saves the energy
    consumption and handles overloads via temporarily disabling some containers, therefore,
    we assume that the services in the target system (e.g. e-commerce system) are
    implemented with microservice paradigm and some services (e.g. recommendation
    engine service) are not necessary to keep running all the time. The main optimization
    objective of our software system is reducing energy consumption, a precise power
    probe to collect energy usage is required. Container scheduling policies may use
    the energy usage data to make decisions on controlling containers. Another requirement
    is that a manager is needed to control all the hosts to turn them into low-power
    mode or active. This manager is used by the brownout controller on master node
    to connect with other worker nodes via the communication protocol. Grid’5000 has
    provided the APIs to switch the status of hosts, and more details will be introduced
    in the following sections. 3.2. BrownoutCon architecture Fig. 1 depicts the architecture
    of BrownoutCon, and the details of the main components are introduced as below:
    Download : Download high-res image (443KB) Download : Download full-size image
    Fig. 1. BrownoutCon architecture. (1) Users: This component contains user and
    requests information. It also captures system configurations such as predefined
    QoS constraints (e.g. average response time and SLA violations), energy budget
    and service deployment patterns according to users’ demand. (2) Cloud service
    repository: This component manages the services offered to users, including service
    information, such as service name and image. Each service may be constructed via
    a set of microservices. In order to manage microservices with brownout, the microservices
    are identified as mandatory or optional. a. Mandatory microservices: These microservices
    keep running all the time when they are started and cannot be temporarily stopped,
    like database related microservices. b. Optional microservices: These microservices
    can be deactivated temporarily depending on system status. Microservices are connected
    if there are communications between them. We consider that if one optional microservice
    is deactivated, then other connected microservices should also be deactivated.
    Notes: A microservice can be identified as optional if the service/content it
    provides is defined as optional by its creators. For instance, the online recommendation
    engine in the online shopping system and the spell checker in the online editor
    system can be identified as optional microservices under resource constrained
    situations. (3) Execution environment: This component provides the container-based
    environment for microservices or containers. The dominant execution environments
    for microservices or containers are Docker, Kubernetes, and Mesos. In BrownoutCon,
    we use Docker as the execution environment for microservices. (4) Brownout controller:
    This component controls optional microservices or containers based on system status.
    It applies policies introduced in Section 4 to provide an efficient solution for
    managing brownout and containers. As noted in Section 1, brownout has a control
    knob called dimmer that represents the probability to execute microservices. We
    make some adjustments to make the dimmer of brownout to be adapted to this component
    as well as our architecture. Our dimmer is only applied to the optional microservices
    and its value is computed according to the severity of system overloads (the number
    of overloaded hosts in the data center). (5) System monitor: It is a component
    that monitors the health of nodes and collects hosts resource consumption status.
    It uses the third-party toolkit to support its function, such as Grid’5000 public
    APIs that provide real-time data on infrastructure metrics, including host health,
    CPU utilization, and power consumption. (6) Scheduling policy manager: This component
    provides and manages the policies for Brownout Controller to schedule microservices/containers.
    In order to ensure the energy budget and QoS constraints, different policies designed
    for different preferences are required. For instance, if the service provider
    wants to balance the trade-off between energy and QoS, then a policy that considers
    the trade-off is preferred. (7) Models management: This component maintains the
    energy consumption and QoS models in the system. In BrownoutCon, the power consumption
    model is closely related to the utilization of microservice or container, and
    the QoS model is applied to define the QoS constraints. (8) Cloud infrastructure:
    Under Infrastructure as a Service model, it is a component that offers physical
    resources to users, where microservices or containers are deployed. In our experiments,
    we use Grid’5000 as our infrastructure. More details are given in Section 5. 3.3.
    Energy-efficient scheduling architecture The main purpose of our software system
    is energy efficiency, and the main approach to achieve this goal is through energy-efficient
    scheduling policies. Deriving from BrownoutCon architecture, Fig. 2 shows the
    energy-efficient scheduling architecture based on brownout, which depicts the
    BrownoutCon from the energy-efficient scheduling perspective. Download : Download
    high-res image (254KB) Download : Download full-size image Fig. 2. Energy-efficient
    scheduling architecture. In this scheduling architecture, clients submit their
    requests to the system, and Docker Swarm Manager dispatches the requests to containers
    and hosts. The System Monitors collect the energy and utilization information
    from hosts, and then send the information to Brownout Controller. With the information
    from System Monitors, the Brownout Controller refers to the host power consumption
    or utilization models to compute how much utilization/energy should be reduced.
    Then the Brownout Controller makes decisions based on scheduling policies to switch
    the states of hosts and containers, such as turning the hosts into low-power mode
    or deactivating containers. 3.4. Integration with Docker Swarm BrownoutCon is
    installed on Docker Swarm node independently of Docker Swarm services. In addition,
    the activities of BrownoutCon are transparent to the Docker Swarm services, which
    means Docker Swarm does not need to reconfigure to fit with BrownoutCon and use
    its brownout feature. In other words, BrownoutCon can be installed on existing
    Docker Swarm cluster without modifying the configurations. BrownoutCon achieves
    the transparency via the interactions with the public APIs of Docker Swarm cluster.
    BrownoutCon uses the APIs to obtain information about containers deployment, containers
    utilization, and containers properties. Although the operations of BrownoutCon
    will affect the system status and containers state by deactivating or activating
    containers, it is transparently processed by Docker Swarm public APIs. The implication
    of this integration approach represents that the container deployment is handled
    by Docker Swarm, and BrownoutCon makes decisions on deactivation or activation
    of containers. Fig. 3 shows how BrownoutCon is integrated into Docker Swarm. In
    Docker Swarm, the nodes are categorized as two classes: swarm master node and
    swarm worker node. The master node is responsible for maintaining cluster state,
    scheduling services (containers) and serving swarm mode with Docker APIs over
    HTTP, while the purpose of worker nodes is executing containers. The respective
    BrownoutCon components are deployed on master and worker nodes. Download : Download
    high-res image (306KB) Download : Download full-size image Fig. 3. BrownoutCon
    integrated with Docker Swarm. 3.5. Containers deployment with compose file Docker
    provides compose2 tool to deploy multiple containers, in which a configuration
    file is used to configure containers properties. With the compose file, the containers
    can be easily deployed and managed on clusters. In the compose file of our web
    application, to identify the recommendation engine microservice as optional, we
    labeled it as optional in the brownout feature. Moreover, as previously mentioned,
    the optional containers are only allowed to be deployed on the worker node, thus,
    we configure the placement constraint of this microservice as the worker node.
    More deployment properties can also be configured in the compose file. 3.6. Entity
    interaction diagram To implement the aforementioned architecture and functionalities,
    we use Java to develop our software system. The main classes of BrownoutCon are
    depicted in Fig. 4. The details of these classes are as below: Download : Download
    high-res image (421KB) Download : Download full-size image Fig. 4. Entity interactions
    in BrownoutCon. Docker Swarm API: This class wraps Docker Swam APIs and provides
    the interface for BrownoutController class to call. The Docker Swarm APIs offer
    the functions to fetch the information of containers and operate on containers,
    such as collecting containers utilization, containers id, containers property
    (optional or mandatory), deploying and updating containers with the compose file,
    deactivating and activating containers. Grid’5000 API: This class uses Grid’5000
    APIs to collect hosts energy consumption and switch status of hosts. Grid’5000
    provides APIs to gather the total power at per second rate for all the hosts in
    data center. The APIs also allow BrownoutController class to switch the hosts
    into low power mode or turn the hosts on. WorkerNode: This class models the host
    in the data center. Attributes of a WorkerNode include the CPU utilization and
    the containers deployed on the host. To be consistent with the status of real
    hosts, when the software system is running, the WorkerNode instances will keep
    updating their CPU utilization and container lists. AbstractMonitor: It provides
    an interface to monitor the status system. With the monitored information, the
    system can know how many hosts are overloaded and make decisions based on this
    information. Other monitors, such as memory or network monitors can be extended
    if they implement the AbstractMonitor. Container: The Container class models the
    containers deployed on hosts. The class defines the basic information of containers,
    including container id, CPU utilization and other information that can be fetched
    via Docker Swarm APIs. AbtractPolicy: It is an interface that defines the functions
    that scheduling policies should implement. To deactivate some containers temporarily
    and reduce energy consumption, the policies that implement the AbstractPolicy
    interface are responsible for finding the containers that should be deactivated.
    The details of our implemented policies in BrownoutCon will be introduced in Section
    4. BrownoutController: This class is the core class of our software system. It
    assembles all the information from different sources and makes the decision for
    controlling hosts and the containers on them. BrownoutController knows system
    status from Docker Swarm APIs, Grid’5000 APIs and WorkerNode instances, and triggers
    brownout to handle overloads and reduce energy consumption via operations on hosts
    or containers. 3.7. Sequence diagram To provide an in-depth understanding of the
    working process of BrownoutCon, Fig. 5 shows a sequence diagram of handling requests
    by our software system. Firstly, the users submit their requests to a web application
    called Weave Shop (more details about this application will be introduced in Section
    5.2) Then the Weave Shop sends the information of requests to BrownoutCon, and
    BrownoutCon keeps collecting nodes and containers information periodically via
    Grid’5000 and Docker Swam public APIs, respectively. When BrownoutCon is collecting
    information, if the system is overloaded, which means the Weave Shop cannot handle
    all the incoming requests, the BrownoutCon adds nodes to serve requests. The BrownoutCon
    also triggers brownout-based policies to deactivate containers to relieve overloads
    and reduce energy consumption. After these operations, the information of the
    nodes and containers are updated. Once the system is not overloaded, BrownoutCon
    activates the containers or removes the nodes from active nodes list (switching
    nodes into low-power mode). Upon the completion of operations on containers and
    nodes, the updated information is sent to BrownoutCon. Download : Download high-res
    image (489KB) Download : Download full-size image Fig. 5. Sequence diagram of
    handling requests by BrownoutCon. 4. Policies implemented in BrownoutCon To demonstrate
    BrownoutCon software system capability, we plugged/incorporated some policies
    originally evaluated by simulations in Xu et al. (2016). As noted in Section 1,
    the scheduling problem can be divided into several sub-problems: (1) workload
    prediction; (2) overloaded status detection (3) brownout trigger; (4) deactivated
    containers selection; and (5) hosts scaling. In this section, we will introduce
    the implemented policies for reference. It is noted that the introduced policies
    are not the main focus of this paper. The focus of this work is designing and
    implementing the software system based on brownout and containers. 4.1. Workload
    prediction We apply workload prediction to avoid overloads and improve system
    robustness. To predict the future workloads based on the previous workloads, we
    adopt the sliding windows as presented in Algorithm 1. The motivation of sliding
    windows is giving more weights to the request rates of recent time intervals.
    Let Lw to be the window size that is a constant integer value, e.g. 5, num(k)
    to be the actual number of requests at time interval k, we estimate the number
    of requests at the time interval t as the average number of requests in the previous
    Lw windows as shown in Eq. (1). To ensure enough historical data to be used for
    prediction, the time interval t for requests prediction should be no less than
    the window size Lw, for instance, if the window size the time interval for requests
    predication can start from 5. The predicted number of requests at time interval
    5 equals to the average number of actual requests of . The sliding window is moving
    forward along with the time. Based on the predicted workloads, the number of active
    hosts can be dynamically scaled in and out, which will be introduced in Section
    4.5. The performance of Algorithm 1 will be evaluated in Section 5.1. (1) Download
    : Download high-res image (101KB) Download : Download full-size image Algorithm
    1. Algorithm for predicting future workload based on sliding windows. 4.2. Overloaded
    host detection In our experiments, we use a predefined overloaded threshold to
    detect whether a host is overloaded or not. For instance, if the overloaded threshold
    is defined as 85%, the host is regarded as overloaded when its CPU utilization
    is above 85%. Currently, we only adopt CPU utilization to detect the overloaded
    host. Eqs. (2)and (3) show the way to calculate the number of the overloaded host.
    We use to denote whether host i is overloaded or not, which is detected by the
    utilization ui and overloaded threshold Tu. If ui is no less than Tu, equals to
    1, otherwise it equals to 0. The total number of the overloaded host is denoted
    as no, which is the sum of for all the hosts. (2) (3) 4.3. Brownout trigger Once
    there are hosts detected as overloaded, the brownout mechanism will be triggered
    to handle the overloads as well as to reduce energy consumption. As noted in Section
    1, firstly, the algorithm is required to calculate the dimmer value, which is
    the control knob to represent the probability to trigger brownout on hosts. The
    dimmer value θt at time t is calculated based on the number of overloaded hosts
    no as shown in Eq. (4): (4) Then the algorithm computes the expected utilization
    reduction on overloaded hosts. The expected utilization reduction of host i is
    the product of dimmer value θt and the host utilization ui as: (5) 4.4. Deactivated
    containers selection Based on the expected utilization reduction, the policies
    select containers to deactivate based on different containers selection policies.
    In BrownoutCon, we have implemented three containers selection policies for deactivation.
    Based on the strategy design pattern3, these policies implement the AbstractPolicy
    interface in Fig. 4 and can be selected independently at runtime. 4.4.1. Lowest
    utilization container first policy The Lowest Utilization Container First (LUCF)
    policy selects a set of containers to reduce the utilization of overloaded hosts.
    The objective of LUCF is that the utilization after reduction is expected to be
    less than the overloaded threshold, and the difference between the expected utilization
    reduction and the sum of deactivated containers utilization is minimized. Thus,
    the host utilization is reduced and the reduced utilization is close to the expected
    reduction. The deactivated container list is defined in Eq. (6). We use to denote
    the utilization of host i after the containers in the deactivated lists are deactivated,
    which equals to . The utilization of all the containers in dcli is denoted as
    . The represents the to minimize the absolute value of . (6) Algorithm 2 presents
    the pseudocode of LUCF. The LUCF sorts the optional containers list ocli based
    on container utilization in ascending order so that the container with the lowest
    utilization is at the head of the list. The size of ocli is ocli.size(). The algorithm
    checks the hosts one by one, if the first container c0 on host i has the utilization
    greater than c0 is put into the deactivated container list dcli. Since we consider
    connected microservices, the policy also adds the container’s connection tag (a
    string value) that indicates how it is connected with other containers into a
    set S for recording connections. However, if the utilization of the first container
    is less than the expected utilization reduction, LUCF finds a containers sublist
    to deactivate more containers. The sublist is the one that has the sum of utilization
    that is closest to the expected utilization reduction than other sublists. Same
    as previous operations, these containers are put into the deactivated container
    list dcli and their connection tags are put into the set S. Then, the algorithm
    finds other connected containers and put them into the deactivated container list.
    Download : Download high-res image (144KB) Download : Download full-size image
    Algorithm 2. Lowest Utilization Container First policy (LUCF). 4.4.2. Minimum
    number of containers first policy As formalized in Eq. (7), in order to deactivate
    fewer containers so that more optional functionalities can be provided, we also
    implement Minimum Number Containers First (MNCF) policy, which selects the minimum
    number of containers while saving the power consumption. Since it is quite similar
    to the LUCF, the pseudocode of MNCF is not provided here. The min (dcli.size())
    represents the objective to minimize the size of the deactivated container list.
    (7) 4.4.3. Random container selection policy Based on a uniformly distributed
    discrete random variable X that selects a subset of dcli randomly, the Random
    Container Selection (RCS) policy uses uniform distribution function to randomly
    select a number of optional containers to reduce energy consumption, as presented
    in Eq. (8). (8) 4.5. Hosts scaling To scale the number of active hosts, we adopt
    the hosts scaling algorithm in (Toosi et al., 2017) as shown in Algorithm 3, which
    is a predefined threshold-based approach. With profiling experiments, we set the
    overloaded requests threshold as the number of requests when the host cannot respond
    within an acceptable time limit. The algorithm computes the required hosts as
    the predicted number of request divided by the profiling number of requests of
    the overloaded threshold. If the required number of hosts is more than current
    active hosts, more hosts will be added to provide services, otherwise, if current
    active hosts are adequate, then the excess machines can be set as low-power mode
    to save energy consumption. Download : Download high-res image (119KB) Download
    : Download full-size image Algorithm 3. Hosts scaling algorithm. 5. Performance
    evaluation In this section, we evaluate our proposed software prototype system
    by conducting experiments under Grid’5000 infrastructure. The goals of this section
    include: (1) evaluating the behavior of the software system in an experimental
    environment, and (2) demonstrating suitability of the proposed system to enable
    experimental evaluations and scheduling policies in a practical setting. 5.1.
    Workload traces To make the experiments reproducible, we use the real trace from
    Wikipedia requests on October 17, 2017 to replay the workload of Wikipedia users.
    The trace includes data on requests time stamp and their accessed pages. We filter4
    the requests based on per second rate and generate the requests rate. The original
    request rate is around 1500–3000 per second. To scale the workload set to fit
    with our experiments, we use 10% of the original user requests size. Fig. 6 shows
    the requests rate per second during the day. The blue line is the actual trace
    derived from Wikipedia and the red line is the predicted trace based on the sliding
    window (sliding window size is 5) as introduced in Section 4. We can observe some
    anomalies during intervals 600–800, which can be due to the unpredicted network
    congestion. While during most time intervals, the variances between actual trace
    and predicted trace are small. Download : Download high-res image (209KB) Download
    : Download full-size image Fig. 6. Requests rate of Wikipedia trace (For interpretation
    of the references to color in this figure, the reader is referred to the web version
    of this article.). We conduct statistical analysis for the actual and predicted
    traces with Root Mean Square Error (RMSE) metric, which has been widely used in
    statistical analysis to verify experimental results and measures the average spread
    of errors as where S is the size of the trace, is the predicted value and Is is
    the actual value. If RMSE is small, it means the predicted values are close to
    the actual values. In our experiments, the which represents the predicted trace
    is close to the actual one, and the predicted trace can serve a guide for host
    scaling strategy. 5.2. Application deployment We use the Weave Shop5 web application
    that implemented with containers as the application in our scenario. The Weave
    Shop is a shopping system for selling socks online and has multiple microservices,
    including user microservice to handle user login, user database microservice for
    user information storage, payment microservice to process transactions, font-end
    microservice to show the user interface, catalog microservice for managing item
    for sale and etc. As these microservices are implemented independently, they can
    be deployed and controlled without impacting other microservices. The application
    is deployed by the compose file as introduced in Section 3.5, and part of the
    microservices are configured as optional, e.g. recommendation engine is noted
    as optional. The user interface may be influenced due to the deactivation of some
    microservices. Fig. 7 shows the user interface of Weave Shop application. Fig.
    7(a) is the user interface when full services are provided during no resource
    saturated scenario, while Fig. 7(b) illustrates the user interface when brownout
    is triggered and the recommendation engine service/container is deactivated. As
    a result, other recommended products are not showed in Fig. 7(b). Download : Download
    high-res image (692KB) Download : Download full-size image Fig. 7. Impact on user
    interface when brownout is triggered. 5.3. Performance metrics We adopt total
    energy consumption, average response time and SLA violation ratio as our performance
    metrics, and their definitions are as follows: Total energy consumption: The total
    energy consumption represents the amount of energy that is consumed by the software
    system. The key reasons for adopting this metric are: (1) one of the objectives
    of BrownoutCon is reducing energy consumption, and (2) this metric is widely used
    in research articles for energy efficient clouds. The total energy consumption
    E(t) during time interval t is formed as the sum of all the host energy consumption
    in the data center as shown in Eq. (9). Here, we only care about the physical
    server’s energy consumption rather than other network devices or cooling equipment.
    (9) where n is the total number of hosts in the data center, and Pi(t) is the
    power at time t of host i. And the total energy consumption of data center during
    observation time period T can be represented as . The Grid’5000 cluster provides
    APIs for the fine-grained power measurement of each physical server at per second
    rate. We use the APIs to collect the power measurement data (in watts unit) during
    our observation period and calculate the power consumption (in kWh unit). Average
    response time: This metric measures the time between users send requests and receive
    the response on average. We choose this metric because (1) another objective of
    BrownoutCon is ensuring QoS, and (2) the average response time is a widely used
    metric of QoS. And the average response time is a widely used metric of QoS. We
    can use this metric to quantify the overall QoS of the system. We aim to ensure
    this metric to be below a specific value, e.g. 1 s. SLA violation ratio: As another
    metric to measure QoS, this metric quantifies the ratio of requests that fail
    to satisfy the predefined SLA. The reason for choosing this metric is the same
    as the reason for using average response time as metric. The metric is formalized
    as: (10) where numa is the total number of requests sent to the system, and numerr
    is the number of requests failing to get the response. SVR should also be optimized
    to be lower than a predefined value, e.g. 1%. 5.4. Experimental testbed The testbed
    we used for evaluation is Grid’5000, and we adopt the cluster equipped with power
    measurement APIs at Lyon site. The hardware specifications are as below: • 11
     ×  Sun Fire V20z6 with AMD Opteron 250 CPU (2 cores, 2.4 GHz) and 2 GB memory,
    and all the hosts are running on Debian Linux operating system. We choose the
    machines in the same site so that we can reduce the network influence of uncontrolled
    traffics from other sites. Among the 11 servers, nine are running as the Docker
    Swarm worker nodes, one is running as the Docker Swarm master node, and another
    node contains workload trace and installed with JMeter7 for sending requests to
    Docker Swarm cluster. The energy consumption of the workload trace node is not
    counted, as it is not regarded as a part of the cluster to provide services. All
    required softwares, such as Docker, Java, Ansible8 and JMeter have been installed
    on these machines to minimize the effects of CPU utilization and network delay.
    5.5. Experimental design and setup In our experiments, the overloaded threshold
    is configured as 85%, as this value has been evaluated in our previous work Xu
    et al. (2018) that it can achieve a better trade-off between energy consumption
    and QoS than other values, e.g. below 80% or above 90%, as small overloaded threshold
    triggers brownout too frequently and large overloaded threshold will not trigger
    brownout. Another configured parameter is the optional utilization percentage,
    which represents how much CPU utilization is allowed to be given to the optional
    containers. According to (Xu et al., 2016), the change of this parameter has an
    impact on energy consumption. We vary this parameter with 10%, 20%, and 30% respectively,
    as the large values, like 40%, can lead to non-negligible performance degradation,
    and the small values, like 5%, cannot reduce energy consumption effectively. The
    experiments are conducted by going through the algorithms as below: 1. NPA algorithm
    (Beloglazov et al., 2012) — A baseline algorithm that does not consider overloads
    and optional containers, where the hosts are running all the time and containers
    are not deactivated. 2. HS (Toosi et al., 2017) — Another baseline algorithm that
    applies the host scaling algorithm in Algorithm 3, while not applying brownout-based
    policies. 3. The LUCF, MNCF and RCS algorithms introduced in Section 4 are with
    the varied optional utilization percentages from 10% to 30% in increments of 10%.
    We evaluate the energy consumption, average response time and SLA violation ratio
    for these algorithms. We run each experiment 5 times to deal with the variance
    resulted from random factors, such as initial containers deployment, network latency,
    and application running status. 5.6. Experimental results and analysis Fig. 8
    depicts the energy consumption comparison of different algorithms. From the results,
    NPA has the highest energy consumption with 69.6 kWh, and HS reduces it to 43.9 kWh.
    For the brownout-based algorithms with varied parameters, the energy consumption
    of LUCF is from 40.5 kWh to 38.8 kWh when the optional utilization percentage
    is increased from 10% to 30%. For the MNCF, its energy is close to LUCF, which
    ranges from 41.1 kWh to 39.2 kWh when optional utilization percentage is varied.
    As for the RCS, it decreases the energy from 41.4 kWh to 38.8 kWh. All the brownout-based
    algorithm have shown a significant reduction around 40% to 44% in energy consumption
    than NPA, and they can also save about 6% to 12% power consumption than HS with
    different parameter settings. Download : Download high-res image (206KB) Download
    : Download full-size image Fig. 8. Energy consumption comparison. We also compare
    the average response time in Fig. 9. Although NPA consumes more energy than other
    algorithms, with the adequate resources, the average response time is the lowest
    as 174.3 ms. The average response time of HS is 611.6ms, while the other brownout-based
    algorithms decrease this value. LUCF lowers the average response time from 472.6
    ms to 425 ms, MNCF reduces it from 485.6 ms and reaches 427.3 ms with 30% optional
    utilization percentage, and the average response time of RCS ranges from 564.0
    ms–511.3 ms. Download : Download high-res image (228KB) Download : Download full-size
    image Fig. 9. Average response time comparison. In Fig. 10, the SLA violation
    ratios are compared. As NPA has enough resources, it does not experience any SLA
    violation, while in HS, it has 4.3% SLA violation. Compared with HS, LUCF relieves
    the SLA violated situation, reducing it from 2.1% to 0.5%. Similar to LUCF, MNCF
    also decreases the SLA violation to 0.5% from 2.3% when more optional utilization
    percentage is allowed. As for RCS, its SLA violation drops from 3.2%–0.9%. Download
    : Download high-res image (259KB) Download : Download full-size image Fig. 10.
    SLA Violation ratio comparison. The mean values of obtained results are also displayed
    in Table 2. HS saves more energy than NPA because it dynamically turns hosts into
    low-power mode, however, since resources are limited and without brownout, HS
    also experiences higher average response time and SLA violation ratio. Assuming
    the average response time and SLA violation ratio of QoS constraints should be
    below 1 second and 1% respectively, we can conclude that the brownout-based algorithms,
    LUCF, MNCF and RCS, can save more energy than NPA and HS while ensuring QoS by
    reducing average response time and SLA violation ratio. The reason lies in that
    the brownout-based algorithms reduce energy by deactivating a set of containers
    and improve the QoS compared with the overloaded situation. And the performance
    differences of brownout-based algorithms are due to the different selections of
    deactivated containers. For the comparison of brownout-based algorithms, when
    more optional utilization percentage is provided, the algorithms perform better.
    Therefore, in practice, our software system works better if more containers are
    allowed to be deactivated, which also means that better performance of brownout-based
    approach can be achieved when more containers are configured as optional. Table
    2. The experiment results. Algorithm Energy Avg. Response Time SLAVR NPA 69.6 kWh
    174.3 ms - HS 43.9 kWh 611.6 ms 4.3% LUCF-10 40.5 kWh 472.6 ms 2.1% LUCF-20 39.5 kWh
    470.3 ms 1.4% LUCF-30 38.8 kWh 425.0 ms 0.5% MNCF-10 41.1 kWh 485.6 ms 2.3% MNCF-20
    40.4 kWh 471.3 ms 1.4% MNCF-30 39.2 kWh 427.3 ms 0.5% RCS-10 41.4 kWh 564.0 ms
    3.2% RCS-20 39.8 kWh 551.6 ms 2.2% RCS-30 38.8 kWh 511.3 ms 0.9% 5.7. Scalability
    discussion The design and implementation of BrownoutCon are based on Docker Swarm,
    therefore, the performance of BrownoutCon is relevant to the scalability of Docker
    Swarm. Scalability tests on Docker Swarm have been conducted in Luzzardi (2015)
    with 1000 nodes and 30,000 containers, which shows that Docker Swarm is highly
    scalable system. 6. Conclusions and future work In this paper, we proposed the
    design and development of a software system based on brownout and containers for
    energy-efficient clouds, called BrownoutCon. BrownoutCon is transparent system
    based on Docker Swarm for containers management and does not require to modify
    the default configurations of Docker Swarm via using its APIs. BrownoutCon can
    be customized for implementing brownout-based algorithms, which dynamically activates
    or deactivates containers to handle overloads and reduce energy consumption. The
    experiments conducted on Grid’5000 infrastructure show that the brownout-based
    algorithms in BrownoutCon are able to reduce energy consumption while ensuring
    QoS. The proposed software can be applied in the container-based environment as
    well as future research in brownout area. As for future work, we would like to
    enable BrownoutCon to be available in other container environments, such as Kubernetes
    and Apache Mesos. We propose to extend the prediction algorithm to handle network
    congestion. We would also like to investigate memory-intensive workloads to enable
    BrownoutCon to be applied to more generic workloads. In addition, we plan to develop
    algorithms for integrated management of all resources of cloud data centers including
    cooling systems Gill and Buyya (2018) to significantly reduce their energy consumption;
    and implement them in BrownoutCon software system. Software availability: We released
    BrownoutCon as open source, and it can be downloaded from: https://github.com/Cloudslab/BrownoutCon.
    Acknowledgments This work is partially supported by China Scholarship Council
    (CSC) and Australia Research Council (ARC) Discovery Project. We thank Marcos
    Assuncao from INRIA (France) for providing the access to Grid’5000 infrastructure.
    We thank Editor-in-Chief (Prof. Paris Avgeriou and Prof. David C. Shepherd), Area
    Editor (Prof. Helen Karatza), and anonymous reviewers for their excellent comments
    on improving the paper. We also thank Sukhpal Singh Gill and Shashikant Ilager
    for their comments on improving this paper. References Adhikary, Das, Razzaque,
    Alrubaian, Hassan, Alamri, 2017 T. Adhikary, A.K. Das, M.A. Razzaque, M. Alrubaian,
    M.M. Hassan, A. Alamri Quality of service aware cloud resource provisioning for
    social multimedia services and applications Multimed. Tools Appl., 76 (12) (2017),
    pp. 14485-14509 CrossRefView in ScopusGoogle Scholar Arroba, Moya, Ayala, Buyya,
    2015 P. Arroba, J.M. Moya, J.L. Ayala, R. Buyya Dvfs-aware consolidation for energy-efficient
    clouds Proceedings of the International Conference on Parallel Architecture and
    Compilation, IEEE (2015), pp. 494-495 CrossRefView in ScopusGoogle Scholar Baresi,
    Guinea, Quattrocchi, Tamburri, 2016 L. Baresi, S. Guinea, G. Quattrocchi, D.A.
    Tamburri Microcloud: a container-based solution for efficient resource management
    in the cloud Proceeding of the IEEE International Conference on Smart Cloud, IEEE
    (2016), pp. 218-223 View in ScopusGoogle Scholar Bawden Bawden, T., 2016. Global
    warming: data centres to consume three times as much energy in next decade, experts
    warn. http://www.independent.co.uk/environment/global-warming-data-centres-to-consume-three-times-as-much-energy-in-next-decade-experts-warn-a6830086.html.
    Google Scholar Belog1azov, Buyya, 2012 A. Belog1azov, R. Buyya Optimal online
    deterministic algorithms and adaptive heuristics for energy and performance efficient
    dynamic consolidation of virtual machines in cloud data centers Concurr. Comput.
    Pract. Exper., 24 (13) (2012), pp. 1397-1420 Google Scholar Beloglazov, Abawajy,
    Buyya, 2012 A. Beloglazov, J. Abawajy, R. Buyya Energy-aware resource allocation
    heuristics for efficient management of data centers for cloud computing Future
    Generat. Comput. Syst., 28 (5) (2012), pp. 755-768 View PDFView articleView in
    ScopusGoogle Scholar Beloglazov, Buyya, 2015 A. Beloglazov, R. Buyya Openstack
    neat: a framework for dynamic and energy-efficient consolidation of virtual machines
    in openstack clouds Concurr. Comput. Pract. Exper., 27 (5) (2015), pp. 1310-1333
    CrossRefView in ScopusGoogle Scholar Bernstein, 2014 D. Bernstein Containers and
    cloud: from Lxc to docker to kubernetes IEEE Cloud Comput., 1 (3) (2014), pp.
    81-84 View in ScopusGoogle Scholar Boutin, Ekanayake, Lin, Shi, Zhou, Qian, Wu,
    Zhou, 2014 E. Boutin, J. Ekanayake, W. Lin, B. Shi, J. Zhou, Z. Qian, M. Wu, L.
    Zhou Apollo: scalable and coordinated scheduling for cloud-scale computing Proceedings
    of the 11th {USENIX} Symposium on Operating Systems Design and Implementation
    ({OSDI} 14) (2014), pp. 285-300 View in ScopusGoogle Scholar Buyya, Srirama, Casale,
    Calheiros, Simmhan, Varghese, Gelenbe, Javadi, Vaquero, Netto, Toosi, Rodriguez,
    Llorente, Vimercati, Samarati, Milojicic, Varela, Bahsoon, Assuncao, Rana, Zhou,
    Jin, Gentzsch, Zomaya, Shen, 2018 R. Buyya, S.N. Srirama, G. Casale, R. Calheiros,
    Y. Simmhan, B. Varghese, E. Gelenbe, B. Javadi, L.M. Vaquero, M.A.S. Netto, A.N.
    Toosi, M.A. Rodriguez, I.M. Llorente, S.D.C.D. Vimercati, P. Samarati, D. Milojicic,
    C. Varela, R. Bahsoon, M.D.D. Assuncao, O. Rana, W. Zhou, H. Jin, W. Gentzsch,
    A.Y. Zomaya, H. Shen A manifesto for future generation cloud computing: research
    directions for the next decade ACM Comput. Surv., 51 (5) (2018), pp. 105:1-105:38
    Google Scholar Chen, Chen, Zheng, Cui, Qian, 2015 Q. Chen, J. Chen, B. Zheng,
    J. Cui, Y. Qian Utilization-based VM consolidation scheme for power efficiency
    in cloud data centers Proceedings of the IEEE International Conference on Communication
    Workshop (2015), pp. 1928-1933 CrossRefView in ScopusGoogle Scholar Delforge Delforge,
    P., 2014. Data center efficiency assessment - scaling up energy efficiency across
    the data center industry: evaluating key drivers and barriers. https://www.nrdc.org/sites/default/files/data-center-efficiency-assessment-IP.pdf
    Google Scholar Dhiman, Marchetti, Rosing, 2009 G. Dhiman, G. Marchetti, T. Rosing
    vgreen: a system for energy efficient computing in virtualized environments Proceedings
    of the ACM/IEEE International Symposium on Low Power Electronics and Design, ACM
    (2009), pp. 243-248 CrossRefView in ScopusGoogle Scholar Docker Docker, 2017.
    Docker documentation | Docker documentation. https://docs.docker.com/ Google Scholar
    Dou, Xu, Meng, Zhang, Hu, Yu, Yang, 2016 W. Dou, X. Xu, S. Meng, X. Zhang, C.
    Hu, S. Yu, J. Yang An energy-aware virtual machine scheduling method for service
    QOS enhancement in clouds over big data Concurr. Comput. Pract. Exper., 29 (14)
    (2016), pp. 1-20 Google Scholar Fan, Han, Yang, 2017 M. Fan, Q. Han, X. Yang Energy
    minimization for on-line real-time scheduling with reliability awareness J. Syst.
    Softw., 127 (2017), pp. 168-176 View PDFView articleView in ScopusGoogle Scholar
    Gill, Buyya, 2018 S.S. Gill, R. Buyya A taxonomy and future directions for sustainable
    cloud computing: 360 ∘ view ACM Comput. Surv., 51 (5) (2018), pp. 104:1-104:33,
    10.1145/3241038 Google Scholar Goiri, Katsak, Le, Nguyen, Bianchini, 2013 Í. Goiri,
    W. Katsak, K. Le, T.D. Nguyen, R. Bianchini Parasol and greenswitch: Managing
    datacenters powered by renewable energy Proceedings of the ACM SIGARCH Computer
    Architecture News, 41, ACM (2013), pp. 51-64 Google Scholar Grid''5000 Grid''5000,
    2017. Grid''5000:home. https://www.grid5000.fr/mediawiki/index.php/Grid5000:Home
    Google Scholar Han, Tan, Chen, Wang, Chen, Lau, 2016 Z. Han, H. Tan, G. Chen,
    R. Wang, Y. Chen, F.C.M. Lau Dynamic virtual machine management via approximate
    Markov decision process Proceedings of the 35th Annual IEEE International Conference
    on Computer Communications (2016), pp. 1-9 Google Scholar Hightower, Burns, Beda,
    2017 K. Hightower, B. Burns, J. Beda Kubernetes: Up and Running: Dive Into the
    Future of Infrastructure “O’Reilly Media, Inc.” (2017) Google Scholar Kaur, Chana,
    2015 T. Kaur, I. Chana Energy efficiency techniques in cloud computing: a survey
    and taxonomy ACM Comput. Surv. (CSUR), 48 (2) (2015), p. 22 CrossRefView in ScopusGoogle
    Scholar Kim, Beloglazov, Buyya, 2011 K.H. Kim, A. Beloglazov, R. Buyya Power-aware
    provisioning of virtual machines for real-time cloud services Concurr. Comput.
    Pract. Exper., 23 (13) (2011), pp. 1491-1505 CrossRefView in ScopusGoogle Scholar
    Klein, Maggio, Årzén, Hernández-Rodriguez, 2014 C. Klein, M. Maggio, K.-E. Årzén,
    F. Hernández-Rodriguez Brownout: building more robust cloud applications Proceedings
    of the 36th International Conference on Software Engineering (2014), pp. 700-711
    CrossRefView in ScopusGoogle Scholar Kong, Liu, 2015 F. Kong, X. Liu A survey
    on green-energy-aware power management for datacenters ACM Comput. Surv. (CSUR),
    47 (2) (2015), p. 30 View in ScopusGoogle Scholar Kozhirbayev, Sinnott, 2017 Z.
    Kozhirbayev, R.O. Sinnott A performance comparison of container-based technologies
    for the cloud Future Generat. Comput. Syst., 68 (2017), pp. 175-182 View PDFView
    articleView in ScopusGoogle Scholar Lavallée Lavallée, B., 2014. Data center energy:
    Reducing your carbon footprint | data center knowledge. http://www.datacenterknowledge.com/archives/2014/12/17/undertaking-challenge-reduce-data-center-carbon-footprint
    Google Scholar Li, Yan, Yu, Yu, 2017 Z. Li, C. Yan, X. Yu, N. Yu Bayesian network-based
    virtual machines consolidation method Future Generat. Comput. Syst., 69 (2017),
    pp. 75-87 View PDFView articleView in ScopusGoogle Scholar Liu, Aida, Yokoyama,
    Masatani, 2016 K. Liu, K. Aida, S. Yokoyama, Y. Masatani Flexible container-based
    computing platform on cloud for scientific Workflows Proceedings of the International
    Conference on Cloud Computing Research and Innovations, IEEE (2016), pp. 56-63
    CrossRefView in ScopusGoogle Scholar Liu, Chen, Bash, Wierman, Gmach, Wang, Marwah,
    Hyser, 2012 Z. Liu, Y. Chen, C. Bash, A. Wierman, D. Gmach, Z. Wang, M. Marwah,
    C. Hyser Renewable and cooling aware workload management for sustainable data
    centers Proceedings of the ACM SIGMETRICS Performance Evaluation Review, 40, ACM
    (2012), pp. 175-186 Google Scholar Luzzardi Luzzardi, A., 2015. Scale testing
    docker swarm to 30,000 containers - Docker blog. https://blog.docker.com/2015/11/scale-testing-docker-swarm-30000-containers/
    Google Scholar Mastelic, Oleksiak, Claussen, Brandic, Pierson, Vasilakos, 2015
    T. Mastelic, A. Oleksiak, H. Claussen, I. Brandic, J.-M. Pierson, A.V. Vasilakos
    Cloud computing: survey on energy efficiency ACM Comput. Surv. (CSUR), 47 (2)
    (2015), p. 33 View in ScopusGoogle Scholar Naik, 2016 N. Naik Building a virtual
    system of systems using docker swarm in multiple clouds Proceedings of the IEEE
    International Symposium on Systems Engineering (ISSE), IEEE (2016), pp. 1-3 Google
    Scholar Newman, 2015 S. Newman Building microservices “O’Reilly Media, Inc.” (2015)
    Google Scholar Pahl, Brogi, Soldani, Jamshidi, 2017 C. Pahl, A. Brogi, J. Soldani,
    P. Jamshidi Cloud container technologies: a state-of-the-art review IEEE Trans.
    Cloud Comput. (2017), pp. 1-14 CrossRefGoogle Scholar Rodriguez, Buyya, 2018 M.A.
    Rodriguez, R. Buyya Container-based cluster orchestration systems: a taxonomy
    and future directions Softw. Pract. Exper. (2018), pp. 1-22 View in ScopusGoogle
    Scholar Santos, McLean, Solinas, Hindle, 2018 E.A. Santos, C. McLean, C. Solinas,
    A. Hindle How does docker affect energy consumption? evaluating workloads in and
    out of Docker containers J. Syst. Softw., 146 (2018), pp. 14-25 View PDFView articleView
    in ScopusGoogle Scholar Schwarzkopf, Konwinski, Abd-El-Malek, Wilkes, 2013 M.
    Schwarzkopf, A. Konwinski, A. Abd-El-Malek, J. Wilkes Omega: flexible, scalable
    schedulers for large compute clusters Proceedings of the 8th ACM European Conference
    on Computer Systems, ACM (2013), pp. 351-364 CrossRefView in ScopusGoogle Scholar
    Teng, Yu, Li, Deng, Magoulès, 2016 F. Teng, L. Yu, T. Li, D. Deng, F. Magoulès
    Energy efficiency of VM consolidation in IAAS clouds J. Supercomput. (2016), pp.
    1-28 Google Scholar Toosi, Qu, de Assunção, Buyya, 2017 A.N. Toosi, C. Qu, M.D.
    de Assunção, R. Buyya Renewable-aware geographical load balancing of web applications
    for sustainable data centers J. Netw. Comput. Appl., 83 (2017), pp. 155-168 Google
    Scholar Vavilapalli, Murthy, Douglas, Agarwal, Konar, Evans, Graves, Lowe, Shah,
    Seth, et al., 2013 V.K. Vavilapalli, A.C. Murthy, C. Douglas, S. Agarwal, M. Konar,
    R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al. Apache Hadoop yarn: yet
    another resource negotiator Proceedings of the 4th annual Symposium on Cloud Computing,
    ACM (2013), p. 5 View in ScopusGoogle Scholar Xu, Buyya, 2017 M. Xu, R. Buyya
    Energy efficient scheduling of application components via brownout and approximate
    Markov decision process Proceedings of the 15th International Conference on Service-Oriented
    Computing (2017), pp. 206-220 CrossRefView in ScopusGoogle Scholar Xu, Buyya,
    2019 M. Xu, R. Buyya Brownout approach for adaptive management of resources and
    applications in cloud computing systems: a taxonomy and future directions ACM
    Comput. Surv., 52 (1) (2019), pp. 8:1-8:27, 10.1145/3234151 Google Scholar Xu,
    Dastjerdi, Buyya, 2016 M. Xu, A.V. Dastjerdi, R. Buyya Energy efficient scheduling
    of cloud application components with brownout IEEE Trans. Sustainable Comput.,
    1 (2) (2016), pp. 40-53 View in ScopusGoogle Scholar Xu, Nadjaran Toosi, Buyya,
    2018 M. Xu, A. Nadjaran Toosi, R. Buyya Ibrownout: an integrated approach for
    managing energy and brownout in container-based clouds IEEE Trans. Sustainable
    Comput. (2018), pp. 1-14 Google Scholar Zhang, Wu, Chen, Wei, Zhou, Hu, Buyya,
    2019 X. Zhang, T. Wu, M. Chen, T. Wei, J. Zhou, S. Hu, R. Buyya Energy-aware virtual
    machine allocation for cloud with resource reservation J. Syst. Softw., 147 (2019),
    pp. 147-161 View PDFView articleCrossRefView in ScopusGoogle Scholar Cited by
    (35) Sustainable computing across datacenters: A review of enabling models and
    techniques 2024, Computer Science Review Show abstract Container scheduling techniques:
    A Survey and assessment 2022, Journal of King Saud University - Computer and Information
    Sciences Show abstract On revisiting energy and performance in microservices applications:
    A cloud elasticity-driven approach 2021, Parallel Computing Citation Excerpt :
    The system can provide this prediction using machine learning algorithms or mathematical
    models. Academically, more works are applying reactive elasticity [6,7,14–17]
    than proactive [18–20] to improve performance. Also, there are few studies related
    to reducing energy consumption for transactional applications [20]. Show abstract
    Energy, performance and cost efficient cloud datacentres: A survey 2021, Computer
    Science Review Show abstract ThermoSim: Deep learning based framework for modeling
    and simulation of thermal-aware resource management for cloud computing environments
    2020, Journal of Systems and Software Show abstract Optimizing Container Deployment
    in Cloud Computing: Review, Opportunities and a PSO based Container Scheduler
    2023, Research Square View all citing articles on Scopus Minxian Xu received the
    B.Sc degree in 2012 and the MSc degree in 2015, both in software engineering from
    University of Electronic Science and Technology of China. He is working towards
    the PhD degree at the Cloud Computing and Distributed Systems (CLOUDS) Laboratory,
    School of Computing and Information Systems, the University of Melbourne, Australia.
    His research interests include resource scheduling and optimization in cloud computing
    with special focus on energy efficiency. He has co-authored several peer-reviewed
    papers published in prominent international journals and conferences, such as
    ACM Computing Surveys, IEEE Transactions on Sustainable Computing, IEEE Transactions
    on Automation Science and Engineering, Concurrency and Computation: Practice and
    Experience, International Conference on Service-Oriented Computing. Rajkumar Buyya
    is a Redmond Barry Distinguished Professor and Director of the Cloud Computing
    and Distributed Systems (CLOUDS) Laboratory at the University of Melbourne, Australia.
    He is also serving as the founding CEO of Manjrasoft, a spin-off company of the
    University, commercializing its innovations in Cloud Computing. He served as a
    Future Fellow of the Australian Research Council during 2012–2016. He has authored
    over 625 publications and seven text books including ”Mastering Cloud Computing”
    published by McGraw Hill, China Machine Press, and Morgan Kaufmann for Indian,
    Chinese and international markets respectively. He is one of the highly cited
    authors in computer science and software engineering worldwide (h-index=123, g-index=271,
    79,800+ citations). Dr. Buyya is recognized as a “Web of Science Highly Cited
    Researcher” in 2016, 2017 and 2018 by Thomson Reuters, a Fellow of IEEE, and Scopus
    Researcher of the Year 2017 with Excellence in Innovative Research Award by Elsevier
    for his outstanding contributions to Cloud computing. He served as the founding
    Editor-in-Chief of the IEEE Transactions on Cloud Computing. He is currently serving
    as Co-Editor-in-Chief of Journal of Software: Practice and Experience, which was
    established over 45 years ago. For further information on Dr. Buyya, please visit
    his cyberhome: www.buyya.com 1 See http://www.wikibench.eu/wiki/2007-10/ for more
    details. 2 See https://docs.docker.com/compose/compose-file/ for more details.
    3 See https://en.wikipedia.org/wiki/Strategy_pattern for more details. 4 The details
    about how we filter the raw data are provided at: https://github.com/Cloudslab/BrownoutCon.
    5 See https://github.com/microservices-demo/microservices-demo for more details.
    6 The maximum power of this model is 237 Watts, and the sleep mode consumes 10
    Watts. 7 See http://jmeter.apache.org/ for more details. 8 See https://www.ansible.com/
    for more details. View Abstract © 2019 Elsevier Inc. All rights reserved. Recommended
    articles Sustainable Software Design Green Information Technology, 2015, pp. 111-127
    Michael Engel Combining Grid Computing and Docker Containers for the Study and
    Parametrization of CT Image Reconstruction Methods Procedia Computer Science,
    Volume 108, 2017, pp. 1195-1204 Mónica Chillarón, …, Gumersindo Verdú View PDF
    Bone marrow trephine biopsy in Hodgkin''s lymphoma. Comparison with PET–CT scan
    in 65 patients Medicina Clínica (English Edition), Volume 150, Issue 3, 2018,
    pp. 104-106 Sunil Lakhwani, …, Miguel T. Hernández-Garcia View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 27 Captures Readers: 65 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply.'
  inline_citation: '>'
  journal: "Journal of systems and software/\x98The \x9CJournal of systems and software"
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'BrownoutCon: A software system based on brownout and containers for energy-efficient
    cloud computing'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/icin.2017.7899408
  analysis: '>'
  authors:
  - Tasneem Salah
  - Mohamed Jamal Zemerly
  - Chan Yeob Yeun
  - Mahmoud Al‐Qutayri
  - Yousof Al-Hammadi
  citation_count: 28
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2017 20th Conference on Innov... Performance comparison
    between container-based and VM-based services Publisher: IEEE Cite This PDF Tasneem
    Salah; M. Jamal Zemerly; Chan Yeob Yeun; Mahmoud Al-Qutayri; Yousof Al-Hammadi
    All Authors 26 Cites in Papers 1 Cites in Patent 3313 Full Text Views Abstract
    Document Sections I. Introduction II. Backgruond and Related Work III. Experimental
    Evaluation IV. Experimental Results V. Conclusion Authors Figures References Citations
    Keywords Metrics Abstract: These days, microservice architecture is widely used
    in the design and development of many real-time, critical, and large-scale online
    services. These services are typically deployed using Docker containers on cloud
    platforms. Container technology supports the deployment of these services with
    high portability, scalability, and performance, when compared to deploying them
    using virtual machines (i.e. VM-based services). It is widely known fact that
    container-based services give better performance than VM-based services. However,
    we show in this paper that services deployed using Amazon AWS ECS (EC2 Container
    Service) surprisingly perform significantly worse when compared with services
    deployed using Amazon EC2 VMs. We study and quantify the performance difference
    in terms of throughput, response time and CPU utilization considering different
    deployment scenarios. Published in: 2017 20th Conference on Innovations in Clouds,
    Internet and Networks (ICIN) Date of Conference: 07-09 March 2017 Date Added to
    IEEE Xplore: 17 April 2017 ISBN Information: Electronic ISSN: 2472-8144 DOI: 10.1109/ICIN.2017.7899408
    Publisher: IEEE Conference Location: Paris, France SECTION I. Introduction The
    cloud has become the preferred platform for deploying applications and services
    designed with microservice architecture. The aim of microservice architecture
    is to break down the application into a set of smaller independent services [1].
    This architecture enables application resilience, scalability, fast software delivery
    and the use of minimal resources. Such features can enhance the development method
    followed in complex systems which require continuously changing algorithms. Such
    algorithms can be found in real-time services as those found in recommender systems
    and stock market predictions. Docker containers are commonly used to support the
    deployment of services developed with microservice architecture [2]. Docker containers
    are lightweight, highly portable and scalable [3]. Such features become attractive
    to develop containerized services (or microservices). In the literature, container-based
    services are reported to always outperform VM-based services. For example, it
    was shown in [4]–[8] that containers outperform virtual machines in terms of execution
    time, latency, throughput, power consumption, CPU utilization and memory usage.
    However, according to Amazon AWS documentation [9], it was reported that Docker
    containers are deployed on top of EC2 (Elastic Compute Cloud) virtual machines
    (VMs), and not on bare-metal hardware. This contradicts the common practice of
    container deployment which is widely adopted in the literature of deploying container
    on bare-metal hardware. This was the primary motivation for our research work
    in which we aim to investigate and assess the performance difference resulting
    from deploying services using VMs (virtual machines) and using Docker containers.
    In our experimental study, we chose Amazon Web Services (AWS) cloud platform,
    as Amazon cloud has been thus far the most popular Infrastructure as a Service
    (IaaS) cloud provider. More specifically, and in Amazon AWS taxonomy, we focus
    to evaluate the performance of web services using: (1) AWS EC2 Container Service
    for container-based deployment, and (2) AWS Elastic Cloud Compute (EC2) for VM-based
    deployment. The main contributions of this paper can be summarized as follows:
    We provide a methodology and experimental setup that can be used in general to
    assess the performance of container-based and VM-based services to be deployed
    on any cloud environment. We present a comparative performance study and results
    considering different type of deployment scenarios for a web service We show that
    the performance of VM-based web services is surprisingly and substantially superior
    when compared to container-based series. The remainder of the paper is organized
    as follows. Section II provides a brief background and related work on microservices,
    virtual machines, Docker containers and their expected performances. Section III
    describes our methodology including the test environment setup, tools, and the
    different test scenarios of EC2 and ECS. Section IV presents and provides interpretations
    for the experimental performance measurements obtained from the three test scenarios.
    The measurements are reported in terms of throughput, CPU utilization and response
    time. Finally, Section V concludes our performance study. SECTION II. Backgruond
    and Related Work In this section, we provide a brief background about microservices
    architecture, Docker containers, and virtual machines. We also described related
    work reported in the literature with regards to performance of Docker containers
    when compared to the performance of VMs. Microservices architecture is often compared
    in the context of the traditional SOA (Service Oriented Architecture) architecture.
    SOA allows services to be loosely coupled, reusable and dynamically assembled
    which supports the changing business environment [10]. SOA supports explicit boundaries
    between autonomous services located on different servers to fulfil application
    requirements. The most crucial drawback that called the need for enhancing SOA
    is its centralized integration [11]. Some prefer referring to microservices as
    an approach to build SOA. Others stated that SOA architecture can be referred
    to as the monolithic representation of an application or as a compressed view
    of microservices. There are strong motivations to consider splitting a monolith
    application some of them would be to ease up service changing, fast delivery of
    features and functions, elastic scaling and fault tolerance [12] [13]. For scalability
    and ease of replications and migration, microservices get deployed in a cloud
    environment and run within a virtual machine. Virtualization is the process of
    splitting up the physical machine into several virtual components in which each
    can host different software to run within. The authors in [3] presented an example
    on how to build a custom VM image. Some examples on virtualization might include
    VMWare and AWS (Amazon Web Services) virtual machine technologies. Unfortunately
    building them takes a long time and some images might result with a large size.
    Transferring those large images across the network will result in a complex activity
    to be performed. The overhead is caused by the Hypervisor layer as illustrated
    in [3] which is responsible for performing virtualizing and managing physical
    resources. These drawbacks entail the need for container technologies such as
    Linux Containers [14]. Containers provide a separate space for the processes without
    the need for a hypervisor to control the machines. Moreover, Docker platform contributed
    on building lightweight and portable containers enabling to run on any infrastructure.
    Docker is an open source platform focuses on building, shipping, and running distributed
    applications within containers to be located on either on local machines or the
    cloud. In fact, one Virtual Machine (VM) can host multiple containerized microservices.
    Containers provide a very appealing feature which is packing up the service with
    its dependencies into a single image also referred to as code portability [15].
    Fig. 1 illustrates the difference in the architecture approach for virtual machines
    and Docker container technology. The structure of the containers architecture
    approach seen in Fig. 1 provides portability and efficiency and less overhead
    as the hypervisor layer is eliminated compared to virtual machines. Many performance
    comparisons were reported in the literature on virtual machines and containers
    especially Docker containers. In most of the comparisons, Docker containers performance
    has been reported to be superior when compared with virtual machines. Services
    deployed using containers are expected to take less execution time thus resulting
    in less latency over virtual machines as in [5]. In addition, containers were
    shown to introduce lower power consumption over virtualized technologies [4].
    Fig. 1: Common approach of hosting of VMs vs. containers Show All Moreover, in
    [6] [7], authors showed that services deployed using Docker containers outperform
    VMs in terms of overall throughput. Significant performance overhead was also
    reported for real-time applications when deployed using virtual machines, which
    was evident in the resulting CPU utilization and memory usage [8]. Furthermore,
    according to Amazon [9], Docker containers are deployed on top of EC2 virtual
    machine, which is against the common practice of deployment as depicted in Fig.
    1. Fig. 2 shows the container deployment approach adopted by Amazon cloud. Fig.
    2: Hosting of containers in amazon cloud Show All As shown in Fig. 2, an overhead
    is clearly introduced which will impact the overall performance of the running
    services within containers. Amazon AWS may have adopted such approach for good
    reasons. First, it is less costly for a service provider to make more use of an
    existing infrastructure (i.e. EC2 VMs) and leverage to provide additional service
    (i.e., container ECS). Second, customers will be able to easily manage their container
    instances as ECS offers high management and auto-scaling capabilities which are
    basically the features and capabilities of EC2 VMs. Third, Amazon argues that
    deploying containers using EC2 VMs would ease management and increase deployment
    speed while maintaining flexibility [9]. Clearly, in Amazon cloud, the performance
    of containerized services has not been considered a top concern [9]. The deployment
    of containerized services within virtualized machines is expected to result in
    an overhead leading to a significant increase in response time and degradation
    in the overall performance of the containerized service. We believe it is important
    to quantify the performance difference. This can be key to the deployment real-time
    applications and services. To the best of our knowledge, no experimental work
    has been conducted on measuring the performance of VM-based services and container-based
    services on Amazon cloud. The closest to our work is that of [6] in which the
    authors compared the performance of containers deployed on a privately owned bare-metal
    hosts against Amazon EC2 VMs. In our work, we focus on measuring the performance
    of container-based services against VM-based services when both services are deployed
    on the same Amazon cloud. SECTION III. Experimental Evaluation This section describes
    our experimental methodology to evaluate the performance of container-based and
    VM-based services. We use a web service as an example to assess and compare the
    performance. A. AWS Environment Setup Amazon was selected to be the cloud platform
    to study and measure the performance of container-based services against VM-based
    services. Amazon Web Services (AWS) provides powerful features and fine-grained
    deployment capabilities as it is an Infrastructure as a Service and not a Platform
    as a Service (PaaS) in which more control over the resources can be achieved.
    The EC2 Container Service (ECS) provided by Amazon will be used to deploy Docker
    containers and the EC2 instances as the virtual machines. 1) EC2 Container Service
    (ECS) The ECS service first allows the developer to create a cluster of resources.
    These resources can be spread among different data centers. The cluster then can
    be filled with EC2 instances. In our case the cluster is created in Tokyo region.
    Each instance has an ECS Agent which is responsible to register the instance to
    the cluster. The ECS agent can talk via API using the Agent communication service
    offered by the container service. Within each Instance multiple services (tasks)
    with different ports can run on the same instance. Each of these tasks can have
    multiple related containers. In order to connect containers of the same task,
    a task definition has to be created and described using JSON Syntax. A Simple
    task definition representing a webpage will be relied on in this experiment [16].
    The task will only require one container which will be using an httpd:2.4 image
    (which is the image for Apache Hypertext Transfer Protocol Server). The task basically
    displays a web page based on the Html command parameter. The web page will be
    accessed using the DNS Name or IP Address of the Container Instance the task is
    running on. In order to test the ECS performance against virtual machines, different
    schenarios need to be considered to support the performance analysis. 2) Evaluation
    Scenarios We consider three evaluation scenarios with different number of web
    services deployed for each. a) Scenario 1 In this scenario, we consider one web
    service running on one container instance. A similar EC2 based structure was set
    in which a single web service was created which uses port 80. Apache server had
    to be installed on the EC2 instance besides ECS where the containers only use
    httpd images. Both instances are of size t2.small (small instance size of 1 virtual
    CPUs and 2 GB of memory). b) Scenario 2 The second scenario is related to the
    way ECS scales a given task. Tasks can simply be distributed among different container
    instances in which each container instance would usually host multiple tasks or
    containers. We run two tasks representing web services each using one container
    running on different ports. The first service will use port 80 and the other will
    use port 8080. The requests will be sent to both services at the same time. Two
    similar web services will be hosted on an EC2 instance to be compared using ports
    80 and 8080. For EC2, httpd had to be re-configured in order to listen to another
    port. c) Scenario 3 In real cases, a container instance would host multiple services
    running at the same time. We demonstrate a third scenaio to monitor the performance
    where more than two web services are running on the same container instance. We
    use three web services running on ports 80, port 8080 and port 81. Another httpd
    port was configures in order to add a web service using port 8181. The requests
    will be sent to the three services at the same time. B. Performance Metrics In
    this section, we outline the set of key performance metrics to assess and compare
    the performance of containerized web services and VM-based web services. In order
    to effectively analyze the performance of the container-based and VM-based web
    services, we consider the following metrics are used in our study: 1) Server Throughput
    (Requests/Sec) It is important to measure server performance with respect of throughput
    at different request rates. Since we are dealing with web services, we focus on
    the rate at which we are able to retrieve the web documents with the GET method
    of HTTP requests sent. 2) Response Time (Millisecond) It is essential to measure
    the time elapsed from sending the request until a response is received which is
    referred to as the response time. The response time is measured in milliseconds.
    Both response time and throughput are measured using the measurement tool JMeter
    described in Section III.C. 3) CPU Utilization (Percent) Average CPU Utilization
    is usually measured to understand and amount of work and CPU usage of the virtual
    machine done when handling incoming number of requests. It is represented in terms
    of percentages out of one hundred. In our experiments, we measured the CPU utilization
    is by recording the CPU idle % and subtracting it from 100. C. Measurement Tool
    In order to test the different scenarios of deploying web services considering
    different scenarios, we need to measure the performance of the services by subjecting
    these services to a large number of HTTP requests, and subsequently, key performance
    metrics (for response time, throughput and CPU utilization) can be gathered and
    recorded at different incoming web workload. We use JMeter [17] as a generation
    and testing tool. JMeter comes with a graphical server performance dashboard.
    JMeter simulates a group of users sending requests to a target server then statistics
    will be provided indicating the performance of that specific target server. JMeter
    will be configured to generate HTTP requests to a web service that will be running
    using Apache server deployed on an EC2 instance to mimic the VM-based service.
    For container-based service, the same EC2 image of the Apache service will be
    used by the ECS machines. JMeter was installed on a Windows EC2 machine locally
    (i.e. within AWS cloud Tokyo region and within the same availability zone) in
    order to minimize network overhead and variability. As shown in Fig. 3, the instance
    size chosen for JMeter is of t2.large type (large instance size of 2 virtual CPUs
    and 8 GB of memory) as we need to examine the performance under large request
    rate that can exceed 10,000 requests/sec. Fig. 3: Experimental setup in amazon
    cloud Show All We have chosen the number of users to vary from 1 to 20 virtual
    users (threads). Each thread will send as many requests possible in duration of
    20 seconds duration. The number of requests sent by each user is indicated by
    how fast the target machine responses are. JMeter will send each request after
    the response of the previous request is received. By the end of each test JMeter
    will provide the performance measurements done within the 20 seconds. The throughput
    is returned as an average of the number of successful requests handled by the
    targeted server per second. JMeter measure the throughput for requests that were
    sent and received back successfully from the service. JMeter does not send any
    request unless the server accepts to open connections. JMeter records the response
    time as the average response times of all requests sent within the 20 seconds
    period. JMeter does not have the ability to measure the average CPU utilization
    of targeted servers. The CPU utilization was measured manually “using Linux sar
    utility” by directly connecting to the instance and measuring the average CPU
    Utilization in the period we are having JMeter sending the requests. Fig. 3 illustrates
    the basic components of our experimental setup. All instances are launched in
    Tokyo region and within the same availability zone. JMeter will only send requests
    for one instance type at a time. The requests were sent from JMeter at midnight
    Tokyo time to prevent any network fluctuations and undesired overhead. Each performance
    metric computed for each number of users was repeated 5 times and averaged. This
    was necessary as the cloud environment is highly variable and fluctuating with
    respect to workload, hosted services, and utilized network and compute resources.
    This can be caused by network workload sizes variations, workload rates, number
    of VMs, and the type of applications running on the physical machine. SECTION
    IV. Experimental Results In this section, we report, analyze, and compare the
    performance metrics for each scenario in terms of average throughput, CPU utilization
    and response time. Performance curves are plotted in relation to varying the number
    of users sending requests. The test was done during 20 seconds based on the setup
    described in Section III. The performance measurements of the first scenario are
    shown in Fig. 4 for the three performance measures. Fig. 4(a) shows the average
    throughput curves for ECS and EC2, Fig 4(b) exhibits the corresponding CPU Utilization,
    and Fig. 4(c) shows the corresponding average response time. It is observed that
    the throughput increases with the increase of number of threads and then saturates
    at thread count of 10 for both ECS and EC2. Accordingly, for EC2, the throughput
    saturates at approximately 10,000 requests per second. Whereas for ECS, maximum
    throughput is obtained at approximately 8000 requests per seconds. The throughput
    count is affected by how fast JMeter is receiving responses and the amount of
    CPU utilization of the target instance. The CPU utilization in Fig. 4(b) is reaching
    100% when handling requests of 10 users and above for both EC2 and ECS instance
    types. The saturation of the CPU utilization reflects the maximum throughout the
    machines are reaching to. The variance of the number of requests handled by both
    types of instances is indicated by the time the server takes to process incoming
    requests. Fig. 4(c) shows that EC2 instance has less response time than ECS which
    means that EC2 is faster in handling incoming requests reflecting on the amount
    of maximum throughput reached for each number of threads. ECS results are showing
    nearly 26.3% of extra time compared to EC2 at workload of 20 users and 44.4% when
    at workload of 10 users. Fig. 5 exhibits the performance measures of throughput,
    CPU utilization and response time for both EC2 and ECS instance types resulting
    from Scenario 2. The throughput in this case is flattening off at 9000 req/s for
    EC2 and at approximately 6000 req/s for ECS which is less than the throughput
    observed in Fig. 4 of the first scenario. The throughput is highly reflected by
    the increase of the response time seen in Fig. 5(c). At workload of 10 users,
    ECS is resulting in 64% of extra time compared to EC2 results. The response time
    is reaching nearly 2 ms and 4.5 ms for EC2 and ECS respectively at workload of
    20 users. Yet ECS has a significant overhead of nearly 2.5 ms which is about 125%
    of extra time compared to EC2. The CPU utilization is shown in Fig. 5(b) to indicate
    where the instance is reaching its saturation point for handling incoming workload.
    When comparing the performance measures of Scenario 1 with Scenario 2, it is clear
    that the performance is degraded as more services get deployed with ECS and EC2
    instances. The degradation is noticeable in terms of the increase of the response
    time and lower throughput. For Scenario 3, another task running one container
    will be added to the ECS instance and similarly another web service to EC2. The
    corresponding performance measures are plotted in Fig. 6. The total average throughput
    of the three web services for both ECS and EC2 are shown in Fig. 6(a). The throughput
    flattens off as the service is reaching a saturation point at approximately 7000
    req/s for EC2 and 5500 req/s for ECS. Lower throughput is observed for EC2 by
    1000 req/s and more response time of 1 ms compared to Fig 6. (c). Fig. 4: Performance
    measures for deploying one web service (scenario 1) Show All It is to be observed
    from Fig. 6(c) that the ECS performance in the third scenario is producing higher
    response time of 19% compared to EC2 at workload of 20 users and 38.4% at 10 users.
    Although CPU Utilization is not as crucial performance metric as throughput and
    response time, yet in all scenarios it shows that ECS is reaching its peak of
    100% CPU utilization way ahead of EC2. ECS performance measures exhibited in Scenario
    3 are very close to that of Scenario 2. Yet performance measures of EC2 are overwhelming
    those of ECS significantly in all scenarios. The reason behind the degraded performance
    measures of the container-based services (ECS) compared with EC2 virtual machines
    is the deployment mechanism adopted by Amazon. The Docker containers in Amazon
    ECS are deployed on top of EC2 VMs instead of bare-metal as illustrated in Amazon''s
    documentations [9]. Our experimental measures showed a significant performance
    overhead reflected by the existence of the hypervisor layer of the EC2 VMs in
    which containers are deployed on. Fig. 5: Performance measures for deploying two
    web services (scenario 2) Show All Fig. 6: Performance measures for deploying
    three web services (scenario 3) Show All SECTION V. Conclusion In this paper,
    we have conducted an experimental performance evaluation to compare and measure
    the performance of cloud-based services when deployed using: (1) Docker containers,
    and (2) Virtual Machines (VMs), on Amazon cloud environment. We considered different
    deployment scenarios in which web services were deployed. We measured the performance
    in terms of important metrics which include throughput, response time and CPU
    utilization while varying the incoming web request workload. Surprisingly, it
    was demonstrated that in general and under the three deployment scenarios, VM-based
    web services outperform container-based web services with respect to all performance
    metrics. Specifically, the performance difference has shown to be significant.
    For example, in terms of response time, VM-based web services can exhibit a performance
    gain of 125% over container-based web services. Therefore, we strongly recommend
    that for deploying applications with stringent performance requirements, the use
    of EC2 in AWS cloud should be recommended over ECS. We pointed out that the main
    reason behind the surprise performance degradation of container-based services
    when deployed on Amazon cloud was attributed to the fact that Amazon cloud runs
    containers on the top of EC2 VMs and not directly on bare-metal physical hosts.
    In this paper, we focused on Amazon cloud platform, which by far remains the most
    popular cloud platform. As a future work, we plan to do similar measurements on
    other popular IaaS cloud platforms as that of GCE (Google Compute Engine) and
    Microsoft Azure. Furthermore, in our experimental study, we limited our performance
    evaluation for web services, but other services (e.g. database, analytics, streaming,
    etc.) can also be used for evaluation. The evaluation of these types of services
    are left for future work. ACKNOWLEDGMENT This research work is funded by the Information
    and Communication Technology Fund (ICTFund), UAE. Authors Figures References Citations
    Keywords Metrics More Like This Virtual Machine Scalability on Multi-Core Processors
    Based Servers for Cloud Computing Workloads 2009 IEEE International Conference
    on Networking, Architecture, and Storage Published: 2009 Performance analysis
    of virtual machines and containers in cloud computing 2016 International Conference
    on Computing, Communication and Automation (ICCCA) Published: 2016 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Performance comparison between container-based and VM-based services
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/confluence.2019.8776985
  analysis: '>'
  authors:
  - Chetna Singh
  - Nikita Seth Gaba
  - Manjot Kaur
  - Baljinder Kaur
  citation_count: 17
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 9th International Confer...
    Comparison of Different CI/CD Tools Integrated with Cloud Platform Publisher:
    IEEE Cite This PDF Charanjot Singh; Nikita Seth Gaba; Manjot Kaur; Bhavleen Kaur
    All Authors 17 Cites in Papers 4421 Full Text Views Abstract Document Sections
    I. Introduction II. Related Work: III. Case Study IV. Comparisons Conclusion Authors
    Figures References Citations Keywords Metrics Abstract: A microservice architecture
    is a routine of enhancing cloud applications which enable large-scale enterprises
    to scale their application as per the demand. These microservices can be deployed
    using virtual machines, Docker containers or as a serverless function such as
    AWS lambda. However, with an increase in the number of microservices, it becomes
    strenuous to manage and deploy these services. Therefore, to overcome the problem,
    continuous integration and continuous delivery tool are used to deploy the microservices
    with minimum possible downtime. In this paper, we will be comparing different
    continuous integration and delivery tools taking into consideration different
    parameters like performance monitoring post-deployment, pipeline integration,
    cloud compatibility, and server monitoring. Published in: 2019 9th International
    Conference on Cloud Computing, Data Science & Engineering (Confluence) Date of
    Conference: 10-11 January 2019 Date Added to IEEE Xplore: 29 July 2019 ISBN Information:
    DOI: 10.1109/CONFLUENCE.2019.8776985 Publisher: IEEE Conference Location: Noida,
    India SECTION I. Introduction With ever emerging cloud technologies, the paradigm
    to develop and deploy applications on the cloud has been changed not only because
    of the stipulation such as scalability and reliability but also because of cloud
    support for continuous integration and delivery with minimum possible downtime
    [5]. The principal architecture used for developing cloud-based applications is
    the microservice architecture [4]. The fundamental behind this architecture is
    a composition of loosely coupled services which achieve business capabilities
    [1]. It focuses on structuring single-function components with well-defined interfaces.
    One of the major reasons for its popularity is that the enterprises have adopted
    agile methodology with CI and capability to update the modules in real time without
    impacting the application. The microservice architecture also solves the problem
    of scaling up the application as each microservice is bundled in its own codebase
    which separates them as opposed to the monolithic approach where the entire application
    is bundled into a single codebase[2]. One of the key factors for implementing
    the microservice architecture is virtualization. Virtualization can be implemented
    using virtual machines and containerization. A. Virtual Machines Figure 1. Architecture
    of a virtual machine Show All In virtual machines, an abstraction of physical
    machine is provided such that every VM (Virtual Machine) that is running on a
    physical server/machine is running a separate instance of an OS which is completely
    independent of the other VM running on the same server. The hypervisor is what
    allows multiple guest operating systems to be installed on the base operating
    system and on each of such guest operating system, a microservice is deployed
    [7]. Figure 1 explains the architecture of a VM. B. Containers A container comprises
    of a complete runtime environment which includes an application along with its
    dependencies, libraries and other binaries which are required to run the application.
    All of this is bundled into one package. By containerizing an application contingency
    between the application and the infrastructure is resolved. One of the leading
    software companies that allow us to build and package the application to make
    it ready for deployment is Docker. Docker allows us to package, build, test and
    deploy each of the application in an isolated linux environment called container.
    Due to this isolation and security provided by docker, we can run multiple containers
    on a single machine. Docker follows a client-server architecture. Figure 2 explains
    the entire workflow of docker as to how the docker-client communicates with docker-daemon
    which pulls the image from docker-registry on the basis of a dockerfile and then
    creates an image which is then deployed to a container. Figure 2. Docker workflow
    diagram Show All The architecture of container is different from that of a virtual
    machine. The container shares the operating system where each running process
    is isolated within the user space, therefore containers end up consuming lesser
    space than the virtual machine. The containers outperform virtual machine in different
    behaviors like better startup time, resource distribution and lesser redundancy.
    In order to make the application independent of the platform, a packaged build
    image of the code is generated using docker [9]. The architecture of docker is
    explained in figure 3. As the application grows the number of microservices also
    increases in number which makes it difficult to manage and deploy the services
    manually. Therefore an approach of continuous integration and continuous deployment
    is used to automate the deployment every time there is a new update in the codebase.
    There are multiple CI/CD tools available in the market to solve the above problem
    and you will see their comparisons in this paper on the basis of different parameters
    Figure 3. Docker container architecture diagram Show All CI is a segment of practices
    involved in software programming principles. CI states that the entire code for
    an application should be kept in a common repository so that whenever the developer
    checks in the code into the repository, a script is triggered which picks the
    latest code from the repository, integrate it with the existing code and run the
    test cases designed according to the application. There are multiple CI (Continuous
    Integration) tools available in the market like Jenkins, Bamboo, Gitlab, Subversion
    etc. For CI to work it has to be integrated with a source code management system.
    Git is a one such common tool which is widely used by developers for maintaining
    versioning of code. Each developer works on its own branch which is cut from a
    master branch. Whenever a developer implements the functionality or fixes a bug,
    it raises a merge request from current branch to the master branch. A script to
    execute test cases is triggered which runs all the unit tests written and if the
    code passes all the test cases, the merge is completed otherwise the merge is
    rejected. Every developer have a much responsible role when merging its code into
    the main branch. This adds a sense of accountability on the developer such that
    one need to make sure that his changes should not impact the build but also should
    not hamper the work of fellow developers. In order to avoid the merge conflicts
    short-lived branches with small features are preferred over the long-lived branches
    with a bigger feature [10]. CD (Continuous Delivery) is defined as an ability
    to deploy new features, bug fixes into the live server as and when required. CD
    part is executed after a successful CI where all the updated code is lying in
    the master branch. A script is run which picks the code from the master branch,
    prepares the build and deploy it to a test environment/production environment.
    This way the developers can test the code beyond unit tests so that updates across
    multiple parameters such as UI testing, integration testing etc. can be tested
    in order to identify the issues pre-emptively. With continuous delivery, there
    also comes a term called continuous deployment and the difference between these
    is that there is no manual intervention/confirmation required when we are following
    continuous deployment which means that with every code check-in, a new build will
    be deployed onto the specified server. This paper contributes to the following
    research and development An experimental setup is created in this paper in order
    to assess the performance of Jenkins and Gitlab CI/ CD A performance comparison
    is provided in this paper by evaluating the deployment time, build creation time
    etc. using multiple tools for several consecutive commits. A conclusion is endowed
    at the end of this paper explaining which tool is best suited for what type of
    project and also explaining the benefits and drawbacks of using each tool. Section
    2 explains the background and related work on the microservices and its integration
    with CI/CD tools for automated deployment and section 3 contains a case study
    where a java based microservice application will be deployed to the cloud using
    different CI/CD tools. Section 4 contains the performance evaluation of these
    available tools and finally, the paper conclusion is provided in section 5. SECTION
    II. Related Work: In the current age of technological revolution, there is immense
    pressure on enterprises like Amazon, Netflix, and Walmart etc. to add new features
    to the already existing services in order to maintain and add to their customer
    base. This can only be made possible by implementing microservice architecture
    rather than the monolithic approach [2]. Microservice architecture is a combination
    of loosely coupled services which are dynamically assembled and can be reused
    to support ever-changing business requirement [6]. Other than fault isolation
    and code isolation, the microservices are also advantageous in scalability, reusability
    and are also efficient in comparison with application build with the monolithic
    approach. JiaQiWu and Tong Wang discussed cloud computing, that how one can share
    configurable computing resources with other users over real-time [5]. They also
    emphasized the relationship between cloud computing and SOA framework and puts
    forward the framework of combining SOA and cloud computing model [5]. But, Microservices
    architecture takes an edge as it can operate and be deployed independently of
    other services, unlike SOA. Microservices being smaller in size are much easier
    to test and deploy frequently or scale a service independently. Also these days
    microservice architecture is widely considered in the designing and development
    of many real-time, critical, and large-scale online applications and service [6].
    Salah et.al compared containerization and VM based services required for deploying
    these micro-services and showed container technology supports high scalability,
    portability, and performance in comparison to VM-based deployment [6]. It is also
    verified that the services deployed using AWS ECS performed significantly worse
    in comparison to the services deployed using AWS EC2 VMs. [6] CI/CD tools are
    also used in order to automate the making of build from source code and further
    its deployment to the server. Code integration is one of the most common practices
    in Software Industry and the reason is that it eliminates the challenge of integrating
    code when the application is developed at different sites helping the developers
    to build, test and deploy application automatically [8]. Vindeep Singh and Sateesh
    K Peddoju discussed various challenges and blockers in deployment and CI microservices
    and showed Microservices based applications outperformed the monolithic architecture
    [2]. Also, they designed an automated system to overcome these challenges and
    demonstrated a significant reduction in time and effort for deployment and CI
    of the application using their proposed system [2]. Taking these researches further
    we have compared different CI/CD tools taking into consideration a case study
    on real-time quiz application as explained in the next section. SECTION III. Case
    Study This section discusses about a case study on real-time quiz application.
    The application follows a pure microservice architecture. The application is divided
    into different micro-services with the functionalities such as registration, login,
    create a quiz and answer a quiz. The technology stack used to develop the application
    was Java 8.0, MySQL database, Spring-Boot framework. A Docker image of each individual
    microservice was created which was deployed on the AWS ECS cluster. Jenkins and
    Gitlab were used in order to implement CI/CD. We have discussed in this section
    about the methodologies with which we can integrate our application with each
    tool. A. JENKINS Jenkins is one of the best and Java-based open-source CI/CD tool
    available in the market. Jenkins is easy to install and provides a good GUI which
    makes it easy for beginners to learn and adapt to the tool. Configuration is much
    easier in Jenkins because of the availability of different plugins which can be
    installed with a single click. How Deployment is done : After the new change is
    committed to the Gitlab repository, a webhook URL is called by the git repository
    to intimate Jenkins to prepare and push the build to the server. After the Jenkins
    receive the webhook trigger, it pulls the latest code from the git repository
    and then a java archive is prepared by executing a set of commands which are provided
    in Jenkins configuration tab. If the build is successful, Jenkins moves to the
    next step of creating the docker image and pushing the build. If the build is
    not successful, the respective developer or team members are intimated via an
    email alert. After the java build is prepared, a docker image is created based
    on the Dockerfile. B. Gitlab CI/CD Gitlab is an open source web-based version
    control system which is based on git. It also provides features such as CI/CD
    pipelines. It does not provide any support for additional plugins to be installed
    and the entire configuration is done on the pipeline itself. How deployment is
    done A pipeline is triggered when a new change is committed to the Gitlab. A pipeline
    for Gitlab is a YML file which contains a series of jobs or stages to be executed.
    Each stage has its own significance and contains a list of steps to be executed
    for the deployment of the latest build on the server. The pipeline triggers a
    runner to make a build. Once a build is prepared after successfully passing all
    the test cases, the YML file moves to the next step of creating a docker image
    from the build using the dockerfile. Below is the code for YML file. C. Common
    Steps Gitlab and Jenkins are both almost similar when it comes to making docker
    image of build and pushing that to ECS repository because they both use the same
    Dockerfile and run the same script. Below is the pseudo code for Dockerfile. Figure
    4. Microservice deployment process to ECS Show All The Docker image on creation
    will be pushed to a container repository called ECR with a new or updated tag.
    The deployment server then triggers the production server to create a new task
    definition by supplying the docker image tag and after that, it deploys a new
    service with the updated task definition. However, if the latest build fails to
    run due to some reason, it will restore the system back to the previous state
    as shown in figure 4. Below is the pseudo code for the deployment script. Algorithm
    1 Create a new service in ECS SECTION IV. Comparisons In this section, a comparison
    of Jenkins and Gitlab has been made in regards to usability and performance. The
    comparison is based on build and deployment time of a POC application. The code
    version control system used in this application is Gitlab. Jenkins and Gitlab
    were used to implement the CI/CD and were configured on an Amazon EC2 instance
    which is a Linux machine with 1GB RAM and 30 GB storage having the ability to
    execute 100-3000 I/O operations per second. Both Jenkins and Gitlab provide users
    with the option to trigger the build at a real-time, occasionally or at manual
    approval. The difference between the two of them is that Jenkins require a webhook
    URL to initiate the trigger point, however, the Gitlab does not require any webhook,
    it simply triggers the configured pipeline which is executed by the runner. A
    runner is a virtual machine that executes code defined in YML file. Gitlab defines
    3 types of runners’ i.e. a shared runner, specific runner and a group runner.
    A shared runner serves all the projects, a specific runner serves only a specific
    project, a group runner is a shared runner but it is shared by projects in a specific
    group. The default runner provided by Gitlab CI is a shared runner. During the
    build creation, it was observed that it took on an average nearly 6 seconds for
    Jenkins to prepare a JAR after the webhook was triggered. However, it took nearly
    17 seconds by Gitlab shared runner to prepare the same JAR file. The difference
    in build preparation time is because the Jenkins was deployed onto an isolated
    server where git and docker were already configured. As a result, only a single
    project have access to the server which is responsible for creating and deploying
    build onto the production server. On the contrary, the default shared runner allocates
    a fresh machine in real time for build generation to eliminate the suspected security
    issues and that is the reason for the lag in the creation of build in comparison
    to Jenkins. The Jenkins demands more initial configuration time like installation
    of git, docker, aws-cli etc but with Gitlab, none of the configurations was required.
    Only a pipeline i.e. a YML file has to be written and the environment configuration
    for build preparation was taken care of by shared runner. Gitlab gives provision
    to install runner on kubernetes and windows machine but it can be installed on
    an isolated machine like EC2 as well and this type of runner is called a specific
    runner. In case of Jenkins, it took around 9 seconds for the overall deployment
    and provision of microservice, however, in case of Gitlab it took around 400 seconds
    with the shared runner and around 11 seconds in case of the specific runner which
    is almost same as that of Jenkins. As the application and the number of plugins
    grows, it sometimes becomes difficult to manage Jenkins. The Jenkins jobs become
    more messy and difficult to change/manage. However, in the Gitlab, every detail
    is lying in the YML file and more importantly, every change in the pipeline is
    documented as the pipeline is also a part of CI. Table 1 contains the comparison
    of Jenkins and Gitlab on a number of metrics. Table 1 Comparison of Gitlab and
    Jenkins CI To further evaluate the efficiency of the CI tools we did a performance
    testing of Gitlab specific runner, shared runner and Jenkins slave. We jotted
    down the time it took by each CI tool to prepare the build and deploy the application
    onto the Figure 5. Deployment time for Jenkins, shared runner and specific runner
    Show All Amazon server. Figure 5 shows a graphical representation of deployment
    time it took for Jenkins, specific runner and shared runner to deploy the build
    for 10 consecutive commits. In the above graph, we can see that Jenkins outperformed
    Gitlab shared runner but at the same time we can see that the specific runner
    is akin to Jenkins because the time taken by Jenkins and Gitlab specific runner
    to deploy the application is almost same however, Jenkins is almost 20 times more
    efficient than Gitlab shared runners. Conclusion In this paper, performance evaluation
    has been done for Gitlab CI and Jenkins CI to deploy dockerized microservices
    on the AWS cloud platform. Different deployment scenarios were considered while
    deploying the microservices. The performance of CI/CD tools was evaluated considering
    the important metrics such as response time, deployment time, ease of use and
    set up the environment for CI to work. Jenkins is easy to use and learn because
    of the support to add different plugins which makes it easy for the developers
    to configure the project for automated deployment but as the application grows
    and the number of added plugins increase, it becomes difficult to manage Jenkins
    but such is not the case with Gitlab because in gitlab-pipeline everything is
    configured in a single YML file and irrespective of how large the project grows,
    it’s easy to understand and modify the pipeline. Based on our work we can say
    that both Gitlab and Jenkins provide good support for CI but to choose which one
    is entirely based on the project needs. For a simpler, sophisticated and small
    microservice based project, going for a Gitlab CI will be a good option but if
    you want to implement CI on a large enterprise project, Jenkins is what can go
    for. LIST OF ACRONYMS VM Virtual Machine SOA Service Oriented Architecture POC
    Proof of Concept CI Continuous Integration CD Continuous Delivery AWS Amazon Web
    Services ECR Elastic Container Repository ECS Elastic Container Service EC2 Elastic
    Compute Cloud OS Operating System Authors Figures References Citations Keywords
    Metrics More Like This Virtual Machine Scalability on Multi-Core Processors Based
    Servers for Cloud Computing Workloads 2009 IEEE International Conference on Networking,
    Architecture, and Storage Published: 2009 Performance analysis of virtual machines
    and containers in cloud computing 2016 International Conference on Computing,
    Communication and Automation (ICCCA) Published: 2016 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Comparison of Different CI/CD Tools Integrated with Cloud Platform
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/canopie-hpc49598.2019.00007
  analysis: '>'
  authors:
  - Angel Beltre
  - Pankaj Saha
  - Madhusudhan Govindaraju
  - Andrew J Younge
  - Ryan Grant
  citation_count: 39
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 IEEE/ACM International W...
    Enabling HPC Workloads on Cloud Infrastructure Using Kubernetes Container Orchestration
    Mechanisms Publisher: IEEE Cite This PDF Angel M. Beltre; Pankaj Saha; Madhusudhan
    Govindaraju; Andrew Younge; Ryan E. Grant All Authors 38 Cites in Papers 1803
    Full Text Views Abstract Document Sections I. Introduction II. Background III.
    Kuberentes InfiniBand Setup IV. Experimental Setup V. Performance Evaluation Show
    Full Outline Authors Figures References Citations Keywords Metrics Abstract: Containers
    offer a broad array of benefits, including a consistent lightweight runtime environment
    through OS-level virtualization, as well as low overhead to maintain and scale
    applications with high efficiency. Moreover, containers are known to package and
    deploy applications consistently across varying infrastructures. Container orchestrators
    manage a large number of containers for microservices based cloud applications.
    However, the use of such service orchestration frameworks towards HPC workloads
    remains relatively unexplored. In this paper we study the potential use of Kubernetes
    on HPC infrastructure for use by the scientific community. We directly compare
    both its features and performance against Docker Swarm and bare metal execution
    of HPC applications. Herein, we detail the configurations required for Kubernetes
    to operate with containerized MPI applications, specifically accounting for operations
    such as (1) underlying device access, (2) inter-container communication across
    different hosts, and (3) configuration limitations. This evaluation quantifies
    the performance difference between representative MPI workloads running both on
    bare metal and containerized orchestration frameworks with Kubernetes, operating
    over both Ethernet and InfiniBand interconnects. Our results show that Kubernetes
    and Docker Swarm can achieve near bare metal performance over RDMA communication
    when high performance transports are enabled. Our results also show that Kubernetes
    presents overheads for several HPC applications over TCP/IP protocol. However,
    Docker Swarm''s throughput is near bare metal performance for the same applications.
    Published in: 2019 IEEE/ACM International Workshop on Containers and New Orchestration
    Paradigms for Isolated Environments in HPC (CANOPIE-HPC) Date of Conference: 18-18
    November 2019 Date Added to IEEE Xplore: 09 January 2020 ISBN Information: DOI:
    10.1109/CANOPIE-HPC49598.2019.00007 Publisher: IEEE Conference Location: Denver,
    CO, USA SECTION I. Introduction Throughout the computing industry, the advent
    of containers has fundamentally shifted how computational workloads are managed
    and orchestrated in distributed computing environments. Containers have emerged
    as a popular choice for application deployments in many cloud computing environments.
    This is due to their relatively simple resource management & isolation, high availability,
    portability, and improved efficiency over more conventional environments, including
    VMs. Containers are a natural fit with microservices based architecture in the
    cloud due to its seamless integration with cloud orchestrators and schedulers
    [1] [2] [3] for efficient resource managements. Microservices are loosely coupled,
    and independently maintained and deployed software modules that can also be auto-scaled
    as required. Given the paradigm shift towards microservices, the notion of container
    orchestration frameworks have quickly become of critical importance in today’s
    distributed and cloud systems. The deployment of container images is made possible
    via container orchestrators. Container orchestration tools have a mechanism to
    launch and manage containers as clusters or pods. The default orchestrator for
    Docker is Docker Swarm [4] . Other container orchestration tools include Google
    Container Engine [5] , Amazon ECS [6] , Mesosphere Marathon [7] , Kubernetes,
    and Azure Container Service [8] . Among this list of container orchestrators,
    Docker Swarm and Kubernetes are the most widely known competing software platforms.
    Concurrently, the use of High Performance Computing (HPC) has grown from humble
    beginnings in high-energy physics simulations to a wide variety of scientific
    endeavors including astrophysics, meteorology, chemistry, bioinformatics, and
    national security applications, to name a few. In this same regard, HPC’s broader
    applicability towards advanced simulations and data analysis for enterprise and
    industrial applications has also grown. However, HPC applications have historically
    struggled with gaining adoption in the cloud, largely due to performance considerations
    at scale [9] . While Docker [4] is widely supported in cloud environments, it
    is not utilized in traditional HPC deployments due to concerns of root-level escalation
    in such a shared environment. However, a few other container technologies have
    been designed specifically for HPC. Modified container runtimes like Singularity
    [10] , Shifter [11] , and Charliecloud [12] , all attempt to enable Docker containers
    to run on shared HPC resources. These mechanisms are meant to work with traditional
    HPC job submission tools. There is little work so far in studying container orchestration
    frameworks, specialized to manage microservices deployments, for executing HPC
    applications. In this paper, we look to answer the question of whether HPC can
    be supported through the use of microservices container orchestration frameworks,
    rather than traditional batch queuing systems. Effectively, we look to provide
    HPC-as-a-Service using Kubernetes, a commodity service orchestration ecosystem.
    We compare the performance of container orchestration with Kubernetes, Docker
    Swarm, and traditional bare-metal, along with the options such as zero-copy RDMA-enabled
    communication, and traditional TCP/IP protocol. We use latency and throughput
    as comparison metrics and the Chameleon cloud [13] infrastructure as the testbed.
    Our goal is that these results, coupled with our detailed and optimized framework,
    can be used to inform the HPC community’s application developers and users on
    how to best leverage and deploy their workloads using a combination of container
    technologies and microservice orchestration mechanisms. The following are the
    key contributions of our work: We identify and present the configuration and settings
    required to deploy Kubernetes to support HPC applications with MPI based workloads.
    We explore the networking aspects and provide guidance on how to setup a Kubernetes
    cluster that can access the underlying hardware device specialized for HPC workloads.
    We evaluate the performance of Kubernetes and Docker Swarm when using the TCP/IP
    protocol, comparing to a bare metal deployment, using several characteristic HPC
    workloads. We present the design and evaluation for execution of MPI workloads
    on Kubernetes using the RDMA protocol and inter-pod communication. We evaluate
    and present the performance of bare metal versus containerized MPI applications
    orchestrated by Kubernetes and Docker Swarm. We use the results to identify the
    similarities and key differences between Kubernetes and Docker Swarm. SECTION
    II. Background We briefly summarize the key container and orchestration technologies
    for use with HPC applications. Our focus in this paper is on the Docker runtime
    engine. It is the leading container solution, and along with CRI-O, it is the
    default container solution for Kubernetes. Singularity, an HPC container solution,
    is the first Container Runtime Interface (CRI) that has been enabled in Kubernetes.
    In future work, we plan to extend our experimentation approach to Singularity,
    Shifter, and Charliecloud. A. Containerization 1) Docker Docker [4] containers
    are isolated applications, which are broken into smaller lightweight execution
    environments while sharing the operating system (OS) kernel. Docker creates a
    layer of abstraction to hide the underlying OS. Docker’s networking capabilities
    support both virtual networks on top of the host network and overlay networks
    , which are commonly used to achieve a higher level of abstraction. 2) Singularity
    Singularity [10] is a well known container solution within the HPC community.
    Unlike most containers, which focus on micro-service level virtualization (e.g.,
    Rkt and Docker), Singularity’s primary purpose is to provide application portability
    through operating system virtualization of namespaces, which is ideal for scientific
    computing ecosystems. 3) Shifter Shifter [11] is a research and development (R&D)
    effort to bring containers into HPC ecosystems. Shifter is implemented to provide
    filesystem isolation through chroot , thus providing stricter security guarantees
    than the standard Docker image. It pulls and converts Docker images into a format
    known as squashfs , a compressed read-only file system format. 4) Charliecloud
    Charliecloud [12] is a research effort by Los Alamos National Laboratory to execute
    Docker Containers in HPC system with minimal deployment requirements (i.e., non-root
    level access) by leveraging user namespaces within the Linux kernel. B. Container
    Orchestrator 1) Docker Swarm Docker Swarm can manage and orchestrate container
    deployment with in-built inter-container communication and software-defined networks.
    Swarm can optimize the underlying node’s resource usage and distribute containers
    into different nodes to improve load balancing. Docker Swarm provides three basic
    strategies to distribute containers: Spread , BinPack and Random . Spread is the
    default strategy, which distributes the containers into different host nodes based
    on available resources. With BinPack, Swarm places containers one node at the
    time until it is fully occupied. The Random strategy places a container into a
    host node in a random order. 2) Kubernetes Kubernetes is a container orchestrator
    that provides automation for running service containers. It provides a flexible
    way of scaling services running inside a container that require load balancing,
    fault tolerance, and horizontal scaling. Kubernetes provides direct support for
    Docker containers. Kubernetes uses Pods for better container management. A Pod
    is a logical envelope around a single container or multiple tightly coupled containers.
    The Kubernetes Pods are designed to be used with other container solutions such
    as Docker, Rkt [14] , and runC [15] . There are two main requirements, (1) inter-container
    communication and (2) hardware device access, for supporting MPI based HPC applications
    on Kubernetes. Flannel [16] provides a mechanism to create a Software Defined
    Network (SDN) such that all pods created in any physical host can reach each other
    via unique IP addresses. Access to the host machines’ hardware devices is essential
    for HPC specific network interconnects. Architectural components for Kubernetes
    can be categorized into two major categories: Master and Node Components. Kubernetes
    uses a Master node to serve as the primary global decision maker in the cluster.
    Four different components live in the master as presented in Figure 1 : (1) Kube-apiserver,
    (2) Etcd, (3) Kube-scheduler, and (4) Kube-controller-manager. All the cluster
    data is stored and backed up by the distributed key-value store, etcd . Etcd can
    only be accessed through Kubernetes API server to prevent unsecured cluster access.
    Kube-scheduler assigns pods based on their resource requirements as well as general
    specifications. TABLE I Kubernetes and Docker Swarm Comparison of essential attributes
    offered by Docker Swarm and Kubernetes. Fig. 1 Kubernetes Components. The Kubernetes
    setup has at least three components: kublet daemon, Container runtime, and Kubeproxy
    for communication across nodes. The Master node contains the scheduling and administrative
    components. Show All C. InfiniBand Interconnect The use of InfiniBand (IB) [17]
    interconnect can provide high throughput and low latency across systems for distributed
    and parallel applications. IB is a standardized technology supported by major
    operating systems and vendors. IB comprises two channel adapters: Host Channel
    Adapter (HCA) and Target Channel Adapter (TCA). To enable communication through
    particular IB devices, the HCA makes different hardware visible at the user-level.
    D. Network Instrumentation Figure 2 shows a traditional network setup where each
    host has a private network, which is the same across all host machines. The IP
    addresses of each of the network interfaces are the same on each host except for
    the host network interface eth0 . As illustrated in Figure 2 (Traditional Network),
    Docker creates the docker0 network interface and every time a container gets launched
    on a virtual network, veth { 0 … N } gets created. On the other hand, Kubernetes
    consolidates the network layout for a single host on a Kubernetes Pod over multiple
    containers by only creating a single virtual network that exchanges packets back
    and forth with the docker0 interface. This approach provides a shared network
    interface with improved isolation, though at the cost of performance. In order
    to enable Pod-to-Pod communication, Kubernetes uses an overlay network as shown
    in Figure 2 (Overlay Network). Inter-pod communication, independent of Pods’ physical
    location, is enabled using the Flannel [16] network package. The overlay network
    is deployed on Kubernetes cluster as a Pod network, which enables each pod to
    communicate with another through SDN with unique IP address over the overlay network.
    Fig. 2 Traditional Network to Overlay Network. In conventional setups, each node
    in a cluster has its default docker bridge network. Docker containers (or Pods)
    are connected to the individual bridge of the host node and cannot communicate
    with containers (or Pods) residing in other host nodes. After applying an overlay
    network, across all the host nodes, containers and Pods that are created in any
    of the host nodes can communicate across the cluster. Show All E. Kubernetes and
    Docker Swarm Attributes In Table I , we highlight the values of attributes that
    are required for both Kubernetes and Docker Swarm Cluster, to execute HPC workloads
    in a cloud infrastructure. SECTION III. Kuberentes InfiniBand Setup In Figure
    3a , we present the configuration and setup of a device Pod for IB support. The
    architecture enables auto discovery of devices from Application Pods. The containers
    have full installations of Open-fabric software to enable zero-copy RDMA communication
    for the containers. For the purposes of this paper, InfiniBand RDMA refers to
    InfiniBand send/recv data movement, not InfiniBand read/write methods as the codes
    used make use of MPI functions that utilize send/recv data movement. The steps
    to configure, install, and deploy a Device Pod container can be described as follows:
    Device Configuration. The virtual network device is configured using a ConfigMap
    , which is a Kubernetes object. The ConfigMap object is placed in the kube-system
    namespace along with its data as shown in Listing 1 . Device Deployment. The device
    deployment is executed as a Kubernetes daemon object. It is also placed in the
    kube-system to enable access of resources to Kubernetes cluster nodes. The device
    plugin directories are mapped and the configuration map is set as shown in Listing
    3 . Application Pod Configuration. This particular Pod configuration consists
    of bridging the device to the application Pod. The application Pod demands at
    most one virtual networking device. In the example yaml file in Listing 2 , we
    show the device being provided as a resource. Also, Listing 2 shows an example
    of how other resources (e.g. CPU, memory, GPU) can be specified. Listing 1 A configuration
    yaml file for HCA device plugin Show All Listing 2 Integrating virtual network
    device as a resource. Show All Figure 3c shows the different networking levels
    that have to be set to ensure a full Kubernetes cluster setup. The Kubernetes
    Network is comprised of multiple layers of communication. For this setup, we list
    the sub-components of the networking layers that enable a Kubernetes cluster to
    work correctly. Here, we describe the different Kubernetes networking communication
    layers: Inter-container Networking. Kubernetes’ default container communication
    makes use of localhost network on default ssh-port network and the Pod’s network
    name-space . In Figure 3c , in order to avoid running the MPI applications as
    a root user and on default ssh-port, we created a user inside the container for
    which a non-standard ssh-port was exposed to execute the application. Inter-container
    networking level essentially enables a container network interface for the communication.
    Inter-Pod Networking. Pods residing on the same host machines can communicate
    with each other as they are created under the same sub-network. However, Pods
    from different hosts are not reachable to each other as they are part of different
    networks of their respective host nodes. Inter-pod communication is enabled by
    using an overlay network. In Figure 3c , we use Flannel as our overlay network
    to enable inter-pod communication across host machines. Application Pod. In Figure
    3c , the Application Pod shows how Docker containers reside on them. It shows
    how Device Pods, discussed in Figure 3a , connect with each Application Pod. It
    also shows the overlay network setup that ties Application Pods together across
    host machines. Listing 3 yaml file to install and deploy a virtual network device.
    Show All Network setup is a crucial step to launch pods within a Kubernetes cluster.
    A Flannel network assigns a network to all pods on all nodes. Then, Docker bridge
    interface uses the Flannel provided network to create containers. Also, the latest
    MOFED user-space drivers were installed inside the Docker container to enable
    execution of MPI code from within the container. In Figure 3b , we show the IB
    setup architecture of Docker Swarm. It presents the connections between nodes
    in terms of network (e.g. overlay network) and devices (e.g. InfiniBand). We list
    the necessary steps to get a Docker Swarm setup for MPI execution: Overlay Network.
    Overlay attachable network was created to maintain containers grouped together
    on a sub-network. So, containers are physically located in different host machines,
    but they share the same sub-net address space and the same SDN. Container SSH
    Communication. The communication between containers is enabled using a non-standard
    ssh-port and a unique IP address through the overlay network. RDMA Enabled Containers.
    The RDMA protocol was used to aid MPI execution of the containers residing across
    different host machines. InfiniBand devices are mapped to the container at the
    time of deployment. Fig. 3 (a) Kubernetes provides a unique way of accessing host
    nodes’ hardware devices through vendor specific system pods. (b) An overlay attachable
    network to group container in a sub-network. A custom ssh port is exposed to enable
    inter-container communication across containers. All containers within the Swarm
    cluster have user-space IB drivers for MPI execution over RDMA protocol. (c) An
    overlay network was setup using Flannel to enable inter-Pod communication across
    nodes. A device Pod was created to allow device discovery from a container within
    a Pod. Show All Listing 4 Accessing container within Pod to execute MPI on a Kubernetes
    cluster. Show All Listing 5 Accessing container and executing an MPI application
    on Docker Swarm cluster Show All SECTION IV. Experimental Setup For our experiments,
    we acquired six nodes from the Chameleon Cloud to set up a bare metal environment
    and also a container based environment. The purpose of these experiments is to
    investigate the efficacy of HPC with Kubernetes and to identify opportunities
    and areas for improvement. A full fledged scalability study is beyond the scope
    of this paper, due to the small cluster size. In future work, we plan to acquire
    larger clusters to perform scalability studies. In our experiments, we measured
    bare metal performance as the baseline for comparison with Kubernetes and Docker
    Swarm. We used the software and hardware components discussed in Table II . In
    addition, for all application execution, we selected the best performing network
    interface provided within each setup. TABLE II Software, Hardware, and Network
    Stack In order to study container orchestrated MPI workloads on clouds such as
    Chameleon Cloud, we launched one container per machine for the Docker Swarm cluster
    and one Pod per machine for the Kubernetes cluster. We used a small cluster of
    six nodes to conduct our experiments, with each node consisting of 48 CPU cores,
    128 GB of memory, 2TB of local storage, and IB hardware devices. For our performance
    evaluation, we first executed all the benchmarks listed on Table III over an Ethernet
    interconnect on Chameleon cloud. Then, we executed the same set of benchmarks
    over RDMA protocol with the same settings. Both interconnects were orchestrated
    with three different environment setups: Bare Metal, Docker Swarm, and Kubernetes.
    SECTION V. Performance Evaluation 1) OSU AlltoAll Latency To study multi-node
    latency, we chose the AlltoAllv collective from the OSU benchmark suite of MPI
    benchmarks. OSU AlltoAllv communication is designed to have each MPI rank send
    a portion of its data to every other MPI rank, which is a global transposition
    operation acting on sub-portions of a particular data set. In essence, AlltoAll
    spreads 128 MPI processes across six host machines. Then, it performs a ping-pong
    between a sender and receiver. In Figure 4a , Kubernetes shows an overhead resulting
    in 4x performance loss in comparison to Docker Swarm over TCP/IP. We observe that
    as the message size increases from 8KB to around 1 MB for Docker Swarm, the latency
    overhead, when compared to bare metal, improves from 66.77% (8 KB) to 15.95% (1
    MB). However, in Figure 5a for InfiniBand, we ran the benchmark for message size
    of 2MB and it shows an average latency deviation of less than 1% for all the message
    sizes for both container-based setups, when compared to bare metal. This demonstrates
    the point at which the communication becomes wire data rate limited rather than
    host latency limited for Docker Swarm. TABLE III Benchmarks, descriptions, and
    metrics collected. Fig. 4 TCP over Ethernet (128 MPI processes, 6 hosts): Latency
    (a) and bandwidth (b) evaluation results for respective OSU benchmarks. Queries/sec
    is evaluated for memory intensive KMI Hash (c) benchmark and runtime evaluation
    for MiniMD (d). Kubernetes presents a latency bottleneck of 4x in comparison to
    bare metal. For bandwidth, Kubernetes presents a visible overhead of 59.34% for
    a 8192-Bytes message in comparison to bare metal. Show All Fig. 5 RDMA over InfiniBand
    (128 MPI processes, 6 hosts): Latency (a) and bandwidth (b) evaluation results
    for respective OSU benchmarks. Queries/sec is evaluated for memory intensive KMI
    Hash (c) benchmark and runtime evaluation for MiniMD (d). Both Docker Swarm and
    Kubernetes for all the benchmarks present a performance deviation of 1% in comparison
    to bare metal. Show All 2) OSU Bidirectional Bandwidth The OSU Bi-directional
    Bandwidth test over Ethernet sends back-to-back messages and waits for a response.
    It allows the measurement of the aggregated bandwidth between two nodes. Figure
    4b measures and tests inter-node bandwidth using OSU Bidirectional Bandwidth over
    Ethernet connection on bare metal, Docker Swarm, and Kubernetes setups. Both Docker
    Swarm and Kubernetes experience similar bandwidth for message sizes up to 4KB.
    Docker Swarm achieves better performance as Kubernetes stagnates. For message
    sizes greater than 4KB, Kubernetes is outperformed by Docker Swarm, as Docker
    Swarm achieves a bandwidth closer to the Bare Metal. For example, for 4 KB message
    size the results show that both Docker Swarm and Kubernetes exhibit a slower performance
    in comparison to bare metal of 67.39% and 64.97% respectively. For a larger message
    of 8KB the overhead was about 42.14% and 59.34% for Docker Swarm and Kubernetes
    respectively. However, in Figure 5b , we can observe that bandwidth for both container-based
    setups have an average overhead of about 2% in comparison to bare metal for all
    messages. 3) KMI-Hash Evaluation We used the KMI-Hash benchmark to characterize
    the behavior of memory intensive applications running on Docker Swarm and Kubernetes
    over Ethernet. We focused our attention on the number of queries that can be executed
    per second. In Figure 4c , both Docker Swarm and Kubernetes show a performance
    overhead of 16.58% and 33.96% respectively in comparison to bare metal. This performance
    variation is associated with the virtual network interfaces created on Docker
    Swarm and Kubernetes. In Figure 5c the execution over InfiniBand, however, yields
    an overhead of less than 1% for both container-based solutions in comparison to
    bare metal. 4) MiniMD Evaluation MiniMD is a parallel molecular dynamics (MD)
    mini-application mainly used for testing purposes across different HPC systems.
    For MiniMD, the results in Figure 4d show the execution overhead using one thread
    per communication between communicating ranks. Docker Swarm and Kubernetes present
    overheads of 61.76% and 9.37% respectively in comparison to the bare metal execution
    time. However, unlike MiniMD over Ethernet, we notice that in Figure 5d the InfiniBand
    support amortizes the overhead presented by Docker Swarm and Kubernetes, as they
    have an average of just 1% overhead in comparison to bare metal. 5) MiniAMR Evaluation
    MiniAMR is a mini-application designed to carry out stencil calculations on a
    unit cube, which can be emulated in different bodies in space. We modified input
    parameters of two spheres to run on 128 cores for 100-time steps with x=4, y=4,
    and z=8 in each direction (i.e., 4×4×8=128). Over Ethernet, Figure 6a shows the
    results for MiniAMR over the different environment setups. We show the total execution
    time of the application where Docker Swarm and Kubernetes environment setups yielded
    an execution time overhead of 18.41% and 22.01% respectively in comparison to
    bare metal setup. Then, in Figure 6b , we present the interblock communication
    results (- - max_blocks = 10000 per processor), which is an essential metric to
    understand the application’s overall performance as about 47% to 59% of the overall
    execution time is spent executing communication tasks on all the environment setups.
    The results of the interblock communication for 10000 blocks do not show a large
    bottleneck. Kubernetes presented a 14.02% overhead in comparison to bare metal,
    whereas Docker Swarm had negligible overhead. In Figure 6c , we show MiniAMR’s
    throughput with an overhead of less than 1% for both container-based solutions
    in comparison to bare metal. Figure 6d shows that Mesh Refinement’s (which handles
    the Block computations) execution time on bare metal outperforms Docker Swarm
    and Kubernetes by 96.17% and 70.41% respectively. Figures 7a, 7b, 7c, and 7d over
    InfiniBand show an overhead of less than 1% for all the metrics collected. 6)
    MiniFE Evaluation To study the throughput performance further, we evaluated MiniFE
    for two problems sizes – (1) nx=ny=nz=512 and (2) nx=ny=nz=1024. The results presented
    in Figure 8a for problem size (1) shows that the amount of data transported across
    processes in MiniFE can directly affect the application when using a low throughput
    transport protocol such as TCP/IP. Docker Swarm environment performs within 2%
    of Bare Metal. Kubernetes has an overhead of 54.29%. Docker Swarm also provides
    similar interfaces as Kubernetes, but we only included the data for the best performing
    interfaces within each setup. For problem size (2), we observe that for Docker
    Swarm and Kubernetes, the overhead was reduced to 1.24% and 15.70% respectively
    in comparison to Bare Metal. We also observe that doubling the problem size from
    512 to 1024 yielded slightly better results for all environment setups, especially
    Kubernetes, which yielded a 94.50% better performance than problem size (1). In
    addition, in Figure 9b for MiniFE, the runtime results show the overhead of Kubernetes
    for problem size (1) and (2). It posts an overhead of 118.75% and 18.62% respectively
    in comparison to the same problem sizes on bare metal, whereas Docker Swarm shows
    a performance degradation of about 2% for both problem sizes. Our results for
    MiniFE over InfiniBand indicate that there is negligible performance overhead
    for the two cloud container orchestrators. Figures 9a and 9b present an overhead
    below 1% when compared to bare metal setup for both problem sizes. 7) HPCG Evaluation
    We chose HPCG as it provides features that have a large impact on the application’s
    overall performance. For HPCG, for a cube of 160 points, each individual MPI process
    (128 MPI processes) is in charge of performing computations for the 160 points.
    In addition, each environment setup consumes 374.91 GB of memory (i.e., 2.92 GB
    of memory per process). In Figure 8c , we can observe that over TCP/IP, Docker
    Swarm achieves close to Bare Metal performance whereas Kubernetes has an overehad
    of about 13.15%. These results highlight the stress HPCG places on network bandwidth
    and latency. In Figure 4 , Kubernetes shows a significant overhead over both individual
    network tests – this trend can also be when executing HPCG as shown in Figure
    8c . In 9c, HPCG over InfiniBand yields interesting results as Docker Swarm shows
    an overhead of 4.19%, which is more than the 2.2% deviation it has with TCP. Also,
    Kubernetes reduced its overhead by 2.84% (from 13.15% to 10.31%). 8) SNAP Evaluation
    We executed SNAP with no OpenMP threads and also with 4 OpenMP threads per process
    with 128 MPI processes. In Figure 8d . with no OpenMP threads, SNAP shows an overhead
    in runtime for Docker Swarm and Kubernetes of 5.40% and 26.49% respectively. Kubernetes
    performs better when no OMP threads are used as there is a reduction in communication
    and computation. When multiple OMP threads are used for a single process on a
    single core, both Docker Swarm and Kubernetes present an increase in runtime of
    8.68% and 46.64% respectively in comparison to bare metal (i.e., with 4 OMP threads
    per process or 512 threads total). In Figure 9d , SNAP achieves nearly bare metal
    performance for InfiniBand. 9) HPL Evaluation Due to the compute intensive feature
    of HPL, with an Ethernet network connection we chose small P and Q values (i.e.,
    HPL grid) to test performance. Note that the performance and scalability of HPL
    has limitations since the chosen grids are small. Another attribute that is important
    to run HPL is problem size (i.e., N) – the size of any problem should be the largest
    problem size that fits in memory, without creating memory swapping limitations.
    Lastly, another important variable for HPL is block size (i.e., NB), a flag used
    for data distribution. In our HPL execution, we used a problem size of 84000 and
    block size of 96, with memory consumption of about 80% of the total across the
    6-node cluster. For our P and Q, we chose a grid of 4 by 32 respectively. In Figure
    10a , we present the performance of HPL over InfiniBand with both container-based
    setups performing within 1% of bare metal. Fig. 6 MiniAMR Evaluation over Ethernet
    (128 MPI processes, 6 hosts): Throughput and run time is evaluated for MiniAMR
    benchmark. Runtime is evaluated considering interblock communication and mesh
    refinement. For Mesh Refinement, Kubernetes has a performance overhead of 70.41%,
    whereas Docker Swarm presents an overehad of 96.17%. Show All Fig. 7 MiniAMR Evaluation
    over InfiniBand (128 MPI processes, 6 hosts): Throughput (c) and run time is evaluated
    for MiniAMR benchmark. Runtime is evaluated considering interblock communication
    and mesh refinement. Both Docker Swarm and Kubernetes for all the benchmarks present
    a performance deviation of 1% in comparison to bare metal. Show All Fig. 8 TCP
    over Ethernet (128 MPI processes, 6 hosts): Throughput (a) and run time (b) evaluation
    for MiniFE benchmarks. Throughput is evaluated for HPCG (c) benchmark and run
    time evaluation for SNAP (d) for 1 and 4 threads per process. For a problem size
    of 512×512×512, the Docker Swarm environment performs within 2% of Bare Metal,
    whereas Kubernetes environment has an overhead of 54.29%. Show All SECTION VI.
    Related Work The cloud and HPC convergence has gained a lot of traction as a topic
    of research. Cloud technologies has gained enough traction in recent time into
    different communities such a way that it enables the adoption of these cloud based
    solutions into their respective domains [24] [25] . The literature survey in this
    paper is limited to related articles, as research on use of Kubernetes in HPC
    is in its early stages. Younge et al. [26] evaluated the feasibility of different
    container mechanisms to improve the development effort and DevOps for MPI applications
    in HPC systems. According to their results, virtual machines, and containerized
    applications on cloud nodes (no bare metal nodes were used) present a substantial
    performance overhead compared to high end HPC clusters. However, the the performance
    of MPI applications on a Cray supercomputer using Singularity containers demonstrated
    near bare metal performance. Saha et al. [27] evaluated the performance of HPC
    applications in a cloud setup wherein containerized applications were run with
    MPI ranks distributed across multiple containers using Docker Swarm. The results
    showed that the performance deviation of Docker Swarm was negligible compared
    to bare metal performance. In this paper, we explained the components of Kubernetes
    and compared its HPC workload performance with Docker Swarm and bare metal setups.
    Fig. 9 RDMA over InfiniBand (128 MPI processes, 6 hosts): Throughput (a) and run
    time (b) evaluation for MiniFE benchmarks. Throughput is evaluated for HPCG (c)
    benchmark and run time evaluation for SNAP (d) for 4 threads per process. Both
    Docker Swarm and Kubernetes for MiniFE and SNAP present a performance deviation
    of 1% in comparison to bare metal. Show All Fig. 10 HPL Evaluation over Ethernet
    (128 MPI processes, 6 hosts): Throughput is evaluated for a small problem size.
    Both Docker Swarm and Kubernetes present a performance deviation of 1% in comparison
    to bare metal. Show All Saha et al. [28] showed how MPI applications can be orchestrated
    in an Apache Mesos cluster with just the use of Docker Swarm as the container
    orchestrator. They provided insights regarding how cloud based resource managers
    like Apache Mesos [1] can be used for HPC workloads using a policy based approach
    [28] for scheduling of MPI ranks based on the nature of tasks (Network or CPU
    intensive). InfiniBand performance versus TCP in the context of bare metal solutions
    has been evaluated for HPC. Grant et al. [29] evaluated different RDMA options
    versus TCP and demonstrated its impact on commercial data center applications
    [30] . Balaji et al. have evaluated traditional TCP versus IB alternative communication
    sockets-compatible libraries for 10G Ethernet [31] . Rashti et al. performed a
    an evaluation of TCP versus IB and Myrinet networks [32] . In this paper, we focus
    on the architecture, configuration, and evaluation of Kubernetes for use with
    scientific workloads in clouds such as Chameleon. Another related work is an effort
    to enable HPC workloads execution on Kubernetes is kube-batch [33] . It is designed
    to support scheduling and OpenMPI execution. In our work, we focused on understanding
    the performance of Kubernetes for HPC workloads using its default scheduler. SECTION
    VII. Conclusions In this paper, we used a diverse set of MPI applications to evaluate
    their performance and determine the feasibility of executing them in two different
    cloud configurations: bare metal and container-based. Our container-based setups
    consisted of Docker Swarm and Kubernetes. Our metrics included memory usage, bandwidth,
    and latency. For TCP/IP, both container solutions show substantial overhead for
    memory, network bandwidth, and latency. For latency, in Figure 4a , OSU AlltoAll
    latency test execution on Kubernetes shows an overhead of 4x compared to Docker
    Swarm. For bandwidth, in Figure 4b , OSU Bi-directional bandwidth test execution
    on Kubernetes and Docker Swarm for a message size of 8192 Bytes, show an overhead
    of 42.14% and 59.34% respectively. In Figure 8 , we observe that a throughput
    execution of HPCG over TCP/IP with Kubernetes results in an overhead of 13.15%
    in comparison to bare metal. For InfiniBand, we learned that for applications
    with low complexity such as OSU latency, OSU bi-directional bandwidth, SNAP, and
    MiniAMR the performance is within 1% of bare metal. For applications such as HPL,
    which is compute intensive and also low complexity, we observe for both Docker
    Swarm and Kubernetes a similar performance overhead of less than 1%. However,
    in Figure 9c for HPCG, InfiniBand transport reduces the overhead presented by
    Kubernetes in comparison to bare metal from 13.15% (55.29 GFlops) to 10.31% (58.26
    GFlops). We observed that there are opportunities when enabling a high performance
    transport in all the setups. We attribute the Kubernetes overhead to the fact
    that its whole network stack is virtualized. In a Docker Swarm setup, users have
    the flexibility to choose between virtualized and non-virtualized interfaces.
    We identified that it is crucial for HPC developers and architects to use an evaluation
    framework, such as the one we have developed, to study and make informed decisions
    on the configurations and setups to use for executing their workloads. ACKNOWLEDGMENT
    Sandia National Laboratories is a multimission laboratory managed and operated
    by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned
    subsidiary of Honeywell International, Inc., for the U.S. DOE National Nuclear
    Security Administration under contract DE-NA-0003525. This research was partially
    supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort
    of the U.S. Department of Energy Office of Science and the National Nuclear Security
    Administration. In addition, this work was supported in part by NSF grant OAC-1740263.
    Authors Figures References Citations Keywords Metrics More Like This Evaluation
    of new CYPHONIC: Overlay network protocol based on Go language 2022 IEEE International
    Conference on Consumer Electronics (ICCE) Published: 2022 Lightweight Virtualization
    Approaches for Software-Defined Systems and Cloud Computing: An Evaluation of
    Unikernels and Containers 2019 Sixth International Conference on Software Defined
    Systems (SDS) Published: 2019 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Enabling HPC Workloads on Cloud Infrastructure Using Kubernetes Container
    Orchestration Mechanisms
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/ficloud.2019.00036
  analysis: '>'
  authors:
  - Areeg Samir
  - Claus Pahl
  citation_count: 25
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 7th International Confer...
    DLA: Detecting and Localizing Anomalies in Containerized Microservice Architectures
    Using Markov Models Publisher: IEEE Cite This PDF Areeg Samir; Claus Pahl All
    Authors 25 Cites in Papers 902 Full Text Views Abstract Document Sections I. Introduction
    II. Background III. Related Work IV. HHMM Customization V. Detection and Localization
    of Anomaly (DLA) Show Full Outline Authors Figures References Citations Keywords
    Metrics Footnotes Abstract: Container-based microservice architectures are emerging
    as a new approach for building distributed applications as a collection of independent
    services that works together. As a result, with microservices, we are able to
    scale and update their applications based on the load attributed to each service.
    Monitoring and managing the load in a distributed system is a complex task as
    the degradation of performance within a single service will cascade reducing the
    performance of other dependent services. Such performance degradations may result
    in anomalous behaviour observed for instance for the response time of a service.
    This paper presents a Detection and Localization system for Anomalies (DLA) that
    monitors and analyzes performance-related anomalies in container-based microservice
    architectures. To evaluate the DLA, an experiment is done using R, Docker and
    Kubernetes, and different performance metrics are considered. The results show
    that DLA is able to accurately detect and localize anomalous behaviour. Published
    in: 2019 7th International Conference on Future Internet of Things and Cloud (FiCloud)
    Date of Conference: 26-28 August 2019 Date Added to IEEE Xplore: 30 January 2020
    ISBN Information: DOI: 10.1109/FiCloud.2019.00036 Publisher: IEEE Conference Location:
    Istanbul, Turkey SECTION I. Introduction Microservice architectures structure
    an application as a collection of services to keep it scalable and reliable [1],
    [6]. Each service can be considered as an application of its own. Services can
    communicate with each other by exposing their endpoints for the same service contract
    in order to provide functionality over different protocols. Microservices introduce
    multiple benefits like high availability, better flexibility, fast responsiveness,
    and scalability. To support microservices, it is beneficial having a light-weight
    deployment mechanism using small independently deployable services. Those requirements
    can be achieved by utilizing the container technology. Containers [3] isolate
    microservices into smaller instances that utilize only the required operating
    system and platform components, thus making them lightweight [15], [22]. However,
    microservices may face several challenges. One of these challenges is changing
    a service workload in a composed microservice may increase the system load as
    changes in one service may influence the workload of other dependent services,
    and may result in response time degradation that would be considered as anomalous.
    These variations may lead to violations of Service Level Agreements (SLAs) between
    providers and users. In such a case, anomaly detection and localization can help
    in capturing and tracking the anomalous behaviour that deviates from the normal
    behaviour [11]. To be able to detect and localize the anomalous behaviour, there
    are plentiful metrics at node, container, service, and application levels. In
    this paper, we propose a Detection and Localization systems for Anomalies (DLA)
    that detects and locates the anomalous behaviour of microservices based on the
    observed response time. The DLA tries to detect the variation in response time,
    and correlates it to the respective component (microservice, container, service),
    which shows anomalous behaviour. To validate the DLA, evaluation based on analyzing
    a dataset extracted from microservice based container is discussed in Section
    VI. The experiment shows that DLA can detect and locate the anomalous behaviour
    with an accuracy percentage of more than 97%. The paper is organized as follows:
    a background is given in Section II. An overview of the literature studies is
    explained in Section III. Adjusting our model to reflect our system topology is
    addressed in Section IV. An overview of the proposed work (DLA) is presented in
    Section V. An evaluation is presented in Section VI. Conclusions are made in Section
    VII. SECTION II. Background Containers are isolated workload environments in a
    virtualized operating system such as cloud. They are used to allocate computing
    resources, help to organize, migrate and develop microservices [8], [9], [12].
    Microservice architectures are an approach to develop a single application that
    is composed of small services, each of which is in a container and runs its own
    process. Microservices are communicating with lightweight mechanisms, often an
    HTTP resource API. They enable continuous de- livery/deployment of large complex
    applications as they are deployed as individual containers within a containerized
    architecture so they can be scaled up or down by adding or removing instances.
    However, there are some characteristics [2] of microservices, which complicate
    its performance monitoring. One of such characteristics is monitoring and deployment.
    To guarantee the performance of container-based microservice application, a coherent
    analysis of the performance metrics across microservices, containers, and nodes
    monitored resources is required. However, variations and uncertainties of performance
    metrics across different microservices, containers and nodes resources complicate
    the assessment of microservice performance. If the anomalous measurements are
    not localized to the causing component, then anomalies will remain untreated.
    In this paper, we adopt Hierarchical Hidden Markov Models (HHMM) [5] to learn
    the model based on different monitored metrics such as CPU, Memory, and Network
    to detect and locate the anomalous behaviour. The aim is to model the relation
    between the monitored metrics of container, node and service, and the variation
    in response time under different load scenarios. HHMM [5] is a generalization
    of the statistical machine learning technique Hidden Markov Model (HMM). It is
    designed to model domains with hierarchical structure (e.g., speech recognition,
    plan recognition, intrusion detection). In most of these applications the model
    has a well performance to model hierarchical structure. HHMM does not emit observable
    symbols directly as it is consisted of one root state that is composed of a sequence
    of sub-states. Each sub-state may compose of another sub-states and generates
    sequences by recursive activation. The process of recursive activations stops
    when it reaches a production state. The production state(s) is the only state
    that emits output symbols like HMM. The other states that emit substates instead
    of output symbols are called "internal states" or "abstract states". The process
    that an internal state activates a sub-state is termed ’vertical transition",
    while a state transition at the same level is called "horizontal transition".
    When a vertical transition is completed, the state which originated the recursive
    action will get the control and then performed a horizontal transition. Each level,
    except the root, has an end state, which ends the horizontal transition at this
    level, and returns the control to the root state of this whole hierarchy. HHMM
    is identified by HHMM =< λ, χ, θ >. The λ is a set of parameters consisted of
    horizontal ξ and vertical χ transitions between states qd, d specifies the number
    of vertical levels in the hierarchy structure; state transition probability A;
    observation probability distribution B; initial transition π; state space SP at
    each level and the hierarchical parent-child relationship q d i , q d+1 i . The
    i specifies the index of horizontal levels in the hierarchy structure. The Σ consists
    of all possible observations O. The states in HHMM are hidden from the observer
    and only the observation space is visible. We expect the model to be able to:
    (1) efficiently analyze large amounts of data to detect potential anomalous observation
    in container based microservice architecture; (2) track the cause of the detected
    anomaly. SECTION III. Related Work In [7], the authors evaluate the performance
    of container-based microservices considering two models: master-slave (one container
    is the master of other containers), nested-container (children containers run
    application, and they are limited by their parent’s boundaries). The authors evaluate
    the models by focusing on the CPU and Network performance data. In [2], the author
    propose an approach to detect and locate anomaly in container-based microservice
    using PAD, RanCorr and EAR. The authors focus on reducing the false alarm observed
    at response time. The paper in [10] presents an anomaly detection system for container-based
    microservices using different machine learning techniques. The authors evaluate
    the performance of the system through focusing on the CPU, Memory and Network.
    In [13], the authors investigate the behaviour of RPCA algorithm and HTM neural
    network on detecting anomalies in microservice architecture. In [14], an approach
    is proposed to detect anomalies that are related to the behaviour of services
    in VMs using machine learning techniques. The authors evaluate their approach
    by focusing on CPU, Memory, Disk and Network performance data. Many literatures
    used HMM and its derivations to detect anomaly. In [17], the author proposes various
    techniques implemented for the detection of anomalies and intrusions in network
    using HMM. In [20] the author detects faults in real-time embedded systems using
    HMM through describing healthy and faulty states of a system’s hardware components.
    In [23], HMM is used to find which anomaly is part of the same anomaly injection
    scenarios. The objective of this paper is to detect and locate (DLA) the anomalous
    behaviour for container-based microservices using HMMs. The proposed DLA consists
    of: (1) Monitoring that collects the performance data of (services, containers,
    nodes ’VM’) such as CPU, memory, and network metrics; (2) Detection that detects
    anomalous behaviour which is observed in response time of a component; (3) Anomaly
    injection which simulates different anomalies and gathers dataset of performance
    data representing normal and abnormal conditions. SECTION IV. HHMM Customization
    In this paper, we use HHMM: (1) to correlate the hidden component metrics with
    the observed response time to find the root cause of the anomaly. (2) We also
    use HHMM to detect components that are affected by the anomaly. We consider that
    our system Under Test (sUT) has hierarchical structure. The microservice architecture
    MS (root state) consists of some nodes VMs (internal states) that consists of
    one or more containers C (substates), and each container has one service S (production
    state) that runs on it. Containers may have communication at the same node or
    at external node. A microservice can be deployed in several containers at the
    same time, and a container is defined as a group of one or more containers constituting
    one microservice. These characters substituted a foundation to apply HHMM to describe
    the behaviours of a system in hierarchical structure. To meaningfully represent
    our system, Fig. 1, we modify the HHMM annotations. So, the qd=1 root state is
    mapped to MSj=1. The internal state q d=2 i represents virtual machine (node),
    and it is mapped to V M j i . The i is the state index at horizontal level, and
    j is the hierarchy index at vertical level d. The substate q d+1 i is mapped to
    C j+1 i to represent containers (e.g., C 3 1 at vertical level 3 and horizontal
    level 1), and the production state q d+2 i is mapped to S j i to represent services
    that emit observations On = {o1 ⋯,on}. The set of observations On is associated
    to the response time fluctuation RTn that are emitted from services in containers.
    The State Space SP is mapped to Microservice Space MSS, which consists of one
    or more Microservice(s) MS. Each MS consists of a set of VMs, containers C, and
    services S. Example, V M 2 1 ={ C 3 1 , C 3 2 } , V M 2 2 , V M 2 3 ={ C 3 3 ,
    C 3 4 }; C 3 1 ={ S 4 1 } , C 3 2 ={ S 4 2 } , C 3 3 ={ S 4 3 } , C 3 4 ={ S 4
    4 } . The HHMM vertically calls one of V M j i with vertical transition χ. Since
    V M j i is internal state, it enters its child HMM substates C j i , which call
    the production state(s) S j i . Since S j i is a production state, it emits observations,
    and may make horizontal transition ξ from C 3 1 to C 3 2 as the service is deployed
    inside a container. Once there is no another transition, C 3 2 transits to the
    end state End, which ends the transition for this substate, to return the control
    to the calling state V M 2 1 . Once the control returns to the state V M 2 1 ,
    it makes a horizontal transition (if exist) to state V M 2 2 , which horizontally
    transits to state V M 2 3 . When all the VM states finish all their substates,
    the last VM state returns the control to the MS to obtain the whole transitions
    and observations. The edge direction (horizontal/vertical transition) between
    the states represents the interaction between the states in our SUT. The horizontal
    transition between containers reflect the request/reply between the client/server
    and the vertical transition refers to parent-child dependency between microservice/VMs/containers/services.
    This type of hierarchical presentation aids in detecting and tracking the anomaly.
    Fig. 1. System Structure Using HHMM. Show All SECTION V. Detection and Localization
    of Anomaly (DLA) A. Resource Performance Monitoring To be able to accurately capture
    anomaly behaviour, performance data at different system levels are being collected
    from ’’Managed Component Pool" to study system resource status to be able to assess
    the overall performance of microservice. Each VM in the pool has an installed
    agent that is responsible for collecting metrics from the pool, and exposing log
    files of containers and nodes to Real-Time/Historical Data storage. The collector
    clears outliers from the collected data. Then, those data will be monitored by
    the resource performance monitoring as will be shown later in this section. As
    there are a huge number of metrics at different system levels, and a microservice
    may be deployed across many servers, identifying the key metrics of microservices
    are necessary for describing its behaviour sufficiently. Thus, we explore the
    Key Performance Indicators (KPIs) metrics at the host- infrastructure and microservice
    levels, and we select the most common performance metrics that will aid us in
    monitoring the anomalous behaviour [4], [24], [25]. Table I shows the metrics
    that are collected at different system levels. Further, several monitoring tools
    are used to collect and store performance metrics of the target system such as
    SignalFX Smart Agent,docker stats command to obtain a live data stream for running
    containers, mpstat to know the percentage of CPU utilization, and vmstat to collect
    information about the memory usage. The collected performance data are grouped
    using Heapster, and stored in a time series database called InfluxDB (Real-Time/Historical
    Data) to determine the time that the data collected belong. The data collected
    in the storage will be accompanied with the user transaction, which is a key created
    to identify the requests in a given time for that transaction. The key and the
    monitored metric parameter values will be used to know how much the resource vary
    based on the user transactions, and to compare it with the response time in the
    same given time of transaction. The user transactions refer to the request rate,
    which is the number of requests per second. Fig. 2. Detection and Localization
    of Anomaly system (DLA). Show All TABLE I. Monitoring Metrics To check if there
    is performance variation at service or container level, we verify if the captured
    variation in a metric is a symptom of anomaly. Thus, we use spearman’s rank correlation
    coefficient to estimate the dissociation between different number of user transactions,
    and the collected parameters from the monitored metrics. The dissociation refers
    to anomalous behaviour occurred at one or more component(s) at a given time that
    deviates from normal behaviours. The dissociation tells us if the value of the
    metric parameter increases or decreases according to the number of requests. If
    there is a decrease in the dissociation degree, then the metric is not associated
    with the increasing number of requests, which means the observed degradation in
    performance is not an anomaly. In case the degree of dissociation increases, this
    refers to the existence of anomaly. In such case, the anomaly detected as the
    impact of dissociation between the number of user transaction, and the monitored
    metric exceeds a certain value. To achieve that we generate general thresholds
    to highlight the occurrence of anomaly at different monitored metrics. In Fig.
    3, Lines 1-5 represent the creation of different parameters to be used during
    the algorithm. Lines 6-7 capture the number of user transactions during t-th time
    interval. Lines 8-14 loop to retrieve users transactions in a specific interval,
    and accumulate all the transactions to be used by the correlation coefficient.
    We retrieve and accumulate the monitored metric parameters for all containers
    that run the same service in the same specified interval for the users transactions.
    Line 15 calculates the spearman correlation for the users transactions’ and the
    measured metric. The correlation is computed for each monitored metric, and the
    number of user transactions processed in a specific t time interval. The result
    of the correlation will tell the strength of the relationship between the two
    measured variables, which means if the number of user transactions increases,
    then the cumulative value of metric parameter should also increase. If the cumulative
    value of a parameter increases, and the number of requests does not increase,
    then the result of correlation indicates that the degradation in the performance
    metric may relate to workload contention or resource exhaustive. Line 16 calculates
    both the variances for the current time interval, and the highest old one for
    both monitored metric and user transactions. Then the product between the estimated
    variances will give one threshold. Such threshold is estimated for each monitored
    metric. Line 17 returns the threshold (degree of dissociation ’DD’) used to detect
    the occurrence of anomaly behaviour for each monitored metric. The degree of dissociation
    (DD = 45 for CPU Utilization; DD = 20 for Memory Utilization; 15 for Network)
    can be used as an indicator for performance degradation. Once the anomaly behaviour
    is captured, we build HHMM to detect and locate the anomaly. B. Detection and
    Localization of Anomaly (DLA) Since anomalies always cause metrics to fluctuate,
    locating anomalous metrics help to narrow down the root causes. Thus, we need
    to detect and localize the metric, which most probably causes the container, node,
    or service status to change to anomalous. Here, we use the generalized Baum-Welch
    algorithm to train the HHMM to estimate model parameters. We assume that each
    service, node, or container has workload, linked to CPU, Memory or Network metrics.
    Such workload is hidden from the observer, and it can be detected through observing
    the variations in response time behaviour, which is emitted by the production
    service states S j i . To obtain the total performance data of a specific microservice,
    the performance data should be collected from various layers of the service that
    emitted the observation (node, container, service). The data, which are collected,
    depict whether a microservice is anomalous. We focus on performance detection
    and identification at container, node and microservice level. Fig. 3. Check Performance
    Degradation. Show All a) Calculate The Likelihood of Observing Sequence: since
    each of the internal state in our HHMM can be viewed as HMM model that consists
    of containers substates and services production states, we calculate the likelihood
    of generating anomalous response time observations given the state sequence P(RT|λ,M
    S j i ),P(RT|λ,V M j i ),P(RT|λ, C j i ),P(RT|λ, S j i ) . Let us assume that
    the last state ended End at the third level C3 to return the control to V M j
    i . The observation sequences RTn emitted at hierarchical length T, which is the
    length of the observation sequence (t = 1, 2, ⋯, T). The i = (i1, ⋯, in) is a
    set of horizontal states index that were visited during the generation of RT at
    the same level of states. j = (j1, ⋯, jn) is a set of vertical state index. To
    calculate the probability that the observation sequence RTn was generated by C,
    we calculate the probability that partial RTn was generated by Cj−1 (at that case:
    V M 2 i ), and Cj was the last state activated by Cj−1 as shown in "(1)". The
    α(t, t+k, C j i , Cj−1) is the probability that the partial observation sequence
    RTn = {rtt, ⋯, rtt+k} was generated by state C j i at time t and end at time t
    + k, and C j i was the last state activated by Cj−1 during the generation of the
    partial observation sequence. Each rtt may generate observation sequence rtt =
    {high response time, normal response time, high response time}. To calculate α,
    we sum all possible states at level j ending at C j−1 End . α(t,t+k, C j i , C
    j−1 )= P(r t t ,…,r t t+k , C j i ,t+k| C j−1 ,t) (1) View Source To calculate
    the probability that the sequence RTn = {rtt, ⋯, rtt+k} was generated by Cj−1,
    we sum all the possible states at level j ending at C j−1 End , as shown in "(2)".
    P(r t t ,⋯,r t t+k | C j−1 )= ∑ i=1 C j−1 α(t,t+k, C j i , C j−1 ) a C j−1 iEnd
    (2) View Source Then, we calculate the likelihood for the entire observation sequence
    through accumulating all the possible starting states (C and VM) called by the
    root state MS that began at time t = 1 and ends at T as in "(3)". The equation
    is applied also for each substate called by internal state, and for each production
    state called by substate by subtracting from the vertical level. For example,
    if we are at the container level Cj=3, then Cj−1 leads to node level VMj=2, and
    Cj−2 from the container level leads to root state MSj=1. We follow that way in
    the rest of the paper for simplicity. The same mechanism is also applied for the
    states at production state level Sj=4. In that case, we accumulate all the states
    from production states to the root state considering hierarchy structure has 4
    levels. P(R T n |λ)= ∑ i=1 C j−2 α(1,T, C j , C j−1 , C j−2 ) P(R T n |λ)= ∑ i=1
    S j−3 α(1,T, S j , S j−1 , S j−2 , S j−3 ) (3) View Source Once we calculate the
    forward variable for all the states in the hierarchy, we calculate the backward
    variable as in (4). The backward variable β(t, t + k, S j i , Sj−1, Sj−2, Sj−3)
    is the probability that the observation sequence (rtt, ⋯, rtt+k) was generated
    by S j i at time t, and S j i was the last state activated by Sj−3 in the hierarchy
    and finished at (t + k) during the generation of the partial observation sequence.
    β(t,t+k, S j i , S j−1 , S j−2 , S j−3 )= P(r t t ,⋯,r t t+k | S j i ,t, S j−3
    ,t+k) (4) View Source Those steps will be applied to each state in the hierarchical
    starting from bottom to up. b) Find The Most State Sequence (Detect Anomalous
    Path): to find the hidden hierarchical states sequence, which is the cause of
    anomaly, and to correlate the observed anomalous RTn behaviour to the path that
    has anomalous component, we use the generalized Viterbi algorithm, and customize
    it to reflect our system. We start from the production states, and calculate the
    likelihood of the most probable hierarchical state sequence generating the observation
    sequence (rtt, ⋯, rtt+k) that Sj−3 was entered at time t, and its substates Sj−2,
    Sj−1 are the last states to be activated by Sj−3, and the control returns to Sj−3
    at time end (t + k). First, we initialize the production state, and then we calculate
    the probabilities from bottom-up: (1) δ(t, t + k, S j i , Sj−1, Sj−2, Sj−3), which
    is the likelihood of the most probable state sequence generating (rtt, ⋯, rtt+k)
    by a recursive activation. Such activation starts from Sj−3, and ended at S j
    i at time t, then it returns to Sj−3 at time end (t + k) as shown in "(5)". (2)
    We calculate the state list ψ(t, t + k, S j i , Sj−1, ⋯, Sj−3), which is the index
    of the most probable production states to be activated by Sj−3 before activating
    S j i . (3) We calculate τ (t, t + k, S j i , Sj−1, ⋯, Sj−3), which is the transition
    time at which S j i was called by Sj−1 at time t and ended at time (t + k) considering
    the hierarchy until Sj−3 as show in "(6)". If S j i generates the entire subsequence,
    we set τ (t, t + k, S j i , Sj−1) = t, where t refers to the started time of the
    generated sequence. Given (τ, ψ), we can obtain the most probable hierarchical
    state sequence from the production states to the root state listed by their activation
    time. From (δ, ψ,τ), for each level, we compute the recursion from the production
    until the root state. Equation "(7)" shows the computation of recursion from production
    state to S j i to root state Sj−3 considering the probabilities of observations
    and states transitions. δ(t,t, S j i , S j−1 ,⋯, S j−3 )= π S j−3 ( S j i ) b
    S j i (r t t ) τ(t,t+k, S j i , S j−1 ,⋯, S j−3 )=t+k (δ(t,t+k, S j i ,⋯, S j−3
    ),ψ(t,t+k, S j i ,⋯, S j−3 ))= max 1≤y≤ S j−3 ⎧ ⎩ ⎨ ⎪ ⎪ δ(t,t+k−1, S j y ,⋯, S
    j−3 ) a S j−3 i.y b S j i (r t t+k ) ⎫ ⎭ ⎬ ⎪ ⎪ (5) (6) (7) View Source To find
    the most probable hierarchical state sequence in a set of hierarchical state sequences,
    we calculate δ of a state sequence at time set t ¯ =t+1,⋯,t+k . Then, we recursively
    calculate ω that represents ψ and Δ( t ¯ ) , where t ¯ is the time when S j=4
    r was activated by S j−1 r , S j−2 r , S j−3 i (e.g., C, VM, MS) at start time
    t and end time (t + k). Then we take the maximum as depicted in "(8)", and "(9)".
    The r and y represent horizontal indexes. Then, we calculate transition probabilities
    (a) from production states until root state. L= max (1≤r≤ S j−1 i ) ⎧ ⎩ ⎨ ⎪ ⎪
    δ( t ¯ ,t+k, S j r , S j−1 r ) a S j−1 i rEnd ⎫ ⎭ ⎬ ⎪ ⎪ ω= max (1≤y≤ S j−3 ) {
    δ(t, t ¯ −1, S j−1 , S j−2 , S j−3 ) a S j−3 yiEnd L } (8) (9) View Source Once
    all the recursive transitions are finished and returned to root state (MS), we
    get the probability of most hierarchy state sequence starting from (MS) following
    by (VM) internal state(s) to obtain all its substates in the hierarchy through
    scanning: the sate list ψ, states likelihood δ, and transition time τ as shown
    in "(10)". stateSeq= max V M 2 i ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ δ(1,T,V M 2 i ,MS), τ(1,T,V M 2
    End ,MS), ψ(1,T,V M 2 End ,MS) ⎫ ⎭ ⎬ ⎪ ⎪ ⎪ ⎪ (10) View Source c) Estimate The
    Model Parameters (Locate the Anomalous Component): the HHMM is trained on response
    time over the hidden metrics. The model is learned using the generalized Bauch-Welch
    algorithm to obtain the transition probabilities of states, observations, and
    state transitions. Once we have a learned model, this output will be used to feed
    the Viterbi algorithm to obtain the hierarchy path of the anomalous state. To
    achieve that, we train the model to get the probabilities over all the states.
    So, four variables will be calculated: forward transition α, backward transition
    β, vertical transition χ, and horizontal transition ξ as shown in "(11)-(15)".
    At equation "(11)", we start with calculating the transition and observation probabilities
    at service level (S) that is happened at time t and ended at time (t + k). α(t,t+k,
    S j i , S j−1 )= [ ∑ S j−1 u=1 α(t,t+k−1, S j u , S j−1 )] b S j−1 i (r t t+k
    ) (11) View Source Then as in "(12)", we calculate the transition probabilities
    at the container ( C j i ) and VM (Cj−1) levels that happened at time t and ended
    at time (t + k) considering the observations of substate that are generated from
    the production state. α(t,t+k, C j i , C j−1 )= ∑ l=0 k−1 [ ∑ C j−1 y=1 α(t,t+l,
    C j y , C j−1 ) a C j−1 i.y ] [ ∑ C j i g=1 α(t+l+1,t+k, C j+1 g , C j i ) a C
    j i gEnd ]+ π C j−1 ( C j i )[ ∑ C j i g=1 α(t,t+k, C j+1 g , C j i ) a C j i
    gEnd ] (12) View Source Then as in "(13)", we calculate the backward probability
    β. The β is the probability of seeing the observations from time (t + 1) to time
    end (t + k) at the production state level. β(t,t+k, S j i , S j−1 )= b S j−1 i
    (r t t )[ ∑ S j−1 u≠End α S j−1 ui β(t+1,t+k, S j u , S j−1 )] (13) View Source
    After that as in "(14)", we calculate the backward transition probabilities at
    the container ( C j i ) and VM (Cj−1) levels considering the observations of substate
    that are generated from the production state (Cj+1). β(t,t+k, C j i , C j−1 )=
    ∑ l=0 k−1 [ ∑ C j i s=1 π C j i ( C j+1 s )β(t,t+l, C j+1 s , C j i )] [ ∑ C j−1
    u=1 a C j−1 i.u β(t+l+1,t+k, C j u , C j−1 )]+ [ ∑ C j i s=1 π C j i ( C j+1 s
    )β(t,t+k, C j+1 s , C j i )] a C j−1 iEnd (14) View Source To obtain the number
    of horizontal transitions from a state C j i to another C j End both are substates
    of C j−1 l , we calculate ξ as in "(15)". The ηin refers to the probability of
    performing vertical transition from substate C j−1 l to state Cj−2 that the RTn
    is started to be emitted for a state at starting time t. The vertical transition
    happened, once all the horizontal transitions are done. ηin is calculated using
    ξ by summing all substates C j−1 l which can perform transition to Cj−2. The ηout
    refers to the vertical state transition probability from C j−1 l to state Cj−2
    in which RTn is emitted and finished at T. ξ(t, C j i , C j End , C j−1 l )= 1
    P(RT|λ) [ ∑ T t=1 η in (t, C j−1 l , C j−2 )α(t,T, C j i , C j−1 l )] a C j−1
    l iEnd η out (T, C j−1 l , C j−2 ) (15) View Source Then at "(16)", we calculate
    χ to obtain the probability of vertical transition from C j−1 l to C j i in which
    C j−1 l is entered at time t before emitting response time observation RTt to
    activate state C j i . Here, ηin, we consider the vertical transition from Cj−1
    (VM) that starts at time t to Cj−2 root state (MS). We also consider the backward
    probability β, which is the transition from C j i to Cj−1. The ηοut calculates
    the transition from Cj−1 to Cj−2 once Cj−1 finishes at time e with observations
    (rtt+1, ⋯, rtT|λ). χ(t, C j i , C j−1 l )= η in (t, C j−1 l , C j−1 ) π C j−1
    l ( C j i ) P(RT|λ) [ ∑ T e=t β(t,e, C j i , C j−1 l ) η out (e, C j−1 l , C j−2
    )] (16) View Source After that, we calculate Production State Vertical transition
    (PSV), to obtain the number of vertical transitions χ from container substate
    Sj−1 to the production service state S j i after finishing all the horizontal
    transitions between states Sj that are substate(s) from Sj−1 at the same level
    as in "(17)". ∑ t=1 T−1 PSV(t, S j i , S j−1 )= [ ∑ T t=1 χ(t, S j i , S j−1 )+
    ∑ T t=2 Q(t, S j i , S j−1 )] (17) View Source To learn the model, and to enhance
    the detection and identification, we train the model to obtain a new set of parameters
    for the state transitions probability Â observation transition probability B ^
    , and state transition hierarchy sequence. The training of HHMM is stored in the
    "Basic Configuration" file to be used in the future. The model is trained on observations
    until parameter convergence become fixed. To train the HHMM parameters on continuously
    updated response time, we use the sliding technique [28] to read the observations
    through specifying window size = 10-200 observations, and feed them to the HHMM
    model. We compare the obtained sequence against the observed one to detect anomalous
    behaviour, and to obtain the anomalous path. If the observed sequence and detected
    sequence are similar, we can conclude that we have a model learned on normal behaviour,
    and an alarm will be issued for further investigation. Learning the model on normal
    behaviour aids in estimating the probability of each observed sequence. We call
    the sequence with the lowest estimated probabilities anomalous. The model tracks
    the detected anomaly to locate its root-cause. The located component(s) (service,
    container, VM, or microservice), will be stored in a storage with its probability,
    time of its activation, and time of detection. SECTION VI. Evaluation To assess
    the DLA, we investigate different anomaly scenarios, which are injected into TPC-W
    benchmark. A. Setup Settings TPC-W1 benchmark is used for resource provisioning,
    scalability, and capacity planning for e-commerce. TPC-W emulates an online bookstore
    that consists of 3 tiers: client application, web server, and database. Each tier
    is installed on VM, and deployed on Kubernetes. For simplicity, we assume no database
    anomalies. To obtain historical data, TPC-W is run for 300 minutes, and workload
    is generated at different times. The workload of each container should be similar.
    The number of records that we obtain from the TPC-W was 2000 records, 50% is used
    for training, and the rest is used to validate the model. The model training lasted
    150 minutes. The experimental environment is consisted of four VMs (nodes) in
    two servers. Each server is equipped with Ubuntu 18.10, Xen 4.112, 2.7 GHz CPU
    and 8 GB RAM. The first server hosts 3 VMs (TPC-W benchmark), and the second one
    hosts 1 VM (Fault Injection). Each VM is equipped with LinuxOS (Ubuntu 18.10 version),
    one VCPU, 2GB of VRAM. The VMs are connected through a 100 Mbps network. An agent
    is deployed on each VM to collect monitoring data from the components in SUT (e.g.,
    host metrics, container, performance metrics, and workloads), and send them to
    the storage to be processed by the performance monitoring. We focus on CPU, Memory,
    Network, and throughput metrics, which are gathered from the web server, while
    the response time is measured from client’s end. We further use docker stats command
    to obtain live data stream for running containers, mpstat to know the percentage
    of CPU utilization, and vmstat to collect information about the memory usage.
    We use Heapster3 to group the collected data, and store them in a time series
    database using InfluxDB4. The gathered data from the monitoring tool, and from
    datasets are stored in the Real-Time/Historical Data storage to enhance the future
    anomaly detection and identification. The SUT services are running in containers,
    and the number of the replica of component is set to two. It means there will
    be two containers running the same service. The performance data of a service
    is the sum of all the containers running this service. Thus, the anomaly of a
    service is often caused by the anomalous behaviours of one or more containers
    belong to this service. By collecting the performance data of all the related
    containers, we can obtain the total performance data of a specific microservice.
    We consider that if multiple containers run on a node, all will get the same proportion
    of resources (i.e., CPU cycles). Consequently, if resources in one container are
    idle, other containers are able to use the leftover of resources (i.e., remaining
    of CPU cycles). There is no guarantee that each container will have a specific
    amount of resources (i.e., CPU time) at runtime. Also, we assume that containers
    are not resource bound to be able to show the overload at SUT. Because the actual
    amount of resource (i.e., CPU cycles) allocated to each container instance will
    differ based on the number of containers running on the same node, and the relative
    settings (i.e., CPU-share) assigned to containers. B. Anomaly Scenarios In the
    beginning of the experiment, the microservice operations get assigned initial
    delays, which are intended to resemble usual response times, and are not treated
    as actual injections. To simulate real anomalies of the system, script is written
    to inject different types of anomalies into the SUT (nodes and containers). a)
    Resource Exhaustion: CPU Hog: such anomaly is injected to consume all CPU cycles
    by employing infinite loops. Memory Leak: an anomaly that exhausts a component
    memory. Network Congestion: components are injected with anomalies to send or
    accept a large amount of requests in network. Pumba5 is used to cause network
    latency and package loss; to create CPU Hog and Memory leak, stress6 is used.
    b) Workload Contention: The TPC-W web server is emulated using client application,
    which generates workload (using Remote Browser Emulator) by simulating a number
    of user requests that is increased iteratively. Since the workload is always described
    by the access behaviour, we consider the container is gradually workloaded within
    [30-100000] emulated users requests, and the number of requests is changed periodically.
    The client application reports response time metric, and the web server reports
    throughput and resource utilization. We create a workload that runs for 2940 seconds.
    To measure the number of requests and response (latency), HTTPing7 is installed
    on each node. Also ZipKin8 is used to trace the request through the system. C.
    Anomaly Detection and Localization To represent the anomaly detection/identification
    of the anomaly scenarios for the SUT, different anomalies were injected at each
    component in the system one at a time. The components are chronologically ordered
    based on the period consumed from the beginning of injection until detection.
    Following shows different anomalies injection occurred for each component at different
    times. a) Resource Exhaustion: Tables II, III, IV depict the anomalies injection,
    detection, and localization time for each component with injection time 300 sec
    for each component, and pause time 120 sec. The start and end times of each anomaly
    are logged. As shown in the tables, VM2.1 and C3.2 were the first components detected
    by the model for the CPU Hog and Memory leak respectively, while for Network congestion,
    C3.1 was the first component captured by the DLA. Hence, we inject one component
    at a time, the reason behind the variation of the detection time returns to the
    variation in the injection time of anomalies. The localization of the components
    differed from one to another, however, the maximum time that the DLA took to locate
    the detected anomalous component was: 10 sec for the CPU Hog injection, 60 sec
    for the Memory leak, and 40 sec for Network Congestion. TABLE II. CPU Hog Detection
    and Localization using HHMM TABLE III. Memory Leak Detection and Localization
    using HHMM b) Workload Contention: For the workload, we generate gradual user
    requests/sec at the container level (that contains TPC-W server). The number of
    users requests increases from 30 to 1000 with a pace of 100 requests/second incrementally,
    then we increase the requests from 1000 to 100,000 with a pace of 500 requests/second.
    Fig. 4 shows the effect of the workload (i.e., number of user requests) on the
    monitored metrics. The monitored metrics utilizations increase when the number
    of requests increases, then it remains constant. This means that the utilizations
    of CPU, memory, and network almost reached maximum capacity at 90%, 89%, and 25%
    respectively with the increasing number of user requests that reached 4000, 11500,
    10000 requests/sec for CPU, memory and network. The result demonstrated that dynamic
    generated workloads have strong impact on the containers’ metrics as the monitored
    containers were unable to process more than those requests. At that point the
    containers become overloaded and the monitored metrics are severely affected.
    Fig. 4. Workload Contention - Metrics and Emulated User Requests. Show All TABLE
    IV. Network Congestion Detection and Localization using HHMM D. Results and Discussion
    The accuracy of results is compared with Dynamic Bayesian Network (DBN), and Hierarchical
    Temporal Memory (HTM). It is further evaluated based on different metrics such
    as: Root Mean Square Error (RMSE), Number of Correctly Detected Anomaly (CDA),
    Number of Correctly Identified Anomaly (CIA), Number of Incorrectly Detected Anomaly
    (IDA), and Number of Incorrectly Identified Anomaly (IIA). As show in Table V,
    the DLA using HHMM achieved good performance with 87.73% and 75.03% for CDA and
    CIA respectively. Also, the HTM relatively performed better than DBN with 79.54%
    for CDA, and 57.81% for CIA, and it gave good results for both CDA and CIA. However,
    in the testing phase in Table VI, the results of the HHMM, DBN and HTM for CDA
    and CIA increased comparing to the training phase. This may return to the configurations
    of the models. TABLE V. Results Summary for Training Phase TABLE VI. Results Summary
    for Testing Phase We compare the efficiency of each algorithm in identifying correctly
    the root-cause with the number of samples (the number of anomalies in a dataset).
    Fig. 5 represents the accuracy of each algorithm in the anomaly identification.
    We define the Accuracy of Root-Cause Identification (ARCI) to be the number of
    anomalies successfully localized out of the total number of anomalies presented
    to the system to be localized. The higher the value, the more correct the identification
    made by the model. As presented by the figure, The root-cause is identified correctly
    by HHMM with an accuracy reached 97%. HHMM gives satisfying results on different
    samples. While the DBN achieved fair results with a moderate number of samples,
    its accuracy reduced once it reached 90 samples to be 93%. For the HTM, its accuracy
    fluctuated with the increasing number of samples to reach at 94% with 100 samples.
    It should be noted that HHMM achieved optimal results with 90 and 100 samples,
    while, DBN, and HTM achieved optimal results with (70-90), and (90, 100) samples
    respectively. As shown in the previous experiments, the workload can significantly
    change depending on various events in runtime environment. Thus, we model our
    system hierarchically to detect and locate anomalous behaviour across different
    system levels through utilizing HHMMs. The mechanism of HHMMs is accompanied with
    our algorithms to detect and identify anomalous behaviour. We conduct performance
    evaluation over small-scale computing clusters. We inject different anomalies,
    and we generate various workloads to measure the effects on the container-based
    microservice at different system settings. This allows us to find the relationships
    among system components at different levels. At the end, the results demonstrate
    that the DLA can detect and identify anomalous behaviour with high accuracy. Fig.
    5. The Accuracy of Anomaly Root-Cause Identification Show All SECTION VII. Conclusions
    and Future Work Monitoring and analyzing container-based microservice performance
    creates challenges in the area of resource performance management at run-time
    [26], [27]. The complexity of the distribution may for instance lead to performance
    variations (i.e., anomalous behaviour) observed at the response time metric. To
    mitigate and reduce the occurrence of anomalies, this paper proposes a Detection
    and Localization system for Anomalies (DLA) that analyzes (i.e., detects and identifies
    anomalies) based on the monitored performance of container-based microservices
    using Hierarchical HMM and Correlation Analysis. To assess the DLA performance,
    different anomalies and workload patterns are generated at different points in
    time to gather performance data in various conditions. The DLA is compared with
    different machine learning algorithms. The results demonstrate the ability of
    the DLA to detect and localize the anomalous behaviours accurately. In the future,
    more anomaly scenarios, metrics, and more detailed experiments will be conducted
    to fully confirm these conclusions. Further, we will provide a self-healing mechanism
    [16], [18], [19], [21] to recover the localized anomaly. Authors Figures References
    Citations Keywords Metrics Footnotes IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'DLA: Detecting and Localizing Anomalies in Containerized Microservice Architectures
    Using Markov Models'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1093/bioinformatics/btz160
  analysis: '>'
  authors:
  - Payam Emami Khoonsari
  - Pablo Moreno-Ger
  - Sven Bergmann
  - Joachim Burman
  - Marco Capuccini
  - Matteo Carone
  - Marta Cascante
  - Pedro de Atauri
  - Carles Foguet
  - Alejandra N. González-Beltrán
  - Thomas Hankemeier
  - Kenneth Haug
  - Sijin He
  - Stephanie Herman
  - David Johnson
  - Namrata Kale
  - Anders Larsson
  - Steffen Neumann
  - Kristian Peters
  - Luca Pireddu
  - Philippe Rocca-Serra
  - Pierrick Roger
  - Rico Rueedi
  - Christoph Ruttkies
  - Noureddin Sadawi
  - Reza M. Salek
  - Susanna‐Assunta Sansone
  - Daniel Schober
  - Vitaly A. Selivanov
  - Etienne Thévenot
  - Michael Van Vliet
  - Gianluigi Zanetti
  - Christoph Steinbeck
  - Kim Kultima
  - Ola Spjuth
  citation_count: 25
  full_citation: '>'
  full_text: '>

    Advertisement Journals Books Issues Advance articles Submit Alerts About Bioinformatics
    This issue Bioinformatics Journals                      Bioinformatics and Computational
    Biology Books Journals Oxford Academic                                   Advanced
    Search Volume 35 Issue 19 October 2019 Article Contents Abstract 1 Introduction
    2 Materials and methods 3 Results 4 Discussion Author contributions Funding References
    Supplementary data < Previous Next > JOURNAL ARTICLE Interoperable and scalable
    data analysis with microservices: applications in metabolomics Payam Emami Khoonsari,
    Pablo Moreno, Sven Bergmann, Joachim Burman, Marco Capuccini, Matteo Carone, Marta
    Cascante, Pedro de Atauri, Carles Foguet, Alejandra N Gonzalez-Beltran ... Show
    more Bioinformatics, Volume 35, Issue 19, October 2019, Pages 3752–3760, https://doi.org/10.1093/bioinformatics/btz160
    Published: 09 March 2019 Article history PDF Split View Cite Permissions Share
    Abstract Motivation Developing a robust and performant data analysis workflow
    that integrates all necessary components whilst still being able to scale over
    multiple compute nodes is a challenging task. We introduce a generic method based
    on the microservice architecture, where software tools are encapsulated as Docker
    containers that can be connected into scientific workflows and executed using
    the Kubernetes container orchestrator. Results We developed a Virtual Research
    Environment (VRE) which facilitates rapid integration of new tools and developing
    scalable and interoperable workflows for performing metabolomics data analysis.
    The environment can be launched on-demand on cloud resources and desktop computers.
    IT-expertise requirements on the user side are kept to a minimum, and workflows
    can be re-used effortlessly by any novice user. We validate our method in the
    field of metabolomics on two mass spectrometry, one nuclear magnetic resonance
    spectroscopy and one fluxomics study. We showed that the method scales dynamically
    with increasing availability of computational resources. We demonstrated that
    the method facilitates interoperability using integration of the major software
    suites resulting in a turn-key workflow encompassing all steps for mass-spectrometry-based
    metabolomics including preprocessing, statistics and identification. Microservices
    is a generic methodology that can serve any scientific discipline and opens up
    for new types of large-scale integrative science. Availability and implementation
    The PhenoMeNal consortium maintains a web portal (https://portal.phenomenal-h2020.eu)
    providing a GUI for launching the Virtual Research Environment. The GitHub repository
    https://github.com/phnmnl/ hosts the source code of all projects. Supplementary
    information Supplementary data are available at Bioinformatics online. Issue Section:
    Systems biology Associate Editor: Jonathan Wren  1 Introduction Biology is becoming
    data-intensive as high throughput experiments in genomics or metabolomics are
    rapidly generating datasets of massive volume and complexity (Marx, 2013; Schadt
    et al., 2010), posing a fundamental challenge on large scale data analytics. Currently,
    the most common large-scale computational infrastructures in science are shared
    High-Performance Computing (HPC) systems. Such systems are usually designed primarily
    to support computationally intensive batch jobs—e.g. for the simulation of physical
    processes—and are managed by specialized system administrators. This model leads
    to rigid constraints on the way these resources can be used. For instance, the
    installation of software must undergo approval and may be restricted, which contrasts
    with the needs in the analysis where a multitude of software components of various
    versions—and their dependencies—are needed, and where these need to be continuously
    updated. Cloud computing offers a compelling alternative to shared HPC systems,
    with the possibility to instantiate and configure on-demand resources such as
    virtual computers, networks and storage, together with operating systems and software
    tools. Users only pay for the time the virtual resources are used, and when they
    are no longer needed they can be released and incur no further costs for usage
    or ownership. For scientists, this constitutes a shift from owning computer hardware,
    to starting up Infrastructure-as-a-Service (IaaS) nodes with virtual machines
    on cloud resources, with the explicit need to then install all necessary software
    for the analysis which in many cases constitutes a demanding and time-consuming
    task (Langmead and Nellore, 2018). Along with infrastructure provisioning, software
    provisioning—i.e. installing and configuring software for users—has also advanced.
    Consider, for instance, containerization (Silver, 2017), which allows entire applications
    with their dependencies to be packaged, shipped and run on a computer but isolated
    from one another in a way analogous to virtual machines, yet much more efficiently.
    Containers are more compact, and since they share the same operating system kernel,
    they are fast to start and stop and incur little overhead in execution. These
    traits make them an ideal solution to implement lightweight microservices, a software
    engineering methodology in which complex applications are divided into a collection
    of smaller, loosely coupled components that communicate over a network (Newman,
    2015). Microservices share many properties with traditional always-on web services
    found on the Internet, but microservices are generally smaller, portable and can
    be started on-demand within a separate computing environment. Another important
    feature of microservices is that they have a technology-agnostic communication
    protocol, and hence can serve as building blocks that can be combined and reused
    in multiple ways (da Veiga Leprevost et al., 2017). Microservices are highly suitable
    to run in elastic cloud environments that can dynamically grow or shrink on demand,
    enabling applications to be scaled-up by simply starting multiple parallel instances
    of the same service. However, to achieve effective scalability a system needs
    to be appropriately sectioned into microservice components and the data to be
    exchanged between the microservices needs to be defined for maximum efficiency—both
    being challenging tasks. One of the omics fields that faces challenges by data
    growth is metabolomics which measures the occurrence, concentrations and changes
    of small molecules (metabolites) in organisms, organs, tissues, cells and cellular
    compartments. Metabolite abundances are assayed in the context of environmental
    or dietary changes, disease or other conditions (Nicholson and Wilson, 2003).
    Metabolomics is, as most other omics technologies, characterized by the use of
    high-throughput experiments performed using a variety of spectroscopic methods
    such as Mass Spectrometry (MS) and Nuclear Magnetic Resonance (NMR) that produce
    large amounts of data (Montenegro-Burke et al., 2017). With increasing data size
    and number of samples, the analysis process becomes intractable for desktop computers
    due to requirements on compute cores, memory, storage, etc. As a result, large-scale
    computing infrastructures have become important components in scientific projects
    (Liew et al., 2016). Moreover, making use of such complex computing resources
    in an analysis workflow presents its own challenges, including achieving efficient
    job parallelism and scheduling as well as error handling (Suplatov et al., 2016).
    In addition, configuring the necessary software tools and chaining them together
    into a complete re-runnable analysis workflow commonly requires substantial IT-expertise,
    while creating portable and fault-tolerant workflows with a robust audit trail
    is even more difficult. Metabolomics has already benefited from cloud-based systems
    enabling the users certain preprocessing and main downstream analysis on e.g.
    MS data. Examples of such systems are XCMS ONLINE (Warth et al., 2017), MetaboAnalyst
    (Xia et al., 2012), Chorus (chorusproject.org) and The Metabolomics Workbench
    (Sud et al., 2016) (www.metabolomicsworkbench.org) which provide tools that scale
    with computational demands. In this manuscript, we present a method that uses
    components for data analysis encapsulated as microservices and connected into
    computational workflows to provide complete, ready-to-run, reproducible data analysis
    solutions that can be easily deployed on desktop computers as well as public and
    private clouds. Our work contrasts to previously reported research environments,
    sometimes termed Virtual Research Environments (Allan, 2009; Candela et al., 2013),
    Scientific Gateways (Lawrence et al., 2015) and Virtual Labs (Waldrop, 2013),
    in that it encompasses the complete setup of the computational infrastructure
    and frameworks to run analysis in a wide range of environments; however our approach
    requires virtually no involvement in the setup and no special IT skills from the
    user. The methodology provides a framework for rapid and efficient integration
    of new tools and developing scalable, and interoperable workflows, supporting
    multiple workflow engines such as Galaxy (Goecks et al., 2010) and Luigi (https://github.com/spotify/luigi).
    We validate the method on four metabolomics studies and show that it enables scalable
    and interoperable data analysis. 2 Materials and methods 2.1 Microservices A detailed
    description of the methods is present in Supplementary Method S1. Briefly, in
    order to construct a microservice architecture for metabolomics we used Docker
    (Merkel, 2014) (https://www.docker.com/) containers to encapsulate software tools.
    Tools are developed as open source and are available in a public repository such
    as GitHub (https://github.com/), and the PhenoMeNal project containers are built
    and tested on a Jenkins continuous integration (CI) server (http://phenomenal-h2020.eu/jenkins/).
    Containers are assembled in different branches using the git versioning system.
    Builds originating from the development branch of each container repository give
    rise to container images tagged as ‘development’; builds coming from the master
    branches result in release images. In order for a container be pushed to the container
    registry, it must pass a testing criteria which is defined by the developer of
    the tool. All published containers are thus available for download and can be
    used in any microservice architecture. Data is exchanged between services by passing
    references to a shared local file system. The CI system constitutes a key part
    of the methodology, as it ensures that containers are continuously successfully
    packaged, versioned, tested and that adequate reporting measures are in place
    to handle any errors in this process over time. 2.2 Virtual Research Environment
    (VRE) We developed a Virtual Research Environment (VRE) which uses Kubernetes
    (https://kubernetes.io/) for orchestration of the containers, including initialization
    and scaling of jobs based on containers, abstractions to file system access for
    running containers, exposure of services, as well as rescheduling of failed jobs
    and long running services. Kubernetes was chosen over other frameworks such as
    Docker Swarm because of its larger momentum and that it is more widely used in
    production environments. Docker also provides Kubernetes as part of their Enterprise
    solutions (and even now the community ones). To enable convenient instantiation
    of a complete virtual infrastructure, we developed KubeNow (https://github.com/kubenow/KubeNow)
    (Capuccini et al., 2018) which includes instantiation of compute nodes, shared
    file system storage, networks, configure DNS, operating system, container implementation
    and orchestration tools, including Kubernetes, on a local computer or server.
    In order to deploy applications, we used two main classes of services: long-lasting
    services, and compute jobs. Long-lasting services were used for applications such
    as the user interface whereas compute jobs were used to perform temporary functions
    in data processing. The VRE includes Galaxy, Luigi workflow engine and Jupyter
    notebook as user-facing services. In the PhenoMeNal CI system, the VRE is instantiated
    and tested on all supported cloud providers nightly in order to ensure a working
    system over time. 2.3 Demonstrators We validated our method in the field of metabolomics
    using four demonstrators. Demonstrators 1 and 2 showcase scalability and interoperability
    of our microservice-based architecture whereas Demonstrators 3 and 4 exemplify
    flexibility to account for new application domains, showing the architecture is
    domain-agnostic. Demonstrator 1: Scalability of microservices in a cloud environment.The
    objective of this analysis was to demonstrate the computational scalability of
    an existing workflow on a large dataset [Metabolomics data have been deposited
    to the EMBL-EBI MetaboLights database (Haug et al., 2013) with the identifier
    MTBLS233 (Ranninger et al., 2016). The complete dataset can be accessed here https://www.ebi.ac.uk/metabolights/MTBLS233].
    The experiment includes 528 mass spectrometry samples from whole cell lysates
    of human renal proximal tubule cells that were pre-processed through a five-step
    workflow (consisting of peak picking, feature finding, linking, file filtering
    and exporting) using the OpenMS software (Sturm et al., 2008). This preprocessing
    workflow was reimplemented using Docker containers and run using the Luigi workflow
    engine. Scalability of concurrent running tools (on 40 Luigi workers, each worker
    receives tasks from the scheduler and executes them) was measured using weak scaling
    efficiency (WSE), where the workload assigned to each worker stays constant and
    additional workers are used to solve a larger total problem. Demonstrator 2: Interoperability
    of microservices.The objective of this analysis was to demonstrate interoperability
    as well as to present a real-world scenario in which patients’ data are processed
    using a microservices-based platform. We used a dataset consisting of 37 clinical
    cerebrospinal fluid (CSF) samples including thirteen relapsing-remitting multiple
    sclerosis (RRMS) patients and 14 secondary progressive multiple sclerosis (SPMS)
    patients as well as 10 non-multiple sclerosis controls. 26 quality controls (19
    blank and 7 dilution series samples) were also added to the experiment. In addition,
    8 pooled CSF samples containing MS/MS data were included in the experiment for
    improving identification [Metabolomics data have been deposited to the EMBL-EBI
    MetaboLights database with the identifier MTBLS558. The complete dataset can be
    accessed here https://www.ebi.ac.uk/metabolights/MTBLS558]. The samples were processed
    and analyzed on the Galaxy platform (Goecks et al., 2010), running in a VRE behind
    the Uppsala University Hospital firewall to be compliant with local ELSI (Ethics,
    Legal, Social implications) regulations. Demonstrator 3: 1 D NMR-analysis workflow.The
    purpose of this demonstrator was to highlight the fact that the microservice architecture
    is indeed domain-agnostic and is not limited to a particular assay technology.
    This NMR-based metabolomics study was originally performed by Salek et al. (2007)
    on urine of type 2 diabetes mellitus (T2DM) patients and controls [Metabolomics
    data have been deposited to the EMBL-EBI MetaboLights database with the identifier
    MTBLS1. The complete dataset can be accessed here https://www.ebi.ac.uk/metabolights/MTBLS1].
    In total, 132 samples (48 T2DM and 84 controls) were processed using a Galaxy
    workflow performing conversion, preprocessing, multivariate data analysis and
    result visualization. Demonstrator 4: Start-to-end fluxomics workflow.The purpose
    of this demonstrator was to show the integrated use of separately developed tools
    covering subsequent steps of the study of metabolic fluxes based on 13C stable
    isotope-resolved metabolomics (SIRM) (Buescher et al., 2015; King et al., 2015;
    Niedenführ et al., 2015). Here we implemented the analysis of flux distributions
    in HUVEC cells under hypoxia [Metabolomics data have been deposited to the EMBL-EBI
    MetaboLights database with the identifier MTBLS412. The complete dataset can be
    accessed here https://www.ebi.ac.uk/metabolights/MTBLS412], from raw mass spectra
    contained in netCDF files, using a workflow implemented in Galaxy including reading
    and extraction of the data, correcting the evaluated mass spectra for natural
    isotopes and computing steady-state distribution of 13C label as function of steady-state
    flux distribution. 2.4 Availability and implementation The PhenoMeNal consortium
    maintains a web portal (https://portal.phenomenal-h2020.eu) providing a GUI for
    launching VREs using KubeNow (Capuccini et al., 2018) on a selection of the largest
    public cloud providers, including Amazon Web Services, Microsoft Azure and Google
    Cloud Platform, or on private OpenStack-based installations. The Wiki containing
    documentation is also hosted on GitHub https://github.com/phnmnl/phenomenal-h2020/wiki.
    The PhenoMeNal Portal can be reached at https://portal.phenomenal-h2020.eu. The
    public instance of Galaxy is accessible at https://public.phenomenal-h2020.eu.
    The containers provisioned by PhenoMeNal comprise tools built as open source software
    that are available in a public repository such as GitHub, and are subject to continuous
    integration testing. The containers that satisfy testing criteria are pushed to
    a public container repository, and containers that are included in stable VRE
    releases are also pushed to Biocontainers (da Veiga Leprevost et al., 2017). The
    GitHub repository https://github.com/phnmnl/hosts the source code of all development
    projects. Source code and documentation are available under the terms of the Apache
    2.0 license. Integrated open source projects are available under the respective
    licensing terms. The Demonstrators can be obtained from: Demonstrator 1: https://github.com/phnmnl/MTBLS233-Jupyter;
    Demonstrator 2: https://public.phenomenal-h2020.eu/u/phenoadmin/w/metabolomics-lcmsms-processing-quantification-annotation-identification-and-statistics-1;
    Demonstrator 3: https://public.phenomenal-h2020.eu/u/phenoadmin/w/metabolomics-nmr-rnmr1d-metabolights-data-processing-and-plot;
    Demonstrator 4: https://public.phenomenal-h2020.eu/u/phenoadmin/w/fluxomics-stationary-13c-ms-iso2flux-with-visualization
    3 Results We developed a VRE based on a microservices architecture encapsulating
    a large suite of software tools for performing metabolomics data analysis (see
    Supplementary Table S1). Scientists can interact with the microservices programmatically
    via an Application Programming Interface (API) or via a web-based graphical user
    interface (GUI), as illustrated in Figure 1. To connect microservices into computational
    workflows, the two frameworks Galaxy (Goecks et al., 2010) and Luigi (https://github.com/spotify/luigi)
    were adapted to execute jobs on Kubernetes. Galaxy is a web-based interface for
    individual tools and allows users to share workflows, analysis histories and result
    datasets. Luigi on the other hand focuses on scheduled execution, monitoring,
    visualization and the implicit dependency resolution of tasks (Leipzig, 2017).
    These basic infrastructure services, together with the Jupyter notebook (Kluyver
    et al., 2016) interactive programming environment, are deployed as long-running
    services in the VRE, whereas the other analysis tools are deployed as transient
    compute jobs to be used on-demand. System and client applications were developed
    for launching the VRE on desktop computers, public and private cloud providers,
    automating all steps required to instantiate the virtual infrastructures. Fig.
    1. Open in new tabDownload slide Overview of the components in a microservices-based
    framework. Complex applications are divided into smaller, focused and well-defined
    (micro-) services. These services are independently deployable and can communicate
    with each other, which allows to couple them into complex task pipelines, i.e.
    data processing workflows. The user can interact with the framework programmatically
    via an Application Program Interface (API) or via a graphical user interface (GUI)
    to construct or run workflows of different services, which are executed independently.
    Multiple instances of services can be launched to execute tasks in parallel, which
    effectively can be used to scale analysis over multiple compute nodes. When run
    in an elastic cloud environment, virtual resources can be added or removed depending
    on the computational requirements Demonstrator 1: Scalability of microservices
    in a cloud environment. The Diagram of scalability-testing on the metabolomics
    dataset is illustrated in Figure 2. The analysis resulted to WSE of 88% with an
    execution time of approximately four hours (online methods, Supplementary Fig.
    S2), compared with the ideal case of 100% where linear scaling is achieved if
    the run time stays constant while the workload is increased. In addition, the
    final result of the workflow (online methods, Supplementary Fig. S3) was identical
    to that presented by the original MTBLS233 study (Ranninger et al., 2016) in negative
    ionization mode. However, in the positive ionization mode, one m/z feature was
    found in a different group (m/z range 400–1000) than it was originally reported
    by Ranninger et al. (m/z range 200–400). Fig. 2. Open in new tabDownload slide
    Diagram of scalability-testing on a metabolomics dataset (MetaboLights ID: MTBLS233)
    in Demonstrator 1 to illustrate the scalability of a microservice approach. A)
    The preprocessing workflow is composed of 5 OpenMS tasks that were run in parallel
    over the 12 groups in the dataset using the Luigi workflow system. The first two
    tasks, peak picking (528 tasks) and feature finding (528 tasks), are trivially
    parallelizable, hence they were run concurrently for each sample. The subsequent
    feature linking task needs to process all of the samples in a group at the same
    time, therefore 12 of these tasks were run in parallel. In order to maximize the
    parallelism, each feature linker container (microservice) was run on 2 CPUs. Feature
    linking produces a single file for each group, that can be processed independently
    by the last two tasks: file filter (12 tasks) and text exporter (12 tasks), resulting
    in total of 1092 tasks. The downstream analysis consisted of 6 tasks that were
    carried out in a Jupyter Notebook. Briefly, the output of preprocessing steps
    was imported into R and the unstable signals were filtered out. The missing values
    were imputed and the resulting number of features were plotted. B) The weak scaling
    efficiency plot for Demonstrator 1. Given the full MTBLS233 dataset, the preprocessing
    was run on 40 Luigi workers. Then for 1/4, 2/4, 3/4 of MTBLS233, the analysis
    was run again on 10, 20 and 30 workers respectively. For each run, we measured
    the processing time T10, T20, T30 and T40, and we computed the WSEn = T10/Tn for
    n = 10, 20, 30, 40. The WSE plot shows scalability up to 40 CPUs, where we achieved
    ∼88% scaling efficiency. The running time for the full dataset (a total of 1092
    tasks) on 40 workers was ∼4 hours Demonstrator 2: Interoperability of microservices.
    We developed a start to end workflow for pre-processing and statistical analysis
    of LC-MS metabolomics data (Fig. 3). The workflow allows seamless integration
    of six major metabolomics data analysis components (26 steps) each was already
    implemented in independent software suites: noise reduction and filtering [OpenMS
    (Rost et al., 2016)], quantification, alignment and matching [XCMS (Smith et al.,
    2006)], filtering of biological non-relevant signals (R), annotation of signals
    [CAMERA (Kuhl et al., 2012)], identification [MetFrag (Wolf et al., 2010)], statistics
    [Workflow4Metabolomics (Giacomoni et al., 2015)]. The result of the workflow (multivariate
    analysis) showed a clear difference in the metabolic constitution between the
    three disease groups of RRMS, SPMS and non-multiple sclerosis controls (Fig. 4A).
    In addition, the univariate analysis resulted in a total of three metabolites
    being significantly altered (p < 0.05) between multiple sclerosis subtypes and
    control samples, namely alanyltryptophan and indoleacetic acid with higher and
    linoleoyl ethanolamide with lower abundance in both RRMS and SPMS compared to
    controls (Fig. 4B). Fig. 3. Open in new tabDownload slide Overview of the workflow
    used to process multiple-sclerosis samples in Demonstrator 2, where a workflow
    was composed of the microservices using the Galaxy system. The data was centroided
    and limited to a specific mass over charge (m/z) range using OpenMS tools. The
    mass traces quantification and retention time correction was done via XCMS (Smith
    et al., 2006). Unstable signals were filtered out based on the blank and dilution
    series samples using an in-house function (implemented in R). Annotation of the
    peaks was performed using CAMERA (Kuhl et al., 2012). To perform the metabolite
    identification, the tandem spectra from the MS/MS samples in mzML format were
    extracted using MSnbase and passed to MetFrag. The MetFrag scores were converted
    to q-values using Passatutto software. The result of identification and quantification
    were used in ‘Multivariate’ and ‘Univariate’ containers from Workflow4Metabolomics
    (Giacomoni et al., 2015) to perform Partial Least Squares Discriminant Analysis
    (PLS-DA) Fig. 4. Open in new tabDownload slide The results from analysis of multiple
    sclerosis data in Demonstrator 2, presenting new scientifically useful biomedical
    knowledge. A) The PLS-DA results suggest that the metabolite distribution in the
    RRMS and SPMS samples are different to controls. B) Three metabolites were identified
    as differentially regulated between multiple sclerosis subtypes and control samples,
    namely Alanyltryptophan and Indoleacetic acid with higher and Linoleoyl ethanolamide
    with lower abundance in both RRMS and SPMS compared to controls. Abbr., RRMS:
    relapsing-remitting multiple sclerosis, SPMS: secondary progressive multiple sclerosis
    Demonstrators 3 and 4: Domain agnosticity (NMR and fluxomics workflows). We developed
    a workflow for analysis of 1 D NMR data. The workflow consisted of automatic downloading
    NMR vendor data (and metadata) from MetaboLights database followed by format standardization,
    spectral processing and statistical analysis. We processed a NMR dataset (demonstrator
    3) resulting to quantification of a total of 726 features which were used to perform
    Orthogonal Projections to Latent Structures Discriminant Analysis (OPLS-DA). This
    resulted in a clear separation between T2DM and controls (Fig. 5), similar to
    that of previous findings (Salek et al., 2007). Lastly, we designed a workflow
    for analyzing metabolite metabolic fluxes. The workflow integrated four main steps
    including data extraction, data correction, calculation of flux distribution and
    visualization. Using this workflow (Fig. 6), we achieved detailed description
    of the magnitudes of the fluxes through the reactions accounting for glycolysis
    and pentose phosphate pathway. Fig. 5. Open in new tabDownload slide Overview
    of the NMR workflow in Demonstrator 3. The raw NMR data and experimental metadata
    (ISATab) was automatically imported from the Metabolights database and converted
    to open source nmrML format. The preprocessing was performed using the rnmr1d
    package part of nmrprocflow tools. All study factors were imported from MetaboLights
    and were fed to the multivariate node to perform an OPLS-DA Fig. 6. Open in new
    tabDownload slide Overview of the workflow for fluxomics, with Ramid, Midcor,
    Iso2Flux and Escher-fluxomics tools supporting subsequent steps of the analysis.
    The example refers to HUVEC cells incubated in the presence of [1,2-13C2]glucose
    and label (13C) propagation to glycogen, RNA ribose and lactate measured by mass
    spectrometry. Ramid reads the raw netCDF files, corrects baseline and extracts
    the peak intensities. The resulting peak intensities are corrected (natural abundance,
    overlapping peaks) by Midcor, which provides isotopologue abundances. Isotopologue
    abundances, together with a model description (SBML model, tracing data, constraints),
    are used by Iso2Flux to provide flux distributions through glycolysis and pentose-phosphate
    pathways, which are shown as numerical values associated to a metabolic scheme
    of the model by the Escher-fluxomics tool 4 Discussion Implementing the different
    tools and processing steps of a data analysis workflow as separate services that
    are made available over a network was in the spotlight in the early 2000s (Foster,
    2005) as service-oriented architectures (SOA) in science. At that time, web services
    were commonly deployed on physical hardware and exposed and consumed publicly
    over the internet. However, it soon became evident that this architecture did
    not fulfill its promises as it was hard to scale from a computational and maintainability
    perspective. In addition, the web services were not portable and mirroring them
    was complicated (if at all possible). Furthermore, API changes and frequent services
    outage made it frustrating to connect them into functioning computational workflows.
    Ultimately, the ability to replicate an analysis on local and remote hardware
    (such as a computer cluster) was very difficult due to heterogeneity in the computing
    environments. At first sight microservices might seem similar to above mentioned
    SOA web services, but microservices can with great benefit be executed in virtual
    environments (abstracting over OS and hardware architectures) in such a way that
    they are only instantiated and executed on-demand, and then terminated when they
    are no longer needed. This makes such virtual environments inherently portable
    and they can be launched on demand on different platforms (e.g. a laptop, a powerful
    physical server or an elastic cloud environment). A key aspect is that workflows
    of microservices are still executed identically, agnostic of the underlying hardware
    platform. Container-based microservices provide a wide flexibility in terms of
    versioning, allowing the execution of newer and older versions of each container
    as needed for reproducibility. Since all software dependencies are encompassed
    within the container, which is versioned, the risk of workflow failure due to
    API changes is minimized. An orchestration framework such as Kubernetes further
    allows for managing errors in execution and transparently handles the restarting
    of services. Hence, technology has caught up with service-oriented science, and
    microservices have taken the methodology to the next level, alleviating many of
    the previous problems related to scalability, portability and interoperability
    of software tools. This is advantageous in the context of omics analysis, which
    produces multidimensional datasets reaching beyond gigabytes, on into terabytes,
    leading to ever-increasing demand on processing performance (Marx, 2013; Schadt
    et al., 2010). However, containerization does not address how services communicate
    with each other, but this has to be implemented inside the container itself. Traditional
    web services addressed this by standardizing the messaging protocol and public-facing
    interfaces (e.g. SOAP and WSDL) (Stockinger et al., 2008), while in a containerized
    environment Representational State Transfer (REST) (Fielding, 2000) or passing
    files by reference to a shared file system is more common. In Demonstrator 1,
    we showed that microservices enable highly efficient and scalable data analyses
    by executing individual modules in parallel, and that they effectively harmonize
    with on-demand elasticity of the cloud computing paradigm. The reached scaling
    efficiency of ∼88% indicates remarkable performance achieved on generic cloud
    providers. Furthermore, although our results in positive ionization model was
    slightly different to that of Ranninger et al. (2016), the results of our analysis
    were replicable regardless of the platform used to perform the computations. In
    addition to the fundamental demand for high performance, the increased throughput
    and complexity of omics experiments has led to a large number of sophisticated
    computational tools (Berger et al., 2013), which in turn necessitates integrative
    workflow engines (Atkinson et al., 2017; Di Tommaso et al., 2017; Liew et al.,
    2016). In order to integrate new tools in such workflow engines, compatibility
    of the target environment, tools and APIs needs to be considered (Di Tommaso et
    al., 2017). Containerization facilitates this by providing a platform-independent
    virtual environment for developing and running the individual tools. However,
    the problem of compatibility between tools/APIs and data formats remains and needs
    to be tackled by international consortia (Wilkinson et al., 2016). Our methodology
    the currently non-trivial task of instantiating the complete microservice environment
    through a web portal that allows for convenient deployment of the VRE on public
    cloud providers. Moreover, using this web portal, microservices and VREs can be
    deployed on a trusted private cloud instance or a local physical server on an
    internal network, such as within a hospital network, allowing for levels of isolation
    and avoiding transfer of data across untrusted networks which often are requirements
    in the analysis of sensitive data. This was exemplified in Demonstrator 2, where
    a complete start-to-end workflow was run on the Galaxy platform on a secure server
    at Uppsala University Hospital, Sweden, leading to the identification of novel
    disease fingerprints in the CSF metabolome of RRMS and SPMS patients. It is worth
    mentioning that the selected metabolites were part of the tryptophan metabolism
    (alanyltryptophan and indoleacetic acid) and endocannabinoids (linoleoyl ethanolamide),
    both of which have been previously implicated in multiple sclerosis (Amirkhani
    et al., 2005; Baker and Pryce, 2008; Centonze et al., 2007; Lim et al., 2017;
    Lovelace et al., 2016; Zamberletti et al., 2012). However, since the cross-validated
    predictive performance (Q2Y = 0.286) is not much higher than some of the models
    generated after random permutation of the response (Fig. 4A), the quality of the
    model needs to be confirmed in a future study on an independent cohort of larger
    size. The microservice architecture is domain-agnostic and not limited to a particular
    assay technology, i.e. mass spectrometry. This was showcased in Demonstrator 3
    and 4, where an automated 1 D NMR workflow and calculation of flux distributions
    (derived from the application of stable isotope resolved metabolomics) were performed.
    In Demonstrator 3, we showed that the pattern of the metabolite expression is
    different between type 2 diabetic and healthy controls, and that a large number
    of metabolites contribute to such separation. In Demonstrator 4, we showed a high
    rate of glycolysis in cells cultured in hypoxia, which is consistent with the
    one expected for endothelial cells (Iyer et al., 1998) and with how these cells
    maintain energy in low oxygen environments and without oxidative phosphorylation
    (Eelen et al., 2015; Polet and Feron, 2013). These two examples further show that
    complex workflows can be applied with minimal effort on other studies (i.e. simply
    by providing a MetaboLights accession number), leading to the capability to re-analyze
    data and compare the results with the original publication findings. Furthermore,
    it demonstrates the value of standardised dataset descriptions such as nmrML (Schober
    et al., 2017) and ISA format (Rocca-Serra et al., 2016; Sansone et al., 2012)
    for representing NMR based studies, as well as the potential of the VRE to foster
    reproducibility. Furthermore, the data processing steps are trackable and replicable
    as each container/tool is versioned for a specific release and data processing
    steps and the corresponding parameters are taken care of by the workflow engine.
    In addition, the cli KubeNow is using speciffic pinned versions of all dependant
    software and all versions of software is stored in the user config dir created
    by the init-command. The specific version of KubeNow used is saved in user config
    directory. While microservices are not confined to metabolomics and generally
    applicable to a large variety of applications, there are some important implications
    and limitations of the method. Firstly, tools need to be containerized in order
    to operate in the environment. This is however not particularly complex, and an
    increasing number of developers provide containerized versions of their tools
    on public container repositories such as Dockerhub or Biocontainers (da Veiga
    Leprevost et al., 2017). Secondly, uploading data to a cloud-based system can
    take a considerable amount of time, and having to re-do this every time a VRE
    is instantiated can be time-consuming. This can be alleviated by using persistent
    storage on a cloud resource, but the availability of such storage varies between
    different cloud providers. Further, the storage system can become a bottleneck
    when many services try to access a shared storage. We observe that using a distributed
    storage system with multiple storage nodes can drastically increase performance,
    and the PhenoMeNal VRE comes with a distributed storage system by default. When
    using a workflow system to orchestrate the microservices, stability and scalability
    are inherently dependent on the workflow system’s job runner. Workflow execution
    is dependent on the underlying workflow engine, and we observed that a large number
    of outputs can make the Galaxy engine unresponsive, whereas the Luigi engine did
    not have these shortcomings. With clouds and microservices maturing, workflow
    systems will need to evolve and further embrace the new possibilities of these
    infrastructures. It is important to note that microservices do not overcome the
    incompatibility between tools with respect to using different data formats, and
    code resolving such incompatibility is still needed. However, using a shared platform
    makes such bridging components easier to maintain and makes them reusable. There
    remains great challenges in establishing interoperable and agreed-upon standards
    and data formats that are widely accepted and implemented by tools, as well as
    achieving complete support for the FAIR principles (Wilkinson et al., 2016). Further,
    not all research can be easily pipelined, for example exploratory research might
    be better carried out in an ad-hoc manner than with workflows and the overhead
    this implies. A Jupyter Notebook as used in in Demonstrator 1 or embedded in Galaxy
    (Grüning et al., 2017) constitutes a promising way to make use of microservices
    for interactive analysis. The serverless architecture, also called Functions as
    a Service (FaaS) architecture, is an interesting methodology when deployed with
    microservices as it allow developers to execute code in response to events without
    managing the underlying infrastructure. While serverless technologies have irrupted
    strongly in areas of software engineering closer to web development, this doesn’t
    mean that their usage can be easily transferred to scientific workloads. This
    is due to the far more complex network of dependencies that scientific software
    will have compared to web applications, where large applications can be managed
    for instance through npm package resolutions only. On scientific software solutions
    one will commonly find dependencies in different programming languages, different
    underlying libraries and even sometimes on different incompatible versions of
    the same frameworks. This level of complexity is not resolvable today through
    server less approaches and requires more isolated approaches based on containers,
    such as the one presented here. In summary, we showed that microservices allow
    for efficient horizontal scaling of analyses on multiple computational nodes,
    enabling the processing of large datasets. By applying a number of data [mzML
    (Martens et al., 2011), nmrML] and metadata standards [ISA serializations for
    study descriptions (Rocca-Serra et al., 2016; Sansone et al., 2012)], we also
    demonstrated a high level of interoperability in the context of metabolomics,
    by providing completely automated start-to-end analysis workflows for mass spectrometry
    and NMR data. In addition, many of the state-of-the-art tools such as components
    of XCMS ONLINE (Warth et al., 2017) and MetaboAnalyst (Xia et al., 2012) can be
    incorporated in the workflows, providing more refined workflows. The ability to
    instantiate VREs close to large datasets, such as on local servers within a hospital
    for Demonstrator 2, makes it possible to use the VRE on sensitive data that is
    not allowed to leave the current environment for ELSI reasons. While the current
    PhenoMeNal VRE implementation uses Docker for software containers and Kubernetes
    for container orchestration, the microservice methodology is general and not restricted
    to these frameworks. Likewise, the choice of Luigi and Galaxy was here used to
    demonstrate the capabilities of workflow management microservices in cloud environments.
    In fact, our microservice architecture supports other major workflow engines such
    as Nextflow (Di Tommaso et al., 2017) or Snakemake (Köster and Rahmann, 2012).
    Hence it is possible to use any of such workflow engines in our VRE and still
    produce reproducible results. In addition, despite some of our workflows were
    novel in the context of metabolomics (e.g. Demonstrator 2) and can be readily
    applied on other datasets, their main contribution in this work is to showcase
    scalability and interoperability of the microservices methodology. Finally, we
    emphasise that the presented methodology goes beyond metabolomics and can be applied
    to virtually any field, lowering the barriers for taking advantage of cloud infrastructures
    and opening up for large-scale integrative science. Author contributions KK, MAC,
    MC, PEK, SH contributed to Demonstrator 1. CR, KK, KP, PEK, SH, SN contributed
    to Demonstrator 2. KK designed the study in Demonstrator 2. JB performed collection
    of samples and characterization of the multiple sclerosis cohort. SH performed
    the mass spectrometry experiment in Demonstrator 2. DS, KP, PEK, PM, RMS, contributed
    to Demonstrator 3. AGB, CF, DJ, MCA, MVV, PDA, PM, PRS, SAS, TH and VS contributed
    to Demonstrator 4. GZ, LP, PEK and PM contributed to developments of Galaxy in
    Kubernetes. AL and MC contributed to the development of Luigi in Kubernetes. AL,
    MAC, MC and NS developed KubeNow. PM contributed to Galaxy-Kubernetes. EAT and
    PR contributed to containerizing of Workflow4Metabolomics tools. AGB, DJ, PRS
    and SAS contributed to ISA-API. DJ, EAT, KP, MVV, NS, OS, PEK, PM, PR, PRS, DS,
    RMS, RR and SB were involved in testing the containers and the VRE. PM, SIH and
    KH were involved in development and maintenance of the portal. MVV, PM and RMS
    contributed to the release. NK coordinated the PhenoMeNal project. CS conceived
    and managed the PhenoMeNal project. OS conceived and coordinated the study and
    e-infrastructure. All authors contributed to manuscript writing. Funding This
    research was supported by The European Commission’s Horizon 2020 programme funded
    under grant agreement number 654241 (PhenoMeNal), The Swedish Research Council
    FORMAS, Uppsala Berzelii Technology Centre for Neurodiagnostics, Åke Wiberg Foundation
    and the Nordic e-Infrastructure Collaboration (NeIC) via the Glenna2 and Tryggve2
    projects. We kindly acknowledge contributions by Daniel Jacob (INRA) and to cloud
    resources by SNIC Science Cloud, Embassy Cloud, c-Pouta and CityCloud. The funders
    had no role in study design, data collection and analysis, decision to publish,
    or preparation of the manuscript. Conflict of Interest: none declared. References
    Allan   R.N. (2009) Virtual Research Environments: From Portals to Science Gateways.
    ChandosŁ Publishing, Oxford, UK. Google Scholar Crossref Google Preview WorldCat
    COPAC  Amirkhani   A.  et al.  (2005) Interferon-beta affects the tryptophan metabolism
    in multiple sclerosis patients. Eur. J. Neurol., 12, 625–631. Google Scholar CrossrefPubMed
    WorldCat   Atkinson   M.  et al.  (2017) Scientific workflows: past, present and
    future. Future Gener. Comput. Syst., 75, 216–227. Google Scholar Crossref WorldCat   Baker   D.
    , Pryce G. (2008) The endocannabinoid system and multiple sclerosis. Curr. Pharm.
    Des., 14, 2326–2336. Google Scholar CrossrefPubMed WorldCat   Berger   B.  et
    al.  (2013) Computational solutions for omics data. Nat. Rev. Genet., 14, 333–346.
    Google Scholar CrossrefPubMed WorldCat   Buescher   J.M.  et al.  (2015) A roadmap
    for interpreting (13)C metabolite labeling patterns from cells. Curr. Opin. Biotechnol.,
    34, 189–201. Google Scholar CrossrefPubMed WorldCat   Candela   L.  et al.  (2013)
    Virtual research environments: an overview and a research agenda. Data Sci. J.,
    12, GRDI75–GRDI81. Google Scholar Crossref WorldCat   Capuccini   M.  et al.  (2018)
    On-Demand Virtual Research Environments using Microservices. arXiv [cs.DC]. Centonze   D.  et
    al.  (2007) The endocannabinoid system is dysregulated in multiple sclerosis and
    in experimental autoimmune encephalomyelitis. Brain, 130, 2543–2553. Google Scholar
    CrossrefPubMed WorldCat   da Veiga Leprevost   F.  et al.  (2017) BioContainers:
    an open-source and community-driven framework for software standardization. Bioinformatics,
    33, 2580–2582. Google Scholar CrossrefPubMed WorldCat   Di Tommaso   P.  et al.  (2017)
    Nextflow enables reproducible computational workflows. Nat. Biotechnol., 35, 316–319.
    Google Scholar CrossrefPubMed WorldCat   Eelen   G.  et al.  (2015) Endothelial
    cell metabolism in normal and diseased vasculature. Circ. Res., 116, 1231–1244.
    Google Scholar CrossrefPubMed WorldCat   Fielding   R.T. (2000) Architectural
    Styles and the Design of Network-Based Software Architectures. Irvine: University
    of California, Irvine. Google Scholar Google Preview WorldCat COPAC  Foster   I.
    (2005) Service-oriented science. Science, 308, 814–817. Google Scholar CrossrefPubMed
    WorldCat   Giacomoni   F.  et al.  (2015) Workflow4Metabolomics: a collaborative
    research infrastructure for computational metabolomics. Bioinformatics, 31, 1493–1495.
    Google Scholar CrossrefPubMed WorldCat   Goecks   J.  et al.  (2010) Galaxy: a
    comprehensive approach for supporting accessible, reproducible, and transparent
    computational research in the life sciences. Genome Biol., 11, R86. Google Scholar
    CrossrefPubMed WorldCat   Grüning   B.A.  et al.  (2017) Jupyter and Galaxy: easing
    entry barriers into complex data analyses for biomedical researchers. PLoS Comput.
    Biol., 13, e1005425. Google Scholar CrossrefPubMed WorldCat   Haug   K.  et al.  (2013)
    MetaboLights–an open-access general-purpose repository for metabolomics studies
    and associated meta-data. Nucleic Acids Res., 41, D781–D786. Google Scholar CrossrefPubMed
    WorldCat   Iyer   N.V.  et al.  (1998) Cellular and developmental control of O2
    homeostasis by hypoxia-inducible factor 1alpha. Genes Dev., 12, 149–162. Google
    Scholar CrossrefPubMed WorldCat   King   Z.A.  et al.  (2015) Escher: a web application
    for building, sharing, and embedding data-rich visualizations of biological pathways.
    PLoS Comput. Biol, 11, e1004321. Google Scholar CrossrefPubMed WorldCat   Kluyver   T.  et
    al.  (2016) Jupyter Notebooks – a publishing format for reproducible computational
    workflows. In: Loizides F., Scmidt B. (eds), Positioning and Power in Academic
    Publishing: Players, Agents and Agendas. IOS Press, pp. 87–90. Google Scholar
    Google Preview WorldCat COPAC  Köster   J. , Rahmann S. (2012) Snakemake—a scalable
    bioinformatics workflow engine. Bioinformatics, 28, 2520–2522. Google Scholar
    CrossrefPubMed WorldCat   Kuhl   C.  et al.  (2012) CAMERA: an integrated strategy
    for compound spectra extraction and annotation of liquid chromatography/mass spectrometry
    data sets. Anal. Chem., 84, 283–289. Google Scholar CrossrefPubMed WorldCat   Langmead   B.
    , Nellore A. (2018) Cloud computing for genomic data analysis and collaboration.
    Nat. Rev. Genet., 19, 325. Google Scholar CrossrefPubMed WorldCat   Lawrence   K.A.  et
    al.  (2015) Science gateways today and tomorrow: positive perspectives of nearly
    5000 members of the research community. Concurr. Comput., 27, 4252–4268. Google
    Scholar Crossref WorldCat   Leipzig   J. (2017) A review of bioinformatic pipeline
    frameworks. Brief. Bioinform., 18, 530–536. Google Scholar PubMed WorldCat   Liew   C.S.  et
    al.  (2016) Scientific workflows: moving across paradigms. ACM Comput. Surv.,
    49, 1–39. Google Scholar Crossref WorldCat   Lim   C.K.  et al.  (2017) Kynurenine
    pathway metabolomics predicts and provides mechanistic insight into multiple sclerosis
    progression. Sci. Rep., 7, 41473. Google Scholar CrossrefPubMed WorldCat   Lovelace   M.D.  et
    al.  (2016) Current evidence for a role of the kynurenine pathway of tryptophan
    metabolism in multiple sclerosis. Front. Immunol., 7, 246. Google Scholar CrossrefPubMed
    WorldCat   Martens   L.  et al.  (2011) mzML—a community standard for mass spectrometry
    data. Mol. Cell. Proteomics, 10, R110.000133. Google Scholar WorldCat   Marx   V.
    (2013) Biology: the big challenges of big data. Nature, 498, 255–260. Google Scholar
    CrossrefPubMed WorldCat   Merkel   D. (2014) Docker: Lightweight Linux Containers
    for Consistent Development and Deployment. Linux Journal, 1–19. Google Scholar
    WorldCat   Montenegro-Burke   J.R.  et al.  (2017) Data streaming for metabolomics:
    accelerating data processing and analysis from days to minutes. Anal. Chem., 89,
    1254–1259. Google Scholar CrossrefPubMed WorldCat   Newman   S. (2015) Building
    Microservices. ‘O’Reilly Media, Inc, Sebastopol, CA. Google Scholar Google Preview
    WorldCat COPAC  Nicholson   J.K. , Wilson I.D. (2003) Opinion: understanding ‘global’
    systems biology: metabonomics and the continuum of metabolism. Nat. Rev. Drug
    Discov., 2, 668–676. Google Scholar CrossrefPubMed WorldCat   Niedenführ   S.  et
    al.  (2015) How to measure metabolic fluxes: a taxonomic guide for 13 C fluxomics.
    Curr. Opin. Biotechnol., 34, 82–90. Google Scholar CrossrefPubMed WorldCat   Polet   F.
    , Feron O. (2013) Endothelial cell metabolism and tumour angiogenesis: glucose
    and glutamine as essential fuels and lactate as the driving force. J. Intern.
    Med., 273, 156–165. Google Scholar CrossrefPubMed WorldCat   Ranninger   C.  et
    al.  (2016) Improving global feature detectabilities through scan range splitting
    for untargeted metabolomics by high-performance liquid chromatography-Orbitrap
    mass spectrometry. Anal. Chim. Acta, 930, 13–22. Google Scholar CrossrefPubMed
    WorldCat   Rocca-Serra   P.  et al.  (2016) Data standards can boost metabolomics
    research, and if there is a will, there is a way. Metabolomics, 12, 14. Google
    Scholar CrossrefPubMed WorldCat   Rost   H.L.  et al.  (2016) OpenMS: a flexible
    open-source software platform for mass spectrometry data analysis. Nat. Methods,
    13, 741–748. Google Scholar CrossrefPubMed WorldCat   Salek   R.M.  et al.  (2007)
    A metabolomic comparison of urinary changes in type 2 diabetes in mouse, rat,
    and human. Physiol. Genomics, 29, 99–108. Google Scholar CrossrefPubMed WorldCat   Sansone   S.-A.  et
    al.  (2012) Toward interoperable bioscience data. Nat. Genet., 44, 121–126. Google
    Scholar CrossrefPubMed WorldCat   Schadt   E.E.  et al.  (2010) Computational
    solutions to large-scale data management and analysis. Nat. Rev. Genet., 11, 647–657.
    Google Scholar CrossrefPubMed WorldCat   Schober   D.  et al.  (2017) nmrML: a
    community supported open data standard for the description, storage, and exchange
    of NMR data. Anal. Chem., 90, 649–656. Google Scholar CrossrefPubMed WorldCat   Silver   A.
    (2017) Software simplified. Nature, 546, 173–174. Google Scholar CrossrefPubMed
    WorldCat   Smith   C.A.  et al.  (2006) XCMS: processing mass spectrometry data
    for metabolite profiling using nonlinear peak alignment, matching, and identification.
    Anal. Chem., 78, 779–787. Google Scholar CrossrefPubMed WorldCat   Stockinger   H.  et
    al.  (2008) Experience using web services for biological sequence analysis. Brief.
    Bioinform., 9, 493–505. Google Scholar CrossrefPubMed WorldCat   Sturm   M.  et
    al.  (2008) OpenMS - an open-source software framework for mass spectrometry.
    BMC Bioinformatics, 9, 163. Google Scholar CrossrefPubMed WorldCat   Sud   M.  et
    al.  (2016) Metabolomics Workbench: an international repository for metabolomics
    data and metadata, metabolite standards, protocols, tutorials and training, and
    analysis tools. Nucleic Acids Res., 44, D463–D470. Google Scholar CrossrefPubMed
    WorldCat   Suplatov   D.  et al.  (2016) Parallel workflow manager for non-parallel
    bioinformatic applications to solve large-scale biological problems on a supercomputer.
    J. Bioinform. Comput. Biol., 14, 1641008. Google Scholar CrossrefPubMed WorldCat   Waldrop   M.M.
    (2013) Education online: the virtual lab. Nature, 499, 268–270. Google Scholar
    CrossrefPubMed WorldCat   Warth   B.  et al.  (2017) Metabolizing data in the
    cloud. Trends Biotechnol., 35, 481–483. Google Scholar CrossrefPubMed WorldCat   Wilkinson   M.D.  et
    al.  (2016) The FAIR guiding principles for scientific data management and stewardship.
    Sci. Data, 3, 160018. Google Scholar WorldCat   Wolf   S.  et al.  (2010) In silico
    fragmentation for computer assisted identification of metabolite mass spectra.
    BMC Bioinformatics, 11, 148. Google Scholar CrossrefPubMed WorldCat   Xia   J.  et
    al.  (2012) MetaboAnalyst 2.0—a comprehensive server for metabolomic data analysis.
    Nucleic Acids Res., 40, W127–W133. p Google Scholar CrossrefPubMed WorldCat   Zamberletti   E.  et
    al.  (2012) The endocannabinoid system and schizophrenia: integration of evidence.
    Curr. Pharm. Des., 18, 4980–4990. Google Scholar CrossrefPubMed WorldCat   © The
    Author(s) 2019. Published by Oxford University Press. This is an Open Access article
    distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/),
    which permits unrestricted reuse, distribution, and reproduction in any medium,
    provided the original work is properly cited. Supplementary data btz160_Supplementary_Materials
    - zip file Advertisement CITATIONS 23 VIEWS 5,159 ALTMETRIC More metrics information
    Email alerts Article activity alert Advance article alerts New issue alert In
    progress issue alert Receive exclusive offers and updates from Oxford Academic
    Recommended Container-based bioinformatics with Pachyderm Jon Ander Novella et
    al., Bioinformatics, 2018 MetaboAnalystR: an R package for flexible and reproducible
    analysis of metabolomics data Jasmine Chong et al., Bioinformatics, 2018 Tibanna:
    software for scalable execution of portable pipelines on the cloud Soohyun Lee
    et al., Bioinformatics, 2019 Association of pre-transplant vancomycin resistant
    enterococcus colonization status on long-term outcomes of allogeneic-hematopoietic
    cell transplantation Amandeep Salhotra et al., Bone Marrow Transplant, 2022 Systems-based
    approaches to study immunometabolism Vinee Purohit et al., Cellular & Molecular
    Immunology, 2022 Lipoprotein and metabolite associations to breast cancer risk
    in the HUNT2 study Julia Debik et al., Br J Cancer, 2022 Powered by Citing articles
    via Web of Science (17) Google Scholar Latest Most Read Most Cited A supervised
    bayesian factor model for the identification of multi-omics signatures scDAC:
    deep adaptive clustering of single-cell transcriptomic data with coupled autoencoder
    and dirichlet process mixture model Prioritization of oligogenic variant combinations
    in whole exomes pyaging: a Python-based compendium of GPU-optimized aging clocks
    DrivR-Base: A Feature Extraction Toolkit For Variant Effect Prediction Model Construction
    More from Oxford Academic Bioinformatics and Computational Biology Biological
    Sciences Science and Mathematics Books Journals Looking for your next opportunity?
    ACADEMIC SURGICAL PATHOLOGIST , Vermont Academic Molecular Genetic Pathologist-Solid
    Tumor & Hematologic Malignancies , Vermont System Chief for the Division of Infectious
    Diseases New York, New York Transplant Infectious Diseases Specialist Long Island,
    New York View all jobs About Bioinformatics Editorial Board Author Guidelines
    Facebook Twitter Purchase Recommend to your Library Advertising and Corporate
    Services Journals Career Network Online ISSN 1367-4811 Copyright © 2024 Oxford
    University Press About Oxford Academic Publish journals with us University press
    partners What we publish New features  Authoring Open access Purchasing Institutional
    account management Rights and permissions Get help with access Accessibility Contact
    us Advertising Media enquiries Oxford University Press News Oxford Languages University
    of Oxford Oxford University Press is a department of the University of Oxford.
    It furthers the University''s objective of excellence in research, scholarship,
    and education by publishing worldwide Copyright © 2024 Oxford University Press
    Cookie settings Cookie policy Privacy policy Legal notice Oxford University Press
    uses cookies to enhance your experience on our website. By selecting ‘accept all’
    you are agreeing to our use of cookies. You can change your cookie settings at
    any time. More information can be found in our Cookie Policy. Cookie settings
    Accept all'
  inline_citation: '>'
  journal: Bioinformatics
  limitations: '>'
  pdf_link: https://academic.oup.com/bioinformatics/article-pdf/35/19/3752/30061666/btz160.pdf
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: 'Interoperable and scalable data analysis with microservices: applications
    in metabolomics'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/mic.2019.2955784
  analysis: '>'
  authors:
  - Fred Douglis
  - Jason Nieh
  citation_count: 12
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet Computing
    >Volume: 23 Issue: 6 Microservices and Containers Publisher: IEEE Cite This PDF
    Fred Douglis; Jason Nieh All Authors 12 Cites in Papers 2179 Full Text Views Abstract
    Document Sections IN THIS ISSUE Authors Citations Keywords Metrics Abstract: The
    articles in this special section focus on microservices and containers. These
    services allow an application to be comprised of many independently operating
    and scalable components, have become a common service paradigm. The ability to
    construct an application by provisioning these interoperating components has various
    advantages, including the isolation and independent development of tools such
    as key-value stores, authentication, logging, and many others. Containers are
    one type of system infrastructure that is commonly used to support microservices.
    With container management systems like Docker and orchestration systems like Kubernetes
    to control applications and dynamically provision their resources, cloud services
    can be extremely scalable, reliable, and reactive. However, other systems beyond
    containers can be used to support microservices, and many applications other than
    microservices benefit from containerization. Published in: IEEE Internet Computing
    ( Volume: 23, Issue: 6, 01 Nov.-Dec. 2019) Page(s): 5 - 6 Date of Publication:
    01 Nov.-Dec. 2019 ISSN Information: DOI: 10.1109/MIC.2019.2955784 Publisher: IEEE
    Microservices, which allow an application to be comprised of many independently
    operating and scalable components, have become a common service paradigm. The
    ability to construct an application by provisioning these interoperating components
    has various advantages, including the isolation and independent development of
    tools such as key-value stores, authentication, logging, and many others. Containers
    are one type of system infrastructure that is commonly used to support microservices.
    With container management systems like Docker and orchestration systems like Kubernetes
    to control applications and dynamically provision their resources, cloud services
    can be extremely scalable, reliable, and reactive. However, other systems beyond
    containers can be used to support microservices, and many applications other than
    microservices benefit from containerization. Containers should be contrasted with
    another virtualization technique, traditional virtual machines. Back in March
    2013, IEEE Internet Computing published a special issue on virtualization. By
    then, virtual machines had become a popular way to isolate applications, as a
    specialized server could run in its own virtual machine while sharing a pool of
    available resources (such as a VMware ESXi server). Virtual machines provide a
    convenient way to encapsulate state (so that machines can be migrated among servers)
    and to deploy a service in a predictable fashion. For instance, a service can
    be wrapped in a deployable template that can be installed into a virtualization
    environment with just a few configuration steps. Containers, by comparison, provide
    a way to virtualize and isolate the operating system, allowing multiple applications
    to run in a single operating system; i.e., it is the software rather than the
    hardware that is virtualized. Without the need to run multiple operating system
    instances, containers can be more lightweight and potentially easier to manage.
    But like virtual machine templates, containers can have specifications that define
    exactly what software environment an application is to be run within. For instance,
    a container might be created with a specific version of Ubuntu, in which a specific
    version of python and python libraries would execute. Given the ability to create
    services with various interacting containers, which may execute on one or many
    nodes, complex applications can be synthesized by combining these services in
    interesting ways. In particular, as demand varies, the individual microservices
    can be replicated to scale with demand or reduced to fit current requirements.
    To support this adaptivity, the overall service must be architected to handle
    parallelism of individual microservices and to perform appropriate selection of
    the available instances to maximize performance. IN THIS ISSUE There are three
    articles in this theme issue on microservices and containers. The first two focus
    on how microservices should best be used, whereas the third provides a case study
    of containerization in a specific application context. The first article reports
    on work from the Standard Performance Evaluation Corporation (SPEC) Research Group
    on Cloud Computing (https://research.spec.org/working-groups/rgcloud.html). In
    “The SPEC-RG Reference Architecture for FaaS: From Microservices and Containers
    to Serverless Platforms,” Van Eyk et al. describe best practices for Functions-as-a-Service.
    FaaS is a key example of the elastic scalability of microservices described above:
    instantiate functions when they are needed, and eliminate them when not. The authors
    evaluate numerous existing examples of serverless computing, which support the
    FaaS model, and describe how it is best deployed. They compare a number of platforms,
    such as Kubernetes, services from Amazon Web Services, Apache, and others; and
    many more. Akbulut and Perros in “Performance Analysis of Microservice Design
    Patterns,” focus more specifically on the performance of microservices. They study
    several metrics (query response time, efficient hardware usage, hosting costs,
    and packet loss rate) as applied to three design patterns. An API gateway, which
    acts as a load balancer and a guard against overload (note that this gateway bears
    some resemblance to the application described in the third paper, below). A chain
    of microservices that pass data from stage to stage for different types of processing.
    Asynchronous messaging, using RabbitMQ. Finally, Amirante and Romano authored
    “Container NATs and Session-Oriented Standards: Friends or Foe?” This article
    takes a very different view of the areas covered by this special issue, with the
    focus on containers and in particular the network isolation that is common in
    container environments. In particular, they observe that Docker isolates applications
    by providing them one Internet Protocol (IP) address and then translating that
    using “network address translation” for use outside the container. Some applications
    need to know the IP address by which they are reached from the outside, and forcing
    NATs on such applications raise a level of complexity. Amirante and Romano explain
    the issues and propose some possible work-arounds. Authors Citations Keywords
    Metrics More Like This Lightweight Virtualization Approaches for Software-Defined
    Systems and Cloud Computing: An Evaluation of Unikernels and Containers 2019 Sixth
    International Conference on Software Defined Systems (SDS) Published: 2019 Performability
    analysis of I/O bound application on container-based server virtualization cluster
    2014 IEEE Symposium on Computers and Communications (ISCC) Published: 2014 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE internet computing
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/4236/8970628/08970636.pdf
  publication_year: 2019
  relevance_score1: 0
  relevance_score2: 0
  title: Microservices and Containers
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/iiswc.2016.7581267
  analysis: '>'
  authors:
  - Tatsushi Inagaki
  - Yohei Ueda
  - Maureen O’Hara
  citation_count: 9
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2016 IEEE International Sympo...
    Container management as emerging workload for operating systems Publisher: IEEE
    Cite This PDF Tatsushi Inagaki; Yohei Ueda; Moriyoshi Ohara All Authors 6 Cites
    in Papers 2 Cites in Patents 818 Full Text Views Abstract Document Sections 1.
    Introduction II. Docker Architecture III. Core Scalability IV. Container Scalability
    V. Related Work Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: Operating-system-level virtualization is becoming increasingly
    important for server applications since it provides containers as a foundation
    of the emerging microservice architecture, which enables agile application development,
    deployment, and operation - the essential characteristics in modern cloud-based
    services. Agility in the microservice architecture heavily depends on fast management
    operations for containers, such as create, start, and stop. Since containers rely
    on administrative kernel services provided by the host operating system, the microservice
    architecture can be considered as a new workload for an operating system as it
    stresses those services differently from traditional workloads. We studied the
    scalability of container management operations for Docker, one of the most popular
    container management systems, from two aspects: core and container scalability,
    which indicate how much the number of processor cores and number of containers
    affect container management performance, respectively. We propose a hierarchical
    analysis approach to identify scalability bottlenecks where we analyze multiple
    layers of a software stack from the top to bottom layer. Our analysis reveals
    that core scalability has bottlenecks at a virtualization layer for storage and
    network devices, and that container scalability has bottlenecks at various components
    that inquire mount points. While those bottlenecks exist in a daemon process of
    Docker, the root causes are a couple of interfaces of the underlying kernel. This
    implies the operating system has room for improvement to more efficiently host
    emerging microservice applications. Published in: 2016 IEEE International Symposium
    on Workload Characterization (IISWC) Date of Conference: 25-27 September 2016
    Date Added to IEEE Xplore: 10 October 2016 ISBN Information: DOI: 10.1109/IISWC.2016.7581267
    Publisher: IEEE Conference Location: Providence, RI, USA SECTION 1. Introduction
    The recent evolution of container technology [1]–[5], that is, operating-system-level
    (OS-level) virtualization, accelerates continuous development, deployment, and
    operation of large-scale web services. A common design practice is to implement
    a service as a set of microservices [6]. Each service is developed as an image
    that contains a complete but minimum set of files required to run the service
    and is deployed as a container, which is a virtual guest OS instance running on
    the kernel of the host OS. This design improves the portability, isolation, and
    robustness of a microservice by confining all the dependencies to dynamically
    linked libraries and configuration files inside its image. While the management
    operations of containers, such as create, start, stop, commit, and remove of containers,
    is much faster than that of traditional virtual machines (VMs) [7], its performance
    is still important to accelerate the following activities for agile development,
    deployment, and operation practices: Test-driven development. A container management
    system such as Docker [2] builds a container image by running a sequence of containers
    to execute each command in a build script called Dockerfile. While this design
    accelerates the building of an image by caching intermediate images, it incurs
    an overhead of container management operations at the first build. Faster container
    management operations enables a developer to be more productive since he or she
    more frequently builds an image and runs a container to test the application manually
    or automatically. Continuous integration and deployment. When a developer adds
    a new feature, or fixes a bug, of a microservice, the change will be immediately
    integrated into a shared code repository, and will be eventually deployed to the
    production environment, if the updated microservice passes an appropriate test
    suite. Faster container management operations accelerate the building of an image
    to detect a build break, pushing it into an image registry, testing it in a container,
    and deploying it as a container to staging and production environments. Autoscaling.
    Popular cloud computing services provide autoscaling, where the amount of computing
    resources is automatically adjusted according to the frequency of service requests
    or utilization of computing resources [8]–[11]. Adding new computing resources
    to an application hosted on containers leads to the deployment of new containers.
    A special case of this scenario is a failover or migration of microservices running
    as a container. We studied the scalability of two frequently used container management
    operations, building an image and running a container, of Docker, one of the most
    popular container management systems. It adopts a client-server architecture,
    in which a Docker daemon as a server manages all the containers associated with
    it and a client sends a request to the daemon to manage a container as necessary.
    We identified the bottlenecks in the operations, typically a system call that
    limits the concurrent execution of multiple management operations, by analyzing
    multiple layers in a software stack hierarchically. In particular, we focused
    on vertical scalability, which is on a single Docker daemon running on a single
    host OS with multiple processor cores (i.e. scale up). Luzzardi demonstrated massive
    horizontal scalability, which is on multiple Docker daemons running on multiple
    host OSs (i.e. scale out) [12]. The high vertical scalability allows each node
    to host a large number of containers by taking advantage of a large number of
    cores in a modern processor, which leads to high cost performance combined with
    the already demonstrated high horizontal scalability. We also studied the effect
    of storage drivers of Docker on scalability. A storage driver implements a copy-on-write
    strategy among container images and a root filesystem of containers to accelerate
    container management operations. The choice of a storage driver has a large impact
    on the performance of container management [13]. Another benefit of vertical scalability
    is disk-space efficiency. The higher vertical scalability becomes, the more containers
    can share a set of images associated with a Docker daemon. In Section II, we briefly
    introduce the architecture of a Docker daemon and its software stack. Fig. 1.
    Breakdown of processor cycles to build nginx image when building eight images
    in single docker daemon running on one and four processor cores with different
    types of storage drivers. The processor cycles are shown in seconds required to
    build one image per processor core, which corresponds to the elapsed time of entire
    operation. Measurements were conducted in the environment described in appendix
    A. Show All We First Investigated Core Scalability. We would like to answer the
    questions “How much can we accelerate container management operations by making
    extra processor cores available to a single Docker daemon?” and “If the operations
    do not scale, why?” Figures 1 and 2 show the processor cycles required to build
    an image of a container and to run a container from the built image. The image
    is built to run Nginx [14], which is widely used as a web server, reverse proxy
    server, and load balancer in web applications. Note that the numbers in Figures
    1 and 2 correspond to the elapsed time since they also include idle cycles. With
    ideal core scalability, the number of processor cycles to build one image or run
    one container on four processors should be a quarter of those on a single processor.
    However, our experimental results show that building an image does not scale with
    the devicemapper storage driver operating in either the loop—lvm or direct—lvm
    mode, and running a container does not scale well with any storage driver. In
    Section III, we analyze idle processor cycles in a Docker daemon to identify the
    bottlenecks to these operations. The daemon is implemented in the Go [15] programming
    language which provides lightweight user-level threads called goroutines. Our
    hierarchical analysis allows us to identify bottlenecks when a number of goroutines
    are idle. For building images, the devicemapper storage driver has bottlenecks
    to remove virtual block devices. For running containers, all storage drivers have
    a bottleneck to allocate a root filesystem of a container and/or in the port mapper
    that connects a container to the network. Fig. 2. Breakdown of processor cycles
    to run nginx container when running sixty-four containers in single docker daemon
    on one and four processor cores with different types of storage drivers. The processor
    cycles are shown in seconds spent to run one container per processor core, which
    correspond to the elapsed time of the entire operation. Show All We Then Investigated
    Container Scalability. We would like to answer the questions, “How much can we
    consolidate multiple containers into a single Docker daemon without extra overhead?”
    and “If overhead exists, why?” Figure 3 shows the processor cycles required to
    run a container when we change the total number of containers run in a single
    Docker daemon. This container runs a shell command of the BusyBox [16] tool, which
    links popular command-line tools into a single binary executable. With ideal container
    scalability, the number of processor cycles to run one container should remain
    constant and be irrelevant to the total number of containers. However, our experimental
    results show that the number of processor cycles to run 1 of 1,024 containers
    is more than twice that to run 1 of 64 containers with any storage driver. This
    means running a new container becomes slower when a large number of containers
    are already running. In Section IV, we identify a bottleneck in the Docker daemon
    by analyzing busy processor cycles. While the daemon has a flat execution profile,
    which the hottest procedure consumes only a few percent of the total number of
    processor cycles, our hierarchical analysis again allows us to identify that the
    root cause is parsing overhead of mountinfo, which is a virtual file to provide
    information of mount points, and the size of which increases proportionally to
    the number of active containers. In contrast to the related work described in
    Section V, our contributions in this paper are as follows: Proposing hierarchical
    bottleneck-analysis approach. We argue that a novel approach of hierarchically
    analyzing idle and busy processor cycles allows us to identify bottlenecks for
    core and container scalability, even when the target application uses a number
    of idle threads and has a flat execution profile. Identifying importance of the
    container management performance. The Emerging Use of Container Technology makes
    the administrative performance of the OS more important to improve productivity
    of developers and responsiveness of cloud services. The performance of these interfaces
    had not been sufficiently investigated. Analyzing vertical scalability of container
    management and identifying bottlenecks. While Horizontal scalability of container
    management has been extensively studied, vertical scalability and where bottlenecks
    exist have not. Identifying these bottlenecks will be useful for developers of
    container management systems and of OSs to efficiently use many processor cores
    in a modern processor. Our conclusion, discussed in detail in Section VI, is that
    container management is a new workload that stresses the administrative performance
    of the host OS. While the bottlenecks identified in Sections III and IV are in
    a Docker daemon, they have origins in system calls, virtual filesystems, and other
    subsystems of the underlying OS for resource management. Previously, human operators
    used these administrative interfaces daily or hourly, but the emerging microservice
    architecture exercises them in milliseconds to build responsive web services.
    Fig. 3. Breakdown of number of processor cycles to run busybox container when
    running 64 and 1,024 containers simultaneously in single docker daemon on 1 processor
    core. The processor cycles are shown in seconds required to run one container,
    which corresponds to the elapsed time divided by the number of total containers.
    Show All SECTION II. Docker Architecture We briefly describe the architecture
    of Docker [2] and related features of the Linux kernel [17] for easy understanding
    of the succeeding Sections III and IV. Docker uses a client-server architecture
    to implement OS-level virtualization. A client connects to a daemon via the application
    programming interface (API) in the style of Representational State Transfer (REST)
    over the Hypertext Transfer Protocol (HTTP). Two common operations that a client
    can do are as follows: Build an image according to a given Dockerfile. Run a container
    based on an image. A container is a unit of OS-level virtualization and has an
    initial process, which is isolated by namespaces and whose resources are managed
    by control groups (cgroups), an isolated root filesystem, which is a copy-on-write
    snapshot of an image, and virtual devices such as networks and terminals. A Dockerfile
    contains a sequence of shell script commands to create files in an image. A unique
    feature of Docker as a container management system is that an image is also built
    as a snapshot from its parent image. That is, Docker runs each line of a Dockerfile
    as a container based on a parent image built by the previous line and commits
    the updated snapshot as a new child image of the parent image. The benefits of
    this implementation are that it can accelerate repeated building of variations
    in the same image with an updated Dockerfile by caching intermediate images, save
    disk spaces to store images by sharing common layers among multiple images, and
    save memory spaces to store containers by sharing file caches among multiple containers
    based on the same image. The downside is the overhead to implement the copy-on-write
    strategy, which is incurred at the runtime and management time of a container.
    The latter is our focus in this paper. A. Storage Drivers A storage driver (also
    known as a “graph driver”) is a pluggable component of a Docker daemon that implements
    a copy-on-write strategy using various file systems and subsystems of the Linux
    OS. The daemon determines the storage driver when it starts and continues using
    the same driver while it is running. Images and containers cannot share snapshots
    across different storage drivers. The storage drivers we investigated in this
    study are aufs, btrfs, devicemapper, and overlay. 1) Aufs The aufs storage driver
    uses the advanced multi- layered unification filesystem (AUFS) [18] on the Linux
    filesystem. The AUFS provides a union mount operation, which can stack multiple
    directories (also called as “branches”) on the base file system into one virtual
    directory. In the context of Docker, the root file system of a container consists
    of a union-mounted directory with a writable empty branch at the top, and read-only
    branches from the base image below. When a container opens a file to read, AUFS
    searches for the file from the top to the bottom branches. When a container opens
    a file to write, AUFS searches for the file and copies it up into a writable branch.
    The copied file in the writable branch hides the later accesses to the original
    file in a lower branch. 2) Btrfs The btrfs storage driver uses the Linux B- Tree
    filesystem (Btrfs) [19], which is a copy-on-write Linux filesys-tern. The Btrfs
    provides a subvolume, which is an independent root directory within a Btrfs or
    another subvolume. A snapshot of a subvolume is a subvolume that looks like a
    full copy of the source subvolume, but the actual data are not copied until the
    snapshot is updated. The btrfs storage driver implements the base layer of an
    image as a subvolume, and a child layer as a snapshot of its parent layer. The
    root file system of a container is a snapshot of the base image. 3) Devicemapper
    The devicemapper storage driver uses the Device Mapper [20] framework of the Linux
    kernel, which can create a virtual block device based on another block device.
    The Device Mapper framework provides features thin-provisioning, which allocates
    a block on-demand when it is written, and snapshot, which can create a virtual
    block device as a snapshot of another block device. Given a logical volume having
    the thin-provisioning feature as a thin pool, devicemapper creates a base device
    from the pool and formats the device with a filesystem such as ext4 or xfs. Then
    a layer of an image or a container is created as a snapshot of its parent layer
    or the base image. When a container writes into a file, the Device Mapper framework
    allocates new blocks into the device for the root filesystem of the container
    and copies the corresponding blocks from the ancestor devices. The devicemapper
    storage driver has two different modes regarding how to allocate the thin pool:
    The loop-lvm mode allocates a sparse file, which is a file allocated with a given
    size, but not yet written, and creates a thin pool on the sparse file as a loop
    device, which is a file accessed as a virtual block device. The direct-lvm mode
    uses a given thin pool which is allocated on a physical block device. 4) Overlay
    The overlay storage driver uses the OverlayFS, which is another union file system
    of the Linux file system. The major difference from AUFS is that OverlayFS stacks
    two directories, an upperdir and lowerdir, and provides a single merged directory.
    In the context of Docker, the root file system of a container is a merged directory
    whose upperdir is a writable directory and lowerdir is a read-only directory,
    where the directories and files from the base image are hard -linked. B. Network
    Driver The network driver of a Docker daemon connects a container to the network.
    While Docker provides a pluggable network driver, we describe the default bridge
    network, which we used for our experiments in this study. The bridge network allocates
    an Ethernet bridge with a set of private Internet Protocol (IP) addresses of the
    host OS. Each container has its own network namespace, is connected to the bridge
    via a Virtual Ethernet pair, and obtains a private IP address from the bridge.
    Thus, the containers connected to the same bridge network can communicate with
    each other using their private IP addresses. For communication with the outside
    world, the bridge network uses the Netfilter framework which provides packet filtering
    and network address and port translation (NAPT). The bridge network uses the iptables
    command to configure the NAPT rules on the host OS. For hairpinning, which is
    communication between containers using an external IP address and a port, the
    bridge network spawns a dedicated process called userland proxy for each container.
    C. Hierarchy Fig. 4. Hierarchy of software stack around docker daemon. Show All
    Figure 4 shows the hierarchy of the software stack inside and outside the Docker
    daemon. Since the daemon receives a request from a client via the REST API, the
    topmost layer contains HTTP request handlers to execute Docker commands. The next
    layer contains a standard runtime library from the Go programming language and
    external libraries, such as libdevmapper, to handle the Device Mapper framework.
    External commands, such as iptables, are also used. The lowest layer is the Linux
    kernel, including subsystems such as AUFS, Btrfs, Device Mapper, OverlayFS, Ethernet
    bridge, and Netfilter. SECTION III. Core Scalability Building images with devicemapper
    and running containers with any storage driver do not scale according to the number
    of processor cores, as shown in Figures 1 and 2. The breakdown in the processor
    cycles shows that the primary reason is idle cycles. It is not terivial to identify
    why processors are idle and what is the bottleneck resource, because we cannot
    charge idle cycles directly to any executable instruction by using a profiling
    tool. Altman et al. [21] demonstrated that categorizing idle threads in a hub
    application can characterize performance problems of server applications due to
    idle cycles, and can infer their root cause. Since their primary target is an
    application server compliant to Java Platform, Enterprise Edition (Java EE) [22],
    they analyzed idle Java threads. In our case, these threads correspond to goroutines.
    A problem is that goroutines are frequently spawned in a typical application written
    in the Go programming language, and most of the goroutines are idle. While a Java
    thread is typically implemented as a native thread, a goroutine is a user-level
    thread implemented by the runtime library of Go. It is spawned by both the runtime
    library and application programs for various purposes such as concurrent input
    and output operations, and event processing. In our examples, the image-building
    scenario in Figure 1 spawns up to 150 goroutines with about 30 different contexts,
    and the container running scenario in Figure 2 spawns up to 1,000 goroutines with
    about 30 different contexts, and almost all of them are idle. A. Hierarchical
    Analysis of Idle Cycles On a typical client-server application, we can rephrase
    the question, “Why does the application not scale according to the number of processor
    cores?” into another question, “Why is the concurrency among the original requests
    from the clients lost after receiving requests and before using processor cores?”
    It is likely because a worker thread is blocked at synchronization with another
    thread to hold a software or hardware resource exclusively. This leads to the
    following approach to identify bottleneck resources based on a layered performance
    model [23]. Given a thread dump, identify the source code of the target application,
    and the topmost layer in the software stack of the application. Identify the set
    of threads that handle incoming requests at the topmost layer. Classify each thread
    into the following three subsets: Running to process a request from a client,
    Blocked by another thread at the same stack layer, or Waiting for completion of
    a service from a lower stack layer. If the majority of the threads are blocked,
    the corresponding resources guarded by mutual exclusion are the bottleneck resources.
    If the majority of the threads are waiting for services from a lower layer, identify
    a new set of threads that are handling the services at the layer. Go down into
    that layer and repeat from Step 3 above. In our case, the topmost layer for Step
    2 is an HTTP request handler in a Docker daemon, which is implemented as the method
    ServeHTTP of the interface net/http. Handler. The Docker daemon accepts a request
    from a client as an HTTP request, as shown in Figure 4. A goroutine blocks other
    goroutines by acquiring a mutual exclusion lock (mutex). This is implemented as
    the method Lock of the structure sync. Mutex. At Step 3b, we can identify whether
    a goroutine is blocked by detecting this method in the stack dump of the goroutine.
    A goroutine also waits for the completion of a service from a lower layer. This
    is implemented as a receive operation from another goroutine. At Step 3c, we can
    identify whether a goroutine is waiting for another goroutine by inspecting a
    receive operation at the top of the stack dump of the goroutine. At Step 5, we
    need to identify a goroutine that will send a message into the corresponding channel
    upon a service completion. Since this information is not available in only a stack
    dump, we analyze the source code to identify which channel is used by the receive
    operation, which statement will send a value to the channel, and which goroutine
    in the stack dump will execute the send statement. B. Bottlenecks to Building
    Images on Multiple Cores Fig. 5. Mean number of goroutines which are serving HTTP
    requests in daemon when building eight nginx images concurrently in one daemon
    running on four processor cores. The component “blocked” represents goroutines
    blocked with sync. Mutex. Lock to acquire a mutex. The component “waiting-build”
    represents goroutines that are waiting for a lower layer running a build script
    command in a dockerfile to build an image. Show All According to the approach
    in Section III-A, we can now investigate why building Nginx images in Figure 1
    does not scale with devicemapper. Figure 5 shows the mean number of goroutines
    serving HTTP requests in a daemon when building eight Nginx images concurrently
    in one daemon running on four processor cores. We can see that: The majority of
    the goroutines are waiting for a build script command in a Dockerfile with aufs,
    btrfs, and overlay, but with the two modes loop-Ivm and direct-Ivm of devicemapper,
    only two out of eight goroutines are waiting for a build script command. That
    is, only two images can be built concurrently on devicemapper, while more images
    can on other storage drivers. 2) About half of the goroutines are blocked with
    devicemaper. The first observation explains that devicemapper scales less than
    other storage drivers. The second observation explains why the difference in the
    first observation occurred. We can then identify the bottleneck resources by investigating
    what mutex each blocked goroutine is acquiring. Figure 6 shows a breakdown of
    the blocked goroutines in Figure 5 by the structure guarded by a mutex. With devicemapper,
    the two dominant bottleneck resources are as follows: The mutex mountL of the
    structure layerStore, which manages layers of images and containers, and is a
    singleton object in the Docker daemon. This mutex is used to serialize creation
    and removal of the topmost read-write layer of the container''s root filesystem.
    The mutex of the structure DeviceSet, which manages the list of virtual devices,
    and is a singleton object in devicemapper. This mutex serializes all invocations
    to the Device Mapper subsystem via the library libdevmapper, which is not thread-safe.
    Fig. 6. Mean number of blocked goroutines serving HTTP requests in daemon when
    building eight nginx images concurrently in one daemon running on four processor
    cores. Each component represents a mutex that a blocked goroutine is acquiring.
    The components “mountl” and “layerl” are two mutexes of the structure layerstore
    which manages layers of images and containers. The component “deviceset” is the
    mutex of the structure deviceset, which manages the set of virtual devices of
    devicemapper. The component “driver” is the mutex of the storage driver. Show
    All When a goroutine acquires both mutexes, the ordering to avoid deadlock is
    mountL first and DeviceSet second. Thus, the primary bottleneck can be DeviceSet.
    Fig. 7. Mean number of blocked goroutines serving HTTP requests in daemon when
    building eight nginx images concurrently in one daemon running on four processor
    cores. The components represent blocking operations in the critical section. The
    components “removedevice” and “activatedevice” are corresponding function calls
    of libdevmapper, which are implemented as ioctl system calls. The components “unmount”
    and “mount” are umount and mount system calls, respectively. The component “blkdiscard”
    is another ioctl system call to discard a block device. The component “lstat”
    is the system call 1stat, which is invoked during traversal of file paths by the
    function path/filepath.Walk. Show All We can further identify the root cause of
    the bottlenecks by investigating what the blocking goroutine is doing while it
    locks the corresponding mutex in its critical section. This is less obvious than
    identifying blocked goroutines because the methods Lock and Unlock are just library
    calls. Fortunately, the scope of mutex and that of callers of the function that
    acquire the mutex are usually nested within each other since a common programming
    practice in Go is locking a mutex once in a function and unlocking it by the next
    defer statement, which is executed at an exit of the function. Thus, the caller
    function of the method Lock is typically on the stack of the blocking goroutine
    while it is in the critical section. Figure 7 shows the breakdown of blocked goroutines
    in Figure 5 by the operation the blocking goroutine is doing. With devicemapper,
    the structure DeviceSet is the primary bottleneck resource since the system calls
    mount, umount, and ioctl for activating and removing mapping of a virtual device
    and for discarding a block device are done while locking DeviceSet. In summary,
    concurrent building of images does not scale with devicemapper due to the bottleneck
    to serialize creation, removal, mounting, and unmounting of virtual devices for
    the container''s root filesystems. C. Bottlenecks to Running Containers on Multiple
    Cores Fig. 8. Mean number of goroutines serving HTTP requests in daemon when running
    thirty-two nginx containers concurrently in one daemon running on four processor
    cores. The component “blocked” represents goroutines blocked with sync.Mutex.Lock.
    The component “waiting-start” represents goroutines waiting for an Nginx daemon
    that had started in a container. Show All The approach in Section III-A also allows
    us to explain why running Nginx containers in Figure 2 does not scale with any
    storage drivers. Figure 8 shows the mean number of goroutines serving HTTP requests
    in the daemon when running thirty-two Nginx containers in one daemon running on
    four processor cores. We can see that the number of goroutines that are actually
    starting Nginx daemon in each container is less than two with any storage driver.
    This is the direct reason that concurrent running of Nginx containers does not
    scale. We can then identify bottleneck resources by investigating mutexes that
    the blocked goroutines are acquiring. Figure 9 shows a breakdown of the blocked
    goroutines by the structure guarded by a mutex. The two dominant bottleneck resources
    are as follows: The mutex mountL of the structure layerStore, which was also a
    bottleneck resource for building images in Section III-B, for aufs and the two
    modes loop-lvm and direct-lvm of devicemapper, and The mutex of the structure
    PortMapper, which manages the connection to the default bridge network for containers,
    for btrfs and overlay. Fig. 9. Mean number of goroutines serving HTTP requests
    in daemon when running thirty-two nginx containers concurrently in one daemon
    running on four processor cores. The component “mountl,” represents a mutex of
    the structure layerstore, which manages layers of images and containers. The component
    “deviceset” represents the mutex of the structure DeviceSet, which manages the
    set of virtual devices of devicemapper. The component “driver” represents the
    mutex of the storage driver. The component “portmap-per” is the mutex of the structure
    portmapper, which manages network connection to the default bridge network for
    containers. Show All Since the network driver is independent of the storage driver,
    these two mutexes do not have dependency. The performance of running containers
    with btrfs or overlay is faster than that with aufs or devicemapper. Thus, when
    mountL of layerStore is not a bottleneck, PortMapper becomes the next bottleneck.
    We can now identify the root causes of the bottlenecks by identifying what the
    blocking goroutine is doing. Figure 10 shows the breakdown of the blocked goroutines
    in Figure 8 by the operation that the blocking goroutine is doing. We can see
    the operations in the critical sections are as follows: Creation and removal of
    virtual devices with devicemapper, as we discussed in Section III-B, Running an
    external process to clean up copied-up hard links with aufs, and Running a proxy
    process for port forwarding with btrfs and overlay. In summary, concurrent running
    of Nginx containers does not scale due to the bottlenecks in aufs and devicemapper,
    and the next bottleneck in the network driver to serialize running of a proxy
    process for a container. SECTION IV. Container Scalability Running many containers
    does not scale, as shown in Figure 3. This time, the direct reason is the increase
    in the number of busy cycles from 64 to 1,024 containers. It is also challenging
    to analyze and optimize a workload that has a flat execution profile, and container
    management is such a workload. Figure 11 shows the ten hottest functions in a
    Docker daemon with aufs when running multiple BusyBox containers concurrently.
    The numbers are those of busy processor cycles (in milliseconds) required in each
    function to run one container. The hottest function runtime.scanobject requires
    15 and 6% of the total number of processor busy cycles of the Docker daemon, with
    64 and 1,024 containers, respectively. Most of these functions are in the standard
    library of Go. We can see that the functions for text processing become quite
    hot with 1,024 containers. Thus, these functions are “bottlenecks” to container
    scalability, that is, the direct reasons running one container out of many slows
    down when we increase the total number of containers. Fig. 10. Mean number of
    blocked goroutines serving HTTP requests when running thirty-two nginx containers
    concurrently in one daemon running on four processor cores. The components represent
    blocking operations in the critical section. The components “removedevice”, “activatedevice”,
    “mount” and “unmount” are the same as those in Figure 7. The component “auplink”
    represents a goroutine waiting for completion of an external script auplink, which
    makes copied-up hard-links permanent. The component “udevwait” represents a call
    to libdevmapper to synchronize with the udev daemon. The component “proxystart”
    represents a goroutine waiting for a userland proxy that had started. Show All
    Fig. 11. Ten hottest functions in docker daemon with aufs running on one processor
    core when running multiple busybox containers concurrently. The numbers are those
    of busy processor cycles required in each function to run one container. The bars
    are sorted in descending order of time spent with the 1,024 containers. Show All
    Unfortunately, optimizing each function separately in a fiat execution profile
    is usually not cost-effective. To obtain insight for more effective optimization,
    we need to identify the root cause to answer the question, “Why do these functions
    become hot with many containers?” A. Hierarchical Analysis of Busy Cycles We can
    identify the root cause of the slowdown by applying another top-down approach
    to analyze the difference in busy cycles of two execution profiles with different
    total number of containers. The observation regarding Figure 11 suggests that
    when we increase the total number of containers from 64 to 1,024, it is more likely
    that the library functions are more frequently invoked, rather than spending longer
    cycles at each invocation. This leads to the following approach to identify the
    root cause: Given two execution profiles with different problem sizes, identify
    the source code of the target application and the topmost layer in the software
    stack of the application. For each function at the given software stack in the
    two execution profiles, calculate busy cycles per unit size of the problem. If
    the same function is significantly slower in one execution profile with a larger
    problem size, it is likely that the function relates to the root cause, since
    the identified function is that at the highest software stack. If the busy cycles
    per unit problem size for all functions in that stack are equivalent between the
    two profiles, go down into the next stack and repeat from Step 2. In our case,
    the topmost layer of the software stack includes the HTTP request handlers, as
    described in Figure 4. The functions in this layer have the package names starting
    with github.com. B. Bottlenecks to Running Many Containers Fig. 12. Top ten hot
    functions of packages github.Com in docker daemon with aufs running on one processor
    core when running multiple busybox containers concurrently. The numbers represent
    those of busy cycles required in each function to run one container. The bars
    are sorted in descending order of time spent with 1,024 containers. Show All According
    to the approach in Section IV-A, we can now explain why the number of busy cycles
    to run one Busy-Box container increases when the total number of containers increases.
    Figure 12 shows the ten hottest functions in the packages starting with github.com.
    While the first function initGranularPredefinedNetworks is hot with both of 64
    and 1,024 containers, the second parseInfoFile and third FindCgroupMountPointAndRoot
    are hot only with 1,024 containers. The hotness and common functionality of these
    two functions gives us the insight that parsing information of mount points can
    be the root cause. The functions with the package names starting with github.com
    consume 1.1 and 0.6% of the total number of busy cycles of the daemon when running
    64 and 1,024 containers, respectively. While their direct contribution is small,
    their difference between two execution profiles gives us the insight that is not
    directly available from the difference in the hot functions in all profiles. C.
    Root Cause: Mountinfo While this bottleneck has been identified and fixed in the
    latest versions of Docker [24], for completeness of discussion, we describe how
    the bottleneck originates from the administrative interface of the current Linux
    kernel. The first problem is that the complete information of mount points is
    only available in the pseudo text file mountinfo on the Process Filesystem. The
    questions asked are as follows: Where is the mount point of a specific subsystem,
    such as cgroups and Security-Enhanced Linux (SELinux), which is mounted as a filesystem?,
    What is the sharing relationship between the global mount namespace and mount
    namespace of a container? Since the Linux kernel does not yet provide convenient
    system calls to answer these questions, we need to parse mountinfo sequentially.
    The second problem is that the size of mountinfo increases proportionally to the
    number of live containers. Each live container usually appends the following three
    lines at the tail of mountinfo: The mount point of the root filesystem, A pseudo
    filesystem “nsfs” to represent the network namespace, and A tmpfs filesystem to
    implement the inter-process communication (IPC) namespace. The btrfs storage driver
    does not append the first item since each subvolume is not mounted. While the
    mount point of those subsystems can be usually found near the beginning of mountinfo,
    searching information of a container''s mount points, searching for a disabled
    subsystem, or parsing all the lines as a list of objects, incurs cost of parsing
    text, creating objects, and collecting garbage proportional to the number of live
    containers. D. Eliminating Redundant Parsing We evaluated how container scalability
    can be improved according to the insight obtained from the bottleneck analysis
    in Section IV-B. As described in Section IV-C, the mount point information is
    used for various purposes and there is no way to obtain one other than parsing
    mountinfo. Thus, according to the fix [24], we modified the client components
    in the Docker daemon so that they do not parse unused lines in mountinfo. More
    specifically, in the optimized daemon: The aufs storage driver checks whether
    the root file system is mounted by checking the filesystem type. The library to
    handle cgroups searches a mount point only for available subsystems. The library
    to handle SELinux quits parsing after it detects the mount point. The sharing
    status between the global mount namespace and mount namespace of a container is
    checked only after the system call pivot_root had failed. Fig. 13. Processor cycles
    to run busybox container when running 1,024 containers simultaneously in single
    docker daemon running on one processor core. The bars labeled “O” represent processor
    cycles with a docker daemon optimized to reduce redundant parsing of mountinfo
    according to the approach in Section IV-D. The bars labeled “b” represent the
    baseline numbers for comparison, which are shown in Figure 3. Show All Figure
    13 shows a comparison of the processor cycles to run one out of 1,024 BusyBox
    containers in a single daemon on a single processor core. Reductions in the total
    number of cycles from the baseline version to the optimized version were 47 and
    39% for aufs and overlay, respectively. Reductions with btrfs and devicemapper
    were smaller since they also have different bottlenecks for container scalability.
    We also observed that the processor cycles required by the daemon systemd also
    significantly increased with 1,024 containers. With devicemapper, it consumed
    almost 50% of the total busy cycles on the host Linux. Since the systemd daemon
    also monitors the current status of the mount points, this observation agrees
    with our analysis that the root cause originates from the Linux kernel. SECTION
    V. Related Work Prior performance studies on the container technologies [25]–[27]
    mainly focused on showing that containers have smaller overhead than VMs when
    a single host machine serves multiple workloads: Felter et al. [25] compared the
    performance of Docker containers against the Kernel-based Virtual Machine (KVM).
    Che et al. [26] analyzed three virtualization technologies, i.e., Open Virtuozzo
    (OpenVZ) using the container technology, Xen using para-virtualization, and the
    KVM using full virtualization. Xavier et al. [27] also analyzed the performance
    of multiple container technologies, i.e., Linux-VServer, OpenVZ, and Linux Containers
    (LXC), against the KVM. However, none of the above analyze bottlenecks in container
    management. The performance of VM management has been studied in the context of
    cloud computing [28]–[30]. The primary overhead of provisioning a VM on a cloud
    environment is the throughput of disk and network operations since an image for
    a VM typically contains the entire files installed on an OS. While block-based
    copy-on-write technologies are used to reduce the size of an image, they do not
    achieve the level of reduction comparable to the file-based copy-on-write technologies
    used by container management systems such as Docker. Another overhead in provisioning
    is the time to boot a VM, which is much larger than the time to initialize a container.
    Our analysis of bottlenecks, in particular of idle cycles for core scalability,
    is an extension of the approach by Altman et al. [21]. The primary difference
    is that our analysis is hierarchical, based on a layered performance model [23].
    Our target application, a Docker daemon written in Go, uses much larger numbers
    of idle workers than their target application, a Java EE application server, since
    a goroutine is a user-level thread but a Java thread is typically a native thread.
    With a number of irrelevant idle threads, our hierarchical analysis is useful
    to identify bottlenecks and to obtain insights into their root cause. SECTION
    VI. Conclusion Container management is a new workload for OSs since it intensively
    stresses their administrative interface. The bottlenecks in container management
    with Docker we identified originate from the performance of administrative operations
    such as mounting and unmounting a filesystem, creating and removing a virtual
    block device, inquiring the mount point of a subsystem, and inquiring an attribute
    of a mount point. We identified them by applying our novel approach to analyze
    multiple layers of a software stack hierarchically, which includes a Docker daemon
    implemented as goroutines, which are different from native threads assumed with
    a current bottleneck analysis tool. We validated our analysis results by applying
    them to improve the container scalability for Docker. As Felter et al. [25] summarize
    in their related work section, OS-level virtualization has a long history, and
    it is not new. The new point is its usage as a unit to build responsive microservices
    for agile and continuous development, deployment, and operation. Operating systems
    have room for improvements to more efficiently host those emerging workloads.
    Appendix A Experimental Environment We now describe the measurement environment
    for the experiments discussed in this paper. SECTION A. Machine The machine is
    IBM 2827 zEnterprise EC12 Model HAl with 16-GB memory. The disk storage is IBM
    2421 DS8870 Model 961, connected via four 8-Gb/s Fibre Channel Protocol cards.
    The network card is OSA-Express5S 1000BASE-T Ethernet Copper. SECTION B. Operating
    System The Linux kernel is a custom build of the stable kernel 4.4.4, which is
    extended with AUFS 4.4 and configured with Red Hat Enterprise Linux (RHEL) 7.2.
    The userland tools are from RHEL 7.2. SECTION C. Docker Docker Engine is version
    1.10.3 compiled with a port of the Go programming language to the IBM LinuxONE.
    The graph volume for the storage driver is allocated on a 20-GB direct access
    storage device. SECTION D. Images The Dockerfile used to build the N ginx image
    is a simplified version of that used to build the official image. To build on
    the s390x architecture, we modified the Dockerfile to install only the main package
    from the Debian repository instead of the Nginx repository. The BusyBox image
    is s390x/busybox on DockerHub. When building an image, the base image is already
    available in the Docker daemon. Similarly, when running a container, the corresponding
    image is available in the daemon. SECTION E. Measurement The time spent to execute
    Docker commands was calculated from the difference in the processor cycles in
    /proc/stat from the start to the end of measurement. The stack dump of goroutines
    are generated using the pprof profiler of the Go programming language. When we
    calculate the mean number of goroutines, we periodically dump the stacks of all
    goroutines in the daemon. The busy cycles required in each function were measured
    using the timer mode of the oprofile profiling tool. Trademarks Linux is a registered
    trademark of Linus Torvalds in the United States, other countries, or both. Java
    and all Java-based trademarks and logos are trademarks or registered trademarks
    of Oracle and/or its affiliates. IBM, the IBM logo, and ibm.com are trademarks
    of International Business Machines Corporation, registered in many jurisdictions
    worldwide. Other product and service names might be trademarks of IBM or other
    companies. Authors Figures References Citations Keywords Metrics More Like This
    Kernel Assisted Container Virtualization and Its Application for Containerized
    CUDA Environment 2018 IEEE 20th International Conference on High Performance Computing
    and Communications; IEEE 16th International Conference on Smart City; IEEE 4th
    International Conference on Data Science and Systems (HPCC/SmartCity/DSS) Published:
    2018 A Performance Comparison of Container-Based Virtualization Systems for MapReduce
    Clusters 2014 22nd Euromicro International Conference on Parallel, Distributed,
    and Network-Based Processing Published: 2014 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2016
  relevance_score1: 0
  relevance_score2: 0
  title: Container management as emerging workload for operating systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.procs.2017.05.457
  analysis: '>'
  authors:
  - Ghyzlane Cherradi
  - Adil El Bouziri
  - Azedine Boulmakoul
  - Karine Zeitouni
  citation_count: 12
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract: Keywords References Cited by (18) Procedia Computer Science
    Volume 109, 2017, Pages 982-987 Real-Time HazMat Environmental Information System:
    A micro-service based architecture Author links open overlay panel Ghyzlane Cherradi
    a, Adil El Bouziri a, Azedine Boulmakoul a, Karine Zeitouni b Show more Add to
    Mendeley Share Cite https://doi.org/10.1016/j.procs.2017.05.457 Get rights and
    content Under a Creative Commons license open access Abstract: The dangerous goods
    are important to businesses and the life of a city. Thousands of tons of oil,
    toxic, chemical, corrosive, flammable and radioactive materials are transported
    each day. However, an accident involving hazardous materials could entail serious
    consequences for road users, infrastructure, and the environment. In order to
    build a planning aid system to reduce the risks of transporting hazardous, we
    adopt a microservices-based architecture on a cloud environment. This type of
    architecture consists of a set of loosely coupled and independently deployable
    services for more scalable applications. Due to distributed nature of microservices,
    the system will be developed as a suite of small services aligned with risk management
    process; each service will be running in its own logical machine or container
    technology such as Docker. Previous article in issue Next article in issue Keywords
    Transportation of dangerous goodsenvironmental information systemmicroservices
    based architecturedata collector View PDF References 1 Belotti J, Transport international
    de marchandises, Vuibert 2015. Google Scholar 2 G David Developing Microservices
    with Node.js (1 ed), Packt Publishing (April 26, 2016) 3 Babak M, Microservice
    Architecture Building microservices with JBoss EAP 7. Version 1.0, June 2016.
    Google Scholar 4 H.K. Chan, X Wang Fuzzy Hierarchical Model for Risk Assessment,
    10, Springer, London (2013), pp. 978-979 5 Hyatt N Guidelines for process hazards
    analysis (PHA, HAZOP), hazards identification, and risk analysis, CRC press (2003)
    6 Tomasoni, A. M., Models and methods of risk assessment and control in dangerous
    goods transportation (DGT) systems, using innovative information and communication
    technologies, Ecole Nationale Supérieure des Mines de Paris; Università degli
    studi di Genova-Italie, 2010. Google Scholar 7 T Aven Quantitative risk assessment:
    the scientific platform, Cambridge University Press (2011) Google Scholar 8 S.
    Biass, C. Frischknecht, C. Bonadonna A fast GIS-based risk assessment for tephra
    fallout: the example of Cotopaxi volcano, Ecuador -Part II: vulnerability and
    risk assessment Natural hazards, 64 (1) (2012), pp. 615-639 CrossRefView in ScopusGoogle
    Scholar 9 G. Centrone, R. Pesenti, W. Ukovich Hazardous materials transportation:
    a literature review and an annotated bibliography C. Bersani, A. Boulmakoul, E.
    Garbolino, R. Sacile (Eds.), Advanced Technologies and Methodologies for Risk
    Management in the Global Transport of Dangerous Goods, IOS NATO Science Series
    Book, Amsterdam (2008) Google Scholar 10 Drouin C. & Leroux, D, Transport et Environnement
    : Analyse des risques associés au transport des matières dangereuses en milieu
    urbaine, Congrès de l’Association des transports du Canada, 2004. Google Scholar
    11 Boulmakoul A., Karim L., Laarabi M. H., Sacile R. & Garbolino, E, Mongodb hadoop
    distributed and scalable framework for spatio-temporal hazardous materials data
    warehousing, in 7th International Congress on Environmental Modelling and Software:
    Bold Visions for Environmental Modeling, iEMSs 2014. Google Scholar 12 Diernhofer
    F., Kohl B. & Hörhan R, Risk Assessment of Transport for the Dangerous Goods in
    Austrian Road Tunnels, in 4th International Symposium on Tunnel Safety and Security,
    Frankfurt am Main, Germany, 2010. Google Scholar 13 Viktor F, Automating the Continuous
    Deployment Pipeline with Containerized Microservices, The DevOps 2.0 Toolkit,
    2016. Google Scholar 14 Bob F, Microservices, IoT and Azure: Leveraging DevOps
    and Microservice Architecture to deliver SaaS Solutions, Apress; 1st ed., 2015
    Google Scholar 15 Karagiannis V, Chatzimisios P, Vazquez-Gallego F, Alonso-Zarate
    J, A Survey on Application Layer Protocols for the Internet of Things, Transaction
    on IoT and Cloud Computing, 2015. Google Scholar 16 Foster A., A Comparison between
    DDS, AMQP, MQTT, JMS, REST and CoAP, Version 1.4, January 2014. Google Scholar
    17 Cherradi G, El Bouziri A, Boulmakoul A, Smart Data Collection Based on IoT
    Protocols, JDSI’16, ISSN 2509-2103, 2016. Google Scholar 18 J Gubbi, R. Buyya,
    S Marusic, M Palaniswami Internet of Things (IoT): A vision, architectural elements,
    and future directions Future Generation Computer Systems, 29 (2013), pp. 1645-1660
    View PDFView articleView in ScopusGoogle Scholar 19 Amirian P, Winstanley A. &
    Basiri A, NoSQL storage and management of geo-spatial data with emphasis on serving
    geospatial data using standard geospatial web services, 2013. Google Scholar 20
    S Holmes Getting MEAN with Mongo, Express, Angular, and Node, Manning Publications
    (2015) Google Scholar 21 Cherradi G, El Bouziri A, Boulmakoul A, Environmental
    Information System for HazMat Transportation and Risk Assessment, INTIS 2016,
    ISBN 978-9954-34-378-4, ISSN 2351-9215, 2016. Google Scholar 22 Aloha (Areal location
    of hazardous atmospheres), technical documentation 2013. Google Scholar Cited
    by (18) Microservice based scalable IoT architecture for device interoperability
    2023, Computer Standards and Interfaces Citation Excerpt : Microservice-based
    IoT architecture can enable creating a set of atomic services for some specific
    IoT functionality. These functionality specific services can be of various types
    such as: a set of services could be devised to collect sensor and controller details
    and present it in the form of data instead of devices [5], whereas another set
    of microservice can process the IoT sensor data by applying the constraints defined
    on that data [10]. Therefore, granular control is attainable for individual IoT
    services as the corresponding microservices are independent of each other. Show
    abstract Design and implementation of a smart beehive and its monitoring system
    using microservices in the context of IoT and open data 2022, Computers and Electronics
    in Agriculture Citation Excerpt : Therefore, the deployment operation should be
    performed flexibly. Thus, microservices architecture solves the inflexible deployment
    problems (Soldani and Tamburri, 2018; Khomh and Amirhossein Abtahizadeh, 2018;
    Cherradi et al., 2017; Taibi and Lenarduzzi, 2018) in the context of beehive monitoring
    systems. Each microservice has been developed as autonomous services or APIs based
    on REST architectural style that utilizes the HTTP protocol and based on GET,
    PUT, POST, and DELETE HTTP verbs. Show abstract Deployment of Future Services
    in a Multi-access Edge Computing Environment Using Intelligence at the Edge 2023,
    Journal of Network and Systems Management TRiP: a transfer learning based rice
    disease phenotype recognition platform using SENet and microservices 2023, Frontiers
    in Plant Science Joint optimization of delay and cost for microservice composition
    in mobile edge computing 2022, World Wide Web Smart COVID-19 GeoStrategies using
    Spatial Network Voronoï Diagrams 2022, Machine Learning and Deep Learning in Medical
    Data Analytics and Healthcare Applications View all citing articles on Scopus
    © 2017 The Author(s). Published by Elsevier B.V. Part of special issue 8th International
    Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th
    International Conference on Sustainable Energy Information Technology, SEIT 2017,
    16-19 May 2017, Madeira, Portugal Edited by Elhadi Shakshuki Download full issue
    Other articles from this issue Methodology for identifying activities from GPS
    data streams 2017 Vladimir Usyukov View PDF Data Replica Placement Mechanism for
    Open Heterogeneous Storage Systems 2017 X. Xu, …, J. Shao View PDF Agent-based
    Modeling of Pedestrian Behavior at an Unmarked Midblock Crossing 2017 Khaled Shaaban,
    Karim Abdel-Warith View PDF View more articles Recommended articles Article Metrics
    Citations Citation Indexes: 18 Captures Readers: 101 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '>'
  journal: Procedia computer science
  limitations: '>'
  pdf_link: null
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: 'Real-Time HazMat Environmental Information System: A micro-service based
    architecture'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/infocomwkshps50562.2020.9163068
  analysis: '>'
  authors:
  - Qian Qu
  - Ronghua Xu
  - Seyed Yahya Nikouei
  - Yu Chen
  citation_count: 14
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >IEEE INFOCOM 2020 - IEEE Conf...
    An Experimental Study on Microservices based Edge Computing Platforms Publisher:
    IEEE Cite This PDF Qian Qu; Ronghua Xu; Seyed Yahya Nikouei; Yu Chen All Authors
    15 Cites in Papers 1113 Full Text Views Abstract Document Sections I. Introduction
    II. Related Work III. Microservices Benchmark Evaluation IV. Experimental Analysis
    V. Conclusions Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: The rapid technological advances in the Internet of Things (IoT) allows
    the blueprint of Smart Cities to become feasible by integrating heterogeneous
    cloud/fog/edge computing paradigms to collaboratively provide variant smart services
    in our cities and communities. Thanks to attractive features like fine granularity
    and loose coupling, the microservices architecture has been proposed to provide
    scalable and extensible services in large scale distributed IoT systems. Recent
    studies have evaluated and analyzed the performance interference between microservices
    based on scenarios on the cloud computing environment. However, they are not holistic
    for IoT applications given the restriction of the edge device like computation
    consumption and network capacity. This paper investigates multiple microservice
    deployment policies on edge computing platform. The microservices are developed
    as docker containers, and comprehensive experimental results demonstrate the performance
    and interference of microservices running on benchmark scenarios. Published in:
    IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM
    WKSHPS) Date of Conference: 06-09 July 2020 Date Added to IEEE Xplore: 10 August
    2020 ISBN Information: DOI: 10.1109/INFOCOMWKSHPS50562.2020.9163068 Publisher:
    IEEE Conference Location: Toronto, ON, Canada SECTION I. Introduction While cloud
    computing has changed human''s life by providing service oriented architecture
    (SOA) as a service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service
    (IaaS) [10], the proliferation of the Internet of Things (IoT), Artificial Intelligence
    (AI) and edge computing technologies leads us enter the era of post-cloud computing
    [27]. According to the estimation of Cisco Global Cloud Index, the total amount
    of data created by any device will reach 847 Zettabytes (ZB) annually by 2021
    while the Internet traffic will reach 19.5 ZB by then [11]. Some of the produced
    data might require timeliness, involve privacy or cause unpredictable impact on
    the network. For example, in applications like smart traffic lights, real-time
    urban surveillance, cloud computing is not able to serve the purpose well [3],
    [5]. Rapid technological advances in cloud computing and Internet of Things (IoT)
    make Smart Cities feasible by integrating heterogeneous computing devices to collaboratively
    provide variant pervasively deployed smart services [19], [20], in which the capability
    of data collection and processing at the edge is the key [18]. Migrating lower-level
    data processing tasks to edge devices enables the system to meet the requirements
    for delay-sensitive, mission-critical applications, including smart urban surveillance
    [4], instant privacy protection [8], [9], real-time public safety [34], etc. However,
    the heterogeneity and resource constraint at edge necessitate a scalable, flexible
    and lightweight system architecture that supports fast development and easy deployment
    among multiple service providers. Because of many attractive features, such as
    good scalability, fine granularity, loose coupling, continuous development, and
    low maintenance cost, the microservices architecture has emerged and gained a
    lot of interests both in industry and academic community [17], [21]. Compared
    to traditional SOAs in which the system is a monolithic unit, the microservices
    architecture divides an monolithic application into multiple atomic microservices
    that run independently on distributed computing platforms. Each microservice performs
    one specific sub-task or service, which requires less computation resource and
    reduces the communication overhead. Such characteristics make the microservices
    architecture an ideal candidate to build a flexible platform, which is easy to
    be developed and maintained for cross-domain applications like smart public safety,
    smart traffic surveillance systems, etc [3], [20]. Motivated by the observations
    obtained in our previous research [21], [33], this work tries to answer three
    questions: Is it suitable to run multiple microservices inside one container just
    considering the performance of the edge device? Which type of microservices with
    different resource consuming inclination could be put together if the answer is
    yes? Is the effect of interference that executes multiple microservices running
    on fog computing and edge computing scenarios different? and What is the trade-off
    between the “one process per container” rule and the existing limitation of resource
    at edge side, such as computation and memory? The rest of this paper is organized
    as follows. Section II discusses the current arts of research on microservices
    technology and performance evaluation. The microservices deployment policy and
    performance matrix are explained in Section III. Given comprehensive experimental
    results, the performance evaluations are discussed in Section IV. Finally, a summary
    is presented in Section V. SECTION II. Related Work A. Microservices Architecture
    Microservices architectures (MSA) are extensions of a Service Oriented Architecture
    (SOA) [30]. The traditional SOA uses a monolithic architecture that constitutes
    different software features in a single interconnected application and database.
    Relaying on the tightly coupled dependence among functions and components, it
    is more challenging to adapt to new requirements in an IoT-enabled system than
    the SOA, which requires scalability, service extensibility, and cross-platform
    interoperability [6]. The main difference between MSA and SOA is that a MSA is
    more decentralized and it distributes all the logic (such as routing, message
    parsing) in to smart end points 1. MSA also adopts a light weight applications
    programming interface (API) gateway for managing services instead of using heavier
    and more sophisticated Enterprise Service Bus [14]. Each microservice runs its
    own process and communicates with peers using light weight communication mechanisms
    (REST API or SOAP protocol [15], etc). The services are built around business
    capabilities and independently deployed by fully automated deployment tools [22].
    Fine granularity and loose coupling are two significant features in MSA [36].
    Fine granularity means that there is a bare minimum of centralized management
    of these services. Moreover, there are instances where the service is not micro
    [26]. Loose coupling implies that each of microservices components has few dependencies
    on other separate components, which makes the deployment and development of Micro-services
    more independent [23]. Granularity should be considered from the standpoint of
    its eventual impact on service performance as the decision process is influenced
    by numerous environmental factors when deploying MSA [24]. B. Performance Evaluation
    Container technology offers a more lightweight method to abstract the applications
    from the system environment which allows the microservices to be deployed quickly
    but also consistently. Compared to VMs, containers not only provide all the libraries
    and other dependencies but also consume less resource and produce lower overhead
    [31], [32]. Applications with different dependencies are wrapped into a container
    image and shared to various users. These features allows the applications to be
    run at the edge of the network. Ruchika et al. [29] evaluated the feasibility
    of using container technology like docker for IoT applications. Recent studies
    compared the performance of containers to virtual machines and showed that in
    most occasions containers work better than or almost equal to virtual machines.
    The IBM Research Division measured the performance of Docker in terms of CPU,
    memory, disk I/O and compared the result with KVM [7]. The research showed that
    in all evaluated cases container works better than VMs. In a study researchers
    have compared the performance of Docker with VMs when running the Spark applications
    [2]. They claimed that Spark works better with docker for calculation intensive
    applications. Similar work has been done in big data area with the interference
    caused by neighbor containers running big data microservices [35]. The performance
    of different single-board computer devices are evaluated by deploying containerized
    benchmarks [16], which focused on hardware comparison and the choice of one device
    rather than another, however, it did not pay much attention on container analysis.
    An optimized version of Docker called Docker-pi was proposed for container deployment
    on single-board machines to speed up docker deployment [1]. Researchers have studied
    the performance of collocated microservices running on a common host and compared
    the result in container with VMs [25]. Evaluation is also reported on how microservices
    are interfered with each other in cloud computing environment [13], in which the
    performances between processed in a container and the situation where they follow
    the “one process per-container” rule are compared. The conclusion was that putting
    microservices inside one container has a positive effect to the overall performance.
    However, the work is based on a cloud computing platform. A survey of VM management
    lists the virtualization frameworks for edge computing and elaborates the techniques
    tailored for it [28]. Focusing on Docker, a container based technology as an edge
    computing platform, researchers evaluated four criteria and conclude that Docker
    is a viable candidate [12]. To the best of our knowledge, there is not a reported
    effort that evaluates the interference effect of containers in the edge computing
    environment. The trade-off between lower overheads and the “one process per container”
    rule seems more important considering the resource limitation at the edge. SECTION
    III. Microservices Benchmark Evaluation For microservices development, we choose
    Docker container owing to popularity and familiarity in container community. Docker
    wraps up the application with all its dependencies into a container so that it
    can easily be executed on any Linux server (on-premise, bare metal, public or
    private cloud). It uses layered file system images along with the other Linux
    kernel features (namespace and cgroups) for the management of containers. This
    feature allows Docker to create any number of containers from a base image, which
    are copies of the base image wrapped up with additional features. This also reduces
    the overall memory and storage requirements that eases fast startup. To measure
    the performance of the containerized microservices deployment policy, several
    widely used benchmarks are considered in designing the microservices benchmark
    based on typical computing paradigms used in the smart public safety system at
    the edge. The key metrics of the benchmark are defined as following: CPU performance:
    To evaluate the performance of the CPU, we chose the Linpack benchmarks which
    measure the system''s floating point computing power by solving linear equations.
    And the capability of the CPU is measured in terms of FLOPS (floating point operations
    per second). Memory performance: We chose the STREAM benchmarks to measure the
    performance of the memory. STREAM is a simple synthetic benchmark program that
    measures sustainable memory bandwidth (in MB/s) and the corresponding computation
    rate for simple vector kernels. Disk performance: To measure the disk I/O capability,
    we chose the Bonnie++ benchmark which gives the results in terms of input, output,
    rewrite (in Kb/s) and seeks (in per second). For each metric evaluation, the containerized
    microservices are deployed both on fog (desktop) side and edge (Raspberry Pi 4)
    side. Given above defined benchmarks, the performance and the interference effect
    of the containerized microservices are evaluated based on a set of microservice
    deployment policies. Figure 1 illustrates the whole containerized benchmark evaluation
    workflow, and the microservice deployment policies are described as following
    four cases: Case 1: One microservice is developed as single container and only
    one container is running on host machine. Since we would put multiple microservices
    in one container later, the system resource is constrained by using the control
    groups or Cgroups, a Linux kernel feature. Such case is considered as a baseline
    for the entire evaluation. Case 2: Multiple microservices are developed as single
    container and only one container is running on host machine. In this case we do
    not employ Cgroups so the microservices can have access to all the system resources.
    This case is used to evaluate resource competition caused by wrapping multiple
    microservices into one container. Case 3: Each container only holds single microservice
    and multiple containers are running on host machine without employing Cgroups.
    This case could evaluate resource competition caused by multiple containers deployed
    on single host platform. Case 4: Each container only holds single microservice
    and multiple containers are running on host machine. Like case 1, this case employs
    Cgroups to limit the system resource during test. Fig. 1. Containerized benchmark
    evaluation workflow. Show All SECTION IV. Experimental Analysis A. Experimental
    Setup A concept-proof prototype system has been implemented on a simulating SPS
    scenarios. Table I describes configurations of experimental testbed. The desk
    top simulates fog node while Raspberry Pi 4 acts as edge node. Table I Configuration
    of experimental testbed. For workload size and configuration on fog, we provide
    a specific configuration for each microservice. For Linpack, we consider the matrix
    of size N as 15000. We also considered the problem size (number of equations to
    solve) as 15000. We configure the STREAM by setting the array size as 60M and
    DNTIMES as 200. The total memory requirement for this configuration is 1373.3
    MiB. For Bonnie++, we considered the file size as 8192 MB and set the uid to use
    as root. For workload size and configuration on edge (Raspberry Pi 4) side, each
    microservice, we provide a specific configuration. For Linpack, we consider the
    matrix of size N as 15000. We also considered the problem size (number of equations
    to solve) as 15000. We configure the STREAM by setting the array size as 30M and
    DNTIMES as 100. The total memory requirement for this configuration is 1373.3
    MiB. For Bonnie++, we considered the file size as 4096 MB and set the uid to use
    as root. The experimental test is conducted based on performance of benchmarks
    and microservices deployment policies. For the ease of presentation of the results,
    we used the following abbreviations for the microservices used in the simulation
    of SPS system, they are namely Linpack (L), STREAM (S), and Bonnie++ (B). To get
    experimental results, for case 1, each microservice (L, S and B) is performed
    30 iterations. For case 2, case 3 and case 4, since the microservices are in all
    possible combinations and corresponding isolated running time of the containers
    are not identical, it is not suitable to repeat the process for a certain number
    of iterations. Hence, for these cases, we keep the container running for a certain
    period of time (60 minutes) and compute the average performance. B. Evaluation
    on Fog Side In this paper we define a desktop as the fog side, the most usual
    physical machine in a LAN system and the role of a manager of the other edge computing
    device in our IoT system. 1. CPU Performance and Analysis According to the CPU
    architecture, we chose the Linpack version for Intel and implemented it in Docker.
    We ran the container(s) and recorded the performance of Linpack microservice as
    described in Section III. Figure 2 shows the average CPU performance of running
    Linpack in terms of GFLOPS. Compared with case 1, all combination of microservices
    have more computation overhead on host performance. The L+B in case 2 has the
    least impact on CPU performance while L+L in case 3 is the worst result that has
    64% degradation compared to the baseline L in case 1. Owing to higher computing
    operations, like float calculation, used by Linpack (L), so L+L combination shows
    worst CPU performance in all deployment policies. Furthermore, in case 3, control
    groups (Cgroups) are disabled so that containers have to compete for the system
    resource. Therefore, test results in case 4 have better performance than those
    in case 3. 2. Memory Performance and Analysis To evaluate the memory performance,
    we wrapped the STREAM benchmark inside a Docker container. The STREAM benchmark
    employs four vector operations and gives a result of bandwidth. The average result
    of all the four vector operations (COPY, SCALE, ADD and TRIAD) demonstrate similar
    shapes in bar chart, so we just present the result of COPY operation as memory
    performance. As shown in Fig. 3, except for combination S+S in case 2, the others
    have limited performance deduction compared to the baseline in case 1. Given the
    results, wrapping memory type workloads microservices as one container introduce
    higher memory overhead on fog computing environment. Similar to the results in
    CPU performance, all the combinations show best performance in case 4 when Cgroups
    is enabled. 3. Disk I/O Performance and Analysis The Bonnie++ bench-mark evaluates
    various performance parameters in which only sequential block input and output
    scenarios are considered during test. The performance evaluation is based on I/O
    throughput in terms of MB/s. Figure 4 shows that the input performance is better
    than output performance on fog node. The results show the similar interference
    pattern that running multiple instances of Bonnie++ leads to lower I/O throughput,
    no matter cases that are deployed in a single container or separate containers.
    From case 2 to case 4, the performance is nearly equal to the baseline case 1
    except for the combination of B+B. In our IoT system, for instance, a local smart
    surveillance, a fog device plays the role of managing and monitoring the performance
    of the edge device like smart cameras. Consider that the nodes could be numerous,
    the fog device is more sensitive to the granularity of the containers. Based on
    the evaluation results, generally it seems more practical to reduce the number
    of containers which are CPU consuming inclined. Fig. 2. CPU performance on fog.
    Show All Fig. 3. Bandwidth of COPY on fog. Show All Meanwhile we should avoid
    deploying same type microservices together. The evaluation gives a general direction
    of setting up the fog device, we should adjust the deployment of the specified
    microservices according to the actual situation of the system and capability of
    the hardware. C. Evaluation on Edge Side The experimental process on edge (Raspberry
    Pi) is quite the same as fog side. However, given limited resource on edge node,
    we set the time interval to 120 minutes for the following evaluations. 1. CPU
    Performance and Analysis Since the CPU architecture of Pi 4 is ARM, we picked
    another Linpack version and adjusted the codes to the system environment. Then
    We containerized the benchmark and ran experiments as per the various case described
    in Section III. Figure 5 shows the CPU performance in terms of MFLOPS. Similar
    to the results on fog side, all the combinations have a significant impact on
    the computing performance of host except for the combination of L+B. The combination
    of L+S in case 2 has worst performance, so it causes 41% degradation compared
    to the baseline of case 1. But all three combinations demonstrate the best performance
    in case 4, knowing the fact that the microservices are separated in two containers
    with the control group enabled. This is quite different from the case in physical
    machine where L+B in case 4 did not give a good performance. Fig. 4. a) Performance
    of input on fog; b) Performance of output on fog. Show All Fig. 5. CPU performance
    on edge. Show All Fig. 6. Bandwidth of COPY on edge. Show All The results also
    indicate that computing type workloads tend to have the worst performance when
    working with memory consuming ones when they are wrapped into a single container
    with cgroups employed. And it shows that CPU consuming microservice works well
    with disk I/O workloads and the performance is much closer to the baseline relatively
    compared to the fog environment. In addition, as Fig. 5 shows, case 4 has better
    performance than case 3 while running all combinations. The reason behind is that
    in case 3 Cgroups are disabled so the containers have to compete for the system
    resource, however, the containers have no interference with each other in case
    4 when Cgroups are enabled. 2. Memory Performance and Analysis Similar to fog
    side, only the of COPY operation is used for evaluation for memory performance
    in terms of GB/s. As shown in Fig. 6, all four vector operations generate almost
    the same shapes in charts. Compared to the baseline in case 1, the results shows
    that all combinations have limited performance reduction except for combination
    S+L in case 2. Hence, enclosing memory type workloads inside one container as
    microservice is not suggested on edge computing platform. Like the results in
    CPU performance, the combinations work best in case 4 that microservices are separated
    in containers with resource control. 3. Disk I/O Performance and Analysis To evaluate
    Disk I/O performance on edge device, we used Bonnie++ microservice that creates
    a large file at least twice the size of inbuilt memory, and only sequential block
    input and output are considered during the test. The throughput performance of
    running microservices on edge is shown in Fig. 7. The result shows the similar
    interference pattern indicating that running multiple instances of Bonnie++ creates
    higher resource contention when they are deployed either in a single container
    or separate containers. The results also indicate that the performance of combinations
    is nearly equal to the baseline performance except for the case of running multiple
    instances of Bonnie++ (B+B). All the combinations of deployment policies show
    very small deviation from the baseline of case 1 except for case of (B+B). Basically
    in our IoT system, the edge device like Raspberry Pi suffer from the limited resource
    so we should carefully consider the combination of microservices or simply isolate
    some inside a single container. Based on the results, generally we should deploy
    the CPU consuming inclined microservice alone, in smart surveillance system for
    example, the Computer Vision(CV) type containerized services. And it is considerable
    to wrap memory and disk inclined ones together to reduce the overheads and spare
    more system resources. Fig. 7. a) Performance of input on edge; b) Performance
    of output on edge. Show All SECTION V. Conclusions In this paper, a set of microservice
    deployment policies are proposed and performance matrix are considered. To evaluate
    the microservice deployment policy, a simulating test is developed and tested
    both on fog and edge computing platform. Given the experimental results, we conclude
    as following: Under edge computing environment, running multiple microservices
    in a single container is an option considering the comparable performance (except
    for certain cases) if we ignore the “one process per container” rule. The story
    behind the rule is that when deploy multiple process such as microservices in
    a single container we may face operations problem. For instance, we may have to
    replace the whole container if one of the microservices need updates which could
    be a disaster in a large scale. Considering the resource limitation, however,
    sometimes we have no choices. Memory intensive microservices are not heavily effected
    except for competing with same type ones inside one container. Disk I/O microservices
    is quite the same situation, however, both of the two kinds have a significant
    effect to the CPU intensive process. This is a preliminary study toward a microservices
    architecture based smart public safety as an edge service. Our ongoing efforts
    include building a larger scale edge surveillance system with 32 smart cameras,
    which will enable a more comprehensive study for deeper insights. Authors Figures
    References Citations Keywords Metrics Footnotes More Like This Design and Implementation
    of the Container Terminal Operating System Based on Service-Oriented Architecture
    (SOA) 2008 International Conference on Cyberworlds Published: 2008 MOMCC: Market-oriented
    architecture for Mobile Cloud Computing based on Service Oriented Architecture
    2012 1st IEEE International Conference on Communications in China Workshops (ICCC)
    Published: 2012 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: An Experimental Study on Microservices based Edge Computing Platforms
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/bmsb.2018.8436912
  analysis: '>'
  authors:
  - Julien Lallet
  - Andrea Enrici
  - Anfel Saffar
  citation_count: 11
  full_citation: '>'
  full_text: '>

    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create
    Account Personal Sign In Browse My Settings Help Institutional Sign In All Books
    Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED
    SEARCH Conferences >2018 IEEE International Sympo... FPGA-Based System for the
    Acceleration of Cloud Microservices Publisher: IEEE Cite This PDF Julien Lallet;
    Andrea Enrici; Anfel Saffar All Authors 12 Cites in Papers 686 Full Text Views
    Abstract Document Sections I. Introduction II. Related Work III. Shared FPGA for
    the Acceleration of Cloud Microservices IV. Case Study V. Conclusion Authors Figures
    References Citations Keywords Metrics Abstract: Scalability, distributivity, interoperability
    and modularity introduced in cloud computing have deeply changed the legacy data
    center''s architecture, implementation and processing capabilities. The atomic
    network services offered by cloud architectures are called microservices. Unlike
    virtual machines, microservices can be implemented in the form of low resources
    footprint applications as containers (Docker, LXC etc.) or even smaller as unikernels
    (IncludeOS, ClickOS, Rumprun, HermitOS etc.). The need to efficiently offload
    the processing of computation-intensive applications has motivated the introduction
    of Field Programmable Gate Arrays (FPGA) boards in servers. FPGAs can nowadays
    be considered as cloud-standard processing resources. However, in today''s cloud
    data centers, FPGAs cannot be accessed to run concurrent microservices. This severely
    limits the efficient deployment of microservices. This paper aims at introducing
    an FPGA-based system for the concurrent acceleration of cloud-native microservices
    onto FPGAs. Published in: 2018 IEEE International Symposium on Broadband Multimedia
    Systems and Broadcasting (BMSB) Date of Conference: 06-08 June 2018 Date Added
    to IEEE Xplore: 16 August 2018 ISBN Information: Electronic ISSN: 2155-5052 DOI:
    10.1109/BMSB.2018.8436912 Publisher: IEEE Conference Location: Valencia, Spain
    I. Introduction In the last decade, successive transformations in the infrastructure
    and architecture of legacy data centers have lead to a new type of cloud systems.
    Scalability, distributivity, interoperability, modularity are some of the main
    characteristics which make today''s data centers cloud systems. To reach this
    target, it has been necessary to invent and develop new tools with new features
    as management and orchestration softwares [1]–[6]. The features proposed by these
    tools mainly revolve around the software virtualization of the processing elements
    that compose servers in data centers. While virtualization works well for general-purpose
    applications (e.g., office applications, messaging etc.), in the case of time-critical
    applications it hinders the overall system''s performance. The need for efficient
    co-processing for general-purpose CPUs was first filled by the use of Graphic
    Processing units (GPU). Originally designed for video processing offloading, GPU
    architectures are perfect for image and video processing. However, for other kind
    of processing as signal processing, ciphering, data mining etc., GPU won''t improve
    performances. Most likely, performances may even decrease due to the specificity
    of the GPU''s architectures [7]. To achieve the required processing efficiency,
    cloud resource providers as Amazon [8] or Microsoft [9] have integrated Field
    Programmable Gate Arrays (FPGA) in their data centers. FPGAs can be configured
    to process specific applications, as this is the case for Application Specific
    Integrated Circuits (ASIC), except that FPGAs can be reprogrammed at runtime to
    modify the functionality of the hardware circuitry. Using FPGAs in cloud infrastructures
    allows to fill the gap between the processing flexibility offered by GPPs and
    the processing efficiency achieved by ASICs. In most of today''s cloud infrastructures,
    FPGAs are used either as full computing resources for the cloud infrastructure
    itself, as this is the case for Microsoft, or, as for almost all other cloud providers,
    are made available for end-users as processing resources. Nevertheless, it is
    our belief that FPGAs, as used today, do not exactly follow the microservices
    and cloud paradigms. The microservice approach allows to simplify complicated
    software systems by breaking them into subcomponents and distributing these components
    across many computing servers. In this approach, an application consists of many
    small independent services where each service is running on its own process. The
    introduction of microservices in cloud infrastructure supports modularity, flexibility
    and distributed software components. Indeed, FPGAs available today in a cloud
    infrastructure are not concurrently shared among running microservices on the
    same host as it is the case for memory or Central Processing Unit (CPU) resources.
    In order to gain cloud paradigms to accelerated applications, dedicated support
    and services must be provided by the underlying hardware platform to share processing
    elements without compromising performance. Sign in to Continue Reading Authors
    Figures References Citations Keywords Metrics More Like This Hardware Acceleration
    for Finite Element Electromagnetics: Efficient Sparse Matrix Floating-Point Computations
    with Field Programmable Gate Arrays 2006 12th Biennial IEEE Conference on Electromagnetic
    Field Computation Published: 2006 Field programmable gate array acceleration of
    bio-inspired optimization techniques for phased array design 2009 IEEE Antennas
    and Propagation Society International Symposium Published: 2009 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2018
  relevance_score1: 0
  relevance_score2: 0
  title: FPGA-Based System for the Acceleration of Cloud Microservices
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
