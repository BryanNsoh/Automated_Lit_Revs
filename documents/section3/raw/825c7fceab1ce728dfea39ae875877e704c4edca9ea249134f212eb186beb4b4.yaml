- DOI: https://doi.org/10.1109/access.2020.3010896
  analysis: '>'
  authors:
  - Ian F. Akyildiz
  - Avinash C. Kak
  - Shuai Nie
  citation_count: 604
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 8 6G and
    Beyond: The Future of Wireless Communications Systems Publisher: IEEE Cite This
    PDF Ian F. Akyildiz; Ahan Kak; Shuai Nie All Authors 617 Cites in Papers 40237
    Full Text Views Open Access Comment(s) Under a Creative Commons License Abstract
    Document Sections I. Introduction II. Use Cases III. Terahertz Band Communications
    IV. Intelligent Communication Environments V. Pervasive Artificial Intelligence
    Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes
    Abstract: 6G and beyond will fulfill the requirements of a fully connected world
    and provide ubiquitous wireless connectivity for all. Transformative solutions
    are expected to drive the surge for accommodating a rapidly growing number of
    intelligent devices and services. Major technological breakthroughs to achieve
    connectivity goals within 6G include: (i) a network operating at the THz band
    with much wider spectrum resources, (ii) intelligent communication environments
    that enable a wireless propagation environment with active signal transmission
    and reception, (iii) pervasive artificial intelligence, (iv) large-scale network
    automation, (v) an all-spectrum reconfigurable front-end for dynamic spectrum
    access, (vi) ambient backscatter communications for energy savings, (vii) the
    Internet of Space Things enabled by CubeSats and UAVs, and (viii) cell-free massive
    MIMO communication networks. In this roadmap paper, use cases for these enabling
    techniques as well as recent advancements on related topics are highlighted, and
    open problems with possible solutions are discussed, followed by a development
    timeline outlining the worldwide efforts in the realization of 6G. Going beyond
    6G, promising early-stage technologies such as the Internet of NanoThings, the
    Internet of BioNanoThings, and quantum communications, which are expected to have
    a far-reaching impact on wireless communications, have also been discussed at
    length in this paper. The envisioned key enabling technologies for 6G and beyond
    wireless communications systems. Published in: IEEE Access ( Volume: 8) Page(s):
    133995 - 134030 Date of Publication: 21 July 2020 Electronic ISSN: 2169-3536 DOI:
    10.1109/ACCESS.2020.3010896 Publisher: IEEE Funding Agency: CCBY - IEEE is not
    the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/
    to obtain full-text articles and stipulations in the API documentation. SECTION
    I. Introduction Wireless communication systems have experienced substantial revolutionary
    progress over the past few years. Various stakeholders, including commercial solutions
    providers, academic research groups, standards bodies, and end-users, have all
    greatly benefited from the radical changes led by the most recent 5G developments,
    which include paradigm-defining techniques such as network softwarization and
    virtualization, massive MIMO, ultra-densification, and the introduction of new
    frequency bands. Numerous burgeoning applications and verticals, including virtual
    and augmented reality (VAR), e-commerce, contactless payment, machine-to-machine
    communications, and enhanced mobile broadband, among others, have demonstrated
    the vast potential of 5G, which continues to evolve and adapt to a wide variety
    of emerging use cases. However, as societal needs continue to evolve, there has
    been a marked rise in a plethora of emerging use cases that cannot be served satisfactorily
    with 5G. For example, the next generation of VAR, i.e., holographic teleportation,
    requires Tbps-level data rates and microsecond-level latency, which cannot be
    achieved with even the millimeter wave (mmWave) frequency bands within 5G. Further,
    increasing industrial automation and the move from Industry 4.0 to the upcoming
    Industry X.0 paradigm will push connectivity density well beyond the 10 6 k m
    2 metric that 5G is designed for, in addition to requiring an overhaul of existing
    network management practices. Further, an increase in the connection density will
    also result in demands for improved energy efficiency, which 5G is not designed
    for. Consequently, the research community has gravitated towards addressing the
    aforementioned major challenges, and we posit that ongoing research in the domains
    of terahertz band communications, intelligent surfaces and environments, and network
    automation, for example, may very well hold the key to the future of wireless.
    To this end, an amalgamation of societal needs and technological breakthroughs
    that serve to enable those needs are the key drivers for a generational leap beyond
    existing wireless systems. Together, these factors make a strong case for a focused
    discourse on the next frontier in wireless communications, i.e., 6G systems. We
    envision that 6G will not only enable a pervasively intelligent, reliable, scalable,
    and secure terrestrial wireless network, but will also incorporate space communications
    to form an omnipresent wireless network, in keeping with the need for true wireless
    ubiquity. This paper details our vision for the future of wireless communications,
    highlighting emerging use cases and detailing the key enabling technologies that
    are essential to the realization of 6G. We begin our discussion on 6G by formally
    introducing the key performance indicators (KPIs) that are expected to guide the
    design of 6G systems. While the ITU Telecommunication Standardization Sector (ITU-T)
    is working on a set of official recommendations for the KPI metrics, the tentative
    values have appeared in the public domain recently [1]. Table 1 presents these
    values and contrasts them against the metrics associated with 5G. These KPIs serve
    as the fundamental metrics to evaluate system performance. In particular, from
    the table, we note the following classes of KPIs: System Capacity: This class
    of KPIs primarily deals with metrics that are associated with system throughput.
    These include peak data rate, experienced data rate, peak spectral efficiency,
    experienced spectral efficiency, maximum channel bandwidth, area traffic capacity,
    and connection density. Within this context, the experienced data rate and spectral
    efficiency metrics refer to the values that should be guaranteed to 95% of all
    user locations [1]. System Latency: This class of KPIs includes the end-to-end
    latency metric, along with delay jitter. We note that jitter is a new KPI for
    6G that quantifies the latency variations in the system, and is absent from 5G.
    System Management: This class of KPIs primarily deals with metrics related to
    the management and orchestration of networks such as energy efficiency, reliability,
    and mobility. Here too we note that while 5G does not specify a target KPI for
    the energy efficiency metric, 6G introduces a target energy efficiency of 1 Tb/J.
    Achieving the KPIs highlighted in Table 1 will require revolutionary breakthroughs
    across all domains of wireless communications. In particular, we identify the
    following major thrusts: New Spectrum Usage and Radio Design Paradigms: While
    5G ensured the mainstream adoption of mmWave spectrum, the need for higher data
    rates and consequently larger channel bandwidths will necessitate the incorporation
    of terahertz (THz) and sub-THz spectrum within 6G. At the same time, the opening
    up of new spectrum bands will also require novel radio designs that can simultaneously
    sense and communicate over the entire EM spectrum. Novel Network Architectures:
    The classical cell-based architecture of wireless networks cannot scale to meet
    the area traffic capacity and connection density requirements put forth by 6G.
    Instead, 6G will need to incorporate communications infrastructure into the very
    fabric of the environment. Increasing Intelligence and Automation: The strict
    spectral efficiency, reliability, and latency requirements associated with 6G
    imply that manual configuration of the network will no longer be possible. Rather,
    network intelligence and automation will occupy centre stage, helping build an
    increasingly autonomous network. Enhancing Network Coverage Beyond the Terrestrial
    Domain: In order to achieve true wireless ubiquity, 6G will need to expand beyond
    terrestrial networks, incorporating both near-Earth as well as deep-space connectivity.
    TABLE 1 The Evolution From 5G to 6G Wireless Systems [1] Towards the fulfillment
    of this grand vision, we note that several enabling solutions have been conceived
    and are being actively studied. As shown in Figure 1, these technologies include:
    (i) a network operating at the THz band with abundant spectrum resources, (ii)
    intelligent communication environments that enable a wireless propagation environment
    with active signal transmission and reception, (iii) pervasive artificial intelligence,
    (iv) large-scale network automation, (v) an all-spectrum reconfigurable front-end
    for dynamic spectrum access, (vi) ambient backscatter communications for energy
    savings, (vii) the Internet of Space Things enabled by CubeSats and UAVs, and
    (viii) cell-free massive MIMO communication networks. We also make note of three
    very promising technologies that are expected to shape the future of communications,
    yet will not be sufficiently mature for 6G. These include: (i) the Internet of
    NanoThings, (ii) the Internet of BioNanoThings, and (iii) quantum communications.
    Further, in addition to the aforementioned key technologies, holistic security
    solutions will also be vital to the success of 6G. However, these fall outside
    the purview of this paper. FIGURE 1. The envisioned key enabling technologies
    for 6G and beyond wireless communications systems. Show All Further, we posit
    that while prior art concerning 6G wireless systems has been become increasingly
    commonplace over the past year [2]–[7], a majority of the past publications in
    this domain focus on a select few topics, potentially hampering a comprehensive
    overview in terms of key aspects that are expected to shape the future of wireless
    communications. In addition, discussion concerning beyond 6G systems such as the
    Internet of NanoThings, the Internet of BioNanoThings, and quantum communications
    is equally critical but largely absent from existing publications. As research
    on 6G wireless systems continues to evolve and break new ground, this paper is
    intended to equip readers with a targeted insight into the next generation of
    wireless communications. More specifically, through this paper, we aim to deliver
    a holistic roadmap for 6G and beyond wireless systems, replete with a detailed
    discussion surrounding the use cases and key enabling technologies. The larger
    goal here is to encourage the scientific community to work together towards tackling
    the critical research challenges associated with the realization of the 6G systems.
    The rest of this paper is organized as follows. In Section II, we present a wide
    variety of use cases that will be enabled by 6G. Further, Sections III–VI present
    details concerning key technologies that are critical to the success of 6G, along
    with a discussion on the major challenges faced by each. Then, in Section XI,
    we discuss promising enablers for beyond 6G systems, followed by a timeline for
    the evolution of 6G in Section XII. Finally, in Section XIII, we conclude this
    paper. SECTION II. Use Cases The lessons learned from the continued evolution
    of 5G systems will serve as the backdrop for use cases that will be best served
    by 6G. 5G first introduced the targeted use cases of enhanced mobile broadband
    (eMBB), ultra-reliable low-latency communication (URLLC), and massive machine
    type communications (mMTC), intended to serve a wide variety of applications.
    However, as noted in Section I, there exist a plethora of applications for which
    the 5G KPIs are not strict enough. As we come to realize the performance trade-offs
    in terms of throughput, latency, coverage, energy efficiency, and reliability,
    associated with 5G systems, we can better posit the applications that would benefit
    the most from 6G. As shown in Figure 2, in the following, we present a variety
    of critical use cases that will be enabled by 6G. FIGURE 2. Use cases best served
    by 6G systems. Show All A. Multi-Sensory Holographic Teleportation While virtual
    reality (VR) and augmented reality (AR) have immensely benefited from eMBB and
    URLLC introduced as part of 5G, there are many applications such as advanced healthcare
    including remote diagnosis and surgery, high-resolution sensing for remote exploration,
    and near-real person video conferencing that cannot be adequately served by a
    combination of AR and VR. To this end, holographic teleportation has been recognized
    as the natural successor to AR and VR-based solutions. Unlike existing solutions,
    holographic teleportation operates in a true three-dimensional space and leverages
    all five senses– sight, hearing, touch, smell, and taste, to provide a truly immersive
    experience. At the same time, we note that holographic teleportation requires
    data rates close to 5 Tbps and an end-to-end latency of less than 1 ms [8], both
    of which are impossible to achieve with 5G systems. Thus, 6G, with its expected
    Tbps-level throughput and sub-millisecond latencies, will play a vital role in
    building upon the groundwork established by eMBB and URLLC. B. Real-Time Remote
    Healthcare The success of remote healthcare solutions primarily depends on both
    the quality as well as the availability of connectivity [9]. Concerning the former,
    we note that through its use of key enabling technologies such as THz band communications
    and network automation solutions, 6G will usher in the highest possible wireless
    communications quality focusing on very-high throughput augmented with ultra-low
    latency. Concerning the latter, the Internet of Space Things will play a vital
    role in providing pervasive connectivity, thus enhancing the availability of rural
    healthcare solutions. Further, we expect that within the domain of healthcare,
    advances in 6G and beyond will not only serve as a connectivity solution, but
    will also play a vital role in the diagnosis and treatment of diseases as detailed
    in Section XI-B. C. Autonomous Cyber-Physical Systems Autonomous vehicles and
    UAVs are some of the most promising cyber-physical systems in existence today
    [10], [11]. The operation of these autonomous systems is characterized by the
    exchange of large amounts of data between the constituent nodes, i.e., both vehicles
    and UAVs, relating to high-resolution real-time mapping of the terrain, route
    optimization, and traffic and safety information. While the resulting large volumes
    of data must be delivered within strict deadlines in an error-free manner, it
    also imperative to note that these nodes typically operate at speeds in excess
    of 100 km/h. Therefore in addition to providing sub-millisecond latency and very
    high reliability, the connectivity solution that enables autonomous cyber-physical
    systems must also offer robust operation at very high speeds, which is not possible
    with existing 5G systems [12]. D. Intelligent Industrial Automation Over the past
    few years, Industry 4.0 [13] has been the driving force behind industrial automation
    based on the concepts of supply chain optimization, autonomous equipment, additive
    manufacturing, data analytics, and the Internet of Things (IoT). Yet, these concepts
    are treated as silos working in isolation, limiting the true potential of industrial
    automation. On the other hand, the upcoming Industry X.0 paradigm [14] seeks to
    realize synergies between the various nuances of industrial automation through
    its use of artificial intelligence. Vital to this vision are networked factories
    that serve as critical sources of big data that helps inform decision making.
    To this end, the modern industrial floor is expected to require reliable high-throughput
    connectivity across thousands of devices often with sub-millisecond response times,
    making it the perfect use case for the next frontier in wireless communications.
    E. High-Performance Precision Agriculture Within the broader domain of precision
    agriculture, soil moisture measurements have been a mainstay in irrigation decisions
    for decades now. However, real-time measurements and irrigation automation solutions
    still face challenges stemming from a lack of robust wireless coverage. Going
    beyond simple automated irrigation solutions, high-performance precision agriculture
    is largely centered around delivering data-driven insights to address the specific
    needs of customers, farms, crop, and soil. At the same time, scalable and timely
    access to such data is a major challenge owing to gaps in rural connectivity.
    Therefore, we expect that 6G, with its focus on ubiquitous wireless access, will
    play a major role in enhancing the adoption of technology in agricultural production.
    F. Space Connectivity While near-Earth and deep-space connectivity are still nascent
    within 5G, there are a wide variety of use cases ranging from radio astronomy
    and remote sensing to navigation and backhauling that would stand to benefit from
    the pervasive connectivity offered by 6G. More specifically, such applications
    include freight tracking, terrestrial cellular offloading, environmental monitoring,
    and long-range UAV coordination, to name a few. To this end, the Internet of Space
    Things as described in Section IX, will serve as the key enabling technology for
    beyond-Earth connectivity, furthering the reach of 6G systems. G. Smart Infrastructure
    and Environments The use cases discussed thus far primarily deal with the use
    of third-party systems that seek to leverage advanced telecommunications infrastructure.
    Simultaneously, the evolution of such infrastructure itself is an important use
    case. Going beyond network optimization strategies, there is also a need for exercising
    control over the propagation of wireless signals. To this end, we note that in
    5G and its predecessor systems, the wireless communication environment has always
    played a passive role. However, with the ever increasing demand for data, as evidenced
    by the applications presented herein, control over the manner in which electromagnetic
    waves interact with the indoor and outdoor environment will be critical to the
    success of 6G. In this direction, we posit that the intelligent communication
    environments described in Section IV will play a leading role in the ubiquity
    and pervasiveness of the next generation of wireless systems. SECTION III. Terahertz
    Band Communications Recent years have witnessed a dramatic rise in wireless data
    traffic brought forth by numerous exciting technologies in wireless communications.
    This exponential growth has been accompanied by the demand for higher data rates
    and better coverage [15]. Among emerging research and development trends in wireless
    communications, terahertz band (0.1-10 THz) communications has been envisioned
    as one of the key enabling technologies for the next decade. Buoyed by the availability
    of ultra-wide spectrum resources, the THz band can provide terabits per second
    (Tbps) links for a plethora of applications, ranging from ultra-fast massive data
    transfer among nearby devices in Terabit Wireless Personal and Local Area Networks
    to high-definition videoconferencing among mobile devices in small cells. Recently,
    the Federal Communications Commission (FCC) has released the frequency bands above
    95 GHz for research purposes [16]. While a handful of cellular operators have
    adopted low millimeter wave frequencies for their 5G services with the intention
    of achieving a maximum data rate of 100 Gbps, the test results thus far leave
    much to be desired, showing a peak data rate of around 1 Gbps.1 This gap between
    the targeted and practically achievable data rates is influenced by multiple factors,
    including a high complexity in realistic communication channels, imperfections
    in circuitry design, and interference from other systems operating in adjacent
    frequency bands, among others. Nevertheless, even though the THz bands have been
    applied in imaging and object detection, as well as for THz radiation spectroscopy
    in astronomical research, their use cases in wireless communications are still
    under investigation. Lying between the mmWave spectrum and infrared light spectrum,
    as shown in Figure 3, THz bands, with their abundant spectrum resources, have
    been previously deemed as a “no-man’s land”. However, major progress in the domains
    of transceiver and antenna design has seen THz links become a promising option
    for realizing indoor communications networks. More recently, there has been significant
    progress on realizing wireless network on chip (WNoC) using THz bands [17]. FIGURE
    3. The THz band offers hundreds of GHz usable spectrum resources for wireless
    communication links in long, medium, short, indoor, and near-field range. Show
    All A. Use Cases of THz Band Communications Different from wireless networks at
    lower frequencies, THz-band wireless communications has several unconventional
    application scenarios, owing to the distinct electromagnetic and photonics characteristics
    of this tremendously high frequency band. In addition to the promised Tbps-level
    links for cellular systems, THz-band spectrum can also be leveraged for the following
    scenarios: Local Area Networks: Several spectrum windows are feasible for short-range
    links within ten meters, including 625-725 GHz and 780-910 GHz [18]. THz band
    communications is expected to form the THz-optics bridge to enable seamless transition
    between fiber-optics and THz-band links with zero latency. Personal Area Networks:
    THz band communications can provide “fiber-like” data rate without the need of
    wires, between multiple devices at a distance of a few meters. Such communication
    scenarios can be found in indoor offices and multimedia kiosks. Data Center Networks:
    Conventional data centers manage and maintain connectivity in wired networks using
    cables, resulting in high costs in terms of both installation and reconfiguration.
    On the other hand, THz links provide promising prospects for seamless connectivity
    at ultra-high-speeds in fixed networks and adaptability for hardware reconfiguration.
    Wireless Network on Chip: As the trend in transceiver hardware development motivates
    a higher level of integration and miniaturization as well as weight reduction,
    the THz band links can serve as a promising candidate to establish wireless connections
    among different modules within the transceiver chassis, in order to replace the
    wired connections commonly found in existing transceiver hardware products. Nano-networks:
    With its wavelength falling into the nanometer (10−9 m) range, the THz band can
    operate better than any other frequencies in nano-networks. Within this context,
    a nano-network is a set of interconnected nano-devices or nano-machines for information
    exchange, storage, and computation. A more detailed overview can be found in Section
    XI-A. Inter-satellite Communications: Lying largely outside the Earth’s atmosphere,
    inter-satellite links are not constrained by atmospheric attenuation, which makes
    the THz band a favorable candidate for such communication links. Compared to existing
    spectrum resources allocated for inter-satellite links, the THz band has a much
    wider bandwidth which can accommodate more satellites and achieve higher link
    performance. Unlike the widely used optical links, the THz band does not impose
    stringent requirements on beam alignment, which can help maintain a high level
    of link stability as satellites drift out of their orbits. B. Devices in the THz
    Band The need for higher output power, lower phase noise, and better receiving
    sensitivity in THz band transceivers has driven advances in corresponding device
    development. Currently, three main directions are deployed in THz band signal
    generation: photonics-based, electronic-based, and emerging material-based, respectively.
    In the photonics-based approach, many III-V semiconductors, including gallium
    arsenide (GaAs) and indium phosphide (InP), which provide high electron mobility,
    are excellent candidates especially for high frequency (i.e., above 100 GHz) applications.
    Such photonics-based techniques generate time-domain pulses with lengths of femto-seconds
    (10−15 s) and experimental works have demonstrated 50 Gbps data links in an indoor
    scenario at 300 GHz using the uni-traveling-carrier photodiode (UTC-PD) technique
    [19]. The UTC-PDs and modified UTC-PD structures, an effective photomixing solution
    which allows wider spectrum tuning and simpler construction compared to laser
    pulse generators, has pushed the output signal range from 300 GHz to 2.5 THz,
    with output powers of 10 μW at 300 GHz and 1 μW at 1 THz [20]. Additionally, a
    design based on slot-antenna-integrated UTC-PDs has shown superior performance
    in generated THz band signal strengths at 350-850 GHz and 900 GHz-1.6 THz, compared
    to that of the bowtie-antenna integrated UTC-PD [21]. Similar photonics-based
    THz signal generation at above 1 THz can be realized using quantum cascade lasers
    (QCLs) and other solid state lasers [22]. However, the operations of such devices
    are limited at room temperatures, requiring liquid helium cooling, which affects
    their deployment in local area networks with space restrictions. Furthermore,
    photoconductive antennas (PCAs) have been utilized widely for both pulse and continuous-wave
    signal generation at THz band [23], demonstrating a wide spectrum (up to 4.5 THz)
    with a remarkable dynamic range of up to 100 dB [24]. In parallel to the photonics-based
    approach for THz band device design which down-converts the optical frequencies,
    the electronic-based THz band signal generation relies on frequency up-conversion
    using multipliers [25], including frequency doublers and triplers, as well as
    backward wave oscillators [26]. A recent experiment has demonstrated an all-electronics-based
    wireless link at 240 GHz with a throughput of 50 Gbps and a maximum 29% error
    vector magnitude using QPSK modulation [27]. The backward wave oscillator, which
    is a compact design to generate THz band signals based on the mechanism of energy
    transfer from an electron beam to an electromagnetic wave through a vacuum tube,
    has been used to generate signals at 300 GHz with an output power of 1 W in plasma
    diagnostics [26]. Among the two commonly used approaches, the photonics-based
    design benefits from a relatively simpler transceiver architecture based on the
    photomixing technique, while the electronics-based solution relies on cascaded
    frequency up-conversion of RF signals, which sets stringent requirements on linear-range
    operation and potentially limits the terahertz bandwidth. On the other hand, the
    electronics-based approach is less sensitive to environmental conditions, such
    as temperature, humidity, among others, which makes it more favorable for outdoor
    operations, whereas the link reliability from its photonics-based counterpart
    is affected by scattered particles in the channel, making the THz band link less
    robust. Besides these classical approaches for THz band device development, new
    materials, including graphene, carbon nanotubes, and graphene nanoribbons, are
    gaining more attention due to their extremely high electron mobility in the order
    of 8,000−10,000c m 2 /(V⋅s) at room temperature, as compared to 1,400 c m 2 /(V⋅s)
    of silicon, and 8,500 c m 2 /(V⋅s) of GaAs, which means that the link throughput
    can be potentially up to ten times higher than is currently achievable with most
    semiconductors [28]. The graphene-based devices, offering outstanding mechanical,
    electrical, and optical properties, have been utilized in the development of power
    detectors at 600 GHz [29] and 200 GHz [30], as well as plasmonic antenna arrays
    and transceivers [31], [32]. Graphene-based devices have the potential to break
    new ground in reaching the desired level of performance at much higher frequencies
    above 1 THz. C. Physical Layer Modeling at the THz Band The realization of wireless
    communications at THz frequencies requires the development of accurate channel
    models to capture the impact of both channel peculiarities including the high
    atmospheric attenuation and molecular absorption rates at various transmission
    windows, as well as the propagation effects including reflection, scattering,
    and diffraction, with respect to different materials. Current research has reported
    several efforts to provide fundamental understanding of such channels. For example,
    an early work in [33] demonstrates the remarkable capacity the THz band channel
    can support for short transmission distances. The model provides a detail analysis
    on the effect of attenuation caused by the molecular absorption and spreading
    loss, on which the performance of channel throughput is heavily dependent. One
    step further, in order to extend the transmission distance at THz bands, an idea
    dubbed as the ultra-massive multiple-input multiple-output (UM MIMO) communications,
    enabled by an element array size of 1024×1024 with plasmonic nano-antennas, can
    drastically boost the signal strength by steering and focusing the transmitted
    beams in both space and frequency [34]. Correspondingly, a UM MIMO channel model
    has been developed in [35] which takes into account the role of such arrays. Additionally,
    a stochastic channel model for indoor THz band communications at 300 GHz has been
    reported in [36] which characterizes both spatial and temporal domain channel
    information. More recently, based on the aforementioned applicable scenarios in
    the THz band, a stochastic channel model for kiosk applications has been reported
    in [37] which ranges from 200-340 GHz. The main takeaway from current models validated
    using either measurements or the ray-tracing technique is that the direct path
    between the transmitter and the receiver and the single-bounce reflected paths
    dominate the received power, while other channel effects, including diffraction
    and scattering, attenuate power significantly along propagation. On the basis
    of the ultra-wideband channel characterization, THz band communications faces
    a critical and challenging task of synchronization in the receiver design. Existing
    pulse-based modulation schemes permit the use of low-complexity non-coherent analogue
    detectors, e.g., energy detector and auto-correlation receiver, which involve
    the multiplication of the received signal with itself, followed by an integrator.
    However, a more advanced non-coherent receiver architecture in [38] based on a
    continuous time moving average symbol detection scheme demonstrates better performance
    compared to previous detection schemes for pulse-based modulations in terms of
    the symbol error rate. In addition, robust frequency and timing synchronization
    for multi-carrier communication in the THz band is desirable to decode multiple
    incoming signal streams. The work presented in [39] realizes both a low-rate sampling
    scheme for channels with high SNR values and a maximum likelihood-based algorithm
    for low SNR channels with satisfying bit-error-rate performance. More recently,
    a synchronization scheme based on medium access control protocols is reported
    in [40], which shows good performance in both a macro-scale scenario to overcome
    the distance limitation at the THz band and a nano-scale scenario for nano-devices
    for energy harvesting. Similarly, given the peculiar ultra-wideband nature and
    frequency selectivity of THz band communications, equalization solutions pose
    another relevant challenge [15], [41]. In [42], three equalization solutions,
    including the Tomlinson-Harashima precoding, a waveform with interference management
    for time-reversal systems, and an iterative algorithm with adaptive soft feedback,
    are reviewed and compared for an indoor channel. Results show that the iterative
    algorithm with adaptive soft feedback yields the most promising BER performance
    [42]. D. Medium Access Control in THz Band Communications On top of physical layer
    channel models, the medium access control (MAC) schemes in THz band communications
    should adopt certain spatial and spectral features in order to provide solutions
    in resolving issues such as the deafness problem and LoS blockage, among others
    [43], [44]. Different from commonly used MAC solutions in RF systems that utilize
    omnidirectional antennas, such as carrier sense multiple access with collision
    avoidance (CSMA/CA), the MAC protocols designed for THz band rely on handshakes
    between transceivers with highly directional beams. These razor-sharp beams can
    provide higher power radiation gain and prolong the transmission distance, but
    when misalignment happens, the deafness problem arises. As such, the deafness
    avoidance approach is required in MAC scheme design. Existing solutions utilized
    in IEEE 802.15.3c [45] and others employ a beam-training phase to estimate and
    steer beams towards destined devices. Recent works also propose methods based
    on angular division multiplexing [46] and a priori aided channel tracking schemes
    [47]. The results in such proposed solutions suggest that with good beam alignment
    strategies the channel throughput can be improved significantly. Moreover, the
    MAC protocols can also resolve the issue of LoS blockage where the received power
    of a user device may undergo deep fading due to the device being held in a manner
    that blocks the LoS path. Studies have shown that such attenuation by the human
    body can be as high as 20 dB at 60 GHz and up [49], [50]. To mitigate the blockage
    problem, researchers have proposed a multi-hop scheme at the mmWave and THz bands
    to form alternative routes [51], [52]. A careful link-level scheduling and neighbor
    discovery process is necessary to achieve high throughput while maintaining low
    interference. E. Open Problems in THz Band Communications Currently, the fabrication
    and testing of THz band antenna arrays remains a relevant challenge. Some techniques
    based on photolithography, electro-beam lithography, among others, are able to
    produce the front-end with hundreds of plasmonic antenna elements. The utilization
    of large antenna arrays can extend the signal coverage by forming array radiation
    patterns with main lobes of high directivity, thus focusing energy towards desired
    directions. However, such highly directional beams limit coverage in the angular
    domain, causing low energy efficiency at the transmitter to serve each user. A
    recent solution named “THzPrism” has been proposed to form multiple beams with
    slight frequency shifts towards different directions while maintaining good distance
    coverage [53] in addition to other proposed solutions discussed in Section III-C
    to solve the transmission distance problem. This design employs true time delays
    for RF chains before phase-shifters to obtain a prism-like effect, which spreads
    the original beam into several beams, each with a slight frequency shift with
    respect to the center frequency. In parallel to the quest for more novel solutions
    in antenna design, other remaining challenges reside in the control and signal
    processing schemes associated with transceiver designs in the THz band. On the
    one hand, real-time control algorithms are needed. On the other hand, communication
    protocols for coordination between the transmitter, receiver, and reflectarrays
    are needed. Among others, in [54], researchers reported a smart reflectarray-assisted
    mmWave system compatible with IEEE 802.11ad. Besides the design of the reflectarray
    and a study on deployment strategies, a three-way beam-searching protocol is developed,
    in which the reflectarray coordinates with the transmitter through a 2.4 GHz control
    channel in order to discover the best joint transmit and reflect sectors for which
    the signal at the receiver is maximized. However, this work does not capture the
    extended functionalities of plasmonic reflectarrays. Furthermore, when highly
    directional beams are utilized at mobile transceivers, a relevant challenge arises
    from the limited field-of-view of antenna arrays for each transceiver to locate
    the next hop to forward its data; thus, new routing solutions are necessary for
    THz band communications to efficiently discover and establish links. A study in
    [55] reports a solution in link discovery at THz band using a leaky-wave antenna
    to sense the angular information of a user. SECTION IV. Intelligent Communication
    Environments Along with the rapid growth in the number of wireless devices, services,
    and applications, a corresponding demand for higher speed wireless communications
    has burgeoned in recent years. Nevertheless, the major challenge at mmWave and
    THz-band frequencies is the limited communication distance because of the remarkably
    high path loss inherent to small wavelengths and the limited transmission power
    of mmWave and THz-band transceivers [56]. Current solutions primarily focus on
    the advancement of wireless transceiver hardware and software, as well as network
    optimization strategies. However, the wireless propagation medium has been largely
    neglected. The wireless communication environments, for both indoor and outdoor
    scenarios, can be actively utilized in order to become controllable for signal
    propagation. To control signal propagation in environments is essentially to control
    how electromagnetic waves interact with scatterers, which include indoor furniture
    and outdoor buildings as well as other infrastructure. Typically, the controllable
    behaviors of electromagnetic waves include controlled reflection, absorption,
    wave collimation, signal waveguiding, and polarization tuning, as illustrated
    in Figure 4. The notion of “Intelligent Communication Environments” resides in
    the control algorithms where deep learning and reinforcement learning are to be
    exploited to dynamically configure the environments. In the following subsections,
    we elaborate on these controllable wave behaviors, current research efforts, as
    well as corresponding open issues. FIGURE 4. Conceptual design of a plasmonic
    reflectarray able to unconventionally manipulate EM waves [48]. Show All A. Basics
    of Intelligent Communication Environments The intelligent environments can be
    seen as a three-dimensional structure with several layers, each with different
    functionalities. Recent research under the EU Research Project “VisorSurf” has
    demonstrated a structure with five main layers, which are (from top to bottom)
    the EM behavior layer, the actuation and sensing layer, the shielding layer, the
    computing layer, and the communication layer, respectively [57]. Specifically,
    the EM behavior layer is composed of metasurfaces, a two-dimensional representation
    of metamaterials, and has a tunable impedance to control directions of reflection
    of the EM waves. Some other works use reflectarray antennas as the top surface
    [58], [59]. The actuation and sensing layer consists of circuits for phase shifting
    and sensors for impinging signal sensing. Some options for actuation include PIN
    diodes with controllable biasing voltage as switches in reflectarray antennas,
    and complementary metal-oxide semiconductors (CMOS) transistors as well as micro-electro-mechanical
    (MEMS) switches for metasurfaces. The shielding layer isolates the upper and lower
    parts of the layered structure so as to minimize the possible interference. The
    computing layer serves to control the phase shifts and process sensed impinging
    waves. To this end, another reported solution makes use of field-programmable
    gated arrays (FPGAs) to fulfill such functions on metasurfaces [60]. Finally,
    the communication layer connects all upper layers and serves as the gateway towards
    the central controller which processes all connection requests, forwards and receives
    signals, and conducts the aforementioned controlled wave functions. Compared to
    existing relays with multiple antennas which are widely deployed in wireless networks,
    Intelligent Communication Environments offer the following advantages: (i) a higher
    spatial diversity due to the wide coverage of the intelligent surface with controllable
    antenna arrays, (ii) a reduced processing time given that the computing and communication
    layers are directly underneath the surface layer, and (iii) a higher flexibility
    in network routing when impinging signals come from different directions and the
    intelligent surfaces are able to collimate waves and reflect them towards desired
    directions. B. Functionalities of Intelligent Environments Bolstered by the layered
    structure, intelligent surfaces can enable controlled EM wave operations. At micro-
    and mmWave frequencies, metasurfaces are considered as a good candidate. While
    at THz bands, graphene-based plasmonic antenna arrays are desirable. In metasurfaces,
    a meta-atom is the smallest unit, which is a conductor with a size smaller than
    half the wavelength of the signal. Metasurfaces can thus control the impinging
    EM waves with a very fine granularity. The meta-atoms are interconnected by a
    set of miniaturized controllers that connect the switches of the metasurfaces
    in the computing layer, while a gateway serves as the connectivity unit in the
    communication layer to provide inter-element and external control. At THz bands,
    when the metasurfaces do not yield optimal performance, the graphene-based plasmonic
    antenna arrays serve as a promising alternative. Compared to metallic antenna
    arrays, the plasmonic antenna arrays can have much denser element layout and go
    beyond the conventional λ/2 sampling of space towards more precise space and frequency
    beamforming, owing to the physics of plasmonics. In our previous work, we have
    demonstrated that, graphene can be used to build nano-transceivers and nano-antennas
    with a maximum dimension of λ/20 at THz frequencies, allowing them to be densely
    integrated in very small footprints (1024 elements in less than 1 mm2), as shown
    in Figure 4 [34]. Therefore, by incorporating the graphene-based plasmonic antenna
    arrays at THz bands and metasurfaces operating at mmWave bands, we can expand
    the operational spectrum of intelligent environments and utilize them in transmission
    and reception in a controllable manner. C. Layered Structure of Intelligent Communication
    Environments Based on the operating principles of the aforementioned Intelligent
    Communication Environments, in this subsection, we anatomize the layered structure
    and detail each layer’s functionality. 1) Metamaterial Plane The metamaterial
    plane is also the surface plane, as shown in Figure 4. In designs based on reflectarrays,
    phase shifts are applied to each element to improve useful signals while canceling
    interference [58], [61]. The metasurface element proposed in [62] with millimeter-scale
    dimensions is connected to a PIN diode with a bias voltage to control operation
    modes, such as altering polarizations. In general, this layer comprises the supported
    EM function of the tile as well as its operation principle. In particular, reflectarray
    employ modifiable phase shifts applied over their surface. In the far field of
    radiation, reflected rays can be considered co-directional, and their superposition–constructive
    or destructive–is controlled by the applied phase shifts [63]. Hence, wave scattering
    or controlled reflecting functions can be attained. Metamaterial tiles, however,
    operate as surfaces with tunable local impedance [64]. Impinging waves create
    inductive surface currents over the tile, which can be routed by tuning the local
    impedance across the tile. Notice that the Huygens Principle dictates that any
    EM wavefront can be traced back to a current distribution over a surface [65].
    As a result, in principle, metamaterials can produce any custom EM function as
    a response to an impinging wave. Common functions include wave steering, focusing,
    collimating (i.e., producing a planar wavefront as a response to an impinging
    wave), polarizing, phase altering, full or partial absorption, frequency selective
    filtering and even modulation [60], [64]. 2) Sensing and Actuation Plane In order
    to control the EM waves per actual channel conditions, the programmable surfaces
    are expected to sense the propagation environment and actuate the upper surface
    plane accordingly. Such layer contains hardware elements that can be controlled
    to achieve a phase shift or impedance distribution across a tile. Commonly, the
    layer comprises arrays of planar antennas–such as copper patches–and multi-state
    switches between them. Reflectarray tiles usually employ PIN diodes with controllable
    biasing voltage as switches [62]. Metamaterials have employed a wider range of
    choices, both in the shape and geometry of the planar antennas and in the nature
    of switches. CMOS transistors, PIN diodes, Micro-Electro-Mechanical Switches (MEMS),
    micro-fluidic switches, magnetic and thermal switches are but a few of the considered
    options in the literature [66]. Notably, some options–such as micro-fluid switches–are
    state-preserving in the sense that they require power only to change state but
    not to maintain it (i.e., contrary to biased PIN diodes). Sensing impinging waves
    are also necessary for exerting efficient control over them. While this information
    can be provided by external systems [67], with dynamic channels and mobile end-users,
    tiles capable of incorporating sensing capabilities can be immune from the channel
    aging problem [68]. The sensing can be direct, employing specialized sensors,
    or indirect, e.g., via deducing some impinging wave attributes from currents or
    voltages between tile elements. 3) Computing Plane The computing functionality
    serves the processing functionality in the controllable surface system. In the
    metasurface designs in [60] and [62], FPGA-based controllers are connected to
    the metasurfaces to implement the computing functions. This layer comprises the
    computing hardware that controls the actuation and sensing elements. Its minimum
    computing duties include the mapping of local phase or impedance values to corresponding
    actuator states. Reflectarray tiles commonly implement this layer using FPGAs
    and shift registers [62]. Metasurfaces, and specifically HyperSurfaces, can alternatively
    employ standard IoT devices for the same purpose. Moreover, they can optionally
    include computing hardware elements (ASICs) distributed over the tile meta-atoms
    [69], [70]. This can enable autonomous and cognitive tiles, where meta-atoms detect
    the presence and state of one another, and take local actuation decisions to meet
    a general functionality objective. Nonetheless, these advanced capabilities are
    not required for programmable wireless environments. 4) Communication Plane The
    communication plane passes the signals from the processing layer to corresponding
    metasurface layer and collects signals from the sensing and actuation plane. In
    complicated programmable surface systems, communication occurs among planes to
    realize various EM wave control functions. The command signals normally operate
    at much lower frequencies compared to the ones emitted from programmable surfaces;
    such signals prove to be more efficient in tuning the bias voltage of the PIN
    diodes [60]. This layer comprises the communication stack and connects actuation
    and sensing layer as well as computing layer with tile-external devices such as
    controllers. In the simplest case, this layer is implemented within the computing
    hardware, acting as a gateway to the external world using any common protocol
    such as the Ethernet. HyperSurface tiles with embedded distributed computing elements
    additionally require inter-tile communication schemes, to handle the information
    exchange between smart meta-atoms. Both wired and wireless intra-tile communication
    is possible [69], [70]. In both cases, the ASIC hardware employs custom and nonstandard
    protocols. D. Use Cases of Intelligent Environments With the utilization of well-coordinated
    tiles in the Intelligent Environments, the wireless system can be greatly improved
    in terms of communication efficacy. 1) On Signal Propagation Enhancements From
    the perspective of multiple users and moving users, the Intelligent Environments
    system is envisioned to serve a large number of users with more realistic user
    patterns, including mobile users and users in a cluster. Additionally, the Intelligent
    Environments system should ensure physical layer security against jamming and
    eavesdropping, an increasingly important problem that remains to be solved. Transmission
    Distance: For users in the NLoS areas relative to the transmitter, the Intelligent
    Environments system is expected to extend the transmission distance and reach
    previously uncovered areas through waveguiding or reflection. Simulation results
    in [67] demonstrate that at 60 GHz the coverage can be extended to an entire NLoS
    area. Interference Mitigation: Due to the scenario with multiple users, there
    is inevitably concern of interference. As in the envisioned scheme, each Intelligent
    Environment unit is dedicated to an individual user, thus the majority of interference
    will reside in the wireless section of the end-to-end link. Reliability: The primary
    efforts in terms of physical layer reliability include using highly directional
    antennas to nullify jamming, forming exclusion areas, assigning secret keys to
    legitimate users, and so on. From the perspective of fundamental propagation channels
    with Intelligent Environments, good reliability is achieved when the eavesdroppers
    do not have the knowledge of the frequencies where packages are transmitted, or
    the eavesdroppers are in the same frequency channel but with much higher noise
    which makes the intercepted data impossible to decode [71]. Therefore, the dedicated
    links in Intelligent Environments are inherently secure. 2) On the Physical Layer
    Security The more frequent data exchange between users and service providers exposes
    a higher risk of personal and private data leakage. The 6G wireless network should
    not only inherit existing network secrecy measures, but also provide enhanced
    physical layer security associated with new enabling techniques. In current 5G
    networks, highly directional beams are used for mmWave communications in spatial
    domain to prevent signals from being intercepted. However, a recent study has
    shown that such pencil-sharp beams are still vulnerable to agile eavesdropping
    [72]. Other physical layer encryption algorithms, including source coding approaches
    such as the low-density parity-check (LDPC) code, are demonstrated with optimal
    performance under specific conditions [73]. Furthermore, existing solutions in
    the reconfigurable intelligence surface apply reflectarrays, which do not have
    the capability to effectively distinguish target users from malicious attackers.
    Hence, a solution based on the Intelligent Environments serves the purpose of
    identifying unintended recipients, creating null areas, and improving link secrecy
    rate. Essentially, the envisioned Intelligent Environments have the capability
    to sense user locations and exchange such information with a system controller
    to verify the user’s authenticity. Only affirmative users shall be served with
    signal streams from the sender. On the other hand, connection requests from unauthorized
    users (i.e., eavesdroppers) will be nullified from attempting to access secure
    information or even trying to establish links with the sender. In practice, the
    Intelligent Environments can be configured to tune the phases of multipath components
    in the channel, such that those arrived at intended users can be coherently combined
    with boosted received signal strengths, whereas those intercepted by eavesdroppers
    will be scrambled or even cancelled due to non-coherent combining [74]. Simulation
    results demonstrate a 6-dB attenuation observed at the received signal strength
    from the eavesdropper, validating the proposed physical layer security solution
    [74]. Hence, the good channel secrecy is achieved when such unintended users do
    not own the knowledge of equalizer to recover the transmitted signals from noise.
    E. Open Problems in Intelligent Environments A number of open problems need to
    be addressed in order to facilitate the Intelligent Environments in becoming a
    market-ready solution. Trade-off Between Dimensions and Energy Consumption: In
    terms of real-world applications, the Intelligent Environments are expected to
    be coated onto surfaces of interior walls and/or ceilings, and building facades,
    which require dimensions that can both fit specific installation areas and satisfy
    link requirements. Meanwhile, with more reflectarray elements and RF chains built
    into the system, the energy consumption will also increase, due to the advanced
    signal processing circuitry. Therefore, how to achieve an economic solution to
    balance the overall dimension and energy consumption while serving users to its
    desired performance is a nontrivial issue. Compatibility With Existing Solutions:
    Current Wi-Fi access points have a mature protocol stack to sense the channel
    and establish links with users. In order for the Intelligent Environments to assist
    with improving indoor signal coverage, it needs to be compatible with the IEEE
    802.11 series standard. For now this is still under research and serves as a worthy
    problem for novel solutions. Standardization: With many candidate approaches being
    investigated in reflectarrays, metasurfaces, frequency selective surfaces, among
    others, there has not been a consensus on how to standardize the device architecture,
    maximum emitted power, and communication protocols. As more ideas evolve, a standardization
    effort within a work group is necessary towards a solidary framework. Inclusion
    of advanced application scenarios: as a promising solution for future generation
    of wireless systems, the Intelligent Communication Environments should be designed
    to fit to more advanced scenarios, for example, in use cases with a very large
    quantity of devices as in the Internet of Things, under high demands of real-time
    video streaming, with devices of high mobility, among others. Smart resource allocation
    solutions: resources in the spatial, temporal, and spectral domains should be
    allocated in an optimal manner to satisfy per-user demand. AI-driven design and
    optimization: in complicated scenarios under which no closed-form solutions can
    be found using conventional optimization approaches, advanced algorithms in artificial
    intelligence can assist in the deployment of Intelligent Communication Environments,
    especially when complex surface layout or structures are in presence. SECTION
    V. Pervasive Artificial Intelligence In the past few years, the field of artificial
    intelligence (AI) has witnessed immense growth, leading to its application in
    a wide variety of fields across both academia and industry. In the realm of communications
    and signal processing, AI can be readily applied to cognitive radios, remote sensing,
    computer vision, and network management. More specifically, in the domain of wireless
    communications, AI and its associated algorithms are also gradually proving their
    utility in various emerging techniques such as massive MIMO communications which
    requires efficient channel estimation and symbol detection. Such tasks often do
    not yield low complexity optimal solutions in complex channels [75], and thus
    parallel processing inherent in machine learning can be favorably leveraged to
    enhance computational capacity. Admittedly, even though current wireless communication
    networks follow a layered structure, in which each layer primarily serves several
    functions, applications of AI and relevant algorithms are gradually bridging the
    gap across layers in a way that can globally optimize the performance in the entire
    wireless network. However, in order to provide a marked trail to navigate through
    a plethora of pervasive AI applications, this section is organized based on the
    existing layers. It is worth noting that AI is a broad concept, under which reside
    several branches covering interleaved research topics, such as robotics, natural
    language processing, machine learning (ML), computer vision, among others. In
    this section, we focus specifically on the ML algorithms and their applications
    in wireless communications under the AI umbrella. These two terms are thus used
    interchangeably in this section. As shown in Figure 5, artificial intelligence
    can be applied to each layer of the wireless network. At the network layer, machine
    learning (ML) algorithms can be used for traffic clustering to further adapt the
    network resources to various scenarios [76]. At the physical and MAC layers, deep
    learning can optimize resource allocation strategies for power distribution, and
    modulation and coding schemes, among others. Furthermore, machine learning algorithms
    can also assist with channel estimation and multi-user detection. FIGURE 5. Applications
    of artificial intelligence at different layers of wireless systems. Show All A.
    AI in the Physical Layer Traditionally, physical layer modeling has been model-oriented—a
    practice in which mathematical models following a certain framework are proposed
    and optimized under constraints to satisfy a series of pre-determined performance
    requirements. For example, in order to conduct channel estimation, a channel model
    is assumed along with other parametric configurations. These model-based solutions
    usually perform well if the derivation of mathematical models is relatively straightforward
    or there exists a closed-form solution. The models can then be validated by field
    measurements or numerical simulations. However, in real-world scenarios, the applicability
    of such model-based solutions falls short in complicated environments, due to
    factors such as non-linearity inside systems and uncontrollable interference,
    among others. On the other hand, another approach, which is based on statistics,
    or data sets, builds the model through learning from the data. This method is
    particularly useful when theoretical analysis is intractable or when a closed-form
    solution is difficult to obtain. For example, in diffusion-based channels commonly
    found in molecular communication, the channel characteristics depend largely on
    the environment, making them challenging to model theoretically [77]. In such
    cases, some data is used for training, which can help establish a model, while
    other data is used for testing in order to validate the model. To date, artificial
    intelligence has demonstrated its usefulness in various physical layer techniques.
    For example, in channel estimation and symbol detection, deep learning approaches
    reported in [78], [79] have shown that the proposed deep learning-based symbol
    detection algorithms can provide robust and accurate results with reduced complexity.
    Furthermore, a deep learning method based on the deep neural network architecture
    also demonstrates an improved channel estimation accuracy under the effects of
    non-linearities of power amplifiers, I/Q imbalance, and quantization errors induced
    by hardware impairments [80]. An autoencoder-based communication system is proposed
    to reconstruct the transmitted signals from channel impairment based on trained
    deep neural networks in an end-to-end manner [81]. Furthermore, self-supervised
    learning is becoming a trend for user localization since it has been demonstrated
    that relevant methods can significantly reduce the size of labeled dataset for
    efficient processing [82] B. AI in Wireless Networks In other essential layers
    of a wireless network, the existence of rich datasets lends itself to the applicability
    of machine learning-based solutions. In routing protocol design for wireless sensor
    networks, researchers have successfully utilized reinforcement learning methods
    to achieve a more energy-efficient routing scheme for underwater sensor networks
    [83]. In the vehicular industry, autonomous driving has already been studied and
    become a reality in some cities in Arizona, US.2 With respect to vehicular communication
    networks, due to the constant movement of vehicles, a predictive model based on
    real-time data has superiority over traditional theoretical models in terms of
    accuracy. Artificial intelligence and its plethora of algorithms can be applied
    in varied ways in such networks: autoencoders for predicting traffic flow [84],
    k -means clustering for traffic congestion control [85], and Q -learning for intelligent
    resource management [86], among others [87]. In the Internet of Space Things,
    with our envisioned multi-band communication capabilities in both inter-satellite
    and ground-to-satellite links, we have proposed a deep neural network-based resource
    allocation strategy to enable a flexible scheme for CubeSats to stay connected
    without human intervention from the ground [88]. Apart from advancements in algorithm
    development, a potential issue dwells on the location of data storage and processing,
    which has largely happened in central data centers in the cloud. For devices distributed
    in a wide geographic range, this undoubtedly adds significant delays in performing
    tasks which require real-time operation. Moreover, a centralized computing manner
    is not desired from perspectives of data security and constraint in processing
    capacity [89]. In order to relieve such challenges, edge AI pushes local devices
    to fulfil operation and management tasks. Admittedly, this could lead to possible
    overburden in local devices, since they are not equipped with as powerful processing
    units as the cloud processing center, however, research efforts in this direction
    are dedicated to: (i) acceleration in boosting the hardware’s processing capability,
    and (ii) leveraging the coordination between local and central processing units
    to optimization task distribution. C. AI in Network Management and Orchestration
    AI, or more specifically ML, has an integral role to play in the management of
    networks [90]–[92]. In fact, Clark et al. introduced the concept of the Knowledge
    Plane [93] back in 2003, describing it as a pervasive ML-based system within the
    network that is geared towards providing services and advice to other elements
    of the network. More recently, with sofware-defined networking (SDN) and network
    function virtualization (NFV) becoming mainstream, large-scale data acquisition
    has become easier than ever before, making a strong case for ML-based management
    and orchestration primitives within 6G, ultimately leading to full network automation
    as discussed in the next section. In particular, the domain of network management
    presents a wide variety of problems that can be broadly categorized into: [91]
    Supervised Learning: Supervised learning is typically applied to problems relating
    to traffic prediction [94] and classification [95], as well as slice resource
    prediction [96]. While the former primarily involves preemptively determining
    the network traffic load, as well as determining the applications, protocols and
    QoS classes the traffic belongs to, for fine-grained traffic engineering, the
    latter involves predicting the resource requirements associated with different
    network slices based on the anticipated traffic load. Reinforcement Learning:
    Reinforcement learning typically finds use in problems associated with resource
    management [97], [98]. For example, the popular virtual network embedding problem
    wherein the network orchestrator performs optimal placement of virtual network
    functions onto the underlying physical substrate, is highly amenable to reinforcement
    learning [99]. Other applications include elastic scaling of network infrastructure
    [100], failure prevention, and configuration rollback [101]. Unsupervised Learning:
    While both supervised learning and reinforcement learning have shown significant
    promise in the network management domain, we note that there exist certain use
    cases such as those relating to optimizing the end-users’ Quality of Experience
    (QoE) [102] and network security [103] where: (i) labeled data for training is
    simply not present and (ii) the real-time nature of the application makes it impractical
    to wait for feedback. In such cases, unsupervised learning can prove to be an
    indispensable tool. For example, intrusion detection systems based on autoencoders
    have been shown to outperform supervised learning-based systems [104]. D. Open
    Problems for Pervasive AI During early years of 5G standardization, researchers
    have discussed potentials of AI in achieving high time and spectrum efficiency
    once incorporated in 5G. Specifically, AI algorithms can help facilitate the following
    tasks which would yield low efficiency with conventional solutions: identify network
    anomalies, allocate network resources, perform network management, among others
    [106]. However, so far these solutions have not been officially adopted in 5G
    standards worldwide. In June 2020, the ITU has initiated an AI/ML in 5G challenge
    to motivate researchers to identify and solve real-world problems using AI/ML
    solutions in relevant 5G directions.3 As such, it is envisioned that such efforts
    would be factored in later 5G development but will be materialized in a more concrete
    and pervasive manner in 6G. While pervasive artificial intelligence in wireless
    communication networks will undoubtedly bring a paradigm shift towards data-oriented
    approaches, there are still open problems to be resolved. First, thus far no agreement
    has ever been reached on which algorithms work the best to solve a generalized
    problem in wireless networks, such as modulation and coding scheme design, channel
    estimation, and resource allocation, among others. Almost all published works
    claim significant accuracy or reduced complexity with either analytical theories
    or practical data sets [107]. Additionally, we note the absence of an effective
    method to draw a fair comparison among all proposed solutions, due to variations
    in selected data sets, assumptions, evaluation criteria, and so on. However, when
    it comes to realistic deployments, a careful gleaning process should be performed
    in order to identify algorithms without loss of general applicability. Second,
    the limited availability of quality datasets is detrimental to the testing and
    validation of proposed classification or regression algorithms. SECTION VI. Network
    Automation SDN and NFV have been widely recognized as the major paradigm shifts
    that occurred with the advent of 5G [108]. In particular, SDN has paved the way
    for the separation of the data and control planes, while NFV has been instrumental
    in decoupling the software from the hardware. Consequently, both wired and wireless
    networks have witnessed significant benefits from the adoption of SDN and NFV,
    including but not limited to simplified network management and service deployment,
    availability of advanced traffic engineering solutions and fine-grained network
    slicing techniques, and reduced CAPEX and OPEX. A major contribution of network
    softwarization has also been the commoditization of key network components such
    as switches [109] and base stations [110], [111], allowing for their implementation
    on commercial off-the-shelf (COTS) hardware. In addition, the large open-source
    community supporting these projects has played a pivotal role in engaging a wider
    set of stakeholders than was previously possible. As networks evolve further,
    the traditional network operations routine, rooted in manual configuration and
    static script-based primitives, cannot keep up with the increasing complexity.
    Instead, we posit that automation will serve as the major driving force behind
    building upon the improvements brought forth by SDN and NFV. More specifically,
    network automation is defined as the process of automating the configuration,
    management, testing, deployment, and operations of physical and virtual devices
    within a network [112]. Network automation is intended to speed up the delivery
    of network services while adhering to dynamic and robust service-level agreements
    (SLAs), and reducing the potential for errors through minimization of manual intervention.
    Standardization efforts in this domain have led to the introduction of the Network
    Data Analytics Function (NWDAF) in the control plane and the Management Data Analytics
    Service (MDAS) in the management plane, for enhanced data collection and analytics
    functionalities within 3GPP Releases 15 and 16 [113], [114]. Both these functions
    form a critical segment of the Service-Based Architecture (SBA) within 5G, highlighting
    the growing importance of network automation. To this end, we explore three key
    tenets of network automation in this section– software-defined programmable data
    planes, automated service decomposition and orchestration, and self-driving networks.
    While the first two are primarily concerned with automating specific aspects of
    the network, i.e., the data plane and the network slicing procedure, self-driving
    networks are the holy grail of network automation, requiring absolutely no manual
    intervention. A. Software-Defined Programmable Data Planes Being the most popular
    Southbound API, OpenFlow [115] is synonymous with SDN and has been featured widely
    in 5G networks. Yet, the stateless match-action abstraction implemented by OpenFlow
    precludes true data plane programmability since it relies largely on static header
    field matching. Within the context of this paper, we define data plane programmability
    as a feature that allows data plane devices, such as switches, to expose their
    packet-processing logic to the control plane in order for it be completely reconfigured
    if required. For example, the controller should be seamlessly able to modify the
    packet parsing and processing pipeline as required, add support for new protocols,
    and modify existing ones. To this end, P4 [116] is being increasingly recognized
    as “the programming language for the data plane”. P4 supports a wide variety of
    hardware ranging from ASICs to commodity CPUs, and allows the controller to specify:
    (i) a packet parser for extracting header fields, and (ii) a collection of match-action
    tables that process these headers. Further, we note that the operation of many
    applications depends upon the real-time state of the system, and relying on the
    controller to update the forwarding state each time introduces a significant latency
    burden. Consequently, there has been a growing body of research that seeks to
    develop stateful data planes, wherein some of the stateful packet processing and
    control tasks are offloaded to the data plane switches [117]–[119]. For example,
    a stateful data plane device may store some form of packet metadata, using it
    to process new packets belonging to the same flow. The general packet forwarding
    rules are still set by the controller, however, the presence of state information
    provides context for rule selection at the switch level. Programmable stateful
    data planes present a variety of inter-related research challenges. First, there
    is a need for a generic broad-based definition of state, along with abstractions
    that expose this state. Second, since packet-level state maintenance will be done
    by distributed switching devices, there is a need for a state consistency mechanism.
    A mechanism of this kind could potentially be enforced through the controller,
    to prevent conflicting forwarding actions. Third, security considerations present
    another important challenge. If the data plane switches are going to perform actions
    based on packet metadata, a malicious actor could easily use malformed packets
    to trigger state transitions for example. In this case, ultra-lightweight mechanisms
    will be needed to verify packet integrity. B. Automated Service Decomposition
    and Orchestration Network slicing allows for the provisioning of differentiated
    services over the same physical infrastructure [120], and has been a major research
    focus in the cellular domain [121]–[124]. However, the slice instantiation and
    deployment process is largely template-driven and requires manual configuration.
    For example, current 3GPP network slicing specification [120] is primarily based
    on the concept of network slice templates (NSTs). An NST explicitly defines the
    virtual network functions (VNFs) and associated service function chain that comprise
    a network service. Consequently, such network slicing primitives allow for the
    deployment of a limited set of network services, i.e., only those services for
    which a template has already been defined. Clearly, an approach of this kind is
    not scalable because: (i) it does not provide a mechanism to deal with new kinds
    of network services, and (ii) as network services increase in complexity, the
    effort required to create and maintain templates will become an operational burden.
    Going beyond the traditional template-driven model, we propose the concept of
    automated service decomposition and orchestration for network slice automation,
    as shown in Figure 6. To this end, and in line with 3GPP terminology, we identify
    three major stakeholders– the communication service customers (CSCs), the communication
    service providers (CSPs), and the virtual infrastructure service providers (VISPs).
    The CSCs request communications services from CSPs, who instantiate network slices
    and deploy them over infrastructure owned by VISPs to deliver the requested services.
    As part of the slice automation workflow, the CSCs provide high-level requirements
    such as those relating to latency, throughput, reliability, etc., along the lines
    of the emerging intent-based networking paradigm [125]. Next, the CSC automatically
    decomposes the request into a constituent VNF-forwarding graph (VNF-FG). It is
    important to note here that the service to VNF-FG mapping is not a based on a
    template, but instead makes use of deep learning models to extract service requirements
    and construct the corresponding VNF-FG. The resulting service-specific VNF-FG
    also contains the resource requirements for the constituent VNFs, allowing for
    seamless deployment onto the underlying infrastructure. Once the service has been
    deployed, continuous monitoring and real-time telemetry are used to ensure operational
    optimality. FIGURE 6. Automated network slicing framework. Show All C. Self-Driving
    Networks For decades, the network operator has served as the centerpiece of network
    operations. However, the increasing complexity of communications networks coupled
    with the constant state of flux brought forth by an ever-increasing number of
    connected devices has made the task of real-time network management nearly impossible
    for human operators. Therefore, there is a strong case for transitioning from
    operator-driven networks to self-driving networks. More specifically, self-driving
    networks are expected to allow for elastic utilization of resources, error-free
    operation, prompt and targeted responses to security incidents, and proactive
    rather than reactive service handling [126, §2]. Seeking complete automation of
    network management, a self-driving network is defined as a network where: (i)
    network measurements are task-driven and tightly integrated with the control of
    the network, and (ii) large-scale data analytics and machine learning models are
    used for network control, as opposed to closed-form models of individual protocols
    [127]. In a nutshell, self-driving networks should be capable of measuring, analyzing,
    and controlling themselves in an automated manner [128]. At the outset, a self-driving
    network should take a high-level goal or intent as input. Expanding upon the concept
    of intents, we note that there are broadly two types of intents– imperative and
    declarative. While the former describes in explicit detail how a particular procedure
    should be carried out, the latter just describes the end-goal without specifying
    how the stated goal should be achieved. For example, “reduce network congestion
    by shifting incoming traffic originating at ingress node 1 from load balancer
    2 to load balancer 3” is an imperative intent since it explicitly defines the
    steps that network must undertake in order to relieve congestion. On the other
    hand, “optimize network operations” is a declarative intent. However, a truly
    declarative intent of the kind described here would be extremely difficult to
    implement in the near future. Instead, semi-declarative intents that define more
    concrete goals would be far more helpful. “Minimize network congestion”, is one
    example of such an intent since it tasks the network with optimizing its operation
    by focusing on a specific objective, i.e., minimizing congestion. Based on the
    intent, the network is expected to determine: (i) the measurements that need to
    be performed, (ii) the corresponding inferences and learning that is required,
    and (iii) the actions that must be undertaken in response to the input intent.
    While a formal representation for self-driving networks is yet to be realized,
    a high-level architecture has been presented in Figure 7, highlighting the importance
    of large-scale data acquisition, real-time analytics and inference, and programmable
    data planes. FIGURE 7. High-level architecture for self-driving networks. Show
    All However, the realization of self-driving networks brings forth several research
    challenges as described next. Accurate Intent Definitions: As discussed previously,
    good intent definitions must toe the line between imperative and declarative.
    If the intent is primarily imperative, it defeats the point of automation. On
    the other hand, if the intent is purely declarative, the automation procedure
    becomes unnecessarily complex. Therefore, there is a pressing need for formal
    guidelines that put forward a clear framework for intent definitions. For example,
    a framework of this kind could take into account customer expectations in terms
    of throughput and latency, network-wide resource optimization goals, along with
    other application-specific functions and services that are required from the network.
    Automated Real-time Inference: Machine learning is vital to the automated decision
    making process in self-driving networks. However, previous work in this domain
    has largely focused on applying pre-existing learning techniques for network control,
    which are not well-suited for network data, given its high-volume, distributed
    nature, and rapid evolution. The major challenge here is the native integration
    of inference and control algorithms with the network’s decision and control fabric.
    In addition, network design needs to evolve to improve the quality of data that
    is input to the designed control algorithms [127]. Within the domain of self-driving
    networks, it is widely accepted that quality of data (QoD) is a pre-requisite
    for quality of service (QoS) [127]. In-band Telemetry: Research into in-band telemetry
    (INT) has been largely driven by the need for high quality network monitoring
    data, without introducing additional overhead. The INT approach makes use of programmable
    data planes to encapsulate additional metadata within the data packets themselves
    [129]. Examples of such metadata include switch processing times, buffer occupancy
    levels, and even specific policy rules. As packets traverses the network, they
    keep accumulating additional metadata, which can be extracted as desired, thus
    providing highly-detailed accurate sets of network data. To this end, there is
    a need to quantify the impact of INT on network performance [130]. In particular,
    metrics such as the relationship between the amount of metadata and packet size,
    the additional processing burden introduced, and the accuracy of the measurements
    obtained are all important parameters that merit careful consideration. SECTION
    VII. 6G Radio: Reconfigurable Transceiver Front-Ends The massive increase in the
    number of wirelessly interconnected devices, combined with the ever-growing demand
    for higher wireless data rates, is leading to an overcrowded electromagnetic (EM)
    spectrum. To overcome the spectrum scarcity problem and increase the capacity
    of wireless networks, communication at frequencies beyond RF (i.e., from the mmWave
    to the THz bands) is required. To meet the data-rate, reliability, and scalability
    requirements from RF to THz, transformative solutions are needed which include
    the design, implementation, and optimization of frequency-agile, ultra-broadband
    reconfigurable systems. A system of this kind is able to simultaneously sense
    and communicate over the full EM spectrum (1 GHz to 10 THz), and serves as a major
    contributor towards the infrastructure needed for the next generation of wireless
    communications. To realize this vision, pioneering contributions are required
    in terms of: (i) new devices that surpass the limits of CMOS technology by leveraging
    the state-of-the-art in materials science and nanoscale physics, (ii) heterogeneous
    integration of such devices which is compatible with the electrical, thermal,
    and EMI requirements for reconfigurability and manufacturing scalability, and
    (iii) novel all-spectrum dynamic sensing and communication algorithms, which maximize
    the achievable network capacity. The primary goal for 6G radio is to establish
    dynamic all-spectrum sensing and communication from RF to THz bands, therefore,
    transforming the way in which wireless devices sense, access, and share the EM
    spectrum. To achieve such a goal, key steps include: (i) intelligent all-spectrum
    sensing solutions, (ii) transceiver hardware design and implementation, and (iii)
    spectral and energy efficiency optimization as well as resource management. It
    is worth noting that currently reported works are fulfilling part of this grand
    goal by achieving dynamic spectrum sensing or multi-band communications over several
    frequency bands. It is our hope to motivate advanced solutions to realize all-spectrum
    communications through this section. A. Dynamic All-Spectrum Sensing and Access
    In recent times, a concentrated research effort at the physical and link layers
    has driven exciting progress at RF frequencies for individual cognitive radios
    (CRs). For example, a recently awarded research project by the Research and Innovation
    Program in the United Kingdom named “6G Mitola Radio” aims to establish self-regulating
    societies for wireless communications with fairness and high efficiency.4 This
    research will facilitate seamless convergence across heterogeneous wireless networks
    with intelligent decisions made by radios to maximize the quality of experience
    for end-users. One step further, a major challenge is to develop innovative spectrum
    sensing and sensing-informed communication and network optimization techniques
    for dynamic access to all-spectral resources. Within this context, the targeted
    breakthrough would be the development of wireless network-aware state inference
    using all-spectrum cartography for cognition over the swath of frequencies from
    RF to the THz bands, along with cartography-constrained algorithms for the physical
    and cross-layer control protocols. Artificial intelligence and associated learning
    algorithms should be investigated for dynamic spectrum sharing with a minimum
    cost in interference. The techniques developed should wholly exploit the capabilities
    of the hybrid front-ends, which include a multi-band transceiver design and solutions
    for resource management. B. Multi-Band Transceiver Design The optimal selection
    of materials and devices needed to enable all-spectrum communications is vital
    to the success of any multi-band transceiver design. Existing solutions mostly
    rely on CMOS for multi-band operations, but such an approach only works well in
    narrowbands. Moreover, solutions based on software-defined radios (SDRs) have
    high energy consumption and carbon footprint, consuming several Watts in operation
    [131], [132]. Instead, novel approaches based on metamaterials, MEMS switches,
    and even nanoelectromechanical systems (NEMS) switches should be sought to implement
    hybrid front-ends (Figure 8), which are able to simultaneously sense the EM spectrum,
    identify the best available band, and communicate over it, at frequencies anywhere
    from 1 GHz to 10 THz. Furthermore, fast-evolving deep learning algorithms serve
    as an efficient solution for identifying available spectrum, tuning channels,
    and adjusting RF power levels. FIGURE 8. Conceptual design of a hybrid front-end
    for dynamic all-spectrum sensing and communication in 6G. Show All To realize
    this vision, new techniques in materials and devices, integration and packaging,
    and spectrum sensing and communication are necessary. At the RF and microwave
    frequency bands, a combination of low-risk mature CMOS technology, with less-mature,
    potentially transformative technologies, including quantum cascade lasers (QCLs)
    and new plasmonic technologies based on graphene and other 2D materials will be
    able to provide optimal tunabilities as well as a high quality factor (i.e., the
    Q-factor). The heterogeneous integration of discrete devices into a fully functional
    front-end will require innovation to satisfy material compatibility, EMI shielding,
    thermal dissipation, and scalability requirements. On the other hand, metamaterial
    and nano-materials will be deployed at sub-THz and THz bands, based on recent
    advances in nano-tubes and graphene, as well as other single-atom-thin semiconductors.
    Space-time-frequency coding in metasurfaces will also allow programmable and fine-tunable
    radio access at mmWaves. Optimal control of the front-ends requires innovative
    all-spectrum sensing, utilization and sharing techniques, new waveform and hierarchical
    modulation designs to maximize the capacity and distance in ultra-broadband systems,
    and scalable networking solutions which are able to support the envisioned node
    density in future cyber-physical systems. The combination of several cutting-edge
    techniques can help maximize: (i) spectrum utilization (toward all-spectrum utilization),
    (ii) data-rates (toward terabit-per-second links) and, (iii) network user capacity
    (billions of interconnected wireless devices). The proposed technology will enable
    a plethora of applications in the consumer, military, industrial and medical fields,
    including transformative networking architectures designed to meet the scalability
    demands in future cyber-physical systems. C. Reconfigurable Front-End Scheme Accompanying
    the dynamic all-spectrum sensing and multi-band operation, agile front-ends should
    also be equipped with reconfigurability. In terms of hardware design, the plasmonic
    reflectarrays can be deployed in the 3D environment, with a size ranging from
    1 mm2 to 100 mm2 depending on the operating frequency (mmWave/THz-band). Owing
    to the sub-wavelength size of their elements, the plasmonic reflectarrays are
    able to reflect signals in non-conventional ways, which include controlled reflections
    in non-specular directions as well as reflections with polarization conversion
    [133]. In order to adapt to dynamic frequency operation, achieve various levels
    of directivity, and allocate multiple beams, the aperture of plasmonic reflectarray
    antenna can be controlled mechanically through folding, splitting, or combining
    in a 3D space. Existing prior art using origami antennas works well in systems
    using a single metal antenna, as reported in [134], [135]. However, to achieve
    reconfigurable continuous aperture antenna arrays, plasmonic reflectarrays offer
    a higher degree of freedom with the compactness of unit element distribution.
    On the other hand, electronically-controlled reconfigurable antenna arrays are
    envisioned by leveraging the tunability of plasmonic antennas. In particular,
    one of the relevant properties of graphene-based plasmonic nano-antennas is the
    possibility to change their resonant frequency by utilizing a small voltage to
    modify their Fermi energy [136]. The possibility to tune an antenna (or group
    of antennas) at different frequencies without any mechanical modification (as
    opposed to other multi-band antenna arrays that utilize MEMS or NEMS to create
    origami type structures [137]) enables beamforming not only across space but also
    across frequencies. D. Open Problems The biggest hurdle to be overcome lies in
    the implementation of an integrated ultra-broadband hybrid front-end that is capable
    of sensing and communication from the RF to the THz bands, over a target distance
    of a few hundred meters. Meeting this multidisciplinary challenge requires us
    to: (i) close the THz gap by developing new device technologies, (ii) design and
    integrate re-programmable circuitry, interconnects and antennas that can support
    all-spectrum operation, (iii) develop new material integration and packaging techniques
    to satisfy the electrical, thermal and EMI requirements of disparate bands, and,
    (iv) develop scalable all-spectrum communication using the front-ends. SECTION
    VIII. Ambient Backscatter Communications In the realm of IoT, sensors are expected
    to function in various environments with long-lasting battery life. Solutions
    such as radio frequency identification (RFID) utilize the backscatter technique
    to modulate and reflect RF signals instead of generating them, which can achieve
    a significant degree of energy saving. However, existing modulated backscatter
    solutions have stringent requirements in terms of the proximity between the backscatter
    transmitter and the RF source, due to the attenuation of the signal over long
    distances. Besides, the modulated backscatter transmitters are passive, which
    means that they cannot transmit data without requests initiated by backscatter
    receivers [138]. Furthermore, the issue of self-interference may arise when the
    backscatter receiver and RF sources are co-located. Hence, in order to achieve
    better energy efficiency with a higher degree of flexibility and scalability,
    new solutions are required in the 6G IoT network. Currently, with more small cells
    being deployed in outdoor and more access points in indoor environments, the RF
    signals are covering a wide range of surroundings, and can be considered as a
    resource to be utilized by secondary radio links without requiring extra power.
    The system that employs such a technique is called an “ambient backscatter communication
    system”. In an ambient backscatter communication system, transmitters can harvest
    the surrounding and continuous electromagnetic waves radiated by TV towers, base
    stations, as well as access points, use simple circuits for modulation, and reflect
    them towards receivers. There is no need for dedicated spectrum bands for such
    ambient backscatter transceivers to operate, nor complex electronic components
    (e.g., analog-to-digital converters) to process signals. A. Operation Principles
    of Backscatter Communications In general, as the name suggests, backscatter communication
    systems reflect signals impinged on a backscatter transmitter in the direction
    of the signals’ origin, and since it is not a perfect specular reflection, the
    signals are scattered within a certain angular range of the environment. A backscatter
    communication receiver within the range can thus pick up the signals. In particular,
    backscatter communications have three variations in terms of architecture, which
    are monostatic, bistatic, and ambient backscatter communications, respectively.
    Here we briefly explain the first two types and then focus on the last one. As
    the most commonly adopted backscatter communication approach with widely-used
    RFID applications, monostatic backscatter communication systems have the simplest
    setup, which consists of a backscatter transmitter and a reader. The reader has
    both an RF signal source and a backscatter receiver embedded with a switch to
    change the operation mode. Once the receiver sends out a request, the RF source
    activates the backscatter transmitter, which then modulates and reflects the EM
    waves impinged on it back to the receiver, as shown in Figure 9. Such a design
    is mainly used for short-range RFID applications [138]. Nonetheless, two drawbacks
    of the monostatic backscatter communication architecture are (i) the reader cannot
    perform full duplex communication due to the switch mechanism, and (ii) the signals
    experience round-trip path loss being sent from the reader to the transmitter
    and then reflected back to the reader. FIGURE 9. Illustration of backscatter communication
    systems. Show All On the contrary, in the bistatic backscatter communication architecture,
    the RF source and receiver are separated, as shown in Figure 9, which provides
    higher flexibility in the spatial domain. With multiple RF sources and backscatter
    transmitters well placed, the serving range can be remarkably extended compared
    to the monostatic backscatter scenario. Despite this improvement, it is more costly
    for bistatic backscatter communication systems to operate in real networks, since
    they require RF sources and transmitters to be well-placed so as to achieve desired
    performance, and most of the times this condition can be difficult to satisfy,
    especially in a sophisticated network environment, such as an indoor office or
    dense urban scenarios. B. Mechanism of Ambient Backscatter Communications Different
    from the monostatic backscatter device where the transmitting and receiving components
    are separately located and the RF source is co-located with the receiver, devices
    of ambient backscatter communication systems consist of both the transmitter and
    receiver. Additionally, distinct from bistatic backscatter communications, ambient
    backscatter communications do not require dedicated RF sources to provide exclusive
    services, which can reduce infrastructure and maintenance expenditure significantly.
    Therefore, the ambient backscatter communications provide the most energy-efficient
    solution for sensors in the IoT network in 6G. Specifically, in one proof-of-concept
    study for utilizing the always-on radio signals for ambient backscatter communications
    reported in [139], TV signals serve as the RF source, which can be amplitude and
    sometimes frequency-division modulated. In the ambient backscatter transmitter
    design, a simple switch consisting of a transistor and connected to the antenna
    can be used to modulate the impedance of the antenna: a mismatch of impedance
    indicates a reflection mode of the impinging signals, whereas a matched impedance
    allows for the signal being absorbed by the antenna [139]. The power consumption
    of such 1-bit modulation of signals is minimal. At the receiver side, by demodulating
    the received sequence of “1” and “0”, signals can be successfully recovered. However,
    it is worth noting that since the reflected signals from the ambient environment
    already contain encoded information from the RF source system (e.g., cellular
    or TV networks), the receiver design should take into consideration how to extract
    the backscattered signals from the mixture. Besides the simplicity in transceiver
    implementation, ambient backscatter communications are not restricted to a single-band
    operation. In fact, the ambient backscatter transceivers can operate in the wide
    range of super high frequency (SHF) bands, covering Bluetooth, Wi-Fi, and other
    bands [138]. C. Open Problems in Ambient Backscatter Communications Spectral and
    Energy Efficiency: Currently, the research and development of ambient backscatter
    communications is still in its infancy. As mentioned before, careful planning
    of backscatter devices is crucial for achieving good performance. Due to the randomness
    in IoT device deployment, current solutions fall behind in terms of the targeted
    spectral efficiency. Specifically, the randomly-located IoT devices should utilize
    ambient backscatter links to achieve a satisfying throughput while maintaining
    an extended transmission distance. Additionally, even though individual backscatter
    communication devices demonstrate good energy performance, an IoT network comprising
    hundreds or even thousands of such devices might still require optimization of
    energy efficiency on a system level. Protocol Design: Existing ambient backscatter
    communication systems are mostly used for dedicated application-specific purposes,
    and thus lack good compatibility with other wireless communication systems. Standardization
    and protocol design are necessary to formalize the key operation and management
    aspects of ambient backscatter communications, such as packet size, routing protocols,
    among others. SECTION IX. Internet of Space Things With CubeSats and UAVs The
    Internet of Space Things (IoST) is a spatial expansion of the Internet of Things
    which primarily focuses on terrestrial use cases. For future communication networks,
    this expansion is necessary for the following reasons: (i) the IoT relies heavily
    on existing infrastructure and hence lacks flexibility as well as scalability,
    (ii) global coverage is impossible using traditional IoT solutions, especially
    in remote areas including the North and South Poles, due to the imbalance of construction
    expenditure and service revenue, and (iii) limited heterogeneity and spectrum
    resources in the IoT network. To this end, IoST is envisioned as a ubiquitous
    cyber-physical system spanning ground, air, and space, with applications in monitoring
    and reconnaissance, in-space backhauling, and holistic data integration [140].
    More specifically, as shown in Figure 10, IoST consists of the ground station,
    customer premises, and on-Earth sensing devices which form the ground segment,
    and the CubeSats, UAVs, and near-Earth sensing devices that form the space segment.
    The ground-to-satellite links (GSLs) connect the IoST Hubs with CubeSats to exchange
    requests and data, and the inter-satellite links (ISLs) relay information to neighboring
    CubeSats, in both the same orbit as well as adjacent orbits. Further, the UAVs
    establish links with each other, as well as sensors and CubeSats to form a localized
    data aggregation layer. To this end, we note that UAVs are expected to feature
    heavily in the upcoming 3GPP Releases 16 and 17 [142]. Further, since the space
    segment forms a vital component of the system, research relating to the development
    of small satellites, or CubeSats [143], for use within IoST is of critical importance
    [140]. FIGURE 10. Illustration of the IoST [140]. Show All CubeSats are a set
    of miniaturized satellites with sizes ranging from 1U to 6U (a “U” is 10×10×10c
    m 3 ). Currently, CubeSats are being deployed for a variety of applications including
    Earth sensing [144], positioning, and IoT and machine-to-machine communications.
    Compared to traditional LEO satellites, CubeSats present a number of advantages
    relating to (i) lower costs and shorter development cycles and (ii) higher flexibility
    and scalability [141]. Normally the development cycles range from three to seven
    years for traditional LEO and GEO satellites and the costs are extremely high.
    Also, since the payloads are pre-determined in the LEO and GEO satellites from
    the period of development until deployment, it is difficult to reconfigure any
    component in the middle of the process. However, CubeSats’ development can be
    done in a remarkably shorter time using COTS components with much lower costs.
    This also guarantees that CubeSats are easily reconfigurable. To this end, we
    have recently proposed a new next-generation CubeSat hardware concept as shown
    in Figure 11 [141]. The proposed CubeSat design includes an all-new communications
    subsystem for seamless operation in a wide variety of frequency bands. More specifically,
    the novelty of our design concept is characterized by the presence of multi-band
    transceivers and antennas that are able to support wireless communications at
    microwave, mmWave, and THz frequencies as detailed in the following section. Through
    this unique CubeSat design, we can potentially achieve data rates in excess of
    100 Gbps [141]. FIGURE 11. Conceptual design of a next-generation CubeSat [141].
    Show All A. Multi-Band Communications Subsystem The primary motivation for a multi-band
    communications subsystem comes from the fact that existing CubeSats have limited
    communications capabilities, largely relying on spectrum ranging from the L- (1–2
    GHz) till the Ka- (26.5–40 GHz) band. There are two major drawbacks associated
    with this approach. First, the traditional frequency bands are becoming increasingly
    prone to congestion [145]. Second, the Tbps-level throughput required by IoST
    cannot be achieved with existing frequency bands. To overcome the spectrum scarcity
    and capacity limitations in current satellite networks, we have proposed the use
    of multiple frequency bands from RF to THz spectrum, in IoST [141]. The use of
    such frequencies has been made possible by advances in high-frequency device development
    [26], [146]. More specifically, as part of the multi-band communications subsystem,
    we have developed both multi-frequency transceivers as well as antenna systems,
    as described next. As shown in Figure 12, in our proposed multi-band transceiver,
    we use two complementary approaches, namely, an electronic frequency up-converting
    chain and an optical frequency down-converting chain, to generate signals at different
    frequencies. With regard to the electronics-based approach, the primary idea is
    to use frequency splitters in order to extract the intermediate frequencies for
    outputs. For example, as shown in Figure 12, the signal at frequency f 1 is considered
    as the intermediate output when producing signals at a higher frequency f 2 .
    FIGURE 12. The proposed all-spectrum signal front-end designs [141]. Show All
    On the other hand, the photonics-based approach involves the down-conversion of
    optical signals. As shown in Figure 12, multi-band signals are generated by heterodyning
    two input signals with a Mach-Zehnder modulator. The resulting RF signal has a
    frequency equal to the difference between the two inputs. The generated signal,
    along with the two input signals, serves as the final output. Distinct from the
    commonly used up-conversion and down-conversion techniques where intermediate
    frequency products are abandoned, our approach harvests them and utilizes them
    as part of the multi-band communication system. These multi-band frequencies can
    be assigned to the GSLs and ISLs dynamically to accommodate various service requirements.
    To this end, within IoST, the GSLs make use of the more robust microwave and mmWave
    frequencies, while high-capacity THz links form the ISLs. In addition to multi-band
    transceivers, CubeSats in the IoST are also equipped with multi-frequency antenna
    systems. In particular, as discussed in Section III, the use of THz links allows
    for very large antenna arrays that serve as the basis for massive MIMO and UM
    MIMO communication schemes. More specifically, we note that there exist multiple
    options when it comes to the design of multi-band antenna arrays. The first approach
    involves the use of NEMS, MEMS, and origami structures to create physically re-configurable
    antennas, wherein the size of the radiating elements can be changed physically
    with a view to adjust their resonant frequency. On the other hand, the second
    approach proposes the use of materials such as graphene to create electronically
    tunable nano-antenna arrays [147], [148]. In this case, the resonant frequency
    can be controlled by modulating the graphene Fermi energy or chemical potential.
    This allows for tuning of the antenna to resonate at different frequencies without
    physically changing its size, in contrast to the MEMS-based approach. B. System
    Constellation Design Within the context of IoST, an ideal constellation design
    is crucial to achieve true global coverage and satisfactory link performance.
    However, conventional LEO constellations are typically characterized by the presence
    of fewer than a hundred satellites, for example, the CubeSat-based IoT system,
    Astrocast, has a maximum of 64 satellites [149]. At the outset, the coverage and
    connectivity offered by such systems leaves much to be desired. Motivated by the
    need for improved coverage, reliable connectivity, and increased redundancy, mega-constellations
    of several hundred satellites have gained significant traction over the past year
    [150]. Mega-constellations provide several advantages over traditional constellations
    including but not limited to increased coverage density, improved connectivity,
    and higher redundancy. More specifically, constellation design typically involves
    solving for several inter-related parameters such as: (i) the apogee and perigee
    radii, (ii) the orbital eccentricity, (iii) the number of CubeSats per orbital
    plane, (iv) the number of orbital planes, and (v) the initial longitude of the
    ascending node, argument of perigee, and true anomaly of the CubeSats. While a
    fairly challenging problem in itself, the presence of an extremely large number
    of satellites further serves to complicate the system design. Consequently, the
    existing state-of-the-art constellation design frameworks are largely geared towards
    the design of systems with a few dozen satellites at best. To this end, we have
    proposed a highly-scalable and customizable constellation design framework that
    takes into consideration both coverage and connectivity parameters [151]. Through
    the use of novel metrics such as spherical Voronoi tessellation based coverage
    characterization and ISL feasibility-based connectivity parameters, we are able
    to demonstrate that the resulting IoST constellation achieves a performance level
    that is similar to existing state-of-the-art mega-constellations such as Starlink,
    while requiring only a quarter, i.e., less than 500, of the satellites [151].
    C. Network Management IoST encompasses a vast infrastructure spanning both the
    Earth as well as space. A complex network of this kind has much to benefit from
    fine-grained real-time control that is well suited for tackling the peculiarities
    of the space environment, namely temporal topological variation and long delays.
    Going beyond the traditional bent pipe nature of satellite communications systems,
    IoST makes extensive use of SDN and NFV to significantly improve network resource
    utilization, simplify network management, and reduce operating costs [140]. In
    a manner similar to the infrastructure-as-a-service (IaaS) paradigm, IoST intends
    to deliver CubeSats-as-a-service, with promising results as shown in [140]. In
    particular, we have demonstrated that through the use of SDN, it is possible to
    achieve sub-second end-to-end latencies. More specifically with the domain of
    network management, IoST introduces the novel concepts of virtual CSI (vCSI) for
    joint optimal physical-link layer resource allocation, and stateful segment routing
    (SSR) for overcoming challenges associated with the high latency space segment.
    In particular, concerning the latter, IoST extends the traditional SDN paradigm
    by including support for state-based packet forwarding that takes into account
    the topological configuration of the network at any given instance of time, while
    the use of segment routing helps in the minimization of control traffic, in addition
    to a higher level of demand satisfaction and load balancing as demonstrated in
    [152]. Further, IoST also employs predictive algorithms to preemptively detect
    GSL outage events, which when coupled with gateway diversity result in the realization
    of proactive handovers that minimize handover interruption time. In addition,
    IoST proposes the use of containzeration [153] in CubeSats for achieving lightweight
    hardware virtualization without significant overhead. Going forward, we note that
    the aforementioned techniques will play a vital role in the realization of pervasive
    cyber-physical systems of this kind. SECTION X. Cell-Free Massive MIMO Communications
    In 5G wireless networks, massive MIMO communications have been tested and deployed
    at base stations (BSs) which are equipped with more than one hundred antenna elements
    to increase the antenna array gain and take advantage of the diversity gain. A
    related concept is network MIMO, which, instead of packing more than one hundred
    antenna elements at a single BS, forms a coordinated framework consisting of multiple
    BSs, each with multiple antennas. A coordination scheme of this kind can achieve
    spatial diversity by allowing a single user to be served by more than one BS at
    the same time, overcoming the disadvantage of bad channel conditions if only one
    BS is connected to the user, and eliminates inter-cell interference [154]. However,
    a detailed comparative study has shown that massive MIMO communication schemes
    outperform network MIMO with respect to end-user received signal strength and
    overall costs in configuration [155]. Going one step further, in order to effectively
    eliminate inter-cell interference caused by users located at cell boundaries,
    based on the ideas of distributed MIMO communications and coordinated multi-point
    (CoMP) communications, researchers have proposed the concept of cell-free massive
    MIMO communication. In a scheme of this kind, the originally densely-packed antenna
    array set with a few hundred elements at the BS is distributed in a fairly large
    area in the form of smaller sets with fewer than 10 antenna elements, while still
    serving a similar number of users in the same area [156]. As shown in Figure 13,
    the main difference between cell-free and classic massive MIMO communication systems
    is that instead of associating each user terminal to a cell with a BS equipped
    with a large number of antenna elements, it relaxes the restriction of cell boundaries,
    which can significantly reduce or even eliminate the inter-cell interference.
    Without cell boundaries, all BSs, or a subset of BSs, can serve users simultaneously
    in a coordinated manner. In coordination, the cell-free massive MIMO BSs can share
    with each other the data to be sent to users through fronthaul links. FIGURE 13.
    Cell-free massive MIMO in comparison with classic massive MIMO. Show All It has
    been shown that the BSs can use their local channel state information (CSI) to
    achieve satisfactory performance and avoid the excessive computation complexity
    associated with sharing global channel conditions with all BSs [157]. The local
    CSI can be estimated in the uplink channel in a time division duplex (TDD) mode.
    Then, precoding is performed based on the obtained channel information at the
    BSs, before data transmission in the downlink channel. The transmit power and
    precoding vector can be determined based on the geographic proximity of users
    to the BSs. Compared to the small-cell architecture in 5G, which consists of non-cooperative
    base stations that can serve up to 100 users per cell with a smaller area (e.g.,
    up to a 200-meter cell radius) and reduced power in signal transmission (e.g.,
    up to 10 Watts), a cell-free massive MIMO communication system achieves significantly
    better performance, since each user can be served by a dedicated access point.
    A reported study in [158] has demonstrated that the cell-free massive MIMO scheme
    improves 95%-likely per-user throughput by five times and by ten times under correlated
    shadow fading, with respect to the small-cell solution. More specifically, the
    same study has reported that when considering realistic channel conditions, including
    pilot contamination and imperfect CSI, cell-free massive MIMO systems demonstrate
    much higher throughput compared to small cells and, more importantly, are more
    robust to impacts such as shadow fading, non-coherence interference, as well as
    noise [158]. However, we also note the following challenges: (i) due to the issue
    of aliasing, channel estimation for signals received by different antenna elements
    is more complicated compared to that of ordinary massive MIMO communications,
    (ii) with the significantly increased synthesized aperture size, the range of
    near-field propagation grows larger, hence requiring a different channel model
    for characterizing the large- and small-scale channel parameters. A. Channel Characteristics
    of Cell-Free Massive MIMO Communication Systems It has been theoretically proven
    that as the number of antenna elements approaches infinity, adversarial channel
    effects, including inter-cell interference, small-scale fading, and others, will
    disappear [159]. In cell-free massive MIMO communications, such effects will also
    have a negligible impact on propagation channels. Specifically, channels under
    a coordinated scheme of this kind will satisfy the conditions of favorable propagation
    [160]. Favorable propagation conditions imply that the channel vectors between
    the BSs and UEs are orthogonal, so that the sum-rate can be maximized. This characteristic
    is most prominent in classic massive MIMO communications [161]. In cell-free massive
    MIMO systems, it has been shown that favorable propagation conditions can be achieved
    given that the number of APs is fairly large (with an approximate density of 1000/km2).
    B. Open Problems in Cell-Free Massive MIMO Communications As the domain of cell-free
    massive MIMO communications is relatively new, there are several open problems
    that merit further investigation. Among them, we posit that coordination and optimization
    challenges will critically affect the entire system performance and future deployments.
    User Scheduling: Extensive studies have been conducted on channel characterization
    and capacity analysis. However, the prior art does not take into consideration
    scenarios involving networks with a large number of users to serve. In such cases,
    there might be an upper bound for the number of APs to serve a user in order to
    maintain an acceptable-level of average throughput. Current works assume that
    all users will be served simultaneously under the same frequency resource block.
    However, when the number of users grows to a threshold where they can no longer
    be served at the same time, a scheduling scheme that achieves fairness should
    be considered. Location Optimization of APs: Existing works in cellular networks
    draw heavily upon stochastic geometry where cell structures follow 2D Voronoi
    tessellation and geographically separated BSs that serve cell-edge users under
    the coordinated multipoint (CoMP) scheme to improve overall system efficiency
    and overcome inter-cell interference through scheduling [162], [163]. It is therefore
    crucial to optimize the placement of BSs under practical link-level constraints
    such as signal-to-interference ratio as well as success probability for individual
    links to enhance network fairness [164]. In cell-free massive MIMO, since no cell
    boundaries are assumed, system-level performance pertaining to locations of APs,
    random scatterers, as well as users should be thoroughly investigated and optimized.
    SECTION XI. Technologies for Beyond 6G Thus far, we have presented in great detail
    the key drivers that are expected to play an integral role in the next-generation
    of wireless networks. However, in addition to these, we also note the presence
    of several promising early-stage technologies that are tipped to revolutionize
    how we perceive data communications in the near future. To this end, in this section,
    we discuss three such promising paradigms, namely, the Internet of NanoThings,
    Internet of BioNanoThings, and quantum communications. A. Internet of NanoThings
    In addition to the need for more spectrum resources to accommodate a plethora
    of wireless devices and services, a variety of transformative wireless communications
    scenarios are also envisioned to become a reality in the near future. In particular,
    with the advent of wireless ubiquity, we note the existence of situations where
    electromagnetic waves do not yield acceptable performance or lack reachability
    due to hardware limitations, such as in high-salinity water or intravascular channels
    where the transmission range can be extremely short. In the aforementioned application
    scenarios for THz band communications, as the frequencies of operation increase,
    the wavelengths of signals fall into the nanometer range (i.e., 10−9 to 10−7 meter
    in size), thereby motivating studies on nano-network communications [165], [166].
    Different from those operating at lower frequencies in the microwave range, the
    devices and transceivers used in the Internet of NanoThings (IoNT) are in the
    scale of nanometers, and thus behave differently from classical wireless communication
    systems. Given the much smaller size, each nano-thing consumes much less energy
    and is envisioned to be self-powered (e.g., via vibrational energy harvesting
    using piezoelectric nano-generators [167]). Besides conducting signal transmission
    tasks, the nano-things can also perform basic processing and data storage, as
    well as enabling new nano-sensing capabilities with higher sensitivity. Current
    advancements in nanotechnologies provide several promising candidate materials
    with various dimensions for creating such nano-machines, including a thin strip
    of graphene named graphene nanoribbons, graphene in form of a three-dimensional
    (3D) roll named carbon nanotubes, and graphene spheres. Communications in the
    paradigm of nano-networks mainly falls under two categories, which are (i) encoded
    signal bits being carried with molecules, which follows a diffusion-based mechanism
    elaborated in Section XI-B1 and (ii) plasmonic radiation on metamaterial-based
    antennas including graphene and carbon nano-tubes operating in the THz band. These
    plasmonic antennas [136], [168]–[170] leverage the physics of Surface Plasmon
    Polariton (SPP) waves, i.e., confined EM waves resulting from the global oscillations
    of electrons at the interface of a conductor material and a dielectric material,
    to efficiently radiate at the target resonant frequency while being much smaller
    than the corresponding wavelength. This property allows them to be integrated
    in very dense arrays, beyond traditional antenna arrays. The ratio between the
    free-space wavelength λ and the SPP wavelength λ SPP is known as the plasmonic
    confinement factor, and depends on the plasmonic material properties and the operation
    frequency. The higher the confinement factor, the smaller the antennas and the
    higher the density in which they can be integrated. 1) Essential Components in
    the IoNT Similar to traditional communication networks, several key components
    are seen in IoNT [171]: Nano-nodes are the basic functional units in the nano-network,
    and have sizes ranging from 1 to 100 nanometers, and can form a cluster to forward
    and receive signals. A typical nano-node with full transceiving capability contains
    the following elements: a nano-antenna and a plasmonic nano-tranceiver based on
    graphene advancements to propagate SPP waves, a nano-processer with operating
    frequency close to 1 THz, nano-actuators, nano-sensors which can sense external
    force, gas molecules, and biological objects such as antigens and antibodies,
    a nano-memory which allows storage of a bit signal in a single atom, a nano-battery,
    and an energy nano-harvester which transfers energy to power other elements [172].
    Limited by the computation capabilities and battery life, the signals are mostly
    pulse-based for the easiness of detection and transmission. Nano-routers control
    the behavioral patterns of the nano-nodes, aggregate information, and determine
    the optimal paths for signals to be forwarded. The nano-routers are equipped with
    higher energy and computational resources. When a specific query is created from
    the command center, nano-routers need to select the optimal routes that can reach
    the nano-nodes to collect their data and report back. Due to the limited transmission
    range, a pulse-based signaling is preferred to assess the reachable range of nano-nodes
    in order to minimize outage probability and establish the desired routes. Gateways
    serve as the remote controller of the IoNT and connect over the internet to service
    providers. The gateways can be common smart devices such as smartphones and tablets,
    among others. In order to achieve a manageable network with hundred or even thousands
    of nano-nodes dissipated in sophisticated communication environments, gateways
    should devise a holistic approach in disseminating commands and queries, coordinating
    between possible collisions, and processing noisy data, which requires a drastically
    different network framework than the conventional network architecture. Based
    on the arbitrary pattern of the nano-nodes, potential solutions can be found with
    the assistance of artificial intelligence, which does not require pre-established
    model for prediction. It is worth noting that major device technology in the IoNT
    is still under design and development. Although a few types of individual components,
    such as nano-sensors, have been made available, it is estimated that a major paradigm
    shift for the IoNT is expected to occur in the second half of the 2020s. 2) Applications
    of the IoNT In the realm of IoNT, applications can be primarily found in body
    area networks and short-distance local environments. Three typical application
    scenarios are described as follows (also illustrated in Figure 14): Nano-Cameras:
    The nano-cameras are based on nano-photosensing and nanotechnology to sense, combine,
    and process light signals before transforming them into electric signals. This
    system includes nano-photodectors, nano-lens, nano-batteries, and nano-memories
    in order to achieve fine-resolution imaging and signal processing. The nano-cameras
    can be applied to a wide range of scenarios including but not limited to intravascular
    imaging and fracture detection in oil pipelines [171]. On-Chip Networks: With
    microchips getting more compact in dimension while the complexity of functionality
    grows, on-chip signal transmission has become significantly more challenging.
    Currently, issues relating to CPU scalability and efficient memory synchronization
    have driven research trends towards wireless network-on-chip (WNoC) solutions,
    which can replace wired connections on conventional chips and take advantage of
    short-range communication in the nano-network at THz-band frequencies [173]. Nano-robots
    for IoNT: The nano-robots in nano-networks can be deployed in environments such
    as nuclear power plants and oil pipelines which might be hazardous for humans
    to perform tasks but require high precision and do not allow massive drilling
    or digging over existing infrastructure. Under these circumstances, nano-robots
    can be dissipated to sense and collect data relating to chemical concentration,
    and fluid speed, among others. By forming ad-hoc networks, the nano-robots can
    aggregate and forward data packets to gateways in the IoNT. Nano-robots are also
    being widely researched in biomedical engineering fields. To this end, Section
    XI-B will delve into nano-robots for healthcare applications. FIGURE 14. Application
    scenarios of molecular communications in IoNT and IoBNT. Show All 3) Open Problems
    in the IoNT The significant size shrinkage brings three major challenges. The
    first one is power efficiency optimization. Even though nano-devices consume power
    at the level of microwatts when transmitting femtosecond long pulses, in order
    to cover an area of a few square meters such as an office or a meeting room, the
    energy consumption factors in as a major constraint in maintaining a satisfying
    overall network performance. New designs on duty cycles for nano-transceivers
    should be proposed and evaluated, as well as new clustering algorithms in order
    to group nano-transceivers in close proximity for adaptive operations. The second
    open issue is interference control, which has been extensively studied in classic
    wireless network scenarios. However, the conventional approaches cannot be directly
    applied to the IoNT realm, due to the higher density of nano-transceivers in space
    and pulse-based signal transmission schemes. Self-interference becomes the most
    prominent issue for nano-transceivers when full duplex mode is deployed and hence
    requires novel scheduling algorithms to mitigate this adversarial effect. Additionally,
    new modulation and coding schemes should be developed to fit the need of nano-devices
    on spectral and power efficiency while maintaining a low probability of crosstalk
    among links. The third challenge resides in the network protocols. Since the IoNT
    will foreseeably function in a manner that is drastically different from the IoT
    due to differences in channel conditions, limited scale of operation, as well
    as miniaturized devices, the protocol stack design still remains an open field
    for exploration. B. Internet of BioNanoThings for Health Applications Highly relevant
    to IoNT, with its unique characteristics and applications, is the concept of the
    Internet of BioNanoThings (IoBNT). First introduced in 2015, the IoBNT has garnered
    significant traction in its efforts to synergistically combine telecommunications
    with healthcare solutions [174]. The IoBNT is a network of molecules which can
    communicate with each other. The types of molecular communications include artificial
    cells which act as gateways to translate between different molecule types, or
    a bio-cyber interface which can convert molecular signals to electrical ones and
    transmit to external devices for further processing [175]. In applications relating
    to human healthcare, the IoBNT harbors many unique challenges and opportunities.
    First, the interdisciplinary research on both communications and data analytics
    can greatly facilitate the modeling of biological processes, including cancer
    cell formations and Alzheimer’s disease, and further design effective control
    measures for such diseases. Second, even though expressions of genetic codes at
    the cell- and organ-level can vary remarkably, in a manner analogous to various
    types of data applications in wireless networks, communication models can be developed
    and exploited to conceive a generally applicable health information framework.
    Third, the holistic network architecture envisioned in the IoBNT will integrate
    components at heterogeneous levels including within cells and among tissues, organs,
    as well as systems, before eventually connecting to the outside Internet for physicians
    to perform metric evaluations and propose treatment plans accordingly. However,
    healthcare solutions that are to be realized in such complicated biological and
    molecular environments should be built upon a solid understanding of the physics
    behind molecular communication and advanced statistical analysis tools in order
    to unveil the principles behind the seemingly random molecular movement. 1) Essential
    Communication Models in IoBNT Different from classic wireless communication channels
    based on the propagation of electromagnetic waves, molecular communication (MC)
    channels rely on the mechanism of molecular movement to transmit information.
    The main difference between an MC channel and the classical wireless channel is
    that the transmission medium presents different forms, such as fluids of several
    chemical compositions in blood vessels, plasma membranes of neurons, and so on.
    Based on the motion of molecules in such diverse mediums, end-to-end channel models
    have been developed to characterize the capacity, noise, and interference in various
    communication scenarios [176]–[178]. Particularly, in the diffusion-based MC model,
    information is encoded in various forms, for example, based on different concentration
    intensities and distinct release times of molecules. The nano-device acting as
    a transmitter emits such encoded molecules to the wireless molecular channel.
    At the receiver side, another nano-device decodes the signals based on the quantified
    received intensities or times of arrival, given that the channel remains stationary
    for the duration of transmission. In such transmissions, some molecules will get
    dispersed in the channel and will not be received by the target nano-devices,
    they are hence treated as noise, and channels with such residual molecules are
    characterized as channels with memory. For such channels, the theories of Fick’s
    diffusion and particle location displacement are used to characterize the channel
    capacity as a function of a collection of parameters, including the diffusion
    coefficient of the channel, the temperature, the distance between end-transceivers,
    and the bandwidth of the transmitted signal [178]. 2) IoBNT in Public Health Applications
    Late 2019 and 2020 have seen the novel coronavirus disease named COVID-19 spread
    worldwide, causing high fatalities and a plethora of other public health issues.
    More generally, such outbreaks, including the severe acute respiratory syndrome
    (SARS) in 2002, the middle east respiratory syndrome (MERS) in 2012, the Ebola
    virus disease in 2014, and the seasonal influenza, raise questions about the manner
    in which public health systems should react to such epidemics and pandemics. The
    widespread havoc caused by pandemics calls for effective means to identify new
    viruses, understand their mechanisms of viral infection, and devise efficient
    tools for treatment and vaccination. In order to facilitate the development of
    antiviral and preventive solutions, researchers have looked into creating biosensors
    that can monitor the cleavage of proteases within infected cells [179]. Proteases
    are generated as a result of the cell being infected by the genome of coronavirus,
    which is a type of RNA virus. Other byproducts include synthesized polyproteins
    which can replicate and transcript to generate more RNAs, and structural proteins
    that can construct new virions [180]. Two types of proteases found in the coronaviruses
    that cause SARS and MERS (i.e., SARS-Cov and MERS-Cov) are papain-like protease
    and 3C-like protease. The biosensor, which is based on luciferase, is used to
    identify potential broad-spectrum coronavirus papain-like or 3C-like protease
    inhibitors. The SARS-CoV-2 virions that cause the COVID-19 disease have a diameter
    of 50–200 nanometers approximately [181], and infect the human respiratory system
    via human-to-human spread, in the form of droplets discharged when an infected
    person coughs or sneezes [182]. COVID-19 has thus far posed unprecedented challenges
    worldwide in testing, treatment, and vaccine development. The IoBNT is envisioned
    to have immense potential in the molecular diagnosis of emerging viruses of this
    kind. The nanosensors, which can be firefly luciferase-based or other reporter
    genes, can be used to examine the reverse transcription polymerase chain reaction
    in collected samples. Other tests include using bio-nano-sensors to identify antibodies
    from blood samples to examine if the person is infected. In terms of treatment,
    although no antiviral drugs are available for COVID-19 yet, studies on influenza
    treatment can shed light on how the IoBNT could assist in future solution development.
    A critical step for treatment is the antiviral intervention, which blocks the
    intracellular signaling pathways to prevent influenza viruses from replication.
    A reported solution preventing the virus from replication is to use engineered
    bacteria (i.e., Escherichia coli) to trap the Ebola virus [183]. In the reported
    work, the blood of a patient with the Ebola virus infection is transmitted to
    a microfluidic chamber tube outside the body which contains the engineered bacteria.
    The scattered bacteria can then achieve protein binding with the Ebola virus using
    chemical bind force and synthetic protein binding receptors [183]. More importantly,
    the IoBNT serves a unique role as a holistic solution to not only monitor limited
    types of cells (e.g., squamous epithelial cells from nasopharyngeal swabs for
    COVID-19 tests), but also across different tissues and systems. It is found that
    such coronaviruses can also cause damage to digestive and neurological systems
    [181], [184]. Hence, a series of connected bio-nano-things consisting of various
    types of engineered bacteria can operate simultaneously to improve test reliability
    and treatment efficiency. 3) Artificial Intelligence in IoBNT for Health Applications
    In the IoBNT network, different systems demonstrate a wide variation in characteristics,
    thereby requiring varied analytical approaches. For example, in cardiovascular
    systems, the speed of molecular transmission is determined by the speed of blood
    flow and heart rate, among other factors, which may vary per person; whereas in
    the nervous system, the time required to propagate information-carrying electro-chemical
    stimuli through neurons depends on the connectivity of synapses. In order to estimate
    the error rate and capacity, the existing diffusion-based MC model normally requires
    several channel parameters to formulate the model for computation. The generic
    modeling approach provides initial insights into the behavior of molecular signal
    transmission, however, recent advances in statistical learning, that utilize artificial
    intelligence, provide increasingly refined solutions for modeling sophisticated
    molecular information exchange processes. For example, in [77], a signal detection
    algorithm based on neural networks has been shown to achieve good performance
    without prior knowledge of the molecular channel, thus lending support to the
    use of statistical inference for characterizing molecular communication channels.
    Furthermore, a neural network-based nano-receiver design has been proposed in
    [185] which shows good bit error rate performance under the effect of inter-symbol
    interference. 4) Open Problems for IoBNT Currently, IoBNT primarily focuses on
    studies in the domains of physical layer channel modeling, capacity analysis,
    modulation and coding schemes, and nano-transceiver design. However, research
    gaps relating to the following aspects need to be overcome: Experimental Validation:
    The theoretical models of molecular communications should be validated under realistic
    channel environments, which include experimental testing. Traditionally, these
    experimental tests have posed high requirements on lab equipment and the nurturing
    process of cells and bacteria. While the procedures for such experiments should
    be strictly followed and executed, at times the cost for testing can be remarkably
    high. In such situations, simulations based on realistic assumptions serve as
    an alternative means, which have been commonly adopted in research. The convergence
    between analytical and experimental approaches should be a joint effors by researchers
    across fields in telecommunications, biomedical engineering, and signal processing.
    Data Storage and Management: The large data sets obtained from experiments or
    simulations can have many control variables which require efforts to manage and
    update. Open databases have become a popular trend for sharing raw data to benefit
    the entire research community for collaboration, and can be a foreseeable direction
    for research in the IoBNT. C. Quantum Communications As networks continue to evolve
    beyond 6G, they are expected to incorporate more spectrum, a larger variety of
    transceiver front-ends, higher complexity in processed signals, and stricter requirement
    on reliability, and therefore, it is expected that the computational requirements
    of wireless systems will also increase [186]. To this end, quantum computing has
    been widely recognized as a key enabling technology for realizing computationally
    complex systems [187]. Quantum systems are particularly useful for solving complex
    optimization problems. For example, in the optimal routing problem with multiple
    objectives, traditional methods, including the geographic routing algorithm, demonstrate
    significant complexities to yield optimal solutions, and less complex ones often
    sacrifice optimality [188]. It has been demonstrated that using quantum computing
    for such problems can efficiently reduce complexity while achieving optimality
    [189]. However, such computationally intensive tasks often require several hundreds
    of thousands or millions of interconnected quantum bits, and therefore cannot
    be performed on a single quantum chip. The need for interconnecting several chips
    of this kind has given rise to the concept of quantum communications. Quantum
    communications is thus indispensable for operating quantum systems at scale [190].
    More specifically, quantum communications is defined as the exchange of information
    that adheres to the laws of quantum mechanics, and offers several key advantages:
    (i) the capability of large-scale parallel computation, (ii) the ability to transfer
    data in a tamper-proof manner, and (iii) the potential to encode and transmit
    a large number of multiple data streams simultaneously. We begin our discussion
    of quantum communications by describing the following four postulates or rules
    that govern the operation of such systems [191]: Postulate 1 - The Quantum Bit:
    Within the context of classical communications, a binary value of either 0 or
    1 per bit is used to represent data. On the other hand, in quantum communications,
    the quantum bit, or qubit, contains the superposition of both logical values at
    the same time, of the form |Φ⟩= a 0 |0⟩+ a 1 |1⟩, (1) View Source where |Φ⟩ represents
    a two-dimensional vector, with the coefficients a 0 and a 1 being complex numbers,
    and 0 and 1 being the two logical values. Postulate 2 - The Quantum Register:
    Just like computing registers that are used to store multiple bits, quantum registers
    are used for storing qubits. However, unlike classical registers that are deterministic,
    the output of a quantum register is probabilistic, i.e., when reading or measuring
    the quantum register, a different value may be returned each time, thus presenting
    a major challenge in the implementation of quantum information exchange systems.
    Postulate 3 - Exponential Speed-up: Exponential speed-up is a key property of
    quantum information processing systems. We know that classical systems employ
    parallelization wherein multiple computing units process parallel streams of data
    simultaneously. On the other hand, in quantum systems, the entire input information
    is placed in a single quantum register, and a single quantum computing unit can
    process multiple register states simultaneously, thus achieving a significant
    reduction in the time required for computation. Postulate 4 - The Q/C Conversion:
    Since it is far easier to perceive information in terms of 0s and 1s, i.e., classical
    information, it becomes imperative to interpret the results of any quantum operation
    in the classical domain. To this end, the classical interpretation of (1) implies
    that, if we were to measure such a qubit, we would receive value 0 with probability
    p 0 =| a 0 | 2 and value 1 with p 1 =| a 1 | 2 . Closely related to the four postulates
    is the concept of entanglement [191]. Entanglement is a phenomenon in which the
    quantum states of two or more particles are described with reference to each other.
    Within this context, these particles exist in a shared state, and are referred
    to as entangled pairs. Any action on a particle within the entangled pair immediately
    affects all other particles within that pair, irrespective of the physical separation
    between them. For example, if a photon traveling through an optical fiber is entangled
    with another photon outside the fiber, the photon inside the fiber will experience
    the same effects as those experienced by the photon on the outside. In this case,
    entanglement serves as a source of noise in the quantum channel. Continuing our
    discussion of quantum channels, we note that classical information theory does
    not apply to these channels. Unlike traditional wireless communication channels
    where the large- and small-scale parameters are deterministic or can be stochastically
    characterized, the capacity of qubit carrying quantum channels is defined as the
    rate at which classical or quantum information increases with each use of the
    quantum channel [192]. Moreover, there exist several different types of capacities
    for quantum channels, including but not limited to the classical capacity, the
    quantum capacity, the private capacity, the entanglement assisted capacity, and
    the zero-error capacity. The classical and quantum capacities are the two most
    commonly used definitions. In particular, while the classical capacity measures
    classical information transmission over a noisy quantum channel, the quantum capacity
    represents the amount of quantum information, i.e., qubits, that can be transmitted
    through a noisy quantum channel. We refer the interested reader to [192] for additional
    insight into each of these channel capacity types. In the following, we discuss
    the different types of channels, data routing, and open problems within the domain
    of quantum communications. 1) Types of Channels Within the broader domain of quantum
    communications, we take into consideration the following types of channel, the
    dephasing channel and the depolarizing channel [193], [194]. The Dephasing Channel:
    The dephasing channel, also known as the phase damping or phase flip channel,
    applies a bit flip in the conjugate basis. The impact of the dephasing channel
    can be best described as equivalent to measuring the qubit in the computational
    basis and then forgetting the result of the measurement. A more detailed treatment
    of the nuances of the dephasing channel can be found in [193, §4]. The Depolarizing
    Channel: The depolarizing channel is often referred to the “worst-case scenario”
    and describes the fact that the qubit may be left unchanged with a probability
    1−p , with p ∈[0,1] , or that an error may occur with probability p . In this
    case, the error could be one of three types, with each being equally likely– the
    bit flip error, the phase flip error, or both. In the event of an error, it is
    assumed that the channel replaces the lost qubit with a maximally mixed state
    [194, §2], i.e, all states become equally likely. For example, the maximally mixed
    state with reference to (1) implies that a 0 = a 1 . 2) Quantum Communications
    Networks Quantum networks are key to the success of distributed quantum computing,
    and in turn rely on the ability to share quantum states between different quantum
    devices. However, unlike conventional networks that are based on the store-and-forward
    paradigm, quantum networks must adhere to the no-cloning theorem which prohibits
    making copies of an arbitrary quantum state [195]. In order to overcome this restriction,
    quantum networks rely on the concept of entanglement [196] described earlier,
    along with quantum teleportation. The process of quantum teleportation [197] leverages
    entanglement to transmit unknown quantum states between remote quantum devices,
    through remote entanglement distribution [198]. Further, as shown in Figure 15,
    we note the following physical entities that constitute quantum networks [190]:
    Quantum Nodes: These are the quantum devices that are interconnected to each other.
    Communication Links: These include both classical as well quantum links that interconnect
    the quantum nodes in the network. Entanglement Generator: This device is responsible
    for generating the entangled pairs that are distributed between the quantum nodes.
    Quantum Memories: These are primarily used for storing quantum states for the
    purpose of communication. Quantum Measurement Devices: Their primary function
    is the assessment of the generated entangled states. FIGURE 15. Physical entities
    associated with quantum networks [190]. Show All While the aforementioned entities
    play a vital role in enabling quantum networks, the process of quantum teleportation
    is affected by the exponential decay of communication rate with distance, which
    in turn is offset by the use of quantum repeaters. The routing problem then involves
    the selection of the optimal path from the source to the destination traversing
    one or more quantum repeaters resulting in a high-quality entanglement distribution.
    Further, the routing framework must also take cognizance of the fact that the
    physical mechanisms underlying quantum entanglement are stochastic, and that the
    passage of time leads to loss of entanglement between the entangled pair [198].
    Expanding upon this, in the following section, we delve into some of the major
    challenges faced by quantum networks today. 3) Open Problems and Major Challenges
    Given the vast differences between the classical and quantum domains, there are
    several fundamental research challenges, that are vital to the success of quantum
    networks, as detailed next. Quantum Error Correction: There are three major challenges
    faced by error correction techniques for qubits [199]. First, while classical
    error correction codes assume that data can be duplicated freely, the no-cloning
    theorem precludes the arbitrary duplication of quantum states. Second, since qubits
    are susceptible to both bit-flip and phase-flip errors, quantum error correction
    techniques need to be able to detect both error types simultaneously, unlike classical
    techniques that take only bit-flips into consideration. Third, there exists the
    possibility of wavefunction collapse [200] due to measurements on the qubits performed
    as part of the error correction procedure. Entanglement Distribution: Long distance
    entanglement distribution is a key challenge in the realization of quantum networks,
    impacting the physical, link, and network layers [196]. More specifically, at
    the physical layer, there is a need for quantum error correction techniques, while
    the no-cloning theorem necessitates a re-design of the link layer. At the network
    layer, novel quantum routing metrics are required to ensure optimal path selection.
    Deployment Challenges: Quantum computing devices require highly specialized data
    centers equipped with ultra-high vacuum systems and ultra-low temperature cryostats.
    Further, while quantum teleportation has been proposed as a means to realize quantum
    networks, it requires the integration of classical and quantum communication resources,
    which is a fairly complex problem in itself. SECTION XII. Tentative Timeline for
    6G Thus far, we have described the manner in which the evolution of societal needs
    will guide the transition from 5G to 6G, along with a plethora of new and upcoming
    use cases that will be best served by 6G. We have also discussed the tentative
    KPIs associated with 6G and the key enabling technologies that will play a vital
    role in achieving these next-generation KPIs, as summarized in Table 2. As shown
    in Figure 16, the increasing technological readiness and worldwide deployments
    of 5G systems have set the stage for a thoughtful discussion on the future of
    wireless communications. TABLE 2 A Brief Summary of the KPI Impacts and Open Problems
    Associated With Each Key Enabling Technology for 6G and Beyond FIGURE 16. Projected
    timeline for 6G and beyond systems. Show All While 3GPP standards over the next
    few years, up to and including Release 18 in 2024, are expected to primarily deal
    with 5G, the ITU has recently convened the Focus Group on Technologies for Network
    2030 (FG NET-2030) [201], to study the capabilities of networks for the year 2030
    and beyond. More generally, as we have seen in the preceding sections, each of
    the presented key technologies has witnessed significant traction in terms of
    research and development, laying the groundwork for the next generation of wireless
    communications. Both the National Science Foundation (NSF) through its Platforms
    for Advanced Wireless Research (PAWR) initiative [202] and the European Commission
    [203] are expected to play a major role in the development of 6G. At the same
    time, going beyond academia, we expect a significant rise in industry involvement
    in the development of these technologies over the next few years, culminating
    in key hardware and software technology demos by 2025, followed by full-scale
    6G testbeds in 2026 and beyond. We envision that these testbeds will serve as
    the perfect backdrop for showcasing the potential of 6G and demonstrating its
    suitability for use cases such as multi-sensory holographic teleportation, real-time
    remote healthcare, industrial automation, and smart infrastructure and environments,
    to name a few. SECTION XIII. Conclusion This paper surveys the key enabling techniques
    for the next generation of wireless communication networks, outlines their essential
    use cases, and provides a perspective on current as well as future research and
    development efforts. We envision that 6G and beyond wireless systems will be largely
    driven by a focus on wireless ubiquity, i.e., the unrestricted availability of
    high quality wireless access. To this end, we have highlighted the key enabling
    technologies that are vital to the success of 6G and beyond systems. By detailing
    both the operational nuances and open challenges associated with each, we not
    only hope to provide a detailed insight into the next frontier in wireless communications,
    but also encourage readers to play their part in the realization of the envisioned
    ubiquitous wireless future. ACKNOWLEDGMENT The authors would like to thank B.
    D. Unluturk, C. Han, D. M. Gutierrez-Estevez, E. C. Reyes, E. Khorov, and X. Wang
    for their valuable insights and suggestions that have played a critical role in
    improving the quality of this paper. Authors Figures References Citations Keywords
    Metrics Footnotes More Like This A Survey of Network Automation for Industrial
    Internet-of-Things Toward Industry 5.0 IEEE Transactions on Industrial Informatics
    Published: 2023 Application of Wireless Communication Technology in Ubiquitous
    Power Internet of Things 2020 IEEE 3rd International Conference on Computer and
    Communication Engineering Technology (CCET) Published: 2020 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/8948470/09145564.pdf
  publication_year: 2020
  relevance_score1: 0
  relevance_score2: 0
  title: '6G and Beyond: The Future of Wireless Communications Systems'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/s22093273
  analysis: '>'
  authors:
  - Amsale Zelalem Bayih
  - Javier Morales
  - Yaregal Assabie
  - R.A. de By
  citation_count: 18
  full_citation: '>'
  full_text: '>

    Web Store Add shortcut Name URL Customize Chrome'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  pdf_link: https://www.mdpi.com/1424-8220/22/9/3273/pdf?version=1650796560
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Utilization of Internet of Things and Wireless Sensor Networks for Sustainable
    Smallholder Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/jiot.2022.3230505
  analysis: '>'
  authors:
  - Antonino Pagano
  - Daniele Croce
  - Ilenia Tinnirello
  - Gianpaolo Vitale
  citation_count: 14
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 10 Issue: 4 A Survey on LoRa for Smart Agriculture: Current Trends and
    Future Perspectives Publisher: IEEE Cite This PDF Antonino Pagano; Daniele Croce;
    Ilenia Tinnirello; Gianpaolo Vitale All Authors 23 Cites in Papers 4697 Full Text
    Views Open Access Under a Creative Commons License Abstract Document Sections
    I. Introduction II. LoRa and Its Perspectives in Agriculture III. Smart Agriculture
    Applications and Challenges IV. Generic LoRa-Based Platforms V. Application-Specific
    LoRa Platforms Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: This article provides a survey on the adoption of LoRa in the
    agricultural field and reviews state-of-the-art solutions for smart agriculture,
    analyzing the potential of this technology in different infield applications.
    In particular, we consider four reference scenarios, namely, irrigation systems,
    plantation and crop monitoring, tree monitoring, and livestock monitoring, which
    exhibit heterogeneous requirements in terms of network bandwidth, density, sensors’
    complexity, and energy demand, as well as latency in the decision process. We
    discuss how LoRa-based solutions can work in these scenarios, analyzing their
    scalability, interoperability, network architecture, and energy efficiency. Finally,
    we present possible future research directions and point out some open issues
    which might become the main research trends for the next years. Published in:
    IEEE Internet of Things Journal ( Volume: 10, Issue: 4, 15 February 2023) Page(s):
    3664 - 3679 Date of Publication: 20 December 2022 ISSN Information: DOI: 10.1109/JIOT.2022.3230505
    Publisher: IEEE Funding Agency: SECTION I. Introduction In the near future, the
    agricultural sector is called to face a significant challenge due to increasingly
    scarce resources, extreme weather conditions, a growing population, and a reduction
    in arable land [1]. Indeed, according to the FAO, by 2050, the world’s population
    will reach about 10 billion and, to be able to feed everyone, we will have to
    produce 70% more food [2], [3]. A practical and feasible solution is to move from
    the old farming concept to smart agriculture, with the adoption of information
    and communications technologies (ICTs) that help farmers to monitor, manage, and
    optimize their operations more effectively [4]. In particular, the introduction
    of the Internet of Things (IoT) applications, every single step of agricultural
    production can be improved: from soil management to minimizing water consumption,
    from plant protection to animal health and farm automation [5], [6]. Smart devices
    located infields are able to collect information and control the evolution of
    the different processes at various production stages. Besides, the miniaturization
    of electronic components allows to implement IoT sensors with reduced form factor
    and energy consumption, monitoring many variables of interest, such as temperature,
    humidity, wind speed and direction, soil conditions, chemical concentrations,
    crop growth, and solar exposure, as well as possible damages caused by drought,
    hail, or flooding. IoT systems integrate all this and other data and turn it into
    useful statistics: for example, predictive analysis allows farmers to use the
    strictly necessary resources (water, pesticides, fertilizers, etc.) and only where
    there is a real need, e.g., for poorly irrigated areas of the field, weak or sick
    plants, etc. [7], [8]. Despite such potential benefits, the deployment of smart
    agriculture systems is still in its infancy. Indeed, an obstacle to the digitization
    of agriculture is the lack or limitations of Internet connectivity in many areas.
    In the literature, several communication protocols have been proposed, with different
    characteristics related to cost, coverage, power consumption, and reliability
    [9]. Among the available technologies (summarized in Fig. 1 in terms of power
    consumption and coverage range), low-power wide-area networks (LPWANs), enlightened
    in a dashed box in Fig. 1 represent the best solution for supporting smart agriculture
    requirements. One of the most adopted LPWAN technology is LoRaWAN, which offers
    wide network coverage, built-in security, low cost, and limited power consumption
    during operation [10]. Fig. 1. Classification of wireless technologies: power
    consumption versus communication range [11]. Show All Indeed, LoRaWAN is an open
    system based on a very robust modulation (called LoRa), which provides several
    interesting features for covering rural areas with simple devices [9]. For such
    reasons, LoRa has been widely employed and tested in the agricultural field, connecting
    environmental sensors measuring temperature, air/soil moisture, etc., or to control
    different kinds of actuators (e.g., irrigation valves), and in applications, such
    as tractor communications, livestock monitoring, and location tracing [12], [13],
    [14]. Differently from existing surveys, which either treat LoRa together with
    all other IoT solutions or consider very specific technological aspects, we consider
    the adoption of these systems from an holistic perspective. For example, LoRa
    is cited in [5] in the general framework of IoT; [7] cites LoRa among the most
    promising technologies for agricultural IoT, and the same plan is followed by
    [8] which dedicated a Section to LoRa in Enabling Communication Technologies;
    similarly [15] includes LoRa in IoT communication protocols suitable for smart
    agriculture. On the other hand, [16] is specific to LoRa but is focused only on
    the protocol performance, [13] takes into consideration only the energy consumption
    of LoRa, [17] discusses the application of the technologies of industry 4.0 in
    the context of smart agriculture. Papers [18] and [19] deal with specific issues
    meaning the decision support system (DSS) and robotics in agriculture, respectively.
    Instead, in this article, we strive to provide a thorough and focused analysis
    on LoRa/LoRaWAN application in smart agriculture, offering a comprehensive view
    of the advancements and in-field applications of this IoT technology. LoRaWAN
    relies on LoRa modulation, a robust chirp-based modulation scheme, patented by
    Semtech [20]. It supports wireless connectivity with limited data rates over large
    areas and without the need of an operator. LoRaWAN is widely used in smart industry,
    smart home, smart city and, increasingly, in the smart agriculture environment.
    In the view of the authors of this article, LoRaWAN possesses five main strengths
    (low-cost, long-range, low-power, no-operator, and unlicensed spectrum) that can
    bridge the gap between smart agriculture and smart cities or industries. Moreover,
    several recent improvements on the resource allocation, channel access protocol,
    and network planning can enhance the efficiency of LoRaWAN networks, reducing
    capital and operational costs [21]. In rural areas, several experiments have demonstrated
    good coverage of LoRa [22], [23], [12]. Coverage ranges of up to 5 and 47 km have
    been obtained in the non-line-of-sight (NLOS) and line-of-sight (LOS) propagation
    conditions, respectively. In addition, in the case of NLOS propagation, the coverage
    range can be increased by using an unmanned aerial vehicle (UAV) [24], [25]. In
    terms of power consumption, LoRa offers up to 15 years of battery life to its
    devices. The low power consumption is a key feature of LoRa that makes it an ideal
    choice for smart agriculture applications. It was demonstrated experimentally
    that the estimated battery lifetime of a LoRa device may be six times that of
    a Wi-Fi device and two times that of a ZigBee device [26]. In this article, we
    provide a broad survey of LoRa-based smart agriculture systems, analyzing the
    state-of-the-art and highlighting for each solution the possible adoption of Machine
    Learning, control automation techniques, and energy autonomy features. We classify
    these works in four main categories: 1) irrigation systems; 2) plantation and
    crop monitoring; 3) tree monitoring; and 4) livestock monitoring. These LoRa systems
    are analyzed in terms of scalability, interoperability, network architecture,
    energy efficiency, and point out some open issues which traverse most of the current
    smart agriculture systems. References have been selected based on recent papers
    dealing with smart agriculture; however, some contributions in different fields
    useful as a benchmark are cited as well. Finally, we present the lessons learned
    and draw future research directions which we think crucial for the success and
    widespread of such technologies. The remainder of this article is structured as
    follows. Section II provides an overview of LoRa technology and its perspectives
    in agriculture, while Section III offers a brief summary on smart agriculture
    applications and their main challenges. Section IV reports on general purpose
    LoRa-based IoT platforms applied to smart agriculture. Section V, instead, discusses
    specific vertical solutions for smart agriculture, according to the four above-mentioned
    categories. Lessons learned and future research directions are proposed in Section
    VI. Finally, conclusions are drawn in Section VII. SECTION II. LoRa and Its Perspectives
    in Agriculture LoRa technology is a proprietary physical layer technology patented
    by Semtech, which is revealing as a promising solution for large-scale low-power
    IoT deployments, including smart agriculture applications. Indeed, by operating
    in the unlicensed industrial, scientific, and medical (ISM) radio bands and with
    a robust chirp-based modulation scheme, LoRa provides a cheap solution for supporting
    wireless connectivity with limited data rates (from 0.3 to 27 kb/s) in large areas
    and without the need of an operator. Moreover, LoRa transmissions are regulated
    by having a maximum transmission power of 25 mW (14 dBm) in the uplink, a configurable
    bandwidth of 125, 250, or 500 kHz, and a duty cycle of 0.1%, 1.0%, and 10%, which
    permit low energy consumption. In some scenarios, the battery of LoRa devices
    can last up to 15 years. Although LoRa technology is limited to the physical layer,
    different network solutions can be built on top of it, by exploiting its transmission
    interfaces. Among these, the most consolidated one is the open-source solution
    promoted by the LoRa Alliance, which is called LoRaWAN [27]. LoRaWAN networks
    are based on a simple star of star topology (Fig. 2): end-devices (EDs), such
    as sensors or actuators deployed infield, transmit packets on the wireless medium
    to fixed nodes called gateways (GWs), which, in turn, forward the collected packets
    to a central network server (NS) interacting with several application servers
    (ASs) [28]. The network infrastructure between GWs, NS, and ASs is typically based
    on a wired Internet technology, while EDs are not associated to a specific GW,
    which greatly simplifies implementation (e.g., in case of mobility [29]): in case
    a duplicate packet is simultaneously received by multiple GWs, the NS is responsible
    of filtering these packets and performs other simple decisions on network configuration.
    Fig. 2. LoRaWAN typical architecture. Show All To minimize the protocol complexity
    and the energy consumption, LoRaWAN employs a simple Aloha MAC protocol and defines
    three classes of devices (Fig. 3). Device classes represent different ways of
    managing the reception operations performed by the EDs. Class A devices, corresponding
    to the lowest energy profile, can receive downlink packets only in two time windows
    following the transmission of their own packet to the GW. In other words, devices
    can sleep all the time and downlink transmissions are triggered only after an
    uplink one. Class B devices add to this possibility a periodic scheduling of reception
    windows, by keeping a time synchronization with the GW. Finally, class C devices
    are constantly listening to the channel for downlink packets. Any time that a
    new packet is ready for transmission, devices attempt to transmit by randomly
    selecting one of the available channels in the ISM bands (e.g., in the 868 MHz
    there are 16 channels in Europe), together with a modulation parameter called
    spreading factor (SF). More into details, six different SFs are used in LoRa (from
    SF7 to SF12), which result in different symbol times and in almost orthogonal
    transmissions: when two signals modulated at different SFs overlap, the GW is
    able to decode both transmissions in a wide range of power ratios among the signals
    [30]. Unlike many other IoT technologies, the LoRaWAN specification offers dedicated
    end-to-end encryption to application providers, together with network-level security
    primitives, which allow sharing the same network among multitenant applications
    [31]. Fig. 3. LoRaWAN technology stack. Show All Summarizing, the ease of deployment
    with excellent coverage, the availability of devices with very low energy demand,
    and intrinsic security mechanisms make these systems very suitable for innovative
    agriculture applications. Indeed, several state-of-the-art IoT applications in
    smart agriculture are based on LoRa/LoRaWAN networks. For example, LoRa is used
    to connect sensor nodes measuring environmental parameters or to control different
    kinds of actuators (e.g., solenoid valve for irrigation purposes), and in applications
    such as livestock monitoring and location tracing [13], [10], [14]. These applications
    are not critical for data rates and latency, but often require to work in large
    rural areas, with limited access to energy grids and the Internet, and with decision
    mechanisms which benefit from data-driven learning. SECTION III. Smart Agriculture
    Applications and Challenges While industrial production processes have already
    become smarter and autonomous thanks to the implementation of the so called Industry
    4.0 concept, the integration of technologies such as IoT, artificial intelligence
    (AI), robotics, and big data is more recent in agriculture. The availability of
    IoT technologies for supporting wireless connectivity in rural areas and controlling
    infields smart objects shows a great potential for improving the agricultural
    sector, toward the so called smart agriculture [17], [18], [19]. Indeed, farm
    monitoring and automation can make production more efficient and sustainable [7],
    by promptly detecting and reacting to water or moisture stress, wastes of raw
    materials, crops’ diseases, pests, and nutrient deficiencies, as well as problems
    related to the wellbeing of farm animals. The interest on the development of smart
    agriculture applications has been demonstrated by the recent commercialization
    of agricultural sensors and robots (called Agribots), specifically designed for
    reducing the intense physical labor traditionally required in agriculture [15].
    Apart from the availability of smart devices for interacting with the farm in
    the physical world, smart agriculture applications require to build a digital
    representation of the farm status and a decision logic based on the collected
    data. Different protocols can be envisioned both for providing the wireless connectivity
    to heterogeneous devices (from simple low-cost temperature sensors, to complex
    remote-controlled robots) and exporting data for analysis and decisions [32],
    [33]. Since a large amount of data can be produced by agricultural sensors, big
    data analysis can provide efficient monitoring and processing methods [34]. Data
    processing may involve various features such as data loading, validation, aggregation,
    prediction, classification, image or video processing, and data mining. Thus,
    based on the acquired data, DSSs can optimize the productivity and reduce the
    ecological footprint of the farm. Researchers recognize that digitization of farming
    processes and activities is an important challenge for the adoption of smart agriculture
    technologies [32], [35]. In particular, the major challenges to digitization in
    agriculture can be categorized as follows. Communication Issues: As we will detail
    later, large-scale implementations of IoT solutions require robust and secure
    network architectures. The reliability of communicating information still represents
    a challenge to be addressed in the agricultural context and justifies the adoption
    of LoRa/LoRaWAN technologies. Energy Management: The power supply in devices for
    smart agriculture is a significant challenge and energy harvesting systems are
    a relevant area of research. The main issue concerns the sensor’s power supply
    and how to optimize efficiently the power consumption. Moreover, distributed nodes
    can execute some computations (Edge computing) which consumes more energy, while
    sensor batteries have a limited capacity. Consequently, smart devices require
    efficient energy storage and supply. Data/Device Heterogeneity: In general, the
    agricultural data is produced by heterogeneous sensors (soil sensors, weather
    sensors, trunk sensors, leaf sensors, etc.). In addition, IoT devices generally
    use different network protocols and platforms. Thus, in addition to sensors heterogeneity,
    network and protocol heterogeneity should be considered as well. Getting these
    technologies to work together is often an issue, especially for unskilled farmers.
    Physical Deployments: Spatial deployment of devices on farms proves to be a significant
    challenge, especially when the entire farm needs to be monitored across a large
    area and with different application scenarios (soil, plants, trees, animals, etc.).
    Data Management: The difficulty of interpreting the data can be a huge barrier:
    indeed, numerous sensors are necessary and big data analysis could be required
    to better understand and forecast the unpredictability of agricultural ecosystems.
    Generic Platform: To promote the adoption of smart agriculture technologies is
    often required to develop user friendly software platforms. The challenge here
    is to build a universal platform that can be easily modified to support different
    types of monitoring ranging from specific crop to livestock. These challenges,
    together with the cost of infrastructure investment, the complexity of technologies,
    lack of farmers’ education and training, data ownership, and privacy and security
    concerns, has motivated the research and development of innovative platforms,
    specific network technologies, and new architectures for smart agriculture [32],
    [35]. SECTION IV. Generic LoRa-Based Platforms Since agricultural applications
    are widely different, varying from soil and air monitoring, to irrigation automation
    and livestock breeding, several general purpose IoT platforms have been adapted
    for farmers to accommodate all these applications together under a unified, easy
    to understand and simple to use interface. Therefore, in this section, we will
    discuss some of these LoRa-based platforms horizontally designed for smart agriculture,
    while in the next one, we will dig into more vertical and application-specific
    systems, focusing on the four reference scenarios depicted in Fig. 4. Fig. 4.
    Four reference applications in smart agriculture. Show All Generic and open IoT
    platforms can indeed help to digitize farms by integrating numerous agriculture
    applications, harmonizing specific sensing devices, actuators, and decision logics,
    which exhibit heterogeneous requirements in terms of network bandwidth, latency,
    sensors’ complexity, and energy requirements. A clear example is constituted by
    FIWARE [36], a powerful open-source platform, sponsored by the European Commission,
    that provides standardized interfaces for many different IoT sectors including
    agriculture. The FIWARE platform includes several parts called generic enablers
    (GEs), which provide components and reference implementations that support specific
    APIs, and can integrate data collected from heterogeneous sensors using different
    communication technologies, to create custom applications [37], [38]. Several
    GEs are available making it easier to interface with IoT systems, and the IoT
    Agent for LoRaWAN offers a bridge between LoRaWAN and the FIWARE Context Broker
    (the core component of the “Powered by FIWARE” platforms). Moreover, FIWARE can
    be combined with other third-party platforms to provide accessible tools to worldwide
    farmers and consumers too [39]. Another example is the work in [40] where low-cost,
    LoRa-based devices are used for soil temperature and humidity monitoring, and
    the data is processed and sent to the Cayenne IoT Platform for storage and visualization
    [41]. This platform is a drag and drop project builder for developers and engineers
    that can be used in different IoT applications. It encompasses cloud-based web
    applications as well as mobile apps for Android and IOS devices. Cayenne can integrate
    any tool into the library with a wide variety of IoT ready-to-use devices and
    connectivity options. Other LoRaWAN-based IoT platforms are more specific to the
    agricultural world, aiming at improving the management of generic farms in a highly
    customizable way. For example, the LoRaFarM platform [42] has a generally applicable
    “core” infrastructure, which can be completed with specialized ad-hoc modules
    depending on the farm’s characteristics and requirements. The LoRaFarM platform
    derives its topological structure from the LoRaWAN architecture, since low-level
    communication patterns are built around the LoRaWAN technology (see Fig. 5). Hence,
    expansion modules can be added at farm level (or low level), if they include physical
    hardware to be installed in the deployment (sensors or actuators), as well as
    a high level, in case data processing is needed. The middleware, in the LoRaFarM
    domain, refers to the set of entities and technologies by which data coming from
    farm-level modules are collected, stored, and exposed to high-level modules. This
    middleware can be defined as a sort of “connecting layer” between the farm and
    the back-end domain. Fig. 5. LoRaFarM platform: levels and parallelism with LoRaWAN
    [42]. Show All One of the main advantages of the LoRaFarM platform is that heterogeneous
    subnetworks, in terms of capabilities (transmission range, data throughput, and
    energy consumption), can be incorporated without altering the platform structure
    and, thus, making it highly scalable, flexible, and suitable for a wide range
    of scenarios. Indeed, this gives the freedom to choose the most suitable communication
    protocols and traffic policy to monitor and control the farm different areas,
    such as greenhouses and fields. Messages between nodes employing different protocols
    are translated by a multiprotocol GateWay (mpGW), enabling communications between
    non-LoRaWAN-enabled nodes and the LoRaFarM middleware, in a seamless way. Its
    protocol translation functionality, the mpGW can be enriched with edge computing
    features, to process and aggregate sensor data. Moreover, LoRaFarM can be extended
    with new functionalities like data analysis and prediction of the evolution of
    environmental parameters to prevent plant diseases, relying on AI and Machine
    Learning techniques. Finally, the mySense environment proposed by Silva et al.
    [58] is a sensor data integration framework aimed to systematize data acquisition
    procedures to address common smart agriculture issues. It facilitates the use
    of low cost platforms such as Arduino and Raspberry Pi, making available a set
    of free tools based on the do it yourself (DIY) concept. The mySense platform
    builds over a 4-layer technological structure (sensor nodes, crop field and sensor
    networks, cloud services and front-end applications) and is accordingly divided
    into four levels of operation: Level 1, for data collection using common data
    transfer technologies (ZigBee, GSM/GPRS, LoRa, etc.); Level 2, for GWs (possibly)
    running local tasks according to the fog or edge computing paradigms; Level 3
    for storing data in the cloud; and Level 4 for high-level applications. Data can
    arrive from any device provided that complies with the data formats allowed by
    the platform. Summary and Insights: This Section discussed LoRa-based platforms
    which can be exploited to unify different applications into one simple and easy-to-use
    platform. Platforms such as Fiware, Cayenne, LoRaFarM, and mySense provide standardized
    interfaces to integrate different agricultural applications with each other. These
    platforms provide ready-to-use solutions and connectivity between heterogeneous
    networks. With these platforms, LoRa can integrate and complement existing systems
    based on other network technologies (ZigBee, Bluetooth, etc.), making them highly
    scalable. SECTION V. Application-Specific LoRa Platforms In this section, we provide
    an in-depth review focusing four reference scenarios: 1) irrigation systems; 2)
    plantation and crop monitoring; 3) tree monitoring; and 4) livestock monitoring,
    which broadly cover most of smart agriculture applications. A. Irrigation Systems
    Accurate monitoring of the soil water status allows to achieve seasonal water
    savings of up to 90% compared to traditional management, increasing productivity
    and introducing significant savings in energy costs for the water pumps management
    [59]. To improve water management in agriculture, it is necessary to analyze and
    monitor the complex water interactions that occur in field, following the concept
    of soil-plant-atmosphere (SPA) continuum systems [60]. Indeed, the knowledge of
    the water status of the SPA system plays a significant role for understanding
    the crop water stress and implement water saving mechanisms with a minimal effect
    on the production [59]. Measuring the evapotranspiration (ET), which refers to
    the amount of water that passes from the soil into the air due to the combined
    effect of plant transpiration and evaporation, is another complex task. Examples
    of these sensors are the lysimeters or sophisticated micro-meteorological sensors
    (e.g., Eddy covariance), whose cost and complexity limit their application to
    research studies [61]. Cheaper systems are the time domain reflectometry (TDR)
    technique or gravimetric methods [62], whose main limit is the difficulty in calibration
    and automation. Some LoRa-based irrigation systems are implemented using development
    boards such as Arduino, ESP32, Pycom, or STM32, e.g., [43], [44], [47], [48],
    [46], and [51]. Few of them also include energy harvesting modules, such as an
    hydroelectric generator, allowing them to operate for decades. For example, the
    LoRaWAN-based irrigation system in [51] comprises an energy-neutral irrigation
    node (Fig. 6) with the following modules: controller module, power module, irrigation
    module, and transmitter module. Fig. 6. Energy-neutral irrigation node described
    in [51]. Show All Exploiting AI and data coming from different sensor, such as
    air temperature and humidity, soil temperature and humidity, light intensity,
    etc. makes possible to develop and train specific irrigation models to calculate
    the exact amount of water to be distributed. For example, the works [54], [55],
    [56] provide machine-learning-based smart irrigation systems, all employing LoRa
    technology. In particular, in [55], a random forest classifier predicts the soil
    moisture and, thus, irrigation is planned accordingly. In [54], instead, multiple
    linear regression algorithm is employed to train the model using two highest correlation
    coefficient features: 1) light intensity and 2) soil humidity. Data is collected
    with a LoRa P2P network, which uses a master–slave and TDMA-based MAC protocol.
    Each slave node has a unique address and can transmit a packet in each of the
    reserved TDMA time slots. Alternatively, a Penman-Monteith-based irrigation model
    allows for an optimal irrigation strategy for different crop growth periods and
    uses the ET parameter to estimate the amount of water [63]. This solution requires
    an integration of actuators, sensors and a meteorological station in a LoRa network
    [49], [50]. In addition, third-party services such as weather information or fog
    computing may be needed to decide on irrigation schedules [57]. Since in LoRaWAN,
    the latency of downlink communication from GW to Class-A nodes (sensors or actuators)
    is relatively long (must first wait for an uplink transmission), few systems employ
    alternative Master/Slave protocols [52], [53], [54]. These protocols increase
    the stability of the LoRa irrigation system, avoiding packet collisions and, thus,
    can save water during the close command of the solenoid valve. Finally, AREThOU5A
    [43] is an example of a water management system that combines data collected from
    wireless sensor networks (WSNs) in the field and satellite data provided by international
    weather forecast services, to achieve efficient water usage strategies for farmers.
    It employs a WSN with two different sensors for measuring the temperature and
    the soil moisture in field. A routing subsystem controls and routes the data and
    information through LoRaWAN and TCP/IP with SSL network interfaces. The LoRa network
    is used to collect data from the EDs and perform administration processes, while
    the TCP/IP SSL works as a bridge to the rest of the network architecture. Summary
    and Insights: Comparing the characteristics of different irrigation systems, summarized
    in Table I, it is relevant to note that most of these LoRa-based irrigation systems
    adopt temperature, humidity, and soil moisture sensors. However, albeit all cited
    papers are recently published, ML is used only in 1/3 of the applications. TABLE
    I LoRa-Based Irrigation System Comparison Furthermore, only 13% of these irrigation
    systems used an evapotranspiration-based methodology. This strategy, which is
    often expensive, may be accomplished by combining inexpensive sensors and AI (with
    a more comprehensive approach integrating meteorological variables measured by
    a weather station with variables measured by soil sensors into the system), significantly
    lowering the cost of direct evapotraspiration measurements. Such improvements
    could lead to more effective water management, with the simultaneous impact of
    decreasing water usage and increase crop output. Finally, LoRaWAN communications
    can be tuned to adapt the duty cycle and manage the system optimally: for example,
    when the irrigation system is not in use, sensor data could be collected every
    hour or even less, while when irrigation is taking place the measurements could
    be increased to every 5–10 min. This way, the use of water and energy could be
    further reduced [71]. B. Plantation and Crop Monitoring Plantation and crop monitoring
    requires a large number of sensors to obtain an effective control and, thus, increase
    productivity, especially when agricultural fields are very heterogeneous. For
    example, in order to optimize the production while minimizing the ecological footprint,
    it is necessary to control the injection of pesticides and fertilizers [72], [73],
    increasing yields up to 10% and saving fertilizers up to 37% [74]. Such control
    can be performed by varying the pesticides and fertilizer application rate over
    time and space. Crops do not always need a uniform application, as some areas
    have different requirements due to their location (sunlight, soil features, etc).
    Over-fertilization can deteriorate water quality, favor weed growth, and reduce
    profit. Vice-versa under-fertilization restricts yield or reduces crop quality
    [75]. The application rate can be modified based on weather impacts, nutrient
    availability, and seasonal cycles [73], [75]. Some optical or ultrasonic sensors
    indirectly assess the nutrient request (nitrogen, phosphorus, potassium, etc.)
    of the crop at the time of application [72]. In addition, to reduce the loss of
    productivity in crops, surveillance systems can be adopted [76]. Providing visual
    monitoring to growers can prevent crops from getting damaged by intruders, ensure
    the field conditions or enable the detection of pests attacks remotely. Although
    there are plenty of devices which can be exploited for building a real-time visual
    monitoring system, deploying them in a wide area and over wireless channels can
    be challenging [77], [78], [79]. Table II summarizes the main characteristics
    of several plantation and crop monitoring systems, based on LoRa technology. The
    nodes used in these systems should be of small dimensions, self-sufficient in
    terms of energy, relatively cheap, and often able to acquire a large variety of
    parameters. For example, three different sensor nodes have been developed by Valente
    et al. [64] and tested in a vineyard field: node 1, with an ultrasonic anemometer
    (that measures the direction and speed of the wind) and a sensor that monitors
    bulk electrical conductivity, in addition to volumetric water content (by measuring
    soil permittivity) and soil and air temperature; node 2, an irrometer watermark
    soil water tension sensor; and node 3, an all-in-one weather station with 12 sensors
    to measure air temperature, relative humidity, vapor pressure, barometric pressure,
    wind speed, gust and direction, solar radiation, precipitation, lightning strike
    counter, and distance. It should be remarked that each node contains sensors which
    differ for the sampling rate, accuracy, and supplied energy. The nodes send data
    using LoRaWAN to a GW that is connected to a the things network (TTN) server.
    In the TTN server, data is decoded and sent to the ThingSpeak [80] platform for
    visualization and possible analysis and aggregation. Fig. 7 summarizes the different
    blocks composing the nodes: 1) a maximum power point tracker (MPPT) applied to
    a photovoltaic source and connected with a storage system; 2) a dc/dc switching
    converter to interface the source with the storage system and loads; 3) the LoRaWAN
    module for communication; and 4) the analog-to-digital converter (ADC) module
    to convert the signals available from sensors. TABLE II LoRa-Based Plantation
    and Crop Monitoring System Comparison Fig. 7. Block diagram of the sensor nodes
    taken from [64]. Show All In [66] a LoRa-based IoT monitoring system for starfruit
    plantation is presented. The LoRa network implemented includes three nodes and
    one master, and it can cover a range of 700 m. For optimal growth, starfruit plants
    need soil pH conditions between 5.5–7.5. Thus, thanks to the proposed LoRa system,
    the farmers can make important and precise decisions about how to grow the crop.
    Similarly, works [65], [67], [68], [69], [58] present solutions to increase production
    and fruit quality, with optimal use of resources through LoRa-based networks.
    The Smart Mushroom Cultivation is a system used to automatize the production of
    expensive mushrooms [67]. The smart system includes devices to monitor and control
    humidity and C O 2 levels through sensors and actuators all connected using LoRaWAN.
    The sensor nodes measure the ambient condition inside Mushroom House (humidity,
    temperature, and C O 2 ), and data is sent to the remote server for monitoring
    and analysis. An automatic control maintains the ambient conditions between the
    required levels. Finally, there are cases where anomaly detection near the sensor
    is required to allow decisions and actions as soon as possible. In this direction,
    Brunelli et al. [70] proposed a new paradigm of monitoring and pest detection
    to improve the performance of an apple orchard. They add intelligence to the LoRa
    nodes, shifting the detection of anomalies near the sensor. The application is
    developed on a low-energy platform powered by a solar panel, realizing an energy-autonomous
    system capable of operating unattended continuously over LoRa networks. Summary
    and Insights: Plantation and crop monitoring requires the control of numerous
    parameters, captured by different heterogeneous sensors deployed in the agricultural
    fields. Some of the sensors used in the cited papers are specific to the type
    of crop, while others (e.g., temperature, humidity, etc.) are deployed in almost
    all of the literature works. In addition, the use of AI is not yet widely adopted,
    and only three out of nine papers adopt automatic control for the implementation
    of DSSs. An innovative approach in this context would be to add intelligence to
    the LoRa nodes, while moving the DSS closer to the sensor. Finally, note that
    the maximum size of the LoRa payload is 250 bytes; this allows a wide variety
    of parameters to be monitored and transferred in a single packet. For example,
    Sacaleanu and Kiss [69] send eight agri-meteorological measurements in a single
    LoRa packet of only 16 bytes. C. Tree Monitoring Trees are essential in modern
    society and are widely applied in a great number of scenarios including soil erosion
    prevention, air purification, wood or fruit production. For supporting the managers
    of urban/rural green infrastructures and forests, it is important to constantly
    monitor the tree conditions, in terms of growth rate and failure risk, as well
    as micro-climate parameters in the tree surrounding areas. The analysis of this
    data allows the characterization of the trees functional responses to their environment
    and a prompt action in case of problems. Tree monitoring also requires reliable
    long-range communications in the presence of foliage, large sensor densification
    (i.e., one sensor per tree), and measurements of various physiological/biological
    parameters from specific locations (at the root, the trunk, or the branch) as
    a function of vegetation type to obtain accurate readings [83]. In these systems,
    it is also important to measure changes in position over time or instantaneous
    trunk accelerations. Table III summarizes the main characteristics of relevant
    Monitoring systems. TABLE III LoRa-Based Sensors Applications in Tree Farms One
    of the main properties to be measured is the water transport in the xylem of the
    trunk (called the sap flow). A possible measurement method is the Heat Balance
    Method, developed by Granier [91], [92], which is based on analyzing the temperature
    difference among two probes inserted into the stem wood at a 10 cm distance along
    the vertical trunk axis. The probe in the higher position is heated, while the
    lower one provides the stem wood reference temperature. The temperature difference
    generated between the probes represents an index of the transpiration activity
    of the plant, expressed as a variation of the flux density. This method can be
    used for accurate measurements of sap flow in plants, providing a reliable calibration
    procedure to relate the temperature difference to the actual sap flow [93]. For
    example, the TreeTalker (TT) [81] is a device that measures sapflow (water transport
    in the trunk), wood temperature and humidity, multispectral signature of light
    transmitted through the canopy, tree trunk radial growth, accelerations along
    a 3-D coordinate system used to detect tree movements, air temperature, and relative
    humidity, which can be additionally complemented by soil temperature and volumetric
    water content. A TT node is connected via LoRa wireless connection to a GW, that
    manages up to 48 devices in one cluster. The GW is, in turn, connected to the
    Internet via GPRS and sends data to a computer server. This technology can be
    applied to monitor the root plate tilt, as well as the flexion and the accelerations
    that tree trunks receive under the force of the wind for the evaluation of tree
    failure risk. Another solution to analyze the health condition of a tree consists
    on the electrical impedance spectroscopy (EIS), it is a well-known technique with
    a wide range of applications. EIS has been applied to characterize solids, liquids,
    both in the laboratory and industrial environments. Moreover, assessment of physiological
    states of some trees (pinus, chestnut, etc.) has also been studied. The method
    based on bioimpedance indexes allows determining three distinct physiological
    states: healthy and watered plants, plants with a high level of hydric stress,
    and plants with disease [94], [95], [96]. For example, Amaro et al. [82] integrated
    an EIS system in a sensor node to analyze the health condition of the tree and
    transfer the results through the LoRaWAN protocol. Finally, tree monitoring systems
    are often influenced by the presence of foliage which can severely impact wireless
    communication systems performance. This generally leads to node densification
    to increase coverage levels, especially in large areas, resulting in additional
    costs and constraining the design of LoRa systems in nonhomogeneous vegetation
    environments [84], [85], [97]. For such reasons, a small drone with a GW is sometimes
    required for collecting data from nodes and solving the Fresnel zone radio propagation
    issues encountered in tree farms [98], [86]. Summary and Insights: In this section,
    some methodologies for monitoring tree health have been discussed. It is important
    to highlight how LoRa can be easily integrated into these systems, e.g., to measure
    the lymph flow or bio-impedance of trees. In case the parameter to be monitored
    involves roots, it has been shown that LoRa can be used for under-ground or near-ground
    communications too [12], [99]. Finally, the use of drones for data collection
    has been exploited to solve the problem of foliage scattering. D. Livestock Monitoring
    System Smart livestock practices aim at improving the productive and reproductive
    parameters, feeding and handling of feces, producing a direct effect on the increment
    of the farmers’ income, and also better milk and meat production [100]. The implementation
    of these practices requires to monitor the general health conditions of the animals,
    by tracking some biological signals to be associated to symptoms of disease, estrus
    and calving [101]. Wearable sensor technologies provide the possibility of remotely
    managing individual animals facilitating urgent interventions, responding to time
    and labor-intensive concerns in a more efficient way [87]. In extensive livestock
    production systems, the absence of access to networking and animal contact presents
    a barrier to the effective use of these technologies. Wearable sensors, to be
    more practical for extensive management settings, must: 1) network over longer
    distances; 2) have reliable power supplies (preferably renewable); 3) be low-cost
    so that damaged and lost sensors are less economically impactful; and 4) transmit
    data in real time. For these reasons, LoRaWAN technology is indicated for above
    described applications, some of which are summarized in Table IV. Primarily, these
    systems are used to monitor the animal health, but by integrating LoRa technology
    with a GPS, remote grazing systems can be implemented [87], [88]. TABLE IV LoRa-Based
    Sensor Applications in Livestock Monitoring Animal monitoring can involve completely
    different scenarios; as a consequence, the LoRa network architecture could require
    a more specific design effort to work either in indoor or outdoor settings. For
    example, the work in [89] proposes two different versions of GWs: an indoor GW,
    designed for installation in sheltered areas such as barns and cowsheds and oriented
    toward dairy cattle livestock scenarios, and an outdoor version, more specific
    for open areas such as paddocks and pasture lands, and designed for beef cattle
    livestock scenarios. The indoor GW is conceived for monitoring several important
    physical parameters typical of the shed environment, such as temperature, relative
    humidity, illuminance, carbon dioxide (C O 2 ) , and ammonia (N H 3 ) concentration,
    while the main purpose of the outdoor GW is to manage nodes in remote areas, far
    from the shed, directly on the pasture land. In the open field scenario, weather
    parameters (temperature and humidity) are collected, for purposes of correlation
    with the animal health status. Moreover, in the presence of large herds, the high
    node density could cause an increase in collisions between sent packets. In such
    scenarios, a MAC layer that includes a listen-before-talk (LBT) mechanism could
    prevent as much as possible packet collisions among nodes. Indeed, LBT-based carrier-sense
    multiple access with collision avoidance (CSMA/CA) can be incorporated with the
    physical layer of LoRa [89]. The CA mechanism is based on a random retransmission
    time that randomizes the access of the nodes to the wireless medium. The size
    of the pasture area is another factor to consider in deploying the LoRaWAN network.
    In particular, it has been shown that in large areas of pasture, the use of a
    mobile GW that moves along the track is a better solution than the use of one
    or more static GWs [90]. Contrarily, when the livestock area is not too large,
    using only one static GW is preferable because the data extraction rate value
    is high enough and the energy consumption is lower compared to multiple static
    GWs or one mobile GW. The instance of cattle monitoring in New Mexico, as described
    by Actility [102], is one of the successful illustrations of a large-scale LoRaWAN-enabled
    deployment. Due to the large size of these desert ranches (10000–20000 hectares)
    and the large number of cows to track (up to 7000), monitoring and obtaining information
    regarding cattle wellbeing can be time consuming and expensive. Indeed, while
    the cattle were previously followed using traditional GPS devices, the absence
    of reliable cellular connection throughout the whole grazing region made this
    method ineffective. These issues were solved with an off-the-shelf LoRaWAN solution
    because of its extensive range and good coverage. Finally, LoRa technology can
    be used for sharing the short text messages and voice messages in the absence
    of cellular coverage. For example, COWShED [103] is used for supporting livestock
    transhumance in Senegal. Summary and Insights: In this section, we showed how
    LoRa is used to collect information about the movements and health of livestock,
    as well as on the conditions of grassland. LoRa can also aid herders in achieving
    remote grazing by combining data with electronic fences, to identify whether animals
    have crossed it. In addition, LoRa has been used to monitor environmental parameters
    of barns, demonstrating how this technology can be adopted in both outdoor and
    indoor scenarios. Additionally, innovative MAC schemes, such as LBT method could
    be implemented to minimize packet collisions when big herds present, and to mitigate
    the limits on the effective duty cycle of channel occupation. Finally, in the
    absence of cellular coverage, LoRa/LoRaWAN solutions have been used for large-scale
    cattle monitoring or even supporting livestock transhumance for text/voice messages.
    SECTION VI. Lessons Learned and Open Issues This Section discusses the lessons
    learned and the open research challenges for using LoRa technology in smart agriculture.
    According to the aim of this article, it was learned that, given the wide variety
    of sensors used in smart farming systems, different communication protocols need
    to be integrated, particularly when different platforms/vendors coexist and data
    must be collected from the various subsystems. In addition, since power supplies
    are frequently unavailable in a large agricultural area, nodes should be as energy
    self-sufficient as possible. Using local or edge data processing could mitigate
    this problem, optimizing the energy consumption. Moreover, keeping the logic on
    the Edge of the network could alleviate the hurdle on LoRa’s centralized communications
    (especially on the downlink). The development of interoperability in smart agriculture
    systems can also be accelerated by platforms such as FIWARE and Cayenne, while
    machine learning can be used to model and analyze technical problems, improving
    scalability of LoRa networks and predicting network congestion. The experience
    gained in Industry 4.0 can be transferred to agriculture, considering some peculiarities,
    including the need to cover large spaces that cannot be manned. In addition, there
    is the need to provide device power supply and data security (partially solved
    by leveraging on LoRaWAN built-in security schemes). Another significant factor
    is the initial cost of the system, which must be as low as possible since the
    pay-back time also depends on elements that cannot be predicted during the year,
    such as weather. Finally, it has been recognized that although ICT has long-term
    sustainability issues to be solved, they show great potential for improving the
    usage of natural resources, especially when cyber–physical systems (CPSs) are
    combined with IoT, AI, machine learning, and neuromorphic computing techniques
    [104]. Through the study carried out in this article, it is also possible to understand
    in which area LoRa has been applied and is emerging in recent years. In particular,
    among the application areas discussed in Section V, Fig. 8 shows in a pie chart
    that more than 40% of the analyzed studies focus on water management, while almost
    25% are dedicated on crop monitoring, followed by tree monitoring. This result
    is in line with recent market surveys on LPWANs (e.g., [105]), and other general
    studies on communications protocols for smart agriculture [9]. Fig. 8. Distribution
    of the LoRa papers according to smart agriculture application areas. Show All
    All this confirms the great potential of implementing smart agriculture solutions
    using IoT, and LoRa technology in particular. However, there are still some open
    issues that need to be faced: for example, LoRaWAN works quite well in uplink
    when it needs to collect data from sensors, while downlink connections might suffer
    high latency. In what follows, we briefly discuss future research directions related
    to downlink latency, energy management, device heterogeneity and interoperability,
    data management, and scalability. These open issues must be solved for LoRa systems
    to be widely adopted in smart agriculture. We conclude the Section with an eye
    on other wireless technologies, different from LoRa/LoRaWAN. A. LoRa Downlink
    Performance The downlink performance of LPWAN systems still represents a challenge
    since it is related to the energy consumption. In particular, LoRaWAN allows different
    tradeoffs between communication latency on the downlink channel and energy consumption.
    Nodes are classified by classes: they can receive only after an uplink transmission
    (Class A), or at regular time intervals (Class B), or at any time (Class C). The
    modern trend is to optimize energy efficiency, hence, data are transmitted only
    when necessary or periodically. According to the authors’ opinion, a further optimization
    could be retrieved by local data processing. As a matter of fact, even if nodes
    remain asleep most of the time, as in [57], or with scheduling intervals of reception
    windows of 10–20 min as in [48] and [50] or a few hours [58], local processing
    always lowers the data to be transmitted decreasing the transmission time; it
    has been successfully tested in [42] where edge processing on the GW allows a
    more effective control of the actuator nodes. This last approach improves also
    reliability since it allows farms to work even if the Internet connection of the
    LoRa GW is absent for a few hours. Reliability can also be improved by a Master/Slave
    access control method for the LoRa network [52], [53], [54]. Alternatively, for
    short-range communication, a Wake-up Radio (WuR) can be adopted. WuR technology
    is an ultralow-power receiver that is continuously listening to the channel while
    spending a few nanowatts or microwatts depending on the circuit’s design. WuRs
    work in parallel to the main LoRa transceiver and allow asynchronous wake-up of
    the nodes with low latency. With the LoRa-WuR scheme, the downlink latency can
    be reduced by almost 90% compared to the traditional LoRa protocol for a ten nodes
    cluster [106], [107]. B. Energy Efficiency Considerations In addition to downlink
    communication performance just explained, the energy consumption in an agricultural
    ED can include turning on booster pumps or solenoid valves, activating sensors
    over a long period, use of GPS and data transmission, etc. Nodes should be autonomous
    as much as possible since usually power supplies are not available in a wide agricultural
    area. Besides, the use of batteries needs to minimize disposal costs and pollution.
    Providing solutions to avoid the use of batteries by harvesting energy from the
    environment would encourage the deployment of wireless devices in smart agriculture.
    The use of different energy sources, such as solar energy, piezoelectricity, thermal,
    wind, water, and radiofrequency is consolidated [108]. However, making a device
    completely energy-neutral requires a thorough analysis of power consumption in
    different working states [109]. One facilitation is the availability of a renewable
    energy source as in [51]; on the other hand, a high energy consumption due to
    the heating of one probe as in [91] requires a different design or the remote
    monitoring of the energy available or harvested as in [110]. It is evident that
    there are many factors that influence the analysis of offering-demanding energy,
    it varies on a case-by-case basis and does not lend itself to systematic analysis;
    on the other hand, in this context, machine learning algorithms can give a significant
    contribution. Infact, the ML approach has been already successfully applied in
    different contexts allowing to implement an efficient renewable energy selection
    based on the geographic location [111], or to retrieve a good energy prediction
    [112]. An application example is given by the energy-neutral system for pest detection
    [70] which takes advantage of ML algorithms. C. Heterogeneity and Interoperability
    Smart agriculture systems are quite heterogeneous in terms of sensors and, in
    some cases, it is also required to integrate different communication technologies,
    e.g., when multiple platforms coexist and data arrives from different subsystems.
    LoRa platforms are used with ZigBee to implement hybrid communications managing
    different sensors clusters or with the IEEE 802.11s-based system to build a mesh
    networking architecture. The path for the integration of different technologies,
    such as cloud, IoT, and software-defined networking, with AI is proposed in [113]
    with the related challenges and opportunities. Assuring communications in heterogeneous
    smart agriculture systems is a critical issue that has been studied for example
    in [114], where LoRa and ZigBee hybrid communications are implemented. Precisely,
    two LoRa sensor clusters and two ZigBee sensor clusters are used and combined
    with two ZigBee-to-LoRa converters to communicate in a network managed by a LoRa
    GW. The token ring protocol in the ZigBee network and polling mechanism in the
    LoRa network is used. The system can work with a packet loss rate of less than
    0.5% when the communication distance is 630 m for the ZigBee network and 3.7 km
    for the LoRa network. An hybrid LoRa/IEEE 802.11s-based mesh networking architecture
    is proposed in [115], where an effective network protocol selection mechanism
    is developed to choose the right interface. Protocol selection is based on multiple
    parameters, including network communication interface type, GNSS position of the
    APs, RSSI of nearby nodes, type and amount of data to be transmitted. Large data
    to be transferred in a short time can rely on the IEEE 802.11s-based network while
    small data can be transmitted through a LoRa-based mesh network. Platforms such
    as FIWARE [116], Cayenne [41] and mySense [58], discussed in Section IV, can also
    give a push to achieve interoperability in the smart agriculture systems. The
    above described solutions can benefit of an “industry 4.0”-based approach where
    the integration of different protocols cooperate to address the needs of automating
    computing and technology processes [113], [18]. D. Machine Learning and Big Data
    Management The integration of big data analysis with machine learning can provide
    predictions about future outcomes, such as fruit quality or detect crops’ diseases
    using historical data, analytical techniques, and statistical modeling [117].
    The benefits of ML in the agriculture are relevant [118]. However, the deployment
    of models is the most challenging step to bring the ML algorithms in the production
    fields, and thanks to its advantages, LoRa technology could make a big contribution
    to taking this step. Collected data can be used to implement an intelligent system
    capable of supporting the identification of varieties and predicting the quality
    of the final product [64]. In fact, exploiting ML, the data can be used by the
    biologists to develop crop models and perform disease prediction [68]. The agricultural
    industry produces a large amount of data collected by heterogeneous sensors, so
    best practices should include the mechanisms to reduce the memory and time for
    data analysis. Thus, to pursue such objectives, edge computing models are also
    applied [119]. Distributed data process, such as MapReduce [120], may avoid bottlenecks
    when transferring all data to a single server, as in [121] where the proposed
    method adopts smart sensors to measure the soil quality indicators, while the
    preelaborated data is transmitted using the LoRaWAN protocol. The Apache Spark
    environment is then used to implement a parallel algorithm for statistical models
    based on the soil indicator data obtained from the experimental field. E. LoRa
    Scalability and Network Improvement As concerns scalability, some open points,
    shared with general applications, are recognized. For this reason, most of the
    reference literature does not directly address issues related to agriculture.
    Indeed, scalability is a key feature in LoRa networks due to its long-range and
    large number of devices can concurrently reach a given GW. The network scales
    quite well if dynamic transmission parameters are used, in combination with multiple
    sinks. However, the correct behavior of the NS is not easy to be evaluated [122].
    In fact, the NS presents some challenges from the point of view of its optimization,
    such as processing duplicate packets or packets from other networks, or bringing
    down the entire network in case of Internet connection loss. LoRa networks are
    bound by strict legal requirements, particularly where no LBT schemes are utilized.
    The transmission duty cycle (TDC) regulates the ISM bands to determine the maximum
    time that the band can be occupied, typically bounded to 1%. This implies that
    devices may not occupy the ISM band for more than 36 s per hour, forbidding the
    transmission of new packets when this limit is attained [123]. Machine learning
    can be applied to model and analyze technical problems, improving the scalability
    of LoRa networks and predicting network congestion [124]. Further developments
    could include enhanced ADR mechanisms, optimization of GW locations, and interference
    cancelation techniques [125]. Finally, some challenges remain such as the widespread
    adoption of multihop communications in LoRaWAN. Literature has shown that multihop
    or mesh topologies can extend the coverage of LoRaWAN networks and improve energy
    efficiency in certain scenarios [126]. These solutions propose intermediate nodes
    to forward messages to other EDs to extend the coverage. Other open points include
    the use of GWs as intermediate nodes, GW-to-GW communications, and practical large-scale
    deployment of LoRaWAN mesh networks. F. Other Communication Technologies The choice
    of a specific communication technology is central to the performance of IoT-based
    agricultural applications. Other than LoRa, many standards for wireless communications
    can be employed, including Bluetooth, ZigBee, Z-Wave, RFID, Sigfox, and NB-IoT.
    Some of them work well in the short-range (within 100 m), while others are more
    useful to cover long distances (up to tens of kilometers). Examples of the former
    are Bluetooth, ZigBee, Z-Wave, and passive and active RFID systems, while in the
    latter, standards are Sigfox and NB-IoT (and LoRa of course). As discussed previously,
    the deployment of a massive number of IoT devices might cause interference problems
    especially for technologies using the unlicensed spectrum, such as ZigBee, Wi-Fi,
    Sigfox, and LoRa. On the other hand, IoT devices operating with a licensed spectrum
    eliminate interference problems but might increase costs significantly. Several
    papers have analyzed different aspects of wireless communication protocols for
    smart agriculture, studying possible applications and comparing their performance.
    For example, ZigBee-based smart agriculture systems are described in [127], [128],
    [129], [130], and [131]. The biggest challenges for ZigBee networks are the limited
    range and increased power consumption (compared to LPWANs) and relatively low
    data rate (e.g., compared to BLE or WiFi). Therefore, ZigBee is better suited
    for small-scale scenarios [130], while the use of this protocol is not suitable
    when the agricultural area is vast and the distance between sensor nodes is large.
    On the other hand, the works [132], [133], [134], [135] represent successful examples
    of NB-IoT applications in smart agriculture. Indeed, extensive coverage, adaptable
    power consumption (depending on the mode of operation), and low interference among
    nodes, are features that make NB-IoT an interesting protocol for various agricultural
    systems [136]. However, NB-IoT employs licensed frequency channels, which results
    in higher subscription prices for the associated system even if it offers a higher
    data throughput than LoRa. Moreover, when there is an existing LTE infrastructure
    already in place, the need for hardware update may be another source of expense
    for such a system. This might be a drawback in the context of smart agriculture
    if the projected return on investment is not high enough to cover these costs
    [137]. Overall, the choice of the communication technology in smart agriculture
    needs to consider many factors and requirements, such as support for roaming,
    suitability of technology to small-scale, medium-scale, and large-scale deployments,
    geographical location, costs, etc. For example, it has been shown that Sigfox
    and LoRaWAN excel on network capacity, battery lifetime, and cost, whereas NB-IoT
    achieves higher quality of service and lower latency [27]. Finally, while LoRaWAN
    has been considered the most suitable communication network for IoT in smart agriculture
    [16], it is still difficult to tell which technology will dominate the market,
    or if several technologies will coexist, perhaps specializing on different application
    domains. SECTION VII. Conclusion Although the expected transition to smart agriculture
    has already begun, researchers around the world are still looking for new solutions
    to improve agricultural productivity through IoT architectures. Indeed, albeit
    applications in agriculture can benefit from the experience gained in Industry
    4.0, they require specific knowledge regarding sensor management, energy optimization,
    and data processing. LoRa technology is widely adopted, as it allows building
    an autonomous network that meets some of the requirements of the smart agriculture,
    such as low-power and long-range communication. The adoption of LoRa-based systems
    in agriculture results in an effective way to improve the connectivity of farms,
    encourage the deployment of DSSs and consequently improve their management, leading
    the agricultural sector toward smart agriculture. In order to provide a more focused
    and comprehensive view of the applications in the field, in this article, we restricted
    our focus to LoRa/LoRaWAN technology and its uses in the context of smart agriculture.
    We presented many LoRa applications in the field, and we discussed some open issues
    and research areas for future improvements. The main challenges analyzed using
    LoRa Technology in smart agriculture, are: latency on the downlink channel, energy
    management, heterogeneity and interoperability of the devices, data management,
    and scalability. All of these can benefit from the use of machine learning algorithms.
    Indeed, AI and edge computing are still scarcely used but related algorithms and
    technologies are now mature and may be successfully applied in this field. Finally,
    the optimization of multiple GW locations and multihop topologies to extend the
    coverage of LoRa networks have been recently tested to further improve the performance
    and coverage. NOTE Open Access funding provided by ‘Università degli Studi di
    Palermo’ within the CRUI CARE Agreement Authors Figures References Citations Keywords
    Metrics More Like This Internet of Things and Wireless Sensor Networks for Smart
    Agriculture Applications: A Survey IEEE Access Published: 2023 Weighted Connected
    Vertex Cover Based Energy-Efficient Link Monitoring for Wireless Sensor Networks
    Towards Secure Internet of Things IEEE Access Published: 2021 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6488907/10038283/09993728.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Survey on LoRa for Smart Agriculture: Current Trends and Future Perspectives'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.23960/jtep-l.v11i3.518-530
  analysis: '>'
  authors:
  - Nurpilihan Bafdal
  - Irfan Ardiansah
  - Sandi Asmara
  citation_count: 0
  full_citation: '>'
  full_text: ">\n \n518 \n \n \n \nABSTRACT \nArticle History : \n \nKeywords : \n\
    \ \n \n \n \n \nThe Internet of Things (IoT) is currently influencing many facets\
    \ \nof human life. Smart agriculture is one system that can use the \nIoT to improve\
    \ production efficiency and consistency across \nagriculture, improve crop quality,\
    \ and reduce negative \nenvironmental impacts. The architecture of an IoT-based\
    \ \nmicroclimate monitoring system tailored for use with the Unpad \nALG greenhouse\
    \ is shown in this paper. The suggested system \ndesign can collect microclimate\
    \ data using the SHT11 and GUVA-\nS12SD microclimate sensors and store it in a\
    \ database on a \nRaspberry Pi with a cloud computing back-end idea. The \nRaspberry\
    \ Pi is also used to process and analyze data in order to \nset up mist-based\
    \ greenhouse cooling systems. The collected \ndata is delivered to a web-based\
    \ front-end node, where users \ncan access from their own device. The results\
    \ reveal that when \nthe temperature rises beyond the predetermined threshold\
    \ of \n30°C or the humidity falls below 80%, the system can activate \nthe mist-based\
    \ cooling system. With a temperature difference of \n6.25 degrees Celsius lower\
    \ and humidity of 28.06 percent \ngreater, the system is able to perform better\
    \ than it was \nintroduced before. The automation system's performance can \n\
    reach 15.22% better, however it declines as the light intensity \nrises. \nApplication\
    \ of Internet of Things (IoT) on Microclimate Monitoring \nSystem in The ALG Unpad\
    \ Greenhouse Based on Raspberry Pi \nNurpilihan Bafdal1, Irfan Ardiansah2 , Sandi\
    \ Asmara3 \n1Departement of Agricultural Engineering and Biosystem, Faculty of\
    \ Agro-Industrial Technology, Universitas Padjadjaran, \nBandung, INDONESIA \n\
    2Departement of Agro-Industrial Technology, Faculty of Agro-Industrial Technology,\
    \ Universitas Padjadjaran, Bandung, \nINDONESIA  \n3Department of Agricultural\
    \ Engineering, Faculty of Agriculture, Universitas Lampung, Lampung, INDONESIA\
    \ \n1. INTRODUCTION \n \nFood security in Indonesia has become an issue that needs\
    \ attention in corresponding to \nwith the change in land use from agriculture\
    \ to industry, which raises concerns about a \nfood crisis due to population growth.\
    \ The average land ownership of Indonesian farmers is \nonly 0.25 hectares per\
    \ farm family. Agricultural productivity has always experienced \nproblems such\
    \ as an inappropriate growing environment; inefficiency of farm production \n\
    scale; limited quality of human resources in managing land and agricultural cultivation\
    \ \npatterns that are difficult to change because they are inherent from year\
    \ to year (Bafdal & \nDwiratna, 2018; Saliem & Ariani, 2016). \nReceived : 24\
    \ February 2022 \nReceived in revised form : 15 June 2022 \nAccepted : 14 July\
    \ 2022  \n \nGreenhouse,  \nInternet of things,  \nMicro-climate,  \nMisting system,\
    \  \nRaspberry pi. \nCorresponding Author: \nnurpilihanbafdal@yahoo.com  \nVol.\
    \ 11, No. 3 (2022) : 518 - 530 \nDOI : http://dx.doi.org/10.23960/jtep-l.v11i3.518-530\
    \ \n519 \nJurnal Teknik Pertanian Lampung Vol 11, No. 3 (2022) : 518-530 \n \n\
    Farmers always maintain conventional cropping patterns; namely a pattern that\
    \ is \noriented towards yield by ignoring the latest techniques in agriculture\
    \ that can be used \nin  proper planning, monitoring and management of their agricultural\
    \ land (Ardiansah \net al., 2017). The era of technological disruption also has\
    \ an impact on the agricultural \nsector and needs the use of smarter agricultural\
    \ systems to meet the rising demand for \nfood (Jespersen et al., 2016). \nThe\
    \ increasing demand for food, both quantity and quality, has increased the need\
    \ \nfor agricultural intensification and industrialization. The \"Internet of\
    \ Things\" (IoT) is a \npromising technology capable of providing many solutions\
    \ for the modernization of \nagriculture. Scientific organizations and research\
    \ institutions, as well as industry, are \ncompeting to bring more IoT products\
    \ to agricultural business stakeholders, a clear \ndivision of roles when IoT\
    \ becomes the main choice in agriculture. Simultaneously, \ncloud computing, which\
    \ has been used previously has provided sufficient resources and \nsolutions to\
    \ sustain, store and analyze data generated by IoT devices (Elijah et al., \n\
    2018; Tzounis et al., 2017). \nThe IoT data management and analysis can be used\
    \ to automate processes, predict \nsituations, and improve many activities in\
    \ real time. Furthermore, the concept of inter-\noperability among heterogeneous\
    \ devices inspires the development of appropriate \ntools, with which new applications\
    \ and services can be created, adding value to the \ngenerated data streams at\
    \ the network edge. The agricultural sector has been heavily \ninfluenced by Wireless\
    \ Sensor Network (WSN) technology and is expected to benefit \nequally from IoT\
    \ (Bafdal & Ardiansah, 2021; Kendarto et al., 2019; Ray, 2017). Internet \nof\
    \ Things (IoT)-based smart greenhouse is one alternative that will allow farmers\
    \ to \nmanage their land more effectively and efficiently so in turn it will improve\
    \ \nperformance and productivity in a sustainable manner (Ardiansah et al., 2021).\
    \ \n \n1.1. Smart Greenhouse \nA smart greenhouse or often called a precision\
    \ greenhouse is a building that has been \nequipped with modern technology with\
    \ the aim of increasing the quantity and quality \nof agricultural products (Bafdal\
    \ & Ardiansah, 2021). In developed countries such as \nEurope, smart agriculture\
    \ applies three types of interconnected technologies, namely \nmanagement information\
    \ systems; precision agriculture and automation (Gondchawar \n& Kawitkar, 2016).\
    \ Castrignanò et al. (2020) stated that: \n1. The information system in question\
    \ is a system that collects, processes, stores \nand disseminates data in the\
    \ form required to carry out land operations and \nfunctions. \n2. Precision agriculture\
    \ is the management of spatial and temporal variability to \nincrease economic\
    \ returns after the use of inputs and reduce environmental \nimpacts \n3. Agricultural\
    \ automation is the process of applying robotics, automatic control \nand artificial\
    \ intelligence techniques at all levels of agricultural production. \nDeveloped\
    \ countries generally have farmers who are accustomed to using IoT-\nbased smart\
    \ greenhouses in particular ensuring monitoring of microclimate \nenvironmental\
    \ conditions; managing proper and efficient irrigation system; enhancing \nsoil\
    \ conditions and determining harvest time (Zaida et al., 2017). The technology\
    \ used \nis a technology system that utilizes a sensor network and connected to\
    \ the internet \nknown as the Internet of Things (IoT). This system is known as\
    \ smart farming, if this \nsystem is carried out in a greenhouse, then it is called\
    \ a smart greenhouse (Bafdal & \nArdiansah, 2021).  Farmers have realized the\
    \ benefits of a greenhouse, especially a \nsmart greenhouse if they want to develop\
    \ their farming business towards industry; \nBafdal et al. : Application of Internet\
    \ of Things (IoT) on Microclimate  ... \n520 \n \nbecause greenhouses do not only\
    \ serve to protect plants from pests and diseases and \nfrom microclimate environment\
    \ that is not favorable for plants, but the function of the \nsmart greenhouse\
    \ is more as a medium to stimulate plants with various engineering in \nthe greenhouse\
    \ such as regulating temperature, humidity, sun irradiation, controlling \nthe\
    \ water and nutrient needed by plants (Hafiz et al., 2020; Ping et al., 2018).\
    \  In \naddition to providing the advantage such as obtaining data and controlling\
    \ the \nmicroclimate environment inside the greenhouse, smart greenhouses  also\
    \ enable \nfarmers to reduce agricultural costs and optimize profits (Paustian\
    \ & Theuvsen, 2017).   \nIndonesian farmers have not fully implemented smart greenhouses;\
    \ because there \nare still some obstacles for farmers to adopt related technology\
    \ such as IoT. To \novercome this problem, patience and continuous socialization\
    \ to adopt this kind of \ntechnology needs to be introduced (Bafdal et al., 2019;\
    \ Sujadi & Nurhidayat, 2019).  \nThe significant benefits of smart greenhouses\
    \ include reducing labor, increasing \nproduction, and facilitate access to existing\
    \ technology, but farmers are required to be \nmore adaptable in terms of the\
    \ use of technology that has begun to develop very \nrapidly (Bafdal et al., 2019;\
    \ Nawandar & Satpute, 2019). \n \n1.2. Internet of Things (IoT) \nThe question\
    \ that is always raised by farming communities, especially in Indonesia is \n\"\
    Does the agricultural sector need to recognize and utilize the Internet of Things?\"\
    . The \nFood Agriculture Organization (FAO) recommends that the future agricultural\
    \ sector \nneeds to be managed by using information technology to obtain optimal\
    \ yields and \nprofit. The green revolution that have been developing in developing\
    \ countries have to \nbe accompanied by applying Internet of Things-based technology\
    \ starting from pre-\nharvest, harvest and post-harvest technology (Ardiansah\
    \ et al., 2020).   \nOne of the information technologies that can help farmers\
    \ in managing agricultural \nland is the Internet of Things (IoT). Internet of\
    \ Things (IoT) is a new paradigm that \ntransforms traditional lifestyles into\
    \ high-tech lifestyles. Smart cities, smart homes, \npollution control, energy\
    \ saving, smart transportation, and smart industries are \nexamples of IoT transformation.\
    \ However, there are many challenges and issues that \nmust be overcome before\
    \ IoT can reach its full potential. These challenges and issues \nmust be addressed\
    \ from multiple perspectives, including the application of \ntechnologies that\
    \ enable social and environmental impacts (Kumar et al., 2019; \nZuraiyah et al.,\
    \ 2019). The conventional way is replaced with an IoT-based smart \ngreenhouse\
    \ with completeness supported by IoT and connected to the internet. This \nIoT\
    \ technology has an important role, especially for a country like Indonesia, where\
    \ \nmost of its citizens still rely on their livelihood as farmers (Burange &\
    \ Misalkar, 2015; \nPing et al., 2018).  If it can be realized, IoT technology\
    \ greatly facilitates the work of \nfarmers (Gondchawar & Kawitkar, 2016). Hidayat\
    \ (2017) argues that IoT is a system \nthat uses sensor technology to process\
    \ data into information. The application of IoT in \nthe agricultural sector along\
    \ with the application of smart greenhouses makes modern \nagricultural systems\
    \ more effective and efficient. This is a solution to the problem of \nlimited\
    \ agricultural land and global climate change which in turn the food crisis can\
    \ be \novercome (Jung et al., 2021).  Figure 1  below shows IoT applications,\
    \ especially in the \nagricultural sector.  All of the applications shown in the\
    \ Figure are very feasible to be \napplied in Indonesia which consists of various\
    \ islands, so using IoT-based smart \nagriculture can make it easier to monitor\
    \ and control agricultural land or greenhouse. \n \n \n521 \nJurnal Teknik Pertanian\
    \ Lampung Vol 11, No. 3 (2022) : 518-530 \n \n \n \n \n \n \n \n \n \n \n \nFigure\
    \ 1. Smart Greenhouse with IoT Integration  \n \n1.3. IoT-Based Smart Greenhouse\
    \ Application \nThe IoT-based smart greenhouse makes the management of agricultural\
    \ cultivation \nmore controlled and accurate. The most significant examples are\
    \ precise sensing of \ncrop water and nutrient requirements, microclimate control\
    \ according to crop \nrequirements; pH; humidity; sunshine until harvest estimation\
    \ (Hafiz et al., 2020). IoT \napplications in smart greenhouses allow farmers\
    \ to monitor crops in greenhouses from \nanywhere without having to come to the\
    \ greenhouse all the time with the help of the \nnecessary sensors and observations\
    \ (temperature, humidity; sunlight and irrigation \nautomation), thereby reducing\
    \ labor costs needed (Carrión et al., 2013). \nThe application of IoT in smart\
    \ greenhouses can handle recording plant growth \ndata, digitally managing data\
    \ and making decisions in the pre-harvest and harvest \nprocesses. Data collection,\
    \ data processing and decision using IoT will allow planning of \nplanting time,\
    \ minimizing the risk of production failure and being able to predict \nagricultural\
    \ yields with more precision. By using IoT, farmers get accurate data so they\
    \ \ncan control irrigation systems that save water and provide nutrition more\
    \ efficiently on \nthe basis of  crop needs (Angelopoulos et al., 2020; Gondchawar\
    \ & Kawitkar, 2016). \nEach plant requires different microclimate conditions,\
    \ for example tomato plants \ngrown in a greenhouse require temperatures between\
    \ 18 oC to a maximum of 24 oC, \nbut when the dry season comes the temperature\
    \ in the greenhouse reaches a \nmaximum of 36 oC, as a result, plant physiology\
    \ will change, and  the growth will be \ndisturbed and greatly interfered (Bafdal\
    \ et al., 2018).  This impact is indicated by \nsuboptimal leaf growth (curly\
    \ leaves); evapotranspiration increases which causes the \nneed for water (consumptive\
    \ use) to increase, and when fertigation is carried out, the \nnutritional needs\
    \ also increase (Bafdal & Dwiratna, 2018; Kendarto et al., 2019).  The \ndynamic\
    \ changes in greenhouses require farmers to implement an IoT-based system \nthat\
    \ can act as a microclimate data monitoring device using the Arduino UNO \nmicrocontroller\
    \ and Raspberry Pi microcomputer.  These devices are integrated with a \nfog-based\
    \ air conditioning system that is connected to a relay and controlled by \nmicroclimate\
    \ changes detected by the SHTII sensor and the GUVA-S12SD sensor (both \nare to\
    \ detect changes in temperature, humidity and light intensity). The data from\
    \ the \nmicroclimate monitoring is then sent to the cloud server to be stored\
    \ in the database \nand processed to produce a decision whether or not the automation\
    \ system is active \nbased on the limits that have been previously set in the\
    \ system.  Moreover; the \nmonitoring data can be accessed through a website that\
    \ is integrated on the Raspberry \nPi (Hafiz et al., 2020). \n \n \nBafdal et\
    \ al. : Application of Internet of Things (IoT) on Microclimate  ... \n522 \n\
    \ \n2. MATERIALS AND METHODS \n \n2.1. Research Instruments \nThis research used\
    \ instruments in the form of hardware and software to build a \nmicroclimate monitoring\
    \ system.  The hardware used was a Raspberry Pi 3 version B+ \nmicrocomputer which\
    \ was used to regulate microclimate conditions in a greenhouse \nusing a mist-based\
    \ cooling system. The device was also used to store microclimate \nmonitoring\
    \ data and display web-based microclimate information. The SHT11 \nmicroclimate\
    \ sensor was used to detect changes in temperature and relative humidity \nof\
    \ the air in the greenhouse.  This sensor was digital type so that the acquired\
    \ value is \nin degrees Celsius. \nThe GUVA-S12D sensor (analog type) was used\
    \ to detect changes in the index of \nsunlight intensity in the greenhouse. To\
    \ make the misting system, an emitter was used \nto spray water (the particle\
    \ size of which is 3 microns) into the greenhouse.  In \naddition, to draw water\
    \ from the water storage into the greenhouse, a low pressure \nwater pump was\
    \ used. To turn the pump off and turn on, a relay connected to the \nRaspberry\
    \ Pi was used.  If the microclimate conditions in the greenhouse were outside\
    \ \nthe set points, the Raspberry Pi would trigger the relay and turn the pump\
    \ on.  On the \ncontrary, if the microclimate conditions were within the set points,\
    \ the Raspberry Pi \nwould trigger relay to turn the pump off. \nThe software\
    \ used in this study was the Raspbian operating system to run the \nRaspberry\
    \ Pi. The Python 3 programming language was used to acquire microclimate \ndata\
    \ from sensors and store microclimate data in the MariaDB database. The MariaDB\
    \ \ndatabase is a data storage system that supports accessibility through Python\
    \ 3 and \nPHP. While, PHP itself is a programming language used to build websites\
    \ based on data \nstored in the MariaDB database. Putty and WinSCP applications\
    \ were used as a bridge \nto connect a Windows-based laptop with a Linux-based\
    \ Raspberry Pi. \n \n2.2. Device Design \nThe method used in this research was\
    \ the engineering design method in the \nmanufacture of automation systems.  Engineering\
    \ design method according to the \nSugandi et al. (2018) is a series of activities\
    \ consisting of planning, design, \ndevelopment, and implementation which in its\
    \ application will result in new \nmodifications in the form of processes or products.\
    \ This method will make the \nmanufacture of automation tools more structured\
    \ and focused at each stage. The use \nof this method can also be strengthened\
    \ by Surakusumah (2009) regarding “Design of \nAutomatic Bottle Filler”, which\
    \ concludes that the system can control the volume of \nwater filled with precision.\
    \ \nFigure 2 shows the proposed design scheme in which the greenhouse is integrated\
    \ \nwith a mist-based cooling system connected to a low-pressure water pump to\
    \ deliver \nwater into the emitter which emits 3 micron water particles. The pump\
    \ was wired to a \nrelay that is also connected to the Raspberry Pi.  If the microclimate\
    \ data set was \noutside the set threshold, the Raspberry Pi would trigger the\
    \ relay to turn the pump on.  \nCloud-based servers were used to store microclimate\
    \ data which can then be \nmonitored by users remotely via their own devices such\
    \ as smartphones, laptops or \ndesktop computers. \n \n2.3. Research methods \
    \  \nThe research was carried out in the Unpad ALG Greenhouse located in North\
    \ Pedca, \nPadjadjaran University with coordinates: 6°55'13.9\" (S) and 107°46'27.5\"\
    \ (E) as shown \nin the image Figure 3. \n523 \nJurnal Teknik Pertanian Lampung\
    \ Vol 11, No. 3 (2022) : 518-530 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\
    \ \n \nFigure 2. Schematic of micro climate monitoring system in Unpad ALG greenhouse\
    \  \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3. Location of Unpad's ALG greenhouse\
    \  \n \nThe research was conducted to design and build a device that took 3 (three)\
    \ \nmonths, and to test the monitoring system in the ALG Unpad greenhouse for\
    \ 25 \n(twenty five) days with the microclimate data acquisition process carried\
    \ out every 1 \n(one) minute, and stored in the database. The data stored in the\
    \ database can then be \ndownloaded in Comma Separated Values (CSV) format, to\
    \ facilitate data processing in a \nspreadsheet application, then analyzed statistically\
    \ and described in the form of charts. \n \n2.4. Research Procedure/Implementation\
    \ \nThe automation device that has been built was then integrated into the ALG\
    \ Unpad \ngreenhouse as shown in Figure 4. The misting system pipe was designed\
    \ in the form of \na trident with a total of 18 (eighteen) fogging valve points\
    \ so that the mist distribution \ncan reach all points in the greenhouse when\
    \ conditions are not ideal. The device was \nrun from 06.00 – 18.00 every day,\
    \ all sensor data is directly read every minute and \nstored into the database.\
    \ The data was then processed and displayed in tabular form \non a website so\
    \ that can be accessed via the internet using the services provided by \nDynDNS.\
    \ Parameters of temperature and relative humidity were set through the admin \n\
    page which is directly updated on the system. \n \n \n \n \nBafdal et al. : Application\
    \ of Internet of Things (IoT) on Microclimate  ... \n524 \n \n \n \n \n \n \n\
    \ \n \n \n \n \n \n \n \nFigure 4. Microclimate monitoring system installation\
    \ design in greenhouse \n \n3. RESULTS AND DISCUSSION \n \nThe design of this\
    \ automation system includes the stages of design, mechanism design, \nfunctional\
    \ design, assembly and programming of tools and integration of the web. \nProgramming\
    \ is done in Python 3, Html5, PHP and SQL. This microclimate automation \nsystem\
    \ is divided into two main parts, namely the technical environment and the \n\
    information environment. The technical environment processes microclimate data\
    \ and \nexecutes commands that will be sent to the automation device, while the\
    \ information \nenvironment displays data information about the microclimate and\
    \ sets the \ntemperature or humidity in the greenhouse. \nSchematic is an arrangement\
    \ of patterns and a series of components that make up a \nsystem. The schematic\
    \ of this automatic microclimate control system consists of a \nRaspberry Pi 3b+\
    \ as a microcomputer and 3 main sensors, namely the SHT11 sensor, \nthe GUVA-S12SD\
    \ sensor, and a Relay which is connected to a microcomputer using a T \ncobbler\
    \ plus and a breadboard. The relay will be connected to the misting cooling \n\
    system adapter so that when the SHT11 sensor detects the greenhouse temperature\
    \ is \nhigher than 30˚C or the humidity is less than 80%, the relay automatically\
    \ activates the \nmisting cooling system. The supply of misting cooling system\
    \ water is obtained from a \nwater tank which is flowed by a water pump with a\
    \ pressure of 80-100 psi so that the \nwater released from the nozzle becomes\
    \ dew. This system is connected to the web so \nthat supervision and regulation\
    \ of the automation system can be carried out with \ngadgets such as smartphones\
    \ or laptops. \nThe layout of misting cooling system used in the greenhouse form\
    \ a three-arrowed \nstructure (trisula) having 6 nozzles for every row. This formation\
    \ is used so that the \nmist released from the nozzle can reach all plants in\
    \ the greenhouse. \nThere are two comparisons made between the automation system\
    \ and the \nprevious greenhouse system, namely the comparison of greenhouse microclimate\
    \ data \ncollection and comparison of greenhouse microclimate conditions. Greenhouse\
    \ \nmicroclimate data collection with an automated system is carried out every\
    \ one \nminute per day. The data is stored in the database and displayed through\
    \ the website. \nThe data that can be stored in the database can be in the thousands.\
    \ For example, the \ndata stored in the database for 25 days is 278,880. If the\
    \ average data that can be \nstored per day reaches ± 11,115. That much data is\
    \ sufficient to accurately describe the \ndaily greenhouse microclimate conditions\
    \ and the data collection is carried out \nautomatically by the system. \n525\
    \ \nJurnal Teknik Pertanian Lampung Vol 11, No. 3 (2022) : 518-530 \n \n \n \n\
    \ \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 5. Layout design of misting cooling\
    \ system \n \nThe greenhouse microclimate data collection was previously done\
    \ manually three \ntimes per day, namely at 07:00, 12:00 and 17:00. Data collection\
    \ is done by writing it \ndown on a small blackboard which allows for some errors,\
    \ namely miswriting/reading a \ncomma (,) or a period (.). Small whiteboards cannot\
    \ store a lot of data and could be \naccidentally erased. Such manual data collection\
    \ also requires the user to go to the \ngreenhouse every day according to the\
    \ specified hours as shown in Figure 6.  On the \nother hand, an automated system\
    \ can conduct greenhouse microclimate data \ncollection more effectively and accurately.\
    \ This is further strengthened by an increase \nin performance which reached 370,400%.\
    \  In this case, the data measured manually \none month earlier than the automatic\
    \ data recorder, but still in the same greenhouse. \nHowever; manual measurement\
    \ data includes the data of sunlight intensity entering \nthe greenhouse, while\
    \ the automatic measurement records the sun's uv index. \nAutomatic recording\
    \ using IoT devices and the microclimate conditioning process using \nthis fog\
    \ misting system convince greenhouse users if the temperature and humidity in\
    \ \nthe greenhouse are accurately measured and inside the threshold that can be\
    \ \ntolerated by plants. \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 6. Previous\
    \ greenhouse microclimate data collection \nComparison of the data from the automation\
    \ system and the previous system \nincludes temperature, average humidity, maximum\
    \ temperature, minimum humidity \nBafdal et al. : Application of Internet of Things\
    \ (IoT) on Microclimate  ... \n526 \n \nof the greenhouse microclimate for 25\
    \ days at 06.00 – 18.00. Comparison of sunlight \ncannot be done because the automation\
    \ system measures the sun's UV index while \nthe previous system measures the\
    \ intensity of sunlight. \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 7. Information\
    \ displayed on the website \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure\
    \ 8. Results of average temperature measurement in greenhouse  \n \nThe data on\
    \ the comparison chart of greenhouse temperatures shows that the \nautomation\
    \ system can work better than the previous system that has not used a \ncooling\
    \ system by recording using a whiteboard. In the previous system, microclimate\
    \ \ndata was recorded manually by reading a thermometer and an analog hygrometer\
    \ \nusing the eyes and the results were written on the whiteboard three times\
    \ a day. \nThis is evidenced by the difference in average temperature per day\
    \ which reaches \n6.25˚C. In addition, using the automation system also makes\
    \ the average daily \ngreenhouse temperature below the maximum temperature you\
    \ want to maintain \n(30˚C) which is 25.97˚C, 4.03˚C smaller than the previous\
    \ system. The average daily \ngreenhouse temperature exceeds the maximum temperature\
    \ to be maintained (30˚C), \nwhich is 32.23˚C, greater than 2.23˚C. More than\
    \ that; The maximum temperature is \n2.68˚C lower than before. If we calculate\
    \ the percentage increase in the performance \nof the automation system compared\
    \ to the previous system in maintaining the \ngreenhouse temperature, the values\
    \ obtained are 19.42% at the average temperature \nand 6.65% at the maximum temperature.\
    \ \n \n \n527 \nJurnal Teknik Pertanian Lampung Vol 11, No. 3 (2022) : 518-530\
    \ \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 9. Average humidity measurement\
    \ in green house \n \nThe data on this greenhouse humidity comparison chart (Figure\
    \ 9) shows that the \nautomation system can also work better than the previous\
    \ system. This is evidenced by \nthe difference in the average humidity per day\
    \ which reaches 28.06%. The automation \nsystem makes the average humidity very\
    \ close to the minimum humidity to be \nmaintained (80%) which is 79.79%, only\
    \ 0.31% smaller. Meanwhile, the previous \nsystem provided that the average daily\
    \ greenhouse humidity was very far from the \nminimum humidity to be maintained\
    \ (80%) which was 51.73%, the difference in value \nwas 28.27%. On the other hand,\
    \ the minimum daily greenhouse humidity from both \nsystems shows a value that\
    \ is very far from the minimum humidity to be maintained \n(80%) which is only\
    \ ±21%, the difference is 59%. However, the minimum average \nhumidity after using\
    \ the automation system is still 0.72% better than the previous \nsystem. Improved\
    \ performance of the automation system over the previous system in \nmaintaining\
    \ greenhouse humidity; however, 54.24% for the average humidity and \n3.42% for\
    \ the minimum humidity. \n \n4. CONCLUSION \n \nThe conclusion that can be drawn\
    \ from this research is that data collection on \ngreenhouse microclimate conditions\
    \ can be carried out more effectively and accurately \nusing an automated system.\
    \ The automation system can maintain and regulate the \ngreenhouse microclimate\
    \ to suit the optimum conditions of plants effectively every day \nbut there will\
    \ be a decrease in performance when the intensity of sunlight is high. The \n\
    mist-based cooling system can be turned on automatically when the microclimate\
    \ is \noutside the set threshold. It is advisable to provide additional tools\
    \ to adjust the \namount of incoming sunlight intensity, because when the solar\
    \ intensity index is too \nhigh, the mist-based cooling system cannot reduce the\
    \ greenhouse temperature \nproperly. \n \nREFERENCES \n \nAngelopoulos, C.M.,\
    \ Filios, G., Nikoletseas, S., & Raptis, T.P. (2020). Keeping data at the \nedge\
    \ of smart irrigation networks: A case study in strawberry greenhouses. \nComputer\
    \ \nNetworks, \n167, \n107039. \nhttps://doi.org/10.1016/\nj.comnet.2019.107039\
    \ \nBafdal et al. : Application of Internet of Things (IoT) on Microclimate  ...\
    \ \n528 \n \nArdiansah, I., Bafdal, N., Bono, A., Suryadi, E., & Husnuzhan, R.\
    \ (2021). Impact of \nventilations in electronic device shield on micro-climate\
    \ data acquired in a \ntropical greenhouse. INMATEH - Agricultural Engineering,\
    \ 63(1), 397–404. https://\ndoi.org/10.35633/INMATEH-63-40 \n \nArdiansah, I.,\
    \ Bafdal, N., Suryadi, E., & Bono, A. (2020). Greenhouse monitoring and \nautomation\
    \ using Arduino: A review on precision farming and Internet of Things \n(IoT).\
    \ International Journal on Advanced Science Engineering Information \nTechnology,\
    \ 10(2), 703-709. https://doi.org/10.18517/ijaseit.10.2.10249 \n \nArdiansah,\
    \ I., Pujianto, T., & Putri, G.A. (2017). Analisis perencanaan dan pengendalian\
    \ \npersediaan beras pada Perum BULOG Divisi Regional Jawa Barat. Jurnal String,\
    \ 2\n(1), 10-17. \n \nBafdal, N., & Ardiansah, I. (2021). Application of Internet\
    \ of Things in smart greenhouse \nmicroclimate management for tomato growth. International\
    \ Journal on Advanced \nScience, Engineering and Information Technology, 11(2),\
    \ 427–432. https://\ndoi.org/10.18517/ ijaseit.11.2.13638 \n \nBafdal, N., & Dwiratna,\
    \ S. (2018). Water harvesting system as an alternative \nappropriate technology\
    \ to supply irrigation on red oval cherry tomato production. \nInternational Journal\
    \ on Advanced Science, Engineering and Information \nTechnology, 8(2), 561–566.\
    \ https://doi.org/10.18517/ijaseit.8.2.5468 \n \nBafdal, N., Dwiratna, S., & Kendarto,\
    \ D.R. (2018). Differences growing media in autopot \nfertigation system and its\
    \ response to cherry tomatoes yield. Indonesian Journal \nof Applied Sciences,\
    \ 7(3), 63–68. https://doi.org/10.24198/ijas.v7i3.14369 \n \nBafdal, N., Dwiratna,\
    \ S., & Sarah, S. (2019). Impact of rainfall harvesting as a fertigation \nresources\
    \ using autopot on quality of melon (Cucumis melo L). International \nConference\
    \ on Food Agriculture and Natural Resources (FAN), 194(FANRes 2019), \n254–257.\
    \ \n \nBurange, A.W., & Misalkar, H.D. (2015). Review of Internet of Things in\
    \ development of \nsmart cities with data management & privacy. Conference Proceeding\
    \ - 2015 \nInternational Conference on Advances in Computer Engineering and Applications,\
    \ \nICACEA 2015. https://doi.org/10.1109/ICACEA.2015.7164693 \n \nCarrión, F.,\
    \ Tarjuelo, J.M., Carrión, P., & Moreno, M.A. (2013). Low-cost microirrigation\
    \ \nsystem supplied by groundwater: An application to pepper and vineyard crops\
    \ in \nSpain. Agricultural Water Management, 127, 107–118. https://doi.org/10.1016/\n\
    j.agwat.2013.06. 005 \n \nCastrignanò, A., Buttafuoco, G., Khosla, R., Mouazen,\
    \ A.M., Moshou, D., & Naud, O. \n(2020). Agricultural Internet of Things and Decision\
    \ Support for Precision Smart \nFarming. Elsevier. \n \nElijah, O., Rahman, T.A.,\
    \ Orikumhi, I., Leow, C.Y., & Hindia, M.N. (2018). An overview of \nInternet of\
    \ Things (IoT) and data analytics in agriculture: Benefits and challenges. \n\
    529 \nJurnal Teknik Pertanian Lampung Vol 11, No. 3 (2022) : 518-530 \n \nIEEE\
    \ Internet of Things Journal, 5(5), 3758–3773. https://doi.org/10.1109/\nJIOT.2018.2844296\
    \ \n \nGondchawar, N., & Kawitkar, R.S. (2016). IoT based smart agriculture. International\
    \ \nJournal of Advanced Research in Computer and Communication Engineering, 5(6),\
    \ \n838–842. https://doi.org/10.17148/IJARCCE.2016.56188 \n \nHafiz, M., Ardiansah,\
    \ I., Bafdal, N., Info, A., & Control, M. (2020). Website based \ngreenhouse microclimate\
    \ control automation system design. JOIN (Jurnal Online \nInformatika), 5(1),\
    \ 105–114. https://doi.org/10.15575/join.v5i1.575 \n \nHidayat, T. (2017). Internet\
    \ of Things smart agriculture on ZigBee: A systematic review. \nJurnal Telekomunikasi\
    \ dan Komputer, 8(1), 75. https://doi.org/10.22441/\nincomtech.v8i1.2146 \n \n\
    Jespersen, L., Griffiths, M., Maclaurin, T., Chapman, B., & Wallace, C.A. (2016).\
    \ \nMeasurement of food safety culture using survey and maturity profiling tools.\
    \ \nFood Control, 66, 174–182. https://doi.org/10.1016/j.foodcont.2016.01.030\
    \ \n \nJung, J., Maeda, M., Chang, A., Bhandari, M., Ashapure, A., & Landivar-Bowles,\
    \ J. \n(2021). The potential of remote sensing and artificial intelligence as\
    \ tools to \nimprove the resilience of agriculture production systems. Current\
    \ Opinion in \nBiotechnology, 70, 15–22. https://doi.org/10.1016/j.copbio.2020.09.003\
    \ \n \nKendarto, D.R., Mulyawan, A., Sophia Dwiratna, N.P., Bafdal, N., & Suryadi,\
    \ E. (2019). \nEffectiveness of ceramics water filter pots with addition of silver\
    \ nitrate to reduce \nof Escherichia coli contents. International Journal on Advanced\
    \ Science, \nEngineering \nand \nInformation \nTechnology, \n9(2), \n526–531.\
    \ \nhttps://\ndoi.org/10.18517/ijaseit.9.2.7142 \n \nKumar, S., Tiwari, P., &\
    \ Zymbler, M. (2019). Internet of Things is a revolutionary \napproach for future\
    \ technology enhancement: A review. Journal of Big Data, 6(1), \n111. https://doi.org/10.1186/s40537-019-0268-2\
    \ \n \nNawandar, N.K., & Satpute, V.R. (2019). IoT based low cost and intelligent\
    \ module for \nsmart irrigation system. Computers and Electronics in Agriculture,\
    \ 162(May), 979–\n990. https://doi.org/10.1016/j.compag.2019.05.027 \n \nPaustian,\
    \ M., & Theuvsen, L. (2017). Adoption of precision agriculture technologies by\
    \ \nGerman \ncrop \nfarmers. \nPrecision \nAgriculture, \n18(5), \n701–716. \n\
    https://\ndoi.org/10.1007/s11119-016-9482-5 \n \nPing, H., Wang, J., Ma, Z., &\
    \ Du, Y. (2018). Mini-review of application of iot technology \nin monitoring\
    \ agricultural products quality and safety. International Journal of \nAgricultural\
    \ and Biological Engineering, 11(5), 35–45. https://doi.org/10.25165/\nijabe.v11i5.3092\
    \ \n \nRay, P.P. (2017). Internet of things for smart agriculture: Technologies,\
    \ practices and \nfuture direction. Journal of Ambient Intelligence and Smart\
    \ Environments, 9, 395–\n420. https://doi.org/10.3233/AIS-170440 \nBafdal et al.\
    \ : Application of Internet of Things (IoT) on Microclimate  ... \n530 \n \nSaliem,\
    \ H.P., & Ariani, M. (2016). Ketahanan pangan, konsep, pengukuran dan strategi.\
    \ \nForum Penelitian Agro Ekonomi, 20(1), 12-24. https://doi.org/10.21082/\nfae.v20n1.2002.12-24\
    \ \n \nSugandi, W.K., Herwanto, T., & Yudi, A.P. (2018). Rancang bangun mesin\
    \ pembersih \ndan pengupas kentang. Agrikultura, 29(2), 110-118. https://doi.org/10.24198/\n\
    agrikultura. v29i2.20850 \n \nSujadi, H., & Nurhidayat, Y. (2019). Smart greenhouse\
    \ monitoring system based on \nInternet of Things. Jurnal J-Ensitec, 06(01), 371–377.\
    \ \n \nSurakusumah, A. P. (2009). Rancang Bangun Pengisi Botol Otomatis. [Undergraduate\
    \ \nThesis] Universitas Indonesia, Depok.  \n \nTzounis, A., Katsoulas, N., Bartzanas,\
    \ T., & Kittas, C. (2017). Internet of Things in \nagriculture, recent advances\
    \ and future challenges. Biosystems Engineering, 164, \n31–48. https://doi.org/https://doi.org/10.1016/j.biosystemseng.2017.09.007\
    \ \n \nZaida, Z., Ardiansah, I., & Rizky, M.A. (2017). Rancang bangun alat pengendali\
    \ suhu dan \nkelembaban relatif pada rumah kaca dengan informasi berbasis web.\
    \ Jurnal \nTeknotan, 11(1), 10-21. https://doi.org/10.24198/jt.vol11n1.2 \n \n\
    Zuraiyah, T.A., Suriansyah, M.I., & Akbar, A.P. (2019). Smart urban farming berbasis\
    \ \nInternet of Things ( IoT ). Information Management for Educators and \nProfessionals,\
    \ 3(2), 139–150. \n \n"
  inline_citation: '>'
  journal: Jurnal Teknik Pertanian
  limitations: '>'
  pdf_link: https://jurnal.fp.unila.ac.id/index.php/JTP/article/download/5701/pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Application of Internet of Things (IoT) on Microclimate Monitoring System
    in The ALG Unpad Greenhouse Based on Raspberry Pi
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.22214/ijraset.2023.49606
  analysis: '>'
  authors:
  - Palash Agrawal
  - Sankalp Bhagwate
  - Dhruv Singhaniya
  - Aditi Tamrakar
  - Prof. Puja S. Agrawal
  citation_count: 0
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: International Journal for Research in Applied Science and Engineering Technology
  limitations: '>'
  pdf_link: null
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Automated Hydroponics System
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
